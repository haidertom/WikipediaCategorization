{"id": "1140512", "url": "https://en.wikipedia.org/wiki?curid=1140512", "title": "Ahab (comics)", "text": "Ahab (comics)\n\nAhab (Dr. Roderick \"Rory\" Campbell) is a fictional supervillain appearing in American comic books published by Marvel Comics.\n\nDr. Roderick Campbell made his live action debut as the main antagonist of the first season of \"The Gifted\" television series portrayed by Garret Dillahunt. This version of the character was shown as the head researcher of Trask Industries.\n\nBased upon Herman Melville's Captain Ahab, but also named after Stan Lee's childhood friend Rory Campbell from Portland, Oregon, Ahab is a cyborg. Ahab's first appearance was a cameo in \"Fantastic Four Annual\" #23, as part of the \"Days of Future Present\" storyline.\n\nIn his future timeline, Ahab was the master of the Hounds, mind controlled mutants who hunted down other mutants for Ahab and his masters, the Sentinels. One of the Hounds, Rachel Summers, escaped from him and eventually time travelled to the present. Ahab followed her to the past and was defeated by the X-Men, X-Factor, the New Mutants and the Fantastic Four during the \"Days of Future Present\" crossover.\n\nLater issues of \"Excalibur\" revealed that scientist Rory Campbell was the man who would become Ahab in the future. Campbell learned of this in the present and tried to prevent this fate, but he was eventually merged with his future self. The merged Ahab joined the Four Horsemen of the immortal supervillain Apocalypse, taking up the position of \"Famine\" within that group. He aided in the capture of several of The Twelve, before being sent into an alternate dimension by Mikhail Rasputin.\n\nIn the not too distant future, he captured then killed Cannonball (who is immortal). He worked beside an armored figure, with light coming from his left eye. Due to time travel, it is not known at what point in Ahab's life this was.\n\nAhab joined the Red Skull as one of his S-Men to fulfill his role during World War Hate.\n\nDuring the \"AXIS\" storyline, Ahab was present when Magneto killed the S-Men.\n\nAhab can generate powerful energy harpoons that are formed from, and attached to, his own life force. Anyone who attempts to grab a harpoon gets burned. Each harpoon is keyed to its target's genetic structure, and cannot be moved or deflected by energy; it only stops when it strikes its target, or a close blood relative thereof. A hit from one of these harpoons can kill or seriously injure the target. A person who survives finds their neural pathways burned out and unable to move.\n\nAhab is a cyborg. His limbs are mostly of artificial construction and presumably this makes him superhumanly strong. So far he has found no need to engage in hand-to-hand combat due to his ability to use ranged attacks, his Hounds, and giant androids. He is largely immune to telepathic intrusion and attack, even from telepaths as powerful as Rachel Summers. Ahab's technology allows him to track, clothe and control Hounds.\n\nAs Famine, Ahab drains his victims' lifeforce resulting in emaciation and worse.\n\n\n"}
{"id": "2787", "url": "https://en.wikipedia.org/wiki?curid=2787", "title": "Astrobiology", "text": "Astrobiology\n\nAstrobiology is an interdisciplinary scientific field concerned with the origins, early evolution, distribution, and future of life in the universe. Astrobiology considers the question of whether extraterrestrial life exists, and how humans can detect it if it does. The term exobiology is similar.\n\nAstrobiology makes use of molecular biology, biophysics, biochemistry, chemistry, astronomy, physical cosmology, exoplanetology and geology to investigate the possibility of life on other worlds and help recognize biospheres that might be different from that on Earth. The origin and early evolution of life is an inseparable part of the discipline of astrobiology. Astrobiology concerns itself with interpretation of existing scientific data, and although speculation is entertained to give context, astrobiology concerns itself primarily with hypotheses that fit firmly into existing scientific theories.\n\nThis interdisciplinary field encompasses research on the origin of planetary systems, origins of organic compounds in space, rock-water-carbon interactions, abiogenesis on Earth, planetary habitability, research on biosignatures for life detection, and studies on the potential for life to adapt to challenges on Earth and in outer space.\n\nBiochemistry may have begun shortly after the Big Bang, 13.8 billion years ago, during a habitable epoch when the Universe was only 10–17 million years old. According to the panspermia hypothesis, microscopic life—distributed by meteoroids, asteroids and other small Solar System bodies—may exist throughout the universe. According to research published in August 2015, very large galaxies may be more favorable to the creation and development of habitable planets than such smaller galaxies as the Milky Way. Nonetheless, Earth is the only place in the universe humans know to harbor life. Estimates of habitable zones around other stars, sometimes referred to as \"Goldilocks zones,\" along with the discovery of hundreds of extrasolar planets and new insights into extreme habitats here on Earth, suggest that there may be many more habitable places in the universe than considered possible until very recently.\n\nCurrent studies on the planet Mars by the \"Curiosity\" and \"Opportunity\" rovers are searching for evidence of ancient life as well as plains related to ancient rivers or lakes that may have been habitable. The search for evidence of habitability, taphonomy (related to fossils), and organic molecules on the planet Mars is now a primary NASA and ESA objective.\n\nEven if extraterrestrial life is never discovered, the interdisciplinary nature of astrobiology, and the cosmic and evolutionary perspectives engendered by it, may still result in a range of benefits here on Earth. \n\nThe term was first proposed by the Russian (Soviet) astronomer Gavriil Tikhov in 1953. \"Astrobiology\" is etymologically derived from the Greek , \"astron\", \"constellation, star\"; , \"bios\", \"life\"; and , \"-logia\", \"study\". The synonyms of astrobiology are diverse; however, the synonyms were structured in relation to the most important sciences implied in its development: astronomy and biology. A close synonym is \"exobiology\" from the Greek , \"external\"; Βίος, \"bios\", \"life\"; and λογία, -logia, \"study\". The term exobiology was coined by molecular biologist and Nobel Prize winner Joshua Lederberg. Exobiology is considered to have a narrow scope limited to search of life external to Earth, whereas subject area of astrobiology is wider and investigates the link between life and the universe, which includes the search for extraterrestrial life, but also includes the study of life on Earth, its origin, evolution and limits. \n\nAnother term used in the past is xenobiology, (\"biology of the foreigners\") a word used in 1954 by science fiction writer Robert Heinlein in his work \"The Star Beast\". The term xenobiology is now used in a more specialized sense, to mean \"biology based on foreign chemistry\", whether of extraterrestrial or terrestrial (possibly synthetic) origin. Since alternate chemistry analogs to some life-processes have been created in the laboratory, xenobiology is now considered as an extant subject.\n\nWhile it is an emerging and developing field, the question of whether life exists elsewhere in the universe is a verifiable hypothesis and thus a valid line of scientific inquiry. Though once considered outside the mainstream of scientific inquiry, astrobiology has become a formalized field of study. Planetary scientist David Grinspoon calls astrobiology a field of natural philosophy, grounding speculation on the unknown, in known scientific theory. NASA's interest in exobiology first began with the development of the U.S. Space Program. In 1959, NASA funded its first exobiology project, and in 1960, NASA founded an Exobiology Program, which is now one of four main elements of NASA's current Astrobiology Program. In 1971, NASA funded the search for extraterrestrial intelligence (SETI) to search radio frequencies of the electromagnetic spectrum for interstellar communications transmitted by extraterrestrial life outside the Solar System. NASA's Viking missions to Mars, launched in 1976, included three biology experiments designed to look for metabolism of present life on Mars.\n\nAdvancements in the fields of astrobiology, observational astronomy and discovery of large varieties of extremophiles with extraordinary capability to thrive in the harshest environments on Earth, have led to speculation that life may possibly be thriving on many of the extraterrestrial bodies in the universe. A particular focus of current astrobiology research is the search for life on Mars due to this planet's proximity to Earth and geological history. There is a growing body of evidence to suggest that Mars has previously had a considerable amount of water on its surface, water being considered an essential precursor to the development of carbon-based life.\n\nMissions specifically designed to search for current life on Mars were the Viking program and Beagle 2 probes. The Viking results were inconclusive, and Beagle 2 failed minutes after landing. A future mission with a strong astrobiology role would have been the Jupiter Icy Moons Orbiter, designed to study the frozen moons of Jupiter—some of which may have liquid water—had it not been cancelled. In late 2008, the Phoenix lander probed the environment for past and present planetary habitability of microbial life on Mars, and researched the history of water there.\n\nThe European Space Agency's astrobiology roadmap from 2016, identified five main research topics, and specifies several key scientific objectives for each topic. The five research topics are: 1) Origin and evolution of planetary systems; 2) Origins of organic compounds in space; 3) Rock-water-carbon interactions, organic synthesis on Earth, and steps to life; 4) Life and habitability; 5) Biosignatures as facilitating life detection.\n\nIn November 2011, NASA launched the Mars Science Laboratory mission carrying the \"Curiosity\" rover, which landed on Mars at Gale Crater in August 2012. The \"Curiosity\" rover is currently probing the environment for past and present planetary habitability of microbial life on Mars. On 9 December 2013, NASA reported that, based on evidence from \"Curiosity\" studying Aeolis Palus, Gale Crater contained an ancient freshwater lake which could have been a hospitable environment for microbial life.\n\nThe European Space Agency is currently collaborating with the Russian Federal Space Agency (Roscosmos) and developing the ExoMars astrobiology rover, which is to be launched in July 2020. Meanwhile, NASA is developing the Mars 2020 astrobiology rover and sample cacher for a later return to Earth.\n\nWhen looking for life on other planets like Earth, some simplifying assumptions are useful to reduce the size of the task of the astrobiologist. One is the informed assumption that the vast majority of life forms in our galaxy are based on carbon chemistries, as are all life forms on Earth. Carbon is well known for the unusually wide variety of molecules that can be formed around it. Carbon is the fourth most abundant element in the universe and the energy required to make or break a bond is at just the appropriate level for building molecules which are not only stable, but also reactive. The fact that carbon atoms bond readily to other carbon atoms allows for the building of extremely long and complex molecules.\n\nThe presence of liquid water is an assumed requirement, as it is a common molecule and provides an excellent environment for the formation of complicated carbon-based molecules that could eventually lead to the emergence of life. Some researchers posit environments of water-ammonia mixtures as possible solvents for hypothetical types of biochemistry.\n\nA third assumption is to focus on planets orbiting Sun-like stars for increased probabilities of planetary habitability. Very large stars have relatively short lifetimes, meaning that life might not have time to emerge on planets orbiting them. Very small stars provide so little heat and warmth that only planets in very close orbits around them would not be frozen solid, and in such close orbits these planets would be tidally \"locked\" to the star. The long lifetimes of red dwarfs could allow the development of habitable environments on planets with thick atmospheres. This is significant, as red dwarfs are extremely common. (See Habitability of red dwarf systems).\n\nSince Earth is the only planet known to harbor life, there is no evident way to know if any of these simplifying assumptions are correct.\n\nResearch on communication with extraterrestrial intelligence (CETI) focuses on composing and deciphering messages that could theoretically be understood by another technological civilization. Communication attempts by humans have included broadcasting mathematical languages, pictorial systems such as the Arecibo message and computational approaches to detecting and deciphering 'natural' language communication. The SETI program, for example, uses both radio telescopes and optical telescopes to search for deliberate signals from an extraterrestrial intelligence.\n\nWhile some high-profile scientists, such as Carl Sagan, have advocated the transmission of messages, scientist Stephen Hawking warned against it, suggesting that aliens might simply raid Earth for its resources and then move on.\n\nMost astronomy-related astrobiology research falls into the category of extrasolar planet (exoplanet) detection, the hypothesis being that if life arose on Earth, then it could also arise on other planets with similar characteristics. To that end, a number of instruments designed to detect Earth-sized exoplanets have been considered, most notably NASA's Terrestrial Planet Finder (TPF) and ESA's Darwin programs, both of which have been cancelled. NASA launched the \"Kepler\" mission in March 2009, and the French Space Agency launched the COROT space mission in 2006. There are also several less ambitious ground-based efforts underway.\n\nThe goal of these missions is not only to detect Earth-sized planets, but also to directly detect light from the planet so that it may be studied spectroscopically. By examining planetary spectra, it would be possible to determine the basic composition of an extrasolar planet's atmosphere and/or surface. Given this knowledge, it may be possible to assess the likelihood of life being found on that planet. A NASA research group, the Virtual Planet Laboratory, is using computer modeling to generate a wide variety of virtual planets to see what they would look like if viewed by TPF or Darwin. It is hoped that once these missions come online, their spectra can be cross-checked with these virtual planetary spectra for features that might indicate the presence of life.\n\nAn estimate for the number of planets with intelligent \"communicative\" extraterrestrial life can be gleaned from the Drake equation, essentially an equation expressing the probability of intelligent life as the product of factors such as the fraction of planets that might be habitable and the fraction of planets on which life might arise:\nwhere:\n\nHowever, whilst the rationale behind the equation is sound, it is unlikely that the equation will be constrained to reasonable limits of error any time soon. The problem with the formula is that it is not usable to generate or support hypotheses because it contains factors that can never be verified. The first term, R*, number of stars, is generally constrained within a few orders of magnitude. The second and third terms, \"f\", stars with planets and \"f\", planets with habitable conditions, are being evaluated for the star's neighborhood. Drake originally formulated the equation merely as an agenda for discussion at the Green Bank conference, but some applications of the formula had been taken literally and related to simplistic or pseudoscientific arguments. Another associated topic is the Fermi paradox, which suggests that if intelligent life is common in the universe, then there should be obvious signs of it.\n\nAnother active research area in astrobiology is planetary system formation. It has been suggested that the peculiarities of the Solar System (for example, the presence of Jupiter as a protective shield) may have greatly increased the probability of intelligent life arising on our planet.\n\nBiology cannot state that a process or phenomenon, by being mathematically possible, has to exist forcibly in an extraterrestrial body. Biologists specify what is speculative and what is not.\n\nUntil the 1970s, life was thought to be entirely dependent on energy from the Sun. Plants on Earth's surface capture energy from sunlight to photosynthesize sugars from carbon dioxide and water, releasing oxygen in the process that is then consumed by oxygen-respiring organisms, passing their energy up the food chain. Even life in the ocean depths, where sunlight cannot reach, was thought to obtain its nourishment either from consuming organic detritus rained down from the surface waters or from eating animals that did. The world's ability to support life was thought to depend on its access to sunlight. However, in 1977, during an exploratory dive to the Galapagos Rift in the deep-sea exploration submersible \"Alvin\", scientists discovered colonies of giant tube worms, clams, crustaceans, mussels, and other assorted creatures clustered around undersea volcanic features known as black smokers. These creatures thrive despite having no access to sunlight, and it was soon discovered that they comprise an entirely independent ecosystem. Although most of these multicellular lifeforms need dissolved oxygen (produced by oxygenic photosynthesis) for their aerobic cellular respiration and thus are not completely independent from sunlight by themselves, the basis for their food chain is a form of bacterium that derives its energy from oxidization of reactive chemicals, such as hydrogen or hydrogen sulfide, that bubble up from the Earth's interior. Other lifeforms entirely decoupled from the energy from sunlight are green sulphur bacteria which are capturing geothermal light for anoxygenic photosynthesis or bacteria running chemolithoautotrophy based on the radioactive decay of uranium. This chemosynthesis revolutionized the study of biology and astrobiology by revealing that life need not be sun-dependent; it only requires water and an energy gradient in order to exist.\n\nExtremophiles, organisms able to survive in extreme environments, are a core research element for astrobiologists. Such organisms include biota which are able to survive several kilometers below the ocean's surface near hydrothermal vents and microbes that thrive in highly acidic environments. It is now known that extremophiles thrive in ice, boiling water, acid, alkali, the water core of nuclear reactors, salt crystals, toxic waste and in a range of other extreme habitats that were previously thought to be inhospitable for life. This opened up a new avenue in astrobiology by massively expanding the number of possible extraterrestrial habitats. Characterization of these organisms, their environments and their evolutionary pathways, is considered a crucial component to understanding how life might evolve elsewhere in the universe. For example, some organisms able to withstand exposure to the vacuum and radiation of outer space include the lichen fungi \"Rhizocarpon geographicum\" and \"Xanthoria elegans\", the bacterium \"Bacillus safensis\", \"Deinococcus radiodurans\", \"Bacillus subtilis\", yeast \"Saccharomyces cerevisiae\", seeds from \"Arabidopsis thaliana\" ('mouse-ear cress'), as well as the invertebrate animal Tardigrade. While Tardigrades are not considered true extremophiles, they are considered extremotolerant microorganisms that have contributed to the field of astrobiology. Their extreme radiation tolerance and presence of DNA protection proteins may provide answers as to whether life can survive away from the protection of the Earth’s atmosphere.\n\nJupiter's moon, Europa, and Saturn's moon, Enceladus, are now considered the most likely locations for extant extraterrestrial life in the Solar System due to their subsurface water oceans where radiogenic and tidal heating enables liquid water to exist.\n\nThe origin of life, known as abiogenesis, distinct from the evolution of life, is another ongoing field of research. Oparin and Haldane postulated that the conditions on the early Earth were conducive to the formation of organic compounds from inorganic elements and thus to the formation of many of the chemicals common to all forms of life we see today. The study of this process, known as prebiotic chemistry, has made some progress, but it is still unclear whether or not life could have formed in such a manner on Earth. The alternative hypothesis of panspermia is that the first elements of life may have formed on another planet with even more favorable conditions (or even in interstellar space, asteroids, etc.) and then have been carried over to Earth—the panspermia hypothesis.\n\nThe cosmic dust permeating the universe contains complex organic compounds (\"amorphous organic solids with a mixed aromatic-aliphatic structure\") that could be created naturally, and rapidly, by stars. Further, a scientist suggested that these compounds may have been related to the development of life on Earth and said that, \"If this is the case, life on Earth may have had an easier time getting started as these organics can serve as basic ingredients for life.\" In September 2012, NASA scientists reported that polycyclic aromatic hydrocarbons (PAHs), subjected to interstellar medium conditions, are transformed through hydrogenation, oxygenation and hydroxylation, to more complex organics – \"a step along the path toward amino acids and nucleotides, the raw materials of proteins and DNA, respectively\".\n\nMore than 20% of the carbon in the universe may be associated with PAHs, possible starting materials for the formation of life. PAHs seem to have been formed shortly after the Big Bang, are widespread throughout the universe, and are associated with new stars and exoplanets.\n\nAstroecology concerns the interactions of life with space environments and resources, in planets, asteroids and comets. On a larger scale, astroecology concerns resources for life about stars in the galaxy through the cosmological future. Astroecology attempts to quantify future life in space, addressing this area of astrobiology.\n\nExperimental astroecology investigates resources in planetary soils, using actual space materials in meteorites. The results suggest that Martian and carbonaceous chondrite materials can support bacteria, algae and plant (asparagus, potato) cultures, with high soil fertilities. The results support that life could have survived in early aqueous asteroids and on similar materials imported to Earth by dust, comets and meteorites, and that such asteroid materials can be used as soil for future space colonies.\n\nOn the largest scale, cosmoecology concerns life in the universe over cosmological times. The main sources of energy may be red giant stars and white and red dwarf stars, sustaining life for 10 years. Astroecologists suggest that their mathematical models may quantify the potential amounts of future life in space, allowing a comparable expansion in biodiversity, potentially leading to diverse intelligent life forms.\n\nAstrogeology is a planetary science discipline concerned with the geology of celestial bodies such as the planets and their moons, asteroids, comets, and meteorites. The information gathered by this discipline allows the measure of a planet's or a natural satellite's potential to develop and sustain life, or planetary habitability.\n\nAn additional discipline of astrogeology is geochemistry, which involves study of the chemical composition of the Earth and other planets, chemical processes and reactions that govern the composition of rocks and soils, the cycles of matter and energy and their interaction with the hydrosphere and the atmosphere of the planet. Specializations include cosmochemistry, biochemistry and organic geochemistry.\n\nThe fossil record provides the oldest known evidence for life on Earth. By examining the fossil evidence, paleontologists are able to better understand the types of organisms that arose on the early Earth. Some regions on Earth, such as the Pilbara in Western Australia and the McMurdo Dry Valleys of Antarctica, are also considered to be geological analogs to regions of Mars, and as such, might be able to provide clues on how to search for past life on Mars.\n\nThe various organic functional groups, composed of hydrogen, oxygen, nitrogen, phosphorus, sulfur, and a host of metals, such as iron, magnesium, and zinc, provide the enormous diversity of chemical reactions necessarily catalyzed by a living organism. Silicon, in contrast, interacts with only a few other atoms, and the large silicon molecules are monotonous compared with the combinatorial universe of organic macromolecules. Indeed, it seems likely that the basic building blocks of life anywhere will be similar those on Earth, in the generality if not in the detail. Although terrestrial life and life that might arise independently of Earth are expected to use many similar, if not identical, building blocks, they also are expected to have some biochemical qualities that are unique. If life has had a comparable impact elsewhere in the Solar System, the relative abundances of chemicals key for its survival – whatever they may be – could betray its presence. Whatever extraterrestrial life may be, its tendency to chemically alter its environment might just give it away.\n\nPeople have long speculated about the possibility of life in settings other than Earth, however, speculation on the nature of life elsewhere often has paid little heed to constraints imposed by the nature of biochemistry. The likelihood that life throughout the universe is probably carbon-based is suggested by the fact that carbon is one of the most abundant of the higher elements. Only two of the natural atoms, carbon and silicon, are known to serve as the backbones of molecules sufficiently large to carry biological information. As the structural basis for life, one of carbon's important features is that unlike silicon, it can readily engage in the formation of chemical bonds with many other atoms, thereby allowing for the chemical versatility required to conduct the reactions of biological metabolism and propagation.\n\nThought on where in the Solar System life might occur, was limited historically by the understanding that life relies ultimately on light and warmth from the Sun and, therefore, is restricted to the surfaces of planets. The three most likely candidates for life in the Solar System are the planet Mars, the Jovian moon Europa, and Saturn's moons Titan, and Enceladus.\n\nMars, Enceladus and Europa are considered likely candidates in the search for life primarily because they may have underground liquid water, a molecule essential for life as we know it for its use as a solvent in cells. Water on Mars is found frozen in its polar ice caps, and newly carved gullies recently observed on Mars suggest that liquid water may exist, at least transiently, on the planet's surface. At the Martian low temperatures and low pressure, liquid water is likely to be highly saline. As for Europa, liquid water likely exists beneath the moon's icy outer crust. This water may be warmed to a liquid state by volcanic vents on the ocean floor, but the primary source of heat is probably tidal heating. On 11 December 2013, NASA reported the detection of \"clay-like minerals\" (specifically, phyllosilicates), often associated with organic materials, on the icy crust of Europa. The presence of the minerals may have been the result of a collision with an asteroid or comet according to the scientists.\n\nAnother planetary body that could potentially sustain extraterrestrial life is Saturn's largest moon, Titan. Titan has been described as having conditions similar to those of early Earth. On its surface, scientists have discovered the first liquid lakes outside Earth, but these lakes seem to be composed of ethane and/or methane, not water. Some scientists think it possible that these liquid hydrocarbons might take the place of water in living cells different from those on Earth. After Cassini data was studied, it was reported on March 2008 that Titan may also have an underground ocean composed of liquid water and ammonia. Additionally, Saturn's moon Enceladus may have an ocean below its icy surface and, according to NASA scientists in May 2011, \"is emerging as the most habitable spot beyond Earth in the Solar System for life as we know it\". On 27 June 2018, astronomers reported the detection of complex macromolecular organics on Enceladus. \n\nMeasuring the ratio of hydrogen and methane levels on Mars may help determine the likelihood of life on Mars. According to the scientists, \"...low H/CH ratios (less than approximately 40) indicate that life is likely present and active.\" Other scientists have recently reported methods of detecting hydrogen and methane in extraterrestrial atmospheres.\n\nComplex organic compounds of life, including uracil, cytosine and thymine, have been formed in a laboratory under outer space conditions, using starting chemicals such as pyrimidine, found in meteorites. Pyrimidine, like polycyclic aromatic hydrocarbons (PAHs), is the most carbon-rich chemical found in the universe.\n\nThe Rare Earth hypothesis postulates that multicellular life forms found on Earth may actually be more of a rarity than scientists assume. It provides a possible answer to the Fermi paradox which suggests, \"If extraterrestrial aliens are common, why aren't they obvious?\" It is apparently in opposition to the principle of mediocrity, assumed by famed astronomers Frank Drake, Carl Sagan, and others. The Principle of Mediocrity suggests that life on Earth is not exceptional, but rather that life is more than likely to be found on innumerable other worlds.\n\nThe systematic search for possible life outside Earth is a valid multidisciplinary scientific endeavor. However, hypotheses and predictions as to its existence and origin vary widely, and at the present, the development of hypotheses firmly grounded on science may be considered astrobiology's most concrete practical application. It has been proposed that viruses are likely to be encountered on other life-bearing planets.\n\n, no evidence of extraterrestrial life has been identified. Examination of the Allan Hills 84001 meteorite, which was recovered in Antarctica in 1984 and originated from Mars, is thought by David McKay, as well as few other scientists, to contain microfossils of extraterrestrial origin; this interpretation is controversial.\n\nYamato 000593, the second largest meteorite from Mars, was found on Earth in 2000. At a microscopic level, spheres are found in the meteorite that are rich in carbon compared to surrounding areas that lack such spheres. The carbon-rich spheres may have been formed by biotic activity according to some NASA scientists.\n\nOn 5 March 2011, Richard B. Hoover, a scientist with the Marshall Space Flight Center, speculated on the finding of alleged microfossils similar to cyanobacteria in CI1 carbonaceous meteorites in the fringe \"Journal of Cosmology\", a story widely reported on by mainstream media. However, NASA formally distanced itself from Hoover's claim. According to American astrophysicist Neil deGrasse Tyson: \"At the moment, life on Earth is the only known life in the universe, but there are compelling arguments to suggest we are not alone.\"\n\n\nOn 17 March 2013, researchers reported that microbial life forms thrive in the Mariana Trench, the deepest spot on the Earth. Other researchers reported that microbes thrive inside rocks up to below the sea floor under of ocean off the coast of the northwestern United States. According to one of the researchers, \"You can find microbes everywhere—they're extremely adaptable to conditions, and survive wherever they are.\" These finds expand the potential habitability of certain niches of other planets.\n\nIn 2004, the spectral signature of methane () was detected in the Martian atmosphere by both Earth-based telescopes as well as by the \"Mars Express\" orbiter. Because of solar radiation and cosmic radiation, methane is predicted to disappear from the Martian atmosphere within several years, so the gas must be actively replenished in order to maintain the present concentration. On June 7, 2018, NASA announced a cyclical seasonal variation in atmospheric methane, which may be produced by geological or biological sources. The European ExoMars Trace Gas Orbiter is currently measuring and mapping the atmospheric methane.\n\nIt is possible that some exoplanets may have moons with solid surfaces or liquid oceans that are hospitable. Most of the planets so far discovered outside the Solar System are hot gas giants thought to be inhospitable to life, so it is not yet known whether the Solar System, with a warm, rocky, metal-rich inner planet such as Earth, is of an aberrant composition. Improved detection methods and increased observation time will undoubtedly discover more planetary systems, and possibly some more like ours. For example, NASA's Kepler Mission seeks to discover Earth-sized planets around other stars by measuring minute changes in the star's light curve as the planet passes between the star and the spacecraft. Progress in infrared astronomy and submillimeter astronomy has revealed the constituents of other star systems.\n\n\nEfforts to answer questions such as the abundance of potentially habitable planets in habitable zones and chemical precursors have had much success. Numerous extrasolar planets have been detected using the wobble method and transit method, showing that planets around other stars are more numerous than previously postulated. The first Earth-sized extrasolar planet to be discovered within its star's habitable zone is Gliese 581 c.\n\nResearch into the environmental limits of life and the workings of extreme ecosystems is ongoing, enabling researchers to better predict what planetary environments might be most likely to harbor life. Missions such as the \"Phoenix\" lander, Mars Science Laboratory, ExoMars, Mars 2020 rover to Mars, and the \"Cassini\" probe to Saturn's moons aim to further explore the possibilities of life on other planets in the Solar System.\n\nThe two Viking landers each carried four types of biological experiments to the surface of Mars in the late 1970s. These were the only Mars landers to carry out experiments looking specifically for metabolism by current microbial life on Mars. The landers used a robotic arm to collect soil samples into sealed test containers on the craft. The two landers were identical, so the same tests were carried out at two places on Mars' surface; Viking 1 near the equator and Viking 2 further north. The result was inconclusive, and is still disputed by some scientists.\n\n\"Beagle 2\" was an unsuccessful British Mars lander that formed part of the European Space Agency's 2003 Mars Express mission. Its primary purpose was to search for signs of life on Mars, past or present. Although it landed safely, it was unable to correctly deploy its solar panels and telecom antenna.\n\nEXPOSE is a multi-user facility mounted in 2008 outside the International Space Station dedicated to astrobiology. EXPOSE was developed by the European Space Agency (ESA) for long-term spaceflights that allow exposure of organic chemicals and biological samples to outer space in low Earth orbit.\n\nThe Mars Science Laboratory (MSL) mission landed the \"Curiosity\" rover that is currently in operation on Mars. It was launched 26 November 2011, and landed at Gale Crater on 6 August 2012. Mission objectives are to help assess Mars' habitability and in doing so, determine whether Mars is or has ever been able to support life, collect data for a future human mission, study Martian geology, its climate, and further assess the role that water, an essential ingredient for life as we know it, played in forming minerals on Mars.\n\nThe \"Tanpopo\" mission is an orbital astrobiology experiment investigating the potential interplanetary transfer of life, organic compounds, and possible terrestrial particles in the low Earth orbit. The purpose is to assess the panspermia hypothesis and the possibility of natural interplanetary transport of microbial life as well as prebiotic organic compounds. Early mission results show evidence that some clumps of microorganism can survive for at least one year in space. This may support the idea that clumps greater than 0.5 millimeters of microorganisms could be one way for life to spread from planet to planet.\n\n\n\"ExoMars rover\" is a robotic mission to Mars to search for possible biosignatures of Martian life, past or present. This astrobiological mission is currently under development by the European Space Agency (ESA) in partnership with the Russian Federal Space Agency (Roscosmos); it is planned for a 2018 launch.\n\n\"Mars 2020\" rover mission is under development by NASA for a launch in 2020. It will investigate environments on Mars relevant to astrobiology, investigate its surface geological processes and history, including the assessment of its past habitability and potential for preservation of biosignatures and biomolecules within accessible geological materials. The Science Definition Team is proposing the rover collect and package at least 31 samples of rock cores and soil for a later mission to bring back for more definitive analysis in laboratories on Earth. The rover could make measurements and technology demonstrations to help designers of a human expedition understand any hazards posed by Martian dust and demonstrate how to collect carbon dioxide (CO), which could be a resource for making molecular oxygen (O) and rocket fuel.\n\n\"Europa Clipper\" is a mission planned by NASA for a 2025 launch that will conduct detailed reconnaissance of Jupiter's moon Europa and will investigate whether its internal ocean could harbor conditions suitable for life. It will also aid in the selection of future landing sites.\n\n\"Icebreaker Life\" is a lander mission that proposed for NASA's Discovery Program for the 2021 launch opportunity, but it was not selected for development. It would have had a stationary lander that would be a near copy of the successful 2008 \"Phoenix\" and it would have carried an upgraded astrobiology scientific payload, including a 1-meter-long core drill to sample ice-cemented ground in the northern plains to conduct a search for organic molecules and evidence of current or past life on Mars. One of the key goals of the \"Icebreaker Life\" mission is to test the hypothesis that the ice-rich ground in the polar regions has significant concentrations of organics due to protection by the ice from oxidants and radiation.\n\n\"Journey to Enceladus and Titan\" (\"JET\") is an astrobiology mission concept to assess the habitability potential of Saturn's moons Enceladus and Titan by means of an orbiter.\n\n\"Enceladus Life Finder\" (\"ELF\") is a proposed astrobiology mission concept for a space probe intended to assess the habitability of the internal aquatic ocean of Enceladus, Saturn's sixth-largest moon.\n\n\"Life Investigation For Enceladus\" (\"LIFE\") is a proposed astrobiology sample-return mission concept. The spacecraft would enter into Saturn orbit and enable multiple flybys through Enceladus' icy plumes to collect icy plume particles and volatiles and return them to Earth on a capsule. The spacecraft may sample Enceladus' plumes, the E ring of Saturn, and the upper atmosphere of Titan.\n\n\"Oceanus\" is an orbiter proposed in 2017 for the New Frontiers mission #4. It would travel to the moon of Saturn, Titan, to assess its habitability. \"Oceanus\" objectives are to reveal Titan's organic chemistry, geology, gravity, topography, collect 3D reconnaissance data, catalog the organics and determine where they may interact with liquid water.\n\n\"Explorer of Enceladus and Titan\" (ET) is an orbiter mission concept that would investigate the evolution and habitability of the Saturnian satellites Enceladus and Titan. The mission concept was proposed in 2017 by the European Space Agency.\n\n\n\n"}
{"id": "2593857", "url": "https://en.wikipedia.org/wiki?curid=2593857", "title": "Basilar skull fracture", "text": "Basilar skull fracture\n\nA basilar skull fracture is a break of a bone in the base of the skull. Symptoms may include bruising behind the ears, bruising around the eyes, or blood behind the ear drum. A cerebrospinal fluid (CSF) leak occurs in about 20% of cases and can result in fluid leaking from the nose or ear. Meningitis is a complication in about 14% of cases. Other complications include cranial nerve or blood vessel injury.\nThey typically require a significant degree of trauma to occur. The break is of at least one of the following bones: temporal bone, occipital bone, sphenoid bone, frontal bone, or ethmoid bone. They are divided into anterior fossa, middle fossa, and posterior fossa fractures. Facial fractures often also occur. Diagnosis is typically by CT scan.\nTreatment is generally based on the injury to structures inside the head. Surgery may be done for a CSF leak that does not stop or an injury to a blood vessel or nerve. Preventative antibiotics are of unclear use. It occurs in about 12% of people with a severe head injury.\n\n\nBasilar skull fractures include breaks in the posterior skull base or anterior skull base. The former involve the occipital bone, temporal bone, and portions of the sphenoid bone; the latter, superior portions of the sphenoid and ethmoid bones. The temporal bone fracture is encountered in 75% of all basilar skull fractures and may be longitudinal, transverse or mixed, depending on the course of the fracture line in relation to the longitudinal axis of the pyramid.\n\nBones may be broken around the foramen magnum, the hole in the base of the skull through which the brain stem exits and becomes the spinal cord, creating the risk that blood vessels and nerves exiting the hole may be damaged.\n\nDue to the proximity of the cranial nerves, injury to those nerves may occur. This can cause loss of function of the facial nerve or oculomotor nerve or hearing loss due to damage of cranial nerve VIII.\n\nEvidence does not support the use of preventative antibiotics regardless of the presence of a cerebrospinal fluid leak.\n\nNon-displaced fractures usually heal without intervention. Patients with basilar skull fractures are especially likely to get meningitis. Unfortunately, the efficacy of prophylactic antibiotics in these cases is uncertain.\n\nAcute injury to the internal carotid artery (carotid dissection, occlusion, pseudoaneurysm formation) may be asymptomatic or result in life-threatening bleeding. They are almost exclusively observed when the carotid canal is fractured, although only a minority of carotid canal fractures result in vascular injury. Involvement of the petrous segment of the carotid canal is associated with a relatively high incidence of carotid injury.\n\nBasilar skull fractures are a common cause of death in many motor racing accidents. Drivers who have died as a result of basilar skull fractures include Formula One drivers Roland Ratzenberger and Ayrton Senna; IndyCar drivers Bill Vukovich Sr., Tony Bettenhausen Sr., Floyd Roberts, and Scott Brayton; NASCAR drivers Dale Earnhardt Sr., Adam Petty, Tony Roper, Kenny Irwin Jr., Neil Bonnett, John Nemechek, J.D. McDuffie, and Richie Evans; CART drivers Jovy Marcelo, Greg Moore, and Gonzalo Rodriguez; and ARCA drivers Blaise Alexander and Slick Johnson.\n\nTo prevent this injury many motor sports sanctioning bodies mandate the use of head and neck restraints, such as the HANS device. The HANS device has demonstrated its life-saving abilities multiple times, including Jeff Gordon at the 2006 Pocono 500, Michael McDowell at the Texas Motor Speedway in 2008, Robert Kubica at the 2007 Canadian Grand Prix, and Elliott Sadler at the Pocono Red Cross 500 in 2010.\n"}
{"id": "1231522", "url": "https://en.wikipedia.org/wiki?curid=1231522", "title": "Biological immortality", "text": "Biological immortality\n\nBiological immortality (sometimes referred to \"bio-indefinite\" mortality) is a state in which the rate of mortality from senescence is stable or decreasing, thus decoupling it from chronological age. Various unicellular and multicellular species, including some vertebrates, achieve this state either throughout their existence or after living long enough. A biologically immortal living being can still die from means other than senescence, such as through injury or disease.\n\nThis definition of immortality has been challenged in the \"Handbook of the Biology of Aging\", because the increase in rate of mortality as a function of chronological age may be negligible at extremely old ages, an idea referred to as the late-life mortality plateau. The rate of mortality may cease to increase in old age, but in most cases that rate is typically very high. As a hypothetical example, there is only a 50% chance of a human surviving another year at age 110 or greater.\n\nThe term is also used by biologists to describe cells that are not subject to the Hayflick limit on how many times they can divide.\n\nBiologists chose the word \"immortal\" to designate cells that are not subject to the Hayflick limit, the point at which cells can no longer divide due to DNA damage or shortened telomeres. Prior to Leonard Hayflick's theory, Alexis Carrel hypothesized that all normal somatic cells were immortal.\n\nThe term \"immortalization\" was first applied to cancer cells that expressed the telomere-lengthening enzyme telomerase, and thereby avoided apoptosis—i.e. cell death caused by intracellular mechanisms. Among the most commonly used cell lines are HeLa and Jurkat, both of which are immortalized cancer cell lines. HeLa cells originated from a sample of cervical cancer taken from Henrietta Lacks in 1951. These cells have been and still are widely used in biological research such as creation of the polio vaccine, sex hormone steroid research, and cell metabolism. Normal stem cells and germ cells can also be said to be immortal (when humans refer to the cell line).\n\nImmortal cell lines of cancer cells can be created by induction of oncogenes or loss of tumor suppressor genes. One way to induce immortality is through viral-mediated induction of the large T‑antigen, commonly introduced through simian virus 40 (SV-40).\n\nAccording to the Animal Aging and Longevity Database, the list of organisms with negligible aging (along with estimated longevity in the wild) includes:\n\n\nIn 2018, scientists working for Calico, a company owned by Alphabet, published a paper in the journal \"eLife\" which presents possible evidence that Heterocephalus glaber (Naked mole rat) do not face increased mortality risk due to aging.\n\nMany unicellular organisms age: as time passes, they divide more slowly and ultimately die. Asymmetrically dividing bacteria and yeast also age. However, symmetrically dividing bacteria and yeast can be biologically immortal under ideal growing conditions. In these conditions, when a cell splits symmetrically to produce two daughter cells, the process of cell division can restore the cell to a youthful state. However, if the parent asymmetrically buds off a daughter only the daughter is reset to the youthful state—the parent isn't restored and will go on to age and die. In a similar manner stem cells and gametes can be regarded as \"immortal\".\n\nHydras are a genus of the Cnidaria phylum. All cnidarians can regenerate, allowing them to recover from injury and to reproduce asexually. Hydras are simple, freshwater animals possessing radial symmetry and no post-mitotic cells. All hydra cells continually divide. It has been suggested that hydras do not undergo senescence, and, as such, are biologically immortal. In a four-year study, 3 cohorts of hydra did not show an increase in mortality with age. It is possible that these animals live much longer, considering that they reach maturity in 5 to 10 days. However, this does not explain how hydras are consequently able to maintain telomere lengths.\n\n\"Turritopsis dohrnii\", or \"Turritopsis nutricula\", is a small () species of jellyfish that uses transdifferentiation to replenish cells after sexual reproduction. This cycle can repeat indefinitely, potentially rendering it biologically immortal. This organism originated in the Caribbean sea, but has now spread around the world. Similar cases include hydrozoan \"Laodicea undulata\" and scyphozoan \"Aurelia\" sp.1.\n\nResearch suggests that lobsters may not slow down, weaken, or lose fertility with age, and that older lobsters may be more fertile than younger lobsters. This does not however make them immortal in the traditional sense, as they are significantly more likely to die at a shell moult the older they get (as detailed below).\n\nTheir longevity may be due to telomerase, an enzyme that repairs long repetitive sections of DNA sequences at the ends of chromosomes, referred to as telomeres. Telomerase is expressed by most vertebrates during embryonic stages but is generally absent from adult stages of life. However, unlike vertebrates, lobsters express telomerase as adults through most tissue, which has been suggested to be related to their longevity. Contrary to popular belief, lobsters are not immortal. Lobsters grow by moulting which requires a lot of energy, and the larger the shell the more energy is required. Eventually, the lobster will die from exhaustion during a moult. Older lobsters are also known to stop moulting, which means that the shell will eventually become damaged, infected, or fall apart and they die. The European lobster has an average life span of 31 years for males and 54 years for females.\n\nPlanarian flatworms have both sexually and asexually reproducing types. Studies on genus Schmidtea mediterranea suggest these planarians appear to regenerate (i.e. heal) indefinitely, and asexual individuals have an \"apparently limitless [telomere] regenerative capacity fueled by a population of highly proliferative adult stem cells\". \"Both asexual and sexual animals display age-related decline in telomere length; however, asexual animals are able to maintain telomere lengths somatically (i.e. during reproduction by fission or when regeneration is induced by amputation), whereas sexual animals restore telomeres by extension during sexual reproduction or during embryogenesis like other sexual species. Homeostatic telomerase activity observed in both asexual and sexual animals is not sufficient to maintain telomere length, whereas the increased activity in regenerating asexuals is sufficient to renew telomere length... \"\n\nLifespan: For sexually reproducing planaria: \"the lifespan of individual planarian can be as long as 3 years, likely due to the ability of neoblasts to constantly replace aging cells\". Whereas for asexually reproducing planaria: \"individual animals in clonal lines of some planarian species replicating by fission have been maintained for over 15 years\". They do not live forever.\n\nAlthough the premise that biological aging can be halted or reversed by foreseeable technology remains controversial, research into developing possible therapeutic interventions is underway. Among the principal drivers of international collaboration in such research is the SENS Research Foundation, a non-profit organization that advocates a number of what it claims are plausible research pathways that might lead to engineered negligible senescence in humans.\n\nIn 2015, Elizabeth Parrish, CEO of BioViva, treated herself using gene therapy, with the goal of not just halting, but reversing aging. She has since reported feeling more energetic, and no obvious negative side effects have been noticed.\n\nFor several decades, researchers have also pursued various forms of suspended animation as a means by which to indefinitely extend mammalian lifespan. Some scientists have voiced support for the feasibility of the cryopreservation of humans, known as cryonics. Cryonics is predicated on the concept that some people considered clinically dead by today's medicolegal standards are not actually dead according to information-theoretic death and can, in principle, be resuscitated given sufficient technological advances. The goal of current cryonics procedures is tissue vitrification, a technique first used to reversibly cryopreserve a viable whole organ in 2005.\n\nSimilar proposals involving suspended animation include chemical brain preservation. The non-profit Brain Preservation Foundation offers a cash prize valued at over $100,000 for demonstrations of techniques that would allow for high-fidelity, long-term storage of a mammalian brain.\n\nIn 2016, scientists at the Buck Institute for Research on Aging and the Mayo Clinic employed genetic and pharmacological approaches to ablate pro-aging senescent cells, extending healthy lifespan of mice by over 25%. The startup Unity Biotechnology is further developing this strategy in human clinical trials.\n\nIn early 2017, Harvard scientists headed by biologist David Sinclair announced they have tested a metabolic precursor that increases NAD+ levels in mice and have successfully reversed the cellular aging process and can protect the DNA from future damage. \"The old mouse and young mouse cells are indistinguishable\", David was quoted. Human trials are to begin shortly in what the team expect is 6 months at Brigham and Women's Hospital, in Boston.\n\nTo achieve the more limited goal of halting the increase in mortality rate with age, a solution must be found to the fact that any intervention to remove senescent cells that creates competition among cells will increase age-related mortality from cancer.\n\nIn 2012 in Russia, and then in the United States, Israel, and the Netherlands, pro-immortality transhumanist political parties were launched. They aim to provide political support to anti-aging and radical life extension research and technologies and want to ensure the fastest possible—and at the same time, the least disruptive—societal transition to radical life extension, life without aging, and ultimately, immortality. They aim to make it possible to provide access to such technologies to the majority of people alive today.\n\nFuture advances in nanomedicine could give rise to life extension through the repair of many processes thought to be responsible for aging. K. Eric Drexler, one of the founders of nanotechnology, postulated cell repair devices, including ones operating within cells and utilizing as yet hypothetical molecular machines, in his 1986 book Engines of Creation. Raymond Kurzweil, a futurist and transhumanist, stated in his book \"The Singularity Is Near\" that he believes that advanced medical nanorobotics could completely remedy the effects of aging by 2030. According to Richard Feynman, it was his former graduate student and collaborator Albert Hibbs who originally suggested to him (circa 1959) the idea of a \"medical\" use for Feynman's theoretical micromachines (see biological machine). Hibbs suggested that certain repair machines might one day be reduced in size to the point that it would, in theory, be possible to (as Feynman put it) \"swallow the doctor\". The idea was incorporated into Feynman's 1959 essay \"There's Plenty of Room at the Bottom.\"\n\n\n"}
{"id": "1006293", "url": "https://en.wikipedia.org/wiki?curid=1006293", "title": "Biorobotics", "text": "Biorobotics\n\nBiorobotics is a term that loosely covers the fields of cybernetics, bionics and even genetic engineering as a collective study.\n\nBiorobotics is often used to refer to a real subfield of robotics: studying how to make robots that emulate or simulate living biological organisms mechanically or even chemically. \n\nThe term is also used in a reverse definition: making biological organisms as manipulatable and functional as robots, or making biological organisms as components of robots. In the latter sense, biorobotics can be referred to as a theoretical discipline of comprehensive genetic engineering in which organisms are created and designed by artificial means. The creation of life from non-living matter for example, would be biorobotics. The field is in its infancy and is sometimes known as synthetic biology or bionanotechnology.\n\nBio-inspired robotics is the practice of making robots that are inspired by real biological systems, while being simpler and more effective. In contrast, the resemblance of animatronics to biological organisms is usually only in general shape and form.\n\nOrel V.E. invented the device of mechanochemiemission microbiorobotics. The phenomenon of mechanochemiemission is related to the processes interconversion of mechanical, chemical, electromagnetic energy in the mitochondria. Microbiorobot may be used for treatment of cancer patients.\n\nA biological brain, grown from cultured neurons which were originally separated, has been developed as the neurological entity subsequently embodied within a robot body by Kevin Warwick and his team at University of Reading. The brain receives input from sensors on the robot body and the resultant output from the brain provides the robot's only motor signals. The biological brain is the only brain of the robot.\n\n"}
{"id": "496995", "url": "https://en.wikipedia.org/wiki?curid=496995", "title": "Bristol-Myers Squibb", "text": "Bristol-Myers Squibb\n\nBristol-Myers Squibb Company (BMS) is an American pharmaceutical company, headquartered in New York City.\n\nBristol-Myers Squibb manufactures prescription pharmaceuticals and biologics in several therapeutic areas, including cancer, HIV/AIDS, cardiovascular disease, diabetes, hepatitis, rheumatoid arthritis and psychiatric disorders.\n\nBMS' primary R&D sites are located in Lawrence, New Jersey (formerly Squibb, near Princeton) and Wallingford, Connecticut (formerly Bristol-Myers); with other sites in East Syracuse, New York; Princeton Pike, Hopewell Township and New Brunswick, New Jersey; and in Swords, Ireland; Braine-l'Alleud, Belgium; Tokyo, Japan; and Bangalore, India.\n\nThe Squibb corporation was founded in 1858 by Edward Robinson Squibb in Brooklyn, New York. Squibb was known as a vigorous advocate of quality control and high purity standards within the fledgling pharmaceutical industry of his time, at one point self-publishing an alternative to the U.S. Pharmacopeia (\"Squibb's Ephemeris of Materia Medica\") after failing to convince the American Medical Association to incorporate higher purity standards. Mentions of the \"Materia Medica\", Squibb products, and Edward Squibb's opinion on the utility and best method of preparation for various medicants are found in many medical papers of the late 1800s. Squibb Corporation served as a major supplier of medical goods to the Union Army during the American Civil War, providing portable medical kits containing morphine, surgical anesthetics, and quinine for the treatment of malaria (which was endemic in most of the eastern United States at that time).\n\nIn 1887, Hamilton College graduates William McLaren Bristol and John Ripley Myers purchased the Clinton Pharmaceutical company of Clinton, New York. In 1898, they decided to rename it Bristol, Myers and Company. Following Myers' death in 1899, Bristol changed the name to the Bristol-Myers Corporation. The first nationally recognized product was Sal Hepatica, a laxative mineral salt in 1903. Its second national success was Ipana toothpaste, from 1901 through the 1960s. Other divisions were Clairol (hair colors and haircare) and Drackett (household products such as Windex and Drano).\n\nIn 1943, Bristol-Myers acquired Cheplin Biological Laboratories, a producer of acidophilus milk in East Syracuse, New York, and converted the plant to produce penicillin for the World War II Allied forces. After the war, the company renamed the plant Bristol Laboratories in 1945 and entered the civilian antibiotics market, where it faced competition from Squibb, which had opened the world's largest penicillin plant in 1944 in New Brunswick, New Jersey. Penicillin production at the East Syracuse plant was ended in 2005, when it became less expensive to produce overseas, but the facility continues to be used for the manufacturing process development and production of other biologic medicines for clinical trials and commercial use.\n\nBristol-Myers and Squibb merged in 1989, with Bristol-Myers as the nominal survivor. The merged company became Bristol-Myers Squibb.\n\nIn 1999, President Clinton awarded Bristol-Myers Squibb the National Medal of Technology, the nation's highest recognition for technological achievement, \"for extending and enhancing human life through innovative pharmaceutical research and development and for redefining the science of clinical study through groundbreaking and hugely complex clinical trials that are recognized models in the industry.\"\n\nIn 2002, the company was involved in a lawsuit of maintaining illegally a monopoly on Taxol, its cancer treatment, and it was again sued for the antitrust lawsuit 5 years later, which cost the company $125 million for settlement.\n\nThe company was involved in an accounting scandal in 2002 that resulted in a significant restatement of revenues from 1999 to 2001. The restatement was the result of an improper booking of sales related to \"channel stuffing\" as the practice of offering excess inventory to customers to create higher sales numbers. The company has since settled with the United States Department of Justice and Securities and Exchange Commission, agreeing to pay $150 million while neither admitting nor denying guilt.\n\nOn October 24, 2002, Bristol-Myers Squibb Co. restated earnings downward for parts of 2000 and 2001 while revising this year's earnings upward because of its massive inventory backlog imbroglio that spurred two government investigations. On March 15, 2004, Bristol-Myers Squibb Co. adjusted upward its fourth-quarter and full-year 2003 results after reversing an earlier decision about how to deal with accounting errors made in prior years. As part of a Deferred Prosecution Agreement, the company was placed under the oversight of a monitor appointed by the U.S. Attorney in New Jersey. In addition, the former head of the Pharma group, Richard Lane, and the ex-CFO, Fred Schiff, were indicted for federal securities violations.\n\nAn investigation of the company was made public in July 2006, and the FBI raided the company's corporate offices. The investigation centered on the distribution of Plavix and charges of collusion. On September 12, 2006, the monitor, former Federal Judge Frederick B. Lacey, urged the company to remove then CEO Peter Dolan over the Plavix dispute. Later that day, BMS announced that Dolan would indeed step down.\n\nThe Deferred Prosecution Agreement expired in June 2007 and the Department of Justice did not take any further legal action against the company for matters covered by the DPA. Under CEO Jim Cornelius, who was CEO following Dolan until May 2010, all executives involved in the \"channel-stuffing\" and generic competition scandals have since left the company.\n\nIn 2009, a major restructuring began focusing on the pharmaceutical business and biologic products, along with productivity initiatives and cost-cutting and streamlining business operations through a multi-year program of on-going layoffs. This was part of a business strategy launched in 2007 to transform the company from a large diversified pharma company to a specialty biopharma company, which also included the closure of half of their manufacturing facilities. As another cost-cutting measure Bristol-Myers also reduced subsidies for health-care to retirees and plans to freeze their pension plan at the end of 2009. In August, BMS acquired the biotechnology firm Medarex as part of the company's \"String of Pearls\" strategy of alliances, partnerships and acquisitions. In November, Bristol-Myers Squibb announced that it was \"splitting off\" Mead Johnson Nutrition by offering BMY shareholders the opportunity to exchange their stock for shares in Mead Johnson. According to Bristol-Myers Squibb, this move was expected to further sharpen the company's focus on biopharmaceuticals.\n\nBMS is a Fortune 500 Company (#114 in 2010 list). Newsweek's 2009 Green Ranking recognized Bristol-Myers Squibb as 8th among 500 of the largest United States corporations. Also, BMS was included in the 2009 Dow Jones Sustainability North America Index of leading sustainability-driven companies.\n\nIn October 2010, the company acquired ZymoGenetics, securing an existing product as well as pipeline assets in hepatitis C, cancer and other therapeutic areas. Lamberto Andreotti was named CEO this year; he had previously served as \"president and COO responsible for all pharmaceutical operations worldwide.\"\n\nIn 2011, Lou Schmukler joined the company as the president of global product development and design; Schmukler led the team that completed the company's strategic transformation to a specialty biopharmaceutical company that had begun in 2007. As of 2011, the company had a dozen manufacturing facilities and six product development sites.\n\nBristol-Myers Squibb agreed to pay around $2.5 billion in cash to buy Inhibitex Inc. in attempt to compete with Gilead/Pharmasset to produce Hepatitis C drugs. The settlement will be finished in 2 months for its Inhibitex's shareholders acceptance of 126 percent premium price of its price over the previous 20 trading days ended on January 6. On June 29, Bristol-Myers Squibb extended its portfolio of diabetes treatments when it agreed to buy Amylin Pharmaceuticals for around in cash and pay to Eli Lilly to cover Amylin's debt and its outstanding collaboration-related obligations. AstraZeneca, who already collaborated on several diabetes treatments with Bristol-Myers Squibb, agreed to pay in cash for the right to continue development of Amylin's products. Two years later, in 2014, the company divested Amylin to AstraZeneca.\n\nCiting major developments and a market capitalization of US$87 billion and stock appreciation of 61.4%, Bristol-Myers Squibb was ranked as the best drug company of 2013 by Forbes Magazine.\n\nIn April 2014, BMS announced its acquisition of iPierian for up to $725 million. In December the company received FDA approval for the use of the PD-1 inhibitor nivolumab (Opdivo) in treating patients whose skin cancer cannot be removed or have not responded to previous drug therapies. In February 2015, the company acquired Flexus Biosciences for $1.25 billion as well as initiating a research partnership with Rigel Pharmaceuticals which could generate more than $339 million. As part of the deal with Flexus, BMS will gain full rights to Flexus' lead small molecule IDO1-inhibitor, F001287. In March, the company obtained an exclusive opportunity to both licence and commercialise PROSTVAC, Bavarian Nordic's phase III prostate specific antigen targeting cancer immunotherapy. Bavarian Nordic will receive an upfront payment of $60 million as well as incremental payments up to $230 million, if the overall survival of test patients exceeds that seen in Phase II tests. Bavarian could also receive milestone payments of between $110 million and $495 million, dependent on regulatory authorization, and these payments have the potential to total up to $975 million.\n\nIn May 2015, Dr. Giovanni Caforio became CEO of the company; Caforio was formerly the company's COO and succeeded Andreotti upon his retirement. Andreotti subsequently succeeded James Cornelius as executive chairman upon his retirement. In November, the company acquired the cardiovascular disease drug developer Cardioxyl for up to $2.075 billion. The deal strengthens the BMS' critical pipelines with the phase II candidate for acute decompensated heart failure, CXL-1427.\n\nIn March 2016, the company announced it would acquire Padlock Therapeutics for up to $600 million. In early July, the company announced it would acquire Cormorant Pharmaceuticals for $520 million, boosting BMS' onclogy offering through Cormorants monoclonal antibody targeted against interleukin-8.\n\nIn late February 2017, the \"Wall Street Journal\" and \"Fortune\" - among others - reported that activist investor, Carl Icahn, had taken a stake in the company, signalling a potential future takeover from the likes of Gilead Sciences. In August the company acquired IFM Therapeutics for $300 million upfront, with contingency payments of $1.01 billion due on certain milestones – allowing BMS to better compete against Merck & Co's cancer rival treatment, Keytruda.\n\nThe following is an illustration of the company's major mergers and acquisitions and historical predecessors (this is not a comprehensive list):\n\nFor the fiscal year 2017, Bristol-Myers Squibb reported earnings of US$1.007 billion, with an annual revenue of US$20.776 billion, an increase of 6.9% over the previous fiscal cycle. Bristol-Myers Squibb's shares traded at over $55 per share, and its market capitalization was valued at over US$81.6 billion in October 2018. Bristol-Myers Squibb ranked 145th on the Fortune 500 list of the largest United States corporations by revenue in 2018.\nThe following is a list of key pharmaceutical products:\n\n\"Cardiovascular diseases\"\n\n\"Diabetes mellitus\"\n\n\"Infectious diseases, including HIV infection and associated conditions\"\n\n\"Inflammatory disorders\"\n\n\"Oncology\"\n\n\"Psychiatry\"\n\n\"Rheumatic disorders\"\n\n\"Transplant rejection\"\n\n\nThe following is a selective list of investigational products under development, as of 2015:\n\n\n"}
{"id": "56658865", "url": "https://en.wikipedia.org/wiki?curid=56658865", "title": "British Tar (1792 ship)", "text": "British Tar (1792 ship)\n\nBritish Tar was launched at Shields in 1792 and made five voyages as a whaler and several as a West Indiaman. She then became a general trader. She was lost on 29 January 1818.\n\n\"British Tar\" enters \"Lloyd's Register\" in 1792 with J. Fitch, master, Mangles, owner, and trade London–South Seas fisheries.\n\nFor her first whale fishing voyage \"British Tar\" sailed to Peru under the command of Jedediah Fitch. On 8 February 1793 she was at St Helena, homeward bound, and she arrived in England with 219 tuns of sperm oil.\n\nOn her second voyage she sailed in 1793 for Peru under the command of Blythe (or Thomas Bligh). She returned on 29 July 1794.\nHer third voyage was again under the command of Bligh (or Blythe, or Blight), but this time he sailed her for the Pacific Ocean, where she was reported to be in August–September 1796. She returned to England on 8 May 1797.\n\"British Tar\" then spent several years sailing to the West Indies. John and James Mangles still owned her and she sailed to San Domingo, but more often to Jamaica.\n\nOn 8 January 1802 Captain Innis sailed \"British Tar\" to the Galápagos Islands for her fourth whaling voyage. At some Point Captain Rowe replaced Innis. Homeward bound, she left St Helena on 4 May 1804, and arrived in England on 17 July.\nCaptain Charles Harrax (or Harrass, or Harrat, or Haddock), left England on 12 November 1804 and returned on 15 April 1807. This is the last whaling voyage for \"British Tar\"s that has any details. However, she may continued to whale into 1810.\n\nThe \"Register of Shipping\" for 1810, in its supplemental pages, shows \"British Tar\" with Anthony, master, J. Pirie, owner, and trade London–Guadeloupe. In 1811 she underwent a large repair, and her trade became London–Newcastle.\n\nThe \"Register of Shipping\" for 1818 shows \"British Tar\" with J. Pirie, owner and master, and trade London–Madeira. The entry has a line through it and the notation \"Lost\".\n\"Lloyd's List\" reported on 3 February 1818 that a gale on 29 January 1818 had caused the total loss of \"British Tar\", John Pirie, master, between Lymington and Christchurch, Dorset. She was on her return voyage from Sierra Leone to London. All on board, at least 14 people, were lost.\n\nThe day after \"British Tar\" was lost, the body of a four-foot long African lizard was found on the beach by Hordle Cliff, near Milford. Apparently this animal was the largest of the kind ever brought to Europe.\n\nCitations\n\nReferences\n"}
{"id": "40409788", "url": "https://en.wikipedia.org/wiki?curid=40409788", "title": "Convolutional neural network", "text": "Convolutional neural network\n\nIn deep learning, a convolutional neural network (CNN, or ConvNet) is a class of deep, feed-forward artificial neural networks, most commonly applied to analyzing visual imagery.\n\nCNNs use a variation of multilayer perceptrons designed to require minimal preprocessing. They are also known as shift invariant or space invariant artificial neural networks (SIANN), based on their shared-weights architecture and translation invariance characteristics.\n\nConvolutional networks were inspired by biological processes in that the connectivity pattern between neurons resembles the organization of the animal visual cortex. Individual cortical neurons respond to stimuli only in a restricted region of the visual field known as the receptive field. The receptive fields of different neurons partially overlap such that they cover the entire visual field.\n\nCNNs use relatively little pre-processing compared to other image classification algorithms. This means that the network learns the filters that in traditional algorithms were hand-engineered. This independence from prior knowledge and human effort in feature design is a major advantage.\n\nThey have applications in image and video recognition, recommender systems, image classification, medical image analysis, and natural language processing.\n\nA CNN consists of an input and an output layer, as well as multiple hidden layers. The hidden layers of a CNN typically consist of convolutional layers, pooling layers, fully connected layers and normalization layers.\n\nDescription of the process as a convolution in neural networks is by convention. Mathematically it is a cross-correlation rather than a convolution (although cross-correlation is a related operation). This only has significance for the indices in the matrix, and thus which weights are placed at which index.\n\nConvolutional layers apply a convolution operation to the input, passing the result to the next layer. The convolution emulates the response of an individual neuron to visual stimuli.\n\nEach convolutional neuron processes data only for its receptive field. Although fully connected feedforward neural networks can be used to learn features as well as classify data, it is not practical to apply this architecture to images. A very high number of neurons would be necessary, even in a shallow (opposite of deep) architecture, due to the very large input sizes associated with images, where each pixel is a relevant variable. For instance, a fully connected layer for a (small) image of size 100 x 100 has 10000 weights for \"each\" neuron in the second layer. The convolution operation brings a solution to this problem as it reduces the number of free parameters, allowing the network to be deeper with fewer parameters. For instance, regardless of image size, tiling regions of size 5 x 5, each with the same shared weights, requires only 25 learnable parameters. In this way, it resolves the vanishing or exploding gradients problem in training traditional multi-layer neural networks with many layers by using backpropagation.\n\nConvolutional networks may include local or global pooling layers, which combine the outputs of neuron clusters at one layer into a single neuron in the next layer. For example, \"max pooling\" uses the maximum value from each of a cluster of neurons at the prior layer. Another example is \"average pooling\", which uses the average value from each of a cluster of neurons at the prior layer.\n\nFully connected layers connect every neuron in one layer to every neuron in another layer. It is in principle the same as the traditional multi-layer perceptron neural network (MLP).\n\nIn neural networks, each neuron receives input from some number of locations in the previous layer. In a fully connected layer, each neuron receives input from \"every\" element of the previous layer. In a convolutional layer, neurons receive input from only a restricted subarea of the previous layer. Typically the subarea is of a square shape (e.g., size 5 by 5). The input area of a neuron is called its \"receptive field\". So, in a fully connected layer, the receptive field is the entire previous layer. In a convolutional layer, the receptive area is smaller than the entire previous layer.\n\nEach neuron in a neural network computes an output value by applying some function to the input values coming from the receptive field in the previous layer. The function that is applied to the input values is specified by a vector of weights and a bias (typically real numbers). Learning in a neural network progresses by making incremental adjustments to the biases and weights. The vector of weights and the bias are called a \"filter\" and represents some feature of the input (e.g., a particular shape). A distinguishing feature of CNNs is that many neurons share the same filter. This reduces memory footprint because a single bias and a single vector of weights is used across all receptive fields sharing that filter, rather than each receptive field having its own bias and vector of weights.\n\nCNN design follows vision processing in living organisms.\n\nWork by Hubel and Wiesel in the 1950s and 1960s showed that cat and monkey visual cortexes contain neurons that individually respond to small regions of the visual field. Provided the eyes are not moving, the region of visual space within which visual stimuli affect the firing of a single neuron is known as its receptive field. Neighboring cells have similar and overlapping receptive fields. Receptive field size and location varies systematically across the cortex to form a complete map of visual space. The cortex in each hemisphere represents the contralateral visual field.\n\nTheir 1968 paper identified two basic visual cell types in the brain:\n\n\nThe \"neocognitron\" was introduced in 1980. The neocognitron does not require units located at multiple network positions to have the same trainable weights. This idea appears in 1986 in the book version of the original back-propagation paper. Neocognitrons were developed in 1988 for temporal signals. Their design was improved in 1998, generalized in 2003 and simplified in the same year.\n\nThe time delay neural network (TDNN) was the first convolutional network.\n\nTDNNs are fixed-size convolutional networks that share weights along the temporal dimension They allow speech signals to be processed time-invariantly, analogous to the translation invariance offered by CNNs. They were introduced in the early 1980s. The tiling of neuron outputs can cover timed stages.\n\nA system to recognize hand-written ZIP Code numbers involved convolutions in which the kernel coefficients had been laboriously hand designed.\n\nYann LeCun et al. (1989) used back-propagation to learn the convolution kernel coefficients directly from images of hand-written numbers. Learning was thus fully automatic, performed better than manual coefficient design, and was suited to a broader range of image recognition problems and image types.\n\nLeNet-5, a pioneering 7-level convolutional network by LeCun et al. in 1998, that classifies digits, was applied by several banks to recognize hand-written numbers on checks () digitized in 32x32 pixel images. The ability to process higher resolution images requires larger and more layers of convolutional neural networks, so this technique is constrained by the availability of computing resources.\n\nSimilarly, a shift invariant neural network was proposed for image character recognition in 1988. The architecture and training algorithm were modified in 1991 and applied for medical image processing and automatic detection of breast cancer in mammograms.\n\nA different convolution-based design was proposed in 1988 for application to decomposition of one-dimensional electromyography convolved signals via de-convolution. This design was modified in 1989 to other de-convolution-based designs.\n\nThe feed-forward architecture of convolutional neural networks was extended in the neural abstraction pyramid by lateral and feedback connections. The resulting recurrent convolutional network allows for the flexible incorporation of contextual information to iteratively resolve local ambiguities. In contrast to previous models, image-like outputs at the highest resolution were generated.\n\nFollowing the 2005 paper that established the value of GPGPU for machine learning, several publications described more efficient ways to train convolutional neural networks using GPUs. In 2011, they were refined and implemented on a GPU, with impressive results. In 2012, Ciresan et al. significantly improved on the best performance in the literature for multiple image databases, including the MNIST database, the NORB database, the HWDB1.0 dataset (Chinese characters) and the CIFAR10 dataset (dataset of 60000 32x32 labeled RGB images).\n\nTraditional multilayer perceptron (MLP) models were successfully used for image recognition. However, due to the full connectivity between nodes they suffer from the curse of dimensionality, and thus do not scale well to higher resolution images. A 1000×1000 pixel image with RGB color channels has 3 million dimensions, which is too high to feasibly process efficiently at scale with full connectivity.\n\nFor example, in CIFAR-10, images are only of size 32×32×3 (32 wide, 32 high, 3 color channels), so a single fully connected neuron in a first hidden layer of a regular neural network would have 32*32*3 = 3,072 weights. A 200×200 image, however, would lead to neurons that have 200*200*3 = 120,000 weights.\n\nAlso, such network architecture does not take into account the spatial structure of data, treating input pixels which are far apart in the same way as pixels that are close together. This ignores locality of reference in image data, both computationally and semantically. Thus, full connectivity of neurons is wasteful for purposes such as image recognition that are dominated by spatially local input patterns.\n\nConvolutional neural networks are biologically inspired variants of multilayer perceptrons that are designed to emulate the behavior of a visual cortex. These models mitigate the challenges posed by the MLP architecture by exploiting the strong spatially local correlation present in natural images. As opposed to MLPs, CNNs have the following distinguishing features:\n\nTogether, these properties allow CNNs to achieve better generalization on vision problems. Weight sharing dramatically reduces the number of free parameters learned, thus lowering the memory requirements for running the network and allowing the training of larger, more powerful networks.\n\nA CNN architecture is formed by a stack of distinct layers that transform the input volume into an output volume (e.g. holding the class scores) through a differentiable function. A few distinct types of layers are commonly used. These are further discussed below.\n\nThe convolutional layer is the core building block of a CNN. The layer's parameters consist of a set of learnable filters (or kernels), which have a small receptive field, but extend through the full depth of the input volume. During the forward pass, each filter is convolved across the width and height of the input volume, computing the dot product between the entries of the filter and the input and producing a 2-dimensional activation map of that filter. As a result, the network learns filters that activate when it detects some specific type of feature at some spatial position in the input.\n\nStacking the activation maps for all filters along the depth dimension forms the full output volume of the convolution layer. Every entry in the output volume can thus also be interpreted as an output of a neuron that looks at a small region in the input and shares parameters with neurons in the same activation map.\n\nWhen dealing with high-dimensional inputs such as images, it is impractical to connect neurons to all neurons in the previous volume because such a network architecture does not take the spatial structure of the data into account. Convolutional networks exploit spatially local correlation by enforcing a sparse local connectivity pattern between neurons of adjacent layers: each neuron is connected to only a small region of the input volume.\n\nThe extent of this connectivity is a hyperparameter called the receptive field of the neuron. The connections are local in space (along width and height), but always extend along the entire depth of the input volume. Such an architecture ensures that the learnt filters produce the strongest response to a spatially local input pattern.\n\nThree hyperparameters control the size of the output volume of the convolutional layer: the depth, stride and zero-padding.\n\n\nThe spatial size of the output volume can be computed as a function of the input volume size formula_3, the kernel field size of the convolutional layer neurons formula_4, the stride with which they are applied formula_5, and the amount of zero padding formula_6 used on the border. The formula for calculating how many neurons \"fit\" in a given volume is given by\n\nformula_7 \n\nIf this number is not an integer, then the strides are incorrect and the neurons cannot be tiled to fit across the input volume in a symmetric way. In general, setting zero padding to be formula_8 when the stride is formula_9 ensures that the input volume and output volume will have the same size spatially. However, it's not always completely necessary to use all of the neurons of the previous layer. For example, a neural network designer may decide to use just a portion of padding.\n\nA parameter sharing scheme is used in convolutional layers to control the number of free parameters. It relies on one reasonable assumption: if a patch feature is useful to compute at some spatial position, then it should also be useful to compute at other positions. In other words, denoting a single 2-dimensional slice of depth as a depth slice, we constrain the neurons in each depth slice to use the same weights and bias.\n\nSince all neurons in a single depth slice share the same parameters, the forward pass in each depth slice of the convolutional layer can be computed as a convolution of the neuron's weights with the input volume. Therefore, it is common to refer to the sets of weights as a filter (or a kernel), which is convolved with the input. The result of this convolution is an activation map, and the set of activation maps for each different filter are stacked together along the depth dimension to produce the output volume. Parameter sharing contributes to the translation invariance of the CNN architecture.\n\nSometimes, the parameter sharing assumption may not make sense. This is especially the case when the input images to a CNN have some specific centered structure; for which we expect completely different features to be learned on different spatial locations. One practical example is when the inputs are faces that have been centered in the image: we might expect different eye-specific or hair-specific features to be learned in different parts of the image. In that case it is common to relax the parameter sharing scheme, and instead simply call the layer a \"locally connected layer\".\n\nAnother important concept of CNNs is pooling, which is a form of non-linear down-sampling. There are several non-linear functions to implement pooling among which \"max pooling\" is the most common. It partitions the input image into a set of non-overlapping rectangles and, for each such sub-region, outputs the maximum. \n\nIntuitively, the exact location of a feature is less important than its rough location relative to other features. This is the idea behind the use of pooling in convolutional neural networks. The pooling layer serves to progressively reduce the spatial size of the representation, to reduce the number of parameters, memory footprint and amount of computation in the network, and hence to also control overfitting. It is common to periodically insert a pooling layer between successive convolutional layers in a CNN architecture. The pooling operation provides another form of translation invariance.\n\nThe pooling layer operates independently on every depth slice of the input and resizes it spatially. The most common form is a pooling layer with filters of size 2×2 applied with a stride of 2 downsamples at every depth slice in the input by 2 along both width and height, discarding 75% of the activations. In this case, every max operation is over 4 numbers. The depth dimension remains unchanged.\n\nIn addition to max pooling, pooling units can use other functions, such as average pooling or ℓ-norm pooling. Average pooling was often used historically but has recently fallen out of favor compared to max pooling, which performs better in practice.\n\nDue to the aggressive reduction in the size of the representation, there is a recent trend towards using smaller filters or discarding pooling layers altogether.\n\"Region of Interest\" pooling (also known as RoI pooling) is a variant of max pooling, in which output size is fixed and input rectangle is a parameter.\n\nPooling is an important component of convolutional neural networks for object detection based on Fast R-CNN architecture.\n\nReLU is the abbreviation of rectified linear unit, which applies the non-saturating activation function formula_10. Effectively, it removes negative values from an activation map by setting them to zero. It increases the nonlinear properties of the decision function and of the overall network without affecting the receptive fields of the convolution layer.\n\nOther functions are also used to increase nonlinearity, for example the saturating hyperbolic tangent formula_11, formula_12, and the sigmoid function formula_13. ReLU is often preferred to other functions because it trains the neural network several times faster without a significant penalty to generalization accuracy.\n\nFinally, after several convolutional and max pooling layers, the high-level reasoning in the neural network is done via fully connected layers. Neurons in a fully connected layer have connections to all activations in the previous layer, as seen in regular (non-convolutional) artificial neural networks. Their activations can thus be computed as an affine transformation, with matrix multiplication followed by a bias offset (vector addition of a learned or fixed bias term).\n\nThe \"loss layer\" specifies how training penalizes the deviation between the predicted (output) and true labels and is normally the final layer of a neural network. Various loss functions appropriate for different tasks may be used. \n\nSoftmax loss is used for predicting a single class of \"K\" mutually exclusive classes. Sigmoid cross-entropy loss is used for predicting \"K\" independent probability values in formula_14. Euclidean loss is used for regressing to real-valued labels formula_15.\n\nCNNs use more hyperparameters than a standard multilayer perceptron (MLP). While the usual rules for learning rates and regularization constants still apply, the following should be kept in mind when optimizing.\n\nSince feature map size decreases with depth, layers near the input layer will tend to have fewer filters while higher layers can have more. To equalize computation at each layer, the product of feature values \"v\" with pixel position is kept roughly constant across layers. Preserving more information about the input would require keeping the total number of activations (number of feature maps times number of pixel positions) non-decreasing from one layer to the next.\n\nThe number of feature maps directly controls the capacity and depends on the number of available examples and task complexity.\n\nCommon filter shapes found in the literature vary greatly, and are usually chosen based on the dataset.\n\nThe challenge is, thus, to find the right level of granularity so as to create abstractions at the proper scale, given a particular dataset, and without overfitting.\n\nTypical values are 2×2. Very large input volumes may warrant 4×4 pooling in the lower layers. However, choosing larger shapes will dramatically reduce the dimension of the signal, and may result in excess information loss. Often, non-overlapping pooling windows perform best.\n\nRegularization is a process of introducing additional information to solve an ill-posed problem or to prevent overfitting. CNNs use various types of regularization.\n\nBecause a fully connected layer occupies most of the parameters, it is prone to overfitting. One method to reduce overfitting is dropout. At each training stage, individual nodes are either \"dropped out\" of the net with probability formula_16 or kept with probability formula_17, so that a reduced network is left; incoming and outgoing edges to a dropped-out node are also removed. Only the reduced network is trained on the data in that stage. The removed nodes are then reinserted into the network with their original weights.\n\nIn the training stages, the probability that a hidden node will be dropped is usually 0.5; for input nodes, this should be much lower, intuitively because information is directly lost when input nodes are ignored.\n\nAt testing time after training has finished, we would ideally like to find a sample average of all possible formula_18 dropped-out networks; unfortunately this is unfeasible for large values of formula_19. However, we can find an approximation by using the full network with each node's output weighted by a factor of formula_17, so the expected value of the output of any node is the same as in the training stages. This is the biggest contribution of the dropout method: although it effectively generates formula_18 neural nets, and as such allows for model combination, at test time only a single network needs to be tested.\n\nBy avoiding training all nodes on all training data, dropout decreases overfitting. The method also significantly improves training speed. This makes model combination practical, even for deep neural nets. The technique seems to reduce node interactions, leading them to learn more robust features that better generalize to new data.\n\nDropConnect is the generalization of dropout in which each connection, rather than each output unit, can be dropped with probability formula_16. Each unit thus receives input from a random subset of units in the previous layer.\n\nDropConnect is similar to dropout as it introduces dynamic sparsity within the model, but differs in that the sparsity is on the weights, rather than the output vectors of a layer. In other words, the fully connected layer with DropConnect becomes a sparsely connected layer in which the connections are chosen at random during the training stage.\n\nA major drawback to Dropout is that it does not have the same benefits for convolutional layers, where the neurons are not fully connected.\n\nIn stochastic pooling, the conventional deterministic pooling operations are replaced with a stochastic procedure, where the activation within each pooling region is picked randomly according to a multinomial distribution, given by the activities within the pooling region. The approach is hyperparameter free and can be combined with other regularization approaches, such as dropout and data augmentation.\n\nAn alternate view of stochastic pooling is that it is equivalent to standard max pooling but with many copies of an input image, each having small local deformations. This is similar to explicit elastic deformations of the input images, which delivers excellent MNIST performance. Using stochastic pooling in a multilayer model gives an exponential number of deformations since the selections in higher layers are independent of those below.\n\nSince the degree of model overfitting is determined by both its power and the amount of training it receives, providing a convolutional network with more training examples can reduce overfitting. Since these networks are usually trained with all available data, one approach is to either generate new data from scratch (if possible) or perturb existing data to create new ones. For example, input images could be asymmetrically cropped by a few percent to create new examples with the same label as the original.\n\nOne of the simplest methods to prevent overfitting of a network is to simply stop the training before overfitting has had a chance to occur. It comes with the disadvantage that the learning process is halted.\n\nAnother simple way to prevent overfitting is to limit the number of parameters, typically by limiting the number of hidden units in each layer or limiting network depth. For convolutional networks, the filter size also affects the number of parameters. Limiting the number of parameters restricts the predictive power of the network directly, reducing the complexity of the function that it can perform on the data, and thus limits the amount of overfitting. This is equivalent to a \"zero norm\".\n\nA simple form of added regularizer is weight decay, which simply adds an additional error, proportional to the sum of weights (L1 norm) or squared magnitude (L2 norm) of the weight vector, to the error at each node. The level of acceptable model complexity can be reduced by increasing the proportionality constant, thus increasing the penalty for large weight vectors.\n\nL2 regularization is the most common form of regularization. It can be implemented by penalizing the squared magnitude of all parameters directly in the objective. The L2 regularization has the intuitive interpretation of heavily penalizing peaky weight vectors and preferring diffuse weight vectors. Due to multiplicative interactions between weights and inputs this has the useful property of encouraging the network to use all of its inputs a little rather than some of its inputs a lot.\n\nL1 regularization is another common form. It is possible to combine L1 with L2 regularization (this is called Elastic net regularization). The L1 regularization leads the weight vectors to become sparse during optimization. In other words, neurons with L1 regularization end up using only a sparse subset of their most important inputs and become nearly invariant to the noisy inputs.\n\nAnother form of regularization is to enforce an absolute upper bound on the magnitude of the weight vector for every neuron and use projected gradient descent to enforce the constraint. In practice, this corresponds to performing the parameter update as normal, and then enforcing the constraint by clamping the weight vector formula_23 of every neuron to satisfy formula_24. Typical values of formula_25 are order of 3–4. Some papers report improvements when using this form of regularization.\n\nPooling loses the precise spatial relationships between high-level parts (such as nose and mouth in a face image). These relationships are needed for identity recognition. Overlapping the pools so that each feature occurs in multiple pools, helps retain the information. Translation alone cannot extrapolate the understanding of geometric relationships to a radically new viewpoint, such as a different orientation or scale. On the other hand, people are very good at extrapolating; after seeing a new shape once they can recognize it from a different viewpoint.\n\nCurrently, the common way to deal with this problem is to train the network on transformed data in different orientations, scales, lighting, etc. so that the network can cope with these variations. This is computationally intensive for large data-sets. The alternative is to use a hierarchy of coordinate frames and to use a group of neurons to represent a conjunction of the shape of the feature and its pose relative to the retina. The pose relative to retina is the relationship between the coordinate frame of the retina and the intrinsic features' coordinate frame.\n\nThus, one way of representing something is to embed the coordinate frame within it. Once this is done, large features can be recognized by using the consistency of the poses of their parts (e.g. nose and mouth poses make a consistent prediction of the pose of the whole face). Using this approach ensures that the higher level entity (e.g. face) is present when the lower level (e.g. nose and mouth) agree on its prediction of the pose. The vectors of neuronal activity that represent pose (\"pose vectors\") allow spatial transformations modeled as linear operations that make it easier for the network to learn the hierarchy of visual entities and generalize across viewpoints. This is similar to the way the human visual system imposes coordinate frames in order to represent shapes.\n\nCNNs are often used in image recognition systems. In 2012 an error rate of 0.23 percent on the MNIST database was reported. Another paper on using CNN for image classification reported that the learning process was \"surprisingly fast\"; in the same paper, the best published results as of 2011 were achieved in the MNIST database and the NORB database. Subsequently, a similar CNN called \nAlexNet won the ImageNet Large Scale Visual Recognition Challenge 2012.\n\nWhen applied to facial recognition, CNNs achieved a large decrease in error rate. Another paper reported a 97.6 percent recognition rate on \"5,600 still images of more than 10 subjects\". CNNs were used to assess video quality in an objective way after manual training; the resulting system had a very low root mean square error.\n\nThe ImageNet Large Scale Visual Recognition Challenge is a benchmark in object classification and detection, with millions of images and hundreds of object classes. In the ILSVRC 2014, a large-scale visual recognition challenge, almost every highly ranked team used CNN as their basic framework. The winner GoogLeNet (the foundation of DeepDream) increased the mean average precision of object detection to 0.439329, and reduced classification error to 0.06656, the best result to date. Its network applied more than 30 layers. That performance of convolutional neural networks on the ImageNet tests was close to that of humans. The best algorithms still struggle with objects that are small or thin, such as a small ant on a stem of a flower or a person holding a quill in their hand. They also have trouble with images that have been distorted with filters, an increasingly common phenomenon with modern digital cameras. By contrast, those kinds of images rarely trouble humans. Humans, however, tend to have trouble with other issues. For example, they are not good at classifying objects into fine-grained categories such as the particular breed of dog or species of bird, whereas convolutional neural networks handle this.\n\nIn 2015 a many-layered CNN demonstrated the ability to spot faces from a wide range of angles, including upside down, even when partially occluded, with competitive performance. The network was trained on a database of 200,000 images that included faces at various angles and orientations and a further 20 million images without faces. They used batches of 128 images over 50,000 iterations.\n\nCompared to image data domains, there is relatively little work on applying CNNs to video classification. Video is more complex than images since it has another (temporal) dimension. However, some extensions of CNNs into the video domain have been explored. One approach is to treat space and time as equivalent dimensions of the input and perform convolutions in both time and space. Another way is to fuse the features of two convolutional neural networks, one for the spatial and one for the temporal stream. LSTM units are typically incorporated after the CNN to account for inter-frame or inter-clip dependencies. Unsupervised learning schemes for training spatio-temporal features have been introduced, based on Convolutional Gated Restricted Boltzmann Machines and Independent Subspace Analysis.\n\nCNNs have also explored natural language processing. CNN models are effective for various NLP problems and achieved excellent results in semantic parsing, search query retrieval, sentence modeling, classification, prediction and other traditional NLP tasks.\n\nCNNs have been used in drug discovery. Predicting the interaction between molecules and biological proteins can identify potential treatments. In 2015, Atomwise introduced AtomNet, the first deep learning neural network for structure-based rational drug design. The system trains directly on 3-dimensional representations of chemical interactions. Similar to how image recognition networks learn to compose smaller, spatially proximate features into larger, complex structures, AtomNet discovers chemical features, such as aromaticity, sp3 carbons and hydrogen bonding. Subsequently, AtomNet was used to predict novel candidate biomolecules for multiple disease targets, most notably treatments for the Ebola virus and multiple sclerosis.\n\nCNNs can be naturally tailored to analyze a sufficiently large collection of time series representing one week long human physical activity streams augmented by the rich clinical data (including the death register, as provided by, e.g., the NHANES study). A simple CNN was combined with Cox-Gompertz proportional hazards model and used to produce a proof-of-concept example of digital biomarkers of aging in the form of all-causes-mortality predictor.\n\nCNNs have been used in the game of checkers. From 1999 to 2001, Fogel and Chellapilla published papers showing how a convolutional neural network could learn to play checkers using co-evolution. The learning process did not use prior human professional games, but rather focused on a minimal set of information contained in the checkerboard: the location and type of pieces, and . Ultimately, the program (Blondie24) was tested on 165 games against players and ranked in the highest 0.4%. It also earned a win against the program Chinook at its \"expert\" level of play.\n\nCNNs have been used in computer Go. In December 2014, Clark and Storkey published a paper showing that a CNN trained by supervised learning from a database of human professional games could outperform GNU Go and win some games against Monte Carlo tree search Fuego 1.1 in a fraction of the time it took Fuego to play. Later it was announced that a large 12-layer convolutional neural network had correctly predicted the professional move in 55% of positions, equalling the accuracy of a 6 dan human player. When the trained convolutional network was used directly to play games of Go, without any search, it beat the traditional search program GNU Go in 97% of games, and matched the performance of the Monte Carlo tree search program Fuego simulating ten thousand playouts (about a million positions) per move.\n\nA couple of CNNs for choosing moves to try (\"policy network\") and evaluating positions (\"value network\") driving MCTS were used by AlphaGo, the first to beat the best human player at the time.\n\nFor many applications, little training data is available. Convolutional neural networks usually require a large amount of training data in order to avoid overfitting. A common technique is to train the network on a larger data set from a related domain. Once the network parameters have converged an additional training step is performed using the in-domain data to fine-tune the network weights. This allows convolutional networks to be successfully applied to problems with small training sets.\n\nEnd-to-end training and prediction are common practice in computer vision. However, human interpretable explanations are required for critical systems such as a self-driving cars. \"Black-box models will not suffice\". With recent advances in visual salience, spatial and temporal attention, the most critical spatial regions/temporal instants could be visualized to justify the CNN predictions.\n\nA deep Q-network (DQN) is a type of deep learning model that combines a deep CNN with Q-learning, a form of reinforcement learning. Unlike earlier reinforcement learning agents, DQNs can learn directly from high-dimensional sensory inputs.\n\nPreliminary results were presented in 2014, with an accompanying paper in February 2015. The research described an application to Atari 2600 gaming. Other deep reinforcement learning models preceded it.\n\nConvolutional deep belief networks (CDBN) have structure very similar to convolutional neural networks and are trained similarly to deep belief networks. Therefore, they exploit the 2D structure of images, like CNNs do, and make use of pre-training like deep belief networks. They provide a generic structure that can be used in many image and signal processing tasks. Benchmark results on standard image datasets like CIFAR have been obtained using CDBNs.\n\n\n\nConvolutional neural networks are mentioned in the 2017 novel \"Infinity Born.\"\n\n\n"}
{"id": "34454189", "url": "https://en.wikipedia.org/wiki?curid=34454189", "title": "Coroners Act", "text": "Coroners Act\n\nCoroners Act is a stock short title used in New Zealand and the United Kingdom for legislation relating to coroners.\n\nActs of the Parliament of the United Kingdom \n\nThe Coroners (Ireland) Acts 1829 to 1881 was the collective title of the following Acts:\n\nAct of the Parliament of Northern Ireland\n\n"}
{"id": "708195", "url": "https://en.wikipedia.org/wiki?curid=708195", "title": "Death certificate", "text": "Death certificate\n\nThe phrase death certificate can refer either to a document issued by a medical practitioner certifying the deceased state of a person or, popularly, to a document issued by a person such as a registrar of vital statistics that declares the date, location and cause of a person's death as later entered in an official register of deaths.\n\nEach governmental jurisdiction prescribes the form of the document for use in its preview and the procedures necessary to legally produce it. One purpose of the certificate is to review the cause of death to determine if foul-play occurred as it can rule out an accidental death or a murder going by the findings and ruling of the medical examiner. It may also be required in order to arrange a burial or cremation to provide prima facie evidence of the fact of death, which can be used to prove a person's will or to claim on a person's life insurance. Lastly, death certificates are used in public health to compile data on leading causes of death among other statistics (See: Descriptive statistics)\n\nBefore issuing a death certificate, the authorities usually require a certificate from a physician or coroner to validate the cause of death and the identity of the deceased. In cases where it is not completely clear that a person is dead (usually because their body is being sustained by life support), a neurologist is often called in to verify brain death and to fill out the appropriate documentation. The failure of a physician to immediately submit the required form to the government (to trigger issuance of the death certificate) is often both a crime and cause for loss of one's license to practice. This is because of past scandals in which dead people continued to receive public benefits or voted in elections.\nDeath certificates may also be issued pursuant to a court order or an executive order in the case of individuals who have been declared dead \"in absentia\". Missing persons and victims of mass disasters (such as the sinking of the \"RMS Lusitania\") may be issued death certificates in one of these manners.\n\nIn some jurisdictions, a police officer or a paramedic may be allowed to sign a death certificate under specific circumstances. This is usually when the cause of death seems obvious and no foul play is suspected, such as in extreme old age. In such cases, an autopsy is rarely performed. This varies from jurisdiction to jurisdiction; in some areas police officers may sign death certificates for victims of SIDS, but in others all deaths of individuals under 18 must be certified by a physician. Accident deaths where there is no chance of survival (decapitations, for instance) may be certified by police or paramedics, but autopsies are still commonly performed if there is any chance that alcohol or other drugs played a role in the accident.\n\nA full explanation of the cause of death includes four items:\n\nIn most of the United States, death certificates are considered public domain documents and can therefore be obtained for any individual regardless of the requester's relationship to the deceased. Other jurisdictions take a different view, and restrict the issue of certificates. For example, in the State of New York, death certificates are only obtainable by close relatives, including the spouse, parent, child or sibling of the deceased, and other persons who have a documented lawful right or claim, documented medical need, or a New York State Court Order.\n\nIn Europe and North America, death records were kept by the local churches, along with baptism and marriage records. In what would become the United States, the Massachusetts Bay Colony was the first to have the secular courts keep these records, in 1639. By the end of the 19th century, European countries were pursuing centralized systems for recording deaths.\n\nIn the United States, a standard model death certificate was developed around 1910. This promoted uniformity and consistency in record keeping.\n\nIn the United States, certificates issued to the general public for deaths after 1990 may in some states be redacted to erase the specific cause of death (in cases where death was from natural causes) to comply with HIV confidentiality rules. In New York State, for instance, the cause of death on a general death certificate is only specified if death was accidental, homicide, suicide, or declared in absentia; all other deaths are only referred to as natural. All states have provisions, however, whereby immediate family members, law enforcement agencies, and governmental authorities (such as occupational health and safety groups) are able to obtain death certificates containing the full cause of death, even in cases of natural death.\n\nIn some cases, such as the death of a minor or infant, certificates may be kept confidential from the public as requested by legal guardian and therefore cannot be obtained by the general public but rather through immediate family members.\n\nRegistration in the UK is organised separately in the constituent jurisdictions. A register of deaths contains the information supplied by an informant, nowadays usually containing and repeating the information given in a \"Medical Certificate of Death\" (MCOD) supplied by the medical practitioner who certifies that life is extinct, this being the real \"death certificate\" distinct from the \"registration of a death\" in a register. Further information might be added after the first registration if the death was the subject of an inquest (Northern Ireland or England and Wales) or a Fatal Accident Inquiry (Scotland); this can result in a later copy of a death registration giving more details of the cause of death or the associated circumstances.\n\nIn England and Wales, compulsory national registration of deaths began in 1837. Originally the death registration listed when and where a person died, their name and surname, the parent or parents (if the deceased was a child), sex, age, occupation, cause of death, the description and residence of the informant, when the death was registered and the registrar's signature. Further details have since been recorded including the deceased's date and place of birth, maiden surnames and other former surnames of women who have been divorced.\n\nBeginning in 1879, a doctor's certificate was necessary for the issuance of a death certificate (prior to that, no cause of death needed to be given).\n\nThe form of indexing and the layout of register pages generally follows that of England and Wales.\n\nNational registration began in 1855; registrations are rather more detailed than in England and Wales. In the first year of registration many more details than in later years were recorded including the children of the deceased with their ages, the deceased's birthplace and how long they were resident in the district where they died. The burial place was recorded from 1855 to 1860. Standard details have until now included the deceased's name, age, marital status, spouse(s) (if any), details of both parents, cause of death and the informant's description. Current (2011) registrations show the date of birth. The prescribed forms are part of secondary legislation and those for recent years can thus be seen online in the Statute Law Database.\n\nUnlike England and Wales, information is not limited to being supplied in the form of certified copies; original register pages (or filmed images) can be viewed in person at local register offices or at the General Register Office in Edinburgh, online (fees apply) on the Scotlands People website or in microfilms (1855-1875, 1881, 1891) available at family history centres operated by The Church of Jesus Christ of Latter-day Saints.\n\nThese jurisdictions do not form part of the United Kingdom and each has its own registration system. Their older records tend to follow the layout used in England and Wales.\n\nStillbirths (beyond 24 weeks gestation) have been registered since 1927 in a register that is closed from public access. A single stillbirth registration takes the place of both birth and death registration for the stillborn infant. Prior to 1960 such certificates gave no cause of death.\n\nStillbirth certificates can only be ordered by the mother or father of the deceased contacting the General Register Office by phone or letter. In the event of the parents both having died, an adult sibling can order the certificate if they can provide the dates of death for both parents.\n\nRegistration of still-births commenced in 1939. The registers are not open to public view and extracts are only issued \"in exceptional circumstances\".\n\nA 2007 article in \"People\" magazine revealed that in the case of a stillbirth it is not standard practice to issue both a birth certificate and a death certificate. Most states instead issue a \"certificate of birth resulting in stillbirth\".\n\n\n"}
{"id": "3889704", "url": "https://en.wikipedia.org/wiki?curid=3889704", "title": "Emerging technologies", "text": "Emerging technologies\n\nEmerging technologies are technologies that are perceived as capable of changing the status quo. These technologies are generally new but include older technologies that are still controversial and relatively undeveloped in potential, such as preimplantation genetic diagnosis and gene therapy which date to 1989 and 1990 respectively.\n\nEmerging technologies are characterized by radical novelty, relatively fast growth, coherence, prominent impact, and uncertainty and ambiguity. In other words, an emerging technology can be defined as \"a radically novel and relatively fast growing technology characterised by a certain degree of coherence persisting over time and with the potential to exert a considerable impact on the socio-economic domain(s) which is observed in terms of the composition of actors, institutions and patterns of interactions among those, along with the associated knowledge production processes. Its most prominent impact, however, lies in the future and so in the emergence phase is still somewhat uncertain and ambiguous.\".\n\nEmerging technologies include a variety of technologies such as educational technology, information technology, nanotechnology, biotechnology, cognitive science, psychotechnology, robotics, and artificial intelligence.\n\nNew technological fields may result from the technological convergence of different systems evolving towards similar goals. Convergence brings previously separate technologies such as voice (and telephony features), data (and productivity applications) and video together so that they share resources and interact with each other, creating new efficiencies.\n\nEmerging technologies are those technical innovations which represent progressive developments within a field for competitive advantage; converging technologies represent previously distinct fields which are in some way moving towards stronger inter-connection and similar goals. However, the opinion on the degree of the impact, status and economic viability of several emerging and converging technologies.\n\nIn the history of technology, emerging technologies are contemporary advances and innovation in various fields of technology.\n\nOver centuries innovative methods and new technologies are developed and opened up. Some of these technologies are due to theoretical research, and others from commercial research and development.\n\nTechnological growth includes incremental developments and disruptive technologies. An example of the former was the gradual roll-out of DVD (digital video disc) as a development intended to follow on from the previous optical technology compact disc. By contrast, disruptive technologies are those where a new method replaces the previous technology and makes it redundant, for example, the replacement of horse-drawn carriages by automobiles and other vehicles.\n\nMany writers, including computer scientist Bill Joy, have identified clusters of technologies that they consider critical to humanity's future. Joy warns that the technology could be used by elites for good or evil. They could use it as \"good shepherds\" for the rest of humanity, or decide everyone else is superfluous and push for mass extinction of those made unnecessary by technology.\n\nAdvocates of the benefits of technological change typically see emerging and converging technologies as offering hope for the betterment of the human condition. Cyberphilosophers Alexander Bard and Jan Söderqvist argue in \"The Futurica Trilogy\" that while Man himself is basically constant throughout human history (genes change very slowly), all relevant change is rather a direct or indirect result of technological innovation (memes change very fast) since new ideas always emanate from technology use and not the other way around. Man should consequently be regarded as history's main constant and technology as its main variable. However, critics of the risks of technological change, and even some advocates such as transhumanist philosopher Nick Bostrom, warn that some of these technologies could pose dangers, perhaps even contribute to the extinction of humanity itself; i.e., some of them could involve existential risks.\n\nMuch ethical debate centers on issues of distributive justice in allocating access to beneficial forms of technology. Some thinkers, such as environmental ethicist Bill McKibben, oppose the continuing development of advanced technology partly out of fear that its benefits will be distributed unequally in ways that could worsen the plight of the poor. By contrast, inventor Ray Kurzweil is among techno-utopians who believe that emerging and converging technologies could and will eliminate poverty and abolish suffering.\n\nSome analysts such as Martin Ford, author of \"The Lights in the Tunnel: Automation, Accelerating Technology and the Economy of the Future\", argue that as information technology advances, robots and other forms of automation will ultimately result in significant unemployment as machines and software begin to match and exceed the capability of workers to perform most routine jobs.\n\nAs robotics and artificial intelligence develop further, even many skilled jobs may be threatened. Technologies such as machine learning may ultimately allow computers to do many knowledge-based jobs that require significant education. This may result in substantial unemployment at all skill levels, stagnant or falling wages for most workers, and increased concentration of income and wealth as the owners of capital capture an ever-larger fraction of the economy. This in turn could lead to depressed consumer spending and economic growth as the bulk of the population lacks sufficient discretionary income to purchase the products and services produced by the economy.\n\n\"Artificial intelligence\" (\"AI\") is the sub intelligence exhibited by machines or software, and the branch of computer science that develops machines and software with animal-like intelligence. Major AI researchers and textbooks define the field as \"the study and design of intelligent agents\", where an intelligent agent is a system that perceives its environment and takes actions that maximize its chances of success. John McCarthy, who coined the term in 1942, defines it as \"the study of making intelligent machines\".\n\nThe central problems (or goals) of AI research include reasoning, knowledge, planning, learning, natural language processing (communication), perception and the ability to move and manipulate objects. General intelligence (or \"strong AI\") is still among the field's long-term goals. Currently popular approaches include deep learning, statistical methods, computational intelligence and traditional symbolic AI. There are an enormous number of tools used in AI, including versions of search and mathematical optimization, logic, methods based on probability and economics, and many others.\n\n3D printing, also known as additive manufacturing, has been posited by Jeremy Rifkin and others as part of the third industrial revolution.\n\nCombined with Internet technology, 3D printing would allow for digital blueprints of virtually any material product to be sent instantly to another person to be produced on the spot, making purchasing a product online almost instantaneous.\n\nAlthough this technology is still too crude to produce most products, it is rapidly developing and created a controversy in 2013 around the issue of 3D printed guns.\n\nGene therapy was first successfully demonstrated in late 1990/early 1991 for adenosine deaminase deficiency, though the treatment was somatic – that is, did not affect the patient's germ line and thus was not heritable. This led the way to treatments for other genetic diseases and increased interest in germ line gene therapy – therapy affecting the gametes and descendants of patients.\n\nBetween September 1990 and January 2014 there were around 2,000 gene therapy trials conducted or approved.\n\nA \"cancer vaccine\" is a vaccine that treats existing cancer or prevents the development of cancer in certain high-risk individuals. Vaccines that treat existing cancer are known as \"therapeutic\" cancer vaccines. There are currently no vaccines able to prevent cancer in general.\n\nOn April 14, 2009, Dendreon Corporation announced that their Phase III clinical trial of Provenge, a cancer vaccine designed to treat prostate cancer, had demonstrated an increase in survival. It received U.S. Food and Drug Administration (FDA) approval for use in the treatment of advanced prostate cancer patients on April 29, 2010. The approval of Provenge has stimulated interest in this type of therapy.\n\n\"In vitro meat\", also called \"cultured meat\", \"clean meat\", \"cruelty-free meat\", \"shmeat\", and \"test-tube meat\", is an animal-flesh product that has never been part of a living animal with exception of the fetal calf serum taken from a slaughtered cow. In the 21st century, several research projects have worked on \"in vitro\" meat in the laboratory. The first in vitro beefburger, created by a Dutch team, was eaten at a demonstration for the press in London in August 2013. There remain difficulties to be overcome before \"in vitro\" meat becomes commercially available. Cultured meat is prohibitively expensive, but it is expected that the cost could be reduced to compete with that of conventionally obtained meat as technology improves. \"In vitro\" meat is also an ethical issue. Some argue that it is less objectionable than traditionally obtained meat because it doesn't involve killing and reduces the risk of animal cruelty, while others disagree with eating meat that has not developed naturally.\n\n\"Nanotechnology\" (sometimes shortened to \"nanotech\") is the manipulation of matter on an atomic, molecular, and supramolecular scale. The earliest, widespread description of nanotechnology referred to the particular technological goal of precisely manipulating atoms and molecules for fabrication of macroscale products, also now referred to as molecular nanotechnology. A more generalized description of nanotechnology was subsequently established by the National Nanotechnology Initiative, which defines nanotechnology as the manipulation of matter with at least one dimension sized from 1 to 100 nanometers. This definition reflects the fact that quantum mechanical effects are important at this quantum-realm scale, and so the definition shifted from a particular technological goal to a research category inclusive of all types of research and technologies that deal with the special properties of matter that occur below the given size threshold.\n\n\"Robotics\" is the branch of technology that deals with the design, construction, operation, and application of robots, as well as computer systems for their control, sensory feedback, and information processing. These technologies deal with automated machines that can take the place of humans in dangerous environments or manufacturing processes, or resemble humans in appearance, behavior, and/or cognition. A good example of robots which resembles humans is Sophia, a social humanoid robot developed by Hong Kong-based company Hanson Robotics which was activated on April 19, 2015. Many of today's robots are inspired by nature contributing to the field of bio-inspired robotics.\n\n\"Stem cell therapy\" is an intervention strategy that introduces new adult stem cells into damaged tissue in order to treat disease or injury. Many medical researchers believe that stem cell treatments have the potential to change the face of human disease and alleviate suffering. The ability of stem cells to self-renew and give rise to subsequent generations with variable degrees of differentiation capacities, offers significant potential for generation of tissues that can potentially replace diseased and damaged areas in the body, with minimal risk of rejection and side effects.\n\n\"Distributed ledger\" or \"blockchain technology\" is a technology which provides transparent and immutable lists of transactions. Blockchains can enable autonomous transactions through the use of smart contracts. Smart contracts are self-executing transactions which occur when pre-defined conditions are met. The original idea of a smart contract was conceived by Nick Szabo in 1994 but these original theories about how these smart contracts could work remained unrealised because there was no technology to support programmable agreements and transactions between parties. His example of a smart contract was the vending machine that holds goods until money has been received and then the goods are released to the buyer. The machine holds the property and is able to enforce the contract. There were two main issues that needed to be addressed before smart contracts could be used in the real world. Firstly, the control of physical assets by smart contracts to be able to enforce agreements. Secondly, the last of trustworthy computers that are reliable and trusted to execute the contract between two or more parties. It is only with the advent of cryptocurrency and encryption that the technology for smart contracts has come to fruition. Many potential applications of smart contracts have been suggested that go beyond the transfer of value from one party to another, such as supply chain management, electronic voting, law and the internet of things.\n\nAs innovation drives economic growth, and large economic rewards come from new inventions, a great deal of resources (funding and effort) go into the development of emerging technologies. Some of the sources of these resources are described below...\n\nResearch and development is directed towards the advancement of technology in general, and therefore includes development of emerging technologies. \"See also List of countries by research and development spending.\"\n\nApplied research is a form of systematic inquiry involving the practical application of science. It accesses and uses some part of the research communities' (the academia's) accumulated theories, knowledge, methods, and techniques, for a specific, often state-, business-, or client-driven purpose.\n\nScience policy is the area of public policy which is concerned with the policies that affect the conduct of the science and research enterprise, including the funding of science, often in pursuance of other national policy goals such as technological innovation to promote commercial product development, weapons development, health care and environmental monitoring.\n\nThe Defense Advanced Research Projects Agency (DARPA) is an agency of the U.S. Department of Defense responsible for the development of emerging technologies for use by the military.\n\nDARPA was created in 1958 as the Advanced Research Projects Agency (ARPA) by President Dwight D. Eisenhower. Its purpose was to formulate and execute research and development projects to expand the frontiers of technology and science, with the aim to reach beyond immediate military requirements.\n\nProjects funded by DARPA have provided significant technologies that influenced many non-military fields, such as the Internet and Global Positioning System technology.\n\nThere are awards that provide incentive to push the limits of technology (generally synonymous with emerging technologies). Note that while some of these awards reward achievement after-the-fact via analysis of the merits of technological breakthroughs, others provide incentive via competitions for awards offered for goals yet to be achieved.\n\nThe Orteig Prize was a $25,000 award offered in 1919 by French hotelier Raymond Orteig for the first nonstop flight between New York City and Paris. In 1927, underdog Charles Lindbergh won the prize in a modified single-engine Ryan aircraft called the Spirit of St. Louis. In total, nine teams spent $400,000 in pursuit of the Orteig Prize.\n\nThe XPRIZE series of awards, public competitions designed and managed by the non-profit organization called the X Prize Foundation, are intended to encourage technological development that could benefit mankind. The most high-profile XPRIZE to date was the $10,000,000 Ansari XPRIZE relating to spacecraft development, which was awarded in 2004 for the development of SpaceShipOne.\n\nThe Turing Award is an annual prize given by the Association for Computing Machinery (ACM) to \"an individual selected for contributions of a technical nature made to the computing community\". It is stipulated that \"The contributions should be of lasting and major technical importance to the computer field\". The Turing Award is generally recognized as the highest distinction in computer science, and in 2014 grew to $1,000,000.\n\nThe Millennium Technology Prize is awarded once every two years by Technology Academy Finland, an independent fund established by Finnish industry and the Finnish state in partnership. The first recipient was Tim Berners-Lee, inventor of the World Wide Web.\n\nIn 2003, David Gobel seed-funded the Methuselah Mouse Prize (Mprize) to encourage the development of new life extension therapies in mice, which are genetically similar to humans. So far, three Mouse Prizes have been awarded: one for breaking longevity records to Dr. Andrzej Bartke of Southern Illinois University; one for late-onset rejuvenation strategies to Dr. Stephen Spindler of the University of California; and one to Dr. Z. Dave Sharp for his work with the pharmaceutical rapamycin.\n\nScience fiction has criticized developing and future technologies, but also inspires innovation and new technology. This topic has been more often discussed in literary and sociological than in scientific forums. Cinema and media theorist Vivian Sobchack examines the dialogue between science fiction films and technological imagination. Technology impacts artists and how they portray their fictionalized subjects, but the fictional world gives back to science by broadening imagination. \"How William Shatner Changed the World\" is a documentary that gave a number of real-world examples of actualized technological imaginations. While more prevalent in the early years of science fiction with writers like Arthur C. Clarke, new authors still find ways to make currently impossible technologies seem closer to being realized.\n\n\n\n\n\n\n"}
{"id": "1260600", "url": "https://en.wikipedia.org/wiki?curid=1260600", "title": "Existential humanism", "text": "Existential humanism\n\nExistential humanism is humanism that validates the human subject as struggling for self-knowledge and self-responsibility.\n\nSøren Kierkegaard suggested that the best use of our capacity for making choices is to freely choose to live a fully human life, rooted in a personal search for values, rather than an external code.\n\nJean-Paul Sartre said \"existentialism is a humanism\" because it expresses the power of human beings to make freely-willed choices, independent of the influence of religion or society. Unlike traditional humanisms, however, Sartre disavowed any reliance on an essential nature of man – on deriving values from the facts of human nature – but rather saw human value as self-created through undertaking projects in the world: experiments in living.\nAlbert Camus, in his book \"The Plague\", suggests that some of us may choose to be heroic, even knowing that it will bring us neither reward nor salvation; and Simone de Beauvoir, in her book \"The Ethics of Ambiguity\", argues that embracing our own personal freedom requires us to fight for the freedoms of all humanity.\n\nMartin Heidegger attacked Sartre's concept of existential humanism in his \"Letter on Humanism\" of 1946, accusing Sartre of elevating Reason above Being.\n\nMichel Foucault followed Heidegger in attacking Sartre's humanism as a kind of theology of man, though in his emphasis on the self-creation of the human being he has in fact been seen as very close to Sartre's existential humanism.\n"}
{"id": "26885650", "url": "https://en.wikipedia.org/wiki?curid=26885650", "title": "Free will in antiquity", "text": "Free will in antiquity\n\nFree will in antiquity was not discussed in the same terms as used in the modern free will debates, but historians of the problem have speculated who exactly was first to take positions as determinist, libertarian, and compatibilist in antiquity. There is wide agreement that these views were essentially fully formed over 2000 years ago. Candidates for the first thinkers to form these views, as well as the idea of a non-physical \"agent-causal\" libertarianism, include Democritus (460–370), Aristotle (384–322), Epicurus (341–270), Chrysippus (280–207), and Carneades (214–129).\n\nEarly religious accounts of man's fate explored the degree of human freedom permitted by superhuman gods. A strong fatalism is present in tales that foretell the future, based on the idea that the gods have foreknowledge of future events. Anxious not to annoy the gods, the myth-makers rarely challenged the idea that the gods' foreknowledge is compatible with human freedom.\n\nThe first thinkers to look for explanatory causes (ἀιτία) in natural phenomena (rather than gods controlling events) were the Greek Pre-Socratic philosophers (\"physiologoi\"). The reasons or rules (λόγοι) behind the physical (φύσις) world became the ideal \"laws\" governing material phenomena. Anaximander (610-546) coined the term physis (φύσις) and perhaps even the cosmological combination of cosmos (κόσμος), as organized nature, and logos (λόγος), as the law behind nature. The Greeks had a separate word for the laws (or conventions) of society, nomos (νόμος). Heraclitus (535–475) claimed that everything changes (\"you can't step twice into the same river\") but that there were laws or rules (the logos) behind all the change. The early cosmologists imagined that the universal laws were all-powerful and must therefore explain the natural causes behind all things, from the regular motions of the heavens to the mind (νοῦς) of man. These \"physiologoi\" transformed pre-philosophical arguments about gods controlling the human will into arguments about pre-existing causes controlling it.\n\nThe materialist philosophers Democritus and his mentor Leucippus were the first determinists. They claimed that all things, including humans, were made of atoms in a void, with individual atomic motions strictly controlled by causal laws. Democritus said:\n\nDemocritus wanted to wrest control of man's fate from arbitrary gods and make us more responsible for our actions. But ironically, he and Leucippus originated two of the great dogmas of determinism, physical determinism and logical necessity, which lead directly to the traditional and modern problem of free will and determinism.\n\nLeucippus dogmatically declared an absolute necessity which left no room in the cosmos for chance.\n\nThe consequence is a world with but one possible future, completely determined by its past.\n\nIn Plato's Gorgias (and in the Protagoras 345c4-e6), Socrates argues that no one does wrong willingly, one of the most famous doctrines to be associated with him. When framed in modern (Western) terms, the implication is that it is ignorance, rather that free individual agency, that is responsible for morally wrong actions.\n\nMichael Frede typifies the prevailing view of recent scholarship, namely that Aristotle did not have a notion of free-will.\n\nAristotle elaborated the four possible causes (material, efficient, formal, and final). Aristotle's word for these causes was ἀιτία, which translates as causes in the sense of the multiple factors responsible for an event. Aristotle did not subscribe to the simplistic \"every event has a (single) cause\" idea that was to come later.\nThen, in his \"Physics\" and \"Metaphysics\", Aristotle also said there were \"accidents\" caused by \"chance (τυχή)\". In his \"Physics\", he noted that the early physicists had found no place for chance among their causes.\n\nAristotle opposed his accidental chance to necessity:\n\nTracing any particular sequence of events back in time will usually come to an accidental event – a \"starting point\" or \"fresh start\" (Aristotle calls it an origin or arche (ἀρχή) – whose major contributing cause (or causes) was itself uncaused.\n\nWhether a particular thing happens, says Aristotle, may depend on a series of causes that\n\nIn general, many such causal sequences contribute to any event, including human decisions. Each sequence has a different time of origin, some going back before we were born, some originating during our deliberations. Beyond causal sequences that are the result of chance or necessity, Aristotle felt that some breaks in the causal chain allow us to feel our actions \"depend on us\" (ἐφ' ἡμῖν). These are the causal chains that originate within us (ἐv ἡμῖν).\n\nRichard Sorabji's 1980 \"Necessity, Cause, and Blame\" surveyed Aristotle's positions on causation and necessity, comparing them to his predecessors and successors, especially the Stoics and Epicurus. Sorabji argues that Aristotle was an indeterminist, that real chance and uncaused events exist, but never that human actions are uncaused in the extreme libertarian sense that some commentators mistakenly attribute to Epicurus.\n\nAristotle accepted the past as fixed, in the sense that past events were irrevocable. But future events cannot be necessitated by claims about the present truth value of statements about the future. Aristotle does not deny the excluded middle (either p or not p), only that the truth value of p does not exist yet. Indeed, although the past is fixed, the truth value of past statements about the future can be changed by the outcome of future events. This is the problem of future contingents.\n\nAlthough he thinks Aristotle was not aware of the \"problem\" of free will vis-a-vis determinism (as first described by Epicurus), Sorabji thinks Aristotle's position on the question is clear enough. Voluntariness is too important to fall before theoretical arguments about necessity and determinism.\n\nIt is with Epicurus and the Stoics that clearly indeterministic and deterministic positions are first formulated. Writing one generation after Aristotle, Epicurus argued that as atoms moved through the void, there were occasions when they would \"swerve\" (\"clinamen\") from their otherwise determined paths, thus initiating new causal chains. Epicurus argued that these swerves would allow us to be more responsible for our actions (\"libertarianism\"), something impossible if every action was deterministically caused.\n\nEpicurus did not say the swerve was directly involved in decisions. But following Aristotle, Epicurus thought human agents have the autonomous ability to transcend necessity and chance (both of which destroy responsibility), so that praise and blame are appropriate. Epicurus finds a \"tertium quid\", beyond necessity (Democritus' physics) and beyond chance. His \"tertium quid\" is agent autonomy, what is \"up to us\".\n\nLucretius (1st century BCE), a strong supporter of Epicurus, saw the randomness as enabling free will, even if he could not explain exactly how, beyond the fact that random swerves would break the causal chain of determinism.\n\nIn 1967, Pamela Huby suggested that Epicurus was the original discoverer of the \"freewill problem\". Huby noted that there had been two main free will problems, corresponding to different determinisms, namely theological determinism (predestination and foreknowledge) and the physical causal determinism of Democritus.\nIt is unfortunate that our knowledge of the early history of the Stoics is so fragmentary, and that we have no agreed account of the relations between them and Epicurus. On the evidence we have, however, it seems to me more probable that Epicurus was the originator of the freewill controversy, and that it was only taken up with enthusiasm among the Stoics by Chrysippus, the third head of the school.\n\nIn 2000 Susanne Bobzien challenged Pamela Huby's 1967 assertion that Epicurus discovered the \"free will problem\".\n\nBobzien thinks Epicurus did not have a model of what she calls \"two-sided freedom,\" because she believes that Epicurus\n\n\"assumed...a gap in the causal chain immediately before, or simultaneously with, the decision or choice, a gap which allows the coming into being of a spontaneous motion. In this way every human decision or choice is directly linked with causal indeterminism...To avoid misunderstandings, I should stress that I do believe that Epicurus was an indeterminist of sorts – only that he did not advocate indeterminist free decision or indeterminist free choice.\nA. A. Long and D. N. Sedley, however, agree with Pamela Huby that Epicurus was the first to notice the modern problem of free will and determinism.\n\nEpicurus' problem is this: if it has been necessary all along that we should act as we do, it cannot be up to us, with the result that we would not be morally responsible for our actions at all. Thus posing the problem of determinism he becomes arguably the first philosopher to recognize the philosophical centrality of what we know as the Free Will Question. His strongly libertarian approach to it can be usefully contrasted with the Stoics' acceptance of determinism.\n\nThe question remains how random swerves can help to explain free action. In her 1992 book, \"The Hellenistic Philosophy of Mind\", Julia Annas wrote:\n\nOne view, going back to the 19th century historian Carlo Giussani, is that Epicurus' atomic swerves are involved directly in every case of human free action, not just somewhere in the past that breaks the causal chain of determinism. In 1928 Cyril Bailey agreed with Giussani that the atoms of the mind-soul provide a break in the continuity of atomic motions, otherwise actions would be necessitated. Bailey imagined complexes of mind-atoms that work together to form a consciousness that is not determined, but also not susceptible to the pure randomness of individual atomic swerves, something that could constitute Epicurus' idea of actions being \"up to us\" (πὰρ' ἡμάς). Bailey states that Epicurus did not \"identify\" freedom of the will with chance.\n\nIt may be that [Giussani's] account presses the Epicurean doctrine slightly beyond the point to which the master had thought it out for himself, but it is a direct deduction from undoubted Epicurean conceptions and is a satisfactory explanation of what Epicurus meant: that he should have thought that the freedom of the will was chance, and fought hard to maintain it as chance and no more, is inconceivable.\nIn 1967 David Furley de-emphasized the importance of the swerve in both Epicurus and Lucretius so as to defend Epicurus from the \"extreme\" libertarian view that our actions are caused directly by random swerves. (Bailey had also denied this \"traditional interpretation\".) Furley argues for a strong connection between the ideas of Aristotle and Epicurus on autonomous actions that are \"up to us\".\n\nThe swerve, then, plays a purely negative part in Epicurean psychology. It saves \"voluntas\" from necessity, as Lucretius says it does, but it does not feature in every act of \"voluntas\".\nOn the other hand, in his 1983 thesis, \"Lucretius on the Clinamen and 'Free Will'\", Don Paul Fowler defended the ancient claim that Epicurus proposed random swerves as directly causing our actions.\n\nI turn to the overall interpretation. Lucretius is arguing from the existence of \"voluntas\" to the existence of the \"clinamen\"; nothing comes to be out of nothing, therefore \"voluntas\" must have a cause at the atomic level, viz. the \"clinamen\". The most natural interpretation of this is that every act of \"voluntas\" is caused by a swerve in the atoms of the animal's mind...There is a close causal, physical relationship between the macroscopic and the atomic. Furley, however, argued that the relationship between \"voluntas\" and the \"clinamen\" was very different; not every act of volition was accompanied by a swerve in the soul-atoms, but the \"clinamen\" was only an occasional event which broke the chain of causation between the σύστασις of our mind at birth and the 'engendered' state (τὸ ἀπογεγεννημένον) which determines our actions.Its role in Epicureanism is\nmerely to make a formal break with physical determinism, and it has no real effect on the outcome of particular actions. (p. 338).\n\nIn a 1999 \"Phronesis\" article, Purinton agreed with Fowler that random swerves directly cause volitions and actions:\"since they do not make volition itself a fresh start of motion, and Sedley's view does not do justice to his atomism...It seems to me, therefore, that there is no good reason to reject the thesis that Epicurus held that swerves cause volitions from the bottom up. And there are a number of good reasons to accept it.\"\n\nIt was the Stoic school of philosophy that solidified the idea of natural laws controlling all things, including the mind. Zeno of Citium, the original founder of Stoicism, had a simple but powerful idea of the causal chain compared to Aristotle. Zeno said that every event has a cause, and that cause necessitates the event. Given exactly the same circumstances, exactly the same result will occur.\n\nThe major developer of Stoicism, Chrysippus, took the edge off strict necessity. Like Democritus, Aristotle, and Epicurus before him, he wanted to strengthen the argument for moral responsibility, in particular defending it from Aristotle's and Epicurus' indeterminate chance causes.\n\nWhereas the past is unchangeable, Chrysippus argued that some future events that are possible do not occur by necessity from past external factors alone, but might (as Aristotle and Epicurus maintained) depend on us. We have a choice to assent or not to assent to an action. Chrysippus said our actions are determined (in part by ourselves as causes) and fated (because of God's foreknowledge), but he also said that they are not necessitated, i.e., pre-determined from the distant past. Chrysippus would be seen today as a compatibilist.\n\nR. W. Sharples describes the first compatibilist arguments to reconcile responsibility and determinism by Chrysippus\n\nThe Stoic position, given definitive expression by Chrysippus (c. 280–207 BC), the third head of the school, represents not the opposite extreme from that of Epicurus but an attempt to compromise, to combine determinism and responsibility. Their theory of the universe is indeed a completely deterministic one; everything is governed by fate, identified with the sequence of causes; nothing could happen otherwise than it does, and in any given set of circumstances one and only one result can follow – otherwise an uncaused motion would occur.\n\nChrysippus was concerned to preserve human responsibility in the context of his determinist system. His position was thus one of 'soft determinism', as opposed on the one hand to that of the 'hard determinist' who claims that determinism excludes responsibility, and on the other to that of the libertarian who agrees on the incompatibility but responsibility by determinism. The Greek to eph' hemin (ἐφ΄ ἡμῖν), 'what depends on us', like the English 'responsibility', was used both by libertarians and by soft determinists, though they differed as to what it involved; thus he occurrence of the expression is not a safe guide to the type of position involved. The situation is complicated by the fact that the debate is in Greek philosophy conducted entirely in terms of responsibility (to eph' hemin) rather than of freedom or free will; nevertheless it can be shown that some thinkers, Alexander among them, have a libertarian rather than a soft-determinist conception of responsibility, and in such cases I have not hesitated to use expressions like 'freedom'.\nAlexander of Aphrodisias (c. 150–210), the most famous ancient commentator on Aristotle, wrote in the age of Stoics, Epicureans, and Skeptics. He defended a view of moral responsibility we would call libertarianism today. Greek philosophy had no precise term for \"free will\" as did Latin (\"liberum arbitrium\" or \"libera voluntas\"). The discussion was in terms of responsibility, what \"depends on us\" (in Greek ἐφ ἡμῖν).\n\nAlexander believed that Aristotle was not a strict determinist like the Stoics, and Alexander himself argued that some events do not have pre-determined causes. In particular, man is responsible for self-caused decisions, and can choose to do or not to do something, as Chrysippus argued. However, Alexander denied the foreknowledge of events that was part of the Stoic identification of God and Nature.\n\nR. W. Sharples described Alexander's \"De Fato\" as perhaps the most comprehensive treatment surviving from classical antiquity of the problem of responsibility (τὸ ἐφ’ ἡμίν) and determinism. It especially shed a great deal of light on Aristotle's position on free will and on the Stoic attempt to make responsibility compatible with determinism.\n\n"}
{"id": "44804130", "url": "https://en.wikipedia.org/wiki?curid=44804130", "title": "Greg Lundgren", "text": "Greg Lundgren\n\nGreg Lundgren is a Seattle-based artist, author, filmmaker and entrepreneur.\n\nVital 5 Productions was a \"one-man arts organization\" for which Lundgren won a Genius Award in 2003. The program created exhibits, publications and issued grants. In 2007, it was the subject of an eight-week 911 Media Arts Center retrospective called \"Straight to Video: the first 10 years of Vital 5\".\n\nLundgren wrote \"The Vital 5 Cookbook\", published in 2006, as a set of \"recipes\" for exhibition and self-expression. The title may have been a reference to \"The Anarchist Cookbook\".\n\nLundgren started Vital 5's Arbitrary Art Grants program in 2009, issuing $500 grants to local artists to \"serve as catalysts to create large-scale group projects and performances\".\n\nIn 2015, Vital 5 Productions retrofitted the 3rd floor of the historic King Street Station in downtown Seattle for contemporary art exhibition. This 22,000 square foot space hosted Out of Sight - a survey of contemporary art in the Pacific Northwest concurrent with the Seattle Art Fair. Giant Steps - a 48 Hour Artist Residency on the Moon, a group exhibition and competition, opened in the space on March 3, 2016. The second year of Out of Sight will launch on August 4, 2016.\n\nHis funeral monument business, Lundgren Monuments, opened in 2004, and he opened a \"death boutique\" showroom on Seattle's First Hill in 2008 including work by other artists such as Jesse Edwards (artist) and Michael Leavitt (artist). Lundgren has been noted for \"bring[ing] more art and design into the world\" of death care, and creating \"a renaissance in the funerary arts in 21st-century America.\"\n\nLundgren Monuments specializes in large-scale cast glass monuments with the intent of bringing more color, light and diversity into the cemetery landscape. They also design and build modern urns and host group exhibitions focused on contemporary design and alternatives to traditional death care.\n\nAn exhibit at Lundgren Monuments in 2010 was called \"the first time in history that a group of architects have focused their talents on the cremation urn as an architectural object\".\n\nAn urn/artwork called \"The Final Turn\", which he collaborated with architect Tom Kundig in designing, was noted in \"Robb Report\" and \"The New York Times\", and is shown in Cooper-Hewitt's National Design Awards gallery.\n\nLundgren, along with mortician and author Caitlin Doughty, TED speaker Jae Rhim Lee, alternative funeral home director Jeff Jorgenson, and other death professionals, founded The Order of the Good Death, promoting alternative death care and putting Seattle in the forefront of this new endeavor.\n\n\nLundgren has written two children's books and one book about making art.\n\nLundgren's feature length one-take film \"CHAT\", starring Rosalie Edholm as a camgirl sex worker, was screened at the Northwest Film Forum in July, 2014, and again in September for Seattle's Local Sightings Film Festival.\n\n\n"}
{"id": "43858552", "url": "https://en.wikipedia.org/wiki?curid=43858552", "title": "High-performance Integrated Virtual Environment", "text": "High-performance Integrated Virtual Environment\n\nThe High-performance Integrated Virtual Environment (HIVE) is a distributed computing environment used for biological research, including analysis of Next Generation Sequencing (NGS) data, post market data, adverse events, metagenomic data, etc.\n\nHIVE is a massively parallel distributed computing environment where the distributed storage library and the distributed computational powerhouse are linked seamlessly. The system is both robust and flexible due to maintaining both storage and the metadata database on the same network. The distributed storage layer of software is the key component for file and archive management and is the backbone for the deposition pipeline. The data deposition back-end allows automatic uploads and downloads of external datasets into HIVE data repositories. The metadata database can be used to maintain specific information about extremely large files ingested into the system (big data) as well as metadata related to computations run on the system. This metadata then allows details of a computational pipeline to be brought up easily in the future in order to validate or replicate experiments. Since the metadata is associated with the computation, it stores the parameters of any computation in the system eliminating manual record keeping.\n\nDifferentiating HIVE from other object oriented databases is that HIVE implements a set of unified APIs to search, view, and manipulate data of all types. The system also facilitates a highly secure hierarchical access control and permission system, allowing determination of data access privileges in a finely granular manner without creating a multiplicity of rules in the security subsystem. The security model, designed for sensitive data, provides comprehensive control and auditing functionality in compliance with HIVE's designation as a FISMA Moderate system.\n\n\n\nFDA launched HIVE Open Source as a platform to support end to end needs for NGS analytics. \nhttps://github.com/FDA/fda-hive\n\nHIVE biocompute harmonization platform is at the core of High-throughput Sequencing Computational Standards for Regulatory Sciences (HTS-CSRS) project. Its mission is to provide the scientific community with a framework to harmonize biocomputing, promote interoperability, and verify bioinformatics protocols (https://hive.biochemistry.gwu.edu/htscsrs). For more information, see the project description on the FDA Extramural Research page (https://www.fda.gov/ScienceResearch/SpecialTopics/RegulatoryScience/ucm491893.htm\n\nSub-clusters of scalable high performance high density compute cores are there to serve as a powerhouse for extra-large distributed parallelized computations of NGS algorithmics. System is extremely scalable and has deployment instances ranging from a single HIVE in a box appliance to massive enterprise level systems of thousands of compute units.\n\n\n\n\n"}
{"id": "11350161", "url": "https://en.wikipedia.org/wiki?curid=11350161", "title": "Human-rating certification", "text": "Human-rating certification\n\nHuman-rating certification, also known as man-rating, is the certification of a spacecraft or launch vehicle as capable of safely transporting humans. (Since the 1990s, NASA and the U.S. GAO prefer to use \"human-rating\" instead of \"man-rating\" when describing requirements for these systems.)\n\nAccording to NASA, human-rating requires not just that a system is designed to be tolerant of failure and to protect the crew even if unrecoverable failure occurs, but also that astronauts aboard a human-rated spacecraft must have some control over it. This set of technical requirements and the associated certification process for crewed space systems is required in addition to the standards and requirements that are mandatory for all of NASA's space flight programs.\n\nIn November 2011, Ed Mango, the agency head of the NASA Commercial Crew Program (CCP), gave an extended interview on the new NASA requirements for human rating of spacecraft that will fly to the International Space Station (ISS).\n\nThe NASA CCP human-rating standards require that the probability of a loss on ascent is no more than 1 in 500, and that the probability of a loss on descent is no more than 1 in 500. The overall mission loss risk, which includes vehicle risk from micrometeorites and orbital debris while in orbit for up to 210 days, is required to be no more than 1 in 270. Maximum sustained G-loads are limited to three Earth-standard g's.\n\nThe development of the Space Shuttle and the International Space Station pre-dates the NASA Human-Rating requirements. After the Challenger and Columbia accidents, the criteria used by NASA for human-rating spacecraft have been made more stringent.\n\nThe United Launch Alliance (ULA) published a paper submitted to AIAA detailing the modifications to its Delta IV and Atlas V launch vehicles that would be needed to conform to NASA Standard 8705.2B. \nULA has since been awarded $6.7 million under NASA's Commercial Crew Development (CCDev) program for development of an Emergency Detection System, one of the final pieces that would be needed to make these launchers suitable for human spaceflight.\n\n"}
{"id": "182970", "url": "https://en.wikipedia.org/wiki?curid=182970", "title": "Immorality", "text": "Immorality\n\nImmorality is the violation of moral laws, norms or standards. Immorality is normally applied to people or actions, or in a broader sense, it can be applied to groups or corporate bodies, beliefs, religions, and works of art.\n\nAristotle saw many vices as excesses or deficits in relation to some virtue, as cowardice and rashness relate to courage. Some attitudes and actionssuch as envy, murder, and thefthe saw as wrong in themselves, with no question of a deficit/excess in relation to the mean.\n\nImmorality is often closely linked with both religion and sexuality. Max Weber saw rational articulated religions as engaged in a long-term struggle with more physical forms of religious experience linked to dance, intoxication and sexual activity. Durkheim pointed out how many primitive rites culminated in abandoning the distinction between licit and immoral behavior.\n\nFreud's dour conclusion was that \"In every age immorality has found no less support in religion than morality has\".\n\nCoding of sexual behavior has historically been a feature of all human societies, as too; has been the policing of breaches of its moressexual immoralityby means of formal and informal social control. Interdictions and taboos among primitive societies were arguably no less severe than in traditional agrarian societies. In the latter, the degree of control might vary from time to time and region to region, being least in urban settlements; however, only the last three centuries of intense urbanisation, commercialisation and modernisation have broken with the restrictions of the pre-modern world, in favor of a successor society of fractured and competing sexual codes and subcultures, where sexual expression is integrated into the workings of the commercial world.\n\nNevertheless, while the meaning of sexual immorality has been drastically redefined in recent times, arguably the boundaries of what is acceptable remain publicly policed and as highly charged as ever, as the decades-long debates in the US over reproductive rights after \"Roe v. Wade\", or 21st-century controversy over child images on Wikipedia and Amazon would tend to suggest.\n\nMichel Foucault considered that the modern world was unable to put forward a coherent moralityan inability underpinned philosophically by emotivism. Nevertheless, modernism has often been accompanied by a cult of immorality, as for example when John Ciardi acclaimed Naked Lunch as \"a monumentally moral descent into the hell of narcotic addiction\".\n\nPsychoanalysis received much early criticism for being the unsavory product of an immoral townVienna; psychoanalysts for being both unscrupulous and dirty-minded.\n\nFreud himself however was of the opinion that \"anyone who has succeeded in educating himself to truth about himself is permanently defended against the danger of immorality, even though his standard of morality may differ\".\n\n\n"}
{"id": "48517570", "url": "https://en.wikipedia.org/wiki?curid=48517570", "title": "Japanese submarine I-185", "text": "Japanese submarine I-185\n\nThe Japanese submarine \"I-185 (originally I-85\") was a \"Kaidai\" type cruiser submarine of the KD7 sub-class built for the Imperial Japanese Navy (IJN) during the 1940s. She was sunk with all hands by an American destroyer during the Battle of the Philippine Sea in mid-1944.\n\nThe submarines of the KD7 sub-class were medium-range attack submarines developed from the preceding KD6 sub-class. They displaced surfaced and submerged. The submarines were long, had a beam of and a draft of . The boats had a diving depth of and a complement of 86 officers and crewmen.\n\nFor surface running, the boats were powered by two diesel engines, each driving one propeller shaft. When submerged each propeller was driven by a electric motor. They could reach on the surface and underwater. On the surface, the KD7s had a range of at ; submerged, they had a range of at .\n\nThe boats were armed with six internal torpedo tubes, all in the bow. They carried one reload for each tube; a total of a dozen torpedoes. They were originally intended to be armed with two twin-gun mounts for the Type 96 anti-aircraft gun, but a deck gun for combat on the surface was substituted for one 25 mm mount during construction.\n\nBuilt by the Yokosuka Naval Arsenal, the boat was laid down on 9 February 1942 as \"I-85\" and renamed \"I-185\" in 1942. She was launched on 16 September 1943 and completed on 23 September. The boat was sunk with the loss of all 95 officers and crewmen aboard by the destroyer on 22 June 1944 near Saipan. \"I-185\" was stricken from the Navy List on 10 September 1944.\n\n"}
{"id": "2969831", "url": "https://en.wikipedia.org/wiki?curid=2969831", "title": "Japanese submarine I-55 (1943)", "text": "Japanese submarine I-55 (1943)\n\nThe Japanese submarine \"I-55\" was one of three Type C cruiser submarines of the C3 sub-class built for the Imperial Japanese Navy (IJN) during the 1940s.\n\nThe Type C3 submarines were derived from the earlier C2 sub-class although with fewer torpedo tubes, an additional deck gun, and less-powerful engines to extend their range. They displaced surfaced and submerged. The submarines were long, had a beam of and a draft of . They had a diving depth of .\n\nFor surface running, the boats were powered by two diesel engines, each driving one propeller shaft. When submerged each propeller was driven by a electric motor. They could reach on the surface and underwater. On the surface, the \"C3\"s had a range of at ; submerged, they had a range of at .\n\nThe boats were armed with six internal bow torpedo tubes and carried a total of 19 torpedoes. They were also armed with two /40 deck guns and one twin mount for Type 96 anti-aircraft guns.\n\nAssigned to the defense of the Marianas, during a mission to rescue staff of the 1st Air Fleet on Tinian, she was presumably found and sunk by the USS \"Gilmer\" and USS \"William C. Miller\" on 14 July 1944, though some sources claim this sinking was actually the Ro-48.\n\nSee also USS \"Wyman\" and USS \"Reynolds\" references to the sinking of this submarine in which Wyman claims credit for this sinking on 28 July 1944.\n\n\n"}
{"id": "27498792", "url": "https://en.wikipedia.org/wiki?curid=27498792", "title": "Jesus predicts his death", "text": "Jesus predicts his death\n\nThere are several references in the Synoptic Gospels (the gospels of Matthew, Mark and Luke) to Jesus predicting his own death, the first two occasions building up to the final prediction of his crucifixion. Matthew's Gospel adds a prediction, before he and his disciples enter Jerusalem, that he will be crucified there.\n\nIn the Gospel of Mark, generally agreed to be the earliest Gospel, written around the year 70, Jesus predicts his death three times. Walter Schmithals, noting that this Gospel also contains verses in which Jesus appears to predict his Passion, suggests that these represent the earlier traditions available to the author, and the three death predictions are redactional creations of the author. The setting for the first prediction is somewhere near Caesarea Philippi, immediately after Peter proclaims Jesus as the Messiah. Jesus tells his followers that \"the Son of Man must suffer many things and be rejected by the elders, chief priests and teachers of the law, and that he must be killed and after three days rise again\". When Peter objects, Jesus tells him: \"Get behind me, Satan! You do not have in mind the things of God, but the things of men\". ()\n\nThe Gospel of includes this episode, saying that Jesus \"from that time\", i.e. on a number of occasions, Jesus \"began to show his disciples that he must go to Jerusalem and suffer many things from the elders and chief priests and scribes, and be killed ...\".\n\nThe Gospel of shortens the account, dropping the dialogue between Jesus and Peter.\n\nEach time Jesus predicts his arrest and death, the disciples in some way or another manifest their incomprehension, and Jesus uses the occasion to teach them new things. The second warning appears in (and also in ) as follows:\nHe said to them, \"The Son of Man is going to be betrayed into the hands of men. They will kill him, and after three days he will rise.\" But they did not understand what he meant and were afraid to ask him about it.\nThe third prediction in the specifically mentions crucifixion:\nNow as Jesus was going up to Jerusalem, he took the twelve disciples aside and said to them, \"We are going up to Jerusalem, and the Son of Man will be betrayed to the chief priests and the teachers of the law. They will condemn him to death and will turn him over to the Gentiles to be mocked and flogged and crucified. On the third day he will be raised to life!\"\nThe fourth prediction in Matthew is found in Matthew 26:1-2 immediately precedes the plot made against him by the religious Jewish leaders:\n\n\"As you know, the Passover is two days away—and the Son of Man will be handed over to be crucified.\"\n\nThe hypothetical Q source, widely considered by scholars to be a collection of sayings of Jesus used, in addition to the Gospel of Mark, by the authors of the Luke and Matthew Gospels, contains no predictions of the death of Jesus.\n\nThe Gospel of John, in chapters 12 to 17, also mentions several occasions where Jesus prepared his disciples for his departure, which the gospel also refers to as his \"glorification\":\nJesus answered them, saying, “The hour has come that the Son of Man should be glorified. Most assuredly, I say to you, unless a grain of wheat falls into the ground and dies, it remains alone; but if it dies, it produces much grain.\nEach of the Synoptic Gospels refers more times Jesus foretelling His death and resurrection after three days. The concordances are summarized in the following table:\nAs shown in the Daily Mass Readings provided in the Latin Rite of the Roman Catholic Church, the prediction given by Jesus in Mark 9:32 has one of its main references in the Wisdom of \"Solomon\":\n\n"}
{"id": "12723149", "url": "https://en.wikipedia.org/wiki?curid=12723149", "title": "Kelgian (Sector General)", "text": "Kelgian (Sector General)\n\nA Kelgian (physiological classification DBLF) is one of about seventy known intelligent species in the fictional Sector General universe, and one of the most common species in Sector 12 General Hospital itself. Overall, A Kelgian resembles a 5–6 ft. long furry caterpillar whose fur is constantly rippling in various patterns. The motion of the fur of a Kelgian is a completely accurate guide to what it is feeling and necessary for communication within the species itself. This trait makes the Kelgians completely incapable of any form of untruth.\n\nKelgians, as indicated by the first letter \"D\" of their classification, are oxygen breathers whose normal gravity approximates the standard gravity of Earth. Their atmosphere is an oxygen-nitrogen combination that also approximates Earth's. They are herbivores, eating plants (such as crelletin vine shoots) by putting their face inside of their bowl. They sleep in an \"S\" or a question mark pose in zero-gravity or, in regular gravity, on a bed.\n\nThey are about 5–6 ft. long once they reach maturity and they walk and lie horizontally, although they can temporarily stand up on their rearmost four legs, at which point they would be at eye level with average humans. They have four extensible eye-stalks which can turn to give a 360° view of their surroundings. Their number of legs varies throughout the series, but is generally held to be 12 pairs, or 24 legs. They have no neck or skeleton, due to an immense and highly vulnerable cardiovascular system. This system is complex, with two hearts and many thin-walled blood vessels near the surface. This, combined with high pressure and a high pulse rate, makes cuts very dangerous; a Kelgian can bleed to death from a minor injury if not controlled. Their skin is thicker than a human's, and they have two mouths, one for their digestive system and one for their respiratory system. A Kelgian female conceives once in her lifetime, producing two offspring of each gender. Their speech sounds like a series of hoots and whines to human ears.\n\nTheir fur, which is always in motion, is a perfect and uncontrollable mirror to their emotional state, and gives emotional context to the Kelgian voice which has no emotion in it. They highly value their fur, as it is very important to their pre-marriage proposals and allows them to speak properly. Doctors operating on them will try to remove as little fur as possible. Because of the properties of their fur, Kelgians never developed the concept of being diplomatic, lying, or politeness (and indeed are infamous for this trait).\n\n"}
{"id": "7218862", "url": "https://en.wikipedia.org/wiki?curid=7218862", "title": "List of human spaceflights, 1981–1990", "text": "List of human spaceflights, 1981–1990\n\nThis is a detailed listing of human spaceflights from 1981 to 1990, spanning the end of the Soviet Union's Salyut space station program, the beginning of Mir, and the start of the US Space Shuttle program.\n"}
{"id": "24640055", "url": "https://en.wikipedia.org/wiki?curid=24640055", "title": "List of rampage killers (workplace killings)", "text": "List of rampage killers (workplace killings)\n\nThe first part of this section of the list of rampage killers contains those mass murders where the perpetrators predominantly targeted their (former) co-workers, while the second part focuses on cases where soldiers willfully killed their own comrades.\n\nA rampage killer has been defined as follows:\n\nThis list should contain every case with at least one of the following features:\n\nAll abbreviations used in the tables are explained below.\n\nW – A basic description of the weapons used in the murders\n"}
{"id": "1181718", "url": "https://en.wikipedia.org/wiki?curid=1181718", "title": "MV Derbyshire", "text": "MV Derbyshire\n\nMV \"Derbyshire\" was an ore-bulk-oil combination carrier built in 1976 by Swan Hunter, as the last in the series of the \"Bridge\"-class sextet. She was registered at Liverpool and owned by Bibby Line.\n\nShe was lost on 9 September 1980 during Typhoon Orchid, south of Japan. All 42 crew members and two of their wives were killed in the sinking. At 91,655 gross register tons, she was—and remains—the largest British ship ever to have been lost at sea.\n\n\"Derbyshire\" was launched in late 1975 and entered service in June 1976, as the last ship of the \"Bridge\"-class combination carrier, originally named \"Liverpool Bridge\". \"Liverpool Bridge\" and \"English Bridge\" (later \"Worcestershire\", and \"Kowloon Bridge\" respectively) were built by Seabridge for Bibby Line. The ship was laid up for two of its four years of service life.\n\nIn 1978, \"Liverpool Bridge\" was renamed \"Derbyshire\", the fourth vessel to carry the name in the company's fleet. On 11 July 1980, on what turned out to be the vessel's final voyage, \"Derbyshire\" left Sept-Îles, Quebec, Canada, her destination being Kawasaki, Japan, though she foundered near Okinawa (Southern Japan). \"Derbyshire\" was carrying a cargo of 157,446 tonnes of iron ore.\n\nOn 9 September 1980, \"Derbyshire\" hove-to in Typhoon Orchid some 230 miles from Okinawa, and was overwhelmed by the tropical storm killing all aboard. \"Derbyshire\" never issued a Mayday distress message. The vessel had been following weather routing advice by \"Ocean routes\", a commercial weather routing company (subsequently renamed WRI Weather routing incorporated).\n\nThe search for \"Derbyshire\" commenced on 15 September 1980 and was called off six days later when no trace of the vessel was found, and it was declared lost. Six weeks after \"Derbyshire\" sank, one of the vessel's lifeboats was sighted by a Japanese tanker.\n\nAfter the loss of her sister ship \"MV Kowloon Bridge\" off the coast of the Republic of Ireland in 1986, after an unfortunate series of events precipitated by deck cracking discovered after an Atlantic crossing the \"Derbyshire\" investigation was re-opened by the relatives of the victims of the sinking. With financial support from the Seafarers Union in 1994 a deep water search began.\n\nIn June 1994, the wreck of \"Derbyshire\" was found at a depth of 4 km, spread over 1.3 km. An additional expedition spent over 40 days photographing and examining the debris field looking for evidence of what sank the ship. Ultimately it was determined that waves crashing over the front of the ship had sheared off the covers of small ventilation pipes near the bow. Over the next 30+ hours, seawater had entered through the exposed pipes into the forward section of the ship, causing the bow to slowly ride lower and lower in the water. Eventually, the bow was completely exposed to the full force of the rough waves which caused the massive hatch on the first cargo hold to buckle inward allowing hundreds of tons of water to enter in moments. As the ship started to sink, the second, then third hatches also failed, dragging the ship underwater. As the ship sank, the water pressure caused the ship to be twisted and torn apart by implosion.\n\nA bronze plaque was placed on the wreckage as a memorial to those who were lost.\n\nOn 21 September 1980, the Bibby Line vessel \"Cambridgeshire\" held a memorial service for the \"Derbyshire\" in the area the vessel was lost.\n\nThe 20th anniversary of the vessel's loss was marked by a memorial service in Liverpool, England, which was attended by Deputy Prime Minister John Prescott, himself a former merchant seaman.\n\nIn 2010, a memorial service was held in the vessel's home port of Liverpool on the 30th anniversary of \"Derbyshire\"'s loss.\n\nA permanent memorial was dedicated on 15th September 2018 in the garden of the Church of Our Lady and Saint Nicholas, Liverpool. \n\n\n"}
{"id": "297924", "url": "https://en.wikipedia.org/wiki?curid=297924", "title": "Mantis shrimp", "text": "Mantis shrimp\n\nMantis shrimps, or stomatopods, are marine crustaceans of the order Stomatopoda. Some species have specialised calcified \"clubs\" which can strike with great power, others have sharp forelimbs used to capture prey. They branched from other members of the class Malacostraca around 400 million years ago. Mantis shrimps typically grow to around in length. A few can reach up to . The largest mantis shrimp ever caught had a length of and was caught in the Indian River near Fort Pierce, Florida, in the United States. A mantis shrimp's carapace (the bony, thick shell that covers crustaceans and some other species) covers only the rear part of the head and the first four segments of the thorax. Varieties range from shades of brown to vivid colors, as there are more than 450 species of mantis shrimp. They are among the most important predators in many shallow, tropical, and subtropical marine habitats. However, despite being common, they are poorly understood as many species spend most of their life tucked away in burrows and holes.\n\nCalled \"sea locusts\" by ancient Assyrians, \"prawn killers\" in Australia and now sometimes referred to as \"thumb splitters\"—because of the animal's ability to inflict painful gashes if handled incautiously—mantis shrimps have powerful claws that are used to attack and kill prey by spearing, stunning, or dismembering. In captivity, some larger species can break through aquarium glass.\n\nAround 400 species of mantis shrimp have currently been discovered worldwide; all living species are in the suborder Unipeltata.\n\nThese aggressive and typically solitary sea creatures spend most of their time hiding in rock formations or burrowing intricate passageways in the sea bed. They rarely exit their homes except to feed and relocate, and can be active during the day, nocturnal, or active primarily at twilight, depending on the species. Unlike most crustaceans, they sometimes hunt, chase, and kill prey. Although some live in temperate seas, most species live in tropical and subtropical waters in the Indian and Pacific oceans between eastern Africa and Hawaii.\n\nThe mantis shrimp's second pair of thoracic appendages has been adapted for powerful close-range combat with high modifications. The appendage differences divide mantis shrimp into two main types: those that hunt by impaling their prey with spear-like structures and those that smash prey with a powerful blow from a heavily mineralised appendage type club. A considerable amount of damage can be inflicted after impact with these robust, hammer-like claws. This club is further divided into three sub-regions: the impact region, the periodic region, and the striated region. Mantis shrimp are commonly separated into two distinct groups determined by the type of claw they possess:\n\nBoth types strike by rapidly unfolding and swinging their raptorial claws at the prey, and can inflict serious damage on victims significantly greater in size than themselves. In smashers, these two weapons are employed with blinding quickness, with an acceleration of 10,400 \"g\" (102,000 m/s or 335,000 ft/s) and speeds of from a standing start. Because they strike so rapidly, they generate vapor-filled bubbles in the water between the appendage and the striking surface—known as cavitation bubbles. The collapse of these cavitation bubbles produces measurable forces on their prey in addition to the instantaneous forces of 1,500 newtons that are caused by the impact of the appendage against the striking surface, which means that the prey is hit twice by a single strike; first by the claw and then by the collapsing cavitation bubbles that immediately follow. Even if the initial strike misses the prey, the resulting shock wave can be enough to stun or kill.\n\nThe impact can also produce sonoluminescence from the collapsing bubble. This will produce a very small amount of light within the collapsing bubble, although the light is too weak and short-lived to be detected without advanced scientific equipment. The light emission probably has no biological significance but is rather a side-effect of the rapid snapping motion. Pistol shrimp produce this effect in a very similar manner.\n\nSmashers use this ability to attack snails, crabs, molluscs, and rock oysters, their blunt clubs enabling them to crack the shells of their prey into pieces. Spearers, on the other hand, prefer the meat of softer animals, like fish, which their barbed claws can more easily slice and snag.\n\nThe appendages are being studied as a micro-scale analogue for new macro-scale material structures.\n\nThe eyes of the mantis shrimp are mounted on mobile stalks and can move independently of each other. Mantis shrimp are thought to have the most complex eyes in the animal kingdom and have the most complex visual system ever discovered. Compared to the three types of photoreceptor cells that humans possess in their eyes, the eyes of a mantis shrimp have between 12 and 16 types of photoreceptors cells. Furthermore, some of these shrimp can tune the sensitivity of their long-wavelength colour vision to adapt to their environment. This phenomenon is called \"spectral tuning\" and is species-specific. Cheroske et al. did not observe spectral tuning in \"Neogonodactylus oerstedii\", the species with the most monotonous natural photic environment. In \"N. bredini\", a species with a variety of habitats ranging from a depth of 5 to 10 metres (although it can be found up to 20 m below the surface), spectral tuning was observed, but the ability to alter wavelengths of maximum absorbance was not as pronounced as in \"N. wennerae\", a species with much higher ecological/photic habitat diversity.\nEach compound eye is made up of up tens of thousands ommatidia, clusters of photoreceptor cells. Each eye consists of two flattened hemispheres separated by parallel rows of specialised ommatidia, collectively called the \"midband\". The number of omatidial rows in the midband ranges from 2 to 6. This divides the eye into three regions. This configuration enables mantis shrimp to see objects with three parts of the same eye. In other words, each eye possesses trinocular vision and therefore depth perception. The upper and lower hemispheres are used primarily for recognition of form and motion, like the eyes of many other crustaceans.\n\nMantis shrimp can perceive wavelengths of light ranging from deep ultraviolet (UVB) to far-red (300 to 720 nanometres) and polarized light. In mantis shrimp in the superfamilies Gonodactyloidea, Lysiosquilloidea and Hemisquilloidea, the midband is made up of six omatodial rows. Rows 1 to 4 process colours while rows 5 to 6 detect circularly or linearly polarized light. There are 12 types of photoreceptor cells in rows 1 to 4, four of which detect ultraviolet light.\n\nRows 1 to 4 of the midband are specialised for colour vision, from deep ultraviolet to far red. Their UV vision can detect five different frequency bands in the deep ultraviolet. To do this, they use two photoreceptors in combination with four different colour filters. They are not currently believed to be sensitive to infrared light. The optical elements in these rows have eight different classes of visual pigments and the rhabdom (area of eye that absorbs light from a single direction) is divided into three different pigmented layers (tiers), each for different wavelengths. The three tiers in rows 2 and 3 are separated by colour filters (intrarhabdomal filters) that can be divided into four distinct classes, two classes in each row. It is organised like a sandwich; a tier, a colour filter of one class, a tier again, a colour filter of another class, and then a last tier. These colour filters allow the mantis shrimp to see with diverse colour vision. Without the filters, the pigments themselves range only a small segment of the visual spectrum: about 490 to 550 nm. Rows 5 to 6 are also segregated into different tiers, but have only one class of visual pigment, the ninth class, and are specialised for polarization vision. Depending upon the species, they can detect circularly polarized light, linearly polarised light or both. A tenth class of visual pigment is found in the upper and lower hemispheres of the eye.\n\nSome species have at least 16 photoreceptor types, which are divided into four classes (their spectral sensitivity is further tuned by colour filters in the retinas), 12 for colour analysis in the different wavelengths (including six which are sensitive to ultraviolet light) and four for analysing polarised light. By comparison, most humans have only four visual pigments, of which three are dedicated to see colour, and human lenses block ultraviolet light. The visual information leaving the retina seems to be processed into numerous parallel data streams leading into the brain, greatly reducing the analytical requirements at higher levels.\n\nSix species of mantis shrimp have been reported to be able to detect circularly polarized light. The ability to detect circularly polarized light has not been documented in any other animal and it is unknown if it is present across all species of mantis shrimp. Some of their biological quarter-waveplates perform more uniformly over the visual spectrum than any current man-made polarising optics, and it has been speculated that this could inspire a new type of optical media that would outperform the current generation of Blu-ray Disc technology.\n\nThe species \"Gonodactylus smithii\" is the only organism known to simultaneously detect the four linear and two circular polarisation components required to measure all four Stokes parameters, which yield a full description of polarisation. It is thus believed to have optimal polarisation vision. It is the only animal known to have dynamic polarisation vision. This is achieved by rotational eye movements to maximise the polarisation contrast between the object in focus and its background. Since each eye moves independently from the other, it creates two separate streams of visual information.\n\nThe midband covers only about 5 to 10 degrees of the visual field at any given instant, but, like most crustaceans, mantis shrimps' eyes are mounted on stalks. In mantis shrimps the movement of the stalked eye is unusually free, and can be driven up to 70° in all possible axes of movement by eight eyecup muscles divided into six functional groups. By using these muscles to scan the surroundings with the midband, they can add information about forms, shapes, and landscape which cannot be detected by the upper and lower hemisphere of the eye. They can also track moving objects using large, rapid eye movements where the two eyes move independently. By combining different techniques, including movements in the same direction, the midband can cover a very wide range of the visual field.\n\nThe huge diversity seen in mantis shrimp photoreceptors likely comes from ancient gene duplication events. One interesting consequence of this duplication is the lack of correlation between opsin transcript number and physiologically expressed photoreceptors. One species may have 6 different opsin genes, but only express one spectrally distinct photoreceptor. Over the years, some mantis shrimp species have lost the ancestral phenotype, although some still maintain 16 distinct photoreceptors and 4 light filters. Species that live in a variety of photic environments have high selective pressure for photoreceptor diversity, and maintain ancestral phenotypes better than species that live in murky waters or are primarily nocturnal.\n\nWhat advantage sensitivity to polarisation confers is unclear; however, polarisation vision is used by other animals for sexual signaling and secret communication that avoids the attention of predators. This mechanism could provide an evolutionary advantage; it only requires small changes to the cell in the eye and could be easily selected for.\n\nThe eyes of mantis shrimp may enable them to recognise different types of coral, prey species (which are often transparent or semi-transparent), or predators, such as barracuda, which have shimmering scales. Alternatively, the manner in which mantis shrimp hunt (very rapid movements of the claws) may require very accurate ranging information, which would require accurate depth perception.\n\nDuring mating rituals, mantis shrimp actively fluoresce, and the wavelength of this fluorescence matches the wavelengths detected by their eye pigments. Females are only fertile during certain phases of the tidal cycle; the ability to perceive the phase of the moon may therefore help prevent wasted mating efforts. It may also give mantis shrimp information about the size of the tide, which is important to species living in shallow water near the shore.\n\nIt has been suggested that the capacity to see UV light enables observation of otherwise hard-to-detect prey on coral reefs.\n\nResearch also shows their visual experience of colours is not very different from humans'. The eyes are actually a mechanism that operates at the level of individual cones and makes the brain more efficient. This system allows visual information to be preprocessed by the eyes instead of the brain, which would otherwise have to be larger to deal with the stream of raw data and thus require more time and energy. While the eyes themselves are complex and not yet fully understood, the principle of the system appears to be simple. It is similar in function to the human eye but works in the opposite manner. In the human brain, the inferior temporal cortex has a huge amount of colour-specific neurons which process visual impulses from the eyes to create colourful experiences. The mantis shrimp instead uses the different types of photoreceptors in its eyes to perform the same function as the human brain neurons, resulting in a hardwired and more efficient system for an animal that requires rapid colour identification. Humans have fewer types of photoreceptors, but more colour-tuned neurons, while mantis shrimps appears to have fewer colour neurons and more classes of photoreceptors.\n\nAn October 2014 publication by researchers from the University of Queensland stated that the compound eyes of mantis shrimp can detect cancer and the activity of neurons, because they are sensitive to detecting polarised light that reflects differently from cancerous and healthy tissue. The study claims that this ability can be replicated through a camera through the use of aluminium nanowires to replicate polarisation-filtering microvilli on top of photodiodes. In February 2016 it was also revealed that the shrimps are using a form of reflector of polarised light not seen in nature or human technology before. It allows the manipulation of light across the structure rather than through its depth, the typical way polarisers work. This allows the structure to be both small and microscopically thin, and still be able to produce big, bright, colourful polarised signals.\n\nMantis shrimp are long-lived and exhibit complex behaviour, such as ritualised fighting. Some species use fluorescent patterns on their bodies for signalling with their own and maybe even other species, expanding their range of behavioural signals. They can learn and remember well, and are able to recognise individual neighbours with whom they frequently interact. They can recognise them by visual signs and even by individual smell. Many have developed complex social behaviour to defend their space from rivals.\n\nIn a lifetime, they can have as many as 20 or 30 breeding episodes. Depending on the species, the eggs can be laid and kept in a burrow, or they can be carried around under the female's tail until they hatch. Also depending on the species, male and female may come together only to mate, or they may bond in monogamous long-term relationships.\n\nIn the monogamous species, the mantis shrimp remain with the same partner for up to 20 years. They share the same burrow and may be able to coordinate their activities. Both sexes often take care of the eggs (biparental care). In \"Pullosquilla\" and some species in \"Nannosquilla\", the female will lay two clutches of eggs: one that the male tends and one that the female tends. In other species, the female will look after the eggs while the male hunts for both of them. After the eggs hatch, the offspring may spend up to three months as plankton.\n\nAlthough stomatopods typically display the standard types of movement seen in true shrimp and lobsters, one species, \"Nannosquilla decemspinosa\", has been observed flipping itself into a crude wheel. The species lives in shallow, sandy areas. At low tides, \"N. decemspinosa\" is often stranded by its short rear legs, which are sufficient for movement when the body is supported by water, but not on dry land. The mantis shrimp then performs a forward flip in an attempt to roll towards the next tide pool. \"N. decemspinosa\" has been observed to roll repeatedly for , but specimens typically travel less than .\n\nIn Japanese cuisine, the mantis shrimp species \"Oratosquilla oratoria\", called , is eaten boiled as a sushi topping and, occasionally, raw as sashimi.\n\nMantis shrimp are abundant in the coastal regions of south Vietnam, known in Vietnamese as \"tôm tít\", or \"tôm tech\". In regions such as Nha Trang, they are called \"bàn chải\", named for its resemblance to a scrub brush. The shrimp can be steamed, boiled, grilled or dried, used with pepper, salt and lime, fish sauce and tamarind, or fennel.\n\nIn Cantonese cuisine, the mantis shrimp is known as \"pissing shrimp\" () because of their tendency to shoot a jet of water when picked up. After cooking, their flesh is closer to that of lobsters than that of shrimp, and like lobsters, their shells are quite hard and require some pressure to crack. Usually they are deep fried with garlic and chili peppers.\n\nIn the Mediterranean countries the mantis shrimp \"Squilla mantis\" is a common seafood, especially on the Adriatic coasts (canocchia) and the Gulf of Cádiz (galera).\n\nIn the Philippines, the mantis shrimp is known as tatampal, hipong-dapa, or alupihang-dagat, and is cooked and eaten like any other shrimp.\n\nIn Hawaii, some mantis shrimp have grown unusually large in the very dirty water of the Grand Ala Wai Canal in Waikiki. The usual dangers associated with consuming seafood caught in contaminated waters still applies to mantis shrimp.\n\nSome saltwater aquarists keep stomatopods in captivity. The peacock mantis is especially colourful and desired in the trade.\n\nWhile some aquarists value mantis shrimp, others consider them harmful pests, because:\n\n\nThe live rock with mantis shrimp burrows are actually considered useful by some in the marine aquarium trade and are often collected. It is not uncommon for a piece of live rock to convey a live mantis shrimp into an aquarium. Once inside the tank, they may feed on fish and other inhabitants. They are notoriously difficult to catch when established in a well-stocked tank, and there are accounts of them breaking glass tanks. While stomatopods do not eat coral, smashers can damage it if they try to make a home within it.\n\nA large number of the mantis shrimp species were first scientifically described by one carcinologist, Raymond B. Manning, and the collection of stomatopods he amassed is the largest in the world, covering 90% of the known species.\n\n"}
{"id": "48350265", "url": "https://en.wikipedia.org/wiki?curid=48350265", "title": "Marius Aam", "text": "Marius Aam\n\nMarius Aam (born 14 November 1980) is a retired Norwegian football defender who notably played in Tippeligaen for Aalesund.\n\nHe started his career in Aalesund, and played first-tier fotball for them in Tippeligaen 2003 and 2005. He was not a first-team regular, and quit the team in early 2006. The next season, he fielded for Ålesund's second best club at the time, Skarbøvik IF.\n\nLeaving Skarbøvik ahead of the 2009 season, he later featured for sixth-tier club Hessa. He also made a comeback in Skarbøvik.\n"}
{"id": "66492", "url": "https://en.wikipedia.org/wiki?curid=66492", "title": "Mi-Go", "text": "Mi-Go\n\nMi-Go are a fictional race of extraterrestrials created by H. P. Lovecraft and used by others in the Cthulhu Mythos setting. The word Mi-Go comes from \"Migou\", a Tibetan word for yeti. The aliens are fungus-based lifeforms which are extremely varied due to their prodigious surgical, biological, chemical, and mechanical skill. The variants witnessed by Akeley in \"The Whisperer in Darkness\" look like winged humanoid crabs, and do not resemble yeti.\n\nMi-Go are first named as such in Lovecraft's short story \"The Whisperer in Darkness\" (1931). However this is considered an elaboration on earlier references in his sonnet cycle \"Fungi from Yuggoth\" (1929–30) to descriptions of alien vegetation on dream-worlds.\n\nThe Mi-Go have appeared across a range of media set in the Cthulhu Mythos.\n\nThe \"Mi-Go\" are large, pinkish, fungoid, crustacean-like entities the size of a man; where a head would be, they have a \"convoluted ellipsoid\" composed of pyramided, fleshy rings and covered in antennae. They are about long, and their crustacean-like bodies bear numerous sets of paired appendages. They possess a pair of membranous bat-like wings which are used to fly through the \"aether\" of outer space. The wings do not function well on Earth. Several other races in Lovecraft's Mythos also have wings like these.\n\nIn the original short story, the creatures cannot be recorded using ordinary photographic film, due to their bodies being formed from otherworldly matter.\nThey are capable of going into suspended animation until softened and reheated by the sun or some other source of heat.\n\nThe Mi-Go can transport humans from Earth to Pluto (and beyond) and back again by removing the subject's brain and placing it into a \"brain cylinder\" as an isolated brain, which can be attached to external devices to allow it to see, hear, and speak.\n\nIn \"The Whisperer in Darkness\" the Mi-Go are heard to give praise to Nyarlathotep and Shub-Niggurath, suggesting some form of worship. Their moral system is completely alien, making them seem highly malicious from a human perspective.\n\nOne of the moons of Yuggoth holds designs that are sacred to the Mi-Go; these are useful in various processes mentioned in the \"Necronomicon\". It is said that transcriptions of these designs can be sensed by the Mi-Go, and those possessing them shall be hunted down by the few remaining on earth.\n\nSupposedly, a group known as the Brotherhood of the Yellow Sign are dedicated to hunting them down and exterminating the fungoid threat, though it is unknown if this is actually true since it was given as a reason for their remaining hidden. Hastur, which is mentioned in passing among several other places and things, was eventually converted into a God-Like alien being by August Derleth who gave it the title \"Him Who is Not to be Named\". However, in \"The Whisperer in Darkness\", a human ally of the Mi-Go mentions \"Him Who Is Not to Be Named\" in the list of honored entities along with Nyarlathotep and Shub-Niggurath. Lovecraft never made a connection between Hastur and \"Him Who Is Not to Be Named\", and indeed didn't even imply Hastur was a being; Derleth was the one to do so.\n\nThe Mi-Go appear in the Lovecraft novella \"At the Mountains of Madness\" (1931), in which they are described as having made war with the Elder Things long before the existence of humans.\n\nThe Mi-Go appeared in comic books in the first three issues of \"\" that featured the Miskatonic Project, created by Mark Ellis.\n\nThe Mi-Go are prominent antagonists in Pagan Publishing's \"Delta Green\" sourcebook for the \"Call of Cthulhu\" role-playing game. It mentions three castes: scientist, soldier, and worker. The book states that the Mi-Go usually have five pairs of appendages, though the number can vary. Normally, the first pair is designed for grasping and manipulating, but in the scientist caste it is usually the first two pairs. The remaining appendages are used for locomotion. The soldiers may have two or more pairs of wings. Some individuals do not have wings at all if they are deemed unnecessary to their task. The Mi-Go apparently can modify their own bodies. This source suggests that all their external accoutrements are actually extruded at will from the central gelatinous mass similar to the way the Shoggoth extrude body parts. In the \"Delta Green\" setting, the \"Greys\" are puppets remotely controlled by the Mi-Go.\n\nThey are distinguished by their mastery in various fields of science, especially surgery. Although they originate from beyond our solar system, they have set up an outpost on Pluto (known as Yuggoth in the mythos) and sometimes visit Earth to mine for minerals and other natural resources. The Mi-Go normally communicate by changing the colors of their orb-like heads or by emitting odd buzzing noises. They can also speak any human language upon receiving the appropriate surgical modification.\n\nThe Mi-Go are one of the main enemies of humanity in the role-playing game CthulhuTech, which combines Lovecraft's fiction with tropes and themes from mecha anime. In the game, their name is spelled “Migou” (see below), but they are commonly referred to as \"bugs\" by humans.\n\nThey are presented in CthulhuTech much as they are in the original Lovecraft stories, and somewhat similar to that in Delta Green: they are masters of science and genetics, and in particular human genetics. Their hostility to humanity could be seen as jealousy that humans had created a technology which they had never thought of, combined with a fear of humanity's growing power. Although it is stated that they have emotions vastly different from our own, their campaign on Earth developed into genocidal hatred of humans.\n\nIn \"Allan and the Sundered Veil\", Allan Quatermain, Randolph Carter, John Carter of Mars, and the Time Traveller encounter the Mi-Go, which are stated to be the same as Morlocks and the Yetis.\n\nIn \"To Mars and Providence\", the Mi-Go engage in trade with the Martians from \"The War of the Worlds\".\n\nThe Mi-Go appear in the final segment of the movie \"Necronomicon\", directed by Brian Yuzna.\n\nA Mi-Go is scripted to walk across the stage during the \"Tentacles\" number of \"A Shoggoth on the Roof\", prompting Armitage to say \"some horrible creature... I do not even want to know what \"that\" is\".\n\nThe horror-themed miniatures game HorrorClix, which features Cthulhu as a colossal figure, includes a Mi-Go as a unique figure in its \"The Lab\" expansion.\n\nThe Mi-Go also appear as sinister brain collectors in the short story \"Boojum\", written by Sarah Monette and Elizabeth Bear.\n\nThe computer game \"\" features the Mi-Go among the numerous different types of monster that assault the earth. This Mi-Go appears to be inspired by, rather than lifted from, the original Lovecraftian creature, since their unique feature in the game is to mimic human speech and other sounds in an attempt to lure people towards them.\n\nIt is possible that Lovecraft encountered the word \"migou\" in his readings. \"Migou\" is the Tibetan equivalent of the yeti, an ape-like cryptid said to inhabit the high mountain ranges of that region. While the Mi-Go of Lovecraft's mythos is completely unlike the migou of Tibetan stories, Lovecraft seems to equate the two, as can be seen in the following excerpt from \"The Whisperer in Darkness\":\n\n"}
{"id": "65071", "url": "https://en.wikipedia.org/wiki?curid=65071", "title": "Monster", "text": "Monster\n\nA monster is often a hideously grotesque animal or human, or a hybrid of both, whose appearance frightens and whose powers of destruction threaten the human world's social or moral order.\n\nAnimal monsters are outside the moral order, but sometimes have their origin in some human violation of the moral law (e.g. in the Greek myth, Minos does not sacrifice the white bull Poseidon sent him to the god, so as punishment Poseidon makes Minos' wife, Pasiphaë, fall in love with the bull, and she copulates with the beast, and gives birth to the man with a bull's head, the Minotaur). Human monsters are those who by birth were never fully human (Medusa and her sisters) or who through some supernatural or unnatural act lost their humanity (werewolves, Frankenstein's monster), and so who can no longer, or who never could, follow the moral law of human society.\n\nMonsters pre-date written history, and the academic study of the particular cultural notions expressed in a society's ideas of monsters is known as \"monstrophy\".\n\nMonsters have appeared in literature and in feature-length films. Well-known monsters in fiction include Count Dracula, Frankenstein's monster, werewolves, mummies, and zombies.\n\n\"Monster\" derives from the Latin \"monstrum\", itself derived ultimately from the verb \"moneo\" (\"to remind, warn, instruct, or foretell\"), and denotes anything \"strange or singular, contrary to the usual course of nature, by which the gods give notice of evil,\" \"a strange, unnatural, hideous person, animal, or thing,\" or any \"monstrous or unusual thing, circumstance, or adventure.\" \n\nIn the words of Tina Marie Boyer, assistant professor of medieval German literature at Wake Forest University, \"monsters do not emerge out of a cultural void; they have a literary and cultural heritage\".\n\nIn the religious context of ancient Greeks and Romans, monsters were seen as signs of \"divine displeasure\", and it was thought that birth defects were especially ominous, being \"an unnatural event\" or \"a malfunctioning of nature\".\n\nMonsters are not necessarily abominations however. The Roman historian Suetonius, for instance, describes a snake's absence of legs or a bird's ability to fly as monstrous, as both are \"against nature\". Nonetheless, the negative connotations of the word quickly established themselves, and by the playwright and philosopher Seneca's time, the word had extended into its philosophical meaning, \"a visual and horrific revelation of the truth\".\n\nIn spite of this, mythological monsters such as the Hydra and Medusa are not natural beings, but divine entities. This seems to be a holdover from Proto-Indo-European religion and other belief systems, in which the divisions between \"spirit,\" \"monster,\" and \"god\" were less evident.\n\nThe history of monsters in fiction is long, for example Grendel in the epic poem \"Beowulf\" is an archetypal monster, deformed, brutal, with enormous strength and raiding a human settlement nightly to slay and feed on his victims. The modern literary monster has its roots on examples such as the monster in Mary Shelley's Frankenstein and the vampire in Bram Stoker's Dracula.\n\nMonsters are a staple of fantasy fiction, horror fiction or science fiction (where the monsters are often extraterrestrial in nature). There is also a burgeoning subgenre of erotic fiction involving monsters, monster erotica.\n\nDuring the age of silent films, monsters tended to be human-sized, e.g. Frankenstein's monster, the Golem, werewolves and vampires. The film \"Siegfried\" featured a dragon that consisted of stop-motion animated models, as in RKO's \"King Kong\", the first giant monster film of the sound era.\n\nUniversal Studios specialized in monsters, with Bela Lugosi's reprisal of his stage role, Dracula, and Boris Karloff playing Frankenstein's monster. The studio also made several lesser films, such as \"Man-Made Monster\", starring Lon Chaney, Jr. as a carnival side-show worker who is turned into an electrically charged killer, able to dispatch victims merely by touching them, causing death by electrocution.\n\nThere was also a variant of Dr. Frankenstein, the mad surgeon Dr. Gogol (played by Peter Lorre), who transplanted hands that were reanimated with malevolent temperaments, in the film \"Mad Love\".\nWerewolves were introduced in films during this period, and similar creatures were presented in \"Cat People\". Mummies were cinematically depicted as fearsome monsters as well. As for giant creatures, the cliffhanger of the first episode of the 1936 \"Flash Gordon\" serial did not use a costumed actor, instead using real-life lizards to depict a pair of battling dragons via use of camera perspective. However, the cliffhanger of the ninth episode of the same serial had a man in a rubber suit play the Fire Dragon, which picks up a doll representing Flash in its claws. The cinematic monster cycle eventually wore thin, having a comedic turn in \"Abbott and Costello Meet Frankenstein\" (1948).\nFRANKENSTEIN\n\nIn the post–World War II era, however, giant monsters returned to the screen with a vigor that has been causally linked to the development of nuclear weapons. One early example occurred in the American film \"The Beast from 20,000 Fathoms\", which was about a dinosaur that attacked a lighthouse. Subsequently, there were Japanese film depictions, (Godzilla, Gamera), British depictions (\"Gorgo\"), and even Danish depictions (\"Reptilicus\"), of giant monsters attacking cities. A recent depiction of a giant monster is depicted in J. J. Abrams's \"Cloverfield\", which was released in theaters January 18, 2008. The intriguing proximity of other planets brought the notion of extraterrestrial monsters to the big screen, some of which were huge in size (such as King Ghidorah and Gigan), while others were of a more human scale. During this period, the fish-man monster Gill-man was developed in the film series \"Creature from the Black Lagoon\".\n\nBritain's Hammer Film Productions brought color to the monster movies in the late 1950s. Around this time, the earlier Universal films were usually shown on American television by independent stations (rather than network stations) by using announcers with strange personas, who gained legions of young fans. Although they have since changed considerably, movie monsters did not entirely disappear from the big screen as they did in the late 1940s.\n\nOccasionally, monsters are depicted as friendly or misunderstood creatures. King Kong and Frankenstein's monster are two examples of misunderstood creatures. Frankenstein's monster is frequently depicted in this manner, in films such as \"Monster Squad\" and \"Van Helsing\". The Hulk is an example of the \"Monster as Hero\" archetype. The theme of the \"Friendly Monster\" is pervasive in pop-culture. Chewbacca, Elmo, and Shrek are notable examples of friendly \"monsters\". The creatures of \"Monsters, Inc.\" scare children in order to create energy for running machinery, while the furry monsters of \"The Muppets\" and \"Sesame Street\" live in harmony with animals and humans alike. Japanese culture also commonly features monsters which are benevolent or likable, with the most famous examples being the \"Pokémon\" franchise and the pioneering anime \"My Neighbor Totoro\". The book series/webisodes/toy line of Monster High is another example.\nMonsters are commonly encountered in fantasy or role-playing games and video games as enemies for players to fight against. They may include aliens, legendary creatures, extra-dimensional entities or mutated versions of regular animals.\n\nEspecially in role-playing games, \"monster\" is a catch-all term for hostile characters that are fought by the player. Sentient fictional races are usually not referred to as monsters. At other times, the term can carry a neutral connotation, such as in the \"Pokémon\" franchise, where it is used to refer to fictional creatures that resemble real-world animals/objects. Characters in games may refer to all animals as \"monsters\".\n\nMonsters in legend\n\nMonsters in fiction\n\nNotes\nCitations\nBibliography\n"}
{"id": "36815492", "url": "https://en.wikipedia.org/wiki?curid=36815492", "title": "NEST (software)", "text": "NEST (software)\n\nNEST is a simulation software for spiking neural network models, including large-scale neuronal networks. NEST was initially developed by Markus Diesmann and Marc-Oliver Gewaltig and is now developed and maintained by the NEST Initiative.\n\nA NEST simulation tries to follow the logic of an electrophysiological experiment that takes place inside a computer with the difference, that the neural system to be investigated must be defined by the experimenter.\n\nThe neural system is defined by a possibly large number of neurons and their connections. In a NEST network, different neuron and synapse models can coexist. Any two neurons can have multiple connections with different properties. Thus, the connectivity can in general not be described by a weight or connectivity matrix but rather as an adjacency list.\n\nTo manipulate or observe the network dynamics, the experimenter can define so-called devices which represent the various instruments (for measuring and stimulation) found in an experiment. These devices write their data either to memory or to file.\n\nNEST is extensible and new models for neurons, synapses, and devices can be added.\n\nThe following example simulates spiking activity in a sparse random network with recurrent excitation and inhibition\n\nThe figure shows the spiking activity of 50 neurons as a raster plot. Time increases along the horizontal axis, neuron id increases along the vertical axis. Each dot corresponds to a spike of the respective neuron at a given time. The lower part of the figure shows a histogram with the mean firing-rate of the neurons.\n\n\n\n\n\n\n\n\nNEST development was started in 1993 by Markus Diesmann and Marc-Oliver Gewaltig at the Ruhr University Bochum, Bochum, Germany and the Weizmann Institute of Science in Rehovot, Israel. At this time, the simulator was called SYNOD and simulations were defined in a stack based simulation language, called SLI.\n\nIn 2001, the software changed its name from SYNOD to NEST. Until 2004, NEST was exclusively developed and used by the founding members of the NEST Initiative. The first public release appeared in summer 2004. Since then, NEST was released regularly, about once or twice per year.\n\nSince 2007, NEST supports hybrid parallelism, using POSIX threads and MPI.\n\nIn 2008, the stack-based simulation language SLI was superseded by a modern Python interface, however, the old simulation language is still used internally.\nAt the same time, the simulator independent specification language PyNN was developed with support for NEST.\nIn 2012, the NEST Initiative changed the license from the proprietary NEST License to GNU GPL V2 or later.\n\n\n\n"}
{"id": "40335832", "url": "https://en.wikipedia.org/wiki?curid=40335832", "title": "Not Dead Yet", "text": "Not Dead Yet\n\nNot Dead Yet (NDY) is a United States disability rights group that opposes assisted suicide and euthanasia for people with disabilities. Diane Coleman, JD, is the founder and president of this national group. Stephen Drake, a research analyst with NDY, is one of the group's chief spokespersons and contacts for press releases.\n\nThe group was founded on April 27, 1996. Its name comes from the film \"Monty Python and the Holy Grail\", in which plague victims are thrown into a cart and hauled off to be buried. A man being given up as a corpse by his family protests that he is \"not dead yet!\"\n\nIn 2004 NDY protested the removal of Terri Schiavo's feeding tube. They also protested the movie \"Million Dollar Baby\", in which the injection of an overdose of epinephrine to euthanize a suicidal quadriplegic woman is depicted as a rational and compassionate act. The group has been highly critical of utilitarian philosophers such as Peter Singer of Princeton University. Coleman has called Professor Singer \"the most dangerous man on earth\" and asserted that he was advocating genocide. In June 2015 NDY organized protests against Singer's position that new-born babies with certain disabilities can morally be killed, which he sees as no different from abortion. The protesters called for Princeton University to dismiss Singer.\n\n\n"}
{"id": "1573719", "url": "https://en.wikipedia.org/wiki?curid=1573719", "title": "Onryō", "text": "Onryō\n\nIn traditional beliefs of Japan and in literature, onryō (怨霊, literally \"vengeful spirit\", sometimes rendered \"wrathful spirit\") refers to a ghost (\"yūrei\") believed capable of causing harm in the world of the living, harming or killing enemies, or even causing natural disasters to exact vengeance to redress the wrongs it received while alive then takes their spirits from their dying bodies.\n\nThe term overlaps somewhat with , except that in the cult of the \"goryō\", the acting agent need not necessarily be a wrathful spirit.\n\nWhile the origin of \"onryō\" is unclear, belief in their existence can be traced back to the 8th century and was based on the idea that powerful and enraged souls of the dead could influence or harm the living people. The earliest \"onryō\" cult that developed was around Prince Nagaya who died in 729; and the first record of possession by the \"onryō\" spirit affecting the health is found in the chronicle \"Shoku Nihongi\" (797), which states that \"'s soul harmed Genbō to death\" (Hirotsugu having died in a failed insurrection, named the \"Fujiwara no Hirotsugu Rebellion\", after failing to remove his rival, the priest Genbō, from power).\n\nTraditionally in Japan, \"onryō\" driven by vengeance were thought capable of causing not only their enemy's death, as in the case of Hirotsugu's vengeful spirit held responsible for killing the priest Genbō,), but causing natural disasters such as earthquakes, fires, storms, drought, famine and pestilence, as in the case of Prince Sawara's spirit embittered against his brother, the Emperor Kanmu. In common parlance, such vengeance exacted by supernatural beings or forces is termed .\n\nThe Emperor Kanmu had accused his brother Sawara of plotting (possibly falsely to remove him as rival to the throne), and the latter who was exiled died by fasting. The reason that the Emperor moved the capital to Nagaoka-kyō thence to Kyoto was an attempt to avoid the wrath of his brother's spirit, according to a number of scholars. This not succeeding entirely, the emperor tried to lift the curse by appeasing his brother's ghost, by performing Buddhist rites to pay respect, and granting Prince Sawara the posthumous title of emperor.\n\nA well-known example of appeasement of the \"onryō\" spirit is the case of Sugawara no Michizane, who had been politically disgraced and died in exile. Believed to cause the death of his calumniators in quick succession, as well as catastrophes (especially lightning damage), and the court tried to appease the wrathful spirit by restoring Michizane's old rank and position. Michizane became deified in the cult of the Tenjin, with Tenman-gū shrines erected around him.\n\nPossibly the most famous \"onryō\" is Oiwa, from the \"Yotsuya Kaidan\". In this story the husband remains unharmed; however, he is the target of the \"onryō\"’s vengeance. Oiwa's vengeance on him isn't physical retribution, but rather psychological torment.\n\nOther examples include:\n\n\n\nHisako (久子, \"Eternal Child\" or \"Everlasting Child\") from the third entry of the fighting game Killer Instinct, is an onryō who died while defending her village. She still haunts her old village and will take vengeance on anyone who desecrates it's ruins with her naginata. She has pale white skin, long black hair like most Onryo. \n\nIn Fall 2018 the asymetrical horror game, Dead By Daylight released The Spirit DLC. The Spirit is an onryō who returns from the dead after being brutally murdered by her Father.\n\nTraditionally, \"onryō\" and other \"yūrei\" (ghosts) had no particular appearance. However, with the rising of popularity of Kabuki during the Edo period, a specific costume was developed.\n\nHighly visual in nature, and with a single actor often assuming various roles within a play, Kabuki developed a system of visual shorthand that allowed the audience to instantly clue in as to which character is on stage, as well as emphasize the emotions and expressions of the actor.\n\nA ghost costume consisted of three main elements:\n\n\n\n"}
{"id": "53316821", "url": "https://en.wikipedia.org/wiki?curid=53316821", "title": "Proximal Centriole-Like", "text": "Proximal Centriole-Like\n\nThe proximal centriole-like or PCL is an atypical type of centriole found in the sperm cells of insects. The PCL name is due to some similarity to the Proximal centriole found in Vertebrates sperm and the hypothesis that the two structures are homologous. The PCL is an atypical type of centriole because it does not have microtubules, a defining feature of centrioles. However, the PCL is a type of centriole for several reasons. (1) the PCL formation is dependent upon the same genetic pathway that mediates the initiation of centriole formation. (2) The PCL is composed of centriolar proteins. (3) After fertilization, the sperm PCL function like a centriole. The PCL recruits pericentriolar material (PCM) forming a centrosome that acts as a microtubule-organizing center (MTOC). The PCL also serves as a platform to form a typical centriole in the zygote, as expected from a centriole. Also, the PCL is essential to form one of the two spindle poles of the dividing zygote.\n\nThe PCL was discovered in flies. However, it is also found in beetles, suggesting it is a common form of atypical centriole in insects.\n\nAnother type of atypical type of centriole was discovered in human and bovine sperm. This is the distal centriole of the spermatozoon, which has atypical structure and composition. This spermatozoon distal centriole is composed of splayed microtubules surrounding previously undescribed rods of centriole luminal proteins, and it has only a subset of the protein found in a typical centriole. After fertilization, the atypical distal centriole that is attached to the sperm tail recruits pericentriolar material, forming a new centriole, and localizing to the spindle pole during mitosis.\n"}
{"id": "171267", "url": "https://en.wikipedia.org/wiki?curid=171267", "title": "Psilotum", "text": "Psilotum\n\nPsilotum is a genus of fern-like vascular plants, commonly known as whisk ferns. It is one of two genera in the family Psilotaceae, the other being \"Tmesipteris\". Plants in these two genera were once thought to be descended from the earliest surviving vascular plants, but more recent phylogenies place them as basal ferns, as a sister group to Ophioglossales. They lack true roots and leaves, the stems being the organs containing conducting tissue. There are only two species in \"Psilotum\" and a hybrid between the two. They differ from those in \"Tmesipteris\" in having stems with many branches and a synangium with three lobes rather than two.\n\nWhisk ferns in the genus \"Psilotum\" lack true roots but are anchored by creeping rhizomes. The stems have many branches with paired enations, which look like small leaves but have no vascular tissue. Above these enations there are synangia formed by the fusion of three sporangia and which produce the spores. When mature, the synangia release yellow to whitish spores which develop into a gametophyte less than long. The gametophyte lives underground as a saprophyte, sometimes in a mycorrhizal association. When the gametophyte is mature, it produces both egg and sperm cells. The sperm cells swim using several flagella and when they reach an egg cell, unite with it to form the young sporophyte. A mature sporophyte may grow to a height of or more but lacks true leaves. The stem has a core of thick-walled protostele in its centre surrounded by an endodermis which regulates the flow of water and nutrients. The surface of the stem is covered with stomata which allow gas exchange with the surroundings.\n\nThe gametophyte of \"Psilotum\" is unusual in that it branches dichotomously, lives underground and possesses vascular tissue. The nutrition of the gametophyte appears to be myco-heterotrophic, assisted by endophytic fungi.\n\nThe genus \"Psilotum\" was first formally described in 1801 by Olof Swartz and the description was published in \"Journal fur die Botanik (Schrader)\". The name of the genus is from the Ancient Greek word \"psilos\" meaning \"bare\", \"smooth\" or \"bald\" referring to the lack of the usual plant organs, such as leaves.\n\nThere are two species, \"Psilotum nudum\" and \"Psilotum complanatum\", with a hybrid between them known, \"Psilotum\" × \"intermedium\" W. H. Wagner.\n\nThe distribution of \"Psilotum\" is tropical and subtropical, in the New World, Asia, and the Pacific, with a few isolated populations in south-west Europe. The highest latitudes known are in South Carolina, Cádiz province in Spain, and southern Japan for \"P. nudum\". In the U.S., \"P. nudum\" is found from Florida to Texas, and \"P. complanatum\" in Hawaii.\n\n\"Psilotum\" superficially resembles certain extinct early vascular plants, such as the rhyniophytes and the trimerophyte genus \"Psilophyton\". The unusual features of \"Psilotum\" that suggest an affinity with early vascular plants include dichotomously branching sporophytes, aerial stems arising from horizontal rhizomes, a simple vascular cylinder, homosporous and terminal eusporangia and a lack of roots. Unfortunately, no fossils of psilophytes are known to exist. A careful study of the morphology and anatomy suggests that whisk ferns are not closely related to rhyniophytes, and that the ancestral features present in living psilophytes represent a reduction from a more typical modern fern plant. Significant differences between \"Psilotum\" and the rhyniophytes and trimerophytes are that the development of its vascular strand is exarch, while it is centrarch in rhyniophytes and trimerophytes. The sporangia of \"Psilotum\" are trilocular synangia resulting from the fusion of three adjacent sporangia, and these are borne laterally on the axes. In the rhyniophytes and trimerophytes the sporangia were single and in a terminal position on branches.\n\nMolecular evidence strongly confirms that \"Psilotum\" is a fern and that psilophytes are sister to ophioglossoid ferns.\n"}
{"id": "1619344", "url": "https://en.wikipedia.org/wiki?curid=1619344", "title": "Pudendal anesthesia", "text": "Pudendal anesthesia\n\nPudendal anesthesia, also known as a pudendal block, or saddle block, is a form of local anesthesia commonly used in the practice of obstetrics to relieve pain during the delivery of baby by forceps. The pudendal nerve block prevents fainting during forceps delivery which was common before pudendal nerve block use was available. The anesthesia is produced by blocking the pudendal nerves near the ischial spine of the pelvis. The ischial spine separates the greater and lesser sciatic foramina at the exit of the bony pelvis. Pelvis in Latin means 'saucepan' and one can view the bony human pelvis as a saucepan, with circular/cylidrical walls, but without a base and a flailed upper rim, or wings to which the gluteal muscles (hip bone stabilisers) attach. The pelvic bony cylindrical walls also have a curve, which follows that of the curve of the sacrum, the fused vertebral bones of the lower end of the spine. \n\nThe pudendal block gets its name because a local anesthetic, such as lidocaine or chloroprocaine, is injected into the pudendal canal where the pudendal nerve is located. This allows quick pain relief to the perineum, vulva, and vagina. A pudendal block is usually given in the second stage of labor just before delivery of the baby. It relieves pain around the vagina and rectum as the baby comes down the birth canal. It is also helpful just before an episiotomy. Lidocaine is usually preferred for a pudendal block because it has a longer duration than chloroprocaine which usually lasts less than one hour.\n"}
{"id": "4401825", "url": "https://en.wikipedia.org/wiki?curid=4401825", "title": "Queen of the Nile (The Twilight Zone)", "text": "Queen of the Nile (The Twilight Zone)\n\n\"Queen of the Nile\" is episode 143 of the American television anthology series \"The Twilight Zone\". In this episode, a journalist becomes romantically involved with a dangerous, secretly centuries-old movie star.\n\nColumnist Jordan Herrick visits actress Pamela Morris, a woman known for her beauty and vitality, for an interview. In Pamela's manor he notices a painting of her that is dated 1940. Pamela still looks just as she did in the painting. When questioned on this, she says the painter drew her when she was a child with a projection of what she would look like as an adult, and deflects questions about her age. Pamela and Jordan flirt during the interview and make dinner plans for that night. As Jordan is leaving, an old woman who Pamela introduced as her mother, Mrs. Draper, warns him to never come back. Mrs. Draper says Pamela is many centuries old and that she is actually Pamela's daughter.\n\nDuring his date with Pamela, Jordan mentions what Mrs. Draper had said. Pamela claims Mrs. Draper is mentally ill, but after the date, Herrick calls his editor and asks him to research Pamela's first film, \"The Queen of the Nile\". The editor reveals that the film was a remake of a silent movie filmed on location in Egypt. Leading lady Constance Taylor was apparently killed in a cave-in near the end of the shooting. The editor compares photos of Constance and Pamela in the same role and says they look alike. Jordan asks the editor to dig up articles on every man Pamela has ever been involved with.\n\nJordan returns to the manor and confronts Pamela with his discovery. Pamela drugs Jordan's coffee and then places a scarab beetle on his unconscious body. The beetle drains his life until he has turned to dust. She then applies the scarab to her own chest.\n\nThe episode ends with another young and handsome columnist arriving to interview Pamela, starting the cycle once again. In the closing narration, it is hinted that Miss Morris is actually Cleopatra VII, and that she has lived for more than 2,000 years.\n\nApparently, in the original first-draft script, a handsome young policeman turns up at the end of the story, asking Pamela Morris as to the whereabouts of the now missing Jordan Herrick. She begins to flatter and flirt with the cop, setting him up to start the cycle all over again.\n\n\n"}
{"id": "185887", "url": "https://en.wikipedia.org/wiki?curid=185887", "title": "Respiratory failure", "text": "Respiratory failure\n\nRespiratory failure results from inadequate gas exchange by the respiratory system, meaning that the arterial oxygen, carbon dioxide or both cannot be kept at normal levels. A drop in the oxygen carried in blood is known as hypoxemia; a rise in arterial carbon dioxide levels is called hypercapnia. Respiratory failure is classified as either Type I or Type II, based on whether there is a high carbon dioxide level. The definition of respiratory failure in clinical trials usually includes increased respiratory rate, abnormal blood gases (hypoxemia, hypercapnia, or both), and evidence of increased work of breathing.\n\nThe normal partial pressure reference values are: oxygen PaO more than , and carbon dioxide PaCO lesser than .\n\nSeveral types of condition can potentially result in respiratory failure:\n\n\nType 1 respiratory failure is defined as a low level of oxygen in the blood (hypoxemia) without an increased level of carbon dioxide in the blood (hypercapnia), and indeed the PCO may be normal or low. It is typically caused by a ventilation/perfusion (V/Q) mismatch; the volume of air flowing in and out of the lungs is not matched with the flow of blood to the lungs. The basic defect in type 1 respiratory failure is failure of oxygenation characterized by:\n\nThis type of respiratory failure is caused by conditions that affect oxygenation such as:\n\nHypoxemia (PaO2 <8kPa) with hypercapnia (PaCO2 >6.0kPa).\n\nThe basic defect in type 2 respiratory failure is characterized by:\n\nType 2 respiratory failure is caused by inadequate alveolar ventilation; both oxygen and carbon dioxide are affected. Defined as the buildup of carbon dioxide levels (PCO) that has been generated by the body but cannot be eliminated. The underlying causes include:\nTypes 3 and 4 - https://www.thoracic.org/professionals/clinical-resources/critical-care/clinical-education/mechanical-ventilation/respiratory-failure-mechanical-ventilation.pdf\n-https://www.physio-pedia.com/Respiratory_Failure\n-https://www.mcgill.ca/criticalcare/teaching/files/acute\n\nTreatment of the underlying cause is required. Endotracheal intubation and mechanical ventilation are required in cases of severe respiratory failure (PaO2 less than 50 mmHg). Respiratory stimulants such as doxapram are rarely used, and if the respiratory failure resulted from an overdose of sedative drugs such as opioids or benzodiazepines, then the appropriate antidote (naloxone or flumazenil, respectively) will be given.\n\nThere is tentative evidence that in those with respiratory failure identified before arrival in hospital, continuous positive airway pressure can be useful when started before conveying to hospital.\n\n"}
{"id": "723789", "url": "https://en.wikipedia.org/wiki?curid=723789", "title": "Riderless horse", "text": "Riderless horse\n\nA riderless horse (which may be caparisoned in ornamental and protective coverings, having a detailed protocol of their own) is a single horse, without a rider, and with boots reversed in the stirrups, which sometimes accompanies a funeral procession. The horse follows the caisson carrying the casket. A riderless horse can also be featured in military parades to symbolize fallen soldiers. In Australia for example, it is traditional for a riderless horse known as the 'Lone Charger' to lead the annual Anzac Day marches.\n\nThe custom is believed to date back to the time of Genghis Khan, when a horse was sacrificed to serve the fallen warrior in the next world. The riderless horse later came to symbolize a warrior who would ride no more.\n\nIn the United States, the riderless horse is part of the military honors given to an Army or Marine Corps officer who was a colonel or above; this includes the President, by virtue of having been the country's commander in chief and the Secretary of Defense, having overseen the armed forces. Alexander Hamilton, former Secretary of the Treasury (1789-1795) was the first American to be given the honor. Historian Ron Chernow noted that Hamilton's gray horse followed the casket \"with the boots and spurs of its former rider reversed in the stirrups.\" Abraham Lincoln was the first president of the United States to be officially honored by the inclusion of the riderless horse in his funeral cortege, although a letter from George Washington's personal secretary recorded the president's horse was part of the president's funeral, carrying his saddle, pistols, and holsters.\nTraditionally, simple black riding boots are reversed in the stirrups to represent a fallen commander looking back on his troops for the last time.\n\nIn 1865, Abraham Lincoln was honored by the inclusion of a riderless horse at his funeral. When Lincoln's funeral train reached Springfield, Illinois, his horse, Old Bob, who was draped in a black mourning blanket, followed the procession and led mourners to Lincoln's burial spot.\n\nA notable riderless horse was \"Black Jack,\" a half-Morgan named for General of the Armies John \"Black Jack\" Pershing. Black Jack took part in the state funerals of Presidents John F. Kennedy (1963),\nHerbert Hoover (1964), and Lyndon Johnson (1973), and General of the Army Douglas MacArthur (1964).\n\nBlack Jack was foaled January 19, 1947, and came to Fort Myer from Fort Reno, Oklahoma, on November 22, 1952. Black Jack was the last of the Quartermaster-issue horses branded with the Army's U.S. brand (on the left shoulder) and his Army serial number 2V56 (on the left side of his neck). He died on February 6, 1976, and was buried on the parade ground of Fort Myer's Summerall Field with full military honors, one of only two US Army horses to be given that honor.\n\n\"Dolly\", was the 22 year old charger (whose official name was Octave) of Admiral of the Fleet The Earl Mountbatten of Burma in his capacity as Colonel of the Life Guards. Following the assassination of Lord Mountbatten by the IRA in Mullaghmore, Dolly served as the riderless horse in the funeral procession being led ahead the head of the gun carriage with the Lord Mountbatten's boots (from his Colonel's uniform) reversed in the stirrups on 5 September 1979.\n\n\"Sergeant York\" was formerly known as \"Allaboard Jules\", a racing standardbred gelding. He was renamed (in honor of famous WWI soldier Alvin C. York) when he was accepted into the military in 1997. He served as the riderless horse in President Ronald Reagan's funeral procession, walking behind the caisson bearing Reagan's flag-draped casket. In the stirrups were President Reagan's personal riding boots.\n\nHe was foaled in 1991, sired by Royce and out of the mare Amtrak Collins sired by Computer. He is a descendant of the great standardbred racing stallions Albatross, Tar Heel and Adios.\n\n"}
{"id": "1802673", "url": "https://en.wikipedia.org/wiki?curid=1802673", "title": "Roadside memorial", "text": "Roadside memorial\n\nA roadside memorial is a marker that usually commemorates a site where a person died suddenly and unexpectedly, away from home. Unlike a grave site headstone, which marks where a body is laid, the memorial marks the last place on earth where a person was alive – although in the past travelers were, out of necessity, often buried where they fell.\n\nUsually the memorial is created and maintained by family members or friends of the person who died. A common type of memorial is simply a bunch of flowers, real or plastic, taped to street furniture or a tree trunk. A handwritten message, personal mementos, etc. may be included. More sophisticated memorials may be a memorial cross, ghost bike, or a plaque with an inscription, decorated with flowers or wreaths.\n\nRoadside memorials tend to be clustered along the busiest roadways and often times at intersections. This is evident when examining the clustering of tribute sites on a map such as this one showing how many sites are along US 30 in northern Indiana.\n\nRoadside memorials are a statement of grief and love from the loved ones of the accident victim(s).\n\nBut apart from their personal significance, these memorials also serve as a reminder and warning to other road users of the dangers of driving, and to encourage safer driving. In the 1940s and 1950s, the Arizona State Highway Patrol began using white crosses to mark the site of fatal car accidents. This practice was continued by families of road-crash victims after it had been abandoned by the police. The ghost bike phenomenon, where an old bicycle is painted white and locked up at an accident site, serves the same purpose in relation to cycling casualties. Some tribute sites include elaborate displays to memorialize the personality of the person to whom the tribute site is dedicated, including action figures and lights.\n\nHistorically, roadside memorials were personal memorials, but there is a modern trend toward public memorials of increasingly large size. Typically little or no effort is made to make the memorials accommodate the natural beauty of the landscape and many roadside memorials, over time, lack proper maintenance.\n\nThe phenomenon of roadside memorials may be associated with another growing trend: public outpouring of grief for celebrities. The death of Diana, Princess of Wales, for example, precipitated an avalanche of flowers and wreaths at the Pont de l'Alma road tunnel in Paris, the site of her death, and at Kensington Palace, her home in London. While car-crash victims are rarely so well known, something of the same sort of impulse to make a public display of emotion at the site of a tragedy may be partly responsible for the growing popularity of roadside memorials. The broad phenomenon of creating improvised and temporary memorials after traumatic death (accidents, murder, disasters etc.) has become popular since the 1980s. Because of their non-institutionalized character they are generically coined as grassroots memorials.\n\nRoadside memorials have been erected around the world for centuries. Their legality varies from country to country. A web site has been created in order to preserve in a more permanent manner the physical roadside tributes in a virtual manner. This helps to preserve in a longer term manner the original beauty of the tribute site that tends to degrade over time without regular maintenance. It also provides a means of creating a virtual tribute site for those who haven't yet built a physical memorial at the accident scene.\n\nThe number of memorials erected in Australia since 1990 has increased considerably. In 2003, it was estimated that one in five road deaths were memorialized at the site of the crash.\n\nIt is traditional in Ukraine to place a roadside memorial on the site of a deadly car or motorcycle crash. It is usually a cross or a small monument with a wreath of flowers. There are also usually fresh flowers regularly placed by the cross if the relatives of the person who died live close enough to look after the memorial. Sometimes Ukrainian roadside memorials can be more elaborate, including a small granite or marble gravestone and/or a picture of the loved one.\n\nIn the United Kingdom, the practice of erecting roadside memorials has recently generated a media debate about the danger these memorials may pose to other road users and to people erecting them in unsafe places. This debate has been sparked by accounts of dangerous actions, such as when an adult crosses a main road with a child to place a tribute. Some jurisdictions already enforce local regulations, and police officials and local councilors have suggested that uniform rules be introduced across the country. For example, according to the BBC, in Merthyr Tydfil, memorials will only be allowed where it is deemed safe and appropriate, and they will be removed after three months.\n\nThe spread of spontaneous roadside memorials to mark the site of fatal traffic accidents in the United States is a relatively new phenomenon. There is a gravestone-style memorial in Ellington, CT marking a child's death in 1812. A typical memorial includes a cross (usually wooden), flowers, hand-painted signs, and, in the case of a child's death, stuffed animals. \n\nThe spread of roadside memorials in the United States has increased in recent decades as a result of large immigrant populations from Mexico entering the country . And while not limited to Mexican populations, roadside memorials are most common in areas with large Mexican populations. Formerly, in funerary processions where a group would proceed from a church to a graveyard carrying a coffin, the bearers would take a rest, or \"descanso\" in Spanish, and wherever they set the coffin down, a cross would be placed there in memory of the event. The modern practice of roadside shrines commemorate the last place a person was alive before receiving fatal injuries, even if they should actually die in a hospital after the crash.\n\nIn the southwestern United States, they are also common at historic parajes on old long distance trails, going back to the roots of the tradition, and also marked the graves of people who died while traveling. A descanso memorial may be decorated especially for the holidays, and for significant anniversaries in the person's life. A descanso memorial for a child may be decorated with special toys, even toy vignettes of family life, and votive candles may be placed there on special nights.\n\nIn the United States, the legal situation varies from state to state.\n\nIn New Mexico, Department of Transportation crews undertaking new construction are not required to protect them, but usually either avoid altering them, or otherwise place them as close to where they originally were as possible once construction has been completed as a courtesy.\n\nIn California, Streets and Highways Code Section 101.10 directs the California Department of Transportation (Caltrans) to place and maintain memorial signs along state highways that read “Please Don't Drink and Drive” followed by “In Memory of {victim's name}.” Caltrans places signs at the request of victims’ relatives when there is a fatality as a result of an alcohol or drug-impaired driver. The signs are to remain in place for a period of seven years. The department shall charge the requesting party a fee to cover the department’s cost in designing, constructing, placing, and maintaining that sign, and the department’s costs in administering this section.\n\nThe states of Colorado, Massachusetts, Kentucky, and Wisconsin ban such memorials.\n\nIn the state of Delaware, roadside memorials are illegal per the Clear Zone Act for safety reasons. As an alternative to roadside memorials, the Delaware Highway Memorial Garden located at the Smyrna Rest Area consists of a path with bricks bearing the names of people who died along roads in Delaware. Other states impose specific requirements for roadside memorials.\nIn Birmingham, Alabama, roadside memorials have been removed from Interstate highways. Some people view unauthorized street memorials as illegal and think they constitute the taking of public property for private purposes, and are also a distraction and therefore dangerous to the motoring public. Others think they serve as a sort of public service announcement that reminds drivers to be careful and drive safely, and are no more distracting than any other roadside advertisement. For anyone but those close to the death, they may do little but clutter the landscape. If the memorial is located on a road that the loved ones seldom or never travel, or in a remote area, it may be seen as a form of grandstanding.\n\nUsing a Christian cross as a memorial along a public highway can be seen as an illegal endorsement of religion and has been challenged in a growing number of lawsuits by secular groups concerned about the separation of church and state. On 18 August 2010 the Tenth Circuit held that the State of Utah violated the Establishment Clause by constructing a series of 12-foot high Latin crosses along the roadside to memorialize fallen state troopers. In Lake Elsinore California, a personal roadside cross was removed following a complaint by the American Humanist Association.\n\n\n"}
{"id": "5430925", "url": "https://en.wikipedia.org/wiki?curid=5430925", "title": "Sasha and Zamani", "text": "Sasha and Zamani\n\nSasha and Zamani are two aspects of time as expressed in some Eastern and Central African cultures. Sasha are spirits known by someone still alive, while Zamani are spirits not known by anyone currently alive. Sasha are concerned with, and are expressed as, the present time, the recent past, and the near future; while Zamani is the limitless past. Potential time is the third part of the space-time continuum in African Thought. People must learn from the past to act wisely in the present to create a good future.\n\nAccording to James Loewen in his book \"Lies My Teacher Told Me\": \"The recently departed whose time overlapped with people still here are the Sasha, the living dead. They are not wholly dead, for they live on in the memories of the living ... when the last person knowing an ancestor dies, that ancestor leaves the Sasha for the Zamani, the dead. As generalized ancestors, the Zamani are not forgotten but revered.\" Loewen cites and paraphrases the explanation from \"African Religions and Philosophy\" by John Mbiti. Sasha and Zamani are not stages of \"death\". They are the two ontological stages (or dimensions) of \"history\" in Swahili culture.\n\nIn Swahili, the two time dimensions are called sasa and zamani. Both have quality and quantity. People speak of them as big, small, little, short, long, etc. in relation to a particular event. Sasha refers to the events that have just taken place or are taking place now at the moment or are just about to take place in the near future. Sasha time can extend into the future for about six months or, at the most, one year. Zamani time overlaps sasha time to some extent in the present, but it also goes back very far into the past time. It absorbs, holds, and stores all the events that have ever occurred. It is more significant than sasha because it stretches endlessly back into the past. It includes the time of myth when all the stories of creation took place and when the great and famous heroes of the past performed their exploits. (Anderson 1986, Mbiti 1994.)\nZamani is not limited to what in English is called the past.… Zamani overlaps with Sasha and the two are not separable. Sasha feeds or disappears into Zamani. But before events become incorporated into the Zamani, they have to become realized or actualized within the Sasha dimension. When this has taken place, the events \"move\" backwards from the Sasha into the Zamani…. It is the final storehouse for all phenomena and events, the ocean of time in which everything becomes absorbed into a reality that is neither after nor before. \nJohn S. Mbiti, African Religions and Philosophy, 2nd ed. (Oxford: Heinemann, 1990), 22. \n"}
{"id": "2064637", "url": "https://en.wikipedia.org/wiki?curid=2064637", "title": "Seni Pramoj", "text": "Seni Pramoj\n\nMom Rajawongse Seni Pramoj (, , ; 26 May 190528 July 1997) was three times the prime minister of Thailand, a politician in the Democrat Party, lawyer, diplomat and professor. A descendant of the Thai royal family, he was the great-grandson of King Rama II. His final two terms as PM sandwiched the only term of his brother, Kukrit Pramoj.\n\nBorn a son of Prince Khamrob and mother Daeng (Bunnag), he was educated at Trent College in Derbyshire before obtaining a BA second class honours degree in jurisprudence from Worcester College, Oxford. He continued his studies at Gray's Inn, London, receiving first honours. After returning to Thailand he studied Thai Law, and following six months as a trainee at the Supreme Court, he started to work at the Justice Civil Court. Later, he was transferred to the Foreign Ministry and in 1940 was sent to the United States as Thai ambassador.\n\nJapanese forces invaded Thailand early on the morning of 8 December 1941, shortly after the attack on the United States at Pearl Harbor, Hawaii. The Prime Minister, Field Marshal Plaek Phibunsongkhram, ordered a ceasefire at noon, entering into an armistice that allowed the Japanese to use Thai military installations in their invasion of Malaya and Burma. On 21 December, a formal military alliance with Japan was concluded.\n\nThe Phibun government declared war on Great Britain and the United States on 25 January 1942. Although the Thai ambassador in London delivered Thailand's declaration of war to the British administration, Seni refused to do so. Instead, he considered organising a resistance movement in the United States.\n\nFollowing a late morning interview with Secretary Cordell Hull on 8 December, Seni returned to his legation to confer with his staff. The ambassador and his staff unanimously decided to cast their lot with the Allies. Late the same afternoon, he returned to the State Department to offer their services to the Allied cause. Blaming pro-Japanese elements for the early Thai surrender, he spoke to Hull of unfreezing Thai assets in the United States for further prosecution of the war and suggested that the Thais in the country might \"organise and preserve a government of true patriotic, liberty-loving Thais while his government is in the clutches of Japan.\"\n\nThe State Department decided to act as if Seni continued to represent Thailand. This enabled him to tap into the frozen Thai assets. When asked to draw up a list of \"reliable and influential Thai nationals known to be definitely patriotic and anti-Japanese\" by the State Department (at the suggestion of John P. Davies), Seni named Regent Pridi Phanomyong, politicians Khuang Aphaiwong and Wilat Osathanon, and diplomats Phraya Sisena and Direk Jayanama as \"reliables\".\n\nSeni advanced plans to mobilise Thai volunteers in support of the Allies. Beyond the legation staffers and their families, most other Thai residents were students enrolled at colleges and universities, including institutions such as Harvard, the Massachusetts Institute of Technology, and Cornell. Many chose to stay following the Thai declaration of war in January, refusing repatriation. Most, like Seni, saw their nation as a victim of Japanese aggression.\n\nSeni became prime minister on 17 September 1945, the day he returned to Bangkok. However, he found his position as the head of a cabinet packed with Pridi's loyalists quite uncomfortable. Northeastern populist politicians like Tiang Sirikhanth and Bangkok newcomers like Sanguan Tularaksa were not people the aristocratic Seni preferred to associate with. They, in turn, viewed Seni as an elitist who was entirely out of touch with Thailand's political realities. Pridi continued to wield power behind the scenes as he had done during the Khuang government. The regent's looming presence and overarching authority rankled the proud, thin-skinned Seni, fuelling a personal animosity that would poison Thailand’s postwar politics.\n\nThe Pramoj brothers subsequently joined the newly formed Democrat Party in 1946, which was for the most part made up of royalists and conservative reactionaries. Seni would spend the next two years vigorously carrying out a personal campaign against Pridi. Earlier in the year he had called for an investigation of the use of the US$500,000 in Thai assets unfrozen by the US government that he had turned over to the OSS. Insinuating the money had been transferred to the senior statesman, he lamented that \"most of the money had not been spent for what it was intended.\" An independent investigatory panel, however, found no mistake, concluding that the Free Thai had \"performed remarkably well\" and that the Thai people \"owe a great deal to them.\" The outcome left the ex-prime minister looking extremely foolish.\n\nSeni soon got his revenge, however. In the immediate aftermath of King Ananda Mahidol's death, Seni and his party launched relentless attacks against the government and accused Pridi of being responsible for the king's assassination, the implausibility of the charge notwithstanding.\n\nIn November 1947 the Democrat Party cooperated with disgruntled army officers to oust the government of Thawan Thamrongnawasawat. As part of the deal, Seni was awarded a cabinet portfolio in Khuang's coup-installed cabinet.\n\nOn Tuesday, 14 June 1949, in a lecture delivered before the Siam Society, Seni pleaded, \"[I] happen to belong to that peculiar species known as politicians who are in the incorrigible habit of attempting to accomplish the impossible.\" Word had gotten around that his brother and he had been \"getting up a little English translation of some of King Mongkut's public papers and private correspondence...without actually putting it to a final execution.\" He chooses to speak of the king in his capacity as a legislator, \"because legislation is the field I am more closely familiar with than any other.\" Seni provides, \"ample evidence to show that the King was the first and foremost democrat of our country,\" and quotes from an Act declaring an election whereby \"any person, even though he be a slave, who is believed to be so sufficiently possessed of wisdom and restraint as to be able to give clear and satisfactory judgment in accordance with truth, justice and the law may be elected as judge.\" With regard to the 1944 semi-fictionalized biographical novel \"Anna and the King of Siam\" and the 1946 Hollywood film of the same title, Seni quotes from Acts and judicial decisions that give the lie to the fiction.\n\nSeni returned to his job as a lawyer, but remained active in the Democrat Party during the period of military rule. He served again briefly as prime minister from 15 February to 13 March 1975, when he was defeated and replaced by his younger brother, Rajawongse Kukrit Pramoj. However, Kukrit's government only lasted until 20 April 1976, when Seni regained the top political office.\n\nSeni's final term was a time of crisis in the nation. A right-wing backlash against leftist student demonstrators culminated in the Thammasat University massacre on 6 October 1976, the military forced him out of office and installed hard-line royalist Tanin Kraivixien as premier.\n\nSeni decided to resign as the leader of the Democrat Party and left politics permanently. He worked as a lawyer until his retirement.\n\n\n"}
{"id": "7211969", "url": "https://en.wikipedia.org/wiki?curid=7211969", "title": "Simple church", "text": "Simple church\n\nThe simple church is an Evangelical Christian movement that reinterprets the nature and practice of church.\n\nA simple church may meet anywhere with or without trained leaders, formal liturgy, programs or structures. To facilitate relationship, discipleship (spiritual formation), multiplication, mobility, and member ownership, a simple church is usually a small group of no more than 20-25 persons. Most Church \"programs\" privately meet during some days of the week and discuss troubles that they are having with their faith, and personal life. Church \"programs\" are virtually nonexistent and small group participation is essential. The process of moving from worship to small group, small group to mission work, and mission work to worship is a primary focus.\n\nAuthors Tony and Felicity Dale, founders of \"House2House Ministries\", have promoted the term \"simple church\" in their book \"Simply Church\". The term is often used interchangeably with other terms like organic church, essential church, primitive church, bodylife, relational church, and micro-church.\n\nIn the early twenty-first century a number of established Christian denominations and mission organizations have officially supported efforts to develop house church networks. These include the Free Methodist Church in Canada, the Foursquare Gospel Church of Canada, the Evangelical Fellowship of Canada, the Presbyterian Church in Canada, Partners in Harvest, the Southern Baptist Convention, Dove Christian Fellowship International, DAWN Ministries (Discipling a Whole Nation), Youth With A Mission (YWAM), and Eternal Grace.\n\nMany in the simple church movement point to the New Testament, especially the Gospels, Acts, and the writings of the Apostle Paul for justification of their model (see House Church, Scriptural Basis). Historically speaking, simple gatherings of Christians were the norm of Early Christianity. Between 100AD and 300AD, Christianity grew from 25,000 to 20 million people in the Roman Empire. In fact, much of the New Testament was written to people who met in house churches.\n\nEarly Christian house churches were patterned after house synagogues which were numerous. Christians took a low-cost and easy-to-multiply model and adapted it to their new Christian context. In addition, the Communion service, sometimes called the Lord's Supper, was uniquely Christian (though modeled on the Passover). Since it did not apply to Jews and therefore did not fit in the Jewish synagogues, it had to be celebrated somewhere else. House churches were the natural place for communion to be shared. As time went on, Christians were banned from Jewish synagogues as persecution intensified (see Split of early Christianity and Judaism). Although house churches flourished in times of persecution, they were well-established before them.\n\nIn the West, simple church can be traced back to the house church movement. In North America and the UK particularly, the house church movement is often viewed as a development and logical extension of the 'Brethren' or Plymouth Brethren movement, where many individuals and assemblies have adopted new approaches to worship and governance, while others recognise a relationship to the Anabaptists, Quakers, Amish, Hutterites, Mennonites, Moravians, Methodists, and the much earlier Waldenses and Priscillianists. Another perspective sees the house church movement as a re-emergence of the movement of the Holy Spirit during the Jesus Movement of the 1970s in the USA or the worldwide Charismatic Renewal of the late 1960s and 1970s. Others see it as a return to a New Testament church restorationist paradigm and a restoration of God's eternal purpose and the natural expression of Christ on the earth, urging Christians to return from hierarchy and rank to practices described and encouraged in Scripture.\n\nSimple church has also been influenced by overseas missions and the growth of church planting movements. Church planting movements are spontaneously growing church multiplication efforts.\n\nThe missional Movement has also influenced simple church.\n\nSIMPLE-CHURCH IN THE U.K.\nSimpleChurch movement has begun to emerge in the Unoted Kingdon. House Churches have existed for some years in places where traditional churches have closed, or not met the needs of evangelicals (or charismatics). Due in some cases to the Church of England being perceived to have moved away from biblical doctrines.\nA website promoting Simple Church in the U.K. is at simplechurch.org.uk\n\nAs in any decentralized, spontaneous movement, a variety of values are expressed in simple church. Due to the influence of some key groups and Acts 2:42-47, three overarching values have emerged in many circles. Adherents Paul Kaak (who began ministry in one of the largest and most systematized mega-churches in America) and Neil Cole originally articulated these values using the letters DNA. According to him:\n\nThese values have since been promoted by House2House Ministries and DAWN North America, and have been adopted by various groups such as New York's MetroSoul\n\nAdherents George Barna and Frank Viola's book \"Pagan Christianity\" points out a number of reforms that organic churches often advocate.\n\nIn the early twenty–first century the growth of the movement has had increased news media coverage:\n\nMany books have been written on the simple church movement, especially by insiders (see House Church, Recommended Books). In the early twentyfirst century books began to appear by those studying the movement from a more objective view, including George Barna's \"Revolution\". Barna says that \"revolutionary\" expressions such as simple church will soon account for one third of American spirituality.\n\nVisibility of the movement also increased due to national and regional gatherings of various kinds. The largest of these is the Annual House Church Conference held in Dallas, USA and occasionally at other locations by House2House.\n\nHow the simple church movement relates to constructing a theology and ecclesiology is the subject of much debate, especially with critics of the movement.\n\nSeveral prominent voices have serious concerns about simple church. For example, J. Lee Grady (Charisma Online Editor) says such a movement wants to \"reinvent the church without its biblical structure and New Testament order — and without the necessary people who are anointed and appointed by God to lead it. To follow this defective thesis to its logical conclusion would require us to fire all pastors, close all seminaries and Bible colleges, padlock our sanctuaries and send everybody home...\" Grady and other critics worry that the simple church movement could encourage people to leave more traditional forms of church, which could lead to further collapse or decline of Christendom.\n\n\n"}
{"id": "2100624", "url": "https://en.wikipedia.org/wiki?curid=2100624", "title": "Spermatheca", "text": "Spermatheca\n\nThe spermatheca (pronounced plural: spermathecae ), also called receptaculum seminis (plural: receptacula seminis), is an organ of the female reproductive tract in insects, e.g. bees, some molluscs, oligochaeta worms and certain other invertebrates and vertebrates. Its purpose is to receive and store sperm from the male or, in the case of hermaphrodites, the male component of the body. Spermathecae can sometimes be the site of fertilization when the oocytes are sufficiently developed.\n\nSome species of animal have multiple spermathecae. For example, certain species of earthworms have four pairs of spermathecae—one pair each in the 6th, 7th, 8th, and 9th segments. The spermathecae receive and store the spermatozoa of another earthworm during copulation. They are lined with epithelium and are variable in shape: some are thin, heavily coiled tubes, while others are vague outpocketings from the main reproductive tract. It is one of the many variations in sexual reproduction.\n\nThe nematode \"Caenorhabditis elegans\" has two spermathecae, one at the end of each gonad. The \"C. elegans\" spermatheca is made up of 24 smooth muscle-like cells that form a stretchable tubular structure. Actin filaments line the spermatheca in a circumferential manner. The \"C. elegans\" spermatheca is used as a model to study mechanotransduction.\n\nAn apiculturist may examine the spermatheca of a dead queen bee to find out whether it had received sperm from a male. In many species of stingless bees, especially Melipona bicolor, the queen lays her eggs during the provisioning and oviposition process and the spermatheca fertilizes the egg as it passes along the oviduct. The haplo-diploid system of sex determination makes it possible for the queen to choose the sex of the egg.\n\n"}
{"id": "27419972", "url": "https://en.wikipedia.org/wiki?curid=27419972", "title": "Synthetic mycoides", "text": "Synthetic mycoides\n\nSynthetic mycoides refers to an artificial life form created by Craig Venter at the J Craig Venter Institute in May 2010. A synthetic genome was transferred into an empty cell to form the bacterium, which was capable of self replication and functioned solely from the transferred chromosomes.\n"}
{"id": "18182366", "url": "https://en.wikipedia.org/wiki?curid=18182366", "title": "Telescoping generations", "text": "Telescoping generations\n\nTelescoping generations can occur in parthenogenetic species, such as aphids or other life forms that have the ability to reproduce without ovum fertilization. This occurrence is characterized by a viviparous female having a daughter growing inside her that is also parthenogenetically pregnant with a daughter cell.\n\nThis pattern of reproduction can also occur in certain mites that are not parthenogenetic, e.g. \"Adactylidium\", in which the young hatch and mate within the mother, eating her from the inside and then escaping; in some species the males never escape, and in others they die shortly afterwards. However, the resulting inbreeding has consequences much like those of parthenogenesis, and the females are not actually pregnant on hatching but become pregnant before emerging into free living.\n"}
{"id": "22154200", "url": "https://en.wikipedia.org/wiki?curid=22154200", "title": "The New Meditation Handbook", "text": "The New Meditation Handbook\n\nThe New Meditation Handbook: Meditations to Make Our Life Happy and Meaningful (Tharpa Publications (2003) ) is a guide to Buddhist philosophy and meditation techniques. It is a compilation of twenty-one concise meditations on Lamrim, or the stages of the path to enlightenment, by Geshe Kelsang Gyatso, a Buddhist teacher and author in the West.\n\n\"The New Meditation Handbook\" contains twenty-one meditations on Lamrim that are designed to be followed in a cycle, thus covering all of Buddha's teachings every 21 days. The meditations are described as \"actual methods to control our mind. Because everyone has different wishes and capacities, many different levels of meditation practice are given.\"\n\nGeshe Kelsang Gyatso, a Tibetan monk, entered the monastery at the age of eight and has spent the past 30 years establishing Buddhist centers throughout the world. He provides 21 contemplative meditations to systematically guide a seeker to enlightenment according to Buddhist philosophy.\n\nThe 21 meditations are intended to give a concise overview of essential Buddhist teachings and philosophy with the purpose of increasing peace of mind, positivity, and spiritual wisdom. \"Foreword Magazine\" says: \"Geshe Kelsang Gyatso's words provide a stimulatingly peaceful perspective.\", Kirkus Reviews called it \"Clear, inspirational writing\" and Booklist says \"\"The New Meditation Handbook\" is a simple, sincere guide to Buddhist philosophy and meditation techniques. According to \"Spirituality and Health Magazine\": \"This manual provides a succinct and inspiring overview of the many ways in which Buddhism can be applied to the situations and activities of daily life.\" \n\nThe book focuses on Buddha's teaching that all human beings have potential for spiritual growth by overcoming negative minds and increasing positive ones in daily life. Donna Seaman of Booklist suggests: \"Geshe Kelsang has a unique gift for simultaneously addressing everyday difficulties--particularly in his useful and inspiring elucidation of how to control anger and practice patience--and bringing into focus the spiritual dimension in which they reverberate.\"\n\nThe author is the founder of the New Kadampa Tradition, which has 1100 Buddhist centers and groups worldwide.\n"}
{"id": "417164", "url": "https://en.wikipedia.org/wiki?curid=417164", "title": "USS Corvina", "text": "USS Corvina\n\nUSS \"Corvina\" (SS-226), a \"Gato\"-class submarine, was the only ship of the United States Navy to be named for the corvina, a saltwater fish.\n\nHer keel was laid down by the Electric Boat Company of Groton, Connecticut on September 21, 1942. She was launched on May 9, 1943 (sponsored by Mrs. LaRene P. Christie, wife of Rear Admiral Ralph. W. Christie, commander of submarine operations in Fremantle, Australia), and commissioned on August 6, 1943 with Commander Roderick S. Rooney (Class of 1929) in command.\n\nClearing New London, Connecticut, on September 18, 1943, \"Corvina \" arrived at Pearl Harbor on October 14. She put out from Pearl Harbor on her maiden war patrol November 4, topped off her fuel tanks at Johnston Island two days later, and was never heard from again.\n\nHer assignment had been a dangerous one: to patrol as closely as possible to the heavily guarded stronghold of Truk and to intercept any Japanese sortie endangering the forthcoming American invasion of the Gilbert Islands. Japanese records report that Japanese submarine \"I-176\" launched three torpedoes at an enemy submarine south of Truk on November 16, claiming two hits which resulted in the explosion of the target. Her loss with her crew of 82 was announced March 14, 1944, making \"Corvina\" the only American submarine to be sunk by a Japanese submarine in the entire war.\n\nThe loss of the \"Corvina\" is referenced in the 1951 John Wayne film Operation Pacific. In the film, the fictitious \"Gato\"-class sub USS \"Thunderfish\" makes an impromptu rendezvous with the \"Corvina\" after the \"Corvina\" had reported problems on Number 4 Main Engine. The subs exchange engine parts and the captains also exchange films, John Wayne offering George Washington Slept Here, and the \"Corvina's\" captain offering \"a submarine picture\", later revealed to be the 1943 film Destination Tokyo. Later, while the crew of the \"Thunderfish\" are watching Destination Tokyo, John Wayne is shown trying to figure out torpedo explosions reported by the sub's sonar operator. The following day, the \"Thunderfish\" comes across wreckage, and discovers the case containing George Washington Slept Here, revealing that the \"Corvina\" had been sunk. The \"Thunderfish's\" radar then reports a single contact, and the sub submerges. John Wayne discovers \"one I-type Jap submarine\" while looking through the periscope. The \"Thunderfish\" then engages, torpedoes, and sinks the Japanese sub, avenging the loss of the \"Corvina\".\n\n\n"}
{"id": "5638483", "url": "https://en.wikipedia.org/wiki?curid=5638483", "title": "Undying Ones", "text": "Undying Ones\n\nThe Undying Ones are a fictional race appearing in American comic books published by Marvel Comics. They are a race of humanoid demons from another dimension.\n\nThe Undying Ones were first depicted as statues in \"Doctor Strange\" #183 (November 1969), by Roy Thomas and Gene Colan. Their first actual appearance was in \"Sub-Mariner\" #22 (February 1970).\n\nThe Undying Ones subsequently appear in \"Incredible Hulk\" #126 (April 1970), \"Defenders\" #1-3 (August–December 1972), #18 (December 1974), #20 (February 1975), \"Incredible Hulk\" #269 (March 1982), and \"Doctor Strange\" #41 (May 1992).\n\nThe Undying Ones received an entry in the \"All-New Official Handbook of the Marvel Universe A-Z\" #12 (2006).\n\nThe Undying Ones once held great power over Earth but were forced to return to their own dimension over a millennium ago when their powers began to fade for unexplained reasons. Their powers having been renewed, the Undying Ones unsuccessfully attempted to invade Earth again in recent years under the leadership of the Nameless One. The dimension of the Undying Ones has since been conquered by the creature known as the Slitherer or the Dark-Crawler.\n\nThe Undying Ones have come into conflict with Doctor Strange, and the Defenders.\n\nThe Undying Ones are led by the Nameless One. The \"Nameless One\" appears to be a title that can be inherited from the previous holder.\n\nDuring the \"Fear Itself\" storyline, the Undying Ones slip through the barriers that are weakened due to the fear and chaos that is occurring.\n"}
{"id": "2421010", "url": "https://en.wikipedia.org/wiki?curid=2421010", "title": "Wear Sunscreen", "text": "Wear Sunscreen\n\n\"Advice, like youth, probably just wasted on the young\", commonly known by the title \"Wear Sunscreen\", is an essay written as a hypothetical commencement speech by columnist Mary Schmich, originally published in June 1997 in the \"Chicago Tribune\". The essay, giving various pieces of advice on how to live a happier life and avoid common frustrations, spread massively via viral email, often erroneously described as a commencement speech given by author Kurt Vonnegut at MIT.\n\nThe essay became the basis for a successful spoken word song released in 1999 by Baz Luhrmann, \"Everybody's Free (To Wear Sunscreen)\", also known as \"The Sunscreen Song\". The song inspired numerous parodies.\n\nMary Schmich's column \"Advice, like youth, probably just wasted on the young\" was published in the \"Chicago Tribune\" on June 1, 1997. In the column's introduction Schmich presents the essay as the commencement speech she would give if she were asked to give one.\n\nIn the speech she insistently recommends the wearing of sunscreen, and dispenses other advice and warnings which are intended to help people live a happier life and avoid common frustrations. She later explained that the initial inspiration for what advice to offer came from seeing a young woman sunbathing, and hoping that she was wearing sunscreen, unlike what she herself did at that age.\n\nThe essay soon became the subject of an urban legend which claimed it was an MIT commencement speech given by author Kurt Vonnegut. In reality, MIT's commencement speaker in 1997 was Kofi Annan and Vonnegut had never been a commencement speaker there. Despite a follow-up article by Schmich on August 3, 1997, the story became so widespread that Vonnegut's lawyer began receiving requests to reprint the speech. Vonnegut commented that he would have been proud had the words been his.\n\nSchmich published a short gift book adaptation of the essay, \"Wear Sunscreen: A Primer for Real Life\", in 1998. A tenth anniversary edition was published in 2008.\n\nThe essay was used in its entirety by Australian film director Baz Luhrmann on his 1998 album \"Something for Everybody\", as \"Everybody's Free (To Wear Sunscreen)\". It was released in some territories in 1997, with the speech (including its opening words, \"Ladies and Gentlemen of the Class of '97\") completely intact. This version appeared in the Triple J Hottest 100 of that year at number 16 in the countdown, and was released on the subsequent CD in early 1998.\n\nAlso known as \"The Sunscreen Song\", it sampled Luhrmann's remixed version of the song \"Everybody's Free (To Feel Good)\" by Rozalla, and opened with the words, \"Ladies and Gentlemen of the Class of '99\" (instead of \"'97\", as in the original column). It was later released as a single in 1999.\n\nLuhrmann explained that Anton Monsted, Josh Abrahams, and he were working on the remix when Monsted received an email with the supposed Vonnegut speech. They decided to use it but were doubtful of getting through to Vonnegut for permission before their deadline, which was only one or two days away. While searching the Internet for contact information they came upon the \"Sunscreen\" authorship controversy and discovered that Schmich was the actual author. They emailed her and, with her permission, recorded the song the next day.\n\nThe song features a spoken-word track set over a mellow backing track. The \"Wear Sunscreen\" speech is narrated by Australian voice actor Lee Perry. The backing is the choral version of \"Everybody's Free (To Feel Good)\", a 1991 song by Rozalla, used in Luhrmann's film \"William Shakespeare's Romeo + Juliet\". The chorus, also from \"Everybody's Free\", is sung by Quindon Tarver.\n\nThe song was a top ten hit across Europe, but largely obscure in the US until Aaron Scofield, a producer in Phoenix, Arizona, edited the original 12\" version into a segment of a syndicated radio show called \"Modern Mix\". This show played on many stations in the United States. In Portland, Oregon—where \"Modern Mix\" played on KNRK—listeners began requesting the track. KNRK program director Mark Hamilton edited the song for time and began playing it regularly. He distributed the song to other program directors that he networked with and the song exploded in the US.\n\nThe song reached number 24 on the Billboard Hot 100 Airplay in the United States; by the time it was released as a commercial single in the country, radio airplay had declined significantly, and only managed to peak at number 45 on the Billboard Hot 100. It also peaked at number one in the United Kingdom and Ireland, partly due to a media campaign by Radio One DJ Chris Moyles.\n\nThere are four versions of the song: the original 7:09 minute mix from the album \"Baz Luhrmann Presents: Something for Everybody\"; a 1999 single release which features a 5:05 minute edit that lacks both choruses; \"Geographic's Factor 15+ Mix\" that runs for 4:42 minutes; and a \"2007 Mix\" of the original 7:09 minute version released on the 10th Anniversary Edition of the \"William Shakespeare's Romeo + Juliet\" soundtrack on which the opening words are changed to \"Ladies and gentlemen of the class of 2007\".\n\nThere are two videos for the song: one which uses the 1999 5:05 minute single edit of the song (the version in which Quindon Tarver is not featured), directed and animated by Bill Barminski; and another using the 7:09 minute edit made by the Brazilian advertising agency DM9DDB.\n\nThe song also appeared in Germany and was soon followed by a German version with the title \"Sonnencreme\". The German translation is narrated by the German actor Dieter Brandecker. A Brazilian version is narrated in Portuguese by Pedro Bial, and a Swedish version is narrated by Rikard Wolff. A Russian adaptation of the song, recorded live by Silver Rain Radio, was performed by Alex Dubas and Yolka.\n\nOn August 10, 2008 the song re-entered the UK Singles Chart at number 72.\n\nCD-Maxi Capitol 8871762 (EMI) / EAN 0724388717625 \n\nThis version was released as a remix to Baz Luhrmann's 1999 version. The remix was written by Mau Kilauea and dubbed as a \"Tropical Remix\" because of the choice of instrumentation. The song was posted to Soundcloud on Mau Kilauea's page and reposted by Spinnin' Records. The song became a first hit for Mau Kilauea.\n\nThe Baz Luhrmann song version inspired numerous parodies. John Safran released a song entitled \"Not the Sunscreen Song\" which peaked at #20 in Australia.\n\nAmerican comedian Chris Rock enjoyed great success with his spoken word song \"No Sex (In the Champagne Room)\" which was in turn parodied on \"Mad TV\" as \"Ain't No Blacks on the TV Screen\" in the style of Rock's stand-up. The song was also parodied in an episode of \"Disney's House of Mouse\" performed by Jiminy Cricket. The comedy group Three Dead Trolls in a Baggie also made a parody entitled \"The Sunscreen Marketing Board\". Jegsy Dodd and the Original Sinners' version, \"Grumpy Old Men\", was voted favourite track of 2005 by BBC Radio 1 listeners in their annual Festive 50 poll.\n\nAngelos Epithemiou's live tour included a parody of the song titled \"Don't Muck About\".\n\nA parody entitled \"Mow Against The Grain\" appears on the \"King of the Hill\" soundtrack album.\n\nAnother parody by comedian Simmi Areff was played on South African radio station East Coast Radio at the conclusion of the 2010 FIFA World Cup.\n\nAnother parody was created in 1999 by London Capital Radio Breakfast Show host Chris Tarrant, who created a version called \"wear slippers\" which consisted of him reading an alternative version of the sunscreen lyrics and demonstrated his disdain for Baz Luhrmann's version.\n\n"}
