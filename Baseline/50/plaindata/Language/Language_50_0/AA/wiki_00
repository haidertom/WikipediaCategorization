{"id": "314951", "url": "https://en.wikipedia.org/wiki?curid=314951", "title": "Academic tenure in North America", "text": "Academic tenure in North America\n\nIn the United States and Canada, tenure is a contractual right that grants a teacher or professor permanent position or employment. It is given as a legal protection against summary dismissal without just cause. Tenure encouraged the development of ideas and thoughts, even if they are not popular among the community. In those countries, tenure is granted only to professors and teachers who prove to be exceptionally productive in their careers.\n\nUnder the tenure systems adopted by many universities and colleges in the United States and Canada, some faculty positions have tenure and some do not. Typical systems (such as the widely adopted \"1940 Statement of Principles on Academic Freedom and Tenure\" of the American Association of University Professors) allow only a limited period to establish a record of published research, ability to attract grant funding, academic visibility, teaching excellence, and administrative or community service. They limit the number of years that any employee can remain employed as a non-tenured instructor or professor, compelling the institution to grant tenure to or terminate an individual, with significant advance notice, at the end of a specified time period. Some institutions require promotion to Associate Professor as a condition of tenure. An institution may also offer other academic positions that are not time-limited, with titles such as Lecturer, Adjunct Professor, or Research Professor, but these positions do not carry the possibility of tenure and are said to be not \"tenure track.\" Typically, they have higher teaching loads, lower compensation, little influence within the institution, few if any benefits, and little protection of academic freedom.\n\nAcademic tenure's original purpose was to guarantee the right to academic freedom: it protects teachers and researchers when they dissent from prevailing opinion, openly disagree with authorities of any sort, or spend time on unfashionable topics. Thus academic tenure is similar to the lifetime tenure that protects some judges from external pressure. Without job security, the scholarly community as a whole may experience pressure to favor noncontroversial lines of academic inquiry. The intent of tenure is to allow original ideas to be more likely to arise, by giving scholars the intellectual autonomy to investigate the problems and solutions as they see fit and to report their honest conclusions. However, it has also become a type of job security for professors.\n\nIn North American universities and colleges, the tenure track has long been a defining feature of professorial employment, although it is less than universal. In North American universities, positions that carry tenure, or the opportunity to attain tenure, have grown more slowly than non-tenure-track positions, leading to a large \"academic underclass\". For example, most US universities currently supplement the work of tenured professors with the services of non-tenured adjunct professors, academics who teach classes for lower wages and fewer employment benefits under relatively short-term contracts. For those that are tenure track, it generally takes about seven years to earn tenure while working as an assistant professor. Tenure is determined by a combination of research, teaching, and service, with each factor weighted according to the values of a particular university, college or department. There is some evidence that professors that share more (e.g. via open access publications, open data, or open source hardware development) gain an advantage in obtaining tenure because they are cited more and can thus develop a higher h-index. This competition for limited resources could lead to ethically questionable behavior.\n\nIn the 19th century, university professors largely served at the pleasure of the board of trustees of the university. Sometimes, major donors could successfully remove professors or prohibit the hiring of certain individuals; nonetheless, a \"de facto\" tenure system existed. Usually professors were only fired for interfering with the religious principles of a college, and most boards were reluctant to discipline professors. The courts rarely intervened in dismissals.\n\nIn one debate of the Cornell University Board of Trustees in the 1870s, a businessman trustee argued against the prevailing system of \"de facto\" tenure, but lost the argument. Despite the power retained in the board, academic freedom prevailed. Another example is the 1894 case of Richard T. Ely, a University of Wisconsin–Madison professor who advocated labor strikes and labor law reform. Though the Wisconsin legislature and business interests pressed for his dismissal, the board of trustees of the university passed a resolution committing itself to academic freedom, and to retaining him (without tenure):\n\nIn all lines of academic investigation it is of the utmost importance that the investigator should be absolutely free to follow the indications of truth wherever they may lead. Whatever may be the limitations which trammel inquiry elsewhere we believe the great state University of Wisconsin should ever encourage that continual and fearless sifting and winnowing by which alone the truth can be found.\n\nThe notorious case of the dismissal of G. B. Halsted by the University of Texas in 1903 after nineteen years of service may have accelerated the adoption of the tenure concept.\n\nA later case at Rollins College, widely publicized and investigated by the American Association of University Professors, which censured Rollins, also played a role in establishing the validity of the tenure concept. This case led breakaway professors to found the innovative and influential Black Mountain College.\n\nIn 1900, the presidents of Harvard University, Columbia University, and the University of Chicago each made clear that no donor could any longer dictate faculty decisions; such a donor's contribution would be unwelcome. In 1915, this was followed by the American Association of University Professors' (AAUP) declaration of principles—the traditional justification for academic freedom and tenure.\n\nThe AAUP's declaration of principles recommended that:\n\nWhile the AAUP pushed reform, tenure battles were a campus non-issue. In 1910, a survey of 22 universities showed that most professors held their positions with \"presumptive permanence\". At a third of colleges, assistant professor appointments were considered permanent, while at most colleges multi-year appointments were subject to renewal. Only at one university did a governing board ratify a president's decisions on granting tenure.\n\nTenure is also offered in many states to public schoolteachers. Louisiana, under state education superintendent T. H. Harris, led the move to establish a teacher protection policy in the 1930s because of past political considerations in hiring and dismissal of educators.\n\nIn 1940, the AAUP recommended that the academic tenure probationary period be seven years—still the current norm. It also suggested that a tenured professor could not be dismissed without adequate cause, except \"under extraordinary circumstances, because of financial emergencies.\" Also, the statement recommended that the professor be given the written reasons for dismissal and an opportunity to be heard in self-defense. Another purpose of the academic tenure probationary period was raising the performance standards of the faculty by pressing new professors to perform to the standard of the school's established faculty.\n\nThe most significant adoption of academic tenure occurred after 1945, when the influx of returning GIs returning to school led to quickly expanding universities with severe professorial faculty shortages. These shortages dogged the Academy for ten years, and that is when the majority of universities started offering formal tenure as a side benefit. The rate of tenure (percent of tenured university faculty) increased to 52 percent. In fact, the demand for professors was so high in the 1950s that the American Council of Learned Societies held a conference in Cuba noting the too-few doctoral candidates to fill positions in English departments. During the McCarthy era, loyalty oaths were required of many state employees, and neither formal academic tenure nor the Constitutional principles of freedom of speech and association were protection from dismissal. Some professors were dismissed for their political affiliations. During the 1960s, many professors supported the anti-war movement against the Vietnam War, and more than 20 state legislatures passed resolutions calling for specific professorial dismissals and a change to the academic tenure system.\n\nTwo landmark U.S. Supreme Court cases changed tenure in 1972: (i) \"Board of Regents of State Colleges v. Roth\", 408 US 564; and (ii) \"Perry v. Sindermann\", 408 US 593. These two cases held that a professor's claim to entitlement must be more than a subjective expectancy of continued employment. Rather, there must be a contractual relationship or a reference in a contract to a specific tenure policy or agreement. Further, the court held that a tenured professor who is discharged from a public college has been deprived of a property interest, and so due process applies, requiring certain procedural safeguards (the right to personally appear in a hearing, the right to examine evidence and respond to accusations, the right to have advisory counsel).\n\nLater cases specified other bases for dismissal: (i) if a professor's conduct were incompatible with his duties (\"Trotman v. Bd. of Trustees of Lincoln Univ.\", 635 F.2d 216 (2d Cir.1980)); (ii) if the discharge decision is based on an objective rule (\"Johnson v. Bd. of Regents of U. Wisc. Sys.\", 377 F. Supp 277, (W.D. Wisc. 1974)). After these cases were judged, the number of reported cases in the matter of academic tenure increased more than two-fold: from 36 cases filed during the decade 1965–1975, to 81 cases filed during the period 1980–1985.\n\nDuring the 1980s there were no notable tenure battles, but three were outstanding in the 1990s. In 1995, the Florida Board of Regents tried to re-evaluate academic tenure, but managed only to institute a weak, post-tenure performance review. Likewise, in 1996 the Arizona Board of Regents attempted to re-evaluate tenure, fearing that few full-time professors actually taught university undergraduate students, mainly because the processes of achieving academic tenure underweighted teaching. However, faculty and administrators defended themselves and the board of trustees dropped its review. Finally, the University of Minnesota Regents tried from 1995 to 1996 to enact 13 proposals, including these policy changes: to allow the regents to cut faculty base- salaries for reasons other than a university financial emergency including poor performance; to fire tenured professors when their programs were eliminated or restructured if the university were unable to retrain or reassign them. In the Minnesota system, 87 percent of university faculty were either tenured or on the tenure track, and the professors vehemently defended themselves. Eventually, the president of the system opposed these changes and weakened a compromise plan by the dean of the law school before it then failed. The board chairman resigned later that year.\n\nThe period since 1972 has seen a steady decline in the percentage of college and university teaching positions in the US that are either tenured or tenure-track. United States Department of Education statistics put the combined tenured/tenure-track rate at 56% for 1975, 46.8% for 1989, and 31.9% for 2005. That is to say, by the year 2005, 68.1% of US college teachers were neither tenured nor eligible for tenure; a full 48% of teachers that year were part-time employees.\n\nIn 1994, a study in \"The Chronicle of Higher Education\" found that \"about 50 tenured professors [in the US] are dismissed each year for cause.\"\nWhile tenure protects the occupant of an academic position, it does not protect against the elimination of that position. For example, a university that is under financial stress may take the drastic step of eliminating or downsizing some departments, in which case both tenured and untenured faculty are let go.\n\nIn 1985, the United States Supreme Court decision \"Cleveland Board of Education v. Loudermill\" determined that a tenured teacher cannot be dismissed without oral or written notice regarding the charges against him or her. Additionally, the Court held that the employer must provide an explanation of the employer's decision, including a discussion of the employer's evidence, and the teacher must be given an opportunity for a fair and meaningful hearing.\n\nIn 2012, tenure for school teachers was challenged in a California lawsuit called \"Vergara v. California\". The primary issue in the case was the impact of tenure on student outcomes and on equity in education. On June 10, 2014, the trial judge ruled that California's teacher tenure statute produced disparities that \"shock the conscience\" and violate the equal protection clause of the California Constitution. On July 7, 2014, U.S. Secretary of Education Arne Duncan commented on the \"Vergara\" decision during a meeting with President Barack Obama and representatives of teacher's unions. Duncan said that tenure for school teachers \"should be earned through demonstrated effectiveness\" and should not be granted too quickly. Specifically, he criticized the 18-month tenure period at the heart of the \"Vergara\" case as being too short to be a \"meaningful bar.\" It has been argued that sometimes it costs money to fire a bad teacher and that the teacher tenure has only become as a scapegoat law to not go through the process of hearings and documentations.\n\nThe AAUP (American Association of University Professors) has handled hundreds of cases where it alleges that tenure candidates were treated unfairly. The AAUP has censured many major and minor universities and colleges for such alleged tenure abuses.\n\nTenure at many universities depends primarily on research publications and research grants although the universities' official policies are that tenure depends on research, teaching and service. The demand that a professor show exemplary production in research is intense. Unless a professor's research is in pedagogy (for instance within a School of Education), articles in refereed teaching journals and obtaining teaching grants often do not contribute greatly towards tenure, as the research is not focused on the professor's creating new knowledge in his or her home discipline.\n\nAt some universities the department chairperson sends forward the department recommendation on tenure. There have been cases, such as one case at The University of Texas at San Antonio (2008), where the faculty voted unanimously to tenure an individual but the chairperson sent forward a recommendation not to grant tenure despite the faculty support. \n\nTenure decisions can result in fierce political battles. In one tenure battle at Indiana University, an untenured professor was accused of threatening violence against those who opposed his promotion, his wife briefly went on a hunger strike, and many called for the entire department to be disbanded. In another instance in February 2010, Amy Bishop with the University of Alabama in Huntsville shot and killed colleagues after losing her appeal for tenure.\n\nSince the 1970s philosopher John Searle has called for major changes to tenure systems, calling the practice \"without adequate justification.\" Searle suggests that to reduce publish or perish pressures that can hamper their classroom teaching, capable professors be given tenure much sooner than the standard four-to-six years. However, Searle also argued that tenured professors be reviewed every seven years to help eliminate \"incompetent\" teachers who can otherwise find refuge in the tenure system.\n\nIt has also been suggested that tenure may have the effect of diminishing political and academic freedom among those seeking it – that they must appear to conform to the political or academic views of the field or the institution where they seek tenure. For example, in \"The Trouble with Physics\", the theoretical physicist Lee Smolin says \"...it is practically career suicide for young theoretical physicists not to join the field of string theory...\". It is certainly possible to view the tenure track as a long-term demonstration of the candidate's political and academic conformity. Patrick J. Michaels, a controversial part-time climate science research professor at the University of Virginia, wrote: \"...tenure has had the exact opposite effect as to its stated goal of diversifying free expression. Instead, it stifles free speech in the formative years of a scientist's academic career, and all but requires a track record in support of paradigms that might have outgrown their usefulness.\" However, this point of view would tend to argue not against the abolition of the tenure system, but the shortening of the probationary period, since after receiving tenure, professors no longer feel the pressure to conform to their discipline's mainline views.\n\nOther criticisms include the publish or perish pressures creating trivial or junk research, a caste system treating those without tenure poorly, and indolence after having achieved tenure. The tenured faculty can resist necessary reforms by administrators who they generally outlast. The tenured faculty also usually can control appointments which contributes to political correctness and groupthink.\n\nAs more academics publish research in Internet and multimedia formats, organizations such as the American Council of Learned Societies and Modern Language Association have recommended changes to promotion and tenure criteria, and some university departments have made such changes to reflect the increasing importance of networked scholarship.\n\nAfter the Ward Churchill controversy, a telephone survey of “a thousand Americans aged 18 and older” by the American Association of University Professors found that, while “generally supportive of the tenure system”, \"only about 17.9 percent of respondents say the tenure system should remain as it is\". Another poll found that 65% believed that \"non-tenured professors are more motivated to do a good job in the classroom\".\n\nThe root of some of these criticisms is that elsewhere in the world, there are very few tenure systems, and no time-limited employment. The system in the rest of the English speaking world, for example, is based on promotion up union-negotiated payscales, usually with automatic advancement towards the top of a grade, with the luckier faculty members being on 'permanent contracts' with no end-date. To go to the next grade (e.g. from associate professor to professor), an application must be submitted, and the criteria are as exacting as in North America. Simon Batterbury argues this system offers less opportunity for sabotage, and more adherence to social justice goals, even though 'permanent' staff members can be fired at any time.\n\n\n"}
{"id": "2907609", "url": "https://en.wikipedia.org/wiki?curid=2907609", "title": "Acadèmia Valenciana de la Llengua", "text": "Acadèmia Valenciana de la Llengua\n\nThe Acadèmia Valenciana de la Llengua (, English: Valencian Language Academy), also known by the acronym AVL, is an institution created on September 16, 1998, by the Valencian Parliament, which belongs to the set of official institutions that compose the Generalitat Valenciana, according to the Act of Autonomy of the Valencian Community.\n\nIts primary function is to determine and set the official standards for the Valencian language as used in the Valencian Community (known in other territories as Catalan), and foster its use. According to its foundational law, the AVL linguistic regulations for Valencian must follow current Valencian genuine linguistic reality, respect Valencian lexicographic and literary tradition and start from the consolidated regulations based upon the Normes de Castelló, a set of orthographic rules for Valencian signed in 1932.\n\nThe headquarters of the Acadèmia Valenciana de la Llengua are in Valencia city.\nIn addition this variety of the language follows the pattern of Pompeu Fabra standards who standardized the Catalan language in Catalonia. The IEC (Institute of Catalan Studies) standards are almost the same as the AVL (Valencian Language Academy) standards.\n\n"}
{"id": "41709910", "url": "https://en.wikipedia.org/wiki?curid=41709910", "title": "Armchair general", "text": "Armchair general\n\nThe term \"armchair general\" has two meanings. It is conventionally used as a derogatory term for a person who regards himself as an expert on military matters, despite having little to no actual experience in the military. Alternatively, it can mean a military commander who doesn't participate in actual combat.\n\nThe most common usage of the term refers to \"[a] person without military experience who regards himself as an expert military strategist.\"\n\nThis person may be a civilian whose only exposure to the military or military history is through academic or self-study, or a former member of the military, who was of low rank and/or has no experience with planning or strategic decision-making. In both cases, these individuals claim to be more capable of analyzing combat conditions and making strategic judgments than past military commanders who have been responsible for such analysis and decisions (\"see also\" Armchair theorizing\").\n\nIn 1967, \"New York Times\" correspondent Max Frankel wrote, \"In most wars, the armchairs are full of generals re-fighting every battle.\"\n\nThe term is also used to describe \"a military commander who is not actively involved in warfare, or who directs troops from a position of comfort or safety.\" These officers' duties are described by the media and the rest of the military as more bureaucratic than functional, and who have little to no experience in combat or warfare, yet hold a great degree of authority over soldiers or commanders who do.\n\nThe term is not exclusively applied to officers of command rank (such as generals or Admirals); it is also a popular term among enlisted personnel and the media to describe high-ranking officers whose rank affords them superior privileges, especially when they have attained rank through higher education, or the influence of their families, rather than combat duty.\n\n\n\n"}
{"id": "7676441", "url": "https://en.wikipedia.org/wiki?curid=7676441", "title": "Australian Aboriginal avoidance practices", "text": "Australian Aboriginal avoidance practices\n\nAboriginal avoidance practices refers to those relationships in traditional Aboriginal society where certain people are required to avoid others in their family or clan. These customs are still active in many parts of Australia, to a greater or lesser extent.\n\nAvoidance relationships are a mark of respect. There are also strong protocols around avoiding, or averting, eye contact, as well as around speaking the name of the dead.\n\nIn general, across most language groups, the two most common avoidance relationships are:\n\nIn what is the strongest kinship avoidance rule, some Australian Aboriginal customs ban a person from talking directly to their mother in law or even seeing her. A mother-in-law also eats apart from her son-in-law or daughter-in-law and their spouse. If the two are present at the same ceremony, they will sit with their backs to each other but they can still communicate via the wife/husband, who remains the main conduit for communication in this relationship. Often there are language customs surrounding these relationships.\n\nThis relationship extends to avoiding all women of the same skin group as the mother-in-law, and, for the mother-in-law, men of the same skin group as the son-in-law. The age of marriage is very different for men and women with girls usually marrying at puberty while a man may not marry until his late 20s or even later. As mothers-in-law and sons-in-law are likely to be of approximately the same age the avoidance practice possibly serves to circumvent potential illicit relationships. It has also been suggested that the custom developed to overcome a common cause of friction in families.\n\nThis usually takes place after initiation. Prior to this, brothers and sisters play together freely.\n\nBoth these avoidance relationships have their grounding in the Australian Aboriginal kinship system, and so are ways of avoiding incest in small bands of closely related people.\n\nThere are many other avoidance relationships, including same-sex relationships, but these are the main two.\n\nOnce children are older, they are viewed as potential marital partners and their sexual behavior becomes one of strict avoidance until married. Permanent relationships are prescribed by traditional law and often arranged before birth.\n\nSame-sex relationships are viewed in the same light as incest or \"wrong\" marriages (i.e., to a partner of their own choice or wrong skin group) which carry the same penalties as a domestic crime against the community. However, intimate bodily contact between women regardless of marital status is not considered sexually suggestive but affirmation of friendship and a \"right to touch\". Touch is particularly important when women tell jokes or discuss matters of a sexual nature. In these circumstances behavior such as \"nipple tweaking\" and \"groin grabbing\" are seen as signs of friendship.\n\nTraditionally, this meant avoiding referring to a deceased individual by name directly after their death as a mark of respect—and also because it is considered too painful for the grieving family. Today the practice continues in many communities, but has also come to encompass avoiding the publication or dissemination of photography or film footage of the deceased as well. Many Australian television programs and films include a title card warning Aboriginal and Torres Strait Islanders that \"Aboriginal and Torres Strait Islander viewers are advised that the following program may contain images and voices of people who have died.\" (as recommended by the Australian Broadcasting Network).\n\nThe avoidance period may last anywhere from 12 months to several years. The person can still be referred to in a roundabout way, such as, \"that old lady\", or by their generic skin name, but not by first name. In some Central Australian communities, if for example, an individual named Alice dies, that name must be avoided in all contexts. This can even include the township Alice Springs being referred to in conversation in a roundabout way (which is usually fine, as the Indigenous name can be defaulted to). Those of the same name as the deceased are referred to by a substitute name during the avoidance period such as \"Kuminjay\", used in the Pintubi-Luritja dialect, or \"Galyardu\", which appears in a mid-western Australia Wajarri dictionary for this purpose. \n\nThis presents some challenges to indigenous people. In traditional society, people lived together in small bands of extended family, and name duplication was less common. Today, as people have moved into larger communities (with upwards of 300 to 600 people), the logistics of name avoidance have become increasingly difficult. Exotic and rare names have therefore become very common, particularly in Central Australia and desert communities, to deal with this new challenge.\n\n"}
{"id": "20367790", "url": "https://en.wikipedia.org/wiki?curid=20367790", "title": "Beacon College", "text": "Beacon College\n\nBeacon College, a private nonprofit college founded in 1989 in Leesburg, Florida, is the first accredited institution of higher learning designed with curriculum and support services to serve those students with dyslexia, ADHD, or other specific learning disabilities.\n\nBeacon College offers Associate of Arts and Bachelor of Arts degrees in Human Services, Interdisciplinary Studies, Computer Information Systems, Psychology, and Business Management. It is regionally accredited through the Southern Association of Colleges and Schools and is licensed by the Florida Department of Education.\n\nGeorge J. Hagerty has served as president since March 2013.\n\nLed by learning-disabilities advocates Dr. Marsha Glines, and Patricia and Peter Latham, a group of parents incorporated Beacon College on May 24, 1989. At the time, the Florida Department of Education approved the college's plan to introduce a focused undergraduate curriculum aligned specifically to the specialized learning and support needs of students who learn differently. Beacon awards both Associate of Arts (A.A.) and Bachelor of Arts (B.A.) degrees.\n\nIn 2003, Beacon College earned full accreditation from the Southern Association of Colleges and Schools Commission on Colleges (SACSCOC). It was reaffirmed for the full 10-year period in 2008, and again in 2017.\n\n"}
{"id": "2786210", "url": "https://en.wikipedia.org/wiki?curid=2786210", "title": "Call capture", "text": "Call capture\n\nCall Capture technology is both a phone and text-based technology that captures personal data from persons who inquire for information on something; usually a property for sale or rent. After the call is placed, the system notifies a client of the name and phone number of the person calling. The system was designed to generate leads specifically for the real estate industry.\n\nReal estate agents for example might display their call capture phone number on a sign rider (usually a 24X6 rider on top of a \"For Sale\" yard sign), with a call to action like \"Info 24-7.\" There is usually a property identification number (usually 2-6 digits) along with the local or toll free phone number that can be called (or texted) for information. Information about a property is delivered audibly via a phone call, or provided by text link. \n\nThe assigned real estate agent receives a lead by email or text that a client is listening to a property presentation, or that a response text link has been sent on a particular property. The lead usually includes the caller ID of the inbound phone number, the name of the party associated to the incoming phone number and the property that was just inquired-on. Some call capture systems allow Multiple agents to be called simultaneously with the first responding agent getting the call/lead.\n\nBasic Call Capture service is generally NOT integrated with MLS (Multiple Listing Service) and requires the recording of each property presentation\n\nMLS-integrated Call Capture service operates on all listings in an MLS database and use street numbers (vs property codes) for indexing all properties. These systems require automated voicing to keep audio presentations updated. \n"}
{"id": "26826023", "url": "https://en.wikipedia.org/wiki?curid=26826023", "title": "Card paradox", "text": "Card paradox\n\nThe card paradox is a variant of the liar paradox constructed by Philip Jourdain. It is also known as the postcard paradox, Jourdain paradox or Jourdain's paradox.\n\nSuppose there is a card with statements printed on both sides:\n\nTrying to assign a truth value to either of them leads to a paradox.\n\n\nThe same mechanism applies to the second statement. Neither of the sentences employs (direct) self-reference, instead this is a case of circular reference. Yablo's paradox is a variation of the liar paradox that is intended to not even rely on circular reference.\n"}
{"id": "9089928", "url": "https://en.wikipedia.org/wiki?curid=9089928", "title": "Cardiac shunt", "text": "Cardiac shunt\n\nA cardiac shunt is a pattern of blood flow in the heart that deviates from the normal circuit of the circulatory system. It may be described as right-left, left-right or bidirectional, or as systemic-to-pulmonary or pulmonary-to-systemic. The direction may be controlled by left and/or right heart pressure, a biological or artificial heart valve or both. The presence of a shunt may also affect left and/or right heart pressure either beneficially or detrimentally.\n\nThe left and right sides of the heart are named from a dorsal view, i.e., looking at the heart from the back or from the perspective of the person whose heart it is. There are four chambers in a heart: an atrium (upper) and a ventricle (lower) on both the left and right sides. In mammals and birds, blood from the body goes to the right side of the heart first. Blood enters the upper right atrium, is pumped down to the right ventricle and from there to the lungs via the pulmonary artery. Blood going to the lungs is called the pulmonary circulation. When the blood returns to the heart from the lungs via the pulmonary vein, it goes to the left side of the heart, entering the upper left atrium. Blood is then pumped to the lower left ventricle and from there out of the heart to the body via the aorta. This is called the systemic circulation. A cardiac shunt is when blood follows a pattern that deviates from the systemic circulation, i.e., from the body to the right atrium, down to the right ventricle, to the lungs, from the lungs to the left atrium, down to the left ventricle and then out of the heart back to the systemic circulation.\n\nA left-to-right shunt is when blood from the left side of the heart goes to the right side of the heart. This can occur either through a hole in the ventricular or atrial septum that divides the left and the right heart or through a hole in the walls of the arteries leaving the heart, called great vessels. Left-to-right shunts occur when the systolic blood pressure in the left heart is higher than the right heart, which is the normal condition in birds and mammals.\n\nThe most common congenital heart defects (CHDs) which cause shunting are atrial septal defects (ASD), patent foramen ovale (PFO), ventricular septal defects (VSD), and patent ductus arteriosi (PDA). In isolation, these defects may be asymptomatic, or they may produce symptoms which can range from mild to severe, and which can either have an acute or a delayed onset. However, these shunts are often present in combination with other defects; in these cases, they may still be asymptomatic, mild or severe, acute or delayed, but they may also work to counteract the negative symptoms caused by another defect (as with d-Transposition of the great arteries).\n\nSome acquired shunts are modifications of congenital ones: a balloon septostomy can enlarge a foramen ovale (if performed on a newborn), PFO or ASD; or prostaglandin can be administered to a newborn to prevent the ductus arteriosus from closing. Biological tissues may also be used to construct artificial passages.\n\nMechanical shunts such as the Blalock-Taussig shunt are used in some cases of CHD to control blood flow or blood pressure.\n\nAll reptiles have the capacity for cardiac shunts.\n"}
{"id": "37307060", "url": "https://en.wikipedia.org/wiki?curid=37307060", "title": "Chronic training load", "text": "Chronic training load\n\nChronic training load, often abbreviated as CTL, is the cumulative training dose that builds up over a long period of time. This concept is especially popular in cycling, among athletes who use a cycling power meter, which simplifies the collection and review of training data. The main concept is that an appropriate level of CTL will cause the body of an athlete to go through fitness adaptations. The time span over which it is measured ranges from weeks to months. The concept of CTL is often used in comparison to that of acute training load to plan or examine the results of a training program.\n"}
{"id": "2069591", "url": "https://en.wikipedia.org/wiki?curid=2069591", "title": "Code (semiotics)", "text": "Code (semiotics)\n\nIn semiotics, a code is a set of conventions or sub-codes currently in use to communicate meaning. The most common is one's spoken language, but the term can also be used to refer to any narrative form: consider the color scheme of an image (e.g. red for danger), or the rules of a board game (e.g. the military signifiers in chess).\n\nFerdinand de Saussure (1857–1913) emphasised that signs only acquire meaning and value when they are interpreted in relation to each other. He believed that the relationship between the signifier and the signified was arbitrary. Hence, interpreting signs requires familiarity with the sets of conventions or codes currently in use to communicate meaning.\n\nRoman Jakobson (1896–1982) elaborated the idea that the production and interpretation of texts depends on the existence of codes or conventions for communication. Since the meaning of a sign depends on the code within which it is situated, codes provide a framework within which signs make sense (see Semiosis).\n\nTo that extent, codes represent a broad interpretative framework used by both addressers and their addressees to encode and decode the messages. Self-evidently, the most effective communications will result when both creator and interpreter use exactly the same code. Since signs may have many levels of meaning from the denotational to the connotational, the addresser's strategy is to select and combine the signs in ways that limit the range of possible meanings likely to be generated when the message is interpreted. This will be achieved by including metalingual contextual clues, e.g. the nature of the medium, the modality of the medium, the style, e.g. academic, literary, genre fiction, etc., and references to, or invocations of, other codes, e.g. a reader may initially interpret a set of signifiers as a literal representation, but clues may indicate a transformation into a metaphorical or allegorical interpretation diachronically. Distinctions of class or memberships of groups will be determined by the social identity each individual constructs through the way the language is spoken (i.e. with an accent or as a dialect) or written (i.e. in sentences or in SMS format), the place of residence (see Americanisms), the nature of any employment undertaken, the style of dress, and nonverbal behaviour (e.g. through differentiating customs as to the extent of private space, whether and where people may touch or stare at each other, etc.). The process of socialisation is learning to understand the prevailing codes and then deciding which to apply at any given time, i.e. acknowledging that there is sometimes an ideological quality to the coding system, determining levels of social acceptability, reflecting current attitudes and beliefs. This includes regulatory codes that are intended to control behaviour and the use of some signifying codes. The human body is a means of using presentational codes through facial expressions, gestures, and dress. So words spoken may change their connotation to unacceptable if accompanied by inappropriate nonverbal signs.\nThe other code forms rely upon knowledge held by, and the interests of, the addressees. Specialised denotational codes may provide a more objective and impersonal form of language for mathematical, philosophical, and scientific texts. Hence, for example, the ability to read this text depends upon a more specialised form of vocabulary and different skills to those required to read a genre text detailing the investigations of a detective or the adventures of a secret agent. There are also specialised connotational and ideological codes to reflect particular social, political, moral, and aesthetic values. Musical and iconic codes would be relevant as between a work by Arnold Schoenberg and a piece of bubblegum pop, and a painting by Rembrandt and a comic book by Frank Miller, etc. Each medium has its own specialised codes and, by making them more explicit, semiotics is attempting to explain the practices and conventions have appeared in each form and to understand how meaning is being communicated. In return, this assists addressers to improve their techniques, no matter what their functional needs, e.g. as politicians, journalists, advertisers, creative artists, etc. Indeed, awareness leads to an intentional blending of codes for effect, e.g. an advertiser may produce a more effective campaign with a slogan, images and a jingle using lexical, social gestural, and musical codes.\nIn communication research and media research, the way receivers act towards the message and the way it is encoded becomes relevant, and generates different reactions: \n\n\n"}
{"id": "3262640", "url": "https://en.wikipedia.org/wiki?curid=3262640", "title": "Corporate design", "text": "Corporate design\n\nA corporate design (CD) is the official graphical design of the logo and name of a company or institution used on letterheads, envelopes, forms, folders, brochures, etc. The house style is created in such a way that all the elements are arranged in a distinctive design and pattern.\n\nThis includes dictating what ink pantones should be used in the coloring, and what typefaces.\n\nGovernments may have corporate designs as well. On June 2, 1999, the German federal cabinet introduced a corporate design for the flag of Germany.\n\nThe term 'corporate design' is not the name of a specific design profession.\n\nCorporations do have special design needs based on their behaviors. They communicate their mission, objectives, needs, and product information -- with users, clients, or members; with suppliers, distributors, service providers; with the surrounding community and the media; with financial institutions and other corporations, and with the state. They create, acquire, modify, organize and distribute large amounts of information and raw data, as well as goods and services. (Sometimes the goods or services are themselves information. For example, \"The Yellow Pages\", or \"The New York Times\".)\n\nA designer whose client is a corporation will include the logo and other elements of the corporate brand as a way to standardize and unify all communication between company and audience, whether in print or online. Scenarios that includes human-computer interactions take place through software and hardware user interfaces that are also branded and designed with the corporate culture in mind. (Examples of user scenarios: update the Web site, transfer funds, document procedures, control security, operate machinery, plan projects, conduct virtual meetings, check inventory, fill an order, or ship a product.)\n\nThese interactions are increasingly taking place through Web sites, through mobile devices and at dedicated terminals, and may include sound, video, animation and user feedback mechanisms. A savvy designer will create designs that can be adapted to all of these applications.\n\n"}
{"id": "4980483", "url": "https://en.wikipedia.org/wiki?curid=4980483", "title": "Crossclaim", "text": "Crossclaim\n\nA crossclaim is a claim asserted between codefendants or coplaintiffs in a case and that relates to the subject of the original claim or counterclaim according to \"Black's Law Dictionary\". A cross claim is filed against someone who is a co-defendant or co-plaintiff to the party who originates the crossclaim. In common law, a crossclaim is a demand made in a pleading that is filed against a party which is on the \"same side\" of the lawsuit.\n\nIn the Federal Rules of Civil Procedure this is codified in Rule 13(g). In the federal rules, a crossclaim is proper if it relates to a matter of the original jurisdiction. Proper jurisdiction is determined by a finding of whether the suit that is being initiated arises from the same transaction or occurrence that is the subject matter of the suit.\n\nThe policy for allowing crossclaims is that they promote efficiency and consistency. Furthermore, the same underlying facts will be litigated on the main claim as well as on the crossclaim preserving efficiency in the judicial system by resolving multiple claims that might arise between the parties as opposed to courts trying each claim individually and re-litigating the same facts. Furthermore, this will prevent inconsistent verdicts that might harm the public perception of the judicial process. Finally, because crossclaims are not mandatory, they allow the moving party the opportunity to sue later. The plaintiff is the master of his complaint, and this also holds true in crossclaims.\n"}
{"id": "147116", "url": "https://en.wikipedia.org/wiki?curid=147116", "title": "Cult of personality", "text": "Cult of personality\n\nA cult of personality arises when a country's regime – or, more rarely, an individual – uses the techniques of mass media, propaganda, the big lie, spectacle, the arts, patriotism, and government-organized demonstrations and rallies to create an idealized, heroic, and worshipful image of a leader, often through unquestioning flattery and praise. A cult of personality is similar to apotheosis, except that it is established by modern social engineering techniques, usually by the state or the party in one-party states. It is often seen in totalitarian or authoritarian countries. \n\nThe term came to prominence in 1956, in Nikita Khrushchev's secret speech \"On the Cult of Personality and Its Consequences\", given on the final day of the 20th Congress of the Communist Party of the Soviet Union. In the speech, Khrushchev, who was the First Secretary of the Communist Party – in effect, the leader of the country – criticized the lionization and idealization of Joseph Stalin, and, by implication, his Communist contemporary Mao Zedong, as being contrary to Marxist doctrine. The speech was later made public and was part of the \"de-Stalinization\" process the Soviet Union went through.\n\nThe Imperial cult of ancient Rome identified emperors and some members of their families with the divinely sanctioned authority (auctoritas) of the Roman State. Throughout history, monarchs and other heads of state were often held in enormous reverence and imputed super-human qualities. Through the principle of the divine right of kings, in medieval Europe for example, rulers were said to hold office by the will of God. Ancient Egypt, Imperial Japan, the Inca, the Aztecs, Tibet, Siam (now Thailand), and the Roman Empire are especially noted for redefining monarchs as \"god-kings\".\n\nThe spread of democratic and secular ideas in Europe and North America in the 18th and 19th centuries made it increasingly difficult for monarchs to preserve this aura. However, the subsequent development of mass media, such as radio, enabled political leaders to project a positive image of themselves onto the masses as never before. It was from these circumstances in the 20th century that the most notorious personality cults arose. Often these cults are a form of political religion.\n\nThe term \"cult of personality\" probably appeared in English around 1800–1850, along with the French and German use. At first it had no political connotations but was instead closely related to the Romantic \"cult of genius\". The political use of the phrase came first in a letter from Karl Marx to German political worker, Wilhelm Blos, 10 November 1877:\n\nThere are various views about what constitutes a cult of personality in a leader. Historian Jan Plamper has written that modern-day personality cults display five characteristics that set them apart from \"their predecessors\": The cults are secular and \"anchored in popular sovereignty\"; their objects are all males; they target the entire population, not only the well-to-do or just the ruling class; they use mass media; and they exist where the mass media can be controlled enough to inhibit the introduction of \"rival cults\".\n\nIn his \"What is character and why it really does matter\", Thomas A. Wright states, \"The cult of personality phenomenon refers to the idealized, even god-like, public image of an individual consciously shaped and molded through constant propaganda and media exposure. As a result, one is able to manipulate others based entirely on the influence of public personality...the cult of personality perspective focuses on the often shallow, external images that many public figures cultivate to create an idealized and heroic image.\"\n\nAdrian Teodor Popan defines cult of personality as a \"quantitatively exaggerated and qualitatively extravagant public demonstration of praise of the leader\". He also identifies three causal \"necessary, but not sufficient, structural conditions, and a path dependent chain of events which, together, lead to the cult formation: a particular combination of patrimonialism and clientelism, lack of dissidence, and systematic falsification pervading the society’s culture.\"\n\nThe media has played an instrumental role in forging national leaders' cults of personality.\n\nThomas A. Wright reports that \"It is becoming evident that the charismatic leader, especially in politics, has increasingly become the product of media and self-exposure.\" And, focusing on the media in the United States, Robert N. Bellah adds that, \"It is hard to determine the extent to which the media reflect the cult of personality in American politics and to what extent they have created it. Surely they did not create it all alone, but just as surely they have contributed to it. In any case, American politics is dominated by the personalities of political leaders to an extent rare in the modern world...in the personalised politics of recent years the \"charisma\" of the leader may be almost entirely a product of media exposure.\"\n\nEnjoying repeated electoral success; particularly his third election victory in 2008, Silvio Berlusconi, an Italian media tycoon and politician who served as Prime Minister of Italy in four governments, was the most controversial head of government in the EU. He is the controlling shareholder of Mediaset and owned the Italian football club A.C. Milan from 1986 to 2017. Forbes magazine ranked him 12th in the List of The World's Most Powerful People due to his domination of Italian politics in 2009.\n\nOften, a single leader became associated with this revolutionary transformation and came to be treated as a benevolent \"guide\" for the nation without whom the transformation to a better future could not occur. This has been generally the justification for personality cults that arose in totalitarian societies, such as those of Adolf Hitler, Joseph Stalin, and Mao Zedong. Jan Plamper argues while Napoleon III made some innovations it was Benito Mussolini in Italy in the 1920s who originated the model of dictator-as-cult-figure that was emulated by Hitler, Stalin and the others, using the propaganda powers of a totalitarian state.\n\nPierre du Bois argues that the Stalin cult was elaborately constructed to legitimize his rule. Many deliberate distortions and falsehoods were used. The Kremlin refused access to archival records that might reveal the truth, and key documents were destroyed. Photographs were altered and documents were invented. People who knew Stalin were forced to provide \"official\" accounts to meet the ideological demands of the cult, especially as Stalin himself presented it in 1938 in \"Short Course on the History of the All-Union Communist Party (Bolsheviks) \", which became the official history.\n\nHistorian David L. Hoffmann states \"The Stalin cult was a central element of Stalinism, and as such it was one of the most salient features of Soviet rule...Many scholars of Stalinism cite the cult as integral to Stalin's power or as evidence of Stalin's megalomania.\"\n\nThe American band Living Colour won a Grammy Award for Best Hard Rock Performance in 1990 for their song \"Cult of Personality\". The song includes references to Mahatma Gandhi, John F. Kennedy, Benito Mussolini, Franklin D. Roosevelt, Joseph Stalin, and Malcolm X.\n\n\n\nNotes\nFurther reading\n\n"}
{"id": "40763026", "url": "https://en.wikipedia.org/wiki?curid=40763026", "title": "Curvilinear principle", "text": "Curvilinear principle\n\nIn sociolinguistics, the curvilinear principle states that there is a tendency for linguistic change from below to originate from members of the central classes in a speech community's socioeconomic hierarchy, rather than from the outermost or exterior classes. \n\nDefined by William Labov, the curvilinear principle departs from traditional nineteenth century notions that language change generally originates in the highest or lowest classes of society. Instead, it states that variant forms leading to language change are typically introduced and motivated by the intermediate groups—the upper-working class and lower-middle class.\n\nThe principle can be seen as one response to an important question in sociolinguistics known as the \"embedding problem\", a problem \"concerned with determining regular patterns in both the linguistic and the extra-linguistic context of change.\" In other words, the embedding problem seeks to identify other changes or factors that have a non-coincidental relationship with the actual linguistic change. The curvilinear principle identifies such a non-trivial factor by proposing that a speaker’s class can indicate the degree to which he or she motivates linguistic change.\n\nThe principle's name refers to the curvilinear correlation that results from plotting the variation of a linguistic variable with respect to the class of the speakers. Because the lowest and highest classes generally tend to use newly emerging forms less frequently than central classes, data points representing variable usage resemble a concave curve when connected on a graph.\n\nIn The Philadelphia study, William Labov examined a series of linguistic variables in various stages of speech integration in order to evaluate whether the interior classes were, in fact, the innovators of linguistic change. In order to determine each speaker’s social position within the community, Labov created a socioeconomic status index based on education and occupation, each ranked on levels from 0 to 6, where 6 was the highest level of education or occupation. He studied a series of “new and vigorous” vowel changes, including the fronting and raising of (aw) and (ey) and the centralization of (ay). The research found that members of the upper working class and lower middle class used these variables more frequently than members of either the lower or upper class. This corroborated his curvilinear hypothesis because the middle classes were leading the use of these \"new and vigorous\" linguistic changes.\n\nIn his study of Norwich, England, Peter Trudgill examined different cases of linguistic variation and whether or not class could be related to realizations of certain linguistic variables. One of the observed variables was the (RP) quality of vowels in words like \"top\", \"hot\", and \"dog\". To determine a subject’s class, Trudgill calculated a score for each subject based on six parameters: subject’s occupation, father’s occupation, income, education, locality, and housing. Trudgill found that middle-class women were introducing the RP vowels in Norwich; working-class men were also introducing variation by borrowing a similar vowel associated with working-class speech from a nearby area. The distribution of linguistic variation in Trudgill’s study thus abides by the curvilinear principle because members of the central classes led the change.\n\nIn 1966, Labov published a study on linguistic variation on the Lower East Side neighborhood of New York City. In the study, he approximated each subject’s class with score that factored in his or her occupation, income, and education. Using these scores, Labov then grouped the subjects into 5 categories defined by sociologist Joseph Kahl: lower class, working class, lower middle class, upper middle class, and upper class (though Labov noted no subjects were considered upper class and that the presence of any upper-class people living on the Lower East Side would be unexpected).\n\nOne variable considered was (oh), the mid-back rounded vowel present in \"caught\", \"talk\", and \"dog\". Labov found that members of the working class and lower middle class—the central socioeconomic classes of the Lower East Side—used higher vowels for (oh) than did either the lower class or the middle class subjects. Labov recognized this as a linguistic change in progress, driven by the central class in accordance with the curvilinear principle.\n"}
{"id": "19367455", "url": "https://en.wikipedia.org/wiki?curid=19367455", "title": "Cut to shape", "text": "Cut to shape\n\nCut to shape is a philatelic term referring to a postage stamp or postal stationery indicium (printed stamp image) that has been cut to the shape of the design, such as an octagon, circle or oval, instead of simply cut into a square or rectangular shape. \n\nStamps cut to shape almost always command a lower price than those cut square, and sometimes have little or no value, especially envelope indicia cut to shape. Although many stamps unfortunately have been cut to shape by stamp collectors, some early stamps were produced without perforations and were often cut to shape by people before they affixed the stamps to their envelopes. This is true, for example, for the octagon-shaped 4 Annas stamp of India issued in 1854, which is most commonly found cut to shape on envelopes or pieces. \n\nAll of the surviving examples of the India 1854 (inverted head) are postally used. Only two (or three) are known cut square; another 24 or so are cut to shape (that is, in an octagonal shape). One from the collection of the Earl of Crawford was exhibited in the World Philatelic Exhibition in Washington in 2006.\n\nThe \"world's most famous stamp\" — the unique 1856 British Guiana 1c magenta — is cut into an octagonal shape. Consequently, it has been referred to as being cut to shape, although technically that term is incorrect as the stamp design is rectangular in shape.\n\n"}
{"id": "9526445", "url": "https://en.wikipedia.org/wiki?curid=9526445", "title": "Display window", "text": "Display window\n\nA display window, also shop window (British English) or store window (American English), is a window in a shop displaying items for sale or otherwise designed to attract customers to the store. Usually, the term refers to larger windows in the front façade of the shop.\n\nThe first display windows in shops were installed in the late 18th century in London, where levels of conspicuous consumption were growing rapidly. Retailer Francis Place was one of the first to experiment with this new retailing method at his tailoring establishment in Charing Cross, where he fitted the shop-front with large plate glass windows. Although this was condemned by many, he defended his practice in his memoirs, claiming that he \"sold from the window more goods...than paid journeymen's wages and the expenses of housekeeping.\n\nDisplay windows at boutiques usually have dressed-up mannequins in them.\n\nDisplaying merchandise in a store window is known as \"window dressing\", which is also used to describe the items displayed themselves. \n\nAs a figure of speech, \"window dressing\" means something done to make a better impression, and sometimes implies something dishonest or deceptive.\n\n"}
{"id": "248295", "url": "https://en.wikipedia.org/wiki?curid=248295", "title": "Division (sport)", "text": "Division (sport)\n\nIn sports, a division is a group of teams who compete against each other for a championship.\n\nIn sports using a league system (also known as a pyramid structure), a division consists of a group of teams who play a sport at a similar competitive level. Teams can move up to a higher division of play or drop down to a lower one via the process of promotion and relegation, based on their performance in the standings at the end of the season. The existence of divisions based on level of competition ensures that teams at one competitive level can play other teams at a similar competitive level, thus creating parity and more exciting matches.\n\nIn North America, where sports usually operate on a franchise system rather than a league system, a division is a group of teams within a league which is organized along geographical lines rather than competitive success. Teams based in cities that are in a particular region of the continent are grouped together in the same division. For instance, in Major League Baseball, both the American and National Leagues have East, Central, and West divisions; the teams in each division are mostly (but not always) located in the eastern, central, and western sections of North America respectively. In a franchise system, teams are not promoted or relegated as are teams in a league system. All teams in the league (and by extension, the divisions of the league) are at the same competitive level and remain so year after year.\n\nNorth American professional sports leagues often construct their season schedules in a way such that teams in a division play matches against each other more often than other teams in the league. This not only has the effect of reducing travel costs, but also creates exciting rivalries between the teams in the division. Moreover, the top teams in a division qualify for the postseason playoff tournament that crowns the league champion, which heightens the rivalries between the teams in a division.\n\nGeographically-based divisions can become skewed if an expansion team joins the league or if one of the franchises within a division moves to another city, necessitating a shuffling or realignment of the teams in a division. Furthermore, the results of the realignment may not always reflect geographical realities. For instance, in 1995, the Los Angeles Rams of the National Football League (NFL) moved to St Louis, Missouri and became the St Louis Rams. The team retained its place in the NFC West division despite the fact that St. Louis is further east than Dallas, Texas, home of the Dallas Cowboys. Although Dallas is located in the south-central United States, the Cowboys are a member of the NFC East division due to their long-standing rivalries with the New York Giants, Philadelphia Eagles, and Washington Redskins, all of whom are located on the Eastern seaboard.\n\nIn U.S. college sports, a \"division\" has a meaning different from either sense listed above, although somewhat closer to that of the league system.\n\nThe major governing bodies for college sports, the NCAA and NAIA, divide their member schools into large competitive groups. These groups are much larger than divisions in either the league or franchise system—for example, the NCAA's highest competitive level, Division I, has more than 300 member schools. The vast majority of teams are members of conferences, smaller groupings that usually have between 6 and 14 members. Conference champions, plus selected other teams, compete in national championship tournaments (with the exception of schools in the highest level of NCAA (American) football, which have never had an NCAA-recognized national championship).\n\nAs an example, the NCAA is split into three divisions:\n\n\nThe term \"division\" is also used in US college sports to indicate the groupings of members of a given conference. However, this usage is more recent. The first conference to divide its teams into divisions was the Southeastern Conference which, upon expanding to 12 members in 1992, divided into Eastern and Western divisions. Other conferences have undergone similar expansion and division. The usage in the section above is still maintained. For example, the Georgia Bulldogs are in Division I, but are also in the Eastern Division of the Southeastern Conference.\n\n"}
{"id": "37549630", "url": "https://en.wikipedia.org/wiki?curid=37549630", "title": "Dual-route hypothesis to reading aloud", "text": "Dual-route hypothesis to reading aloud\n\nThe dual-route theory of reading aloud was first described in the early 1970s. This theory suggests that two separate mental mechanisms, or cognitive routes, are involved in reading aloud, with output of both mechanisms contributing to the pronunciation of a written stimulus.\n\nOne mechanism, termed the lexical route, is the process whereby skilled readers can recognize known words by sight alone, through a \"dictionary\" lookup procedure. According to this model, every word a reader has learned is represented in a mental database of words and their pronunciations that resembles a dictionary, or internal lexicon. When a skilled reader sees and visually recognizes a written word, he or she is then able to access the dictionary entry for the word and retrieve the information about its pronunciation. The internal lexicon encompasses every learned word, even exception words like 'colonel' or 'pint' that don't follow letter-to-sound rules. This route doesn't enable reading of nonwords (example 'zuce'). There is still no conclusive evidence whether the lexical route functions as a direct pathway going from visual word recognition straight to speech production, or a less direct pathway going from visual word recognition to semantic processing and finally to speech production.\n\nThe nonlexical or sublexical route, on the other hand, is the process whereby the reader can \"sound out\" a written word. This is done by identifying the word's constituent parts (letters, phonemes, graphemes) and, applying knowledge of how these parts are associated with each other, for example how a string of neighboring letters sound together. This mechanism can be thought of as a letter-sound rule system that allows the reader to actively build a phonological representation and read the word aloud. The nonlexical route allows the correct reading of nonwords as well as regular words that follow spelling-sound rules, but not exception words.\nThe dual-route hypothesis of reading has helped researchers explain and understand various facts about normal and abnormal reading.\n\nAccording to research, the amount of time required to master reading depends on the language's adherence to phonological rules. A written language is described as transparent when it strongly adheres to spelling-sound rules and contains few exception words. Because of this, the English language (low transparency) is considered less transparent that French (medium transparency) and Spanish (high transparency) which contain more consistent grapheme-phoneme mappings. This difference explains why it takes more time for children to learn to read English, due to its frequent irregular orthography, compared to French and Spanish. The Spanish language's reliance on phonological rules can account for the fact that Spanish-speaking children exhibit a higher level of performance in nonword reading, compared to English and French-speaking children. Similarly, Spanish surface dyslexics exhibit less impairment in reading overall, because they can rely upon consistent pronunciation rules instead of processing many exception words as a whole in the internal lexicon. The dual-route system thus provides an explanation for the differences in reading acquisition rates as well as dyslexia rates between different languages.\n\nSkilled readers demonstrate longer reaction times when reading aloud irregular words that do not follow spelling-sound rules compared to regular words. When an irregular word is presented, both the lexical and nonlexical pathways are activated but they generate conflicting information that takes time to be resolved. The decision-making process that appears to take place indicates that the two routes are not entirely independent from one another. This data further explains why regular words, that follow spelling-sound rules but also have been stored in long-term memory, are read faster since both pathways can \"agree\" about the issue of pronunciation.\n\nAccording to the current model of dual-route processing, each of the two pathways consumes different amounts of limited attentional resources. The nonlexical pathway is thought to be more active and constructive as it assembles and selects the correct subword units from various potential combinations. For example, when reading the word \"leaf\", that adheres to spelling to sound rules, the reader must assemble and recognize the two-letter grapheme \"ea\" in order to produce the sound \"ee\" that corresponds to it. It engages in controlled processing and thus requires more attentional capacities, which can vary in amount depending on the complexity of the words being assembled. On the other hand, the processing that takes place in the lexical pathway appears to be more automatic, since the word-sound units in it are pre-assembled. Lexical processing is thus considered to be more passive, consuming less attentional resources.\n\nThe dual-route hypothesis to reading can help explain patterns of data connected to certain types of disordered reading, both developmental and acquired.\n\nChildren with reading disorders rely primarily on the sub-lexical route while reading. Research shows that children can decode non-words, letter by letter, accurately but with slow speed. However, in decision tasks, they have trouble differentiating between words and pseudohomophones (non words that sound like real words but are incorrectly spelled), thereby showing that they had impaired internal lexicons.\nBecause children with reading disorders (RD) have both slow reading speeds and impaired lexical routes, there are suggestions that the same processes are involved in lexical route and fast naming of words. Other studies have also confirmed this idea that rapid naming of words is more strongly correlated with orthographical knowledge (lexical route) than with phonological representations (sub-lexical route). Similar results were observed for patients with ADHD.\nResearch concludes that reading disorders and ADHD have common properties: lexical route processing, rapid reading and sublexical route processing deficits as well.\n\nSurface dyslexia was first described by Marshall and Newcombe, and is characterized by the inability to read words that do not follow traditional pronunciation rules. English is also an example of a language that has numerous exceptions to the rules of pronunciation and thus presents a particular challenge to those with surface dyslexia. Patients with surface dyslexia may fail to read such words as yacht, or island for example, because they do not follow prescribed rules of pronunciation. The words will typically be sounded out using “regularizations”, such as pronouncing colonel as Kollonel. Words like state, and abdominal, are examples of words that surface dyslexia sufferers will not have a problem pronouncing, as they do follow proscribed pronunciation rules. Surface dyslexics will read some irregular words correctly if they are high frequency words such as “have” and \"some”. It has been postulated that surface dyslexics are able to read regular words by retrieving the pronunciation through semantic means. Or in other words, retrieving the words by meaning. \n\nSurface dyslexia is also semantically mediated. Meaning that there is a relationship between the word and its meaning and not just the mechanisms in how it is pronounced. People who suffer with surface dyslexia also have the ability to read words and non-words alike. This means the physical production of phonological sounds is not affected by surface dyslexia. \n\nThe mechanism behind surface dyslexia is thought to be involved with the phonologic output of the lexicon and is also often attributed to the disruption of semantics. It is also hypothesized that three deficits cause surface dyslexia. The first deficit is at the visual level in recognizing and processing the irregular word. The second deficit may be located at the level of the output lexicon. This is because patients are able to recognize the semantic meaning of irregular words even if they pronounce them incorrectly in spoken word. This suggests the visual word form system and semantics are relatively preserved. The third deficit is likely related to semantic loss.\n\nWhile Surface dyslexia can be observed in patients with lesions in their temporal lobe, it is primarily associated with patients who have dementias. Such as Alzheimer’s or fronto-temporal dementia. Surface dyslexia is also a characteristic of semantic dementia, in which subjects lose knowledge of the world around them.\n\nTreatment of surface dyslexics involves neuropsychological rehabilitation. The aim of the treatment is to improve the operation of the sub lexical reading route, or the patient’s ability to sound out new words. As well as the operation of the visual word recognition system, to increase the recognition of words. On a more micro level, treatment can also focus on the ability to sound out individual letters before the patient goes on to increasing the ability to sound out entire words.\n\nAcquired phonological dyslexia is a type of dyslexia that results in an inability to read nonwords aloud and to identify the sounds of single letters. However, patients with this disability can holistically read and correctly pronounce words, regardless of length, meaning, or how common they are, as long as they are stored in memory. This type of dyslexia is thought to be caused by damage in the nonlexical route, while the lexical route, that allows reading familiar words, remains intact.\n\nA computational model of a cognitive task is essentially a computer program that aims to mimic human cognitive processing This type of model helps bring out the precise parts of a theory and disregards the ambiguous sections, as only the clearly understood parts of the theory can be converted into a computer program. The ultimate goal of a computational model is to resemble human behavior as closely as possible, such that factors affecting the functioning of the program would similarly affect human behavior and vice versa.\nReading is an area that has been extensively studied via the computational model system. The dual-route cascaded model (DRC) was developed to understand the dual-route to reading in humans.\nSome commonalities between human reading and the DRC model are:\n\n\nThe DRC model has been useful as it was also made to mimic dyslexia. Surface dyslexia was imitated by damaging the orthographic lexicon so that the program made more errors on irregular words than on regular or non-words, just as is observed in surface dyslexia. Phonological dyslexia was similarly modeled by selectively damaging the non-lexical route thereby causing the program to mispronounce non words. As with any model, the DRC model has some limitations and a newer version is currently being developed.\n"}
{"id": "2509036", "url": "https://en.wikipedia.org/wiki?curid=2509036", "title": "Extra-short", "text": "Extra-short\n\nThe International Phonetic Alphabet uses a breve, , to indicate a speech sound (usually a vowel) with less than normal or extra short duration. That is, is a very short vowel with the quality of .\n\nAn example from English is the short schwa of the word \"police\" . This is typical of vowel reduction.\n\nBefore 1989, the breve was used for a non-syllabic vowel (that is, part of a diphthong), which is now indicated by an breve placed under the vowel letter, as in \"eye\" . It is also sometimes used for flap consonants which do not have dedicated symbols in the IPA, since a flap is in effect a very brief stop.\n"}
{"id": "18085730", "url": "https://en.wikipedia.org/wiki?curid=18085730", "title": "Flow show", "text": "Flow show\n\nA flow show is a device on a drilling rig that is attached to the flow line and has a paddle that swings out as the flow of drilling fluid passes by it. The angle of the paddle increases as the volume of drilling fluid increases and pushes it out further and vice versa.\n\nThe purpose of the flow show is to allow the driller (and other rig personnel) to monitor the flow of drilling fluid as it comes out of the hole. The reason for monitoring the flow:\n\n\nSee Drilling rig (petroleum) for a diagram of a drilling rig.\n"}
{"id": "49072410", "url": "https://en.wikipedia.org/wiki?curid=49072410", "title": "German Luftwaffe and Kriegsmarine Radar Equipment of World War II", "text": "German Luftwaffe and Kriegsmarine Radar Equipment of World War II\n\nGerman Luftwaffe and Navy Kriegsmarine Radar Equipment during World War II, relied on an increasingly diverse array of communications, IFF and RDF equipment for its function. Most of this equipment received the generic prefix FuG (), meaning \"radio equipment\". During the war, Germany renumbered their radars. From using the year of introduction as their number they moved to a different numbering scheme.\n\nNo German ground radar was accurate enough for Flak fire direction. The operation method for Flak during the day was to use the radar to be used to set the optical fire control for the flak onto the target. Once acquired the flak was controlled by the optical equipment to complete the engagement. During the night the Radar would be used to indicate the target to the search light crews. The rest of the engagement would be carried out optically. During the day fighters would be directed with sufficient precision that they would be able to see their targets and during The night they would use their onboard AI radar to find the target after initial direction from the ground-based radars.\n\nThe Würzburg was first operational in the summer of 1940, had a parabolic shaped antenna with a diameter of about 3metres and in some models could be folded in half for transport. The Würzburg was produced in the thousands with various estimated figures being between 3000 and 4000 with up to 1500 sets of Würzburg Riese. The antenna of the Würzburg weighed over 9.5 tons and its parabolic surface had a diameter equal to 7.5 metres and a focal length of one metres and 70 cm. Only one German company had the technical skill to build these radars, and that was \nZeppelin. The name of the unit was chosen at random by pointing at a map of Germany and Würzburg was chosen.\n\nFuMG 62 / FuMG 39 Würzburg: 3D fire-control radar. Used to direct the flack optical directors and searchlights. Wavelength 50 cm approx. In response to jamming various models of Würzburg radar were developed to operate on various frequencies called \"Islands\".\n\nWürzburg A First production version introduced in 1940. 50 cm operating wavelength. Operation range was approximately 30 km. Included an IFF system that worked with the FuG 25z airborne unit.\n\nWürzburg B Integrated IR telescope to increase accuracy. Proved unsatisfactory and not placed into production.\n\nWürzburg C Replaced the model A in production in 1941. Had lobe switching to improve accuracy. On this unit the integral IFF system was replaced by a system based on the FuG 25a airborne. To support this system which worked at approx 125-160 MHz two antenna were placed inside the main dish. A separate interrogation and receiving units were attached to show the IFF responses.\n\nWürzburg D Replaced the model C in production in 1942. It now had a usable range of approximately 40 km. Conical scan was used for fine accuracy. The IFF antenna was now fitted in the center of the dish rather than on the sides. Better instruments were fitted and generally it was the best of the small Würzburg.\n\nFuMG 65 Würzburg Riese(Giant): The electronics of the D model Würzburg combined with a 7-meter dish to improve resolution and range. Range approx 70 km. Version E was a modified unit to fit on railroad flatcars to produce a mobile Flak radar system. Version G had the 2.4-meter antenna and electronics from a Freya installed. The antenna dipoles were inside the reflector. The reason for this was that the allies were flying very high recon flights which were above the maximum height of the Freya. The standard Würzburg Riese's 50 cm beam was too narrow to find them directly. By combining the two systems the Freya could set the Würzburg Riese onto the target.\n\nFuMG 63 Mainz The Mainz, introduced in 1941, was a development from the Wurzburg with its 3-meter solid metal reflector mounted on top of the same type of control car as used by the ‘Kurmark’. Its range was 25–35 km with an accuracy of ±10–20 meters, azimuth 0.1 degrees, and elevation ±0.3-0.5 degrees. Only 51 units were produced before being superseded by the ‘Mannheim’.\n\nFuMG 64 Mannheim The Mannheim was an advanced development from the ‘Mainz’. It also had a 3-meter reflector, which was now made from a lattice framework covered in a fine mesh. This was fixed to the front of a control cabin and the whole apparatus was rotated electrically. Its range was 25–35 km, with an accuracy of ±10–15 meters; azimuth and elevation accuracy of ±0.15 degrees. Though accurate enough to control Flak guns it was not deployed in large numbers. This was due to its cost (time and materials to manufacture was about 3 time that of a Würzburg D.\n\nFuMG 75 Mannheim Riese Just as the Wurzburg’s performance was greatly improved when fitted with a 7-meter reflector, so was the Mannheim’s, and the result called a Mannheim Riese (Giant Mannheim). There was an optical device for the initial visual acquisition of the target. With its narrow beam it was relatively immune from ‘Window’. Its accuracy and automatic tracking enabled it to be used in anti-aircraft missile research to track and control the missiles in flight. Only a handful were manufactured.\n\nFuMG 68 Ansbach There was a need for a mobile radar with the range and accuracy of the ‘Mannheim’. The result, in 1944, was the Ansbach. It had a collapsible reflector of diameter 4.5 meters, operating on a wavelength of 53.6 cm, and peak power of 8 kW, giving it a normal range 25–35 km (70 km in search mode) with an accuracy of 30–40 meters. Azimuth and elevation accuracy was around ±0.2°. The antenna and reflector were remote controlled from a Bayern control van up to 30 meters away. The control system was based on the remote control system of the Michael microwave communication system, this was based on the Ward-Leonard AC/DC control system. The Ansbach was to be installed in large Flak batteries with six or more guns, but only a few were produced by the end of the war, and these didn’t see operational service\n\nFuMG 450 Freya / FuMG 41G: This was a 2D Early warning radar. (2D means unable to indicate height). It was used for fighter direction and target indication for the Würzburg. Operating wavelength of approx 2.4 meters (125 MHz). In response to jamming various models were developed to operate on various frequencies called \"Islands\". Over 1000 units delivered in various models\n\nFuMG 401 / FMG 42 FREYA - LZ (Models A - D). An Air portable version, the model differences were due to an operating frequency range being in 4 descrete bands between 91 and 200 MHz.\n\nFreya-Rotschwarz and Freya-Grünschwarz: These two systems were Freya modified to operate on the same frequency as the British radio navigation system GEE to avoid jamming. However, as by the time they were ready the Germans were jamming GEE it is not clear whether any were ever deployed.\n\nFuMG 451 A Freya Flamme: Freya which had been built to use the \"Island D\" band were modified to be able to trigger the British IFF equipment. Ranges of up to 450 km were obtained. Fell from use as British IFF procedures improved.\n\nFuMG 401 Freya Fahrstuhl: A 3D version of the Fraya. (3D means could measure height). Measurements made by moving the antenna up and down on a rack. Only a very rough estimation of height available. originally intended for early warning most of the systems produced went to help \"jammed\" Würzburg\n\nFreya EGON: EGON stood for Erstling Gemse Offensive Navigation system. Where Erstling was the codename of the Fug25a transceiver in the aircraft and Gemse was the codename for the receiver. The system operated on a principle similar to the British OBOE navigation system. An IFF signal was sent from a Freya, that had had its receiver antenna removed, to the aircraft. The Fug25a in the aircraft responded and the received signal was displayed as a range offset on the Freya display. Using a second transmitter and triangulation the position of the aircraft was resolved. Though the system was tested to guide night fighters it was found to be to limited by the number of aircraft that it could control at one time (the same limitation was found with Oboe). The \"Y system\" was used instead for night fighter control. The EGON system was used to control pathfinders for bombing raids over both England and Russia, however by now the Luftwaffe bomber force was running out of planes, pilots and fuel so the results were minimal. Work was done using a third transmitter to improve system performance. Range with a normal Freya was up to 250 km, work was underway to use a Wasserman system instead of a Freya to increase range too 350 km. (the Freya signal was to weak to trigger the Fug25a at ranges beyond 250 km), but this was not completed.\n\nFor area air defense (vs point defense) Freya's range was found to be insufficient. This led to attempts to use Freya technology to achieve greater range. This resulted in the Wassermann and Mammut. All though the Mammut units achieved their aims they were large installations, with large arrays built on bunkers. This resulted in long building times and vulnerability to air attacks. The Wassermann was a better solution in that being smaller they were harder to locate and quicker to build, 3–4 weeks. However sources indicate that they never achieved the desired range of 400 km, the best was approx 300 km. This may be why there were so many variants deployed.\n\nFuMG 401 Mammut: First deployed in 1942 this was a long range 2 D search radar. It consisted of 8 Freya class antenna arranged in a 4 x 2 configuration. It measured 25 meters wide and ten meters high and was mounted on four pylons fixed in concrete. Some installations had a second array mounted back-to-back. Each array could be electronically swung through about 100 degrees, so the dual sided array could look behind itself to continue to track bombers as they flew into Germany. Frequency was the same as Freya (125 MHz). Range was up to 300 km with a transmit power of 200 KW. Installations being very large took up to 4 months to build.\n\nFuMG 402 Wassermann: This system was deployed in 1942. It was basically 6 Freya antenna mounted on a rotation cylinder. Frequencies were similar to Freya (125 MHz) transmit power was 100 KW resulting in a usable range of approx 200 km. Three main versions were produced with sub variants in each class.\n\nWassermann L: The original light version. Some sources indicate that it had structural problems.\n\nWassermann S: The heavy version. first deployed late 1942 Some sources indicate it had more than 6 arrays.\n\nWassermann M: The last family were the medium class units. Again it is not clear exactly how many Freya arrays were attached to the mast. In 1944 this version received a modification that allowed it to electronically tilt its beams by 16 degrees which allowed it to perform height determination turning its into a 3 D search radar.\n\nElefant & See Elefant: These bi-static radars were an attempt to combine jamming resistance with long range. They operated in two bands 23–28 MHz or 32–38 MHz. Range was approximately 400 km but under certain RF conditions much greater ranges were obtained. Antenna were usually mounted on Wassermann towers (all units differed in detail from each other). 3 Elefants were in operation at the end of the war with one See Elefant. Sources are unclear what the difference between the two types were.\n\nThe first type of early-warning radar set giving panoramic display which come into operation in usually referred to as the Jagdschloss, although it’s official designation is Jagdschloss F, to distinguish it from later types, such as the Michael B and Z.\n\nJagdschloss F: The antenna was 24 m wide and 3 m high, consisting of sixteen pairs of double horizontal transmit and receive dipoles. Above this, an 8.5 metre wide antenna array of eight vertical dipoles was mounted for the IFF.The first 62 Jagdschloss were of the Voll Wismar type using wide band antenna covering the band 1.90-2.20 metres. Another 18, used the band 1.20–1.90 meters. Range was 100 km. An optional feature known as Landbriefträger (Postman) was a remote PPI display for use with Jagdschloss. This allowed the PPI display from the radar station to be sent simultaneously to command HQ by HF cable, or by a UHF radio link.\n\nJagdschloss Michael B: A ponderous aerial array of two rows of eighteen Würzburg mirrors measuring 56 metres long x 7 metres high was used in the Würzmann experimental early-warning radar, and formed the serial array for Jagdschloss Michael B with the array in a horizontal position. The wavelength employed, was\nthat of a Voll Wismar 53.0-63.8 cm. Range approx 250 km. None may have entered service, though one source mentions one entering service.\n\nForsthaus F: This system was a development of the Jagdschloss Michael B using the so-called Euklid 25–29 cm. waveband employed by the Navy. Once more a\nvery long aerial array 48 metres long and about 8 metres high was used, employing a cylindrical paraboloid. A wave guide antenna (Hohlraumstrahler) was placed along the focal line with a second and a third wave guide parallel to it above and below respectively.Range was expected to be over 200 km. Probably none completed.\n\nForsthaus KF: Development of the Forsthaus F. Reduced in size so that the system would fit in a railway carriage. Antenna 24 meters long. Range 120 km.\n\nDreh Freya: This set, which was also known as Freya Panorama, was first introduced in June 1944. It consisted of a Freya aerial of the Breitband type working in Bereich I (1.90-2.50), the frequency of which could be adjusted at will. The aerial was so built that it rotated through 360° and gave a remote\npanoramic presentation. About 20 equipments were in use in January 1945. The range claimed for it was only about 100 km.\n\nJagdhütte:This apparatus, which was produced by Siemens, gave a panoramic P.P.I. display of the German I.F.F. responses, using 24 metre or 36 metre rotating aerials. The wavelength employed was 2.40 metres and it was planned, with its aid, to trigger off the FuGe 25A. In this way friendly fighters were to be controlled from the ground at ranges up to about 300 km. It was fully realised that if the FuGe 25A frequency was ever jammed the Jagdhütte would be useless, but it was not considered likely that the Allies would attempt to jam it. Small numbers may have been completed at the wend of the war.\n\nJagdwagen: Jagdwagen was designed as a mobile Panoramic radar to control fighters at close ranges immediately behind the front. It was a project of the firm of Lorenz. The aerials were considerably smaller than the Jagdhütte, the array being only 8 metres long. The aerial array was to be mounted on the Kumbach stand as used in the Egerland Flak set. The frequency bend used was that of the A.S.V. set Hohentwiel namely 53–59 cm. Range 40–60 km. Prototypes only.\n\nJagdhaus (FuMG 404): Jagdhaus was designed and built by Lorenz in 1944 as an early warning radar. It was the most powerful radar built by the Germans, with a peak pulse power of 300 kW, which Lorenz planned to increase to 750 kW. The whole assembly was the size of a house, which is possibly how it got its name; ‘haus’ being the German for ‘house’. The rotating upper part of the construction housed the separate parabolic transmit and receive antennae and reflectors, with the IFF above them as usual. It weighed 48 tons and rotated at 10 rpm. It operated on wavelengths of 1.4 to 1.8 metres, and had a range of about 300 km. It could measure altitude, azimuth and range. The control room was located below the antennae, from which its PPI image was also transmitted to command HQ at Charlottenberg by Landbrieftrager, similar to the Jagdschloss system. It is believed that only one Jagdhaus was constructed, which fell into Soviet hands when it was captured by their troops in 1945, during which time it was damaged. The Soviets compelled the Germans to repair it and instruct them in its operation.\n\nLichtenstein B/C - FuG 202: Low-UHF band frequency range, introduced in 1941 it was the initial AI radar. Deployed in large numbers with 32-dipole element \"Matratze\" (mattress) antenna arrays, it operated on the 61 cm wavelength. Its range was (in theory) 2–3 km but in practise was found to be dependent on factors such as height. Compromised to the Allies on May 9, 1943.\n\nLichtenstein C-1 - FuG 212: Introduced in 1943, this was an improved version of the FuG 202.\n\nLichtenstein SN2 - FuG 220: Low-mid VHF band frequency range, introduced in 1943 in response to Allied jamming, and used an eight-dipole \"Hirschgeweih\" (atag's antlers) antenna array. Transmitter power of 2Kw on 3.3 meters. Range was increased to 6 km. Minimum range was 400 m which was found to be a problem, hence aircraft carried it and FuG202. Later versions did away for the need for the Fug 202. Compromised to the Allies in July 1944.\n\nLichtenstein SN3 - FuG 228: A higher powered version of the SN2. Range increased to 8 km. Only a small number accepted into service, perhaps only prototypes.\n\nNeptun 3 - Fug 218: A replacement for SN2, deployed late 1944 after SN2 was jammed. Wavelength 1.6 to 1.9 meters, most often using same, eight-dipole \"stag's antlers\" antenna array with shorter dipole elements. Range up to 5 km. Some were fitted to me 262 to create night fighters that could catch Mosquito intruders.\n\nBerlin N3/N4: Experimental units.\n\nBremen - FuG 244: (also known as Berlin D) Berlin A with the frequency changed to 3 cm (10 GHz) rather than 9 cm. Experimental.\n\nBremen O - FuG 245: Another experiments 3 cm unit.\n\nNeptun: Early system - It failed its acceptance tests - the system was later reworked into an Airborne intercept set.\n\nHohentwiel (FuG 200); UHF-band radar, operated at wavelength between 52 and 57 cm. Range was between 10 km for a small vessel like a surfaced submarine to 70 km for a large ship. Under the best circumstances it could see the coast at approx 150 km. It had separate antennae for transmit and receive. The transmit antenna was centrally mounted, pointing forward, while the two receive antennae were mounted either side, pointing outwards by 30 degrees, giving it a search beam width of about 120 degrees. Each antenna array consisted of sixteen horizontally polarised dipoles, mounted in four groups of four in a vertical stack.\n\nA variant of the Hohentwiel the Tiefentwiel (FuMG407); was tried as an Air Surveillance radar on the coast to try and detect low flying aircraft.\n\nFuMO 1 - Calis A: Its 6.2 x 2.5m antenna consisted of 2 rows of eight full wave vertical dipoles. Its wavelength was 82 cm and its range depended on the height it was installed above sea level, but typically was about 15–20 km. Given the frequency low angle reflections from the surface, also known a clutter would have been an issue.\n\nFuMO 2 - Calis B: Improved version of the FuMO 1 - similar clutter problems but improved transmitter and accuracy.\n\nFuMO 3 - Zerstorersaule: A version of the destroyer radar modified for land use.\n\nFuMO 4 - Dunkirchen : Improved version of the FuMo 2 - similar otherwise\n\nFuMO 5 - Boulogne: Yet another improved version of the FuMO 2 - increased transmitter power again with an improved aerial - usable range now 40 – 50 km.\n\nFuMO 11 - Renner: 3M antenna from a Wurzburg combined with a 9 cm \"Berlin\" unit and mounted on a Seetakt base optimized for sea search rather than air search. Sources differ on usable range.\n\nFuMO 12 & 13: Improved Renner units to attempt to compensate for poor reliability with the original unit.\n\nFuMO 15 - Sheer: Combination of a Berlin 9 cm and an Antenna from a Giant Wurzburg - seems to have been optimized for surface search in the same was as the Renner series was.\n\nFuMO 51 - Mammut G: Version of the Luftwaffe FuMO401 but with Seetakt antenna and waveforms to optimise it for surface search rather than air search.\n\nFuMO 214 - Giant Wurzburg: Naval designation for the airforce unit.\n\nFuMO 215 See Reise: Improved FuMO 214\n\nFuMO 52: Naval designation for the FuMG 401 Mammut C.\n\nFuMO 64 : A version of the Hohentwiel L ASV radar modifier for coastal air search - different from the unsuccessful xxx\n\nFuMO 221: Naval designation for the FuMG 64 Mannheim.\n\nFuMO 301 - 303: Versions of the FuMG 39-41 Freya\n\nFuMO 311 - 318: Versions of the Freya working on other frequencies (Around 2.2 Meters) from the normal Freya. Sometimes known as the Freiburg\n\nFuMO 321 - 328: Based on the fuMO311 family of units but workig at 1.5 meters.\n\nFuMO 331: Naval designation for the FuMG 402 Wassermann M\n\nFuMO 371: Naval designation for the FuMG 403 Jagdschloss\n\nFuMO 201: Flakleit - Using Seetakt 80 cm technology a 3D radar mounted on an underground armoured turret ( originally an optical rangefinder) small numbers produced. Multiple antenna. Manufactured by GEMA.\n\nFuMO 211 - 213: Naval designation for the FuMG 62 family or radars - the Wurzburg A,C & D.\n\nFuMO 215: See Reise.\n\nFuMO 221: Mannheim.\n\nFuMO 111: Barbara, 9 cm fire control radar based on modifying a FuMO 15 Giant Wurzburg to operate at 9 cm. Only experimental radars produced\n\nFuMO 214: A Wurzburg Reise reconfigured for use as a naval radar with a range of approximately 50 – 70 km against surface targets.\n\nFuMO 215: Improved range version of the FuMO214.\n\nAll though the Germans were carrying out research at centimeter wavelengths at the start of the war the work was abandoned as it was decided that the war would be over before the research & development could be completed. In February 1943 a RAF Stirling bomber was shot down over Rotterdam and a damaged H2S system was recovered. The Germans started a crash development program to use the information deduced from the captured system. All though a range of prototypes were produced, very few reached front line troops. Due to the device being recovered near Rotterdam the Germans used that name in several code names for the Centimeter (9 cm) systems, such as \"Rotterdam Device\".\n\nRotterdam: To get the quickest start with development, German industry copied, as far as possible, the H2S system. Approximately 20 systems were manufactured for R&D work. They led to the Roderich jammer and the Berlin & Korfu receivers.\n\nJagdschloss Z: The 9 cm version of the Jagdschloss F panoramic radar system. Prototypes only.\n\nForsthaus Z: The 9 cm version of the Forsthaus panoramic search radar. Prototypes only.\n\nFuMG 77: Rotterheim. A combination of the 9 cm receiver/transmitter of the Berlin system with the Antenna and other systems from a Mannheim. Its range was about 30 km and it was found to be unaffected by allied jamming. Its name changed to Marbach V later in the war.\n\nFuMG 76: Marbach. A combination of the Berlin transmitter/receiver with the Ansback 4.5 meter reflector and systems. Controlled by the \"Michael\" remote control system. Sources suggest that 3 systems were completed.\n\nFuMG 74: Kulmbach. A 9 cm panoramic search radar. 6 meter antenna and remote controlled like the FuMG76. When combined with that radar it was known as the Egerland system. Only 2 completed. range of approx 50 km.\n\nFuG 221 Freya-Halbe : This was a Freya modified to locate British airborne jammers. Development completed but due to lack of parts never deployed.\n\nFuG 221 Rosendahl; This was a Freya modified to locate British bombers by tracking their Monica warning radar emissions. By the time development was completed the British had ceased using Monica, so never deployed\n\nFuG 223: A family of passive airborne receivers tuned to various radar bands such as Freya and Würzburg. Designed to allow night fighters to home onto bombers fitted with jammers against those radars. The Fug223 was a version build from surplus FuG 227 components that detected reflected energy from an aircraft being illuminated by a ground radar. In this way it was an example of an early semi-active radar homing system. In order to work it seems that the radar beam had to illuminate the target and the night fighter so that the two receivers could be synchronized. Used by one test and development squadron at the end of the war.\n\nFuG 227: Built using some components from the FuG 220 range of AI equipment. This was a passive device which allowed night fighters to home onto bombers which had their rear warning 'Monica' active. Monica was a short range VHF radar (200 MHz band) which was fitted to the tail of British heavy bombers facing down and back to give the rear turret gunner a warning display. Using this equipment the night fighters could achieve intercept with apparent ease. Extremely effective until the British captured a Me 110 with FuG227 installed and realised its mode of operation. there after Monica was removed from bombers and FuG227 ceased to have any value.\n\nKlein Heidelberg was the code-name give to a passive radar system devised in 1941. The system was a bi-static radar system. What was unusual was that the transmitters were British rather than German! The system worked by using the reflections from the Chain Home (British coastal radar system) rather than transmitters associated with the receivers. Klein Heidelberg worked by sensing Chain Home (CH) transmission pulses directly with a small auxiliary antenna, close to the main antenna, whose receiver was tuned to a particular CH station whose exact location, bearing and range was known. The CH signal was then used to synchronise the KH with the CH transmission pulses. The CH pulse started a circular trace on a cathode ray tube (CRT) divided into forty sections. The main antenna received the reflection of these pulses from the target and displayed them on the CRT. Range was between 300–600 km. The display was 2D. Resolution was not very good but it allowed the Germans to see bomber formations forming up over England and the general path of the bomber streams. Its big advantage was it was not possible for the British to jam without jamming their own radars. The system entered service in late 1943 and by late 1944 six system were commissioned on the Dutch coast.\nFuG 350 Naxos & FuG 351 Korfu: This was a family or radar detectors that operated in the 8 to 12 cm band. They were primarily designed to locate Allied H2S radar transmissions. A range of antenna were used some stationary and some rotating. There were intended to be air, land and maritime versions. However Naxos had a resolution problem that limited its ability to distinguish individual aircraft. This allowed the night fighter to locate the bomber stream but not usually individual bombers. This was not usually an issue with the maritime based system (primarily U boats) as there was usually only one aircraft detected at a time. To reduce this issue an improved version the Korfu was developed. It was intended to field Korfu as a replacement for Naxos in all three versions but due to a shortage of components only the land based version was fielded where is resolution could be used to the best effect.\n\nFuG 350 Naxos Z: The original system, detected H2S radar system on bombers. Unable to distinguish individual bombers nor the 10 GHz H2X Allied bombing radar, but could reliably guide the fighter into the bomber stream.\n\nFuG 350 Naxos ZR: Additional aerials added a tail warning system which allowed British night-fighters to be detected.\n\nFuG 350 Naxos ZX: 3 cm version for detecting allied H2X radars. Not known to have ever been fielded.\n\nFuG 350 Naxos RX: 3 cm version of the Naxos ZR. Not known to have ever been fielded.\n\nFuG 350 Naxos ZD: Combined Z and ZX, allowing 9 cm and 3 cm detection in the same system.\n\nFuG 351 Korfu Z: Entered production late 1944, due to shortage of components only ground-based versions deployed though an airborne version completed development. better range and discrimination than Naxos.\n\nFuG 280 Kiel Z: IR based passive receiver. 10 degree field of view - display via CRT. problems with discrimination between fires aircraft and other IR sources.\n\n\n"}
{"id": "18620305", "url": "https://en.wikipedia.org/wiki?curid=18620305", "title": "Hebrew spelling", "text": "Hebrew spelling\n\nHebrew spelling ( , \"Hebrew spelling\") refers to the way words are spelled in the Hebrew language. The Hebrew alphabet contains 22 letters, all of which are primarily consonants. This is because the Hebrew script is an abjad, that is, its letters indicate consonants, not vowels or syllables. An early system to overcome this, still used today, is \"matres lectionis\", where four of these letters, alef, he, waw and yodh also serve as vowel letters. Later, a system of vowel points to indicate vowels (diacritics), called niqqud, was developed.\n\nThroughout history, there have been two main systems of Hebrew spelling. One is vocalized spelling, the other is unvocalized spelling.\n\nIn vocalized spelling (\"ktiv menuqad\"), all of the vowels are indicated by vowel points (called niqqud). In unvocalized spelling (\"ktiv hasar niqqud\"), the vowel points are omitted, but some of them are substituted by additional vowel letters - waw and yodh (\"Ktiv malē\"). This system is the spelling system commonly used in Israel today.\n\nVowel points are always optional in Hebrew. They can be used fully, partially or not used at all. The recommended approach endorsed today by the Academy of the Hebrew Language and other Israeli educational institutions is to use \"plēnē\" spelling (\"matres lectionis\") when not adding vowel dots (which is the usual case), and place a vocalization sign on a letter only when ambiguity cannot be resolved otherwise. The \"defective\" spelling is recommended for a fully vocalized text, hence its use is becoming rare. Texts older than 50–60 years may be written in an unvocalized \"defective\" spelling (for example, the word \"ħamiším\" \"fifty\", was written חמשים on banknotes issued in Mandatory Palestine or the Bank of Israel in its early days. Today, the common spelling is חמישים). A vocalized \"plene\" spelling system is common in children books, when it is better to accustom the children to the more popular \"plene\" spelling, while still letting them benefit from the vowel dots as a reading aid in early learning stages.\n\nA third system that was endorsed in the past by the Academy of the Hebrew Language as an optimal system, but abandoned due to low popularity, calls for the use of \"ħolám\" (וֹ), \"šurúq\" (וּ), \"dagéš\" in \"Bet\", \"Kaf\" and \"Pe\" (בּ, כּ, פּ vs. ב, כ, פ), \"Šin Smalít\" (שׂ) and \"mappíq\" (הּ), while abandoning all other vowel dots (in everyday writing). According to this system, \"matres lectionis\" are still introduced to mark vowels, but the letter \"Vav\" is used only as a consonant, while its variants \"ħolám\" and \"šurúq\" serve as vowel letters. This system also makes clear distinction between final \"He\" used as a vowel marker (e.g. ילדה \"a girl\" ) and as a consonant (e.g. ילדהּ \"her child\"). This system was never extensively used, and the Academy of the Hebrew Language finally abandoned it in 1992, when new rules were published not assuming any use of vowel dots.\n\nRules for unvocalized spelling were first issued by the Hebrew Language Committee in 1890 (which became the Academy of the Hebrew Language in 1953) and formally standardised in 1996. Even though the rules are established, some of the rules and specific spellings are disputed by writers and publishers, who often create their own in-house spelling system. Also, because having two spelling systems within the same language is confusing, some would like to reform it. In 2004, Mordechai Mishor, one of the academy's linguists, proposed in a session of the Academy of the Hebrew Language a modest reform.\n\nThere are three systems of spelling used for Modern Hebrew.\n\nTo illustrate the problem with Ktiv haser:<br>\n\nIn practice, many times two or more spelling systems are used in one text. The most common example of this is a word may be vowelized (using niqqud, the \"dots\") partially, for instance with , where only the vav () is vowelized. This clarifies that the vowel is an \"o\" () and not \"u\" (). In addition, 3 letters (historically 6), can take a different sound depending on if there is a dot (called a dagesh) in the middle of the letter (a bet, kaf, and pe). In full spelling, the dot is not included, regardless if it is making one sound or the other. An example when a mixture of systems would be used is to clarify when the letter is taking a dagesh. An example of this is shown in the adjacent picture, where for the word kosher ( , , ) may be written as כּשר (a mixture of the two systems) to be unambiguous that it is the letter כּ and not כ . Words may be written in \"ktiv haser\" (\"missing spelling\") if it is unambiguous and clear enough (ex. חנכה instead of the \"full\" form חנוכה). In this case, the reader deciphers the word mostly by its context.\n\nAlso, some words are almost always written in the \"missing\" form (\"ktiv haser\") in everyday life: לא (, no), אמא (, mother), אם (, if), and כנרת (, Kinneret).\n\n"}
{"id": "5540323", "url": "https://en.wikipedia.org/wiki?curid=5540323", "title": "ISO 639:f", "text": "ISO 639:f\n\n! \n! \n! \n! \n! \n! \n! \n! \n! \n! \n! \n! \n! \n! \n! \n! \n! \n! \n! \n! \n! \n! \n! \n! \n! \n! \n! \n! \n! \n! \n! \n! \n! \n! \n! \n! \n!() \n! \n! \n! \n! \n! \n! \n!() \n! \n! \n! \n! \n! \n! \n! \n! \n! \n! \n! \n! \n! \n! \n! \n! \n! \n! \n!() \n! \n! \n! \n! \n! \n! \n! \n! \n! \n! \n! \n! \n! \n! \n! \n! \n! \n! \n! \n! \n! \n! \n! \n! \n! \n! \n! \n! \n! \n! \n! \n! \n"}
{"id": "8370136", "url": "https://en.wikipedia.org/wiki?curid=8370136", "title": "Ico-D", "text": "Ico-D\n\nThe International Council of Design (ico-D; formerly known as International Council of Communication Design or Icograda, which was formerly an initialism for International Council of Graphic Design Associations) is a world organisation for design professionals. ico-D was founded in London in 1963 and celebrated its 50th anniversary on 27 April 2013. It is a non-profit, non-partisan organisation and a \"member-based network of independent organisations and stakeholders working within the multidisciplinary scope of design.\"\n\nico-D members include professional design organisations, design promotion bodies, design media, design education institutions and individuals with a vested interest in professional design. Design media are affiliated through the International Design Media Network (IDMN). Individuals are affiliated through the ico-D's Friends Network which was established in 1991.\n\nico-D coordinates best international practices for communication design. It maintains affiliations with other international organisations such as Cumulus Association, IFRRO, ISO, UNESCO, UNIDO, WIPO, and ECOSOC.\n\nPeter Kneebone proposed the idea to establish an international organisation for graphic design and was involved in Icograda's founding. The Society of Industrial Artists (which changed its name in 1963 to the Society of Industrial Artists & Designers, and is now the Chartered Society of Designers, or CSD) set up a working group under the chairmanship of Willy de Majo, to promote the creation of an organisation to represent internationally the many professional graphic design associations throughout the world. No such organisation existed. The profession was rapidly growing in importance, and also attempting to clarify its identity and objectives. It was involved in increasingly complex social and technological situations. National associations were developing, but international dialogue and action were intermittent and uncoordinated. It was important to create links between the professional associations in all countries, and between the profession and the rest of the world.\n\nThe inaugural conference of Icograda took place at Watney House, London, from 26–28 April 1963, attended by delegates from 28 associations in 17 European countries. The meeting was chaired by H.K. Henrion and supported by Kneebone as secretary. On 27 April, the meeting agreed to formally establish Icograda. Proposals that were ratified include the development and drafts of a \"Code of Ethics and Professional Practice\", a \"Code of Contract and Conditions of Engagement for Graphic Designers\", \"Rules and Regulations for International Graphic Design Competitions\", an \"International Directory of Organizations Concerned with Graphic Design\" and the publication of a News Bulletin.\n\nThe first Executive Board was elected at the meeting in London, composed of Willy de Majo (Great Britain, President), Wim H. Crouwel (The Netherlands, Secretary General), Martin Gavier (Sweden, Treasurer), Peter Hatch (Great Britain, Vice President), Hans Neuburg (Switzerland, Vice President), Jukka Pellinen (Finland, Vice President), D. Stojannovic-Sip (Yugoslavia, Vice President), John Tandy (Great Britain, Member), Pieter Brattinga (The Netherlands, Member) and Paul Schwarz (The Netherlands, Honorary Treasurer). This Executive Board served from 1963 to 1966.\n\nAt the same meeting, the first Icograda Congress and General Assembly in 1964 in Zurich was called in order to establish the aims and structures upon which the future of the organisation rested.\n\nOn 27 April 2013, Icograda celebrated its 50th anniversary and its 25th General Assembly.\n\nThe first Icograda Congress and General Assembly took place in 1964 in Zurich. It was attended by about 200 designers from 17 countries and 23 associations. The theme of the Congress was Commercial Artist or Graphic Designer\". The Congress opened with the reading of a message by Prince Philip, Duke of Edinburgh who wrote: “Every day designers of all kinds are becoming responsible for a greater proportion of man's environment. Almost everything that we see and use that was not made by the Almighty has come from some designer's drawing board. This is a very heavy responsibility and every effort by designers to improve standards, to encourage proper training and to develop a sense of social awareness is to be welcomed.” At the General Assembly, delegates Icograda's aims and objectives, and it was further agreed to establish an audio-visual archive and library, to publish a news bulletin and to award student scholarships. Jenny Toynbee from Edinburgh School of Art won the first student scholarship. Ernest Hoch's proposals for a unified system of typographic measurement was accepted for further development.\n\nIn 1965 Icograda establishes the Signs and Symbols Commission. It first collaborates closely with the International Chamber of Commerce and then with the International Organization for Standardization (ISO). In the same year Icograda publishes \"Graphic journalism: Catalogue of Magazines and Annuals of Graphic Design and Allied Subjects\" featuring around 300 journals from 30 countries.\n\nIn April 1966 the first Icograda International Student Project was judged in Belgrade. The theme was 'Public information signs' and the jury included Abram Games, Josef Müller-Brockmann, Paul Rand, Masaru Katzumie and Ivan Picelj. The second Icogarada Congress, 'Graphic Design and Visual Communication Technology', and General Assembly were then held from 11–16 July 1966 in Bled, then in Yugoslavia. R Buckminster Fuller was one of the main speakers at the Congress. At the General Assembly, four key policy documents were ratified: \"Rules and Regulations for International Graphic Design Competitions\", the \"Honorarium for Judges of International Competitions\" document and the revised \"Code of Ethics and Professional Practice\", and the \"Code of Contract and Conditions of Engagement for Graphic Designers\". , head of design at Phillips in Eindhoven, was elected President and Pieter Brattinga from the Netherlands, Secretary-General. The first student seminar was also held in Bled from 11–13 July 1966 and the theme was 'Breaking the Language Barrier with Signs and Symbols'. In addition, the production of \"Equality of Man\", a film project for students and young people in support of the United Nations Human Rights Year, was the first of several Icograda collaborations with International Animated Film Association (ASIFA)\n\nThe third Icograda Congress 'Design Destinations in a Changing World' was held in 1968 in Eindhoven. An exhibition of Belgian graphics was organised by Michel Olyff at the same time and the entire congress decamped to Brussels for a day. \"Sachez que tout en ce monde n’est que signes et signes de signes\" (beware that all in the world is only signs and signs of signs), were the words of Marcel Schwab quoted by Alderman van der Harten in opening the congress. Der Harten went on to make observations about communication, information systems, symbols, codes and signals, \"but away from the congress there were different signs that could not be ignored. A government minister failed to appear to make an address on the third morning of the congress; he was in an emergency cabinet meeting.\"\n\nIcograda has always been totally non-political but the 1968 Congress coincided with the end of the Prague Spring. As the tanks rolled into Czechoslovakia the congress paused in a moment of silent respect. As the crisis deepened over the next few days Netherlanders offered hospitality to those wishing to stay on in the country for a few days, and many of the delegates marched, protested and petitioned their embassies as military repression threatened the freedoms of friends and colleagues. Whilst the SIAD had facilitated the inaugural meeting in London five years before, it must be remembered that most of the founders of Icograda were central Europeans, an assortment of Yugoslav, Austrian, German, Polish and Hungarian political émigrés who had made their home in the UK to escape persecution. The response of the Congress was therefore only to be expected. Some presentations from Teunissen van Manen, Richard Gregory, Benno Wissing, Jerome Gould, and Massimo Vignelli, were thus overshadowed by political events.\n\nAt the 3rd Icograda General Assembly FHK Henrion was elected President and Pieter Brattinga re-elected Secretary-General. In addition, the book \"ICOGRADA The First Five Years\" by Wynkyn de Worde, was published and Icograda was commissioned by UNESCO to produce their first series of slides on graphic design.\n\nIn 1971 the fourth General Assembly was held in Vienna, John Halas was elected President, and Marijke Singer Secretary-General. The newly established President's Trophy was awarded to Peter Kneebone. In 1970, the congress and exhibition 'The Visual Communicator and the Learning Industry' took place in Vienna. Despite a raft of excellent speakers, there was considerable dissatisfaction with the way in which the event had been organised, in particular the commercial nature of the exhibition, and the congress effectively turned on itself for half a day with an open discussion about how matters should be arranged in the future. Memorably Herb Rohn from Carbondale, Illinois, standing on a table, demanded metaphorically that \"the windows of the Hofburg be thrown open to let some fresh air into this place where there has been none this week.\"\n\nThe first issue of 'icographic', Icograda's biannual magazine, was published in 1971. Founded by John Halas, it was edited by Patrick Burke. The complete ten-year cycle was tapped out on an IBM Selectric typewriter, with many idiosyncratic but significant design issues being presented to a widening design audience.\n\nIn 1972 Icograda achieved consultative status with UNESCO in Paris. The fifth General Assembly and the Symposium 'Towards a Working Congress on Education' were held in London. Kurt Weidemann was elected President, Marijke Singer Secretary-General, and the President's Trophy was awarded to Patrick Wallis Burke.\n\nIn 1973 the first Icograda Student Seminar was held in London, chaired by FHK Henrion. The London seminars were a regular feature of the design calendar until 1999. After Henrion died in 1990, the event was chaired by Alan Fletcher, and for its final three years by Mervyn Kurlansky.\n\nIn 1974 the sixth Icograda General Assembly took place in Krefeld, Germany. Walter Jungkind was elected Icograda president, the first non-European resident to hold the office. He was also the first whose main career was in design education. Marijke Singer was again elected Secretary-General and Ernest Hoch won the President's Trophy. The assembly was followed by the 'Edugra' Congress held in Neuss, across the Rhine from Düsseldorf. The theme was 'Graphic Design Education'.\n\nIn 1975 the Icograda Edugraphic '75 International Conference was held in Edmonton with the theme 'Education for Graphic Design/Graphic Design for Education'. In 1976 a symposium 'Design for Need', jointly sponsored by Icograda and ICSID was held at the Royal College of Art in London. Following a study undertaken for UNESCO, an Icograda travelling exhibition 'The Image of Women in Advertising' opened in Eindhoven.\n\nThe 1977 Zurich Congress − 'Graphic Design for Social Communication' − was complimented by an entire issue of \"Graphis\" magazine devoted to the Congress exhibition held at the Kunstgewerbe Museum Zurich. ICSID president Kenji Ekuan was one of the keynote speakers. The congress was followed by a seminar in Lausanne − 'A Town and its Image' − and the General Assembly at which Flemming Ljorring was elected President and Peter Kneebone Secretary-General. The President's Trophy was awarded to Kenneth and Shelagh Adshead for their work on the audio-visual archive.\n\nIn 1978 Icograda met at Evanston outside Chicago for the 'Design that Works' congress devoted to design evaluation; how designers could prove the commercial and social value of what they did. Speakers included Josef Müller-Brockmann, Milton Glaser and Massimo Vignelli. The eighth general assembly took place the following year in Paris at the Centre Pompidou. Peter Kneebone was elected President, Keith Murgatroyd Secretary-General and Bob Vogele was awarded the President's Trophy for his work organising the event in Chicago. The assembly was preceded by a seminar − 'Is Graphic Design a Reflection of Society or a Factor in its Evolution?'\n\nIn 1980, Mauro Kunst organised the first Icograda Latin American conference in Guadalajara, Mexico. In the same year, \"World design sources directory 1980 = Répertoire des sources d'information en design 1980\", edited by Centre de Creation Industrielle, was published on behalf of Icograda and ICSID.\n\nThe first Icograda, ICSID and IFI Joint Congress took place in Helsinki from 1–8 August 1981, an event that had been under discussion since 1977. The theme was 'Design Integration'. Three separate general assemblies followed the congress. At the 9th Icograda General Assembly, Stig Hogdal was elected President, Marijke Singer was once again elected Secretary-General and the President's Trophy was awarded to Geoffrey Bensusan for his work on the \"Icograda News Bulletin\". The first Icograda-Philips design award was awarded to Benoit de Pierrepont and S S Satie focusing on the theme \"The design of instructions and warnings, and the related problems of society in any area of human activity\". Also in 1981, Icograda organised a poster competition on behalf of UNESCO to celebrate the International Year of Disabled Persons.\n\nIn 1982 the first of six issues of \"icographic/volume 2\" was published on behalf of Icograda by Mobilia Press, with Jorge Frascara as associate editor. Each issue was devoted to a single theme and abstracts were included in four languages. Also in 1982, Haig David-West organised the first African regional meeting in Port Harcourt, Nigeria and in 1983, Haig David-West (the first ever elected Icograda Executive board member from Africa) edited the publication \"Dialogue on graphic design problems in Africa\", based on this meeting.\n\nIn 1983 the 10th General Assembly and Icograda Congress '\"Design Interaction\"' were held in Dublin. The keynote address was given by Erskine Childers of the United Nations. In one of the most remarkable addresses ever made to an Icograda congress, Childers mixed thousands of years of history with personal experience as he set out examples where \"I can tell you that again and again down the years I have confronted communication needs for which graphic visuals were indispensable – the only answer\". Setting out a powerful argument for the potential of creating \"rapid economic and social development through a major effort in endogenously researched and programmed use of the visual media leading to – but in that order – literacy\" he called on the delegates to \"offer and apply your centuries-evolved skills and sensitivities to help humanity see – literally see – both its marvelous capacity for progress and its primeval capacity for error, inhumanity, social neglect, even apocalyptic destruction\". Admitting that he was presenting the delegates with challenges that were 'especially heavy' Childers said \"I never knew a good graphic designer who did not explode with creativity and blossom when faced with an ostensibly impossible task... History will once again make its awesome judgement of you – but from now on with greater respect because you cannot any longer be underestimated. You help all of us perceive and place ourselves in life. Now you must help all of us understand so that we can more wisely and assuredly manage all our futures\". Raymond Kyne was elected President and Robert Blaich won the President's Award for his promotion of Icograda-Philips design award. The number of member organisations had by grown to 50. A multidisciplinary design competition on the theme of \"Shu/Collectivity\", held under the auspices of Icograda, ICSID and IFI, and sponsored by the Japan Design Foundation, was won by Charles Owen from the Illinois Institute of Technology.\n\nIn 1985, the 11th Icograda Congress \"The place and influence of graphic design in everyday life\" was organised by Philippe Gentil in Nice, the first time that there were major presentations from India ('design without designers' by people whose needs were too urgent to await intervention by professionals) and Australia (from boomerang to bicentenary); from the design of books in China to street signs in Buenos Aires, and provocatively 'the backside of design' – what happens after the consultant has moved on to new, more rewarding problems. According to Stephen Hitchins (Icograda board member from 1987−1991), Ivan Chermayeff made a long presentation totally devoid of visuals stating that he \"wanted graphics to grow up\" – comparing much of it \"to WC Fields' definition of a virgin: a child about four years old, extremely ugly\". So much of the pleasure Chermayeff claimed that he gained from graphic design was from process rather than product, \"a sensation of feeling something is still wet long after it's dry\". Improvisation, accidents, speed, freshness, and individuality were what mattered to him. Even without pictures, he won many delegates over. At the 11th Icograda General Assembly (also in Nice) Jorge Frascara was elected president. The President's Trophy went to Jan Railich, the chairman of the Brno Biennale. Also in 1985, the \"World Directory of Design Schools and Programmes\" was published jointly by Icograda, ICSID and IFI, and edited by Maarten Regouin, secretary of the Icograda Education Working Group, which also coincided with Icograda's establishment of the Design History and Design Management working groups chaired by Michael Twyman and Abe Rosenfeld respectively.\n\nIn 1986 Icograda Excellence Awards were made for the first time at the Brno and Warsaw Biennales. The recipients were Christof Gassner and Henryk Tomaszewski.\n\nIn 1987, Amrik Kalsi, Jorge Frascara and Peter Kneebone organised the \"Graphic Design for Development Seminar\" on behalf of Icograda and UNESCO. It was hosted in Nairobi, Kenya, from 6–10 July and attracted 91 participants from 14 countries.\n\nThe 2nd Joint Congress of Icograda, ICSID, IFI \"Design 87\" − with as its theme 'Design Response' − was held in Amsterdam from 16–20 August 1987. At the 12th Icograda General Assembly which took place from 21–22 August (also in Amsterdam), Niko Spelbrink was elected president and Mary V. Mullin became Secretary-General (and Director of the Icograda Secretariat – a position she held until 1999). Susumu Sakane was awarded the President's Trophy. Uwe Loesch won the first Icograda Excellence Award to be presented at the Lahti Biennale. Icograda published the \"History of Design Bibliography\" edited by Victor Margolin, and \"Projects in Graphic Design Education\" edited by Jorge Frascara.\n\nIn 1988, Nils Tvengsberg organised a special event in Oslo, from 13–15 May, to mark the 25th anniversary of Icograda. All 11 presidents from 1963-onwards attended together with many former board members, partners, friends and supporters. It coincided with Norway's national day and according to Stephen Hitchins, \"it was one of the more remarkable Icograda events in a history of remarkable events\". Notably, Nils Tvengsberg was the first person to serve on the boards of both Icograda and ICSID simultaneously. His goal was to look at the possibility of joining their common interests. It would be over 20 years before this partnership, the International Design Alliance (IDA), would be headquartered in Montreal, contemplating the first IDA joint congress in 2011.\n\nIn 1989 the 13th Congress took place at Tel Aviv University, chaired by Abe Rosenfeld. The theme was 'Graphic Design Through High Technology?' At the General Assembly Simon de Hartog won the President's Award, and Helmut Langer became President.\n\nIn 1990, \"Graphic Design, World Views. A celebration of ICOGRADA’s 25th Anniversary\", edited by Jorge Frascara, designed by Niko Spelbrink, and with a cover designed by Grapus, was published jointly by Icograda and Kodansha, a major book on graphic design that celebrated Icograda's 25th anniversary.\n\nThe Icograda Foundation was established in 1991 for the advancement of worldwide understanding and education through the effective use of graphic design. A limited company, the Foundation was a registered charity funded by corporate sponsorships, individual donations, legacies, and various fundraising activities. Mary Mullin received the President's Award for her work in establishing the Foundation.\n\nIn 1991 the 14th Icograda Congress and General Assembly took place in Montréal, from 25–29 August, attended by around 2,000 delegates. At the Icograda General Assembly (30–31 August 1991), Giancarlo Iliprandi was elected president. Highlights of the Assembly included: the hosting of the 3rd Marijke Singer Memorial Lecture presented by Margaret Catley Carlson, entitled \"Images for the Next Century\"; the Icograda President's Award was bestowed on Mary V. Mullin for the inauguration of the Icograda Foundation and the Friends of Icograda; establishment of the Icograda World Graphic Design Day; establishment of Icograda Steering Committee for the International Design Archive and Research Centre Project; and the establishment of Icograda Friends with seed donations contributed by 68 Japanese founders, chaired by Hiroshi Kojitani. In the same year, the first \"Icograda/IFI/ICSID Joint Newsletter\" was published.\n\nIn 1993 the 3rd Joint Congress of Icograda, ICSID and IFI was held in Glasgow, Ireland from 5–9 September. Stephen Hitchins chaired the organising committee and there were 101 speakers and over 1,000 delegates. Rick Poynor writing in \"Creative Review\" in 2007, said it had been “the most sophisticated and future-orientated discussion of design in the UK for 15 years”. Jeremy Myerson called it \"a watershed event, one of those rare occasions when the design community comes together and presses the pause button, and stops to reflect on what designers do, how things have changed, and where design practice could go in the future... It explored the limits of design. Yet it also opened up new horizons\". To quote Christopher Frayling who rounded off the event, \"Two moments from Design Renaissance will stick in my mind for a long time. One is Victor Papanek quoting in discussion Charles Rennie Mackintosh, who said, 'there is hope in honest error, none in icy perfection'. The other is Yuri Soloviev's story about persuading Eduard Shevardnadze not to go ahead with a town planning scheme in Georgia, while swimming out into the Black Sea, on holiday. I guess the moral of the tale is: if you are up to your neck in it − keep talking, keep persuading, grab the moment when it comes, have a healthy disregard for bureaucracy, and be a strong swimmer. Oh, and as you are swimming, try not to lose your bearings and always remember that the word 'utopia' means 'of no place at all'.\"\n\nAs with all Icograda events, it was an invigorating experience, and it swam out into some uncharted waters. At the 15th General Assembly held in Glasgow immediately after the Congress, Philippe Gentil was elected President. Hiroshi Kojitani received the President's Award for his work in establishing Friends of Icograda.\n\nFrom 23–27 July 1995, the 16th Icograda Congress took place in Lisbon, Portugal, followed by the General Assembly in Porto. The theme of the Congress was \"Shifting Frontiers\". At the 16th Icograda General Assembly, Jose Korn Bruzzone from Chile became President and the Icograda President's Award was presented to Marion Wesel-Henrion and J. Brian Davies, jointly awarded for outstanding work for the Icograda Foundation and the organisation of the Poster Auction. Icograda hosted the International Poster Auction managed by Sotheby's London in May 1996.\n\nIn 1996, Icograda released a policy document \"Digital Immortality: Encapsulating the Work of the World's Top 20th Century Designers for an Icograda Archive in digital form\" aimed to establish a comprehensive archive for Icograda and the work of leading graphic design masters. In the same year, Icograda participated in a meeting of the Icograda/ICSID/IFI working group founded to develop closer working relations between the three organisations which took place from 7–8 December in Copenhagen, Denmark.\n\nThe 17th Icograda Congress and General Assembly took place from 21–23 October in Punte del Este, Uruguay. This was the first Icograda Congress to be hosted in the Southern Hemisphere and attracted ±1 000 delegates. The Congress theme was 'INTERCAMBIOS/EXCHANGES'. At this General Assembly in Punte del Este, the Icograda Assembly joined the ICSID and IFI General Assemblies to unanimously pass a resolution which called for closer contact between the \"three sisters,\" paving the path towards the formation of the International Design Alliance (IDA), which was eventually established in 2005. Guy-A Schockaert from Belgium became President and Mary V. Mullin was again elected Secretary General.\n\nIn early 1999, Mary V. Mullin resigned as the Icograda Secretary General and Director of the Icograda Secretariat after serving 12 years in the position. The Secretariat relocated from London to Brussels under the new leadership of Thierry van Kerm who was appointed as Icograda Director in June of the same year.\n\n\"Sydney Design 99: Viewpoints in Time\", the 4th joint Icograda/ICSID/IFI Congress took place from 26–29 September in Sydney, Australia. Around 1 400 delegates from 45 countries attended the event. The 18th Icograda General Assembly followed immediately after the Congress, 30 September-1 October in Sydney. At the Assembly, David Grossman from Israel become President and Martha Bateman from South Africa was elected as Secretary General (Bateman was only the second person from Africa to serve on the Executive Board and the first to serve in a senior position). At this GA the Council officially separated the positions of Secretary General and Director of the Secretariat (paid position) functions. In addition, the GA ratified a Joint motion of Icograda, ICSID and IFI to establish Design for the World. The Icograda President's Award was presented to Federico Mayor, Director-General of UNESCO for the organisation's long track-record in supporting Icograda's activities.\n\nAt the 17th Icograda General Assembly a motion was ratified to hold a special millennium congress in October 2000 in Seoul, South Korea. The theme was \"Oullim\", meaning 'great harmony'. The event was attended by around 1,600 delegates and was also the first Icograda event to be webcast live on the Internet. The Congress also included the launch of the \"Icograda Education Manifesto 2000\", which was published in 17 languages.\n\nIn 2001, Icograda held regional meetings in Zagreb, Croatia in April, and in La Habana, Cuba in June. The meeting in Cuba coincided with the establishment of a new series of seminars entitled \"Design Perspectives\" which attracted 335 delegates from 23 countries. In September 2001, the \"Continental Shift 2001: World Design Convergence\" Congress took place in Johannesburg, South Africa. This was the first time that Icograda hosted a congress in Africa and also the first joint Icograda/IFI Congress. The Congress opened on 11 September, only hours after the 9/11 terror attacks in the USA. At the 19th Icograda General Assembly (15–16 September, Johannesburg), Robert L. Peters (Canada) became President and Tiffany Turkington (South Africa) was elected Secretary General. One of the most important agenda items was the adoption of an Icograda/ICSID Joint Resolution to establish a joint committee to study institutionalised collaboration. In November of the same year, the first Icograda Design Week was held in Melbourne, Australia. The Design Week included a \"Design Perspectives\" seminar and Regional Meeting. The year also saw the publication of \"Masters of the 20th century design: Icograda Hall of Fame (1974-1999)\" (book and CD-ROM), edited and designed by Mervyn Kurlansky, published by Graphis Inc. The book provided an in-depth view of the Icograda Student Seminars which had been held annually in London from 1973 to 1999.\n\nIn March 2002, the Icograda Design Week took place in Vancouver, British Columbia, Canada and consisted of the \"Environs'002: Design Without Borders\" \"Design Perspectives\" seminar and a North American Regional Meeting. Smaller versions of the \"Design Perspectives\" seminar were also hosted in Victoria, Canada, as well as in Seattle, USA. The Icograda Design Week in Brno took place from 17–21 June in the Czech Republic. The event included \"Over the fence: Design in Central and Eastern Europe\" \"Design Perspectives\" seminar, a Regional Meeting attended by delegates from eight countries, the \"Identity/Integrity Icograda Conference\", \"East Meets West, Icograda Student Workshop\", a symposium and inauguration of the Icograda Education Network (IEN) and a workshop to conceptualise the formation of the International Design Media Network (IDMN). The Design Week was further preceded by a joint meeting of the boards of Icograda, ICSID and IFI, resulting in the conceptualisation of the International Design Alliance (IDA) and the Host City Project to establish a joint Icograda/ICSID Secretariat.\n\nIn September of the same year, the Icograda board embarked on a three-week visit to China and Taiwan where they presented a \"Design Perspectives\" seminar in Beijing, \"logo2002: Identity and Communication Conference,\" and met with various Chinese design stakeholders aiming to seed the formation of new professional associations. In addition Icograda presented the \"Branding and Innovalue\" seminar and an Asian Regional Meeting in Taipei, followed by a Design Education Symposium and Student Workshop in Kaohsiung, Taiwan. Furthermore, the Icograda, ICSID and IFI presidents met on several occasions in Hornbaek, Denmark, to develop a Joint Resolution aimed at establishing the International Design Alliance and launching the Host City Project to establish a joint Secretariat in one location.\n\nThe year's activities ended with the hosting of a Regional Meeting, and \"Graphic Design for Social Causes\" workshop co-hosted by Design for the World as well as a \"Design Perspectives\" Seminar in Barcelona, Spain which attracted delegates from 15 countries.\n\nIn January 2003 Icograda initiated two surveys focusing on \"Design for Social Causes\" and \"Members’ interest and activity in sustainability issues\". In March, the Board visited India where they met with various Indian design stakeholders in Mumbai to seed the formation of new associations, followed by further meetings and co-hosting the \"Brands-Identities-Graphics 2003\", Icograda \"Design Perspective\" Seminar in Ahmedabad. The Icograda Education Network Conference and Assembly of Icograda Education Network was held June in Brighton, UK. These events coincided with the signing of an agreement to establish the Icograda Archive at the Design History Research Centre Archives at the University of Brighton, which resulted in 145.11 linear metres of official documentation and publications, including c. 1,500 posters and c. 800 books and journals being transferred to University of Brighton.\n\nThe \"VISUALOGUE: Icograda World Design Congress\" took place from 8–11 October 2003 in Nagoya, Japan, preceded by the \"Icograda Education Network Symposium\". More than 80 speakers and around 3 700 delegates from 48 countries participated in these events. At the 20th Icograda General Assembly which followed the Congress (the first GA to be held in Asia), representatives from 57 members attended in addition to observers from 16 related organisations, with official membership increasing to 80 associations from 57 countries. The GA agenda included the ratification of the Icograda/ICSID \"Hornbaek Joint Resolution\" to establish the International Design Alliance (IDA), and the election of the 2003−2005 Icograda Board, the first in history to represent six continents. Mervyn Kurlansky became President but no candidate was nominated for the Secretary General position.\n\nIn January 2004 the \"Icograda Design Week in Istanbul\" was held in Turkey. The Week's programme included \"6 Alfabe, Icograda Student Workshop\", an IEN Symposium, Regional Meeting, \"Building Bridges: Icograda Design Conference\", \"Design Perspectives Icograda Regional Design Seminar\", as well as the launch of the Icograda Design Media Network (IDMN). This was followed by the Icograda Design Week in São Paulo, Brazil, in April of the same year. The Week's programme included \"The Language of the City Student Workshop\", a Latin American Regional Meeting, \"Design in Latin America Regional Design Seminar\", and the \"Fronteiras! Icograda Design Conference\". In August, Icograda and ICSID hosted a Joint Board Meeting and evaluation of the final six bids for the Joint Secretariat in Essen, Germany. Montreal's bid was selected as most ambitious and beneficial, followed by a site visit by representatives of Icograda and ICSID in September to initiate the final negotiations regarding relocation of the two organisations’ Secretariates to Montreal.\n\nIn January 2005, Icograda and ICSID signed a 10-year contract with Montreal International (MI) to relocate to a shared office in Montreal, sponsored by MI. After six years in Brussels, the Secretariat relocated to Montreal. In February, Thierry Van Kerm resigned and Brenda Sanderson was appointed as Icograda Director, officially moving into the new shared Secretariat in May. During the same time, ICSID and Icograda collaborate on the first international \"Women in Design\" exploratory study as well as on the \"Interdesign on Sustainable Rural Transport – Technology for Developing Countries\", which was hosted in April in Rustenburg, South Africa. The book, \"Worldwide Identity: Inspired Design from Forty Countries\", written and produced by Robert L. Peters, published by Rockport, was also published and INDIGO (International Indigenous Design Network) was conceptualised.\n\n\"Era '05\" was another major collaborative effort by Icograda, ICSID and IFI to stage a fifth Joint Congress. The theme for the main Congress was 'The Changing Role and Challenges of Design' which attracted participation by designers, business leaders, politicians, legal practitioners and social scientists. A milestone in the cooperative efforts between leading design organisations in Denmark, Norway, Sweden and Finland, \"Era 05\" was built on the ideal that as a creative force, design and designers were integral in helping to address the challenges all faced and identifying solutions to cope with a rapidly changing and increasingly complex world. In September, \"Era '05\" began with small-scale pre-congress events hosted in Oslo, Helsinki and Gothenburg. The main Congress took place in Copenhagen and featured 126 speakers from 27 countries and was attended by around 900 delegates. At the 21st Icograda General Assembly that followed, Jacques Lange (South Africa) became president and Lise Vejse Klint (Denmark) was elected Secretary General. The newly elected board was now the most geographically diverse in Icograda's history with members from South Africa, South Korea, Canada, Denmark, Lebanon, USA, Australia and Brazil. Craig Halgreen received the Icograda President's Award for supporting and sustaining Sappi's \"Ideas that Matter\" program.\n\nIn 2006, Icograda co-organised three Design Weeks. In January, \"So Tiny, So Many: Icograda Design Week in Hong Kong\" took place in China and consisted of a student workshop, an evening lecture series and a Regional Meeting. In July, \"Defining design on a changing planet: Icograda Design Week in Seattle\", took place in the USA. The Week included a student workshop that focussed on the UN Millennium Development Goals (in collaboration with United Nations Department of Economic and Social Affairs' Programme on Youth, a North American Regional Meeting, an \"Icograda Design Perspectives\" seminar, \"Defining Design on Changing Planet: Icograda Conference\", as well as the launch of the \"+design\" programme. The \"Icograda Design Week in South Africa\" took place in September in Pretoria and Johannesburg. It included the \"Design for Development Lekgotla\" (in collaboration with ICSID member SABS), the \"Icograda/think Conference 2006\", and the \"IEN Colloquium on Virtual Design Archives\". In May of the same year, the presidents of Icograda, ICSID and IFI met in Montreal for the signing of an IDA joint venture agreement between Icograda and ICSID, together with an agreement with IFI to host the 2011 Joint Congress under the banner of the IDA.\n\nIn 2007, Icograda launched the \"IDA World Design Survey Pilot Project\" which aimed to map design development in several global regions based on a standardised set of indicators. The project was developed with advisory support from UNESCO Centre for Statistics and UNCTAD. Icograda also hosted two major Design Weeks. \"Design Local: Stop at all stations, Icograda Design Week in Mumbai\", India took place from 5–9 February and included a student workshop and international conference. \"Design/Culture: Icograda World Design Congress in La Habana\" was held in Cuba from 20–26 October. It included an education conference at which 119 papers were presented originating from 25 countries (papers can be accessed http://www.icograda.org/events/event_archive/articles1014.htm), as well as the \"Posters for Cultural Diversity\" international poster exhibition, organised in collaboration with Prografica and UNESCO. At the 22nd Icograda General Assembly that followed, Don Ryun Chang (South Korea) became president and Lise Vejse Klint (Denmark) was again elected Secretary General. Highlights from the GA included the ratification of update definition of the profession from ‘Graphic Design’ to ‘Communication Design', approval of revisions to \"Regulations and best practice for organizing design awards competitions\", the introduction of \"Icograda best practice statement on soliciting work from professional communication designers\", substantial revisions to the \"Articles of Incorporation and Bylaws\" to include a ‘one member one vote’ system, and granting of voting rights to Education Members (excluding on professional practice issues), establishment of an IDA Taskforce to explore options for future development, launch of INDIGO (Indigenous Design Network) as part of an IDA portfolio led by Icograda. In addition, the Icograda Foundation Lecture was re-introduced on 25 October at Museo Nacional Bellas Artes, La Habana which included the presentation of the Icograda President's Award which went to Guy-A Schockaert for his tireless advocacy of professional design organisations and his commitment to Icograda, and the presentation of the first Icograda Education Award which was bestowed on Hazel Gamec from the Wanganui School of Design, New Zealand. \n\nIn 2008, Icograda again held two Design Weeks. \"Color Value, Icograda Design Week in Daegu\", South Korea took place in July, and in October \"Multiverso: Icograda Design Week in Torino\" Italy. In the same year, Icograda conducted the first survey of European member organisations which was followed up by a more in-depth study in 2010.\n\nThe \"Mousharaka: Icograda Design Week in Doha\", Qatar, was held from 28 February – 5 March 2009, and included an international conference, student exhibitions and a Regional Meeting. In October, the \"XIN: Icograda World Design Congress in Beijing\" took place China. This was the last dedicated Icograda Congress and was attended by 1 750 delegates from 48 countries. The Congress was followed by the \"Icograda Education Conference\", 29–30 October, Beijing. At the 23rd Icograda General Assembly 125 delegates from 45 countries attended, making it the largest and most representative in Icograda's GA history. Highlights of the Assembly included the ratification of a resolution on Sustainable Practice of Communication Design, as well as the adoption of new Best Practice documents, \"Regulations and Best Practices for Design Exhibitions\" and \"Jury Guidelines and Guidelines for Organising Design Conferences\". Russell Kennedy (Australia) became president and Grégoire Serikoff, (France) was elected Secretary General (he resigned in 2011). The Icograda President's Award went to Pan Gongkai for helping to redefine the direction of art and design education in China, as well as to Robert L. Peters for his many achievements as a Board member and as a member of the founding executive of the International Design Alliance (IDA). The Icograda Education Award was awarded to Ahn Sang-Soo from Hongik University, South Korea.\n\n\nIn 2011, the inaugural IDA Congress replaced the biannual Icograda World Design Congress as well as the Joint Congresses of Icograda, ICSID and IFI which have taken place every six years since 1981. The first IDA Congress took place in Taipei with the theme \"Design at the Edges\". The 24th Icograda General Assembly took place in Taipei from 27–28 October.\n\nico-D is a member of the steering committee that's organising the world's first ever full design dedicated summit named the World Design Summit 2016 which will be hosted in Montreal in October 2016. ico-D also continues to partner with Adobe Education in the Adobe Design Achievement Awards (ADAA).\n\nico-D's Executive Board consists of individuals who are duly nominated and elected by ico-D's Member organisations at the biennial ico-D General Assembly. Members of the Executive Board serve in a volunteer position and donate their time and expertise to further Icograda's mandate. Board meetings are typically held four times a year in different locations around the world, usually in conjunction with regional meetings, seminars or other scheduled design events.\n\n\nSince 1963 and 2016, 26 ico-D boards have donated their leadership skills and time to furthering ico-D's goals and objectives. View a complete list of Executive Board members here.\n\nBeginning in June 2005, the Secretariat for ico-D was headquartered in Montreal's International Quartier district (Quebec, Canada). In June 2015, the Secretariat moved to a new office building on 456 rue de la Gauchetière Ouest.\n\nCurrent Secretariat staff:\n\n\n"}
{"id": "8075025", "url": "https://en.wikipedia.org/wiki?curid=8075025", "title": "Indication (medicine)", "text": "Indication (medicine)\n\nIn medicine, an indication is a valid reason to use a certain test, medication, procedure, or surgery. There can be multiple indications to use a procedure or medication. An indication can commonly be confused with the term diagnosis. A diagnosis is a particular [medical] condition while an indication is a reason for use. The opposite of an indication is a contraindication, a reason to withhold a certain medical treatment because the risks of treatment clearly outweigh the benefits.\n\nIn the United States, medication indications are determined by the FDA and are grouped into either labeled indications, which are FDA-approved or off-label indications which are not approved. Currently in the US, including the indication or purpose of use for a drug is not required on prescription labels. However, manufacturers of prescription drugs are required to include an \"Indications and usage\" section on the prescribing information or package insert. \n\nMost countries and jurisdictions have a licensing body whose duty is to determine whether to approve a drug for a specific indication, based on the relative \"safety\" of the drug and its \"efficacy\" for the particular use. In the United States, indications for medications are regulated by the Food and Drug Administration (FDA), and are included in the package insert under the phrase \"Indications and Usage\". The European Medicines Agency (EMA) holds this responsibility for centrally authorized drugs in the European Union.\n\nIn the United States there are label indications and off-label indications\n\n\"Label indications:\"\n\nMedication that have label indications mean that they were approved by the FDA. This means that they are clinically significant for the indication and manufacturers are allowed to market their drug for the indication. A drug can have more than one FDA labeled indication, this means they can be used for multiple medical conditions. As the evidence and consensus for use of the drug increases and strengthens, its class of indication is improved. Preferred drugs (and other treatments) are also referred to a \"first line\" or \"primary\" while others are called \"second line\", \"third line\" etc. A drug may be indicated as an \"adjunct\" or \"adjuvant\", added to a first line drug.\n\n\"Off-label:\"\n\nOff-label indications are drugs that are used for medical indications that have not been approved by the FDA. Off label indications often have some clinical significance to back the use, but they have not gone through the extensive testing required by the FDA to have an official labeled indication. Drug companies can not provide any official medication information (e.g package inserts) for off label indications. \n\nThe purpose for adding FDA-approved indications in the United States is to ensure that healthcare providers can easily identify appropriate use of drug therapy. Gaining FDA approval is based on the body of scientific evidence supporting the effectiveness of a drug treatment. The scientific evidence is gathered in the first 3 steps in the drug development process: discovery and development, pre-clinical research (testing safety), and clinical research (testing efficacy). After there is adequate completion of research and development phases by the drug companies, they send a New Drug Application (NDA) for approval to the FDA's Center for Drug Evaluation and Research (CDER) and the proposed scientific evidence for use in an intended population is evaluated by a team of physicians, statisticians, chemists, pharmacologists, and other scientists. Essentially, if it is found that there is substantial evidence that benefits of treatment outweigh the risks, the proposed labeling in the package insert is appropriate, and the manufacturing process is safe and adequate, then the drug is approved to go to market under that now FDA-approved indication. Even after approval, the FDA CDER continues to do postmarking surveillance of the drug through MedWatch and FDA Adverse Event Reporting System (FAERS).\n\nIndications can impact pricing of medications through Value-based Pricing, also known as indication specific pricing or indication value-based pricing. Since drugs can be used for different indications, this form of pricing would set different prices for each indication based on the value the drug offers for whatever it is being used to treat. This pricing scheme is often discussed in relation to oncology drugs, which are costly and not always successful in treating patients. Oncology drugs can be used for multiple different types of cancers so by applying indication-specific pricing, the drug would be priced based on how effective it is for treating each type of cancer. If the drug is more effective for Cancer A than Cancer B, then the patient taking the drug to treat Cancer A will pay more than the person using it for Cancer B because they are getting more value out of it. \n\nCurrently, most medications in the United States are priced the same regardless of what they're being used for or how effective they are at improving outcomes. Companies like CVS and Express Scripts in the US have began implementing pricing based on indication and in countries like Italy, similar forms of pricing are already being used. For example, Express Scripts' \"Oncology Care Value Program,\" uses indication-based pricing for certain oncology medications and was launched in 2016. Italy on the other hand, uses a model similar to indication-based pricing where the amount the hospital pays for certain drugs varies based on what it's used for. Patients can receive reimbursements for treatments based on their response and either be fully or partially refunded. Italy's reimbursement system is run by AIFA, the Italian Medicines Agency, which is the national authority that regulates drugs in Italy.\n\nThere has been some thought on incorporating the indication of use on prescription drug labels as an approach to improve patient understanding of the medications they are on. This information can help healthcare providers reduce medication errors related to drugs that may look and sound alike. Knowing the indication of the drug can also help providers determine if the dose of the drug is appropriate per indication, and this can greatly improve patient safety and drug effectiveness.\n\nHowever, there are still some challenges with incorporating the indication of use on prescription drug labels. Revealing the indication of use on prescription drug labels can breach patient confidentiality since the label will disclose private information publicly. Some medications can also be used for multiple diseases and one disease may have multiple medications for its prevention or treatment, therefore adding an indication on prescription labels in these cases may cause some confusion and may not be able to actually fit onto the label. \n\nEach test has its own indications and contraindications, but in a simplified fashion, how much a test is indicated for an individual depends largely on its \"net benefit\" for that individual, which largely depends on the absolute difference between pre- and post-test probability of conditions (such as diseases) that the test is expected to achieve. Additional factors that influence a decision whether a medical test should be performed or not include: cost of the test, time taken for the test or other practical or administrative aspects. The possible benefits of a diagnostic test may also be weighed against the costs of unnecessary tests and resulting unnecessary follow-up and possibly even unnecessary treatment of incidental findings. Also, even if not beneficial for the individual being tested, the results may be useful for the establishment of statistics in order to improve health care for other individuals.\n"}
{"id": "676485", "url": "https://en.wikipedia.org/wiki?curid=676485", "title": "Iroquoian languages", "text": "Iroquoian languages\n\nThe Iroquoian languages are a language family of indigenous peoples of North America. They are known for their general lack of labial consonants. The Iroquoian languages are polysynthetic and head-marking.\n\nToday, all surviving Iroquoian languages except Cherokee in Oklahoma and Mohawk are severely endangered or critically endangered, with only a few elderly speakers remaining. Cherokee in North Carolina is considered severely endangered.\n\n(†) — language extinct\n\nEvidence is emerging that what has been called the \"Laurentian\" language appears to be more than one dialect or language. Ethnographic and linguistic field work with the Wyandot tribal elders (Barbeau 1960) yielded enough documentation for scholars to characterize and classify the Huron and Petun languages.\n\nThe languages of the tribes that constituted the tiny Wenrohronon, the powerful Susquehannock and the confederations of the Neutral Nation and the Erie Nation are very poorly documented. They are historically grouped together, and geographically the Wenro's range on the eastern end of Lake Erie placed them between the two much larger confederations. To the east of the Wenro, beyond the Genesee Gorge, were the lands of the Iroquois and southeast, beyond the headwaters of the Allegheny River, lay the Susquehannocks. The Susquehannocks and Erie were militarily powerful and respected by neighboring tribes. These groups were called \"Atiwandaronk,\" meaning 'they who understand the language' by the surviving Huron (Wyandot people). By 1660 all of these peoples but the Susquehannocks and Iroquois were defeated and scattered, migrating to form new tribes or to be adopted into others—the practice of adopting valiant enemies into the tribe was a common cultural tradition of the Iroquoian peoples.\n\nThe group known as the Meherrin were neighbors to the Tuscarora and the Nottoway (Binford 1967) in the American South and may have spoken an Iroquoian language. There is not enough data to determine this with certainty.\n\nAttempts to link the Iroquoian, Siouan, and Caddoan languages in a Macro-Siouan family are suggestive but remain unproven (Mithun 1999:305).\n\nAs of 2012, a program in Iroquois linguistics at Syracuse University, the \"Certificate in Iroquois Linguistics for Language Learners\", is designed for students and language teachers working in language revitalization.\n\nSix Nations Polytechnic in Ohsweken, Ontario offers Ogwehoweh language Diploma and Degree Programs in Mohawk or Cayuga.\n\n\n\n"}
{"id": "21168623", "url": "https://en.wikipedia.org/wiki?curid=21168623", "title": "Layering (linguistics)", "text": "Layering (linguistics)\n\nLayering in linguistics refers to one of the five principles by which grammaticalisation can be detected while it is taking place. The other four are: divergence, specialisation, persistence, and de-categorialisation.\n\nLayering refers to the phenomenon that a language can have and develop multiple expressions for the same function, that language, in the \"lexical\" as well as in the \"grammatical\" domain, tolerates and permanently creates multiple synonymy. \"Within a broad functional domain, new layers are continually emerging. As this happens, the older layers are not necessarily discarded, but may remain to coexist with and interact with the newer layers.\"\n\nDuring the process of grammaticalisation, new layers are added to older ones whereby the functional domain is broadened, i.e. several items may fulfil the same linguistic function.\nAn example from English: 'I am going to study' / 'I will study' / 'I shall study'.\n\n"}
{"id": "50364007", "url": "https://en.wikipedia.org/wiki?curid=50364007", "title": "Life Is Strange (film)", "text": "Life Is Strange (film)\n\nLife Is Strange is a 2012 American documentary film of interviews giving an oral history portrait of the life of pre-World War II European Jews by filmmaker Isaac Hertz. The film juxtaposes the childhood memories of interviewees with archival footage. Interviewees include Israeli president Shimon Peres, author Uri Orlev, academics, Nobel laureates, and friends of the filmmaker, Walter Kohn, Robert J. Aumann, Alain Jakubowicz, Sammy Grundwerg, Ron Samuels, Zachary Cirino, Chaim Hertz, Arianne Brown, Artem Zuev and Jean-Michel Guirao.\n\nThe film is 96 minutes long and explores pre-war Yiddish culture and the experience of living in a world very different from what it was like in the past. It has received criticism for the unclear reasoning behind the assortment of people interviewed.\n"}
{"id": "376588", "url": "https://en.wikipedia.org/wiki?curid=376588", "title": "List of linguistic example sentences", "text": "List of linguistic example sentences\n\nThe following is a partial list of linguistic example sentences illustrating various linguistic phenomena.\n\nDifferent types of ambiguity which are possible in language.\n\nDemonstrations of words which have multiple meanings dependent on context.\n\nDemonstrations of ambiguity between alternative syntactic structures underlying a sentence.\n\nDemonstrations of how \"incremental\" and (at least partially) \"local\" syntactic parsing leads to infelicitous constructions and interpretations.\n\n\n\nPunctuation can be used to introduce ambiguity or misunderstandings where none needed to exist. One well known example, for comedic effect, is from \"A Midsummer Night's Dream\" by William Shakespeare (ignore the punctuation to get the alternate reading).\n\nThis adjectival order is an example of the \"Royal Order of Adjectives\".\n\nSome prescriptive grammar prohibits \"preposition stranding\": ending sentences with prepositions.\n\n\n\n\nSentences with unexpected endings.\nComparative illusion:\n\nDemonstrations of sentences which are unlikely to have ever been said, although the combinatorial complexity of the linguistic system makes them possible.\n\nDemonstrations of sentences where the semantic interpretation is bound to context or knowledge of the world.\n\nConditionals where the prejacent (\"if\" clause) is not strictly required for the consequent to be true.\n\n\n\n\n\n\n\n\n"}
{"id": "3750838", "url": "https://en.wikipedia.org/wiki?curid=3750838", "title": "Mundart des Kürzungsgebiets", "text": "Mundart des Kürzungsgebiets\n\nThe Mundart des Kürzungsgebiets is a subdialect of Low Prussian, part of Low German, spoken in today's Poland. \nIn 1918, it was spoken in East Prussia and West Prussia in their respective then borders. Mundart des Kürzungsgebiets was spoken around Braniewo and Frombork and had a border to \nNatangian, Westkäslausch, Mundart der Elbinger Höhe and Oberländisch. \n\nPart of its Southern border was undetermined by political or religious borders.\n"}
{"id": "2969445", "url": "https://en.wikipedia.org/wiki?curid=2969445", "title": "Normative statement", "text": "Normative statement\n\nIn economics and philosophy, a normative statement expresses a value judgment about whether a situation is desirable or undesirable. It looks at the world as it \"should\" be. \"The world would be a better place if the moon were made of green cheese\" is a normative statement because it expresses a judgment about what ought to be. Normative statements are characterised by the modal verbs \"should\", \"would\", \"could\" or \"must\". They form the basis of normative economics, and are the opposite of positive statements.\n\n"}
{"id": "21441787", "url": "https://en.wikipedia.org/wiki?curid=21441787", "title": "Oxford University Phonetics Lab", "text": "Oxford University Phonetics Lab\n\nThe Phonetics Laboratory is the phonetics laboratory at the University of Oxford, England. It is located at 41 Wellington Square, Oxford.\n\nThe laboratory focusses on experimental tests of linguistic assumptions and empirical linguistics. It provides teaching at the undergraduate and graduate level. Research students in the laboratory are normally reading for a higher degree in Experimental Linguistics, though students from other disciplines touching on the subject of speech are sometimes based in Phonetics.\n\nThe Phonetics Laboratory was established in 1980. It occupies the basement of 41 Wellington Square, a mid-Victorian brick building, expanded since. It has experimental areas (sound-insulated recording booths), and general experimental space. The lab also supports signal processing research via software, speech corpora, and processor clusters.\n\nThe Phonetics Lab has published collections of working papers (ongoing research and research getting ready for peer-reviewed publication) since 1996. The 2009 Working Papers are titled \"Papers in Phonetics and Computational Linguistics.\"\n\n"}
{"id": "4621146", "url": "https://en.wikipedia.org/wiki?curid=4621146", "title": "ProEnglish", "text": "ProEnglish\n\nProEnglish is an American nonprofit organization that advocates making English the official language of government operations at all levels of government in the United States. The group supports making English the only official language of the United States. It is organized as a self-governing project of US Inc., a 501(c)(3) educational foundation in Petoskey MI, concerned with immigration, assimilation, and conservation issues. The group has also campaigned against immigration reform and bilingual education. \n\nThe Southern Poverty Law Center (SPLC) and Anti-Defamation League, which track extremist groups in the United States, identify the group as an anti-immigrant group. The SPLC designated the organization as a hate group.\n\nThe group was founded in 1994 as English Language Advocates., by John Tanton, M.D., a Petoskey, Michigan eye surgeon and a leading figure in the national immigration reform movement, along with three former board members of U.S. English who had resigned to protest that organization’s decision not to defend an Arizona official English law it had sponsored. Tanton co-founded US English in 1983 with U.S. Senator S.I. Hayakawa of California, but had resigned from it earlier following the disclosure of a joke deemed racially insensitive in an internal memo he had written). The organization is part of Tanton's broader \"loose-knit\" network of anti-immigration organizations; others include Californians for Population Stabilization, the Center for Immigration Studies, NumbersUSA, and Social Contract Press. As of 2015, ProEnglish \"is one of the few remaining groups in Tanton's network in which he remains actively involved.\" ProEnglish is a project of US Inc., a Petoskey, Michigan-based 501(c)(3) group that is also part of the Tanton network.\nThe group was originally based in Arlington, Virginia, where it shared office space with NumbersUSA. Its headquarters are now located in Washington, D.C. In 2010 ELA changed its name to ProEnglish. Robert D. Park was the first chair of the group. He was succeeded by Rosalie Pedalino Porter, EdD, a naturalized U.S. citizen and nationally recognized authority on the failures of bilingual education.\n\nThe group's former executive directors are K.C. McAlpin and Robert \"Bob\" Vandervoort. In 2016, Sam Pimm, former executive director of Young Americans for Freedom and former executive director of a pro-Ben Carson super PAC, became executive director of the group. Subsequently, Stephen D. Guschov, a lawyer who formerly worked at Liberty Counsel, because executive director of the group.\n\nProEnglish has been a major part of the \"English-as-official-language movement.\" The group also has opposed comprehensive immigration reform. The chief purpose of the organization at the time of its founding was to defend the Arizona \"Official English\" ballot initiative, which was adopted in 1988, overturned by the Arizona Supreme Court in 1998, and re-enacted in revised form by Arizona voters in 2006. The group has also supported federal English-only legislation, specifically the English Language Unity Act. In addition to seeking the enactment of laws and policies declaring English to be the official language, ProEnglish \"seeks to end bilingual education, repeal federal mandates for the translation of government documents and voting ballots in languages other than English.\" ProEnglish also opposes Puerto Rican statehood unless Puerto Rico were to adopt English as its official language.\n\nThe group's reported ties to the white nationalist movement have drawn scrutiny. The Anti-Defamation League wrote in 2014 that the group had a \"nativist agenda and xenophobic origins and ties.\" Robert Vandervoort of Illinois, the former executive director of ProEnglish, was head of the Chicagoland Friends of \"American Renaissance\", the racist magazine led by Jared Taylor that serves as an outlet for white nationalist ideology. In 2012, ProEnglish hosted a panel discussion at the Conservative Political Action Conference (CPAC) on \"The Failure of Multiculturalism,\" on which one of the panelists was VDARE founder Peter Brimelow. The Southern Poverty Law Center, which tracks extremist groups in the United States, designated the group as a hate group in its 2014, 2015, and 2016 annual reports. The group has dismissed such criticism.\n\nProEnglish was a major backer of the unsuccessful 2009 Nashville Charter Amendment 1, a local \"English First\" ballot referendum in Nashville, Tennessee, which would have generally required government communication and publications to be printed in English only. ProEnglish donated $82,500, about 92% of the total amount raised by the referendum's supporters. The referendum was rejected by Nashville voters. In 2012, ProEnglish was the leading force behind a successful effort to make English the official language of Frederick County, Maryland; the county enacted an ordinance closely based on one drafted by the group. However, in 2015, the country repealed the ordinance, marking a defeat for the organization.\n\nProEnglish has also worked for the adoption of official English laws at the local level, most notably in Hazleton, Pennsylvania.\n\nIn 2013, ProEnglish vocally opposed the comprehensive immigration reform bill sponsored by the \"Gang of Eight,\" a bipartisan group of U.S. senators. ProEnglish carried out a radio ad campaign against U.S. Senator Lindsey Graham, who was part of the Gang of Eight.\n\nIn 2014, ProEnglish criticized The Coca-Cola Company for airing a Super Bowl commercial that showed people of different ethnicities singing \"America, the Beautiful\" in a variety of languages. ProEnglish condemned Coca-Cola (saying the ad fostered \"disunity\") and urged its supporters to contact the company to express opposition.\n\nIn 2017, ProEnglish endorsed the section of the Reforming American Immigration for a Strong Economy (RAISE) Act that calls for a merit-based legal immigration system in which applicants would get points for speaking English.In 2017, ProEnglish endorsed the section of the Reforming American Immigration for a Strong Economy (RAISE) Act that calls for a merit-based legal immigration system in which applicants would get points for speaking English.\n\nIn 2005, ProEnglish was helping pay the legal fees of at least two employers who had an \"English-only rule\" requiring employees to speak only English while on the job. ProEnglish paid the legal fees of Terri Bennett, a former nursing student at Pima Community College (PCC) in Tucson, Arizona, \"who claimed she was wrongly suspended for complaining when fellow students spoke Spanish to one another in class.\" At trial, the evidence showed that Bennett had called Hispanic classmates \"spics, beaners and illegals\" and the Spanish language \"gibberish.\" A jury unanimously rejected the Bennett's claims, and in 2015 ordered her to pay $111,000 in attorney's fees to PCC.\n\nIn 2008, ProEnglish, along with the Pacific Legal Foundation, filed a lawsuit in federal court challenging 2004 U.S. Department of Health and Human Service regulations that required federally funded healthcare providers to provide translation services for patients who do not speak English. The challengers claimed that the regulations were an \"illegal intrusion\" on healthcare providers. U.S. District Judge Barry Ted Moskowitz dismissed the suit in 2009.\n\nIn \"EEOC v. Kidmans\" (2005), ProEnglish helped fund the litigation costs of a small drive-in restaurant in Page, Arizona, that was sued by the Equal Employment Opportunity Commission after it refused to retract an English-on-the-job rule. The restaurant said that the rule was adopted to stop \"trash talking\" in the Navajo language among employees, most of whom are Navajo. The EEOC and the restaurant owners ultimately negotiated a settlement, in which the employees \"may require employees to speak English while dealing with the public, but not at other times.\"\n\nProEnglish opposes the method of teaching English language learning (ELL) students that teaches them basic courses in their native language while they learn English, known as ‘bilingual education,’ as a failed 40-year experiment. Instead, it favors the method known as ‘structured English immersion,’ in which ELLs are taught in English while being given supplemental learning assistance on the side.\n\nAccordingly, it backed successful citizen initiative that ended bilingual education programs in California (1998), Arizona (2000), and Massachusetts (2002), as well as one unsuccessful such citizen’s initiative in Colorado (2002). ProEnglish Chair Dr. Rosalie Porter co-chaired the English for the Children initiative in Massachusetts that passed with 68 percent of the vote. Much research comparing students in bilingual programs to students in structured English immersion consistently shows far better results in English immersion classes. \n\nExecutive Order 13166, also known as “Improving Access to Services for Persons with Limited English Proficiency,” states that any entity receiving federal funds must provide translations or interpreters for the services it provides in the language spoken by persons likely to use those services. It states that failure to do so is likely to constitute “discrimination on the basis of national origin” as prohibited by Title VI of the 1964 Civil Rights law.\nAn executive order signed by President Bill Clinton on August 11, 2000, the order directed all federal agencies to draft plans to “improve access to federally conducted or federally assisted programs for persons who, as a result of national origin, are limited in their English proficiency.” The mandate also applies to all state and local government agencies and their subdivisions, and government contractors that receive federal funds. \n"}
{"id": "23007105", "url": "https://en.wikipedia.org/wiki?curid=23007105", "title": "RTFB", "text": "RTFB\n\nThe most usual meaning is in reference to instruction manuals, and means \"Read the F'ing Book\" more politely rendered as \"Read the Fine Book.\"\n\n\"RTFB\" is a parodical extension to the Internet slang term \"RTFM\" and its extension \"RTFS\".\n\n\"RTFB\" is short for \"read the fucking binary\", in a similar way as \"RTFM\" means \"read the fucking manual\" and \"RTFS\" means \"read the fucking source\". While both \"RTFM\" and \"RTFS\" have legitimate uses, \"RTFB\" is usually intended only as a parody of them. While any user, no matter how tech-savvy, can be expected to read the manual, and experienced computer programmers can deduce the working logic of a computer program directly from reading its human-readable source code, \"RTFB\" expects users to directly read the machine language code that the microprocessor executes natively. Today's microprocessors are usually so complex that, except for trivial textbook programs, this is a difficult task even for experienced programmers, and because of anti-reverse-engineering laws, may even be illegal.\n\nRTFB has also been used meaning \"Read the fucking Bible\" in relation to WWJD (What Would Jesus Do) jokes.\n\nA more recent use, applied to members of the legislative branch, means \"read the fucking bill\". It is a reaction to the practice of passing massive legislation where the final bill is available for only a few hours before it is voted upon.\n\n"}
{"id": "22705150", "url": "https://en.wikipedia.org/wiki?curid=22705150", "title": "Referring expression generation", "text": "Referring expression generation\n\nReferring expression generation (REG) is the subtask of natural language generation (NLG) that received most scholarly attention. While NLG is concerned with the conversion of non-linguistic information into natural language, REG focuses only on the creation of referring expressions (noun phrases) that identify specific entities called \"targets\".\n\nThis task can be split into two sections. The \"content selection\" part determines which set of properties distinguish the intended target and the \"linguistic realization\" part defines how these properties are translated into natural language.\nA variety of algorithms have been developed in the NLG community to generate different types of referring expressions.\n\nA referring expression (RE), in linguistics, is any noun phrase, or surrogate for a noun phrase, whose function in discourse is to \"identify\" some individual object (thing, being, event...) The technical terminology for \"identify\" differs a great deal from one school of linguistics to another. The most widespread term is probably \"refer\", and a thing identified is a \"referent\", as for example in the work of John Lyons. In linguistics, the study of reference relations belongs to pragmatics, the study of language use, though it is also a matter of great interest to philosophers, especially those wishing to understand the nature of knowledge, perception and cognition more generally.\n\nVarious devices can be used for reference: determiners, pronouns, proper names... Reference relations can be of different kinds; referents can be in a \"real\" or imaginary world, in discourse itself, and they may be singular, plural, or collective.\n\nThe simplest type of referring expressions are pronoun such as \"he\" and \"it\". The linguistics and natural language processing communities have developed various models for predicting anaphor referents, such as centering theory, and ideally referring-expression generation would be based on such models. However most NLG systems use much simpler algorithms, for example using a pronoun if the referent was mentioned in the previous sentence (or sentential clause), and no other entity of the same gender was mentioned in this sentence.\n\nThere has been a considerable amount of research on generating definite noun phrases, such as \"the big red book\". Much of this builds on the model proposed by Dale and Reiter. This has been extended in various ways, for example Krahmer \"et al.\" present a graph-theoretic model of definite NP generation with many nice properties. In recent years a shared-task event has compared different algorithms for definite NP generation, using the TUNA corpus.\n\nRecently there has been more research on generating referring expressions for time and space. Such references tend to be imprecise (what is the exact meaning of \"tonight\"?), and also to be interpreted in different ways by different people. Hence it may be necessary to explicitly reason about false positive vs false negative tradeoffs, and even calculate the utility of different possible referring expressions in a particular task context.\n\nIdeally, a good referring expression should satisfy a number of criteria:\n\nREG goes back to the early days of NLG. One of the first approaches was done by Winograd in 1972 who developed an \"incremental\" REG algorithm for his SHRDLU program. Afterwards researchers started to model the human abilities to create referring expressions in the 1980s. This new approach to the topic was influenced by the researchers Appelt and Kronfeld who created the programs KAMP and BERTRAND and considered referring expressions as parts of bigger speech acts.\n\nSome of their most interesting findings were the fact that referring expressions can be used to add information beyond the identification of the referent as well as the influence of communicative context and the Gricean maxims on referring expressions. Furthermore, its skepticism concerning the naturalness of minimal descriptions made Appelt and Kronfeld's research a foundation of later work on REG.\n\nThe search for simple, well-defined problems changed the direction of research in the early 1990s. This new approach was led by Dale and Reiter who stressed the identification of the referent as the central goal.\nLike Appelt they discuss the connection between the Gricean maxims and referring expressions in their culminant paper in which they also propose a formal problem definition. Furthermore, Reiter and Dale discuss the Full Brevity and Greedy Heuristics algorithms as well as their Incremental Algorithm(IA) which became one of the most important algorithms in REG.\nAfter 2000 the research began to lift some of the simplifying assumptions, that had been made in early REG research in order to create more simple algorithms. Different research groups concentrated on different limitations creating several expanded algorithms. Often these extend the IA in a single perspective for example in relation to:\n\n\nMany simplifying assumptions are still in place or have just begun to be worked on. Also a combination of the different extensions has yet to be done and is called a \"non-trivial enterprise\" by Krahmer and van Deemter.\n\nAnother important change after 2000 was the increasing use of empirical studies in order to evaluate algorithms. This development took place due to the emergence of transparent corpora. Although there are still discussions about what the best evaluation metrics are, the use of experimental evaluation has already led to a better comparability of algorithms, a discussion about the goals of REG and more task-oriented research.\n\nFurthermore, research has extended its range to related topics such as the choice of \"Knowledge Representation(KR) Frameworks\". In this area the main question, which KR framework is most suitable for the use in REG remains open. The answer to this question depends on how well descriptions can be expressed or found. A lot of the potential of KR frameworks has been left unused so far.\n\nSome of the different approaches are the usage of:\n\nDale and Reiter (1995) think about referring expressions as distinguishing descriptions.\n\nThey define:\nEach entity in the domain can be characterised as a set of attribute-value pairs for example formula_1type, dogformula_2, formula_1gender, femaleformula_2 or formula_1age, 10 yearsformula_2.\n\nThe problem then is defined as follows:\n\nLet formula_7 be the intended referent, and formula_8 be the contrast set. Then, a set formula_9 of attribute–value pairs will represent a distinguishing description if the following two conditions hold:\nIn other words, to generate a referring expression one is looking for a set of properties that apply to the referent but not to the distractors.\n\nThe problem could be easily solved by conjoining all the properties of the referent which often leads to long descriptions violating the second Gricean Maxim of Quantity. Another approach would be to find the shortest distinguishing description like the Full Brevity algorithm does.\nYet in practice it is most common to instead include the condition that referring expressions produced by an algorithm should be as similar to human-produced ones as possible although this is often not explicitly mentioned.\n\nThe Full Brevity algorithm always finds a minimal distinguishing description meaning there is no shorter distinguishing description in regard to properties used.\n\nTherefore, it iterates over formula_24 and checks every description of a length of formula_25 properties until a distinguishing description is found.\n\nTwo problems arise from this way of creating referring expressions. Firstly the algorithm has a high complexity meaning it is NP-hard which makes it impractical to use. Secondly human speakers produce descriptions that are not minimal in many situations.\n\nThe Greedy Heuristics algorithm approximates the Full Brevity algorithm by iteratively adding the most distinguishing property to the description. The most distinguishing property means the property that rules out most of the remaining distractors. The Greedy Heuristics algorithm is more efficient than the Full Brevity algorithm.\n\nDale and Reiter(1995) present the following algorithm for the Greedy Heuristic:\n\nLet formula_9 be the set of properties to be realised in our description; let formula_27 be the set of properties known to be true of our intended referent formula_7 (we assume that formula_27 is non-empty); and let formula_8 be the set of distractors (the contrast set). The initial conditions are thus as follows:\n\nIn order to describe the intended referent formula_7 with respect to the contrast set formula_8, we do the\nfollowing:\n\nThe Incremental Algorithm (IA) by Dale and Reiter was the most influential algorithm before 2000. It is based on the idea of a \"preferential\" order of attributes or properties that speakers go by. So in order to run the Incremental Algorithm, first a preference order of attributes has to be given. Now the algorithm follows that order and adds those properties to the description which rule out any remaining distractors. Furthermore, Dale and Reiter stress the attribute type which is always included in their descriptions even if it does not rule out any distractors.\n\nAlso the type values are part of a \"subsumption hierarchy\" including some \"basic level values\". For example, in the \"pet\" domain \"chihuahua\" is subsumed by \"dog\" and \"dog\" by \"animal\". Because \"dog\" is defined as a basic level \"dog\" would be preferred by the algorithms, if \"chihuahua\" does not rule out any distractors.\n\nThe Incremental Algorithm is easy to implement and also computationally efficient running in polynomial time. The description generated by the IA can contain redundant properties that are superfluous because of later added properties. The creators do not consider this as a weakness, but rather as making the expressions less \"psycholinguistically implausible\".\n\nThe following algorithm is a simplified version of Dale and Reiter’s Incremental Algorithm by Krahmer and van Deemter that takes as input the referent \"r\", the \"D\" containing a collection of domain objects and a domain-specific ordered list \"Pref\" of preferred attributes. In the notation \"L\" is the description, \"C\" the context set of distractors and the function RulesOut() returns the set of objects which have a value different to V for attribute A.\n\nBefore 2000 evaluation of REG systems has been of theoretical nature like the one done by Dale and Reiter. More recently, empirical studies have become popular which are mostly based on the assumption that the generated expressions should be similar to human-produced ones. Corpus-based evaluation began quite late in REG due to a lack of suitable data sets. Still corpus-based evaluation is the most dominant method at the moment though there is also evaluation by human judgement.\n\nFirst the distinction between text corpora and experimental corpora has to be made. Text corpora like the GNOME corpus can contain texts from all kind of domains. In REG they are used to evaluate the \"realization\" part of algorithms. The \"content selection\" part of REG on the other hand requires a corpus that contains the properties of all domain objects as well as the properties used in references. Typically those fully \"semantically transparent\" created in experiments using simple and controlled settings.\n\nThese experimental corpora once again can be separated into \"General-Purpose Corpora\" that were collected for another purpose but have been analysed for referring expressions and \"Dedicated Corpora\" that focus specifically on referring expressions. Examples of General-Purpose Corpora are the Pear Stories, the Map Task corpus or the Coconut corpus while the Bishop corpus, the Drawer corpus and the TUNA corpus count to the Dedicated Corpora. \nThe TUNA corpus which contains web-collected data on the two domains furniture and people has been used in three shared REG challenges already.\n\nTo measure the correspondence between corpora and the results of REG algorithms several Metrics have been developed.\n\nTo measure the \"content selection\" part the Dice coefficient or the MASI (Measuring Agreement on Set-valued Items) metric are used. These measure the overlap of properties in two descriptions. In an evaluation the scores are usually averaged over references made by different human participants in the corpus. Also sometimes a measure called Perfect Recall Percentage (PRP) or Accuracy is used which calculates the percentage of perfect matches between an algorithm-produced and a human-produced reference.\n\nFor the \"linguistic realization\" part of REG the overlap between strings has been measured using metrics like BLEU or NIST. A problem that occurs with string-based metrics is that for example \"The small monkey\" is measured closer to \"The small donkey\" than to \"The little monkey\".\n\nA more time consuming way to evaluate REG algorithms is by letting humans judge the \"Adequacy\" (How clear is the description?) and \"Fluency\" (Is the description given in good and clear English?) of the generated expression. Also Belz and Gatt evaluated referring expressions using an experimental setup. The participants get a generated description and then have to click on the target. Here the extrinsic metrics reading time, identification time and error rate could be evaluated.\n"}
{"id": "34608657", "url": "https://en.wikipedia.org/wiki?curid=34608657", "title": "Rongowhakaata Halbert", "text": "Rongowhakaata Halbert\n\nRongowhakaata \"Rongo\" Pere Halbert (2 February 1894 – 11 April 1973) was a Māori tribal leader, interpreter, historian and genealogist. He was considered \"an eminent authority on Māori literature\". Halbert's book \"Horouta: The History of the Horouta Canoe, Gisborne and East Coast\", published posthumously in 1999, is considered a classic of tribal history.\n\nHalbert was born on 2 February 1894 in Waerenga-a-Hika, Gisborne, to Hetekia Te Kani Pere (or Halbert), a farmer, and his wife, Riripeti Rangikohera Ranginui. His grandfather was noted Māori politician Wi Pere. Halbert attended school in Gisborne up until 1911. He then attended Nelson College from 1911 to 1914, where he was a prefect and excelled at sports and music.\n\nOn 11 September 1915, he married Patehepa Tamatea and together they had seven children. For a time they ran a dairy farm at Pukepapa, near Waituhi.\n\nHalbert became a licensed interpreter in 1915. His strong command of the Māori language combined with his knowledge of Māori literature gained him important roles such as assisting the New Zealand Geographic Board with Māori place names, and revising the sixth edition of H. W. Williams's Māori dictionary. Later he acted as an adviser on Māori texts for the Polynesian Society. Halbert was a founding member of the Gisborne Art Gallery and Museum in 1955, and the first chairman of the Maori Museum Committee, advising on the Māori collections.\n\nAs a historian and genealogist, Halbert contributed to John Hikawera Mitchell's \"Takitimu\" (1944), a record of the migration of the Ngati Kahungunu. He also contributed to the Whakatane and District Historical Society's first memoir, \"Te Tini o Toi\", as well as papers on the dating of Maori genealogies.\n\nFrom 1940 onwards Halbert devoted most of his time to studying the history and genealogy of the East Coast iwi. In 1999 his book \"Horouta: The History of the Horouta Canoe, Gisborne and East Coast\" was posthumously published by Reed Publishing.\n\nPoor health forced Halbert's retirement from the Maori Purposes Fund Board in 1968. He died at Lavington Private Hospital in Epsom, Auckland on 11 April 1973 and was buried at Taruheru Cemetery in Gisborne. He was survived by three daughters and three sons.\n\nAt the time of his death, Halbert was preparing a major historical and genealogical work. He left three main collections of papers: a manuscript called 'Horouta', 130 complete whakapapa charts, and a series of maps and historical data with iwi and hapū.\n\n"}
{"id": "161761", "url": "https://en.wikipedia.org/wiki?curid=161761", "title": "Rosetta Project", "text": "Rosetta Project\n\nThe Rosetta Project is a global collaboration of language specialists and native speakers working to develop a contemporary version of the historic Rosetta Stone to last from 2000 to 12,000 AD; it is run by the Long Now Foundation. Its goal is a meaningful survey and near permanent archive of 1,500 languages. Some of these languages have fewer than one thousand speakers left. Others are considered to be dying out because government centralization and globalization are increasing the prevalence of English and other major languages. The intention is to create a unique platform for comparative linguistic research and education, as well as a functional linguistic tool that might help in the recovery or revitalisation of lost languages in the future.\n\nThe project is creating this broad language archive through an open contribution, open review process similar to the strategy that created the original \"Oxford English Dictionary\". The resulting archive will be publicly available in three different media: a HD-Rosetta micro-etched nickel alloy disc three inches (7.62 cm) across with a 2,000 year life expectancy; a single volume monumental reference book; and a growing online archive. \n\nFifty to ninety percent of the world's languages are predicted to disappear in the next century, many with little or no significant documentation. Much linguistic description, especially the description of languages with few speakers, remains hidden in personal research files or poorly preserved in under-funded archives.\n\nAs part of the effort to secure this critical legacy of linguistic diversity, the Long Now Foundation plans a broad online survey and near-permanent physical archive of 1,500 of the approximately 7,000 human languages.\n\nThe project has three overlapping goals:\n\n\nThe 1,500-language corpus expands on the parallel-text structure of the original Rosetta Stone through archiving ten descriptive components for each of the 1,500 selected languages.\n\nThe goal is an open source \"Linux of Linguistics\"—an effort of collaborative online scholarship drawing on the expertise and contributions of thousands of academic specialists and native speakers around the world. The project is also organising formal archive research groups at Stanford, Yale, Berkeley, the American Library of Congress, and the American Summer Institute of Linguistics (and its offices in Dallas).\n\nThe resulting Rosetta archives are publicly available in three different media:\n\n\nA \"Version 1.0\" of the HD-Rosetta disc was completed on November 3, 2008. The disc contains over 13,000 pages of information using over 1,500 languages, which can be read after magnifying by 650 times with a microscope.\n\nIn early 2017, the Rosetta Wearable Disk was released. It was developed using a similar manufacturing process as the first edition of the Rosetta Disk, the main difference being that the final archive is about 2 cm in diameter, thus enabling wearing as an ornament on the human body. One side has instructions in eight different languages and scripts (Bahasa Indonesia, English, Hindi, Mandarin, Modern Standard Arabic, Spanish, Swahili, and Russian), and the other an archive of over 1000 human languages assembled in 2016. By November 2017, the initial run of 100 disks had all been sold, but new releases are planned.\n\n\n"}
{"id": "4797748", "url": "https://en.wikipedia.org/wiki?curid=4797748", "title": "Semi-postal stamp", "text": "Semi-postal stamp\n\nA semi-postal stamp or semipostal stamp, also known as a charity stamp, is a postage stamp issued to raise money for a particular purpose (such as a charitable cause) and sold at a premium over the postal value. Typically the stamp shows two denominations separated by a plus sign, but in many cases the only denomination shown is for the postage rate, and the postal customer simply pays the higher price when purchasing the stamps.\n\nThe first semi-postal was actually a postal card; to commemorate the Uniform Penny Post in 1890, the United Kingdom of Great Britain and Ireland issued a card with a face value of one penny, but sold it for sixpence, with the difference given to a fund for postal workers. The first semi-postal stamps were issued by the Australian colonies of New South Wales and Victoria, who both marked the Diamond Jubilee of Queen Victoria in 1897 with stamps denominated in pennies, but sold for shillings, a 12x increase over the face value.\n\nSemi-postals became widespread in European countries at the beginning of the 20th century. In many cases they have become standard annual issues, such as the Pro Juventute series of Switzerland started in 1913. Many countries issued semi-postal stamps to raise money for the Red Cross in World War I. The surcharges are typically a fraction of the face value; at one point the Fédération Internationale de Philatélie was officially boycotting stamps with surcharges greater than 50 per cent of face value, saying that such issues were exploitive of stamp collectors. The United Kingdom's Royal Mail, a relative newcomer to semi-postals, issued its first stamp of this type in 1975 with a 4½p denomination and a premium of 1½p for charitable causes making the total cost 6p, with funds going to health and handicap charities. The stamp issue was not considered a success and there have been few UK semi-postal issues since.\n\nSome non-European countries followed suit (such as New Zealand, which has issued health stamps annually since 1929); the New Zealand associated territories of the Cook Islands and Niue often issue Christmas or Easter stamps in two sets of values, with one set having a charitable surcharge. But semi-postal stamps are still predominantly European. By contrast, the United States is a newcomer to semi-postals, with its first semi-postal being the Breast cancer research stamp issued in July 1998. Through 2016 four additional stamps were issued, three for other causes and the fourth a reissue of the Breast cancer stamp. A subsequent law allowed five more stamps to be issued at two year intervals over the next decade. The first, for Alzheimer's disease, was released in November, 2017.\n\nSemi-postal issues are not always issued on a regular basis for health and similar causes; they have been on occasion issued as a means of raising funds for disaster relief. These are usually sold as charity stamps, though occasionally, as with the 1971 refugee relief stamps of India, the excess cost has been levied as an obligatory tax. One of the highest value semi-postal stamps is the Falkland islands' \"rebuilding fund\" stamp, issued in 1982 after the Anglo-Argentinian war over the islands. This was aimed primarily at collectors, and had a postal value of £1 with an added £1 surcharge.\n\nCharitable disaster relief stamps are commonly found in smaller island nations as a result of natural disasters, as in the case of St. Vincent and the Grenadines' 1980 hurricane relief issues, and Tonga's 1982 cyclone relief stamp. In these cases, existing issues are often used, overprinted with text indicating the reason for the surcharge.\n\n\n\n"}
{"id": "9768381", "url": "https://en.wikipedia.org/wiki?curid=9768381", "title": "Semitic Museum", "text": "Semitic Museum\n\nThe Harvard Semitic Museum was founded in 1889, and moved into its present location at 6 Divinity Avenue in Cambridge, Massachusetts, in 1903.\n\nFrom the beginning, it was the home of the Department of Near Eastern Languages and Civilizations, a departmental library, a repository for research collections, a public educational institute, and a center for archaeological exploration. Among the museum's early achievements were the first scientific excavations in the Holy Land (at Samaria in 1907–1912) and excavations at Nuzi and Tell el-Khaleifeh in the Sinai, where the earliest alphabet was found.\n\nThe museum's artifacts include pottery, cylinder seals, sculpture, coins, cuneiform tablets, and Egyptian mummy sarcophagi. Many are from museum-sponsored excavations in Jordan, Iraq, Egypt, Cyprus, Israel, and Tunisia. The museum holds plaster casts of the Black Obelisk of Shalmaneser III, the Laws of Hammurabi, and the Stele of Esarhaddon, as well as a full-scale model of an Iron Age Israelite house. The museum is dedicated to the use of these collections for the teaching, research, and publication of Near Eastern archaeology, history, and culture. \n\nArchitectural firm A. W. Longfellow broke ground on the present Harvard Semitic Museum site on September 27, 1900. Construction was completed in spring of 1902, and the public portions of the museum were opened on February 5, 1903. \n\nThe museum's facilities were repurposed during World War II, and it was closed to the public from August 1942 through April 1946. Twelve years later it was closed again to the public, lasting from 1958 through 1982. \n\nThe museum reopened in April 1982, and then Harvard President Derek Bok spoke at the reopening ceremony. In December 2012, Harvard announced a new consortium, the Harvard Museums of Science and Culture, whose members are the Harvard Museum of Natural History, the Harvard Semitic Museum, the Peabody Museum of Archaeology and Ethnology, and the Collection of Historical Scientific Instruments.\n\nThe Semitic Museum, through the collaborative efforts of departmental faculty, curators, museum curatorial staff, and students; mounts exhibits, often in conjunction with university courses, which not only serve the university community, but also attract the general public. The museum has an active public outreach program featuring tours for school groups and teacher training workshops. The museum also sponsors, either alone or in conjunction with other institutions, a number of public lectures each year. Through these educational efforts, the museum seeks to promote a wider understanding of the civilizations of the Near East and their great cultural legacies.\n\nThere are many opportunities for Harvard University students and faculty to make use of the museum collections and facilities. Objects can be used for coursework, viewing assignments, research papers, senior theses, dissertations, and teaching displays. The basement seminar room is available for sections scheduled to view the collection.\n\nThe Harvard Semitic Museum sponsors archaeological field research into the complex societies of the Near East, with special emphasis on those ancient cultures related to the world of the Bible. Each year more than 100 staff, students, and volunteers participate in the Ashkelon Excavations (The Leon Levy Expedition), led by Honorary Museum Director Lawrence E. Stager, Dr. Daniel M. Master, and Dr. Adam Aja. The museum, through its Harvard Semitic Series and Harvard Semitic Monographs, publishes archaeological, historical, philological, and cultural studies of the Near East, many of which present the research of the department faculty and their students.\n\n"}
{"id": "1688561", "url": "https://en.wikipedia.org/wiki?curid=1688561", "title": "Specific language impairment", "text": "Specific language impairment\n\nSpecific language impairment (SLI) is diagnosed when a child's language does not develop normally and the difficulties cannot be accounted for by generally slow development, physical abnormality of the speech apparatus, autism spectrum disorder, apraxia, acquired brain damage or hearing loss. Twin studies have shown that it is under genetic influence. Although language impairment can result from a single-gene mutation, this is unusual. More commonly SLI results from the combined influence of multiple genetic variants, each of which is found in the general population, as well as environmental influences.\n\nSpecific language impairment (SLI) is diagnosed when a child has delayed or disordered language development for no apparent reason. Usually the first indication of SLI is that the child is later than usual in starting to speak and subsequently is delayed in putting words together to form sentences. Spoken language may be immature. In many children with SLI, understanding of language, or \"receptive\" language, is also impaired, though this may not be obvious unless the child is given a formal assessment.\n\nAlthough difficulties with use and understanding of complex sentences are a common feature of SLI, the diagnostic criteria encompass a wide range of problems, and for some children other aspects of language are problematic (see below). In general, the term SLI is reserved for children whose language difficulties persist into school age, and so it would not be applied to toddlers who are late to start talking, most of whom catch up with their peer group after a late start.\n\nThe terminology for children's language disorders is extremely wide-ranging and confusing, with many labels that have overlapping but not necessarily identical meanings. In part this confusion reflects uncertainty about the boundaries of SLI, and the existence of different subtypes. Historically, the terms \"developmental dysphasia\" or \"developmental aphasia\" were used to describe children with the clinical picture of SLI. These terms have, however, largely been abandoned, as they suggest parallels with adult acquired aphasia. This is misleading, as SLI is not caused by brain damage.\n\nSome synonyms currently in use for specific language impairment are language impairment, developmental language delay (DLD), language disorder, and language-learning disability. Researcher Bonnie Brinton argues that the term \"specific language impairment\" is misleading because the disorder does not only affect language, but also affects reading, writing, and social/pragmatics. \n\nIn medical circles, terms such as \"specific developmental language disorder\" are often used, but this has the disadvantage of being wordy, and is also rejected by some people who think SLI should not be seen as a \"disorder\". In the UK educational system, \"speech, language and communication needs\" (SLCN) is currently the term of choice, but this is far broader than SLI, and includes children with speech and language difficulties arising from a wide range of causes.\n\nAlthough most experts agree that children with SLI are quite variable, there is little agreement on how best to subtype them. There is no widely accepted classification system. In 1983 Rapin and Allen proposed a classification of developmental language disorders based on the linguistic features of language impairment, which was subsequently updated by Rapin. Note that Rapin is a child neurologist, and she refers to different subtypes as \"syndromes\"; many of those coming from the perspective of education or speech-language therapy reject this kind of medical label, and argue that there is not a clear dividing line between SLI and normal variation. Also, although most experts would agree that children with characteristics of the Rapin subtypes can be identified, there are many cases who are less easy to categorise, and there is also evidence that categorisation can change over time.\n\nRapin's subgroups fall into three broad categories:\n\nReceptive/expressive phonologic/syntactic deficit syndrome is the most common form of SLI, in which the child's most obvious problems are a tendency to speak in short, simplified sentences, with omission of some grammatical features, such as past tense -ed. It is common also to see simplified speech production when the child is young. For instance, clusters of consonants may be reduced, so that \"string\" is pronounced as \"ting\". Vocabulary is often limited, with a tendency to use \"general all-purpose\" terms, rather than more specific words.\n\nVerbal auditory agnosia is a very rare form of language impairment, in which the child appears unable to make sense of speech sounds. It typically occurs as a symptom of Landau-Kleffner syndrome, in which case a diagnosis of SLI would not be appropriate, as there is a known neurological origin of the language difficulties.\n\nDevelopmental verbal dyspraxia (DVD)￼￼ – in the child with DVD, comprehension is adequate; the onset of speech is very delayed and extremely limited with impaired production of speech sounds and short utterances. The poor speech production cannot be explained in terms of structural or neurological damage of the articulators. There is much disagreement about diagnostic criteria, but the label most often used for children whose intelligibility declines markedly when they attempt complex utterances, compared to when they are producing individual sounds or syllables.\n\nAnother key feature is inconsistency of speech sound production from one occasion to another. Although the term \"dyspraxia\" suggests a pure output disorder, many – perhaps all – of these children have difficulty in doing tasks that involve mentally manipulating speech sounds, such as phonological awareness tasks. Children with DVD also typically have major literacy problems, and receptive language levels may be poor on tests of vocabulary and grammar.\n\nPhonologic programming deficit syndrome – the child speaks in long but poorly intelligible utterances, producing what sounds like jargon. Outside Rapin's group, little has been written about this subtype, which is not generally recognised in diagnostic frameworks.\n\nLexical deficit disorder – the child has word finding problems and difficulty putting ideas into words. There is poor comprehension for connected speech. Again, there is little research on this subtype, which is not widely recognised.\n\nPragmatic language impairment – the child speaks in fluent and well-formed utterances with adequate articulation; content of language is unusual; comprehension may be over-literal and language use is odd. The child may chatter incessantly and be poor at turn-taking in conversation and maintaining a topic. There has been a great deal of controversy about this category, which is termed \"pragmatic language impairment (PLI)\" in the UK. Debate has centred over the question of whether it is a subtype of SLI, part of the autistic spectrum, or a separate condition. In DSM-5, the term \"Social Communication Disorder\" has been introduced; this is equivalent to PLI.\n\nAlthough textbooks draw clear boundaries between different neurodevelopmental disorders, there is much debate about overlaps between them. Many children with SLI meet diagnostic criteria for developmental dyslexia, and others have features of autism.\n\nSLI is defined purely in behavioural terms: there is no biological test for SLI. There are three points that need to be met for a diagnosis of SLI:\nThere is considerable variation in how this last criterion is implemented. Tombin et al. (1996) proposed the EpiSLI criterion, based on five composite scores representing performance in three domains of language (vocabulary, grammar, and narration) and two modalities (comprehension and production). Children scoring in the lowest 10% on two or more composite scores are identified as having language disorder.\n\nAssessment will usually include an interview with the child's caregiver, observation of the child in an unstructured setting, a hearing test, and standardized tests of language and nonverbal ability. There is a wide range of language assessments in English. Some are restricted for use by speech and language professionals (therapists or SALTs in the UK, speech-language pathologists, SLPs, in the US and Australia).\nA commonly used test battery for diagnosis of SLI is the Clinical Evaluation of Language Fundamentals (CELF). Assessments that can be completed by a parent or teacher can be useful to identify children who may require more in-depth evaluation.\n\nThe Grammar and Phonology Screening (GAPS) test is a quick (ten minute) simple and accurate screening test developed and standardized in the UK. It is suitable for children from 3;4 to 6;8 years;months and can be administered by professionals and non-professionals (including parents) alike, and has been demonstrated to be highly accurate (98% accuracy) in identifying impaired children who need specialist help vs non-impaired children. This makes it potentially a feasible test for widespread screening.\n\nThe Children’s Communication Checklist (CCC–2) is a parent questionnaire suitable for testing language skills in school-aged children.\nInformal assessments, such as language samples, may also be used. This procedure is useful when the normative sample of a given test is inappropriate for a given child, for instance, if the child is bilingual and the sample was of monolingual children. It is also an ecologically valid measure of all aspects of language (e.g. semantics, syntax, pragmatics, etc.).\n\nTo complete a language sample, the SLP will spend about 15 minutes talking with the child. The sample may be of a conversation (Hadley, 1998), or narrative retell. In a narrative language sample, the SLP will tell the child a story using a wordless picture book (e.g. \"Frog Where Are You?\", Mayer, 1969), then ask the child to use the pictures and tell the story back.\n\nLanguage samples are typically transcribed using computer software such as the systematic analysis of language software (SALT, Miller et al. 2012), and then analyzed. For example, the SLP might look for whether the child introduces characters to their story or jumps right in, whether the events follow a logical order, and whether the narrative includes a main idea or theme and supporting details.\n\nEpidemiological surveys, in the US and Canada, estimated the prevalence of SLI in five-year-olds at around 7%. However, neither study adopted the stringent \"discrepancy\" criteria of the Diagnostic and Statistical Manual of Mental Disorders or ICD-10; SLI was diagnosed if the child scored below cut-off on standardized language tests, but had a nonverbal IQ of 90 or above and no other exclusionary criteria.\n\nLongitudinal studies indicate that problems are largely resolved by five years in around 40% of 4-year-olds with SLI. However, for children who still have significant language difficulties at school entry low levels of literacy are common, even for children who receive specialist help, and educational attainments are typically poor. Poor outcomes are most common in cases where comprehension as well as expressive language is affected. There is also evidence that the nonverbal IQ of children with SLI decreases over the course of development.\n\nSLI is associated with a high rate of psychiatric disorder. For instance, Conti-Ramsden and Botting (2004) found that 64% of a sample of 11-year-olds with SLI scored above a clinical threshold on a questionnaire for psychiatric difficulties, and 36% were regularly bullied, compared with 12% of comparison children. In the longer-term, studies of adult outcomes of children with SLI find elevated rates of unemployment, social isolation and psychiatric disorder. However, most studies focused on children with severe problems, where comprehension as well as expressive language was affected. Better outcomes are found for children who have milder difficulties and do not require special educational provision.\n\nIt is now generally accepted that SLI is a strongly genetic disorder. The best evidence comes from studies of twins. Two twins growing up together are exposed to the same home environment, yet may differ radically in their language skills. Such different outcomes are, however, seen almost exclusively in fraternal (non-identical) twins, who are genetically different. Identical twins share the same genes and tend to be much more similar in language ability.\n\nThere can be some variation in the severity and persistence of SLI in identical twins, indicating that environmental factors affect the course of disorder, but it is unusual to find a child with SLI who has an identical twin with normal language.\n\nSLI is not usually caused by a mutation in a single gene. Current evidence suggests that there are many different genes that can influence language learning, and SLI results when a child inherits a particularly detrimental combination of risk factors, each of which may have only a small effect. It has been hypothesized, however, that a mutation of the FOXP2 gene may have an influence on the development on SLI to a certain degree, as it regulates genes pertinent to neural pathways related to language.\n\nOnly a handful of non-genetic factors have been found selectively to impact on language development in children. Later-born children in large families are at greater risk than earlier born.\n\nOverall, genetic mutation, hereditary influences, and environmental factors may all have a role in the development and manifestation of SLI. It is important, therefore, to not associate the development to a single factor, but recognize that it is oftentimes the result of complex interactions between any or all of these factors.\n\nMuch research has focused on trying to identify what makes language learning so hard for some children. A major divide is between theories that attribute the difficulties to a low-level problem with auditory temporal processing, and those that propose there is a deficit in a specialised language-learning system. Other accounts emphasise deficits in specific aspects of memory. It can be difficult to choose between theories because they do not always make distinctive predictions, and there is considerable heterogeneity among children with SLI. It has also been suggested that SLI may only arise when more than one underlying deficit is present.\n\nMales are more affected by SLI than females. In clinical samples, the sex ratio of affected males: females is around 3 or 4:1. The reason for this association is not known: no linkage has been found to genes on the sex chromosomes.\nPoor motor skills are commonly found in children with SLI. Brain scans do not usually reveal any obvious abnormalities in children with SLI, although quantitative comparisons have found differences in brain size or relative proportions of white or grey matter in specific regions. In some cases, unusual brain gyri are found. To date, no consistent \"neural signature\" for SLI has been found. Differences in the brains of children with SLI vs typically developing children are subtle and may overlap with atypical patterns seen in other neurodevelopmental disorders.\n\nIntervention is usually carried out by speech and language therapists, who use a wide range of techniques to stimulate language learning. In the past, there was a vogue for drilling children in grammatical exercises, using imitation and elicitation methods, but such methods fell into disuse when it became apparent that there was little generalisation to everyday situations. Contemporary approaches to enhancing development of language structure are more likely to adopt 'milieu' methods, in which the intervention is interwoven into natural episodes of communication, and the therapist builds on the child's utterances, rather than dictating what will be talked about. In addition, there has been a move away from a focus solely on grammar and phonology toward interventions that develop children's social use of language, often working in small groups that may include typically developing as well as language-impaired peers.\n\nAnother way in which modern approaches to remediation differ from the past is that parents are more likely to be directly involved, particularly with preschool children.\n\nA radically different approach has been developed by Tallal and colleagues, who have devised a computer-based intervention, Fast Forword, that involves prolonged and intensive training on specific components of language and auditory processing. The theory underlying this approach maintains that language difficulties are caused by a failure to make fine-grained auditory discriminations in the temporal dimension, and the computerised training materials are designed to sharpen perceptual acuity.\n\nFor all these types of intervention, there are few adequately controlled trials that allow one to assess clinical efficacy. In general, where studies have been done, results have been disappointing, though some more positive outcomes have been reported. In 2010, a systematic review of clinical trials assessing the FastForword approach was published, and reported no significant gains relative to a control group.\n\n\n"}
{"id": "20563662", "url": "https://en.wikipedia.org/wiki?curid=20563662", "title": "Spotface", "text": "Spotface\n\nA spotface or spot face is a machined feature in which a certain region of the workpiece (a spot) is faced, providing a smooth, flat, accurately located surface. This is especially relevant on workpieces cast or forged, where the spotface's smooth, flat, accurately located surface stands in distinction to the surrounding surface whose roughness, flatness, and location are subject to wider tolerances and thus not assured with a machining level of precision. The most common application of spotfacing (spot facing) is facing the area around a bolt hole where the bolt's head will sit, which is often done by cutting a shallow counterbore, just deep enough \"to clean up\"—that is, only enough material is removed to get down past any irregularity and thus make the surface flat. Other common applications of spotfacing involve facing a pad onto a boss, creating planar surfaces in known locations that can orient a casting or forging into position in the assembly; allow part marking such as stamping or nameplate riveting; or offer machine-finish visual appeal in spots, without the need for finishing all over (FAO). \n\nThe cutters most often used to cut spotfaces are counterbore cutters and endmills. In manual machining especially, the former is useful because its pilot guides the cutter into the correct location (established by the bolt hole), and its cutting lips are perpendicular to the hole axis with no relief angle, meaning that a plunging cut, moving in only the Z-axis, will leave a flat surface. In contrast, most general-purpose endmills have a relief angle such that a plunging cut (Z-axis-only toolpath) will leave a very slightly convex surface. But in CNC machining this is irrelevant, because the flat spotface is left by circular interpolation of the endmill as it traces a circular toolpath in the XY plane. Zero-angle cutters obviate this, functioning similarly to the traditional counterbore cutters of manual machining but needing no pilot. Spot facing of larger planar surfaces sometimes employs face mills. \n\nBackspotfacing (back spot facing) is analogous to backboring, meaning that the tool manages to reach to the far side of the workpiece (away from the spindle side) and cut back toward the spindle. Such operations can be done with boring bars reaching through a hole while shifted off-center, then shifting onto center for the cut (for example, in the G76 fine boring cycle), or with back-deburring style tools whose cutting edges open and close in umbrella-like or check-valve-like fashion. Such back-cutting operations can obviate second operations, with their attendant setup time and part rehandling time. Sometimes the limiting factor in applying this advantage is the difficulty of chip clearance; the operator needs to intervene because chip nests prevent the tool from passing freely and opening and closing freely. In such cases, improved chipbreaking via different inserts in the main operation may help, and backspotfacing tools powered by coolant pressure and using through-tool coolant feed may succeed where others cannot (opening, cutting, and retracting despite some chips), not only obviating a second operation but also making the main operation less reliant on operator intervention and program stops (M00 or M01) than a lesser backspotfacing tool would. \n\nSome fastener designs can obviate spotfacing for some applications. For example, a self-aligning nut allows the nut to find its own perpendicularity to the thread axis by floating via a pair of spherical bearing surfaces (a ball-in-cup arrangement). \n\nStandards exist for the sizes of spotfaces, especially for fastener head seating areas. These standards can vary between corporations and between standards organizations. For example, in Boeing Design Manual BDM-1327 section 3.5, the nominal diameter of the spot-faced surface is the same as the nominal size of the cutter, and is equal to the flat seat diameter plus twice the fillet radius. This is in contrast to the ASME Y14.5-2009 definition of a spotface, which is equal to the flat seat diameter.\n"}
{"id": "24176725", "url": "https://en.wikipedia.org/wiki?curid=24176725", "title": "Starfish Project", "text": "Starfish Project\n\nThe Starfish Project is a UK-based not-for-profit therapy programme which helps people who stammer or stutter to overcome their speech through the use of diaphragmatic breathing (also known as \"costal breathing\"). The programme also teaches participants to reassess negative emotions surrounding their stammering through the use of avoidance reduction therapy (see stuttering therapy). \n\nParticipants attend a three-day residential course near Hailsham in the UK during which a diaphragmatic breathing technique and non-avoidance strategies are taught. Following this initial course, participants are able to attend additional follow-up courses at the same venue without charge from the Starfish Project as often as they feel necessary. Course participants are also encouraged to make use of a list of phone contacts of other course participants and can attend free support meetings at various locations around the UK. \n\nAnne Blight, founder of the Starfish Project, has been working with people who stammer for many years, first as a volunteer with the Association for Stammerers in the UK, then with its successor organisation The British Stammering Association. Anne Blight was previously involved in the early days of McGuire Programme before founding the Starfish Project in 1998.\n\nIn 2016, the Starfish Project announced its first courses outside of the UK. The Starfish Project had been invited to start courses for people who stammer in the Gulf Cooperation Council (GCC) countries. Courses are scheduled quarterly in Dubai with instruction in English and monthly at various cities within the GCC countries with instruction in Arabic. \n\nAlthough diaphragmatic breathing may be an effective way for people who stammer to manage their condition, it should not be regarded as a cure and, while anecdotally effective, the existence of beneficial outcomes for people who stammer from diaphragmatic breathing-based therapies such as the Starfish Project is not supported by specific peer-reviewed research.\n\nA similar breathing technique is used by several other stammering treatment initiatives including the programme offered by the Del Ferro Institute in Amsterdam (where the application of diaphragmatic breathing to stammering was first developed) and the McGuire Programme.\n\n"}
{"id": "33378197", "url": "https://en.wikipedia.org/wiki?curid=33378197", "title": "Subject (documents)", "text": "Subject (documents)\n\nIn library and information science documents (such as books, articles and pictures) are classified and searched by subject – as well as by other attributes such as author, genre and document type. This makes \"subject\" a fundamental term in this field. Library and information specialists assign subject labels to documents to make them findable. There are many ways to do this and in general there is not always consensus about which subject should be assigned to a given document. To optimize subject indexing and searching, we need to have a deeper understanding of what a subject is. The question: \"what is to be understood by the statement 'document A belongs to subject category X'?\" has been debated in the field for more than 100 years (see below).\n\nHjørland defined subjects as the \"epistemological potentials of documents\". This definition is in line with the request oriented understanding of indexing quoted below. The idea is that a document is assigned a subject to ease retrieval and findability. And the criteria for what should be found – what constitutes knowledge – is in the end an epistemological question.\n\nFor Cutter the stability of subjects depends on a social process in which their meaning is stabilized in a name or a designation. A subject \"referred [...] to those intellections [...] that had received a name that itself represented a distinct consensus in usage\" (Miksa, 1983a, p. 60) and: the \"systematic structure of established subjects\" is \"resident in the public realm\" (Miksa, 1983a, p. 69); \"[s]ubjects are by their very nature locations in a classificatory structure of publicly accumulated knowledge (Miksa, 1983a, p. 61). Bernd Frohmann adds: \n\n\"The stability of the public realm in turn relies upon natural and objective mental structures which, with proper education, govern a natural progression from particular to general concepts. \nSince for Cutter, mind, society, and SKO [Systems of Knowledge Organization] stand one behind the other, each supporting each, all manifesting the same structure, his discursive construction of subjects invites connections with discourses of mind, education, and society. The Dewey Decimal Classification (DDC), by contrast, severs those connections. Melvil Dewey emphasized more than once that his system maps no structure beyond its own; there is neither a \"transcendental deduction\" of its categories nor any reference to Cutter's objective structure of social consensus. It is content-free: Dewey disdained any philosophical excogitation of the meaning of his class symbols, leaving the job of finding verbal equivalents to others. His innovation and the essence of the system lay in the notation. The DDC is a poorly semiotic system of expanding nests of ten digits, lacking any referent beyond itself. In it, a subject is wholly constituted in terms of its position in the system. The essential characteristic of a subject is a class symbol which refers only to other symbols. Its verbal equivalent is accidental, a merely pragmatic characteristic...\nThe conflict of interpretations over \"subjects\" became explicit in the battles between \"bibliography\" (an approach to subjects having much in common with Cutter's) and Dewey's \"close classification\". William Fletcher spoke for the scholarly bibliographer... Fletcher's \"subjects\", like Cutter's, referred to the categories of a fantasized, stable social order, whereas Dewey's subjects were elements of a semiological system of standardized, techno-bureaucratic administrative software for the library in its corporate, rather than high culture, incarnation\". (Frohmann, 1994, 112–113). \n\nCutter's early view on what a subject is, is probably wiser than most understandings that dominated the 20th century – and also the understanding reflected in the ISO-standard quoted below. The early statements quoted by Frohmann indicate that subjects are somehow shaped in social processes. When that is said, it should be added that they are not particularly detailed or clear. We only get a vague idea of the social nature of subjects.\n\nA system, which has en explicit theoretical foundation is Ranganathan's Colon Classification. Ranganathan provided an explicit definition of the concept of \"subject\":\nA related definition is given by one of Ranganathan's students: \n\nRanganathan's definition of \"subject\" is strongly influenced by his Colon Classification system. The colon system is based on the combination of single elements from facets to subject designation. This is the reason why the combined nature of subjects are emphasized so strongly. It leads, however, to absurdities such as the claim that gold cannot be a subject (but is alternatively termed \"an isolate\"). This aspect of the theory has been criticized by Metcalfe (1973, p. 318). Metcalfe's skepticism regarding Ranganathan's theory is formulated in hard words (op. cit., p. 317): \"This pseudo-science imposed itself on British disciples from about 1950 on...\". \n\nIt seems unacceptable that Ranganathan defines the word subject in a way that favors his own system. A scientific concept like \"subject\" should make it possible to compare different ways of establishing access to information. Whether or not subjects are combined or not should be examined once their definition has been given, it should not determined a priori, in the definition.\n\nBesides the emphasis on the combined, organizing and systematizing nature of subjects contains Ranganathan's definition of subject the pragmatic demand, that a subject should be determined in a way that suits a normal person's competency or specialization. Again we see a strange kind of wishful thinking mixing a general understanding of a concept with demands put by his own specific system. One thing is what the word subject means, quite another issue is how to provide subject descriptions that fulfill demands such as the specificity of a given information retrieval language which fulfill demands put on the system, such as precision and recall. If researchers too often define terms in ways that favor specific kinds of systems, that are such definitions not useful to provide more general theories about subjects, subject analysis and IR. Among other things are comparative studies of different kinds of systems made difficult. \n\nBased on these arguments (as well as additional arguments which have been used in the literature) we may conclude that Ranganathan's definition of the concept \"subject\" is not suited for scientific use. Like the definition of \"subject\" given by the ISO-standard for topic maps may Ranganathan's definition be useful within his own closed system. The purpose of a scientific and scholarly field is, however, to examine the relative fruitfulness of systems such as topic maps and Colon classification. For such purpose is another understanding of \"subject\" necessary.\n\nIn his book Wilson (1968) examined – in particular by thought experiments – the suitability of different methods of examining the subject of a document. The methods were:\n\n\nPatrick Wilson shows convincingly that each of these methods are insufficient to determine the subject of a document and is led to conclude ( p. 89): \"The notion of the subject of a writing is indeterminate...\" or, on p. 92 (about what users may expect to find using a particular position in a library classification system): \"For nothing definite can be expected of the things found at any given position\". In connection to the last quote has Wilson an interesting footnote in which he writes that authors of documents often use terms in ambiguous ways (\"hostility\" is used as an example). Even if the librarian could personally develop a very precise understanding of a concept, he would be unable to use it in his classification, because none of the documents use the term in the same precise way. Based on this argumentation is Wilson led to conclude: \"If people write on what are for them ill-defined phenomena, a correct description of their subjects must reflect the ill-definedness\". \n\nWilson's concept of subject was discussed by Hjørland (1992) who found that it is problematic to give up the precise understanding of such a basic term in LIS. Wilson's arguments led him to an agnostic position which Hjørland found unacceptable and unnecessary. Concerning the authors' use of ambiguous terms, the role of the subject analysis is to determine which documents would be fruitful for users to identify whether or not the documents use one or another term or whether a given term in a document is used in one or another meaning. Clear and relevant concepts and distinctions in classification systems and controlled vocabularies may be fruitful even if they are applied to documents with ambiguous terminology.\n\nRequest oriented indexing is indexing in which the anticipated request from users is influencing how documents are being indexed. The indexer ask himself: \"Under which descriptors should this entity be found?\" and \"think of all the possible queries and decide for which ones the entity at hand is relevant\" (Soergel, 1985, p. 230).\n\nRequest oriented indexing may be indexing that is targeted towards a particular audience or user group. For example, a library or a database for feminist studies may index documents different compared to a historical library. It is probably better, however, to understand request oriented indexing as policy based indexing: The indexing is done according to some ideals and reflects the purpose of the library or database doing the indexing. In this way it is not necessarily a kind of indexing based on user studies. Only if empirical data about use or users are applied should request oriented indexing be regarded as a user-based approach.\n\nThe problem of whether the subject is in the content of a document (objectively) or in the mind of the individual users (subjectively) or in a community (intersubjectively, as a social construction) is a part of the philosophical subject–object problem.\n\nRowley & Hartley (2008, p. 109) wrote \"In order to achieve good consistent indexing, the indexer must have a through appreciation of the structure of the subject and the nature of the contribution that the document is making to the advancement of knowledge within a particular discipline\". This is accordance with Hjørland's definition given above.\n\nIn the ISO-standard for topic maps the concept of subject is defined this way: \n\n\"Subject\nAnything whatsoever, regardless of whether it exists or has any other specific characteristics, about which anything whatsoever may be asserted by any means whatsoever.\" ISO 13250-1, here cited from draft: http://www1.y12.doe.gov/capabilities/sgml/sc34/document/0446.htm#overview)\n\nThis definition may work well with the closed system of concepts provided by the topic maps standard. In broader contexts, however, is not fruitful because it does not contain any specification of what to identify in a document or in a discourse when ascribing subject identification terms or symbols to it. If different methods of subject analysis imply different results, which of these results can then be said to reflect the (true) subject? (Given that the expression \"a true subject assignment\" is meaningful at all, which is an important part of the problem). Different persons may have different opinions about what the subject of a specific document is. How can a theoretical understanding of the term \"subject\" be helpful deciding principles of subject analysis?\n\nA proposal for the differentiation between concept indexing and subject indexing was given by Bernier (1980). In his opinion subject indexes are different from, and can be contrasted with, indexes to concepts, topics and words. Subjects are what authors are working and reporting on. A document can have the subject of Chromatography if this is what the author wishes to inform about. Papers using Chromatography as a\nresearch method or discussing it in a subsection do not have Chromatography as subjects. Indexers can easily drift into indexing concepts and words rather than subjects, but this is not good indexing. Bernier does not, however, differentiate author's subjects from those of the information seeker. A user may want a document about a subject, which is different from the one intended by its author. From the point of view of information systems, the subject of a document is related to the questions that the document can answer for the users (cf. the distinction between a content oriented and a request-oriented approach). \n\nHjørland & Nicolaisen (2005) investigated the concept of subject in relation to Bradford's law of scattering and made a distinction between three kinds of scattering:\n\n\"The FRSAR Working Group is aware that some controlled vocabularies provide terminology to express other aspects of works in addition to subject (such as form, genre, and target audience of resources). While very important and the focus of many user queries, these aspects describe isness or what class the work belongs to based on form or genre (e.g., novel, play, poem, essay, biography, symphony, concerto, sonata, map, drawing, painting, photograph, etc.) rather than what the work is about.\" (IFLA, 2010, p. 10).\n\n\"Those LIS authors who have focused on the subjects of visual resources, such as artworks and photographs, have often been concerned with how to distinguish between the \"aboutness\" and the \"ofness\" (both specific and generic depiction or representation) of such works (Shatford, 1986). In this sense, \"aboutness\" has a narrower meaning than that used above. A painting of a sunset over San Francisco, for instance, might be analyzed as being (generically) \"of\" sunsets and (specifically) \"of\" San Francisco, but also \"about\" the passage of time.\" (IFLA, 2010, p. 11).\nSee also: Baca & Harpring (2000) and Shatford (1986).\n\n\nDrake, C. L. (1960). What is a subject? Australian Library Journal, 9, 34–41.\n\nEnglebretsen, George (1987). Subjects. Studia Leibnitiana, Bd. 19, H. 1, pp. 85–90. Published by: Franz Steiner Verlag. \n\nHjørland, Birger (1997): Information Seeking and Subject Representation. An Activity-theoretical approach to Information Science. Westport & London: Greenwood Press.\n\nHjørland, Birger (2009). Book review of: Rowley, Jennifer & Hartley, Richard (2008). Organizing Knowledge. An Introduction to Managing Access to Information. Aldershot: Ashgate Publishing Limited. IN: Journal of Documentation, 65(1), 166–169. Manuscript retrieved 2011-10-15 from: http://arizona.openrepository.com/arizona/bitstream/10150/106533/1/Book_review_Rowley_&_Hartley.doc\n\nIFLA (2010).Functional Requirements for Subject Authority Data (FRSAD): A Conceptual Model. By IFLA Working Group on the Functional Requirements for Subject Authority Records (FRSAR). Edited by Marcia Lei Zeng, Maja umer, Athena Salaba. International Federation of Library Associations and Institutions. Berlin: De Gruyter. Retrieved 2011-09-14 from: http://www.ifla.org/files/classification-and-indexing/functional-requirements-for-subject-authority-data/frsad-final-report.pdf\n\nMiksa, F. (1983b): The Subject in the Dictionary Catalog from Cutter to the Present. Chicago: American Library Association. \n\nWelty, C. A. (1998). The Ontological Nature of Subject Taxonomies. IN: N. Guarino (ed.), Proceedings of the First Conference on Formal Ontology and Information Systems, Amsterdam, IOS Press. http://www.cs.vassar.edu/faculty/welty/papers/fois-98/fois-98-1.html\n"}
{"id": "3701109", "url": "https://en.wikipedia.org/wiki?curid=3701109", "title": "Tajwid", "text": "Tajwid\n\nTajweed ( ', , meaning \"elocution\"), Tajweed is an Arabic word, which literally means to beautify or adorn something. In the context of the recitation of the Quran Tajweed refers to a set of rules for the correct pronunciation of the letters with all its qualities and applying the various of recitation. sometimes rendered as tajweed, refers to the rules governing pronunciation during recitation of the Quran. The term is derived from the triliteral root ' meaning \"to make well, make better, improve\". \"Tajweed\" is a \"fard\" (compulsory-one must learn this best they can) when reciting the Quran to the best of one's ability.\n\nThe Arabic alphabet has 28 basic letters, plus hamzah (ء).\n\nThe Arabic word for \"the\" is ال \"al-\" (i.e. the letter \"alif\" followed by \"lām\"). The \"lām\" in \"al-\" is pronounced if the letter after it is \"qamarīyah\" (\"lunar\"), but if the letter after it is \"shamsīyah\" (\"solar\"), the \"lām\" after it becomes part of the following letter (is assimilated). \"Solar\" and \"lunar\" became descriptions for these instances as the words for \"the moon\" and \"the sun\" (\"al-qamar\" and \"ash-shams\", respectively) are examples of this rule.\n\nLunar letters:\n\nSolar letters:\n\nThere are 17 emission points (\"makhārij al-ḥurūf\") of the letters, located in various regions of the throat, tongue, lips, nose, and the mouth as a whole for the prolonged (\"mudd\") letters.\n\nThe manner of articulation (\"ṣifat al-ḥurūf\") refers to the different attributes of the letters. Some of the characteristics have opposites, while some are individual. An example of a characteristic would be the fricative consonant sound called \"ṣafīr\", which is an attribute of air escaping from a tube.\n\nThe emphatic consonants , known as ' letters, are pronounced with a “heavy accent” ('). This is done by either pharyngealization /ˤ/, i.e. pronounced while squeezing one's voicebox, or by velarization /ˠ/. The remaining letters – the \"muraqqaq\" – have a “light accent” (\"tarqīq\") as they are pronounced normally, without pharyngealization (except , which is often considered a pharyngeal sound).\n\n (') is heavy when accompanied by a ' or ' and light when accompanied by a \"kasrah\". If its vowel sound is cancelled, such as by a ' or the end of a sentence, then it is light when the first preceding voweled letter (without a \"sukun\") has a \"kasrah\". It is heavy if the first preceding voweled letter is accompanied by a \"fatḥah\" or \"ḍammah\". For example, the at the end of the first word of the \"Sūrat \"al-ʻAṣr\"\" is heavy because the  (\"\") has a \"fatḥah\":\n\nProlongation refers to the number of morae (beats of time) that are pronounced when a voweled letter (', ', ') is followed by a \"mudd\" letter (\"alif\", \"yāʼ\" or \"wāw\"). The number of morae then becomes two. If these are at the end of the sentence, such as in all the verses in \"al-Fatiha\", then the number of morae can be more than two, but must be consistent from verse to verse. Additionally, if there is a \"maddah\" sign over the \"mudd\" letter, it is held for four or five morae when followed by a ' () and six morae when followed by a '. For example, the end of the last verse in \"al-Fatiha\" has a six-mora \"maddah\" due to the \"shaddah\" on the (').\n\n\"Nūn sākinah\" refers to instances where the letter \"nūn\" is accompanied by a \"tanwīn\" or \"sukun\" sign. There are then four ways it should be pronounced, depending on which letter immediately follows:\n\n\nThe term \"mīm sākinah\" refers to instances where the letter \"mīm\" is accompanied by a \"sukun\". There are then three ways it should be pronounced, depending on which letter immediately follows:\n\nThe five ' letters are the consonants . ' is the addition of a slight \"bounce\" or reduced vowel sound /ə/ to the consonant whose vowel sound is otherwise cancelled, such as by a ', ', or the end of sentence. The \"lesser bounce\" occurs when the letter is in the middle of a word or at the end of the word but the reader joins it to the next word. A \"medium bounce\" is given when the letter is at the end of the word but is not accompanied by a \"shaddah\", such as the end of the first verse of the \"Sūrat\" \"al-Falaq\":\n\nThe biggest bounce is when the letter is at the end of the word and is accompanied by a \"\", such as the end of the first verse of \"Sūrat\" \"al-Masad\"\":\n\nWaṣl is the rule of not pronouncing alif as a glottal stop /ʔ/, assimilating to its adjacent vowel. It is indicated with the diacritic waṣlah, a small ṣād on the letter alif (ٱ). In Arabic, words starting with alif not using a hamzah (ا) are capable to get a waṣlah.\n\nIn most of cases the vowel that must be used before alif waṣlah is obvious (the short or long vowel before alif waṣlah); but if it is before a syllable ending on another ḥarakah plus sukūn, then these are the rules:\n\nIn the case of Tanwin and alif waṣlah, the intrusive kasrah between them is not graphically represented.\n\nWaqf is the Arabic pausa rule; all words whose last letter end on a harakah become mute (sukūn) when being the last word of a sentence.\n\nIn the case of the proper name عمرو /ʕamrun/, it is pronounced /ʕamr/ in pausa, and the last letter و wāw has no phonetical valor. And in fact, عمرو is a triptote (something irregular in most proper nouns, since they are usually diptotes).\n\n\n\n\n\n\n"}
{"id": "1982558", "url": "https://en.wikipedia.org/wiki?curid=1982558", "title": "Tangut language", "text": "Tangut language\n\nTangut (also Xīxià or Hsi-Hsia or Mi-nia) is an ancient northeastern Tibeto-Burman language once spoken in the Western Xia, also known as the Tangut Empire. It is classified by some linguists as a Qiangic language, which includes the Northern and Southern Qiang languages and the Rgyalrong languages, among others.\n\nTangut was one of the official languages of the Western Xia (known in Tibetan as \"Mi nyag\" and in Chinese as 彌藥 Míyào), which was founded by the Tangut people and obtained its independence from the Song dynasty at the beginning of the 11th century. The Western Xia were annihilated when Genghis Khan invaded in 1226.\n\nThe Tangut language has its own script, the Tangut script.\n\nThe latest known text written in the Tangut language, the Tangut dharani pillars, dates to 1502, suggesting that the language was still in use nearly three hundred years after the destruction of the Tangut Empire.\n\nModern research into the Tangut languages began in the late 19th century and early 20th century when S. W. Bushell, Gabriel Devéria, and Georges Morisse separately published decipherments of a number of Tangut characters found on Western Xia coins, in a Chinese-Tangut bilingual inscription on a stele at Wuwei, Gansu, and in a copy of the Tangut translation of the Lotus Sutra.\n\nThe majority of extant Tangut texts were excavated at Khara-Khoto in 1909 by Pyotr Kozlov, and the script was identified as that of the Tangut state of Xixia. Such scholars as Aleksei Ivanovich Ivanov, Ishihama Juntaro (石濱純太郎), Berthold Laufer, Luo Fuchang (羅福萇), Luo Fucheng (羅福成), and Wang Jingru (王靜如) have contributed to research on the Tangut language. The most significant contribution was made by the Russian scholar Nikolai Aleksandrovich Nevsky (1892–1937), who compiled the first Tangut Dictionary and reconstructed the meaning of a number of Tangut grammatical particles, thus making it possible to actually read and understand Tangut texts. His scholarly achievements were published posthumously in 1960 under the title \"Tangutskaya Filologiya\" (Tangut Philology) and the scholar was eventually (and posthumously) awarded the Soviet Lenin Prize for his work. The understanding of the Tangut language is far from perfect: Although certain aspects of the morphology (Ksenia Kepping, \"The Morphology of the Tangut Language\", Moscow: Nauka, 1985) and grammar (Tatsuo Nishida, \"Seika go no kenkyū\", etc.) are understood, the syntactic structure of Tangut remains largely unexplored.\n\nThe Khara-Khoto documents are at present preserved in the Institute of Oriental Manuscripts of the Russian Academy of Sciences in Saint Petersburg. These survived the Siege of Leningrad, but a number of manuscripts in the possession of Nevsky at the time of his arrest by the NKVD in 1937 went missing, and were returned, under mysterious circumstances, to the Institute of Oriental Manuscripts only in October 1991. The collections amount to about 10,000 volumes, of mostly Buddhist texts, law codes and legal documents dating from mid-11th up to early 13th centuries. Among the Buddhist texts a number of unique compilations, not known either in Chinese or in Tibetan versions, were recently discovered. Furthermore, the Buddhist canon, the Chinese classics, and a great number of indigenous texts written in Tangut have been preserved. These other major Tangut collections, though much smaller in size, belong to the British Library, the National Library in Beijing, the Library of Beijing University and other libraries.\n\nThe connection between the writing and the pronunciation of the Tangut language is even more tenuous than that between Chinese writing and the modern Chinese varieties. Thus although in Chinese more than 90% of the characters possess a phonetic element, this proportion is limited to about 10% in Tangut according to Sofronov. The reconstruction of Tangut pronunciation must resort to other sources.\n\nThe discovery of the \"Pearl in the Palm\", a Tangut-Chinese bilingual glossary, permitted Ivanov (1909) and Laufer (1916) to propose initial reconstructions and to undertake the comparative study of Tangut. This glossary in effect indicates the pronunciation of each Tangut character with one or several Chinese characters, and inversely each Chinese character with one or more Tangut characters. The second source is the corpus of Tibetan transcriptions of Tangut. These data were studied for the first time by Nevsky (Nevskij) (1925).\n\nNonetheless, these two sources were not in themselves sufficient for a systematic reconstruction of Tangut. In effect, these transcriptions were not written with the intention of representing with precision the pronunciation of Tangut, but instead simply to help foreigners to pronounce and memorize the words of one language with the words of another which they could understand.\n\nThe third source, which constitutes the basis of the modern reconstructions, consists of monolingual Tangut dictionaries: the \"Wenhai\" (文海), two editions of the \"Tongyin\" (同音), the \"Wenhai zalei\" (文海雜類) and an untitled dictionary. The record of the pronunciation in these dictionaries is made using the principle of \"fǎnqiè\", borrowed from the Chinese lexicographic tradition. Although these dictionaries may differ on small details (e.g. the \"Tongyin\" categorizes the characters according to syllable initial and rime without taking any account of tone), they all adopt the same system of 105 rimes. A certain number of rimes are in complementary distribution with respect to the place of articulation of the initials, e.g. rimes 10 and 11 or rimes 36 and 37, which shows that the scholars who composed these dictionaries had made a very precise phonological analysis of their language.\n\nIn distinction to the transcription in foreign languages, the Tangut \"fanqie\" makes distinctions among the rhymes in a systematic and very precise manner. Due to the \"fǎnqiè\", we now have a good understanding of the phonological categories of the language. Nonetheless, it is necessary to compare the phonological system of the dictionaries with the other sources in order to \"fill in\" the categories with a phonetic value.\n\nN. A. Nevsky reconstructed Tangut grammar and provided the first Tangut–Chinese–English–Russian dictionary, which together with the collection of his papers was published posthumously in 1960 under the title \"Tangut Philology\" (Moscow: 1960). Later, substantial contribution to the research of Tangut language was done by , Ksenia Kepping, Gong Hwang-cherng (龔煌城), M.V. Sofronov and Li Fanwen (李範文). Marc Miyake has published on Tangut phonology and diachronics. There are four Tangut dictionaries available: the one composed by N.A. Nevsky, one composed by Nishida (1966), one composed by Li Fanwen (1997, revised edition 2008) and one composed by Yevgeny Kychanov (2006).\n\nThere is growing a school of Tangut studies in China. Leading scholars include Shi Jinbo (史金波), Li Fanwen, Nie Hongyin (聶鴻音), Bai Bin (白濱) in mainland China, and Gong Hwang-cherng and Lin Ying-chin (林英津) in Taiwan. In other countries, leading scholars in the field include Yevgeny Kychanov and his student K. J. Solonin in Russia, Nishida Tatsuo and in Japan, and Ruth W. Dunnell in the United States.\n\nThe Tangut syllable has a CV structure and carries one of two distinctive tones, flat or rising. Following the tradition of Chinese phonological analysis the Tangut syllable is divided into initial (声母) and rhyme (韻母) (i.e. the remaining syllable minus the initial).\n\nThe consonants are divided into the following categories:\n\nThe rhyme books distinguish 105 rhyme classes, which are, in turn, classified in several ways:/grade (等), type (環), and class (攝).\n\nTangut rhymes occur in three types (環). They are seen in the tradition of Nishida, followed by both Arakawa and Gong as 'normal' (普通母音), 'tense' (緊候母音), and 'retroflex' (捲舌母音). Gong leaves normal vowels unmarked and places a dot under tense vowels and an -r after retroflex vowels. Arakawa differs only by indicating tense vowels with a final -q.\n\nThe rhyme books distinguish four vowel grades (等). In early phonetic reconstructions, all four were separately accounted for, but it has since been realized that grades three and four are in complementary distribution, depending on the initial. Consequently, the reconstructions of Arakawa and Gong do not account for this distinction. Gong represents these three grades as V, iV, and jV. Arakawa accounts for them as V, iV, and V.\n\nIn general rhyme class (攝), corresponds to the set of all rhymes under the same rhyme type which have the same main vowel.\n\nGong further posits phonemic vowel length and points to evidence that indicates that Tangut had a distinction that Chinese lacked. There is no certainty that the distinction was vowel length and so other researchers have remained skeptical.\n\nMiyake reconstructs the vowels differently. In his reconstruction, the 95 vowels of Tangut formed from a six-vowel system in Pre-Tangut because of preinitial loss. (The two vowels in parentheses appeared only in loanwords from Chinese, and many of the vowels in class III were in complementary distribution with their equivalents in class IV.)\n\n"}
{"id": "18958780", "url": "https://en.wikipedia.org/wiki?curid=18958780", "title": "Technical time-out (volleyball)", "text": "Technical time-out (volleyball)\n\nA technical time-out in volleyball and beach volleyball is a time-out stipulated by the Fédération Internationale de Volleyball (FIVB) in each non-tie-breaking set. It is the formalized equivalent of a television timeout in other sports:\n\n\n"}
{"id": "392216", "url": "https://en.wikipedia.org/wiki?curid=392216", "title": "Two-finger salute", "text": "Two-finger salute\n\nThe two-finger salute is a salute given using only the middle and index fingers, while bending the other fingers at the second knuckle, and with the palm facing the signer. This salute is used by the Polish Armed Forces, other uniformed services, and, in some countries, the Cub Scouts.\n\nThe Polish two-finger salute is only used while wearing a headdress with the emblem of the Polish eagle (such as military hat rogatywka) or without this emblem (such as Boonie hat or helmet). The salute is performed with the middle and index fingers extended and touching each other, while the ring and little fingers are bent and touched by the thumb. The tips of the middle and index fingers touch the peak of the cap, two fingers meaning \"honour\" and \"fatherland\" (\"Honor i Ojczyzna\").\n\nIt is not clear when the two-fingers salute appeared in Polish military forces. Some see its origin in Tadeusz Kościuszko's 1794 . Others state that it came from Polish soldiers in the Congress Kingdom army around 1815 (partitioned Poland). At that time, apparently the Tsar's Viceroy in Poland Grand Duke Constantine said that Poles salute him with two fingers, while using the other two to hold a stone to throw at him. Another legend attributes the salute to the remembrance of Battle of Olszynka Grochowska in 1831, when a soldier who lost in the battle all his fingers but the middle and index ones, saluted his superior with the wounded hand, and died after it.\n\nIt symbolized God and Country.\n\nThe two-fingers salute caused problems for Polish units serving with the Allies on the western front during World War II. Allied officers, seeing what they perceived as a Cub Scout's salute, thought that Polish soldiers were deliberately being disrespectful. As a result, many soldiers were arrested, until the misunderstanding could be explained. This led to the temporary use of the full hand salute when saluting foreign officers.\n\nMany Cub Scout sections also use a two-finger salute. The salute was devised by Robert Baden-Powell and originally represented the two ears of a wolf cub, since the original programme was based on Rudyard Kipling's \"The Jungle Book\". However, Cubs in several national associations now use the three-finger Scout salute used by the rest of the Scout Movement.\n\n"}
{"id": "1187424", "url": "https://en.wikipedia.org/wiki?curid=1187424", "title": "Wutana language", "text": "Wutana language\n\nA hypothetical Wutana language was mentioned in early editions of the Ethnologue as spoken in Nigeria, but has now been removed. The inclusion of Wutana in the Ethnologue was based on two sentences in a 1922 article by Olive Temple:\n\nand\n\nRoger Blench also cites Temple in his Atlas of Nigerian languages. Nothing is known of this language apart from its name and location, including whether it even exists. \n\n"}
{"id": "34259", "url": "https://en.wikipedia.org/wiki?curid=34259", "title": "Yet another", "text": "Yet another\n\nAmong programmers, yet another (often abbreviated ya, Ya, or YA in the initial part of an acronym) is an idiomatic qualifier in the name of a computer program, organisation, or event that is confessedly unoriginal.\n\nStephen C. Johnson is credited with establishing the naming convention in the late 1970s when he named his compiler-compiler yacc (Yet Another Compiler-Compiler), since he felt there were already numerous compiler-compilers in circulation at the time.\n\n\n"}
