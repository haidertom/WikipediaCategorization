{"id": "16289784", "url": "https://en.wikipedia.org/wiki?curid=16289784", "title": "A History of Vector Analysis", "text": "A History of Vector Analysis\n\nA History of Vector Analysis (1967) is a book on the history of vector analysis by Michael J. Crowe, originally published by the University of Notre Dame Press.\nAs a scholarly treatment of a reformation in technical communication, the text is a contribution to the history of science. In 2002, Crowe gave a talk summarizing the book, including an entertaining introduction in which he covered its publication history and related the award of a Jean Scott prize of $4000. Crowe had entered the book in a competition for \"a study on the history of complex and hypercomplex numbers\" twenty-five years after his book was first published.\n\nThe book has eight chapters: the first on the origins of vector analysis including Ancient Greek and 16th and 17th century influences; the second on the 19th century William Rowan Hamilton and quaternions; the third on other 19th and 18th century vectorial systems including equipollence of Giusto Bellavitis and the exterior algebra of Hermann Grassmann.\n\nChapter four is on the general interest in the 19th century on vectorial systems including analysis of journal publications as well as sections on major figures and their views (e.g., Peter Guthrie Tait as an advocate of Quaternions and James Clerk Maxwell as a critic of Quaternions); the fifth chapter describes the development of the modern system of vector analysis by Josiah Willard Gibbs and Oliver Heaviside.\n\nIn chapter six, \"Struggle for existence\",\nMichael J. Crowe delves into the zeitgeist that pruned down quaternion theory into vector analysis on three-dimensional space. He makes clear the ambition of this effort by considering five major texts as well as a couple dozen articles authored by participants in \"The Great Vector Debate\". These are the books:\n\nTwenty of the ancillary articles appeared in Nature; others were in Philosophical Magazine, London or Edinburgh Proceedings of the Royal Society, Physical Review, and Proceedings of the American Association for the Advancement of Science. The authors included Cargill Gilston Knott and a half-dozen other hands.\n\nThe \"struggle for existence\" is a phrase from Charles Darwin’s Origin of Species and Crowe quotes Darwin: \"…young and rising naturalists,…will be able to view both sides of the question with impartiality.\" After 1901 with the Gibbs/Wilson/Yale publication Vector Analysis, the question was decided in favour of the vectorialists with separate dot and cross products. The pragmatic temper of the times set aside the four-dimensional source of vector algebra.\n\nCrowe's chapter seven is a survey of \"Twelve major publications in Vector Analysis from 1894 to 1910\". Of these twelve, seven are in German, two in Italian, one in Russian, and two in English. Whereas the previous chapter examined a debate in English, the final chapter notes the influence of Heinrich Hertz' results with radio and the rush of German research using vectors. Joseph George Coffin of MIT and Clark University published his \"Vector Analysis\" in 1909; it too leaned heavily into applications. Thus Crowe provides a context for Gibbs and Wilson’s famous textbook of 1901.\n\nThe eighth chapter is the author's summary and conclusions. The book relies on references in chapter endnotes instead of a bibliography section. Crowe also states that the \"Bibliography\" of the Quaternion Society, and its supplements to 1912, already listed all the primary literature for the study.\n\nThere were significant reviews given near the time of original publication. Stanley Goldberg wrote \"The polemics on both sides make very rich reading, especially when they are spiced with the sarcastic wit of a Heaviside, and the fervent, almost religious railing of a Tait.\" Morris Kline begins his 1969 review with \"Since historical publications on modern developments are rare, this book is welcome.\" and ends with \"the subtitle [,The Evolution of the Idea of a Vectorial System,] is a better description of the contents than the title proper.\" Then William C. Waterhouse—picking up where Kline's review left off—writes in 1972 \"Crowe's book on vector analysis seems a little anemic in comparison, perhaps because its title is misleading. ... [Crowe] does succeed in his goal of tracing the genealogy of the 3-space system, concluding that it was developed out of quaternions by physicists.\"\n\nIn 2003 Sandro Caparrini challenged Crowe’s conclusions by noting that \"geometrical representations of forces and velocities by means of directed line segments...was already fairly well known by the middle of the eighteenth century\" in his essay \"Early Theories of Vectors\". Caparrini cites several sources, in particular Gaetano Giorgini (1795 — 1874) and his appreciation in an 1830 article by Michel Chasles. Caparrini goes on to indicate that moments of forces and angular velocities were recognized as vectorial entities in the second half of the eighteenth century.\n\n"}
{"id": "54426651", "url": "https://en.wikipedia.org/wiki?curid=54426651", "title": "Ackermann's formula", "text": "Ackermann's formula\n\nAckermann's formula is a control system design method for solving the pole allocation problem. One of the primary problems in control system design is the creation of controllers that will alter the dynamics of a system and alter the poles to a more suitable, and sometimes more stable, state. Such a problem can be tackled by many different methods; one such solution is the addition of a feedback loop in such a way that a gain is added to the input with which one can change the poles of the original system. If the system is controllable, an efficient method for pole placement is Ackermann's formula, which allows one to choose arbitrary poles within the system. \n\nConsider a linear time invariant system with a state-space representation\n\nwhere formula_3 is the state vector, formula_4 is the input vector, and formula_5 are matrices of compatible dimensions that represent the dynamics of our system, and, for simplicity's sake, assume formula_6. An input-output description of this system is given by the transfer function\n\nSince formula_8, the denominator of the system is given by the characteristic polynomial of formula_9. Thus, the poles of the system are the eigenvalues of formula_10.\n\nIf the system is unstable, or has a slow response or any other characteristic that does not specify the design criteria, it could be advantageous to make changes to it. The realization given by formula_11, however, represents the dynamics of the system, and sometimes cannot be altered. Thus, one approach to this problem might be to create a feedback loop with a gain formula_12 that will feed the state variable into the input.\n\nIf the system is controllable, there is always an input formula_13 such that any state formula_14 can be transferred to any other state formula_15. With that in mind, a feedback loop can be added to the system with the control input formula_16, such that the new dynamics of the system will be \n\nIn this new realization, the poles will be dependent on the characteristic polynomial formula_19 of formula_20, that is \n\nComputing the characteristic polynomial and choosing a suitable feedback matrix can be a challenging task, especially in larger systems. One way to make computations easier is through Ackermann's formula. For simplicity's sake, consider a single input vector with no reference parameter formula_22, such as \n\nwhere formula_25 is a feedback vector of compatible dimensions. Given that the system is still controllable, Ackermann's method states that the design process can be simplified by the following equation: \n\nin which formula_27 is the desired characteristic polynomial evaluated at matrix formula_10. \n\nAssume that the system is controllable. Defining formula_29 gives\n\nCalculating the powers of formula_31 results in \n\nReplacing the previous equations into formula_36 yields \n\nNoting that formula_39.\n\nRewriting the above equation as a matrix product and omitting the terms that formula_25 does not appear isolated gives\n\nFrom the Cayley–Hamilton theorem, formula_43, thus\n\nformula_44\n\nNote that formula_45 is the controllability matrix of the system. Since the system is controllable, formula_46 is invertible. Thus, \n\nTo find formula_48, both sides can be multiplied by the vector formula_49. This yields \n\nThus,\n\nConsider\n\nformula_52\n\nWe know from the characteristic polynomial of formula_10 that the system is unstable since formula_54, the matrix formula_10 will only have positive eigenvalues. Thus, to stabilize the system we shall put a feedback gain formula_56\n\nFrom Ackermann's formula, we can find a matrix formula_57 that will change the system so that its characteristic equation will be equal to a desired polynomial. Suppose we want formula_58.\n\nThus, formula_59 and computing the controllability matrix yields\n\nAlso, we have that formula_62\n\nFinally, from Ackermann's formula\n"}
{"id": "351908", "url": "https://en.wikipedia.org/wiki?curid=351908", "title": "Almost surely", "text": "Almost surely\n\nIn probability theory, one says that an event happens almost surely (sometimes abbreviated as a.s.) if it happens with probability one. In other words, the set of possible exceptions may be non-empty, but it has probability zero. The concept is precisely the same as the concept of \"almost everywhere\" in measure theory.\n\nIn probability experiments on a finite sample space, there is often no difference between \"almost surely\" and \"surely\". However, the distinction becomes important when the sample space is an infinite set, because an infinite set can have non-empty subsets of probability zero.\n\nSome examples of the use of this concept include the strong and uniform versions of the law of large numbers, and the continuity of the paths of Brownian motion.\n\nThe terms almost certainly (a.c.) and almost always (a.a.) are also used. Almost never describes the opposite of \"almost surely\": an event that happens with probability zero happens \"almost never\".\n\nLet formula_1 be a probability space. An event formula_2 happens \"almost surely\" if formula_3. Equivalently, formula_4 happens almost surely if the probability of formula_4 not occurring is zero: formula_6. More generally, any event formula_7 (not necessarily in formula_8) happens almost surely if formula_9 is contained in a null set: a subset of some formula_10 such that The notion of almost sureness depends on the probability measure formula_11. If it is necessary to emphasize this dependence, it is customary to say that the event formula_4 occurs \"P\"-almost surely, or almost surely (\"P\").\n\nIn general, an event can happen \"almost surely\" even if the probability space in question includes outcomes which do not belong to the event, as is illustrated in the examples below.\n\nImagine throwing a dart at a unit square (i.e. a square with area 1) so that the dart always hits exactly one point of the square, and so that each point in the square is equally likely to be hit. \n\nNow, notice that since the square has area 1, the probability that the dart will hit any particular subregion of the square equals the area of that subregion. For example, the probability that the dart will hit the right half of the square is 0.5, since the right half has area 0.5.\n\nNext, consider the event that \"the dart hits a diagonal of the unit square exactly\". Since the areas of the diagonals of the square are zero, the probability that the dart lands exactly on a diagonal is zero. So, the dart will almost never land on a diagonal (i.e. it will almost surely \"not\" land on a diagonal). Nonetheless the set of points on the diagonals is not empty and a point on a diagonal is no less possible than any other point: the diagonal does contain valid outcomes of the experiment.\n\nConsider the case where a (possibly biased) coin is tossed, corresponding to the probability space formula_13, where the event formula_14 occurs if heads is flipped, and formula_15 if tails. For this particular coin, assume the probability of flipping heads is formula_16 from which it follows that the complement event, flipping tails, has formula_17.\n\nSuppose we were to conduct an experiment where the coin is tossed repeatedly, with outcomes formula_18, and it is assumed each flip's outcome is independent of all the others. That is, they are \"i.i.d.\". Define the sequence of random variables on the coin toss space, formula_19 where formula_20. \"i.e.\" each formula_21 records the outcome of the formula_22'th flip.\n\nAny infinite sequence of heads and tails is a possible outcome of the experiment. However, any particular infinite sequence of heads and tails has probability zero of being the exact outcome of the (infinite) experiment. To see why, note that the \"i.i.d.\" assumption implies that the probability of flipping all heads over formula_23 flips is simply formula_24. Letting formula_25 yields zero, since formula_26 by assumption. Note that the result is the same no matter how much we bias the coin towards heads, so long as we constrain formula_27 to be greater than 0, and less than 1.\n\nIn particular, the event \"the sequence contains at least one formula_28\" happens almost surely (i.e., with probability 1).\nHowever, if instead of an infinite number of flips we stop flipping after some finite time, say a million flips, then the all-heads sequence has non-zero probability. The all-heads sequence has probability formula_29, while the probability of getting at least one tails is formula_30 and the event is no longer almost sure.\n\nIn asymptotic analysis, one says that a property holds asymptotically almost surely (a.a.s.) if, over a sequence of sets, the probability converges to 1. For instance, a large number is asymptotically almost surely composite, by the prime number theorem; and in random graph theory, the statement \"formula_31 is connected\" (where formula_32 denotes the graphs on formula_23 vertices with edge probability formula_27) is true a.a.s. when, for any formula_35\n\nIn number theory this is referred to as \"almost all\", as in \"almost all numbers are composite\". Similarly, in graph theory, this is sometimes referred to as \"almost surely\".\n\n\n"}
{"id": "420595", "url": "https://en.wikipedia.org/wiki?curid=420595", "title": "Axioms (journal)", "text": "Axioms (journal)\n\nAxioms is a peer-reviewed open access scientific journal that focuses on all aspects of mathematics, mathematical logic and mathematical physics. It was established in June 2012 and is published quarterly by MDPI. \n\nThe editor-in-chief is Humberto Bustince ( Public University of Navarra).\nThe journal is abstracted and indexed in:\n"}
{"id": "21867395", "url": "https://en.wikipedia.org/wiki?curid=21867395", "title": "Ax–Grothendieck theorem", "text": "Ax–Grothendieck theorem\n\nIn mathematics, the Ax–Grothendieck theorem is a result about injectivity and surjectivity of polynomials that was proved independently by James Ax and Alexander Grothendieck.\n\nThe theorem is often given as this special case: If \"P\" is an injective polynomial function from an \"n\"-dimensional complex vector space to itself then \"P\" is bijective. That is, if \"P\" always maps distinct arguments to distinct values, then the values of \"P\" cover all of C.\n\nThe full theorem generalizes to any algebraic variety over an algebraically closed field.\n\nGrothendieck's proof of the theorem is based on proving the analogous theorem for finite fields and their algebraic closures. That is, for any field \"F\" that is itself finite or that is the closure of a finite field, if a polynomial \"P\" from \"F\" to itself is injective then it is bijective.\n\nIf \"F\" is a finite field, then \"F\" is finite. In this case the theorem is true for trivial reasons having nothing to do with the representation of the function as a polynomial: any injection of a finite set to itself is a bijection. When \"F\" is the algebraic closure of a finite field, the result follows from Hilbert's Nullstellensatz. The Ax–Grothendieck theorem for complex numbers can therefore be proven by showing that a counterexample over C would translate into a counterexample in some algebraic extension of a finite field.\n\nThis method of proof is noteworthy in that it is an example of the idea that finitistic algebraic relations in fields of characteristic 0 translate into algebraic relations over finite fields with large characteristic. Thus, one can use the arithmetic of finite fields to prove a statement about C even though there is no homomorphism from any finite field to C. The proof thus uses model-theoretic principles to prove an elementary statement about polynomials. The proof for the general case uses a similar method.\n\nThere are other proofs of the theorem. Armand Borel gave a proof using topology. The case of \"n\" = 1 and field C follows since C is algebraically closed and can also be thought of as a special case of the result that for any analytic function \"f\" on C, injectivity of \"f\" implies surjectivity of \"f\". This is a corollary of Picard's theorem.\n\nAnother example of reducing theorems about morphisms of finite type to finite fields can be found in EGA IV: There, it is proved that a radicial \"S\"-endomorphism of a scheme \"X\" of finite type over \"S\" is bijective (10.4.11), and that if \"X\"/\"S\" is of finite presentation, and the endomorphism is a monomorphism, then it is an automorphism (17.9.6). Therefore, a scheme of finite presentation over a base \"S\" is a cohopfian object in the category of \"S\"-schemes.\n\nThe Ax–Grothendieck theorem may also be used to prove the Garden of Eden theorem, a result that like the Ax–Grothendieck theorem relates injectivity with surjectivity but in cellular automata rather than in algebraic fields. Although direct proofs of this theorem are known, the proof via the Ax–Grothendieck theorem extends more broadly, to automata acting on amenable groups.\n\nSome partial converses to the Ax-Grothendieck Theorem:\n\n"}
{"id": "35831726", "url": "https://en.wikipedia.org/wiki?curid=35831726", "title": "Bandwidth-sharing game", "text": "Bandwidth-sharing game\n\nA bandwidth-sharing game is a type of resource allocation game designed to model the real-world allocation of bandwidth to many users in a network. The game is popular in game theory because the conclusions can be applied to real-life networks. The game is described as follows:\n\n\nWe also use assumptions regarding formula_3\n\nThe game arises from trying to find a price formula_14 so that every player individually optimizes their own welfare. This implies every player must individually find formula_15. Solving for the maximum yields formula_16.\n\nWith this maximum condition, the game then becomes a matter of finding a price that satisfies an equilibrium. Such a price is called a market clearing price.\n\nA popular idea to find the price is a method called fair sharing. In this game, every player formula_2 is asked for amount they are willing to pay for the given resource denoted by formula_6. The resource is then distributed in formula_19 amounts by the formula formula_20. This method yields an effective price formula_21.\nThis price can proven to be market clearing thus the distribution formula_22 is optimal. The proof is as so:\n\nformula_23\n\nformula_24\nformula_25\nformula_26\nformula_27\n\nComparing this result to the equilibrium condition above, we see that when formula_28 is very small, the two conditions equal each other and thus, the fair sharing game is almost optimal.\n"}
{"id": "37165004", "url": "https://en.wikipedia.org/wiki?curid=37165004", "title": "Bucket evaluations", "text": "Bucket evaluations\n\nIn statistics, bucket evaluations is a method for correlating vectors. This method is a non-parametric, unsupervised correlation method first published in 2012 by Shabtai et al.\n\nBucket evaluations was initially constructed for genetic research, and was used for finding a new potential anti-cancer drug.\n\nBucket evaluations is named after the technique used to compare vectors in a matrix. Values in the vector are compared in sections (buckets). The buckets are set in a descending order, where the smallest buckets hold the highest scores, and have the strongest effect on the final correlation score. The similarity between vectors is calculated by comparing the ranks of the scores in each bucket, which are summed up to a similarity score.\n"}
{"id": "2921396", "url": "https://en.wikipedia.org/wiki?curid=2921396", "title": "Cabri Geometry", "text": "Cabri Geometry\n\nCabri Geometry is a commercial interactive geometry software produced by the French company Cabrilog for teaching and learning geometry and trigonometry. It was designed with ease-of-use in mind. The program allows the user to animate geometric figures, proving a significant advantage over those drawn on a blackboard. Relationships between points on a geometric object may easily be demonstrated, which can be useful in the learning process. There are also graphing and display functions which allow exploration of the connections between geometry and algebra. The program can be run under Windows or the Mac OS.\n\n"}
{"id": "33868", "url": "https://en.wikipedia.org/wiki?curid=33868", "title": "Casorati–Weierstrass theorem", "text": "Casorati–Weierstrass theorem\n\nIn complex analysis, a branch of mathematics, the Casorati–Weierstrass theorem describes the behaviour of holomorphic functions near their essential singularities. It is named for Karl Theodor Wilhelm Weierstrass and Felice Casorati. In Russian literature it is called Sokhotski's theorem.\n\nStart with some open subset formula_1 in the complex plane containing the number formula_2, and a function formula_3 that is holomorphic on formula_4, but has an essential singularity at formula_2 . The \"Casorati–Weierstrass theorem\" then states that \n\nThis can also be stated as follows: \n\nOr in still more descriptive terms:\n\nThe theorem is considerably strengthened by Picard's great theorem, which states, in the notation above, that formula_3 assumes \"every\" complex value, with one possible exception, infinitely often on formula_6.\n\nIn the case that formula_3 is an entire function and formula_22, the theorem says that the values formula_23\napproach every complex number and formula_24, as formula_13 tends to infinity.\nIt is remarkable that this does not hold for holomorphic maps in higher dimensions,\nas the famous example of Pierre Fatou shows.\n\nThe function \"f\"(\"z\") = exp(1/\"z\") has an essential singularity at 0, but the function \"g\"(\"z\") = 1/\"z\" does not (it has a pole at 0).\n\nConsider the function\n\nThis function has the following Taylor series about the essential singular point at 0:\n\nBecause formula_28 exists for all points \"z\" ≠ 0 we know that \"ƒ\"(\"z\") is analytic in a punctured neighborhood of \"z\" = 0. Hence it is an isolated singularity, as well as being an essential singularity. \n\nUsing a change of variable to polar coordinates formula_29 our function, \"ƒ\"(\"z\") = \"e\" becomes:\n\nTaking the absolute value of both sides:\n\nThus, for values of \"θ\" such that cos \"θ\" > 0, we have formula_32 as formula_33, and for formula_34, formula_35 as formula_33.\n\nConsider what happens, for example when \"z\" takes values on a circle of diameter 1/\"R\" tangent to the imaginary axis. This circle is given by \"r\" = (1/\"R\") cos \"θ\". Then,\n\nand\n\nThus,formula_39 may take any positive value other than zero by the appropriate choice of \"R\". As formula_40 on the circle, formula_41 with \"R\" fixed. So this part of the equation:\n\ntakes on all values on the unit circle infinitely often. Hence \"f\"(\"z\") takes on the value of every number in the complex plane except for zero infinitely often.\n\nA short proof of the theorem is as follows:\n\nTake as given that function \"f\" is meromorphic on some punctured neighborhood \"V\" \\ {\"z\"}, and that \"z\" is an essential singularity. Assume by way of contradiction that some value \"b\" exists that the function can never get close to; that is: assume that there is some complex value \"b\" and some ε > 0 such that |\"f\"(\"z\") − \"b\"| ≥ ε for all \"z\" in \"V\" at which \"f\" is defined.\n\nThen the new function:\n\nmust be holomorphic on \"V\" \\ {\"z\"}, with zeroes at the poles of \"f\", and bounded by 1/ε. It can therefore be analytically continued (or continuously extended, or holomorphically extended) to \"all\" of \"V\" by Riemann's analytic continuation theorem. So the original function can be expressed in terms of \"g\":\n\nfor all arguments \"z\" in \"V\" \\ {\"z\"}. Consider the two possible cases for\n\nIf the limit is 0, then \"f\" has a pole at \"z\" . If the limit is not 0, then \"z\" is a removable singularity of \"f\" . Both possibilities contradict the assumption that the point \"z\" is an essential singularity of the function \"f\" . Hence the assumption is false and the theorem holds.\n\nThe history of this important theorem is described by\nCollingwood and Lohwater.\nIt was published by Weierstrass in 1876 (in German) and by Sokhotski in 1868 in his Master thesis (in Russian).\nSo it was called Sokhotski's theorem in the Russian literature and Weierstrass's theorem in\nthe Western literature. \nThe same theorem was published by Casorati in 1868, and\nby Briot and Bouquet in the \"first edition\" of their book (1859).\nHowever, Briot and Bouquet \"removed\" this theorem from the second edition (1875).\n\n"}
{"id": "3948656", "url": "https://en.wikipedia.org/wiki?curid=3948656", "title": "Center manifold", "text": "Center manifold\n\nIn mathematics, the center manifold of an equilibrium point of a dynamical system consists of orbits whose behavior around the equilibrium point is not controlled by either the attraction of the stable manifold or the repulsion of the unstable manifold. The first step when studying equilibrium points of dynamical systems is to linearize the system. The eigenvectors corresponding to eigenvalues with negative real part form the stable eigenspace, which gives rise to the stable manifold. Similarly, eigenvalues with positive real part yield the unstable manifold.\n\nThis concludes the story if the equilibrium point is hyperbolic (i.e., all eigenvalues of the linearization have nonzero real part). However, if there are eigenvalues whose real part is zero, then these give rise to the center manifold. If the eigenvalues are precisely zero, rather than just real part being zero, then these more specifically give rise to a slow manifold. The behavior on the center (slow) manifold is generally not determined by the linearization and thus is more difficult to study.\n\nCenter manifolds play an important role in: bifurcation theory because interesting behavior takes place on the center manifold; and multiscale mathematics because the long time dynamics often are attracted to a relatively simple center manifold.\n\nLet formula_1 be a dynamical system with equilibrium point formula_2. The linearization of the system near the equilibrium point is\n\nThe Jacobian matrix formula_4 defines three main subspaces:\nDepending upon the application, other subspaces of interest include center-stable, center-unstable, sub-center, slow, and fast subspaces.\nThese subspaces are all invariant subspaces of the linearized equation.\n\nCorresponding to the linearized system, the nonlinear system has invariant manifolds, each consisting of sets of orbits of the nonlinear system.\n\nThe center manifold existence theorem states that if the right-hand side function formula_11 is formula_12 (formula_13 times continuously differentiable), then at every equilibrium point there exists a neighborhood of some finite size in which there is at least one of \n\nIn example applications, a nonlinear coordinate transform to a normal form can clearly separate these three manifolds. A web service currently undertakes the necessary computer algebra for a range of finite-dimensional systems.\n\nIn the case when the unstable manifold does not exist, center manifolds are often relevant to modelling.\nThe center manifold emergence theorem then says that the neighborhood may be chosen so that all solutions of the system staying in the neighborhood tend exponentially quickly to some solution formula_17 on the center manifold.\nThat is, \nformula_18\nfor some rate formula_19.\nThis theorem asserts that for a wide variety of initial conditions the solutions of the full system decay exponentially quickly to a solution on the relatively low dimensional center manifold.\n\nA third theorem, the approximation theorem, asserts that if an approximate expression for such invariant manifolds, say formula_20, satisfies the differential equation for the system to residuals formula_21 as formula_22, then the invariant manifold is approximated by formula_20 to an error of the same order, namely formula_21.\n\nHowever, some applications, such as to shear dispersion, require an infinite-dimensional center manifold. The most general and powerful theory was developed by Aulbach and Wanner. They addressed non-autonomous dynamical systems formula_25 in infinite dimensions, with potentially infinite dimensional stable, unstable and center manifolds. Further, they usefully generalised the definition of the manifolds so that the center manifold is associated with eigenvalues such that formula_26, the stable manifold with eigenvalues formula_27, and unstable manifold with eigenvalues formula_28. They proved existence of these manifolds, and the emergence of a center manifold, via nonlinear coordinate transforms. Potzsche and Rasmussen established a corresponding approximation theorem for such infinite dimensional, non-autonomous systems.\n\nAs the stability of the equilibrium correlates with the \"stability\" of its manifolds, the existence of a center manifold brings up the question about the dynamics on the center manifold. This is analyzed by the center manifold reduction, which, in combination with some system parameter μ, leads to the concepts of bifurcations.\n\nCorrespondingly, two web services currently undertake the necessary computer algebra to construct just the center manifold for a wide range of finite-dimensional systems (provided they are in multinomial form). \n\nThe Wikipedia entry on slow manifolds gives more examples.\n\nConsider the system\nThe unstable manifold at the origin is the \"y\" axis, and the stable manifold is the trivial set {(0, 0)}. Any orbit not on the stable manifold satisfies an equation on the form formula_30 for some real constant \"A\". It follows that for any real \"A\", we can create a center manifold by piecing together the curve formula_30 for \"x\" > 0 with the negative \"x\" axis (including the origin). Moreover, all center manifolds have this potential non-uniqueness, although often the non-uniqueness only occurs in unphysical complex values of the variables.\n\nAnother example shows how a center manifold\nmodels the Hopf bifurcation that occurs\nfor parameter formula_32 in the\ndelay differential equation\nformula_33. \nStrictly, the delay makes this DE infinite-dimensional.\n\nFortunately, we may approximate such delays by the following trick that keeps the dimensionality finite.\nDefine formula_34\nand approximate the time delayed variable, \nformula_35, by using the intermediaries\nformula_36 and\nformula_37.\n\nFor parameter near\ncritical, formula_38, the delay differential equation is then approximated by the system\nCopying and pasting the appropriate entries, \nthe web service finds that in terms of a complex amplitude formula_40 \nand its complex conjugate formula_41, the center manifold\nand the evolution on the center manifold is\nThis evolution shows the origin is linearly unstable for formula_44, but the cubic nonlinearity then stabilises nearby limit cycles as in classic Hopf bifurcation.\n"}
{"id": "53165511", "url": "https://en.wikipedia.org/wiki?curid=53165511", "title": "Characterization of probability distributions", "text": "Characterization of probability distributions\n\nIn mathematics in general, a characterization theorem says that a particular object – a function, a space, etc. – is the only one that possesses properties specified in the theorem. A characterization of a probability distribution accordingly states that it is the only probability distribution that satisfies specified conditions. More precisely, the model of characterization of\nprobability distribution was described by V.M. Zolotarev in such manner. On the probability space we define the space formula_1 of random variables with values in measurable metric space formula_2 and the space formula_3 of random variables with values in measurable metric space formula_4. By characterizations of probability distributions we understand general problems of description of some set formula_5 in the space formula_6 by extracting the sets formula_7 and formula_8 which describe the properties of random variables formula_9 and their images formula_10, obtained by means of a specially chosen mapping formula_11.\n<br>The description of the properties of the random variables formula_12 and of their images formula_13 is equivalent to the indication of the set formula_7 from which formula_12 must be taken and of the set formula_8 into which its image must fall. So, the set which interests us appears therefore in the following form:\n\nwhere formula_18 denotes the complete inverse image of formula_19 in formula_20. This is the general model of characterization of probability distribution. Some examples of characterization theorems:\n\n\n<br>\nVerification of conditions of characterization theorems in practice is possible only with some error formula_45, i.e., only to a certain degree of accuracy. Such a situation is observed, for instance, in the cases where a sample of finite size is considered. That is why there arises the following natural question. Suppose that the conditions of the characterization theorem are fulfilled not exactly but only approximately. May we assert that the conclusion of the theorem is also fulfilled approximately? The theorems in which the problems of this kind are considered are called stability characterizations of probability distributions.\n\n"}
{"id": "2156387", "url": "https://en.wikipedia.org/wiki?curid=2156387", "title": "Common Algebraic Specification Language", "text": "Common Algebraic Specification Language\n\nThe Common Algebraic Specification Language (CASL) is a general-purpose specification language based on first-order logic with induction. Partial functions and subsorting are also supported.\n\nCASL has been designed by CoFI, the Common Framework Initiative, with the aim to subsume many existing specification languages.\n\nCASL comprises four levels:\n\nThe four levels are orthogonal to each other. In particular, it is possible to use CASL structured and architectural specifications and libraries with logics other than CASL. For this purpose, the logic has to be formalized as an institution. This feature is also used by the CASL extensions.\n\nSeveral extensions of CASL have been designed: \n\n"}
{"id": "2471934", "url": "https://en.wikipedia.org/wiki?curid=2471934", "title": "Constraint counting", "text": "Constraint counting\n\nIn mathematics, constraint counting is counting the number of constraints in order to compare it with the number of variables, parameters, etc. that are free to be determined, the idea being that in most cases the number of independent choices that can be made is the excess of the latter over the former.\n\nFor example, in linear algebra if the number of constraints (independent equations) in a system of linear equations equals the number of unknowns then precisely one solution exists; if there are fewer independent equations than unknowns, an infinite number of solutions exist; and if the number of independent equations exceeds the number of unknowns, then no solutions exist.\n\nIn the context of partial differential equations, constraint counting is a crude but often useful way of counting the number of \"free functions\" needed to specify a solution to a partial differential equation.\n\nConsider a second order partial differential equation in three variables, such as the two-dimensional wave equation\nIt is often profitable to think of such an equation as a \"rewrite rule\" allowing us to rewrite arbitrary partial derivatives of the function formula_2 using fewer partials than would be needed for an arbitrary function. For example, if formula_3 satisfies the wave equation, we can rewrite \nwhere in the first equality, we appealed to the fact that \"partial derivatives commute\".\n\nTo answer this in the important special case of a linear partial differential equation, Einstein asked: how many of the partial derivatives of a solution can be linearly independent? It is convenient to record his answer using an ordinary generating function\nwhere formula_6 is a natural number counting the number of linearly independent partial derivatives (of order k) of an arbitrary function in the solution space of the equation in question.\n\nWhenever a function satisfies some partial differential equation, we can use the corresponding rewrite rule to eliminate some of them, because \"further mixed partials have necessarily become linearly dependent\". Specifically, the power series counting the variety of \"arbitrary\" functions of three variables (no constraints) is\nbut the power series counting those in the solution space of some second order p.d.e. is\nwhich records that we can eliminate \"one\" second order partial formula_9, \"three\" third order partials formula_10, and so forth.\n\nMore generally, the o.g.f. for an arbitrary function of n variables is\nwhere the coefficients of the infinite power series of the generating function are constructed using an appropriate infinite sequence of binomial coefficients, and the power series for a function required to satisfy a linear m-th order equation is\n\nNext,\nwhich can be interpreted to predict that a solution to a second order linear p.d.e. in \"three\" variables is expressible by two \"freely chosen\" functions of \"two\" variables, one of which is used immediately, and the second, only after taking a \"first derivative\", in order to express the solution.\n\nTo verify this prediction, recall the solution of the initial value problem\nApplying the Laplace transform formula_15 gives\nApplying the Fourier transform formula_17 to the two spatial variables gives\nor\nApplying the inverse Laplace transform gives\nApplying the inverse Fourier transform gives\nwhere\nHere, p,q are arbitrary (sufficiently smooth) functions of two variables, so (due their modest time dependence) the integrals P,Q also count as \"freely chosen\" functions of two variables; as promised, one of them is differentiated once before adding to the other to express the general solution of the initial value problem for the two dimensional wave equation.\n\nIn the case of a nonlinear equation, it will only rarely be possible to obtain the general solution in closed form. However, if the equation is \"quasilinear\" (linear in the highest order derivatives), then we can still obtain approximate information similar to the above: specifying a member of the solution space will be \"modulo nonlinear quibbles\" equivalent to specifying a certain number of functions in a smaller number of variables. The number of these functions is the \"Einstein strength\" of the p.d.e. In the simple example above, the strength is two, although in this case we were able to obtain more precise information.\n\n"}
{"id": "33522862", "url": "https://en.wikipedia.org/wiki?curid=33522862", "title": "Equational logic", "text": "Equational logic\n\nFirst-order equational logic consists of quantifier-free terms of ordinary first-order logic, with equality as the only predicate symbol. The model theory of this logic was developed into Universal algebra by Birkhoff, Grätzer and Cohn. It was later made into a branch of category theory by Lawvere (\"algebraic theories\").\nThe terms of equational logic are built up from variables and constants using function symbols (or operations).\n\nHere are the four inference rules of logic formula_1. formula_2 denotes textual substitution of expression formula_1 for variable formula_4 in expression formula_5. formula_6 denotes equality, for formula_7 and formula_8 of the same type, while formula_9, or equivalence, is defined only for formula_7 and formula_8 of type boolean. For formula_7 and formula_8 of type boolean, formula_6 and formula_9 have the same meaning.\n\nEquational logic was developed over the years (beginning in the early 1980s) by researchers in the formal development of programs, who felt a need for an effective style of manipulation, of calculation. Involved were people like Roland Carl Backhouse, Edsger W. Dijkstra, Wim H.J. Feijen, David Gries, Carel S. Scholten, and Netty van Gasteren. Wim Feijen is responsible for important details of the proof format.\n\nThe axioms are similar to those use by Dijkstra and Scholten in their monograph \"Predicate calculus and program semantics\" (Springer Verlag, 1990), but our order of presentation is slightly different.\n\nIn their monograph, Dijkstra and Scholten use the three inference rules Leibniz, Substitution, and Transitivity. However, Dijkstra/Scholten system is not a logic, as logicians use the word. Some of their manipulations are based on the meanings of the terms involved, and not on clearly presented syntactical rules of manipulation. The first attempt at making a real logic out of it appeared in \"A Logical Approach to Discrete Math\". However, inference rule Equanimity is missing there, and the definition of theorem is contorted to account for it. The introduction of Equanimity and its use in the proof format is due to Gries and Schneider. It is used, for example, in the proofs of soundness and completeness, and it will appear in the second edition of \"A Logical Approach to Discrete Math\".\n\nWe explain how the four inference rules are used in proofs, using the proof of formula_16. The logic symbols formula_17 and formula_18 indicate \"true\" and \"false,\" respectively, and formula_19 indicates \"not.\" The theorem numbers refer to theorems of \"A Logical Approach to Discrete Math\".\n\nformula_20\n\nFirst, lines formula_21–formula_22 show a use of inference rule Leibniz:\n\nformula_23\n\nis the conclusion of Leibniz, and its premise formula_24 is given on line formula_25. In the same way, the equality on lines formula_22–formula_27 are substantiated using Leibniz.\n\nThe \"hint\" on line formula_25 is supposed to give a premise of Leibniz, showing what substitution of equals for equals is being used. This premise is theorem formula_29 with the substitution formula_30, i.e.\n\nformula_31\n\nThis shows how inference rule Substitution is used within hints.\n\nFrom formula_32 and formula_33, we conclude by inference rule Transitivity that formula_34. This shows how Transitivity is used.\n\nFinally, note that line formula_27, formula_36, is a theorem, as indicated by the hint to its right. Hence, by inference rule Equanimity, we conclude that line formula_21 is also a theorem. And formula_21 is what we wanted to prove.\n\n"}
{"id": "50704764", "url": "https://en.wikipedia.org/wiki?curid=50704764", "title": "Francisco Gomes Teixeira", "text": "Francisco Gomes Teixeira\n\nFrancisco Gomes Teixeira (28 January 1851, São Cosmado, Armamar – 8 February 1933, Porto) was a Portuguese mathematician and historian of mathematics.\n\nIn 1876 he became a corresponding member of the Academia Real das Ciências de Lisboa.\n\nHe published over 140 articles in prestigious international scientific journals. Before the year 1890 most of his publications were on mathematical analysis but from 1890 onwards most were on geometry.\nHe was named the third astronomer of the Observatório Astronómico de Lisboa in 1878, but only held this positions for about four months before returning to the University of Coimbra.\n\nHe was elected a parliamentary deputy by the Partido Regenerador in 1879 and participated in sessions of Parliament for that year and also in 1883 and 1884. In November 1879 he was put in charge of the University of Coimbra's chair of mathematical analysis and in February 1880 was formally appointed to this professorial chair.\n\nIn 1884 Gomes Teixeira was appointed to the chair of differential and integral calculus of the \"Academia Politécnica do Porto\". In 1905 the \"Jornal de Sciencias Mathematicas e Astronomicas\" (founded by Gomes in 1877) was integrated into the newly created \"Anais Scientificos da Academia Politécnica do Porto\".\n\nHis \"Tratado de las Curvas Especiales Notables\" won an award in 1899 from the Spanish Royal Academy of Sciences. A 3-volume French translation (with additions) was published in 1908 and 1909 as \"Traité des Courbes Spéciales Remarquables Planes et Gauches\". He received in 1917 the \"prix Binoux d'histoire des sciences\" from the French Academy of Sciences.\n\nGomes Teixeira received honorary doctorates from the University of Madrid and the University of Toulouse. In 1911 at the newly formed University of Porto he became the first rector, retiring in 1917. \n\nHis body is entombed in the \"Igreja Matriz de São Cosmado\". The tomb consists of a granite sarcophagus with the following inscription:\n(\"Divo Antonio\" is Latin for St. Anthony. Olissipóna was the ancient name for Lisbon. Gomes Teixeira wrote a 1931 book \"Santo António de Lisboa (história, tradição e lenda)\" and a 1926 book \"Santuários de montahna (impressões de viagens\".)\n\n\n\n"}
{"id": "394527", "url": "https://en.wikipedia.org/wiki?curid=394527", "title": "Frieder Nake", "text": "Frieder Nake\n\nFrieder Nake (born December 16, 1938 in Stuttgart, Germany) is a mathematician, computer scientist, and pioneer of computer art. He is best known internationally for his contributions to the earliest manifestations of computer art, a field of computing that made its first public appearances with three small exhibitions in 1965.\n\nWithout knowing of each other, A. Michael Noll, Georg Nees, and Frieder Nake in 1963/64 had begun to write computer programs to automatically generate drawings of an aesthetic quality and without other (technical or economic) purposes. Georg Nees became the first to exhibit his works (February 5–19, 1965, in Stuttgart). A. Michael Noll followed (April 6–24, 1965 in New York City, with Bela Julesz). Frieder Nake had his first exhibition at Galerie Wendelin Niedlich in Stuttgart (November 5–26, 1965, with Georg Nees).\n\nUntil 1969, Nake generated in rapid sequence a large number of works that he showed in many exhibitions over the years. He estimates his production at about 300 to 400 works during those years. A few were limited screenprint editions, single pieces and portfolios. The bulk were done as China ink on paper graphics, carried out by a flatbed high precision plotter (the Zuse Graphomat Z64).\n\nNake participated in the important group shows of the 1960s, such as, most prominently, Cybernetic Serendipity (London, UK, 1968), Tendencies 4: Computers and Visual Research (Zagreb, Yugoslavia, 1968), Ricerca e Progettazione. Proposte per una esposizione sperimentale (35th Venice Biennale, Italy, 1970), Arteonica (São Paulo, Brazil, 1971).\n\nIn 1971, he wrote a short and provocative note for Page, the Bulletin of the Computer Arts Society (whose member he was and still is), under the title „There Should Be No Computer-Art“ (Page No. 18, Oct. 1971, p. 1-2. Reprinted in Arie Altena, Lucas van der Velden (eds.): The anthology of computer art. Amsterdam: Sonic Acts 2006, p. 59-60). The note sparked a lively controversial debate among those who had meanwhile started to build an active community of artists, writers, musicians, and designers in the digital domain. His statement was rooted in a moral position. The involvement of computer technology in the Vietnam War and in massive attempts by capital to automate productive processes and, thereby, generate unemployment, should not allow artists to close their eyes and become silent servants of the ruling classes by reconciling high technology with the masses of the poor and suppressed.\n\nFrieder Nake has been a professor of interactive computer graphics at the Department of Computer Science at Bremen, Germany, since 1972. Since 2005, he has also been teaching digital media design there. After studying mathematics at the University of Stuttgart, where he earned his Diplom and doctoral degrees (in probability theory), he has taught in Stuttgart, Toronto and Vancouver, before coming to Bremen. His courses and seminars, besides computer graphics, interactivity, and digital media, are in the areas of computer art, aesthetics, semiotics, computers and society, and theory of computing. He has been a visiting professor to Universitetet Oslo, Aarhus Universitet, Universität Wien, Danube University Krems, University of Colorado, University of Lübeck, University of Basel, University of Costa Rica, Xi'an University of Science and Technology and Tongji University.\n\nHe won the First Prize of the Computer Art Contest of Computers & Automation in 1966. In 1997, his teaching work was honored by the Berninghausen Award for Excellence and Innovation in Teaching (University of Bremen).\n\nHis book \"Ästhetik als Informationsverarbeitung\" (1974) is one of the first to study connections between aesthetics, computing, and information theory, which has become important to the transdisciplinary area of digital media. This book and many of his ca. 300 publications (2012) evince his intellectual position between science and the humanities – a position that has always made him critical about the marvels and wonders of information technology.\n\nIn his publications, seminars, and lectures, he has developed the following fundamental concepts of a (cultural) theory of computing:\n\n\nBesides being represented in many private collections, his works are held by Abteiberg Museum Mönchengladbach (Germany), Kunsthalle Bremen (Germany), Victoria and Albert Museum London (UK), Museum of Contemporary Art, Zagreb (Croatia), Sprengel Museum Hannover (Germany), Mary and Leigh Block Museum of Art Evanston (USA), Tama Art University Museum Tokyo (Japan).\n\n\n"}
{"id": "332256", "url": "https://en.wikipedia.org/wiki?curid=332256", "title": "Gabriel's Horn", "text": "Gabriel's Horn\n\nGabriel's horn (also called Torricelli's trumpet) is a geometric figure which has infinite surface area but finite volume. The name refers to the biblical tradition identifying the Archangel Gabriel as the angel who blows the horn to announce Judgment Day, associating the divine, or infinite, with the finite. The properties of this figure were first studied by Italian physicist and mathematician Evangelista Torricelli in the 17th century.\n\nGabriel's horn is formed by taking the graph of\nwith the domain (thus avoiding the asymptote at ) and rotating it in three dimensions about the -axis. The discovery was made using Cavalieri's principle before the invention of calculus, but today calculus can be used to calculate the volume and surface area of the horn between and , where . Using integration (see Solid of revolution and Surface of revolution for details), it is possible to find the volume and the surface area :\n\nThe surface area formula above gives a lower bound for the area as 2 times the natural logarithm of . There is no upper bound for the natural logarithm of as approaches infinity. That means, in this case, that the horn has an infinite surface area. That is to say;\n\nWhen the properties of Gabriel's horn were discovered, the fact that the rotation of an infinitely large section of the -plane about the -axis generates an object of finite volume was considered paradoxical. While the section lying in the -plane has an infinite area, any other section parallel to it has a finite area. Thus the volume, being calculated from the 'weighted sum' of sections, is finite.\n\nAnother approach is to treat the horn as a stack of disks with diminishing radii. The sum of the radii produces a harmonic series that goes to infinity. However, the correct calculation is the sum of their squares. Every disk has a radius and an area or . The series diverges but converges. In general, for any real , converges.\n\nThe apparent paradox formed part of a dispute over the nature of infinity involving many of the key thinkers of the time including Thomas Hobbes, John Wallis and Galileo Galilei.\n\nThere is a similar phenomenon which applies to lengths and areas in the plane. The area between the curves and from 1 to infinity is finite, but the lengths of the two curves are clearly infinite.\n\nSince the horn has finite volume but infinite surface area, there is an apparent paradox that the horn could be filled with a finite quantity of paint, and yet that paint would not be sufficient to coat its inner surface. The \"paradox\" is resolved by realizing that a finite amount of paint can in fact coat an infinite surface area — it simply needs to get thinner at a fast enough rate. (Much like the series gets smaller fast enough that its sum is finite.) In the case where the horn is filled with paint, this thinning is accomplished by the increasing reduction in diameter of the throat of the horn.\n\nThe converse phenomenon of Gabriel's horn – a surface of revolution that has a \"finite\" surface area but an \"infinite\" volume – cannot occur:\n\nLet be a continuously differentiable function. Write for the solid of revolution of the graph about the -axis. \"If the surface area of is finite, then so is the volume.\"\n\nSince the lateral surface area is finite, the limit superior:\nTherefore, there exists a such that the supremum } is finite. Hence,\nFinally, the volume:\nTherefore: \"if the area is finite, then the volume must also be finite.\"\n\n\n\n"}
{"id": "1058719", "url": "https://en.wikipedia.org/wiki?curid=1058719", "title": "Harmonic spectrum", "text": "Harmonic spectrum\n\nA harmonic spectrum is a spectrum containing only frequency components whose frequencies are whole number multiples of the fundamental frequency; such frequencies are known as harmonics. \"The individual partials are not heard separately but are blended together by the ear into a single tone.\"\n\nIn other words, if formula_1 is the fundamental frequency, then a harmonic spectrum has the form \n\nA standard result of Fourier analysis is that a function has a harmonic spectrum if and only if it is periodic.\n\n"}
{"id": "22386332", "url": "https://en.wikipedia.org/wiki?curid=22386332", "title": "Hausdorff–Young inequality", "text": "Hausdorff–Young inequality\n\nIn mathematics, the Hausdorff−Young inequality bounds the \"L\"-norm of the Fourier coefficients of a periodic function for \"q\" ≥ 2. proved the inequality for some special values of \"q\", and proved it in general. More generally the inequality also applies to the Fourier transform of a function on a locally compact group, such as R, and in this case and gave a sharper form of it called the Babenko–Beckner inequality.\n\nWe consider the Fourier operator, namely let \"T\" be the operator that takes a function formula_1 on the unit circle and outputs \nthe sequence of its Fourier coefficients\n\nParseval's theorem shows that \"T\" is bounded from formula_3 to formula_4 with norm 1. On the other hand, clearly,\n\nso \"T\" is bounded from formula_6 to formula_7 with norm 1. Therefore we may invoke the Riesz–Thorin theorem to get, for any 1 < \"p\" < 2 that \"T\", as an operator from formula_8 to formula_9, is bounded with norm 1, where\n\nIn a short formula, this says that\n\nThis is the well known Hausdorff–Young inequality. For \"p\" > 2 the natural extrapolation of this inequality fails, and the fact that a function belongs to formula_8, does not give any additional information on the order of growth of its Fourier series beyond the fact that it is in formula_4.\n\nThe constant involved in the Hausdorff–Young inequality can be made optimal by using careful estimates from the theory of harmonic analysis. If formula_14 for formula_15\nwhere formula_16 is the Hölder conjugate of formula_17 \n\n"}
{"id": "32573740", "url": "https://en.wikipedia.org/wiki?curid=32573740", "title": "Hautus lemma", "text": "Hautus lemma\n\nIn control theory and in particular when studying the properties of a linear time-invariant system in state space form, the Hautus lemma, named after Malo Hautus, can prove to be a powerful tool. This result appeared first in and. Today it can be found in most textbooks on control theory.\n\nThere exist multiple forms of the lemma.\nThe Hautus lemma for controllability says that given a square matrix formula_1 and a formula_2 the following are equivalent:\n\nThe Hautus lemma for stabilizability says that given a square matrix formula_1 and a formula_2 the following are equivalent:\nThe Hautus lemma for observability says that given a square matrix formula_1 and a formula_17 the following are equivalent:\n\nThe Hautus lemma for detectability says that given a square matrix formula_1 and a formula_17 the following are equivalent:\n\n"}
{"id": "11273721", "url": "https://en.wikipedia.org/wiki?curid=11273721", "title": "Hierarchical temporal memory", "text": "Hierarchical temporal memory\n\nHierarchical temporal memory (HTM) is a biologically constrained theory (or model) of intelligence, originally described in the 2004 book \"On Intelligence\" by Jeff Hawkins with Sandra Blakeslee. HTM is based on neuroscience and the physiology and interaction of pyramidal neurons in the neocortex of the mammalian (in particular, human) brain. \n\nAt the core of HTM are learning algorithms that can store, learn, infer and recall high-order sequences. Unlike most other machine learning methods, HTM learns (in an unsupervised fashion) time-based patterns in unlabeled data on a continuous basis. HTM is robust to noise, and it has high capacity, meaning that it can learn multiple patterns simultaneously. When applied to computers, HTM is well suited for prediction, anomaly detection, classification and ultimately sensorimotor applications.\n\nThe theory has been tested and implemented in software through example applications from Numenta and a few commercial applications from Numenta's partners.\n\nA typical HTM network is a tree-shaped hierarchy of \"levels\" (which should \"not\" be confused with the \"layers\" of the neocortex, as described ) that are composed of smaller elements called \"region\"s (or nodes). A single level in the hierarchy possibly contains several regions. Higher hierarchy levels often have fewer regions. Higher hierarchy levels can reuse patterns learned at the lower levels by combining them to memorize more complex patterns.\n\nEach HTM region has the same basic functionality. In learning and inference modes, sensory data (e.g. data from the eyes) comes into the bottom level regions. In generation mode, the bottom level regions output the generated pattern of a given category. The top level usually has a single region that stores the most general categories (concepts) which determine, or are determined by, smaller concepts in the lower levels which are more restricted in time and space. When set in inference mode, a region (in each level) interprets information coming in from its child regions in the lower level as probabilities of the categories it has in memory.\n\nEach HTM region learns by identifying and memorizing spatial patterns, which are combinations of input bits that often occur at the same time. It then identifies temporal sequences of spatial patterns that are likely to occur one after another.\n\nHTM is an evolving theory (or model), that is, it isn't yet a complete theory, as our knowledge of the neocortex is incomplete. The new findings on the neocortex are thus progressively incorporated into the HTM model, which can thus change over time. The new findings do not necessarily invalidate the previous ones, so ideas from one generation are not necessarily excluded in its successive one. Because of this evolving nature of the theory, there have been several generations of HTM algorithms , which are briefly described below.\n\nThe first generation of HTM algorithms (or the first version of the HTM theory) is sometimes referred to as \"zeta 1\".\n\nDuring \"training\", a node (or region) receives a temporal sequence of spatial patterns as its input. The learning process consists of two stages: \n\n\nThe concepts of \"spatial pooling\" and \"temporal pooling\" are still quite important in the current HTM theory. Temporal pooling is not yet well understood, and its meaning has changed over time (as the HTM theory evolved).\n\nDuring inference, the node calculates the set of probabilities that a pattern belongs to each known coincidence. Then it calculates the probabilities that the input represents each temporal group. The set of probabilities assigned to the groups is called a node's \"belief\" about the input pattern. (In a simplified implementation, node's belief consists of only one winning group). This belief is the result of the inference that is passed to one or more \"parent\" nodes in the next higher level of the hierarchy.\n\n\"Unexpected\" patterns to the node do not have a dominant probability of belonging to any one temporal group, but have nearly equal probabilities of belonging to several of the groups. If sequences of patterns are similar to the training sequences, then the assigned probabilities to the groups will not change as often as patterns are received. The output of the node will not change as much, and a resolution in time is lost.\n\nIn a more general scheme, the node's belief can be sent to the input of any node(s) in any level(s), but the connections between the nodes are still fixed. The higher-level node combines this output with the output from other child nodes thus forming its own input pattern.\n\nSince resolution in space and time is lost in each node as described above, beliefs formed by higher-level nodes represent an even larger range of space and time. This is meant to reflect the organization of the physical world as it is perceived by human brain. Larger concepts (e.g. causes, actions and objects) are perceived to change more slowly and consist of smaller concepts that change more quickly. Jeff Hawkins postulates that brains evolved this type of hierarchy to match, predict, and affect the organization of the external world.\nMore details about the functioning of Zeta 1 HTM can be found in Numenta's old documentation.\n\nThe second generation of HTM learning algorithms, often referred to as cortical learning algorithms (CLA), was drastically different from zeta 1. It relies on a data structure called sparse distributed representations (that is, a data structure whose elements are binary, 1 or 0, and whose number of 1 bits is small compared to the number of 0 bits) to represent the brain activity and a more biologically-realistic neuron model (often also referred to as cell, in the context of the HTM theory). There are two core components in this HTM theory: a spatial pooling algorithm , which outputs sparse distributed representations (SDR), and a sequence memory algorithm, which learns to represent and predict complex sequences.\n\nIn this new generation, the layers and minicolumns of the cerebral cortex are addressed and partially modeled. Each HTM layer (not to be confused with an HTM level of an HTM hierarchy, as described above) consists of a number of highly interconnected minicolumns. An HTM layer creates a sparse distributed representation from its input, so that a fixed percentage of \"minicolumns\" are active at any one time. A minicolumn is understood as a group of cells that have the same receptive field. Each minicolumn has a number of cells that are able to remember several previous states. A cell can be in one of three states: \"active\", \"inactive\" and \"predictive\" state.\n\nThe receptive field of each minicolumn is a fixed number of inputs that are randomly selected from a much larger number of node inputs. Based on the (specific) input pattern, some minicolumns will be more or less associated with the active input values. Spatial pooling selects a relatively constant number of the most active minicolumns and inactivates (inhibits) other minicolumns in the vicinity of the active ones. Similar input patterns tend to activate a stable set of minicolumns. The amount of memory used by each layer can be increased to learn more complex spatial patterns or decreased to learn simpler patterns.\n\nAs mentioned above, a cell (or a neuron) of a minicolumn, at any point in time, can be in an active, inactive or predictive state. Initially, cells are inactive.\n\nIf one or more cells in the active minicolumn are in the \"predictive\" state (see below), they will be the only cells to become active in the current time step. If none of the cells in the active minicolumn are in the predictive state (which happens during the initial time step or when the activation of this minicolumn was not expected), all cells are made active.\n\nWhen a cell becomes active, it gradually forms connections to nearby cells that tend to be active during several previous time steps. Thus a cell learns to recognize a known sequence by checking whether the connected cells are active. If a large number of connected cells are active, this cell switches to the \"predictive\" state in anticipation of one of the few next inputs of the sequence. \n\nThe output of a layer includes minicolumns in both active and predictive states. Thus minicolumns are active over longer periods of time, which leads to greater temporal stability seen by the parent layer.\n\nCortical learning algorithms are able to learn continuously from each new input pattern, therefore no separate inference mode is necessary. During inference, HTM tries to match the stream of inputs to fragments of previously learned sequences. This allows each HTM layer to be constantly predicting the likely continuation of the recognized sequences. The index of the predicted sequence is the output of the layer. Since predictions tend to change less frequently than the input patterns, this leads to increasing temporal stability of the output in higher hierarchy levels. Prediction also helps to fill in missing patterns in the sequence and to interpret ambiguous data by biasing the system to infer what it predicted.\n\nCortical learning algorithms are currently being offered as commercial SaaS by Numenta (such as Grok).\n\nThe following question was posed to Jeff Hawkins September 2011 with regard to cortical learning algorithms: \"How do you know if the changes you are making to the model are good or not?\" To which Jeff's response was \"There are two categories for the answer: one is to look at neuroscience, and the other is methods for machine intelligence. In the neuroscience realm there are many predictions that we can make, and those can be tested. If our theories explain a vast array of neuroscience observations then it tells us that we’re on the right track. In the machine learning world they don’t care about that, only how well it works on practical problems. In our case that remains to be seen. To the extent you can solve a problem that no one was able to solve before, people will take notice.\"\n\nThe third generation builds on the second generation and adds in a theory of sensorimotor inference in the neocortex. This theory proposes that cortical columns at every level of the hierarchy can learn complete models of objects over time and that features are learned at specific locations on the objects.\n\nHTM attempts to implement the functionality that is characteristic of a hierarchically related group of cortical regions in the neocortex. A \"region\" of the neocortex corresponds to one or more \"levels\" in the HTM hierarchy, while the hippocampus is remotely similar to the highest HTM level. A single HTM node may represent a group of cortical columns within a certain region.\n\nAlthough it is primarily a functional model, several attempts have been made to relate the algorithms of the HTM with the structure of neuronal connections in the layers of neocortex. The neocortex is organized in vertical columns of 6 horizontal layers. The 6 layers of cells in the neocortex should not be confused with levels in an HTM hierarchy.\n\nHTM nodes attempt to model a portion of cortical columns (80 to 100 neurons) with approximately 20 HTM \"cells\" per column. HTMs model only layers 2 and 3 to detect spatial and temporal features of the input with 1 cell per column in layer 2 for spatial \"pooling\", and 1 to 2 dozen per column in layer 3 for temporal pooling. A key to HTMs and the cortex's is their ability to deal with noise and variation in the input which is a result of using a \"sparse distributive representation\" where only about 2% of the columns are active at any given time.\n\nAn HTM attempts to model a portion of the cortex's learning and plasticity as described above. Differences between HTMs and neurons include:\n\nIntegrating memory component with neural networks has a long history dating back to early research in distributed representations and self-organizing maps. For example, in sparse distributed memory (SDM), the patterns encoded by neural networks are used as memory addresses for content-addressable memory, with \"neurons\" essentially serving as address encoders and decoders.\n\nComputers store information in \"dense\" representations such as a 32 bit word where all combinations of 1s and 0s are possible.\nBy contrast, brains use sparse distributed representations (SDR). The human neocortex has roughly 100 billion neurons, but at any given time only a small percent are active. The activity of neurons are like bits in a computer, and therefore the representation is sparse. Similarly to SDM developed by NASA in the 80s and vector space models used in Latent semantic analysis, HTM also uses Sparse Distributed Representations.\n\nThe SDRs used in HTM are binary representations of data consisting of many bits with a small percentage of the bits active (1s); a typical implementation might have 2048 columns and 64K artificial neurons where as few as 40 might be active at once. Although it may seem less efficient for the majority of bits to go \"unused\" in any given representation, SDRs have two major advantages over traditional dense representations. First, SDRs are tolerant of corruption and ambiguity due to the meaning of the representation being shared (\"distributed\") across a small percentage (\"sparse\") of active bits. In a dense representation, flipping a single bit completely changes the meaning, while in an SDR a single bit may not affect the overall meaning much. This leads to the second advantage of SDRs: because the meaning of a representation is distributed across all active bits, similarity between two representations can be used as a measure of semantic similarity in the objects they represent. That is, if two vectors in an SDR have 1s in the same position, then they are semantically similar in that attribute. The bits in SDRs have semantic meaning, and that meaning is distributed across the bits.\n\nThe semantic folding theory builds on these SDR properties to propose a new model for language semantics, where words are encoded into word-SDRs and the similarity between terms, sentences and texts can be calculated with simple distance measures.\n\nLikened to a Bayesian network, an HTM comprises a collection of nodes that are arranged in a tree-shaped hierarchy. Each node in the hierarchy discovers an array of causes in the input patterns and temporal sequences it receives. A Bayesian belief revision algorithm is used to propagate feed-forward and feedback beliefs from child to parent nodes and vice versa. However, the analogy to Bayesian networks is limited, because HTMs can be self-trained (such that each node has an unambiguous family relationship), cope with time-sensitive data, and grant mechanisms for covert attention.\n\nA theory of hierarchical cortical computation based on Bayesian belief propagation was proposed earlier by Tai Sing Lee and David Mumford. While HTM is mostly consistent with these ideas, it adds details about handling invariant representations in the visual cortex.\n\nLike any system that models details of the neocortex, HTM can be viewed as an artificial neural network. The tree-shaped hierarchy commonly used in HTMs resembles the usual topology of traditional neural networks. HTMs attempt to model cortical columns (80 to 100 neurons) and their interactions with fewer HTM \"neurons\". The goal of current HTMs is to capture as much of the functions of neurons and the network (as they are currently understood) within the capability of typical computers and in areas that can be made readily useful such as image processing. For example, feedback from higher levels and motor control are not attempted because it is not yet understood how to incorporate them and binary instead of variable synapses are used because they were determined to be sufficient in the current HTM capabilities.\n\nLAMINART and similar neural networks researched by Stephen Grossberg attempt to model both the infrastructure of the cortex and the behavior of neurons in a temporal framework to explain neurophysiological and psychophysical data. However, these networks are, at present, too complex for realistic application.\n\nHTM is also related to work by Tomaso Poggio, including an approach for modeling the ventral stream of the visual cortex known as HMAX. Similarities of HTM to various AI ideas are described in the December 2005 issue of the Artificial Intelligence journal.\n\nNeocognitron, a hierarchical multilayered neural network proposed by Professor Kunihiko Fukushima in 1987, is one of the first Deep Learning Neural Networks models.\n\nThe Numenta Platform for Intelligent Computer (NuPIC) is one of several available HTM implementations. Some are provided by Numenta, while some are developed and maintained by the HTM open source community.\n\nNuPIC includes implementations of Spatial Pooling and Temporal Memory in both C++ and Python. It also includes 3 APIs. Users can construct HTM systems using direct implementations of the algorithms, or construct a Network using the Network API, which is a flexible framework for constructing complicated associations between different Layers of cortex.\n\nNuPIC 1.0 was released on July 2017, after which the codebase was put into maintenance mode. Current research continues in Numenta research codebases.\n\nThe following commercial applications are available using NuPIC: \nThe following tools are available on NuPIC:\nThe following example applications are available on NuPIC, see http://numenta.com/applications/:\n\n\n\n\n"}
{"id": "4329360", "url": "https://en.wikipedia.org/wiki?curid=4329360", "title": "Holland's schema theorem", "text": "Holland's schema theorem\n\nHolland's schema theorem, also called the fundamental theorem of genetic algorithms, is an inequality that results from coarse-graining an equation for evolutionary dynamics. The Schema Theorem says that short, low-order schemata with above-average fitness increase exponentially in frequency in successive generations. The theorem was proposed by John Holland in the 1970s. It was initially widely taken to be the foundation for explanations of the power of genetic algorithms. However, this interpretation of its implications has been criticized in several publications reviewed in , where the Schema Theorem is shown to be a special case of the Price equation with the schema indicator function as the macroscopic measurement. \n\nA schema is a template that identifies a subset of strings with similarities at certain string positions. Schemata are a special case of cylinder sets, and hence form a topological space.\n\nConsider binary strings of length 6. The schema codice_1 describes the set of all strings of length 6 with 1's at positions 1, 3 and 6 and a 0 at position 4. The * is a wildcard symbol, which means that positions 2 and 5 can have a value of either 1 or 0. The \"order of a schema\" formula_1 is defined as the number of fixed positions in the template, while the \"defining length\" formula_2 is the distance between the first and last specific positions. The order of codice_1 is 4 and its defining length is 5. The \"fitness of a schema\" is the average fitness of all strings matching the schema. The fitness of a string is a measure of the value of the encoded problem solution, as computed by a problem-specific evaluation function. Using the established methods and genetic operators of genetic algorithms, the schema theorem states that short, low-order schemata with above-average fitness increase exponentially in successive generations. Expressed as an equation:\n\nHere formula_4 is the number of strings belonging to schema formula_5 at generation formula_6, formula_7 is the \"observed\" average fitness of schema formula_5 and formula_9 is the \"observed\" average fitness at generation formula_6. The probability of disruption formula_11 is the probability that crossover or mutation will destroy the schema formula_5. It can be expressed as:\n\nwhere formula_1 is the order of the schema, formula_15 is the length of the code, formula_16 is the probability of mutation and formula_17 is the probability of crossover. So a schema with a shorter defining length formula_2 is less likely to be disrupted.An often misunderstood point is why the Schema Theorem is an \"inequality\" rather than an equality. The answer is in fact simple: the Theorem neglects the small, yet non-zero, probability that a string belonging to the schema formula_5 will be created \"from scratch\" by mutation of a single string (or recombination of two strings) that did \"not\" belong to formula_5 in the previous generation.\n\nThe schema theorem holds under the assumption of a genetic algorithm that maintains an infinitely large population, but does not always carry over to (finite) practice: due to sampling error in the initial population, genetic algorithms may converge on schemata that have no selective advantage. This happens in particular in multimodal optimization, where a function can have multiple peaks: the population may drift to prefer one of the peaks, ignoring the others.\n\nThe reason that the Schema Theorem cannot explain the power of genetic algorithms is that it holds for all problem instances, and cannot distinguish between problems in which genetic algorithms perform poorly, and problems for which genetic algorithms perform well.\n\n"}
{"id": "25302674", "url": "https://en.wikipedia.org/wiki?curid=25302674", "title": "Horatio Scott Carslaw", "text": "Horatio Scott Carslaw\n\nDr Horatio Scott Carslaw FRSE LLD (12 February 1870, Helensburgh, Dumbartonshire, Scotland – 11 November 1954, Burradoo, New South Wales, Australia) was a Scottish-Australian mathematician. The book he wrote with his colleague John Conrad Jaeger, \"Conduction of Heat in Solids\", remains a classic in the field.\n\nHe was born in Helensburgh the son of the Rev Dr William Henderson Carslaw (a Free Church minister) and his wife, Elizabeth Lockhead. He was educated at The Glasgow Academy. He went on to study at Cambridge University and then obtained a postgraduate doctorate at Glasgow University. He was elected a Fellow of the Royal Society of Edinburgh in 1901.\n\nIn 1903, upon the retirement of Theodore Thomas Gurney, Carslaw was appointed Professor and the Chair of Pure and Applied Mathematics in the now School of Mathematics and Statistics at the University of Sydney. He retired in 1935 to his house in Burradoo where he produced most of his best work. The Carslaw Building at the University, completed in the 1960s and containing the School, is named after him.\n\nHe died at home in Burradoo and was buried in the Anglican section of Bowral Cemetery.\n\nHe married Ethel Maude Clarke (daughter of Sir William Clarke, 1st Baronet)in 1907 but she died later in the same year.\n\n\n\n"}
{"id": "37542163", "url": "https://en.wikipedia.org/wiki?curid=37542163", "title": "Horrocks construction", "text": "Horrocks construction\n\nIn mathematics, the Horrocks construction is a method for constructing vector bundles, especially over projective spaces, introduced by . His original construction gave an example of an indecomposable rank 2 vector bundle over 3-dimensional projective space, and generalizes to give examples of vector bundles of higher ranks over other projective spaces. The Horrocks construction is used in the ADHM construction to construct instantons over the 4-sphere.\n\n"}
{"id": "54978454", "url": "https://en.wikipedia.org/wiki?curid=54978454", "title": "Ignaz Schütz", "text": "Ignaz Schütz\n\nIgnaz Robert Schütz (1867, Březová (Moravia) – 1927, Brno) was a Czech–German mathematician and a physicist.\n\nHe studied at the University of Munich where in 1894 he obtained a Ph.D in physics. Schütz was assistant to Ludwig Boltzmann in Munich from 1891 to 1894, the year of Boltzmann's departure from Munich. In 1897, Ignaz R. Schütz, then a member of the Institute for Theoretical Physics at Göttingen, showed how time translational symmetry induces conservation of energy.\n"}
{"id": "89489", "url": "https://en.wikipedia.org/wiki?curid=89489", "title": "Inequality (mathematics)", "text": "Inequality (mathematics)\n\nIn mathematics, an inequality is a relation that holds between two values when they are different (see also: equality).\n\nIf the values in question are elements of an ordered set, such as the integers or the real numbers, they can be compared in size.\n\nIn contrast to strict inequalities, there are two types of inequality relations that are not strict:\n\nIn engineering sciences, a less formal use of the notation is to state that one quantity is \"much greater\" than another, normally by several orders of magnitude.\n\n\nInequalities are governed by the following properties. All of these properties also hold if all of the non-strict inequalities (≤ and ≥) are replaced by their corresponding strict inequalities (< and >) and (in the case of applying a function) monotonic\nfunctions are limited to \"strictly\" monotonic functions.\n\nThe transitive property of inequality states:\n\nThe relations ≤ and ≥ are each other's converse:\n\nA common constant \"c\" may be added to or subtracted from both sides of an inequality:\ni.e., the real numbers are an ordered group under addition.\n\nThe properties that deal with multiplication and division state:\n\nMore generally, this applies for an ordered field; see #Ordered fields.\n\nThe properties for the additive inverse state:\n\n\nThe properties for the multiplicative inverse state:\n\n\nThese can also be written in chained notation as:\n\nAny monotonically increasing function may be applied to both sides of an inequality (provided they are in the domain of that function) and it will still hold. Applying a monotonically decreasing function to both sides of an inequality means the opposite inequality now holds. The rules for the additive inverse, and the multiplicative inverse for positive numbers, are both examples of applying a monotonically decreasing function.\n\nIf the inequality is strict (\"a\" < \"b\", \"a\" > \"b\") \"and\" the function is strictly monotonic, then the inequality remains strict. If only one of these conditions is strict, then the resultant inequality is non-strict. The rules for additive and multiplicative inverses are both examples of applying a \"strictly\" monotonically decreasing function.\n\nA few examples of this rule are: \n\nIf (\"F\", +, ×) is a field and ≤ is a total order on \"F\", then (\"F\", +, ×, ≤) is called an ordered field if and only if:\n\nNote that both (Q, +, ×, ≤) and (R, +, ×, ≤) are ordered fields, but ≤ cannot be defined in order to make (C, +, ×, ≤) an ordered field, because −1 is the square of \"i\" and would therefore be positive.\n\nThe non-strict inequalities ≤ and ≥ on real numbers are total orders. That is, given arbitrary \"a\",\"b\"∈R, at least one of \"a\"≤\"b\" and \"b\"≤\"a\" holds; at the same time, at least one of \"a\"≥\"b\" and \"b\"≥\"a\" holds. The strict inequalities < and > on real numbers are strict total orders. That is, < on R has trichotomy property: given arbitrary \"a\",\"b\"∈R, exactly one of \"a\"<\"b\", \"b\"<\"a\" and \"a\"=\"b\" is true; likewise, > on R has the trichotomy property.\n\nThe notation a\" < \"b\" < \"c stands for \"\"a\" < \"b\" and \"b\" < \"c\"\", from which, by the transitivity property above, it also follows that \"a\" < \"c\". By the above laws, one can add or subtract the same number to all three terms, or multiply or divide all three terms by same nonzero number and reverse all inequalities if that number is negative. Hence, for example, \"a\" < \"b\" + \"e\" < \"c\" is equivalent to \"a\" − \"e\" < \"b\" < \"c\" − \"e\".\n\nThis notation can be generalized to any number of terms: for instance, \"a\" ≤ \"a\" ≤ ... ≤ \"a\" means that \"a\" ≤ \"a\" for \"i\" = 1, 2, ..., \"n\" − 1. By transitivity, this condition is equivalent to \"a\" ≤ \"a\" for any 1 ≤ \"i\" ≤ \"j\" ≤ \"n\".\n\nWhen solving inequalities using chained notation, it is possible and sometimes necessary to evaluate the terms independently. For instance to solve the inequality 4\"x\" < 2\"x\" + 1 ≤ 3\"x\" + 2, it is not possible to isolate \"x\" in any one part of the inequality through addition or subtraction. Instead, the inequalities must be solved independently, yielding \"x\" < 1/2 and \"x\" ≥ −1 respectively, which can be combined into the final solution −1 ≤ \"x\" < 1/2.\n\nOccasionally, chained notation is used with inequalities in different directions, in which case the meaning is the logical conjunction of the inequalities between adjacent terms. For instance, \"a\" < \"b\" = \"c\" ≤ \"d\" means that \"a\" < \"b\", \"b\" = \"c\", and \"c\" ≤ \"d\". This notation exists in a few programming languages such as Python.\n\nAn inequality is said to be \"sharp\", if it cannot be \"relaxed\" and still be valid in general. Formally, a universally quantified inequality \"φ\" is called sharp if, for every valid universally quantified inequality \"ψ\", if holds, then also holds. For instance, the inequality is sharp, whereas the inequality is not sharp.\n\nThere are many inequalities between means. For example, for any positive numbers \"a\", \"a\", …, \"a\" we have where\n\nA \"power inequality\" is an inequality containing terms of the form \"a\", where \"a\" and \"b\" are real positive numbers or variable expressions. They often appear in mathematical olympiads exercises.\n\n\nMathematicians often use inequalities to bound quantities for which exact formulas cannot be computed easily. Some inequalities are used so often that they have names:\n\nThe set of complex numbers formula_13 with its operations of addition and multiplication is a field, but it is impossible to define any relation ≤ so that formula_14 becomes an ordered field. To make formula_14 an ordered field, it would have to satisfy the following two properties:\n\n\nBecause ≤ is a total order, for any number \"a\", either 0 ≤ \"a\" or \"a\" ≤ 0 (in which case the first property above implies that 0 ≤ −\"a\"). In either case 0 ≤ \"a\"; this means that formula_16 and formula_17; so formula_18 and formula_19, which means formula_20; contradiction.\n\nHowever, an operation ≤ can be defined so as to satisfy only the first property (namely, \"if \"a\" ≤ \"b\" then \"a\" + \"c\" ≤ \"b\" + \"c\"\"). Sometimes the lexicographical order definition is used:\nIt can easily be proven that for this definition \"a\" ≤ \"b\" implies \"a\" + \"c\" ≤ \"b\" + \"c\".\n\nInequality relationships similar to those defined above can also be defined for column vectors. If we let the vectors formula_25 (meaning that formula_26 and formula_27 where formula_28 and formula_29 are real numbers for formula_30), we can define the following relationships.\n\n\nSimilarly, we can define relationships for formula_44, formula_45, and formula_46. We note that this notation is consistent with that used by Matthias Ehrgott in \"Multicriteria Optimization\" (see References).\n\nThe trichotomy property (as stated above) is not valid for vector relationships. For example, when formula_47 and formula_48, there exists no valid inequality relationship between these two vectors. Also, a multiplicative inverse would need to be defined on a vector before this property could be considered. However, for the rest of the aforementioned properties, a parallel property for vector inequalities exists.\n\nFor a general system of polynomial inequalities, one can find a condition for a solution to exist. Firstly, any system of polynomial inequalities can be reduced to a system of quadratic inequalities by increasing the number of variables and equations (for example by setting a square of a variable equal to a new variable). A single quadratic polynomial inequality in \"n\" − 1 variables can be written as:\n\nwhere \"X\" is a vector of the variables formula_50 and \"A\" is a matrix. This has a solution, for example, when there is at least one positive element on the main diagonal of \"A\".\n\nSystems of inequalities can be written in terms of matrices A, B, C, etc. and the conditions for existence of solutions can be written as complicated expressions in terms of these matrices. The solution for two polynomial inequalities in two variables tells us whether two conic section regions overlap or are inside each other. The general solution is not known but such a solution could be theoretically used to solve such unsolved problems as the kissing number problem. However, the conditions would be so complicated as to require a great deal of computing time or clever algorithms.\n\n\n"}
{"id": "41385213", "url": "https://en.wikipedia.org/wiki?curid=41385213", "title": "Integral of inverse functions", "text": "Integral of inverse functions\n\nIn mathematics, integrals of inverse functions can be computed by means of a formula that expresses the antiderivatives of the inverse formula_1 of a continuous and invertible function formula_2, in terms of formula_1 and an antiderivative of formula_2. This formula was published in 1905 by Charles-Ange Laisant.\n\nLet formula_5 and formula_6 be two intervals of formula_7. \nAssume that formula_8 is a continuous and invertible function, and let formula_1 denote its inverse formula_10.\nThen formula_2 and formula_1 have antiderivatives, and if formula_13 is an antiderivative of formula_2, the possible antiderivatives of formula_1 are:\nwhere formula_17 is an arbitrary real number.\n\nIn his 1905 article, Laisant gave three proofs. First, under the additional hypothesis that formula_1 is differentiable, one may differentiate the above formula, which completes the proof immediately. His second proof was geometric. If formula_19 and formula_20, the theorem can be written: \nThe figure on the right is a proof without words of this formula. Laisant does not discuss the hypotheses necessary to make this proof rigorous, but it can be made explicit with the help of the Darboux integral (or Fubini's theorem if a demonstration based on the Lebesgue integral is desired). Laisant's third proof uses the additional hypothesis that formula_2 is differentiable. Beginning with formula_23, one multiplies by formula_24 and integrates both sides. The right-hand side is calculated using integration by parts to be formula_25, and the formula follows.\n\nNevertheless, it can be shown that this theorem holds even if formula_2 or formula_1 is not differentiable: it suffices, for example, to use the Stieltjes integral in the previous argument. On the other hand, even though general monotonic functions are differentiable almost everywhere, the proof of the general formula does not follow, unless formula_1 is absolutely continuous.\n\nIt is also possible to check that for every formula_29 in formula_6, the derivative of the function formula_31 is equal to formula_32. In other words:\nTo this end, it suffices to apply the mean value theorem to formula_13 between formula_35 and formula_36, taking into account that formula_2 is monotonic.\n\n\nApparently, this theorem of integration was discovered for the first time in 1905 by Charles-Ange Laisant, who \"could hardly believe that this theorem is new\", and hoped its use would henceforth spread out among students and teachers. This result was published independently in 1912 by an Italian engineer, Alberto Caprilli, in an opuscule entitled \"Nuove formole d'integrazione\". It was rediscovered in 1955 by Parker, and by a number of mathematicians following him. Nevertheless, they all assume that or is differentiable. \nThe general version of the theorem, free from this additional assumption, was proposed by Michael Spivak in 1965, as an exercise in the \"Calculus\", and a fairly complete proof following the same lines was published by Eric Key in 1994.\nThis proof relies on the very definition of the Darboux integral, and consists in showing that the upper Darboux sums of the function are in 1-1 correspondence with the lower Darboux sums of . \nIn 2013, Michael Bensimhoun, estimating that the general theorem was still insufficiently known, gave two other proofs: The second proof, based on the Stieltjes integral and on its formulae of integration by parts and of homeomorphic change of variables, is the most suitable to establish more complex formulae.\n\nThe above theorem generalizes in the obvious way to holomorphic functions:\nLet formula_47 and formula_48 be two open and simply connected sets of formula_49, and assume that formula_50 is a biholomorphism. Then formula_2 and formula_1 have antiderivatives, and if formula_13 is an antiderivative of formula_2, the general antiderivative of formula_1 is\n\nBecause all holomorphic functions are differentiable, the proof is immediate by complex differentiation.\n\n"}
{"id": "47161006", "url": "https://en.wikipedia.org/wiki?curid=47161006", "title": "L(h, k)-coloring", "text": "L(h, k)-coloring\n\nL(\"h\", \"k\") coloring in graph theory, is a (proper) vertex coloring in which every pair of adjacent vertices has color numbers that differ by at least \"h\", and any pair of vertices at distance 2 have their colors differ by at least \"k\". When \"h\"=1 and \"k\"=0, it is the usual (proper) vertex coloring.\n"}
{"id": "48539131", "url": "https://en.wikipedia.org/wiki?curid=48539131", "title": "List of Fields medalists affiliated with the Institute for Advanced Study", "text": "List of Fields medalists affiliated with the Institute for Advanced Study\n\nThis is a comprehensive list of Fields Medal winners affiliated with the Institute for Advanced Study in Princeton, New Jersey as current and former faculty members, visiting scholars, and other affiliates. Of the 56 individuals who have received the Fields Medal as of 2015, 41 are mathematicians who have been affiliated with the IAS as some point in their career. \n\nThe Fields Medal is the world’s most prestigious award in mathematics. It is presented every four years by the International Mathematical Union and is often referred to as the \"Nobel prize of mathematics.\" It is generally shared by four different researchers. Members of the IAS have dominated the award since its inception in 1936 and in 2010 they took all four of them.\n\n"}
{"id": "34559803", "url": "https://en.wikipedia.org/wiki?curid=34559803", "title": "Locally finite variety", "text": "Locally finite variety\n\nIn universal algebra, a variety of algebras means the class of all algebraic structures of a given signature satisfying a given set of identities. One calls a variety locally finite if every finitely generated algebra has finite cardinality, or equivalently, if every finitely generated free algebra has finite cardinality.\n\nThe variety of Boolean algebras constitutes a famous example. The free Boolean algebra on \"n\" generators has cardinality 2, consisting of the \"n\"-ary operations 2→2.\n\nThe variety of sets constitutes a degenerate example: the free set on \"n\" generators has cardinality \"n\", consisting of just the generators themselves.\n\nThe variety of pointed sets constitutes a trivial example: the free pointed set on \"n\" generators has cardinality \"n\"+1, consisting of the generators along with the basepoint.\n\nThe variety of graphs defined as follows constitutes a combinatorial example. Define a graph \"G\" = (\"E\",\"s\",\"t\") to be a set \"E\" of edges and unary operations \"s\", \"t\" of source and target satisfying \"s\"(\"s\"(\"e\")) = \"t\"(\"s\"(\"e\")) and \"s\"(\"t\"(\"e\")) = \"t\"(\"t\"(\"e\")). Vertices are those edges in the (common) image of \"s\" and \"t\". The free graph on \"n\" generators has cardinality 3\"n\" and consists of \"n\" edges \"e\" each with two endpoints \"s\"(\"e\") and \"t\"(\"e\"). Graphs with nontrivial incidence relations arise as quotients of free graphs, most usefully by identifying vertices.\n\nThe variety of sets and the variety of graphs so defined each forms a presheaf category and hence a topos. This is not the case for the variety of Boolean algebras or of pointed sets.\n\n"}
{"id": "748686", "url": "https://en.wikipedia.org/wiki?curid=748686", "title": "Lubell–Yamamoto–Meshalkin inequality", "text": "Lubell–Yamamoto–Meshalkin inequality\n\nIn combinatorial mathematics, the Lubell–Yamamoto–Meshalkin inequality, more commonly known as the LYM inequality, is an inequality on the sizes of sets in a Sperner family, proved by , , , and . It is named for the initials of three of its discoverers.\n\nThis inequality belongs to the field of combinatorics of sets, and has many applications in combinatorics. In particular, it can be used to prove Sperner's theorem. Its name is also used for similar inequalities.\n\nLet \"U\" be an \"n\"-element set, let \"A\" be a family of subsets of \"U\" such that no set in \"A\" is a subset of another set in \"A\", and let \"a\" denote the number of sets of size \"k\" in \"A\". Then\n\n proves the Lubell–Yamamoto–Meshalkin inequality by a double counting argument in which he counts the permutations of \"U\" in two different ways. First, by counting all permutations of \"U\" directly, one finds that there are \"n\"! of them. But secondly, one can generate a permutation (i.e., an order) of the elements of \"U\" by selecting a set \"S\" in \"A\" and concatenating a permutation of the elements of \"S\" with a permutation of the nonmembers (elements of \"U\\S\"). If |\"S\"| = \"k\", it will be associated in this way with \"k\"!(\"n\" − \"k\")! permutations, and in each of them the first \"k\" elements will be just the elements of \"S\". Each permutation can only be associated with a single set in \"A\", for if two prefixes of a permutation both formed sets in \"A\" then one would be a subset of the other. Therefore, the number of permutations that can be generated by this procedure is\nSince this number is at most the total number of all permutations,\nFinally dividing the above inequality by \"n\"! leads to the result.\n"}
{"id": "52806824", "url": "https://en.wikipedia.org/wiki?curid=52806824", "title": "Mathematicism", "text": "Mathematicism\n\nMathematicism is any opinion, viewpoint, school of thought, or philosophy that states that everything can be described/defined/modelled ultimately by mathematics, or that the universe and reality (both material and mental/spiritual) are fundamentally/fully/only mathematical, i.e. that 'everything is mathematics' necessitating the ideas of logic, reason, mind, and spirit.\n\nMathematicism is a form of rationalist idealist or mentalist/spiritualist monism). The idea started in the West with ancient Greece's Pythagoreanism, and continued in other rationalist idealist schools of thought such as Platonism. The term 'mathematicism' has additional meanings among Cartesian idealist philosophers and mathematicians, such as describing the ability and process to study reality mathematically.\n\nMathematicism includes (but is not limited to) the following (chronological order):\n\n\n"}
{"id": "1653013", "url": "https://en.wikipedia.org/wiki?curid=1653013", "title": "Millionth", "text": "Millionth\n\nOne millionth is equal to 0.000 001, or 1 x 10 in scientific notation. It is the reciprocal of a million, and can be also written as 1/1 000 000. Units using this fraction can be indicated using the prefix \"micro-\" from Greek, meaning \"small\". Numbers of this quantity are expressed in terms of µ (the Greek letter mu).\n\n\"Millionth\" can also mean the ordinal number that comes after the nine hundred, ninety-nine thousand, nine hundred, ninety-ninth and before the million and first.\n\n"}
{"id": "34609626", "url": "https://en.wikipedia.org/wiki?curid=34609626", "title": "Modern elementary mathematics", "text": "Modern elementary mathematics\n\nModern elementary mathematics is the theory and practice of teaching elementary mathematics according to contemporary research and thinking about learning. This can include pedagogical ideas, mathematics education research frameworks, and curricular material. \n\nIn practicing modern elementary mathematics, teachers may use new and emerging media and technologies like social media and video games, as well as applying new teaching techniques based on the individualization of learning, in-depth study of the psychology of mathematics education, and integrating mathematics with science, technology, engineering and the arts.\n\nMaking all areas of mathematics accessible to young children is a key goal of modern elementary mathematics. Author and academic Liping Ma calls for \"profound understanding of fundamental mathematics\" by elementary teachers and parents of learners, as well as learners themselves.\n\n\nOther areas of mathematics such as logical reasoning and paradoxes, which used to be reserved for advanced groups of learners, are now being integrated into more mainstream curricula.\n\nPsychology in mathematics education is an applied research domain, with many recent developments relevant to elementary mathematics. A major aspect is the study of motivation; while most young children enjoy some mathematical practices, by the age of seven to ten many lose interest and begin to experience mathematical anxiety. Constructivism and other learning theories consider the ways young children learn mathematics, taking child developmental psychology into account.\n\nBoth practitioners and researchers focus on children's memory, mnemonic devices, and computer-assisted techniques such as spaces repetition. There is an ongoing discussion of relationships between memory, procedural fluency with algorithms, and conceptual understanding of elementary mathematics. Sharing songs, rhymes, visuals and other mnemonics is popular in teacher social networks.\n\nThe understanding that young children benefit from hands-on learning is more than a century old, going back to the work of Maria Montessori. However, there are modern developments of the theme. Traditional manipulatives are now available on computers as virtual manipulatives, with many offering options not available in the physical world, such as zoom or cross-section of geometric shapes. Embodied mathematics, such as studies of numerical cognition or gestures in learning, are growing research topics in mathematics education.\n\nModern tools such as computer-based expert systems allow higher individualization of learning. Students do mathematical work at their own pace, providing for each student's learning style, and scaling the same activity for multiple levels. Special education and gifted education in particular require level and style accommodations, such as using different presentation and response options. Changing some aspects of the environment, such as giving an auditory learner headphones with quiet music, can help children concentrate on mathematical tasks.\n\nModern learning materials, both computer and physical, accommodate learners through the use of multiple representation, such as graphs, pictures, words, animations, symbols, and sounds. For example, recent research suggests that sign language isn’t only a means of speaking for those who are deaf, but also a visual approach to communication and learning, appealing to many others students and particularly helping with mathematics.\n\nAnother aspect of individual education is child-led learning, which is called unschooling when it encompasses most of the child's experiences. Child-led learning means incorporating mathematically rich projects that stem from personal interests and passions. Educators who support child-led learning need to provide tasks that are open to interpretation, and be ready to improvise, rather than prepare lessons ahead of time. This modern approach often involves seizing opportunities for discovery, and learning as the child's curiosity demands. This departure from conventional structured learning leaves the child free to explore his/her innate desires and curiosities. Child-led learning taps into the child's intrinsic love of learning.\n\nProblem solving can be an intensely individualized activity, with students working in their own ways and also sharing insights and results within groups. There are many means to one end, emphasizing the importance of creative approaches. Promoting discourse and focusing on language are important concepts for helping each students participate in problem solving meaningfully.\n\nData-based assessment and comparison of learning methods, and ways children learn, is another big aspect of modern elementary mathematics.\n\nModern computation technologies change elementary mathematics in several ways. Technology reduces the amount of attention, memory, and computation required by users, making higher mathematical topics accessible to young children. However, the main opportunity technology provides is not in making traditional mathematical tasks more accessible, but in introducing children to novel activities that are not possible without computers.\n\nFor example, computer modeling allows children to change parameters in virtual systems created by educators and observe emergent mathematical behaviors, or remix and create their own models. The pedagogical approach of constructionism describes how creating algorithms, programs and models on computers promotes deep mathematical thinking. Technology allows children to experience these complex concepts in a more visual manner.\nComputer algebra systems are software environments that support and scaffold working with symbolic expressions. Some computer algebra systems have intuitive, child-friendly interfaces and therefore can be used in Early Algebra. Interactive geometry software supports creation and manipulation of geometric constructions. Both computer algebra systems and interactive geometry software help with several cognitive limitations of young children, such as attention and memory. The software scaffolds step-by-step procedures, helping children focus attention. It has \"undo\" capabilities, lowering frustration when errors happen, and promoting creativity and exploration. Also, such software supports metacognition by making all steps in a problem or a construction visible and editable, so children can reflect on individual steps or the whole journey.\n\nOnline communities and forums allow educators, researchers and students to share, discuss and remix elementary mathematical content they find or create. Sometimes, traditional media such as texts, pictures and movies are digitized and turned into online social objects, such as open textbooks. Other times, web-native mathematical objects are created, remixed and shared within the integrated authoring and discussion environment, such as applets made with Scratch or Geogebra constructions.\n\nRich media, including video, virtual manipulatives, interactive models and mobile applications is a characteristic feature of online mathematical communication. Some global collaboration projects between teachers or groups of students with teachers use the web mostly for communication, but others happen in virtual worlds, such as Whyville.\n\nProfessional development for elementary mathematics educators uses social media in the form of online courses, discussion forums, webinars, and web conferences. This supports teachers in forming PLNs (Personal Learning Networks). Some communities include both students and teachers, such as Art of Problem Solving.\n\nLearning through play is not new, but the themes of computer and mobile games are relatively more modern. Most teachers now use games in elementary classrooms, and most children in developed countries play learning games at home. Computer games with intrinsically mathematical game mechanics can help children learn novel topics. More extrinsic game mechanics and gamification can be used for time and task management, fluency, and memorization. Sometimes it's not obvious what mathematics children learn by \"just playing,\" but basic spatial and numerical skills gained in free play help with mathematical concepts.\n\nSome abstract games such as chess can benefit learning mathematics by developing systems thinking, logic, and reasoning. Roleplaying games invite children to become a character who uses mathematics in daily life or epic adventures, and often use mathematical storytelling. Sandbox, also called open world games, such as Minecraft help children explore patterns, improvise, be mathematically artistic, and develop their own algorithms. Board games can have all of the above aspects, and also promote communication about mathematics in small groups.\n\nTeachers working with disadvantaged kids note especially large mathematical skill gains after using games in the classroom, possibly because kids don't play such games at home.\n\nMany teachers, parents and students design their own games or create versions of existing games. Designing mathematically rich games is one of staple tasks in constructionism.\n\nThere is a concern that children who use computer games and technology in general may be more stressed when exposed to pen-and-paper tests.\n\nWhile learning mathematics in daily life, such as cooking and shopping, can't be considered modern, social media provides new twists. Online networks help parents and teachers share tips on how to integrate daily routines and more formal mathematical learning for children. For example, the \"Let's play math\" blog hosts carnivals for sharing family mathematics ideas, such as using egg cartoons for quick mathematical games.\n\nSchool tasks may involve families collecting data and aggregating it online for mathematical explorations. Pastimes such as geocaching involve families sharing mathematically rich sporting activities that depend on GPS systems or mobile devices. Museums, clubs, stores, and other public places provide blended learning opportunities, with visiting families accessing science and mathematics activities related to the place on their mobile devices.\n\nIn the last several decades, many prominent mathematicians and mathematics enthusiasts embraced mathematical arts, from popular fractal art to origami. Likewise, elementary mathematics is becoming more artistic. Some popular topics for children include tessellation, computer art, symmetry, patterns, transformations and reflections. The discipline of ethnomathematics studies relationships between mathematics and cultures, including arts and crafts. Some hands-on activities, such as creating tiling, can help children and grown-ups see mathematical art all around them.\n\nProject-based learning approaches help students explore mathematics together with other disciplines. For example, children's robotics projects and competitions include mathematical tasks.\n\nSome elementary mathematical topics, such as measurement, apply to tasks in many professions and subject areas. Unit studies centered on such concepts contrast with project-based learning, where students use many concepts to achieve the project's goal.\n"}
{"id": "55972236", "url": "https://en.wikipedia.org/wiki?curid=55972236", "title": "Oberwolfach problem", "text": "Oberwolfach problem\n\nThe Oberwolfach problem is an unsolved problem in mathematics that may be formulated either as a problem scheduling seating assignments for diners,\nor more abstractly as a problem in graph theory, on the edge cycle covers of complete graphs. It is named after the Mathematical Research Institute of Oberwolfach, where the problem was posed in 1967 by Gerhard Ringel.\n\nIn conferences held at Oberwolfach, it is the custom for the participants to dine together in a room with circular tables, not all the same size, and with assigned seating that rearranges the participants from meal to meal. The Oberwolfach problem asks how to make a seating chart for a given set of tables so that all tables are full at each meal and all pairs of conference participants are seated next to each other exactly once. An instance of the problem can be denoted as formula_1 where formula_2 are the given table sizes. Alternatively, when some table sizes are repeated, they may be denoted using exponential notation; for instance, formula_3 describes an instance with three tables of size five.\n\nFormulated as a problem in graph theory, the pairs of people sitting next to each other at a single meal can be represented as a disjoint union of cycle graphs formula_4 of the specified lengths, with one cycle for each of the dining tables. This union of cycles is a 2-regular graph, and every 2-regular graph has this form. If formula_5 is this 2-regular graph and has formula_6 vertices, the question is whether the complete graph formula_7 can be represented as an edge-disjoint union of copies of formula_5.\n\nIn order for a solution to exist, the total number of conference participants (or equivalently, the total capacity of the tables, or the total number of vertices of the given cycle graphs) must be an odd number. For, at each meal, each participant sits next to two neighbors, so the total number of neighbors of each participant must be even, and this is only possible when the total number of participants is odd. The problem has, however, also been extended to even values of formula_6 by asking, for those formula_6, whether all of the edges of the complete graph except for a perfect matching can be covered by copies of the given 2-regular graph. Like the ménage problem (a different mathematical problem involving seating arrangements of diners and tables), this variant of the problem can be formulated by supposing that the formula_6 diners are arranged into formula_12 married couples, and that the seating arrangements should place each diner next to each other diner except their own spouse exactly once.\n\nThe only instances of the Oberwolfach problem that are known not to be solvable are formula_13, formula_14, formula_15, and formula_16. It is widely believed that all other instances have a solution, but only special cases have been provable to be solvable.\n\nThe cases for which a solution is known include:\n\nKirkman's schoolgirl problem, of grouping fifteen schoolgirls into rows of three in seven different ways so that each pair of girls appears once in each triple, is a special case of the Oberwolfach problem, formula_25. The problem of Hamiltonian decomposition of a complete graph formula_7 is another special case, formula_27.\n\nAlspach's conjecture, on the decomposition of a complete graph into cycles of given sizes, is related to the Oberwolfach problem, but neither is a special case of the other.\nIf formula_5 is a 2-regular graph, with formula_6 vertices, formed from a disjoint union of cycles of certain lengths, then a solution to the Oberwolfach problem for formula_5 would also provide a decomposition of the complete graph into formula_31 copies of each of the cycles of formula_5. However, not every decomposition of formula_7 into this many cycles of each size can be grouped into disjoint cycles that form copies of formula_5, and on the other hand not every instance of Alspach's conjecture involves sets of cycles that have formula_31 copies of each cycle.\n"}
{"id": "55155568", "url": "https://en.wikipedia.org/wiki?curid=55155568", "title": "Ogive (statistics)", "text": "Ogive (statistics)\n\nIn statistics, an ogive is a free-hand graph showing the curve of a cumulative distribution function. The points plotted are the upper class limit and the corresponding cumulative frequency. (which, for the normal distribution, resembles one side of an Arabesque or ogival arch). The term can also be used to refer to the empirical cumulative distribution function.\n"}
{"id": "24184879", "url": "https://en.wikipedia.org/wiki?curid=24184879", "title": "Panlogism", "text": "Panlogism\n\nIn philosophy, panlogism is a Hegelian doctrine that holds that the universe is the act or realization of Logos. According to the doctrine of panlogism, logic and ontology are the same study.\n"}
{"id": "36585685", "url": "https://en.wikipedia.org/wiki?curid=36585685", "title": "Pernicious number", "text": "Pernicious number\n\nIn number theory, a pernicious number is a positive integer such that the Hamming weight (or digit sum) of its binary representation is prime.\n\nThe first pernicious number is 3, since 3 = 11 and 1 + 1 = 2, which is a prime. The next pernicious number is 5, since 5 = 101, followed by 6, 7 and 9 .\n\n\n\n\n"}
{"id": "593693", "url": "https://en.wikipedia.org/wiki?curid=593693", "title": "Point (geometry)", "text": "Point (geometry)\n\nIn modern mathematics, a point refers usually to an element of some set called a space.\n\nMore specifically, in Euclidean geometry, a point is a primitive notion upon which the geometry is built, meaning that a point cannot be defined in terms of previously defined objects. That is, a point is defined only by some properties, called axioms, that it must satisfy. In particular, the geometric points do not have any length, area, volume or any other dimensional attribute. A common interpretation is that the concept of a point is meant to capture the notion of a unique location in Euclidean space.\n\nPoints, considered within the framework of Euclidean geometry, are one of the most fundamental objects. Euclid originally defined the point as \"that which has no part\". In two-dimensional Euclidean space, a point is represented by an ordered pair (, ) of numbers, where the first number conventionally represents the horizontal and is often denoted by , and the second number conventionally represents the vertical and is often denoted by . This idea is easily generalized to three-dimensional Euclidean space, where a point is represented by an ordered triplet (, , ) with the additional third number representing depth and often denoted by . Further generalizations are represented by an ordered tuplet of terms, where is the dimension of the space in which the point is located.\n\nMany constructs within Euclidean geometry consist of an infinite collection of points that conform to certain axioms. This is usually represented by a set of points; As an example, a line is an infinite set of points of the form formula_1, where through and are constants and is the dimension of the space. Similar constructions exist that define the plane, line segment and other related concepts. A line segment consisting of only a single point is called a degenerate line segment.\n\nIn addition to defining points and constructs related to points, Euclid also postulated a key idea about points, that any two points can be connected by a straight line. This is easily confirmed under modern extensions of Euclidean geometry, and had lasting consequences at its introduction, allowing the construction of almost all the geometric concepts known at the time. However, Euclid's postulation of points was neither complete nor definitive, and he occasionally assumed facts about points that did not follow directly from his axioms, such as the ordering of points on the line or the existence of specific points. In spite of this, modern expansions of the system serve to remove these assumptions.\n\nThere are several inequivalent definitions of dimension in mathematics. In all of the common definitions, a point is 0-dimensional.\n\nThe dimension of a vector space is the maximum size of a linearly independent subset. In a vector space consisting of a single point (which must be the zero vector 0), there is no linearly independent subset. The zero vector is not itself linearly independent, because there is a non trivial linear combination making it zero: formula_2.\n\nThe topological dimension of a topological space \"X\" is defined to be the minimum value of \"n\", such that every finite open cover formula_3 of \"X\" admits a finite open cover formula_4 of \"X\" which refines formula_3 in which no point is included in more than \"n\"+1 elements. If no such minimal \"n\" exists, the space is said to be of infinite covering dimension.\n\nA point is zero-dimensional with respect to the covering dimension because every open cover of the space has a refinement consisting of a single open set.\n\nLet \"X\" be a metric space. If \"S\" ⊂ \"X\" and \"d\" ∈ [0, ∞), the \"d\"-dimensional Hausdorff content of \"S\" is the infimum of the set of numbers δ ≥ 0 such that there is some (indexed) collection of balls formula_6 covering \"S\" with \"r\" > 0 for each \"i\" ∈ \"I\" that satisfies formula_7.\n\nThe Hausdorff dimension of \"X\" is defined by\n\nA point has Hausdorff dimension 0 because it can be covered by a single ball of arbitrarily small radius.\n\nAlthough the notion of a point is generally considered fundamental in mainstream geometry and topology, there are some systems that forgo it, e.g. noncommutative geometry and pointless topology. A \"pointless\" or \"pointfree\" space is defined not as a set, but via some structure (algebraic or logical respectively) which looks like a well-known function space on the set: an algebra of continuous functions or an algebra of sets respectively. More precisely, such structures generalize well-known spaces of functions in a way that the operation \"take a value at this point\" may not be defined.\nA further tradition starts from some books of A. N. Whitehead in which the notion of region is assumed as a primitive together with the one of \"inclusion\" or \"connection\".\n\nOften in physics and mathematics, it is useful to think of a point as having non-zero mass or charge (this is especially common in classical electromagnetism, where electrons are idealized as points with non-zero charge). The Dirac delta function, or function, is (informally) a generalized function on the real number line that is zero everywhere except at zero, with an integral of one over the entire real line. The delta function is sometimes thought of as an infinitely high, infinitely thin spike at the origin, with total area one under the spike, and physically represents an idealized point mass or point charge. It was introduced by theoretical physicist Paul Dirac. In the context of signal processing it is often referred to as the unit impulse symbol (or function). Its discrete analog is the Kronecker delta function which is usually defined on a finite domain and takes values 0 and 1.\n\n\n\n"}
{"id": "11360852", "url": "https://en.wikipedia.org/wiki?curid=11360852", "title": "Predictive state representation", "text": "Predictive state representation\n\nIn computer science, a predictive state representation (PSR) is a way to model a state of controlled dynamical system from a history of actions taken and resulting observations. PSR captures the state of a system as a vector of predictions for future tests (experiments) that can be done on the system. A test is a sequence of action-observation pairs and its prediction is the probability of the test's observation-sequence happening if the test's action-sequence were to be executed on the system. One of the advantage of using PSR is that the predictions are directly related to observable quantities. This is in contrast to other models of dynamical systems, such as partially observable Markov decision processes (POMDPs) where the state of the system is represented as a probability distribution over unobserved nominal states.\n"}
{"id": "42993804", "url": "https://en.wikipedia.org/wiki?curid=42993804", "title": "Rational series", "text": "Rational series\n\nIn mathematics and computer science, a rational series is a generalisation of the concept of formal power series over a ring to the case when the basic algebraic structure is no longer a ring but a semiring, and the indeterminates adjoined are not assumed to commute. They can be regarded as algebraic expressions of a formal language over a finite alphabet.\n\nLet \"R\" be a semiring and \"A\" a finite alphabet.\n\nA \"noncommutative polynomial\" over \"A\" is a finite formal sum of words over \"A\". They form a semiring formula_1.\n\nA \"formal series\" is a \"R\"-valued function \"c\", on the free monoid \"A\", which may be written as\n\nThe set of formal series is denoted formula_3 and becomes a semiring under the operations\n\nA non-commutative polynomial thus corresponds to a function \"c\" on \"A\" of finite support.\n\nIn the case when \"R\" is a ring, then this is the \"Magnus ring\" over \"R\".\n\nIf \"L\" is a language over \"A\", regarded as a subset of \"A\" we can form the \"characteristic series\" of \"L\" as the formal series\n\ncorresponding to the characteristic function of \"L\".\n\nIn formula_3 one can define an operation of iteration expressed as\n\nand formalised as\n\nThe \"rational operations\" are the addition and multiplication of formal series, together with iteration.\nA rational series is a formal series obtained by rational operations from formula_1.\n\n\n"}
{"id": "23756785", "url": "https://en.wikipedia.org/wiki?curid=23756785", "title": "Right conoid", "text": "Right conoid\n\nIn geometry, a right conoid is a ruled surface generated by a family of straight lines that all intersect perpendicularly to a fixed straight line, called the \"axis\" of the right conoid.\n\nUsing a Cartesian coordinate system in three-dimensional space, if we take the z-axis to be the axis of a right conoid, then the right conoid can be represented by the parametric equations:\n\nwhere \"h\"(\"u\") is some function for representing the \"height\" of the moving line.\n\nA typical example of right conoids is given by the parametric equations\n\nThe image on the right shows how the coplanar lines generate the right conoid.\n\nOther right conoids include:\n\n\n"}
{"id": "46418011", "url": "https://en.wikipedia.org/wiki?curid=46418011", "title": "Robert Edmund Edwards", "text": "Robert Edmund Edwards\n\nRobert Edmund Edwards (1926–2000), usually cited simply as R. E. Edwards, was a British-born Australian mathematician who specialized in functional analysis.\nHe is the author of several volumes in Springer's Graduate Texts in Mathematics.\n\nHe received his PhD at Birkbeck College, University of London in 1951 under Lionel Cooper. His dissertation topic was \"Theory of Normed Rings, and Translations in Function Spaces\". He continued to teach there as a lecturer until 1959, and then spent a few years at Manchester, before migrating to Australia in 1961, where he worked at the Institute of Advanced Studies at ANU as a professorial fellow (1961-1970) and professor of mathematics (1970-1978).\n\n"}
{"id": "29618699", "url": "https://en.wikipedia.org/wiki?curid=29618699", "title": "Shearer's inequality", "text": "Shearer's inequality\n\nIn information theory, Shearer's inequality, named after James Shearer, states that if \"X\", ..., \"X\" are random variables and \"S\", ..., \"S\" are subsets of {1, 2, ..., \"d\"} such that every integer between 1 and \"d\" lies in at least \"r\" of these subsets, then\n\nwhere formula_2 is the Cartesian product of random variables formula_3 with indices \"j\" in formula_4 (so the dimension of this vector is equal to the size of formula_4).\n"}
{"id": "1866685", "url": "https://en.wikipedia.org/wiki?curid=1866685", "title": "Slope field", "text": "Slope field\n\nThe solutions of a first-order differential equation of a scalar function y(x) can be drawn in a 2-dimensional space with the x in horizontal and y in vertical direction. Possible solutions are functions y(x) drawn as solid curves. Sometimes it is too cumbersome solving the differential equation analytically. Then one can still draw the tangents of the function curves e.g. on a regular grid. The tangents are touching the functions at the grid points. However, the direction field is rather agnostic about chaotic aspects of the differential equation.\n\nThe slope field can be defined for the following type of differential equations \nwhich can be interpreted geometrically as giving the slope of the tangent to the graph of the differential equation's solution (\"integral curve\") at each point (\"x\", \"y\") as a function of the point coordinates.\n\nIt can be viewed as a creative way to plot a real-valued function of two real variables formula_2 as a planar picture. Specifically, for a given pair formula_3, a vector with the components formula_4 is drawn at the point formula_3 on the formula_3-plane. Sometimes, the vector formula_4 is normalized to make the plot better looking for a human eye. A set of pairs formula_3 making a rectangular grid is typically used for the drawing.\n\nAn isocline (a series of lines with the same slope) is often used to supplement the slope field. In an equation of the form formula_1, the isocline is a line in the formula_3-plane obtained by setting formula_2 equal to a constant.\n\nGiven a system of differential equations,\nthe slope field is an array of slope marks in the phase space (in any number of dimensions depending on the number of relevant variables; for example, two in the case of a first-order linear ODE, as seen to the right). Each slope mark is centered at a point formula_16 and is parallel to the vector\n\nThe number, position, and length of the slope marks can be arbitrary. The positions are usually chosen such that the points formula_16 make a uniform grid. The standard case, described above, represents formula_19. The general case of the slope field for systems of differential equations is not easy to visualize for formula_20.\n\nWith computers, complicated slope fields can be quickly made without tedium, and so an only recently practical application is to use them merely to get the feel for what a solution should be before an explicit general solution is sought. Of course, computers can also just solve for one, if it exists.\n\nIf there is no explicit general solution, computers can use slope fields (even if they aren’t shown) to numerically find graphical solutions. Examples of such routines are Euler's method, or better, the Runge–Kutta methods.\n\nDifferent software packages can plot slope fields.\n\n /* field for y'=xy (click on a point to get an integral curve) */\n\n\n\n"}
{"id": "19411601", "url": "https://en.wikipedia.org/wiki?curid=19411601", "title": "Symplectic representation", "text": "Symplectic representation\n\nIn mathematical field of representation theory, a symplectic representation is a representation of a group or a Lie algebra on a symplectic vector space (\"V\", \"ω\") which preserves the symplectic form \"ω\". Here \"ω\" is a nondegenerate skew symmetric bilinear form\nwhere F is the field of scalars. A representation of a group \"G\" preserves \"ω\" if\nfor all \"g\" in \"G\" and \"v\", \"w\" in \"V\", whereas a representation of a Lie algebra g preserves \"ω\" if\nfor all \"ξ\" in g and \"v\", \"w\" in \"V\". Thus a representation of \"G\" or g is equivalently a group or Lie algebra homomorphism from \"G\" or g to the symplectic group Sp(\"V\",\"ω\") or its Lie algebra sp(\"V\",\"ω\")\n\nIf \"G\" is a compact group (for example, a finite group), and F is the field of complex numbers, then by introducing a compatible unitary structure (which exists by an averaging argument), one can show that any complex symplectic representation is a quaternionic representation. Quaternionic representations of finite or compact groups are often called symplectic representations, and may be identified using the Frobenius-Schur indicator.\n"}
{"id": "16303447", "url": "https://en.wikipedia.org/wiki?curid=16303447", "title": "Vectors in three-dimensional space", "text": "Vectors in three-dimensional space\n\nVectors in three-dimensional space (1978) is a book concerned with physical quantities defined in \"ordinary\" 3-space. It was written by J.S.R.Chisholm, an English mathematical physicist, and published by Cambridge University Press. According to the author, such physical quantities are studied in Newtonian mechanics, fluid mechanics, theories of elasticity and plasticity, non-relativistic quantum mechanics, and many parts of solid state physics. The author further states that \"the vector concept developed in two different ways: in a wide variety of physical applications, vector notation and techniques became, by the middle of this century, almost universal; on the other hand, pure mathematicians reduced vector algebra to an axiomatic system, and introduced wide generalisations of the concept of a three-dimensional 'vector space'.\" Chisholm explains that since these two developments proceeded largely independently, there is a need to show how one can be applied to the other.\n\n\"Vectors in three-dimensional space\" has six chapters, each divided into five or more subsections. The first on linear spaces and displacements including these sections: Introduction, Scalar multiplication of vectors, Addition and subtraction of vectors, Displacements in Euclidean space, Geometrical applications. The second on Scalar products and components including these sections: Scalar products, Linear dependence and dimension, Components of a vector, Geometrical applications, Coordinate systems. The third on Other products of vectors. The last three chapters round out Chisholm's integration of these two largely independent developments.\n\n"}
{"id": "51347294", "url": "https://en.wikipedia.org/wiki?curid=51347294", "title": "Wilhelm Winkler", "text": "Wilhelm Winkler\n\nWilhelm Winkler (June 29, 1884 – September 3, 1984) had successful careers as both an academic statistician (despite receiving no formal academic training in the field), and a program director in the Austrian government.\n\nWilhelm was the fifth of the eight children of Anne and music teacher Julius Winkler, a family situation that required him to work starting at age 13. He attended law school at Karl Friedrich University (now Charles University) in Prague, practiced law briefly in 1908, did a stint in the Austrian army, then settled into a position at the Statistical Bureau of Bohemia as the sole German-speaking statistician. While working there, he attended many university classes and reached the conclusion that \"the German statistical literature did not offer too many ideas. New life came into statistics from England and Russia where the importance of mathematical tools was recognized.\"\n\nWinkler re-enlisted in the Austrian army at the outbreak of World War I in 1914, and was decorated twice for bravery before being wounded in November 1915. During a lengthy recovery, he worked for the War Economy committee; his talents were recognized and he was appointed Secretary of State for Military Affairs at the end of the war in 1918 and he was a delegate to the Versailles Peace Conference. That year, he also married a Jewish woman named Clara Deutch. He joined the Austrian Central Statistics Office in 1920, and was promoted to director of its department of population statistics in 1925. Concurrently, he became a \"Privat-Dozent\" (assistant professor) at the University of Vienna in 1921 and an \"Ausserordentlicher Professor\" in 1929. He founded an institute for the study of minority populations, which published a constant stream of progressive and influential papers that made him unpopular with colleagues in his government job. Despite his lack of formal education, he was elected a member of the International Statistical Institute in 1926 where he actively promoted applied and precise mathematical formulations in contrast to the wordy generalizations that he had criticized 20 years earlier. As both the husband of a Jew and an outspoken critic of the unfair treatment of European minorities, Winkler was promptly fired from both his government and academic positions following the 1938 Nazi annexation of Austria. Despite severe persecution from the Nazi party, he wrote the textbook \"Basic Course in Demography\" during the occupation.\n\nAt the end of the war, he was rehired by the University of Vienna as the first full professor of statistics since 1883, and became Dean of the School of Law and Statecraft from 1950 to 1955. He was also restored as Austria's lead government statistician from 1945 to 1955. Despite these influential positions and growing international recognition, Winkler spent many years defending the statistical department from opposition within the university. The regressive attitude of Austrian and German academics towards statistics as a truly independent discipline meant that his contributions to international developments became more difficult. He did not retire until age 71, and continued to publish and vigorously promote statistics thereafter. He died just after his 100th birthday, having published 20 textbooks and over 200 papers, founded two statistical societies, edited two statistical journals, been awarded two honorary degrees, and reshaped the development of German-speaking statistics through his progressive education initiatives.\n\n"}
{"id": "40249069", "url": "https://en.wikipedia.org/wiki?curid=40249069", "title": "William Gooch (astronomer)", "text": "William Gooch (astronomer)\n\nWilliam Gooch (3 April 1770 – 12 May 1792) was an English astronomer.\n\nGooch was born on 3 April 1770 to William and Sarah Gooch and baptised on 8 May 1770 at the church of St. Peter and St. Paul, Brockdish near Diss in Norfolk. William was schooled in nearby Harleston, Norfolk and later Stradbroke School in Suffolk. His sister, Sarah, succumbed to smallpox at the age of 10 in 1777 and so, in \"tender solicitude\", his parents were protective over their son and would not let him \"mix with other children, except in their presence\".\n\nGooch's father was barber and peruke maker to the gentry of Brockdish, church warden and also fulfilled the role of village constable for a short time. It was due to his father's services that the young William came to the attention of the owner of the village hall, Thomas Maynard (later Sir Thomas Maynard Hesilrige, 10th Baronet of Noseley Hall), who proved key in providing Gooch with the opportunity to go to Gonville and Caius College, Cambridge.\n\nWilliam Gooch was admitted as a sizar to Gonville and Caius College, Cambridge at the age of 16 on 30 May 1786, matriculated in the Michaelmas term of 1787, achieved his B.A. (Second Wrangler) in 1791 being awarded with the Smith's Prize and the Schuldham college prize and became a junior Fellow until his death in 1792.\n\nAt Cambridge, Gooch formed a friendship with a Miss Sally Smithson, the daughter of a cook at St John's College, Cambridge, with whom he continued to correspond after setting sail to join the Vancouver Expedition. He affectionately referred to her as \"little goody two-shoes\".\n\nIt had been presumed that Gooch would follow his academic career into the priesthood, but perhaps being aware of the need for an astronomer to join the Vancouver Expedition, before he had sat his final exams, his Cambridge associates Samuel Vince and John Brinkley, who had both attended the same school as Gooch at Harleston, suggested him to Nevil Maskelyne, the Astronomer Royal, who sat on the Board of Longitude.\n\nAfter some lobbying from his Cambridge supporters and friends, Gooch travelled to London in April 1791, and Maskelyne began preparing him for the expedition. He also received advice on his forthcoming adventure from William Wales, who had served as astronomer to Cook. In July 1791, Gooch was making his final preparations and boarding the \"Daedalus\" at Deptford.\n\nGooch was appointed at a salary of £400 a year and issued with a suit of navigational and astronomical instruments. He sailed on the storeship \"Daedelus\" in August 1791, aiming to meet George Vancouver at Nootka Sound.\n\nIn Gooch's final letter to his parents, written on 2 May 1792, he speaks of his concerns over having used a false meridian and his hopes of being re-united with them in the autumn of 1794. On 11 May 1792, Lieutenant Richard Hergest, commander of the \"Daedalus\", embarked on the ship's cutter along with Gooch and a small crew to trade with the locals and re-supply with fresh water at Waimea on Oahu, in the Hawaiian islands. The party was attacked by \"Pahupu\" (Hawaiian warriors) on 12 May. Hergest, Gooch and a sailor were cut off from the rest of the group and killed.\n\nGooch's story is recorded in his letters, many of which were to his parents, and journal which are housed as part of the Board of Longitude archive at Cambridge University Library.\n\nHis biography, \"The Death of William Gooch: A History's Anthropology\", was written in 1995 by Greg Dening and illustrates the dangers of cross-cultural encounters in the exploration era.\n\n\n\n"}
