{"id": "2139612", "url": "https://en.wikipedia.org/wiki?curid=2139612", "title": "A Course of Modern Analysis", "text": "A Course of Modern Analysis\n\n\"A Course of Modern Analysis: an introduction to the general theory of infinite processes and of analytic functions; with an account of the principal transcendental functions\" (colloquially known as \"Whittaker and Watson\") is a landmark textbook on mathematical analysis written by E. T. Whittaker and G. N. Watson, first published by Cambridge University Press in 1902. (The first edition was Whittaker's alone; it was in later editions with Watson that this book is best known.)\n\nIts first, second, third, and the fourth, last edition were published in 1902, 1915, 1920, and 1927, respectively. Since then, it has continuously been reprinted and still in print today.\n\nThe book is notable for being the standard reference and textbook for a generation of Cambridge mathematicians including Littlewood and G. H. Hardy. Mary Cartwright studied it as preparation for her final honours on the advice of fellow student V.C. Morton, later Professor of Mathematics at Aberystwyth University. But its reach was much further than just the Cambridge school; André Weil in his obituary of the French mathematician Jean Delsarte noted that Delsarte always had a copy on his desk.\n\nToday, the book retains much of its original appeal. Some idiosyncratic but interesting problems from the salad days of the Cambridge Mathematical Tripos are to be found in the exercises. It is terse, yet readable by the motivated student. It conforms to high standards of mathematical rigour, while compressing much actual formulaic information also.\n\nThe book was one of the earliest to use decimal numbering for its sections, an innovation the authors attribute to Giuseppe Peano.\n\nBelow is the contens of the fourth edition:\n\n\n\n"}
{"id": "1698066", "url": "https://en.wikipedia.org/wiki?curid=1698066", "title": "Abuse of notation", "text": "Abuse of notation\n\nIn mathematics, abuse of notation occurs when an author uses a mathematical notation in a way that is not formally correct but that seems likely to simplify the exposition or suggest the correct intuition (while being unlikely to introduce errors or cause confusion). However, the concept of formal correctness depends on time and on the context. Therefore, many notations in mathematics are qualified as abuse of notation in some context and are formally correct in other contexts; as many notations were introduced a long time before any formalization of the theory in which they are used, the qualification of abuse of notation is strongly time dependent. Moreover, many abuses of notation may be made formally correct by improving the theory. \"Abuse of notation\" should be contrasted with \"misuse\" of notation, which should be avoided.\n\nMany mathematical objects consist of a set, often called the underlying set, equipped with some additional structure, typically a mathematical operation or a topology. It is a common abuse of notation to use the same notation for the underlying set and the structured object. For example, formula_2 may denote the set of the integers, the group of integers together with addition, or the ring of integers with addition and multiplication. In general, there is no problem with this, and avoiding such an abuse of notation would make mathematical texts pedantic and difficult to read. When this abuse of notation may be confusing, one may distinguish between these structures by denoting formula_3 the group of integers with addition, and formula_4 the ring of integers.\n\nSimilarly, a topological space consists of a set (the underlying set) and a topology formula_5 which is characterized by a set of subsets of (the open sets). Most frequently, one considers only one topology on , and there is no problem to denote by both the underlying set, and the pair consisting of and its topology formula_5 although they are different mathematical objects. Nevertheless, it occurs sometimes that two different topologies are considered simultaneously on the same set; for distinguishing the corresponding topological spaces, one must use notation such as formula_7 and formula_8\n\nOne encounters, in many textbooks, sentences such as \"Let be a function ...\". This is an abuse of notation, as the name of the function is , and denotes normally the value of the function for the element of its domain. The correct phrase would be \"Let be a function of the variable ...\" or \"Let be a function ...\" This abuse of notation is widely used, as it simplifies the formulation, and the systematic use of a correct notation quickly becomes pedantic.\n\nA similar abuse of notation occurs in sentences such as \"Let us consider the function ...\" In fact is not a function. The function is the operation that associates to , often denoted as . Nevertheless, this abuse of notation is widely used since it is generally not confusing.\n\nMany mathematical structures are defined through a characterizing property (often a universal property). Once this desired property is defined, there may be various ways to construct the structure, and the corresponding results are formally different objects, but which have exactly the same properties – they are isomorphic. As there is no way to distinguish these isomorphic objects through their properties, it is standard to consider them as equal, even if this is formally wrong.\n\nOne example of this the Cartesian product, which is often seen as associative:\n\nBut this is not strictly true: if formula_10, formula_11 and formula_12, the identity formula_13 would imply that formula_14 and formula_15, and so formula_16 would mean nothing.\n\nThis notion can be made rigorous in category theory, using the idea of a natural isomorphism.\n\nAnother example occurs in such statements as \"there are two non-Abelian groups of order 8\", which more strictly stated means \"there are two isomorphism classes of non-Abelian groups of order 8\".\n\nReferring to an equivalence class of an equivalence relation by \"x\" instead of [\"x\"] is an abuse of notation. Formally, if a set \"X\" is partitioned by an equivalence relation ~, then for each \"x\" ∈ \"X\", the equivalence class {\"y\" ∈ \"X\" | \"y\" ~ \"x\"} is denoted [\"x\"]. But in practice, if the remainder of the discussion is focused on equivalence classes rather than individual elements of the underlying set, it is common to drop the square brackets in the discussion.\n\nFor example, in modular arithmetic, a finite group of order \"n\" can be formed by partitioning the integers via the equivalence relation \"x\" ~ \"y\" if and only if \"x\" ≡ \"y\" (mod \"n\"). The elements of that group would then be [0], [1], …, [\"n\" − 1], but in practice they are usually just denoted 0, 1, …, \"n\" − 1.\n\nAnother example is the space of (classes of) measurable functions over a measure space, or classes of Lebesgue integrable functions, where the equivalence relation is equality \"almost everywhere\".\n\nThe terms \"abuse of language\" and \"abuse of notation\" depend on context. Writing \"\"f\": \"A\" → \"B\"\" for a partial function from \"A\" to \"B\" is almost always an abuse of notation, but not in a category theoretic context, where \"f\" can be seen as a morphism in the category of sets and partial functions.\n\n"}
{"id": "34905254", "url": "https://en.wikipedia.org/wiki?curid=34905254", "title": "Actuarial credentialing and exams", "text": "Actuarial credentialing and exams\n\nThe actuarial credentialing and exam process usually requires passing a rigorous series of professional examinations, most often taking several years in total, before one can become recognized as a credentialed actuary. In some countries, such as Denmark, most study takes place in a university setting. In others, such as the U.S., most study takes place during employment through a series of examinations. In the UK, and countries based on its process, there is a hybrid university-exam structure.\n\nThe education system in Australia is divided into three components: an exam-based curriculum; a professionalism course; and work experience . The system is governed by the Institute of Actuaries of Australia.\n\nThe exam-based curriculum is in three parts. Part I relies on exemptions from an accredited under-graduate degree from either Monash University, Macquarie University, University of New South Wales, University of Melbourne, Australian National University or Curtin University . The courses cover subjects including finance, financial mathematics, economics, contingencies, demography, models, probability and statistics. Students may also gain exemptions by passing the exams of the Institute of Actuaries in London . Part II is the Actuarial control cycle and is also offered by each of the universities above . Part III consists of four half-year courses of which two are compulsory and the other two allow specialization .\n\nTo become an Associate, one needs to complete Part I and Part II of the accreditation process, perform 3 years of recognized work experience, and complete a professionalism course.\n\nTo become a Fellow, candidates must complete Part I, II, III, and take a professionalism course. Work experience is not required however, as the Institute deems that those who've successfully completed Part III have shown enough level of professionalism.\n\nThe Actuarial Society of Bangladesh is the unique Professional Body of Actuaries in Bangladesh. Actuarial Society of Bangladesh follow the curriculum of The Institute and Faculty of Actuaries, United Kingdom. \nNo University in Bangladesh provide the academic degrees like bachelor's degree, master's degrees etc. in Actuarial Science. University of Dhaka, Rajshahi University, Jahangirnagar University provide a few course of Actuarial Science in Statistics, Applied Statistics department. From 2015 Dhaka university and East west university are providing Master's in Actuarial Science.\n\nThe Canadian Institute of Actuaries (the CIA) recognizes fellows of both the Society of Actuaries and the Casualty Actuary Society, provided that they have specialized study in Canadian actuarial practice. For fellows of the SOA, this is fulfilled by taking the CIA’s Practice Education Course (PEC). For fellows of the Casualty Actuarial Society, this is fulfilled by taking the nation specific Exam 6-Canada, instead of Exam 6-United States . Unlike their American counterparts, the CIA only has one class of actuary: Fellow. Further, the CIA requires three years of actuarial practice within the previous decade, and 18 months of Canadian actuarial practice within the last three years, to become a fellow .\nThe CIA also offers an associate designation.\n\nIn Denmark it normally takes five years of study at the University of Copenhagen to become an actuary with no professional experience requirement. There is a focus on statistics and probability theory, and a requirement for a master's thesis . By Danish law, responsibility for the practise of any life insurance business must be taken by a formally acknowledged and approved actuary. Approval as a formally responsible actuary requires three to five years of professional experience.\n\nCurrent rules for the German Actuarial Society require an actuary to pass more than 13 exams. \n\nIn Greece the only specialized school of actuaries is the Department of Statistics and Actuary-Finance Mathematics of the University of the Aegean, in Samos. The duration of studies is four years, with a practice period included, and the certificate given is a bachelor's degree. The Diploma of Actuary is given by the Actuaries Union of Greece, after successful exams within the Union. Other schools that offer actuary directions can be found throughout the rest departments of Statistics in the various universities of the country, most notably that of the Athens University of Economics and Business (OPA/ASOEE), which is also the top economic university of Greece.\n\nThe Actuarial Society of India (now converted into Institute of Actuaries of India) offers both associate-ship and fellowship classes of membership. However, prospective candidates must be admitted to the society as students before they achieve associate-ship or fellowship. The exam sequence is similar to the British model, with Core and Specialty technical and application exams. The exams are conducted twice a year during the months of May–June and October–November . Starting from January 2012, the institute has started conducting entrance exam. Only those applicants who clear the entrance test can appear for the Core Technical papers.\n\nItalian actuaries also receive their training through university plus a single examination given by the state (\"Esame di Stato\"). The studies usually take a total of five years to complete, three (\"Triennale\") plus two (\"Magistrale\"), because students need to pass at least 30 exams (the exact number depends on the university and curriculum), many with both written and oral components on actuarial and economic topics. After university, to become qualified to sign statements of actuarial opinion, students must pass the \"Esame di Stato\", which is offered twice a year in Rome and Trieste; the \"Esame di Stato\" consists of two written sections, a practical portion, and an oral exam. The association of qualified actuaries is called \"Ordine degli Attuari\" (\"Order of Actuaries\").\n\nUnlike in the United States, in Mexico actuarial training consists of a full four or five-year licenciatura (bachelor) degree course. Only a few universities in the country offer the degree; some of them are the National Autonomous University of Mexico (UNAM), Autonomous University of Yucatán (UADY), Universidad de las Americas Puebla (UDLAP), Universidad Anahuac, Autonomous Technological Institute of Mexico (ITAM), Autonomous University of Guadalajara (UAG), and Autonomous University of Nuevo León (UANL).\n\nIn Norway the education to become an actuary takes five years. The education usually consists of a bachelor's degree (three years) and a master's degree (two years). The bachelor's degree needs to contain a specific amount of courses in mathematics and statistics. The master's degree usually consists of one year of courses and one year writing a master's degree about a topic related to the actuarial profession. The University of Bergen and The University of Oslo offers the education to become an actuary in Norway .\nTo become an international qualified actuary, a person with a Norwegian actuarial education must also take two courses in economics (macroeconomics and accounting) and a course in ethics. The ethics course, which lasts a day, is offered by the Norwegian Society of Actuaries .\n\nIn Portugal the only school that offers a degree in actuarial science is ISEG at the University of Lisbon. It is a two-year master's degree, fully integrated into the Bologna regimen. The programme is, since 2017/18, accredited by the Institute and Faculty of Actuaries in the UK, leading to exemptions based on the student overall performance during the course, or leading to exemptions from individual exams based on the student's performance in certain modules of the Masters.\n\nActuaries in South Africa are served by the Actuarial Society of South Africa (ASSA). Until recently the requirement to qualify as an actuary in South Africa was to pass the exams hosted by the UK bodies. Starting in 2010, a South African actuarial qualification hosted by ASSA has replaced this arrangement (ASSA's website). Key changes include exam syllabuses based on South African specific content. The UK actuarial professional bodies however still supports Actuaries qualification through the UK. Students may receive exemption from part of the examinations for qualification from approved universities. The South Africa qualification does have mutual recognition with many of the international actuarial bodies as well as approval of the syllabus from the International Actuarial Association.\n\nOne may obtain the Chartered Enterprise Risk Actuary (CERA) designation through the ASSA.\n\nActuarial training in Sweden takes place at Stockholm University. The five-year master's program (for those with no previous university-level knowledge in mathematics, or without a bachelor's degree in mathematics) covers the subjects mathematics, mathematical statistics, insurance mathematics, financial mathematics, insurance law and insurance economics. The program operates under the Division of Mathematical Statistics . For those with a bachelor's degree in mathematics statistics or with a master's degree in mathematics, a two years full-time master's degree Aktuarieprogrammet has been created since 2002, at Stockholm University, which has a long history of research on insurance mathematics.\n\nQualification in Turkey consists of a series of exams administered by an exam board made up of representatives of the Actuarial Society of Turkey, the government and universities. The exams are split into 3 levels: first level (essentials of insurance and economy, mathematics, statistics and probability, financial mathematics); second level (accounting and financial reporting, insurance mathematics (life and non-life), risk analysis, actuarial modeling); and third level (investment and risk management, non-life insurance, life insurance, health insurance, pension systems). After completing the first level exams, a candidate becomes an \"actuarial trainee\", after the second level an \"assistant actuary\", and after the third level and 3 years of related work experience the candidate becomes an \"actuary\".\n\nQualification in the United Kingdom and Ireland consists of a combination of exams and courses provided by the professional body, the Institute and Faculty of Actuaries. The exams may only be taken upon having officially joined the body, unlike many other countries where exams may be taken earlier. Most trainee actuaries study while working for an actuarial employer using resources provided by ActEd (The Actuarial Education Company, a subsidiary of BPP Actuarial Education Ltd.), which is contracted to provide actuarial tuition for students on behalf of Institute and Faculty Education Ltd (IFE), a subsidiary of the Institute and Faculty of Actuaries.\n\nHowever, a candidate may offer proof of having previously covered topics (at a high enough standard, usually while at university) to be exempt from taking certain subjects.\n\nThe exams themselves are split into four sections: Core Technical (CT), Core Applications (CA), Specialist Technical (ST), and Specialist Applications (SA). For students who joined the Profession after June 2004, a further requirement that the student carry out a \"Work-based skills\" exercise has been brought into effect. This involves the student submitting a series of essays to the Profession detailing the work that he or she has performed. In addition to exams, essays and courses, it is required that the candidate have at least three years' experience of actuarial work under supervision of a recognized actuary to qualify as a Fellow of the Institute of Actuaries (FIA) or of the Faculty of Actuaries (FFA) .\n\nActuaries can also gain partial credit towards Fellowship of the Institute and Faculty of Actuaries by following an actuarial science degree at an accredited university. At the undergraduate level the only locally accredited programmes are currently at University of Manchester, University College Dublin, Queen's University Belfast, Heriot-Watt University, University of Edinburgh, the London School of Economics, University of Southampton, City University, London, University of Leicester and the University of Kent. Full-time accredited masters programmes are provided only by the University of Kent, Heriot-Watt University, University of Leicester and City University; part-time accredited master's degrees are offered by Imperial College London and the University of Leicester. Actuarial programmes that offer the possibility of exemption from individual professional exams are also available at City University, London, Heriot-Watt University, the London School of Economics, the University of Southampton, Swansea University, the University of Kent and the University of Warwick. In Ireland exemptions are offered by National University of Ireland, Galway, Dublin City University, University College Cork. Some South African universities are also accredited by the Institute and Faculty of Actuaries. These universities include the University of Pretoria, University of Cape Town, Stellenbosch University, University of the Free State and the University of the Witwatersrand. ISEG in Lisbon, Portugal, offers the possibility of exemption from some professional exams of the Institute and Faculty of Actuaries.\n\nNote that the UK Profession is currently introducing the Certified Actuarial Analyst (CAA) qualification to \"provide those working in financial and actuarial roles at a technical level around the world with valuable skills and a well respected qualification\".\n\nIn the U.S., for life, health, and pension actuaries, exams are given by the Society of Actuaries, while for property-casualty actuaries the exams are administered by the Casualty Actuarial Society. The Society of Actuaries’ requirements for Associateship include passing five preliminary examinations, demonstrating educational experience in economics, corporate finance and applied statistics—called validation by educational experience (VEE), completing an eight-module self-learning series, and taking a course on professionalism . For Fellowship, three other modules, three or four exams depending on specialty track, and a special fellowship admission course is added . The Casualty Actuarial Society requires the successful completion of seven examinations, two modules, and economics and corporate finance VEE's for Associateship and three additional exams for Fellowship. In addition to these requirements, casualty actuarial candidates must also complete professionalism education and be recommended for membership by existing members . One may become a Chartered (or Certified) Enterprise Risk Analyst (CERA) through either the SOA or the CAS.\n\nTo sign certain statements of actuarial opinion, however, American actuaries must be members of the American Academy of Actuaries. Academy membership requirements include membership in one of the recognized actuarial societies, at least three years of full-time equivalent experience in responsible actuarial work, and either residency in the United States for at least three years or a non-resident or new resident who meets certain requirements . Continuing education is required after certification for all actuaries who sign statements of actuarial opinion .\n\nIn the pension area, American actuaries must pass three examinations to become an Enrolled Actuary. Some pension-related filings to the Internal Revenue Service and the Pension Benefit Guaranty Corporation require the signature of an Enrolled Actuary. Many Enrolled Actuaries belong to the Conference of Consulting Actuaries or the American Society of Pension Professionals and Actuaries.\n\nIn 2009, the Society of Actuaries began a high-level accreditation system for universities, recognizing the best actuarial schools as Centers of Actuarial Excellence. There are two sets of criteria that must be met: A Criteria and B Criteria. Additionally, a site visit must be performed by a team of CAE committee members who evaluate the University and conduct interviews with students and faculty. The designation is retained for five years and if a criteria is not met, then the University must provide a plan for how they will address the problem within a reasonable time frame.\n\nThere are five preliminary exams. Most of the exams are multiple choice and administered on computers at Prometric testing centers. Candidates are allowed to use a calculator from an approved list. The exams are timed and last between three and four hours. Some tests provide instant feedback as to whether or not a candidate has passed that particular exam (see table below). All test scores (on a 0-10 scale with 6 or higher passing) are posted six to eight weeks after the test. However, due to the way the test is scaled, the scores can range from 0-10, but there are also situations where the highest grade for a test is a 9 even if every single question was answered correctly.\n\nThrough the end of 2013, four of the preliminary exams (all but MLC) were jointly sponsored by CAS and SOA. In late 2012, SOA announced its intention to end joint sponsorship beginning with tests administered in January 2014. CAS has not announced plans to develop alternative forms of the jointly sponsored exams; however, it accepts SOA exams for CAS credit.\n\nSOA administers exam MLC, which covers life contingencies topics. Starting in May 2014, MLC includes both multiple choice and open-response questions. SOA made this change because, in their view, strict multiple-choice questions are not sufficient or adequate to test whether the candidates are familiar and fluent in the material. The test is four-hours long, allows calculators, and is administered via a paper-and-pencil format. Multiple choice questions account for 40% of the exam, and open-response questions account for 60% of the exam. Candidates may freely move between the two sections. The two sections are graded separately. However, since the multiple-choice questions are easier, only candidates who have answered a certain percentage of the multiple-choice questions correctly have their written answers graded.\n\nCAS develops exam S, as a full alternative to SOA's exam MLC. Exam S covers many topics within statistics, survival models, and stochastic processes. Between the beginning of 2014 and the end of 2015, CAS offered two interim exams: exam LC, covering many life contingencies topics, and exam ST, covering statistical and stochastic methods. Those candidates who passed 3L or MLC before 2014 are exempt from taking LC and ST. Additionally, those candidates passing exam LC, exam ST, and the Statistics VEE by August 2016 are exempt from taking exam S.\n\nCandidates for CAS and SOA membership must pass standardized tests in introductory economics and corporate finance. Candidates for SOA membership must pass an additional standardized test in applied statistics. Economics has two components: macroeconomics and microeconomics. Applied statistics has two components: regression and time series. Instead of passing exams, candidates may earn credit by passing an approved college class with a B- or better grade or by completing an approved correspondence class.\n\nTo earn associate membership (ACAS), a candidate must pass the preliminary exams, VEE, two online modules, exam five, and exam six. For an associate to become a fellow (FCAS), exams seven through nine must be passed. The exams are administered on paper-and-pencil. Exam questions generally require an open answer; however, multiple-choice questions are allowed, too. Exam six comes in two versions: one for candidates in the United States and one for candidates in Canada.\n\nThe two modules that must be completed to become an associate are...\n\nThe exam schedule above started in 2011. Candidates who passed exams offered before 2011 are granted credit according to the following schedule. To earn credit for new exam five, a candidate must pass old exam five and old exam six. Candidates who passed \"one\" of old exam five and old exam six must pass a special exam covering the remaining material on new exam five.\n\nAfter passing the preliminary exams, SOA candidates complete the Fundamentals of Actuarial Practice e-learning course and the Associateship Professionalism Course. FAP contains eight learning modules and two assessments. APC is a live, in-person seminar held in different places around the country. This completes the requirements for associate membership (ASA).\n\nAssociates select one of six areas of competence for further training. Each area has three or four exams and three learning modules. Exam one for \"Retirement Benefits\" has alternative requirements specific to Canada and the United States. For all tracks other than \"Corporate Finance and Enterprise Risk Management,\" a candidate may pass the \"Enterprise Risk Management\" exam as a substitute for exam three. After completing the exams and modules, candidates must pass the \"Decision Making and Communication Module\" and the \"Fellowship Admissions Course\" before earning promotion to fellow (FSA).\n\nChartered Enterprise Risk Analyst (CERA) is a global designation awarded by more than ten international actuarial bodies, including the CAS and SOA. Each body designs its own syllabus and requirements to award the designation, subject to approval by the international CERA body.\n\nCAS candidates must complete all of the requirements to become a FCAS except for exam eight. They must also complete exam ST9, Enterprise Risk Management Specialist Technical, administered by the Institute and Faculty of Actuaries (U.K.) and the Enterprise Risk Management and Modeling Seminar for CERA Qualification\n\nSOA candidates must complete all of the preliminary exams except for exam MLC. Candidates must also pass VEE Economics, VEE Corporate Finance, Fundamentals of Actuarial Practice, the Enterprise Risk Management exam, the Enterprise Risk Management module, and the Associateship Professionalism Course.\n\nIn the US the term \"Enrolled Actuary\" is applied to an individual who has taken certain exams sponsored by the Joint Board for the Enrollment of Actuaries relating to pension plans. Enrollment in the Joint Board is a requirement for SOA Retirement Fellows working in the US.\n\nThe American equivalent of the Canadian Institute of Actuaries is the American Academy of Actuaries (AAA).\n\nMany other countries pattern their requirements after the larger societies of the US or UK. In general, the websites of these organizations are often the easiest source for finding out about membership requirements and resources.\n\n\n\n\n\n\n"}
{"id": "1309", "url": "https://en.wikipedia.org/wiki?curid=1309", "title": "Almost all", "text": "Almost all\n\nIn mathematics, the term \"almost all\" means \"all but a negligible amount\". More precisely, if X is a set, \"almost all elements of X\" means \"all elements of X but those in a negligible subset of X\". The meaning of \"negligible\" depends on the mathematical context; for instance, it can mean finite, countable, or null.\n\nIn contrast, \"almost no\" means \"a negligible amount\"; that is, \"almost no elements of X\" means \"the elements of some negligible subset of X\".\n\nThroughout mathematics, \"almost all\" is sometimes used to mean \"all (elements of an infinite set) but finitely many\". This use occurs in philosophy as well. Similarly, \"almost all\" can mean \"all (elements of an uncountable set) but countably many\".\n\nExamples:\n\nWhen speaking about the reals, sometimes \"almost all\" means \"all reals but a null set\". Similarly, if S is some set of reals, \"almost all numbers in S\" can mean \"all numbers in S but those in a null set\". The real line can be thought of as a one-dimensional Euclidean space. In the more general case of an n-dimensional space (where n is a positive integer), these definitions can be generalised to \"all points but those in a null set\" or \"all points in S but those in a null set\" (this time, S is a set of points in the space). Even more generally, \"almost all\" is sometimes used in the sense of \"almost everywhere\" in measure theory, or in the closely related sense of \"almost surely\" in probability theory.\n\nExamples:\n\nIn number theory, \"almost all positive integers\" can mean \"the positive integers in a set whose natural density is 1\". That is, if A is a set of positive integers, and if the proportion of positive integers below n that are in A (out of all positive integers below n) tends to 1 as n tends to infinity, then almost all positive integers are in A. More generally, let S be an infinite set of positive integers, such as the set of even positive numbers or of primes. If A is a subset of S, and if the proportion of elements of S below n that are in A (out of all elements of S below n) tends to 1 as n tends to infinity, then it can be said that almost all elements of S are in A.\n\nExamples:\n\nIn graph theory, if A is a set of (finite labelled) graphs, it can be said to contain almost all graphs if the proportion of graphs with n vertices that are in A tends to 1 as n tends to infinity. However, it is sometimes easier to work with probabilities, so the definition is reformulated as follows. The proportion of graphs with n vertices that are in A equals the probability that a random graph with n vertices (chosen with the uniform distribution) is in A, and choosing a graph in this way has the same outcome as generating a graph by flipping a coin for each pair of vertices to decide whether to connect them. Therefore, equivalently to the preceding definition, A contains almost all graphs if the probability that a coin flip-generated graph with n vertices is in A tends to 1 as n tends to infinity. Sometimes the latter definition is modified so that the graph is chosen randomly in some other way, where not all graphs with n vertices have the same probability, and those modified definitions are not always equivalent to the main one.\n\nThe use of the term \"almost all\" in graph theory is not standard; the term \"asymptotically almost surely\" is more commonly used for this concept.\n\nExample:\n\nIn topology and especially dynamical systems theory (including applications in economics), \"almost all\" of a topological space's points can mean \"all of the space's points but those in a meagre set\". Some use a more limited definition, where a subset only contains almost all of the space's points if it contains some open dense set.\n\nExample:\n\nIn abstract algebra and mathematical logic, if U is an on a set X, \"almost all elements of X\" sometimes means \"the elements of some \"element\" of U\". For any partition of X into two disjoint sets, one of them necessarily contains almost all elements of X. It is possible to think of the elements of a filter on X as containing almost all elements of X even if it isn't an ultrafilter.\n"}
{"id": "351908", "url": "https://en.wikipedia.org/wiki?curid=351908", "title": "Almost surely", "text": "Almost surely\n\nIn probability theory, one says that an event happens almost surely (sometimes abbreviated as a.s.) if it happens with probability one. In other words, the set of possible exceptions may be non-empty, but it has probability zero. The concept is precisely the same as the concept of \"almost everywhere\" in measure theory.\n\nIn probability experiments on a finite sample space, there is often no difference between \"almost surely\" and \"surely\". However, the distinction becomes important when the sample space is an infinite set, because an infinite set can have non-empty subsets of probability zero.\n\nSome examples of the use of this concept include the strong and uniform versions of the law of large numbers, and the continuity of the paths of Brownian motion.\n\nThe terms almost certainly (a.c.) and almost always (a.a.) are also used. Almost never describes the opposite of \"almost surely\": an event that happens with probability zero happens \"almost never\".\n\nLet formula_1 be a probability space. An event formula_2 happens \"almost surely\" if formula_3. Equivalently, formula_4 happens almost surely if the probability of formula_4 not occurring is zero: formula_6. More generally, any event formula_7 (not necessarily in formula_8) happens almost surely if formula_9 is contained in a null set: a subset of some formula_10 such that The notion of almost sureness depends on the probability measure formula_11. If it is necessary to emphasize this dependence, it is customary to say that the event formula_4 occurs \"P\"-almost surely, or almost surely (\"P\").\n\nIn general, an event can happen \"almost surely\" even if the probability space in question includes outcomes which do not belong to the event, as is illustrated in the examples below.\n\nImagine throwing a dart at a unit square (i.e. a square with area 1) so that the dart always hits exactly one point of the square, and so that each point in the square is equally likely to be hit. \n\nNow, notice that since the square has area 1, the probability that the dart will hit any particular subregion of the square equals the area of that subregion. For example, the probability that the dart will hit the right half of the square is 0.5, since the right half has area 0.5.\n\nNext, consider the event that \"the dart hits a diagonal of the unit square exactly\". Since the areas of the diagonals of the square are zero, the probability that the dart lands exactly on a diagonal is zero. So, the dart will almost never land on a diagonal (i.e. it will almost surely \"not\" land on a diagonal). Nonetheless the set of points on the diagonals is not empty and a point on a diagonal is no less possible than any other point: the diagonal does contain valid outcomes of the experiment.\n\nConsider the case where a (possibly biased) coin is tossed, corresponding to the probability space formula_13, where the event formula_14 occurs if heads is flipped, and formula_15 if tails. For this particular coin, assume the probability of flipping heads is formula_16 from which it follows that the complement event, flipping tails, has formula_17.\n\nSuppose we were to conduct an experiment where the coin is tossed repeatedly, with outcomes formula_18, and it is assumed each flip's outcome is independent of all the others. That is, they are \"i.i.d.\". Define the sequence of random variables on the coin toss space, formula_19 where formula_20. \"i.e.\" each formula_21 records the outcome of the formula_22'th flip.\n\nAny infinite sequence of heads and tails is a possible outcome of the experiment. However, any particular infinite sequence of heads and tails has probability zero of being the exact outcome of the (infinite) experiment. To see why, note that the \"i.i.d.\" assumption implies that the probability of flipping all heads over formula_23 flips is simply formula_24. Letting formula_25 yields zero, since formula_26 by assumption. Note that the result is the same no matter how much we bias the coin towards heads, so long as we constrain formula_27 to be greater than 0, and less than 1.\n\nIn particular, the event \"the sequence contains at least one formula_28\" happens almost surely (i.e., with probability 1).\nHowever, if instead of an infinite number of flips we stop flipping after some finite time, say a million flips, then the all-heads sequence has non-zero probability. The all-heads sequence has probability formula_29, while the probability of getting at least one tails is formula_30 and the event is no longer almost sure.\n\nIn asymptotic analysis, one says that a property holds asymptotically almost surely (a.a.s.) if, over a sequence of sets, the probability converges to 1. For instance, a large number is asymptotically almost surely composite, by the prime number theorem; and in random graph theory, the statement \"formula_31 is connected\" (where formula_32 denotes the graphs on formula_23 vertices with edge probability formula_27) is true a.a.s. when, for any formula_35\n\nIn number theory this is referred to as \"almost all\", as in \"almost all numbers are composite\". Similarly, in graph theory, this is sometimes referred to as \"almost surely\".\n\n\n"}
{"id": "38907720", "url": "https://en.wikipedia.org/wiki?curid=38907720", "title": "Autowave", "text": "Autowave\n\nAutowaves are self-supporting non-linear waves in active media (i.e. those that provide distributed energy sources). The term is generally used in processes where the waves carry relatively low energy, which is necessary for synchronization or switching the active medium.\n\nIn 1980, the Soviet scientists G.R. Ivanitsky, V.I. Krinsky, A.N. Zaikin, A.M. Zhabotinsky, B.P. Belousov became winners of the highest state award of the USSR, \"\"for the discovery of a new class of autowave processes and the study of them in disturbance of stability of the distributed excitable systems\".\"\n\nThe first who studied actively the self-oscillations was Academician AA Andronov, and the term \"auto-oscillations\" in Russian terminology was introduced by AA Andronov in 1928. His followers from Lobachevsky University further contributed greatly to the development of \"autowave theory\".\n\nThe simplest autowave equations describing combustion processes have been studied by A.N. Kolmogorov, I.E. Petrovsky, in 1937., as well as by Ya.B. Zel'dovich и D.A. Frank-Kamenetsky in 1938.\n\nThe classical axiomatic model with autowaves in myocardium was published in 1946 by Norbert Wiener and Arturo Rosenblueth.\n\nDuring 1970-80, major efforts to study autowaves were concentrated in the Institute of Biological Physics of the USSR Academy of Sciences, located in the suburban town Pushchino, near Moscow. It was here, under the guidance of V.I.Krinsky, such world-famous now experts in the field of the autowave researches as A.V.Panfilov, I.R.Efimov, R.R.Aliev, K.I. Agladze, O.A.Mornev, M.A.Tsyganov were educated and trained. V.V.Biktashev, Yu.E. Elkin, A.V. Moskalenko gained their experience with the autowave theory also in Pushchino, in the neighboring Institute of Mathematical Problems of Biology, under the guidance of E.E.Shnoll.\n\nThe term \"\"autowaves\" was proposed, probably, on the analogy of previously \"auto-oscillations\"\".\n\nAlmost immediately after the Dissolution of the Soviet Union, many of these Russian scientists left their native country for working in foreign institutions, where they still continue their studies of autowaves. In particular, E.R.Efimov is developing the \"theory of virtual electrode\", which describes some effects occurring during defibrillation.\n\nAmong other notable scientists, who are engaged in these investigation, there are A.N. Zaikin and E.E.Shnoll (autowaves and bifurcation memory in the blood coagulation system); A.Yu. Loskutov (general autowave theory as well as dynamic chaos in autowaves); V.G. Yakhno (general autowave theory as well as connections between autowaves and process of thinking); K.I. Agladze (autowaves in chemical media); V.N.Biktashev (general autowave theory as well as different sorts of autowave drift); O.A.Mornev (general autowave theory); M.A.Tsyganov (the role of autowave in population dynamics); Yu.E. Elkin, A.V. Moskalenko, (bifurcation memory in a model of cardiac tissue).\n\nA huge role in the study of autowave models of cardiac tissue belongs to Denis Noble and members of his team from the University of Oxford.\n\nOne of the first definitions of autowaves was as follows: \n\nUnlike linear waves — such as sound waves, electromagnetic waves and other, which are inherent in conservative systems and mathematically described by linear second order hyperbolic equations (wave equations), — dynamics of an \"autowave\" in terms of differential equations can be described by parabolic equation \"with nonlinear free member of a special form\".\n\nThe concrete form of the free member formula_1 is extremely important, because: = \\vec{f}(\\vec{u})</math>, which is a self-oscillating or potentially self-oscillating.\n\nвсе волновые процессы порождаются динамикой нелинейной точечной системы formula_2, которая является автоколебательной или потенциально автоколебательной. \n\nCommonly, formula_3 have the form of formula_4-shaped dependence on formula_5. In this sense, the system of equations, known as the Aliev–Panfilov model, is a very exotic example, because formula_6 has in it a very complex form of two intersecting parabolas, besides more crossed with two straight lines, resulting in a more pronounced nonlinear properties of this model.\n\n\"Autowaves\" is an example of a self-sustaining wave process in extensive nonlinear systems containing distributed energy sources. It is correct for simple autowaves, that period, wavelength, propagation speed, amplitude, and some other characteristics of an autowave are determined solely by local properties of the medium. However, in the 21st century, researchers began to discover a growing number of examples of self-wave solutions when the \"classical\" principle is violated.\n\n(See also general information in literature, for example, in).\n\nThe simplest model of autowave is a rank of dominoes that are falling one after another, if you drop an outermost one (so called \"domino effect\"). This is an example of a \"switching wave\".\n\nAs another example of autowaves, imagine that you stand on a field and set fire to the grass. While the temperature is below the threshold, the grass will not take fire. Upon reaching the \"threshold\" temperature (autoignition temperature) the combustion process begins, with the release of heat sufficient to ignite the nearest areas. The result is that the combustion front has been shaped, which spreads through the field. It can be said in such cases that autowave arose, which is one of the results of self-organization in non-equilibrium thermodynamic systems. After some time new grass replaces the burnt grass, and the field acquires again the ability for igniting. This is an example of an \"excitation wave\".\n\nThere are a great deal of other natural objects that are also considered among autowave processes: oscillatory chemical reactions in active media (e.g., Belousov–Zhabotinsky reaction), the spread of excitation pulses along nerve fibres, wave chemical signalling in the colonies of certain microorganisms, autowaves in ferroelectric and semiconductor films, population waves, spread of epidemics and of genes, and many other phenomena.\n\nNerve impulses, which serve as a typical example of autowaves in an active medium with recovery, were studied as far back as 1850 by Hermann von Helmholtz. The properties of nerve impulses that are typical for the simplest self-wave solutions (universal shape and amplitude, independent of the initial conditions, and annihilation under collisions) were ascertained in the 1920s and 1930s.\nLet's consider a 2D active medium consisting of elements, each of which can be found in three different states: rest, excitation and refractoriness. In the absence of external influence, elements are at rest. As a result of an influence upon it, when the concentration of the activator reaches the threshold, the element will switch to an excited state, acquiring the ability to excite the neighbouring elements. Some time after the excitation the element switches to a refractory state, in which it cannot be excited. Then the element return to its initial state of rest, gaining again the ability to transform into an excited state.\n\nAny \"classical\" excitation wave moves in an excitable medium without attenuation, maintaining its shape and amplitude constant. As it passes, the energy loss (dissipation) is completely offset by the energy input from the elements of the active medium. The leading front of an autowave (the transition from rest to a state of excitation) is usually very small: for example, the ratio of the leading front duration to the entire duration of the pulse for a myocardium sample is about 1:330.\n\nUnique opportunities to study the autowave processes in two- and three-dimensional active media with very different kinetics are provided with methods of mathematical modelling using computers. For computer simulation of autowaves, one uses a generalized Wiener–Rosenblueth model, as well as a large number of other models, among which a special place is occupied by The FitzHugh–Nagumo model (the simplest model of an active medium, and its various versions) and The Hodgkin–Huxley model (nerve impulse). There are also many autowave myocardial models: The Beeler–Reuter model, several Noble models (developed by Denis Noble), The Aliev–Panfilov model, The Fenton-Karma model, etc.\n\nIt was also proven that the simplest autowave regimes should be common to every system of differential equations of any complexity that describe a particular active media, because such a system can be simplified to two differential equations.\n\nFirst of all, we should notice, that the elements of the active media can be, at least, of three very different types; these are \"self-exciting\", \"excitable\" and \"trigger\" (or \"bistable\") regimes. Accordingly, there are three types of homogeneous active media composed of these elements.\n\nA bistable element has two stable stationary states, transitions between which occur when external influence exceeds a certain threshold. In media of such elements, \"switching waves\" arise, which switch the medium from one of its states to the other. For instance, a classic case of such a switching autowave — perhaps, the simplest autowave phenomena — is falling dominoes (the example already given). Another simple example of a bistable medium is burning paper: the switching wave propagates in the form of a flame, switching paper from the normal state to its ashes.\n\nAn excitable element has only one stable stationary state. External influence over a threshold level can bring such an element out of its stationary state and perform an evolution before the element will return again to its stationary state. During such evolution, the active element can affect the adjacent elements and, in turn, lead them out of the stationary state too. As a result, the \"excitation wave\" propagates in this medium. This is the most common form of autowaves in biological media, such as nervous tissue, or the myocardium.\nA self-oscillating element has no stationary states and continually performs stable oscillations of some fixed form, amplitude and frequency. External influence can disturb these oscillations. After some relaxation time, all their characteristics except for the phase back to its stable value, but the phase can be changed. As a result, the \"phase waves\" spread in the medium of such elements. Such phase waves can be observed in electro-garlands or in certain chemical media. An example of a self-oscillating medium is the SA node in the heart, in which excitation pulses arise spontaneously.\n\nIt can be clearly seen on the phase portrait of the basic system of equations describing the active medium (see Fig.) that a significant difference between these three types of behaviour of an active medium is caused by the quantity and the position of its singular points. The shape of autowaves observed in reality can be very similar to each other, and therefore it can be difficult to assess the type of element only by the form of the excitation pulse.\n\nBesides, autowave phenomena, which can be observed and investigated, depend greatly on geometrical and topological peculiarities of an active medium.\n\nOne-dimensional cases include autowave spread in cable and its spread in the ring, with the latter mode considering as a limiting case of a rotating wave in two-dimensional active medium, while the first case is considered as spread of the autowave in the ring with zero curvature (i.e., with an infinite radius).\n\nA number of autowave sources is known in the two-dimensional active media. In such a way, it is distinguished at least five type of re-entry, which are \"running around the ring\", \"spiral wave\", \"reverberator\" (i.e., \"two-dimensional autowave vortex\") and \"fibrillation\". The literature identifies two types of sources of \"concentric autowaves\" in 2D active media; these are \"pacemakers\" and \"leading centres\". Both the \"leading centres\" and \"reverberators\" are interesting, because they are not tied to the structure of the medium and can appear and disappear in its different parts. Areas of increased automation may also be an example of a sources of autowaves. Three different types of increased automation are known now:\n\nIn addition about 2D\n\nSee also details in the article rotating autowaves, which may appears as spiral wave or autowave reverberator.\n\nPhenomena of bifurcation memory were observed in behaviour of the autowave reverberator in the Aliev–Panfilov model.\n\n3D.\n\nAn example of a chemical reaction, which in certain circumstances may produce autowave, is the Belousov–Zhabotinsky reaction.\n\nThe main item on the page \"Hodgkin–Huxley model\"\n\nThe classical Wiener—Rosenblueth model, which is, accordingly, developed by Norbert Wiener and Arturo Rosenblueth.\n\nAmong other examples are the following: FitxHue-Nagumo, the Beeler-Reuter model.\n\nMain article is planned to be on the special page \"Autowave models of myocardium\"\n\nSee References.\n\n\n"}
{"id": "42148664", "url": "https://en.wikipedia.org/wiki?curid=42148664", "title": "Benacerraf's identification problem", "text": "Benacerraf's identification problem\n\nIn the philosophy of mathematics, Benacerraf's identification problem is a philosophical argument, developed by Paul Benacerraf, against set-theoretic Platonism. In 1965, Benacerraf published a paradigm changing article entitled \"What Numbers Could Not Be\". Historically, the work became a significant catalyst in motivating the development of mathematical structuralism.\n\nThe identification problem argues that there exists a fundamental problem in reducing natural numbers to pure sets. Since there exists an infinite number of ways of identifying the natural numbers with pure sets, no particular set-theoretic method can be determined as the \"true\" reduction. Benacerraf infers that any attempt to make such a choice of reduction immediately results in generating a meta-level, set-theoretic falsehood, namely in relation to other elementarily-equivalent set-theories not identical to the one chosen. The identification problem argues that this creates a fundamental problem for Platonism, which maintains that mathematical objects have a real, abstract existence. Benacerraf's dilemma to Platonic set-theory is arguing that the Platonic attempt to identify the \"true\" reduction of natural numbers to pure sets, as revealing the intrinsic properties of these abstract mathematical objects, is impossible. As a result, the identification problem ultimately argues that the relation of set theory to natural numbers cannot have an ontologically Platonic nature.\n\nThe historical motivation for the development of Benacerraf's identification problem derives from a fundamental problem of ontology. Since Medieval times, philosophers have argued as to whether the ontology of mathematics contains abstract objects. In the philosophy of mathematics, an abstract object is traditionally defined as an entity that: (1) exists independent of the mind; (2) exists independent of the empirical world; and (3) has eternal, unchangeable properties. Traditional mathematical Platonism maintains that some set of mathematical elements–natural numbers, real numbers, functions, relations, systems–are such abstract objects. Contrarily, mathematical nominalism denies the existence of any such abstract objects in the ontology of mathematics. \nIn the late 19th and early 20th century, a number of anti-Platonist programs gained in popularity. These included intuitionism, formalism, and predicativism. By the mid-20th century, however, these anti-Platonist theories had a number of their own issues. This subsequently resulted in a resurgence of interest in Platonism. It was in this historic context that the motivations for the identification problem developed.\n\nThe identification problem begins by evidencing some set of elementarily-equivalent, set-theoretic models of the natural numbers. Benacerraf considers two such set-theoretic methods:\n\nAs Benacerraf demonstrates, both method I and II reduce natural numbers to sets. Benacerraf formulates the dilemma as a question: which of these set-theoretic methods uniquely provides the true identity statements, which elucidates the true ontological nature of the natural numbers? Either method I or II could be used to define the natural numbers and subsequently generate true arithmetical statements to form a mathematical system. In their relation, the elements of such mathematical systems are isomorphic in their structure. However, the problem arises when these isomorphic structures are related together on the meta-level. The definitions and arithmetical statements from system I are not identical to the definitions and arithmetical statements from system II. For example, the two systems differ in their answer to whether 0 ∈ 2, insofar as ∅ is not an element of . Thus, in terms of failing the transitivity of identity, the search for true identity statements similarly fails. By attempting to reduce the natural numbers to sets, this renders a set-theoretic falsehood between the isomorphic structures of different mathematical systems. This is the essence of the identification problem.\n\nAccording to Benacerraf, the philosophical ramifications of this identification problem result in Platonic approaches failing the ontological test. The argument is used to demonstrate the impossibility for Platonism to reduce numbers to sets that reveals the existence of abstract objects.\n\n\n"}
{"id": "33153333", "url": "https://en.wikipedia.org/wiki?curid=33153333", "title": "Blackwell–Tapia prize", "text": "Blackwell–Tapia prize\n\nThe Blackwell–Tapia Prize is a mathematics award presented every other year at the Blackwell-Tapia conference.\nThe conference is sponsored by the National Science Foundation and is organized and hosted by various mathematics institutes.\nThe prize is named for David Blackwell and Richard Tapia.\nIt recognizes someone who has made significant research contributions in their field, and who has worked to address the problem of under-representation of minority groups in mathematics.\n\nThe following mathematicians have been honored with the Blackwell–Tapia Prize:\n"}
{"id": "59338", "url": "https://en.wikipedia.org/wiki?curid=59338", "title": "Bracket", "text": "Bracket\n\nA bracket is a tall punctuation mark commonly used in matched pairs within text, to set apart or interject other text. The matched pair is best described as \"opening\" and \"closing\". Less formally, in a left-to-right context, it may be described as \"left\" and \"right\", and in a right-to-left context, as \"right\" and \"left\".\n\nForms include round (also called \"parentheses\"), square, curly (also called \"braces\"), and angle brackets (also called \"chevrons\"), as well as various other pairs of symbols.\n\nIn addition to referring to the class of all types of brackets, the unqualified word \"bracket\" is most commonly used to refer to a specific type of bracket: in modern American usage, this is usually the square bracket and in modern British usage, this is usually the round bracket.\n\nChevrons (⟨ ⟩) were the earliest type of bracket to appear in written English. Desiderius Erasmus coined the term \"lunula\" to refer to the rounded parentheses (), recalling the shape of the crescent moon.\n\nSome of the following names are regional or contextual.\n\n\nThe characters ‹ › and « », known as s or angular quote brackets, are actually quotation mark glyphs used in several European languages. Which one of each pair is the opening quote mark and which is the closing quote varies between languages.\n\nSimilarly, the corner-brackets ｢ ｣ are quotation marks used in East Asian languages (see Quotation mark § Chinese, Japanese and Korean quotation marks).\n\nIn English, typographers mostly prefer to not set brackets in italics, even when the enclosed text is italic. However, in other languages like German, if brackets enclose text in italics, they are usually set in italics too.\n\nParentheses (singular, parenthesis ) (also called simply brackets, or round brackets, curves, curved brackets, oval brackets, stalls or, colloquially, parens ) contain material that serves to clarify (in the manner of a gloss) or is aside from the main point. A milder effect may be obtained by using a pair of commas as the delimiter, though if the sentence contains commas for other purposes, visual confusion may result.\n\nIn American usage, parentheses are usually considered separate from other brackets, and calling them \"brackets\" is unusual.\n\nParentheses may be used in formal writing to add supplementary information, such as \"Sen. John McCain (R-Arizona) spoke at length\". They can also indicate shorthand for \"either singular or plural\" for nouns, e.g. \"the claim(s)\". It can also be used for gender neutral language, especially in languages with grammatical gender, e.g. \"(s)he agreed with his (her) physician\".\n\nParenthetical phrases have been used extensively in informal writing and stream of consciousness literature. Examples include the southern American author William Faulkner (see \"Absalom, Absalom!\" and ) as well as poet E. E. Cummings.\n\nParentheses have historically been used where the dash is currently used in alternatives, such as \"parenthesis)(parentheses\". Examples of this usage can be seen in editions of \"Fowler's\".\n\nParentheses may be nested (generally with one set (such as this) inside another set). This is not commonly used in formal writing (though sometimes other brackets [especially square brackets] will be used for one or more inner set of parentheses [in other words, secondary {or even tertiary} phrases can be found within the main parenthetical sentence]).\n\nAny punctuation inside parentheses or other brackets is independent of the rest of the text: \"Mrs. Pennyfarthing (What? Yes, that was her name!) was my landlady.\" In this usage, the explanatory text in the parentheses is a parenthesis. (Parenthesized text is usually short and within a single sentence. Where several sentences of supplemental material are used in parentheses the final full stop would be within the parentheses, or simply omitted. Again, the parenthesis implies that the meaning and flow of the text is supplemental to the rest of the text and the whole would be unchanged were the parenthesized sentences removed.)\n\nIn more formal usage, \"parenthesis\" may refer to the entire bracketed text, not just to the punctuation marks used (so all the text in this set of round brackets may be said to be \"a parenthesis\", \"a parenthetical\", or \"a parenthetical phrase\").\n\nLower-case Latin letters used as indexes, rather than bullets or numbers, followed by an unpaired parenthesis, are used in ordered lists especially in:\n\nSince 2014, antisemites have used triple parentheses around the names of people to denote them as Jewish.\n\nParentheses are used in mathematical notation to indicate grouping, often inducing a different order of operations. For example: in the usual order of algebraic operations, equals 14, since the multiplication is done before the addition. However, equals 20, because the parentheses override normal precedence, causing the addition to be done first. Some authors follow the convention in mathematical equations that, when parentheses have one level of nesting, the inner pair are parentheses and the outer pair are square brackets. Example:\n\nA related convention is that when parentheses have two levels of nesting, curly brackets (braces) are the outermost pair. Following this convention, when more than three levels of nesting are needed, often a cycle of parentheses, square brackets, and curly brackets will continue. This helps to distinguish between one such level and the next.\n\nParentheses are also used to set apart the arguments in mathematical functions. For example, is the function ' applied to the variable '. In coordinate systems parentheses are used to denote a set of coordinates; so in the Cartesian coordinate system may represent the point located at 4 on the \"x\"-axis and 7 on the \"y\"-axis. \n\nParentheses may also be used to represent a binomial coefficient.\n\nParentheses are included in the syntaxes of many programming languages. Typically needed to denote an argument; to tell the compiler what data type the Method/Function needs to look for first in order to initialise. In some cases, such as in LISP, parentheses are a fundamental construct of the language.\n\nParentheses are used in chemistry to denote a polyatomic ion. They can be used in various fields as notation to indicate the amount of uncertainty. For example:\nis equivalent to:\n\nMany online Roleplayers use double parentheses to connotate out-of-character (OOC) messages that one may send another.\n\nSquare brackets—also called crotchets or simply brackets (US)—are mainly used to insert explanatory material or to mark where a passage was omitted from an original material by someone other than the original author, or to mark modifications in quotations.\n\nA bracketed ellipsis, […], is often used to indicate omitted material: \"I'd like to thank [several unimportant people] for their tolerance [...]\"\nBracketed comments inserted into a quote indicate when the original has been modified for clarity: \"I appreciate it [the honor], but I must refuse\", and \"the future of psionics [see definition] is in doubt\". Or one can quote the original statement \"I hate to do laundry\" with a (sometimes grammatical) modification inserted: He \"hate[s] to do laundry\".\n\nAdditionally, a small letter can be replaced by a capital one, when the beginning of the original text is omitted for succinctness, for example, when referring to a verbose original: \"To the extent that policymakers and elite opinion in general have made use of economic analysis at all, they have, as the saying goes, done so the way a drunkard uses a lamppost: for support, not illumination\", it can be quoted succinctly as: \"[P]olicymakers […] made use of economic analysis […] the way a drunkard uses a lamppost: for support, not illumination.\" When nested parentheses are needed, brackets are used as a substitute for the inner pair of parentheses within the outer pair. When deeper levels of nesting are needed, convention is to alternate between parentheses and brackets at each level.\n\nAlternatively, empty square brackets can also indicate omitted material, usually single letter only. The original \"Reading is also a process and it also changes you.\" can be rewritten in a quote as: It has been suggested that reading can \"also change[] you\".\n\nThe bracketed expression \"[\"sic\"]\" is used after a quote or reprinted text to indicate the passage appears exactly as in the original source, where it may otherwise appear that a mistake has been made in reproduction.\n\nIn translated works, brackets are used to signify the same word or phrase in the original language to avoid ambiguity.\nFor example: \"He is trained in the way of the open hand [karate].\"\n\nBrackets (called \"move-left symbols\" or \"move right symbols\") are added to the sides of text in proofreading to indicate changes in indentation:\n\nBrackets are used in mathematics in a variety of notations, including standard notations for commutators, the floor function, the Lie bracket, equivalence classes, the Iverson bracket, and matrices. Square brackets may also represent closed intervals; formula_2 for example, represents the set of real numbers from 0 to 5 inclusive.\n\nBrackets can also be used in chemistry to represent the concentration of a chemical substance or to denote distributed charge in a complex ion.\n\nBrackets are used in many computer programming languages, primarily to force the order of evaluation and for parameter lists and array indexing. But they are also used to denote general tuples, sets and other structures, just as in mathematics. There may be several other uses as well, depending on the language at hand.\n\nIn linguistics, phonetic transcriptions are generally enclosed within brackets, often using the International Phonetic Alphabet, whereas phonemic transcriptions typically use paired slashes. Pipes (| |) are often used to indicate a morphophonemic rather than phonemic representation. Other conventions are double slashes (// //), double pipes (|| ||) and curly brackets ({ }). In lexicography, square brackets usually surround the section of a dictionary entry which contains the etymology of the word the entry defines.\n\nBrackets are used to denote parts of the text that need to be checked when preparing drafts prior to finalizing a document. They often denote points that have not yet been agreed to in legal drafts and the year in which a report was made for certain case law decisions.\n\nCurly brackets { and } are also called braces in the United States (or, colloquially, squiggly brackets). They are rarely used in prose and have no widely accepted use in formal writing, but may be used to mark words or sentences that should be taken as a group, to avoid confusion when other types of brackets are already in use, or for a special purpose specific to the publication (such as in a dictionary). More commonly, they are used to indicate a group of lines that should be taken together, as in when referring to several lines of poetry that should be repeated.\n\nIn music, they are known as accolades or \"braces\", and connect two or more lines (staves) of music that are played simultaneously.\n\nIn mathematics they delimit sets, and in writing, they may be used similarly, \"Select your animal {goat, sheep, cow, horse} and follow me\". In many programming languages, they enclose groups of statements and create a local scope. Such languages (C being one of the best-known examples) are therefore called curly bracket languages. In classical mechanics, curly brackets are often also used to denote the Poisson bracket between two quantities.\n\n , similar to the commonly used less-than (<) and greater-than sign (>), are often used to enclose highlighted material.\n\nIn physical sciences, chevrons are used to denote an average over time or over another continuous parameter. For example,\n\nThe inner product of two vectors is commonly written as , but the notation is also used.\n\nIn mathematical physics, especially quantum mechanics, it is common to write the inner product between elements as , as a short version of , or , where is an operator. This is known as Dirac notation or bra–ket notation.\n\nIn set theory, chevrons or parentheses are used to denote ordered pairs and other tuples, whereas curly brackets are used for unordered sets.\n\nIn linguistics, chevrons indicate graphemes (i.e., written letters) or orthography, as in \"The English word is spelled .\"\n\nIn epigraphy, they may be used for mechanical transliterations of a text into the Latin script.\n\nIn textual criticism, and hence in many editions of pre-modern works, chevrons denote sections of the text which are illegible or otherwise lost; the editor will often insert their own reconstruction where possible within them.\n\nChevrons are infrequently used to denote words that are thought instead of spoken, such as:\n\nThe mathematical or logical symbols for \"greater-than\" (>) and \"less-than\" (<) are inequality symbols; when either symbol is bisected by a vertical line, it represents \"not greater than\" or \"not less than,\" respectively. These symbols are not punctuation marks when used, as intended, to represent an inequality. However, as true chevrons are not present on computer keyboards, the available less-than and greater-than symbols are often used instead. They are loosely referred to as \"angle[d] brackets\" or \"chevrons\" in this case, but more properly — and less confusingly — as \"pointy brackets\" (see the Names section above).\n\nSingle and double pairs of comparison operators («, ») (meaning \"much smaller than\" and \"much greater than\") are sometimes used as a fallback instead of guillemets («, ») (used as quotation marks in many languages) when the proper characters are not available on the keyboard nor in the input editor. Similarly, early Internet messaging conventions developed to use the \"greater-than\" sign (>), available in the ASCII character set, to mark quoted lines. This format, known as Usenet quoting, is used by email clients when operating in plain text mode.\n\nIn comic books, chevrons are often used to mark dialogue that has been translated notionally from another language; in other words, if a character is speaking another language, instead of writing in the other language and providing a translation, one writes the translated text within chevrons. Of course, since no foreign language is actually written, this is only \"notionally\" translated.\n\nIn continuum mechanics, chevrons may be used as Macaulay brackets.\n\nIn East Asian punctuation, angle brackets are used as quotation marks. Chevron-like symbols are part of standard Chinese, and Korean punctuation, where they generally enclose the titles of books: ︿ and ﹀ or ︽ and ︾ for traditional vertical printing, and 〈 and 〉 or 《 and 》 for horizontal printing.\n\nSome East Asian languages use lenticular brackets 【 】, a combination of square brackets and round brackets called (\"fāngtóu kuòhào\") in Chinese and (\"sumitsuki\") in Japanese. and used in titles and headings in Japanese.\n\nThe floor corner brackets ⌊ and ⌋, the ceiling corner brackets ⌈ and ⌉ are used to denote the integer floor and ceiling functions.\n\nThe Quine corners ⌜ and ⌝ have at least two uses in mathematical logic: either as quasi-quotation, a generalization of quotation marks, or to denote the Gödel number of the enclosed expression.\n\nHalf brackets are used in English to mark added text, such as in translations: \"Bill saw ⸤her⸥\".\n\nIn editions of papyrological texts, half brackets, ⸤ and ⸥ or ⸢ and ⸣, enclose text which is lacking in the papyrus due to damage, but can be restored by virtue of another source, such as an ancient quotation of the text transmitted by the papyrus. For example, Callimachus \"Iambus\" 1.2 reads: ἐκ τῶν ὅκου βοῦν κολλύ⸤βου π⸥ιπρήσκουσιν. A hole in the papyrus has obliterated βου π, but these letters are supplied by an ancient commentary on the poem. Second intermittent sources can be between ⸢ and ⸣. Quine corners are sometimes used instead of half brackets.\n\nDouble brackets (or white square brackets), ⟦ ⟧, are used to indicate the \"semantic evaluation function\" in formal semantics for natural language and denotational semantics for programming languages. The brackets stand for a function that maps a linguistic expression to its “denotation” or semantic value. Double brackets may also refer to the mathematical floor function.\n\nKnown as \"spike parentheses\" () ⁅ and ⁆ are used in Swedish dictionaries.\n\nThe various bracket characters are frequently used in many programming languages as operators or for other syntax markup. For instance, in C-like languages, codice_1 and codice_2 are often used to delimit a code block, and the parameters of method calls are generally enclosed by codice_3 and codice_4.\n\nIn C, C++, Java and other C-derived languages—as well as in Scheme-influenced languages that have adopted C/Java syntax, such as JavaScript—the \"codice_5\" symbols are referred to as \"braces\" or \"curly braces\" and never as brackets. Since the term \"brace\" is documented in the definitive programming specifications for these languages, it is preferable to use the correct term brace so there is no confusion between the brace (used to denote compound statements) and the bracket, used to denote other concepts, such as array indices.\n\nIn addition to the use of parentheses to specify the order of operations, both parentheses and brackets are used to denote an interval, also referred to as a half-open range. The notation is used to indicate an interval from to that is inclusive of but exclusive of . That is, would be the set of all real numbers between 5 and 12, including 5 but not 12. The numbers may come as close as they like to 12, including 11.999 and so forth (with any finite number of 9s), but 12.0 is not included. In some European countries, the notation is also used for this. The endpoint adjoining the bracket is known as \"closed\", whereas the endpoint adjoining the parenthesis is known as \"open\". If both types of brackets are the same, the entire interval may be referred to as \"closed\" or \"open\" as appropriate. Whenever +∞ or −∞ is used as an endpoint, it is normally considered \"open\" and adjoined to a parenthesis. See Interval (mathematics) for a more complete treatment.\n\nIn quantum mechanics, chevrons are also used as part of Dirac's formalism, bra–ket notation, to note vectors from the dual spaces of the Bra . Mathematicians will also commonly write for the inner product of two vectors. In statistical mechanics, chevrons denote ensemble or time average. Chevrons are used in group theory to write group presentations, and to denote the subgroup generated by a collection of elements. Note that obtuse angled chevrons are not always (and even not by all users) distinguished from a pair of less-than and greater-than signs <>, which are sometimes used as a typographic approximation of chevrons.\n\nIn group theory and ring theory, brackets denote the commutator. In group theory, the commutator is commonly defined as . In ring theory, the commutator is defined as . Furthermore, in ring theory, braces denote the anticommutator where is defined as . The bracket is also used to denote the Lie derivative, or more generally the Lie bracket in any Lie algebra.\n\nVarious notations, like the vinculum have a similar effect to brackets in specifying order of operations, or otherwise grouping several characters together for a common purpose.\n\nIn the Z formal specification language, braces define a set and chevrons define a sequence.\n\nTraditionally in accounting, contra amounts are placed in parentheses. A debit balance account in a series of credit balances will have brackets and vice versa.\n\nBrackets are used in some countries in the citation of law reports to identify parallel citations to non-official reporters. For example: Chronicle Pub. Co. v. Superior Court, (1998) 54 Cal.2d 548, [7 Cal.Rptr. 109]. In some other countries (such as England and Wales), square brackets are used to indicate that the year is part of the citation and parentheses are used to indicate the year the judgment was given. For example, National Coal Board v England [1954] AC 403, is in the 1954 volume of the Appeal Cases reports although the decision may have been given in 1953 or earlier, whereas (1954) 98 Sol Jo 176 reports a decision from 1954, in volume 98 of the Solicitor's Journal which may be published in 1955 or later.\n\nWhen quoted material is in any way altered, the alterations are enclosed in brackets within the quotation. For example: Plaintiff asserts his cause is just, stating, \"[m]y causes is just.\" Although in the original quoted sentence the word \"my\" was capitalized, it has been modified in the quotation and the change signalled with brackets. Similarly, where the quotation contained a grammatical error, the quoting author signalled that the error was in the original with \"[\"sic\"]\" (Latin for 'thus'). (\"California Style Manual\", section 4:59 (4th ed.))\n\nTournament brackets, the diagrammatic representation of the series of games played during a tournament usually leading to a single winner, are so named for their resemblance to brackets or braces.\n\nRepresentations of various kinds of brackets in Unicode and HTML are given below.\n\nBraces (curly brackets) first became part of a character set with the 8-bit code of the IBM 7030 Stretch.\n\nThe angle brackets or chevrons at U+27E8 and U+27E9 are for mathematical use and Western languages, whereas U+3008 and U+3009 are for East Asian languages. The chevrons at U+2329 and U+232A are deprecated in favour of the U+3008 and U+3009 East Asian angle brackets. Unicode discourages their use for mathematics and in Western texts, because they are canonically equivalent to the CJK code points U+300x and thus likely to render as double-width symbols. The \"less-than\" and \"greater-than\" symbols are often used as replacements for chevrons.\n\n\n\n"}
{"id": "31211209", "url": "https://en.wikipedia.org/wiki?curid=31211209", "title": "Breath gas analysis", "text": "Breath gas analysis\n\nBreath gas analysis is a method for gaining non-invasive information on the clinical state of an individual by monitoring volatile organic compounds present in the exhaled breath. Breath gas concentration can then be related to blood concentrations via mathematical modeling as for example in blood alcohol testing.\n\nThe area of modern breath testing commenced in 1971, when Nobel Prize winner Linus Pauling demonstrated that human breath is a complex gas, containing more than 200 different volatile organic compounds. However, physicians have used breath analysis since the days of Hippocrates.\n\nEndogenous volatile organic compounds (VOCs) are released within the human organism as a result of normal metabolic activity or due to pathological disorders. They enter the blood stream and are eventually metabolized or excreted via exhalation, skin emission, urine, etc.\n\nBreath sampling is non-invasive and breath samples can be extracted as often as desired.\n\nIdentification and quantification of potential disease biomarkers can be seen as the driving force for the analysis of exhaled breath. Moreover, future applications for medical diagnosis and therapy control with dynamic assessments of normal physiological function or pharmacodynamics are intended.\n\nExogenous VOCs penetrating the body as a result of environmental exposure can be used to quantify body burden. Also breath tests are often based on the ingestion of isotopically labeled precursors, producing isotopically labeled carbon dioxide and potentially many other metabolites.\n\nHowever, breath sampling is far from being a standardized procedure due to the numerous confounding factors biasing the concentrations of volatiles in breath. These factors are related to both the breath sampling protocols as well as the complex physiological mechanisms underlying pulmonary gas exchange. Even under resting conditions exhaled breath concentrations of VOCs can strongly be influenced by specific physiological parameters such as cardiac output and breathing patterns, depending on the physico-chemical properties of the compound under study.\n\nUnderstanding the influence of all this factors and their control is necessary for achieving an accurate standardization of breath sample collection and for the correct deduction of the corresponding blood concentration levels.\n\nThe simplest model relating breath gas concentration to blood concentrations was developed by Farhi\nwhere formula_2 denotes the alveolar concentration which is assumed to be equal to the measured concentration.\nIt expresses the fact that the concentration of an inert gas in the alveolar air depends on the mixed venous concentration formula_3, the substance-specific blood:air partition coefficient formula_4, and the ventilation-perfusion ratio formula_5.\nBut this model fails when two prototypical substances like acetone (partition coefficient formula_6) or isoprene (partition coefficient formula_7 ) are measured.\n\nE.g., multiplying the proposed population mean of approximately formula_8 acetone in end-tidal breath by the partition coefficient formula_6 at body temperature grossly underestimates observed (arterial) blood levels spreading around formula_10. Furthermore, breath profiles of acetone (and other highly soluble volatile compounds such as 2-pentanone or methyl acetate) associated with moderate workload ergometer challenges of normal healthy volunteers drastically depart from the trend suggested by the equation above.\n\nHence some more refined models are necessary. Such models have been developed recently.\n\nBreath gas analysis is used in a number of breath tests.\n\n\nBreath can be collected using a variety of home-made and commercially available devices. The three basic types of breath collector for VOC analysis are:\n\n\nEach of these can be used as a vehicle for direct introduction of a gas sample into an appropriate analytical instrument, or serve as a reservoir of breath gas into which an absorption device such as an SPME fiber is placed to collect specific compounds.\n\nBreath analysis can be done with various forms of mass spectrometry, but there are also simpler methods for specific purposes, such as the Halimeter and the breathalyzer.\n\n\n"}
{"id": "42063759", "url": "https://en.wikipedia.org/wiki?curid=42063759", "title": "Broer–Kaup equations", "text": "Broer–Kaup equations\n\nThe Broer–Kaup equations are a set of two coupled nonlinear partial differential equations\n\n"}
{"id": "35321461", "url": "https://en.wikipedia.org/wiki?curid=35321461", "title": "Canonical cover", "text": "Canonical cover\n\nA canonical cover formula_1 for F (a set of functional dependencies on a relation scheme) is a set of dependencies such that F logically implies all dependencies in formula_1, and formula_1 logically implies all dependencies in F.\n\nThe set formula_1 has two important properties:\n\nA canonical cover is not unique for a given set of functional dependencies, therefore one set F can have multiple covers formula_1.\n\n"}
{"id": "780886", "url": "https://en.wikipedia.org/wiki?curid=780886", "title": "Characterization (mathematics)", "text": "Characterization (mathematics)\n\nIn mathematics, the statement that \"Property \"P\" characterizes object \"X\"\" means that not only does \"X\" have property \"P\", but that \"X\" is the \"only\" thing that has property \"P\". In other words, \"P\" is a defining property of \"X\". It is also common to find statements such as \"Property \"Q\" characterises \"Y\" up to isomorphism\". The first type of statement says in different words that the extension of \"P\" is a singleton set. The second says that the extension of \"Q\" is a single equivalence class (for isomorphism, in the given example — depending on how \"up to\" is being used, some other equivalence relation might be involved).\n\n\n"}
{"id": "1081339", "url": "https://en.wikipedia.org/wiki?curid=1081339", "title": "Chisini mean", "text": "Chisini mean\n\nIn mathematics, a function \"f\" of \"n\" variables \n\nleads to a Chisini mean \"M\" if for every vector <x ... x>, there exists a unique \"M\" such that \n\nThe arithmetic, harmonic, geometric, generalised, Heronian and quadratic means are all Chisini means, as are their weighted variants.\n\nThey were introduced by Oscar Chisini in 1929.\n\n\n"}
{"id": "1979078", "url": "https://en.wikipedia.org/wiki?curid=1979078", "title": "Color model", "text": "Color model\n\nA color model is an abstract mathematical model describing the way colors can be represented as tuples of numbers, typically as three or four values or color components. When this model is associated with a precise description of how the components are to be interpreted (viewing conditions, etc.), the resulting set of colors is called \"color space.\" This section describes ways in which human color vision can be modeled.\n\nOne can picture this space as a region in three-dimensional Euclidean space if one identifies the \"x\", \"y\", and \"z\" axes with the stimuli for the long-wavelength (\"L\"), medium-wavelength (\"M\"), and short-wavelength (\"S\") light receptors. The origin, (\"S\",\"M\",\"L\") = (0,0,0), corresponds to black. White has no definite position in this diagram; rather it is defined according to the color temperature or white balance as desired or as available from ambient lighting. The human color space is a horse-shoe-shaped cone such as shown here (see also CIE chromaticity diagram below), extending from the origin to, in principle, infinity. In practice, the human color receptors will be saturated or even be damaged at extremely high light intensities, but such behavior is not part of the CIE color space and neither is the changing color perception at low light levels (see: Kruithof curve). \nThe most saturated colors are located at the outer rim of the region, with brighter colors farther removed from the origin. As far as the responses of the receptors in the eye are concerned, there is no such thing as \"brown\" or \"gray\" light. The latter color names refer to orange and white light respectively, with an intensity that is lower than the light from surrounding areas. One can observe this by watching the screen of an overhead projector during a meeting: one sees black lettering on a white background, even though the \"black\" has in fact not become darker than the white screen on which it is projected before the projector was turned on. The \"black\" areas have not actually become darker but appear \"black\" relative to the higher intensity \"white\" projected onto the screen around it. See also color constancy.\n\nThe human tristimulus space has the property that additive mixing of colors corresponds to the adding of vectors in this space. This makes it easy to, for example, describe the possible colors (gamut) that can be constructed from the red, green, and blue primaries in a computer display.\n\nOne of the first mathematically defined color spaces is the CIE XYZ color space (also known as CIE 1931 color space), created by the International Commission on Illumination in 1931. These data were measured for human observers and a 2-degree field of view. In 1964, supplemental data for a 10-degree field of view were published.\n\nNote that the tabulated sensitivity curves have a certain amount of arbitrariness in them. The shapes of the individual X, Y and Z sensitivity curves can be measured with a reasonable accuracy. However, the overall luminosity function (which in fact is a weighted sum of these three curves) is subjective, since it involves asking a test person whether two light sources have the same brightness, even if they are in completely different colors. Along the same lines, the relative magnitudes of the X, Y, and Z curves are arbitrarily chosen to produce equal areas under the curves. One could as well define a valid color space with an X sensitivity curve that has twice the amplitude. This new color space would have a different shape. The sensitivity curves in the CIE 1931 and 1964 xyz color space are scaled to have equal areas under the curves.\n\nSometimes XYZ colors are represented by the luminance, Y, and chromaticity coordinates \"x\" and \"y\", defined by:\n\nMathematically, \"x\" and \"y\" are projective coordinates and the colors of the chromaticity diagram occupy a region of the real projective plane. Because the CIE sensitivity curves have equal areas under the curves, light with a flat energy spectrum corresponds to the point (\"x\",\"y\") = (0.333,0.333).\n\nThe values for \"X\", \"Y\", and \"Z\" are obtained by integrating the product of the spectrum of a light beam and the published color-matching functions.\n\nMedia that transmit light (such as television) use additive color mixing with primary colors of red, green, and blue, each of which stimulates one of the three types of the eye's color receptors with as little stimulation as possible of the other two. This is called \"RGB\" color space. Mixtures of light of these primary colors cover a large part of the human color space and thus produce a large part of human color experiences. This is why color television sets or color computer monitors need only produce mixtures of red, green and blue light. See Additive color.\n\nOther primary colors could in principle be used, but with red, green and blue the largest portion of the human color space can be captured. Unfortunately there is no exact consensus as to what loci in the chromaticity diagram the red, green, and blue colors should have, so the same RGB values can give rise to slightly different colors on different screens.\n\nIt is possible to achieve a large range of colors seen by humans by combining cyan, magenta, and yellow transparent dyes/inks on a white substrate. These are the \"subtractive\" primary colors. Often a fourth ink, black, is added to improve reproduction of some dark colors. This is called the \"CMY\" or \"CMYK\" color space.\n\nThe cyan ink absorbs red light but transmits green and blue, the magenta ink absorbs green light but transmits red and blue, and the yellow ink absorbs blue light but transmits red and green. The white substrate reflects the transmitted light back to the viewer. Because in practice the CMY inks suitable for printing also reflect a little bit of color, making a deep and neutral black impossible, the K (black ink) component, usually printed last, is needed to compensate for their deficiencies. Use of a separate black ink is also economically driven when a lot of black content is expected, e.g. in text media, to reduce simultaneous use of the three colored inks. The dyes used in traditional color photographic prints and slides are much more perfectly transparent, so a K component is normally not needed or used in those media.\nA number of color models exist in which colors are fit into conic, cylindrical or spherical shapes, with neutrals running from black to white in a central axis, and hues corresponding to angles around that axis. Arrangements of this type date back to the 18th century, and continue to be developed in the most modern and scientific models.\n\nDifferent color theorists have each designed unique color solids. Many are in the shape of a sphere, whereas others are warped three-dimensional ellipsoid figures—these variations being designed to express some aspect of the relationship of the colors more clearly. The color spheres conceived by Phillip Otto Runge and Johannes Itten are typical examples and prototypes for many other color solid schematics. The models of Runge and Itten are basically identical, and form the basis for the description below.\n\nPure, saturated hues of equal brightness are located around the equator at the periphery of the color sphere. As in the color wheel, contrasting (or complementary) hues are located opposite each other. Moving toward the center of the color sphere on the equatorial plane, colors become less and less saturated, until all colors meet at the central axis as a neutral gray. Moving vertically in the color sphere, colors become lighter (toward the top) and darker (toward the bottom). At the upper pole, all hues meet in white; at the bottom pole, all hues meet in black. \n\nThe vertical axis of the color sphere, then, is gray all along its length, varying from black at the bottom to white at the top. All pure (saturated) hues are located on the surface of the sphere, varying from light to dark down the color sphere. All impure (unsaturated hues, created by mixing contrasting colors) comprise the sphere's interior, likewise varying in brightness from top to bottom.\n\nHSL and HSV are both cylindrical geometries, with hue, their angular dimension, starting at the red primary at 0°, passing through the green primary at 120° and the blue primary at 240°, and then wrapping back to red at 360°. In each geometry, the central vertical axis comprises the \"neutral\", \"achromatic\", or \"gray\" colors, ranging from black at lightness 0 or value 0, the bottom, to white at lightness 1 or value 1, the top.\n\nMost televisions, computer displays, and projectors produce colors by combining red, green, and blue light in varying intensities—the so-called RGB additive primary colors. However, the relationship between the constituent amounts of red, green, and blue light and the resulting color is unintuitive, especially for inexperienced users, and for users familiar with subtractive color mixing of paints or traditional artists’ models based on tints and shades.\n\nIn an attempt to accommodate more traditional and intuitive color mixing models, computer graphics pioneers at PARC and NYIT developed the HSV model in the mid-1970s, formally described by Alvy Ray Smith in the August 1978 issue of \"Computer Graphics\". In the same issue, Joblove and Greenberg described the HSL model—whose dimensions they labeled \"hue\", \"relative chroma\", and \"intensity\"—and compared it to HSV. Their model was based more upon how colors are organized and conceptualized in human vision in terms of other color-making attributes, such as hue, lightness, and chroma; as well as upon traditional color mixing methods—e.g., in painting—that involve mixing brightly colored pigments with black or white to achieve lighter, darker, or less colorful colors.\n\nThe following year, 1979, at SIGGRAPH, Tektronix introduced graphics terminals using HSL for color designation, and the Computer Graphics Standards Committee recommended it in their annual status report. These models were useful not only because they were more intuitive than raw RGB values, but also because the conversions to and from RGB were extremely fast to compute: they could run in real time on the hardware of the 1970s. Consequently, these models and similar ones have become ubiquitous throughout image editing and graphics software since then.\n\nAnother influential older cylindrical color model is the early-20th-century Munsell color system. Albert Munsell began with a spherical arrangement in his 1905 book \"A Color Notation\", but he wished to properly separate color-making attributes into separate dimensions, which he called \"hue\", \"value\", and \"chroma\", and after taking careful measurements of perceptual responses, he realized that no symmetrical shape would do, so he reorganized his system into a lumpy blob.\n\nMunsell’s system became extremely popular, the de facto reference for American color standards—used not only for specifying the color of paints and crayons, but also, e.g., electrical wire, beer, and soil color—because it was organized based on perceptual measurements, specified colors via an easily learned and systematic triple of numbers, because the color chips sold in the \"Munsell Book of Color\" covered a wide gamut and remained stable over time (rather than fading), and because it was effectively marketed by Munsell’s Company. In the 1940s, the Optical Society of America made extensive measurements, and adjusted the arrangement of Munsell colors, issuing a set of \"renotations\". The trouble with the Munsell system for computer graphics applications is that its colors are not specified via any set of simple equations, but only via its foundational measurements: effectively a lookup table. Converting from requires interpolating between that table’s entries, and is extremely computationally expensive in comparison with converting from or which only requires a few simple arithmetic operations.\n\nIn densitometry, a model quite similar to the hue defined above is used for describing colors of CMYK process inks. In 1953, Frank Preucil developed two geometric arrangements of hue, the \"Preucil hue circle\" and the \"Preucil hue hexagon\", analogous to our \"H\" and \"H\", respectively, but defined relative to idealized cyan, yellow, and magenta ink colors. The \"Preucil \"hue error\"\" of an ink indicates the difference in the \"hue circle\" between its color and the hue of the corresponding idealized ink color. The \"grayness\" of an ink is , where \"m\" and \"M\" are the minimum and maximum among the amounts of idealized cyan, magenta, and yellow in a density measurement.\n\nThe Swedish Natural Color System (NCS), widely used in Europe, takes a similar approach to the Ostwald bicone at right. Because it attempts to fit color into a familiarly shaped solid based on \"phenomenological\" instead of photometric or psychological characteristics, it suffers from some of the same disadvantages as HSL and HSV: in particular, its lightness dimension differs from perceived lightness, because it forces colorful yellow, red, green, and blue into a plane.\n\nThe International Commission on Illumination (CIE) developed the XYZ model for describing the colors of light spectra in 1931, but its goal was to match human visual metamerism, rather than to be perceptually uniform, geometrically. In the 1960s and 70s, attempts were made to transform XYZ colors into a more relevant geometry, influenced by the Munsell system. These efforts culminated in the 1976 CIELUV and CIELAB models. The dimensions of these models— and , respectively—are cartesian, based on the opponent process theory of color, but both are also often described using polar coordinates— and , respectively—where \"L\"* is lightness, \"C\"* is chroma, and \"h\"* is hue angle. Officially, both CIELAB and CIELUV were created for their color difference metrics ∆\"E\"* and ∆\"E\"*, particularly for use defining color tolerances, but both have become widely used as color order systems and color appearance models, including in computer graphics and computer vision. For example, gamut mapping in ICC color management is usually performed in CIELAB space, and Adobe Photoshop includes a CIELAB mode for editing images. CIELAB and CIELUV geometries are much more perceptually relevant than many others such as RGB, HSL, HSV, YUV/YIQ/YCbCr or XYZ, but are not perceptually perfect, and in particular have trouble adapting to unusual lighting conditions.\n\nThe HCL color space seems to be synonymous with CIELCH.\n\nThe CIE’s most recent model, CIECAM02 (CAM stands for \"color appearance model\"), is more theoretically sophisticated and computationally complex than earlier models. Its aims are to fix several of the problems with models such as CIELAB and CIELUV, and to explain not only responses in carefully controlled experimental environments, but also to model the color appearance of real-world scenes. Its dimensions \"J\" (lightness), \"C\" (chroma), and \"h\" (hue) define a polar-coordinate geometry.\n\nThere are various types of color systems that classify color and analyse their effects. The American Munsell color system devised by Albert H. Munsell is a famous classification that organises various colors into a color solid based on hue, saturation and value. Other important color systems include the Swedish Natural Color System (NCS), the Optical Society of America's Uniform Color Space (OSA-UCS), and the Hungarian Coloroid system developed by Antal Nemcsics from the Budapest University of Technology and Economics. Of those, the NCS is based on the opponent-process color model, while the Munsell, the OSA-UCS and the Coloroid attempt to model color uniformity. The American Pantone and the German RAL commercial color-matching systems differ from the previous ones in that their color spaces are not based on an underlying color model.\n\nWe also use \"color model\" to indicate a model or mechanism of color vision for explaining how color signals are processed from visual cones to ganglion cells. For simplicity, we call these models color mechanism models. The classical color mechanism models are Young–Helmholtz's trichromatic model and Hering's opponent-process model. Though these two theories were initially thought to be at odds, it later came to be understood that the mechanisms responsible for color opponency receive signals from the three types of cones and process them at a more complex level.\n\nVertebrate animals were primitively tetrachromatic. They possessed four types of cones—long, mid, short wavelength cones, and ultraviolet sensitive cones. Today, fish, amphibians, reptiles and birds are all tetrachromatic. Placental mammals lost both the mid and short wavelength cones. Thus, most mammals do not have complex color vision—they are dichromatic but they are sensitive to ultraviolet light, though they cannot see its colors. Human trichromatic color vision is a recent evolutionary novelty that first evolved in the common ancestor of the Old World Primates. Our trichromatic color vision evolved by duplication of the long wavelength sensitive opsin, found on the X chromosome. One of these copies evolved to be sensitive to green light and constitutes our mid wavelength opsin. At the same time, our short wavelength opsin evolved from the ultraviolet opsin of our vertebrate and mammalian ancestors.\n\nHuman red-green color blindness occurs because the two copies of the red and green opsin genes remain in close proximity on the X chromosome. Because of frequent recombination during meiosis, these gene pairs can get easily rearranged, creating versions of the genes that do not have distinct spectral sensitivities.\n\n\n\n"}
{"id": "41007672", "url": "https://en.wikipedia.org/wiki?curid=41007672", "title": "Cours d'Analyse", "text": "Cours d'Analyse\n\nCours d'Analyse de l’École Royale Polytechnique; I.re Partie. Analyse algébrique is a seminal textbook in infinitesimal calculus published by Augustin-Louis Cauchy in 1821. The article follows the translation by Bradley and Sandifer in describing its contents.\n\nOn page 1 of the Introduction, Cauchy writes: \"In speaking of the continuity of functions, I could not dispense with a treatment of the principal properties of infinitely small quantities, properties which serve as the foundation of the infinitesimal calculus.\" The translators comment in a footnote: \"It is interesting that Cauchy does not also mention limits here.\"\n\nCauchy continues: \"As for the methods, I have sought to give them all the rigor which one demands from geometry, so that one need never rely on arguments drawn from the generality of algebra.\"\n\nOn page 6, Cauchy first discusses variable quantities and then introduces the limit notion in the following terms: \"When the values successively attributed to a particular variable indefinitely approach a fixed value in such a way as to end up by differing from it by as little as we wish, this fixed value is called the \"limit\" of all the other values.\"\n\nOn page 7, Cauchy defines an infinitesimal as follows: \"When the successive numerical values of such a variable decrease indefinitely, in such a way as to fall below any given number, this variable becomes what we call \"infinitesimal\", or an \"infinitely small quantity\".\" Cauchy adds: \"A variable of this kind has zero as its limit.\"\n\nOn page 10, Bradley and Sandifer confuse the versed cosine with the coversed sine. Cauchy originally defined the \"sinus versus\" (versine) as siv(\"θ\") = 1-cos(\"θ\") and the \"cosinus versus\" (what is now also known as coversine) as cosiv(\"θ\") = 1-sin(\"θ\"). In the translation, however, the \"cosinus versus\" (and cosiv) are incorrectly associated with the \"versed cosine\" (what is now also known as vercosine) rather than the \"coversed sine\".\n\nThe notation\n\nis introduced on page 12. The translators observe in a footnote: \"The notation “Lim.” for limit was first used by Simon Antoine Jean L'Huilier (1750–1840) in [L’Huilier 1787, p. 31]. Cauchy wrote this as “lim.” in [Cauchy 1821, p. 13]. The period had disappeared by [Cauchy 1897, p. 26].\"\n\nThis chapter has the long title \"On infinitely small and infinitely large quantities, and on the continuity of functions. Singular values of functions in various particular cases.\" On page 21, Cauchy writes: \"We say that a variable quantity becomes \"infinitely small\" when its numerical value decreases indefinitely in such a way as to converge towards the limit zero.\" On the same page, we find the only explicit example of such a variable to be found in Cauchy, namely \nOn page 22, Cauchy starts the discussion of orders of magnitude of infinitesimals as follows: \"Let formula_2 be an infinitely small quantity, that is a variable whose numerical value decreases indefinitely. When the various integer powers of formula_2, namely \nenter into the same calculation, these various powers are called, respectively, infinitely small of the \"first\", the \"second\", the \"third order\", etc. Cauchy notes that \"the general form of infinitely small quantities of order \"n\" (where \"n\" represents an integer number) will be\n\nOn pages 23-25, Cauchy presents eight theorems on properties of infinitesimals of various orders.\n\nThis is entitled \"Continuity of functions\". Cauchy writes: \"If, beginning with a value of \"x\" contained between these limits, we add to the variable \"x\" an\ninfinitely small increment formula_2, the function itself is incremented by the difference \nand states that \nCauchy goes on to provide an italicized definition of continuity in the following terms: \n\nOn page 32 Cauchy states the intermediate value theorem.\n\nIn Theorem I in section 6.1 (page 90 in the translation by Bradley and Sandifer), Cauchy presents the sum theorem in the following terms.\n\n\"When the various terms of series (1) are functions of the same variable x, continuous with respect to this variable in the neighborhood of a particular value for which the series converges, the sum s of the series is also a continuous function of x in the neighborhood of this particular value.\"\n\nHere the series (1) appears on page 86: (1) formula_11\n\n"}
{"id": "54727095", "url": "https://en.wikipedia.org/wiki?curid=54727095", "title": "Cumulative accuracy profile", "text": "Cumulative accuracy profile\n\nThe cumulative accuracy profile (CAP) is used in data science to visualize the discriminative power of a model. The CAP of a model represents the cumulative number of positive outcomes along the \"y\"-axis versus the corresponding cumulative number of a classifying parameter along the \"x\"-axis. The CAP is distinct from the receiver operating characteristic (ROC), which plots the true-positive rate against the false-positive rate.\n\nAn example is a model that predicts whether a product is bought (positive outcome) by each individual from a group of people (classifying parameter) based on factors such as their gender, age, income etc. If group members would be contacted at random, the cumulative number of products sold would rise linearly toward a maximum value corresponding to the total number of buyers within the group. This distribution is called the \"random\" CAP. A perfect prediction, on the other hand, determines exactly which group members will buy the product, such that the maximum number of products sold will be reached with a minimum number of calls. This produces a steep line on the CAP curve that stays flat once the maximum is reached (contacting all other group members will not lead to more products sold), which is the \"perfect\" CAP.\n\nA successful model predicts the likelihood of individuals purchasing the product and ranks these probabilities to produce a list of potential customers to be contacted first. The resulting cumulative number of sold products will increase rapidly and eventually flatten out to the given maximum as more group members are contacted. This results in a distribution that lies between the random and the perfect CAP curves.\n\nThe CAP can be used to evaluate a model by comparing the curve to the perfect CAP in which the maximum number of positive outcomes is achieved directly and to the random CAP in which the positive outcomes are distributed equally. A good model will have a CAP between the perfect CAP and the random CAP with a better model tending to the perfect CAP.\n\nThe accuracy ratio (AR) is defined as the ratio of the area between the model CAP and the random CAP and the area between the perfect CAP and the random CAP. For a successful model the AR has values between zero and one, with a higher value for a stronger model.\n\nAnother indication of the model strength is given by the cumulative number of positive outcomes at 50% of the classifying parameter. For a successful model this value should lie between 50% and 100% of the maximum, with a higher percentage for stronger models.\n\nThe CAP and the ROC are both commonly used by banks and regulators to analyze the discriminatory ability of rating systems that evaluate the credit risks \n"}
{"id": "20565885", "url": "https://en.wikipedia.org/wiki?curid=20565885", "title": "Early numeracy", "text": "Early numeracy\n\nEarly numeracy is a branch of numeracy that aims to enhance numeracy learning for younger learners, particularly those at-risk in the area of mathematics. Usually the mathematical learning begins with simply learning the digits, being 1-10. This is done because it acts as an entry way to the expansion of counting. One can keep track of the digits using any of the phalanges\n\n"}
{"id": "3003070", "url": "https://en.wikipedia.org/wiki?curid=3003070", "title": "Empirical modelling", "text": "Empirical modelling\n\nEmpirical modelling refers to any kind of (computer) modelling based on empirical observations rather than on mathematically describable relationships of the system modelled.\n\nEmpirical Modelling as a variety of empirical modelling\n\nEmpirical modelling is a generic term for activities that create models by observation and experiment. Empirical Modelling (with the initial letters capitalised, and often abbreviated to EM) refers to a specific variety of empirical modelling in which models are constructed following particular principles. Though the extent to which these principles can be applied to model-building without computers is an interesting issue (to be revisited below), there are at least two good reasons to consider Empirical Modelling in the first instance as computer-based. Without doubt, new technologies that are based on computers have had a transformative impact where the full exploitation of Empirical Modelling principles is concerned. What is more, the conception of Empirical Modelling has been closely associated with thinking about the role of the computer in model-building.\n\nAn empirical model operates on a simple semantic principle: the maker observes a close correspondence between the behaviour of the model and that of its referent. The crafting of this correspondence can be 'empirical' in a wide variety of senses: it may entail a trial-and-error process, may be based on computational approximation to analytic formulae, it may be derived as a black-box relation that affords no insight into 'why it works'.\n\nEmpirical Modelling is rooted on the key principle of William James's 'radical empiricism', which postulates that all knowing is rooted in connections that are given-in-experience. Empirical Modelling aspires to craft the correspondence between the model and its referent in such a way that its derivation can be traced to connections given-in-experience. Making connections in experience is an essentially individual human activity that requires skill and is highly context-dependent. Examples of such connections include: identifying familiar objects in the stream of thought, associating natural languages words with objects to which they refer, and subliminally interpreting the rows and columns of a spreadsheet as exam results of particular students in particular subjects.\n\nEmpirical Modelling principles\n\nIn Empirical Modelling, the process of construction is an incremental one in which the intermediate products are artefacts that evoke aspects of the intended (and sometimes emerging) referent through live interaction and observation. The connections evoked in this way have distinctive qualities: they are of their essence personal and experiential in character and are provisional in so far as they may be undermined, refined and reinforced as the model builder's experience and understanding of the referent develops. Following a precedent established by David Gooding in his account of the role that artefacts played in Michael Faraday's experimental investigation of electromagnetism, the intermediate products of the Empirical Modelling process are described as 'construals'. Gooding's account is a powerful illustration of how \"making construals\" can support the sense-making activities that lead to conceptual insights (cf. the contribution that Faraday's work made to electromagnetic theory) and to practical products (cf. Faraday's invention of the electric motor).\n\nThe activities associated with making a construal in the Empirical Modelling framework are depicted in Figure 1.\n\nThe eye icon at the centre the figure represents the maker's observation of the current state of development of the construal and its referent. The two arrows emanating from the eye represent the connection given-in-experience between the construal and its referent that is established in the mind of the maker. This connection is crafted through experimental interaction with the construal under construction and its emerging referent. As in genuine experiment, the scope of the interactions that can be entertained by the maker is inconceivably broad. At the maker's discretion, the interactions that characterise the construal are those that respect the connection given in the maker's experience. As the Empirical Modelling process unfolds, the construal, the referent, the maker's understanding and the context for the maker's engagement co-evolve in such a way that:\nEmpirical Modelling concepts\n\nIn Empirical Modelling. making and maintaining the connection given-in-experience between the construal and referent is based on three primary concepts: \"observables\", \"dependencies\" and \"agency\". Within both the construal and its referent, the maker identifies observables as entities that can take on a range of different values, and whose current values determine its current state. All state-changing interactions with the construal and referent are conceived as changes to the values of observables. A change to the value of one observable may be directly attributable to a change in the value of another observable, in which case these values are linked by a dependency. Changes to observable values are attributed to agents, amongst which the most important is the maker of the construal. When changes to observable values are observed to occur simultaneously, this can be construed as concurrent action on the part of different agents, or as concomitant changes to observables derived from a single agent action via dependencies. To craft the connection given-in-experience between the construal and referent, the maker constructs the construal in such a way that its observables, dependencies and agency correspond closely to those that are observed in the referent. To this end, the maker must conceive appropriate ways in which observables and agent actions in the referent can be given suitable experiential counterparts in the construal.\n\nThe semantic framework shown in Figure 1 resembles that adopted in working with spreadsheets, where the state that is currently displayed in the grid is meaningful only when experienced in conjunction with an external referent. In this setting, the cells serve as observables, their definitions specify the dependencies, and agency is enacted by changing the values or the definitions of cells. In making a construal, the maker explores the roles of each relevant agent by projecting agency upon it as if it were a human agent and identifying observables and dependencies from that perspective. By automating agency, construals can then be used to specify behaviours in much the same way that behaviours can be expressed using macros in conjunction with spreadsheets. In this way, animated construals can emulate program-like behaviours in which the intermediate states are meaningful and live to auditing by the maker.\n\nEnvironments to support Empirical Modelling\n\nThe development of computer environments for making construals has been an ongoing subject of research over the last thirty years. The many variants of such environments that have been implemented are based on common principles. The network of dependencies that currently connect observables is recorded as a family of definitions. Semantically such definitions resemble the definitions of spreadsheet cells, whereby changes to the values of observables on the right hand side propagate so as to change the value of the observable on the LHS in a conceptually indivisble manner. The dependencies in these networks are acyclic but are also reconfigurable: redefining an observable may introduce a new definition that alters the dependency structure. Observables built into the environment include scalars, geometric and screen display elements: these can be elaborated using multi-level list structures. A dependency is typically represented by a definition which uses a relatively simple functional expression to relate the value of an observable to the values of other observables. Such functions have typically been expressed in fragments of simple procedural code, but the most recent variants of environments of making construals also enable dependency relations to be expressed by suitably contextualised families of definitions. The maker can interact with a construal through redefining existing observables or introducing new observables in an open-ended unconstrained manner. Such interaction has a crucial role in the experimental activity that informs the incremental development of the construal. Triggered actions can be introduced to automate state-change: these perform redefinitions in response to specified changes in the values of observables.\n\nEmpirical Modelling as a broader view of computing\n\nIn Figure 1, identifying 'the computer' as the medium in which the construal is created is potentially misleading. The term COMPUTER is not merely a reference to a powerful computational device. In making construals, the primary emphasis is on the rich potential scope for interaction and perceptualisation that the computer enables when used in conjunction with other technologies and devices. The primary motivation for developing Empirical Modelling is to give a satisfactory account of computing that integrates these two complementary roles of the computer. The principles by which James and Dewey sought to reconcile perspectives on agency informed by logic and experience play a crucial role in achieving this integration.\n\nThe dual role for the computer implicit in Figure 1 is widely relevant to contemporary computing applications. On this basis, Empirical Modelling can be viewed as providing a foundation for a broader view of computing. This perspective is reflected in numerous Empirical Modelling publications on topics such as educational technology, computer-aided design and software development. Making construals has also been proposed as a suitable technique to support constructionism, as conceived by Seymour Papert, and to meet the guarantees for 'construction' as identified by Bruno Latour.\n\nEmpirical Modelling as generic sense-making?\n\nThe Turing machine provides the theoretical foundation for the role of the computer as a computational device: it can be regarded as modelling 'a mind following rules'. The practical applications of Empirical Modelling to date suggest that making construals is well-suited to supporting the supplementary role the computer can play in orchestrating rich experience. In particular, in keeping with the pragmatic philosophical stance of James and Dewey, making construals can fulfill an explanatory role by offering contingent explanations for human experience in contexts where computational rules cannot be invoked. In this respect, making construals may be regarded as modelling 'a mind making sense of a situation'.\n\nIn the same way that the Turing machine is a conceptual tool for understanding the nature of algorithms whose value is independent of the existence of the computer, Empirical Modelling principles and concepts may have generic relevance as a framework for thinking about sense-making without specific reference to the use of a computer. The contribution that William James's analysis of human experience makes to the concept of Empirical Modelling may be seen as evidence for this. By this token, Empirical Modelling principles may be an appropriate way to analyse varieties of empirical modelling that are not computer-based. For instance, it is plausible that the analysis in terms of observables, dependencies and agency that applies to interaction with electronic spreadsheets would also be appropriate for the manual spreadsheets that predated them.\n\nBackground\n\nEmpirical Modelling has been pioneered since the early 1980s by Meurig Beynon and the Empirical Modelling Research Group in Computer Science at the University of Warwick.\n\nThe term 'Empirical Modelling' (EM) has been adopted for this work since about 1995 to reflect the experiential basis of the modelling process in observation and experiment. Special purpose software supporting the central concepts of observable, dependency and agency has been under continuous development (mainly led by research students) since the late 1980s.\n\nThe principles and tools of EM have been used and developed by many hundreds of students within coursework, project work, and research theses. The undergraduate and MSc module 'Introduction to Empirical Modelling' was taught for many years up to 2013-14 until the retirement of Meurig Beynon and Steve Russ (authors of this article). There is a large website [1] containing research and teaching material with an extensive collection of refereed publications and conference proceedings.\n\nThe term 'construal' has been used since the early 2000s for the artefacts, or models, made with EM tools. The term has been adapted from its use by David Gooding in the book 'Experiment and the Making of Meaning' (1990) to describe the emerging, provisional ideas that formed in Faraday's mind, and were recorded in his notebooks, as he investigated electromagnetism, and made the first electric motors, in the 1800s.\n\nThe main practical activity associated with EM - that of 'making construals' - was the subject of an Erasmus+ Project CONSTRUIT! (2014-2017)[2].\n\n[1] http://www.dcs.warwick.ac.uk/modelling/ Empirical Modelling Research Group\n\n[2] https://warwick.ac.uk/fac/sci/dcs/research/em/welcome/ CONSTRUIT! Project web pages\n"}
{"id": "52722478", "url": "https://en.wikipedia.org/wiki?curid=52722478", "title": "Fractal expressionism", "text": "Fractal expressionism\n\nThe term fractal expressionism was coined by physicist-artist Richard Taylor and co-authors to distinguish fractal art generated directly by artists from fractal art generated using mathematics and/or computers. Fractals are patterns that repeat at increasingly fine scales and are prevalent in natural scenery (examples include clouds, rivers, and mountains). Fractal expressionism implies a direct expression of nature's patterns in an art work.\n\nThe initial studies of fractal expressionism focused on the poured paintings by the American artist Jackson Pollock (1912-1956), whose work has traditionally been associated with the abstract expressionist movement. Pollock’s patterns had previously been referred to as “natural” and “organic”, inviting speculation by author John Briggs in 1992 that Pollock's work featured fractals. In 1997, Taylor built a pendulum device called the Pollockizer which painted fractal patterns bearing a similarity to Pollock’s work. Computer analysis of Pollock's work published by Taylor et al. in a 1999 Nature article found that Pollock's painted patterns have characteristics that match those displayed by nature's fractals (specifically, they demonstrate a statistical self-similarity quantified by a non-integer dimension over a magnification range of 1.5-2 orders of magnitude). This analysis supported clues (see below) that Pollock's patterns are fractal and reflect \"the fingerprint of nature\".\n\nTaylor noted several similarities between Pollock's painting style and the processes used by nature to construct its landscapes. For instance, he cites Pollock's propensity to revisit paintings that he had not adjusted in several weeks as being comparable to cyclic processes in nature, such as the seasons or the tides. Furthermore, Taylor observed several visual similarities between the patterns produced by nature and those produced by Pollock as he painted. He points out that Pollock abandoned the use of a traditional frame for his paintings, preferring instead to roll out his canvas on the floor; this, Taylor asserts, is more compatible with how nature works than traditional painting techniques because the patterns in nature's scenery are not artificially bounded.\n\nThe perceived similarities between the processes and patterns involved in Pollock's paintings and those of nature compelled Taylor to posit that the same \"basic trademark\" of nature's pattern construction also appears in Pollock's work. Since some natural fractals are generated by a process known as \"chaos\", including fractals in human physiology, Taylor believed that Pollock's painting process might also have been chaotic, and could therefore leave behind a fractal pattern. Taylor's hypothesis seems to be reflected in Pollock's statement \"I am nature\", which he made when asked if nature was a source of inspiration for his work. Furthermore, Pollock is also quoted as stating \"No chaos, damn it\", in response to a Time magazine article that referred to his paintings as \"chaotic\". However, chaos theory was not understood until after Pollock's death, so he could not have been referring to the chaotic systems in nature but rather its common usage to mean disorder. In the famous film footage of Hans Namuth, Pollock says his paintings are no accident, and that he was able to control the flow of paint onto the canvas.\n\nTaylor points to two aspects of Pollock's painting process that have the potential to introduce fractal patterns. The first is Pollock's motion as he moved around the canvas, which Taylor hypothesized followed a Levy flight, a type of chaotic motion that is known to leave behind a fractal pattern. More specifically, a number of studies have shown that the motions associated with human balance have fractal characteristics. The second source of chaos could be introduced through Pollock's pouring technique. Falling fluid has the capability of changing from a non-chaotic to a chaotic flow, meaning that Pollock could have introduced a chaotic flow of paint as he dripped it onto the canvas. Although the fractal characteristics of human balance and falling liquid are generated on Pollock's painting time and length scales, physicist Predrag Cvitanovic notes that it would be quite an artistic challenge to control them: such parameters \"are in no sense observable and measurable on the length-scales and time-scales dominated by chaotic dynamics\".\n\nSince Taylor's initial Pollock analysis in 1999, more than ten research groups have used various forms of fractal analysis to successfully quantify Pollock’s work. In addition to analyzing Pollock's work for fractal content, some groups such as that of computer scientist Bruce Gooch, have used computers to generate Pollock-like images by varying their fractal characteristics. Mathematician Benoit Mandelbrot (who invented the term fractal) and art theorist Francis O’Connor (the chief Pollock scholar) are well known advocates of fractal expressionism.\n\nFractal expressionism is related to fractal fluency because the latter provides an appealing motivation for why artists such as Pollock might aspire to Fractal Expressionism. Fractal fluency is a neuroscience model that proposes that, through exposure to nature’s fractal scenery, people’s visual systems have adapted to efficiently process fractals with ease. This adaptation occurs at many stages of the visual system, from the way people’s eyes move to which regions of the brain get activated. Fluency puts the viewer in a ‘comfort zone’ so inducing an aesthetic experience. Neuroscience experiments have shown that Pollock’s paintings induce the same positive physiological responses in the observer as nature’s fractals and mathematical fractals.\n\nIn light of fractal fluency and the associated aesthetics, other artists might be expected to display fractal expressionism. One year before Taylor’s publication, mathematician Richard Voss quantified Chinese art using fractal analysis. Subsequently, other groups have used computer analysis to identify fractal content in a number of Western and Eastern artists, most recently in Pollock’s colleague Willem De Kooning’s work.\n\nIn addition to the above analyzed works, symbolic representations of fractals can be found in cultures across the continents spanning several centuries, including Roman, Egyptian, Aztec, Incan and Mayan civilizations. They frequently predate patterns named after the mathematicians who subsequently developed their visual characteristics. For example, although von Koch is famous for developing The Koch Curve in 1904, a similar shape featuring repeating triangles was first used to depict waves in friezes by Hellenic artists (300 B.C.E.). In the 13th century, repetition of triangles in Cosmati Mosaics generated a shape later known in mathematics as The Sierpinski Triangle (named after Sierpinski’s 1915 pattern). Triangular repetitions are also found in the 12th century pulpit of The Ravello Cathedral in Italy. The lavish artwork within The Book of Kells (circa 800 C.E.) and the sculpted arabesques in The Jain Dilwara Temple in Mount Abu, India (1031 C.E.) also both reveal stunning examples of exact fractals. The artistic works of Leonardo da Vinci and Katsushika Hokusai serve as more recent examples from Europe and Asia, each reproducing the recurring patterns that they saw in nature. Da Vinci’s sketch of turbulence in water, The Deluge (1571–1518), was composed of small swirls within larger swirls of water. In The Great Wave off Kanagawa (1830–1833), Hokusai portrayed a wave crashing on a shore with small waves on top of a large wave. Other woodcuts from the same period also feature repeating patterns at several size scales: The Ghost of Kohada Koheiji shows fissures in a skull and The Falls At Mt. Kurokami features branching channels in a waterfall.\n\nVoss's 1998 study of Chinese art was the first demonstration of using fractal analysis to distinguish between the works of different artists. Following Taylor's 1999 Pollock publication, Art conservator Jim Coddington proposed that fractal analysis should be explored as a technique to help authenticate Pollock paintings. In 2005, Taylor and colleagues published a fractal analysis of 14 authentic and 37 imitation Pollocks suggesting that, when combined with other techniques, fractal analysis might be useful for authenticating Pollock's work. In the same year, The Pollock-Krasner Foundation requested a fractal analysis to be used for the first time in an authenticity dispute, The analysis identified “significant deviations from Pollock’s characteristics.” Taylor cautioned that the results should be “coupled with other important information such as provenance, connoisseurship and materials analysis.” Two years later, materials scientists showed that pigments on the paintings dated from after Pollock’s death.\nIn 2006, the use of fractals to authenticate Pollocks stirred controversy. This controversy was triggered by physicists Katherine Jones-Smith and Harsh Mathur who claimed that the fractal characteristics identified by Taylor et al. are also present in crude sketches made in Adobe Photoshop, and deliberately fraudulent poured paintings made by other artists Thus, according to Jones-Smith and Mathur, labeling Pollock's paintings as \"fractal\" is meaningless, because the same characteristics are found in other non-fractal images. However, Taylor's rebuttal published in Nature showed that Taylor's group's fractal analysis could distinguish between Pollock paintings and the crude sketches, and identified further limitations in Jones-Smith and Mathur's analysis.\n\nJones-Smith and Mathur raised a valid concern applicable to all forms of fractal expressionism: are art works too small for the painted patterns to repeat over sufficient magnifications to assume the visual characteristics of fractals? In the case of Pollock paintings, the largest range used by Taylor et al. to determine each fractal parameter in a Pollock painting is less than two orders of magnitude in magnification. Nature's fractals repeat over limited magnification ranges (typically just over one order of magnitude), prompting scientists to debate what range is required to reliably establish fractal behavior.<ref name=\"Ave/Man\">[Avnir, David, Ofer Biham, Daniel M. Lidar, and Ofer Malcai. \"Is the Geometry of Nature Fractal?\" Science 279.5347 (1998): 39-40. Print.]</ref> Mandelbrot refused to include a required magnification range in his definition of fractals and instead noted that it is the range necessary to generate the properties associated with fractal repetition. In the case of Pollock's work, this would be the magnification range necessary for the patterns to generate the fractal aesthetics. Neuroscience experiments have shown that this magnification range is less than two orders and that Pollock’s paintings do indeed induce the same physiological responses as nature’s fractals and mathematical fractals Mandelbrot concluded \"I do believe that Pollocks are fractal.\"\n\nAt the time of the controversy, Coddington summarized as follows: “Fractal geometry has begun to play an important role in the authentication of the work of Jackson Pollock. We believe such analyses are necessary for pushing the field forward.” The most recent results, In 2015, by computer scientist Lior Shamir showed that, when combined with other pattern parameters, fractal analysis can be used to distinguish between real and imitation Pollocks with 93% accuracy. He found that the fractal parameters were the most powerful contributors to the detection accuracy\n"}
{"id": "34824761", "url": "https://en.wikipedia.org/wiki?curid=34824761", "title": "Fraïssé's theorem", "text": "Fraïssé's theorem\n\nIn mathematics, Fraïssé's theorem, named after Roland Fraïssé, states that a class \"K\" of finite relational structures is the age of a countable homogeneous relational structure if and only if it satisfies the following four conditions:\n\n\nIf these conditions hold, then the countable homogeneous structure whose age is \"K\" is unique up to isomorphism.\n\nFraïssé proved the theorem in the 1950s. \n\nFor a proof and more details see Section 1.2 and Appendix A of this thesis. \n"}
{"id": "9098286", "url": "https://en.wikipedia.org/wiki?curid=9098286", "title": "Hekat (unit)", "text": "Hekat (unit)\n\nThe hekat or heqat (transcribed \"HqA.t\") was an ancient Egyptian volume unit used to measure grain, bread, and beer. \nIt equals 4.8 litres in today's measurements.\n\nUntil the New Kingdom (NK), the hekat was one tenth of a khar, later one sixteenth; while the New Kingdom \"oipe\" (transcribed \"ip.t\") contained 4 hekat. It was sub-divided into other units – some for medical prescriptions – the \"hin\" (1/10), \"dja\" (1/64) and \"ro\" (1/320). The \"dja\" was recently evaluated by Tanja Pommerening in 2002 to 1/64 of a hekat (75 cc) in the MK, and 1/64 of an oipe (1/16 of a hekat, or 300 cc) in the NK, meaning that the \"dja\" was denoted by Horus-Eye imagery. It has been suggested by Pommerening that the NK change came about related to the oipe replacing the hekat as the Pharaonic volume control unit in official lists.\n\nHana Vymazalova evaluated the hekat unit in 2002 within the Akhmim Wooden Tablet by showing that five answers were returned to (64/64) when multiplied by the divisors 3, 7, 10, 11 and 13. The RMP also divided a hekat unity (64/64) by prime and composite numbers \"n\" when 1/64 < \"n\" < 64. The binary quotient used Eye of Horus numbers. The remainder scaled Egyptian fractions to 1/320 units named ro. Quotients and unscaled remainders were obtained for the dja, ro and other units when the divisor \"n\" was greater than 64. For example, one the 1/320 ro unit was written by Ahmes by solving 320/n ro. Gillings cites 29 examples of two-part statements converted to one-part statements in RMP 82. Ahmes recorded the \"n\" = 3 case by showing (64/64)/3 = 21/64 + 1/192 (a modern statement) as written as(16 + 4 + 1)/64 + 5/3 × 1/320 = 1/4 + 1/16 + 1/64 + 1 2/3ro (two-part ancient statement). Two-part statements were also converted by Ahmes to an unscaled hin unit by writing 3 1/3 hin.\n\nThe hekat measurement unit, and its double entry accounting system, was found beyond the Rhind Mathematical Papyrus. Another text was the Ebers Papyrus, the best known medical text. The hekat unit was defined, in terms of its volume size, in the Moscow Mathematical Papyrus by MMP #10, by approximating \"π\" to around 3.16. The approximation of \"π\" was achieved by squaring a circle, increasingly (i.e. for the denominator in terms of setats: 9, 18, 36, 72, and 81, Gillings, page 141) until the vulgar fraction 256/81 was reached, the only relationship that was used in the Egyptian Middle Kingdom. The MMP scribe found the surface area of a basket equal to: (8d/9) = 64d/81, within a cylinder relationship to the hekat. MMP 10 data meant that \"d\" = 2 defined \"π\" for use in hekat volumes as 256/81. The 256/81 approximation was also used by Ahmes and other scribes. The ancient Egyptian weights and measures discussion further shows that the hekat was 1/30 of a royal cubit, an analysis that needs to double checked, against the \"d\" = 2 suggestion, which means that \"r\" = 1, a suggestion that does make sense. One royal cubit of the Ancient Egyptian weights and measures = 523.5 millimeters. ((523.5 mm)) / 30 = 4.78221176 liters.\n\n"}
{"id": "37822732", "url": "https://en.wikipedia.org/wiki?curid=37822732", "title": "History of network traffic models", "text": "History of network traffic models\n\nDesign of robust and reliable networks and network services relies on an understanding of the traffic characteristics of the network. Throughout history, different models of network traffic have been developed and used for evaluating existing and proposed networks and services.\n\nDemands on computer networks are not entirely predictable. Performance modeling is necessary for deciding the quality of service (QoS) level. Performance models in turn, require accurate traffic models that have the ability to capture the statistical characteristics of the actual traffic on the network. Many traffic models have been developed based on traffic measurement data. If the underlying traffic models do not efficiently capture the characteristics of the actual traffic, the result may be the under-estimation or over-estimation of the performance of the network. This impairs the design of the network. Traffic models are hence, a core component of any performance evaluation of networks and they need to be very accurate.\n\n“Teletraffic theory is the application of mathematics to the measurement, modeling, and control of traffic in telecommunications networks. The aim of traffic modeling is to find stochastic processes to represent the behavior of traffic. Working at the Copenhagen Telephone Company in the 1910s, A. K. Erlang famously characterized telephone traffic at the call level by certain probability distributions for arrivals of new calls and their holding times. Erlang applied the traffic models to estimate the telephone switch capacity needed to achieve a given call blocking probability. The Erlang blocking formulas had tremendous practical interest for public carriers because telephone facilities (switching and transmission) involved considerable investments. Over several decades, Erlang’s work stimulated the use of queuing theory, and applied probability in general, to engineer the public switched telephone network. Teletraffic theory for packet networks has seen considerable progress in recent decades. Significant advances have been made in long-range dependence, wavelet, and multifractal approaches. At the same time, traffic modeling continues to be challenged by evolving network technologies and new multimedia applications. For example, wireless technologies allow greater mobility of users. Mobility must be an additional consideration for modeling traffic in wireless networks. Traffic modeling is an ongoing process without a real end. Traffic models represent our best current understanding of traffic behavior, but our understanding will change and grow over time.”\n\nMeasurements are useful and necessary for verifying the actual network performance. However, measurements do not have the level of abstraction that makes traffic models useful. Traffic models can be used for hypothetical problem solving whereas traffic measurements only reflect current reality. In probabilistic terms, a traffic trace is a realization of a random process, whereas a traffic model is a random process. Thus, traffic models have universality. A traffic trace gives insight about a particular traffic source, but a traffic model gives insight about all traffic sources of that type. Traffic models have three major uses. One important use of traffic models is to properly dimension network resources for a target level of QoS. It was mentioned earlier that Erlang developed models of voice calls to estimate telephone switch capacity to achieve a target call blocking probability. Similarly, models of packet traffic are needed to estimate the bandwidth and buffer resources to provide acceptable packet delays and packet loss probability. Knowledge of the average traffic rate is not sufficient. It is known from queuing theory that queue lengths increase with the variability of traffic. Hence, an understanding of traffic burstiness or variability is needed to determine sufficient buffer sizes at nodes and link capacities. A second important use of traffic models is to verify network performance under specific traffic controls. For example, given a packet scheduling algorithm, it would be possible to evaluate the network performance resulting from different traffic scenarios. For another example, a popular area of research is new improvements to the TCP congestion avoidance algorithm. It is critical that any algorithm is stable and allows multiple hosts to share bandwidth fairly, while sustaining a high throughput. Effective evaluation of the stability, fairness, and throughput of new algorithms would not be possible without realistic source models. A third important use of traffic models is admission control. In particular, connection oriented networks such as ATM depends on admission control to block new connections to maintain QOS guarantees. A simple admission strategy could be based on the peak rate of a new connection; a new connection is admitted if the available bandwidth is greater than the peak rate. However, that strategy would be overly conservative because a variable bit-rate connection may need significantly less bandwidth than its peak rate. A more sophisticated admission strategy is based on effective bandwidths. The source traffic behavior is translated into an effective bandwidth between the peak rate and average rate, which is the specific amount of bandwidth required to meet a given QoS constraint. The effective bandwidth depends on the variability of the source.\n\nTraffic modeling consists of three steps:\nParameter estimation is based on a set of statistics (e.g. mean, variance, density function or auto covariance function, multifractal characteristics) that are measured or calculated from observed data. The set of statistics used in the inference process depends on the impact they may have in the main performance metrics of interest.\n\nIn recent years several types of traffic behavior, that can have significant impact on network performance, were discovered: long-range dependence, self-similarity and, more recently, multifractality.\nThere are two major parameters generated by network traffic models: packet length distributions and packet inter-arrival distributions. Other parameters, such as routes, distribution of destinations, etc., are of less importance. Simulations that use traces generated by network traffic models usually examine a single node in the network, such as a router or switch; factors that depend on specific network topologies or routing information are specific to those topologies and simulations. The problem of packet size distribution is fairly well-understood today. Existing models of packet sizes have proven to be valid and simple. Most packet size models do not consider the problem of order in packet sizes. For example, a TCP datagram in one direction is likely to be followed by a tiny ACK in the other direction about half of one Round-Trip Time (RTT) later. The problem of packet inter-arrival distribution is much more difficult. Understanding of network traffic has evolved significantly over the years, leading to a series of evolutions in network traffic models.\n\nOne of the earliest objections to self-similar traffic models was the difficulty in mathematical analysis. Existing self-similar models could not be used in conventional queuing models. This limitation was rapidly overturned and workable models were constructed. Once basic self-similar models became feasible, the traffic modeling community settled into the “detail” concerns. TCP’s congestion control algorithm complicated the matter of modeling traffic, so solutions needed to be created. Parameter estimation of self-similar models was always difficult, and recent research addresses ways to model network traffic without fully understanding it.\n\nWhen self-similar traffic models were first introduced, there were no efficient, analytically tractable processes to generate the models. Ilkka Norros devised a stochastic process for a storage model with self-similar input and constant bit-rate output. While this initial model was continuous rather than discrete, the model was effective, simple, and attractive.\nAll self-similar traffic models suffer from one significant drawback: estimating the self-similarity parameters from real network traffic requires huge amounts of data and takes extended computation. The most modern method, wavelet multi-resolution analysis, is more efficient, but still very costly. This is undesirable in a traffic model. SWING uses a surprisingly simple model for the network traffic analysis and generation. The model examines characteristics of users, Request-Response Exchanges (RREs), connections, individual packets, and the overall network. No attempt is made to analyze self-similarity characteristics; any self-similarity in the generated traffic comes naturally from the aggregation of many ON/OFF sources.\nThe Pareto distribution process produces independent and identically distributed (IID) inter-arrival times. In general if X is a random variable with a Pareto distribution, then the probability that X is greater than some number x is given by P(X > x) = (x/x_m)-k for all x ≥ x_m where k is a positive parameter and x_m is the minimum possible value of Xi The probability distribution and the density functions are represented as:\nF(t) = 1 – (α/t)β where α,β ≥ 0 & t ≥ α\nf(t) = βαβ t-β-1\nThe parameters β and α are the shape and location parameters, respectively. The Pareto distribution is applied to model self-similar arrival in packet traffic. It is also referred to as double exponential, power law distribution. Other important characteristics of the model are that the Pareto distribution has infinite variance, when β ≥ 2 and achieves infinite mean, when β ≤ 1.\nThe Weibull distributed process is heavy-tailed and can model the fixed rate in ON period and ON/OFF period lengths, when producing self-similar traffic by multiplexing ON/OFF sources. The distribution function in this case is given by:\nF(t) = 1 – e-(t/β)α t > 0\nand the density function of the weibull distribution is given as:\nf(t) = αβ-α tα-1 e -(t/β)α t > 0\nwhere parameters β ≥ 0 and α > 0 are the scale and location parameters respectively.\nThe Weibull distribution is close to a normal distribution. For β ≤ 1 the density function of the distribution is L shaped and for values of β > 1, it is bell shaped. This distribution gives a failure rate increasing with time. For β > 1, the failure rate decreases with time. At, β = 1, the failure rate is constant and the lifetimes are exponentially distributed.\nThe Autoregressive model is one of a group of linear prediction formulas that attempt to predict an output y_n of a system based on previous set of outputs {y_k} where k < n and inputs x_n and {x_k} where k < n. There exist minor changes in the way the predictions are computed based on which, several variations of the model are developed. Basically, when the model depends only on the previous outputs of the system, it is referred to as an auto-regressive model. It is referred to as a Moving Average Model (MAM), if it depends on only the inputs to the system. Finally, Autoregressive-Moving Average models are those that depend both on the inputs and the outputs, for prediction of current output. Autoregressive model of order p, denoted as AR(p), has the following form:\nXt = R1 Xt-1 + R2 Xt-2 + ... + Rp Xt-p + Wt\nwhere Wt is the white noise, Ri are real numbers and Xt are prescribed correlated random numbers. The auto-correlation function of the AR(p) process consists of damped sine waves depending on whether the roots (solutions) of the model are real or imaginary. Discrete Autoregressive Model of order p, denoted as DAR(p), generates a stationary sequence of discrete random variables with a probability distribution and with an auto-correlation structure similar to that of the Autoregressive model of order p.[3]\nRegression models define explicitly the next random variable in the sequence by previous ones within a specified time window and a moving average of a white noise.[5]\nTransform-expand-sample (TES) models are non-linear regression models with modulo-1 arithmetic. They aim to capture both auto-correlation and marginal distribution of empirical data. TES models consist of two major TES processes: TES+ and TES–. TES+ produces a sequence which has positive correlation at lag 1, while TES– produces a negative correlation at lag 1.\n\nEarly traffic models were derived from telecommunications models and focused on simplicity of analysis. They generally operated under the assumption that aggregating traffic from a large number of sources tended to smooth out bursts; that burstiness decreased as the number of traffic sources increased.\nOne of the most widely used and oldest traffic models is the Poisson Model. The memoryless Poisson distribution is the predominant model used for analyzing traffic in traditional telephony networks. The Poisson process is characterized as a renewal process. In a Poisson process the inter-arrival times are exponentially distributed with a rate parameter λ: P{An ≤ t} = 1 – exp(-λt). The Poisson distribution is appropriate if the arrivals are from a large number of independent sources, referred to as Poisson sources. The distribution has a mean and variance equal to the parameter λ.\nThe Poisson distribution can be visualized as a limiting form of the binomial distribution, and is also used widely in queuing models. There are a number of interesting mathematical properties exhibited by Poisson processes. Primarily, superposition of independent Poisson processes results in a new Poisson process whose rate is the sum of the rates of the independent Poisson processes. Further, the independent increment property renders a Poisson process memoryless. Poisson processes are common in traffic applications scenarios that consist of a large number of independent traffic streams. The reason behind the usage stems from Palm's Theorem which states that under suitable conditions, such large number of independent multiplexed streams approach a Poisson process as the number of processes grows, but the individual rates decrease in order to keep the aggregate rate constant. Nevertheless, it is to be noted that traffic aggregation need not always result in a Poisson process. The two primary assumptions that the Poisson model makes are:\n1. The number of sources is infinite\n2. The traffic arrival pattern is random.\nIn the compound Poisson model, the base Poisson model is extended to deliver batches of packets at once. The inter-batch arrival times are exponentially distributed, while the batch size is geometric Mathematically, this model has two parameters, λ, the arrival rate, and ρ in (0,1), the batch parameter. Thus, the mean number of packets in a batch is 1/ ρ, while the mean inter-batch arrival time is 1/ λ. Mean packet arrivals over time period t are tλ/ ρ.\nThe compound Poisson model shares some of the analytical benefits of the pure Poisson model: the model is still memoryless, aggregation of streams is still (compound) Poisson, and the steady-state equation is still reasonably simple to calculate, although varying batch parameters for differing flows would complicate the derivation.\nMarkov models attempt to model the activities of a traffic source on a network, by a finite number of states. The accuracy of the model increases linearly with the number of states used in the model. However, the complexity of the model also increases proportionally with increasing number of states. An important aspect of the Markov model - the Markov Property, states that the next (future) state depends only on the current state. In other words, the probability of the next state, denoted by some random variable Xn+1, depends only on the current state, indicated by Xn, and not on any other state Xi, where i<n. The set of random variables referring to different states {Xn} is referred to as a Discrete Markov Chain.\nAnother attempt at providing a bursty traffic model is found in Jain and Routhier’s Packet Trains model. This model was principally designed to recognize that address locality applies to routing decisions; that is, packets that arrive near each other in time are frequently going to the same destination. In generating a traffic model that allows for easier analysis of locality, the authors created the notion of packet trains, a sequence of packets from the same source, traveling to the same destination (with replies in the opposite direction). Packet trains are optionally sub-divided into tandem trailers. Traffic between a source and a destination usually consists of a series of messages back and forth. Thus, a series of packets go one direction, followed by one or more reply packets, followed by a new series in the initial direction. Traffic quantity is then a superposition of packet trains, which generates substantial bursty behavior. This refines the general conception of the compound Poisson model, which recognized that packets arrived in groups, by analyzing why they arrive in groups, and better characterizing the attributes of the group. Finally, the authors demonstrate that packet arrival times are not Poisson distributed, which led to a model that departs from variations on the Poisson theme. The packet train model is characterized by the following parameters and their associated probability distributions:\nThe train model is designed for analyzing and categorizing real traffic, not for generating synthetic loads for simulation. Thus, little claim has been made about the feasibility of packet trains for generating synthetic traffic. Given accurate parameters and distributions, generation should be straightforward, but derivation of these parameters is not addressed.\n\nNS-2 is a popular network simulator; PackMimeHTTP is a web traffic generator for NS-2, published in 2004. It does take long-range dependencies into account, and uses the Weibull distribution. Thus, it relies on heavy tails to emulate true self-similarity. Over most time scales, the effort is a success; only a long-running simulation would allow a distinction to be drawn. This follows suggestions from where it is suggested that self-similar processes can be represented as a superposition of many sources each individually modeled with a heavy-tailed distribution. It is clear that self-similar traffic models are in the mainstream.\n\n\n"}
{"id": "302178", "url": "https://en.wikipedia.org/wiki?curid=302178", "title": "Index of logic articles", "text": "Index of logic articles\n\nA System of Logic --\nA priori and a posteriori --\nAbacus logic --\nAbduction (logic) --\nAbductive validation --\nAcademia Analitica --\nAccuracy and precision --\nAd captandum --\nAd hoc hypothesis --\nAd hominem --\nAffine logic --\nAffirming the antecedent --\nAffirming the consequent --\nAlgebraic logic --\nAmbiguity --\nAnalysis --\nAnalysis (journal) --\nAnalytic reasoning --\nAnalytic–synthetic distinction --\nAnangeon --\nAnecdotal evidence --\nAntecedent (logic) --\nAntepredicament --\nAnti-psychologism --\nAntinomy --\nApophasis --\nAppeal to probability --\nAppeal to ridicule --\nArchive for Mathematical Logic --\nArché --\nArgument --\nArgument by example --\nArgument form --\nArgument from authority --\nArgument map --\nArgumentation ethics --\nArgumentation theory --\nArgumentum ad baculum --\nArgumentum e contrario --\nAriadne's thread (logic) --\nAristotelian logic --\nAristotle --\nAssociation for Informal Logic and Critical Thinking --\nAssociation for Logic, Language and Information --\nAssociation for Symbolic Logic --\nAttacking Faulty Reasoning --\nAustralasian Association for Logic --\nAxiom --\nAxiom independence --\nAxiom of reducibility --\nAxiomatic system --\nAxiomatization --\n\nBackward chaining --\nBarcan formula --\nBegging the question --\nBegriffsschrift --\nBelief --\nBelief bias --\nBelief revision --\nBenson Mates --\nBertrand Russell Society --\nBiconditional elimination --\nBiconditional introduction --\nBivalence and related laws --\nBlue and Brown Books --\nBoole's syllogistic --\nBoolean algebra (logic) --\nBoolean algebra (structure) --\nBoolean network --\n\nCanon (basic principle) --\nCanonical form --\nCanonical form (Boolean algebra) --\nCartesian circle --\nCase-based reasoning --\nCategorical logic --\nCategories (Aristotle) --\nCategories (Peirce) --\nCategory mistake --\nCatuṣkoṭi --\nCircular definition --\nCircular reasoning --\nCircular reference --\nCircular reporting --\nCircumscription (logic) --\nCircumscription (taxonomy) --\nClassical logic --\nClocked logic --\nCognitive bias --\nCointerpretability --\nColorless green ideas sleep furiously --\nCombinational logic --\nCombinatory logic --\nCombs method --\nCommon knowledge (logic) --\nCommutativity of conjunction --\nCompleteness (logic) --\nComposition of Causes --\nCompossibility --\nComprehension (logic) --\nComputability logic --\nConcept --\nConceptualism --\nCondensed detachment --\nConditional disjunction --\nConditional probability --\nConditional proof --\nConditional quantifier --\nConfirmation bias --\nConflation --\nConfusion of the inverse --\nConjunction elimination --\nConjunction fallacy --\nConjunction introduction --\nConjunctive normal form --\nConnexive logic --\nConnotation --\nConsequent --\nConsistency --\nConstructive dilemma --\nContra principia negantem non est disputandum --\nContradiction --\nContrapositive --\nControl logic --\nConventionalism --\nConverse (logic) --\nConverse Barcan formula --\nCorrelative-based fallacies --\nCounterexample --\nCounterfactual conditional --\nCounterintuitive --\nCratylism --\nCredibility --\nCriteria of truth --\nCritical-Creative Thinking and Behavioral Research Laboratory --\nCritical pedagogy --\nCritical reading --\nCritical thinking --\nCritique of Pure Reason --\nCurry's paradox --\nCyclic negation --\n\nDagfinn Føllesdal --\nDe Interpretatione --\nDe Morgan's laws --\nDecidability (logic) --\nDecidophobia --\nDecision making --\nDecisional balance sheet --\nDeductive closure --\nDeduction theorem --\nDeductive fallacy --\nDeductive reasoning --\nDefault logic --\nDefeasible logic --\nDefeasible reasoning --\nDefinable set --\nDefinist fallacy --\nDefinition --\nDefinitions of logic --\nDegree of truth --\nDenying the antecedent --\nDenying the correlative --\nDeontic logic --\nDescription --\nDescription logic --\nDescriptive fallacy --\nDeviant logic --\nDharmakirti --\nDiagrammatic reasoning --\nDialectica --\nDialectica space --\nDialetheism --\nDichotomy --\nDifference (philosophy) --\nDigital timing diagram --\nDignāga --\nDilemma --\nDisjunction elimination --\nDisjunction introduction --\nDisjunctive normal form --\nDisjunctive syllogism --\nDispositional and occurrent belief --\nDisquotational principle --\nDissoi logoi --\nDivision of Logic, Methodology, and Philosophy of Science --\nDon't-care term --\nDonald Davidson (philosopher) --\nDouble counting (fallacy) --\nDouble negation --\nDouble negative --\nDouble negative elimination --\nDoxa --\nDrinking the Kool-Aid --\n\nEL++ --\nEcological fallacy --\nEffective method --\nElimination rule --\nEmotional reasoning --\nEmotions in decision-making --\nEmpty name --\nEncyclopedia of the Philosophical Sciences --\nEnd term --\nEngineered language --\nEntailment --\nEntitative graph --\nEnumerative definition --\nEpicureanism --\nEpilogism --\nEpistemic closure --\nEquisatisfiability --\nErotetics --\nEternal statement --\nEtymological fallacy --\nEuropean Summer School in Logic, Language and Information --\nEvidence --\nExclusive nor --\nExclusive or --\nExistential fallacy --\nExistential graph --\nExistential quantification --\nExpert --\nExplanandum --\nExplanation --\nExplanatory power --\nExtension (semantics) --\nExtensional context --\nExtensional definition --\n\nFa (concept) --\nFact --\nFallacies of definition --\nFallacy --\nFallacy of distribution --\nFallacy of four terms --\nFallacy of quoting out of context --\nFallacy of the four terms --\nFalse attribution --\nFalse dilemma --\nFalse equivalence --\nFalse premise --\nFictionalism --\nFinitary relation --\nFinite model property --\nFirst-order logic --\nFirst-order predicate --\nFirst-order predicate calculus --\nFirst-order resolution --\nFitch-style calculus --\nFluidic logic --\nFluidics --\nFormal fallacy --\nFormal ontology --\nFormal system --\nFormalism (philosophy) --\nForward chaining --\nFree logic --\nFree variables and bound variables --\nFunction and Concept --\nFuzzy logic --\n\nGame semantics --\nGanto's Ax --\nGeometry of interaction --\nGilles-Gaston Granger --\nGongsun Long --\nGrammaticality --\nGreedy reductionism --\nGrundlagen der Mathematik --\n\nHPO formalism --\nHalo effect --\nHandbook of Automated Reasoning --\nHanlon's razor --\nHasty generalization --\nHerbrandization --\nHetucakra --\nHeyting algebra --\nHigher-order predicate --\nHigher-order thinking --\nHistorian's fallacy --\nHistorical fallacy --\nHistory of logic --\nHistory of the function concept --\nHold come what may --\nHomunculus argument --\nHorn clause --\nHume's fork --\nHume's principle --\nHypothetical syllogism --\n\nIdentity (philosophy) --\nIdentity of indiscernibles --\nIdola fori --\nIdola specus --\nIdola theatri --\nIdola tribus --\nIf-by-whiskey --\nIff --\nIllicit major --\nIllicit minor --\nIlluminationism --\nImmutable truth --\nImperative logic --\nImplicant --\nInclusion (logic) --\nIncomplete comparison --\nInconsistent comparison --\nInconsistent triad --\nIndependence-friendly logic --\nIndian logic --\nInductive logic --\nInductive logic programming --\nInference --\nInference procedure --\nInference rule --\nInferential role semantics --\nInfinitary logic --\nInfinite regress --\nInfinity --\nInformal fallacy --\nInformal logic --\nInquiry --\nInquiry (philosophy journal) --\nInsolubilia --\nInstitute for Logic, Language and Computation --\nIntellectual responsibility --\nIntended interpretation --\nIntension --\nIntensional fallacy --\nIntensional logic --\nIntensional statement --\nIntentional Logic --\nIntermediate logic --\nInterpretability --\nInterpretability logic --\nInterpretive discussion --\nIntroduction rule --\nIntroduction to Mathematical Philosophy --\nIntuitionistic linear logic --\nIntuitionistic logic --\nInvalid proof --\nInventor's paradox --\nInverse (logic) --\nInverse consequences --\nIrreducibility --\nIs Logic Empirical? --\nIsagoge --\nIvor Grattan-Guinness --\n\nJacobus Naveros --\nJayanta Bhatta --\nJingle-jangle fallacies --\nJohn Corcoran (logician) --\nJohn W. Dawson, Jr --\nJournal of Applied Non-Classical Logics --\nJournal of Automated Reasoning --\nJournal of Logic, Language and Information --\nJournal of Logic and Computation --\nJournal of Mathematical Logic --\nJournal of Philosophical Logic --\nJournal of Symbolic Logic --\nJudgment (mathematical logic) --\nJudgmental language --\nJust-so story --\n\nKarnaugh map --\nKinetic logic --\nKnowing and the Known --\nKripke semantics --\nKurt Gödel Society --\n\nLanguage --\nLanguage, Proof and Logic --\nLateral thinking --\nLaw of excluded middle --\nLaw of identity --\nLaw of non-contradiction --\nLaw of noncontradiction --\nLaw of thought --\nLaws of Form --\nLaws of logic --\nLeap of faith --\nLemma (logic) --\nLexical definition --\nLinear logic --\nLinguistic and Philosophical Investigations --\nLinguistics and Philosophy --\nList of fallacies --\nList of incomplete proofs --\nList of logic journals --\nList of paradoxes --\nLogic --\nLogic Lane --\nLogic Spectacles --\nLogic gate --\nLogic in China --\nLogic in Islamic philosophy --\nLogic of class --\nLogic of information --\nLogic programming --\nLogica Universalis --\nLogica nova --\nLogical Analysis and History of Philosophy --\nLogical Investigations (Husserl) --\nLogical Methods in Computer Science --\nLogical abacus --\nLogical argument --\nLogical assertion --\nLogical atomism --\nLogical biconditional --\nLogical conditional --\nLogical conjunction --\nLogical constant --\nLogical disjunction --\nLogical equality --\nLogical equivalence --\nLogical extreme --\nLogical form --\nLogical harmony --\nLogical holism --\nLogical nand --\nLogical nor --\nLogical operator --\nLogical quality --\nLogical truth --\nLogicism --\nLogico-linguistic modeling --\nLogos --\nLoosely associated statements --\nŁoś–Tarski preservation theorem --\nLudic fallacy --\nLwów–Warsaw school of logic --\n\nMain contention --\nMajor term --\nMarkov's principle --\nMartin Gardner bibliography --\nMasked man fallacy --\nMaterial conditional --\nMathematical fallacy --\nMathematical logic --\nMeaning (linguistics) --\nMeaning (non-linguistic) --\nMeaning (philosophy of language) --\nMeaningless statement --\nMegarian school --\nMental model theory of reasoning --\nMereology --\nMeta-communication --\nMetalanguage --\nMetalogic --\nMetamathematics --\nMetasyntactic variable --\nMetatheorem --\nMetavariable --\nMiddle term --\nMinimal axioms for Boolean algebra --\nMinimal logic --\nMinor premise --\nMiscellanea Logica --\nMissing dollar riddle --\nModal fallacy --\nModal fictionalism --\nModal logic --\nModel theory --\nModus ponens --\nModus tollens --\nMoral reasoning --\nMotivated reasoning --\nMoving the goalposts --\nMultigrade predicate --\nMulti-valued logic --\nMultiple-conclusion logic --\nMutatis mutandis --\nMutual knowledge (logic) --\nMutually exclusive events --\nMünchhausen trilemma --\n\nNaive set theory --\nName --\nNarrative logic --\nNatural deduction --\nNatural kind --\nNatural language --\nNecessary and sufficient --\nNecessity and sufficiency --\nNegation --\nNeutrality (philosophy) --\nNirvana fallacy --\nNixon diamond --\nNo true Scotsman --\nNominal identity --\nNon-Aristotelian logic --\nNon-classical logic --\nNon-monotonic logic --\nNon-rigid designator --\nNon sequitur (logic) --\nNoneism --\nNonfirstorderizability --\nNordic Journal of Philosophical Logic --\nNormal form (natural deduction) --\nNovum Organum --\nNyaya --\nNyāya Sūtras --\n\nObject language --\nObject of the mind --\nObject theory --\nOccam's razor --\nOn Formally Undecidable Propositions of Principia Mathematica and Related Systems --\nOne-sided argument --\nOntological commitment --\nOpen sentence --\nOpinion --\nOpposing Viewpoints series --\nOrdered logic --\nOrganon --\nOriginal proof of Gödel's completeness theorem --\nOsmund Lewry --\nOstensive definition --\nOutline of logic --\nOverbelief --\n\nPackage-deal fallacy --\nPanlogism --\nParaconsistent logic --\nParaconsistent logics --\nParade of horribles --\nParadox --\nPars destruens/pars construens --\nPathetic fallacy --\nPer fas et nefas --\nPersuasive definition --\nPeter Simons (academic) --\nPhilosophia Mathematica --\nPhilosophical logic --\nPhilosophy of logic --\nPierce's law --\nPlural quantification --\nPoisoning the well --\nPolarity item --\nPolish Logic --\nPolish notation --\nPolitician's syllogism --\nPolychotomous key --\nPolylogism --\nPolysyllogism --\nPort-Royal Logic --\nPossible world --\nPost's lattice --\nPost disputation argument --\nPost hoc ergo propter hoc --\nPosterior Analytics --\nPractical syllogism --\nPragmatic mapping --\nPragmatic maxim --\nPragmatic theory of truth --\nPramāṇa --\nPramāṇa-samuccaya --\nPrecising definition --\nPrecision questioning --\nPredicable --\nPredicate (logic) --\nPredicate abstraction --\nPredicate logic --\nPreferential entailment --\nPreintuitionism --\nPrescriptivity --\nPresentism (literary and historical analysis) --\nPresupposition --\nPrincipia Mathematica --\nPrinciple of bivalence --\nPrinciple of explosion --\nPrinciple of nonvacuous contrast --\nPrinciple of sufficient reason --\nPrinciples of Mathematical Logic --\nPrior Analytics --\nPrivate Eye Project --\nPro hominem --\nProbabilistic logic --\nProbabilistic logic network --\nProblem of future contingents --\nProblem of induction --\nProcess of elimination --\nProject Reason --\nProof-theoretic semantics --\nProof (truth) --\nProof by assertion --\nProof theory --\nPropaganda techniques --\nProposition --\nPropositional calculus --\nPropositional function --\nPropositional representation --\nPropositional variable --\nProsecutor's fallacy --\nProvability logic --\nProving too much --\nPrudence --\nPseudophilosophy --\nPsychologism --\nPsychologist's fallacy --\n\nQ.E.D. --\nQuantification --\nQuantization (linguistics) --\nQuantum logic --\n\nRamism --\nRationality --\nRazor (philosophy) --\nReason --\nReductio ad absurdum --\nReference --\nReflective equilibrium --\nRegression fallacy --\nRegular modal logic --\nReification (fallacy) --\nRelativist fallacy --\nRelevance --\nRelevance logic --\nRelevant logic --\nRemarks on the Foundations of Mathematics --\nRetroduction --\nRetrospective determinism --\nRevolutions in Mathematics --\nRhetoric --\nRigour --\nRolandas Pavilionis --\nRound square copula --\nRudolf Carnap --\nRule of inference --\nRvachev function --\n\nSEE-I --\nSalva congruitate --\nSalva veritate --\nSatisfiability --\nScholastic logic --\nSchool of Names --\nScience of Logic --\nScientific temper --\nSecond-order predicate --\nSegment addition postulate --\nSelf-reference --\nSelf-refuting idea --\nSelf-verifying theories --\nSemantic theory of truth --\nSemantics --\nSense and reference --\nSequent --\nSequent calculus --\nSequential logic --\nSet (mathematics) --\nSeven Types of Ambiguity (Empson) --\nSheffer stroke --\nShip of Theseus --\nSimple non-inferential passage --\nSingular term --\nSituation --\nSituational analysis --\nSkeptic's Toolbox --\nSlingshot argument --\nSocial software (social procedure) --\nSocratic questioning --\nSoku hi --\nSome Remarks on Logical Form --\nSophism --\nSophistical Refutations --\nSoundness --\nSource credibility --\nSource criticism --\nSpecial case --\nSpecialization (logic) --\nSpeculative reason --\nSpurious relationship --\nSquare of opposition --\nState of affairs (philosophy) --\nStatement (logic) --\nStraight and Crooked Thinking --\nStraight face test --\nStraw man --\nStrength (mathematical logic) --\nStrict conditional --\nStrict implication --\nStrict logic --\nStructural rule --\nStudia Logica --\nStudies in Logic, Grammar and Rhetoric --\nSubjective logic --\nSubstitution (logic) --\nSubstructural logic --\nSufficient condition --\nSum of Logic --\nSunk costs --\nSupertask --\nSupervaluationism --\nSupposition theory --\nSurvivorship bias --\nSyllogism --\nSyllogistic fallacy --\nSymbol (formal) --\nSyntactic Structures --\nSyntax (logic) --\nSynthese --\nSystems of Logic Based on Ordinals --\n\nT-schema --\nTacit assumption --\nTarski's undefinability theorem --\nTautology (logic) --\nTemporal logic --\nTemporal parts --\nTeorema (journal) --\nTerm (argumentation) --\nTerm logic --\nTernary logic --\nTestability --\nTetralemma --\nTextual case based reasoning --\nThe False Subtlety of the Four Syllogistic Figures --\nThe Foundations of Arithmetic --\nThe Geography of Thought --\nThe Laws of Thought --\nThe Paradoxes of the Infinite --\nTheorem --\nTheoretical definition --\nTheory and Decision --\nTheory of justification --\nTheory of obligationes --\nThird-cause fallacy --\nThree men make a tiger --\nTolerance (in logic) --\nTopical logic --\nTopics (Aristotle) --\nTractatus Logico-Philosophicus --\nTrain of thought --\nTrairūpya --\nTransferable belief model --\nTransparent Intensional Logic --\nTregoED --\nTrikonic --\nTrilemma --\nTrivial objections --\nTrivialism --\nTruth --\nTruth-bearer --\nTruth claim --\nTruth condition --\nTruth function --\nTruth value --\nTruthiness --\nTruthmaker --\nType (model theory) --\nType theory --\nType–token distinction --\n\nUltrafinitism --\nUnification (computer science) --\nUnifying theories in mathematics --\nUniqueness quantification --\nUniversal logic --\nUniversal quantification --\nUnivocity --\nUnspoken rule --\nUse–mention distinction --\n\nVacuous truth --\nVagrant predicate --\nVagueness --\nValidity --\nValuation-based system --\nVan Gogh fallacy --\nVenn diagram --\nVicious circle principle --\n\nWarnier/Orr diagram --\nWell-formed formula --\nWhat the Tortoise Said to Achilles --\nWillard Van Orman Quine --\nWilliam Kneale --\nWindow operator --\nWisdom of repugnance --\nWitness (mathematics) --\nWord sense --\n\nZhegalkin polynomial --\n\n"}
{"id": "185493", "url": "https://en.wikipedia.org/wiki?curid=185493", "title": "Informal mathematics", "text": "Informal mathematics\n\nInformal mathematics, also called naïve mathematics, has historically been the predominant form of mathematics at most times and in most cultures, and is the subject of modern ethno-cultural studies of mathematics. The philosopher Imre Lakatos in his \"Proofs and Refutations\" aimed to sharpen the formulation of informal mathematics, by reconstructing its role in nineteenth century mathematical debates and concept formation, opposing the predominant assumptions of mathematical formalism. Informality may not discern between statements given by \"inductive reasoning\" (as in approximations which are deemed \"correct\" merely because they are useful), and statements derived by \"deductive reasoning\".\n\n\"Informal mathematics\" means any informal mathematical practices, as used in everyday life, or by aboriginal or ancient peoples, without historical or geographical limitation. Modern mathematics, exceptionally from that point of view, emphasizes formal and strict proofs of all statements from given axioms. This can usefully be called therefore \"formal mathematics\". Informal practices are usually understood intuitively and justified with examples—there are no axioms. This is of direct interest in anthropology and psychology: it casts light on the perceptions and agreements of other cultures. It is also of interest in developmental psychology as it reflects a naïve understanding of the relationships between numbers and things. Another term used for informal mathematics is folk mathematics, which is ambiguous; the mathematical folklore article is dedicated to the usage of that term among professional mathematicians.\n\nThe field of naïve physics is concerned with similar understandings of physics. People use mathematics and physics in everyday life, without really understanding (or caring) how mathematical and physical ideas were historically derived and justified.\n\nThere has long been a standard account of the development of geometry in ancient Egypt, followed by Greek mathematics and the emergence of deductive logic. The modern sense of the term \"mathematics\", as meaning only those systems justified with reference to axioms, is however an anachronism if read back into history. Several ancient societies built impressive mathematical systems and carried out complex calculations based on proofless heuristics and practical approaches. Mathematical facts were accepted on a pragmatic basis. Empirical methods, as in science, provided the justification for a given technique. Commerce, engineering, calendar creation and the prediction of eclipses and stellar progression were practiced by ancient cultures on at least three continents. N.C. Ghosh included Informal Mathematics in the list of Folk Mathematics.\n\n"}
{"id": "4703917", "url": "https://en.wikipedia.org/wiki?curid=4703917", "title": "International Society for Mathematical Sciences", "text": "International Society for Mathematical Sciences\n\nThe International Society for Mathematical Sciences is a mathematics society, primarily based in Japan. It was formerly known as the Japanese Association of Mathematical Sciences, and was founded in 1948 by Tatsujiro Shimizu.\n\nThe ISMS publishes a bimonthly scientific journal, \"Scientiae Mathematicae Japonicae\" (), which was formed in 2001 from the merger of two journals previously published by the same society, \"Mathematica Japonica\", founded in 1948, and \"Scientiae Mathematicae\", which published nine issues over three volumes in 1998, 1999, and 2000. In addition the ISMS holds an annual meeting and publishes a Japanese language mathematics magazine, \"Kaiho\", and a monthly newsletter, \"Notices from the ISMS\".\n\n"}
{"id": "57258626", "url": "https://en.wikipedia.org/wiki?curid=57258626", "title": "Jouanolou's trick", "text": "Jouanolou's trick\n\nIn algebraic geometry, Jouanolou's trick is a theorem that asserts, for an algebraic variety \"X\", the existence of a surjection with affine fibers from an affine variety \"W\" to \"X\". The variety \"W\" is therefore homotopy-equivalent to \"X\", but it has the technically advantageous property of being affine. Jouanolou's original statement of the theorem required that \"X\" be quasi-projective over an affine scheme, but this has since been considerably weakened.\n\nJouanolou's original statement was:\n\nBy the definition of a torsor, \"W\" comes with a surjective map to \"X\" and is Zariski-locally on \"X\" an affine space bundle.\n\nJouanolou's proof used an explicit construction. Let \"S\" be an affine scheme and formula_1. Interpret the affine space formula_2 as the space of (\"r\" + 1) × (\"r\" + 1) matrices over \"S\". Within this affine space, there is a subvariety \"W\" consisting of idempotent matrices of rank one. The image of such a matrix is therefore a point in \"X\", and the map formula_3 that sends a matrix to the point corresponding to its image is the map claimed in the statement of the theorem. To show that this map has the desired properties, Jouanolou notes that there is a short exact sequence of vector bundles:\nwhere the first map is defined by multiplication by a basis of sections of formula_5 and the second map is the cokernel. Jouanolou then asserts that \"W\" is a torsor for formula_6.\n\nJouanolou deduces the theorem in general by reducing to the above case. If \"X\" is projective over an affine scheme \"S\", then it admits a closed immersion into some projective space formula_7. Pulling back the variety \"W\" constructed above for formula_7 along this immersion yields the desired variety \"W\" for \"X\". Finally, if \"X\" is quasi-projective, then it may be realized as an open subscheme of a projective \"S\"-scheme. Blow up the complement of \"X\" to get formula_9, and let formula_10 denote the inclusion morphism. The complement of \"X\" in formula_9 is a Cartier divisor, and therefore \"i\" is an affine morphism. Now perform the previous construction for formula_9 and pull back along \"i\".\n\nRobert Thomason observed that, by making a less explicit construction, it was possible to obtain the same conclusion under significantly weaker hypotheses. Thomason's construction first appeared in a paper of Weibel. Thomason's theorem asserts:\n\nHaving an ample family of line bundles was first defined in SGA 6 Exposé II Définition 2.2.4. Any quasi-projective scheme over an affine scheme has an ample family of line bundles, as does any separated locally factorial Noetherian scheme.\n\nThomason's proof abstracts the key features of Jouanolou's. By hypothesis, \"X\" admits a set of line bundles \"L\", ..., \"L\" and sections \"s\", ..., \"s\" whose non-vanishing loci are affine and cover \"X\". Define \"X\" to be the non-vanishing locus of \"s\", and define formula_13 to be the direct sum of \"L\", ..., \"L\". The sections define a morphism of vector bundles formula_14. Define formula_15 to be the cokernel of \"s\". On \"X\", \"s\" is a split monomorphism since it is inverted by the inverse of \"s\". Therefore formula_15 is a vector bundle over \"X\", and because these open sets cover \"X\", formula_15 is a vector bundle.\n\nDefine formula_18 and similarly for formula_19. Let \"W\" be the complement of formula_19 in formula_21. There is an equivalent description of \"W\" as formula_22, and from this description, it is easy to check that it is a torsor for formula_15. Therefore the projection formula_24 is affine. To see that \"W\" is itself affine, apply a criterion of Serre (EGA II 5.2.1(b), EGA IV 1.7.17). Each \"s\" determines a global section \"f\" of \"W\". The non-vanishing locus \"W\" of \"f\" is contained in formula_25, which is affine, and hence \"W\" is affine. The sum of the sections \"f\", ..., \"f\" is 1, so the ideal they generate is the ring of global sections. Serre's criterion now implies that \"W\" is affine.\n\n"}
{"id": "14258729", "url": "https://en.wikipedia.org/wiki?curid=14258729", "title": "Kleene–Rosser paradox", "text": "Kleene–Rosser paradox\n\nIn mathematics, the Kleene–Rosser paradox is a paradox that shows that certain systems of formal logic are inconsistent, in particular the version of Curry's combinatory logic introduced in 1930, and Church's original lambda calculus, introduced in 1932–1933, both originally intended as systems of formal logic. The paradox was exhibited by Stephen Kleene and J. B. Rosser in 1935.\n\nKleene and Rosser were able to show that both systems are able to characterize and enumerate their provably total, definable number-theoretic functions, which enabled them to construct a term that essentially replicates the Richard paradox in formal language.\n\nCurry later managed to identify the crucial ingredients of the calculi that allowed the construction of this paradox, and used this to construct a much simpler paradox, now known as Curry's paradox.\n\n\n"}
{"id": "353748", "url": "https://en.wikipedia.org/wiki?curid=353748", "title": "List of computability and complexity topics", "text": "List of computability and complexity topics\n\nThis is a list of computability and complexity topics, by Wikipedia page.\n\nComputability theory is the part of the theory of computation that deals with what can be computed, in principle. Computational complexity theory deals with how hard computations are, in quantitative terms, both with upper bounds (algorithms whose complexity in the worst cases, as use of computing resources, can be estimated), and from below (proofs that no procedure to carry out some task can be very fast).\n\nFor more abstract foundational matters, see the list of mathematical logic topics. See also list of algorithms, list of algorithm general topics.\n\n\n\n\n\n\n\"See the list of complexity classes\"\n\n\n\n"}
{"id": "490054", "url": "https://en.wikipedia.org/wiki?curid=490054", "title": "List of exponential topics", "text": "List of exponential topics\n\nThis is a list of exponential topics, by Wikipedia page. See also list of logarithm topics.\n"}
{"id": "169823", "url": "https://en.wikipedia.org/wiki?curid=169823", "title": "List of two-dimensional geometric shapes", "text": "List of two-dimensional geometric shapes\n\nThis is a list of two-dimensional geometric shapes in Euclidean and other geometries. For mathematical objects in more dimensions, see list of mathematical shapes. For a broader scope, see list of shapes.\n\n\n\n\n\n"}
{"id": "17841907", "url": "https://en.wikipedia.org/wiki?curid=17841907", "title": "Malthusian equilibrium", "text": "Malthusian equilibrium\n\nA population is in Malthusian equilibrium when all of its production is used only for subsistence. Malthusian equilibrium is a locally stable and a dynamic equilibrium.\n\n"}
{"id": "2164767", "url": "https://en.wikipedia.org/wiki?curid=2164767", "title": "Mathemagician", "text": "Mathemagician\n\nA mathemagician is a mathematician who is also a magician.\n\nThe name \"mathemagician\" was probably first applied to Martin Gardner, but has since been used to describe many mathematician/magicians, including Arthur T. Benjamin, Persi Diaconis, and Colm Mulcahy. Diaconis has suggested that the reason so many mathematicians are magicians is that \"inventing a magic trick and inventing a theorem are very similar activities.\"\n\nA great number of self-working mentalism tricks rely on mathematical principles. Max Maven often utilizes this type of magic in his performance.\n\n\n"}
{"id": "23522969", "url": "https://en.wikipedia.org/wiki?curid=23522969", "title": "Mathematical elimination", "text": "Mathematical elimination\n\nThe terms \"mathematical elimination\" and \"mathematically eliminated\" mean to be excluded in a decision, based on numerical counts, due to insufficient total numbers, even if all remaining events were 100% in favor. The excluded outcome is considered to be eliminated due to the mathematical probability being zero (0%).\n\nThe term is used in elections when a candidate lacks sufficient votes to win, even if that candidate could garner all remaining votes. In sports, the term \"mathematically eliminated\"\n\nrefers to situations where there are not enough future games or competitive events remaining to be played to avoid defeat, even if all future events were won.\n\nThe term \"mathematically eliminated\" has been in use for more than 100 years,\nalthough the meaning has varied. In a 1904 article, in the \"American Journal of Psychology\", Volume XV, errors of measurement were described as quantifiable to be \"mathematically eliminated\" from the analysis of the remaining data.\n"}
{"id": "23236792", "url": "https://en.wikipedia.org/wiki?curid=23236792", "title": "Mathematical exposure modeling", "text": "Mathematical exposure modeling\n\nMathematical exposure modeling is an indirect method of determining exposure, particularly for human exposure to environmental contaminants. It is useful when direct measurement of pollutant concentration is not feasible because direct measurement sometimes requires skilled professionals and complex, expensive laboratory equipment. The ability to make inferences in the absence of direct measurements, makes exposure modeling a powerful tool for predicting exposures by exploring hypothetical situations. It allows researchers to ask \"what if\" questions about exposure scenarios.\n\nMathematical modeling is commonly used to determine human exposure to indoor air pollution. Studies have shown that humans spend about 90% of their time indoors, and contaminant levels may be as high or higher inside than outside, due to the presence of multiple indoor contaminant sources, in combination with poor ventilation. Indoor air modeling requires information on a number of parameters including the air exchange rate, deposition rate, source emission rate, and physical volume of the indoor setting. Indoor environments can basically be thought of as closed systems, so models describing them are usually based on the \"mass balance\" equation. It is also assumed that a pollutant emitted into an indoor environment instantly spreads uniformly throughout the system, so that the concentration is the same at any point in space at any point in time. Mathematically, the total pollutant mass emitted inside a chamber during time T can be expressed as<br>\n\nThe total mass lost during time T can be expressed as<br>\n\nFollowing the principle of the \"mass balance\" equation, the total mass in the chamber at time T, is the difference between the two equations above, mass generated during time T minus mass lost during time T. This value may also be calculated from the equation<br>\n\nThere are two critical pieces of information that are needed to calculate human exposure. These include data on 1) the whereabouts of the individual or individuals being exposed and 2) the concentration of the pollutants in the different locations. This can be expressed mathematically as the sum of the products of time spent by a person in those different locations by the time-averaged air pollutant concentrations occurring in those locations.<br>\n\nAs mentioned above, knowing the whereabouts of the individual or individuals, is very important when trying to determine air pollution exposure. In the absence of data obtained from direct observation, human activity pattern data may be used. This data can be found in several reports conducted by the U.S. Environmental Protection Agency. The data was collected through the National Human Activity Pattern Survey (NHAPS), and contains a representative cross-section of 24-hour daily activity patterns. This data can be used to create inhalation exposure models which can serve as useful public health tools for epidemiology, education, intervention, risk assessment, and creation of air quality guidelines.\n\n\nOtt, W.R., Steinemann, A.C., Wallace, L.A.. Exposure Analysis. CRC Press (2007)\n\nThe Inside Story: A Guide to Indoor Air Quality. U.S. EPA (2009)\n"}
{"id": "159730", "url": "https://en.wikipedia.org/wiki?curid=159730", "title": "Mathematical practice", "text": "Mathematical practice\n\nMathematical practice comprises the working practices of professional mathematicians: selecting theorems to prove, using informal notations to persuade themselves and others that various steps in the final proof are convincing, and seeking peer review and publication, as opposed to the end result of proven and published theorems.\n\nPhilip Kitcher has proposed a more formal definition of a mathematical practice, as a quintuple. His intention was primarily to document mathematical practice through its historical changes.\n\nThe evolution of mathematical practice was slow, and some contributors to modern mathematics did not follow even the practice of their time. For example, Pierre de Fermat was infamous for withholding his proofs, but nonetheless had a vast reputation for correct assertions of results.\n\nOne motivation to study mathematical practice is that, despite much work in the 20th century, some still feel that the foundations of mathematics remain unclear and ambiguous. One proposed remedy is to shift focus to some degree onto 'what is meant by a proof', and other such questions of method.\n\nIf mathematics has been informally used throughout history, in numerous cultures and continents, then it could be argued that \"mathematical practice\" is the practice, or use, of mathematics in everyday life. One definition of mathematical practice, as described above, is the \"working practices of professional mathematicians.\" However, another definition, more in keeping with the predominant usage of mathematics, is that mathematical practice is the everyday practice, or use, of math. Whether one is estimating the total cost of their groceries, calculating miles per gallon, or figuring out how many minutes on the treadmill that chocolate éclair will require, math as used by most people relies less on proof than on practicality (i. e., does it answer the question?).\n\nMathematical teaching usually requires the use of several important teaching pedagogies or components. Most GCSE, A-Level and undergraduate mathematics require the following components: \n\n\n"}
{"id": "22468661", "url": "https://en.wikipedia.org/wiki?curid=22468661", "title": "Mathematics and art", "text": "Mathematics and art\n\nMathematics and art are related in a variety of ways. Mathematics has itself been described as an art motivated by beauty. Mathematics can be discerned in arts such as music, dance, painting, architecture, sculpture, and textiles. This article focuses, however, on mathematics in the visual arts.\n\nMathematics and art have a long historical relationship. Artists have used mathematics since the 4th century BC when the Greek sculptor Polykleitos wrote his \"Canon\", prescribing proportions based on the ratio 1: for the ideal male nude. Persistent popular claims have been made for the use of the golden ratio in ancient art and architecture, without reliable evidence. In the Italian Renaissance, Luca Pacioli wrote the influential treatise \"De Divina Proportione\" (1509), illustrated with woodcuts by Leonardo da Vinci, on the use of the golden ratio in art. Another Italian painter, Piero della Francesca, developed Euclid's ideas on perspective in treatises such as \"De Prospectiva Pingendi\", and in his paintings. The engraver Albrecht Dürer made many references to mathematics in his work \"Melencolia I\". In modern times, the graphic artist M. C. Escher made intensive use of tessellation and hyperbolic geometry, with the help of the mathematician H. S. M. Coxeter, while the De Stijl movement led by Theo van Doesburg and Piet Mondrian explicitly embraced geometrical forms. Mathematics has inspired textile arts such as quilting, knitting, cross-stitch, crochet, embroidery, weaving, Turkish and other carpet-making, as well as kilim. In Islamic art, symmetries are evident in forms as varied as Persian girih and Moroccan zellige tilework, Mughal jali pierced stone screens, and widespread muqarnas vaulting.\n\nMathematics has directly influenced art with conceptual tools such as linear perspective, the analysis of symmetry, and mathematical objects such as polyhedra and the Möbius strip. Magnus Wenninger creates colourful stellated polyhedra, originally as models for teaching. Mathematical concepts such as recursion and logical paradox can be seen in paintings by Rene Magritte and in engravings by M. C. Escher. Computer art often makes use of fractals including the Mandelbrot set, and sometimes explores other mathematical objects such as cellular automata. Controversially, the artist David Hockney has argued that artists from the Renaissance onwards made use of the camera lucida to draw precise representations of scenes; the architect Philip Steadman similarly argued that Vermeer used the camera obscura in his distinctively observed paintings.\n\nOther relationships include the algorithmic analysis of artworks by X-ray fluorescence spectroscopy, the finding that traditional batiks from different regions of Java have distinct fractal dimensions, and stimuli to mathematics research, especially Filippo Brunelleschi's theory of perspective, which eventually led to Girard Desargues's projective geometry. A persistent view, based ultimately on the Pythagorean notion of harmony in music, holds that everything was arranged by Number, that God is the geometer of the world, and that therefore the world's geometry is sacred, as seen in artworks such as William Blake's \"The Ancient of Days\".\n\nPolykleitos the elder (c.450–420 BC) was a Greek sculptor from the school of Argos, and a contemporary of Phidias. His works and statues consisted mainly of bronze and were of athletes. According to the philosopher and mathematician Xenocrates, Polykleitos is ranked as one of the most important sculptors of Classical antiquity for his work on the \"Doryphorus\" and the statue of Hera in the Heraion of Argos. While his sculptures may not be as famous as those of Phidias, they are much admired. In the \"Canon\" of Polykleitos, a treatise he wrote designed to document the \"perfect\" anatomical proportions of the male nude, Polykleitos gives us a mathematical approach towards sculpturing the human body.\n\nPolykleitos uses the distal phalanx of the little finger as the basic module for determining the proportions of the human body. Polykleitos multiplies the length of the distal phalanx by the square root of two () to get the distance of the second phalanges and multiplies the length again by to get the length of the third phalanges. Next, he takes the finger length and multiplies that by to get the length of the palm from the base of the finger to the ulna. This geometric series of measurements progresses until Polykleitos has formed the arm, chest, body, and so on.\n\nThe influence of the \"Canon\" of Polykleitos is immense in Classical Greek, Roman, and Renaissance sculpture, many sculptors following Polykleitos's prescription. While none of Polykleitos's original works survive, Roman copies demonstrate his ideal of physical perfection and mathematical precision. Some scholars argue that Pythagorean thought influenced the \"Canon\" of Polykleitos. The \"Canon\" applies the basic mathematical concepts of Greek geometry, such as the ratio, proportion, and \"symmetria\" (Greek for \"harmonious proportions\") and turns it into a system capable of describing the human form through a series of continuous geometric progressions.\n\nIn classical times, rather than making distant figures smaller with linear perspective, painters sized objects and figures according to their thematic importance. In the Middle Ages, some artists used reverse perspective for special emphasis. The Muslim mathematician Alhazen (Ibn al-Haytham) described a theory of optics in his \"Book of Optics\" in 1021, but never applied it to art. The Renaissance saw a rebirth of Classical Greek and Roman culture and ideas, among them the study of mathematics to understand nature and the arts. Two major motives drove artists in the late Middle Ages and the Renaissance towards mathematics. First, painters needed to figure out how to depict three-dimensional scenes on a two-dimensional canvas. Second, philosophers and artists alike were convinced that mathematics was the true essence of the physical world and that the entire universe, including the arts, could be explained in geometric terms.\n\nThe rudiments of perspective arrived with Giotto (1266/7 – 1337), who attempted to draw in perspective using an algebraic method to determine the placement of distant lines. In 1415, the Italian architect Filippo Brunelleschi and his friend Leon Battista Alberti demonstrated the geometrical method of applying perspective in Florence, using similar triangles as formulated by Euclid, to find the apparent height of distant objects. Brunelleschi's own perspective paintings are lost, but Masaccio's painting of the Holy Trinity shows his principles at work.\nThe Italian painter Paolo Uccello (1397–1475) was fascinated by perspective, as shown in his paintings of \"The Battle of San Romano\" (c. 1435–1460): broken lances lie conveniently along perspective lines.\n\nThe painter Piero della Francesca (c.1415–1492) exemplified this new shift in Italian Renaissance thinking. He was an expert mathematician and geometer, writing books on solid geometry and perspective, including \"De Prospectiva Pingendi (On Perspective for Painting)\", \"Trattato d'Abaco (Abacus Treatise)\", and \"De corporibus regularibus (On Regular Solids)\". The historian Vasari in his \"Lives of the Painters\" calls Piero the \"greatest geometer of his time, or perhaps of any time.\" Piero's interest in perspective can be seen in his paintings including the Polyptych of Perugia, the \"San Agostino altarpiece\" and \"The Flagellation of Christ\". His work on geometry influenced later mathematicians and artists including Luca Pacioli in his \"De Divina Proportione\" and Leonardo da Vinci. Piero studied classical mathematics and the works of Archimedes. He was taught commercial arithmetic in \"abacus schools\"; his writings are formatted like abacus school textbooks, perhaps including Leonardo Pisano (Fibonacci)'s 1202 \"Liber Abaci\". Linear perspective was just being introduced into the artistic world. Alberti explained in his 1435 \"De pictura\": \"light rays travel in straight lines from points in the observed scene to the eye, forming a kind of pyramid with the eye as vertex.\" A painting constructed with linear perspective is a cross-section of that pyramid.\n\nIn \"De Prospectiva Pingendi\", Piero transforms his empirical observations of the way aspects of a figure change with point of view into mathematical proofs. His treatise starts in the vein of Euclid: he defines the point as \"the tiniest thing that is possible for the eye to comprehend\". He uses deductive logic to lead the reader to the perspective representation of a three-dimensional body.\n\nThe artist David Hockney argued in his book \"\" that artists started using a camera lucida from the 1420s, resulting in a sudden change in precision and realism, and that this practice was continued by major artists including Ingres, Van Eyck, and Caravaggio. Critics disagree on whether Hockney was correct. Similarly, the architect Philip Steadman argued controversially that Vermeer had used a different device, the camera obscura, to help him create his distinctively observed paintings.\n\nIn 1509, Luca Pacioli (c. 1447–1517) published \"De divina proportione\" on mathematical and artistic proportion, including in the human face. Leonardo da Vinci (1452–1519) illustrated the text with woodcuts of regular solids while he studied under Pacioli in the 1490s. Leonardo's drawings are probably the first illustrations of skeletonic solids. These, such as the rhombicuboctahedron, were among the first to be drawn to demonstrate perspective by being overlaid on top of each other. The work discusses perspective in the works of Piero della Francesca, Melozzo da Forlì, and Marco Palmezzano. Da Vinci studied Pacioli's \"Summa\", from which he copied tables of proportions. In \"Mona Lisa\" and \"The Last Supper\", Da Vinci's work incorporated linear perspective with a vanishing point to provide apparent depth. \"The Last Supper\" is constructed in a tight ratio of 12:6:4:3, as is Raphael's \"The School of Athens\", which includes Pythagoras with a tablet of ideal ratios, sacred to the Pythagoreans. In \"Vitruvian Man\", Leonardo expressed the ideas of the Roman architect Vitruvius, innovatively showing the male figure twice, and centring him in both a circle and a square.\n\nAs early as the 15th century, curvilinear perspective found its way into paintings by artists interested in image distortions. Jan van Eyck's 1434 \"Arnolfini Portrait\" contains a convex mirror with reflections of the people in the scene, while Parmigianino's \"Self-portrait in a Convex Mirror\", c. 1523–1524, shows the artist's largely undistorted face at the centre, with a strongly curved background and artist's hand around the edge.\n\nThree-dimensional space can be represented convincingly in art, as in technical drawing, by means other than perspective. Oblique projections, including cavalier perspective (used by French military artists to depict fortifications in the 18th century), were used continuously and ubiquitously by Chinese artists from the first or second centuries until the 18th century. The Chinese acquired the technique from India, which acquired it from Ancient Rome. Oblique projection is seen in Japanese art, such as in the Ukiyo-e paintings of Torii Kiyonaga (1752–1815).\n\nThe golden ratio (roughly equal to 1.618) was known to Euclid. The golden ratio has persistently been claimed in modern times to have been used in art and architecture by the ancients in Egypt, Greece and elsewhere, without reliable evidence. The claim may derive from confusion with \"golden mean\", which to the Ancient Greeks meant \"avoidance of excess in either direction\", not a ratio. Pyramidologists since the nineteenth century have argued on dubious mathematical grounds for the golden ratio in pyramid design. The Parthenon, a 5th-century BC temple in Athens, has been claimed to use the golden ratio in its façade and floor plan, but these claims too are disproved by measurement. The Great Mosque of Kairouan in Tunisia has similarly been claimed to use the golden ratio in its design, but the ratio does not appear in the original parts of the mosque. The historian of architecture Frederik Macody Lund argued in 1919 that the Cathedral of Chartres (12th century), Notre-Dame of Laon (1157–1205) and Notre Dame de Paris (1160) are designed according to the golden ratio, drawing regulator lines to make his case. Other scholars argue that until Pacioli's work in 1509, the golden ratio was unknown to artists and architects. For example, the height and width of the front of Notre-Dame of Laon have the ratio 8/5 or 1.6, not 1.618. Such Fibonacci ratios quickly become hard to distinguish from the golden ratio. After Pacioli, the golden ratio is more definitely discernible in artworks including Leonardo's \"Mona Lisa\".\n\nAnother ratio, the only other morphic number, was named the plastic number in 1928 by the Dutch architect Hans van der Laan (originally named \"le nombre radiant\" in French). Its value is the solution of the cubic equation\n\nan irrational number which is approximately 1.325. According to the architect Richard Padovan, this has characteristic ratios and , which govern the limits of human perception in relating one physical size to another. Van der Laan used these ratios when designing the 1967 St. Benedictusberg Abbey church in the Netherlands.\n\nPlanar symmetries have for millennia been exploited in artworks such as carpets, lattices, textiles and tilings.\n\nMany traditional rugs, whether pile carpets or flatweave kilims, are divided into a central field and a framing border; both can have symmetries, though in handwoven carpets these are often slightly broken by small details, variations of pattern and shifts in colour introduced by the weaver. In kilims from Anatolia, the motifs used are themselves usually symmetrical. The general layout, too, is usually present, with arrangements such as stripes, stripes alternating with rows of motifs, and packed arrays of roughly hexagonal motifs. The field is commonly laid out as a wallpaper with a wallpaper group such as pmm, while the border may be laid out as a frieze of frieze group pm11, pmm2 or pma2. Turkish and Central Asian kilims often have three or more borders in different frieze groups. Weavers certainly had the intention of symmetry, without explicit knowledge of its mathematics.\nThe mathematician and architectural theorist Nikos Salingaros suggests that the \"powerful presence\" (aesthetic effect) of a \"great carpet\" such as the best Konya two-medallion carpets of the 17th century is created by mathematical techniques related to the theories of the architect Christopher Alexander. These techniques include making opposites couple; opposing colour values; differentiating areas geometrically, whether by using complementary shapes or balancing the directionality of sharp angles; providing small-scale complexity (from the knot level upwards) and both small- and large-scale symmetry; repeating elements at a hierarchy of different scales (with a ratio of about 2.7 from each level to the next). Salingaros argues that \"all successful carpets satisfy at least nine of the above ten rules\", and suggests that it might be possible to create a metric from these rules.\n\nElaborate lattices are found in Indian Jali work, carved in marble to adorn tombs and palaces. Chinese lattices, always with some symmetry, exist in 14 of the 17 wallpaper groups; they often have mirror, double mirror, or rotational symmetry. Some have a central medallion, and some have a border in a frieze group. Many Chinese lattices have been analysed mathematically by Daniel S. Dye; he identifies Sichuan as the centre of the craft.\n\nSymmetries are prominent in textile arts including quilting, knitting, cross-stitch, crochet, embroidery and weaving, where they may be purely decorative or may be marks of status. Rotational symmetry is found in circular structures such as domes; these are sometimes elaborately decorated with symmetric patterns inside and out, as at the 1619 Sheikh Lotfollah Mosque in Isfahan. Items of embroidery and lace work such as tablecloths and table mats, made using bobbins or by tatting, can have a wide variety of reflectional and rotational symmetries which are being explored mathematically.\n\nIslamic art exploits symmetries in many of its artforms, notably in girih tilings. These are formed using a set of five tile shapes, namely a regular decagon, an elongated hexagon, a bow tie, a rhombus, and a regular pentagon. All the sides of these tiles have the same length; and all their angles are multiples of 36° (π/5 radians), offering fivefold and tenfold symmetries. The tiles are decorated with strapwork lines (girih), generally more visible than the tile boundaries. In 2007, the physicists Peter Lu and Paul Steinhardt argued that girih resembled quasicrystalline Penrose tilings. Elaborate geometric zellige tilework is a distinctive element in Moroccan architecture. Muqarnas vaults are three-dimensional but were designed in two dimensions with drawings of geometrical cells.\n\nThe Platonic solids and other polyhedra are a recurring theme in Western art. They are found, for instance, in a marble mosaic featuring the small stellated dodecahedron, attributed to Paolo Uccello, in the floor of the San Marco Basilica in Venice; in Leonardo da Vinci's diagrams of regular polyhedra drawn as illustrations for Luca Pacioli's 1509 book \"The Divine Proportion\"; as a glass rhombicuboctahedron in Jacopo de Barbari's portrait of Pacioli, painted in 1495; in the truncated polyhedron (and various other mathematical objects) in Albrecht Dürer's engraving Melencolia I; and in Salvador Dalí's painting \"The Last Supper\" in which Christ and his disciples are pictured inside a giant dodecahedron.\n\nAlbrecht Dürer (1471–1528) was a German Renaissance printmaker who made important contributions to polyhedral literature in his 1525 book, \"Underweysung der Messung (Education on Measurement)\", meant to teach the subjects of linear perspective, geometry in architecture, Platonic solids, and regular polygons. Dürer was likely influenced by the works of Luca Pacioli and Piero della Francesca during his trips to Italy. While the examples of perspective in \"Underweysung der Messung\" are underdeveloped and contain inaccuracies, there is a detailed discussion of polyhedra. Dürer is also the first to introduce in text the idea of polyhedral nets, polyhedra unfolded to lie flat for printing. Dürer published another influential book on human proportions called \"Vier Bücher von Menschlicher Proportion (Four Books on Human Proportion)\" in 1528.\n\nDürer's well-known engraving \"Melencolia I\" depicts a frustrated thinker sitting by a truncated triangular trapezohedron and a magic square. These two objects, and the engraving as a whole, have been the subject of more modern interpretation than the contents of almost any other print, including a two-volume book by Peter-Klaus Schuster, and an influential discussion in Erwin Panofsky's monograph of Dürer.\nSalvador Dalí's \"Corpus Hypercubus\" depicts an unfolded three-dimensional net for a hypercube, a four-dimensional regular polyhedron.\n\nTraditional Indonesian wax-resist batik designs on cloth combine representational motifs (such as floral and vegetal elements) with abstract and somewhat chaotic elements, including imprecision in applying the wax resist, and random variation introduced by cracking of the wax. Batik designs have a fractal dimension between 1 and 2, varying in different regional styles. For example, the batik of Cirebon has a fractal dimension of 1.1; the batiks of Yogyakarta and Surakarta (Solo) in Central Java have a fractal dimension of 1.2 to 1.5; and the batiks of Lasem on the north coast of Java and of Tasikmalaya in West Java have a fractal dimension between 1.5 and 1.7.\n\nThe drip painting works of the modern artist Jackson Pollock are similarly distinctive in their fractal dimension. His 1948 \"Number 14\" has a coastline-like dimension of 1.45, while his later paintings had successively higher fractal dimensions and accordingly more elaborate patterns. One of his last works, \"Blue Poles\", took six months to create, and has the fractal dimension of 1.72.\n\nThe astronomer Galileo Galilei in his \"Il Saggiatore\" wrote that \"[The universe] is written in the language of mathematics, and its characters are triangles, circles, and other geometric figures.\" Artists who strive and seek to study nature must first, in Galileo's view, fully understand mathematics. Mathematicians, conversely, have sought to interpret and analyse art through the lens of geometry and rationality. The mathematician Felipe Cucker suggests that mathematics, and especially geometry, is a source of rules for \"rule-driven artistic creation\", though not the only one. Some of the many strands of the resulting complex relationship are described below.\n\nThe mathematician Jerry P. King describes mathematics as an art, stating that \"the keys to mathematics are beauty and elegance and not dullness and technicality\", and that beauty is the motivating force for mathematical research. King cites the mathematician G. H. Hardy's 1940 essay \"A Mathematician's Apology\". In it, Hardy discusses why he finds two theorems of classical times as first rate, namely Euclid's proof there are infinitely many prime numbers, and the proof that the square root of 2 is irrational. King evaluates this last against Hardy's criteria for mathematical elegance: \"\"seriousness, depth, generality, unexpectedness, inevitability\", and \"economy\"\" (King's italics), and describes the proof as \"aesthetically pleasing\". The Hungarian mathematician Paul Erdős agreed that mathematics possessed beauty but considered the reasons beyond explanation: \"Why are numbers beautiful? It's like asking why is Beethoven's Ninth Symphony beautiful. If you don't see why, someone can't tell you. I \"know\" numbers are beautiful.\"\n\nMathematics can be discerned in many of the arts, such as music, dance, painting, architecture, and sculpture. Each of these is richly associated with mathematics. Among the connections to the visual arts, mathematics can provide tools for artists, such as the rules of linear perspective as described by Brook Taylor and Johann Lambert, or the methods of descriptive geometry, now applied in software modelling of solids, dating back to Albrecht Dürer and Gaspard Monge. Artists from Luca Pacioli in the Middle Ages and Leonardo da Vinci and Albrecht Dürer in the Renaissance have made use of and developed mathematical ideas in the pursuit of their artistic work. The use of perspective began, despite some embryonic usages in the architecture of Ancient Greece, with Italian painters such as Giotto in the 13th century; rules such as the vanishing point were first formulated by Brunelleschi in about 1413, his theory influencing Leonardo and Dürer. Isaac Newton's work on the optical spectrum influenced Goethe's \"Theory of Colours\" and in turn artists such as Philipp Otto Runge, J. M. W. Turner, the Pre-Raphaelites and Wassily Kandinsky. Artists may also choose to analyse the symmetry of a scene. Tools may be applied by mathematicians who are exploring art, or artists inspired by mathematics, such as M. C. Escher (inspired by H. S. M. Coxeter) and the architect Frank Gehry, who more tenuously argued that computer aided design enabled him to express himself in a wholly new way.\n\nThe artist Richard Wright argues that mathematical objects that can be constructed can be seen either \"as processes to simulate phenomena\" or as works of \"computer art\". He considers the nature of mathematical thought, observing that fractals were known to mathematicians for a century before they were recognised as such. Wright concludes by stating that it is appropriate to subject mathematical objects to any methods used to \"come to terms with cultural artifacts like art, the tension between objectivity and subjectivity, their metaphorical meanings and the character of representational systems.\" He gives as instances an image from the Mandelbrot set, an image generated by a cellular automaton algorithm, and a computer-rendered image, and discusses, with reference to the Turing test, whether algorithmic products can be art. Sasho Kalajdzievski's \"Math and Art: An Introduction to Visual Mathematics\" takes a similar approach, looking at suitably visual mathematics topics such as tilings, fractals and hyperbolic geometry.\n\nSome of the first works of computer art were created by Desmond Paul Henry's \"Drawing Machine 1\", an analogue machine based on a bombsight computer and exhibited in 1962. The machine was capable of creating complex, abstract, asymmetrical, curvilinear, but repetitive line drawings. More recently, Hamid Naderi Yeganeh has created shapes suggestive of real world objects such as fish and birds, using formulae that are successively varied to draw families of curves or angled lines. Artists such as Mikael Hvidtfeldt Christensen create works of generative or algorithmic art by writing scripts for a software system such as \"Structure Synth\": the artist effectively directs the system to apply a desired combination of mathematical operations to a chosen set of data.\n\nThe mathematician and theoretical physicist Henri Poincaré's \"Science and Hypothesis\" was widely read by the Cubists, including Pablo Picasso and Jean Metzinger. Poincaré viewed Euclidean geometry as just one of many possible geometric configurations, rather than as an absolute objective truth. The possible existence of a fourth dimension inspired artists to question classical : non-Euclidean geometry became a valid alternative. The concept that painting could be expressed mathematically, in colour and form, contributed to Cubism, the art movement that led to abstract art. Metzinger, in 1910, wrote that: \"[Picasso] lays out a free, mobile perspective, from which that ingenious mathematician Maurice Princet has deduced a whole geometry\". Later, Metzinger wrote in his memoirs:\n\nMaurice Princet joined us often ... it was as an artist that he conceptualized mathematics, as an aesthetician that he invoked \"n\"-dimensional continuums. He loved to get the artists interested in the new views on space that had been opened up by Schlegel and some others. He succeeded at that.\n\nThe impulse to make teaching or research models of mathematical forms naturally creates objects that have symmetries and surprising or pleasing shapes. Some of these have inspired artists such as the Dadaists Man Ray, Marcel Duchamp and Max Ernst, and following Man Ray, Hiroshi Sugimoto.\nMan Ray photographed some of the mathematical models in the Institut Henri Poincaré in Paris, including \"Objet mathematique\" (Mathematical object). He noted that this represented Enneper surfaces with constant negative curvature, derived from the pseudo-sphere. This mathematical foundation was important to him, as it allowed him to deny that the object was \"abstract\", instead claiming that it was as real as the urinal that Duchamp made into a work of art. Man Ray admitted that the object's [Enneper surface] formula \"meant nothing to me, but the forms themselves were as varied and authentic as any in nature.\" He used his photographs of the mathematical models as figures in his series he did on Shakespeare's plays, such as his 1934 painting \"Antony and Cleopatra\". The art reporter Jonathan Keats, writing in \"ForbesLife\", argues that Man Ray photographed \"the elliptic paraboloids and conic points in the same sensual light as his pictures of Kiki de Montparnasse\", and \"ingeniously repurposes the cool calculations of mathematics to reveal the topology of desire\". Twentieth century sculptors such as Henry Moore, Barbara Hepworth and Naum Gabo took inspiration from mathematical models. Moore wrote of his 1938 \"Stringed Mother and Child\": \"Undoubtedly the source of my stringed figures was the Science Museum ... I was fascinated by the mathematical models I saw there ... It wasn't the scientific study of these models but the ability to look through the strings as with a bird cage and to see one form within another which excited me.\"\n\nThe artists Theo van Doesburg and Piet Mondrian founded the De Stijl movement, which they wanted to \"establish a visual vocabulary elementary geometrical forms comprehensible by all and adaptable to any discipline\". Many of their artworks visibly consist of ruled squares and triangles, sometimes also with circles. De Stijl artists worked in painting, furniture, interior design and architecture. After the breakup of De Stijl, Van Doesburg founded the Avant-garde Art Concret movement, describing his 1929–1930 \"Arithmetic Composition\", a series of four black squares on the diagonal of a squared background, as \"a structure that can be controlled, a \"definite\" surface without chance elements or individual caprice\", yet \"not lacking in spirit, not lacking the universal and not ... empty as there is \"everything\" which fits the internal rhythm\". The art critic Gladys Fabre observes that two progressions are at work in the painting, namely the growing black squares and the alternating backgrounds.\n\nThe mathematics of tessellation, polyhedra, shaping of space, and self-reference provided the graphic artist M. C. Escher (1898—1972) with a lifetime's worth of materials for his woodcuts. In the \"Alhambra Sketch\", Escher showed that art can be created with polygons or regular shapes such as triangles, squares, and hexagons. Escher used irregular polygons when tiling the plane and often used reflections, glide reflections, and translations to obtain further patterns. Many of his works contain impossible constructions, made using geometrical objects which set up a contradiction between perspective projection and three dimensions, but are pleasant to the human sight. Escher's \"Ascending and Descending\" is based on the \"impossible staircase\" created by the medical scientist Lionel Penrose and his son the mathematician Roger Penrose.\n\nSome of Escher's many tessellation drawings were inspired by conversations with the mathematician H. S. M. Coxeter on hyperbolic geometry. Escher was especially interested in five specific polyhedra, which appear many times in his work. The Platonic solids—tetrahedrons, cubes, octahedrons, dodecahedrons, and icosahedrons—are especially prominent in \"Order and Chaos\" and \"Four Regular Solids\". These stellated figures often reside within another figure which further distorts the viewing angle and conformation of the polyhedrons and provides a multifaceted perspective artwork.\n\nThe visual intricacy of mathematical structures such as tessellations and polyhedra have inspired a variety of mathematical artworks. Stewart Coffin makes polyhedral puzzles in rare and beautiful woods; George W. Hart works on the theory of polyhedra and sculpts objects inspired by them; Magnus Wenninger makes \"especially beautiful\" models of complex stellated polyhedra.\n\nThe distorted perspectives of anamorphosis have been explored in art since the sixteenth century, when Hans Holbein the Younger incorporated a severely distorted skull in his 1533 painting \"The Ambassadors\". Many artists since then, including Escher, have make use of anamorphic tricks.\n\nThe mathematics of topology has inspired several artists in modern times. The sculptor John Robinson (1935–2007) created works such as \"Gordian Knot\" and \"Bands of Friendship\", displaying knot theory in polished bronze. Other works by Robinson explore the topology of toruses. \"Genesis\" is based on Borromean rings – a set of three circles, no two of which link but in which the whole structure cannot be taken apart without breaking. The sculptor Helaman Ferguson creates complex surfaces and other topological objects. His works are visual representations of mathematical objects; \"The Eightfold Way\" is based on the projective special linear group PSL(2,7), a finite group of 168 elements. The sculptor Bathsheba Grossman similarly bases her work on mathematical structures.\n\nA liberal arts inquiry project examines connections between mathematics and art through the Möbius strip, flexagons, origami and panorama photography.\n\nMathematical objects including the Lorenz manifold and the hyperbolic plane have been crafted using fiber arts including crochet. The American weaver Ada Dietz wrote a 1949 monograph \"Algebraic Expressions in Handwoven Textiles\", defining weaving patterns based on the expansion of multivariate polynomials. The mathematician J. C. P. Miller used the Rule 90 cellular automaton to design tapestries depicting both trees and abstract patterns of triangles. The \"mathekniticians\" Pat Ashforth and Steve Plummer use knitted versions of mathematical objects such as hexaflexagons in their teaching, though their Menger sponge proved too troublesome to knit and was made of plastic canvas instead. Their \"mathghans\" (Afghans for Schools) project introduced knitting into the British mathematics and technology curriculum.\n\nModelling is far from the only possible way to illustrate mathematical concepts. Giotto's \"Stefaneschi Triptych\", 1320, illustrates recursion in the form of \"mise en abyme\"; the central panel of the triptych contains, lower left, the kneeling figure of Cardinal Stefaneschi, holding up the triptych as an offering. Giorgio Chirico's metaphysical paintings such as his 1917 \"Great Metaphysical Interior\" explore the question of levels of representation in art by depicting paintings within his paintings.\n\nArt can exemplify logical paradoxes, as in some paintings by the surrealist René Magritte, which can be read as semiotic jokes about confusion between levels. In \"La condition humaine\" (1933), Magritte depicts an easel (on the real canvas), seamlessly supporting a view through a window which is framed by \"real\" curtains in the painting. Similarly, Escher's \"Print Gallery\" (1956) is a print which depicts a distorted city which contains a gallery which recursively contains the picture, and so \"ad infinitum\". Magritte made use of spheres and cuboids to distort reality in a different way, painting them alongside an assortment of houses in his 1931 \"Mental Arithmetic\" as if they were children's building blocks, but house-sized. \"The Guardian\" observed that the \"eerie toytown image\" prophesied Modernism's usurpation of \"cosy traditional forms\", but also plays with the human tendency to seek patterns in nature.\n\nSalvador Dalí's last painting, \"The Swallow's Tail\" (1983), was part of a series inspired by René Thom's catastrophe theory. The Spanish painter and sculptor Pablo Palazuelo (1916–2007) focused on the investigation of form. He developed a style that he described as the geometry of life and the geometry of all nature. Consisting of simple geometric shapes with detailed patterning and coloring, in works such as \"Angular I\" and \"Automnes\", Palazuelo expressed himself in geometric transformations.\n\nThe artist Adrian Gray practises stone balancing, exploiting friction and the centre of gravity to create striking and seemingly impossible compositions.\n\nArtists, however, do not necessarily take geometry literally. As Douglas Hofstadter writes in his 1980 reflection on human thought, \"Gödel, Escher, Bach\", by way of (among other things) the mathematics of art: \"The difference between an Escher drawing and non-Euclidean geometry is that in the latter, comprehensible interpretations can be found for the undefined terms, resulting in a comprehensible total system, whereas for the former, the end result is not reconcilable with one's conception of the world, no matter how long one stares at the pictures.\" Hofstadter discusses the seemingly paradoxical lithograph \"Print Gallery\" by M. C. Escher; it depicts a seaside town containing an art gallery which seems to contain a painting of the seaside town, there being a \"strange loop, or tangled hierarchy\" to the levels of reality in the image. The artist himself, Hofstadter observes, is not seen; his reality and his relation to the lithograph are not paradoxical. The image's central void has also attracted the interest of mathematicians Bart de Smit and Hendrik Lenstra, who propose that it could contain a Droste effect copy of itself, rotated and shrunk; this would be a further illustration of recursion beyond that noted by Hofstadter.\n\nAlgorithmic analysis of images of artworks, for example using X-ray fluorescence spectroscopy, can reveal information about art. Such techniques can uncover images in layers of paint later covered over by an artist; help art historians to visualize an artwork before it cracked or faded; help to tell a copy from an original, or distinguish the brushstroke style of a master from those of his apprentices.\nJackson Pollock's drip painting style has a definite fractal dimension; among the artists who may have influenced Pollock's controlled chaos, Max Ernst painted Lissajous figures directly by swinging a punctured bucket of paint over a canvas.\n\nThe computer scientist Neil Dodgson investigated whether Bridget Riley's stripe paintings could be characterised mathematically, concluding that while separation distance could \"provide some characterisation\" and global entropy worked on some paintings, autocorrelation failed as Riley's patterns were irregular. Local entropy worked best, and correlated well with the description given by the art critic Robert Kudielka.\n\nThe American mathematician George Birkhoff's 1933 \"Aesthetic Measure\" proposes a quantitative metric of the aesthetic quality of an artwork. It does not attempt to measure the connotations of a work, such as what a painting might mean, but is limited to the \"elements of order\" of a polygonal figure. Birkhoff first combines (as a sum) five such elements: whether there is a vertical axis of symmetry; whether there is optical equilibrium; how many rotational symmetries it has; how wallpaper-like the figure is; and whether there are unsatisfactory features such as having two vertices too close together. This metric, \"O\", takes a value between −3 and 7. The second metric, \"C\", counts elements of the figure, which for a polygon is the number of different straight lines containing at least one of its sides. Birkhoff then defines his aesthetic measure of an object's beauty as \"O/C\". This can be interpreted as a balance between the pleasure looking at the object gives, and the amount of effort needed to take it in. Birkhoff's proposal has been criticized in various ways, not least for trying to put beauty in a formula, but he never claimed to have done that.\n\nArt has sometimes stimulated the development of mathematics, as when Brunelleschi's theory of perspective in architecture and painting started a cycle of research that led to the work of Brook Taylor and Johann Heinrich Lambert on the mathematical foundations of perspective drawing, and ultimately to the mathematics of projective geometry of Girard Desargues and Jean-Victor Poncelet.\n\nThe Japanese paper-folding art of origami has been reworked mathematically by Tomoko Fusé using modules, congruent pieces of paper such as squares, and making them into polyhedra or tilings. Paper-folding was used in 1893 by T. Sundara Rao in his \"Geometric Exercises in Paper Folding\" to demonstrate geometrical proofs. The mathematics of paper folding has been explored in Maekawa's theorem, Kawasaki's theorem, and the Huzita–Hatori axioms.\n\nOptical illusions such as the Fraser spiral strikingly demonstrate limitations in human visual perception, creating what the art historian Ernst Gombrich called a \"baffling trick.\" The black and white ropes that appear to form spirals are in fact concentric circles. The mid-twentieth century Op art or optical art style of painting and graphics exploited such effects to create the impression of movement and flashing or vibrating patterns seen in the work of artists such as Bridget Riley, Spyros Horemis, and Victor Vasarely.\n\nA strand of art from Ancient Greece onwards sees God as the geometer of the world, and the world's geometry therefore as sacred. The belief that God created the universe according to a geometric plan has ancient origins. Plutarch attributed the belief to Plato, writing that \"Plato said God geometrizes continually\" (\"Convivialium disputationum\", liber 8,2). This image has influenced Western thought ever since. The Platonic concept derived in its turn from a Pythagorean notion of harmony in music, where the notes were spaced in perfect proportions, corresponding to the lengths of the lyre's strings; indeed, the Pythagoreans held that everything was arranged by Number. In the same way, in Platonic thought, the regular or Platonic solids dictate the proportions found in nature, and in art. A Mediaeval manuscript illustration may refer to a verse in the Old Testament: \"When he established the heavens I was there: when he set a compass upon the face of the deep\" (Proverbs 8:27), showing God drawing out the universe with a pair of compasses. In 1596, the mathematical astronomer Johannes Kepler modelled the universe as a set of nested Platonic solids, determining the relative sizes of the orbits of the planets. William Blake's \"Ancient of Days\" and his painting of the physicist Isaac Newton, naked and drawing with a compass, attempt to depict the contrast between the mathematically perfect spiritual world and the imperfect physical world, as in a different way does Salvador Dalí's 1954 \"Crucifixion (Corpus Hypercubus)\", which depicts the cross as a hypercube, representing the divine perspective with four dimensions rather than the usual three. In Dali's \"The Sacrament of the Last Supper\" (1955) Christ and his disciples are pictured inside a giant dodecahedron.\n\n\n"}
{"id": "500004", "url": "https://en.wikipedia.org/wiki?curid=500004", "title": "On-Line Encyclopedia of Integer Sequences", "text": "On-Line Encyclopedia of Integer Sequences\n\nThe On-Line Encyclopedia of Integer Sequences (OEIS), also cited simply as Sloane's, is an online database of integer sequences. It was created and maintained by Neil Sloane while a researcher at AT&T Labs. Foreseeing his retirement from AT&T Labs in 2012 and the need for an independent foundation, Sloane agreed to transfer the intellectual property and hosting of the OEIS to the OEIS Foundation in October 2009. Sloane continues to be involved in the OEIS in his role as President of the OEIS Foundation.\n\nOEIS records information on integer sequences of interest to both professional mathematicians and amateurs, and is widely cited. it contains over 300,000 sequences, making it the largest database of its kind.\n\nEach entry contains the leading terms of the sequence, keywords, mathematical motivations, literature links, and more, including the option to generate a graph or play a musical representation of the sequence. The database is searchable by keyword and by subsequence.\n\nNeil Sloane started collecting integer sequences as a graduate student in 1965 to support his work in combinatorics. The database was at first stored on punched cards. He published selections from the database in book form twice:\nThese books were well received and, especially after the second publication, mathematicians supplied Sloane with a steady flow of new sequences. The collection became unmanageable in book form, and when the database had reached 16,000 entries Sloane decided to go online—first as an e-mail service (August 1994), and soon after as a web site (1996). As a spin-off from the database work, Sloane founded the \"Journal of Integer Sequences\" in 1998.\nThe database continues to grow at a rate of some 10,000 entries a year.\nSloane has personally managed 'his' sequences for almost 40 years, but starting in 2002, a board of associate editors and volunteers has helped maintain the database.\nIn 2004, Sloane celebrated the addition of the 100,000th sequence to the database, , which counts the marks on the Ishango bone. In 2006, the user interface was overhauled and more advanced search capabilities were added. In 2010 an OEIS wiki at OEIS.org was created to simplify the collaboration of the OEIS editors and contributors. The 200,000th sequence, , was added to the database in November 2011; it was initially entered as A200715, and moved to A200000 after a week of discussion on the SeqFan mailing list, following a proposal by OEIS Editor-in-Chief Charles Greathouse to choose a special sequence for A200000.\n\nBesides integer sequences, the OEIS also catalogs sequences of fractions, the digits of transcendental numbers, complex numbers and so on by transforming them into integer sequences.\nSequences of rationals are represented by two sequences (named with the keyword 'frac'): the sequence of numerators and the sequence of denominators. For example, the fifth order Farey sequence, formula_1, is catalogued as the numerator sequence 1, 1, 1, 2, 1, 3, 2, 3, 4 () and the denominator sequence 5, 4, 3, 5, 2, 5, 3, 4, 5 ().\nImportant irrational numbers such as π = 3.1415926535897... are catalogued under representative integer sequences such as decimal expansions (here 3, 1, 4, 1, 5, 9, 2, 6, 5, 3, 5, 8, 9, 7, 9, 3, 2, 3, 8, 4, 6, 2, 6, 4, 3, 3, 8, 3, 2, 7, 9, 5, 0, 2, 8, 8, ... ()), binary expansions (here 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, ... ()), or continued fraction expansions (here 3, 7, 15, 1, 292, 1, 1, 1, 2, 1, 3, 1, 14, 2, 1, 1, 2, 2, 2, 2, 1, 84, 2, 1, 1, ... ()).\n\nThe OEIS was limited to plain ASCII text until 2011, and it still uses a linear form of conventional mathematical notation (such as \"f\"(\"n\") for functions, \"n\" for running variables, etc.). Greek letters are usually represented by their full names, \"e.g.\", mu for μ, phi for φ.\nEvery sequence is identified by the letter A followed by six digits, almost always referred to with leading zeros, \"e.g.\", A000315 rather than A315.\nIndividual terms of sequences are separated by commas. Digit groups are not separated by commas, periods, or spaces.\nIn comments, formulas, etc., a(n) represents the \"n\"th term of the sequence.\n\nZero is often used to represent non-existent sequence elements. For example, enumerates the \"smallest prime of \"n\"² consecutive primes to form an \"n\"×\"n\" magic square of least magic constant, or 0 if no such magic square exists.\" The value of \"a\"(1) (a 1×1 magic square) is 2; \"a\"(3) is 1480028129. But there is no such 2×2 magic square, so \"a\"(2) is 0.\nThis special usage has a solid mathematical basis in certain counting functions. For example, the totient valence function \"N\"(\"m\") () counts the solutions of φ(\"x\") = \"m\". There are 4 solutions for 4, but no solutions for 14, hence \"a\"(14) of A014197 is 0—there are no solutions.\nOccasionally −1 is used for this purpose instead, as in .\n\nThe OEIS maintains the lexicographical order of the sequences, so each sequence has a predecessor and a successor (its \"context\"). OEIS normalizes the sequences for lexicographical ordering, (usually) ignoring all initial zeros and ones, and also the sign of each element. Sequences of weight distribution codes often omit periodically recurring zeros.\n\nFor example, consider: the prime numbers, the palindromic primes, the Fibonacci sequence, the lazy caterer's sequence, and the coefficients in the series expansion of formula_2. In OEIS lexicographic order, they are:\nwhereas unnormalized lexicographic ordering would order these sequences thus: #3, #5, #4, #1, #2.\n\nVery early in the history of the OEIS, sequences defined in terms of the numbering of sequences in the OEIS itself were proposed. \"I resisted adding these sequences for a long time, partly out of a desire to maintain the dignity of the database, and partly because A22 was only known to 11 terms!\", Sloane reminisced.\nOne of the earliest self-referential sequences Sloane accepted into the OEIS was (later ) \"\"a\"(\"n\") = \"n\"-th term of sequence A or -1 if A has fewer than n terms\". This sequence spurred progress on finding more terms of . \nThis line of thought leads to the question \"Does sequence A contain the number \"n\" ?\" and the sequences , \"Numbers \"n\" such that OEIS sequence A contains \"n\", and , \"n\" is in this sequence if and only if \"n\" is not in sequence A\". Thus, the composite number 2808 is in A053873 because is the sequence of composite numbers, while the non-prime 40 is in A053169 because it's not in , the prime numbers. Each \"n\" is a member of exactly one of these two sequences, and in principle it can be determined \"which\" sequence each \"n\" belongs to, with two exceptions (related to the two sequences themselves):\n\nThis entry, , was chosen because it contains every field an OEIS entry can have.\n\n\nIn 2009, the OEIS database was used by an amateur mathematician to measure the \"importance\" of each integer number. The result shown in the plot on the right shows a clear \"gap\" between two distinct point clouds the \"uninteresting numbers\" (blue dots) and the \"interesting\" numbers that occur comparatively more often in sequences from the OEIS. It contains essentially prime numbers (red), numbers of the form a^n (green) and highly composite numbers (yellow). This phenomenon was studied by Nicolas Gauvrit, Jean-Paul Delahaye and Hector Zenil who explained the speed of the 2 clouds in terms of algorithmic complexity and the gap by social factors based on an artificial preference for sequences of primes, even numbers, geometric and Fibonacci-type sequences and so on. Sloane's gap was featured on a Numberphile video.\n\n\n\n\n"}
{"id": "145865", "url": "https://en.wikipedia.org/wiki?curid=145865", "title": "Parts-per notation", "text": "Parts-per notation\n\nIn science and engineering, the parts-per notation is a set of pseudo-units to describe small values of miscellaneous dimensionless quantities, e.g. mole fraction or mass fraction. Since these fractions are quantity-per-quantity measures, they are pure numbers with no associated units of measurement. Commonly used are ppm (parts-per-million, ), ppb (parts-per-billion, ), ppt (parts-per-trillion, ) and ppq (parts-per-quadrillion, ). This notation is not part of the SI system and its meaning is ambiguous.\n\nParts-per notation is often used describing dilute solutions in chemistry, for instance, the relative abundance of dissolved minerals or pollutants in water. The quantity “1 ppm” can be used for a mass fraction if a water-borne pollutant is present at one-millionth of a gram per gram of sample solution. When working with aqueous solutions, it is common to assume that the density of water is 1.00 g/mL. Therefore, it is common to equate 1 kilogram of water with 1 L of water. Consequently, 1 ppm corresponds to 1 mg/L and 1 ppb corresponds to 1 μg/L.\n\nSimilarly, parts-per notation is used also in physics and engineering to express the value of various proportional phenomena. For instance, a special metal alloy might expand 1.2 micrometers per meter of length for every degree Celsius and this would be expressed as “\"α\" = 1.2 ppm/°C.” Parts-per notation is also employed to denote the change, stability, or uncertainty in measurements. For instance, the accuracy of land-survey distance measurements when using a laser rangefinder might be 1 millimeter per kilometer of distance; this could be expressed as “Accuracy = 1 ppm.”\n\nParts-per notations are all dimensionless quantities: in mathematical expressions, the units of measurement always cancel. In fractions like “2 nanometers per meter” (2 nm/m = 2 nano = 2 × 10 = 2 ppb = 2 × 0.000000001) so the quotients are pure-number coefficients with positive values less than 1. When parts-per notations, including the percent symbol (%), are used in regular prose (as opposed to mathematical expressions), they are still pure-number dimensionless quantities. However, they generally take the literal “parts per” meaning of a comparative ratio (e.g., “2 ppb” would generally be interpreted as “two parts in a billion parts”).\n\nParts-per notations may be expressed in terms of any unit of the same measure. For instance, the coefficient of thermal expansion of a certain brass alloy, \"α\" = 18.7 ppm/°C, may be expressed as 18.7 (µm/m)/°C, or as 18.7 (µin/in)/°C; the numeric value representing a relative proportion does not change with the adoption of a different unit of measure. Similarly, a metering pump that injects a trace chemical into the main process line at the proportional flow rate \"Q\" = 125 ppm, is doing so at a rate that may be expressed in a variety of volumetric units, including 125 µL/L, 125 µgal/gal, 125 cm/m, etc.\n\nIn nuclear magnetic resonance spectroscopy (NMR), chemical shift is usually expressed in ppm. It represents the difference of a measured frequency in parts per million from the reference frequency. The reference frequency depends on the instrument's magnetic field and the element being measured. It is usually expressed in MHz. Typical chemical shifts are rarely more than a few hundred Hz from the reference frequency, so chemical shifts are conveniently expressed in ppm (Hz/MHz). Parts-per notation gives a dimensionless quantity that does not depend on the instrument's field strength.\n\n\n\n\n\n\nAlthough the International Bureau of Weights and Measures (an international standards organization known also by its French-language initials BIPM) recognizes the use of parts-per notation, it is not formally part of the International System of Units (SI). Note that although “percent” (%) is not formally part of the SI, both the BIPM and the ISO take the position that \"“in mathematical expressions, the internationally recognized symbol % (percent) may be used with the SI to represent the number 0.01”\" for dimensionless quantities. According to IUPAP, \"“a continued source of annoyance to unit purists has been the continued use of percent, ppm, ppb, and ppt.”\" Although SI-compliant expressions should be used as an alternative, the parts-per notation remains nevertheless widely used in technical disciplines. The main problems with the parts-per notation are the following:\n\nBecause the named numbers starting with a “billion” have different values in different countries, the BIPM suggests avoiding the use of “ppb” and “ppt” to prevent misunderstanding. The U.S. National Institute of Standards and Technology (NIST) takes the stringent position, stating that \"“the language-dependent terms [ . . . ] are not acceptable for use with the SI to express the values of quantities.”\"\n\nAlthough \"ppt\" usually means \"parts per trillion\", it occasionally means \"parts per thousand\". Unless the meaning of \"ppt\" is defined explicitly, it has to be determined from the context.\n\nAnother problem of the parts-per notation is that it may refer to mass fraction, mole fraction or volume fraction. Since it is usually not stated which quantity is used, it is better to write the unit as kg/kg, mol/mol or m/m (even though they are all dimensionless). The difference is quite significant when dealing with gases and it is very important to specify which quantity is being used. For example, the conversion factor between a mass fraction of 1 ppb and a mole fraction of 1 ppb is about 4.7 for the greenhouse gas CFC-11 in air. For volume fraction, the suffix \"V\" or \"v\" is sometimes appended to the parts-per notation (e.g., ppmV, ppbv, pptv). Unfortunately, ppbv and pptv are also often used for mole fractions (which is identical to volume fraction only for ideal gases).\n\nTo distinguish the mass fraction from volume fraction or mole fraction, the letter w (standing for weight) is sometimes added to the abbreviation (e.g., ppmw, ppbw).\n\nThe usage of the parts-per notation is generally quite fixed inside most specific branches of science, leading some researchers to draw the conclusion that their own usage (mass/mass, mol/mol, volume/volume, or others) is the only correct one. This, in turn, leads them to not specify their usage in their publications, and others may therefore misinterpret their results. For example, electrochemists often use volume/volume, while chemical engineers may use mass/mass as well as volume/volume. Many academic papers of otherwise excellent level fail to specify their usage of the parts-per notation.\n\nSI-compliant units that can be used as alternatives are shown in the chart below. Expressions that the BIPM explicitly does not recognize as being suitable for denoting dimensionless quantities with the SI are shown in red text.\n\nNote that the notations in the “SI units” column above are all dimensionless quantities; that is, the units of measurement factor out in expressions like “1 nm/m” (1 nm/m = 1 nano = 1 × 10) so the quotients are pure-number coefficients with values less than 1.\n\nBecause of the cumbersome nature of expressing certain dimensionless quantities per SI guidelines, the International Union of Pure and Applied Physics (IUPAP) in 1999 proposed the adoption of the special name \"uno\" (symbol: U) to represent the number 1 in dimensionless quantities. This symbol is not to be confused with the always-italicized symbol for the variable \"uncertainty\" (symbol: \"U\"). This unit name \"uno\" and its symbol could be used in combination with the SI prefixes to express the values of dimensionless quantities that are much less—or even \"greater\"—than one.\n\nCommon parts-per notations in terms of the uno are given in the table below.\n\nIn 2004, a report to the International Committee for Weights and Measures (known also by its French-language initials CIPM) stated that response to the proposal of the uno \"had been almost entirely negative\" and the principal proponent \"recommended dropping the idea\". To date, the uno has not been adopted by any standards organization and it appears unlikely it will ever become an officially sanctioned way to express low-value (high-ratio) dimensionless quantities. The proposal was instructive, however, as to the perceived shortcomings of the current options for denoting dimensionless quantities.\n\nParts-per notation may properly be used only to express true dimensionless quantities; that is, the units of measurement \"must\" cancel in expressions like \"1 mg/kg\" so that the quotients are pure numbers with values less than 1. Mixed-unit quantities such as \"a radon concentration of 15 pCi/L\" are not dimensionless quantities and may not be expressed using any form of parts-per notation, such as \"15 ppt\". Other examples of measures that are not dimensionless quantities are as follows:\n\n\nNote however, that it is not uncommon to express aqueous concentrations—particularly in drinking-water reports intended for the general public—using parts-per notation (2.1 ppm, 0.8 ppb, etc.) and further, for those reports to state that the notations denote milligrams per liter or micrograms per liter. Although \"2.1 mg/L\" is not a dimensionless quantity, it is assumed in scientific circles that \"2.1 mg/kg\" (2.1 ppm) is the true measure because one liter of water has a mass of about one kilogram. The goal in all technical writing (including drinking-water reports for the general public) is to clearly communicate to the intended audience with minimal confusion. Drinking water is intuitively a volumetric quantity in the public’s mind so measures of contamination expressed on a per-liter basis are considered to be easier to grasp. Still, it is technically possible, for example, to \"dissolve\" more than one liter of a very hydrophilic chemical in 1 liter of water; parts-per notation would be confusing when describing its solubility in water (greater than a million parts per million), so one would simply state the volume (or mass) that will dissolve into a liter, instead.\n\nWhen reporting air-borne rather than water-borne densities, a slightly different convention is used since air is approximately 1000 times less dense than water. In water, 1 µg/m is roughly equivalent to parts-per-trillion whereas in air, it is roughly equivalent to parts-per-billion. Note also, that in the case of air, this convention is much less accurate. Whereas one liter of water is almost exactly 1 kg, one cubic meter of air is often taken as 1.143 kg—much less accurate, but still close enough for many practical uses.\n\n\n"}
{"id": "25308", "url": "https://en.wikipedia.org/wiki?curid=25308", "title": "Quasispecies model", "text": "Quasispecies model\n\nThe quasispecies model is a description of the process of the Darwinian evolution of certain self-replicating entities within the framework of physical chemistry. A quasispecies is a large group or \"cloud\" of related genotypes that exist in an environment of high mutation rate (at stationary state), where a large fraction of offspring are expected to contain one or more mutations relative to the parent. This is in contrast to a species, which from an evolutionary perspective is a more-or-less stable single genotype, most of the offspring of which will be genetically accurate copies.\n\nIt is useful mainly in providing a qualitative understanding of the evolutionary processes of self-replicating macromolecules such as RNA or DNA or simple asexual organisms such as bacteria or viruses (see also viral quasispecies), and is helpful in explaining something of the early stages of the origin of life. Quantitative predictions based on this model are difficult because the parameters that serve as its input are impossible to obtain from actual biological systems. The quasispecies model was put forward by Manfred Eigen and Peter Schuster based on initial work done by Eigen.\n\nWhen evolutionary biologists describe competition between species, they generally assume that each species is a single genotype whose descendants are mostly accurate copies. (Such genotypes are said to have a high reproductive \"fidelity\".) Evolutionarily, we are interested in the behavior and fitness of that one species or genotype over time.\n\nSome organisms or genotypes, however, may exist in circumstances of low fidelity, where most descendants contain one or more mutations. A group of such genotypes is constantly changing, so discussions of which single genotype is the most fit become meaningless. Importantly, if many closely related genotypes are only one mutation away from each other, then genotypes in the group can mutate back and forth into each other. For example, with one mutation per generation, a child of the sequence AGGT could be AGTT, and a grandchild could be AGGT again. Thus we can envision a \"cloud\" of related genotypes that is rapidly mutating, with sequences going back and forth among different points in the cloud. Though the proper definition is mathematical, that cloud, roughly speaking, is a quasispecies.\n\nQuasispecies behavior exists for large numbers of individuals existing at a certain (high) range of mutation rates.\n\nIn a species, though reproduction may be mostly accurate, periodic mutations will give rise to one or more competing genotypes. If a mutation results in greater replication and survival, the mutant genotype may out-compete the parent genotype and come to dominate the species. Thus, the individual genotypes (or species) may be seen as the units on which selection acts and biologists will often speak of a single genotype's fitness.\n\nIn a quasispecies, however, mutations are ubiquitous and so the fitness of an individual genotype becomes meaningless: if one particular mutation generates a boost in reproductive success, it can't amount to much because that genotype's offspring are unlikely to be accurate copies with the same properties. Instead, what matters is the \"connectedness\" of the cloud. For example, the sequence AGGT has 12 (3+3+3+3) possible single point mutants AGGA, AGGG, and so on. If 10 of those mutants are viable genotypes that may reproduce (and some of whose offspring or grandchildren may mutate back into AGGT again), we would consider that sequence a well-connected node in the cloud. If instead only two of those mutants are viable, the rest being lethal mutations, then that sequence is poorly connected and most of its descendants will not reproduce. The analog of fitness for a quasispecies is the tendency of nearby relatives within the cloud to be well-connected, meaning that more of the mutant descendants will be viable and give rise to further descendants within the cloud.\n\nWhen the fitness of a single genotype becomes meaningless because of the high rate of mutations, the cloud as a whole or quasispecies becomes the natural unit of selection.\n\nQuasispecies represents the evolution of high-mutation-rate viruses such as HIV and sometimes single genes or molecules within the genomes of other organisms. Quasispecies models have also been proposed by Jose Fontanari and Emmanuel David Tannenbaum to model the evolution of sexual reproduction. Quasispecies was also shown in compositional replicators (based on the Gard model for abiogenesis) and was also suggested to be applicable to describe cell's replication, which amongst other things requires the maintenance and evolution of the internal composition of the parent and bud.\n\nThe model rests on four assumptions:\n\nIn the quasispecies model, mutations occur through errors made in the process of copying already existing sequences. Further, selection arises because different types of sequences tend to replicate at different rates, which leads to the suppression of sequences that replicate more slowly in favor of sequences that replicate faster. However, the quasispecies model does not predict the ultimate extinction of all but the fastest replicating sequence. Although the sequences that replicate more slowly cannot sustain their abundance level by themselves, they are constantly replenished as sequences that replicate faster mutate into them. At equilibrium, removal of slowly replicating sequences due to decay or outflow is balanced by replenishing, so that even relatively slowly replicating sequences can remain present in finite abundance.\n\nDue to the ongoing production of mutant sequences, selection does not act on single sequences, but on mutational \"clouds\" of closely related sequences, referred to as \"quasispecies\". In other words, the evolutionary success of a particular sequence depends not only on its own replication rate, but also on the replication rates of the mutant sequences it produces, and on the replication rates of the sequences of which it is a mutant. As a consequence, the sequence that replicates fastest may even disappear completely in selection-mutation equilibrium, in favor of more slowly replicating sequences that are part of a quasispecies with a higher average growth rate. Mutational clouds as predicted by the quasispecies model have been observed in RNA viruses and in \"in vitro\" RNA replication.\n\nThe mutation rate and the general fitness of the molecular sequences and their neighbors is crucial to the formation of a quasispecies. If the mutation rate is zero, there is no exchange by mutation, and each sequence is its own species. If the mutation rate is too high, exceeding what is known as the error threshold, the quasispecies will break down and be dispersed over the entire range of available sequences.\n\nA simple mathematical model for a quasispecies is as follows: let there be formula_1 possible sequences and let there be formula_2 organisms with sequence \"i\". Let's say that each of these organisms asexually gives rise to formula_3 offspring. Some are duplicates of their parent, having sequence \"i\", but some are mutant and have some other sequence. Let the mutation rate formula_4 correspond to the probability that a \"j\" type parent will produce an \"i\" type organism. Then the expected fraction of offspring generated by \"j\" type organisms that would be \"i\" type organisms is formula_5,\n\nwhere formula_6.\n\nThen the total number of \"i\"-type organisms after the first round of reproduction, given as formula_7, is\n\nSometimes a death rate term formula_9 is included so that:\n\nwhere formula_11 is equal to 1 when i=j and is zero otherwise. Note that the \"n-th\" generation can be found by just taking the \"n-th\" power of W substituting it in place of W in the above formula.\n\nThis is just a system of linear equations. The usual way to solve such a system is to first diagonalize the W matrix. Its diagonal entries will be eigenvalues corresponding to certain linear combinations of certain subsets of sequences which will be eigenvectors of the W matrix. These subsets of sequences are the quasispecies. Assuming that the matrix W is a primitive matrix (irreducible and aperiodic), then after very many generations only the eigenvector with the largest eigenvalue will prevail, and it is this quasispecies that will eventually dominate. The components of this eigenvector give the relative abundance of each sequence at equilibrium.\n\nW being primitive means that for some integer formula_12, that the formula_13 power of W is > 0, i.e. all the entries are positive. If W is primitive then each type can, through a sequence of mutations (i.e. powers of W) mutate into all the other types after some number of generations. W is not primitive if it is periodic, where the population can perpetually cycle through different disjoint sets of compositions, or if it is reducible, where the dominant species (or quasispecies) that develops can depend on the initial population, as is the case in the simple example given below.\n\nThe quasispecies formulae may be expressed as a set of linear differential equations. If we consider the difference between the new state formula_7 and the old state formula_2 to be the state change over one moment of time, then we can state that the time derivative of formula_2 is given by this difference, formula_17 we can write:\n\nThe quasispecies equations are usually expressed in terms of concentrations formula_19 where\n\nThe above equations for the quasispecies then become for the discrete version:\n\nor, for the continuum version:\n\nThe quasispecies concept can be illustrated by a simple system consisting of 4 sequences. Sequences [0,0], [0,1], [1,0], and [1,1] are numbered 1, 2, 3, and 4, respectively. Let's say the [0,0] sequence never mutates and always produces a single offspring. Let's say the other 3 sequences all produce, on average, formula_24 replicas of themselves, and formula_25 of each of the other two types, where formula_26. The W matrix is then:\n\nThe diagonalized matrix is:\n\nAnd the eigenvectors corresponding to these eigenvalues are:\n\nOnly the eigenvalue formula_29 is more than unity. For the n-th generation, the corresponding eigenvalue will be formula_30 and so will increase without bound as time goes by. This eigenvalue corresponds to the eigenvector [0,1,1,1], which represents the quasispecies consisting of sequences 2, 3, and 4, which will be present in equal numbers after a very long time. Since all population numbers must be positive, the first two quasispecies are not legitimate. The third quasispecies consists of only the non-mutating sequence 1. It's seen that even though sequence 1 is the most fit in the sense that it reproduces more of itself than any other sequence, the quasispecies consisting of the other three sequences will eventually dominate (assuming that the initial population was not homogeneous of the sequence 1 type).\n\n"}
{"id": "19271448", "url": "https://en.wikipedia.org/wiki?curid=19271448", "title": "Road coloring theorem", "text": "Road coloring theorem\n\nIn graph theory the road coloring theorem, known until recently as the road coloring conjecture, deals with synchronized instructions. The issue involves whether by using such instructions, one can reach or locate an object or destination from any other point within a network (which might be a representation of city streets or a maze). In the real world, this phenomenon would be as if you called a friend to ask for directions to his house, and he gave you a set of directions that worked no matter where you started from. This theorem also has implications in symbolic dynamics.\n\nThe theorem was first conjectured by . It was proved by .\n\nThe image to the right shows a directed graph on eight vertices in which each vertex has out-degree 2. (Each vertex in this case also has in-degree 2, but that is not necessary for a synchronizing coloring to exist.) The edges of this graph have been colored red and blue to create a synchronizing coloring.\n\nFor example, consider the vertex marked in yellow. No matter where in the graph you start, if you traverse all nine edges in the walk \"blue-red-red—blue-red-red—blue-red-red\", you will end up at the yellow vertex. Similarly, if you traverse all nine edges in the walk \"blue-blue-red—blue-blue-red—blue-blue-red\", you will always end up at the vertex marked in green, no matter where you started.\n\nThe road coloring theorem states that for a certain category of directed graphs, it is always possible to create such a coloring.\n\nLet \"G\" be a finite, strongly connected, directed graph where all the vertices have the same out-degree \"k\". Let \"A\" be the alphabet containing the letters 1, ..., \"k\". A \"synchronizing coloring\" (also known as a \"collapsible coloring\") in \"G\" is a labeling of the edges in \"G\" with letters from \"A\" such that (1) each vertex has exactly one outgoing edge with a given label and (2) for every vertex \"v\" in the graph, there exists a word \"w\" over \"A\" such that all paths in \"G\" corresponding to \"w\" terminate at \"v\".\n\nThe terminology \"synchronizing coloring\" is due to the relation between this notion and that of a synchronizing word in finite automata theory.\n\nFor such a coloring to exist at all, it is necessary that \"G\" be aperiodic. The road coloring theorem states that aperiodicity is also \"sufficient\" for such a coloring to exist. Therefore, the road coloring problem can be stated briefly as:\n\nPrevious partial or special-case results include the following:\n\n\n\n"}
{"id": "17056775", "url": "https://en.wikipedia.org/wiki?curid=17056775", "title": "Software calculator", "text": "Software calculator\n\nA software calculator is a calculator that has been implemented as a computer program, rather than as a physical hardware device.\n\nThey are among the simpler interactive software tools, and, as such, they:\n\nAs a \"calculator\", rather than a computer, they usually:\n\nSoftware calculators are available for many different platforms, and they can be:\n\nComputers as we know them today first emerged in the 1940s and 1950s. The software that they ran was naturally used to perform calculations, but it was specially designed for a substantial application that was not limited to simple calculations. For example, the LEO computer was designed to run business application software such as payroll.\n\nSoftware specifically to perform calculations as its main purpose was first written in the 1960s, and the first software package for general calculations to obtain widespread use was released in 1978. This was VisiCalc and it was called an \"interactive visible calculator\", but it was actually a spreadsheet, and these are now not normally known simply as calculators.\n\nThe Unix version released in 1979, V7 Unix, contained a command-line accessible calculator.\n\nCalculators have been used since ancient times and until the advent of software calculators they were physical, hardware machines. The most recent hardware calculators are electronic hand-held devices with buttons for digits and operations, and a small window for inputs and results.\n\nThe first software calculators imitated these hardware calculators by implementing the same functionality with mouse-operated, rather than finger-operated, buttons. Such software calculators first emerged in the 1980s as part of the original Macintosh operating system (System 1) and the Windows operating system (Windows 1.0).\n\nSome software calculators directly simulate one of the hardware calculators, by presenting an image that looks like the calculator, and by providing the same functionality.\n\nThere is now a very wide range of software calculators, and searching the Internet produces very large numbers of programs that are called \"calculators\".\n\nThe results include numerical calculators that apply arithmetic operations or mathematical functions to numbers, and that produce numerical results or graphs of numerical functions, plus some non-numerical tools and games that are also called calculators. \n\nMany of the results are calculators that do not imitate or simulate hardware calculators, but that take advantage of the greater power of computer software to implement alternative types of calculators. Software calculators are provided on the Internet which are customizable to use any conceivable algebraic expression. These user-customizable software calculators can also be used in conjunction with formula or equation creation capabilities so that the software calculator can now be created to perform all possible mathematical functions. No longer limited to a set of trigonometric and simple algebraic expressions, versions of the software calculator are now tailored to any and all topical applications.\n\nEvery type of hardware calculator has been implemented in software, including conversion, financial, graphing, programmable and scientific calculators.\n\nOther numerical calculators that do not imitate hardware calculators include:\n\nWindows-based calculators present a dialog box that allows users to enter data, rather than data \"and\" operations, and they have a built-in formula that is automatically applied to this data. There are many examples of such calculators in finance, mathematics, science and other disciplines.\n\nThere are software calculators that contain operations relevant to a specific application area and profession, including automotive, construction and electrical engineering.\n\nNon-numerical calculators include life-style and scientific calculators:\n\nThere are some software games that are called calculators, including:\n\nThere are many interactive software packages that provide user-accessible calculation features, but that are not normally called \"calculators\", because the calculation features play only a supporting role rather than being an end in themselves. These include:\n\nSpreadsheets are not normally called \"calculators\" because their main purpose is to organise data in rows and columns, and to automatically update the values of possibly many dependent cells when the value in another cell changes. The calculation features are only used in a supporting role to specify the values in some cells.\n\nComputer algebra systems are not normally called \"calculators\" because their main purpose is to perform symbolic manipulation of mathematical expressions that can contain variables and complex operations, such as integration. However, the expressions can be basic calculations that do not use variables, and that are simply evaluated, as with a calculator.\n\nDatabases are not normally called \"calculators\" because their main purpose is data entry and storage, plus reporting against this data. The calculation features are only used in a supporting role to specify the values in some fields.\n\n"}
{"id": "1887212", "url": "https://en.wikipedia.org/wiki?curid=1887212", "title": "Stein's example", "text": "Stein's example\n\nStein's example (or phenomenon or paradox), in decision theory and estimation theory, is the phenomenon that when three or more parameters are estimated simultaneously, there exist combined estimators more accurate on average (that is, having lower expected mean squared error) than any method that handles the parameters separately. It is named after Charles Stein of Stanford University, who discovered the phenomenon in 1955.\n\nAn intuitive explanation is that optimizing for the mean-squared error of a \"combined\" estimator is not the same as optimizing for the errors of separate estimators of the individual parameters. In practical terms, if the combined error is in fact of interest, then a combined estimator should be used, even if the underlying parameters are independent; this occurs in channel estimation in telecommunications, for instance (different factors affect overall channel performance). On the other hand, if one is instead interested in estimating an individual parameter, then using a combined estimator does not help and is in fact worse.\n\nThe following is perhaps the simplest form of the paradox, the special case in which the number of observations is equal to (rather than greater than) the number of parameters to be estimated. Let θ be a vector consisting of \"n\" ≥ 3 unknown parameters. To estimate these parameters, a single measurement \"X\" is performed for each parameter \"θ\", resulting in a vector X of length \"n\". Suppose the measurements are independent, Gaussian random variables, with mean θ and variance 1, i.e.,\n\nThus, each parameter is estimated using a single noisy measurement, and each measurement is equally inaccurate.\n\nUnder such conditions, it is most intuitive (and most common) to use each measurement as an estimate of its corresponding parameter. This so-called \"ordinary\" decision rule can be written as\n\nThe quality of such an estimator is measured by its risk function. A commonly used risk function is the mean squared error, defined as\n\nSurprisingly, it turns out that the \"ordinary\" estimator proposed above is suboptimal in terms of mean squared error when \"n\" ≥ 3. In other words, in the setting discussed here, there exist alternative estimators which \"always\" achieve lower mean squared error, no matter what the value of formula_4 is.\n\nFor a given \"θ\" one could obviously define a perfect \"estimator\" which is always just \"θ\", but this estimator would be bad for other values of \"θ\". The estimators of Stein's paradox are, for a given \"θ\", better than \"X\" for some values of \"X\" but necessarily worse for others (except perhaps for one particular \"θ\" vector, for which the new estimate is always better than \"X\"). It is only on average that they are better.\n\nMore accurately, an estimator formula_5 is said to dominate another estimator formula_6 if, for all values of formula_4, the risk of formula_5 is lower than, or equal to, the risk of formula_6, \"and\" if the inequality is strict for some formula_4. An estimator is said to be admissible if no other estimator dominates it, otherwise it is \"inadmissible\". Thus, Stein's example can be simply stated as follows: \"The ordinary decision rule for estimating the mean of a multivariate Gaussian distribution is inadmissible under mean squared error risk.\"\n\nMany simple, practical estimators achieve better performance than the ordinary estimator. The best-known example is the James–Stein estimator, which works by starting at \"X\" and moving towards a particular point (such as the origin) by an amount inversely proportional to the distance of \"X\" from that point.\n\nFor a sketch of the proof of this result, see Proof of Stein's example.\n\nStein's example is surprising, since the \"ordinary\" decision rule is intuitive and commonly used. In fact, numerous methods for estimator construction, including maximum likelihood estimation, best linear unbiased estimation, least squares estimation and optimal equivariant estimation, all result in the \"ordinary\" estimator. Yet, as discussed above, this estimator is suboptimal.\n\nTo demonstrate the unintuitive nature of Stein's example, consider the following real-world example. Suppose we are to estimate three unrelated parameters, such as the US wheat yield for 1993, the number of spectators at the Wimbledon tennis tournament in 2001, and the weight of a randomly chosen candy bar from the supermarket. Suppose we have independent Gaussian measurements of each of these quantities. Stein's example now tells us that we can get a better estimate (on average) for the vector of three parameters by simultaneously using the three unrelated measurements.\n\nAt first sight it appears that somehow we get a better estimator for US wheat yield by measuring some other unrelated statistics such as the number of spectators at Wimbledon and the weight of a candy bar. This is of course absurd; we have not obtained a better estimator for US wheat yield by itself, but we have produced an estimator for the vector of the means of all three random variables, which has a reduced \"total\" risk. This occurs because the cost of a bad estimate in one component of the vector is compensated by a better estimate in another component. Also, a specific set of the three estimated mean values obtained with the new estimator will not necessarily be better than the ordinary set (the measured values). It is only on average that the new estimator is better.\n\nFor any particular value of θ the new estimator will improve at least one of the individual mean square errors formula_11 This is not hard − for instance, if formula_12 is between −1 and 1, and σ = 1, then an estimator that moves formula_13 towards 0 by 0.5 (or sets it to zero if its absolute value was less than 0.5) will have a lower mean square error than formula_13 itself. But there are other values of formula_12 for which this estimator is worse than formula_13 itself. The trick of the Stein estimator, and others that yield the Stein paradox, is that they adjust the shift in such a way that there is always (for any θ vector) at least one formula_17 whose mean square error is improved, and its improvement more than compensates for any degradation in mean square error that might occur for another formula_18. The trouble is that, without knowing θ, you don't know which of the \"n\" mean square errors are improved, so you can't use the Stein estimator only for those parameters.\n\n\n"}
{"id": "41400343", "url": "https://en.wikipedia.org/wiki?curid=41400343", "title": "Timeline of women in mathematics", "text": "Timeline of women in mathematics\n\nThis is a timeline of women in mathematics.\n\n350–370 until 415: The lifetime of Hypatia, a Greek Alexandrine Neoplatonist philosopher in Egypt who was the first well-documented woman in mathematics.\n\n1748: Italian mathematician Maria Agnesi published the first book discussing both differential and integral calculus, called \"Instituzioni analitiche ad uso della gioventù italiana\".\n\n1759: French mathematician Émilie du Châtelet's translation and commentary on Isaac Newton's work \"Principia Mathematica\" was published posthumously; it is still considered the standard French translation.\n\n1827: French mathematician Sophie Germain saw her theorem, known as Germain's Theorem, published in a footnote of a book by the mathematician Adrien-Marie Legendre. In this theorem Germain proved that if \"x\", \"y\", and \"z\" are integers and if \"x\" + \"y\" = \"z\" then either \"x\", \"y\", or \"z\" must be divisible by 5. Germain's theorem was a major step toward proving Fermat's last theorem for the case where n equals 5.\n\n1829: The first public examination of an American girl in geometry was held.\n\n1874: Russian mathematician Sofia Kovalevskaya became the first woman in modern Europe to gain a doctorate in mathematics, which she earned from the University of Göttingen in Germany. \n\n1880: Charlotte Angas Scott of Britain obtained special permission to take the Cambridge Mathematical Tripos Exam, as women were not normally allowed to sit for the exam. She came eighth on the Tripos of all students taking them, but due to her sex, the title of \"eighth wrangler,\" a high honour, went officially to a male student. At the ceremony, however, after the seventh wrangler had been announced, all the students in the audience shouted her name. Because she could not attend the award ceremony, Scott celebrated her accomplishment at Girton College where there were cheers and clapping at dinner, and a special evening ceremony where the students sang \"See the Conquering Hero Comes\", and she received an ode written by a staff member, and was crowned with laurels. \n\n1886: Winifred Edgerton Merrill became the first American woman to earn a PhD in mathematics, which she earned from Columbia University.\n\n1888: The Kovalevskaya top, one of a brief list of known examples of integrable rigid body motion, was discovered by Sofia Kovalevskaya.\n\n1889: Sofia Kovalevskaya was appointed as the first female professor in Northern Europe, at the University of Stockholm.\n\n1890: British woman Philippa Fawcett became the first woman to obtain the top score in the Cambridge Mathematical Tripos Exam.\n\n1913: American mathematician Mildred Sanderson published her theorem about modular invariants in her thesis. It states: “To any modular invariant i of a system of forms under any group G of linear transformations with coefficients in the GF[pn], there corresponds a formal invariant I under G such that I = i for all sets of values in the field of the coefficients of the system of forms.” She was Leonard Dickson’s first female graduate student, and he later wrote of her thesis, “This paper is a highly important contribution to this field of work; its importance lies partly in the fact that it establishes a correspondence between modular and formal invariants. Her main theorem has already been frequently quoted on account of its fundamental character. Her proof is a remarkable piece of mathematics.” E.T. Bell wrote, “Miss Sanderson’s single contribution (1913) to modular invariants has been rated by competent judges as one of the classics of the subject.”\n\n1918: German mathematician Emmy Noether published Noether's (first) theorem, which states that any differentiable symmetry of the action of a physical system has a corresponding conservation law. \n\n1927: American mathematician Anna Pell-Wheeler became the first woman to present a lecture at the American Mathematical Society Colloquium.\n\n1930: Cecilia Kreiger became the first woman to earn a PhD in mathematics in Canada, at the University of Toronto.\n\n1930s: British mathematician Mary Cartwright proved her theorem, now known as Cartwright's theorem, which gives an estimate for the maximum modulus of an analytic function that takes the same value no more than p times in the unit disc. To prove the theorem she used a new approach, applying a technique introduced by Lars Ahlfors for conformal mappings.\n\n1943: Euphemia Haynes became the first African-American woman to earn a Ph.D. in mathematics, which she earned from Catholic University.\n\n1949: American mathematician Gertrude Mary Cox became the first woman elected into the International Statistical Institute.\n\n1956: American mathematician Gladys West began collecting data from satellites at the Naval Surface Warfare Center Dahlgren Division. Her calculations directly impacted the development of accurate GPS systems.\n\n1962: American mathematician Mina Rees became the first woman to win the Yueh-Gin Gung and Dr. Charles Y. Hu Award for Distinguished Service to Mathematics, which is the most prestigious award made by the Mathematical Association of America.\n\n1964: Mary Cartwright became the first woman to win the Sylvester Medal of the Royal Society of London, which is given every three years since 1901 for the encouragement of mathematical research, without regard to nationality.\n\n1966: American mathematician and physics professor Mary L. Boas published \"Mathematical Methods in the Physical Sciences\", which was still widely used in college classrooms as of 1999.\n\n1968: Mary Cartwright became the first woman to win the De Morgan Medal, the London Mathematical Society's premier award.\n\n1970: American mathematician Mina Rees became the first female president of the American Association for the Advancement of Science.\n\n1971: American mathematician Mary Ellen Rudin constructed the first Dowker space.\n\n1971: The Association for Women in Mathematics (AWM) was founded. It is a professional society whose mission is to encourage women and girls to study and to have active careers in the mathematical sciences, and to promote equal opportunity for and the equal treatment of women and girls in the mathematical sciences. It is incorporated in America in the state of Massachusetts.\n\n1971: The Joint Committee on Women in the Mathematical Sciences (JCW), was founded as a committee of the American Mathematical Society (AMS). It is now a joint committee of seven mathematical and statistical societies which works to identify mechanisms for the enhancement of opportunities for women in the mathematical and statistical sciences, recommend actions to the governing bodies of the member societies in support of these opportunities, and document its recommendations by presenting data.\n\n1973: American mathematician Jean Taylor published her dissertation on “Regularity of the Singular Set of Two-Dimensional Area-Minimizing Flat Chains Modulo 3 in R3” which solved a long-standing problem about length and smoothness of soap-film triple function curves.\n\n1974: American mathematician Joan Birman published the book \"Braids, Links, and Mapping Class Groups\". It has become a standard introduction, with many of today’s researchers having learned the subject through it.\n\n1975–1977: American amateur mathematician Marjorie Rice, who had no formal training in mathematics beyond high school, discovered three new types of tessellating pentagons and more than sixty distinct tessellations by pentagons.\n\n1975: American mathematician Julia Robinson became the first female mathematician elected to the National Academy of Sciences.\n\n1979: Mary Ellen Rudin became the first woman to present the Earle Raymond Hedrick Lectures; these lectures were established by the Mathematical Association of America in 1952 to present to the Association a lecturer of known skill as an expositor of mathematics \"who will present a series of at most three lectures accessible to a large fraction of those who teach college mathematics.\"\n\n1979: American mathematician Dorothy Lewis Bernstein became the first female president of the Mathematical Association of America.\n\n1981: Canadian-American mathematician Cathleen Morawetz became the first female mathematician to give a Josiah Willard Gibbs Lecture; these lectures are of a semi-popular nature and are given by invitation, and are usually devoted to mathematics or its applications.\n\n1981: American mathematician Doris Schattschneider became the first female editor of \"Mathematics Magazine.\"\n\n1983: Julia Robinson became the first female president of the American Mathematical Society.\n\n1983: Julia Robinson became the first female mathematician to be awarded a MacArthur Fellowship.\n\n1988: Doris Schattschneider became the first woman to present the J. Sutherland Frame Lectures, which are presented at the summer meeting of the Mathematical Association of America.\n\n1992: American mathematician Gloria Gilmer became the first woman to deliver a major National Association of Mathematicians lecture (it was the Cox–Talbot address).\n\n1995: American mathematician Margaret Wright became the first female president of the Society for Industrial and Applied Mathematics.\n\n1995: Israeli-Canadian mathematician Leah Edelstein-Keshet became the first female president of the Society for Mathematical Biology.\n\n1996: Joan Birman became the first woman to receive the Chauvenet Prize, which is awarded annually by the Mathematical Association of America to the author of an outstanding expository article on a mathematical topic by a member of the association.\n\n1996: Ioana Dumitriu, a New York University sophomore from Romania, became the first woman to be named a Putnam Fellow. Putnam Fellows are the top five (or six, in case of a tie) scorers on The William Lowell Putnam Mathematical Competition.\n\n1998: Melanie Wood became the first female American to make the U.S. International Math Olympiad Team. She won silver medals in the 1998 and 1999 International Mathematical Olympiads.\n\n2002: Susan Howson became the first woman to win the Adams Prize, given annually by the University of Cambridge to a British mathematician under the age of 40.\n\n2002: Melanie Wood became the first American woman and second woman overall to be named a Putnam Fellow in 2002.\n\n2004: Melanie Wood became the first woman to win the Frank and Brennie Morgan Prize for Outstanding Research in Mathematics by an Undergraduate Student. It is an annual award given to an undergraduate student in the US, Canada, or Mexico who demonstrates superior mathematics research.\n\n2004: American Alison Miller became the first ever female gold medal winner on the U.S. International Math Olympiad Team.\n\n2006: Polish-Canadian mathematician Nicole Tomczak-Jaegermann became the first woman to win the CRM-Fields-PIMS prize, which recognizes exceptional achievement in the mathematical sciences.\n\n2006: Stefanie Petermichl, a German mathematical analyst then at the University of Texas at Austin, became the first woman to win the Salem Prize, an annual award given to young mathematicians considered to have done outstanding work in Raphael Salem's field of interest, primarily Fourier series and related areas in analysis. She shared the prize with Artur Avila.\n\n2012: Latvian mathematician Daina Taimina became the first woman to win the Euler Book Prize, which is awarded annually to an author or authors of an outstanding book about mathematics, for her book \"Crocheting Adventures with Hyperbolic Planes.\"\n\n2012: The Working Committee for Women in Mathematics, Chinese Mathematical Society (WCWM-CMS) was founded; it is a national non-profit academic organization in which female mathematicians who are engaged in research, teaching, and applications of mathematics can share their scientific research through academic exchanges both in China and abroad. It is one of the branches of the Chinese Mathematical Society (CMS).\n\n2014: Maryam Mirzakhani became the first woman as well as the first Iranian to be awarded the Fields Medal, which she was awarded for \"her outstanding contributions to the dynamics and geometry of Riemann surfaces and their moduli spaces.\" She shared the prize with Martin Hairer, Manjul Bhargava, and Artur Avila. It is a prize awarded to two, three, or four mathematicians not over 40 years of age at each International Congress of the International Mathematical Union, and is often viewed as the greatest honor a mathematician can receive.\n\n2016: French mathematician Claire Voisin received the CNRS Gold medal, the highest scientific research award in France.\n\nTimeline of women in mathematics in the United States\n"}
{"id": "1458024", "url": "https://en.wikipedia.org/wiki?curid=1458024", "title": "Toy theorem", "text": "Toy theorem\n\nIn mathematics, a toy theorem is a simplified version (special case) of a more general theorem. For instance, by introducing some simplifying assumptions in a theorem, one obtains a toy theorem.\n\nUsually, a toy theorem is used to illustrate the claim of a theorem. It can also be insightful to study proofs of a toy theorem derived from a non-trivial theorem. Toy theorems can also have education value. After presenting a theorem (with, say, a highly non-trivial proof), one can sometimes give some assurance that the theorem really holds, by proving a toy version of the theorem.\n\nFor instance, a toy theorem of the Brouwer fixed-point theorem is obtained by restricting the dimension to one. In this case, the Brouwer fixed-point theorem follows almost immediately from the intermediate value theorem.\n\n"}
{"id": "276894", "url": "https://en.wikipedia.org/wiki?curid=276894", "title": "Uniqueness quantification", "text": "Uniqueness quantification\n\nIn mathematics and logic, the phrase \"there is one and only one\" is used to indicate that exactly one object with a certain property exists. In mathematical logic, this sort of quantification is known as uniqueness quantification or unique existential quantification.\n\nUniqueness quantification is often denoted with the symbols \"∃!\" or \"∃\". For example, the formal statement \nmay be read aloud as \"there is exactly one natural number \"n\" such that \"n\" − 2 = 4\".\n\nThe most common technique to proving unique existence is to first prove existence of entity with the desired condition; then, to assume there exist two entities (say, \"a\" and \"b\") that both satisfy the condition, and logically deduce their equality, i.e. \"a\" = \"b\".\n\nAs a simple high school example, to show \"x\" + 2 = 5 has exactly one solution, we first show by demonstration that at least one solution exists, namely 3; the proof of this part is simply the calculation\n\nWe now assume that there are two solutions, namely \"a\" and \"b\", satisfying \"x\" + 2 = 5. Thus\n\nBy transitivity of equality,\n\nBy cancellation,\n\nThis simple example shows how a proof of uniqueness is done, the end result being the equality of the two quantities that satisfy the condition.\n\nBoth existence and uniqueness must be proven, in order to conclude that there exists exactly one solution.\n\nAn alternative way to prove uniqueness is to prove there exists a value formula_6 satisfying the condition, and then proving that, for all formula_7, the condition for formula_7 implies formula_9.\n\nUniqueness quantification can be expressed in terms of the existential and universal quantifiers of predicate logic by defining the formula\n∃!\"x\" \"P(x)\" to mean literally,\nwhich is the same as\nAn equivalent definition that has the virtue of separating the notions of existence and uniqueness into two clauses, at the expense of brevity, is\nAnother equivalent definition with the advantage of brevity is\n\nOne generalization of uniqueness quantification is counting quantification. This includes both quantification of the form \"exactly \"k\" objects exist such that …\" as well as \"infinitely many objects exist such that …\" and \"only finitely many objects exist such that…\". The first of these forms is expressible using ordinary quantifiers, but the latter two cannot be expressed in ordinary first-order logic.\n\nUniqueness depends on a notion of equality. Loosening this to some coarser equivalence relation yields quantification of uniqueness up to that equivalence (under this framework, regular uniqueness is \"uniqueness up to equality\"). For example, many concepts in category theory are defined to be unique up to isomorphism.\n\n\n"}
{"id": "37842037", "url": "https://en.wikipedia.org/wiki?curid=37842037", "title": "WIRIS", "text": "WIRIS\n\nWIRIS is a set of proprietary HTML-based JavaScript tools which can author and edit mathematical formulas, execute mathematical problems and show mathematical graphics on the Cartesian coordinate system. WIRIS equation editor is a native browser application, with a light server-side, that supports both MathML and LaTeX.\n"}
