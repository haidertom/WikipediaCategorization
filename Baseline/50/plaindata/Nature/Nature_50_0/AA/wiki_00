{"id": "184495", "url": "https://en.wikipedia.org/wiki?curid=184495", "title": "Biefeld–Brown effect", "text": "Biefeld–Brown effect\n\nThe Biefeld–Brown effect is an electrical phenomenon that produces an ionic wind that transfers its momentum to surrounding neutral particles. It describes a force observed on an asymmetric capacitor when high voltage is applied to the capacitor's electrodes. Once suitably charged up to high DC potentials, a thrust from the negative terminal to the positive terminal is generated. The effect was named by inventor Thomas Townsend Brown who claimed that he did a series of experiments with professor of astronomy Paul Alfred Biefeld, a former teacher of Brown whom Brown claimed was his mentor and co-experimenter at Denison University in Ohio.\n\nThe use of an asymmetric capacitor, with the negative electrode being larger than the positive electrode, allowed for more thrust to be produced in the direction from the low-flux to the high-flux region compared to a conventional capacitor. These asymmetric capacitors became known as Asymmetrical Capacitor Thrusters (ACT). The Biefeld–Brown effect can be observed in ionocrafts and lifters, which utilize the effect to produce thrust in the air without requiring any combustion or moving parts.\n\nIn his 1960 patent titled \"Electrokinetic Apparatus,\" Brown refers to electrokinesis to describe the Biefeld–Brown effect, linking the phenomenon to the field of electrohydrodynamics (EHD). Brown also believed the Biefeld–Brown effect could produce an anti-gravity force, referred to as \"electrogravitics\" based on it being an electricity/gravity phenomenon. However, there is little evidence that supports Brown's claim on the effect's anti-gravity properties.\n\nThe \"Biefeld–Brown effect\" was the name given to a phenomenon observed by Thomas Townsend Brown while he was experimenting with X-ray tubes during the 1920s while he was still in high school. When he applied a high voltage electrical charge to a Coolidge tube that he placed on a scale, Brown noticed a difference in the tubes mass depending on orientation, implying some kind of net force. This discovery caused him to assume that he had somehow influenced gravity electronically and led him to design a propulsion system based on this phenomenon. On 15 April 1927, he applied for a patent, entitled \"Method of Producing Force or Motion,\" that described his invention as an electrical-based method that could control gravity to produce linear force or motion. In 1929, Brown published an article for the popular American magazine \"Science and Invention\", which detailed his work. The article also mentioned the \"gravitator,\" an invention by Brown which produced motion without the use of electromagnetism, gears, propellers, or wheels, but instead using the principles of what he called \"electro-gravitation.\" He also claimed that the asymmetric capacitors were capable of generating mysterious fields that interacted with the Earth's gravitational pull and envisioned a future where gravitators would propel ocean liners and even space cars. At some point this effect also gained the moniker \"Biefeld–Brown effect\", probably coined by Brown to claim Denison University professor of physics and astronomy Paul Alfred Biefeld as his mentor and co-experimenter. Brown attended Denison for a year before he dropped out and records of him even having an association with Biefeld are sketchy at best.\n\nBrown filed another patent in 1960 that detailed the physics of the Biefeld–Brown effect, making the following claims:\n\n\nIn 1965, Brown filed a patent that claimed that a net force on the asymmetric capacitor can exist even in a vacuum. However, there is little experimental evidence that serves to validate his claims.\n\nThe effect is generally believed to rely on corona discharge, which allows air molecules to become ionized near sharp points and edges. Usually, two electrodes are used with a high voltage between them, ranging from a few kilovolts and up to megavolt levels, where one electrode is small or sharp, and the other larger and smoother. The most effective distance between electrodes occurs at an electric potential gradient of about 10 kV/cm, which is just below the nominal breakdown voltage of air between two sharp points, at a current density level usually referred to as the saturated corona current condition. This creates a high field gradient around the smaller, positively charged electrode. Around this electrode, ionization occurs, that is, electrons are stripped from the atoms in the surrounding medium; they are literally pulled right off by the electrode's charge.\n\nThis leaves a cloud of positively charged ions in the medium, which are attracted to the negative smooth electrode by Coulomb's Law, where they are neutralized again. This produces an equally scaled opposing force in the lower electrode. This effect can be used for propulsion (see EHD thruster), fluid pumps and recently also in EHD cooling systems. The velocity achievable by such setups is limited by the momentum achievable by the ionized air, which is reduced by ion impact with neutral air. A theoretical derivation of this force has been proposed (see the external links below).\n\nHowever, this effect works using either polarity for the electrodes: the small or thin electrode can be either positive or negative, and the larger electrode must have the opposite polarity. On many experimental sites it is reported that the thrust effect of a lifter is actually a bit stronger when the small electrode is the positive one. This is possibly an effect of the differences between the ionization energy and electron affinity energy of the constituent parts of air; thus the ease of which ions are created at the 'sharp' electrode.\n\nAs air pressure is removed from the system, several effects combine to reduce the force and momentum available to the system. The number of air molecules around the ionizing electrode is reduced, decreasing the quantity of ionized particles. At the same time, the number of impacts between ionized and neutral particles is reduced. Whether this increases or decreases the maximum momentum of the ionized air is not typically measured, although the force acting upon the electrodes reduces, until the glow discharge region is entered. The reduction in force is also a product of the reducing breakdown voltage of air, as a lower potential must be applied between the electrodes, thereby reducing the force dictated by Coulomb's Law.\n\nDuring the glow discharge region, the air becomes a conductor. Though the applied voltage and current will propagate at nearly the speed of light, the movement of the conductors themselves is almost negligible. This leads to a Coulomb force and change of momentum so small as to be zero.\n\nBelow the glow discharge region, the breakdown voltage increases again, whilst the number of potential ions decreases, and the chance of impact lowers. Experiments have been conducted and found to both prove and disprove a force at very low pressure. It is likely that the reason for this is that at very low pressures, only experiments which used very large voltages produced positive results, as a product of a greater chance of ionization of the extremely limited number of available air molecules, and a greater force from each ion from Coulomb's Law; experiments which used lower voltages have a lower chance of ionization and a lower force per ion. Common to positive results is that the force observed is small in comparison to experiments conducted at standard pressure.\n\nBrown believed that his large, high voltage, high capacity capacitors produced an electric field strong enough to marginally interacted with the Earth's gravitational pull, a phenomenon he labeled electrogravitics. Several researchers claim that conventional physics cannot adequately explain the phenomenon. The effect has become something of a cause célèbre in the UFO community, where it is seen as an example of something much more exotic than electrokinetics. Charles Berlitz devoted an entire chapter of his book \"The Philadelphia Experiment\" to a retelling of Brown's early work with the effect, implying he had discovered a new electrogravity effect and that it was being used by UFOs. Today, the Internet is filled with sites devoted to this interpretation of the effect.\n\nThere have been follow-ups on the claims that this force can be produced in a full vacuum, meaning it is an unknown anti-gravity force, and not just the more well known ion wind. As part of a study in 1990, U.S. Air Force researcher R. L. Talley conducted a test on a Biefeld–Brown-style capacitor to replicate the effect in a vacuum. Despite attempts that increased the driving DC voltage to about 19 kV in vacuum chambers up to 10 torr, Talley observed no thrust in terms of static DC potential applied to the electrodes. In 2003, NASA scientist Jonathan Campbell tested a lifter in a vacuum at 10 torr with a voltage of up to 50 kV, only to observe no movement from the lifter. Campbell pointed out to a Wired magazine reporter that creating a true vacuum similar to space for the test requires tens of thousands of dollars in equipment.\n\nAround the same time in 2003, researchers from the Army Research Laboratory (ARL) tested the Biefeld–Brown effect by building four different-sized asymmetric capacitors based on simple designs found on the Internet and then applying a high voltage of around 30 kV to them. According to their report, the researchers claimed that the effects of ion wind was at least three orders of magnitude too small to account for the observed force on the asymmetric capacitor in the air. Instead, they proposed that the Biefeld–Brown effect may be better explained using ion drift instead of ion wind due to how the former involves collisions instead of ballistic trajectories. Around ten years later, researchers from the Technical University of Liberec conducted experiments on the Biefeld–Brown effect that supported ARL's claim that assigned ion drift as the most likely source of the generated force.\n\nIn 2004, Martin Tajmar published a paper that also failed to replicate Brown's work and suggested that Brown may have instead observed the effects of a corona wind triggered by insufficient outgassing of the electrode assembly in the vacuum chamber and therefore misinterpreted the corona wind effects as a possible connection between gravitation and electromagnetism.\n\nT. T. Brown was granted a number of patents on his discovery:\n\nHistorically, numerous patents have been granted for various applications of the effect, from electrostatic dust precipitation, to air ionizers, and also for flight. A particularly notable patent — — was granted to G.E. Hagen in 1964, for apparatus more or less identical to the later so-called 'lifter' devices. Other ionic US patents of interest: , , , , , , , , , , , .\n\n\n"}
{"id": "5129726", "url": "https://en.wikipedia.org/wiki?curid=5129726", "title": "Casimir pressure", "text": "Casimir pressure\n\nCasimir pressure is created by the Casimir force of virtual particles.\n\nAccording to experiments, the Casimir force formula_1 between two closely spaced neutral parallel plate conductors is directly proportional to their surface area formula_2:\n\nformula_3\n\nTherefore, dividing the magnitude of Casimir force by the area of each conductor, Casimir pressure formula_4 can be found. Because the Casimir force between conductors is attractive, the Casimir pressure in space between the conductors is negative.\n\nBecause virtual particles are physical representations of the zero point energy of physical vacuum, the Casimir pressure is the difference in the density of the zero point energy of empty space inside and outside of cavity made by conductive plates.\n\nSome scientists believe that zero point energy is the dominant energy of the Universe and that the Casimir pressure of this energy is the main cause of the observed accelerated expansion of the Universe. In other words, virtual particles drive the accelerated expansion of the Universe.\n\n"}
{"id": "7807", "url": "https://en.wikipedia.org/wiki?curid=7807", "title": "Cavitation", "text": "Cavitation\n\nCavitation is the formation of vapour cavities in a liquid, small liquid-free zones (\"bubbles\" or \"voids\"), that are the consequence of forces acting upon the liquid. It usually occurs when a liquid is subjected to rapid changes of pressure that cause the formation of cavities in the liquid where the pressure is relatively low. When subjected to higher pressure, the voids implode and can generate an intense shock wave.\n\nCavitation is a significant cause of wear in some engineering contexts. Collapsing voids that implode near to a metal surface cause cyclic stress through repeated implosion. This results in surface fatigue of the metal causing a type of wear also called \"cavitation\". The most common examples of this kind of wear are to pump impellers, and bends where a sudden change in the direction of liquid occurs. Cavitation is usually divided into two classes of behavior: inertial (or transient) cavitation and non-inertial cavitation.\n\nInertial cavitation is the process where a void or bubble in a liquid rapidly collapses, producing a shock wave. Inertial cavitation occurs in nature in the strikes of mantis shrimps and pistol shrimps, as well as in the vascular tissues of plants. In man-made objects, it can occur in control valves, pumps, propellers and impellers.\n\nNon-inertial cavitation is the process in which a bubble in a fluid is forced to oscillate in size or shape due to some form of energy input, such as an acoustic field. Such cavitation is often employed in ultrasonic cleaning baths and can also be observed in pumps, propellers, etc.\n\nSince the shock waves formed by collapse of the voids are strong enough to cause significant damage to moving parts, cavitation is usually an undesirable phenomenon. It is very often specifically avoided in the design of machines such as turbines or propellers, and eliminating cavitation is a major field in the study of fluid dynamics. However, it is sometimes useful and does not cause damage when the bubbles collapse away from machinery, such as in supercavitation.\n\nInertial cavitation was first observed in the late 19th century, considering the collapse of a spherical void within a liquid. When a volume of liquid is subjected to a sufficiently low pressure, it may rupture and form a cavity. This phenomenon is coined \"cavitation inception\" and may occur behind the blade of a rapidly rotating propeller or on any surface vibrating in the liquid with sufficient amplitude and acceleration. A fast-flowing river can cause cavitation on rock surfaces, particularly when there is a drop-off, such as on a waterfall.\n\nOther ways of generating cavitation voids involve the local deposition of energy, such as an intense focused laser pulse (optic cavitation) or with an electrical discharge through a spark. Vapor gases evaporate into the cavity from the surrounding medium; thus, the cavity is not a perfect vacuum, but has a relatively low gas pressure. Such a low-pressure bubble in a liquid begins to collapse due to the higher pressure of the surrounding medium. As the bubble collapses, the pressure and temperature of the vapor within increases. The bubble eventually collapses to a minute fraction of its original size, at which point the gas within dissipates into the surrounding liquid via a rather violent mechanism which releases a significant amount of energy in the form of an acoustic shock wave and as visible light. At the point of total collapse, the temperature of the vapor within the bubble may be several thousand kelvin, and the pressure several hundred atmospheres.\n\nInertial cavitation can also occur in the presence of an acoustic field. Microscopic gas bubbles that are generally present in a liquid will be forced to oscillate due to an applied acoustic field. If the acoustic intensity is sufficiently high, the bubbles will first grow in size and then rapidly collapse. Hence, inertial cavitation can occur even if the rarefaction in the liquid is insufficient for a Rayleigh-like void to occur. High-power ultrasonics usually utilize the inertial cavitation of microscopic vacuum bubbles for treatment of surfaces, liquids, and slurries.\n\nThe physical process of cavitation inception is similar to boiling. The major difference between the two is the thermodynamic paths that precede the formation of the vapor. Boiling occurs when the local temperature of the liquid reaches the saturation temperature, and further heat is supplied to allow the liquid to sufficiently phase change into a gas. Cavitation inception occurs when the local pressure falls sufficiently far below the saturated vapor pressure, a value given by the tensile strength of the liquid at a certain temperature.\n\nIn order for cavitation inception to occur, the cavitation \"bubbles\" generally need a surface on which they can nucleate. This surface can be provided by the sides of a container, by impurities in the liquid, or by small undissolved microbubbles within the liquid. It is generally accepted that hydrophobic surfaces stabilize small bubbles. These pre-existing bubbles start to grow unbounded when they are exposed to a pressure below the threshold pressure, termed Blake's threshold.\n\nThe vapor pressure here differs from the meteorological definition of vapor pressure, which describes the partial pressure of water in the atmosphere at some value less than 100% saturation. Vapor pressure as relating to cavitation refers to the vapor pressure in equilibrium conditions and can therefore be more accurately defined as the equilibrium (or saturated) vapor pressure.\n\nNon-inertial cavitation is the process in which small bubbles in a liquid are forced to oscillate in the presence of an acoustic field, when the intensity of the acoustic field is insufficient to cause total bubble collapse. This form of cavitation causes significantly less erosion than inertial cavitation, and is often used for the cleaning of delicate materials, such as silicon wafers.\n\nHydrodynamic cavitation describes the process of vaporisation, bubble generation and bubble implosion which occurs in a flowing liquid as a result of a decrease and subsequent increase in local pressure. Cavitation will only occur if the local pressure declines to some point below the saturated vapor pressure of the liquid and subsequent recovery above the vapor pressure. If the recovery pressure is not above the vapor pressure then flashing is said to have occurred. In pipe systems, cavitation typically occurs either as the result of an increase in the kinetic energy (through an area constriction) or an increase in the pipe elevation.\n\nHydrodynamic cavitation can be produced by passing a liquid through a constricted channel at a specific flow velocity or by mechanical rotation of an object through a liquid. In the case of the constricted channel and based on the specific (or unique) geometry of the system, the combination of pressure and kinetic energy can create the hydrodynamic cavitation cavern downstream of the local constriction generating high energy cavitation bubbles.\n\nThe process of bubble generation, and the subsequent growth and collapse of the cavitation bubbles, results in very high energy densities and in very high local temperatures and local pressures at the surface of the bubbles for a very short time. The overall liquid medium environment, therefore, remains at ambient conditions. When uncontrolled, cavitation is damaging; by controlling the flow of the cavitation, however, the power can be harnessed and non-destructive. Controlled cavitation can be used to enhance chemical reactions or propagate certain unexpected reactions because free radicals are generated in the process due to disassociation of vapors trapped in the cavitating bubbles.\n\nOrifices and venturi are reported to be widely used for generating cavitation. A venturi has an inherent advantage over an orifice because of its smooth converging and diverging sections, such that it can generate a higher flow velocity at the throat for a given pressure drop across it. On the other hand, an orifice has an advantage that it can accommodate a greater number of holes (larger perimeter of holes) in a given cross sectional area of the pipe.\n\nThe cavitation phenomenon can be controlled to enhance the performance of high-speed marine vessels and projectiles, as well as in material processing technologies, in medicine, etc. Controlling the cavitating flows in liquids can be achieved only by advancing the mathematical foundation of the cavitation processes. These processes are manifested in different ways, the most common ones and promising for control being bubble cavitation and supercavitation. The first exact classical solution should perhaps be credited to the well- known solution by H. Helmholtz in 1868. The earliest distinguished studies of academic type on the theory of a cavitating flow with free boundaries and supercavitation were published in the book \"Jets, wakes and cavities\" followed by \"Theory of jets of ideal fluid\". Widely used in these books was the well-developed theory of conformal mappings of functions of a complex variable, allowing one to derive a large number of exact solutions of plane problems. Another venue combining the existing exact solutions with approximated and heuristic models was explored in the work \"Hydrodynamics of Flows with Free Boundaries\" that refined the applied calculation techniques based on the principle of cavity expansion independence, theory of pulsations and stability of elongated axisymmetric cavities, etc. and in \"Dimensionality and similarity methods in the problems of the hydromechanics of vessels\".\n\nA natural continuation of these studies was recently presented in \"The Hydrodynamics of Cavitating Flows\" – an encyclopedic work encompassing all the best advances in this domain for the last three decades, and blending the classical methods of mathematical research with the modern capabilities of computer technologies. These include elaboration of nonlinear numerical methods of solving 3D cavitation problems, refinement of the known plane linear theories, development of asymptotic theories of axisymmetric and nearly axisymmetric flows, etc. As compared to the classical approaches, the new trend is characterized by expansion of the theory into the 3D flows. It also reflects a certain correlation with current works of an applied character on the hydrodynamics of supercavitating bodies.\n\nHydrodynamic cavitation can also improve some industrial processes. For instance, cavitated corn slurry shows higher yields in ethanol production compared to uncavitated corn slurry in dry milling facilities.\n\nThis is also used in the mineralization of bio-refractory compounds which otherwise would need extremely high temperature and pressure conditions since free radicals are generated in the process due to the dissociation of vapors trapped in the cavitating bubbles, which results in either the intensification of the chemical reaction or may even result in the propagation of certain reactions not possible under otherwise ambient conditions.\n\nIn industry, cavitation is often used to homogenize, or mix and break down, suspended particles in a colloidal liquid compound such as paint mixtures or milk. Many industrial mixing machines are based upon this design principle. It is usually achieved through impeller design or by forcing the mixture through an annular opening that has a narrow entrance orifice with a much larger exit orifice. In the latter case, the drastic decrease in pressure as the liquid accelerates into a larger volume induces cavitation. This method can be controlled with hydraulic devices that control inlet orifice size, allowing for dynamic adjustment during the process, or modification for different substances. The surface of this type of mixing valve, against which surface the cavitation bubbles are driven causing their implosion, undergoes tremendous mechanical and thermal localized stress; they are therefore often constructed of super-hard or tough materials such as stainless steel, Stellite, or even polycrystalline diamond (PCD).\n\nCavitating water purification devices have also been designed, in which the extreme conditions of cavitation can break down pollutants and organic molecules. Spectral analysis of light emitted in sonochemical reactions reveal chemical and plasma-based mechanisms of energy transfer. The light emitted from cavitation bubbles is termed sonoluminescence.\n\nUse of this technology has been tried successfully in alkali refining of vegetable oils.\n\nHydrophobic chemicals are attracted underwater by cavitation as the pressure difference between the bubbles and the liquid water forces them to join together. This effect may assist in protein folding.\n\nCavitation plays an important role for the destruction of kidney stones in shock wave lithotripsy. Currently, tests are being conducted as to whether cavitation can be used to transfer large molecules into biological cells (sonoporation). Nitrogen cavitation is a method used in research to lyse cell membranes while leaving organelles intact.\n\nCavitation plays a key role in non-thermal, non-invasive fractionation of tissue for treatment of a variety of diseases and can be used to open the blood-brain barrier to increase uptake of neurological drugs in the brain.\n\nCavitation also plays a role in HIFU, a thermal non-invasive treatment methodology for cancer. \n\nUltrasound sometimes is used to increase bone formation, for instance in post-surgical applications.\nUltrasound treatments or exposure can create cavitation that potentially may \"result in a syndrome involving manifestations of nausea, headache, tinnitus, pain, dizziness, and fatigue.\".\n\nIt has been suggested that the sound of \"cracking\" knuckles derives from the collapse of cavitation in the synovial fluid within the joint. Movements that cause cracking expand the joint space, thus reducing pressure to the point of cavitation. It remains controversial whether this is associated with clinically significant joint injury such as osteoarthritis. Some physicians say that osteoarthritis is caused by cracking knuckles regularly, as this causes wear and tear and may cause the bone to weaken. The implication being that, it is not the \"bubbles popping,\" but rather, the bones rubbing together, that causes osteoarthritis.\n\nIn industrial cleaning applications, cavitation has sufficient power to overcome the particle-to-substrate adhesion forces, loosening contaminants. The threshold pressure required to initiate cavitation is a strong function of the pulse width and the power input. This method works by generating controlled acoustic cavitation in the cleaning fluid, picking up and carrying contaminant particles away so that they do not reattach to the material being cleaned.\n\nCavitation has been applied to egg pasteurization. A hole-filled rotor produces cavitation bubbles, heating the liquid from within. Equipment surfaces stay cooler than the passing liquid, so eggs don't harden as they did on the hot surfaces of older equipment. The intensity of cavitation can be adjusted, making it possible to tune the process for minimum protein damage.\n\nCavitation is, in many cases, an undesirable occurrence. In devices such as propellers and pumps, cavitation causes a great deal of noise, damage to components, vibrations, and a loss of efficiency. Cavitation has also become a concern in the renewable energy sector as it may occur on the blade surface of tidal stream turbines.\n\nWhen the cavitation bubbles collapse, they force energetic liquid into very small volumes, thereby creating spots of high temperature and emitting shock waves, the latter of which are a source of noise. The noise created by cavitation is a particular problem for military submarines, as it increases the chances of being detected by passive sonar.\n\nAlthough the collapse of a small cavity is a relatively low-energy event, highly localized collapses can erode metals, such as steel, over time. The pitting caused by the collapse of cavities produces great wear on components and can dramatically shorten a propeller's or pump's lifetime.\n\nAfter a surface is initially affected by cavitation, it tends to erode at an accelerating pace. The cavitation pits increase the turbulence of the fluid flow and create crevices that act as nucleation sites for additional cavitation bubbles. The pits also increase the components' surface area and leave behind residual stresses. This makes the surface more prone to stress corrosion.\n\nMajor places where cavitation occurs are in pumps, on propellers, or at restrictions in a flowing liquid.\n\nAs an impeller's (in a pump) or propeller's (as in the case of a ship or submarine) blades move through a fluid, low-pressure areas are formed as the fluid accelerates around and moves past the blades. The faster the blade moves, the lower the pressure around it can become. As it reaches vapor pressure, the fluid vaporizes and forms small bubbles of gas. This is cavitation. When the bubbles collapse later, they typically cause very strong local shock waves in the fluid, which may be audible and may even damage the blades.\n\nCavitation in pumps may occur in two different forms:\n\nSuction cavitation occurs when the pump suction is under a low-pressure/high-vacuum condition where the liquid turns into a vapor at the eye of the pump impeller. This vapor is carried over to the discharge side of the pump, where it no longer sees vacuum and is compressed back into a liquid by the discharge pressure. This imploding action occurs violently and attacks the face of the impeller. An impeller that has been operating under a suction cavitation condition can have large chunks of material removed from its face or very small bits of material removed, causing the impeller to look spongelike. Both cases will cause premature failure of the pump, often due to bearing failure. Suction cavitation is often identified by a sound like gravel or marbles in the pump casing.\n\nCommon causes of suction cavitation can include clogged filters, pipe blockage on the suction side, poor piping design, pump running too far right on the pump curve, or conditions not meeting NPSH (net positive suction head) requirements.\n\nIn automotive applications, a clogged filter in a hydraulic system (power steering, power brakes) can cause suction cavitation making a noise that rises and falls in synch with engine RPM. It is fairly often a high pitched whine, like set of nylon gears not quite meshing correctly.\n\nDischarge cavitation occurs when the pump discharge pressure is extremely high, normally occurring in a pump that is running at less than 10% of its best efficiency point. The high discharge pressure causes the majority of the fluid to circulate inside the pump instead of being allowed to flow out the discharge. As the liquid flows around the impeller, it must pass through the small clearance between the impeller and the pump housing at extremely high flow velocity. This flow velocity causes a vacuum to develop at the housing wall (similar to what occurs in a venturi), which turns the liquid into a vapor. A pump that has been operating under these conditions shows premature wear of the impeller vane tips and the pump housing. In addition, due to the high pressure conditions, premature failure of the pump's mechanical seal and bearings can be expected. Under extreme conditions, this can break the impeller shaft.\n\nDischarge cavitation in joint fluid is thought to cause the popping sound produced by bone joint cracking, for example by deliberately cracking one's knuckles.\n\nSince all pumps require well-developed inlet flow to meet their potential, a pump may not perform or be as reliable as expected due to a faulty suction piping layout such as a close-coupled elbow on the inlet flange. When poorly developed flow enters the pump impeller, it strikes the vanes and is unable to follow the impeller passage. The liquid then separates from the vanes causing mechanical problems due to cavitation, vibration and performance problems due to turbulence and poor filling of the impeller. This results in premature seal, bearing and impeller failure, high maintenance costs, high power consumption, and less-than-specified head and/or flow.\n\nTo have a well-developed flow pattern, pump manufacturer's manuals recommend about (10 diameters?) of straight pipe run upstream of the pump inlet flange. Unfortunately, piping designers and plant personnel must contend with space and equipment layout constraints and usually cannot comply with this recommendation. Instead, it is common to use an elbow close-coupled to the pump suction which creates a poorly developed flow pattern at the pump suction.\n\nWith a double-suction pump tied to a close-coupled elbow, flow distribution to the impeller is poor and causes reliability and performance shortfalls. The elbow divides the flow unevenly with more channeled to the outside of the elbow. Consequently, one side of the double-suction impeller receives more flow at a higher flow velocity and pressure while the starved side receives a highly turbulent and potentially damaging flow. This degrades overall pump performance (delivered head, flow and power consumption) and causes axial imbalance which shortens seal, bearing and impeller life.\nTo overcome cavitation:\nIncrease suction pressure if possible.\nDecrease liquid temperature if possible.\nThrottle back on the discharge valve to decrease flow-rate.\nVent gases off the pump casing.\n\nCavitation can occur in control valves. If the actual pressure drop across the valve as defined by the upstream and downstream pressures in the system is greater than the sizing calculations allow, pressure drop flashing or cavitation may occur. The change from a liquid state to a vapor state results from the increase in flow velocity at or just downstream of the greatest flow restriction which is normally the valve port. To maintain a steady flow of liquid through a valve the flow velocity must be greatest at the vena contracta or the point where the cross sectional area is the smallest. This increase in flow velocity is accompanied by a substantial decrease in the fluid pressure which is partially recovered downstream as the area increases and flow velocity decreases. This pressure recovery is never completely to the level of the upstream pressure. If the pressure at the vena contracta drops below the vapor pressure of the fluid bubbles will form in the flow stream. If the pressure recovers after the valve to a pressure that is once again above the vapor pressure, then the vapor bubbles will collapse and cavitation will occur.\n\nWhen water flows over a dam spillway, the irregularities on the spillway surface will cause small areas of flow separation in a high-speed flow, and, in these regions, the pressure will be lowered. If the flow velocities are high enough the pressure may fall to below the local vapor pressure of the water and vapor bubbles will form. When these are carried downstream into a high pressure region the bubbles collapse giving rise to high pressures and possible cavitation damage.\n\nExperimental investigations show that the damage on concrete chute and tunnel spillways can start at clear water flow velocities of between 12 and 15 m/s, and, up to flow velocities of 20 m/s, it may be possible to protect the surface by streamlining the boundaries, improving the surface finishes or using resistant materials.\n\nWhen some air is present in the water the resulting mixture is compressible and this damps the high pressure caused\nby the bubble collapses. If the flow velocities near the spillway invert are sufficiently high, aerators (or aeration devices) must be introduced to prevent cavitation. Although these have been installed for some years, the mechanisms of air entrainment at the aerators and the slow movement of the air away from the spillway surface are still challenging.\n\nThe spillway aeration device design is based upon a small deflection of the spillway bed (or sidewall) such as a ramp and offset to deflect the high flow velocity flow away from the spillway surface. In the cavity formed below the nappe, a local subpressure beneath the nappe is produced by which air is sucked into the flow. The complete design includes the deflection device (ramp, offset) and the air supply system.\n\nSome larger diesel engines suffer from cavitation due to high compression and undersized cylinder walls. Vibrations of the cylinder wall induce alternating low and high pressure in the coolant against the cylinder wall. The result is pitting of the cylinder wall, which will eventually let cooling fluid leak into the cylinder and combustion gases to leak into the coolant.\n\nIt is possible to prevent this from happening with the use of chemical additives in the cooling fluid that form a protective layer on the cylinder wall. This layer will be exposed to the same cavitation, but rebuilds itself. Additionally a regulated overpressure in the cooling system (regulated and maintained by the coolant filler cap spring pressure) prevents the forming of cavitation.\n\nFrom about the 1980s, new designs of smaller gasoline engines also displayed cavitation phenomena. One answer to the need for smaller and lighter engines was a smaller coolant volume and a correspondingly higher coolant flow velocity. This gave rise to rapid changes in flow velocity and therefore rapid changes of static pressure in areas of high heat transfer. Where resulting vapor bubbles collapsed against a surface, they had the effect of first disrupting protective oxide layers (of cast aluminium materials) and then repeatedly damaging the newly formed surface, preventing the action of some types of corrosion inhibitor (such as silicate based inhibitors). A final problem was the effect that increased material temperature had on the relative electrochemical reactivity of the base metal and its alloying constituents. The result was deep pits that could form and penetrate the engine head in a matter of hours when the engine was running at high load and high speed. These effects could largely be avoided by the use of organic corrosion inhibitors or (preferably) by designing the engine head in such a way as to avoid certain cavitation inducing conditions.\n\nSome hypotheses relating to diamond formation posit a possible role for cavitation—namely cavitiation in the kimberlite pipes providing the extreme pressure needed to change pure carbon into the rare allotrope that is diamond.\n\nThe loudest three sounds ever recorded, during the 1883 eruption of Krakatoa, are now understood as the bursts of three huge cavitation bubbles, each larger than the last, formed in the volcano's throat. Rising magma, filled with dissolved gasses and under immense pressure, encountered a different magma that compressed easily, allowing bubbles to grow and combine. \n\nThere exist macroscopic white lamellae inside quartz and other minerals in the Bohemian Massif and even at another places in whole of the world like wavefronts generated by a meteorite impact according to the Rajlich's Hypothesis. The hypothetical wavefronts are composed of many microcavities. Their origin is seen in a physical phenomenon of ultrasonic cavitation, which is well known from the technical practice.\n\nCavitation occurs in the xylem of vascular plants when the tension of water within the xylem exceeds atmospheric pressure. The sap vaporizes locally so that either the vessel elements or tracheids are filled with water vapor. Plants are able to repair cavitated xylem in a number of ways. For plants less than 50 cm tall, root pressure can be sufficient to redissolve the vapor. Larger plants direct solutes into the xylem via \"ray cells\", or in tracheids, via osmosis through bordered pits. Solutes attract water, the pressure rises and vapor can redissolve. In some trees, the sound of the cavitation is audible, particularly in summer, when the rate of evapotranspiration is highest. Some deciduous trees have to shed leaves in the autumn partly because cavitation increases as temperatures decrease.\n\nJust as cavitation bubbles form on a fast-spinning boat propeller, they may also form on the tails and fins of aquatic animals. This primarily occurs near the surface of the ocean, where the ambient water pressure is low.\n\nCavitation may limit the maximum swimming speed of powerful swimming animals like dolphins and tuna. Dolphins may have to restrict their speed because collapsing cavitation bubbles on their tail are painful. Tuna have bony fins without nerve endings and do not feel pain from cavitation. They are slowed down when cavitation bubbles create a vapor film around their fins. Lesions have been found on tuna that are consistent with cavitation damage.\n\nSome sea animals have found ways to use cavitation to their advantage when hunting prey. The pistol shrimp snaps a specialized claw to create cavitation, which can kill small fish. The mantis shrimp (of the \"smasher\" variety) uses cavitation as well in order to stun, smash open, or kill the shellfish that it feasts upon.\n\nThresher sharks use 'tail slaps' to debilitate their small fish prey and cavitation bubbles have been seen rising from the apex of the tail arc.\n\nIn the last half-decade, coastal erosion in the form of inertial cavitation has been generally accepted. Bubbles in an incoming wave are forced into cracks in the cliff being eroded. Varying pressure decompresses some vapor pockets which subsequently implode. The resulting pressure peaks can blast apart fractions of the rock.\n\n\n\n"}
{"id": "1867894", "url": "https://en.wikipedia.org/wiki?curid=1867894", "title": "Certified wood", "text": "Certified wood\n\nCertified wood and paper products come from responsibly managed forests – as defined by a particular standard. With third-party forest certification, an independent organization develops standards of good forest management, and independent auditors issue certificates to forest operations that comply with those standards.\n\nForest certification programs typically require that forest management practices conform to existing laws. Other basic requirements or characteristics of forest certification programs include:\n\nBasic requirements of credible forest certification programs include:\n\nToday there are more than 50 certification programs worldwide addressing the many types of forests and tenures around the world. The two largest international forest certification programs are the Forest Stewardship Council (FSC) and the Programme for the Endorsement of Forest Certification (PEFC).\n\nThe PEFC is the largest certification framework in terms of forest area, with approximately two-thirds of the total certified area. The FSC program is the fastest growing.\n\nThird-party forest certification was pioneered in the early 1990s by the FSC, a collaboration between environmental NGOs, forest product companies and social interests. Competing systems quickly emerged throughout the world. Some commentators, including Jared Diamond, have suggested that many competing standards were set up by logging companies specifically aiming to confuse consumers with less rigorously enforced but similarly named competing standards.\n\nIn the United States and Canada, there are a number of forest certification programs. Three of these programs are endorsed by the PEFC. They are the American Tree Farm System (ATFS), the Canadian Standards Association’s Sustainable Forest Management Standard and the Sustainable Forestry Initiative (SFI) Program. ATFS is applicable only in the United States; the Canadian Standards Association SFM Standard is applicable only in Canada. SFI is applicable to both the United States and Canada. The FSC, program is applied throughout North America. SFI is the world’s largest regional forest certification program in terms of total certified forest area[1].\n\nThe National Association of State Foresters in the USA passed a resolution in 2008 that supports \"all\" of the forest certification systems used in the USA and recognized the value of their differences: “... the ATFS, FSC, and SFI systems include the fundamental elements of credibility and make positive contributions to forest sustainability... No certification program can credibly claim to be ‘best’, and no certification program that promotes itself as the only certification option can maintain credibility. Forest ecosystems are complex and a simplistic ‘one size fits all’ approach to certification cannot address all sustainability needs.”.\n\nThe Canadian Council of Forest Ministers issued a statement in 2008 on forest certification standards in Canada, which said: “In Canada, each jurisdiction’s forest laws, policies and administrative requirements comprise an framework that fully characterizes what sustainable forest management (SFM) means in that jurisdiction, and what actions may take place on public and/or private forest land. Governments in Canada support third-party forest certification as a tool to demonstrate the rigor of Canada’s forest management laws, and to document the country’s world-class sustainable forest management record. The forest management standards of the Canadian Standards Association (CSA), the FSC and the Sustainable Forestry Initiative (SFI) are all used in Canada. Governments in Canada accept that these standards demonstrate, and promote the sustainability of forest management practices in Canada.” \n\nChain of Custody certification tracks the certified material through the production process – from the forest to the consumer, including all successive stages of processing, transformation, manufacturing and distribution. It also provides evidence that certified material in a certified product originates from certified forests.\n\nThe United Nations reports that between January 2009 and May 2010, the total number of PEFC and FSC chain-of-custody certificates issued worldwide increased by 88% for a total of 23,717 certificates (this does not include SFI certificates).\n\nForest certification is a voluntary process. About 10% of the world’s forest under at least one certification program. Customers that choose to buy certified products are supporting land managers, land owners and forest product companies that have made a commitment to meeting the standards of forest certification.\n\nThird-party forest certification is a useful tool for those seeking to purchase paper and wood products that come from forests that are well-managed and use materials that are legally harvested. Incorporating third-party certification into forest product buying practices can be a centerpiece for responsible wood and paper purchasing policies that include factors such as the protection of sensitive forest values, thoughtful material selection and efficient use of products.\n\nThe 2009-2010 United Nations Market Review reported that companies that produced or traded in certified forest products often had a market advantage during the 2008-2009 recession because, in a buyers’ market, buyers could be more selective in choosing their sources of supply. The report cites four demand drivers for certification:\n\nThe World Resources Institute, in partnership with the Environmental Investigation Agency, released a fact sheet designed to answer some of the frequently asked questions about the Lacey Act, which was amended in 2008 to ban commerce in illegally sourced plants and their products—including timber, wood, and paper products. The fact sheet says forest certification is a very good approach for demonstrating due care by showing government and customers that a company has taken proactive steps to eliminate illegal wood or plant material from its supply chain. Certification does not relieve importers of the requirement to submit appropriate import declaration information to U.S. government agencies.\n\n"}
{"id": "53643626", "url": "https://en.wikipedia.org/wiki?curid=53643626", "title": "Conservation and restoration of insect specimens", "text": "Conservation and restoration of insect specimens\n\nThe conservation and restoration of insect specimens is the process of caring for and preserving insects as a part of a collection. Conservation concerns begin at collection and continue through preparation, storage, examination, documentation, research and treatment when restoration is needed.\n\nInsect collecting can be done in many different ways depending on the kind of insects being collected and from which habitats. Both hobbyists and professional entomologist have found particular ways to collect with minimal damage to their specimens. Following established techniques helps begin the conservation of insect specimens from the beginning by eliminating as much potential damage as possible. It must be done delicately to ensure that neither the collector nor the live insect itself will cause harm to the distinctive features such as wings, legs and antennae that give purpose to the collection. Special collection nets, traps and techniques must be utilized in consideration of how easily breakage can happen. A kill jar is often used to immediately immobilize the insect before it can damage itself.\n\nThe way an insect specimen is prepared is often the first step in their conservation. They have to be carefully prepared with the appropriate methods depending on their size, anatomy, and potentially delicate features to ensure they will not break before they begin their role as a specimen available for study, research and display. Some specimens must be prepared using a dry method and others with liquid to preserve. The choice made will preserve key features necessary for identification and represent the insect's living form as closely as possible.\n\nThe process of pinning insect specimens is a dry method to preserve and display collections and requires special entomological equipment to accomplish effectively. It is used primarily for hard-bodied, medium to large specimens and is beneficial for easier study and color preservation. Flies and butterflies, though they are partially soft-bodied, are also best preserved through pinning because when preserved in fluid their hairs and/or scales will either clot or fall off. Some smaller specimens may still be pinned use minuten pins, which are much thinner, to avoid breakage. Insects are pinned on foam block or specialized pinning blocks that provide support for the limbs while drying and may be moved to another specialized, protected display case after they have dried completely, at which point they will be more brittle. The pin is most often driven through the thorax of the insect just to the right of the mid-line to preserve the appearance of at least one side should any damage occur from pinning. The exception is butterflies, dragonflies and damselflies, which are pinned through the middle of the thorax. Enough of the pin must be left both above and below the specimen to allow for labeling below and handling.\nCarding is used when pinning would destroy too much of the specimen because its size is quite small. A triangular point is cut from acid free card to ensure best conservation practice because it comes in direct contact with the specimen. A pin is then driven through the broad side of the point for mounting. A soluble glue that can be removed with solvents when necessary is used to adhere the right side of the thorax of the specimen to the point opposite the pinned side. The point is sometimes bent to allow the specimen to present in the same position as normally pinned specimens.\n\nA wet specimen is a specimen preserved in fluid, often 70% alcohol. Specimens that would receive this preservation technique are usually soft-bodied, such as caterpillars, larva, and spiders because of their soft abdomens. This is done to minimize shriveling allowing the identifying characteristics to be preserved as true to life as possible. Hard-bodied insects may also be preserved temporarily in alcohol before pinning.\n\nSlides of very small insects are also kept as part of insect collections.\n\nThe American Institute for Conservation (AIC) describes in their Code of Ethics the aspects of conservation to include: preventive conservation, examination, documentation, treatment, research and education. Each of these areas also apply to the conservation and restoration of insect specimens.\n\nInsect collections may suffer multiple types of degradation including fading colors from light exposure, mold growth from improper humidity and temperature levels, and infestations from pests that feed on dried insect, but much of this is avoidable when proper preventive conservation practices are followed.\n\nIn addition to maintaining a clean storage environment for specimens, it is sometimes necessary to clean the specimens themselves. Cleaning incredibly delicate and brittle dry insect specimens is done carefully and methodically. The conservator chooses the appropriate method based on the kind of insect that needs cleaning and how robust it is. Cleaning tools vary widely, but generally clean watercolor brushes are used to gently dust the specimens, sometimes with a stereo microscope for very small specimens, warm water and/or alcohol baths used with or without an ultrasonic cleaner, and lens blowers to gently blow away dust, or dry the specimen after a cleansing bath.\n\nA common way to prevent damage from pests intent to feed on insect specimens and dust accumulation in a collection is to keep them in sealed boxes or cabinets. When properly sealed, they can also aid in preventing damages cause by relative humidity (RH) and temperature fluctuations. Wet specimens are kept in separate vials or jars and in a secure cabinet, tray or shelf. Fluid levels are regularly monitored to ensure specimens are completely immersed in fluid, though a well-sealed jar or vial will prevent excessive evaporation.\n\nProper handling for insect specimens prevents the excessive breaking of legs, antennae and other body parts that could then easily be lost. Curved forceps may be used to allow more precision and less chance of the brittle specimen coming in contact with the handler. The handler picks up the specimen by the pin, which is placed with enough space below the specimen for the handler to put in the pinning block and enough space above to grip without touching the specimen.\nIntegrated pest management (IPM) is a specialized modern pest control used in museums. All IPM systems begin with regular sanitation and monitoring of collections to detect castings from various pests, and checking insect traps laid out to capture and identify which pests are present. Some pests, such as carpet beetles and flour beetles, feed on dried insects. When an infestation is present, treatment may be necessary. Freezing is commonly used to rid insect collections of pests. Alternatively, inert gases may be used for an anoxic fumigation - depriving the pests of oxygen to exterminate, and in extreme cases chemical fumigation proven to be safe for collections and people may be used.\n\nAssessing the condition of an insect collection is done regularly and the results are recorded accordingly. The conservator observes the specimens in high detail remarking all areas of damage, or altered states of the specimen. Tools used during this process may include a strong light source, magnifying glass and handling tools that allow the conservator to pick up the specimen from the pin without touching it. The observations made during the examination process result in the conclusions drawn for a treatment plan if necessary. The conservator is knowledgeable of the kinds of deterioration to look for specific to insect specimens.\n\n\nThe documentation of insect specimens is carried out in many different aspects of a specimen's existence. Documented information begins with the capture of an insect. The collector records information about capture method, place and date of capture and any relevant habitat information in field notes. This information is then transferred to labels and collection records. The documentation path then continues with every recorded observation or treatment the specimen receives. Killing agents, preservation agents, rehydrating agents, and fumigants are all important to record. This then informs any future decisions for conservation actions.\n\nAs a minimum, labels contain the place and date of collection, and the identification of genus and species. On pinned insects, the labels are likewise pinned with the space left under the specimen on the same pin. There are various ways to write the information on labels, but an ink that will not fade or come off in liquid is generally used. The paper is ideally 100% cotton or linen rag to avoid yellowing or embrittlement of the paper as it ages.\nWith improvements in digital photography and web resources, many natural history museums have begun a new kind of documentation through digitization, bringing high quality images and associated information to anyone with access to the Internet. Large databases can hold vast amounts of information improving research efforts.\n\nScientific illustration of insects is an older technique used to preserve information about collected insects. It visually documents insects, and unlike photography, can add intellectual ideas about anatomy and behavior of the insect through artists' renditions. Scientifically informed observation of specimens combined with technical and aesthetic skill yields the highly detailed illustrations necessary for the documentation of each species that is illustrated.\n\nThis area of insect specimen documentation comes after the close examination of specimens by conservators. The conservator records all of the visual information about the specimen that can be gleaned from detailed inspection. Conclusions are drawn from inspection and potential treatments are also documented to inform researchers and future conservators. \nResearching the collections of insects provides important information that develops knowledge about ecology, human health and crops. Well-kept records aid the researcher in identifying whether there are differences in an observed specimen because of damages, treatments or deterioration. Research of the insect collections in museums can lead to new discoveries of species, and provide an important historical resource.\n\nOnce a close observation reveals what issues are present in insect collections, the conservator makes a decision on actions to be taken. It is highly preferable that any treatment applied be reversible or done with little risk to the specimen. For example, broken limbs may be glued back on, which has traditionally been done with white glue. The advantage to white glue being that it is removable in warm water. Another common problem is pest infestation. When dried insect collections have suffered an infestation, the affected specimens can be frozen or sealed with inert gases to kill the pests without harming the specimens. Other treatments might include simply refilling wet specimens' jars with alcohol to ensure the specimens are completely submerged, cleaning specimens of dust and debris, or repositioning specimens for display or research. In the case that a specimen needs to be repositioned, the conservator will \"relax\" the specimen in a jar with a rehydrating substrate to move the limbs without breaking them. The technique used will vary among conservators. Some use a relaxing jar that the specimen is left in for days with the substrate of choice, others may choose to use a warm water bath with a drop of detergent. Whatever treatments are used are diligently documented.\n\nConservation of insect specimens is done in large part to preserve the information for the public. The display of collections in museums and their interpretation offer one avenue that accomplishes this effort. However, websites offer a unique opportunity to disseminate information to a broad audience with layers of information to give general information or to provide depth where desired. These websites are often also provided by museums and their collections. Below is a list of some major educational endeavors with interests in insect specimens.\n\n"}
{"id": "33256286", "url": "https://en.wikipedia.org/wiki?curid=33256286", "title": "Demographics of the world", "text": "Demographics of the world\n\nDemographics of the world include population density, ethnicity, education level, health measures, economic status, religious affiliations and other aspects of the human population of the planet Earth. \n\nThe overall total population of the world is approximately 7.5 billion, as of March 2018. \n\nIts overall population density is 50 people per km² (129.28 per sq. mile), excluding Antarctica. Nearly two-thirds of the population lives in Asia and is predominantly urban and suburban, with more than 2.5 billion in the countries of China and India combined. The world's fairly low literacy rate (83.7%) is attributable to poverty. Lower literacy rates are common in South Asia, West Asia, and Sub-Saharan Africa.\n\nThe world's largest ethnic group is Han Chinese with Mandarin being the world's most spoken language in terms of native speakers.\n\nHuman migration has been shifting toward cities and urban centers, with the urban population jumping from 29% in 1950, to 50.5% in 2005. Working backwards from the United Nations prediction that the world will be 51.3 percent urban by 2010, Dr. Ron Wimberley, Dr. Libby Morris and Dr. Gregory Fulkerson estimated 23 May 2007 to be the first time the urban population outnumbered the rural population in history.\nChina and India are the most populous countries, as the birth rate has consistently dropped in developed countries and until recently remained high in developing countries. Tokyo is the largest urban conglomeration in the world.\n\nThe total fertility rate of the World is estimated as 2.52 children per woman, which is above the global average for the replacement fertility rate of approximately 2.33 (as of 2003). However, world population growth is unevenly distributed, with the total fertility rate going from .91 in Macau, to 7.68 in Niger. The United Nations estimated an annual population increase of 1.14% for the year of 2000.\nThe current world population growth is approximately 1.09%\nPeople under 18 years of age made up over a quarter of the world population (29.3%), and people age 65 and over made up less than one-tenth (7.9%) in 2011.\n\nThe world population more than tripled during the 20th century from about 1.65 billion in 1900 to 5.97 billion in 1999.\n\nIt reached the 2 billion mark in 1927, the 3 billion mark in 1960, 4 billion in 1974, and 5 billion in 1987. Currently, population growth is fastest among low wealth, Least Developed Countries countries.\n\nThe UN projects a world population of 9.15 billion in 2050, which is a 32.69% increase from 2010 (6.89 billion).\n\nHistorical migration of human populations begins with the movement of \"Homo erectus\" out of Africa across Eurasia about a million years ago. \"Homo sapiens\" appear to have occupied all of Africa about 150,000 years ago, moved out of Africa 50,000 - 60,000 years ago, and had spread across Australia, Asia and Europe by 30,000 years BC. Migration to the Americas took place 20,000 to 15,000 years ago, and by 2,000 years ago, most of the Pacific Islands were colonized.\n\nUntil c. 10,000 years ago, humans lived as hunter-gatherers. They generally lived in small nomadic groups known as band societies. The advent of agriculture prompted the Neolithic Revolution, when access to food surplus led to the formation of permanent human settlements. About 6,000 years ago, the first proto-states developed in Mesopotamia, Egypt's Nile Valley and the Indus Valley. Early human settlements were dependent on proximity to water and, depending on the lifestyle, other natural resources used for subsistence. But humans have a great capacity for altering their habitats by means of technology.\n\nSince 1800, the human population has increased from one billion to over seven billion, In 2004, some 2.5 billion out of 6.3 billion people (39.7%) lived in urban areas. In February 2008, the U.N. estimated that half the world's population would live in urban areas by the end of the year. Problems for humans living in cities include various forms of pollution and crime, especially in inner city and suburban slums. Both overall population numbers and the proportion residing in cities are expected to increase significantly in the coming decades.\n\nThe World has hundreds of major cities spread across 6 continents. Most are in coastal regions.\n\n, the World had 62 metropolitan areas with a population of over 3,000,000 people each.\n\nAs of 2010, about 3 billion people live in or around urban areas.\n\nThe following table shows the populations of the top ten conglomerations.\n\nThe world's population is 7 billion and Earth's total area (including land and water) is 510 million square kilometers (197 million square miles). Therefore, the worldwide human population density is 7 billion ÷ 510 million = 13.7 per km² (35.5 per sq. mile). If only the Earth's land area of 150 million km² (58 million sq. miles) is taken into account, then human population density increases to 46.7 per km² (117.2 per sq. mile). This calculation includes all continental and island land area, including Antarctica. If Antarctica is also excluded, then population density rises to 50 people per km² (129.28 per sq. mile). Considering that over half of the Earth's land mass consists of areas inhospitable to human inhabitation, such as deserts and high mountains, and that population tends to cluster around seaports and fresh water sources, this number by itself does not give any meaningful measurement of human population density.\n\nSeveral of the most densely populated territories in the world are city-states, microstates or dependencies. These territories share a relatively small area and a high urbanization level, with an economically specialized city population drawing also on rural resources outside the area, illustrating the difference between high population density and overpopulation.\n\nThe table below lists religions classified by philosophy; however, religious philosophy is not always the determining factor in local practice. Please note that this table includes heterodox movements as adherents to their larger philosophical category, although this may be disputed by others within that category. For example, Cao Đài is listed because it claims to be a separate category from Buddhism, while Hòa Hảo is not, even though they are similar new religious movements.\n\nThe population numbers below are computed by a combination of census reports, random surveys (in countries where religion data is not collected in census, for example United States or France), and self-reported attendance numbers, but results can vary widely depending on the way questions are phrased, the definitions of religion used and the bias of the agencies or organizations conducting the survey. Informal or unorganized religions are especially difficult to count. Some organizations may wildly inflate their numbers.\n\nSince the late 19th century, the demographics of religion have changed a great deal. Some countries with a historically large Christian population have experienced a significant decline in the numbers of professed active Christians: see demographics of atheism. Symptoms of the decline in active participation in Christian religious life include declining recruitment for the priesthood and monastic life, as well as diminishing attendance at church. On the other hand, since the 19th century, large areas of sub-Saharan Africa have been converted to Christianity, and this area of the world has the highest population growth rate. In the realm of Western civilization, there has been an increase in the number of people who identify themselves as secular humanists. In many countries, such as the People's Republic of China, communist governments have discouraged religion, making it difficult to count the actual number of believers. However, after the collapse of communism in numerous countries of Eastern Europe and the former Soviet Union, religious life has been experiencing resurgence there, both in the form of traditional Eastern Christianity and in the forms of Neopaganism and Far Eastern religions.\n\nFollowing is some available data based on the work of the \"World Christian Encyclopedia\":\n\nStudies conducted by the Pew Research Center have found that, generally, poorer nations had a larger proportion of citizens who found religion to be very important than richer nations, with the exceptions of the United States and Kuwait.\n\nThe average age of marriage varies greatly from country to country and has varied through time. Women tend to marry earlier than men and currently varies from 17.6 for women in Niger, to 32.4 for women in Denmark while men range from 22.6 in Mozambique to 35.1 in Sweden.\n\nThe average number of hospital beds per 1,000 population is 2.94. Compare to Switzerland (18.3) and Mexico (1.1)\n\n96% of the urban population has access to improved drinking water, while only 78% of rural inhabitants have improved drinking water. A total average of 87% of urban and rural have access to improved drinking water.\n\n4% of the urban population does not have access to improved drinking water, leaving 22% of rural people without improved drinking water with a total world population of 13% not having access to drinking water.\n\n76% of the urban population has access to sanitation facilities, while only 45% of the rural population has access. A total world average of 39% do not have access to sanitation facilities.\n\nAs of 2009, there are an estimated 33.3 million people living with HIV/AIDS, which is approximately 0.8% of the world population, and there have been an estimated 1.8 million deaths attributed to HIV/AIDS.\n\nAs of 2010, 925 million people are undernourished.\n\nLife Expectancy at Birth:\n\nInfant Mortality\n\nThe following demographic statistics are from the CIA World Factbook, unless otherwise indicated.\n\nAccording to the 2006 CIA World Factbook, around 27% of the world's population is below 15 years of age.\n\nAccording to a report by the Global Social Change Research Project, worldwide, the percent of the population age 0-14 declined from 34% in 1950 to 27% in 2010. On the other hand, the percent elderly (60+) increased during the same period from 8% to 11%.\n\nGlobally, the growth rate of the human population has been declining since peaking in 1962 and 1963 at 2.20% per annum. In 2009, the estimated annual growth rate was 1.1%. The CIA World Factbook gives the world annual birthrate, mortality rate, and growth rate as 1.915%, 0.812%, and 1.092% respectively The last one hundred years have seen a rapid increase in population due to medical advances and massive increase in agricultural productivity made possible by the Green Revolution.\nThe actual annual growth in the number of humans fell from its peak of 88.0 million in 1989, to a low of 73.9 million in 2003, after which it rose again to 75.2 million in 2006. Since then, annual growth has declined. In 2009, the human population increased by 74.6 million, which is projected to fall steadily to about 41 million per annum in 2050, at which time the population will have increased to about 9.2 billion. Each region of the globe has seen great reductions in growth rate in recent decades, though growth rates remain above 2% in some countries of the Middle East and Sub-Saharan Africa, and also in South Asia, Southeast Asia, and Latin America.\n\nSome countries experienced negative population growth, especially in Eastern Europe mainly due to low fertility rates, high death rates and emigration. In Southern Africa, growth is slowing due to the high number of HIV-related deaths. Some Western Europe countries might also encounter negative population growth. Japan's population began decreasing in 2005.\n\nPopulation in the world increased from 1990 to 2008 with 1,423 million and 27% growth. Measured by persons, the increase was highest in India (290 million) and China (192 million). Population growth was highest in Qatar (174%) and United Arab Emirates (140%).\n\nData required on total number of births per year, and distribution by country.\n\nAs of 2009, the average birth rate (unclear whether this is the weighted average rate per country [with each country getting a weight of 1], or the unweighted average of the entire world population) for the whole world is 19.95 per year per 1000 total population, a 0.48% decline from 2003's world birth rate of 20.43 per 1000 total population. \n\nAccording to the CIA - The World Factbook, the country with the highest birth rate currently is Niger at 51.26 births per 1000 people. The country with the lowest birth rate is Japan at 7.64 births per 1000 people. Hong Kong, a Special Administrative Region of China, is at 7.42 births per 1000 people. As compared to the 1950s, birth rate was at 36 births per 1000 in the 1950s, birth rate has declined by 16 births per 1000 people. In July 2011, the U.S. National Institutes of Health announced that the adolescent birth rate continues to decline. \n\nBirth rates vary even within the same geographic areas. In Europe, as of July 2011, Ireland's birth rate is 16.5 per cent, which is 3.5 per cent higher than the next-ranked country, the UK. France has a birth rate of 12.8 per cent while Sweden is at 12.3 per cent. In July 2011, the UK's Office for National Statistics (ONS) announced a 2.4% increase in live births in the UK in 2010 alone. This is the highest birth rate in the UK in 40 years. By contrast, the birth rate in Germany is only 8.3 per 1,000, which is so low that both the UK and France, which have significantly smaller populations, produced more births in 2010. Birth rates also vary within the same geographic area, based on different demographic groups. For example, in April 2011, the U.S. CDC announced that the birth rate for women over the age of 40 in the U.S. rose between 2007 and 2009, while it fell among every other age group during the same time span. In August 2011, Taiwan's government announced that its birth rate declined in the previous year, despite the fact that it implemented a host of approaches to encourage its citizens to have babies.\n\nBirth rates ranging from 10-20 births per 1000 are considered low, while rates from 40-50 births per 1000 are considered high. There are problems associated with both an extremely high birth rate and an extremely low birth rate. High birth rates can cause stress on the government welfare and family programs to support a youthful population. Additional problems faced by a country with a high birth rate include educating a growing number of children, creating jobs for these children when they enter the workforce, and dealing with the environmental effects that a large population can produce. Low birth rates can put stress on the government to provide adequate senior welfare systems and also the stress on families to support the elders themselves. There will be less children or working age population to support the constantly growing aging population.\n\nThe ten countries with the highest crude death rate, according to the 2015 CIA World Factbook estimates, are:\n\nSee list of countries by death rate for worldwide statistics.\n\nAccording to the World Health Organization, the 10 leading causes of death in 2002 were:\n\n\nCauses of death vary greatly between first and third world countries.\n\nAccording to Jean Ziegler (the United Nations Special Rapporteur on the Right to Food for 2000 to March 2008), mortality due to malnutrition accounted for 58% of the total mortality in 2006: \"In the world, approximately 62 millions people, all causes of death combined, die each year. In 2006, more than 36 millions died of hunger or diseases due to deficiencies in micronutrients\".\n\nOf the roughly 150,000 people who died each day across the globe, about two thirds—100,000 per day—died of age-related causes in 2001, according to an article which counts all deaths \"due to causes that kill hardly anyone under the age of 40\" as age-related.<ref name=\"doi10.2202/1941-6008.1011\"></ref> In industrialized nations, the proportion was even higher according to that article, reaching 90%.\n\nThe Northern Mariana Islands have the highest female ratio with 0.77 males per female. Qatar has the highest male ratio, with 2.87 males/female. For the group aged below 15, Sierra Leone has the highest female ratio with 0.96 males/female, and the Republic of Georgia and the People's Republic of China are tied for the highest male ratio with 1.13 males/female (according to the 2006 CIA World Factbook).\n\nThe value for the entire world population is 1.02 males/female, with 1.07 at birth, 1.06 for those under 15, 1.02 for those between 15 and 64, and 0.78 for those over 65.\n\nThe \"First World\" G7 members all have a sex ratio in the range of 0.95–0.98 for the total population, of 1.05–1.07 at birth, of 1.05–1.06 for the group below 15, of 1.00–1.04 for the group aged 15–64, and of 0.70–0.75 for those over 65.\n\nCountries on the Arabian Peninsula tend to have a 'natural' ratio of about 1.05 at birth but a very high ratio of males for those over 65 (Saudi Arabia 1.13, United Arab Emirates 2.73, Qatar 2.84), indicating either an above-average mortality rate for females or a below-average mortality for males, or, more likely in this case, a large population of aging male guest workers. Conversely, countries of Eastern Europe (the Baltic states, Belarus, Ukraine, Russia) tend to have a 'normal' ratio at birth but a very low ratio of males among those over 65 (Russia 0.46, Latvia 0.48, Ukraine 0.52); similarly, Armenia has a far above average male ratio at birth (1.17), and a below-average male ratio above 65 (0.67). This effect may be caused by emigration and higher male mortality as result of higher post-Soviet era deaths; it may also be related to the enormous (by western standards) rate of alcoholism in the former Soviet states. Another possible contributory factor is an aging population, with a higher than normal proportion of relatively elderly people: we recall that due to higher differential mortality rates the ratio of males to females reduces for each year of age.\n\nThere is an inverse correlation between income and fertility, wherein developed countries usually have a much lower fertility rate. Various fertility factors may be involved, such as education and urbanization. Mortality rates are low, birth control is understood and easily accessible, and costs are often deemed very high because of education, clothing, feeding, and social amenities. With wealth, contraception becomes affordable. However, in countries like Iran where contraception was made artificially affordable before the economy accelerated, birth rate also rapidly declined. Further, longer periods of time spent getting higher education often mean women have children later in life. Female labor participation rate also has substantial negative impact on fertility. However, this effect is neutralized among Nordic or liberalist countries.\n\nIn undeveloped countries on the other hand, families desire children for their labour and as caregivers for their parents in old age. Fertility rates are also higher due to the lack of access to contraceptives, generally lower levels of female education, and lower rates of female employment in industry.\n\n8.7% (2010 est.)\n8.2% (2009 est.)\nnote: 30% combined unemployment and underemployment in many non-industrialized countries; developed countries typically 4%-12% unemployment (2007 est.)\n\nThe demonym for Earth's inhabitants is Earthling or, as either a noun or an adjective, Terran.\n\nWorldwide, English is used widely as a lingua franca and can be seen to be the dominant language at this time. The world's largest language by native speakers is Mandarin Chinese which is a first language of around 960 million people, or 12.44% of the population, predominantly in Greater China. Spanish is spoken by around 330 to 400 million people, predominantly in the Americas and Spain. Arabic is spoken by around 280 million people. Hindi is spoken by about 200 million speakers, mostly in India. Bengali is spoken by around 230 million people, predominantly in India and Bangladesh. Portuguese is spoken by about 230 million speakers in Portugal, Brazil, East Timor, and Southern Africa.\n\nThere are numerous other languages, grouped into nine major families:\n\n\nThere are also hundreds of non-verbal sign languages.\n\nTotal population: 83.7% over the age of 15 can read and write, 88.3% male and 79.2% female \nnote: over two-thirds of the world's 793 million illiterate adults are found in only eight countries (Bangladesh, China, Egypt, Ethiopia, India, Indonesia, Nigeria, and Pakistan); of all the illiterate adults in the world, two-thirds are women; extremely low literacy rates are concentrated in three regions, the Arab states, South and West Asia, and Sub-Saharan Africa, where around one-third of the men and half of all women are illiterate (2005-09 est.) \n\nAs of 2008, the school life expectancy (primary to tertiary education) for a man or woman is 11 years. \n"}
{"id": "944638", "url": "https://en.wikipedia.org/wiki?curid=944638", "title": "Earth's energy budget", "text": "Earth's energy budget\n\nEarth's energy budget accounts for the balance between the energy Earth receives from the Sun, the energy Earth radiates back into outer space after having been distributed throughout the five components of Earth's climate system and having thus powered the so-called Earth’s heat engine. This system is made up of earth's water, ice, atmosphere, rocky crust, and all living things.\n\nQuantifying changes in these amounts is required to accurately model the Earth's climate. \n\nReceived radiation is unevenly distributed over the planet, because the Sun heats equatorial regions more than polar regions. The atmosphere and ocean work non-stop to even out solar heating imbalances through evaporation of surface water, convection, rainfall, winds, and ocean circulation. Earth is very close to being in radiative equilibrium, the situation where the incoming solar energy is balanced by an equal flow of heat to space; under that condition, global temperatures will be \"relatively\" stable. Globally, over the course of the year, the Earth system—land surfaces, oceans, and atmosphere—absorbs and then radiates back to space an average of about 240 watts of solar power per square meter. Anything that increases or decreases the amount of incoming or outgoing energy will change global temperatures in response.\n\nHowever, Earth's energy balance and heat fluxes depend on many factors, such as atmospheric composition (mainly aerosols and greenhouse gases), the albedo (reflectivity) of surface properties, cloud cover and vegetation and land use patterns.\n\nChanges in surface temperature due to Earth's energy budget do not occur instantaneously, due to the inertia of the oceans and the cryosphere. The net heat flux is buffered primarily by becoming part of the ocean's heat content, until a new equilibrium state is established between radiative forcings and the climate response.\n\nIn spite of the enormous transfers of energy into and from the Earth, it maintains a relatively constant temperature because, as a whole, there is little net gain or loss: Earth emits via atmospheric and terrestrial radiation (shifted to longer electromagnetic wavelengths) to space about the same amount of energy as it receives via insolation (all forms of electromagnetic radiation).\n\nTo quantify Earth's \"heat budget\" or \"heat balance\", let the insolation received at the top of the atmosphere be 100 units (100 units = about 1,360 watts per square meter facing the sun), as shown in the accompanying illustration. Called the albedo of Earth, around 35 units are reflected back to space: 27 from the top of clouds, 2 from snow and ice-covered areas, and 6 by other parts of the atmosphere. The 65 remaining units are absorbed: 14 within the atmosphere and 51 by the Earth’s surface. These 51 units are radiated to space in the form of terrestrial radiation: 17 directly radiated to space and 34 absorbed by the atmosphere (19 through latent heat of condensation, 9 via convection and turbulence, and 6 directly absorbed). The 48 units absorbed by the atmosphere (34 units from terrestrial radiation and 14 from insolation) are finally radiated back to space. These 65 units (17 from the ground and 48 from the atmosphere) balance the 65 units absorbed from the sun in order to maintain zero net gain of energy by the Earth.\n\nThe total amount of energy received per second at the top of Earth's atmosphere (TOA) is measured in watts and is given by the solar constant times the cross-sectional area of the Earth. Because the surface area of a sphere is four times the cross-sectional surface area of a sphere (i.e. the area of a circle), the average TOA flux is one quarter of the solar constant and so is approximately 340 W/m². Since the absorption varies with location as well as with diurnal, seasonal and annual variations, the numbers quoted are long-term averages, typically averaged from multiple satellite measurements.\n\nOf the ~340 W/m² of solar radiation received by the Earth, an average of ~77 W/m² is reflected back to space by clouds and the atmosphere and ~23 W/m² is reflected by the surface albedo, leaving ~240 W/m² of solar energy input to the Earth's energy budget. This gives the earth a mean net albedo of 0.29.\n\nThe geothermal heat flux from the Earth's interior is estimated to be 47 terawatts. This comes to 0.087 watt/square metre, which represents only 0.027% of Earth's total energy budget at the surface, which is dominated by 173,000 terawatts of incoming solar radiation.\n\nHuman production of energy is even lower, at an estimated 18 TW.\n\nPhotosynthesis has a larger effect: photosynthetic efficiency turns up to 2% of incoming sunlight into biomass, for a total photosynthetic productivity of earth between ~1500–2250 TW (~1%+/-0.26% solar energy hitting the Earth's surface).\n\nOther minor sources of energy are usually ignored in these calculations, including accretion of interplanetary dust and solar wind, light from stars other than the Sun and the thermal radiation from space. Earlier, Joseph Fourier had claimed that deep space radiation was significant in a paper often cited as the first on the greenhouse effect.\n\nLongwave radiation is usually defined as outgoing infrared energy leaving the planet. However, the atmosphere absorbs parts initially, or cloud cover can reflect radiation. Generally, heat energy is transported between the planet's surface layers (land and ocean) to the atmosphere, transported via evapotranspiration and latent heat fluxes or conduction/convection processes. Ultimately, energy is radiated in the form of longwave infrared radiation back into space.\n\nRecent satellite observations indicate additional precipitation, which is sustained by increased energy leaving the surface through evaporation (the latent heat flux), offsetting increases in longwave flux to the surface.\n\nIf the incoming energy flux is not equal to the outgoing energy flux, net heat is added to or lost by the planet (if the incoming flux is larger or smaller than the outgoing respectively).\n\nAn imbalance must show in something on Earth warming or cooling (depending on the direction of the imbalance), and the ocean being the larger thermal reservoir on Earth, is a prime candidate for measurements.\n\nEarth's energy imbalance measurements provided by Argo floats have detected an accumulation of ocean heat content (OHC). The estimated imbalance was measured during a deep solar minimum of 2005–2010 to be 0.58 ± 0.15 W/m². This level of detail cannot be inferred directly from measurements of surface energy fluxes, which have combined uncertainties of the order of ± 17 W/m².\n\nSeveral satellites indirectly measure the energy absorbed and radiated by Earth and by inference the energy imbalance. The NASA Earth Radiation Budget Experiment (ERBE) project involves three such satellites: the Earth Radiation Budget Satellite (ERBS), launched October 1984; NOAA-9, launched December 1984; and NOAA-10, launched September 1986.\n\nToday NASA's satellite instruments, provided by CERES, part of the NASA's Earth Observing System (EOS), are designed to measure both solar-reflected and Earth-emitted radiation.\n\nThe major atmospheric gases (oxygen and nitrogen) are transparent to incoming sunlight but are also transparent to outgoing thermal (infrared) radiation. However, water vapor, carbon dioxide, methane and other trace gases are opaque to many wavelengths of thermal radiation. The Earth's surface radiates the net equivalent of 17 percent of the incoming solar energy in the form of thermal infrared. However, the amount that directly escapes to space is only about 12 percent of incoming solar energy. The remaining fraction, 5 to 6 percent, is absorbed by the atmosphere by greenhouse gas molecules.\nWhen greenhouse gas molecules absorb thermal infrared energy, their temperature rises. Those gases then radiate an increased amount of thermal infrared energy in all directions. Heat radiated upward continues to encounter greenhouse gas molecules; those molecules also absorb the heat, and their temperature rises and the amount of heat they radiate increases. The atmosphere thins with altitude, and at roughly 5–6 kilometres, the concentration of greenhouse gases in the overlying atmosphere is so thin that heat can escape to space.\n\nBecause greenhouse gas molecules radiate infrared energy in all directions, some of it spreads downward and ultimately returns to the Earth's surface, where it is absorbed. The Earth's surface temperature is thus higher than it would be if it were heated only by direct solar heating. This supplemental heating is the natural greenhouse effect. It is as if the Earth is covered by a blanket that allows high frequency radiation (sunlight) to enter, but slows the rate at which the low frequency infrared radiant energy emitted by the Earth leaves.\n\nA change in the incident radiated portion of the energy budget is referred to as a radiative forcing.\n\nClimate sensitivity is the steady state change in the equilibrium temperature as a result of changes in the energy budget.\n\nClimate forcings are changes that cause temperatures to rise or fall, disrupting the energy balance. Natural climate forcings include changes in the Sun's brightness, Milankovitch cycles (small variations in the shape of Earth's orbit and its axis of rotation that occur over thousands of years) and volcanic eruptions that inject light-reflecting particles as high as the stratosphere. Man-made forcings include particle pollution (aerosols) that absorb and reflect incoming sunlight; deforestation, which changes how the surface reflects and absorbs sunlight; and the rising concentration of atmospheric carbon dioxide and other greenhouse gases, which decreases the rate at which heat is radiated to space.\n\nA forcing can trigger feedbacks that intensify (positive feedback) or weaken (negative feedback) the original forcing. For example, loss of ice at the poles, which makes them less reflective, causes greater absorption of energy and so increases the rate at which the ice melts, is an example of a positive feedback.\n\nThe observed planetary energy imbalance during the recent solar minimum shows that solar forcing of climate, although natural and significant, is overwhelmed by anthropogenic climate forcing.\n\nIn 2012, NASA scientists reported that to stop global warming atmospheric CO content would have to be reduced to 350 ppm or less, assuming all other climate forcings were fixed. The impact of anthropogenic aerosols has not been quantified, but individual aerosol types are thought to have substantial heating and cooling effects.\n\n\n"}
{"id": "40159918", "url": "https://en.wikipedia.org/wiki?curid=40159918", "title": "Ecosystem health", "text": "Ecosystem health\n\nEcosystem health is a metaphor used to describe the condition of an ecosystem. Ecosystem condition can vary as a result of fire, flooding, drought, extinctions, invasive species, climate change, mining, overexploitation in fishing, farming or logging, chemical spills, and a host of other reasons. There is no universally accepted benchmark for a healthy ecosystem, rather the apparent health status of an ecosystem can vary depending upon which health metrics are employed in judging it and which societal aspirations are driving the assessment. Advocates of the health metaphor argue for its simplicity as a communication tool. \"Policy-makers and the public need simple, understandable concepts like health.\" Critics worry that ecosystem health, a \"value-laden construct\", is often \"passed off as science to unsuspecting policy makers and the public.\"\n\nThe health metaphor applied to the environment has been in use at least since the early 1800s and the great American conservationist Aldo Leopold (1887–1948) spoke metaphorically of land health, land sickness, mutilation, and violence when describing land use practices. The term \"ecosystem management\" has been in use at least since the 1950s. The term \"ecosystem health\" has become widespread in the ecological literature, as a general metaphor meaning something good, and as an environmental quality goal in field assessments of rivers, lakes, seas, and forests.\n\nRecently however this metaphor has been subject of quantitative formulation using complex systems concepts such as criticality, meaning that a healthy ecosystem is in some sort of balance between adaptability (randomness) and robustness (order) . Nevertheless the universality of criticality is still under examination and is known as the Criticality Hypothesis, which states that systems in a dynamic regime shifting between order and disorder, attain the highest level of computational capabilities and achieve an optimal trade-off between robustness and flexibility. Recent results in cell and evolutionary biology, neuroscience and computer science have great interest in the criticality hypothesis, emphasizing its role as a viable candidate general law in the realm of adaptive complex systems (see and references therein).\n\nThe term ecosystem health has been employed to embrace some suite of environmental goals deemed desirable. Edward Grumbine's highly cited paper \"What is ecosystem management?\" surveyed ecosystem management and ecosystem health literature and summarized frequently encountered goal statements:\n\nGrumbine describes each of these goals as a \"value statement\" and stresses the role of human values in setting ecosystem management goals.\n\nIt is the last goal mentioned in the survey, accommodating humans, that is most contentious. \"We have observed that when groups of stakeholders work to define … visions, this leads to debate over whether to emphasize ecosystem health or human well-being … Whether the priority is ecosystems or people greatly influences stakeholders' assessment of desirable ecological and social states.\" and, for example, \"For some, wolves are critical to ecosystem health and an essential part of nature, for others they are a symbol of government overreach threatening their livelihoods and cultural values.\"\n\nMeasuring ecosystem health requires extensive goal-driven environmental sampling. For example, a vision for ecosystem health of Lake Superior was developed by a public forum and a series of objectives were prepared for protection of habitat and maintenance of populations of some 70 indigenous fish species. A suite of 80 lake health indicators was developed for the Great Lakes Basin including monitoring native fish species, exotic species, water levels, phosphorus levels, toxic chemicals, phytoplankton, zooplankton, fish tissue contaminants, etc.\n\nSome authors have attempted broad definitions of ecosystem health, such as benchmarking as healthy the historical ecosystem state \"prior to the onset of anthropogenic stress.\" A difficulty is that the historical composition of many human-altered ecosystems is unknown or unknowable. Also, fossil and pollen records indicate that the species that occupy an ecosystem reshuffle through time, so it is difficult to identify one snapshot in time as optimum or \"healthy.\".\n\nA commonly cited broad definition states that a healthy ecosystem has three attributes:\n\nWhile this captures significant ecosystem properties, a generalization is elusive as those properties do not necessarily co-vary in nature. For example, there is not necessarily a clear or consistent relationship between productivity and species richness. Similarly, the relationship between resilience and diversity is complex, and ecosystem stability may depend upon one or a few species rather than overall diversity. And some undesirable ecosystems are highly productive.\n\n\"Resilience is not desirable per se. There can be highly resilient states of ecosystems which are very undesirable from some human perspectives , such as algal-dominated coral reefs.\" Ecological resilience is a \"capacity\" that varies depending upon which properties of the ecosystem are to be studied and depending upon what kinds of disturbances are considered and how they are to be quantified. Approaches to assessing it \"face high uncertainties and still require a considerable amount of empirical and theoretical research.\"\n\nOther authors have sought a numerical index of ecosystem health that would permit quantitative comparisons among ecosystems and within ecosystems over time. One such system employs ratings of the three properties mentioned above: Health = system vigor x system organization x system resilience. Ecologist Glenn Suter argues that such indices employ \"nonsense units,\" the indices have \"no meaning; they cannot be predicted, so they are not applicable to most regulatory problems; they have no diagnostic power; effects of one component are eclipsed by responses of other components, and the reason for a high or low index value is unknown.\"\n\nHealth metrics are determined by stakeholder goals, which drive ecosystem definition. An ecosystem is an abstraction. \"Ecosystems cannot be identified or found in nature. Instead, they must be delimited by an observer. This can be done in many different ways for the same chunk of nature, depending on the specific perspectives of interest.\"\n\nEcosystem definition determines the acceptable range of variability (reference conditions) and determines measurement variables. The latter are used as indicators of ecosystem structure and function, and can be used as indicators of \"health\".\n\nAn indicator is a variable, such as a chemical or biological property, that when measured, is used to infer trends in another (unmeasured) environmental variable or cluster of unmeasured variables (the indicandum). For example, rising mortality rate of canaries in a coal mine is an indicator of rising carbon monoxide levels. Rising chlorophyll-a levels in a lake may signal eutrophication.\n\nEcosystem assessments employ two kinds of indicators, descriptive indicators and normative indicators. \"Indicators can be used descriptively for a scientific purpose or normatively for a political purpose.\"\n\nUsed descriptively, high chlorophyll-a is an indicator of eutrophication, but it may also be used as an ecosystem health indicator. When used as a normative (health) indicator, it indicates a rank on a health scale, a rank that can vary widely depending on societal preferences as to what is desirable. A high chlorophyll-a level in a natural successional wetland might be viewed as healthy whereas a human-impacted wetland with the \"same\" indicator value may be judged unhealthy.\n\nEstimation of ecosystem health has been criticized for intermingling the two types of environmental indicators. A health indicator is a normative indicator, and if conflated with descriptive indicators \"implies that normative values can be measured objectively, which is certainly not true. Thus, implicit values are insinuated to the reader, a situation which has to be avoided.\"\n\nIt can be argued that the very act of selecting indicators of any kind is biased by the observer's perspective but separation of goals from descriptions has been advocated as a step toward transparency: \"A separation of descriptive and normative indicators is essential from the perspective of the philosophy of science … Goals and values cannot be deduced directly from descriptions … a fact that is emphasized repeatedly in the literature of environmental ethics … Hence, we advise always specifying the definition of indicators and propose clearly distinguishing ecological indicators in science from policy indicators used for decision-making processes.\"\n\nAnd integration of multiple, possibly conflicting, normative indicators into a single measure of \"ecosystem health\" is problematic. Using 56 indicators, \"determining environmental status and assessing marine ecosystems health in an integrative way is still one of the grand challenges in marine ecosystems ecology, research and management\"\n\nAnother issue with indicators is validity. Good indicators must have an independently validated high predictive value, that is high sensitivity (high probability of indicating a significant change in the indicandum) and high specificity (low probability of wrongly indicating a change). The reliability of various health metrics has been questioned and \"what combination of measurements should be used to evaluate ecosystems is a matter of current scientific debate.\" Most attempts to identify ecological indicators have been correlative rather than derived from prospective testing of their predictive value and the selection process for many indicators has been based upon weak evidence or has been lacking in evidence.\n\nIn some cases no reliable indicators are known: \"We found no examples of invertebrates successfully used in [forest] monitoring programs. Their richness and abundance ensure that they play significant roles in ecosystem function but thwart focus on a few key species.\" And, \"Reviews of species-based monitoring approaches reveal that no single species, nor even a group of species, accurately reflects entire communities. Understanding the response of a single species may not provide reliable predictions about a group of species even when the group is a few very similar species.\"\n\nA trade-off between human health and the \"health\" of nature has been termed the \"health paradox\" and it illuminates how human values drive perceptions of ecosystem health.\n\nHuman health has benefited by sacrificing the \"health\" of wild ecosystems, such as dismantling and damming of wild valleys, destruction of mosquito-bearing wetlands, diversion of water for irrigation, conversion of wilderness to farmland, timber removal, and extirpation of tigers, whales, ferrets, and wolves.\n\nThere has been an acrimonious schism among conservationists and resource managers over the question of whether to \"ratchet back human domination of the biosphere\" or whether to embrace it. These two perspectives have been characterized as utilitarian vs protectionist.\n\nThe utilitarian view treats human health and well-being as criteria of ecosystem health. For example, destruction of wetlands to control malaria mosquitoes \"resulted in an improvement in ecosystem health.\"\nThe protectionist view treats humans as an invasive species: \"If there was ever a species that qualified as an invasive pest, it is \"Homo sapiens\",\"\n\nProponents of the utilitarian view argue that \"healthy ecosystems are characterized by their capability to sustain healthy human populations,\" and \"healthy ecosystems must be economically viable,\" as it is \"unhealthy\" ecosystems that are likely to result in increases in contamination, infectious diseases, fires, floods, crop failures and fishery collapse.\n\nProtectionists argue that privileging of human health is a conflict of interest as humans have demolished massive numbers of ecosystems to maintain their welfare, also disease and parasitism are historically normal in pre-industrial nature. Diseases and parasites promote ecosystem functioning, driving biodiversity and productivity, and parasites may constitute a significant fraction of ecosystem biomass.\n\nThe very choice of the word \"health\" applied to ecology has been questioned as lacking in neutrality in a BioScience article on responsible use of scientific language: \"Some conservationists fear that these terms could endorse human domination of the planet … and could exacerbate the shifting cognitive baseline whereby humans tend to become accustomed to new and often degraded ecosystems and thus forget the nature of the past.\"\n\nCriticism of ecosystem health largely targets the failure of proponents to explicitly distinguish the normative dimension from the descriptive dimension, and has included the following:\n\nAlternatives have been proposed for the term ecosystem health, including more neutral language such as ecosystem status, ecosystem prognosis, and ecosystem sustainability. Another alternative to the use of a health metaphor is to \"express exactly and clearly the public policy and the management objective\", to employ habitat descriptors and real properties of ecosystems. An example of a policy statement is \"The maintenance of viable natural populations of wildlife and ecological functions always takes precedence over any human use of wildlife.\" An example of a goal is \"Maintain viable populations of all native species in situ.\" An example of a management objective is \"Maintain self-sustaining populations of lake whitefish within the range of abundance observed during 1990-99.\"\n\nKurt Jax presented an ecosystem assessment format that avoids imposing a preconceived notion of normality, that avoids the muddling of normative and descriptive, and that gives serious attention to ecosystem definition. (1) Societal purposes for the ecosystem are negotiated by stakeholders, (2) a functioning ecosystem is defined with emphasis on phenomena relevant to stakeholder goals, (3) benchmark reference conditions and permissible variation of the system are established, (4) measurement variables are chosen for use as indicators, and (5) the time scale and spatial scale of assessment are decided.\n\nEcological health has been used as a medical term in reference to human allergy and multiple chemical sensitivity and as a public health term for programs to modify health risks (diabetes, obesity, smoking, etc.). Human health itself, when viewed in its broadest sense, is viewed as having ecological foundations. It is also an urban planning term in reference to \"green\" cities (composting, recycling), and has been used loosely with regard to various environmental issues, and as the condition of human-disturbed environmental sites. Ecosystem integrity implies a condition of an ecosystem exposed to a minimum of human influence. Ecohealth is the relationship of human health to the environment, including the effect of climate change, wars, food production, urbanization, and ecosystem structure and function. Ecosystem management and ecosystem-based management refer to the sustainable management of ecosystems and in some cases may employ the terms ecosystem health or ecosystem integrity as a goal.\n"}
{"id": "7900498", "url": "https://en.wikipedia.org/wiki?curid=7900498", "title": "Energy being", "text": "Energy being\n\nAn energy being or astral being is a theoretical life form that is composed of energy rather than matter. They appear in myths/legends, paranormal/UFO accounts, and in various works of speculative fiction.\n\nEnergy beings are typically rendered as a translucent glowing fluid or as a collection of flames or electrical sparks or bolts; somewhat in common with the representations of ghosts.\n\nEnergy beings have a variety of capacities. The Taelons (from \"\") are barely more powerful than mortals, while others such as \"Star Trek\"s Q, \"Stargate SG-1\"s Ascended Ancients/Ori, \"\"s Anodites, or the Meekrob from \"Invader Zim\" possess god-like powers.\n\n\n"}
{"id": "40745870", "url": "https://en.wikipedia.org/wiki?curid=40745870", "title": "Escape and radiate coevolution", "text": "Escape and radiate coevolution\n\nEscape and radiate coevolution is a multistep process that hypothesizes that an organism under constraints from other organisms will develop new defenses, allowing it to \"escape\" and then \"radiate\" into differing species. After a novel defense has been acquired, an organism is able to escape predation and rapidly multiply into new species because of relaxed selective pressure. There are many possible mechanisms available varying between different types of organisms, however they must be novel in order for escape to allow for radiation. This theory applies to predator-prey associations, but is most often applied to plant-herbivore associations.\n\nThis form of coevolution can be complex but is essential to understanding the vast biological diversity among organisms today. Out of the many forms of coevolution, escape and radiate is most likely responsible for providing the most diversity. This is due to the nature of the \"evolutionary arms race\" and the continuous cycle of counter adaptations. It is a relatively new field of study and is rapidly gaining credibility. To date, there has not been a formal study published specifically for escape and radiate coevolution.\n\nThis theory originated in a paper by Ehrlich and Raven, 1964, \"Butterflies and plants: a study in coevolution\". It outlined and laid the foundations of the concept. However, the term \"escape and radiate\" was not coined until Thompson's 1989 \"Concepts of Coevolution\". The theory has not yet been fully analyzed, however, as since its origins it has grown in importance among evolutionary biologists and botanists.\n\nIn order for an organism to \"escape\", and then radiate into varying species it needs a mechanism to escape. These defense mechanisms vary widely and differ for different types of organisms. Plants use chemical defenses in the form of secondary metabolites or allelochemicals. These allelochemicals inhibit the growth, behavior, and health of herbivores, allowing plants to escape. An example of a plant allelochemical are alkaloids that can inhibit protein synthesis in herbivores. Other forms of plant defense include mechanical defenses such as thigmonasty movements which have the plant leaves close in response to tactile stimulation. Indirect mechanisms plant include shedding of plant leaves so less leaves are available which deters herbivores, growth in locations in that are difficult to reach, and even mimicry. For organisms other than plants, examples of defense mechanisms allowing for escape include camouflage, aposematism, heightened senses and physical capabilities, and even defensive behaviors such as feigning death. An example of an organism using one of these defense mechanisms is the granular poison frog which defends itself through aposematism. It is important to understand that in order for escape and radiate coevolution to occur, it is necessary that the developed defense is novel rather than previously established.\n\nInduced defense stemming from adaptive phenotypic plasticity may help a plant defend itself against multiple enemies. Phenotypic plasticity occurs when an organism undergoes an environmental change forcing a change altering its behavior, physiology, etc. These induced defenses allow for an organism to escape.\n\nRadiation is the evolutionary process of diversification of a single species into multiple forms. It includes the physiological and ecological diversity within a rapidly multiplying lineage. There are many types of radiation including adaptive, concordant, and discordant radiation however escape and radiate coevolution does not always follow those specific types.\n\nThis eventually leads to the question, why does escape allow for radiation? Once a novel defense has been acquired, the attacking organism which had evolved adaptations that allowed it to predate is now up against a new defense that it has not yet been evolved to encounter. This gives the defending organism the advantage, and therefore time to rapidly multiply unopposed by the previously attacking organism. This ultimately leads to the physiological and ecological diversity within the rapidly multiplying lineage, hence radiation.\n\nA full study analyzing the effects of escape and radiate coevolution has not yet been completed which hinders knowing how applicable this form of coevolution could be to other areas of study, or global concerns, only hypotheses of its effects can be made. Improved agriculture, conservation, biological diversity, and epidemiology are just some of the areas that could potentially be helped through the study of coevolution and its specific hypotheses such as escape and radiate coevolution.\nA theory as to why we see such vast biological diversity today may be because of escape and radiate coevolution. After the organism escapes, it then radiates into multiple species, and spreads geographically. Evidence of escape and radiate coevolution can be seen through the starburst effect in plant and herbivore clades. When analyzing clades of predator-prey associations, although it varies, the starburst effect is a good indicator that escape and radiate coevolution may be occurring. Eventually this cycle must come to an end because adaptations that entail costs (allocation of resources, vulnerability to other predators) that at some point outweigh their benefits.\nEscape and radiate coevolution may support parallel cladogenesis, wherein plant and herbivore phylogenies might match with ancestral insects exploiting ancestral plants. This is significant because it allows researchers to hypothesize about the relationships between ancestral organisms. Unfortunately, there have not yet been any known examples specifically involving escape and radiate coevolution being used for hypothesizing ancestral relationships.\n\nMany times the organism that has \"escaped\" continuously undergoes selective pressure because the predator it has escaped from evolves to create another adaptation in response, causing the process to continue. These \"offensive\" traits developed by predators range widely. For example, herbivores can develop an adaptation that allows for improved detoxification which allow to overcome plant defenses, thus causing escape and radiate coevolution to continue. Often the term \"evolutionary arms race\" is used to illustrate the idea that continuous evolution is needed to maintain the same relative fitness while the two species are coevolving. This idea also ties in with the Red Queen hypothesis. Counter adaptations among two organisms through escape and radiate coevolution is a major driving force behind diversity.\n\nEscape and radiate coevolution produces much more biological variation than other evolutionary mechanisms. For instance, cospeciation is important for diversity amongst species that share a symbiotic relationship, however this does not create nearly as much diversity in comparison to reciprocal evolutionary change due to natural selection.\nEvidence of rapid diversification following a novel adaptation is shown through the evolution of resin and latex canal tubes in 16 different lineages of plants. Plants with resin or latex canals can easily defend themselves against insect herbivores. When lineages of canal bearing plants are compared to the lineages of canal free plants, it is apparent that canal bearing plants are far more diverse, supporting escape and radiate coevolution.\n\nThe most popular examples of escape and radiate coevolution are of plant-herbivore associations. The most classic example is of butterflies and plants outlined in Ehrlich and Raven's original paper, \"Butterflies and plants: a study in coevolution.\". Erlich and Raven found in 1964 that hostplants for butterflies had a wide range of chemical defenses, allowing them to escape herbivory. Butterflies who developed novel counter detoxification mechanisms against the hostplants chemical defenses were able to utilize the hostplant resources. The process of stepwise adaptation and counteradaptation among the butterflies and hostplants is continuous and creates vast diversity.\n\nTropical trees may also escape and defend themselves. Trees growing in high light were predicted to have few chemical defenses, but rapid synchronous leaf expansion and low leaf nutritional quality during expansion. Species growing in low light have high levels of different chemical defenses, poor nutritional quality and asynchronous leaf expansion. Depending on the level of light the trees were growing in influenced the type of defenses they obtained, either chemical or through leaf expansion. The trees exposed to less light developed various chemicals to defend themselves against herbivores, a defense not utilizing light. This study was significant because it illustrates the separation between defenses and their relationship with an organism escaping and radiating into other species. Development of novel defenses does not necessarily imply that escape is possible for a species of plant if herbivores are adapting at a faster rate.\n\nMilkweed plants contain latex-filled canals which deter insect herbivores. Latex is toxic for small herbivores because it disrupts sodium and potassium levels. This has allowed for milkweeds to \"escape\" and become extremely diverse. There are over 100 different species of milkweeds which shows how diverse the plant is, with escape and radiate coevolution playing a very large role in creating such a high number of species.\n\nKey adaptations are adaptations that allow a group of organisms to diversify. \"Daphnia lumholtzi\" is a water flea that is able to form rigid head spines in response to chemicals released when fish are present. These phenotypically plastic traits serve as an induced defense against these predators. A study showed that \"Daphnia pulicaria\" is competitively superior to \"D. lumholtzi\" in the absence of predators. However, in the presence of fish predation the invasive species formed its defenses and became the dominant water flea in the region. This switch in dominance suggests that the induced defense against fish predation could represent a key adaptation for the invasion success of \"D. lumholtzi\". A defensive trait that qualifies as a key adaptation is most likely an example of escape and radiate coevolution.\n\nThe theory can be applied at the microscopic level such as to bacteria-phage relationships. Bacteria were able to diversify and escape through resistance to phages. The diversity among the hosts and parasites differed among the range of infection and resistance. The implication of this study to humans is its important to understanding the evolution of infectious organisms, and preventing diseases.\n"}
{"id": "4387132", "url": "https://en.wikipedia.org/wiki?curid=4387132", "title": "Gravity of Earth", "text": "Gravity of Earth\n\nThe gravity of Earth, denoted by , is the net acceleration that is imparted to objects due to the combined effect of gravitation (from distribution of mass within Earth) and the centrifugal force (from the Earth's rotation).\n\nIn SI units this acceleration is measured in metres per second squared (in symbols, m/s or m·s) or equivalently in newtons per kilogram (N/kg or N·kg). Near Earth's surface, gravitational acceleration is approximately 9.8 m/s, which means that, ignoring the effects of air resistance, the speed of an object falling freely will increase by about 9.8 metres per second every second. This quantity is sometimes referred to informally as \"little \" (in contrast, the gravitational constant is referred to as \"big \").\n\nThe precise strength of Earth's gravity varies depending on location. The nominal \"average\" value at Earth's surface, known as is, by definition, 9.80665 m/s. This quantity is denoted variously as , (though this sometimes means the normal equatorial value on Earth, 9.78033 m/s), , gee, or simply (which is also used for the variable local value). \n\nThe weight of an object on Earth's surface is the downwards force on that object, given by Newton's second law of motion, or (). Gravitational acceleration contributes to the total gravity acceleration, but other factors, such as the rotation of Earth, also contribute, and, therefore, affect the weight of the object.\nGravity does not normally include the gravitational pull of the Moon and Sun, which are accounted for in terms of tidal effects.\nIt is a vector (physics) quantity, whose direction coincides with a plumb bob.\n\nA non-rotating perfect sphere of uniform mass density, or whose density varies solely with distance from the centre (spherical symmetry), would produce a gravitational field of uniform magnitude at all points on its surface. The Earth is rotating and is also not spherically symmetric; rather, it is slightly flatter at the poles while bulging at the Equator: an oblate spheroid. There are consequently slight deviations in the magnitude of gravity across its surface.\n\nGravity on the Earth's surface varies by around 0.7%, from 9.7639 m/s on the Nevado Huascarán mountain in Peru to 9.8337 m/s at the surface of the Arctic Ocean. In large cities, it ranges from 9.7760 in Kuala Lumpur, Mexico City, and Singapore to 9.825 in Oslo and Helsinki.\n\nIn 1901 the third General Conference on Weights and Measures defined a standard gravitational acceleration for the surface of the Earth: \"g\" = 9.80665 m/s. It was based on measurements done at the Pavillon de Breteuil near Paris in 1888, with a theoretical correction applied in order to convert to a latitude of 45° at sea level. This definition is thus not a value of any particular place or carefully worked out average, but an agreement for a value to use if a better actual local value is not known or not important. It is also used to define the units kilogram force and pound force.\n\nThe surface of the Earth is rotating, so it is not an inertial frame of reference. At latitudes nearer the Equator, the outward centrifugal force produced by Earth's rotation is larger than at polar latitudes. This counteracts the Earth's gravity to a small degree – up to a maximum of 0.3% at the Equator – and reduces the apparent downward acceleration of falling objects.\n\nThe second major reason for the difference in gravity at different latitudes is that the Earth's equatorial bulge (itself also caused by centrifugal force from rotation) causes objects at the Equator to be farther from the planet's centre than objects at the poles. Because the force due to gravitational attraction between two bodies (the Earth and the object being weighed) varies inversely with the square of the distance between them, an object at the Equator experiences a weaker gravitational pull than an object at the poles.\n\nIn combination, the equatorial bulge and the effects of the surface centrifugal force due to rotation mean that sea-level gravity increases from about 9.780 m/s at the Equator to about 9.832 m/s at the poles, so an object will weigh about 0.5% more at the poles than at the Equator.\n\nGravity decreases with altitude as one rises above the Earth's surface because greater altitude means greater distance from the Earth's centre. All other things being equal, an increase in altitude from sea level to causes a weight decrease of about 0.29%. (An additional factor affecting apparent weight is the decrease in air density at altitude, which lessens an object's buoyancy. This would increase a person's apparent weight at an altitude of 9,000 metres by about 0.08%)\n\nIt is a common misconception that astronauts in orbit are weightless because they have flown high enough to escape the Earth's gravity. In fact, at an altitude of , equivalent to a typical orbit of the ISS, gravity is still nearly 90% as strong as at the Earth's surface. Weightlessness actually occurs because orbiting objects are in free-fall.\n\nThe effect of ground elevation depends on the density of the ground (see Slab correction section). A person flying at 30 000 ft above sea level over mountains will feel more gravity than someone at the same elevation but over the sea. However, a person standing on the earth's surface feels less gravity when the elevation is higher.\n\nThe following formula approximates the Earth's gravity variation with altitude:\n\nWhere\n\nThe formula treats the Earth as a perfect sphere with a radially symmetric distribution of mass; a more accurate mathematical treatment is discussed below.\n\nAn approximate value for gravity at a distance from the center of the Earth can be obtained by assuming that the Earth's density is spherically symmetric. The gravity depends only on the mass inside the sphere of radius . All the contributions from outside cancel out as a consequence of the inverse-square law of gravitation. Another consequence is that the gravity is the same as if all the mass were concentrated at the center. Thus, the gravitational acceleration at this radius is\nwhere is the gravitational constant and is the total mass enclosed within radius . If the Earth had a constant density , the mass would be and the dependence of gravity on depth would be\nIf the density decreased linearly with increasing radius from a density at the center to at the surface, then , and the dependence would be\n\nThe actual depth dependence of density and gravity, inferred from seismic travel times (see Adams–Williamson equation), is shown in the graphs below.\n\nLocal differences in topography (such as the presence of mountains), geology (such as the density of rocks in the vicinity), and deeper tectonic structure cause local and regional differences in the Earth's gravitational field, known as gravitational anomalies. Some of these anomalies can be very extensive, resulting in bulges in sea level, and throwing pendulum clocks out of synchronisation.\n\nThe study of these anomalies forms the basis of gravitational geophysics. The fluctuations are measured with highly sensitive gravimeters, the effect of topography and other known factors is subtracted, and from the resulting data conclusions are drawn. Such techniques are now used by prospectors to find oil and mineral deposits. Denser rocks (often containing mineral ores) cause higher than normal local gravitational fields on the Earth's surface. Less dense sedimentary rocks cause the opposite.\n\nIn air, objects experience a supporting buoyancy force which reduces the apparent strength of gravity (as measured by an object's weight). The magnitude of the effect depends on air density (and hence air pressure); see Apparent weight for details.\n\nThe gravitational effects of the Moon and the Sun (also the cause of the tides) have a very small effect on the apparent strength of Earth's gravity, depending on their relative positions; typical variations are 2 µm/s (0.2 mGal) over the course of a day.\n\nGravity acceleration is a vector quantity. In a spherically symmetric Earth, gravity would point directly towards the sphere's centre. As the Earth is slightly flatter, there are consequently slight deviations in the direction of gravity.\n\nTools exist for calculating the strength of gravity at various cities around the world. The effect of latitude can be clearly seen with gravity in high-latitude cities: Anchorage (9.826 m/s), Helsinki (9.825 m/s), being about 0.5% greater than that in cities near the equator: Kuala Lumpur (9.776 m/s), Manila (9.780 m/s). The effect of altitude can be seen in Mexico City (9.776 m/s; altitude ), and by comparing Denver (9.798 m/s; ) with Washington, D.C. (9.801 m/s; ), both of which are near 39° N. Measured values can be obtained from Physical and Mathematical Tables by T.M. Yarwood and F. Castle, Macmillan, revised edition 1970.\n\nIf the terrain is at sea level, we can estimate formula_5, the acceleration at latitude formula_6:\n\nThis is the International Gravity Formula 1967, the 1967 Geodetic Reference System Formula, Helmert's equation or Clairaut's formula.\n\nAn alternative formula for \"g\" as a function of latitude is the WGS (World Geodetic System) 84 Ellipsoidal Gravity Formula:\nwhere,\n\n\nthen, where formula_13,\n\nThe difference between the WGS-84 formula and Helmert's equation is less than 0.68 μm·s.\n\nThe first correction to be applied to the model is the free air correction (FAC) that accounts for heights above sea level. Near the surface of the Earth (sea level), gravity decreases with height such that linear extrapolation would give zero gravity at a height of one half of the earth's radius - (9.8 m·s per 3,200 km.)\n\nUsing the mass and radius of the Earth:\n\nThe FAC correction factor (Δ\"g\") can be derived from the definition of the acceleration due to gravity in terms of G, the Gravitational Constant (see Estimating \"g\" from the law of universal gravitation, below):\n\nwhere:\n\nAt a height \"h\" above the nominal surface of the earth \"g\" is given by:\n\nSo the FAC for a height \"h\" above the nominal earth radius can be expressed:\n\nThis expression can be readily used for programming or inclusion in a spreadsheet. Collecting terms, simplifying and neglecting small terms (\"h\"«\"r\"), however yields the good approximation:\n\nUsing the numerical values above and for a height \"h\" in metres:\n\nGrouping the latitude and FAC altitude factors the expression most commonly found in the literature is:\nwhere formula_24 = acceleration in m·s at latitude formula_25 and altitude \"h\" in metres.\n\nFor flat terrain above sea level a second term is added for the gravity due to the extra mass; for this purpose the extra mass can be approximated by an infinite horizontal slab, and we get 2π\"G\" times the mass per unit area, i.e. 4.2 m·s·kg (0.042 μGal·kg·m) (the Bouguer correction). For a mean rock density of 2.67 g·cm this gives 1.1 s (0.11 mGal·m). Combined with the free-air correction this means a reduction of gravity at the surface of ca. 2 µm·s (0.20 mGal) for every metre of elevation of the terrain. (The two effects would cancel at a surface rock density of 4/3 times the average density of the whole earth. The density of the whole earth is 5.515 g·cm, so standing on a slab of something like iron whose density is over 7.35 g·cm would increase one's weight.)\n\nFor the gravity below the surface we have to apply the free-air correction as well as a double Bouguer correction. With the infinite slab model this is because moving the point of observation below the slab changes the gravity due to it to its opposite. Alternatively, we can consider a spherically symmetrical Earth and subtract from the mass of the Earth that of the shell outside the point of observation, because that does not cause gravity inside. This gives the same result.\n\nFrom the law of universal gravitation, the force on a body acted upon by Earth's gravity is given by\n\nwhere \"r\" is the distance between the centre of the Earth and the body (see below), and here we take \"m\" to be the mass of the Earth and \"m\" to be the mass of the body.\n\nAdditionally, Newton's second law, \"F\" = \"ma\", where \"m\" is mass and \"a\" is acceleration, here tells us that\n\nComparing the two formulas it is seen that:\n\nSo, to find the acceleration due to gravity at sea level, substitute the values of the gravitational constant, \"G\", the Earth's mass (in kilograms), \"m\", and the Earth's radius (in metres), \"r\", to obtain the value of \"g\":\n\nNote that this formula only works because of the mathematical fact that the gravity of a uniform spherical body, as measured on or above its surface, is the same as if all its mass were concentrated at a point at its centre. This is what allows us to use the Earth's radius for \"r\".\n\nThe value obtained agrees approximately with the measured value of \"g\". The difference may be attributed to several factors, mentioned above under \"Variations\":\nThere are significant uncertainties in the values of \"r\" and \"m\" as used in this calculation, and the value of \"G\" is also rather difficult to measure precisely.\n\nIf \"G\", \"g\" and \"r\" are known then a reverse calculation will give an estimate of the mass of the Earth. This method was used by Henry Cavendish.\n\n\n"}
{"id": "13844012", "url": "https://en.wikipedia.org/wiki?curid=13844012", "title": "Greenhouse and icehouse Earth", "text": "Greenhouse and icehouse Earth\n\nThroughout the Phanerozoic history of the Earth, the planet's climate has been fluctuating between two dominant climate states: the greenhouse Earth and the icehouse Earth. \nThese two climate states last for millions of years and should not be confused with glacial and interglacial periods, which occur only during an icehouse period and tend to last less than 1 million years. There are five known great glaciations in Earth's climate history; the main factors involved in changes of the paleoclimate are believed to be the concentration of atmospheric carbon dioxide, changes in the Earth's orbit, and oceanic and orogenic changes due to tectonic plate dynamics. Greenhouse and icehouse periods have profoundly shaped the evolution of life on Earth.\n\nA \"greenhouse Earth\" or \"hothouse Earth\" is a period in which there are no continental glaciers whatsoever on the planet, the levels of carbon dioxide and other greenhouse gases (such as water vapor and methane) are high, and sea surface temperatures (SSTs) range from 28 °C (82.4 °F) in the tropics to 0 °C (32 °F) in the polar regions. \n\nThis state should not be confused with a hypothetical \"hothouse earth\", which is an irreversible tipping point corresponding to the ongoing runaway greenhouse effect on Venus. The IPCC states that \"a 'runaway greenhouse effect'—analogous to [that of] Venus—appears to have virtually no chance of being induced by anthropogenic activities.\"\n\nThere are several theories as to how a greenhouse Earth can come about. The geological record shows CO and other greenhouse gases are abundant during this time. Tectonic movements were extremely active during the more well-known greenhouse ages (such as 368 million years ago in the Paleozoic Era). Because of continental rifting (continental plates moving away from each other) volcanic activity becomes more prominent, producing more CO and heating up the Earth's atmosphere. Earth is more commonly placed in a greenhouse state throughout the epochs, and the Earth has been in this state for approximately 80% of the past 500 million years, which makes understanding the direct causes somewhat difficult.\n\nAn \"icehouse Earth\" is the earth as it experiences an ice age. Unlike a greenhouse Earth, an icehouse Earth has ice sheets present, and these sheets wax and wane throughout times known as glacial periods and interglacial periods. During an icehouse Earth, greenhouse gases tend to be less abundant, and temperatures tend to be cooler globally. The Earth is currently in an icehouse stage, as ice sheets are present on both poles and glacial periods have occurred at regular intervals over the past million years.\n\nThe causes of an icehouse state are much debated, because not much is really known about the transition periods between greenhouse to icehouse climates and what could make the climate so different. One important aspect is clearly the decline of CO in the atmosphere, possibly due to low volcanic activity.\n\nOther important issues are the movement of the tectonic plates and the opening and closing of oceanic gateways. These seem to play a crucial part in icehouse Earths because they can bring forth cool waters from very deep water circulations that could assist in creating ice sheets or thermal isolation of areas. Examples of this occurring are the opening of the Tasmanian gateway 36.5 million years ago that separated Australia and Antarctica and which is believed to have set off the Cenozoic icehouse, and the creation of the Drake Passage 32.8 million years ago by the separation of South America and Antarctica, though it was believed by other scientists that this did not come into effect until around 23 million years ago. The closing of the Isthmus of Panama and the Indonesian seaway approximately 3 or 4 million years ago may have been a major cause for our current icehouse state. For the icehouse climate, tectonic activity also creates mountains, which are produced by one continental plate colliding with another one and continuing forward. The revealed fresh soils act as scrubbers of carbon dioxide, which can significantly affect the amount of this greenhouse gas in the atmosphere. An example of this is the collision between the Indian subcontinent and the Asian continent, which created the Himalayan Mountains about 50 million years ago.\n\nWithin icehouse states, there are \"glacial\" and \"interglacial\" periods that cause ice sheets to build up or retreat. The causes for these glacial and interglacial periods are mainly variations in the movement of the earth around the Sun. The astronomical components, discovered by the Serbian geophysicist Milutin Milanković and now known as Milankovitch cycles, include the axial tilt of the Earth, the orbital eccentricity (or shape of the orbit) and the precession (or wobble) of the Earth's spin. The tilt of the axis tends to fluctuate between 21.5° to 24.5° and back every 41,000 years on the vertical axis. This change actually affects the seasonality upon the earth, since more or less solar radiation hits certain areas of the planet more often on a higher tilt, while less of a tilt would create a more even set of seasons worldwide. These changes can be seen in ice cores, which also contains information that shows that during glacial times (at the maximum extension of the ice sheets), the atmosphere had lower levels of carbon dioxide. This may be caused by the increase or redistribution of the acid/base balance with bicarbonate and carbonate ions that deals with alkalinity. During an Icehouse, only 20% of the time is spent in interglacial, or warmer times.\n\nA \"snowball earth\" is the complete opposite of greenhouse Earth, in which the earth's surface is completely frozen over; however, a snowball earth technically does not have continental ice sheets like during the icehouse state. \"The Great Infra-Cambrian Ice Age\" has been claimed to be the host of such a world, and in 1964, the scientist W. Brian Harland brought forth his discovery of indications of glaciers in low latitudes (Harland and Rudwick). This became a problem for Harland because of the thought of the \"Runaway Snowball Paradox\" (a kind of Snowball effect) that, once the earth enters the route of becoming a snowball earth, it would never be able to leave that state. However, in 1992 brought up a solution to the paradox. It is believed that since the continents at this time were huddled at the low and mid-latitudes that there was a great cooling event by planetary albedo, or reflection of the earth’s surface. Kirschvink explained that the way to get out of the snowball could be connected to carbon dioxide, since volcanic activity would not halt, and that the buildup and lack of \"scrubbing\" of this carbon dioxide in the atmosphere, that the earth would return to a greenhouse state. Some scientists believe that the end of the snowball Earth caused an event known as the Cambrian Explosion, which produced the beginnings of multi-cellular life. However some biologists claim that a complete snowball Earth could not have happened since photosynthetic life would not have survived underneath many meters of ice without sunlight. However, it has been observed that, even under meters of thick ice around Antarctica, sunlight shows through. Most scientists today believe that a \"hard\" Snowball Earth, one completely covered by ice, is probably impossible. However, a \"slushball earth\", with points of openings near the equator, is possible.\n\nRecent studies may have again complicated the idea of a snowball earth. In October 2011, a team of French researchers announced that the carbon dioxide during the last speculated \"snowball earth\" may have been lower than originally stated, which provides a challenge in finding out how Earth was able to get out of its state and if it were a snowball or slushball.\n\nThe Eocene, which occurred between 53 and 49 million years ago, was the Earth's warmest temperature period for 100 million years. However, this \"super-greenhouse\" eventually became an icehouse by the late Eocene. It was believed that the decline of CO caused this change, though there are possible positive feedbacks, or added influence that contributes to the cooling.\n\nThe best record we have for a transition from an icehouse to a greenhouse period where plant life exists is during the Permian epoch that occurred around 300 million years ago. In 40 million years a major transition took place, causing the Earth to change from a moist, icy planet where rainforests covered the tropics, into a hot, dry, and windy location where little could survive. Professor Isabel P. Montañez of University of California, Davis, who has researched this time period, found the climate to be \"highly unstable\" and \"marked by dips and rises in carbon dioxide\".\n\nThe Eocene-Oligocene transition, the latest transition occurring approximately 34 million years ago, resulted in rapid global temperature decrease, the glaciation of Antarctica and a series of biotic extinction events. The most dramatic species turnover event associated with this time period is the Grande Coupure, a period which saw the replacement of European tree-dwelling and leaf-eating mammal species by migratory species from Asia.\n\nThe science of paleoclimatology attempts to understand the history of greenhouse and icehouse conditions over geological time. Through the study of ice cores, dendrochronology, ocean and lake sediments (varve), palynology (fossilized pollen) and isotope analysis (such as Radiometric dating and stable isotope analysis), scientists can create models of past climate. One study has shown that atmospheric carbon dioxide levels during the Permian age rocked back and forth between 250 parts per million (which is close to present-day levels) up to 2,000 parts per million. Studies on lake sediments suggest that the \"Hothouse\" or \"super-Greenhouse\" Eocene was in a \"permanent El Nino state\" after the 10 °C warming of the deep ocean and high latitude surface temperatures shut down the Pacific Ocean's El Nino-Southern Oscillation. A theory was suggested for the Paleocene–Eocene Thermal Maximum on the sudden decrease of carbon isotopic composition of global inorganic carbon pool by 2.5 parts per million. A hypothesis noted for this negative drop of isotopes could be the increase of methane hydrates, the trigger for which remains a mystery. This increase of methane in the atmosphere, which happens to be a potent, but short-lived greenhouse gas, increased the global temperatures by 6 °C with the assistance of the less potent carbon dioxide.\n\n\nCurrently, the Earth is in an icehouse climate state. About 34 million years ago, ice sheets began to form in Antarctica; the ice sheets in the Arctic did not start forming until 2 million years ago. Some processes that may have led to our current icehouse may be connected to the development of the Himalayan Mountains and the opening of the Drake Passage between South America and Antarctica. Scientists have been attempting to compare the past transitions between icehouse and greenhouse, and vice versa to understand where our planet is now heading.\n\nWithout the human influence on the greenhouse gas concentration, the Earth would be heading toward a glacial period. Predicted changes in orbital forcing suggest that in absence of human-made global warming the next glacial period would begin at least 50,000 years from now (see Milankovitch cycles).\n\nBut due to the ongoing anthropogenic greenhouse gas emissions, the Earth is instead heading toward a greenhouse Earth period. Permanent ice is actually a rare phenomenon in the history of the Earth, occurring only in coincidence with the icehouse effect, which has affected about 20% of Earth's history.\n\n"}
{"id": "32236479", "url": "https://en.wikipedia.org/wiki?curid=32236479", "title": "Hanle effect", "text": "Hanle effect\n\nThe Hanle effect is a reduction in the polarization of light when the atoms emitting the light are subject to a magnetic field in a particular direction, and when they have themselves been excited by polarized light.\n\nIt is named after Wilhelm Hanle, who first described it in \"Zeitschrift für Physik\" in 1924. Attempts to understand the phenomenon were important in the subsequent development of quantum physics.\n\n"}
{"id": "7149688", "url": "https://en.wikipedia.org/wiki?curid=7149688", "title": "Hard inheritance", "text": "Hard inheritance\n\nHard inheritance was a model of heredity that explicitly excludes any acquired characteristics, such as of Lamarckism. It is the exact opposite of soft inheritance, coined by Ernst Mayr to contrast ideas about inheritance.\n\nHard inheritance states that characteristics of an organism's offspring (passed on through DNA) will not be affected by the actions that the parental organism performs during its lifetime. For example: a medieval blacksmith who uses only his right arm to forge steel will not sire a son with a stronger right arm than left because the blacksmith's actions do not alter his genetic code. Inheritance due to usage and non-usage is excluded. Inheritance works as described in the modern synthesis of evolutionary biology.\n\nThe existence of inherited epigenetic variants has led to renewed interest in soft inheritance.\n"}
{"id": "43421", "url": "https://en.wikipedia.org/wiki?curid=43421", "title": "Henry David Thoreau", "text": "Henry David Thoreau\n\nHenry David Thoreau (see name pronunciation; July 12, 1817 – May 6, 1862) was an American essayist, poet, philosopher, abolitionist, naturalist, tax resister, development critic, surveyor, and historian. A leading transcendentalist, Thoreau is best known for his book \"Walden\", a reflection upon simple living in natural surroundings, and his essay \"Civil Disobedience\" (originally published as \"Resistance to Civil Government\"), an argument for disobedience to an unjust state.\n\nThoreau's books, articles, essays, journals, and poetry amount to more than 20 volumes. Among his lasting contributions are his writings on natural history and philosophy, in which he anticipated the methods and findings of ecology and environmental history, two sources of modern-day environmentalism. His literary style interweaves close observation of nature, personal experience, pointed rhetoric, symbolic meanings, and historical lore, while displaying a poetic sensibility, philosophical austerity, and Yankee attention to practical detail. He was also deeply interested in the idea of survival in the face of hostile elements, historical change, and natural decay; at the same time he advocated abandoning waste and illusion in order to discover life's true essential needs.\n\nHe was a lifelong abolitionist, delivering lectures that attacked the Fugitive Slave Law while praising the writings of Wendell Phillips and defending the abolitionist John Brown. Thoreau's philosophy of civil disobedience later influenced the political thoughts and actions of such notable figures as Leo Tolstoy, Mahatma Gandhi, and Martin Luther King Jr.\n\nThoreau is sometimes referred to as an anarchist. Though \"Civil Disobedience\" seems to call for improving rather than abolishing government—\"I ask for, not at once no government, but \"at once\" a better government\"—the direction of this improvement contrarily points toward anarchism: \"'That government is best which governs not at all;' and when men are prepared for it, that will be the kind of government which they will have.\"\n\nAmos Bronson Alcott and Thoreau's aunt each wrote that \"Thoreau\" is pronounced like the word \"thorough\" ( —in General American, but more precisely —in 19th-century New England). Edward Waldo Emerson wrote that the name should be pronounced \"Thó-row\", with the \"h\" sounded and stress on the first syllable. Among modern-day American speakers, it is perhaps more commonly pronounced —with stress on the second syllable.\n\nThoreau had a distinctive appearance, with a nose that he called his \"most prominent feature\". Of his appearance and disposition, Ellery Channing wrote:\n\nHis face, once seen, could not be forgotten. The features were quite marked: the nose aquiline or very Roman, like one of the portraits of Caesar (more like a beak, as was said); large overhanging brows above the deepest set blue eyes that could be seen, in certain lights, and in others gray,—eyes expressive of all shades of feeling, but never weak or near-sighted; the forehead not unusually broad or high, full of concentrated energy and purpose; the mouth with prominent lips, pursed up with meaning and thought when silent, and giving out when open with the most varied and unusual instructive sayings.\n\nHenry David Thoreau was born David Henry Thoreau in Concord, Massachusetts, into the \"modest New England family\" of John Thoreau, a pencil maker, and Cynthia Dunbar. His paternal grandfather had been born on the UK crown dependency island of Jersey. His maternal grandfather, Asa Dunbar, led Harvard's 1766 student \"Butter Rebellion\", the first recorded student protest in the American colonies. David Henry was named after his recently deceased paternal uncle, David Thoreau. He began to call himself Henry David after he finished college; he never petitioned to make a legal name change. He had two older siblings, Helen and John Jr., and a younger sister, Sophia. Thoreau's birthplace still exists on Virginia Road in Concord. The house has been restored by the Thoreau Farm Trust, a nonprofit organization, and is now open to the public.\n\nHe studied at Harvard College between 1833 and 1837. He lived in Hollis Hall and took courses in rhetoric, classics, philosophy, mathematics, and science. He was a member of the Institute of 1770 (now the Hasty Pudding Club). According to legend, Thoreau refused to pay the five-dollar fee (approximately ) for a Harvard diploma. In fact, the master's degree he declined to purchase had no academic merit: Harvard College offered it to graduates \"who proved their physical worth by being alive three years after graduating, and their saving, earning, or inheriting quality or condition by having Five Dollars to give the college.\" He commented, \"Let every sheep keep its own skin\", a reference to the tradition of using sheepskin vellum for diplomas.\n\nThe traditional professions open to college graduates—law, the church, business, medicine—did not interest Thoreau, so in 1835 he took a leave of absence from Harvard, during which he taught school in Canton, Massachusetts. After he graduated in 1837, he joined the faculty of the Concord public school, but he resigned after a few weeks rather than administer corporal punishment. He and his brother John then opened the Concord Academy, a grammar school in Concord, in 1838. They introduced several progressive concepts, including nature walks and visits to local shops and businesses. The school closed when John became fatally ill from tetanus in 1842 after cutting himself while shaving. He died in Henry's arms.\n\nUpon graduation Thoreau returned home to Concord, where he met Ralph Waldo Emerson through a mutual friend. Emerson, who was 14 years his senior, took a paternal and at times patron-like interest in Thoreau, advising the young man and introducing him to a circle of local writers and thinkers, including Ellery Channing, Margaret Fuller, Bronson Alcott, and Nathaniel Hawthorne and his son Julian Hawthorne, who was a boy at the time.\n\nEmerson urged Thoreau to contribute essays and poems to a quarterly periodical, \"The Dial\", and lobbied the editor, Margaret Fuller, to publish those writings. Thoreau's first essay published in \"The Dial\" was \"Aulus Persius Flaccus,\" an essay on the Roman playwright, in July 1840. It consisted of revised passages from his journal, which he had begun keeping at Emerson's suggestion. The first journal entry, on October 22, 1837, reads, \"'What are you doing now?' he asked. 'Do you keep a journal?' So I make my first entry to-day.\"\n\nThoreau was a philosopher of nature and its relation to the human condition. In his early years he followed Transcendentalism, a loose and eclectic idealist philosophy advocated by Emerson, Fuller, and Alcott. They held that an ideal spiritual state transcends, or goes beyond, the physical and empirical, and that one achieves that insight via personal intuition rather than religious doctrine. In their view, Nature is the outward sign of inward spirit, expressing the \"radical correspondence of visible things and human thoughts\", as Emerson wrote in \"Nature\" (1836).\n\nOn April 18, 1841, Thoreau moved into the Emerson house. There, from 1841 to 1844, he served as the children's tutor; he was also an editorial assistant, repairman and gardener. For a few months in 1843, he moved to the home of William Emerson on Staten Island, and tutored the family's sons while seeking contacts among literary men and journalists in the city who might help publish his writings, including his future literary representative Horace Greeley.\n\nThoreau returned to Concord and worked in his family's pencil factory, which he would continue to do alongside his writing and other work for most of his adult life. He rediscovered the process of making good pencils with inferior graphite by using clay as the binder. This invention allowed profitable use of a graphite source found in New Hampshire that had been purchased in 1821 by Thoreau's brother-in-law, Charles Dunbar. The process of mixing graphite and clay, known as the Conté process, had been first patented by Nicolas-Jacques Conté in 1795. The company's other source of graphite had been Tantiusques, a mine operated by Native Americans in Sturbridge, Massachusetts. Later, Thoreau converted the pencil factory to produce plumbago, a name for graphite at the time, which was used in the electrotyping process.\n\nOnce back in Concord, Thoreau went through a restless period. In April 1844 he and his friend Edward Hoar accidentally set a fire that consumed of Walden Woods.\n\nThoreau felt a need to concentrate and work more on his writing. In March 1845, Ellery Channing told Thoreau, \"Go out upon that, build yourself a hut, & there begin the grand process of devouring yourself alive. I see no other alternative, no other hope for you.\" Two months later, Thoreau embarked on a two-year experiment in simple living on July 4, 1845, when he moved to a small house he had built on land owned by Emerson in a second-growth forest around the shores of Walden Pond. The house was in \"a pretty pasture and woodlot\" of that Emerson had bought, from his family home.\nOn July 24 or July 25, 1846, Thoreau ran into the local tax collector, Sam Staples, who asked him to pay six years of delinquent poll taxes. Thoreau refused because of his opposition to the Mexican–American War and slavery, and he spent a night in jail because of this refusal. The next day Thoreau was freed when someone, likely to have been his aunt, paid the tax, against his wishes. The experience had a strong impact on Thoreau. In January and February 1848, he delivered lectures on \"The Rights and Duties of the Individual in relation to Government\", explaining his tax resistance at the Concord Lyceum. Bronson Alcott attended the lecture, writing in his journal on January 26:\n\nThoreau revised the lecture into an essay titled \"Resistance to Civil Government\" (also known as \"Civil Disobedience\"). It was published by Elizabeth Peabody in the \"Aesthetic Papers\" in May 1849. Thoreau had taken up a version of Percy Shelley's principle in the political poem \"The Mask of Anarchy\" (1819), which begins with the powerful images of the unjust forms of authority of his time and then imagines the stirrings of a radically new form of social action.\n\nAt Walden Pond, Thoreau completed a first draft of \"A Week on the Concord and Merrimack Rivers\", an elegy to his brother John, describing their trip to the White Mountains in 1839. Thoreau did not find a publisher for the book and instead printed 1,000 copies at his own expense; fewer than 300 were sold. He self-published the book on the advice of Emerson, using Emerson's publisher, Munroe, who did little to publicize the book.\n\nIn August 1846, Thoreau briefly left Walden to make a trip to Mount Katahdin in Maine, a journey later recorded in \"Ktaadn\", the first part of \"The Maine Woods\".\n\nThoreau left Walden Pond on September 6, 1847. At Emerson's request, he immediately moved back to the Emerson house to help Emerson's wife, Lidian, manage the household while her husband was on an extended trip to Europe. Over several years, as he worked to pay off his debts, he continuously revised the manuscript of what he eventually published as \"Walden, or Life in the Woods\" in 1854, recounting the two years, two months, and two days he had spent at Walden Pond. The book compresses that time into a single calendar year, using the passage of the four seasons to symbolize human development. Part memoir and part spiritual quest, \"Walden\" at first won few admirers, but later critics have regarded it as a classic American work that explores natural simplicity, harmony, and beauty as models for just social and cultural conditions.\n\nThe American poet Robert Frost wrote of Thoreau, \"In one book ... he surpasses everything we have had in America.\"\n\nThe American author John Updike said of the book, \"A century and a half after its publication, Walden has become such a totem of the back-to-nature, preservationist, anti-business, civil-disobedience mindset, and Thoreau so vivid a protester, so perfect a crank and hermit saint, that the book risks being as revered and unread as the Bible.\"\n\nThoreau moved out of Emerson's house in July 1848 and stayed at a house on nearby Belknap Street. In 1850, he and his family moved into a house at 255 Main Street, where he lived until his death.\n\nIn the summer of 1850, Thoreau and Channing journeyed from Boston to Montreal and Quebec City. These would be Thoreau's only travels outside the United States. It is as a result of this trip that he developed lectures that eventually became \"A Yankee in Canada\". He jested that all he got from this adventure \"was a cold.\" In fact, this proved an opportunity to contrast American civic spirit and democratic values with a colony apparently ruled by illegitimate religious and military power. Whereas his own country had had its revolution, in Canada history had failed to turn.\n\nIn 1851, Thoreau became increasingly fascinated with natural history and narratives of travel and expedition. He read avidly on botany and often wrote observations on this topic into his journal. He admired William Bartram and Charles Darwin's \"Voyage of the Beagle\". He kept detailed observations on Concord's nature lore, recording everything from how the fruit ripened over time to the fluctuating depths of Walden Pond and the days certain birds migrated. The point of this task was to \"anticipate\" the seasons of nature, in his word.\n\nHe became a land surveyor and continued to write increasingly detailed observations on the natural history of the town, covering an area of , in his journal, a two-million-word document he kept for 24 years. He also kept a series of notebooks, and these observations became the source of his late writings on natural history, such as \"Autumnal Tints\", \"The Succession of Trees\", and \"Wild Apples\", an essay lamenting the destruction of indigenous wild apple species.\n\nUntil the 1970s, literary critics dismissed Thoreau's late pursuits as amateur science and philosophy. With the rise of environmental history and ecocriticism as academic disciplines, several new readings of Thoreau began to emerge, showing him to have been both a philosopher and an analyst of ecological patterns in fields and woodlots. For instance, his late essay \"The Succession of Forest Trees\" shows that he used experimentation and analysis to explain how forests regenerate after fire or human destruction, through the dispersal of seeds by winds or animals.\nHe traveled to Canada East once, Cape Cod four times, and Maine three times; these landscapes inspired his \"excursion\" books, \"A Yankee in Canada\", \"Cape Cod\", and \"The Maine Woods\", in which travel itineraries frame his thoughts about geography, history and philosophy. Other travels took him southwest to Philadelphia and New York City in 1854 and west across the Great Lakes region in 1861, when he visited Niagara Falls, Detroit, Chicago, Milwaukee, St. Paul and Mackinac Island. He was provincial in his own travels, but he read widely about travel in other lands. He devoured all the first-hand travel accounts available in his day, at a time when the last unmapped regions of the earth were being explored. He read Magellan and James Cook; the arctic explorers John Franklin, Alexander Mackenzie and William Parry; David Livingstone and Richard Francis Burton on Africa; Lewis and Clark; and hundreds of lesser-known works by explorers and literate travelers. Astonishing amounts of reading fed his endless curiosity about the peoples, cultures, religions and natural history of the world and left its traces as commentaries in his voluminous journals. He processed everything he read, in the local laboratory of his Concord experience. Among his famous aphorisms is his advice to \"live at home like a traveler.\"\n\nAfter John Brown's raid on Harpers Ferry, many prominent voices in the abolitionist movement distanced themselves from Brown or damned him with faint praise. Thoreau was disgusted by this, and he composed a key speech, \"A Plea for Captain John Brown\", which was uncompromising in its defense of Brown and his actions. Thoreau's speech proved persuasive: the abolitionist movement began to accept Brown as a martyr, and by the time of the American Civil War entire armies of the North were literally singing Brown's praises. As a biographer of Brown put it, \"If, as Alfred Kazin suggests, without John Brown there would have been no Civil War, we would add that without the Concord Transcendentalists, John Brown would have had little cultural impact.\"\nThoreau contracted tuberculosis in 1835 and suffered from it sporadically afterwards. In 1860, following a late-night excursion to count the rings of tree stumps during a rainstorm, he became ill with bronchitis. His health declined, with brief periods of remission, and he eventually became bedridden. Recognizing the terminal nature of his disease, Thoreau spent his last years revising and editing his unpublished works, particularly \"The Maine Woods\" and \"Excursions\", and petitioning publishers to print revised editions of \"A Week\" and \"Walden\". He wrote letters and journal entries until he became too weak to continue. His friends were alarmed at his diminished appearance and were fascinated by his tranquil acceptance of death. When his aunt Louisa asked him in his last weeks if he had made his peace with God, Thoreau responded, \"I did not know we had ever quarreled.\"\nAware he was dying, Thoreau's last words were \"Now comes good sailing\", followed by two lone words, \"moose\" and \"Indian\". He died on May 6, 1862, at age 44. Amos Bronson Alcott planned the service and read selections from Thoreau's works, and Channing presented a hymn. Emerson wrote the eulogy spoken at the funeral. Thoreau was buried in the Dunbar family plot; his remains and those of members of his immediate family were eventually moved to Sleepy Hollow Cemetery () in Concord, Massachusetts.\n\nThoreau's friend William Ellery Channing published his first biography, \"Thoreau the Poet-Naturalist\", in 1873. Channing and another friend, Harrison Blake, edited some poems, essays, and journal entries for posthumous publication in the 1890s. Thoreau's journals, which he often mined for his published works but which remained largely unpublished at his death, were first published in 1906 and helped to build his modern reputation. A new, expanded edition of the journals is under way, published by Princeton University Press. Today, Thoreau is regarded as one of the foremost American writers, both for the modern clarity of his prose style and the prescience of his views on nature and politics. His memory is honored by the international Thoreau Society and his legacy honored by the Thoreau Institute at Walden Woods, established in 1998 in Lincoln, Massachusetts.\n\nThoreau was an early advocate of recreational hiking and canoeing, of conserving natural resources on private land, and of preserving wilderness as public land. He was himself a highly skilled canoeist; Nathaniel Hawthorne, after a ride with him, noted that \"Mr. Thoreau managed the boat so perfectly, either with two paddles or with one, that it seemed instinct with his own will, and to require no physical effort to guide it.\" \n\nHe was not a strict vegetarian, though he said he preferred that diet and advocated it as a means of self-improvement. He wrote in \"Walden\", \"The practical objection to animal food in my case was its uncleanness; and besides, when I had caught and cleaned and cooked and eaten my fish, they seemed not to have fed me essentially. It was insignificant and unnecessary, and cost more than it came to. A little bread or a few potatoes would have done as well, with less trouble and filth.\"\nThoreau neither rejected civilization nor fully embraced wilderness. Instead he sought a middle ground, the pastoral realm that integrates nature and culture. His philosophy required that he be a didactic arbitrator between the wilderness he based so much on and the spreading mass of humanity in North America. He decried the latter endlessly but felt that a teacher needs to be close to those who needed to hear what he wanted to tell them. The wildness he enjoyed was the nearby swamp or forest, and he preferred \"partially cultivated country.\" His idea of being \"far in the recesses of the wilderness\" of Maine was to \"travel the logger's path and the Indian trail\", but he also hiked on pristine land. In the essay \"Henry David Thoreau, Philosopher\" Roderick Nash wrote, \"Thoreau left Concord in 1846 for the first of three trips to northern Maine. His expectations were high because he hoped to find genuine, primeval America. But contact with real wilderness in Maine affected him far differently than had the idea of wilderness in Concord. Instead of coming out of the woods with a deepened appreciation of the wilds, Thoreau felt a greater respect for civilization and realized the necessity of balance.\"\nOf alcohol, Thoreau wrote, \"I would fain keep sober always. ... I believe that water is the only drink for a wise man; wine is not so noble a liquor. ... Of all ebriosity, who does not prefer to be intoxicated by the air he breathes?\"\n\nThoreau never married and was childless. He strove to portray himself as an ascetic puritan. However, his sexuality has long been the subject of speculation, including by his contemporaries. Critics have called him heterosexual, homosexual, or asexual. There is no evidence to suggest he had physical relations with anyone, man or woman. Some scholars have suggested that homoerotic sentiments run through his writings and concluded that he was homosexual. The elegy \"Sympathy\" was inspired by the eleven-year-old Edmund Sewell, with whom he hiked for five days in 1839. One scholar has suggested that he wrote the poem to Edmund because he could not bring himself to write it to Edmund's sister, and another that Thoreau's \"emotional experiences with women are memorialized under a camouflage of masculine pronouns\", but other scholars dismiss this. It has been argued that the long paean in \"Walden\" to the French-Canadian woodchopper Alek Therien, which includes allusions to Achilles and Patroclus, is an expression of conflicted desire. In some of Thoreau's writing there is the sense of a secret self. In 1840 he writes in his journal: \"My friend is the apology for my life. In him are the spaces which my orbit traverses\". Thoreau was strongly influenced by the moral reformers of his time, and this may have instilled anxiety and guilt over sexual desire.\n\nThoreau was fervently against slavery and actively supported the abolitionist movement. He participated in the Underground Railroad, delivered lectures that attacked the Fugitive Slave Law, and in opposition to the popular opinion of the time, supported radical abolitionist militia leader John Brown and his party. Two weeks after the ill-fated raid on Harpers Ferry and in the weeks leading up to Brown's execution, Thoreau regularly delivered a speech to the citizens of Concord, Massachusetts, in which he compared the American government to Pontius Pilate and likened Brown's execution to the crucifixion of Jesus Christ:\n\nIn \"The Last Days of John Brown\", Thoreau described the words and deeds of John Brown as noble and an example of heroism. In addition, he lamented the newspaper editors who dismissed Brown and his scheme as \"crazy\".\n\nThoreau was a proponent of limited government and individualism. Although he was hopeful that mankind could potentially have, through self-betterment, the kind of government which \"governs not at all\", he distanced himself from contemporary \"no-government men\" (anarchists), writing: \"I ask for, not at once no government, but at once a better government.\"\n\nThoreau deemed the evolution from absolute monarchy to limited monarchy to democracy as \"a progress toward true respect for the individual\" and theorized about further improvements \"towards recognizing and organizing the rights of man.\" Echoing this belief, he went on to write: \"There will never be a really free and enlightened State until the State comes to recognize the individual as a higher and independent power, from which all its power and authority are derived, and treats him accordingly.\"\n\nIt is on this basis that Thoreau could so strongly inveigh against British and Catholic power in \"A Yankee in Canada\". Despotic authority had crushed the people's sense of ingenuity and enterprise; the Canadian \"habitants\" had been reduced, in his view, to a perpetual childlike state. Ignoring the recent Rebellions, he argued that there would be no revolution in the St. Lawrence River valley.\n\nAlthough Thoreau believed resistance to unjustly exercised authority could be both violent (exemplified in his support for John Brown) and nonviolent (his own example of tax resistance displayed in \"Resistance to Civil Government\"), he regarded pacifist nonresistance as temptation to passivity, writing: \"Let not our Peace be proclaimed by the rust on our swords, or our inability to draw them from their scabbards; but let her at least have so much work on her hands as to keep those swords bright and sharp.\" Furthermore, in a formal lyceum debate in 1841, he debated the subject \"Is it ever proper to offer forcible resistance?\", arguing the affirmative.\n\nLikewise, his condemnation of the Mexican–American War did not stem from pacifism, but rather because he considered Mexico \"unjustly overrun and conquered by a foreign army\" as a means to expand the slave territory.\n\nThoreau was ambivalent towards industrialization and capitalism. On one hand he regarded commerce as \"unexpectedly confident and serene, adventurous, and unwearied\" and expressed admiration for its associated cosmopolitanism, writing:\n\nOn the other hand, he wrote disparagingly of the factory system:\n\nThoreau also favored bioregionalism, the protection of animals and wild areas, free trade, and taxation for schools and highways. He disapproved of the subjugation of Native Americans, slavery, technological utopianism, consumerism, philistinism, mass entertainment, and frivolous applications of technology.\n\nThoreau was influenced by Indian spiritual thought. In \"Walden\", there are many overt references to the sacred texts of India. For example, in the first chapter (\"Economy\"), he writes: \"How much more admirable the Bhagvat-Geeta than all the ruins of the East!\" \"American Philosophy: An Encyclopedia\" classes him as one of several figures who \"took a more pantheist or pandeist approach by rejecting views of God as separate from the world\", also a characteristic of Hinduism.\n\nFurthermore, in \"The Pond in Winter\", he equates Walden Pond with the sacred Ganges river, writing:\n\nThoreau was aware his Ganges imagery could have been factual. He wrote about ice harvesting at Walden Pond. And he knew that New England's ice merchants were shipping ice to foreign ports, including Calcutta.\n\nAdditionally, Thoreau followed various Hindu customs, including following a diet of rice (\"It was fit that I should live on rice, mainly, who loved so well the philosophy of India.\"), flute playing (reminiscent of the favorite musical pastime of Krishna), and yoga.\n\nIn an 1849 letter to his friend H.G.O. Blake, he wrote about yoga and its meaning to him:\n\nThoreau read contemporary works in the new science of biology, including the works of Alexander von Humboldt, Charles Darwin, and Asa Gray (Charles Darwin's staunchest American ally). Thoreau was deeply influenced by Humboldt, especially his work Kosmos.\n\nIn 1859, Thoreau purchased and read Darwin's \"On the Origin of Species\". Unlike many natural historians at the time, including Louis Agassiz who publicly opposed Darwinism in favor of a static view of nature, Thoreau was immediately enthusiastic about the theory of evolution by natural selection and endorsed it, stating:\n\nThoreau's political writings had little impact during his lifetime, as \"his contemporaries did not see him as a theorist or as a radical,\" viewing him instead as a naturalist. They either dismissed or ignored his political essays, including \"Civil Disobedience\". The only two complete books (as opposed to essays) published in his lifetime, \"Walden\" and \"A Week on the Concord and Merrimack Rivers\" (1849), both dealt with nature, in which he loved to wander.\" His obituary was lumped in with others rather than as a separate article in an 1862 yearbook. Nevertheless, Thoreau's writings went on to influence many public figures. Political leaders and reformers like Mohandas Gandhi, U.S. President John F. Kennedy, American civil rights activist Martin Luther King Jr., U.S. Supreme Court Justice William O. Douglas, and Russian author Leo Tolstoy all spoke of being strongly affected by Thoreau's work, particularly \"Civil Disobedience\", as did \"right-wing theorist Frank Chodorov [who] devoted an entire issue of his monthly, \"Analysis\", to an appreciation of Thoreau.\"\n\nThoreau also influenced many artists and authors including Edward Abbey, Willa Cather, Marcel Proust, William Butler Yeats, Sinclair Lewis, Ernest Hemingway, Upton Sinclair, E. B. White, Lewis Mumford, Frank Lloyd Wright, Alexander Posey, and Gustav Stickley. Thoreau also influenced naturalists like John Burroughs, John Muir, E. O. Wilson, Edwin Way Teale, Joseph Wood Krutch, B. F. Skinner, David Brower, and Loren Eiseley, whom \"Publishers Weekly\" called \"the modern Thoreau\". English writer Henry Stephens Salt wrote a biography of Thoreau in 1890, which popularized Thoreau's ideas in Britain: George Bernard Shaw, Edward Carpenter, and Robert Blatchford were among those who became Thoreau enthusiasts as a result of Salt's advocacy. Mohandas Gandhi first read \"Walden\" in 1906 while working as a civil rights activist in Johannesburg, South Africa. He first read \"Civil Disobedience\" \"while he sat in a South African prison for the crime of nonviolently protesting discrimination against the Indian population in the Transvaal. The essay galvanized Gandhi, who wrote and published a synopsis of Thoreau's argument, calling its 'incisive logic ... unanswerable' and referring to Thoreau as 'one of the greatest and most moral men America has produced'.\" He told American reporter Webb Miller, \"[Thoreau's] ideas influenced me greatly. I adopted some of them and recommended the study of Thoreau to all of my friends who were helping me in the cause of Indian Independence. Why I actually took the name of my movement from Thoreau's essay 'On the Duty of Civil Disobedience', written about 80 years ago.\"\n\nMartin Luther King, Jr. noted in his autobiography that his first encounter with the idea of nonviolent resistance was reading \"On Civil Disobedience\" in 1944 while attending Morehouse College. He wrote in his autobiography that it was,\n\nHere, in this courageous New Englander's refusal to pay his taxes and his choice of jail rather than support a war that would spread slavery's territory into Mexico, I made my first contact with the theory of nonviolent resistance. Fascinated by the idea of refusing to cooperate with an evil system, I was so deeply moved that I reread the work several times. I became convinced that noncooperation with evil is as much a moral obligation as is cooperation with good. No other person has been more eloquent and passionate in getting this idea across than Henry David Thoreau. As a result of his writings and personal witness, we are the heirs of a legacy of creative protest. The teachings of Thoreau came alive in our civil rights movement; indeed, they are more alive than ever before. Whether expressed in a sit-in at lunch counters, a freedom ride into Mississippi, a peaceful protest in Albany, Georgia, a bus boycott in Montgomery, Alabama, these are outgrowths of Thoreau's insistence that evil must be resisted and that no moral man can patiently adjust to injustice.\n\nAmerican psychologist B. F. Skinner wrote that he carried a copy of Thoreau's \"Walden\" with him in his youth. and, in 1945, wrote \"Walden Two\", a fictional utopia about 1,000 members of a community living together inspired by the life of Thoreau. Thoreau and his fellow Transcendentalists from Concord were a major inspiration of the composer Charles Ives. The 4th movement of the Concord Sonata for piano (with a part for flute, Thoreau's instrument) is a character picture and he also set Thoreau's words.\n\nActor Ron Thompson did a dramatic portrayal of Henry David Thoreau on the 1976 NBC television series \"The Rebels\".\n\nThoreau's ideas have impacted and resonated with various strains in the anarchist movement, with Emma Goldman referring to him as \"the greatest American anarchist\". Green anarchism and anarcho-primitivism in particular have both derived inspiration and ecological points-of-view from the writings of Thoreau. John Zerzan included Thoreau's text \"Excursions\" (1863) in his edited compilation of works in the anarcho-primitivist tradition titled \"Against civilization: Readings and reflections\". Additionally, Murray Rothbard, the founder of anarcho-capitalism, has opined that Thoreau was one of the \"great intellectual heroes\" of his movement. Thoreau was also an important influence on late-19th-century anarchist naturism. Globally, Thoreau's concepts also held importance within individualist anarchist circles in Spain, France, and Portugal.\n\nFor the 200th anniversary of his birth, publishers released several new editions of his work: a recreation of \"Walden\" 1902 edition with illustrations, a picture book with excerpts from \"Walden\", and an annotated collection of Thoreau's essays on slavery. The United States Postal Service issued a commemorative stamp honoring Thoreau on May 23, 2017 in Concord, MA.\n\nAlthough his writings would receive widespread acclaim, Thoreau's ideas were not universally applauded. Scottish author Robert Louis Stevenson judged Thoreau's endorsement of living alone and apart from modern society in natural simplicity to be a mark of \"unmanly\" effeminacy and \"womanish solitude\", while deeming him a self-indulgent \"skulker\".\n\nNathaniel Hawthorne had mixed feelings about Thoreau. He noted that \"He is a keen and delicate observer of nature—a genuine observer—which, I suspect, is almost as rare a character as even an original poet; and Nature, in return for his love, seems to adopt him as her especial child, and shows him secrets which few others are allowed to witness.\" On the other hand, he also wrote that Thoreau \"repudiated all regular modes of getting a living, and seems inclined to lead a sort of Indian life among civilized men\".\n\nIn a similar vein, poet John Greenleaf Whittier detested what he deemed to be the \"wicked\" and \"heathenish\" message of \"Walden\", claiming that Thoreau wanted man to \"lower himself to the level of a woodchuck and walk on four legs\".\n\nIn response to such criticisms, English novelist George Eliot, writing for the \"Westminster Review\", characterized such critics as uninspired and narrow-minded:\nThoreau himself also responded to the criticism in a paragraph of his work \"Walden\" by illustrating the irrelevance of their inquiries:\n\nRecent criticism has accused Thoreau of hypocrisy, misanthropy, and being sanctimonious, based on his writings in \"Walden\", although this criticism has been perceived as highly selective.\n\n\n\n\n"}
{"id": "1420409", "url": "https://en.wikipedia.org/wiki?curid=1420409", "title": "International Early Warning Programme", "text": "International Early Warning Programme\n\nThe International Early Warning Program (IEWP), was first proposed at the Second International Early Warning Conference (EWCII) in 2003 in Bonn, Germany. It developed increasing importance in the wake of the 2004 Indian Ocean tsunami, which claimed over 200,000 lives and injured over half a million people. \n\nIn January 2005, the United Nations (UN) launched extensive plans to create a global warning system to lessen the impact of deadly natural disasters at the World Conference on Disaster Reduction, held in Kobe, Japan. The UN programme would help improve prevention and resilience to all types of natural disasters, including droughts, wildfires, floods, typhoons, hurricanes, landslides, volcanoes and tsunamis, by using a comprehensive set of methods including rapid information sharing and training communities at risk. It is believed that the loss of human life would have been dramatically reduced, if a tsunami warning system, like the one that exists for the volcano-and-earthquake prone Pacific Rim, had been operational in the Indian Ocean. Technology, such as tremor and tidal gauges, fast data transfer and alarm mechanisms, used in combination with training in the danger zones, would have given hundreds of thousands of people time to move to the safety of higher ground. \n\nEarly warning systems are now widely recognized as worthwhile and necessary investments to help save lives. In 2004, millions of people in the Americas and Asia were evacuated when tropical storms struck, which saved thousands of lives. According to Michel Jarraud, Secretary-General of the World Meteorological Organisation, about 90% of all natural disasters were caused by hazards related to weather and water. Speaking at the conference, he said: \"It is WMO's aim to halve the number of deaths due to natural disasters of meteorological, hydrological and climatic origin over the next 15 years, more specifically to reduce by half the associated ten-year average fatality from the period 1995-2004 to the period 2010-2019 for these disasters.\"\n\nThere was unanimous support among participants to the January 2005 conference, as an initial step towards an International Early Warning Programme, for UN-led efforts to establish an Indian Ocean Tsunami Warning System.\n\n"}
{"id": "13680406", "url": "https://en.wikipedia.org/wiki?curid=13680406", "title": "Ion vibration current", "text": "Ion vibration current\n\nThe ion vibration current (IVI) and the associated ion vibration potential is an electric signal that arises when an acoustic wave propagates through a homogeneous fluid.\n\nHistorically, the IVI was the first known electroacoustic phenomenon. It was predicted by Peter Debye in 1933.\n\nWhen a longitudinal sound wave travels through a solvent, the associated pressure gradients push the fluid particles back and forth, and it is easy in practice to create such accelerations that measure thousands or millions of g's. If a solute molecule is more dense or less dense than the surrounding liquid, then in this accelerating environment, the molecule will move relative to the surrounding liquid. This relative motion is essentially the same phenomenon that occurs in a centrifuge, or more simply, it is essentially the same phenomenon that occurs when low-density objects float to the top of a glass of water, and high-density particles sink to the bottom (see the equivalence principle, which states that gravity is just like any other acceleration). The amount of relative motion depends on the balance between the molecule's effective mass (which includes both the mass of the molecule itself and any solvent molecules that are so tightly bound to the molecule that they follow along with the molecule's motion), its effective volume (related to buoyant force), and the viscous drag (friction) between the molecule and the surrounding fluid.\n\nIVI concerns the case where the particles in question are anions and cations. In general, they will have different amounts of motion relative to the fluid during the sound wave oscillations, and that discrepancy creates an alternating electric potential between various points in a sound wave.\n\nThis effect was extensively used in the 1950s and 1960s for characterizing ion solvation. These works are mostly associated with the names of Zana and Yaeger, who published a review of their studies in 1982.\n"}
{"id": "8746727", "url": "https://en.wikipedia.org/wiki?curid=8746727", "title": "Level of support for evolution", "text": "Level of support for evolution\n\nThe level of support for evolution among scientists, the public and other groups is a topic that frequently arises in the creation-evolution controversy and touches on educational, religious, philosophical, scientific and political issues. The subject is especially contentious in countries where significant levels of non-acceptance of evolution by general society exist although evolution is taught at school and university.\n\nNearly all (around 97%) of the scientific community accepts evolution as the dominant scientific theory of biological diversity. Scientific associations have strongly rebutted and refuted the challenges to evolution proposed by intelligent design proponents.\n\nThere are religious sects and denominations in several countries for whom the theory of evolution is in conflict with creationism that is central to their beliefs, and who therefore reject it: in the United States, South Africa, India, South Korea, Singapore, the Philippines, and Brazil, with smaller followings in the United Kingdom, the Republic of Ireland, Japan, Italy, Germany, Israel, Australia, New Zealand, and Canada.\n\nSeveral publications discuss the subject of acceptance, including a document produced by the United States National Academy of Sciences.\n\nThe vast majority of the scientific community and academia supports evolutionary theory as the only explanation that can fully account for observations in the fields of biology, paleontology, molecular biology, genetics, anthropology, and others. A 1991 Gallup poll found that about 5% of American scientists (including those with training outside biology) identified themselves as creationists.\n\nAdditionally, the scientific community considers intelligent design, a neo-creationist offshoot, to be unscientific, pseudoscience, or junk science. The U.S. National Academy of Sciences has stated that intelligent design \"and other claims of supernatural intervention in the origin of life\" are not science because they cannot be tested by experiment, do not generate any predictions, and propose no new hypotheses of their own. In September 2005, 38 Nobel laureates issued a statement saying \"Intelligent design is fundamentally unscientific; it cannot be tested as scientific theory because its central conclusion is based on belief in the intervention of a supernatural agent.\" In October 2005, a coalition representing more than 70,000 Australian scientists and science teachers issued a statement saying \"intelligent design is not science\" and calling on \"all schools not to teach Intelligent Design (ID) as science, because it fails to qualify on every count as a scientific theory\".\n\nIn 1986, an \"amicus curiae\" brief, signed by 72 US Nobel Prize winners, 17 state academies of science and 7 other scientific societies, asked the US Supreme Court in \"Edwards v. Aguillard\", to reject a Louisiana state law requiring that where evolutionary science was taught in public schools, creation science must also be taught. The brief also stated that the term \"creation science\" as used by the law embodied religious dogma, and that \"teaching religious ideas mislabeled as science is detrimental to scientific education\". This was the largest collection of Nobel Prize winners to sign a petition up to that point. According to anthropologists Almquist and Cronin, the brief is the \"clearest statement by scientists in support of evolution yet produced.\"\n\nThere are many scientific and scholarly organizations from around the world that have issued statements in support of the theory of evolution. The American Association for the Advancement of Science, the world's largest general scientific society with more than 130,000 members and over 262 affiliated societies and academies of science including over 10 million individuals, has made several statements and issued several press releases in support of evolution. The prestigious United States National Academy of Sciences, which provides science advice to the nation, has published several books supporting evolution and criticising creationism and intelligent design.\n\nThere is a notable difference between the opinion of scientists and that of the general public in the United States. A 2009 poll by Pew Research Center found that \"Nearly all scientists (97%) say humans and other living things have evolved over time – 87% say evolution is due to natural processes, such as natural selection. The dominant position among scientists – that living things have evolved due to natural processes – is shared by only about a third (32%) of the public.\"\n\nOne of the earliest resolutions in support of evolution was issued by the American Association for the Advancement of Science in 1922, and readopted in 1929.\n\nAnother early effort to express support for evolution by scientists was organized by Nobel Prize–winning American biologist Hermann J. Muller in 1966. Muller circulated a petition entitled \"Is Biological Evolution a Principle of Nature that has been well established by Science?\" in May 1966:\n\nThis manifesto was signed by 177 of the leading American biologists, including George G. Simpson of Harvard University, Nobel Prize Winner Peter Agre of Duke University, Carl Sagan of Cornell, John Tyler Bonner of Princeton, Nobel Prize Winner George Beadle, President of the University of Chicago, and Donald F. Kennedy of Stanford University, formerly head of the United States Food and Drug Administration.\n\nThis was followed by the passing of a resolution by the American Association for the Advancement of Science (AAAS) in the fall of 1972 that stated, in part, \"the theory of creation ... is neither scientifically grounded nor capable of performing the rules required of science theories\". The United States National Academy of Sciences also passed a similar resolution in the fall of 1972. A statement on evolution called \"A Statement Affirming Evolution as a Principle of Science.\" was signed by Nobel Prize Winner Linus Pauling, Isaac Asimov, George G. Simpson, Caltech Biology Professor Norman H. Horowitz, Ernst Mayr, and others, and published in 1977. The governing board of the American Geological Institute issued a statement supporting resolution in November 1981.\nShortly thereafter, the AAAS passed another resolution supporting evolution and disparaging efforts to teach creationism in science classes.\n\nTo date, there are no scientifically peer-reviewed research articles that disclaim evolution listed in the scientific and medical journal search engine Pubmed.\n\nThe Discovery Institute announced that over 700 scientists had expressed support for intelligent design as of February 8, 2007. This prompted the National Center for Science Education to produce a \"light-hearted\" petition called \"Project Steve\" in support of evolution. Only scientists named \"Steve\" or some variation (such as Stephen, Stephanie, and Stefan) are eligible to sign the petition. It is intended to be a \"tongue-in-cheek parody\" of the lists of alleged \"scientists\" supposedly supporting creationist principles that creationist organizations produce. The petition demonstrates that there are more scientists who accept evolution with a name like \"Steve\" alone (over 1370) than there are in total who support intelligent design. This is, again, why the percentage of scientists who support evolution has been estimated by Brian Alters to be about 99.9 percent.\n\nMany creationists act as evangelists and their organizations are registered as tax-free religious organizations. Creationists have claimed that they represent the interests of true Christians, and evolution is associated only with atheism.\n\nHowever, not all religious organizations find support for evolution incompatible with their religious faith. For example, 12 of the plaintiffs opposing the teaching of creation science in the influential \"McLean v. Arkansas\" court case were clergy representing Methodist, Episcopal, African Methodist Episcopal, Catholic, Southern Baptist, Reform Jewish, and Presbyterian groups. There are several religious organizations that have issued statements advocating the teaching of evolution in public schools. In addition, the Archbishop of Canterbury, Dr. Rowan Williams, issued statements in support of evolution in 2006. The Clergy Letter Project is a signed statement by 12,808 (as of 28 May 2012) American Christian clergy of different denominations rejecting creationism organized in 2004. Molleen Matsumura of the National Center for Science Education found, of Americans in the twelve largest Christian denominations, at least 77% belong to churches that support evolution education (and that at one point, this figure was as high as 89.6%). These religious groups include the Catholic Church, as well as various denominations of Protestantism, including the United Methodist Church, National Baptist Convention, USA, Evangelical Lutheran Church in America, Presbyterian Church (USA), National Baptist Convention of America, African Methodist Episcopal Church, the Episcopal Church, and others. A figure closer to about 71% is presented by the analysis of Walter B. Murfin and David F. Beck.\n\nMichael Shermer argued in Scientific American in October 2006 that evolution supports concepts like family values, avoiding lies, fidelity, moral codes and the rule of law. Shermer also suggests that evolution gives more support to the notion of an omnipotent creator, rather than a tinkerer with limitations based on a human model.\n\nThe Ahmadiyya Movement universally accepts evolution and actively promotes it. Mirza Tahir Ahmad, Fourth Caliph of the Ahmadiyya Muslim Community has stated in his magnum opus \"Revelation, Rationality, Knowledge & Truth\" that evolution did occur but only through God being the One who brings it about. It does not occur itself, according to the Ahmadiyya Muslim Community. The Ahmadis do not believe Adam was the first human on earth, but merely the first prophet to receive a revelation of God.\n\nA fundamental part of `Abdul-Bahá's teachings on evolution is the belief that all life came from the same origin: \"the origin of all material life is one...\" He states that from this sole origin, the complete diversity of life was generated: \"Consider the world of created beings, how varied and diverse they are in species, yet with one sole origin\" He explains that a slow, gradual process led to the development of complex entities:\n\nThe 1950 encyclical \"Humani generis\" advocated scepticism towards evolution without explicitly rejecting it; this was substantially amended by Pope John-Paul II in 1996 in an address to the Pontifical Academy of Sciences in which he said, \"Today, almost half a century after publication of the encyclical, new knowledge has led to the recognition of the theory of evolution as more than a hypothesis.\" Between 2000 and 2002 the International Theological Commission found that \"Converging evidence from many studies in the physical and biological sciences furnishes mounting support for some theory of evolution to account for the development and diversification of life on earth, while controversy continues over the pace and mechanisms of evolution.\" This statement was published by the Vatican on July 2004 by the authority of Cardinal Ratzinger (who became Pope Benedict XVI) who was the president of the Commission at the time.\n\nThe Magisterium has not made an authoritative statement on intelligent design, and has permitted arguments on both sides of the issue. In 2005, Cardinal Christoph Schönborn of Vienna appeared to endorse intelligent design when he denounced philosophically materialist interpretations of evolution. In an op-ed in the New York Times he said \"Evolution in the sense of common ancestry might be true, but evolution in the neo-Darwinian sense - an unguided, unplanned process of random variation and natural selection - is not.\" This common line of reasoning among fundamentalist theologians is flawed, as evolution by natural selection is not random at all; only mutations occur in a stochastic manner, while natural selection establishes genes which aid survival in a particular environment.\n\nIn the January 16–17 2006 edition of the official Vatican newspaper \"L'Osservatore Romano\", University of Bologna evolutionary biology Professor Fiorenzo Facchini wrote an article agreeing with the judge's ruling in \"Kitzmiller v. Dover\" and stating that intelligent design was unscientific. Jesuit Father George Coyne, former director of the Vatican Observatory, has also denounced intelligent design.\n\nHindus believe in the concept of evolution of life on Earth. The concepts of Dashavatara—different incarnations of God starting from simple organisms and progressively becoming complex beings—and Day and Night of Brahma are generally cited as instances of Hindu acceptance of evolution.\n\nIn the United States, many Protestant denominations promote creationism, preach against evolution, and sponsor lectures and debates on the subject. Denominations that explicitly advocate creationism instead of evolution or \"Darwinism\" include the Assemblies of God, the Free Methodist Church, Lutheran Church–Missouri Synod, Pentecostal Churches, Seventh-day Adventist Churches, Wisconsin Evangelical Lutheran Synod, Christian Reformed Church, Southern Baptist Convention, and the Pentecostal Oneness churches. Jehovah's Witnesses produce gap creationism and day-age creationism literature to refute evolution but reject the \"creationist\" label, which they consider to apply only to Young Earth creationism.\n\nA common complaint of creationists is that evolution is of no value, has never been used for anything, and will never be of any use. According to many creationists, nothing would be lost by getting rid of evolution, and science and industry might even benefit.\n\nIn fact, evolution is being put to practical use in industry and widely used on a daily basis by researchers in medicine, biochemistry, molecular biology, and genetics to both formulate hypotheses about biological systems for the purposes of experimental design, as well as to rationalise observed data and prepare applications. As of August 2017 there are 487,558 scientific papers in PubMed that mention 'evolution'. Pharmaceutical companies utilize biological evolution in their development of new products, and also use these medicines to combat evolving bacteria and viruses.\n\nBecause of the perceived value of evolution in applications, there have been some expressions of support for evolution on the part of corporations. In Kansas, there has been some widespread concern in the corporate and academic communities that a move to weaken the teaching of evolution in schools will hurt the state's ability to recruit the best talent, particularly in the biotech industry. Paul Hanle of the Biotechnology Institute warned that the United States risks falling behind in the biotechnology race with other nations if it does not do a better job of teaching evolution. James McCarter of Divergence Incorporated stated that the work of 2001 Nobel Prize winner Leland Hartwell relied heavily on the use of evolutionary knowledge and predictions, both of which have significant implications for the treatment of cancers. Furthermore, McCarter concluded that 47 of the last 50 Nobel Prizes in medicine or physiology depended on an understanding of evolutionary theory (according to McCarter's unspecified personal criteria).\n\nThere are also many educational organizations that have issued statements in support of the theory of evolution.\n\nRepeatedly, creationists and intelligent design advocates have lost suits in US courts. Here is a list of important court cases in which creationists have suffered setbacks:\n\n\nThere does not appear to be significant correlation between believing in evolution and understanding evolutionary science. In some countries, creationist beliefs (or a lack of support for evolutionary theory) are relatively widespread, even garnering a majority of public opinion. A study published in \"Science\" compared attitudes about evolution in the United States, 32 European countries (including Turkey) and Japan. The only country where acceptance of evolution was lower than in the United States was Turkey (25%). Public acceptance of evolution was most widespread (at over 80% of the population) in Iceland, Denmark and Sweden.\nAccording to the PEW research center, Afghanistan has the lowest acceptance of evolution in the Muslim countries. Only 26% of people in Afghanistan accept evolution. 62% deny human evolution and believe that humans have always existed in their present form..\n\nAccording to a 2014 poll produced by the Pew Research Center, 71% of people in Argentina believe \"humans and other living things evolved over time\" while 23% believe they have \"always existed in the present form.\"\nAccording to the PEW research, 56 percent of Armenians deny human evolution & claim that humans have always existed in their present and only 34 percent of Armenians accept human evolution.\n\nA 2009 poll showed that almost a quarter of Australians believe \"the biblical account of human origins\" over the Darwinian account. 42 percent of Australians believe in a \"wholly scientific\" explanation for the origins of life, while 32 percent believe in an evolutionary process \"guided by God\".\n\nA 2010 survey conducted by Auspoll and the Australian Academy of Science found that 79% of Australians believe in evolution (71% believe it is currently occurring, 8% believe in evolution but do not think it is currently occurring), 11% were not sure and 10% stated they do not believe in evolution.\n\nAccording to a 2014 poll by the Pew Research Center, 44% of people in Bolivia believe \"humans and other living things evolved over time\" while 39% believe they have \"always existed in the present form.\"\n\nIn a 2010 poll, 59% of respondents said they believe in theistic evolution, or evolution guided by God. A further 8% believe in evolution without divine intervention, while 25% were creationists. Support for creationism was stronger among the poor and the least educated. According to a 2014 poll produced by the Pew Research Center, 66% of Brazilians agree that humans evolved over time and 29% think they have always existed in the present form.\n\nIn a 2012 poll, 61% of Canadians believe that humans evolved from less advanced life forms, while 22% believe that God created human beings in their present form within the last 10,000 years.\n\nAccording to a 2014 poll by the Pew Research Center, 69% of people in Chile believe \"humans and other living things evolved over time\" while 26% believe they have \"always existed in the present form.\"\n\nAccording to a 2014 poll by the Pew Research Center, 59% of people in Colombia believe \"humans and other living things evolved over time\" while 35% believe they have \"always existed in the present form.\"\n\nAccording to a 2014 poll by the Pew Research Center, 56% of people in Costa Rica believe \"humans and other living things evolved over time\" while 38% believe they have \"always existed in the present form.\"\n\nAccording to the PEW research center,the Czech Republic has the highest acceptance of evolution in Eastern Europe. 83 percent people in the Czech Republic believe that humans evolved over time.\n\nAccording to a 2014 poll by the Pew Research Center, 41% of people in Dominican Republic believe \"humans and other living things evolved over time\" while 56% believe they have \"always existed in the present form.\"\n\nAccording to a 2014 poll by the Pew Research Center, 50% of people in Ecuador believe \"humans and other living things evolved over time\" while 44% believe they have \"always existed in the present form.\"\n\nAccording to a 2014 poll by the Pew Research Center, 46% of people in El Salvador believe \"humans and other living things evolved over time\" while 45% believe they have \"always existed in the present form.\"\n\nAccording to a 2014 poll by the Pew Research Center, 55% of people in Guatemala believe \"humans and other living things evolved over time\" while 38% believe they have \"always existed in the present form.\"\n\nAccording to a 2014 poll by the Pew Research Center, 49% of people in Honduras believe \"humans and other living things evolved over time\" while 45% believe they have \"always existed in the present form.\"\n\nAccording to the PEW research center, Kazakhstan has the highest acceptance of evolution in the Muslim countries. 79% of \npeople in Kazakhstan acceptance the theory of evolution.\n\nAmong those who had heard of Charles Darwin and knew something about the theory of evolution, 77% of people in India agree that enough scientific evidence exists to support Charles Darwin’s Theory of Evolution. Also, 85% of God believing Indians who know about evolution agree that life on earth evolved over time as a result of natural selection.\n\nIn a survey carried among 10 major nations, the highest proportion that agreed that evolutionary theories alone should be taught in schools was in India, at 49%.\n\nIn a recent survey conducted across 12 states in India, public acceptence of evolution stood at 65.5% across Indian population. Highest acceptence was found in Delhi, Maharashtra and Kerala (all above 78%) while the least was found to be in Haryana (41.3%). Males were marginally more likely to accept the evolution compared with females (72% vs. 69%), and non-religious people compared with religious people (74% vs. 67%). Surprisingly people who identified as ‘rightists’ accepted the evolution more than those who identified themselves as ‘leftists’ (66% vs. 61%) in political spectra. The study also identified teachers and students (over 73%) as most likely to accept evolution while employed adults (59%) least. While at the international level, the trend is quite clear that religiosity is inversely proportional to public acceptance of evolution, situation in India was strikingly opposite. Lead author, Dr. Felix Bast from Central University of Punjab conjectured possible reason for high public acceptance of evolution in India despite the fact of high religiosity is that Hinduism does not conflict Darwin’s theory of evolution to a large extent. According to 2011 census, Hindus encompass 80.3% of Indian population. Many concepts of Vedas and Hinduism support the scientific consensus of geology, climate science and evolution to a large extent. For example, according to Rigveda, the age of earth is 1.97 billion years, which is very old compared with that of creation myth propounded by Abrahamic religions (according to creationism-also called Intelligent Design, the age of earth is around 6000 years). Current scientific consensus of the age of earth is 4.543 billion years. A number of evolutionary biologists in the past as well were baffled about the surprising similarity between evolutionary theory and Hinduism. British evolutionary biologist JBS Haldane, for instance, suggested that Hindu concept of \"dashavatara\"- the ten incarnations of lord Vishnu- is a rough idea of vertebrate evolution (fish-the vertebrate to tortoise-reptile to boar-mammal to man). Vedic concepts of \"pralaya\" and \"mahapralaya\" too surprisingly capture the cyclic nature of global climate (glacial-interglacial cycles).\n\nA 2009 survey conducted by the McGill researchers and their international collaborators found that 85% of Indonesian high school students agreed with the statement, \"Millions of fossils show that life has existed for billions of years and changed over time.\"\n\nThe theory of evolution is a 'hard sell' in schools in Israel. More than half of Israeli Jews accept the human evolution while more than 40% deny human evolution & claim that humans have always existed in their present form. \n\nAccording to a 2014 poll by the Pew Research Center, 64% of people in Mexico believe \"humans and other living things evolved over time\" while 32% believe they have \"always existed in the present form.\"\n\nAccording to a 2014 poll by the Pew Research Center, 47% of people in Nicaragua believe \"humans and other living things evolved over time\" while 48% believe they have \"always existed in the present form.\"\n\nAccording to a 2008 Norstat poll for NRK, 59% of the Norwegian population fully accept evolution, 24% somewhat agree with the theory, 4% somewhat disagree with the theory while 8% do not accept evolution. 4% did not know.\n\nA 2009 survey conducted by the McGill researchers and their international collaborators found that 86% of Pakistani high school students agreed with the statement, \"Millions of fossils show that life has existed for billions of years and changed over time.\"\n\nAccording to a 2014 poll by the Pew Research Center, 61% of people in Panama believe \"humans and other living things evolved over time\" while 34% believe they have \"always existed in the present form.\"\n\nAccording to a 2014 poll by the Pew Research Center, 59% of people in Paraguay believe \"humans and other living things evolved over time\" while 30% believe they have \"always existed in the present form.\"\n\nAccording to a 2014 poll by the Pew Research Center, 51% of people in Peru believe \"humans and other living things evolved over time\" while 39% believe they have \"always existed in the present form.\"\n\nA 2006 UK poll on the \"origin and development of life\" asked participants to choose between three different explanations for the origin of life: 22% chose (Young Earth) creationism, 17% opted for intelligent design (\"certain features of living things are best explained by the intervention of a supernatural being, e.g. God\"), 48% selected evolution theory (with a divine role explicitly excluded) and the rest did not know. A 2009 poll found that only 38% of Britons believe God played no role in evolution. In a 2012 poll, 69% of Britons believe that humans evolved from less advanced life forms, while 17% believe that God created human beings in their present forms within the last 10,000 years.\n\nUS courts have ruled in favor of teaching evolution in science classrooms, and against teaching creationism, in numerous cases such as Edwards v. Aguillard, Hendren v. Campbell, McLean v. Arkansas and Kitzmiller v. Dover Area School District.\n\nA prominent organization in the United States behind the intelligent design movement is the Discovery Institute, which, through its Center for Science and Culture, conducts a number of public relations and lobbying campaigns aimed at influencing the public and policy makers in order to advance its position in academia. The Discovery Institute claims that because there is a significant lack of public support for evolution, that public schools should, as their campaign states, \"Teach the Controversy\", although there is no controversy over the validity of evolution within the scientific community.\n\nThe US has one of the highest levels of public belief in biblical or other religious accounts of the origins of life on earth among industrialized countries. However according to the PEW research center, 62 percent of adults in the United States accept human evolution and while 34 percent of adults believe that humans have always existed in their present form. The poll involved over 35,000 adults in the United States. However acceptance of evolution varies per state. For example the State of Vermont has the highest acceptance of evolution of any other State in the United States. 79% people in Vermont accept human evolution. Mississippi has the lowest acceptance of evolution of any other State in the United States.\nA 2017 Gallup creationism survey found that 38% of adults in the United States inclined to the view that \"God created humans in their present form at one time within the last 10,000 years\" when asked for their views on the origin and development of human beings, which was noted as being at the lowest level in 35 years. 19% believed that \"human beings have developed over millions of years from less advanced forms of life, but God had no part in this process\", despite 49% of respondents indicating they believed in evolution. Belief in creationism is inversely correlated to education; only 22% of those with post-graduate degrees believe in strict creationism. (The level of support for strict creationism could be even lower when poll results are adjusted after comparison with other polls with questions that more specifically account for uncertainty and ambivalence.A 2000 poll for People for the American Way found 70% of the American public felt that evolution was compatible with a belief in God.\n\nA 2005 Pew Research Center poll found that 70% of evangelical Christians believed that living organisms have not changed since their creation, but only 31% of Catholics and 32% of mainline Protestants shared this opinion. A 2005 Harris Poll estimated that 63% of liberals and 37% of conservatives agreed that humans and other primates have a common ancestry.\n\nAccording to a 2014 poll produced by the Pew Research Center, 74% of people in Uruguay believe \"humans and other living things evolved over time\" while 20% believe they have \"always existed in the present form.\"\n\nAccording to a 2014 poll by the Pew Research Center, 63% of people in Venezuela believe \"humans and other living things evolved over time\" while 33% believe they have \"always existed in the present form.\"\n\nThe level of assent that evolution garners has changed with time. The trends in acceptance of evolution can be estimated.\n\nThe level of support for evolution in different communities has varied with time. Darwin's theory had convinced almost every naturalist within 20 years of its publication in 1858, and was making serious inroads with the public and the more liberal clergy. It had reached such extremes, that by 1880, one\nAmerican religious weekly publication estimated that \"perhaps a quarter, perhaps a half of the educated ministers in our leading Evangelical denominations\" felt \"that the story of the creation and fall of man, told in Genesis, is no more the record of actual occurrences than is the parable of the Prodigal Son.\"\n\nBy the late 19th century, many of the most conservative Christians accepted an ancient earth, and life on earth before Eden. Victorian Era Creationists were more akin to people who subscribe to theistic evolution today. Even fervent anti-evolutionist Scopes Trial prosecutor William Jennings Bryan interpreted the \"days\" of Genesis as ages of the earth, and acknowledged that biochemical evolution took place, drawing the line only at the story of Adam and Eve's creation. Prominent pre-World War II creationist Harry Rimmer allowed an Old Earth by slipping millions of years into putative gaps in the Genesis account, and claimed that the Noachian Flood was only a local phenomenon.\n\nIn the decades of the 20th century, George McCready Price and a tiny group of Seventh-day Adventist followers were among the very few believers in a Young Earth and a worldwide flood, which Price championed in his \"new catastrophism\" theories. It was not until the publication of John C. Whitcomb, Jr., and Henry M. Morris’s book \"Genesis Flood\" in 1961 that Price's idea was revived. In the last few decades, many creationists have adopted Price's beliefs, becoming progressively more strict biblical literalists.\n\nIn a 1991 Gallup poll, 47% of the US population, and 25% of college graduates agreed with the statement, \"God created man pretty much in his present form at one time within the last 10,000 years.\"\n\nFourteen years\nlater, in 2005, Gallup found that 53% of Americans expressed the belief that \"God created human beings in their present form exactly the way the Bible describes it.\" About 2/3 (65.5%) of those surveyed thought that creationism was definitely or probably true. In 2005 a Newsweek poll discovered that 80 percent of the American public thought that \"God created the universe.\" and the Pew Research Center reported that \"nearly two-thirds of Americans say that creationism should be taught alongside evolution in public schools.\" Ronald Numbers commented on that with \"Most surprising of all was the discovery that large numbers of high-school biology teachers — from 30% in Illinois and 38% in Ohio to a whopping 69% in Kentucky — supported the teaching of creationism.\"\n\nThe National Center for Science Education reports that from 1985 to 2005, the number of Americans unsure about evolution increased from 7% to 21%, while the number rejecting evolution declined from 48% to 39%. Jon Miller of Michigan State University has found in his polls that the number of Americans who accept evolution has declined from 45% to 40% from 1985 to 2005.\n\nIn light of these somewhat contradictory results, it is difficult to know for sure what is happening to public opinion on evolution in the US. It does not appear that either side is making unequivocal progress. It does appear that uncertainty about the issue is increasing, however.\n\nAnecdotal evidence is that creationism is becoming more of an issue in the UK as well. One report in 2006 was that UK students are increasingly arriving ill-prepared to participate in medical studies or other advanced education.\n\nThe level of support for creationism among relevant scientists is minimal. In 2007 the Discovery Institute reported that about 600 scientists signed their \"A Scientific Dissent from Darwinism\" list, up from 100 in 2001. The actual statement of the Scientific Dissent from Darwinism is a relatively mild one that expresses skepticism about the absoluteness of 'Darwinism' (and is in line with the falsifiability required of scientific theories) to explain all features of life, and does not in any way represent an absolute denial or rejection of evolution. By contrast, a tongue-in-cheek response known as Project Steve, a list restricted to scientists named Steve, Stephanie etc. who agree that evolution is \"a vital, well-supported, unifying principle of the biological sciences,\" has 1,382 signatories . People with these names make up approximately 1% of the total U.S. population.\n\nThe United States National Science Foundation statistics on US yearly science graduates demonstrate that from 1987 to 2001, the number of biological science graduates increased by 59% while the number of geological science graduates decreased by 20.5%. However, the number of geology graduates in 2001 was only 5.4% of the number of graduates in the biological sciences, while it was 10.7% of the number of biological science graduates in 1987. The Science Resources Statistics Division of the National Science Foundation estimated that in 1999, there were 955,300 biological scientists in the US (about 1/3 of who hold graduate degrees). There were also 152,800 earth scientists in the US as well.\n\nA large fraction of the Darwin Dissenters have specialties unrelated to research on evolution; of the dissenters, three-quarters are not biologists. As of 2006, the dissenter list was expanded to include non-US scientists.\n\nSome researchers are attempting to understand the factors that affect people's acceptance of evolution. Studies have yielded inconsistent results, explains associate professor of education at Ohio State University, David Haury. He recently performed a study that found people are likely to reject evolution if they have feelings of uncertainty, regardless of how well they understand evolutionary theory. Haury believes that teachers need to show students that their intuitive feelings may be misleading (for example, using the Wason selection task), and thus to exercise caution when relying on them as they judge the rational merits of ideas.\n\n\n"}
{"id": "16657872", "url": "https://en.wikipedia.org/wiki?curid=16657872", "title": "List of alluvial sites in Switzerland", "text": "List of alluvial sites in Switzerland\n\nThe List of alluvial sites in Switzerland, listing the Swiss heritage floodplains. It is based on the \"Federal Inventory of Alluvial sites of National Importance\", part of a 1992 Ordinance of the Swiss Federal Council implementing the Federal Law on the Protection of Nature and Cultural Heritage.\n\n\n\n[[Category:Switzerland geography-related lists]\n[[Category:Nature-related lists]]\n[[Category:Protected areas of Switzerland]]\n[[Category:European drainage basins of the Atlantic Ocean|Switzerland]]"}
{"id": "31744838", "url": "https://en.wikipedia.org/wiki?curid=31744838", "title": "List of dates predicted for apocalyptic events", "text": "List of dates predicted for apocalyptic events\n\nPredictions of apocalyptic events that would result in the extinction of humanity, a collapse of civilization, or the destruction of the planet have been made since at least the beginning of the Common Era. Most predictions are related to Abrahamic religions, often standing for or similar to the eschatological events described in their scriptures. Christian predictions typically refer to events like the Rapture, Great Tribulation, Last Judgment, and the Second Coming of Christ. End-time events are usually predicted to occur within the lifetime of the person making the prediction, and are usually made using the Bible, and in particular the New Testament, as either the primary or exclusive source for the predictions. Often this takes the form of mathematical calculations, such as trying to calculate the point where it will have been 6000 years since the supposed creation of the Earth by the Abrahamic God, which according to the Talmud marks the deadline for the Messiah to appear. Predictions of the end from natural events have also been theorised by various scientists and scientific groups. While these predictions are generally accepted as plausible within the scientific community, the events and phenomena are not expected to occur for hundreds of thousands or even billions of years from now.\n\nLittle research has been done into why people make apocalyptic predictions. Historically, it has been done for reasons such as diverting attention from actual crises like poverty and war, pushing political agendas, and promoting hatred of certain groups; antisemitism was a popular theme of Christian apocalyptic predictions in medieval times, while French and Lutheran depictions of the apocalypse were known to feature English and Catholic antagonists respectively. According to psychologists, possible explanations for why people believe in modern apocalyptic predictions include mentally reducing the actual danger in the world to a single and definable source, an innate human fascination with fear, personality traits of paranoia and powerlessness and a modern romanticism involved with end-times due to its portrayal in contemporary fiction. The prevalence of Abrahamic religions throughout modern history is said to have created a culture which encourages the embracement of a future that will be drastically different from the present. Such a culture is credited with the rise in popularity of predictions that are more secular in nature, such as the 2012 phenomenon, while maintaining the centuries-old theme that a powerful force will bring the end of humanity.\n\nPolls conducted in 2012 across 20 countries found over 14% of people believe the world will end in their lifetime, with percentages ranging from 6% of people in France to 22% in the US and Turkey. Belief in the apocalypse is most prevalent in people with lower rates of education, lower household incomes, and those under the age of 35. In the UK in 2015, 23% of the general public believed the apocalypse was likely to occur in their lifetime, compared to 10% of experts from the Global Challenges Foundation. The general public believed the likeliest cause would be nuclear war, while experts thought it would be artificial intelligence. Only 3% of Britons thought the end would be caused by the Last Judgement, compared to 16% of Americans. Between one and three percent of people from both countries thought the apocalypse would be caused by zombies or alien invasion.\n\nThis section lists eschatological predictions, mostly by religious individuals or groups. Most predictions are related to Abrahamic religions, with numerous predictions standing for the eschatological events described in their scriptures. Christian predictions typically refer to events like the Rapture, Great Tribulation, Last Judgment, and the Second Coming of Christ.\n\n\n"}
{"id": "18027464", "url": "https://en.wikipedia.org/wiki?curid=18027464", "title": "List of herbaria in North America", "text": "List of herbaria in North America\n\nThis is a list of herbaria in North America, organized first by country or region where the herbarium is located, then within each region by size of the collection. For other continents, see List of herbaria.\n\nThe table below lists herbaria located in Central America and the Caribbean.\n\nAdditional Collection Resources:\n\nListed alphabetically by Herbarium Code. Note that this list includes herbaria that are inactive, meaning that the institutions are not currently adding new materials to their collections. This list also includes herbaria that have been incorporated into other herbaria.\n\nSee also: List of herbaria.\n"}
{"id": "24694990", "url": "https://en.wikipedia.org/wiki?curid=24694990", "title": "List of invasive species in Europe", "text": "List of invasive species in Europe\n\nThis is a list of invasive species in Europe. A species is regarded as invasive if it has become introduced to a location, area, or region where it did not previously occur naturally (i.e., is not a native species) and becomes capable of establishing a breeding population in the new location. An invasive species will be one that thrives in its new environment and negatively influences the ecology and biodiversity of that ecosystem.\n\nThe term invasive species refers to a subset of those species defined as introduced species. If a species has been introduced but remains local, and is not problematic to agriculture or to the local biodiversity, then it cannot be considered to be invasive, and does not belong on this list.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "24695021", "url": "https://en.wikipedia.org/wiki?curid=24695021", "title": "List of invasive species in North America", "text": "List of invasive species in North America\n\nThis is a list of invasive species in North America. A species is regarded as invasive if it has been introduced by human action to a location, area, or region where it did not previously occur naturally (i.e., is not a native species), becomes capable of establishing a breeding population in the new location without further intervention by humans, and becomes a pest in the new location, directly threatening human industry, such as agriculture, or the local biodiversity.\n\nThe term \"invasive species\" refers to a subset of those species defined as introduced species. If a species has been introduced, but remains local, and is not problematic for human industry or the local biodiversity, then it is not considered invasive, and does not belong on this list.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "146118", "url": "https://en.wikipedia.org/wiki?curid=146118", "title": "List of long-distance footpaths", "text": "List of long-distance footpaths\n\nThis is a list of some long-distance footpaths used for walking and hiking.\n\n\n\n\nThe merit of hiking trails in Hong Kong is that hikers can enjoy scenery of both the sea and highland.\n\nTranscaucasian Trail:\n\nA long distance trail in the caucasus has been a lingering idea for trekkers and hikers for many years since they started hiking remote parts of the Caucasus. \nMany sections of the TCT already exist, used by local community members and shepherds for centuries. These trail cross long valleys and traverse mammoth mountains to connect mountain villages together. Unfortunately, in recent years many of these trails have fallen into disrepair, and while many trails are known to locals, they are difficult to navigate for visitors and tourists. \nIn 2015, two Peace Corps volunteers, Paul Stephens and Jeff Haack, mapped and charted known routes in The Republic of Georgia. During this time they succeeded in locating many connections between known trails and publicizing the concept of the trail. In 2016, Tom Allen and Alessandro Mambelli scouted new trail routes in Armenia while the first trail building project began in Svaneti, Georgia. In 2017, the trail building expanded to Dilijan National Park in Armenia while trail building continued in the Svaneti region. \nToday, over 300 km of trail has been improved and marked in Georgia and Armenia. Many 7-10 day guided hikes are available on the TCT this summer. Over the next 5 years, the trail will be expanded to connect all of the sections and create even longer hikes.\nThe TCT can serve many purposes in the Caucasus region. For one, the natural diversity of the area needs to be protected. This habitat fosters many species of animal and provides unique ecosystems created by the mountains. \nMore information about the trail can be found at transcaucasiantrail.org . \nDonations can be sustaining or one-time.\n\n\n\n\n\nHkakabo Razi Trail, climbing the highest peak in Myanmar, in Khakaborazi National Park, and various footpaths in Putao\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSee: \n\n\n\n\n\n\nSee: List of long-distance hiking tracks in Australia\n\n\n\n\n\n\n"}
{"id": "2395906", "url": "https://en.wikipedia.org/wiki?curid=2395906", "title": "List of pine barrens", "text": "List of pine barrens\n\nThe following is a list of pine barrens.\n\n\nPennsylvania\n"}
{"id": "822008", "url": "https://en.wikipedia.org/wiki?curid=822008", "title": "Lists of invasive species", "text": "Lists of invasive species\n\nThese are lists of invasive species by country or region. A species is regarded as invasive if it has been introduced by human action to a location, area, or region where it did not previously occur naturally (i.e., is not a native species), becomes capable of establishing a breeding population in the new location without further intervention by humans, and becomes a pest in the new location, threatening agriculture and/or the local biodiversity.\n\nThe term invasive species refers to a subset of those species defined as introduced species, for which see List of introduced species.\n\n\n\n"}
{"id": "39127306", "url": "https://en.wikipedia.org/wiki?curid=39127306", "title": "Mechanism (philosophy)", "text": "Mechanism (philosophy)\n\nMechanism is the belief that natural wholes (principally living things) are like complicated machines or artifacts, composed of parts lacking any intrinsic relationship to each other. Thus, the source of an apparent thing's activities is not the whole itself, but its parts or an external influence on the parts.\n\nThe doctrine of mechanism in philosophy comes in two different flavors. They are both doctrines of metaphysics, but they are different in scope and ambitions: the first is a global doctrine about nature; the second is a local doctrine about humans and their minds, which is hotly contested. For clarity, we might distinguish these two doctrines as universal mechanism and anthropic mechanism.\n\nThe older doctrine, here called universal mechanism, is the ancient philosophies closely linked with materialism and reductionism, especially that of the atomists and to a large extent, stoic physics. They held that the universe is reducible to completely mechanical principles—that is, the motion and collision of matter. Later mechanists believed the achievements of the scientific revolution had shown that all phenomena could eventually be explained in terms of 'mechanical' laws, natural laws governing the motion and collision of matter that implied a thorough going determinism: if \"all\" phenomena could be explained \"entirely\" through the motion of matter under the laws of classical physics, then even more surely than the gears of a clock determine that it must strike 2:00 an hour after striking 1:00, \"all\" phenomena must be completely determined: whether past, present or future. (One of the philosophical implications of modern quantum mechanics is that this view of determinism is not defensible.)\n\nThe French mechanist and determinist Pierre Simon de Laplace formulated the sweeping implications of this thesis by saying:\n\nOne of the first and most famous expositions of universal mechanism is found in the opening passages of \"Leviathan\" by Thomas Hobbes (1651). What is less frequently appreciated is that René Descartes was a staunch mechanist, though today, in the philosophy of mind, he is remembered for introducing the mind–body problem in terms of dualism and physicalism.\n\nDescartes was a substance dualist, and argued that reality was composed of two radically different types of substance: extended matter, on the one hand, and immaterial mind, on the other. Descartes argued that one cannot explain the conscious mind in terms of the spatial dynamics of mechanistic bits of matter cannoning off each other. Nevertheless, his understanding of biology was thoroughly mechanistic in nature:\n\nHis scientific work was based on the traditional mechanistic understanding that animals and humans are completely mechanistic automata. Descartes' dualism was motivated by the seeming impossibility that mechanical dynamics could yield mental experiences.\n\nIsaac Newton ushered in a much weaker acceptation of mechanism that tolerated the antithetical, and as yet inexplicable, action at a distance of gravity. However, his work seemed to successfully predict the motion of both celestial and terrestrial bodies according to that principle, and the generation of philosophers who were inspired by Newton's example carried the mechanist banner nonetheless. Chief among them were French philosophers such as Julien Offray de La Mettrie and Denis Diderot (see also: French materialism).\n\nThe thesis in anthropic mechanism is not that everything can be completely explained in mechanical terms (although some anthropic mechanists may \"also\" believe that), but rather that everything \"about human beings\" can be completely explained in mechanical terms, as surely as can everything about clocks or the internal combustion engine.\n\nOne of the chief obstacles that all mechanistic theories have faced is providing a mechanistic explanation of the human mind; Descartes, for one, endorsed dualism in spite of endorsing a completely mechanistic conception of the material world because he argued that mechanism and the notion of a mind were logically incompatible. Hobbes, on the other hand, conceived of the mind and the will as purely mechanistic, completely explicable in terms of the effects of perception and the pursuit of desire, which in turn he held to be completely explicable in terms of the materialistic operations of the nervous system. Following Hobbes, other mechanists argued for a thoroughly mechanistic explanation of the mind, with one of the most influential and controversial expositions of the doctrine being offered by Julien Offray de La Mettrie in his \"Man a Machine\" (1748).\n\nToday, as in the past, the main points of debate between anthropic mechanists and anti-mechanists are mainly occupied with two topics: the mind — and consciousness, in particular — and free will. Anti-mechanists argue that anthropic mechanism is incompatible with our commonsense intuitions: in philosophy of mind they argue that unconscious matter cannot completely explain the phenomenon of consciousness, and in metaphysics they argue that anthropic mechanism implies determinism about human action, which (they argue) is incompatible with our understanding of ourselves as creatures with free will. Contemporary philosophers who have argued for this position include Norman Malcolm and\nDavid Chalmers.\n\nAnthropic mechanists typically respond in one of two ways. In the first, they agree with anti-mechanists that mechanism conflicts with some of our commonsense intuitions, but go on to argue that our commonsense intuitions are simply mistaken and need to be revised. Down this path lies eliminative materialism in philosophy of mind, and hard determinism on the question of free will. This option is accepted by the eliminative materialist philosopher Paul Churchland. Some have questioned how eliminative materialism is compatible with the freedom of will apparently required for anyone (including its adherents) to make truth claims. The second option, common amongst philosophers who adopt anthropic mechanism, is to argue that the arguments given for incompatibility are specious: whatever it is we mean by \"consciousness\" and \"free will,\" they urge, it is fully compatible with a mechanistic understanding of the human mind and will. As a result, they tend to argue for one or another non-eliminativist physicalist theories of mind, and for compatibilism on the question of free will. Contemporary philosophers who have argued for this sort of account include J. J. C. Smart and Daniel Dennett.\n\nSome scholars have debated over what, if anything, Gödel's incompleteness theorems imply about anthropic mechanism. Much of the debate centers on whether the human mind is equivalent to a Turing machine, or by the Church-Turing thesis, any finite machine at all. If it is, and if the machine is consistent, then Gödel's incompleteness theorems would apply to it.\n\nGödelian arguments claim that a system of human mathematicians (or some idealization of human mathematicians) is both consistent and powerful enough to recognize its own consistency. Since this is impossible for a Turing machine, the Gödelian concludes that human reasoning must be non-mechanical.\n\nHowever, the modern consensus in the scientific and mathematical community is that actual human reasoning is inconsistent; that any consistent \"idealized version\" \"H\" of human reasoning would logically be forced to adopt a healthy but counter-intuitive open-minded skepticism about the consistency of \"H\" (otherwise \"H\" is provably inconsistent); and that Gödel's theorems do not lead to any valid argument against mechanism. This consensus that Gödelian anti-mechanist arguments are doomed to failure is laid out strongly in \"Artificial Intelligence\": \"\"any\" attempt to utilize (Gödel's incompleteness results) to attack the computationalist thesis is bound to be illegitimate, since these results are quite consistent with the computationalist thesis.\"\n\nOne of the earliest attempts to use incompleteness to reason about human intelligence was by Gödel himself in his 1951 Gibbs Lecture entitled \"Some basic theorems on the foundations of mathematics and their philosophical implications\". In this lecture, Gödel uses the incompleteness theorem to arrive at the following disjunction: (a) the human mind is not a consistent finite machine, or (b) there exist Diophantine equations for which it cannot decide whether solutions exist. Gödel finds (b) implausible, and thus seems to have believed the human mind was not equivalent to a finite machine, i.e., its power exceeded that of any finite machine. He recognized that this was only a conjecture, since one could never disprove (b). Yet he considered the disjunctive conclusion to be a \"certain fact\".\n\nIn subsequent years, more direct anti-mechanist lines of reasoning were apparently floating around the intellectual atmosphere. In 1960, Hilary Putnam published a paper entitled \"Minds and Machines,\" in which he points out the flaws of a typical anti-mechanist argument. Informally, this is the argument that the (alleged) difference between \"what can be mechanically proven\" and \"what can be seen to be true by humans\" shows that human intelligence is not mechanical in nature. Or, as Putnam puts it:\n\nLet T be a Turing machine which \"represents\" me in the sense that T can prove just the mathematical statements I prove. Then using Gödel's technique I can discover a proposition that T cannot prove, and moreover I can prove this proposition. This refutes the assumption that T \"represents\" me, hence I am not a Turing machine.\n\nHilary Putnam objects that this argument ignores the issue of consistency. Gödel's technique can only be applied to consistent systems. It is conceivable, argues Putnam, that the human mind is inconsistent. If one is to use Gödel's technique to prove the proposition that T cannot prove, one must first prove (the mathematical statement representing) the consistency of T, a daunting and perhaps impossible task. Later Putnam suggested that while Gödel's theorems cannot be applied to humans, since they make mistakes and are therefore inconsistent, it may be applied to the human faculty of science or mathematics in general. If we are to believe that it is consistent, then either we cannot prove its consistency, or it cannot be represented by a Turing machine.\n\nJ. R. Lucas in \"Minds, Machines and Gödel\" (1961), and later in his book \"The Freedom of the Will\" (1970), lays out an anti-mechanist argument closely following the one described by Putnam, including reasons for why the human mind can be considered consistent. Lucas admits that, by Gödel's second theorem, a human mind cannot formally prove its own consistency, and even says (perhaps facetiously) that women and politicians are inconsistent. Nevertheless, he sets out arguments for why a male non-politician can be considered consistent. These arguments are philosophical in nature and are the subject of much debate; Lucas provides references to responses on his own website.\n\nAnother work was done by Judson Webb in his 1968 paper \"Metamathematics and the Philosophy of Mind\". Webb claims that previous attempts have glossed over whether one truly can see that the Gödelian statement \"p\" pertaining to oneself, is true. Using a different formulation of Gödel's theorems, namely, that of Raymond Smullyan and Emil Post, Webb shows one can derive convincing arguments for oneself of both the truth and falsity of \"p\". He furthermore argues that all arguments about the philosophical implications of Gödel's theorems are really arguments about whether the Church-Turing thesis is true.\n\nLater, Roger Penrose entered the fray, providing somewhat novel anti-mechanist arguments in his books, \"The Emperor's New Mind\" (1989) [ENM] and \"Shadows of the Mind\" (1994) [SM]. These books have proved highly controversial. Martin Davis responded to ENM in his paper \"Is Mathematical Insight Algorithmic?\" (ps), where he argues that Penrose ignores the issue of consistency. Solomon Feferman gives a critical examination of SM in his paper \"Penrose's Gödelian argument\" (pdf). The response of the scientific community to Penrose's arguments has been negative, with one group of scholars calling Penrose's repeated attempts to form a persuasive Gödelian argument \"a kind of intellectual shell game, in which a precisely defined notion to which a mathematical result applies... is switched for a vaguer notion\".\n\nA Gödel-based anti-mechanism argument can be found in Douglas Hofstadter's book \"\", though Hofstadter is widely viewed as a known skeptic of such arguments:\nLooked at this way, Gödel's proof suggests – though by no means does it prove! – that there could be some high-level way of viewing the mind/brain, involving concepts which do not appear on lower levels, and that this level might have explanatory power that does not exist – not even in principle – on lower levels. It would mean that some facts could be explained on the high level quite easily, but not on lower levels at all. No matter how long and cumbersome a low-level statement were made, it would not explain the phenomena in question.\nIt is analogous to the fact that, if you make derivation after derivation in Peano arithmetic, no matter how long and cumbersome you make them, you will never come up with one for G – despite the fact that on a higher level, you can see that the Gödel sentence is true.\n\nWhat might such high-level concepts be? It has been proposed for eons, by various holistically or \"soulistically\" inclined scientists and humanists that consciousness is a phenomenon that escapes explanation in terms of brain components; so here is a candidate at least. There is also the ever-puzzling notion of free will. So perhaps these qualities could be \"emergent\" in the sense of requiring explanations which cannot be furnished by the physiology alone\n\n\n"}
{"id": "47958635", "url": "https://en.wikipedia.org/wiki?curid=47958635", "title": "National Hydrogen and Fuel Cell Day", "text": "National Hydrogen and Fuel Cell Day\n\nNational Hydrogen and Fuel Cell Day was created by the Fuel Cell and Hydrogen Energy Association to help raise awareness of fuel cell and hydrogen technologies and to celebrate how far the industry has come as well as the vast potential the technologies have today and in future. \n\nOctober 8th (10.08) was chosen in reference to the atomic weight of hydrogen (1.008).\n\nNational Hydrogen and Fuel Cell Day was officially launched on October 8, 2015. \n\nThe Fuel Cell and Hydrogen Energy Association (FCHEA), its members, industry organizations, allied groups, state and federal governments and individuals are commemorating National Hydrogen and Fuel Cell Day with a variety of activities and events across the country.\n\n\n"}
{"id": "166380", "url": "https://en.wikipedia.org/wiki?curid=166380", "title": "Natural history", "text": "Natural history\n\nNatural history is a domain of inquiry involving organisms including animals, fungi and plants in their environment; leaning more towards observational than experimental methods of study. A person who studies natural history is called a naturalist or natural historian.\n\nNatural history encompasses scientific research but is not limited to it. It involves the systematic study of any category of natural objects or organisms. So while it dates from studies in the ancient Greco-Roman world and the mediaeval Arabic world, through to European Renaissance naturalists working in near isolation, today's natural history is a cross discipline umbrella of many specialty sciences; e.g., geobiology has a strong multi-disciplinary nature.\n\nThe meaning of the English term \"natural history\" (a calque of the Latin \"historia naturalis\") has narrowed progressively with time; while, by contrast, the meaning of the related term \"nature\" has widened (see also History below).\n\nIn antiquity, \"natural history\" covered essentially anything connected with nature, or which used materials drawn from nature, such as Pliny the Elder's encyclopedia of this title, published circa 77 to 79 AD, which covers astronomy, geography, humans and their technology, medicine, and superstition, as well as animals and plants.\n\nMedieval European academics considered knowledge to have two main divisions: the humanities (primarily what is now known as classics) and divinity, with science studied largely through texts rather than observation or experiment. The study of nature revived in the Renaissance, and quickly became a third branch of academic knowledge, itself divided into descriptive natural history and natural philosophy, the analytical study of nature. In modern terms, natural philosophy roughly corresponded to modern physics and chemistry, while natural history included the biological and geological sciences. The two were strongly associated. During the heyday of the gentleman scientists, many people contributed to both fields, and early papers in both were commonly read at professional science society meetings such as the Royal Society and the French Academy of Sciences – both founded during the seventeenth century.\n\nNatural history had been encouraged by practical motives, such as Linnaeus' aspiration to improve the economic condition of Sweden. Similarly, the Industrial Revolution prompted the development of geology to help find useful mineral deposits.\n\nModern definitions of natural history come from a variety of fields and sources, and many of the modern definitions emphasize a particular aspect of the field, creating a plurality of definitions with a number of common themes among them. For example, while natural history is most often defined as a type of observation and a subject of study, it can also be defined as a body of knowledge, and as a craft or a practice, in which the emphasis is placed more on the observer than on the observed.\n\nDefinitions from biologists often focus on the scientific study of individual organisms in their environment, as seen in this definition by Marston Bates: \"Natural history is the study of animals and Plants – of organisms. ... I like to think, then, of natural history as the study of life at the level of the individual – of what plants and animals do, how they react to each other and their environment, how they are organized into larger groupings like populations and communities\" and this more recent definition by D.S. Wilcove and T. Eisner: \"The close observation of organisms—their origins, their evolution, their behavior, and their relationships with other species\".\n\nThis focus on organisms in their environment is also echoed by H.W. Greene and J.B. Losos: \"Natural history focuses on where organisms are and what they do in their environment, including interactions with other organisms. It encompasses changes in internal states insofar as they pertain to what organisms do\".\n\nSome definitions go further, focusing on direct observation of organisms in their environment, both past and present, such as this one by G.A. Bartholomew: \"A student of natural history, or a naturalist, studies the world by observing plants and animals directly. Because organisms are functionally inseparable from the environment in which they live and because their structure and function cannot be adequately interpreted without knowing some of their evolutionary history, the study of natural history embraces the study of fossils as well as physiographic and other aspects of the physical environment\".\n\nA common thread in many definitions of natural history is the inclusion of a descriptive component, as seen in a recent definition by H.W. Greene: \"Descriptive ecology and ethology\". Several authors have argued for a more expansive view of natural history, including S. Herman, who defines the field as \"the scientific study of plants and animals in their natural environments. It is concerned with levels of organization from the individual organism to the ecosystem, and stresses identification, life history, distribution, abundance, and inter-relationships.\n\nIt often and appropriately includes an esthetic component\", and T. Fleischner, who defines the field even more broadly, as \"A practice of intentional, focused attentiveness and receptivity to the more-than-human world, guided by honesty and accuracy\". These definitions explicitly include the arts in the field of natural history, and are aligned with the broad definition outlined by B. Lopez, who defines the field as the \"Patient interrogation of a landscape\" while referring to the natural history knowledge of the Eskimo (Inuit).\n\nA slightly different framework for natural history, covering a similar range of themes, is also implied in the scope of work encompassed by many leading natural history museums, which often include elements of anthropology, geology, paleontology and astronomy along with botany and zoology, or include both cultural and natural components of the world.\n\nThe plurality of definitions for this field has been recognized as both a weakness and a strength, and a range of definitions have recently been offered by practitioners in a recent collection of views on natural history.\n\nNatural history begins with Aristotle and other ancient philosophers who analyzed the diversity of the natural world. Natural history was understood by Pliny the Elder to cover anything that could be found in the world, including living things, geology, astronomy, technology, art and humanity.\n\n\"De Materia Medica\" was written between 50 and 70 AD by Pedanius Dioscorides, a Roman physician of Greek origin. It was widely read for more than 1,500 years until supplanted in the Renaissance, making it one of the longest-lasting of all natural history books.\n\nFrom the ancient Greeks until the work of Carl Linnaeus and other 18th century naturalists, a major concept of natural history was the \"scala naturae\" or Great Chain of Being, an arrangement of minerals, vegetables, more primitive forms of animals, and more complex life forms on a linear scale of supposedly increasing perfection, culminating in our species.\n\nNatural history was basically static through the Middle Ages in Europe – although in the Arabic and Oriental world it proceeded at a much brisker pace. From the thirteenth century, the work of Aristotle was adapted rather rigidly into Christian philosophy, particularly by Thomas Aquinas, forming the basis for natural theology. During the Renaissance, scholars (herbalists and humanists, particularly) returned to direct observation of plants and animals for natural history, and many began to accumulate large collections of exotic specimens and unusual monsters. Leonhart Fuchs was one of the three founding fathers of botany, along with Otto Brunfels and Hieronymus Bock. Other important contributors to the field were Valerius Cordus, Konrad Gesner (\"Historiae animalium\"), Frederik Ruysch, or Gaspard Bauhin. The rapid increase in the number of known organisms prompted many attempts at classifying and organizing species into taxonomic groups, culminating in the system of the Swedish naturalist Carl Linnaeus.\n\nA significant contribution to English natural history was made by parson-naturalists such as Gilbert White, William Kirby, John George Wood, and John Ray, who wrote about plants, animals, and other aspects of nature. Many of these men wrote about nature to make the natural theology argument for the existence or goodness of God.\n\nIn modern Europe, professional disciplines such as botany, geology, mycology, palaeontology, physiology and zoology were formed. \"Natural history\", formerly the main subject taught by college science professors, was increasingly scorned by scientists of a more specialized manner and relegated to an \"amateur\" activity, rather than a part of science proper. In Victorian Scotland it was believed that the study of natural history contributed to good mental health. Particularly in Britain and the United States, this grew into specialist hobbies such as the study of birds, butterflies, seashells (malacology/conchology), beetles and wildflowers; meanwhile, scientists tried to define a unified discipline of biology (though with only partial success, at least until the modern evolutionary synthesis). Still, the traditions of natural history continue to play a part in the study of biology, especially ecology (the study of natural systems involving living organisms and the inorganic components of the Earth's biosphere that support them), ethology (the scientific study of animal behavior), and evolutionary biology (the study of the relationships between life-forms over very long periods of time), and re-emerges today as integrative organismal biology.\n\nAmateur collectors and natural history entrepreneurs played an important role in building the world's large natural history collections, such as the Natural History Museum, London, and the National Museum of Natural History in Washington D.C.\n\nThree of the greatest English naturalists of the nineteenth century, Henry Walter Bates, Charles Darwin, and Alfred Russel Wallace—who all knew each other—each made natural history travels that took years, collected thousands of specimens, many of them new to science, and by their writings both advanced knowledge of \"remote\" parts of the world—the Amazon basin, the Galápagos Islands, and the Malay archipelago, among others—and in so doing helped to transform biology from a descriptive to a theory based science.\n\nThe understanding of \"Nature\" as \"an organism and not as a mechanism\" can be traced to the writings of Alexander von Humboldt (Prussia, 1769–1859). Humboldt's copious writings and research were seminal influences for Charles Darwin, Simón Bolívar, Henry David Thoreau, Ernst Haeckel, and John Muir.\n\nNatural history museums, which evolved from cabinets of curiosities, played an important role in the emergence of professional biological disciplines and research programs. Particularly in the 19th century, scientists began to use their natural history collections as teaching tools for advanced students and the basis for their own morphological research.\n\nThe term \"natural history\" alone, or sometimes together with archaeology, forms the name of many national, regional and local natural history societies that maintain records for animals (including birds (ornithology), insects (entomology) and mammals (mammalogy)), fungi (mycology), plants (botany) and other organisms. They may also have geological and microscopical sections.\n\nExamples of these societies in Britain include the Natural History Society of Northumbria founded in 1829, London Natural History Society (1858), Birmingham Natural History Society (1859), British Entomological and Natural History Society founded in 1872, Glasgow Natural History Society, Manchester Microscopical and Natural History Society established in 1880, Whitby Naturalists' Club founded in 1913, Scarborough Field Naturalists' Society and the Sorby Natural History Society, Sheffield, founded in 1918. The growth of natural history societies was also spurred due to the growth of British colonies in tropical regions with numerous new species to be discovered. Many civil servants took an interest in their new surroundings, sending specimens back to museums in Britain. (See also: Indian natural history)\n\nSocieties in other countries include the American Society of Naturalists and Polish Copernicus Society of Naturalists.\n\n"}
{"id": "1548703", "url": "https://en.wikipedia.org/wiki?curid=1548703", "title": "Nature worship", "text": "Nature worship\n\nNature worship is any of a variety of religious, spiritual and devotional practices that focus on the worship of the nature spirits considered to be behind the natural phenomena visible throughout nature. A nature deity can be in charge of nature, a place, a biotope, the biosphere, the cosmos, or the universe. Nature worship is often considered the primitive source of modern religious beliefs and can be found in theism, panentheism, pantheism, deism, polytheism, animism, totemism, shamanism, paganism. Common to most forms of nature worship is a spiritual focus on the individual's connection and influence on some aspects of the natural world and reverence towards it.\n"}
{"id": "18400577", "url": "https://en.wikipedia.org/wiki?curid=18400577", "title": "Naturhistorieselskabet", "text": "Naturhistorieselskabet\n\nNaturhistorieselskabet - the Society for Natural History - was a private society that was the only institution to offer education in natural history in Denmark in the late 18th century. The spirit of the Age of Enlightenment and an escalating agricultural crisis, led the king and the Danish elite to call foreign experts on economy, including botany and silviculture, to the country. The autonomous University of Copenhagen, on the other hand, was reluctant to employ foreign experts in little-established disciplines. Naturhistorieselskabet was formed in 1788 in order to ensure education in botany, zoology and mineralogy based on private funds. For example, Martin Vahl lectured in botany. After the appointment in 1795 of a professor in geology and in 1797 one in botany, the society gradually lost its importance. It was soon abolished and its collections donated to the state (much later united with the university collections).\n\nWagner, P.H. 2001. Institutionaliseringen af botanik og geologi i Danmark-Norge i det 18. århundrede (colloquium). Institut for Videnskabshistorie.\n"}
{"id": "1841288", "url": "https://en.wikipedia.org/wiki?curid=1841288", "title": "Outline of energy", "text": "Outline of energy\n\nThe following outline is provided as an overview of and topical guide to energy:\n\nEnergy – in physics, this is an indirectly observed quantity often understood as the ability of a physical system to do work on other physical systems. Since work is defined as a force acting through a distance (a length of space), energy is always equivalent to the ability to exert force (a pull or a push) against an object that is moving along a definite path of certain length.\n\n\nUnits of energy\n\n\nEnergy industry\n\nSee especially and for a large number of conventional energy related topics.\n\n\nHistory of energy\n\n\n\n\n\n\nEnergy economics\n\n\n\n\n\n\n"}
{"id": "314610", "url": "https://en.wikipedia.org/wiki?curid=314610", "title": "Pebble", "text": "Pebble\n\nA pebble is a clast of rock with a particle size of 2 to 64 millimetres based on the Krumbein phi scale of sedimentology. Pebbles are generally considered larger than granules (2 to 4 millimetres diameter) and smaller than cobbles (64 to 256 millimetres diameter). A rock made predominantly of pebbles is termed a conglomerate. Pebble tools are among the earliest known man-made artifacts, dating from the Palaeolithic period of human history.\n\nA beach composed chiefly of surface pebbles is commonly termed a shingle beach. This type of beach has armoring characteristics with respect to wave erosion, as well as ecological niches that provide habitat for animals and plants.\n\nInshore banks of shingle (large quantities of pebbles) exist in some locations, such as the entrance to the River Ore, where the moving banks of shingle give notable navigational challenges.\n\nPebbles come in various colors and textures and can have streaks, known as veins, of quartz or other minerals. Pebbles are mostly smooth but, dependent on how frequently they come in contact with the sea, they can have marks of contact with other rocks or other pebbles. Pebbles left above the high water mark may have growths of organisms such as lichen on them, signifying the lack of contact with seawater.\n\nPebbles are found in two locations – on the beaches of various oceans and seas, and inland where ancient seas used to cover the land. When then the seas retreated, the rocks became landlocked. They can also be found in lakes and ponds. Pebbles can also form in rivers, and travel into estuaries where the smoothing continues in the sea.\n\nBeach pebbles and river pebbles (also known as river rock) are distinct in their geological formation and appearance.\n\nBeach pebbles form gradually over time as the ocean water washes over loose rock particles. The result is a smooth, rounded appearance. The typical size range is from 2 mm to 50 mm. The colors range from translucent white to black, and include shades of yellow, brown, red and green. Some of the more plentiful pebble beaches are found along the coast of the Pacific Ocean, beginning in the United States and extending down to the tip of South America in Argentina. Other pebble beaches are found in northern Europe (particularly on the beaches of the Norwegian Sea), along the coast of the U.K. and Ireland, on the shores of Australia, and around the islands of Indonesia and Japan.\n\nInland pebbles (river pebbles of river rock) are usually found along the shores of large rivers and lakes. These pebbles form as the flowing water washes over rock particles on the bottom and along the shores of the river. The smoothness and color of river pebbles depends on several factors, such as the composition of the soil of the river banks, the chemical characteristics of the water, and the speed of the current. Because river current is gentler than the ocean waves, river pebbles are usually not as smooth as beach pebbles. The most common colors of river rock are black, grey, green, brown and white.\n\nBeach pebbles and river pebbles are used for a variety of purposes, both outdoors and indoors. They can be sorted by colour and size, and they can also be polished to improve the texture and colour. Outdoors, beach pebbles are often used for landscaping, construction and as decorative elements. Beach pebbles are often used to cover walkways and driveways, around pools, in and around plant containers, on patios and decks. Beach and river pebbles are also used to create water-smart gardens in areas where water is scarce. Small pebbles are also used to create living spaces and gardens on the rooftops of buildings. Indoors, pebbles can be used as bookends and paperweights. Large pebbles are also used to create \"pet rocks\" for children.\n\nOn Mars, slabs of pebbly conglomerate rock have been found and have been interpreted by scientists as having formed in an ancient streambed. The gravels, which were discovered by NASA's Mars rover Curiosity, range from the size of sand particles to the size of golf balls. Analysis has shown that the pebbles were deposited by a stream that flowed at walking pace and was ankle- to hip-deep.\n\n"}
{"id": "5308780", "url": "https://en.wikipedia.org/wiki?curid=5308780", "title": "Philo (journal)", "text": "Philo (journal)\n\nPhilo was a peer-reviewed academic journal published by the Society of Humanist Philosophers from 1998 to 2014. It is published at the Center for Inquiry with assistance from Purdue University. It focused on the discussion of philosophical issues from an explicitly naturalist perspective. The journal published articles, critical discussions, review essays, and book reviews in all fields of philosophy, and particularly invited work on the philosophical credentials of both naturalism and various supernaturalist alternatives to naturalism. Electronic access to the journal is provided by the Philosophy Documentation Center.\n\n\n"}
{"id": "36104129", "url": "https://en.wikipedia.org/wiki?curid=36104129", "title": "Playa de Gulpiyuri", "text": "Playa de Gulpiyuri\n\nPlaya de Gulpiyuri is a flooded sinkhole with an inland beach located near Llanes, in Asturias Northern Spain, around 100 m from the Cantabrian Sea. Roughly 40 meters in length, it is fully tidal due to a series of underground tunnels carved by the salt water of the Cantabrian Sea which allows water from the Bay of Biscay to create small waves. \n\nIt is a popular tourist destination, natural monument, and part of Spain's Regional Network of Protected Natural Areas.\n"}
{"id": "50283749", "url": "https://en.wikipedia.org/wiki?curid=50283749", "title": "Rangeland management", "text": "Rangeland management\n\nRangeland management (also range management, range science, or arid-land management) is a professional natural science that centers around the study of rangelands and the \"conservation and sustainable management [of Arid-Lands] for the benefit of current societies and future generations.\" Range management is defined by Holechek et al. as the \"manipulation of rangeland components to obtain optimum combination of goods and services for society on a sustained basis.\"\n\nThe earliest form of Rangeland Management is not formally deemed part of the natural science studied today, although its roots can be traced to nomadic grazing practices of the neolithic agricultural revolution when humans domesticated plants and animals under pressures from population growth and environmental change. Humans might even have altered the environment in times preceding the Neolithic through hunting of large-game, whereby large losses of grazing herbivores could have resulted in altered ecological states; meaning humans have been inadvertently managing land throughout prehistory.\n\nRangeland management was developed in the United States in response to rangeland deterioration and in some cases, denudation, due to overgrazing and other misuse of arid lands, as was described by Hardin’s 1968 \"Tragedy of the Commons\" and evidenced previously by the 20th century \"Dust Bowl\". Historically, the discipline focused on the manipulation of grazing and the proper use of rangeland vegetation for livestock.\n\nToday, range management's focus has been expanded to include the host of ecosystem services that rangelands provide to humans world-wide. Key management components seek to optimize such goods and services through the protection and enhancement of soils, riparian zones, watersheds, and vegetation complexes, sustainably improving outputs of consumable range products such as red meat, wildlife, water, wood, fiber, leather, energy resource extraction, and outdoor recreation, as well as maintaining a focus on the manipulation of grazing activities of large herbivores to maintain or improve animal and plant production.\n\nThe Society for Range Management is \"the professional society dedicated to supporting persons who work with rangelands and have a commitment to their sustainable use.\" The primary Rangeland Management publications include the \"Journal of Range Management\", \"Rangelands\", and \"Rangeland Ecology & Management\".\n\nPastoralism has become a contemporary anthropological and ecological study as it faces many threats including fragmentation of land, conversion of rangeland into urban development, lack of grazing movement, impending threats on global diversity, damage to species with large terrain, decreases in shared public goods, decreased biological movements, threats of a \"tragedy of enclosures\", limitation of key resources, reduced biomass and invasive plant species growth. Interest in contemporary pastoralist cultures like the Maasai has continued to increase, especially because the traditional syncreticly-adaptive ability of pastoralists could promise lessons in collaborative and adaptive management for contemporary pastoralist societies threatened by globalization as well as for contemporary non-pastoralist societies that are managing livestock on rangelands.\n\nIn the United States, the study of range science is commonly offered at land-grant universities including New Mexico State University, Colorado State University, Oregon State University, South Dakota State University, Texas A&M University, Texas Tech University, the University of Arizona, the University of Idaho, the University of Wyoming, Utah State University, and Montana State University. The Range Science curriculum is strongly tied to animal science, as well as plant ecology, soil science, wildlife management, climatology and anthropology. Courses in a typical Range Science curriculum may include ethology, range animal nutrition, plant physiology, plant ecology, plant identification, plant communities, microbiology, soil sciences, fire control, agricultural economics, wildlife ecology, ranch management, Socioeconomics, cartography, hydrology, Ecophysiology, and environmental policy. These courses are essential to entering a range science profession.\n\nStudents with degrees in range science are eligible for a host of technician-type careers working for the federal government under the Bureau of Land Management, the United States Fish and Wildlife Service, the Agricultural Research Service, the United States Environmental Protection Agency, the NRCS, or the US Forest Service as range conservationists, inventory technicians, range monitoring/animal science agents, field botanists, natural-resource technicians, vegetation/habitat monitors, GIS programming assistants, general range technicians, and as ecological assessors, as well as working in the private sector as range managers, ranch managers, producers, commercial consultants, mining and agricultural real estate agents, or as Range/ Ranch Consultants. Individuals who complete degrees at the M.S. or P.h.D. level, can seek academic careers as professors, extension specialists, research assistants, and adjunct staff, in addition to a number of professional research positions for government agencies such as the US Department of Agriculture and other state run departments.\n\n"}
{"id": "30635101", "url": "https://en.wikipedia.org/wiki?curid=30635101", "title": "Regius Professor of Natural History (Aberdeen)", "text": "Regius Professor of Natural History (Aberdeen)\n\nThe Regius Professor of Natural History is a Regius Professorship at the University of Aberdeen in Scotland. It was originally called the Regius Professor of Civil and Natural History at Marischal College until in 1860 Marischal College and King's Colleges merged to form the University of Aberdeen, and the title changed to Natural History.\n"}
{"id": "3601462", "url": "https://en.wikipedia.org/wiki?curid=3601462", "title": "Shock diamond", "text": "Shock diamond\n\nShock diamonds (also known as Mach diamonds, Mach disks, Mach rings, donut tails or thrust diamonds) are a formation of standing wave patterns that appear in the supersonic exhaust plume of an aerospace propulsion system, such as a supersonic jet engine, rocket, ramjet, or scramjet, when it is operated in an atmosphere. The diamonds are formed from a complex flow field and are visible due to the abrupt changes in local density and pressure caused by standing shock waves. Mach diamonds (or disks) are named after Ernst Mach, the physicist who first described them.\n\nShock diamonds form when the supersonic exhaust from a propelling nozzle is slightly over-expanded, meaning that the static pressure of the gases exiting the nozzle is less than the ambient air pressure. The higher ambient pressure compresses the flow, and since the resulting pressure increase in the exhaust gas stream is adiabatic, and reduction in velocity causes its static temperature to be substantially increased. This causes reignition of the unburned combustion products in the engine’s exhaust. The exhaust is generally over-expanded at low altitudes, where air pressure is higher.\n\nAs the flow exits the nozzle, ambient air pressure will compress the flow. The external compression is caused by oblique shock waves inclined at an angle to the flow. The compressed flow is alternately expanded by Prandtl-Meyer expansion fans, and each \"diamond\" is formed by the pairing of an oblique shock with an expansion fan. When the compressed flow becomes parallel to the center line, a shock wave perpendicular to the flow forms, called a normal shock wave. The first shock diamond is located here, and the space between it and the nozzle is called the \"zone of silence\". The distance from the nozzle to the first shock diamond can be approximated by\n\nwhere \"x\" is the distance, \"D\" is the nozzle diameter, \"P\" is flow pressure, and \"P\" is atmospheric pressure.\n\nAs the exhaust passes through the normal shock wave, its temperature increases, igniting excess fuel and causing the glow that makes the shock diamonds visible. The illuminated regions either appear as disks or diamonds, giving them their name.\n\nEventually the flow expands enough so that its pressure is again below ambient, at which point the expansion fan reflects from the contact discontinuity (the outer edge of the flow). The reflected waves, called the compression fan, cause the flow to compress. If the compression fan is strong enough, another oblique shock wave will form, creating a second shock diamond. The pattern of disks would repeat indefinitely if the gases were ideal and frictionless, however, turbulent shear at the contact discontinuity causes the wave pattern to dissipate with distance.\n\nDiamond patterns can similarly form when a nozzle is under-expanded (exit pressure higher than ambient), in lower atmospheric pressure at higher altitudes. In this case, the expansion fan is first to form, followed by the oblique shock.\n\nShock diamonds are most commonly associated with jet and rocket propulsion, but they can form in other systems.\n\nShock diamonds can be seen during gas pipeline blowdowns because the gas is under high pressure and exits the blowdown valve at extreme speeds.\n\nWhen artillery pieces are fired, gas exits the cannon muzzle at supersonic speeds and produces a series of shock diamonds. The diamonds cause a bright muzzle flash which can expose the location of gun emplacements to the enemy. It was found that when the ratio between the flow pressure and atmospheric pressure is close, the shock diamonds were greatly minimized. Adding a muzzle brake to the end of the muzzle balances the pressures and prevents shock diamonds.\n\nSome radio jets, powerful jets of plasma that emanate from quasars and radio galaxies, are observed to have regularly-spaced knots of enhanced radio emissions. The jets travel at supersonic speed through a thin \"atmosphere\" of gas in space, so it is hypothesized that these knots are shock diamonds.\n\n\n"}
{"id": "46324244", "url": "https://en.wikipedia.org/wiki?curid=46324244", "title": "Skeletal changes of organisms transitioning from water to land", "text": "Skeletal changes of organisms transitioning from water to land\n\nInnovations conventionally associated with terrestrially first appeared in aquatic elpistostegalians such as \"Panderichthys rhombolepis\", \"Elpistostege watsoni\", and \"Tiktaalik roseae\". Phylogenetic analyses distribute the features that developed along the tetrapod stem and display a stepwise process of character acquisition, rather than abrupt. The complete transition occurred over a period of 25 million years beginning with the tetrapodomorph diversification in the Middle Devonian (380 myr).\n\nBy the Upper Devonian period, the fin-limb transition as well as other skeletal changes such as gill arch reduction, opercular series loss, mid-line fin loss, and scale reduction were already completed in many aquatic organisms. As aquatic tetrapods began their transition to land, several skeletal changes are thought to have occurred to allow for movement and respiration on land. Some adaptations required to adjust to non-aquatic life include the movement and use of alternating limbs, the use of pelvic appendages as sturdy propulsors, and the use of a solid surface at the organism’s base to generate propulsive force required for walking.\n\nThe Osteolepiformes and Elpistostegalia are two crown groups of rhipidistians with respect to the tetrapods. The development of skull roof and cheekbone patterns in these organisms match those found in the first tetrapods. Palatal and nasal skeletal features like choanae are present in these groups and are also observed in modern amphibians. This indicates that incipient air breathing was developed, as well as modification of the hyoid arch towards stapes development. These characteristics account for why osteichthyans are accepted as the sister group of tetrapods.\n\nThe elpistostegalid fish are considered the most apomorphic of fish in comparison to tetrapods. From well-preserved fossils, it is observed that they share a paltybasic skull with eye ridges, and external nares situated on the margin of the mouth. Development of eye ridges and flatting of the skull are also observed in primitive fossil amphibians and reptiles. The most likely reason for the traits to be adaptive was for their use in aerial vision above the waterline. The traits enabled animals to check area on land for safe spots if being chased by a predator in water, as well as being useful for searching for prey items above the water. The water-based lateral line system was used substantially by these aquatic tetrapods to detect danger from predators. Within the Osteichthyan diversification, there were no changes related to respiration in the transition as can be seen by the nasal region and palatal morphology in elpistostegalid fishes. The primary change from basic ostelepiform ancestors to the first elpistostegalid in the middle Devonian was to the pre-existing roof skulls.\n\nIn \"Elginerpeton pancheni\", a prototetrapod from the late Frasnian, basic tetrapod characteristics in the lower jaw and the cranium are observed. The taxon is believed to fill the gap between elpistostegalid fishes and well-preserved Devonian tetrapods. The \"Elginerpeton\" is considered more derived than the elpistostegalid fishes due to presence of paired fangs on the parasymphysial toothplate, a slender shaped anterior coronoid, and in the loss of the intracranial joint and coronoid fossa. The loss of the intercranial joint was a direct functional necessity to strengthen the broad and long platybasic skull when the animal was out of the water. The tubular lower jaw of the \"Elginerpeton\", compared to the flat-lamina jaw shape of fishes gave it superior cross-sectional force, required when not supported in an aquatic setting – allowing for opening of the mouth outside of water. The adaptation may also be interpreted as a specialization for buccopharyngeal breathing. It is speculated to be the first step towards aerial respiration in the transition from fish to tetrapod.\n\nIn the tetrapod and higher clades from the lower-middle Famennian there are several defining changes on the basis of anatomy of \"Ichthyostega\", \"Tulerpeton\", and \"Acanthostega\". In the cranium, there is a stapes derived from the hyomandibular of fishes; a single bilateral pair of nasal bones, and a fenestra ovalis in the otic capsule of the braincase. The opening of the otic wall of the braincase can be considered a paedomorphic feature for tetrapods and is linked to the stapes functionally. The stapes was thought to be just a structural support between the palate and the stapedial plate of the braincase. In the \"Acanthostega\", it is likely that due to the otic capsule of the brain case being mesial to the stapedial plate, sound was picked up from the palate or the otic notch to allow for rudimentary hearing. It was able to perceive vibrations by opening its mouth by way of the palate. Other factors that caused aquatic tetrapods to spend more time on land caused the development of terrestrial hearing with the development of a tympanum within an otic notch and developed by convergent evolution at least three times.\nThere was also a change in the dermal bones of the skull in the aquatic tetrapods. It involved the enlargement of the jugal, ceasing the contact of the maxilla with the squamosal and the single bilateral pair of nasal bones. The feature allows for a stronger bite as well as increasing the strength of the skull.\n\nFeeding on land is a completely different task than feeding in water. Water is much more dense and viscous compared to air, causing hunting techniques adapted in water to be less successful when applied on land. The main technique used in water is suction feeding and is used by most aquatic vertebrates. This technique does not function in air so animals use methods of overtaking prey with jaws followed by biting down. Transitional forms prior to fully developed terrestrial tetrapods such as \"Acanthostega\", are thought to have captured prey in the water. Large coronoid fangs are present in the fishes \"Eusthenopteron\", \"Panderichthys\", and \"Tiktaalik\", and the early tetrapod, \"Ventasega\". In \"Acanthostega\", which is more derived, the large teeth are absent. In \"Eusthenopetron\" and \"Panderichthys\", an ossified operculum is exhibited unlike in the \"Tiktaalik\", \"Ventastega\", and \"Acanthostega\". These differences as well as reductions of the gill chamber and changes in the nature of the lower jaw are hypothesized to indicate a reduced reliance on suction feeding in early tetrapods in comparison to osteolepiform fish. This morphological data is not enough however to prove that suction feeding was less used as the morphological changes have been found in fish that use the suction feeding mechanism.\n\nCranial sutures are indicators of skull function and morphologies can be linked to specific feeding modes. Transitional feeding changes can be observed by examining cross sectional morphology of a suture in taxa of the fish-tetrapod transition. Comparing positionally comparable sutures in extant fish allows for the creation of a sutural morphospace. The main cause of sutural deformation is caused by strain during feeding activity, most prominent with feeding mechanisms involving sucking a prey into the mouth. There is a tension anteriorly, and compression posteriorly strain patterns are observed in \"Polypterus\", a prey-sucking predator. In terrestrial tetrapod \"Phonerpeton\", there is compression between the frontals and parietals and a complex loading between the post parietals. There is no evidence of tensile strain in any sutures. \"Acanthostega\" fossil records demonstrate that no strain pattern was exhibited that relate to prey capture by means of suction. The load compression is similar to extant tetrapods. It is most likely that the organism captured prey by biting in the water or near the edge of the water. This finding indicates that the terrestrial mode of feeding first emerged in an aquatic environment.\n\nThe cranial endoskeleton of \"T. roseae\" shares derived features with tetrapods. There was a loss of opercular and extrascapular elements, enhancing head mobility in \"T. roseae\" compared to other tetrapodomorph fish. The formation of the neck allowed for locomotion in shallow waters. This environment allows for less motility compared to the three-dimensional space that fish are able to orient themselves in. The body of the organism in these environments would be fixed in the shallow pools with appendages planted on a substrate.\n\nIn the \"Acanthostega\" and \"Ichthyostega\", which are considered to be more derived than other basal aquatic tetrapods, the pectoral girdle is decoupled from the skull. There is also a loss of the dorsal pectoral girdle bones, which permits a large degree of movement for the shoulder. This allowed for a greater degree of movement, and is a necessity for improving aquatic maneuveurs and terrestrial locomotion. This could have been driven by the need to lift the head to aid aerial respiration by using nostrils and choanae.\n\nLimbs in vertebrates are occasionally organized into stylopod (relating to the humerus and femur), zeugopod (relating to the radius and tibia, along with associated structures) and autopod (relating to digits) categories, although anatomically, the evolutionary differences between these groups in early tetrapods tends to be vague.\nThe transition from fins to limbs occurred once an endoskeleton entered the base of the fin, as seen in today's lungfish. This is thought to have originated in the group Sarcopterygians, including osteolipiforms like \"Eusthenopteron\", due to the homology of the tetrapod forelimb and the osteolepiform fin endoskeleton.\n\n\"Acanthostega\" is a partially aquatic tetrapod with developed limbs that shares features common with the earlier tetrapods, \"Panderichthys\" and \"Eusthenopteron\". Like \"Panderichthys\", the humerus of \"Acanthostega\" is flattened dorso-ventrally, the intermedium terminates level with the radius, and the endoskeleton can be divided into stylopodium, zeugopodium and autopodium segments. Similar to \"Eusthenopteron\", the radials do not articulate with the radius on the distal end. \"Acanthostega\" also has a 1:2 ratio of humerus to radius and ulna, a feature seen in all tetrapods higher than \"Acanthostega\" on the phylogeny.\n\nUnlike \"Panderichthys\", \"Acanthostega\" hind limbs are at least the size of its fore limbs, if not larger. This development of larger limbs is required to physically support the organism during emergence from an aquatic setting to land. The humerus and femur of \"Acanthostega\" also contain evidence of greater development of the appendicular muscles compared to more aquatic tetrapods, hinting at the presence of digits.\n\nSimilarly, \"Ossinodus\" has two hindlimbs located bilaterally and proximodistally aymmetrical. Due to the presence of a small femur during juvenile development, this Carboniferous- period tetrapod is thought to be aquatic during juvenile development; only emerging onto land once it reaches adulthood. \"Ossinodus\" also has a broad, flat tibia, akin to \"Acanthostega\", and is thought to be only partially terrestrial.\n\nThe development of the pelvic region was crucial for the adaptation from water to land, yet some features of tetrapod locomotion are thought to have arose before the origin of digited limbs or the transition from water to land. The fossil record of early tetrapods shows evidence of distinct pelvic development occurring in osteolepiforms, further supporting osteolepiform ancestry of terrestrial tetrapods.\n\n\"Acanthostega\" has a large pelvis, with the iliac region articulating with the axial skeleton and a broad ischial plate. It has a sacrum; a fundamental skeletal feature that allows the organism to transfer force produced in its hindlimbs to its axial skeleton, and move in a terrestrial environment. A pubo-ischiadic symphysis is also observed, uniting the two pelvic halves.\n\nIn contrast, \"Protopterus annectens\" (a member of lungfish, thought to be a sister group to tetrapods) has a small, anatomically simpler pelvis, a derived limb endoskeleton and a lack of digits. Yet, it shares the ability to lift itself using a solid surface as a base with its pelvic region with \"Acanthostega\" and is also observed to move with tetrapod-like locomotion in an aquatic environment. This illustrates that a fundamental innovation in tetrapods is also found in a lower, sister taxon, in which members lack a sacrum.\n\n\"Acanthostega\" is the earliest example of a digitized tetrapod. The humerus and femur of \"Acanthostega\" contain evidence of greater development of the appendicular muscles compared to more aquatic tetrapods. \"Acanthostega\" has a total lack of dermal fin rays and displays the presence of two or more spool-shaped bones or cartilages articulating individually in antero-posterial sets on the distal end of its limbs. This feature can now be distinguished as digits instead of the endoskeletal radials seen in earlier tetrapods.\n\n\"Pederpes\", a tetrapod from the Early Carboniferous period, also has hindlimbs containing 5 digits that are rotated to face anteriorly. Unlike previous tetrapods, who have been only partially adapted to land, \"Pederpes\" has the novel ability to bend its limbs and propel itself forwards in a terrestrial setting. This is attributed to the symmetry of the digits and limbs in \"Pederpes\", allowing it to rotate its hindlimbs to an anteriorly facing position and propel itself from the edge of the foot when moving forward. This morphological development of bendable wrists and ankles can distinguish \"Pederpes\" the first true terrestrial tetrapod.\n"}
{"id": "37738", "url": "https://en.wikipedia.org/wiki?curid=37738", "title": "Soil", "text": "Soil\n\nSoil is a mixture of organic matter, minerals, gases, liquids, and organisms that together support life. Earth's body of soil is the pedosphere, which has four important functions: it is a medium for plant growth; it is a means of water storage, supply and purification; it is a modifier of Earth's atmosphere; it is a habitat for organisms; all of which, in turn, modify the soil.\n\nThe pedosphere interfaces with the lithosphere, the hydrosphere, the atmosphere, and the biosphere. The term \"pedolith\", used commonly to refer to the soil, translates to \"ground stone\". Soil consists of a solid phase of minerals and organic matter (the soil matrix), as well as a porous phase that holds gases (the soil atmosphere) and water (the soil solution). Accordingly, soils are often treated as a three-state system of solids, liquids, and gases.\n\nSoil is a product of the influence of climate, relief (elevation, orientation, and slope of terrain), organisms, and its parent materials (original minerals) interacting over time. It continually undergoes development by way of numerous physical, chemical and biological processes, which include weathering with associated erosion. Given its complexity and strong internal connectedness, it is considered an ecosystem by soil ecologists.\n\nMost soils have a dry bulk density (density of soil taking into account voids when dry) between 1.1 and 1.6 g/cm, while the soil particle density is much higher, in the range of 2.6 to 2.7 g/cm. Little of the soil of planet Earth is older than the Pleistocene and none is older than the Cenozoic, although fossilized soils are preserved from as far back as the Archean.\n\nSoil science has two basic branches of study: edaphology and pedology. Edaphology is concerned with the influence of soils on living things. Pedology is focused on the formation, description (morphology), and classification of soils in their natural environment. In engineering terms, soil is included in the broader concept of regolith, which also includes other loose material that lies above the bedrock, as can be found on the Moon and other celestial objects, as well. Soil is also commonly referred to as earth or dirt; some scientific definitions distinguish \"dirt\" from \"soil\" by restricting the former term specifically to the displaced soil.\n\nSoil is a major component of the Earth's ecosystem. The world's ecosystems are impacted in far-reaching ways by the processes carried out in the soil, from ozone depletion and global warming to rainforest destruction and water pollution. With respect to Earth's carbon cycle, soil is an important carbon reservoir, and it is potentially one of the most reactive to human disturbance and climate change. As the planet warms, it has been predicted that soils will add carbon dioxide to the atmosphere due to increased biological activity at higher temperatures, a positive feedback (amplification). This prediction has, however, been questioned on consideration of more recent knowledge on soil carbon turnover.\n\nSoil acts as an engineering medium, a habitat for soil organisms, a recycling system for nutrients and organic wastes, a regulator of water quality, a modifier of atmospheric composition, and a medium for plant growth, making it a critically important provider of ecosystem services. Since soil has a tremendous range of available niches and habitats, it contains most of the Earth's genetic diversity. A gram of soil can contain billions of organisms, belonging to thousands of species, mostly microbial and in the main still unexplored. Soil has a mean prokaryotic density of roughly 10 organisms per gram, whereas the ocean has no more than 10 procaryotic organisms per milliliter (gram) of seawater. Organic carbon held in soil is eventually returned to the atmosphere through the process of respiration carried out by heterotrophic organisms, but a substantial part is retained in the soil in the form of soil organic matter; tillage usually increases the rate of soil respiration, leading to the depletion of soil organic matter. Since plant roots need oxygen, ventilation is an important characteristic of soil. This ventilation can be accomplished via networks of interconnected soil pores, which also absorb and hold rainwater making it readily available for uptake by plants. Since plants require a nearly continuous supply of water, but most regions receive sporadic rainfall, the water-holding capacity of soils is vital for plant survival.\n\nSoils can effectively remove impurities, kill disease agents, and degrade contaminants, this latter property being called natural attenuation. Typically, soils maintain a net absorption of oxygen and methane and undergo a net release of carbon dioxide and nitrous oxide. Soils offer plants physical support, air, water, temperature moderation, nutrients, and protection from toxins. Soils provide readily available nutrients to plants and animals by converting dead organic matter into various nutrient forms.\n\nA typical soil is about 50% solids (45% mineral and 5% organic matter), and 50% voids (or pores) of which half is occupied by water and half by gas. The percent soil mineral and organic content can be treated as a constant (in the short term), while the percent soil water and gas content is considered highly variable whereby a rise in one is simultaneously balanced by a reduction in the other. The pore space allows for the infiltration and movement of air and water, both of which are critical for life existing in soil. Compaction, a common problem with soils, reduces this space, preventing air and water from reaching plant roots and soil organisms.\n\nGiven sufficient time, an undifferentiated soil will evolve a soil profile which consists of two or more layers, referred to as soil horizons, that differ in one or more properties such as in their texture, structure, density, porosity, consistency, temperature, color, and reactivity. The horizons differ greatly in thickness and generally lack sharp boundaries; their development is dependent on the type of parent material, the processes that modify those parent materials, and the soil-forming factors that influence those processes. The biological influences on soil properties are strongest near the surface, while the geochemical influences on soil properties increase with depth. Mature soil profiles typically include three basic master horizons: A, B, and C. The solum normally includes the A and B horizons. The living component of the soil is largely confined to the solum, and is generally more prominent in the A horizon.\n\nThe soil texture is determined by the relative proportions of the individual particles of sand, silt, and clay that make up the soil. The interaction of the individual mineral particles with organic matter, water, gases via biotic and abiotic processes causes those particles to flocculate (stick together) to form aggregates or peds. Where these aggregates can be identified, a soil can be said to be developed, and can be described further in terms of color, porosity, consistency, reaction (acidity), etc.\n\nWater is a critical agent in soil development due to its involvement in the dissolution, precipitation, erosion, transport, and deposition of the materials of which a soil is composed. The mixture of water and dissolved or suspended materials that occupy the soil pore space is called the soil solution. Since soil water is never pure water, but contains hundreds of dissolved organic and mineral substances, it may be more accurately called the soil solution. Water is central to the dissolution, precipitation and leaching of minerals from the soil profile. Finally, water affects the type of vegetation that grows in a soil, which in turn affects the development of the soil, a complex feedback which is exemplified in the dynamics of banded vegetation patterns in semi-arid regions.\n\nSoils supply plants with nutrients, most of which are held in place by particles of clay and organic matter (colloids) The nutrients may be adsorbed on clay mineral surfaces, bound within clay minerals (absorbed), or bound within organic compounds as part of the living organisms or dead soil organic matter. These bound nutrients interact with soil water to buffer the soil solution composition (attenuate changes in the soil solution) as soils wet up or dry out, as plants take up nutrients, as salts are leached, or as acids or alkalis are added.\n\nPlant nutrient availability is affected by soil pH, which is a measure of the hydrogen ion activity in the soil solution. Soil pH is a function of many soil forming factors, and is generally lower (more acid) where weathering is more advanced.\n\nMost plant nutrients, with the exception of nitrogen, originate from the minerals that make up the soil parent material. Some nitrogen originates from rain as dilute nitric acid and ammonia, but most of the nitrogen is available in soils as a result of nitrogen fixation by bacteria. Once in the soil-plant system, most nutrients are recycled through living organisms, plant and microbial residues (soil organic matter), mineral-bound forms, and the soil solution. Both living microorganisms and soil organic matter are of critical importance to this recycling, and thereby to soil formation and soil fertility. Microbial activity in soils may release nutrients from minerals or organic matter for use by plants and other microorganisms, sequester (incorporate) them into living cells, or cause their loss from the soil by volatilisation (loss to the atmosphere as gases) or leaching.\n\nThe history of the study of soil is intimately tied to humans' urgent need to provide food for themselves and forage for our animals. Throughout history, civilizations have prospered or declined as a function of the availability and productivity of their soils.\n\nThe Greek historian Xenophon (450–355 BCE) is credited with being the first to expound upon the merits of green-manuring crops: \"But then whatever weeds are upon the ground, being turned into earth, enrich the soil as much as dung.\"\n\nColumella's \"Husbandry,\" circa 60 CE, advocated the use of lime and that clover and alfalfa (green manure) should be turned under, and was used by 15 generations (450 years) under the Roman Empire until its collapse. From the fall of Rome to the French Revolution, knowledge of soil and agriculture was passed on from parent to child and as a result, crop yields were low. During the European Dark Ages, Yahya Ibn al-'Awwam's handbook, with its emphasis on irrigation, guided the people of North Africa, Spain and the Middle East; a translation of this work was finally carried to the southwest of the United States when under Spanish influence. Olivier de Serres, considered as the father of French agronomy, was the first to suggest the abandonment of fallowing and its replacement by hay meadows within crop rotations, and he highlighted the importance of soil (the French terroir) in the management of vineyards. His famous book \"Le Théâtre d’Agriculture et mesnage des champs\" contributed to the rise of modern, sustainable agriculture and to the collapse of old agricultural practices such as the lifting of forest litter for the amendment of crops (the French \"soutrage\") and assarting, which ruined the soils of western Europe during Middle Ages and even later on according to regions.\n\nExperiments into what made plants grow first led to the idea that the ash left behind when plant matter was burned was the essential element but overlooked the role of nitrogen, which is not left on the ground after combustion, a belief which prevailed until the 19th century. In about 1635, the Flemish chemist Jan Baptist van Helmont thought he had proved water to be the essential element from his famous five years' experiment with a willow tree grown with only the addition of rainwater. His conclusion came from the fact that the increase in the plant's weight had apparently been produced only by the addition of water, with no reduction in the soil's weight. John Woodward (d. 1728) experimented with various types of water ranging from clean to muddy and found muddy water the best, and so he concluded that earthy matter was the essential element. Others concluded it was humus in the soil that passed some essence to the growing plant. Still others held that the vital growth principal was something passed from dead plants or animals to the new plants. At the start of the 18th century, Jethro Tull demonstrated that it was beneficial to cultivate (stir) the soil, but his opinion that the stirring made the fine parts of soil available for plant absorption was erroneous.\n\nAs chemistry developed, it was applied to the investigation of soil fertility. The French chemist Antoine Lavoisier showed in about 1778 that plants and animals must [combust] oxygen internally to live and was able to deduce that most of the 165-pound weight of van Helmont's willow tree derived from air. It was the French agriculturalist Jean-Baptiste Boussingault who by means of experimentation obtained evidence showing that the main sources of carbon, hydrogen and oxygen for plants were air and water, while nitrogen was taken from soil. Justus von Liebig in his book \"Organic chemistry in its applications to agriculture and physiology\" (published 1840), asserted that the chemicals in plants must have come from the soil and air and that to maintain soil fertility, the used minerals must be replaced. Liebig nevertheless believed the nitrogen was supplied from the air. The enrichment of soil with guano by the Incas was rediscovered in 1802, by Alexander von Humboldt. This led to its mining and that of Chilean nitrate and to its application to soil in the United States and Europe after 1840.\n\nThe work of Liebig was a revolution for agriculture, and so other investigators started experimentation based on it. In England John Bennet Lawes and Joseph Henry Gilbert worked in the Rothamsted Experimental Station, founded by the former, and (re)discovered that plants took nitrogen from the soil, and that salts needed to be in an available state to be absorbed by plants. Their investigations also produced the \"superphosphate\", consisting in the acid treatment of phosphate rock. This led to the invention and use of salts of potassium (K) and nitrogen (N) as fertilizers. Ammonia generated by the production of coke was recovered and used as fertiliser. Finally, the chemical basis of nutrients delivered to the soil in manure was understood and in the mid-19th century chemical fertilisers were applied. However, the dynamic interaction of soil and its life forms still awaited discovery.\n\nIn 1856 J. Thomas Way discovered that ammonia contained in fertilisers was transformed into nitrates, and twenty years later Robert Warington proved that this transformation was done by living organisms. In 1890 Sergei Winogradsky announced he had found the bacteria responsible for this transformation.\n\nIt was known that certain legumes could take up nitrogen from the air and fix it to the soil but it took the development of bacteriology towards the end of the 19th century to lead to an understanding of the role played in nitrogen fixation by bacteria. The symbiosis of bacteria and leguminous roots, and the fixation of nitrogen by the bacteria, were simultaneously discovered by the German agronomist Hermann Hellriegel and the Dutch microbiologist Martinus Beijerinck.\n\nCrop rotation, mechanisation, chemical and natural fertilisers led to a doubling of wheat yields in western Europe between 1800 and 1900.\n\nThe scientists who studied the soil in connection with agricultural practices had considered it mainly as a static substrate. However, soil is the result of evolution from more ancient geological materials, under the action of biotic and abiotic (not associated with life) processes. After studies of the improvement of the soil commenced, others began to study soil genesis and as a result also soil types and classifications.\n\nIn 1860, in Mississippi, Eugene W. Hilgard studied the relationship among rock material, climate, and vegetation, and the type of soils that were developed. He realised that the soils were dynamic, and considered soil types classification. Unfortunately his work was not continued. At the same time Vasily Dokuchaev (about 1870) was leading a team of soil scientists in Russia who conducted an extensive survey of soils, finding that similar basic rocks, climate and vegetation types lead to similar soil layering and types, and established the concepts for soil classifications. Due to language barriers, the work of this team was not communicated to western Europe until 1914 through a publication in German by Konstantin Dmitrievich Glinka, a member of the Russian team.\n\nCurtis F. Marbut was influenced by the work of the Russian team, translated Glinka's publication into English, and as he was placed in charge of the U. S. National Cooperative Soil Survey, applied it to a national soil classification system.\n\nSoil formation, or pedogenesis, is the combined effect of physical, chemical, biological and anthropogenic processes working on soil parent material. Soil is said to be formed when organic matter has accumulated and colloids are washed downward, leaving deposits of clay, humus, iron oxide, carbonate, and gypsum, producing a distinct layer called the B horizon. This is a somewhat arbitrary definition as mixtures of sand, silt, clay and humus will support biological and agricultural activity before that time. These constituents are moved from one level to another by water and animal activity. As a result, layers (horizons) form in the soil profile. The alteration and movement of materials within a soil causes the formation of distinctive soil horizons. However, more recent definitions of soil embrace soils without any organic matter, such as those regoliths that formed on Mars and analogous conditions in planet Earth deserts.\n\nAn example of the development of a soil would begin with the weathering of lava flow bedrock, which would produce the purely mineral-based parent material from which the soil texture forms. Soil development would proceed most rapidly from bare rock of recent flows in a warm climate, under heavy and frequent rainfall. Under such conditions, plants (in a first stage nitrogen-fixing lichens and cyanobacteria then epilithic higher plants) become established very quickly on basaltic lava, even though there is very little organic material. The plants are supported by the porous rock as it is filled with nutrient-bearing water that carries minerals dissolved from the rocks. Crevasses and pockets, local topography of the rocks, would hold fine materials and harbour plant roots. The developing plant roots are associated with mineral-weathering mycorrhizal fungi that assist in breaking up the porous lava, and by these means organic matter and a finer mineral soil accumulate with time. Such initial stages of soil development have been described on volcanoes, inselbergs, and glacial moraines.\n\nHow soil formation proceeds is influenced by at least five classic factors that are intertwined in the evolution of a soil. They are: parent material, climate, topography (relief), organisms, and time. When reordered to climate, relief, organisms, parent material, and time, they form the acronym CROPT.\n\nThe mineral material from which a soil forms is called parent material. Rock, whether its origin is igneous, sedimentary, or metamorphic, is the source of all soil mineral materials and the origin of all plant nutrients with the exceptions of nitrogen, hydrogen and carbon. As the parent material is chemically and physically weathered, transported, deposited and precipitated, it is transformed into a soil.\n\nTypical soil parent mineral materials are:\nParent materials are classified according to how they came to be deposited. Residual materials are mineral materials that have weathered in place from primary bedrock. Transported materials are those that have been deposited by water, wind, ice or gravity. Cumulose material is organic matter that has grown and accumulates in place.\n\nResidual soils are soils that develop from their underlying parent rocks and have the same general chemistry as those rocks. The soils found on mesas, plateaux, and plains are residual soils. In the United States as little as three percent of the soils are residual.\n\nMost soils derive from transported materials that have been moved many miles by wind, water, ice and gravity.\n\nCumulose parent material is not moved but originates from deposited organic material. This includes peat and muck soils and results from preservation of plant residues by the low oxygen content of a high water table. While peat may form sterile soils, muck soils may be very fertile.\n\nThe weathering of parent material takes the form of physical weathering (disintegration), chemical weathering (decomposition) and chemical transformation. Generally, minerals that are formed under high temperatures and pressures at great depths within the Earth's mantle are less resistant to weathering, while minerals formed at low temperature and pressure environment of the surface are more resistant to weathering. Weathering is usually confined to the top few meters of geologic material, because physical, chemical, and biological stresses and fluctuations generally decrease with depth. Physical disintegration begins as rocks that have solidified deep in the Earth are exposed to lower pressure near the surface and swell and become mechanically unstable. Chemical decomposition is a function of mineral solubility, the rate of which doubles with each 10 °C rise in temperature, but is strongly dependent on water to effect chemical changes. Rocks that will decompose in a few years in tropical climates will remain unaltered for millennia in deserts. Structural changes are the result of hydration, oxidation, and reduction. Chemical weathering mainly results from the excretion of organic acids and chelating compounds by bacteria and fungi, thought to increase under present-day greenhouse effect.\n\n\nOf the above, hydrolysis and carbonation are the most effective, in particular in regions of high rainfall, temperature and physical erosion. Chemical weathering becomes more effective as the surface area of the rock increases, thus is favoured by physical disintegration. This stems in latitudinal and altitudinal climate gradients in regolith formation.\n\nSaprolite is a particular example of a residual soil formed from the transformation of granite, metamorphic and other types of bedrock into clay minerals. Often called [weathered granite], saprolite is the result of weathering processes that include: hydrolysis, chelation from organic compounds, hydration (the solution of minerals in water with resulting cation and anion pairs) and physical processes that include freezing and thawing. The mineralogical and chemical composition of the primary bedrock material, its physical features, including grain size and degree of consolidation, and the rate and type of weathering transforms the parent material into a different mineral. The texture, pH and mineral constituents of saprolite are inherited from its parent material. This process is also called \"arenization\", resulting in the formation of sandy soils (granitic arenas), thanks to the much higher resistance of quartz compared to other mineral components of granite (micas, amphiboles, feldspars).\n\nThe principal climatic variables influencing soil formation are effective precipitation (i.e., precipitation minus evapotranspiration) and temperature, both of which affect the rates of chemical, physical, and biological processes. Temperature and moisture both influence the organic matter content of soil through their effects on the balance between primary production and decomposition: the colder or drier the climate the lesser atmospheric carbon is fixed as organic matter while the lesser organic matter is decomposed.\n\nClimate is the dominant factor in soil formation, and soils show the distinctive characteristics of the climate zones in which they form, with a feedback to climate through transfer of carbon stocked in soil horizons back to the atmosphere. If warm temperatures and abundant water are present in the profile at the same time, the processes of weathering, leaching, and plant growth will be maximized. According to the climatic determination of biomes, humid climates favor the growth of trees. In contrast, grasses are the dominant native vegetation in subhumid and semiarid regions, while shrubs and brush of various kinds dominate in arid areas.\n\nWater is essential for all the major chemical weathering reactions. To be effective in soil formation, water must penetrate the regolith. The seasonal rainfall distribution, evaporative losses, site topography, and soil permeability interact to determine how effectively precipitation can influence soil formation. The greater the depth of water penetration, the greater the depth of weathering of the soil and its development. Surplus water percolating through the soil profile transports soluble and suspended materials from the upper layers (eluviation) to the lower layers (illuviation), including clay particles and dissolved organic matter. It may also carry away soluble materials in the surface drainage waters. Thus, percolating water stimulates weathering reactions and helps differentiate soil horizons. Likewise, a deficiency of water is a major factor in determining the characteristics of soils of dry regions. Soluble salts are not leached from these soils, and in some cases they build up to levels that curtail plant and microbial growth. Soil profiles in arid and semi-arid regions are also apt to accumulate carbonates and certain types of expansive clays (calcrete or caliche horizons). In tropical soils, when the soil has been deprived of vegetation (e.g. by deforestation) and thereby is submitted to intense evaporation, the upward capillary movement of water, which has dissolved iron and aluminum salts, is responsible for the formation of a superficial hard pan of laterite or bauxite, respectively, which is improper for cutivation, a known case of irreversible soil degradation (lateritization, bauxitization).\n\nThe direct influences of climate include:\n\nClimate directly affects the rate of weathering and leaching. Wind moves sand and smaller particles (dust), especially in arid regions where there is little plant cover, depositing it close or far from the entrainment source. The type and amount of precipitation influence soil formation by affecting the movement of ions and particles through the soil, and aid in the development of different soil profiles. Soil profiles are more distinct in wet and cool climates, where organic materials may accumulate, than in wet and warm climates, where organic materials are rapidly consumed. The effectiveness of water in weathering parent rock material depends on seasonal and daily temperature fluctuations, which favour tensile stresses in rock minerals, and thus their mechanical disaggregation, a process called \"thermal fatigue\". By the same process freeze-thaw cycles are an effective mechanism which breaks up rocks and other consolidated materials.\n\nClimate also indirectly influences soil formation through the effects of vegetation cover and biological activity, which modify the rates of chemical reactions in the soil.\n\nThe topography, or relief, is characterized by the inclination (slope), elevation, and orientation of the terrain. Topography determines the rate of precipitation or runoff and rate of formation or erosion of the surface soil profile. The topographical setting may either hasten or retard the work of climatic forces.\n\nSteep slopes encourage rapid soil loss by erosion and allow less rainfall to enter the soil before running off and hence, little mineral deposition in lower profiles. In semiarid regions, the lower effective rainfall on steeper slopes also results in less complete vegetative cover, so there is less plant contribution to soil formation. For all of these reasons, steep slopes prevent the formation of soil from getting very far ahead of soil destruction. Therefore, soils on steep terrain tend to have rather shallow, poorly developed profiles in comparison to soils on nearby, more level sites.\n\nIn swales and depressions where runoff water tends to concentrate, the regolith is usually more deeply weathered and soil profile development is more advanced. However, in the lowest landscape positions, water may saturate the regolith to such a degree that drainage and aeration are restricted. Here, the weathering of some minerals and the decomposition of organic matter are retarded, while the loss of iron and manganese is accelerated. In such low-lying topography, special profile features characteristic of wetland soils may develop. Depressions allow the accumulation of water, minerals and organic matter and in the extreme, the resulting soils will be saline marshes or peat bogs. Intermediate topography affords the best conditions for the formation of an agriculturally productive soil.\n\nSoil is the most abundant ecosystem on Earth, but the vast majority of organisms in soil are microbes, a great many of which have not been described. There may be a population limit of around one billion cells per gram of soil, but estimates of the number of species vary widely from 50,000 per gram to over a million per gram of soil. The total number of organisms and species can vary widely according to soil type, location, and depth.\n\nPlants, animals, fungi, bacteria and humans affect soil formation (see soil biomantle and stonelayer). Soil animals, including soil macrofauna and soil mesofauna, mix soils as they form burrows and pores, allowing moisture and gases to move about, a process called bioturbation. In the same way, plant roots penetrate soil horizons and open channels upon decomposition. Plants with deep taproots can penetrate many metres through the different soil layers to bring up nutrients from deeper in the profile. Plants have fine roots that excrete organic compounds (sugars, organic acids, mucigel), slough off cells (in particular at their tip) and are easily decomposed, adding organic matter to soil, a process called \"rhizodeposition\". Micro-organisms, including fungi and bacteria, effect chemical exchanges between roots and soil and act as a reserve of nutrients in a soil biological \"hotspot\" called rhizosphere. The growth of roots through the soil stimulates microbial populations, stimulating in turn the activity of their predators (notably amoeba), thereby increasing the mineralization rate, and in last turn root growth, a positive feedback called the soil microbial loop. Out of root influence, in the bulk soil, most bacteria are in a quiescent stage, forming microaggregates, i.e. mucilaginous colonies to which clay particles are glued, offering them a protection against desiccation and predation by soil microfauna (bacteriophagous protozoa and nematodes). Microaggregates (20-250 µm) are ingested by soil mesofauna and macrofauna, and bacterial bodies are partly or totally digested in their guts.\n\nHumans impact soil formation by removing vegetation cover with erosion, waterlogging, lateritization or podzolization (according to climate and topography) as the result. Their tillage also mixes the different soil layers, restarting the soil formation process as less weathered material is mixed with the more developed upper layers, resulting in net increased rate of mineral weathering.\n\nEarthworms, ants, termites, moles, gophers, as well as some millipedes and tenebrionid beetles mix the soil as they burrow, significantly affecting soil formation. Earthworms ingest soil particles and organic residues, enhancing the availability of plant nutrients in the material that passes through their bodies. They aerate and stir the soil and create stable soil aggregates, after having disrupted links between soil particles during the intestinal transit of ingested soil, thereby assuring ready infiltration of water. In addition, as ants and termites build mounds, they transport soil materials from one horizon to another. Other important functions are fulfilled by earthworms in the soil ecosystem, in particular their intense mucus production, both within the intestine and as a lining in their galleries, exert a priming effect on soil microflora, giving them the status of ecosystem engineers, which they share with ants and termites.\n\nIn general, the mixing of the soil by the activities of animals, sometimes called pedoturbation, tends to undo or counteract the tendency of other soil-forming processes that create distinct horizons. Termites and ants may also retard soil profile development by denuding large areas of soil around their nests, leading to increased loss of soil by erosion. Large animals such as gophers, moles, and prairie dogs bore into the lower soil horizons, bringing materials to the surface. Their tunnels are often open to the surface, encouraging the movement of water and air into the subsurface layers. In localized areas, they enhance mixing of the lower and upper horizons by creating, and later refilling, underground tunnels. Old animal burrows in the lower horizons often become filled with soil material from the overlying A horizon, creating profile features known as crotovinas.\n\nVegetation impacts soils in numerous ways. It can prevent erosion caused by excessive rain that might result from surface runoff. Plants shade soils, keeping them cooler and slow evaporation of soil moisture, or conversely, by way of transpiration, plants can cause soils to lose moisture, resulting in complex and highly variable relationships between leaf area index (measuring light interception) and moisture loss: more generally plants prevent soil from desiccation during driest months while they dry it during moister months, thereby acting as a buffer against strong moisture variation. Plants can form new chemicals that can break down minerals, both directly and indirectly through mycorrhizal fungi and rhizosphere bacteria, and improve the soil structure. The type and amount of vegetation depends on climate, topography, soil characteristics and biological factors, mediated or not by human activities. Soil factors such as density, depth, chemistry, pH, temperature and moisture greatly affect the type of plants that can grow in a given location. Dead plants and fallen leaves and stems begin their decomposition on the surface. There, organisms feed on them and mix the organic material with the upper soil layers; these added organic compounds become part of the soil formation process.\n\nHuman activities widely influence soil formation. For example, it is believed that Native Americans regularly set fires to maintain several large areas of prairie grasslands in Indiana and Michigan, although climate and mammalian grazers (e.g. bisons) are also advocated to explain the maintenance of the Great Plains of North America. In more recent times, human destruction of natural vegetation and subsequent tillage of the soil for crop production has abruptly modified soil formation. Likewise, irrigating soil in an arid region drastically influences soil-forming factors, as does adding fertilizer and lime to soils of low fertility.\n\nTime is a factor in the interactions of all the above. While a mixture of sand, silt and clay constitute the texture of a soil and the aggregation of those components produces peds, the development of a distinct B horizon marks the development of a soil or pedogenesis. With time, soils will evolve features that depend on the interplay of the prior listed soil-forming factors. It takes decades to several thousand years for a soil to develop a profile, although the notion of soil development has been criticized, soil being in a constant state-of-change under the influence of fluctuating soil-forming factors. That time period depends strongly on climate, parent material, relief, and biotic activity. For example, recently deposited material from a flood exhibits no soil development as there has not been enough time for the material to form a structure that further defines soil. The original soil surface is buried, and the formation process must begin anew for this deposit. Over time the soil will develop a profile that depends on the intensities of biota and climate. While a soil can achieve relative stability of its properties for extended periods, the soil life cycle ultimately ends in soil conditions that leave it vulnerable to erosion. Despite the inevitability of soil retrogression and degradation, most soil cycles are long.\n\nSoil-forming factors continue to affect soils during their existence, even on \"stable\" landscapes that are long-enduring, some for millions of years. Materials are deposited on top or are blown or washed from the surface. With additions, removals and alterations, soils are always subject to new conditions. Whether these are slow or rapid changes depends on climate, topography and biological activity.\n\nThe physical properties of soils, in order of decreasing importance for ecosystem services such as crop production, are texture, structure, bulk density, porosity, consistency, temperature, colour and resistivity. Soil texture is determined by the relative proportion of the three kinds of soil mineral particles, called soil separates: sand, silt, and clay. At the next larger scale, soil structures called peds or more commonly \"soil aggregates\" are created from the soil separates when iron oxides, carbonates, clay, silica and humus, coat particles and cause them to adhere into larger, relatively stable secondary structures. Soil bulk density, when determined at standardized moisture conditions, is an estimate of soil compaction. Soil porosity consists of the void part of the soil volume and is occupied by gases or water. Soil consistency is the ability of soil materials to stick together. Soil temperature and colour are self-defining. Resistivity refers to the resistance to conduction of electric currents and affects the rate of corrosion of metal and concrete structures which are buried in soil. These properties vary through the depth of a soil profile, i.e. through soil horizons. Most of these properties determine the aeration of the soil and the ability of water to infiltrate and to be held within the soil.\n\nThe mineral components of soil are sand, silt and clay, and their relative proportions determine a soil's texture. Properties that are influenced by soil texture include porosity, permeability, infiltration, shrink-swell rate, water-holding capacity, and susceptibility to erosion. In the illustrated USDA textural classification triangle, the only soil in which neither sand, silt nor clay predominates is called loam. While even pure sand, silt or clay may be considered a soil, from the perspective of conventional agriculture a loam soil with a small amount of organic material is considered \"ideal\", inasmuch as fertilizers or manure are currently used to mitigate nutrient losses due to crop yields in the long term. The mineral constituents of a loam soil might be 40% sand, 40% silt and the balance 20% clay by weight. Soil texture affects soil behaviour, in particular, its retention capacity for nutrients (e.g., cation exchange capacity) and water.\n\nSand and silt are the products of physical and chemical weathering of the parent rock; clay, on the other hand, is most often the product of the precipitation of the dissolved parent rock as a secondary mineral, except when derived from the weathering of mica. It is the surface area to volume ratio (specific surface area) of soil particles and the unbalanced ionic electric charges within those that determine their role in the fertility of soil, as measured by its cation exchange capacity. Sand is least active, having the least specific surface area, followed by silt; clay is the most active. Sand's greatest benefit to soil is that it resists compaction and increases soil porosity, although this property stands only for pure sand, not for sand mixed with smaller minerals which fill the voids among sand grains. Silt is mineralogically like sand but with its higher specific surface area it is more chemically and physically active than sand. But it is the clay content of soil, with its very high specific surface area and generally large number of negative charges, that gives a soil its high retention capacity for water and nutrients. Clay soils also resist wind and water erosion better than silty and sandy soils, as the particles bond tightly to each other,\nand that with a strong mitigation effect of organic matter.\n\nSand is the most stable of the mineral components of soil; it consists of rock fragments, primarily quartz particles, ranging in size from in diameter. Silt ranges in size from . Clay cannot be resolved by optical microscopes as its particles are or less in diameter and a thickness of only 10 angstroms (10 m). In medium-textured soils, clay is often washed downward through the soil profile (a process called eluviation) and accumulates in the subsoil (a process called illuviation). There is no clear relationship between the size of soil mineral components and their mineralogical nature: sand and silt particles can be calcareous as well as siliceous, while textural clay () can be made of very fine quartz particles as well as of multi-layered secondary minerals. Soil mineral components belonging to a given textural class may thus share properties linked to their specific surface area (e.g. moisture retention) but not those linked to their chemical composition (e.g. cation exchange capacity).\n\nSoil components larger than are classed as rock and gravel and are removed before determining the percentages of the remaining components and the textural class of the soil, but are included in the name. For example, a sandy loam soil with 20% gravel would be called gravelly sandy loam.\n\nWhen the organic component of a soil is substantial, the soil is called organic soil rather than mineral soil. A soil is called organic if:\n\n\nThe clumping of the soil textural components of sand, silt and clay causes aggregates to form and the further association of those aggregates into larger units creates soil structures called peds (a contraction of the word pedolith). The adhesion of the soil textural components by organic substances, iron oxides, carbonates, clays, and silica, the breakage of those aggregates from expansion-contraction caused by freezing-thawing and wetting-drying cycles, and the build-up of aggregates by soil animals, microbial colonies and root tips shape soil into distinct geometric forms. The peds evolve into units which have various shapes, sizes and degrees of development. A soil clod, however, is not a ped but rather a mass of soil that results from mechanical disturbance of the soil such as cultivation. Soil structure affects aeration, water movement, conduction of heat, plant root growth and resistance to erosion. Water, in turn, has a strong effect on soil structure, directly via the dissolution and precipitation of minerals, the mechanical destruction of aggregates (slaking) and indirectly by promoting plant, animal and microbial growth.\n\nSoil structure often gives clues to its texture, organic matter content, biological activity, past soil evolution, human use, and the chemical and mineralogical conditions under which the soil formed. While texture is defined by the mineral component of a soil and is an innate property of the soil that does not change with agricultural activities, soil structure can be improved or destroyed by the choice and timing of farming practices.\n\nSoil structural classes:\n\n\nAt the largest scale, the forces that shape a soil's structure result from swelling and shrinkage that initially tend to act horizontally, causing vertically oriented prismatic peds. This mechanical process is mainly exemplified in the development of vertisols. Clayey soil, due to its differential drying rate with respect to the surface, will induce horizontal cracks, reducing columns to blocky peds. Roots, rodents, worms, and freezing-thawing cycles further break the peds into smaller peds of a more or less spherical shape.\n\nAt a smaller scale, plant roots extend into voids (macropores) and remove water causing macroporosity to increase and microporosity to decrease, thereby decreasing aggregate size. At the same time, root hairs and fungal hyphae create microscopic tunnels that break up peds.\n\nAt an even smaller scale, soil aggregation continues as bacteria and fungi exude sticky polysaccharides which bind soil into smaller peds. The addition of the raw organic matter that bacteria and fungi feed upon encourages the formation of this desirable soil structure.\n\nAt the lowest scale, the soil chemistry affects the aggregation or dispersal of soil particles. The clay particles contain polyvalent cations which give the faces of clay layers localized negative charges. At the same time, the edges of the clay plates have a slight positive charge, thereby allowing the edges to adhere to the negative charges on the faces of other clay particles or to flocculate (form clumps). On the other hand, when monovalent ions, such as sodium, invade and displace the polyvalent cations, they weaken the positive charges on the edges, while the negative surface charges are relatively strengthened. This leaves negative charge on the clay faces that repel other clay, causing the particles to push apart, and by doing so deflocculate clay suspensions. As a result, the clay disperses and settles into voids between peds, causing those to close. In this way the open structure of the soil is destroyed and the soil is made impenetrable to air and water. Such sodic soil (also called haline soil) tends to form columnar peds near the surface.\n\nSoil particle density is typically 2.60 to 2.75 grams per cm and is usually unchanging for a given soil. Soil particle density is lower for soils with high organic matter content, and is higher for soils with high iron-oxides content. Soil bulk density is equal to the dry mass of the soil divided by the volume of the soil; i.e., it includes air space and organic materials of the soil volume. Thereby soil bulk density is always less than soil particle density and is a good indicator of soil compaction. The soil bulk density of cultivated loam is about 1.1 to 1.4 g/cm (for comparison water is 1.0 g/cm). Contrary to particle density, soil bulk density is highly variable for a given soil, with a strong causal relationship with soil biological activity and management strategies. However, it has been shown that, depending on species and the size of their aggregates (faeces), earthworms may either increase or decrease soil bulk density. A lower bulk density by itself does not indicate suitability for plant growth due to the confounding influence of soil texture and structure. A high bulk density is indicative of either soil compaction or a mixture of soil textural classes in which small particles fill the voids among coarser particles. Hence the positive correlation between the fractal dimension of soil, considered as a porous medium, and its bulk density, that explains the poor hydraulic conductivity of silty clay loam in the absence of a faunal structure.\n\nPore space is that part of the bulk volume of soil that is not occupied by either mineral or organic matter but is open space occupied by either gases or water. In a productive, medium-textured soil the total pore space is typically about 50% of the soil volume. Pore size varies considerably; the smallest pores (cryptopores; <0.1 µm) hold water too tightly for use by plant roots; plant-available water is held in ultramicropores, micropores and mesopores (0.1–75 µm); and macropores (>75 µm) are generally air-filled when the soil is at field capacity.\n\nSoil texture determines total volume of the smallest pores; clay soils have smaller pores, but more total pore space than sands, despite of a much lower permeability. Soil structure has a strong influence on the larger pores that affect soil aeration, water infiltration and drainage. Tillage has the short-term benefit of temporarily increasing the number of pores of largest size, but these can be rapidly degraded by the destruction of soil aggregation.\n\nThe pore size distribution affects the ability of plants and other organisms to access water and oxygen; large, continuous pores allow rapid transmission of air, water and dissolved nutrients through soil, and small pores store water between rainfall or irrigation events. Pore size variation also compartmentalizes the soil pore space such that many microbial and faunal organisms are not in direct competition with one another, which may explain not only the large number of species present, but the fact that functionally redundant organisms (organisms with the same ecological niche) can co-exist within the same soil.\n\nConsistency is the ability of soil to stick to itself or to other objects (cohesion and adhesion, respectively) and its ability to resist deformation and rupture. It is of approximate use in predicting cultivation problems and the engineering of foundations. Consistency is measured at three moisture conditions: air-dry, moist, and wet. In those conditions the consistency quality depends upon the clay content. In the wet state, the two qualities of stickiness and plasticity are assessed. A soil's resistance to fragmentation and crumbling is assessed in the dry state by rubbing the sample. Its resistance to shearing forces is assessed in the moist state by thumb and finger pressure. Additionally, the cemented consistency depends on cementation by substances other than clay, such as calcium carbonate, silica, oxides and salts; moisture content has little effect on its assessment. The measures of consistency border on subjective compared to other measures such as pH, since they employ the apparent feel of the soil in those states.\n\nThe terms used to describe the soil consistency in three moisture states and a last not affected by the amount of moisture are as follows:\n\n\nSoil consistency is useful in estimating the ability of soil to support buildings and roads. More precise measures of soil strength are often made prior to construction.\n\nSoil temperature depends on the ratio of the energy absorbed to that lost. Soil has a temperature range between -20 to 60 °C, with a mean annual temperature from -10 to 26 °C according to biomes. Soil temperature regulates seed germination, breaking of seed dormancy, plant and root growth and the availability of nutrients. Soil temperature has important seasonal, monthly and daily variations, fluctuations in soil temperature being much lower with increasing soil depth. Heavy mulching (a type of soil cover) can slow the warming of soil in summer, and, at the same time, reduce fluctuations in surface temperature.\n\nMost often, agricultural activities must adapt to soil temperatures by:\n\n\nSoil temperatures can be raised by drying soils or the use of clear plastic mulches. Organic mulches slow the warming of the soil.\n\nThere are various factors that affect soil temperature, such as water content, soil color, and relief (slope, orientation, and elevation), and soil cover (shading and insulation), in addition to air temperature. The color of the ground cover and its insulating properties have a strong influence on soil temperature. Whiter soil tends to have a higher albedo than blacker soil cover, which encourages whiter soils to have lower soil temperatures. The specific heat of soil is the energy required to raise the temperature of soil by 1 °C. The specific heat of soil increases as water content increases, since the heat capacity of water is greater than that of dry soil. The specific heat of pure water is ~ 1 calorie per gram, the specific heat of dry soil is ~ 0.2 calories per gram, hence, the specific heat of wet soil is ~ 0.2 to 1 calories per gram (0.8 to 4.2 kJ per kilogram). Also, a tremendous energy (~540 cal/g or 2260 kJ/kg) is required to evaporate water (known as the heat of vaporization). As such, wet soil usually warms more slowly than dry soil – wet surface soil is typically 3 to 6 °C colder than dry surface soil.\n\nSoil heat flux refers to the rate at which heat energy moves through the soil in response to a temperature difference between two points in the soil. The heat flux density is the amount of energy that flows through soil per unit area per unit time and has both magnitude and direction. For the simple case of conduction into or out of the soil in the vertical direction, which is most often applicable the heat flux density is:\n\nIn SI units\n\nHeat flux is in the direction opposite the temperature gradient, hence the minus sign. That is to say, if the temperature of the surface is higher than at depth x the negative sign will result in a positive value for the heat flux q, and which is interpreted as the heat being conducted into the soil.\n\nSoil temperature is important for the survival and early growth of seedlings. Soil temperatures affect the anatomical and morphological character of root systems. All physical, chemical, and biological processes in soil and roots are affected in particular because of the increased viscosities of water and protoplasm at low temperatures. In general, climates that do not preclude survival and growth of white spruce above ground are sufficiently benign to provide soil temperatures able to maintain white spruce root systems. In some northwestern parts of the range, white spruce occurs on permafrost sites and although young unlignified roots of conifers may have little resistance to freezing, the root system of containerized white spruce was not affected by exposure to a temperature of less than 30 °C.\n\nOptimum temperatures for tree root growth range between 10 °C and 25 °C in general and for spruce in particular. In 2-week-old white spruce seedlings that were then grown for 6 weeks in soil at temperatures of 15 °C, 19 °C, 23 °C, 27 °C, and 31 °C; shoot height, shoot dry weight, stem diameter, root penetration, root volume, and root dry weight all reached maxima at 19 °C.\n\nHowever, whereas strong positive relationships between soil temperature (5 °C to 25 °C) and growth have been found in trembling aspen and balsam poplar, white and other spruce species have shown little or no changes in growth with increasing soil temperature. Such insensitivity to soil low temperature may be common among a number of western and boreal conifers.\n\nSoil temperatures are increasing worldwide under the influence of present-day global climate warming, with opposing views about expected effects on carbon capture and storage and feedback loops to climate change Most threats are about permafrost thawing and attended effects on carbon destocking and ecosystem collapse.\n\nSoil colour is often the first impression one has when viewing soil. Striking colours and contrasting patterns are especially noticeable. The Red River of the South carries sediment eroded from extensive reddish soils like Port Silt Loam in Oklahoma. The Yellow River in China carries yellow sediment from eroding loess soils. Mollisols in the Great Plains of North America are darkened and enriched by organic matter. Podsols in boreal forests have highly contrasting layers due to acidity and leaching.\n\nIn general, color is determined by the organic matter content, drainage conditions, and degree of oxidation. Soil color, while easily discerned, has little use in predicting soil characteristics. It is of use in distinguishing boundaries of horizons within a soil profile, determining the origin of a soil's parent material, as an indication of wetness and waterlogged conditions, and as a qualitative means of measuring organic, iron oxide and clay contents of soils. Color is recorded in the Munsell color system as for instance 10YR3/4 \"Dusky Red\", with 10YR as \"hue\", 3 as \"value\" and 4 as \"chroma\". Munsell color dimensions (hue, value and chroma) can be averaged among samples and treated as quantitative parameters, displaying significant correlations with various soil and vegetation properties.\n\nSoil color is primarily influenced by soil mineralogy. Many soil colours are due to various iron minerals. The development and distribution of colour in a soil profile result from chemical and biological weathering, especially redox reactions. As the primary minerals in soil parent material weather, the elements combine into new and colourful compounds. Iron forms secondary minerals of a yellow or red colour, organic matter decomposes into black and brown humic compounds, and manganese and sulfur can form black mineral deposits. These pigments can produce various colour patterns within a soil. Aerobic conditions produce uniform or gradual colour changes, while reducing environments (anaerobic) result in rapid colour flow with complex, mottled patterns and points of colour concentration.\n\nSoil resistivity is a measure of a soil's ability to retard the conduction of an electric current. The electrical resistivity of soil can affect the rate of galvanic corrosion of metallic structures in contact with the soil. Higher moisture content or increased electrolyte concentration can lower resistivity and increase conductivity, thereby increasing the rate of corrosion. Soil resistivity values typically range from about 1 to 100000 Ω·m, extreme values being for saline soils and dry soils overlaying cristalline rocks, respectively.\n\nWater that enters a field is removed from a field by runoff, drainage, evaporation or transpiration. Runoff is the water that flows on the surface to the edge of the field; drainage is the water that flows through the soil downward or toward the edge of the field underground; evaporative water loss from a field is that part of the water that evaporates into the atmosphere directly from the field's surface; transpiration is the loss of water from the field by its evaporation from the plant itself.\n\nWater affects soil formation, structure, stability and erosion but is of primary concern with respect to plant growth. Water is essential to plants for four reasons:\n\n\nIn addition, water alters the soil profile by dissolving and re-depositing minerals, often at lower levels, and possibly leaving the soil sterile in the case of extreme rainfall and drainage. In a loam soil, solids constitute half the volume, gas one-quarter of the volume, and water one-quarter of the volume of which only half will be available to most plants, with a strong variation according to matric potential.\n\nA flooded field will drain the gravitational water under the influence of gravity until water's adhesive and cohesive forces resist further drainage at which point it is said to have reached field capacity. At that point, plants must apply suction to draw water from a soil. The water that plants may draw from the soil is called the available water. Once the available water is used up the remaining moisture is called unavailable water as the plant cannot produce sufficient suction to draw that water in. A plant must produce suction that increases from zero for a flooded field to 1/3 bar at field dry condition (one bar is a little less than one atmosphere pressure). At 15 bar suction, wilting point, seeds will not germinate, plants begin to wilt and then die. Water moves in soil under the influence of gravity, osmosis and capillarity. When water enters the soil, it displaces air from interconnected macropores by buoyancy, and breaks aggregates into which air is entrapped, a process called slaking.\n\nThe rate at which a soil can absorb water depends on the soil and its other conditions. As a plant grows, its roots remove water from the largest pores (macropores) first. Soon the larger pores hold only air, and the remaining water is found only in the intermediate- and smallest-sized pores (micropores). The water in the smallest pores is so strongly held to particle surfaces that plant roots cannot pull it away. Consequently, not all soil water is available to plants, with a strong dependence on texture. When saturated, the soil may lose nutrients as the water drains. Water moves in a draining field under the influence of pressure where the soil is locally saturated and by capillarity pull to drier parts of the soil. Most plant water needs are supplied from the suction caused by evaporation from plant leaves (transpiration) and a lower fraction is supplied by suction created by osmotic pressure differences between the plant interior and the soil solution. Plant roots must seek out water and grow preferentially in moister soil microsites, but some parts of the root system are also able to remoisten dry parts of the soil. Insufficient water will damage the yield of a crop. Most of the available water is used in transpiration to pull nutrients into the plant.\n\nWater is retained in a soil when the adhesive force of attraction that water's hydrogen atoms have for the oxygen of soil particles is stronger than the cohesive forces that water's hydrogen feels for other water oxygen atoms. When a field is flooded, the soil pore space is completely filled by water. The field will drain under the force of gravity until it reaches what is called field capacity, at which point the smallest pores are filled with water and the largest with water and gases. The total amount of water held when field capacity is reached is a function of the specific surface area of the soil particles. As a result, high clay and high organic soils have higher field capacities. The total force required to pull or push water out of soil is termed suction and usually expressed in units of bars (10 pascal) which is just a little less than one-atmosphere pressure. Alternatively, the terms \"soil moisture tension\" or water potential may be used.\n\nThe forces with which water is held in soils determine its availability to plants. Forces of adhesion hold water strongly to mineral and humus surfaces and less strongly to itself by cohesive forces. A plant's root may penetrate a very small volume of water that is adhering to soil and be initially able to draw in water that is only lightly held by the cohesive forces. But as the droplet is drawn down, the forces of adhesion of the water for the soil particles produce increasingly higher suction, finally up to 15 bar. At 15 bar suction, the soil water amount is called wilting point. At that suction the plant cannot sustain its water needs as water is still being lost from the plant by transpiration, the plant's turgidity is lost, and it wilts, although stomatal closure may decrease transpiration and thus may retard wilting below the wilting point, in particular under adaptation or acclimatization to drought. The next level, called air-dry, occurs at 1000 bar suction. Finally the oven dry condition is reached at 10,000 bar suction. All water below wilting percentage is called unavailable water.\n\nWhen the soil moisture content is optimal for plant growth, the water in the large and intermediate size pores can move about in the soil and be easily used by plants. The amount of water remaining in a soil drained to field capacity and the amount that is available are functions of the soil type. Sandy soil will retain very little water, while clay will hold the maximum amount. The time required to drain a field from flooded condition for a clay loam that begins at 43% water by weight to a field capacity of 22% is six days, whereas a sand loam that is flooded to its maximum of 22% water will take two days to reach field capacity of 11% water. The available water for the clay loam might be 11% whereas for the sand loam it might be only 8% by weight.\n\nThe above are average values for the soil textures as the percentages of sand, silt and clay vary.\n\nWater moves through soil due to the force of gravity, osmosis and capillarity. At zero to one-third bar suction, water is pushed through soil from the point of its application under the force of gravity and the pressure gradient created by the pressure of the water; this is called saturated flow. At higher suction, water movement is pulled by capillarity from wetter toward drier soil. This is caused by water's adhesion to soil solids, and is called unsaturated flow.\n\nWater infiltration and movement in soil is controlled by six factors:\n\n\nWater infiltration rates range from per hour for high clay soils to per hour for sand and well stabilised and aggregated soil structures. Water flows through the ground unevenly, in the form of so-called \"gravity fingers\", because of the surface tension between water particles.\n\nTree roots, whether living or dead, create preferential channels for rainwater flow through soil, magnifying infiltration rates of water up to 27 times.\n\nFlooding temporarily increases soil permeability in river beds, helping to recharge aquifers.\n\nWater applied to a soil is pushed by pressure gradients from the point of its application where it is saturated locally, to less saturated areas, such as the vadose zone. Once soil is completely wetted, any more water will move downward, or percolate out of the range of plant roots, carrying with it clay, humus, nutrients, primarily cations, and various contaminants, including pesticides, pollutants, viruses and bacteria, potentially causing groundwater contamination. In order of decreasing solubility, the leached nutrients are:\n\nIn the United States percolation water due to rainfall ranges from zero inches just east of the Rocky Mountains to twenty or more inches in the Appalachian Mountains and the north coast of the Gulf of Mexico.\n\nSoil physics (Darcy-type model) predicts that at suctions less than one-third bar, water moves theoretically in all directions via unsaturated flow at a rate that is dependent on the square of the diameter of the water-filled pores, but there is still not an adequate physical theory linking all types of waterflow in soil. Preferential flow occurs along interconnected macropores, crevices, root and worm channels, which drain water under gravity. Water is also pulled by capillary action due to the adhesion force of water to the soil solids, producing a suction gradient from wet towards drier soil and from macropores to micropores. Water flow (also called hydraulic conductivity) is primarily from coarse-textured soil into fine-textured soil horizons and is slowest in fine-textured soils such as clay.\n\nOf equal importance to the storage and movement of water in soil is the means by which plants acquire it and their nutrients. Most soil water is taken up by plants as passive absorption caused by the pulling force of water evaporating (transpiring) from the long column of water (xylem sap flow) that leads from the plant's roots to its leaves, according to the cohesion-tension theory. The upward movement of water and solutes (hydraulic lift) is regulated in the roots by the endodermis and in the plant foliage by stomatal conductance, and can be interrupted in root and shoot xylem vessels by cavitation, also called \"xylem embolism\". In addition, the high concentration of salts within plant roots creates an osmotic pressure gradient that pushes soil water into the roots. Osmotic absorption becomes more important during times of low water transpiration caused by lower temperatures (for example at night) or high humidity, and the reverse occurs under high temperature or low humidity. It is these process that cause guttation and wilting, respectively.\n\nRoot extension is vital for plant survival. A study of a single winter rye plant grown for four months in one cubic foot of loam soil showed that the plant developed 13,800,000 roots, a total of 385 miles in length with 2,550 square feet in surface area; and 14 billion hair roots of 6,600 miles total length and 4,320 square feet total area; for a total surface area of 6,870 square feet (83 ft squared). The total surface area of the loam soil was estimated to be 560,000 square feet. In other words, the roots were in contact with only 1.2% of the soil. However, root extension should be viewed as a dynamic process, allowing new roots to explore a new volume of soil each day, increasing dramatically the total volume of soil explored over a given growth period, and thus the volume of water taken up by the root system over this period. Root architecture, i.e. the spatial configuration of the root system, plays a prominent role in the adaptation of plants to soil water and nutrient availabiity, and thus in plant productivity.\n\nRoots must seek out water as the unsaturated flow of water in soil can move only at a rate of up to 2.5 cm (one inch) per day; as a result they are constantly dying and growing as they seek out high concentrations of soil moisture. Insufficient soil moisture, to the point of causing wilting, will cause permanent damage and crop yields will suffer. When grain sorghum was exposed to soil suction as low as 13.0 bar during the seed head emergence through bloom and seed set stages of growth, its production was reduced by 34%.\n\nOnly a small fraction (0.1% to 1%) of the water used by a plant is held within the plant. The majority is ultimately lost via transpiration, while evaporation from the soil surface is also substantial, the transpiration:evaporation ratio varying according to vegetation type and climate, peaking in tropical rainforests and dipping in steppes and deserts. Transpiration plus evaporative soil moisture loss is called evapotranspiration. Evapotranspiration plus water held in the plant totals to consumptive use, which is nearly identical to evapotranspiration.\n\nThe total water used in an agricultural field includes surface runoff, drainage and consumptive use. The use of loose mulches will reduce evaporative losses for a period after a field is irrigated, but in the end the total evaporative loss (plant plus soil) will approach that of an uncovered soil, while more water is immediately available for plant growth. Water use efficiency is measured by the transpiration ratio, which is the ratio of the total water transpired by a plant to the dry weight of the harvested plant. Transpiration ratios for crops range from 300 to 700. For example, alfalfa may have a transpiration ratio of 500 and as a result 500 kilograms of water will produce one kilogram of dry alfalfa.\n\nThe atmosphere of soil, or soil gas, is radically different from the atmosphere above. The consumption of oxygen by microbes and plant roots, and their release of carbon dioxide, decrease oxygen and increase carbon dioxide concentration. Atmospheric CO concentration is 0.04%, but in the soil pore space it may range from 10 to 100 times that level, thus potentially contributing to the inhibition of root respiration. Calcareous soils regulate CO concentration thanks to carbonate buffering, contrary to acid soils in which all CO respired accumulates in the soil pore system. At extreme levels CO is toxic. This suggests a possible negative feedback control of soil CO concentration through its inhibitory effects on root and microbial respiration (also called 'soil respiration'). In addition, the soil voids are saturated with water vapour, at least until the point of maximal hygroscopicity, beyond which a vapour-pressure deficit occurs in the soil pore space. Adequate porosity is necessary, not just to allow the penetration of water, but also to allow gases to diffuse in and out. Movement of gases is by diffusion from high concentrations to lower, the diffusion coefficient decreasing with soil compaction. Oxygen from above atmosphere diffuses in the soil where it is consumed and levels of carbon dioxide in excess of above atmosphere diffuse out with other gases (including greenhouse gases) as well as water. Soil texture and structure strongly affect soil porosity and gas diffusion. It is the total pore space (porosity) of soil, not the pore size, and the degree of pore interconnection (or conversely pore sealing), together with water content, air turbulence and temperature, that determine the rate of diffusion of gases into and out of soil. Platy soil structure and soil compaction (low porosity) impede gas flow, and a deficiency of oxygen may encourage anaerobic bacteria to reduce (strip oxygen) from nitrate NO to the gases N, NO, and NO, which are then lost to the atmosphere, thereby depleting the soil of nitrogen. Aerated soil is also a net sink of methane CH but a net producer of methane (a strong heat-absorbing greenhouse gas) when soils are depleted of oxygen and subject to elevated temperatures.\n\nSoil atmosphere is also the seat of emissions of volatiles other than carbon and nitrogen oxides from various soil organisms, e.g. roots, bacteria, fungi, animals. These volatiles are used as chemical cues, making soil atmosphere the seat of interaction networks playing a decisive role in the stability, dynamics and evolution of soil ecosystems. Biogenic soil volatile organic compounds are exchanged with the aboveground atmosphere, in which they are just 1–2 orders of magnitude lower than those from aboveground vegetation.\n\nWe humans can get some idea of the soil atmosphere through the well-known 'after-the-rain' scent, when infiltering rainwater flushes out the whole soil atmosphere after a drought period, or when soil is excavated, a bulk property attributed in a reductionist manner to particular biochemical compounds such as petrichor or geosmin.\n\nSoil particles can be classified by their chemical composition (mineralogy) as well as their size. The particle size distribution of a soil, its texture, determines many of the properties of that soil, in particular hydraulic conductivity and water potential but the mineralogy of those particles can strongly modify those properties. The mineralogy of the finest soil particles, clay, is especially important.\n\nGravel, sand and silt are the larger soil particles, and their mineralogy is often inherited from the parent material of the soil, but may include products of weathering (such as concretions of calcium carbonate or iron oxide), or residues of plant and animal life (such as silica phytoliths). Quartz is the most common mineral in the sand or silt fraction as it is resistant to chemical weathering, except under hot climate; other common minerals are feldspars, micas and ferromagnesian minerals such as pyroxenes, amphiboles and olivines, which are dissolved or transformed in clay under the combined influence of physico-chemical and biological processes.\n\nDue to its high specific surface area and its unbalanced negative electric charges, clay is the most active mineral component of soil. It is a colloidal and most often a crystalline material. In soils, clay is a soil textural class and is defined in a physical sense as any mineral particle less than in effective diameter. Many soil minerals, such as gypsum, carbonates, or quartz, are small enough to be classified as clay based on their physical size, but chemically they do not afford the same utility as do mineralogically-defined clay minerals. Chemically, clay minerals are a range of phyllosilicate minerals with certain reactive properties.\n\nBefore the advent of X-ray diffraction clay was thought to be very small particles of quartz, feldspar, mica, hornblende or augite, but it is now known to be (with the exception of mica-based clays) a precipitate with a mineralogical composition that is dependent on but different from its parent materials and is classed as a secondary mineral. The type of clay that is formed is a function of the parent material and the composition of the minerals in solution. Clay minerals continue to be formed as long as the soil exists. Mica-based clays result from a modification of the primary mica mineral in such a way that it behaves and is classed as a clay. Most clays are crystalline, but some clays or some parts of clay minerals are amorphous. The clays of a soil are a mixture of the various types of clay, but one type predominates.\n\nTypically there are four main groups of clay minerals: kaolinite, montmorillonite-smectite, illite, and chlorite. Most clays are crystalline and most are made up of three or four planes of oxygen held together by planes of aluminium and silicon by way of ionic bonds that together form a single layer of clay. The spatial arrangement of the oxygen atoms determines clay's structure. Half of the weight of clay is oxygen, but on a volume basis oxygen is ninety percent. The layers of clay are sometimes held together through hydrogen bonds, sodium or potassium bridges and as a result will swell less in the presence of water. Clays such as montmorillonite have layers that are loosely attached and will swell greatly when water intervenes between the layers.\n\nIn a wider sense clays can be classified as:\n\n\nAlumino-silica clays or aluminosilicate clays are characterised by their regular crystalline or quasi-crystalline structure. Oxygen in ionic bonds with silicon forms a tetrahedral coordination (silicon at the center) which in turn forms sheets of silica. Two sheets of silica are bonded together by a plane of aluminium which forms an octahedral coordination, called alumina, with the oxygens of the silica sheet above and that below it. Hydroxyl ions (OH) sometimes substitute for oxygen. During the clay formation process, Al may substitute for Si in the silica layer, and as much as one fourth of the aluminium Al may be substituted by Zn, Mg or Fe in the alumina layer. The substitution of lower-valence cations for higher-valence cations (isomorphous substitution) gives clay a local negative charge on an oxygen atom that attracts and holds water and positively charged soil cations, some of which are of value for plant growth. Isomorphous substitution occurs during the clay's formation and does not change with time.\n\nThe carbonate and sulfate minerals are much more soluble and hence are found primarily in desert soils where leaching is less active.\n\nAmorphous clays are young, and commonly found in volcanic ash. They are mixtures of alumina and silica which have not formed the ordered crystal shape of alumino-silica clays which time would provide. The majority of their negative charges originates from hydroxyl ions, which can gain or lose a hydrogen ion (H) in response to soil pH, in such way was as to buffer the soil pH. They may have either a negative charge provided by the attached hydroxyl ion (OH), which can attract a cation, or lose the hydrogen of the hydroxyl to solution and display a positive charge which can attract anions. As a result, they may display either high CEC in an acid soil solution, or high anion exchange capacity in a basic soil solution.\n\nSesquioxide clays are a product of heavy rainfall that has leached most of the silica from alumino-silica clay, leaving the less soluble oxides iron hematite (FeO), iron hydroxide (Fe(OH)), aluminium hydroxide gibbsite (Al(OH)), hydrated manganese birnessite (MnO). It takes hundreds of thousands of years of leaching to create sesquioxide clays. \"Sesqui\" is Latin for \"one and one-half\": there are three parts oxygen to two parts iron or aluminium; hence the ratio is one and one-half (not true for all). They are hydrated and act as either amorphous or crystalline. They are not sticky and do not swell, and soils high in them behave much like sand and can rapidly pass water. They are able to hold large quantities of phosphates. Sesquioxides have low CEC but are able to hold anions as well as cations. Such soils range from yellow to red in colour. Such clays tend to hold phosphorus so tightly that it is unavailable for absorption by plants.\n\nHumus is the final state of decomposition of organic matter. While it may linger for a thousand years, on the larger scale of the age of the mineral soil components, it is temporary. It is composed of the very stable lignins (30%) and complex sugars (polyuronides, 30%), proteins (30%), waxes, and fats that are resistant to breakdown by microbes. Its chemical assay is 60% carbon, 5% nitrogen, some oxygen and the remainder hydrogen, sulfur, and phosphorus. On a dry weight basis, the CEC of humus is many times greater than that of clay.\n\nIn the extreme environment of high temperatures and the leaching caused by the heavy rain of tropical rain forests, the clay and organic colloids are largely destroyed. The heavy rains wash the alumino-silicate clays from the soil leaving only sesquioxide clays of low CEC. The high temperatures and humidity allow bacteria and fungi to virtually dissolve any organic matter on the rain-forest floor overnight and much of the nutrients are volatilized or leached from the soil and lost. However, carbon in the form of charcoal is far more stable than soil colloids and is capable of performing many of the functions of the soil colloids of sub-tropical soils. Soil containing substantial quantities of charcoal, of an anthropogenic origin, is called terra preta. Research into terra preta is still young but is promising. Fallow periods \"on the Amazonian Dark Earths can be as short as 6 months, whereas fallow periods on oxisols are usually 8 to 10 years long\"\n\nThe chemistry of a soil determines its ability to supply available plant nutrients and affects its physical properties and the health of its microbial population. In addition, a soil's chemistry also determines its corrosivity, stability, and ability to absorb pollutants and to filter water. It is the surface chemistry of mineral and organic colloids that determines soil's chemical properties. \"A colloid is a small, insoluble, nondiffusible particle larger than a molecule but small enough to remain suspended in a fluid medium without settling. Most soils contain organic colloidal particles called humus as well as the inorganic colloidal particles of clays.\" The very high specific surface area of colloids and their net charges, gives soil its ability to hold and release ions. Negatively charged sites on colloids attract and release cations in what is referred to as cation exchange. Cation-exchange capacity (CEC) is the amount of exchangeable cations per unit weight of dry soil and is expressed in terms of milliequivalents of positively charged ions per 100 grams of soil (or centimoles of positive charge per kilogram of soil; cmol/kg). Similarly, positively charged sites on colloids can attract and release anions in the soil giving the soil anion exchange capacity (AEC).\n\nThe cation exchange, that takes place between colloids and soil water, buffers (moderates) soil pH, alters soil structure, and purifies percolating water by adsorbing cations of all types, both useful and harmful.\n\nThe negative or positive charges on colloid particles make them able to hold cations or anions, respectively, to their surfaces. The charges result from four sources.\n\n\nCations held to the negatively charged colloids resist being washed downward by water and out of reach of plants' roots, thereby preserving the fertility of soils in areas of moderate rainfall and low temperatures.\n\nThere is a hierarchy in the process of cation exchange on colloids, as they differ in the strength of adsorption by the colloid and hence their ability to replace one another. If present in equal amounts in the soil water solution:\n\nAl replaces H replaces Ca replaces Mg replaces K same as NH replaces Na\n\nIf one cation is added in large amounts, it may replace the others by the sheer force of its numbers. This is called mass action. This is largely what occurs with the addition of fertiliser.\n\nAs the soil solution becomes more acidic (low pH, and an abundance of H), the other cations more weakly bound to colloids are pushed into solution as hydrogen ions occupy those sites. A low pH may cause hydrogen of hydroxyl groups to be pulled into solution, leaving charged sites on the colloid available to be occupied by other cations. This ionisation of hydroxyl groups on the surface of soil colloids creates what is described as pH-dependent charges. Unlike permanent charges developed by isomorphous substitution, pH-dependent charges are variable and increase with increasing pH. Freed cations can be made available to plants but are also prone to be leached from the soil, possibly making the soil less fertile. Plants are able to excrete H into the soil and by that means, change the pH of the soil near the root and push cations off the colloids, thus making those available to the plant.\n\nCation exchange capacity should be thought of as the soil's ability to remove cations from the soil water solution and sequester those to be exchanged later as the plant roots release hydrogen ions to the solution. CEC is the amount of exchangeable hydrogen cation (H) that will combine with 100 grams dry weight of soil and whose measure is one milliequivalents per 100 grams of soil (1 meq/100 g). Hydrogen ions have a single charge and one-thousandth of a gram of hydrogen ions per 100 grams dry soil gives a measure of one milliequivalent of hydrogen ion. Calcium, with an atomic weight 40 times that of hydrogen and with a valence of two, converts to (40/2) x 1 milliequivalent = 20 milliequivalents of hydrogen ion per 100 grams of dry soil or 20 meq/100 g. The modern measure of CEC is expressed as centimoles of positive charge per kilogram (cmol/kg) of oven-dry soil.\n\nMost of the soil's CEC occurs on clay and humus colloids, and the lack of those in hot, humid, wet climates, due to leaching and decomposition respectively, explains the relative sterility of tropical soils. Live plant roots also have some CEC.\n\nAnion exchange capacity should be thought of as the soil's ability to remove anions from the soil water solution and sequester those for later exchange as the plant roots release carbonate anions to the soil water solution. Those colloids which have low CEC tend to have some AEC. Amorphous and sesquioxide clays have the highest AEC, followed by the iron oxides. Levels of AEC are much lower than for CEC. Phosphates tend to be held at anion exchange sites.\n\nIron and aluminum hydroxide clays are able to exchange their hydroxide anions (OH) for other anions. The order reflecting the strength of anion adhesion is as follows:\n\nThe amount of exchangeable anions is of a magnitude of tenths to a few milliequivalents per 100 g dry soil. As pH rises, there are relatively more hydroxyls, which will displace anions from the colloids and force them into solution and out of storage; hence AEC decreases with increasing pH (alkalinity).\n\nSoil reactivity is expressed in terms of pH and is a measure of the acidity or alkalinity of the soil. More precisely, it is a measure of hydrogen ion concentration in an aqueous solution and ranges in values from 0 to 14 (acidic to basic) but practically speaking for soils, pH ranges from 3.5 to 9.5, as pH values beyond those extremes are toxic to life forms.\n\nAt 25 °C an aqueous solution that has a pH of 3.5 has 10 moles H (hydrogen ions) per litre of solution (and also 10 mole/litre OH). A pH of 7, defined as neutral, has 10 moles hydrogen ions per litre of solution and also 10 moles of OH per litre; since the two concentrations are equal, they are said to neutralise each other. A pH of 9.5 has 10 moles hydrogen ions per litre of solution (and also 10 mole per litre OH). A pH of 3.5 has one million times more hydrogen ions per litre than a solution with pH of 9.5 (9.5 - 3.5 = 6 or 10) and is more acidic.\n\nThe effect of pH on a soil is to remove from the soil or to make available certain ions. Soils with high acidity tend to have toxic amounts of aluminium and manganese. Plants which need calcium need moderate alkalinity, but most minerals are more soluble in acid soils. Soil organisms are hindered by high acidity, and most agricultural crops do best with mineral soils of pH 6.5 and organic soils of pH 5.5.\n\nIn high rainfall areas, soils tend to acidity as the basic cations are forced off the soil colloids by the mass action of hydrogen ions from the rain as those attach to the colloids. High rainfall rates can then wash the nutrients out, leaving the soil sterile. Once the colloids are saturated with H, the addition of any more hydrogen ions or aluminum hydroxyl cations drives the pH even lower (more acidic) as the soil has been left with no buffering capacity. In areas of extreme rainfall and high temperatures, the clay and humus may be washed out, further reducing the buffering capacity of the soil. In low rainfall areas, unleached calcium pushes pH to 8.5 and with the addition of exchangeable sodium, soils may reach pH 10. Beyond a pH of 9, plant growth is reduced. High pH results in low micro-nutrient mobility, but water-soluble chelates of those nutrients can correct the deficit. Sodium can be reduced by the addition of gypsum (calcium sulphate) as calcium adheres to clay more tightly than does sodium causing sodium to be pushed into the soil water solution where it can be washed out by an abundance of water.\n\nThere are acid-forming cations (hydrogen and aluminium) and there are base-forming cations. The fraction of the base-forming cations that occupy positions on the soil colloids is called the base saturation percentage. If a soil has a CEC of 20 meq and 5 meq are aluminium and hydrogen cations (acid-forming), the remainder of positions on the colloids (20-5 = 15 meq) are assumed occupied by base-forming cations, so that the percentage base saturation is 15/20 x 100% = 75% (the compliment 25% is assumed acid-forming cations). When the soil pH is 7 (neutral), base saturation is 100 percent and there are no hydrogen ions stored on the colloids. Base saturation is almost in direct proportion to pH (increases with increasing pH). It is of use in calculating the amount of lime needed to neutralise an acid soil. The amount of lime needed to neutralize a soil must take account of the amount of acid forming ions on the colloids not just those in the soil water solution. The addition of enough lime to neutralize the soil water solution will be insufficient to change the pH, as the acid forming cations stored on the soil colloids will tend to restore the original pH condition as they are pushed off those colloids by the calcium of the added lime.\n\nThe resistance of soil to change in pH, as a result of the addition of acid or basic material, is a measure of the buffering capacity of a soil and (for a particular soil type) increases as the CEC increases. Hence, pure sand has almost no buffering ability, while soils high in colloids have high buffering capacity. Buffering occurs by cation exchange and neutralisation.\n\nThe addition of a small amount highly basic aqueous ammonia to a soil will cause the ammonium to displace hydrogen ions from the colloids, and the end product is water and colloidally fixed ammonium, but little permanent change overall in soil pH.\n\nThe addition of a small amount of lime, Ca(OH), will displace hydrogen ions from the soil colloids, causing the fixation of calcium to colloids and the evolution of CO and water, with little permanent change in soil pH.\n\nThe above are examples of the buffering of soil pH. The general principal is that an increase in a particular cation in the soil water solution will cause that cation to be fixed to colloids (buffered) and a decrease in solution of that cation will cause it to be withdrawn from the colloid and moved into solution (buffered). The degree of buffering is often related to the CEC of the soil; the greater the CEC, the greater the buffering capacity of the soil.\n\nSixteen elements or nutrients are essential for plant growth and reproduction. They are carbon C, hydrogen H, oxygen O, nitrogen N, phosphorus P, potassium K, sulfur S, calcium Ca, magnesium Mg, iron Fe, boron B, manganese Mn, copper Cu, zinc Zn, molybdenum Mo, nickel Ni and chlorine Cl. Nutrients required for plants to complete their life cycle are considered essential nutrients. Nutrients that enhance the growth of plants but are not necessary to complete the plant's life cycle are considered non-essential. With the exception of carbon, hydrogen and oxygen, which are supplied by carbon dioxide and water, and nitrogen, provided through nitrogen fixation, the nutrients derive originally from the mineral component of the soil.\n\nPlant uptake of nutrients can only proceed when they are present in a plant-available form. In most situations, nutrients are absorbed in an ionic form from (or together with) soil water. Although minerals are the origin of most nutrients, and the bulk of most nutrient elements in the soil is held in crystalline form within primary and secondary minerals, they weather too slowly to support rapid plant growth. For example, The application of finely ground minerals, feldspar and apatite, to soil seldom provides the necessary amounts of potassium and phosphorus at a rate sufficient for good plant growth, as most of the nutrients remain bound in the crystals of those minerals.\n\nThe nutrients adsorbed onto the surfaces of clay colloids and soil organic matter provide a more accessible reservoir of many plant nutrients (e.g. K, Ca, Mg, P, Zn). As plants absorb the nutrients from the soil water, the soluble pool is replenished from the surface-bound pool. The decomposition of soil organic matter by microorganisms is another mechanism whereby the soluble pool of nutrients is replenished – this is important for the supply of plant-available N, S, P, and B from soil.\n\nGram for gram, the capacity of humus to hold nutrients and water is far greater than that of clay minerals. All in all, small amounts of humus may remarkably increase the soil's capacity to promote plant growth.\n\nNutrients in the soil are taken up by the plant through its roots. To be taken up by a plant, a nutrient element must be located near the root surface; however, the supply of nutrients in contact with the root is rapidly depleted. There are three basic mechanisms whereby nutrient ions dissolved in the soil solution are brought into contact with plant roots:\n\n\nAll three mechanisms operate simultaneously, but one mechanism or another may be most important for a particular nutrient. For example, in the case of calcium, which is generally plentiful in the soil solution, mass flow alone can usually bring sufficient amounts to the root surface. However, in the case of phosphorus, diffusion is needed to supplement mass flow. For the most part, nutrient ions must travel some distance in the soil solution to reach the root surface. This movement can take place by mass flow, as when dissolved nutrients are carried along with the soil water flowing toward a root that is actively drawing water from the soil. In this type of movement, the nutrient ions are somewhat analogous to leaves floating down a stream. In addition, nutrient ions continually move by diffusion from areas of greater concentration toward the nutrient-depleted areas of lower concentration around the root surface. That process is due to random motion of molecules. By this means, plants can continue to take up nutrients even at night, when water is only slowly absorbed into the roots as transpiration has almost stopped. Finally, root interception comes into play as roots continually grow into new, undepleted soil.\n\nIn the above table, phosphorus and potassium nutrients move more by diffusion than they do by mass flow in the soil water solution, as they are rapidly taken up by the roots creating a concentration of almost zero near the roots (the plants cannot transpire enough water to draw more of those nutrients near the roots). The very steep concentration gradient is of greater influence in the movement of those ions than is the movement of those by mass flow. The movement by mass flow requires the transpiration of water from the plant causing water and solution ions to also move toward the roots. Movement by root interception is slowest as the plants must extend their roots.\n\nPlants move ions out of their roots in an effort to move nutrients in from the soil. Hydrogen H is exchanged for other cations, and carbonate (HCO) and hydroxide (OH) anions are exchanged for nutrient anions. As plant roots remove nutrients from the soil water solution, they are replenished as other ions move off of clay and humus (by ion exchange or desorption), are added from the weathering of soil minerals, and are released by the decomposition of soil organic matter. Plants derive a large proportion of their anion nutrients from decomposing organic matter, which typically holds about 95 percent of the soil nitrogen, 5 to 60 percent of the soil phosphorus and about 80 percent of the soil sulfur. Where crops are produced, the replenishment of nutrients in the soil must usually be augmented by the addition of fertilizer or organic matter.\n\nBecause nutrient uptake is an active metabolic process, conditions that inhibit root metabolism may also inhibit nutrient uptake. Examples of such conditions include waterlogging or soil compaction resulting in poor soil aeration, excessively high or low soil temperatures, and above-ground conditions that result in low translocation of sugars to plant roots.\n\nPlants obtain their carbon from atmospheric carbon dioxide. About 45% of a plant's dry mass is carbon; plant residues typically have a carbon to nitrogen ratio (C/N) of between 13:1 and 100:1. As the soil organic material is digested by arthropods and micro-organisms, the C/N decreases as the carbonaceous material is metabolized and carbon dioxide (CO) is released as a byproduct which then finds its way out of the soil and into the atmosphere. The nitrogen is sequestered in the bodies of the living matter of those decomposing organisms and so it builds up in the soil. Normal CO concentration in the atmosphere is 0.03%, this can be the factor limiting plant growth. In a field of maize on a still day during high light conditions in the growing season, the CO concentration drops very low, but under such conditions the crop could use up to 20 times the normal concentration. The respiration of CO by soil micro-organisms decomposing soil organic matter contributes an important amount of CO to the photosynthesising plants. Within the soil, CO concentration is 10 to 100 times that of atmospheric levels but may rise to toxic levels if the soil porosity is low or if diffusion is impeded by flooding.\n\nNitrogen is the most critical element obtained by plants from the soil and nitrogen deficiency often limits plant growth. Plants can use the nitrogen as either the ammonium cation (NH) or the anion nitrate (NO). Usually, most of the nitrogen in soil is bound within organic compounds that make up the soil organic matter, and must be mineralized to the ammonium or nitrate form before it can be taken up by most plants. The total nitrogen content depends largely on the soil organic matter content, which in turn depends on the climate, vegetation, topography, age and soil management. Soil nitrogen typically decreases by 0.2 to 0.3% for every temperature increase by 10 °C. Usually, grassland soils contain more soil nitrogen than forest soils. Cultivation decreases soil nitrogen by exposing soil organic matter to decomposition by microorganisms, and soils under no-tillage maintain more soil nitrogen than tilled soils.\n\nSome micro-organisms are able to metabolise organic matter and release ammonium in a process called \"mineralisation\". Others take free ammonium and oxidise it to nitrate. Nitrogen-fixing bacteria are capable of metabolising N into the form of ammonia in a process called nitrogen fixation. Both ammonium and nitrate can be \"immobilized\" by their incorporation into the microbes' living cells, where it is temporarily sequestered in the form of amino acids and protein. Nitrate may also be lost from the soil when bacteria metabolise it to the gases N and NO. The loss of gaseous forms of nitrogen to the atmosphere due to microbial action is called \"denitrification\". Nitrogen may also be \"leached\" from the soil if it is in the form of nitrate or lost to the atmosphere as ammonia due to a chemical reaction of ammonium with alkaline soil by way of a process called \"volatilisation\". Ammonium may also be sequestered in clay by \"fixation\". A small amount of nitrogen is added to soil by rainfall.\n\nIn the process of mineralisation, microbes feed on organic matter, releasing ammonia (NH), ammonium (NH) and other nutrients. As long as the carbon to nitrogen ratio (C/N) of fresh residues in the soil is above 30:1, nitrogen will be in short supply and other bacteria will feed on the ammonium and incorporate its nitrogen into their cells in the immobilization process. In that form the nitrogen is said to be \"immobilised\". Later, when such bacteria die, they too are \"mineralised\" and some of the nitrogen is released as ammonium and nitrate. If the C/N is less than 15, ammonia is freed to the soil, where it may be used by bacteria which oxidise it to nitrate (nitrification). Bacteria may on average add nitrogen per acre, and in an unfertilised field, this is the most important source of usable nitrogen. In a soil with 5% organic matter perhaps 2 to 5% of that is released to the soil by such decomposition. It occurs fastest in warm, moist, well aerated soil. The mineralisation of 3% of the organic material of a soil that is 4% organic matter overall, would release of nitrogen as ammonium per acre.\n\nIn nitrogen fixation, rhizobium bacteria convert N to ammonia (NH). Rhizobia share a symbiotic relationship with host plants, since rhizobia supply the host with nitrogen and the host provides rhizobia with nutrients and a safe environment. It is estimated that such symbiotic bacteria in the root nodules of legumes add 45 to 250 pounds of nitrogen per acre per year, which may be sufficient for the crop. Other, free-living nitrogen-fixing bacteria and blue-green algae live independently in the soil and release nitrate when their dead bodies are converted by way of mineralisation.\n\nSome amount of usable nitrogen is fixed by lightning as nitric oxide (NO) and nitrogen dioxide (NO). Nitrogen dioxide is soluble in water to form nitric acid (HNO) solution of H and NO. Ammonia, NH, previously released from the soil or from combustion, may fall with precipitation as nitric acid at a rate of about five pounds nitrogen per acre per year.\n\nWhen bacteria feed on soluble forms of nitrogen (ammonium and nitrate), they temporarily sequester that nitrogen in their bodies in a process called \"immobilisation\". At a later time when those bacteria die, their nitrogen may be released as ammonium by the processes of mineralisation.\n\nProtein material is easily broken down, but the rate of its decomposition is slowed by its attachment to the crystalline structure of clay and when trapped between the clay layers. The layers are small enough that bacteria cannot enter. Some organisms can exude extracellular enzymes that can act on the sequestered proteins. However, those enzymes too may be trapped on the clay crystals.\n\nAmmonium fixation occurs when ammonium pushes potassium ions from between the layers of clay such as illite or montmorillonite. Only a small fraction of soil nitrogen is held this way.\n\nUsable nitrogen may be lost from soils when it is in the form of nitrate, as it is easily leached. Further losses of nitrogen occur by denitrification, the process whereby soil bacteria convert nitrate (NO) to nitrogen gas, N or NO. This occurs when poor soil aeration limits free oxygen, forcing bacteria to use the oxygen in nitrate for their respiratory process. Denitrification increases when oxidisable organic material is available and when soils are warm and slightly acidic. Denitrification may vary throughout a soil as the aeration varies from place to place. Denitrification may cause the loss of 10 to 20 percent of the available nitrates within a day and when conditions are favourable to that process, losses of up to 60 percent of nitrate applied as fertiliser may occur.\n\n\"Ammonium volatilisation\" occurs when ammonium reacts chemically with an alkaline soil, converting NH to NH. The application of ammonium fertiliser to such a field can result in volatilisation losses of as much as 30 percent.\n\nAfter nitrogen, phosphorus is probably the element most likely to be deficient in soils. The soil mineral apatite is the most common mineral source of phosphorus. While there is on average 1000 lb of phosphorus per acre in the soil, it is generally in the form of phosphates with low solubility. Total phosphorus is about 0.1 percent by weight of the soil, but only one percent of that is available. Of the part available, more than half comes from the mineralisation of organic matter. Agricultural fields may need to be fertilised to make up for the phosphorus that has been removed in the crop.\n\nWhen phosphorus does form solubilised ions of HPO, they rapidly form insoluble phosphates of calcium or hydrous oxides of iron and aluminum. Phosphorus is largely immobile in the soil and is not leached but actually builds up in the surface layer if not cropped. The application of soluble fertilisers to soils may result in zinc deficiencies as zinc phosphates form. Conversely, the application of zinc to soils may immobilise phosphorus again as zinc phosphate. Lack of phosphorus may interfere with the normal opening of the plant leaf stomata, resulting in plant temperatures 10 percent higher than normal. Phosphorus is most available when soil pH is 6.5 in mineral soils and 5.5 in organic soils.\n\nThe amount of potassium in a soil may be as much as 80,000 lb per acre-foot, of which only 150 lb is available for plant growth. Common mineral sources of potassium are the mica biotite and potassium feldspar, KAlSiO. When solubilised, half will be held as exchangeable cations on clay while the other half is in the soil water solution. Potassium fixation often occurs when soils dry and the potassium is bonded between layers of illite clay. Under certain conditions, dependent on the soil texture, intensity of drying, and initial amount of exchangeable potassium, the fixed percentage may be as much as 90 percent within ten minutes. Potassium may be leached from soils low in clay.\n\nCalcium is one percent by weight of soils and is generally available but may be low as it is soluble and can be leached. It is thus low in sandy and heavily leached soil or strongly acidic mineral soil. Calcium is supplied to the plant in the form of exchangeable ions and moderately soluble minerals. Calcium is more available on the soil colloids than is potassium because the common mineral calcite, CaCO, is more soluble than potassium-bearing minerals.\n\nMagnesium is one of the dominant exchangeable cations in most soils (as are calcium and potassium). Primary minerals that weather to release magnesium include hornblende, biotite and vermiculite. Soil magnesium concentrations are generally sufficient for optimal plant growth, but highly weathered and sandy soils may be magnesium deficient due to leaching by heavy precipitation.\n\nMost sulfur is made available to plants, like phosphorus, by its release from decomposing organic matter. Deficiencies may exist in some soils (especially sandy soils) and if cropped, sulfur needs to be added. The application of large quantities of nitrogen to fields that have marginal amounts of sulfur may cause sulfur deficiency in the rapidly growing plants by the plant's growth outpacing the supply of sulfur. A 15-ton crop of onions uses up to 19 lb of sulfur and 4 tons of alfalfa uses 15 lb per acre. Sulfur abundance varies with depth. In a sample of soils in Ohio, United States, the sulfur abundance varied with depths, 0-6 inches, 6-12 inches, 12-18 inches, 18-24 inches in the amounts: 1056, 830, 686, 528 lb per acre respectively.\n\nThe micronutrients essential in plant life, in their order of importance, include iron, manganese, zinc, copper, boron, chlorine and molybdenum. The term refers to plants' needs, not to their abundance in soil. They are required in very small amounts but are essential to plant health in that most are required parts of some enzyme system which speeds up plants' metabolisms. They are generally available in the mineral component of the soil, but the heavy application of phosphates can cause a deficiency in zinc and iron by the formation of insoluble zinc and iron phosphates. Iron deficiency may also result from excessive amounts of heavy metals or calcium minerals (lime) in the soil. Excess amounts of soluble boron, molybdenum and chloride are toxic.\n\nNutrients which enhance the health but whose deficiency does not stop the life cycle of plants include: cobalt, strontium, vanadium, silicon and nickel. As their importance are evaluated they may be added to the list of essential plant nutrients.\n\nSoil organic matter is made up of organic compounds and includes plant, animal and microbial material, both living and dead. A typical soil has a biomass composition of 70% microorganisms, 22% macrofauna, and 8% roots. The living component of an acre of soil may include 900 lb of earthworms, 2400 lb of fungi, 1500 lb of bacteria, 133 lb of protozoa and 890 lb of arthropods and algae.\n\nA small part of the organic matter consists of the living cells such as bacteria, molds, and actinomycetes that work to break down the dead organic matter. Were it not for the action of these micro-organisms, the entire carbon dioxide part of the atmosphere would be sequestered as organic matter in the soil.\n\nChemically, organic matter is classed as follows:\n\n\nMost living things in soils, including plants, insects, bacteria, and fungi, are dependent on organic matter for nutrients and/or energy. Soils have organic compounds in varying degrees of decomposition which rate is dependent on the temperature, soil moisture, and aeration. Bacteria and fungi feed on the raw organic matter, which are fed upon by amoebas, which in turn are fed upon by nematodes and arthropods. Organic matter holds soils open, allowing the infiltration of air and water, and may hold as much as twice its weight in water. Many soils, including desert and rocky-gravel soils, have little or no organic matter. Soils that are all organic matter, such as peat (histosols), are infertile. In its earliest stage of decomposition, the original organic material is often called raw organic matter. The final stage of decomposition is called humus.\n\nIn grassland, much of the organic matter added to the soil is from the deep, fibrous, grass root systems. By contrast, tree leaves falling on the forest floor are the principal source of soil organic matter in the forest. Another difference is the frequent occurrence in the grasslands of fires that destroy large amounts of aboveground material but stimulate even greater contributions from roots. Also, the much greater acidity under any forests inhibits the action of certain soil organisms that otherwise would mix much of the surface litter into the mineral soil. As a result, the soils under grasslands generally develop a thicker A horizon with a deeper distribution of organic matter than in comparable soils under forests, which characteristically store most of their organic matter in the forest floor (O horizon) and thin A horizon.\n\nHumus refers to organic matter that has been decomposed by soil flora and fauna to the point where it is resistant to further breakdown. Humus usually constitutes only five percent of the soil or less by volume, but it is an essential source of nutrients and adds important textural qualities crucial to soil health and plant growth. Humus also hold bits of undecomposed organic matter which feed arthropods and worms which further improve the soil. The end product, humus, is soluble in water and forms a weak acid that can attack silicate minerals. Humus is a colloid with a high cation and anion exchange capacity that on a dry weight basis is many times greater than that of clay colloids. It also acts as a buffer, like clay, against changes in pH and soil moisture.\n\nHumic acids and fulvic acids, which begin as raw organic matter, are important constituents of humus. After the death of plants and animals, microbes begin to feed on the residues, resulting finally in the formation of humus. With decomposition, there is a reduction of water-soluble constituents, cellulose and hemicellulose, and nutrients such as nitrogen, phosphorus, and sulfur. As the residues break down, only stable molecules made of aromatic carbon rings, oxygen and hydrogen remain in the form of humin, lignin and lignin complexes collectively called humus. While the structure of humus has few nutrients, it is able to attract and hold cation and anion nutrients by weak bonds that can be released into the soil solution in response to changes in soil pH.\n\nLignin is resistant to breakdown and accumulates within the soil. It also reacts with amino acids, which further increases its resistance to decomposition, including enzymatic decomposition by microbes. Fats and waxes from plant matter have some resistance to decomposition and persist in soils for a while. Clay soils often have higher organic contents that persist longer than soils without clay as the organic molecules adhere to and are stabilised by the clay. Proteins normally decompose readily, but when bound to clay particles, they become more resistant to decomposition. Clay particles also absorb the enzymes exuded by microbes which would normally break down proteins. The addition of organic matter to clay soils can render that organic matter and any added nutrients inaccessible to plants and microbes for many years. High soil tannin (polyphenol) content can cause nitrogen to be sequestered in proteins or cause nitrogen immobilisation.\n\nHumus formation is a process dependent on the amount of plant material added each year and the type of base soil. Both are affected by climate and the type of organisms present. Soils with humus can vary in nitrogen content but typically have 3 to 6 percent nitrogen. Raw organic matter, as a reserve of nitrogen and phosphorus, is a vital component affecting soil fertility. Humus also absorbs water, and expands and shrinks between dry and wet states, increasing soil porosity. Humus is less stable than the soil's mineral constituents, as it is reduced by microbial decomposition, and over time its concentration diminshes without the addition of new organic matter. However, humus may persist over centuries if not millennia.\n\nThe production, accumulation and degradation of organic matter are greatly dependent on climate. Temperature, soil moisture and topography are the major factors affecting the accumulation of organic matter in soils. Organic matter tends to accumulate under wet or cold conditions where decomposer activity is impeded by low temperature or excess moisture which results in anaerobic conditions. Conversely, excessive rain and high temperatures of tropical climates enables rapid decomposition of organic matter and leaching of plant nutrients; forest ecosystems on these soils rely on efficient recycling of nutrients and plant matter to maintain their productivity. Excessive slope may encourage the erosion of the top layer of soil which holds most of the raw organic material that would otherwise eventually become humus.\n\nCellulose and hemicellulose undergo fast decomposition by fungi and bacteria, with a half-life of 12–18 days in a temperate climate. Brown rot fungi can decompose the cellulose and hemicellulose, leaving the lignin and phenolic compounds behind. Starch, which is an energy storage system for plants, undergoes fast decomposition by bacteria and fungi. Lignin consists of polymers composed of 500 to 600 units with a highly branched, amorphous structure. Lignin undergoes very slow decomposition, mainly by white rot fungi and actinomycetes; its half-life under temperate conditions is about six months.\n\nA horizontal layer of the soil, whose physical features, composition and age are distinct from those above and beneath, is referred to as a soil horizon. The naming of a horizon is based on the type of material of which it is composed. Those materials reflect the duration of specific processes of soil formation. They are labelled using a shorthand notation of letters and numbers which describe the horizon in terms of its colour, size, texture, structure, consistency, root quantity, pH, voids, boundary characteristics and presence of nodules or concretions. No soil profile has all the major horizons. Some may have only one horizon.\n\nThe exposure of parent material to favourable conditions produces mineral soils that are marginally suitable for plant growth. That growth often results in the accumulation of organic residues. The accumulated organic layer called the O horizon produces a more active soil due to the effect of the organisms that live within it. Organisms colonise and break down organic materials, making available nutrients upon which other plants and animals can live. After sufficient time, humus moves downward and is deposited in a distinctive organic surface layer called the A horizon.\n\nSoil is classified into categories in order to understand relationships between different soils and to determine the suitability of a soil for a particular use. One of the first classification systems was developed by Russian scientist Dokuchaev around 1880. It was modified a number of times by American and European researchers, and developed into the system commonly used until the 1960s. It was based on the idea that soils have a particular morphology based on the materials and factors that form them. In the 1960s, a different classification system began to emerge which focused on soil morphology instead of parental materials and soil-forming factors. Since then it has undergone further modifications. The World Reference Base for Soil Resources (WRB) aims to establish an international reference base for soil classification.\n\nThere are fourteen soil orders at the top level of the Australian Soil Classification. They are: Anthroposols, Organosols, Podosols, Vertosols, Hydrosols, Kurosols, Sodosols, Chromosols, Calcarosols, Ferrosols, Dermosols, Kandosols, Rudosols and Tenosols.\n\nThe EU's soil taxonomy is based on a new standard soil classification in the World Reference Base for Soil Resources produced by the UN's Food and Agriculture Organization. According to this, the major soils in the European Union are:\n\nA taxonomy is an arrangement in a systematic manner; the USDA soil taxonomy has six levels of classification. They are, from most general to specific: order, suborder, great group, subgroup, family and series. Soil properties that can be measured quantitatively are used in this classification system – they include: depth, moisture, temperature, texture, structure, cation exchange capacity, base saturation, clay mineralogy, organic matter content and salt content. There are 12 soil orders (the top hierarchical level) in soil taxonomy. The names of the orders end with the suffix \"-sol\". The criteria for the different soil orders include properties that reflect major differences in the genesis of soils. The orders are:\n\nThe percentages listed above are for land area free of ice. \"Soils of Mountains\", which constitute the balance (11.6%), have a mixture of those listed above, or are classified as \"Rugged Mountains\" which have no soil.\n\nThe above soil orders in sequence of increasing degree of development are Entisols, Inceptisols, Aridisols, Mollisols, Alfisols, Spodosols, Ultisols, and Oxisols. Histosols and Vertisols may appear in any of the above at any time during their development.\n\nThe soil suborders within an order are differentiated on the basis of soil properties and horizons which depend on soil moisture and temperature. Forty-seven suborders are recognized in the United States.\n\nThe soil great group category is a subdivision of a suborder in which the kind and sequence of soil horizons distinguish one soil from another. About 185 great groups are recognized in the United States. Horizons marked by clay, iron, humus and hard pans and soil features such as the expansion-contraction of clays (that produce self-mixing provided by clay), temperature, and marked quantities of various salts are used as distinguishing features.\n\nThe great group categories are divided into three kinds of soil subgroups: typic, intergrade and extragrade. A typic subgroup represents the basic or 'typical' concept of the great group to which the described subgroup belongs. An intergrade subgroup describes the properties that suggest how it grades towards (is similar to) soils of other soil great groups, suborders or orders. These properties are not developed or expressed well enough to cause the soil to be included within the great group towards which they grade, but suggest similarities. Extragrade features are aberrant properties which prevent that soil from being included in another soil classification. About 1,000 soil subgroups are defined in the United States.\n\nA soil family category is a group of soils within a subgroup and describes the physical and chemical properties which affect the response of soil to agricultural management and engineering applications. The principal characteristics used to differentiate soil families include texture, mineralogy, pH, permeability, structure, consistency, the locale's precipitation pattern, and soil temperature. For some soils the criteria also specify the percentage of silt, sand and coarse fragments such as gravel, cobbles and rocks. About 4,500 soil families are recognised in the United States.\n\nA family may contain several soil series which describe the physical location using the name of a prominent physical feature such as a river or town near where the soil sample was taken. An example would be Merrimac for the Merrimack River in New Hampshire. More than 14,000 soil series are recognised in the United States. This permits very specific descriptions of soils.\n\nA soil phase of series, originally called 'soil type' describes the soil surface texture, slope, stoniness, saltiness, erosion, and other conditions.\n\nSoil is used in agriculture, where it serves as the anchor and primary nutrient base for plants; however, as demonstrated by hydroponics, it is not essential to plant growth if the soil-contained nutrients can be dissolved in a solution. The types of soil and available moisture determine the species of plants that can be cultivated.\n\nSoil material is also a critical component in the mining, construction and landscape development industries. Soil serves as a foundation for most construction projects. The movement of massive volumes of soil can be involved in surface mining, road building and dam construction. Earth sheltering is the architectural practice of using soil for external thermal mass against building walls. Many building materials are soil based.\n\nSoil resources are critical to the environment, as well as to food and fibre production. Soil provides minerals and water to plants. Soil absorbs rainwater and releases it later, thus preventing floods and drought. Soil cleans water as it percolates through it. Soil is the habitat for many organisms: the major part of known and unknown biodiversity is in the soil, in the form of invertebrates (earthworms, woodlice, millipedes, centipedes, snails, slugs, mites, springtails, enchytraeids, nematodes, protists), bacteria, archaea, fungi and algae; and most organisms living above ground have part of them (plants) or spend part of their life cycle (insects) below-ground. Above-ground and below-ground biodiversities are tightly interconnected, making soil protection of paramount importance for any restoration or conservation plan.\n\nThe biological component of soil is an extremely important carbon sink since about 57% of the biotic content is carbon. Even on desert crusts, cyanobacteria, lichens and mosses capture and sequester a significant amount of carbon by photosynthesis. Poor farming and grazing methods have degraded soils and released much of this sequestered carbon to the atmosphere. Restoring the world's soils could offset the effect of increases in greenhouse gas emissions and slow global warming, while improving crop yields and reducing water needs.\n\nWaste management often has a soil component. Septic drain fields treat septic tank effluent using aerobic soil processes. Landfills use soil for daily cover. Land application of waste water relies on soil biology to aerobically treat BOD.\n\nOrganic soils, especially peat, serve as a significant fuel resource; but wide areas of peat production, such as sphagnum bogs, are now protected because of patrimonial interest.\n\nGeophagy is the practice of eating soil-like substances. Both animals and human cultures occasionally consume soil for medicinal, recreational, or religious purposes. It has been shown that some monkeys consume soil, together with their preferred food (tree foliage and fruits), in order to alleviate tannin toxicity.\n\nSoils filter and purify water and affect its chemistry. Rain water and pooled water from ponds, lakes and rivers percolate through the soil horizons and the upper rock strata, thus becoming groundwater. Pests (viruses) and pollutants, such as persistent organic pollutants (chlorinated pesticides, polychlorinated biphenyls), oils (hydrocarbons), heavy metals (lead, zinc, cadmium), and excess nutrients (nitrates, sulfates, phosphates) are filtered out by the soil. Soil organisms metabolise them or immobilise them in their biomass and necromass, thereby incorporating them into stable humus. The physical integrity of soil is also a prerequisite for avoiding landslides in rugged landscapes.\n\nLand degradation refers to a human-induced or natural process which impairs the capacity of land to function. Soils degradation involves the acidification, contamination, desertification, erosion or salination.\n\nSoil acidification is beneficial in the case of alkaline soils, but it degrades land when it lowers crop productivity and increases soil vulnerability to contamination and erosion. Soils are often initially acid because their parent materials were acid and initially low in the basic cations (calcium, magnesium, potassium and sodium). Acidification occurs when these elements are leached from the soil profile by rainfall or by the harvesting of forest or agricultural crops. Soil acidification is accelerated by the use of acid-forming nitrogenous fertilizers and by the effects of acid precipitation.\n\nSoil contamination at low levels is often within a soil's capacity to treat and assimilate waste material. Soil biota can treat waste by transforming it; soil colloids can adsorb the waste material. Many waste treatment processes rely on this treatment capacity. Exceeding treatment capacity can damage soil biota and limit soil function. Derelict soils occur where industrial contamination or other development activity damages the soil to such a degree that the land cannot be used safely or productively. Remediation of derelict soil uses principles of geology, physics, chemistry and biology to degrade, attenuate, isolate or remove soil contaminants to restore soil functions and values. Techniques include leaching, air sparging, chemical amendments, phytoremediation, bioremediation and natural degradation.\n\nDesertification is an environmental process of ecosystem degradation in arid and semi-arid regions, often caused by human activity. It is a common misconception that droughts cause desertification. Droughts are common in arid and semiarid lands. Well-managed lands can recover from drought when the rains return. Soil management tools include maintaining soil nutrient and organic matter levels, reduced tillage and increased cover. These practices help to control erosion and maintain productivity during periods when moisture is available. Continued land abuse during droughts, however, increases land degradation. Increased population and livestock pressure on marginal lands accelerates desertification.\n\nErosion of soil is caused by water, wind, ice, and movement in response to gravity. More than one kind of erosion can occur simultaneously. Erosion is distinguished from weathering, since erosion also transports eroded soil away from its place of origin (soil in transit may be described as sediment). Erosion is an intrinsic natural process, but in many places it is greatly increased by human activity, especially poor land use practices. These include agricultural activities which leave the soil bare during times of heavy rain or strong winds, overgrazing, deforestation, and improper construction activity. Improved management can limit erosion. Soil conservation techniques which are employed include changes of land use (such as replacing erosion-prone crops with grass or other soil-binding plants), changes to the timing or type of agricultural operations, terrace building, use of erosion-suppressing cover materials (including cover crops and other plants), limiting disturbance during construction, and avoiding construction during erosion-prone periods.\n\nA serious and long-running water erosion problem occurs in China, on the middle reaches of the Yellow River and the upper reaches of the Yangtze River. From the Yellow River, over 1.6 billion tons of sediment flow each year into the ocean. The sediment originates primarily from water erosion (gully erosion) in the Loess Plateau region of northwest China.\n\nSoil piping is a particular form of soil erosion that occurs below the soil surface. It causes levee and dam failure, as well as sink hole formation. Turbulent flow removes soil starting at the mouth of the seep flow and the subsoil erosion advances up-gradient. The term sand boil is used to describe the appearance of the discharging end of an active soil pipe.\n\nSoil salination is the accumulation of free salts to such an extent that it leads to degradation of the agricultural value of soils and vegetation. Consequences include corrosion damage, reduced plant growth, erosion due to loss of plant cover and soil structure, and water quality problems due to sedimentation. Salination occurs due to a combination of natural and human-caused processes. Arid conditions favour salt accumulation. This is especially apparent when soil parent material is saline. Irrigation of arid lands is especially problematic. All irrigation water has some level of salinity. Irrigation, especially when it involves leakage from canals and overirrigation in the field, often raises the underlying water table. Rapid salination occurs when the land surface is within the capillary fringe of saline groundwater. Soil salinity control involves watertable control and flushing with higher levels of applied water in combination with tile drainage or another form of subsurface drainage.\n\nSoils which contain high levels of particular clays, such as smectites, are often very fertile. For example, the smectite-rich clays of Thailand's Central Plains are among the most productive in the world.\n\nMany farmers in tropical areas, however, struggle to retain organic matter in the soils they work. In recent years, for example, productivity has declined in the low-clay soils of northern Thailand. Farmers initially responded by adding organic matter from termite mounds, but this was unsustainable in the long-term. Scientists experimented with adding bentonite, one of the smectite family of clays, to the soil. In field trials, conducted by scientists from the International Water Management Institute in cooperation with Khon Kaen University and local farmers, this had the effect of helping retain water and nutrients. Supplementing the farmer's usual practice with a single application of 200 kg bentonite per rai (6.26 rai = 1 hectare) resulted in an average yield increase of 73%. More work showed that applying bentonite to degraded sandy soils reduced the risk of crop failure during drought years.\n\nIn 2008, three years after the initial trials, IWMI scientists conducted a survey among 250 farmers in northeast Thailand, half of whom had applied bentonite to their fields. The average improvement for those using the clay addition was 18% higher than for non-clay users. Using the clay had enabled some farmers to switch to growing vegetables, which need more fertile soil. This helped to increase their income. The researchers estimated that 200 farmers in northeast Thailand and 400 in Cambodia had adopted the use of clays, and that a further 20,000 farmers were introduced to the new technique.\n\nIf the soil is too high in clay, adding gypsum, washed river sand and organic matter will balance the composition. Adding organic matter (like ramial chipped wood for instance) to soil which is depleted in nutrients and too high in sand will boost its quality.\n\n"}
{"id": "3469441", "url": "https://en.wikipedia.org/wiki?curid=3469441", "title": "Sorbent", "text": "Sorbent\n\nA sorbent is a material used to absorb or adsorb liquids or gases. Examples include:\n\n"}
{"id": "49021319", "url": "https://en.wikipedia.org/wiki?curid=49021319", "title": "TALE-likes", "text": "TALE-likes\n\nTranscription Activator Like Effector Likes (TALE-likes) are a group of bacterial DNA binding proteins named for the first and still best studied group, the TALEs of \"Xanthomonas\" bacteria. TALEs are important factors in the plant diseases caused by \"Xanthomonas\" bacteria, but are known primarily for their role in biotechnology as programmable DNA binding proteins, particularly in the context of TALE nucleases. TALE-likes have additionally been found in many strains of the \"Ralstonia solanacearum\" bacterial species complex, in \"Burkholderia rhizoxinica\" strain HKI 454, and in two unknown marine bacteria. Whether or not all these proteins from a single phylogenetic grouping is as yet unclear.\n\nThe unifying feature of the TALE-likes are their tandem arrays of DNA binding repeats. These repeats are, with few exceptions, 33-35 amino acids in length, and composed of two alpha-helices on either side of a flexible loop containing the DNA base binding residues and with neighbouring repeats joined by flexible linker loops. Evidence for this common structure comes in part from solved crystal structures of TALEs and a \"Burkholderia\" TALE-like, but also from the conservation of the code that all TALE-likes use to recognise DNA-sequences.\n\nTALEs are the first identified, best-studied and largest group within the TALE-likes. TALEs are found throughout the bacterial genus \"Xanthomonas\", comprising mostly plant pathogens. Those TALEs which have been studied have all been shown to be secreted as part of the Type III secretion system into host plant cells. Once inside the host cell they translocate to the nucleus, bind specific DNA sequences within host promoters and turn on downstream genes. Every part of this process is thought to be conserved across all TALEs. The single meaningful difference between individual TALEs, based on current understanding, is the specific DNA sequence that each TALE binds. TALEs from even closely related strains differ in the composition of repeats that make up their DNA binding domain. Repeat composition determines DNA binding preference. In particular position 13 of each repeat confers the DNA base preference of each repeat. During early research it was noted that almost all the differences between repeats of a single TALE repeat array are found in positions 12 and 13 and this finding led to the hypothesis that these residues determine base preference. In fact repeat positions 12 and 13, referred to jointly as the Repeat Variable Diresidue (RVD) are commonly said to confer base specificity despite clear evidence that position 13 is the base determining residue. In addition to the repeat domain TALEs also possess a number of conserved features in the domains flanking the repeats. These include domains for type-III-secretion, nuclear localization and transcriptional activation. This allows TALEs to carry out their biological role as effector proteins secreted into host plant cells to activate expression of specific host genes.\n\nDiversity and evolution\n\nWhilst the RVD positions are commonly the only variable positions within a single TALE repeat array it should be noted that there are more differences when comparing repeat arrays of different TALEs. The diversity of TALEs across the Xanthomonas genus is considerable, but a particularly striking finding is that the evolutionary history one arrives at by comparing repeat compositions differs from that found when comparing non-repeat sequences. Repeat arrays of TALEs are thought to evolve rapidly, with a number of recombinatorial processes suggested to shape repeat array evolution. Recombination of TALE repeat arrays has been demonstrated in a forced-selection experiment. This evolutionary dynamism is though to be made possible by the very high sequence identity of TALE repeats, which is a unique feature of TALEs as opposed to other TALE-likes.\n\nT-zero\n\nAnother unique feature of TALEs is a set of four repeat structures at the N-terminal flank of the core repeat array. These structures, termed non-canonical or degenerate repeats have been shown to be vital for DNA binding, though all but one do not contact DNA bases and thus make no contribution to sequence preference. The one exception is repeat -1, which encodes a fixed T-zero preference to all TALEs. This means that the target sequences of TALEs are always preceded by a thymine base. This is thought to be common to all TALEs, with the possible exception of TalC from \"Xanthomonas oryzae pv. oryzae\" strain AXO1947.\n\nDiscovery and molecular properties\n\nIt was noted in the 2002 publication of the genome of reference strain \"Ralstonia solanacearum\" GMI1000 that its genome encodes a protein similar to \"Xanthomonas\" TALEs. Based on similar domain structure and repeat sequences it was presumed that this gene and homologs in other \"Ralstonia\" strains would encode proteins with the same molecular properties as TALEs, including sequence-specific DNA binding. In 2013 this was confirmed by two studies. These genes and the proteins they encode are referred to as RipTALs (Ralstonia injected protein TALE-like) in line with the standard nomenclature of Ralstonia effectors. Whilst the DNA binding code of the core repeats is conserved with TALEs, RipTALs do not share the T-zero preference, instead they have a strict G-zero requirement. In addition repeats within a single RipTAL repeat array have multiple sequence differences beyond the RVD positions, unlike the near-identical repeats of TALEs.\n\nBiological role\n\nSeveral lines of evidence support the idea that RipTALs function as effector proteins, promoting bacterial growth or disease by manipulating the expression of plant genes. They are secreted into plant cells by the Type III secretion system, which is the main delivery system for effector proteins. They are able to function as sequence-specific transcription factors in plant cells. In addition a strain lacking its RipTAL was shown to grow slower inside eggplant leaf tissue than the wild type. Furthermore, a study based on DNA polymorphisms in \"ripTAL\" repeat domain sequences and host plants found a statistically significant connection between host plant and repeat domain variants. This is expected if the RipTALs of different strains are adapted to target genes in specific host plants. Despite this to date no target genes have been identified for any RipTAL.\n\nDiscovery\n\nThe publication of the genome of bacterial strain \"Bukrholderia rhizoxinica\" HKI 454, in 2011 led to the discovery of a set of TALE-like genes that differed considerably in nature from the TALEs and RipTALS. The proteins encoded by these genes were studied for their DNA binding properties by two groups independently and named the Bats (Burkholderia TALE-likes ) or BurrH. This research showed that the repeat units of the \"Burkholderia\" TALE-likes bind DNA with the same code as TALEs, governed by position 13 of each repeat. There are, however, a number of differences.\n\nBiological role\n\n\"Burkholderia\" TALE-likes are composed almost entirely of repeats, lacking the large non-repetitive domains found flanking the repeats in TALEs and RpTALs. Those domains are key to the functions of TALEs and RipTALs allowing them to infiltrate the plant nucleus and turn on gene expression. It is therefore currently unclear what the biological roles of \"Burkholderia\" TALE-likes are. What is clear is that they are not effector proteins secreted into plant cells to act as transcription factors, the biological role of TALEs and RipTALs. It is not unexpected that they may differ in biological roles from TALEs and RipTALs since the life style of the bacterium they derive from is very unlike that of TALE and RipTAL bearing bacteria. \"B. rhizoxinica\" is an endosymbiont, living inside a fungus, \"Rhizopus microsporus\", a plant pathogen. The same fungus is also an opportunistic human pathogen in immuno-compromised patients, but whereas \"B. rhizoxinica\" is necessary for pathogenicity on plant hosts it is irrelevant to human infection. It is unclear whether the \"Burkholderia\" TALE-likes are ever secreted either into the fungus, let alone into host plants.\nUses in Biotechnology\n\nAs noted in the publications on \"Burkholderia\" TALE-likes there may be some advantages to using these proteins as a scaffold for programmable DNA-binding proteins to function as transcription factors or designer-nucleases, compared to TALEs. These advantages are a shorter repeat size, more compact domain structure (no large non-repeat domains), greater repeat sequence diversity enabling the use of PCR on the genes encoding them and making them less vulnerable to recombinatorial repeat loss. In addition Burkholderia TALE-likes have no T-zero requirement relaxing the constraints on DNA target selection. However, to uses of Burkholderia TALE-likes as programmable DNA binding proteins have been published, outside of the original characterization publications.\n\nDiscovery\n\nIn 2007 the results of a sweep of the world's oceans by the J. Craig Venter Institute were made publicly available. The paper in 2014 on \"Burkholderia\" TALE-likes was also the first to report that two entries from that database resembled TALE-likes, based on sequence similarity. These were further characterized and assessed for their DNA-binding potential in 2015. The repeat units encoded by these sequences were found to mediate DNA binding with base preference matching the TALE code, and judged likely to form structures nearly identical to Bat1 repeats based on molecular dynamics simulations. The proteins encoded by these DNA sequences were therefore designated Marine Organism TALE-likes (MOrTLs) 1 and 2.\n\nEvolutionary relationship to other TALE-likes\n\nWhilst repeats of MOrTL1 and 2 both conform structurally and functionally to the TALE-like norm, they differ considerably at the sequence level both from all other TALE-likes and from one another. It is not known whether they are truly homologous to the other TALE-likes, and thus constitute together with the TALEs, RipTALs and Bats a true protein-family. Alternatively they may have evolved independently. It is particularly difficult to judge the relationship to the other TALE-likes because almost nothing is known of the organisms that MOrTL1 and MOrTL2 come from. It is known only that they were found in two separate sea-water samples from the Gulf of Mexico and are likely to be bacteria based on size-exclusion before DNA sequencing.\n"}
{"id": "31016", "url": "https://en.wikipedia.org/wiki?curid=31016", "title": "Terrestrial Time", "text": "Terrestrial Time\n\nTerrestrial Time (TT) is a modern astronomical time standard defined by the International Astronomical Union, primarily for time-measurements of astronomical observations made from the surface of Earth.\nFor example, the Astronomical Almanac uses TT for its tables of positions (ephemerides) of the Sun, Moon and planets as seen from Earth. In this role, TT continues Terrestrial Dynamical Time (TDT or TD), which in turn succeeded ephemeris time (ET). TT shares the original purpose for which ET was designed, to be free of the irregularities in the rotation of Earth.\n\nThe unit of TT is the SI second, the definition of which is currently based on the caesium atomic clock, but TT is not itself defined by atomic clocks. It is a theoretical ideal, and real clocks can only approximate it.\n\nTT is distinct from the time scale often used as a basis for civil purposes, Coordinated Universal Time (UTC). TT indirectly underlies UTC, via International Atomic Time (TAI). Because of the historical difference between TAI and ET when TT was introduced, TT is approximately 32.184 s ahead of TAI.\n\nA definition of a terrestrial time standard was adopted by the International Astronomical Union (IAU) in 1976 at its XVI General Assembly, and later named \"Terrestrial Dynamical Time\" (TDT). It was the counterpart to Barycentric Dynamical Time (TDB), which was a time standard for Solar system ephemerides, to be based on a dynamical time scale. Both of these time standards turned out to be imperfectly defined. Doubts were also expressed about the meaning of 'dynamical' in the name TDT.\n\nIn 1991, in Recommendation IV of the XXI General Assembly, the IAU redefined TDT, also renaming it \"Terrestrial Time\". TT was formally defined in terms of Geocentric Coordinate Time (TCG), defined by the IAU on the same occasion. TT was defined to be a linear scaling of TCG, such that the unit of TT is the SI second on the geoid (Earth surface at mean sea level). This left the exact ratio between TT time and TCG time as something to be determined by experiment. Experimental determination of the gravitational potential at the geoid surface is a task in physical geodesy.\n\nIn 2000, the IAU very slightly altered the definition of TT by adopting an exact value for the ratio between TT and TCG time, as 1 − × 10. (As measured on the geoid surface, the rate of TCG is very slightly faster than that of TT, see below, Relativistic relationships of TT.)\n\nTT differs from Geocentric Coordinate Time (TCG) by a constant rate. Formally it is defined by the equation\n\nformula_1\n\nwhere TT and TCG are linear counts of SI seconds in Terrestrial Time and Geocentric Coordinate Time respectively, L is the constant difference in the rates of the two time scales, and E is a constant to resolve the epochs (see below). L is defined as exactly × 10. (In 1991 when TT was first defined, L was to be determined by experiment, and the best available estimate was × 10.)\n\nThe equation linking TT and TCG is more commonly seen in the form\n\nformula_2\n\nwhere JD is the TCG time expressed as a Julian date (JD). This is just a transformation of the raw count of seconds represented by the variable TCG, so this form of the equation is needlessly complex. The use of a Julian Date specifies the epoch fully. The above equation is often given with the Julian Date for the epoch, but that is inexact (though inappreciably so, because of the small size of the multiplier L). The value is exactly in accord with the definition.\n\nTime coordinates on the TT and TCG scales are conventionally specified using traditional means of specifying days, carried over from non-uniform time standards based on the rotation of Earth. Specifically, both Julian Dates and the Gregorian calendar are used. For continuity with their predecessor Ephemeris Time (ET), TT and TCG were set to match ET at around Julian Date (1977-01-01T00Z). More precisely, it was defined that TT instant 1977-01-01T00:00:32.184 exactly and TCG instant 1977-01-01T00:00:32.184 exactly correspond to the International Atomic Time (TAI) instant 1977-01-01T00:00:00.000 exactly. This is also the instant at which TAI introduced corrections for gravitational time dilation.\n\nTT and TCG expressed as Julian Dates can be related precisely and most simply by the equation\n\nwhere E is exactly.\n\nTT is a theoretical ideal, not dependent on a particular realization. For practical purposes, TT must be realized by actual clocks in the Earth system.\n\nThe main realization of TT is supplied by TAI. The TAI service, running since 1958, attempts to match the rate of proper time on the geoid, using an ensemble of atomic clocks spread over the surface and low orbital space of Earth. TAI is canonically defined retrospectively, in monthly bulletins, in relation to the readings that particular groups of atomic clocks showed at the time. Estimates of TAI are also provided in real time by the institutions that operate the participating clocks. Because of the historical difference between TAI and ET when TT was introduced, the TAI realization of TT is defined thus:\n\nBecause TAI is never revised once published, it is possible for errors in it to become known and remain uncorrected. It is thus possible to produce a better realization of TT based on reanalysis of historical TAI data. The BIPM has done this approximately annually since 1992. These realizations of TT are named in the form \"TT(BIPM08)\", with the digits indicating the year of publication. They are published in the form of table of differences from TT(TAI). The latest is TT(BIPM17).\n\nThe international communities of precision timekeeping, astronomy, and radio broadcasts have considered creating a new precision time scale based on observations of an ensemble of pulsars. This new pulsar time scale will serve as an independent means of computing TT, and it may eventually be useful to identify defects in TAI.\n\nSometimes times described in TT must be handled in situations where TT's detailed theoretical properties are not significant. Where millisecond accuracy is enough (or more than enough), TT can be summarized in the following ways:\n\n\nObservers in different locations, that are in relative motion or at different altitudes, can disagree about the rates of each other's clocks, owing to effects described by the theory of relativity. As a result, TT (even as a theoretical ideal) does not match the proper time of all observers.\n\nIn relativistic terms, TT is described as the proper time of a clock located on the geoid (essentially mean sea level).\nHowever,\nTT is now actually defined as a coordinate time scale.\nThe redefinition did not quantitatively change TT, but rather made the existing definition more precise. In effect it defined the geoid (mean sea level) in terms of a particular level of gravitational time dilation relative to a notional observer located at infinitely high altitude.\n\nThe present definition of TT is a linear scaling of Geocentric Coordinate Time (TCG), which is the proper time of a notional observer who is infinitely far away (so not affected by gravitational time dilation) and at rest relative to Earth. TCG is used so far mainly for theoretical purposes in astronomy. From the point of view of an observer on Earth's surface the second of TCG passes in slightly less than the observer's SI second. The comparison of the observer's clock against TT depends on the observer's altitude: they will match on the geoid, and clocks at higher altitude tick slightly faster.\n\n\n"}
{"id": "12533306", "url": "https://en.wikipedia.org/wiki?curid=12533306", "title": "Volcanic dam", "text": "Volcanic dam\n\nA volcanic dam is a type of natural dam produced directly or indirectly by volcanism, which holds or temporarily restricts the flow of surface water in existing streams, like a man-made dam. There are two main types of volcanic dams, those created by the flow of molten lava, and those created by the primary or secondary deposition of pyroclastic material and debris. This classification generally excludes other, often larger and longer lived dam-type geologic features, separately termed crater lakes, although these volcanic centers may be associated with the source of material for volcanic dams, and the lowest portion of its confining rim may be considered as such a dam, especially if the lake level within the crater is relatively high.\n\nVolcanic dams generally occur worldwide, in association with former and active volcanic provinces, and are known to have existed in the geologic record, in historic times and occur in the present day. Their removal or failure is similarly recorded. The longevity, and extent varies widely, having periods ranging from a few days, weeks or years to several hundred thousand years or more, and dimensions ranging from a few meters to hundreds, to several thousand.\n\nThe emplacement, internal structure, distribution and longevity of such dams can be related variously to the amount, rapidity and duration of (primary) geothermal energy released, and the rock material made available; other considerations include the rock types produced, their physical and toughness characteristics, and their various modes of deposition. Depositional modes include gravity flow of molten lava at the surface, gravity flow or fall of pyroclastics through the air, as well as the redistribution and transportation of those materials by gravity and water.\n\nLava dams are formed by lava flowing or spilling into a river valley in sufficient quantity and height to temporarily overcome the explosive nature (steam) of its contact with water, and the erosive force of flowing water to remove it. The latter depends on the quantity of water flow and stream gradient. The lava may flow during numerous successive or repetitive eruptions and may emanate from single or numerous vents or fissures. Lava of this nature, like basalt, is usually associated with less explosive eruptions; more viscous lavas with lower mafic content, like dacites and rhyolites, can also flow, but tend to be more closely associated with eruptions of greater explosiveness and the formation of pyroclastics.\n\nOnce initially established, continued lava flow creates a steeper upstream face as it battles the rising water, but with most lava flowing unimpeded downstream covering the now-dried river bed and its alluvial sediments, sometimes for miles. Thus emplaced, the shape of a lava dam resembles an elongated blob, wedged in the valley bottom. At the same time, the water continues to flow, the lake continues to rise and accumulate sediment, which previously had migrated unimpeded downstream. Sediment filling, over-topping, downward erosion, waterfalls and under-cutting inevitably follow, unless an alternative outlet is established, for water and sediment elsewhere in the drainage.\n\nLarge examples of lava dams from the geologic record include those repeatedly developed from the western side of the Grand Canyon, with the largest remnant now termed Prospect Dam, and in several locations within the Snake River drainage. The former 'Lake Idaho', which existed for more than 6.5 million years, filled the western portion of the behind such a structure and created the western section of the Snake River Plain, and accumulated 4000 ft of lake sediments. Other locations include near American Falls, Idaho, and numerous others. Many of these were overtopped, washed out, or skirted by the outburst flood originating from ancestral Lake Bonneville.\n\nMany other examples exist globally including, Caburgua Lake in Chile, Mývatn in Iceland and Lake Reporoa in New Zealand. Examples in western Canada and others in northwestern United States include, Lava Lake and The Barrier, which still impounds Garibaldi Lake, and Lava Butte.\n\nPyroclastic dams are created in an existing drainage either by their direct emplacement or by the accumulation of widely variable pyroclastic particles, broadly termed tephra. Unlike lava dams, which are formed by coherent, molten liquid gravity surface flow, filling the valley bottom directly and solidifying rapidly from the outside inward, pyroclastic dams are produced by less coherent airborne gravity currents or falls of tephra particles from the atmosphere, which solidify on the surface more slowly from the inner portion outward; pyroclastics are also deposited both in the valley bottom and widely distributed on the adjacent slopes. Their airborne nature is less restricted to the immediate drainage and they may roil over drainage boundaries; their particulate components allow for continued redistribution after initial placement by gravity and water. The explosiveness of pyroclastic eruptions, both laterally and vertically, range from fiery surges, to hot flows, to warm falls of tephra; the former may tend to emplace a dam directly while the latter tends to enhance placement or provide additional material. Unless violently expelled and generally speaking, larger sized tephra falls closest to the crater and smaller tephra landing farther away, with its distribution more highly influenced by prevailing wind velocity and direction.\n\nOnce initially established, a pyroclastic dam's continued longevity remains a balance between its slowly consolidating hardness and toughness, and the amount and velocity of flowing water's erosive capacity to remove it from its outset. Unconsolidated tephra is quickly moved by precipitation and flowing water in drainages, at times creating a lahar. Upstream of the dam this material would rapidly accumulate to fill the lake, and downstream it would tend to erode its slopes and base. The often rapid accumulation of unconsolidated pyroclastic material on steep sideslopes tends to be inherently unstable over time; pyroclastic dams may be emplaced by the landsliding of such material into rivers and streams. Pyroclastic material, given sufficient time to consolidate or 'weld' into hard rock, produce assemblages variously classified as ignimbrites, variously brecciated or agglomerated, along with various types of tuffs and volcanic ash, and are mostly of felsic composition.\n\nWhile evidence of pyroclastic dams occur within the geologic record, they are best known and studied in relation to recent and current volcanic eruptions. Examples worldwide include associations with El Chichon in Mexico, and the Karymsky Volcano in Russia. The caldera lake associated with Taal Volcano, which was previously open to the East China Sea, was permanently closed by a pyroclastic dam during the 1749 eruption, and remains in equilibrium at a higher level to this day, while the pyroclastic dam comprising the low rim of crater Lake Nyos in Cameroon is considered less stable.\n\nLike all forms of natural dams, the erosion or failure of volcanic dams can produce catastrophic floods, debris flows and associated landslides, depending on the size of the impounded lake.\n\n"}
{"id": "73231", "url": "https://en.wikipedia.org/wiki?curid=73231", "title": "Weather forecasting", "text": "Weather forecasting\n\nWeather forecasting is the application of science and technology to predict the conditions of the atmosphere for a given location and time. People have attempted to predict the weather informally for millennia and formally since the 19th century. Weather forecasts are made by collecting quantitative data about the current state of the atmosphere at a given place and using meteorology to project how the atmosphere will change.\n\nOnce calculated by hand based mainly upon changes in barometric pressure, current weather conditions, and sky condition or cloud cover, weather forecasting now relies on computer-based models that take many atmospheric factors into account. Human input is still required to pick the best possible forecast model to base the forecast upon, which involves pattern recognition skills, teleconnections, knowledge of model performance, and knowledge of model biases. The inaccuracy of forecasting is due to the chaotic nature of the atmosphere, the massive computational power required to solve the equations that describe the atmosphere, the error involved in measuring the initial conditions, and an incomplete understanding of atmospheric processes. Hence, forecasts become less accurate as the difference between current time and the time for which the forecast is being made (the \"range\" of the forecast) increases. The use of ensembles and model consensus help narrow the error and pick the most likely outcome.\n\nThere are a variety of end uses to weather forecasts. Weather warnings are important forecasts because they are used to protect life and property. Forecasts based on temperature and precipitation are important to agriculture, and therefore to traders within commodity markets. Temperature forecasts are used by utility companies to estimate demand over coming days. On an everyday basis, people use weather forecasts to determine what to wear on a given day. Since outdoor activities are severely curtailed by heavy rain, snow and wind chill, forecasts can be used to plan activities around these events, and to plan ahead and survive them. In 2009, the US spent $5.1 billion on weather forecasting.\n\nFor millennia people have tried to forecast the weather. In 650 BC, the Babylonians predicted the weather from cloud patterns as well as astrology. In about 350 BC, Aristotle described weather patterns in \"Meteorologica\". Later, Theophrastus compiled a book on weather forecasting, called the \"Book of Signs\". Chinese weather prediction lore extends at least as far back as 300 BC, which was also around the same time ancient Indian astronomers developed weather-prediction methods. In New Testament times, Christ himself referred to deciphering and understanding local weather patterns, by saying, \"When evening comes, you say, 'It will be fair weather, for the sky is red', and in the morning, 'Today it will be stormy, for the sky is red and overcast.' You know how to interpret the appearance of the sky, but you cannot interpret the signs of the times.\"\n\nIn 904 AD, Ibn Wahshiyya's \"Nabatean Agriculture\", translated into Arabic from an earlier Aramaic work, discussed the weather forecasting of atmospheric changes and signs from the planetary astral alterations; signs of rain based on observation of the lunar phases; and weather forecasts based on the movement of winds.\n\nAncient weather forecasting methods usually relied on observed patterns of events, also termed pattern recognition. For example, it might be observed that if the sunset was particularly red, the following day often brought fair weather. This experience accumulated over the generations to produce weather lore. However, not all of these predictions prove reliable, and many of them have since been found not to stand up to rigorous statistical testing.\n\nIt was not until the invention of the electric telegraph in 1835 that the modern age of weather forecasting began. Before that, the fastest that distant weather reports could travel was around 100 miles per day (160 km/d), but was more typically 40–75 miles per day (60–120 km/day) (whether by land or by sea). By the late 1840s, the telegraph allowed reports of weather conditions from a wide area to be received almost instantaneously, allowing forecasts to be made from knowledge of weather conditions further upwind.\n\nThe two men credited with the birth of forecasting as a science were an officer of the Royal Navy Francis Beaufort and his protégé Robert FitzRoy. Both were influential men in British naval and governmental circles, and though ridiculed in the press at the time, their work gained scientific credence, was accepted by the Royal Navy, and formed the basis for all of today's weather forecasting knowledge.\n\nBeaufort developed the Wind Force Scale and Weather Notation coding, which he was to use in his journals for the remainder of his life. He also promoted the development of reliable tide tables around British shores, and with his friend William Whewell, expanded weather record-keeping at 200 British Coast guard stations.\n\nRobert FitzRoy was appointed in 1854 as chief of a new department within the Board of Trade to deal with the collection of weather data at sea as a service to mariners. This was the forerunner of the modern Meteorological Office. All ship captains were tasked with collating data on the weather and computing it, with the use of tested instruments that were loaned for this purpose.\nA storm in 1859 that caused the loss of the \"Royal Charter\" inspired FitzRoy to develop charts to allow predictions to be made, which he called \"forecasting the weather\", thus coining the term \"weather forecast\". Fifteen land stations were established to use the telegraph to transmit to him daily reports of weather at set times leading to the first gale warning service. His warning service for shipping was initiated in February 1861, with the use of telegraph communications. The first daily weather forecasts were published in \"The Times\" in 1861. In the following year a system was introduced of hoisting storm warning cones at the principal ports when a gale was expected. The \"Weather Book\" which FitzRoy published in 1863 was far in advance of the scientific opinion of the time.\n\nAs the electric telegraph network expanded, allowing for the more rapid dissemination of warnings, a national observational network was developed, which could then be used to provide synoptic analyses. Instruments to continuously record variations in meteorological parameters using photography were supplied to the observing stations from Kew Observatory – these cameras had been invented by Francis Ronalds in 1845 and his barograph had earlier been used by FitzRoy.\n\nTo convey accurate information, it soon became necessary to have a standard vocabulary describing clouds; this was achieved by means of a series of classifications first achieved by Luke Howard in 1802, and standardized in the \"International Cloud Atlas\" of 1896.\n\nIt was not until the 20th century that advances in the understanding of atmospheric physics led to the foundation of modern numerical weather prediction. In 1922, English scientist Lewis Fry Richardson published \"Weather Prediction By Numerical Process\", after finding notes and derivations he worked on as an ambulance driver in World War I. He described therein how small terms in the prognostic fluid dynamics equations governing atmospheric flow could be neglected, and a finite differencing scheme in time and space could be devised, to allow numerical prediction solutions to be found.\n\nRichardson envisioned a large auditorium of thousands of people performing the calculations and passing them to others. However, the sheer number of calculations required was too large to be completed without the use of computers, and the size of the grid and time steps led to unrealistic results in deepening systems. It was later found, through numerical analysis, that this was due to numerical instability. The first computerised weather forecast was performed by a team composed of American meteorologists Jule Charney, Philip Thompson, Larry Gates, and Norwegian meteorologist Ragnar Fjørtoft, applied mathematician John von Neumann, and ENIAC programmer Klara Dan von Neumann. Practical use of numerical weather prediction began in 1955, spurred by the development of programmable electronic computers.\n\nThe first ever daily weather forecasts were published in \"The Times\" on August 1, 1861, and the first weather maps were produced later in the same year. In 1911, the Met Office began issuing the first marine weather forecasts via radio transmission. These included gale and storm warnings for areas around Great Britain. In the United States, the first public radio forecasts were made in 1925 by Edward B. \"E.B.\" Rideout, on WEEI, the Edison Electric Illuminating station in Boston. Rideout came from the U.S. Weather Bureau, as did WBZ weather forecaster G. Harold Noyes in 1931.\n\nThe world's first televised weather forecasts, including the use of weather maps, were experimentally broadcast by the BBC in 1936. This was brought into practice in 1949 after World War II. George Cowling gave the first weather forecast while being televised in front of the map in 1954. In America, experimental television forecasts were made by James C Fidler in Cincinnati in either 1940 or 1947 on the DuMont Television Network. In the late 1970s and early 80s, John Coleman, the first weatherman on ABC-TV's Good Morning America, pioneered the use of on-screen weather satellite information and computer graphics for television forecasts. Coleman was a co-founder of The Weather Channel (TWC) in 1982. TWC is now a 24-hour cable network. Some weather channels have started broadcasting on live broadcasting programs such as YouTube and Periscope to reach more viewers.\n\nThe basic idea of numerical weather prediction is to sample the state of the fluid at a given time and use the equations of fluid dynamics and thermodynamics to estimate the state of the fluid at some time in the future. The main inputs from country-based weather services are surface observations from automated weather stations at ground level over land and from weather buoys at sea. The World Meteorological Organization acts to standardize the instrumentation, observing practices and timing of these observations worldwide. Stations either report hourly in METAR reports, or every six hours in SYNOP reports. Sites launch radiosondes, which rise through the depth of the troposphere and well into the stratosphere. Data from weather satellites are used in areas where traditional data sources are not available. Compared with similar data from radiosondes, the satellite data has the advantage of global coverage, however at a lower accuracy and resolution. Meteorological radar provide information on precipitation location and intensity, which can be used to estimate precipitation accumulations over time. Additionally, if a pulse Doppler weather radar is used then wind speed and direction can be determined.\nCommerce provides pilot reports along aircraft routes, and ship reports along shipping routes. Research flights using reconnaissance aircraft fly in and around weather systems of interest such as tropical cyclones. Reconnaissance aircraft are also flown over the open oceans during the cold season into systems that cause significant uncertainty in forecast guidance, or are expected to be of high impact 3–7 days into the future over the downstream continent.\n\nModels are \"initialized\" using this observed data. The irregularly spaced observations are processed by data assimilation and objective analysis methods, which perform quality control and obtain values at locations usable by the model's mathematical algorithms (usually an evenly spaced grid). The data are then used in the model as the starting point for a forecast. Commonly, the set of equations used to predict the known as the physics and dynamics of the atmosphere are called primitive equations. These equations are initialized from the analysis data and rates of change are determined. The rates of change predict the state of the atmosphere a short time into the future. The equations are then applied to this new atmospheric state to find new rates of change, and these new rates of change predict the atmosphere at a yet further time into the future. This \"time stepping\" procedure is continually repeated until the solution reaches the desired forecast time.\n\nThe length of the time step chosen within the model is related to the distance between the points on the computational grid, and is chosen to maintain numerical stability. Time steps for global models are on the order of tens of minutes, while time steps for regional models are between one and four minutes. The global models are run at varying times into the future. The Met Office's Unified Model is run six days into the future, the European Centre for Medium-Range Weather Forecasts model is run out to 10 days into the future, while the Global Forecast System model run by the Environmental Modeling Center is run 16 days into the future. The visual output produced by a model solution is known as a prognostic chart, or \"prog\". The raw output is often modified before being presented as the forecast. This can be in the form of statistical techniques to remove known biases in the model, or of adjustment to take into account consensus among other numerical weather forecasts. MOS or model output statistics is a technique used to interpret numerical model output and produce site-specific guidance. This guidance is presented in coded numerical form, and can be obtained for nearly all National Weather Service reporting stations in the United States. As proposed by Edward Lorenz in 1963, long range forecasts, those made at a range of two weeks or more, are impossible to definitively predict the state of the atmosphere, owing to the chaotic nature of the fluid dynamics equations involved. In numerical models, extremely small errors in initial values double roughly every five days for variables such as temperature and wind velocity.\n\nEssentially, a model is a computer program that produces meteorological information for future times at given locations and altitudes. Within any modern model is a set of equations, known as the primitive equations, used to predict the future state of the atmosphere. These equations—along with the ideal gas law—are used to evolve the density, pressure, and potential temperature scalar fields and the velocity vector field of the atmosphere through time. Additional transport equations for pollutants and other aerosols are included in some primitive-equation mesoscale models as well. The equations used are nonlinear partial differential equations, which are impossible to solve exactly through analytical methods, with the exception of a few idealized cases. Therefore, numerical methods obtain approximate solutions. Different models use different solution methods: some global models use spectral methods for the horizontal dimensions and finite difference methods for the vertical dimension, while regional models and other global models usually use finite-difference methods in all three dimensions.\n\nThe simplest method of forecasting the weather, persistence, relies upon today's conditions to forecast the conditions tomorrow. This can be a valid way of forecasting the weather when it is in a steady state, such as during the summer season in the tropics. This method of forecasting strongly depends upon the presence of a stagnant weather pattern. Therefore, when in a fluctuating weather pattern, this method of forecasting becomes inaccurate. It can be useful in both short range forecasts and long range forecasts.\n\nMeasurements of barometric pressure and the pressure tendency (the change of pressure over time) have been used in forecasting since the late 19th century. The larger the change in pressure, especially if more than , the larger the change in weather can be expected. If the pressure drop is rapid, a low pressure system is approaching, and there is a greater chance of rain. Rapid pressure rises are associated with improving weather conditions, such as clearing skies.\n\nAlong with pressure tendency, the condition of the sky is one of the more important parameters used to forecast weather in mountainous areas. Thickening of cloud cover or the invasion of a higher cloud deck is indicative of rain in the near future. High thin cirrostratus clouds can create halos around the sun or moon, which indicates an approach of a warm front and its associated rain. Morning fog portends fair conditions, as rainy conditions are preceded by wind or clouds that prevent fog formation. The approach of a line of thunderstorms could indicate the approach of a cold front. Cloud-free skies are indicative of fair weather for the near future. A bar can indicate a coming tropical cyclone. The use of sky cover in weather prediction has led to various weather lore over the centuries.\n\nThe forecasting of the weather within the next six hours is often referred to as nowcasting. In this time range it is possible to forecast smaller features such as individual showers and thunderstorms with reasonable accuracy, as well as other features too small to be resolved by a computer model. A human given the latest radar, satellite and observational data will be able to make a better analysis of the small scale features present and so will be able to make a more accurate forecast for the following few hours. However, there are now expert systems using those data and mesoscale numerical model to make better extrapolation, including evolution of those features in time.\n\nIn the past, the human forecaster was responsible for generating the entire weather forecast based upon available observations. Today, human input is generally confined to choosing a model based on various parameters, such as model biases and performance. Using a consensus of forecast models, as well as ensemble members of the various models, can help reduce forecast error. However, regardless how small the average error becomes with any individual system, large errors within any particular piece of guidance are still possible on any given model run. Humans are required to interpret the model data into weather forecasts that are understandable to the end user. Humans can use knowledge of local effects that may be too small in size to be resolved by the model to add information to the forecast. While increasing accuracy of forecast models implies that humans may no longer be needed in the forecast process at some point in the future, there is currently still a need for human intervention.\n\nThe analog technique is a complex way of making a forecast, requiring the forecaster to remember a previous weather event that is expected to be mimicked by an upcoming event. What makes it a difficult technique to use is that there is rarely a perfect analog for an event in the future. Some call this type of forecasting pattern recognition. It remains a useful method of observing rainfall over data voids such as oceans, as well as the forecasting of precipitation amounts and distribution in the future. A similar technique is used in medium range forecasting, which is known as teleconnections, when systems in other locations are used to help pin down the location of another system within the surrounding regime. An example of teleconnections are by using El Niño-Southern Oscillation (ENSO) related phenomena.\n\nMost end users of forecasts are members of the general public. Thunderstorms can create strong winds and dangerous lightning strikes that can lead to deaths, power outages, and widespread hail damage. Heavy snow or rain can bring transportation and commerce to a stand-still, as well as cause flooding in low-lying areas. Excessive heat or cold waves can sicken or kill those with inadequate utilities, and droughts can impact water usage and destroy vegetation.\n\nSeveral countries employ government agencies to provide forecasts and watches/warnings/advisories to the public in order to protect life and property and maintain commercial interests. Knowledge of what the end user needs from a weather forecast must be taken into account to present the information in a useful and understandable way. Examples include the National Oceanic and Atmospheric Administration's National Weather Service (NWS) and Environment Canada's Meteorological Service (MSC). Traditionally, newspaper, television, and radio have been the primary outlets for presenting weather forecast information to the public. In addition, some cities had weather beacons. Increasingly, the internet is being used due to the vast amount of specific information that can be found. In all cases, these outlets update their forecasts on a regular basis.\n\nA major part of modern weather forecasting is the severe weather alerts and advisories that the national weather services issue in the case that severe or hazardous weather is expected. This is done to protect life and property. Some of the most commonly known of severe weather advisories are the severe thunderstorm and tornado warning, as well as the severe thunderstorm and tornado watch. Other forms of these advisories include winter weather, high wind, flood, tropical cyclone, and fog. Severe weather advisories and alerts are broadcast through the media, including radio, using emergency systems as the Emergency Alert System, which break into regular programming.\n\nThe low temperature forecast for the current day is calculated using the lowest temperature found between 7pm that evening through 7am the following morning. So, in short, today's forecasted low is most likely tomorrow's low temperature.\n\nThere are a number of sectors with their own specific needs for weather forecasts and specialist services are provided to these users.\n\nBecause the aviation industry is especially sensitive to the weather, accurate weather forecasting is essential. Fog or exceptionally low ceilings can prevent many aircraft from landing and taking off. Turbulence and icing are also significant in-flight hazards. Thunderstorms are a problem for all aircraft because of severe turbulence due to their updrafts and outflow boundaries, icing due to the heavy precipitation, as well as large hail, strong winds, and lightning, all of which can cause severe damage to an aircraft in flight. Volcanic ash is also a significant problem for aviation, as aircraft can lose engine power within ash clouds. On a day-to-day basis airliners are routed to take advantage of the jet stream tailwind to improve fuel efficiency. Aircrews are briefed prior to takeoff on the conditions to expect en route and at their destination. Additionally, airports often change which runway is being used to take advantage of a headwind. This reduces the distance required for takeoff, and eliminates potential crosswinds.\n\nCommercial and recreational use of waterways can be limited significantly by wind direction and speed, wave periodicity and heights, tides, and precipitation. These factors can each influence the safety of marine transit. Consequently, a variety of codes have been established to efficiently transmit detailed marine weather forecasts to vessel pilots via radio, for example the MAFOR (marine forecast). Typical weather forecasts can be received at sea through the use of RTTY, Navtex and Radiofax.\n\nFarmers rely on weather forecasts to decide what work to do on any particular day. For example, drying hay is only feasible in dry weather. Prolonged periods of dryness can ruin cotton, wheat, and corn crops. While corn crops can be ruined by drought, their dried remains can be used as a cattle feed substitute in the form of silage. Frosts and freezes play havoc with crops both during the spring and fall. For example, peach trees in full bloom can have their potential peach crop decimated by a spring freeze. Orange groves can suffer significant damage during frosts and freezes, regardless of their timing.\n\nWeather forecasting of wind, precipitations and humidity is essential for preventing and controlling wildfires. Different indices, like the \"Forest fire weather index\" and the \"Haines Index\", have been developed to predict the areas more at risk to experience fire from natural or human causes. Conditions for the development of harmful insects can be predicted by forecasting the evolution of weather, too.\n\nElectricity and gas companies rely on weather forecasts to anticipate demand, which can be strongly affected by the weather. They use the quantity termed the degree day to determine how strong of a use there will be for heating (heating degree day) or cooling (cooling degree day). These quantities are based on a daily average temperature of . Cooler temperatures force heating degree days (one per degree Fahrenheit), while warmer temperatures force cooling degree days. In winter, severe cold weather can cause a surge in demand as people turn up their heating. Similarly, in summer a surge in demand can be linked with the increased use of air conditioning systems in hot weather. By anticipating a surge in demand, utility companies can purchase additional supplies of power or natural gas before the price increases, or in some circumstances, supplies are restricted through the use of brownouts and blackouts.\n\nIncreasingly, private companies pay for weather forecasts tailored to their needs so that they can increase their profits or avoid large losses. For example, supermarket chains may change the stocks on their shelves in anticipation of different consumer spending habits in different weather conditions. Weather forecasts can be used to invest in the commodity market, such as futures in oranges, corn, soybeans, and oil.\n\nRoyal Navy\n\nThe UK Royal Navy, working with the UK Met Office, has its own specialist branch of weather observers and forecasters, as part of the Hydrographic and Meteorological (HM) specialisation, who monitor and forecast operational conditions across the globe, to provide accurate and timely weather and oceanographic information to submarines, ships and Fleet Air Arm aircraft.\n\nA mobile unit in the RAF, working with the UK Met Office, forecasts the weather for regions in which British, allied servicemen and women are deployed. A group based at Camp Bastion provides forecasts for the British armed forces in Afghanistan.\n\nSimilar to the private sector, military weather forecasters present weather conditions to the war fighter community. Military weather forecasters provide pre-flight and in-flight weather briefs to pilots and provide real time resource protection services for military installations. Naval forecasters cover the waters and ship weather forecasts. The United States Navy provides a special service to both themselves and the rest of the federal government by issuing forecasts for tropical cyclones across the Pacific and Indian Oceans through their Joint Typhoon Warning Center.\n\nWithin the United States, Air Force Weather provides weather forecasting for the Air Force and the Army. Air Force forecasters cover air operations in both wartime and peacetime operations and provide Army support; United States Coast Guard marine science technicians provide ship forecasts for ice breakers and other various operations within their realm; and Marine forecasters provide support for ground- and air-based United States Marine Corps operations. All four military branches take their initial enlisted meteorology technical training at Keesler Air Force Base. Military and civilian forecasters actively cooperate in analyzing, creating and critiquing weather forecast products.\n\n\nThese are academic or governmental meteorology organizations. Most provide at least a limited forecast for their area of interest on their website.\n\n"}
{"id": "1460862", "url": "https://en.wikipedia.org/wiki?curid=1460862", "title": "Yakov Perelman", "text": "Yakov Perelman\n\nYakov Isidorovich Perelman (; December 4, 1882 – March 16, 1942) was a Russian and Soviet science writer and author of many popular science books, including \"Physics Can Be Fun\" and \"Mathematics Can Be Fun\" (both translated from Russian into English).\n\nPerelman was born in 1882 in the town of Białystok, Congress Poland. He obtained the Diploma in Forestry from the Imperial Forestry Institute (Now Saint Petersburg State Forest Technical University) in Saint Petersburg, in 1909. He was influenced by Ernst Mach and probably the Russian Machist Alexander Bogdanov in his pedagogical approach to popularising science. After the success of \"Physics for Entertainment\", Perelman set out to produce other books, in which he showed himself to be an imaginative populariser of science. Especially popular were \"\"Arithmetic for entertainment\", \"Mechanics for entertainment\", \"Geometry for Entertainment\", \"Astronomy for entertainment\", \"Lively Mathematics\", \" Physics Everywhere\", and \"Tricks and Amusements\".\n\nHis famous books on physics and astronomy were translated into various languages by the erstwhile Soviet Union.\n\nThe scientist Konstantin Tsiolkovsky thought highly of Perelman's talents and creative genius, writing of him in the preface of \"Interplanetary Journeys\": \"The author has long been known by his popular, witty and quite scientific works on physics, astronomy and mathematics, which are, moreover written in a marvelous language and are very readable.\"\n\nPerelman has also authored a number of textbooks and articles in Soviet popular science magazines.\n\nIn addition to his educational and scientific writings, he also worked as an editor of science magazines, including \"Nature and People\" and \"In the Workshop of Nature\".\n\nPerelman died from starvation in 1942, during the German Siege of Leningrad. The siege started at 9 September 1941 and lasted 872 days, until \n27 January 1944. The Siege of Leningrad was one of the longest, most destructive sieges of a major city in modern history and one of the costliest in terms of casualties (1,117,000).\n\nHis older brother Yosif was a writer who published under the pseudonym Osip Dymov. He is not related to the Russian mathematician Grigori Perelman, who was born in 1966 to a different Yakov Perelman. However, Grigori Perelman told The New Yorker that his father gave him \"Physics for Entertainment\", and it inspired his interest in mathematics.\n\n\nHe has also written several books on interplanetary travel (\"Interplanetary Journeys, On a Rocket to Stars, and World Expanses\")\n\nIn 1913, Russian bookshops began carrying \"Physics for Entertainment\". The educationalist's new book attracted young readers seeking answers to scientific questions.\n\n\"Physics for Entertainment\" had a unique layout as well as an instructive style. In the preface (11th ed.) Perelman wrote: \"The main objective of \"Physics for entertainment\" is to arouse the activity of scientific imagination, to teach the reader to think in the spirit of the science of physics and to create in his mind a wide variety of associations of physical knowledge with the widely differing facts of life, with all that he normally comes into contact with.\"\n\nIn the foreword, Perelman describes the contents as “conundrums, brain-teasers, entertaining anecdotes, and unexpected comparisons,” adding, “I have quoted extensively from Jules Verne, H. G. Wells, Mark Twain and other writers, because, besides providing entertainment, the fantastic experiments these writers describe may well serve as instructive illustrations at physics classes.” The 13th edition (1936) would be the last published during the author's lifetime. Among the book's notable topics was the idea of a perpetual machine: a hypothetical machine which could run incessantly performing useful work. The author discusses perpetual motion, highlighting many attempts to build such a machine, and explains why they failed. Other topics included how to jump from a moving car, and why, “according to the law of buoyancy, we would never drown in the Dead Sea.”\n\nRandall Munroe, the creator of the web comic xkcd and author of his own popular science books, wrote: \nThe book is a series of a few hundred examples, no more than one or two pages each, asking a question that illustrates some idea in basic physics.\n\nIt’s neat to see what has and hasn’t changed in the last century or so. Many of the examples he uses seem to be straight out of a modern high school physics textbook, while others were totally new to me. And some of the answers to the questions he poses seem obvious, but others made me stop and think. [This] diagram ... shows a design for a fountain with no pump — it took me a while to get why it works... Later in the book, he explains the physics of that drinking bird toy.\nIt’s written in a fun, engaging, conversational style, as if he’s in the room chatting with you about these neat ideas.\n\n\n"}
{"id": "31040618", "url": "https://en.wikipedia.org/wiki?curid=31040618", "title": "Zeeman energy", "text": "Zeeman energy\n\nZeeman energy, or the external field energy, is the potential energy of a magnetised body in an external magnetic field. It is named after the Dutch physicist Pieter Zeeman, primarily known by the Zeeman effect. In SI units, it is given by\n\nwhere H is the external field, M the local magnetisation, and the integral is done over the volume of the body. This is the statistical average (over a \nunit volume macroscopic sample) of a corresponding microscopic Hamiltonial (energy) for each individual magnetic moment m, which is however experiencing a \"local\" induction B:\n\n"}
