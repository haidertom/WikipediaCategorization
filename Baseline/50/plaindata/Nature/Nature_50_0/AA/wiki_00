{"id": "8892305", "url": "https://en.wikipedia.org/wiki?curid=8892305", "title": "2002 Eastern Mediterranean event", "text": "2002 Eastern Mediterranean event\n\nThe 2002 Eastern Mediterranean Event was a high-energy upper atmosphere explosion over the Mediterranean Sea, around 34°N 21°E (between Libya and Crete) on June 6, 2002. This explosion, similar in power to a small atomic bomb, has been related to a small asteroid undetected while approaching Earth. The object disintegrated in an air burst impact and no meteorite fragments were recovered. The air burst occurred over the sea.\n\nThe event occurred during the 2001–2002 India–Pakistan standoff, and there were concerns by General Simon Worden of the U.S. Air Force that if the upper atmosphere explosion had occurred closer to Pakistan or India, it could have sparked a nuclear war between the two countries.\n\n"}
{"id": "28703342", "url": "https://en.wikipedia.org/wiki?curid=28703342", "title": "Ahmad Aali", "text": "Ahmad Aali\n\nAhmad Aali (, (born 1935 in Tabriz) is an Iranian photographer and artist.\n\n\n\n\nInterviews, critics, published articles and essays about photography\n\nBefore 1979 (before Iranian Revolution)\nMore than 18 published materials in:\nKhoushe, Keyhan international, Journal de Teheran, Tehran Journal, Bamshad Magazine, Etela’at newspaper, Tamasha Magazine, Tasvir Magazine, Ayandegan Magazine, Donya-ye Jadid Magazine, Omid-e Iran Magazine.\n\nAfter 1979 (after Iranian Revolution)\nMore than 18 published materials in:\nAks Magazine, Kelk Magazine, Hamshahri newspaper, Mehr Magazine, Salam newspaper, Eghtesad newspaper, Akhabr newspoaper, Rooz-e Haftom weekley magazine, Zaman Monthley magazine, visual art’s magazine, Me’yar magazine, Iran newspaper, Tandis Magazine, Etemad Magazine, Herfeh Honarmand magazine.\n\n\n"}
{"id": "27639095", "url": "https://en.wikipedia.org/wiki?curid=27639095", "title": "Antimatter tests of Lorentz violation", "text": "Antimatter tests of Lorentz violation\n\nHigh-precision experiments could reveal\nsmall previously unseen differences between the behavior\nof matter and antimatter.\nThis prospect is appealing to physicists because it may\nshow that nature is not Lorentz symmetric.\n\nOrdinary matter is made up of protons, electrons, and neutrons.\nThe quantum behavior of these particles can be predicted with excellent accuracy\nusing the Dirac equation, named after P.A.M. Dirac.\nOne of the triumphs of the Dirac equation is\nits prediction of the existence of antimatter particles.\nAntiprotons, positrons, and antineutrons\nare now well understood,\nand can be created and studied in experiments.\n\nHigh-precision experiments have been unable to\ndetect any difference between the masses\nof particles and\nthose of the corresponding antiparticles.\nThey also have been unable to detect any difference between the magnitudes of\nthe charges,\nor between the lifetimes,\nof particles and antiparticles.\nThese mass, charge, and lifetime symmetries\nare required in a Lorentz and CPT symmetric universe,\nbut are only a small number of the properties that need to match\nif the universe is Lorentz and CPT symmetric.\n\nThe Standard-Model Extension (SME),\na comprehensive theoretical framework for Lorentz and CPT violation,\nmakes specific predictions\nabout how particles and antiparticles\nwould behave differently in a universe\nthat is very close to,\nbut not exactly,\nLorentz symmetric.\nIn loose terms,\nthe SME can be visualized\nas being constructed from\nfixed background fields\nthat interact weakly, but differently,\nwith particles and antiparticles.\n\nThe behavioral differences between\nmatter and antimatter\nare specific to each individual experiment.\nFactors that determine the behavior include\nthe particle species involved,\nthe electromagnetic, gravitational, and nuclear fields controlling the system.\nFurthermore,\nfor any Earth-bound experiment,\nthe rotational and orbital motion of the Earth is important,\nleading to sidereal and seasonal signals.\nFor experiments conducted in space, the orbital motion of the craft\nis an important factor in determining the signals\nof Lorentz violation that might arise.\nTo harness the predictive power of the SME in any specific system,\na calculation has to be performed\nso that all these factors can be accounted for.\nThese calculations are facilitated by the reasonable assumption that Lorentz\nviolations, if they exist,\nare small. This makes it possible to use perturbation theory to obtain results\nthat would otherwise be extremely difficult to find.\n\nThe SME generates a modified Dirac equation\nthat breaks Lorentz symmetry\nfor some types of particle motions, but not others.\nIt therefore holds important information\nabout how Lorentz violations might have been hidden\nin past experiments,\nor might be revealed in future ones.\n\nA Penning trap\nis a research apparatus\ncapable of trapping individual charged particles\nand their antimatter counterparts.\nThe trapping mechanism is\na strong magnetic field that keeps the particles near a central axis,\nand an electric field that turns the particles around\nwhen they stray too far along the axis.\nThe motional frequencies of the trapped particle\ncan be monitored and measured with astonishing precision.\nOne of these frequencies is the anomaly frequency,\nwhich has played an important role in the measurement\nof the gyromagnetic ratio of the electron (see ).\n\nThe first calculations of SME effects\nin Penning traps\nwere published in 1997\nand 1998.\nThey showed that,\nin identical Penning traps,\nif the\nanomaly frequency of an electron was increased,\nthen the anomaly frequency of a positron\nwould be decreased.\nThe size of the increase or decrease\nin the frequency\nwould be a measure of\nthe strength of one of the SME background fields.\nMore specifically,\nit is a measure\nof the component of the background field\nalong the direction of the axial magnetic field.\n\nIn tests of Lorentz symmetry,\nthe noninertial nature of the laboratory\ndue to the rotational and orbital motion of the Earth\nhas to be taken into account.\nEach Penning-trap measurement\nis the projection of the background SME fields\nalong the axis of the experimental magnetic field\nat the time of the experiment.\nThis is further complicated if the experiment takes\nhours, days, or longer to perform.\n\nOne approach is to seek instantaneous differences,\nby comparing anomaly frequencies\nfor a particle and an antiparticle\nmeasured at the same time on different days.\nAnother approach is to seek\nsidereal variations,\nby continuously monitoring\nthe anomaly frequency for just one species of particle\nover an extended time.\nEach offers different challenges.\nFor example,\ninstantaneous comparisons\nrequire the electric field in the trap to be\nprecisely reversed,\nwhile sidereal tests are limited\nby the stability of the magnetic field.\n\nAn experiment conducted by the physicist Gerald Gabrielse of Harvard University involved two particles confined in a Penning trap. The idea was to compare a proton and an antiproton, but to overcome the technicalities of having opposite charges,\na negatively charged hydrogen ion was used in place of the proton. The ion, two electrons bound electrostatically with a proton, and the antiproton have the same charge and can therefore be simultaneously trapped. This design allows for quick interchange of the proton and the antiproton and so an instantaneous-type Lorentz test can be performed. The cyclotron frequencies of the two trapped particles\nwere about 90 MHz, and the apparatus was capable of resolving differences\nin these of about 1.0 Hz. The absence of Lorentz violating effects of this type\nplaced a limit on combinations of formula_1-type SME coefficients that had not been accessed in other experiments. The results\nappeared in Physical Review Letters in 1999.\n\nThe Penning-trap group at the University of Washington, headed by the Nobel Laureate Hans Dehmelt, conducted a search for sidereal variations in the anomaly frequency of a trapped electron. The results were extracted from an experiment that ran for several weeks, and the analysis required splitting the data into \"bins\" according to the orientation of the apparatus in the inertial reference frame of the Sun. At a resolution of 0.20 Hz, they were unable to discern any sidereal variations in the anomaly frequency, which runs around 185,000,000 Hz. Translating this into an upper bound on the relevant\nSME background field, places a bound of about\n10 GeV on a formula_2-type electron coefficient.\nThis work\nwas published in Physical Review Letters in 1999.\n\nAnother experimental result from the Dehmelt group involved a comparison of the instantaneous type. Using data from a single trapped electron\nand a single trapped positron, they again found no difference\nbetween the two anomaly frequencies at a resolution of about 0.2 Hz.\nThis result placed a bound on a simpler combination of\nformula_2-type coefficients at a level of about 10 GeV.\nIn addition to being a limit on Lorentz violation,\nthis also limits the CPT violation.\nThis result\nappeared in Physical Review Letters in 1999.\n\nThe antihydrogen atom is\nthe antimatter counterpart of the hydrogen atom.\nIt has a negatively charged antiproton\nat the nucleus\nthat attracts a positively charged positron\norbiting around it.\n\nThe spectral lines of hydrogen have frequencies\ndetermined by the energy differences\nbetween the quantum-mechanical orbital states\nof the electron.\nThese lines\nhave been studied in thousands of spectroscopic experiments\nand are understood in great detail.\nThe quantum mechanics of the positron orbiting an antiproton\nin the antihydrogen atom is expected to be very similar\nto that of the hydrogen atom.\nIn fact,\nconventional physics predicts that the spectrum of antihydrogen\nis identical to that of regular hydrogen.\n\nIn the presence of the background fields of the SME,\nthe spectra of hydrogen and antihydrogen\nare expected to show tiny differences\nin some lines,\nand no differences in others.\nCalculations of these SME effects\nin antihydrogen and hydrogen\nwere published\nin Physical Review Letters\nin 1999.\nOne of the main results found\nis that hyperfine transitions\nare sensitive to Lorentz breaking effects.\n\nSeveral experimental groups at CERN\nare working on producing antihydrogen.\nThey are:\n\n\nCreating trapped antihydrogen\nin sufficient quantities\nto do spectroscopy\nis an enormous experimental challenge.\nSignatures of Lorentz violation\nare similar to those expected in Penning traps.\nThere would be sidereal effects\ncausing variations in the spectral frequencies\nas the experimental laboratory turns with the Earth.\nThere would also be the possibility of finding instantaneous\nLorentz breaking signals\nwhen antihydrogen spectra are compared directly with conventional hydrogen spectra\n\nIn October 2017, the BASE experiment at CERN reported a measurement of the antiproton magnetic moment to a precision of 1.5 parts per billion. It is consistent with the most precise measurement of the proton magnetic moment (also made by BASE in 2014), which supports the hypothesis of CPT symmetry. It should be noted that this measurement represents the first time that a property of antimatter is known more precisely than the equivalent property in matter.\n\nThe muon and its positively charged antiparticle\nhave been used to perform tests of Lorentz symmetry.\nSince the lifetime of the muon is only a few microseconds,\nthe experiments are quite different\nfrom ones with electrons and positrons.\nCalculations for muon experiments\naimed at probing Lorentz violation\nin the SME\nwere first published in the year 2000.\n\nIn the year 2001,\nHughes and collaborators published their results\nfrom a search for sidereal signals in the spectrum\nof muonium,\nan \"atom\" consisting of an electron bound to a negatively charged muon.\nTheir data,\ntaken over a two-year period,\nshowed no evidence for Lorentz violation.\nThis placed a stringent constraint on\na combination of formula_2-type coefficients in the SME,\npublished in Physical Review Letters.\n\nIn 2008,\nthe Muon formula_5 Collaboration at the Brookhaven National Laboratory published results after searching for signals of Lorentz violation with muons and antimuons.\nIn one type of analysis, they compared the anomaly frequencies\nfor the muon and its antiparticle. In another, they looked for sidereal variations by allocating their data into one-hour \"bins\" according to the orientation of the Earth relative to the Sun-centered inertial reference frame.\nTheir results, published in Physical Review Letters in 2008,\nshow no signatures of Lorentz violation at the resolution of the Brookhaven experiment.\n\nExperimental results in all sectors of the\nSME are summarized in the Data Tables for Lorentz and CPT violation.\n\n"}
{"id": "20250538", "url": "https://en.wikipedia.org/wiki?curid=20250538", "title": "Big Coal: The Dirty Secret Behind America's Energy Future", "text": "Big Coal: The Dirty Secret Behind America's Energy Future\n\nBig Coal: The Dirty Secret Behind America's Energy Future is a book by Jeff Goodell which claims that coal mining is one of America's largest and most influential industries. Goodell suggests that coal mining is deadly and environmentally destructive.\n\n\n"}
{"id": "23206350", "url": "https://en.wikipedia.org/wiki?curid=23206350", "title": "Chaplygin gas", "text": "Chaplygin gas\n\nChaplygin gas, which occurs in certain theories of cosmology, is a hypothetical substance that satisfies an exotic equation of state in the form\nformula_1,\nwhere formula_2 is the pressure, formula_3 is the density, with formula_4 and formula_5 a positive constant. The substance is named after Sergey Chaplygin.\n\nIn some models, generalized Chaplygin gas is considered, where formula_6 is a parameter, which can take on values formula_7.\n\n"}
{"id": "10109665", "url": "https://en.wikipedia.org/wiki?curid=10109665", "title": "Chilton and Colburn J-factor analogy", "text": "Chilton and Colburn J-factor analogy\n\nChilton–Colburn J-factor analogy is a successful and widely used analogy between heat, momentum, and mass transfer. The basic mechanisms and mathematics of heat, mass, and momentum transport are essentially the same. Among many analogies (like Reynolds analogy, Prandtl–Taylor analogy) developed to directly relate heat transfer coefficients, mass transfer coefficients, and friction factors Chilton and Colburn J-factor analogy proved to be the most accurate. \n\nIt is written as follows,\n\nformula_1\n\nThis equation permits the prediction of an unknown transfer coefficient when one of the other coefficients is known. The analogy is valid for fully developed turbulent flow in conduits with \"Re\" > 10000, 0.7 < \"Pr\" < 160, and tubes where \"L\"/\"d\" > 60 (the same constraints as the Sieder–Tate correlation). The wider range of data can be correlated by Friend–Metzner analogy.\n\nRelationship between Heat and Mass;\n\nformula_2\n\n\n\n"}
{"id": "32276019", "url": "https://en.wikipedia.org/wiki?curid=32276019", "title": "Coalition for Green Capital", "text": "Coalition for Green Capital\n\nThe Coalition for Green Capital (CGC) is a 501c3 non-profit that works with governments at the international, national, state and local level to establish Green Bank finance institutions to accelerate the deployment of clean energy technology. CGC has designed and created clean energy financing institutions, or “Green Banks.” In the US, Green Banks have collectively sparked over $2 billion in clean energy investments.\n\nThe Coalition for Green Capital’s mission is to accelerate the transition to the clean energy economy by establishing Green Banks at the local, state, federal, and international levels to spur greater private investment in renewables, energy efficiency, and clean transportation.\n\nCGC partners with governments throughout the U.S. and abroad to design and launch Green Banks tailored to the specific needs of each market. Through partnerships with existing Green Banks and research, CGC investigates and develops new ways that Green Banks can grow clean energy markets. CGC also works with policymakers, industry associations, and other key stakeholders to share information about the Green Bank concept, and what Green Banks have already achieved. As a non-profit, CGC is primarily funded by many of the leading climate and energy focused-foundations in the nation.\n\nThe Coalition for Green Capital (CGC) was created as an outgrowth of Reed Hundt and Kenneth Berlin’s efforts with the Obama-Biden Transition Team to promote financing for clean energy and energy efficiency. CGC has since become the nation’s leading expert on Green Bank institution creation and financing.\n\nThe CGC was created coming off of the Obama-Biden Transition Team in 2009, and initially focused on advocating for a federal-level Green Bank. The Green Bank concept supported by the CGC was included in the Waxman-Markey climate change bill that passed the US House of Representatives in 2009. However, climate change legislation was unable to pass the Senate, and the CGC turned its attention to creating a state-level Green Bank.\n\nThe CGC helped newly elected Governor Dannel Malloy (working closely with his newly appointed Commissioner of Energy and Environmental Protection, Dan Esty) re-purpose the Connecticut Clean Energy Fund to create the nation’s first Green Bank. The Connecticut Green Bank has since demonstrated the power of the Green Bank model, using limited public funds to attract over $491.2m of private investment in the Connecticut clean energy economy from 2012-2015.\n\nSince that time, and based on Connecticut’s success, interest in the Green Bank concept has grown significantly. CGC now works in over a dozen states that are at some state of Green Bank development or consideration. CGC continues to work at the federal level, supporting the introduce of the Green Bank Acts of 2014 and 2016. And CGC now works increasingly at the international level, particularly with the formation of the Global Green Bank Network.\n\nCGC lists its on-going work on its website.\n\nCGC provided technical support for the Green Bank Act of 2016, which was introduced in the House by Congressman Chris Van Hollen (D-MD) on July 14, 2016 with seven co-sponsors. CGC also provided technical support for the companion legislation that was introduced in the Senate by Senators Chris Murphy (D-CT), Richard Blumenthal (D-CT) and Sheldon Whitehouse (D-RI) on September 22, 2016.\n\nCGC has played an extensive role in the creation of the Montgomery County Green Bank (MCGB), the first local green bank in the country. CGC provided technical guidance in drafting legislation in line with the County’s needs, worked with County staff to manage the working group process, and is helping stand up and operationalize the new green bank.\n\nCGC and NRDC, along with six founding green bank members, launched the Green Bank Network (GBN) in December 2015. Since then, CGC and NRDC been building the organization. At a White House-sponsored side event of the Clean Energy Ministerial (CEM) in May 2016, the GBN announced the signing of a Memorandum of Understanding among founding members agreeing to principles and mission of the GBN.\n\n----External links\n"}
{"id": "421129", "url": "https://en.wikipedia.org/wiki?curid=421129", "title": "Cob (material)", "text": "Cob (material)\n\nCob, cobb or clom (in Wales) is a natural building material made from subsoil, water, fibrous organic material (typically straw), and sometimes lime. The contents of subsoil naturally vary, and if it does not contain the right mixture it can be modified with sand or clay. Cob is fireproof, resistant to seismic activity, and inexpensive. It can be used to create artistic, sculptural forms, and its use has been revived in recent years by the natural building and sustainability movements.\n\nIn technical building and engineering documents such as the Uniform Building Code, cob may be referred to as an \"unburned clay masonry\" when used in a structural context. It might also be referred to as an \"aggregate\" in non-structural contexts, such as a \"clay and sand aggregate\" or more simply an \"organic aggregate,\" such as where the cob is an insulating filler between post and beam construction.\n\n\"Cob\" is an English term attested to around the year 1600 for an ancient building material that has been used for building since prehistoric times. The etymology of \"cob\" and \"cobbing\" is unclear, but in several senses means to \"beat\" or \"strike\", which is how cob material is applied to a wall.\n\nSome of the oldest man-made structures in Afghanistan are composed of rammed earth and cob. Cobwork (\"tabya\") was used in the Maghreb and al-Andalus in the 11th and 12th centuries, and was described in detail by Ibn Khaldun in the 14th century.\n\nCob material is known by many names including \"adobe\", \"lump clay\", \"puddled clay\", \"chalk mud\", \"wichert\", \"clay daubins\", \"swish\" (Asante Twi), \"torchis\" (French), \"bauge\" (French), \"bousille\" (French mud with moss), and \"cat and clay\".\n\nCob structures can be found in a variety of climates across the globe. European examples include:\n\n\nMany old cob buildings can be found in Africa, the Middle East, and many parts of the southwestern United States. A number of cob cottages survive from mid-19th-century New Zealand.\n\nTraditionally, English cob was made by mixing the clay-based subsoil with sand, straw and water using oxen to trample it. English soils contain varying amounts of chalk, and cob made with significant amounts of chalk are called \"chalk cob\" or \"wychert\". The earthen mixture was then ladled onto a stone foundation in courses and trodden onto the wall by workers in a process known as \"cobbing\". The construction would progress according to the time required for the prior course to dry. After drying, the walls would be trimmed and the next course built, with lintels for later openings such as doors and windows being placed as the wall takes shape.\n\nThe walls of a cob house are generally about thick, and windows were correspondingly deep-set, giving the homes a characteristic internal appearance. The thick walls provided excellent thermal mass which was easy to keep warm in winter and cool in summer. Walls with a high thermal mass value act as a thermal buffer inside the home.\nThe material has a long life-span even in rainy and/or humid climates, provided a tall foundation and large roof overhang are present.\n\nCob is fireproof, while \"fire cob\" (cob without straw or fiber) is a refractory material (the same material, essentially, as unfired common red brick), and historically, has been used to make chimneys, fireplaces, forges and crucibles. Without fiber, however, cob loses most of its tensile strength.\n\nWhen Kevin McCabe constructed a two-story, four bedroom cob house in England, UK in 1994, it was reputedly the first cob residence built in the country in 70 years. His techniques remained very traditional; the only innovations he made were using a tractor to mix the cob and adding sand or shillet, a gravel of crushed shale, to reduce shrinkage.\n\nIn 2000-01, a modern, four bedroom cob house in Worcestershire, England, UK, designed by Associated Architects, was sold for £999,000. Cobtun House was erected in 2001 and won the Royal Institute of British Architects' Sustainable Building of the Year award in 2005. The total construction cost was £300,000, but the metre-thick outer cob wall cost only £20,000.\n\nIn the Pacific Northwest of the United States there has been a resurgence of cob construction, both as an alternative building practice and one desired for its form, function, and cost effectiveness. Pat Hennebery, Tracy Calvert, Elke Cole, and the Cobworks workshops erected more than ten cob houses in the Southern Gulf Islands of British Columbia, Canada.\n\nIn 2010, Sota Construction Services in Pittsburgh, Pennsylvania, United States completed construction on its new 7,500 square foot corporate headquarters, which featured exterior cob walls along with other energy saving features like radiant heat flooring, a rooftop solar panel array, and daylighting. The cob walls, in conjunction with the other sustainable features, enabled the edifice to earn a LEED Platinum rating in 2012, and it also received one of the highest scores by percentage of total points earned in any LEED category.\n\nIn 2007, Ann and Gord Baird began constructing a two-storey cob house in Victoria, British Columbia, Canada for an estimated $210,000 CDN. The home of 2,150 square feet includes heated floors, solar panels, and a southern exposure to enable passive solar heating.\n\nWelsh architect Ianto Evans and researcher Linda Smiley refined the construction technique known as \"Oregon Cob\" in the 1980s and 1990s. Oregon Cob integrates the variation of wall layup technique which uses loaves of mud mixed with sand and straw with a rounded architectural stylism. They are experimenting with a mixture of cob and straw bale denominated \"balecob\".\n\n\n\n"}
{"id": "7722", "url": "https://en.wikipedia.org/wiki?curid=7722", "title": "Compactron", "text": "Compactron\n\nCompactrons are a type of thermionic valve, or vacuum tube, which contain multiple electrode structures packed into a single enclosure. They were designed to compete with early transistor electronics and were used in televisions, radios, and similar roles.\n\nThe Compactron was a trade name applied to multi-electrode structure tubes specifically constructed on a 12-pin Duodecar base. This vacuum tube family was introduced in 1961 by General Electric in Owensboro, Kentucky to compete with transistorized electronics during the solid state transition. Television sets were a primary application. The idea of multi-electrode tubes itself was far from new and indeed the Loewe company of Germany was producing multi-electrode tubes as far back as 1926, and they even included all of the required passive components as well.\n\nUse was prevalent in televisions because transistors were slow to achieve the high power and frequency capabilities needed particularly in color television sets. The first portable color television, the General Electric Porta-Color, was designed using 13 tubes, 10 of which were Compactrons. Even before the compactron design was unveiled, nearly all tube based electronic equipment used multi-electrode tubes of one type or another. Virtually every AM/FM radio receiver of the 1950's and 60's used a 6AK8 (EABC80) tube (or equivalent) consisting of three diodes and a triode which was designed in 1954.\n\nCompactron's integrated valve design helped lower power consumption and heat generation (they were to tubes what integrated circuits were to transistors). Compactrons were also used in a few high end Hi-Fi stereos. They were also used by the Ampeg guitar amplifier company in some of their guitar amps. No modern tube based Hi-Fi systems are known to use this tube type, as simpler and more readily available tubes have again filled this niche.\n\nA distinguishing feature of most Compactrons is the placement of the evacuation tip on the bottom end, rather than the top end as was customary with \"miniature\" tubes, and a characteristic 3/4\" diameter circle pin pattern.\n\nExamples of Compactrons type types include:\n\nDue to their specific applications in television circuits, many different Compactron types were produced. Almost all were assigned using standard US tube numbers.\n\nIntegrated circuits (of the analogue and digital type) gradually took over all of the functions that the Compactron was designed for. \"Hybrid\" television sets produced in the early to mid-1970s made use of a combination of tubes (typically Compactrons), transistors, and integrated circuits in the same set. By the mid-1980s this type of tube was functionally obsolete. Compactrons simply don't exist in any TV sets designed after 1986. Other specialist uses of the tube declined in parallel with the television set manufacture. Manufacture of Compactrons ceased in the early 1990s. New old stock replacements for almost all Compactron types produced are easily found for sale on the Internet.\n"}
{"id": "191064", "url": "https://en.wikipedia.org/wiki?curid=191064", "title": "Dusk", "text": "Dusk\n\nDusk occurs at the darkest stage of twilight, or at the very end of astronomical twilight after sunset and just before night. Pre-dusk, during early to intermediate stages of twilight, there may be enough light in the sky under clear conditions to read outdoors without artificial illumination, but at the end of civil twilight, when Earth rotates to a point at which the center of the Sun is 6° below the local horizon, artificial illumination is required to read outside. The term \"dusk\" usually refers to astronomical dusk, or the darkest part of twilight before night begins.\n\nThe time of dusk is the moment at the very end of astronomical twilight, just before the minimum brightness of the night sky sets in, or may be thought of as \"the darkest part of evening twilight\".\nBut technically, the three stages of dusk are as follows.\n\n"}
{"id": "57731730", "url": "https://en.wikipedia.org/wiki?curid=57731730", "title": "Ecological unit", "text": "Ecological unit\n\nEcological units, comprise concepts such as population, community, and - in particular - the ecosystem as the basic unit, which are at the basis of ecological theory and research. Two common traits can be identified across cultures:\n\n"}
{"id": "48629578", "url": "https://en.wikipedia.org/wiki?curid=48629578", "title": "Energy in Myanmar", "text": "Energy in Myanmar\n\nMyanmar had a total primary energy supply (TPES) of 16.57 Mtoe in 2013. Electricity consumption was 8.71 TWh. 65% of the primary energy supply consists of biomass energy, used almost exclusively (97%) in the residential sector. Myanmar’s energy consumption per capita is one of the lowest in Southeast Asia. Contributing factors are the low income and the low electrification rate. Energy consumption is however growing rapidly, with an average annual growth rate of 3.3% from 2000 to 2007.\n\nMost of electricity (74.7%) is produced by hydroelectricity. The rest is from fossil fuels, with gas as the main fuel (20.5%) followed by coal and oil.In 2011, Myanmar had an installed electricity generation capacity of about 3,344 MW, with a low electrification rate of 27%. Electrification rate is especially low in rural villages, which are mainly not connected to the power grid. Firewood is used as a primary source of energy in these areas, a contributing factor to the observed decrease in forests in the country.\n\nMyanmar has abundant energy resources, particularly hydropower and natural gas. In 2013, Myanmar exported 8561 ktoe of natural gas and 144 ktoe of crude oil. The country is one of the five major energy exporters in the region and is the second biggest exporter of natural gas in the Asia Pacific region after Indonesia. According to the World Energy Council, gas reserves are estimated at 244 Mtoe. Oil and coal play a smaller role with reserves estimated at 7 and 1 Mtoe, respectively.\n\nHydropower resources are estimated to be about 40 GW at a capacity factor of 0.40, giving a total yearly hydropower generation capacity of about 140 TWh. Installed hydropower capacity as of 2011 was 1.54 GW with a total generation of 3.9 TWh, there is therefore substantial opportunity for further growth of this energy source.\nThe Shweli 1 hydroelectric power plant, with a capacity of 600 MW, started operation in 2008. The Yeywa hydropower plant opened in 2010 with a capacity of 790 MW, the largest in the country. Several other hydropower projects are under construction or planned. Planned major hydropower plants have been designed mainly for export. The Myitsone Dam project, with a capacity of 6,000 MW, is expected to supply 100% of its electricity to China, while the Tasang Dam project with a planned capacity of 7,110 MW is planned to supply 1,500 MW to Thailand.\n\n"}
{"id": "41850992", "url": "https://en.wikipedia.org/wiki?curid=41850992", "title": "European Bureau for Conservation and Development", "text": "European Bureau for Conservation and Development\n\nThe European Bureau for Conservation and Development (EBCD) is an environmentally focused non governmental organisation (NGO) founded in 1989 and based in Brussels, Belgium. EBCD aims to promote sustainable use of natural renewable resources not just in Europe, but worldwide as well. The EBCD works closely with the European Union (EU) institutions, tracking EU work on environmental policy, concentrating on fisheries and marine policies.\n\nEBCD provides, in association with the International Union for Conservation of Nature, the Secretariat of the European Parliament Intergroup: “Climate Change, Biodiversity and Sustainable Development”.\n\nEBCD's mission: to ensure the conservation and sustainable use of natural renewable resources including species and ecosystems both for their intrinsic and direct value to the benefit of humanity \n\n"}
{"id": "33183470", "url": "https://en.wikipedia.org/wiki?curid=33183470", "title": "European Movement for Efficient Energy", "text": "European Movement for Efficient Energy\n\nThe European Movement for Efficient Energy (EME²) is a stakeholder platform that seek to promote efficiency in the energy sector as a means to achieve overall energy and resource efficiency in Europe. This would mean producing and delivering more energy for final consumption from less primary energy and other natural resources, notably water and land. This is EME² refers to as \"efficient energy”. The current primary focus or their campaign is on the supply of more resource-efficient electricity and combined heat and power.\n\nThe global energy system has enormous potential to make energy savings. GE Energy estimates that from a total primary energy input for electricity production of 49,555 TWh, only 15,623 TWh of electricity was delivered to customers worldwide. At the European Union level, the energy sector consumes 30% of primary energy consumption and an analysis by Delta Energy and Environment of the France, Poland and the UK shows that increases supply-side efficiency has the same overall energy saving potential as demand-side efficiency. In France, Poland and the UK, Delta estimates that supply-side options can contribute up to 32% of the overall goal of a 20% carbon emission reduction and up to 26% of an overall goal to reduce primary energy consumption by 20%.\n\nEME² argues that ensuring supply-side energy efficiency requires system-wide cohesion and investment, from the supply and conversion of primary energy through to the transmission and distribution of power and heat. The movement places no emphasis on a specific energy source and argues that Europe will continue to rely on a broad mix of energy sources and innovative technologies over the coming decades. Such a mix would evolve and vary from region to region and country to country for both political and practical reasons. EME² define's its purpose as to \"help drive understanding and uptake of policy strategies and technologies for maximising supply-side, system-wide efficiencies whatever the mix\".\n\nThe concept of efficient energy goes beyond just reducing greenhouse gases; it is about the efficient use of our resources, our fuel, water and land. It is about developing a more resource efficient energy production and delivery system in Europe.\n\nEU policy-makers have given significant attention has been given to energy efficiency amongst energy consumers. Policies addressing to this 'end-use' energy efficiency including encouraging the renovation and increased insulation of buildings, promoting energy labels for electrical appliances and phasing out high-energy light bulbs. EME² argues more attention should now be paid to supply-side energy efficiency.\n\nThe European Commission has begun to address these issues. In its Energy Efficiency Plan (March 2011), the European Commission called on EU member states to expand the scope of their energy efficiency policies beyond the demand side to the supply side, i.e. energy production and delivery. Specifically the plan calls for the increaseduse of energy production from co-generation and combined heat and power systems to help Europe achieve its energy goals. The Commission is further looking to strengthen the basis for national grid regulators consider energy efficiency issues in their decisions and in monitoring the management and operation of gas and electricity grids and markets.\n\nIn September 2011, the European Commission also released a Roadmap for a Resource Efficient Europe. That roadmap described the economic benefits of using resources more efficiently, by creating major economic opportunities for companies and workers, improving productivity, reducing costs and enhancing competitiveness. A key and potentially most controversial plank of the Commission proposal is to shift taxation from labour to resource use. It seeks to address market prices that do not reflect the true costs of using resources and their environmental impacts, by removing Environmentally Harmful Subsidies (EHS). The roadmap also suggests turning waste into a key resource with the aim of decreasing the EU's dependency on imports of raw materials, lower impacts on the environment and open up new markets. The roadmap sets a 2020 target for all key resources – ecosystem services, biodiversity, minerals and metals, water, air, land and soil and marine resources – and lists a series of actions and initiatives the EU and its member states should embark on.\n\nEME² is funded by participants contributions with initial funding from GE Energy. EME² has appointed Burson-Marsteller Brussels to act as the secretariat of the movement. Its members include:\n"}
{"id": "2490200", "url": "https://en.wikipedia.org/wiki?curid=2490200", "title": "Evolutionary ethics", "text": "Evolutionary ethics\n\nEvolutionary ethics is a field of inquiry that explores how evolutionary theory might bear on our understanding of ethics or morality. The range of issues investigated by evolutionary ethics is quite broad. Supporters of evolutionary ethics have claimed that it has important implications in the fields of descriptive ethics, normative ethics, and metaethics.\n\nDescriptive evolutionary ethics consists of biological approaches to morality based on the alleged role of evolution in shaping human psychology and behavior. Such approaches may be based in scientific fields such as evolutionary psychology, sociobiology, or ethology, and seek to explain certain human moral behaviors, capacities, and tendencies in evolutionary terms. For example, the nearly universal belief that incest is morally wrong might be explained as an evolutionary adaptation that furthered human survival.\n\nNormative (or prescriptive) evolutionary ethics, by contrast, seeks not to explain moral behavior, but to justify or debunk certain normative ethical theories or claims. For instance, some proponents of normative evolutionary ethics have argued that evolutionary theory undermines certain widely held views of humans' moral superiority over other animals.\n\nEvolutionary metaethics asks how evolutionary theory bears on theories of ethical discourse, the question of whether objective moral values exist, and the possibility of objective moral knowledge. For example, some evolutionary ethicists have appealed to evolutionary theory to defend various forms of moral anti-realism (the claim, roughly, that objective moral facts do not exist) and moral skepticism.\n\nThe first notable attempt to explore links between evolution and ethics was made by Charles Darwin in \"The Descent of Man\" (1871). In Chapters IV and V of that work Darwin set out to explain the origin of human morality in order to show that there was no absolute gap between man and animals. Darwin sought to show how a refined moral sense, or conscience, could have developed through a natural evolutionary process that began with social instincts rooted in our nature as social animals.\n\nNot long after the publication of Darwin's \"The Descent of Man\", evolutionary ethics took a very different—and far more dubious—turn in the form of Social Darwinism. Leading Social Darwinists such as Herbert Spencer and William Graham Sumner sought to apply the lessons of biological evolution to social and political life. Just as in nature, they claimed, progress occurs through a ruthless process of competitive struggle and \"survival of the fittest,\" so human progress will occur only if government allows unrestricted business competition and makes no effort to protect the \"weak\" or \"unfit\" by means of social welfare laws. Critics such as Thomas Henry Huxley, G. E. Moore, William James, and John Dewey roundly criticized such attempts to draw ethical and political lessons from Darwinism, and by the early decades of the twentieth century Social Darwinism was widely viewed as discredited.\n\nThe modern revival of evolutionary ethics owes much to E. O. Wilson's 1975 book, \"Sociobiology: The New Synthesis\". In that work, Wilson argues that there is a genetic basis for a wide variety of human and nonhuman social behaviors. In recent decades, evolutionary ethics has become a lively topic of debate in both scientific and philosophical circles.\n\nThe most widely accepted form of evolutionary ethics is descriptive evolutionary ethics. Descriptive evolutionary ethics seeks to explain various kinds of moral phenomena wholly or partly in genetic terms. Ethical topics addressed include altruistic behaviors, an innate sense of fairness, a capacity for normative guidance, feelings of kindness or love, self-sacrifice, incest-avoidance, parental care, in-group loyalty, monogamy, feelings related to competitiveness and retribution, moral \"cheating,\" and hypocrisy.\n\nA key issue in evolutionary psychology has been how altruistic feelings and behaviors could have evolved, in both humans and nonhumans, when the process of natural selection is based on the multiplication over time only of those genes that adapt better to changes in the environment of the species. Theories addressing this have included kin selection, group selection, and reciprocal altruism (both direct and indirect, and on a society-wide scale). Descriptive evolutionary ethicists have also debated whether various types of moral phenomena should be seen as adaptations which have evolved because of their direct adaptive benefits, or spin-offs that evolved as side-effects of adaptive behaviors.\n\nNormative evolutionary ethics is the most controversial branch of evolutionary ethics. Normative evolutionary ethics aims at defining which acts are right or wrong, and which things are good or bad, in evolutionary terms. It is not merely \"describing\", but it is \"prescribing\" goals, values and obligations. Social Darwinism, discussed above, is the most historically influential version of normative evolutionary ethics. As philosopher G. E. Moore famously argued, many early versions of normative evolutionary ethics seemed to commit a logical mistake that Moore dubbed the \"naturalistic fallacy\". This was the mistake of defining a normative property, such as goodness, in terms of some non-normative, naturalistic property, such as pleasure or survival.\n\nMore sophisticated forms of normative evolutionary ethics need not commit either the naturalistic fallacy or the is-ought fallacy. But all varieties of normative evolutionary ethics face the difficult challenge of explaining how evolutionary facts can have normative authority for rational agents. \"Regardless of why one has a given trait, the question for a rational agent is always: is it right for me to exercise it, or should I instead renounce and resist it as far as I am able?\"\n\nEvolutionary theory may not be able to tell us what is morally right or wrong, but it might be able to illuminate our use of moral language, or to cast doubt on the existence of objective moral facts or the possibility of moral knowledge. Evolutionary ethicists such as Michael Ruse, E. O. Wilson, Richard Joyce, and Sharon Street have defended such claims.\n\nSome philosophers who support evolutionary meta-ethics use it to undermine views of human well-being that rely upon Aristotelian teleology, or other goal-directed accounts of human flourishing. A number of thinkers have appealed to evolutionary theory in an attempt to debunk moral realism or support moral skepticism. Sharon Street is one prominent ethicist who argues that evolutionary psychology undercuts moral realism. According to Street, human moral decision-making is \"thoroughly saturated\" with evolutionary influences. Natural selection, she argues, would have rewarded moral dispositions that increased fitness, not ones that track moral truths, should they exist. It would be a remarkable and unlikely coincidence if \"morally blind\" ethical traits aimed solely at survival and reproduction aligned closely with independent moral truths. So we cannot be confident that our moral beliefs accurately track objective moral truth. Consequently, realism forces us to embrace moral skepticism. Such skepticism, Street claims, is implausible. So we should reject realism and instead embrace some antirealist view that allows for rationally justified moral beliefs.\n\nDefenders of moral realism have offered two sorts of replies. One is to deny that evolved moral responses would likely diverge sharply from moral truth. According to David Copp, for example, evolution would favor moral responses that promote social peace, harmony, and cooperation. But such qualities are precisely those that lie at the core of any plausible theory of objective moral truth. So Street's alleged \"dilemma\"—deny evolution or embrace moral skepticism—is a false choice.\n\nA second response to Street is to deny that morality is as \"saturated\" with evolutionary influences as Street claims. William Fitzpatrick, for instance, argues that \"[e]ven if there is significant evolutionary influence on the content of many of our moral beliefs, it remains possible that many of our moral beliefs are arrived at partly (or in some cases wholly) through autonomous moral reflection and reasoning, just as with our mathematical, scientific and philosophical beliefs.\" The wide variability of moral codes, both across cultures and historical time periods, is difficult to explain if morality is as pervasively shaped by genetic factors as Street claims.\n\nAnother common argument evolutionary ethicists use to debunk moral realism is to claim that the success of evolutionary psychology in explaining human ethical responses makes the notion of moral truth \"explanatorily superfluous.\" If we can fully explain, for example, why parents naturally love and care for their children in purely evolutionary terms, there is no need to invoke any \"spooky\" realist moral truths to do any explanatory work. Thus, for reasons of theoretical simplicity we should not posit the existence of such truths and, instead, should explain the widely held belief in objective moral truth as \"an illusion fobbed off on us by our genes in order to get us to cooperate with one another (so that our genes survive).\"\n\nHere again the central question is whether the influence of evolution on morality is as pervasive as the critics of moral realism claim. If, as seems likely, there are important aspects of morality that cannot be explained in genetic terms, appeals to moral truth may not be explanatory fifth-wheels.\n\n\n\n\n"}
{"id": "30389209", "url": "https://en.wikipedia.org/wiki?curid=30389209", "title": "Extinction debt", "text": "Extinction debt\n\nIn ecology, extinction debt is the future extinction of species due to events in the past. The phrases dead clade walking and survival without recovery express the same idea.\n\nExtinction debt occurs because of time delays between impacts on a species, such as destruction of habitat, and the species' ultimate disappearance. For instance, long-lived trees may survive for many years even after reproduction of new trees has become impossible, and thus they may be committed to extinction. Technically, extinction debt generally refers to the \"number of species\" in an area likely to become extinct, rather than the prospects of any one species, but colloquially it refers to any occurrence of delayed extinction.\n\nExtinction debt may be local or global, but most examples are local as these are easier to observe and model. It is most likely to be found in long-lived species and species with very specific habitat requirements (specialists). Extinction debt has important implications for conservation, as it implies that species may become extinct due to past habitat destruction, even if continued impacts cease, and that current reserves may not be sufficient to maintain the species that occupy them. Interventions such as habitat restoration may reverse extinction debt.\n\n\"Immigration credit\" is the corollary to extinction debt. It refers to the number of species likely to immigrate to an area after an event such as the restoration of an ecosystem.\n\nThe term \"extinction debt\" was first used in 1994 in a paper by David Tilman, Robert May, Clarence Lehman and Martin Nowak, although Jared Diamond used the term \"relaxation time\" to describe a similar phenomenon in 1972.\n\nExtinction debt is also known by the terms \"dead clade walking\" and \"survival without recovery\" when referring to the species affected. The phrase \"dead clade walking\" was coined by David Jablonski as early as 2001 as a reference to \"Dead Man Walking\", a film whose title is based on American prison slang for a condemned prisoner's last walk to the execution chamber. \"Dead clade walking\" has since appeared in other scientists' writings about the aftermaths of mass extinctions.\n\nIn discussions of threats to biodiversity, extinction debt is analogous to the \"climate commitment\" in climate change, which states that inertia will cause the earth to continue to warm for centuries even if no more greenhouse gasses are emitted. Similarly, the current extinction may continue long after human impacts on species halt.\n\nJablonski recognized at least four patterns in the fossil record following mass extinctions:\n\nExtinction debt is caused by many of the same drivers as extinction. The most well-known drivers of extinction debt are habitat fragmentation and habitat destruction. These cause extinction debt by reducing the ability of species to persist via immigration to new habitats. Under equilibrium conditions, species may become extinct in one habitat patch, yet continues to survive because it can disperse to other patches. However, as other patches have been destroyed or rendered inaccessible due to fragmentation, this \"insurance\" effect is reduced and the species may ultimately become extinct.\n\nPollution may also cause extinction debt by reducing a species' birth rate or increasing its death rate so that its population slowly declines. Extinction debts may be caused by invasive species or by climate change.\n\nExtinction debt may also occur due to the loss of mutualist species. In New Zealand, the local extinction of several species of pollinating birds in 1870 has caused a long-term reduction in the reproduction of the shrub species \"Rhabdothamnus solandri\", which requires these birds to produce seeds. However, as the plant is slow-growing and long-lived, its populations persist.\n\nJablonski found that the extinction rate of marine invertebrates was significantly higher in the stage (major subdivision of an epoch – typically 2–10 million years' duration) following a mass extinction than in the stages preceding the mass extinction. His analysis focused on marine molluscs since they constitute the most abundant group of fossils and are therefore the least likely to produce sampling errors. Jablonski suggested that two possible explanations deserved further study:\n\nThe time to \"payoff\" of extinction debt can be very long. Islands that lost habitat at the end of the last ice age 10,000 years ago still appear to be losing species as a result. It has been shown that some bryozoans, a type of microscopic marine organism, became extinct due to the volcanic rise of the Isthmus of Panama. This event cut off the flow of nutrients from the Pacific Ocean to the Caribbean 3–4.5 million years ago. While bryozoan populations dropped severely at this time, extinction of these species took another 1–2 million years.\n\nExtinction debts incurred due to human actions have shorter timescales. Local extinction of birds from rainforest fragmentation occurs over years or decades, while plants in fragmented grasslands show debts lasting 50–100 years. Tree species in fragmented temperate forests have debts lasting 200 years or more.\n\nTilman et al. demonstrated that extinction debt could occur using a mathematical ecosystem model of species metapopulations. Metapopulations are multiple populations of a species that live in separate habitat patches or islands but interact via immigration between the patches. In this model, species persist via a balance between random local extinctions in patches and colonization of new patches. Tilman \"et al.\" used this model to predict that species would persist long after they no longer had sufficient habitat to support them. When used to estimate extinction debts of tropical tree species, the model predicted debts lasting 50–400 years.\n\nOne of the assumptions underlying the original extinction debt model was a trade-off between species' competitive ability and colonization ability. That is, a species that competes well against other species, and is more likely to become dominant in an area, is less likely to colonize new habitats due to evolutionary trade-offs. One of the implications of this assumption is that better competitors, which may even be more common than other species, are more likely to become extinct than rarer, less competitive, better dispersing species. This has been one of the more controversial components of the model, as there is little evidence for this trade-off in many ecosystems, and in many empirical studies dominant competitors were least likely species to become extinct. A later modification of the model showed that these trade-off assumptions may be relaxed, but need to exist partially, in order for the theory to work.\n\nFurther theoretical work has shown that extinction debt can occur under many different circumstances, driven by different mechanisms and under different model assumptions. The original model predicted extinction debt as a result of habitat destruction in a system of small, isolated habitats such as islands. Later models showed that extinction debt could occur in systems where habitat destruction occurs in small areas within a large area of habitat, as in slash-and-burn agriculture in forests, and could also occur due to decreased growth of species from pollutants. Predicted patterns of extinction debt differ between models, though. For instance, habitat destruction resembling slash-and-burn agriculture is thought to affect rare species rather than poor colonizers. Models that incorporate stochasticity, or random fluctuation in populations, show extinction debt occurring over different time scales than classic models.\n\nMost recently, extinction debts have been estimated through the use models derived from neutral theory. Neutral theory has very different assumptions than the metapopulation models described above. It predicts that the abundance and distribution of species can be predicted entirely through random processes, without considering the traits of individual species. As extinction debt arises in models under such different assumptions, it is robust to different kinds of models. Models derived from neutral theory have successfully predicted extinction times for a number of bird species, but perform poorly at both very small and very large spatial scales.\n\nMathematical models have also shown that extinction debt will last longer if it occurs in response to large habitat impacts (as the system will move farther from equilibrium), and if species are long-lived. Also, species just below their extinction threshold, that is, just below the population level or habitat occupancy levels required sustain their population, will have long-term extinction debts. Finally, extinction debts are predicted to last longer in landscapes with a few large patches of habitat, rather than many small ones.\n\nExtinction debt is difficult to detect and measure. Processes that drive extinction debt are inherently slow and highly variable (noisy), and it is difficult to locate or count the very small populations of near-extinct species. Because of these issues, most measures of extinction debt have a great deal of uncertainty.\n\nDue to the logistical and ethical difficulties of inciting extinction debt, there are few studies of extinction debt in controlled experiments. However, experiments of insects living on moss habitats demonstrated that extinction debt occurs after habitat destruction. In these experiments, it took 6–12 months for species to die out following the destruction of habitat.\n\nExtinction debts that reach equilibrium in relatively short time scales (years to decades) can be observed via measuring the change in species numbers in the time following an impact on habitat. For instance, in the Amazon rainforest, researchers have measured the rate at which bird species disappear after forest is cut down. As even short-term extinction debts can take years to decades to reach equilibrium, though, such studies take many years and good data are rare.\n\nMost studies of extinction debt compare species numbers with habitat patterns from the past and habitat patterns in the present. If the present populations of species are more closely related to past habitat patterns than present, extinction debt is a likely explanation. The magnitude of extinction debt (i.e., number of species likely to become extinct) can not be estimated by this method.\n\nIf one has information on species populations from the past in addition to the present, the magnitude of extinction debt can be estimated. One can use the relationship between species and habitat from the past to predict the number of species expected in the present. The difference between this estimate and the actual number of species is the extinction debt.\n\nThis method requires the assumption that in the past species and their habitat were in equilibrium, which is often unknown. Also, a common relationship used to equate habitat and species number is the species-area curve, but as the species-area curve arises from very different mechanisms than those in metapopulation based models, extinction debts measured in this way may not conform with metapopulation models' predictions. The relationship between habitat and species number can also be represented by much more complex models that simulate the behavior of many species independently.\n\nIf data on past species numbers or habitat are not available, species debt can also be estimated by comparing two different habitats: one which is mostly intact, and another which has had areas cleared and is smaller and more fragmented. One can then measure the relationship of species with the condition of habitat in the intact habitat, and, assuming this represents equilibrium, use it to predict the number of species in the cleared habitat. If this prediction is lower than the actual number of species in the cleared habitat, then the difference represents extinction debt. This method requires many of the same assumptions as methods comparing the past and present.\n\nStudies of European grasslands show evidence of extinction debt through both comparisons with the past and between present-day systems with different levels of human impacts. The species diversity of grasslands in Sweden appears to be a remnant of more connected landscapes present 50 to 100 years ago. In alvar grasslands in Estonia that have lost area since the 1930s, 17–70% of species are estimated to be committed to extinction. However, studies of similar grasslands in Belgium, where similar impacts have occurred, show no evidence of extinction debt. This may be due to differences in the scale of measurement or the level of specialization of grass species.\n\nForests in Vlaams-Brabant, Belgium, show evidence of extinction debt remaining from deforestation that occurred between 1775 and 1900. Detailed modeling of species behavior, based on similar forests in England that did not experience deforestation, showed that long-lived and slow-growing species were more common than equilibrium models would predict, indicating that their presence was due to lingering extinction debt.\n\nIn Sweden, some species of lichens show an extinction debt in fragments of ancient forest. However, species of lichens that are habitat generalists, rather than specialists, do not.\n\nExtinction debt has been found among species of butterflies living in the grasslands on Saaremaa and Muhu – islands off the western coast of Estonia. Butterfly species distributions on these islands are better explained by the habitat in the past than current habitats.\n\nOn the islands of the Azores Archipelago, more than 95% of native forests have been destroyed in the past 600 years. As a result, more than half of arthropods on these islands are believed to be committed to extinction, with many islands likely to lose more than 90% of species.\n\n80–90% of extinction from past deforestation in the Amazon has yet to occur, based on modeling based on species-area relationships. Local extinctions of approximately 6 species are expected in each 2500 km region by 2050 due to past deforestation. Birds in the Amazon rain forest continued to become extinct locally for 12 years following logging that broke up contiguous forest into smaller fragments. The extinction rate slowed, however, as forest regrew in the spaces in between habitat fragments.\n\nCountries in Africa are estimated to have, on average, a local extinction debt of 30% for forest-dwelling primates. That is, they are expected to have 30% of their forest primate species to become extinct in the future due to loss of forest habitat. The time scale for these extinctions has not been estimated.\n\nBased on historical species-area relationships, Hungary currently has approximately nine more species of raptors than are thought to be able to be supported by current nature reserves.\n\nThe existence of extinction debt in many different ecosystems has important implications for conservation. It implies that in the absence of further habitat destruction or other environmental impacts, many species are still likely to become extinct. Protection of existing habitats may not be sufficient to protect species from extinction. However, the long time scales of extinction debt may allow for habitat restoration in order to prevent extinction, as occurred in the slowing of extinction in Amazon forest birds above. In another example, it has been found that grizzly bears in very small reserves in the Rocky Mountains are likely to become extinct, but this finding allows the modification of reserve networks to better support their populations.\n\nThe extinction debt concept may require revision of the value of land for species conservation, as the number of species currently present in a habitat may not be a good measure of the habitat's ability to support species (see carrying capacity) in the future. As extinction debt may last longest near extinction thresholds, it may be hardest to detect the threat of extinction for species that conservation could benefit the most.\n\nEconomic analyses have shown that including extinction in management decision-making process changes decision outcomes, as the decision to destroy habitat changes conservation value in the future as well as the present. It is estimated that in Costa Rica, ongoing extinction debt may cost between $88 million and $467 million.\n\n"}
{"id": "16694145", "url": "https://en.wikipedia.org/wiki?curid=16694145", "title": "Forests in Lithuania", "text": "Forests in Lithuania\n\nForests in Lithuania cover approximately 33% of Lithuania′s territory. Of these, about 50% are publicly owned, and 30% are privately owned; the remainder is reserved for possible future privatization. The dominant species are Scots pine (\"Pinus sylvestris\") (42%) and spruce (\"Picea abies\") (22.8%). The average age of the forest stands is 53 years. The largest forest is Dainava Forest at 1,350 km.\n\n"}
{"id": "898161", "url": "https://en.wikipedia.org/wiki?curid=898161", "title": "Geopark", "text": "Geopark\n\nA geopark is a unified area that advances the protection and use of geological heritage in a sustainable way, and promotes the economic well-being of the people who live there. There are global geoparks and national geoparks.\n\nA UNESCO definition of \"global geopark\" is a unified area with a geological heritage of international significance. Geoparks use that heritage to promote awareness of key issues facing society in the context of our dynamic planet. Many geoparks promote awareness of geological hazards, including volcanoes, earthquakes and tsunamis and many help prepare disaster mitigation strategies with local communities. Geoparks embody records of past climate changes and are indicators of current climate changes as well as demonstrating a \"best practise\" approach to using renewable energy and employing the best standards of \"green tourism\". Tourism industry promotion in geoparks, as a geographically sustainable and applicable tourism model, aims to sustain, and even enhance, the geographical character of a place.\n\nGeoparks also inform about the sustainable use and need for natural resources, whether they are mined, quarried or harnessed from the surrounding environment while at the same time promoting respect for the environment and the integrity of the landscape. Geoparks are not a legislative designation though the key heritage sites within a geopark are often protected under local, regional or national legislation. The multidisciplinary nature of the concept of geopark and tourism promotion in geoparks differentiates itself from other models of sustainable tourism. In fact, sustainable tourism promotion within geoparks encompasses many of the features of sustainable tourism including geo-tourism (geo-site tourism: as a basic factor), community-based tourism and integrated rural tourism (as a vital need), ecotourism, and cultural heritage tourism.\n\nThe Global Geoparks Network (GGN) is supported by United Nations Educational, Scientific and Cultural Organization (UNESCO). Many national geoparks and other local geoparks projects also exist which are not included in the Global Geoparks Network.\n\nThe geoparks initiative was launched by UNESCO in response to the perceived need for an international initiative that recognizes sites representing an earth science interest. Global Geoparks Network aims at enhancing the value of such sites while at the same time creating employment and promoting regional economic development. The Global Geoparks Network works in synergy with UNESCO's World Heritage Centre and Man and the Biosphere (MAB) World Network of Biosphere Reserves.\n\nThe Global Geoparks Network (GGN) is a UNESCO activity established in 1998. According to UNESCO, for a geopark to apply to be included in the GGN, it needs to:\n\n\nSee Members of the Global Geoparks Network.\n\n\n"}
{"id": "1849366", "url": "https://en.wikipedia.org/wiki?curid=1849366", "title": "IEC 60870", "text": "IEC 60870\n\nIn electrical engineering and power system automation, the International Electrotechnical Commission 60870 standards define systems used for telecontrol (supervisory control and data acquisition). Such systems are used for controlling electric power transmission grids and other geographically widespread control systems. By use of standardized protocols, equipment from many different suppliers can be made to interoperate. IEC standard 60870 has six parts, defining general information related to the standard, operating conditions, electrical interfaces, performance requirements, and data transmission protocols. The 60870 standards are developed by IEC Technical Committee 57 (Working Group 03).\n\n\nIEC 60870 part 5, known as Transmission protocols, provides a communication profile for sending basic telecontrol messages between two systems, which uses permanent directly connected data circuits between the systems. The IEC TC 57 WG3 have developed a protocol standard for telecontrol, teleprotection, and associated telecommunications for electric power systems. The result of this work is IEC 60870-5. Five documents specify the base IEC 60870-5:\n\nThe IEC TC 57 has also generated companion standards:\n\nIEC 60870-5-101/102/103/104 are companion standards generated for basic telecontrol tasks, transmission of integrated totals, data exchange from protection equipment & network access of IEC101 respectively.\n\nIEC TC 57 WG3 also generated standards for telecontrol protocols compatible with ISO standards and ITU-T recommendations. These standards include:\n"}
{"id": "7982164", "url": "https://en.wikipedia.org/wiki?curid=7982164", "title": "Intermittent energy source", "text": "Intermittent energy source\n\nAn intermittent energy source is any source of energy that is not continuously available for conversion into electricity and outside direct control because the used primary energy cannot be stored. Intermittent energy sources may be predictable but cannot be dispatched to meet the demand of an electric power system.\n\nThe use of intermittent sources in an electric power system usually displaces storable primary energy that would otherwise be consumed by other power stations. Another option is to store electricity generated by non-dispatchable energy sources for later use when needed, e.g. in the form of pumped storage, compressed air or in batteries. A third option is the sector coupling e.g. by electric heating for district heating schemes.\n\nThe use of small amounts of intermittent power has little effect on grid operations. Using larger amounts of intermittent power may require upgrades or even a redesign of the grid infrastructure.\n\nSeveral key terms are useful for understanding the issue of intermittent power sources. These terms are not standardized, and variations may be used. Most of these terms also apply to traditional power plants.\n\n\nIntermittency inherently affects solar energy, as the production of renewable electricity from solar sources depends on the amount of sunlight at a given place and time. Solar output varies throughout the day and through the seasons, and is affected by dust, fog, cloud cover, frost or snow. Many of the seasonal factors are fairly predictable, and some solar thermal systems make use of heat storage to produce grid power for a full day.\n\n\nThe impact of intermittency of solar-generated electricity will depend on the correlation of generation with demand. For example, solar thermal power plants such as Nevada Solar One are somewhat matched to summer peak loads in areas with significant cooling demands, such as the south-western United States. Thermal energy storage systems like the small Spanish Gemasolar Thermosolar Plant can improve the match between solar supply and local consumption. The improved capacity factor using thermal storage represents a decrease in maximum capacity, and extends the total time the system generates power.\n\nWind-generated power is a variable resource, and the amount of electricity produced at any given point in time by a given plant will depend on wind speeds, air density, and turbine characteristics (among other factors). If wind speed is too low (less than about 2.5 m/s) then the wind turbines will not be able to make electricity, and if it is too high (more than about 25 m/s) the turbines will have to be shut down to avoid damage. While the output from a single turbine can vary greatly and rapidly as local wind speeds vary, as more turbines are connected over larger and larger areas the average power output becomes less variable.\n\n\nAccording to a 2007 study of wind in the United States, ten or more widely separated wind farms connected through the grid could be relied upon for from 33 to 47% of their average output (15–20% of nominal capacity) as reliable, baseload power, as long as minimum criteria are met for wind speed and turbine height. When calculating the generating capacity available to meet summer peak demand, ERCOT (manages Texas grid) counts wind generation at 8.7% of nameplate capacity.\n\nWind generates about 16% (EWEA – 2011 European Statistics, February 2012) of electric energy in Spain and Portugal, 9% in Ireland, and 7% in Germany. Wind provides around 40% of the annual electricity generated in Denmark (up from 20% in 2005); to meet this percentage Denmark exports surpluses and imports during shortfalls to and from the EU grid, particularly Norwegian Hydro, to balance supply with demand. \n\nBecause wind power is generated by large numbers of small generators, individual failures do not have large impacts on power grids. This feature of wind has been referred to as resiliency.\n\nWind power is affected by air temperature because colder air is more dense and therefore more effective at producing wind power. As a result, wind power is affected seasonally (more output in winter than summer) and by daily temperature variations. During the 2006 California heat wave output from wind power in California significantly decreased to an average of 4% of capacity for seven days. A similar result was seen during the 2003 European heat wave, when the output of wind power in France, Germany, and Spain fell below 10% during peak demand times. Heat waves are partially caused by large amounts of solar radiation.\nAccording to an article in EnergyPulse, \"the development and expansion of well-functioning day-ahead and real time markets will provide an effective means of dealing with the variability of wind generation.\"\n\nSeveral authors have said that no energy resource is totally reliable. Amory Lovins says that nuclear power plants are intermittent in that they will sometimes fail unexpectedly, often for long periods of time. For example, in the United States, 132 nuclear plants were built, and 21% were permanently and prematurely closed due to reliability or cost problems, while another 27% have at least once completely failed for a year or more. The remaining U.S. nuclear plants produce approximately 90% of their full-time full-load potential, but even they must shut down (on average) for 39 days every 17 months for scheduled refueling and maintenance. To cope with such intermittence by nuclear (and centralized fossil-fuelled) power plants, utilities install a \"reserve margin\" of roughly 15% extra capacity spinning ready for instant use.\n\nThe penetration of intermittent renewables in most power grids is low, global electricity production in 2014 was supplied by 3.1% wind, and 1% solar. Wind generates roughly 16% of electric energy in Spain and Portugal, 15.3% in Ireland, and 7% in Germany. , wind provides 39% of the electricity generated in Denmark. To operate with this level of penetration, Denmark exports surpluses and imports during shortfalls to and from neighbouring countries, particularly hydroelectric power from Norway, to balance supply with demand. It also uses large numbers of combined heat and power (CHP) stations which can rapidly adjust output.\n\nThe intermittency and variability of renewable energy sources can be reduced and accommodated by diversifying their technology type and geographical location, forecasting their variation, and integrating them with dispatchable renewables (such as hydropower, geothermal, and biomass). Combining this with energy storage and demand response can create a power system that can reliably match real-time energy demand. The integration of ever-higher levels of renewables has already been successfully demonstrated:\nIn 2009, eight American and three European authorities, writing in the leading electrical engineers' professional journal, didn't find \"a credible and firm technical limit to the amount of wind energy that can be accommodated by electricity grids\". In fact, not one of more than 200 international studies, nor official studies for the eastern and western U.S. regions, nor the International Energy Agency, has found major costs or technical barriers to reliably integrating up to 30% variable renewable supplies into the grid, and in some studies much more.\nA research group at Harvard University quantified the meteorologically defined limits to reduction in the variability of outputs from a coupled wind farm system in the Central US:\nThe problem with the output from a single wind farm located in any particular region is that it is variable on time scales ranging from minutes to days posing difficulties for incorporating relevant outputs into an integrated power system. The high frequency (shorter than once per day) variability of contributions from individual wind farms is determined mainly by locally generated small scale boundary layer. The low frequency variability (longer than once per day) is associated with the passage of transient waves in the atmosphere with a characteristic time scale of several days. The high frequency variability of wind-generated power can be significantly reduced by coupling outputs from 5 to 10 wind farms distributed uniformly over a ten state region of the Central US. More than 95% of the remaining variability of the coupled system is concentrated at time scales longer than a day, allowing operators to take advantage of multi-day weather forecasts in scheduling projected contributions from wind.\nMark Z. Jacobson has studied how wind, water and solar technologies can be integrated to provide the majority of the world's energy needs. He advocates a \"smart mix\" of renewable energy sources to reliably meet electricity demand:\nBecause the wind blows during stormy conditions when the sun does not shine and the sun often shines on calm days with little wind, combining wind and solar can go a long way toward meeting demand, especially when geothermal provides a steady base and hydroelectric can be called on to fill in the gaps.\nMark A. Delucchi and Mark Z. Jacobson argue that there are at least seven ways to design and operate renewable energy systems so that they will reliably satisfy electricity demand:\nTechnological solutions to mitigate large-scale wind energy type intermittency exist such as increased interconnection (the European super grid), Demand response, load management, diesel generators (in the British National Grid, Frequency Response / National Grid Reserve Service type schemes, and use of existing power stations on standby. Studies by academics and grid operators indicate that the cost of compensating for intermittency is expected to be high at levels of penetration above the low levels currently in use today Large, distributed power grids are better able to deal with high levels of penetration than small, isolated grids. For a hypothetical European-wide power grid, analysis has shown that wind energy penetration levels as high as 70% are viable, and that the cost of the extra transmission lines would be only around 10% of the turbine cost, yielding power at around present day prices. Smaller grids may be less tolerant to high levels of penetration.\n\nMatching power demand to supply is not a problem specific to intermittent power sources. Existing power grids already contain elements of uncertainty including sudden and large changes in demand and unforeseen power plant failures. Though power grids are already designed to have some capacity in excess of projected peak demand to deal with these problems, significant upgrades may be required to accommodate large amounts of intermittent power. The International Energy Agency (IEA) states:\n\"In the case of wind power, operational reserve is the additional generating reserve needed to ensure that differences between forecast and actual volumes of generation and demand can be met. Again, it has to be noted that already significant amounts of this reserve are operating on the grid due to the general safety and quality demands of the grid. Wind imposes additional demands only inasmuch as it increases variability and unpredictability. However, these factors are nothing completely new to system operators. By adding another variable, wind power changes the degree of uncertainty, but not the kind...\"\n\nWith sufficient energy storage, highly variable and intermittent sources can supply all of a regions electrical power. For solar to provide half of all electricity and using a solar capacity factor of 20%, the total capacity for solar would be 250% of the grids average daily load. For wind to provide half of all electricity and using a wind capacity factor of 30% the total capacity for wind would be 160% of the grids average daily load.\n\nA pumped storage facility would then store enough water for the grids weekly load, with a capacity for peak demand i.e.:200% of the grid average. This would allow for one week of overcast and windless conditions. There are unusual costs associated with building storage and total generating capacity being six times the grid average.\n\nAll sources of electrical power have some degree of variability, as do demand patterns which routinely drive large swings in the amount of electricity that suppliers feed into the grid. Wherever possible, grid operations procedures are designed to match supply with demand at high levels of reliability, and the tools to influence supply and demand are well-developed. The introduction of large amounts of highly variable power generation may require changes to existing procedures and additional investments.\n\nThe capacity of a reliable renewable power supply, can be fulfilled by the use of backup or extra infrastructure and technology, using mixed renewables to produce electricity above the intermittent average, which may be used to meet regular and unanticipated supply demands. Additionally, the storage of energy to fill the shortfall intermittency or for emergencies can be part of a reliable power supply.\n\nAll managed grids already have existing operational and \"spinning\" reserve to compensate for existing uncertainties in the power grid. The addition of intermittent resources such as wind does not require 100% \"back-up\" because operating reserves and balancing requirements are calculated on a system-wide basis, and not dedicated to a specific generating plant.\n\n\n\nAt times of low load where non-dispatchable output from wind and solar may be high, grid stability requires lowering the output of various dispatchable generating sources or even increasing controllable loads, possibly by using energy storage to time-shift output to times of higher demand. Such mechanisms can include:\n\n\nStorage of electrical energy results in some lost energy because storage and retrieval are not perfectly efficient. Storage may also require substantial capital investment and space for storage facilities.\n\nThe variability of production from a single wind turbine can be high. Combining any additional number of turbines (for example, in a wind farm) results in lower statistical variation, as long as the correlation between the output of each turbine is imperfect, and the correlations are always imperfect due to the distance between each turbine. Similarly, geographically distant wind turbines or wind farms have lower correlations, reducing overall variability. Since wind power is dependent on weather systems, there is a limit to the benefit of this geographic diversity for any power system.\n\nMultiple wind farms spread over a wide geographic area and gridded together produce power more constantly and with less variability than smaller installations. Wind output can be predicted with some degree of confidence using weather forecasts, especially from large numbers of turbines/farms. The ability to predict wind output is expected to increase over time as data is collected, especially from newer facilities.\n\nIn the past electrical generation was mostly dispatchable and consumer demand led how much and when to dispatch power. The trend in adding intermittent sources such as wind, solar, and run-of-river hydro means the grid is beginning to be led by the intermittent supply. The use of intermittent sources relies on electric power grids that are carefully managed, for instance using highly dispatchable generation that is able to shut itself down whenever an intermittent source starts to generate power, and to successfully startup without warning when the intermittents stop generating. Ideally the capacity of the intermittents would grow to be larger than consumer demand for periods of time, creating excess low price electricity to displace heating fuels or be converted to mechanical or chemical storage for later use.\n\nThe displaced dispatchable generation could be coal, natural gas, biomass, nuclear, geothermal or storage hydro. Rather than starting and stopping nuclear or geothermal it is cheaper to use them as constant base load power. Any power generated in excess of demand can displace heating fuels, be converted to storage or sold to another grid. Biofuels and conventional hydro can be saved for later when intermittents are not generating power. Alternatives to burning coal and natural gas which produce fewer greenhouse gases may eventually make fossil fuels a stranded asset that is left in the ground. Highly integrated grids favor flexibility and performance over cost, resulting in more plants that operate for fewer hours and lower capacity factors.\n\n\nPenetration refers to the proportion of a primary energy (PE) source in an electric power system, expressed as a percentage. There are several methods of calculation yielding different penetrations. The penetration can be calculated either as:\n\n\nThe level of penetration of intermittent variable sources is significant for the following reasons:\n\n\nRenewable electricity supply in the 20-50+% penetration range has already been implemented in several European systems, albeit in the context of an integrated European grid system:\nIn 2010, four German states, totaling 10 million people, relied on wind power for 43-52% of their annual electricity needs. Denmark isn't far behind, supplying 22% of its power from wind in 2010 (26% in an average wind year). The Extremadura region of Spain is getting up to 25% of its electricity from solar, while the whole country meets 16% of its demand from wind. Just during 2005-2010, Portugal vaulted from 17% to 45% renewable electricity.\nThere is no generally accepted maximum level of penetration, as each system's capacity to compensate for intermittency differs, and the systems themselves will change over time. Discussion of acceptable or unacceptable penetration figures should be treated and used with caution, as the relevance or significance will be highly dependent on local factors, grid structure and management, and existing generation capacity.\n\nFor most systems worldwide, existing penetration levels are significantly lower than practical or theoretical maximums; for example, a UK study found that \"it is clear that intermittent generation need not compromise electricity system reliability at any level of penetration foreseeable in Britain over the next 20 years, although it may increase costs.\"\n\nThere is no generally accepted maximum penetration of wind energy that would be feasible in any given grid. Rather, economic efficiency and cost considerations are more likely to dominate as critical factors; technical solutions may allow higher penetration levels to be considered in future, particularly if cost considerations are secondary.\n\nHigh penetration scenarios may be feasible in certain circumstances:\n\n\nStudies have been conducted to assess the viability of specific penetration levels in specific energy markets.\n\nA series of detailed modelling studies by Dr. Gregor Czisch, which looked at the European wide adoption of renewable energy and interlinking power grids the European super grid using HVDC cables, indicates that the entire European power usage could come from renewables, with 70% total energy from wind at the same sort of costs or lower than at present. This proposed large European power grid has been called a \"super grid.\"\n\nThe model deals with intermittent power issues by using base-load renewables such as hydroelectric and biomass for a substantial portion of the remaining 30% and by heavy use of HVDC to shift power from windy areas to non-windy areas. The report states that \"electricity transport proves to be one of the keys to an economical electricity supply\" and underscores the importance of \"international co-operation in the field of renewable energy use [and] transmission.\"\n\nDr. Czisch described the concept in an interview, saying \"For example, if we look at wind energy in Europe. We have a winter wind region where the maximum production is in winter and in the Sahara region in northern Africa the highest wind production is in the summer and if you combine both, you come quite close to the needs of the people living in the whole area - let's say from northern Russia down to the southern part of the Sahara.\"\n\nA study of the grid in Ireland indicates that it would be feasible to accommodate 42% (of demand) renewables\nin the electricity mix. This acceptable level of renewable penetration was found in what the study called Scenario 5, provided 47% of electrical capacity (different from demand) with the following mix of renewable energies:\n\n\nThe study cautions that various assumptions were made that \"may have understated dispatch restrictions, resulting in an underestimation of operational costs, required wind curtailment, and CO emissions\" and that \"The limitations of the study may overstate the technical feasibility of the portfolios analyzed...\"\n\nScenario 6, which proposed renewables providing 59% of electrical capacity and 54% of demand had problems. Scenario 6 proposed the following mix of renewable energies:\n\n\nThe study found that for Scenario 6, \"a significant number of hours characterized by extreme system situations occurred where load and reserve requirements could not be met. The results of the network study indicated that for such extreme renewable penetration scenarios, a system re-design is required, rather than a reinforcement exercise.\" The study declined to analyze the cost effectiveness of the required changes because \"determination of costs and benefits had become extremely dependent on the assumptions made\" and this uncertainty would have impacted the robustness of the results.\n\nA study published in October 2006, by the Ontario Independent Electric System Operator (IESO) found that \"there would be minimal system operation impacts for levels of wind capacity up to 5,000 MW,\" which corresponds to a peak penetration of 17%\n\nA November 2006 analysis, found that \"wind power may be able to cover more than 50% of the Danish electricity consumption in 2025\" under conditions of high oil prices and higher costs for CO allowances. Denmark's two grids (covering West Denmark and East Denmark separately) each incorporate high-capacity interconnectors to neighbouring grids where some of the variations from wind are absorbed. In 2012 the Danish government adopted a plan to increase the share of electricity production from wind to 50% by 2020, and to 84% in 2035.\n\nEstimates of the cost of wind energy may include estimates of the \"external\" costs of wind variability, or be limited to the cost of production. All electrical plant has costs that are separate from the cost of production, including, for example, the cost of any necessary transmission capacity or reserve capacity in case of loss of generating capacity. Many types of generation, particularly fossil fuel derived, will also have cost externalities such as pollution, greenhouse gas emission, and habitat destruction which are generally not directly accounted for. The magnitude of the economic impacts is debated and will vary by location, but is expected to rise with higher penetration levels. At low penetration levels, costs such as operating reserve and balancing costs are believed to be insignificant.\n\nIntermittency may introduce additional costs that are distinct from or of a different magnitude than for traditional generation types. These may include:\n\n\nStudies have been performed to determine the costs of variability. RenewableUK states:\n\nAn official at Xcel Energy claimed that at 20 percent penetration, additional standby generators to compensate for wind in Colorado would cost $8 per MWh, adding between 13% and 16% to the US$50–60 cost per MWh of wind energy.\n\nThe Union of Concerned Scientists conducted a study of the costs to increase the renewable penetration in Colorado to 10% and found that for an average residential bill \"customers of municipal utilities and rural electric cooperatives that opt out of the solar energy requirement\" would save 4 cents per month, but that for Xcel Energy customers there would be additional cost of about 10 cents per month. Total impact on all consumers would be $4.5 million or 0.01% over two decades.\n\nA detailed study for UK National Grid (a private power company) states \"We have estimated that for the case with 8,000 MW of wind needed to meet the 10% renewables target for 2010, balancing costs can be expected to increase by around £2 per MWh of wind production. This would represent an additional £40million per annum, just over 10% of existing annual balancing costs.\"\n\nIn evidence to the UK House of Lords Economic Affairs Select Committee, National Grid have quoted estimates of balancing costs for 40% wind and these lie in the range £500-1000M per annum. \"These balancing costs represent an additional £6 to £12 per annum on average consumer electricity bill of around £390.\"\n\nNational Grid notes that \"increasing levels of such renewable generation on the system would increase the costs of balancing the system and managing system frequency.\"\n\nA 2003 report, by Carbon Trust and the UK Department of Trade and Industry (DTI), projected costs of £1.6 to £2.4 billion for reinforcement and new build of transmission and distribution systems to support 10% renewable electricity in the UK by 2010, and £3.2bn to £4.5bn for 20% by 2020. The study classified \"Intermittency\" as \"Not a significant issue\" for the 2010 target but a \"Significant Issue\" for the 2020 target. \"See grid balancing\"\n\nA Minnesota study on wind penetration levels and found that \"total integration operating cost for up to 25% wind energy\" would be less than $0.0045 per kWh (additional).\n\nThere are differing views about some sources of renewable energy and intermittency. The World Nuclear Association argues that the sun, wind, tides and waves cannot be controlled to provide directly either continuous base-load power, or peak-load power when it is needed. Proponents of renewable energy use argue that the issue of intermittency of renewables is over-stated, and that practical experience demonstrates this. In any case, geothermal renewable energy has, like nuclear, no intermittency (but they both use the energy in radioactive materials like uranium, thorium and potassium).\n\nFor many years there was a consensus within the electric utilities in the U.S. that renewable electricity generators such as wind and solar are so unreliable and intermittent that they will never be able to contribute significantly to electric supply or provide baseload power. Thomas Petersnik, an analyst with the U.S. Energy Information Administration put it this way: \"by and large, renewable energy sources are too rare, too distant, too uncertain, and too ill-timed to provide significant supplies at times and places of need\".\nAccording to a transatlantic collaborative research paper on Energy return on energy Invested(EROEI), conducted by 6 analysts and led by D. Weißbach, as published in the peer reviewed journal \"Energy\" in 2013. The uncorrected for their intermittency(\"unbuffered\") EROEI for each energy source analyzed is as depicted in the attached table at right, while the buffered(corrected for their intermittency) EROEI stated in the paper for all low carbon power sources, with the exception of nuclear and biomass, were yet lower still. As when corrected for their weather intermittency/\"buffered\", the EROEI figures for intermittent energy sources as stated in the paper is diminished - a reduction of EROEI dependent on how reliant they are on back up energy sources.\n\nThe U.S. Federal Energy Regulatory Commission (FERC) Chairman Jon Wellinghoff has stated that \"baseload capacity is going to become an anachronism\" and that no new nuclear or coal plants may ever be needed in the United States. Some renewable electricity sources have identical variability to coal-fired power stations, so they are base-load, and can be integrated into the electricity supply system without any additional back-up. Examples include:\n\n\nGrid operators in countries like Denmark and Spain now integrate large quantities of renewable energy into their electricity grids, with Denmark receiving 40% of its electricity from wind power during some months.\n\nSupporters say that the total electricity generated from a large-scale array of dispersed wind farms, located in different wind regimes, cannot be accurately described as intermittent, because it does not start up or switch off instantaneously at irregular intervals. With a small amount of supplementary peak-load plant, which operates infrequently, large-scale distributed wind power can substitute for some base-load power and be equally reliable.\n\nHydropower can be intermittent and/or dispatchable, depending on the configuration of the plant. Typical hydroelectric plants in the dam configuration may have substantial storage capacity, and be considered dispatchable. Run of the river hydroelectric generation will typically have limited or no storage capacity, and will be variable on a seasonal or annual basis (dependent on rainfall and snow melt).\n\nAmory Lovins suggests a few basic strategies to deal with these issues:\n\nMoreover, efficient energy use and energy conservation measures can reliably reduce demand for base-load and peak-load electricity.\n\nInternational groups are studying much higher penetrations (30-100% renewable energy), and conclusions are that these levels are also technically feasible. In the UK, one summary of other studies indicated that if assuming that wind power contributed less than 20% of UK power consumption, then the intermittency would cause only moderate cost.\n\nMethods to manage wind power integration range from those that are commonly used at present (e.g. demand management) to potential new technologies for grid energy storage. Improved forecasting can also contribute as the daily and seasonal variations in wind and solar sources are to some extent predictable. The Pembina Institute and the World Wide Fund for Nature state in the Renewable is Doable plan that resilience is a feature of renewable energy:\n\n\nThese peer-reviewed papers examine the impacts of intermittency:\n\n\n"}
{"id": "35305141", "url": "https://en.wikipedia.org/wiki?curid=35305141", "title": "International communication", "text": "International communication\n\nInternational communication (also referred to as the \"study of global communication\" or transnational communication) is the communication practice that occurs across international borders. The need for international communication was due to the increasing effects and influences of globalization. As a field of study, international communication is a branch of communication studies, concerned with the scope of \"government-to-government\", \"business-to-business\", and \"people-to-people\" interactions at a global level. Currently, international communication is being taught at colleges across the United States. Due to the increasingly globalized market, employees who possess the ability to effectively communicate across cultures are in high demand. International communication \"encompasses political, economic, social, cultural and military concerns\".\n\nEfficient communication networks played crucial roles in establishing ancient imperial authority and international trade. The extent of empire could be used as an 'indication of the efficiency of communication'. Ancient empires such as Rome, Persia and China, all utilized writing in collecting information and dispersing, creating enormous postal and dispatch systems. As early as in fifteenth century, news had been disseminated trans-nationally in Europe. 'The wheat traders of Venice, the silver traders of Antwerp, the merchants of Nuremberg and their trading partners shared economic newsletters and created common values and beliefs in the rights of capital.'\n\nIn 1837, Samuel Morse invented telegraph. Given its speed and reliability in delivering information, telegraph offered opportunities for capital and military expansion. As showed in Table 1.1, the establishment of cable hardware signifies global power order in late nineteenth and early twentieth century.\n\nTable 1.1 Cabling the world\nThe newspaper industry and international telegraph networks mutually facilitated each other. As the supply and demand of newspaper industry rapidly increased in nineteenth century, news agencies were established successively. The French Havas Agency was founded in 1835, the German agency Wolffin 1849 and the British Reuters in 1851. These three European agencies, which started to operate internationally, were all subsidized by their respective governments.\n\nWestern countries seized the chances to implement radio communication after the first radio transmissions of human voice in 1902. But the two mechanisms of radio broadcasting were distinctively different. In the USA, the Radio Act of 1927 confirm its status as an advertising-funded commercial enterprise, while in Britain, the public broadcasting pioneer British Broadcasting Corporation set up in the same year. During the First World War and the Second World War, radio broadcasting played a significant role in both domestic public opinion management and international diplomacy propaganda abroad. Even in the Cold War times, this radio-dominated international communication still featured in propaganda respective ideologies. The prominent example is the Voice of America, which ran a global network to indoctrinate \"American dream\" to its international audience.\n\nSince the cold war officially ended in 1990, the intense relations of super powers halted with the collapse of the Soviet Union, and the emergence of the Third World countries, the unequally developed communication order can no longer exist. The Third World called for ceasing their marginalized communication status. Especially when international communications stepped into the information age, 'the convergence of telecommunication and computing and the ability to move all type of data – pictures, words, sounds – via the Internet have revolutionized international information exchange.'\n\nWhen communicating internationally it is important to take culture into consideration. Though English has become the language of business, many businesses fail to recognize that the language used does not determine how business is conducted. Therefore, it is important to understand that intercultural and international communication are interchangeable.\n\nAs a tourist it may be acceptable to maintain the cultural norms from a country of origin when visiting, though attempting to adapt would be appreciated. However, when conducting business it is important to recognize cultural differences, especially when communicating. At the turn of the century there was a large amount of research based on the needs of those that travel abroad in order to commercialize products or services. The list of researchers includes Hofstede, 1991; Storti, 1994; Ansari & Jackson, 1995; Cushner & Brislin, 1996; Adler, 1997; Mead, 1998; and Marx, 1999. From those studies Gibson's volume becomes an important source of information for business professionals interested in succeeding internationally. As explained by Douglas Storey, there was a change in style and strategy of American diplomacy since 1979 after the first addition of Glen Fisher's book appeared.\n\nDespite the reason for international communication it is important to understand that international communication is not limited to the language spoken during communication.\n\nInternational communication is widely spread and multilayered in contemporary society, however it is not considered as a separate academic discipline because of its overlapping with other subjects. International communication is 'a topic field rather than a discipline field' and international communication studies is a mode of 'organizing inquiry'.\n\nJohn D. H. Downing proposed ten categories within which international communication should be conducted\n\nMehdi Semati listed the wide range of research subjects in international communication, which includes, but not limited to the following.\n\nHamid Mowlana stated four key interrelated approaches to international communication\n\nOne of the most obvious manifestations of international communication are world news, when the media of one country cover news from abroad. But, apart from journalism, international communication also occurs in other areas (culture, technology, sciences) and the nature of the \"information\" that is circulated can be classified in a wide variety of categories, such as cultural (music, films, sports, TV shows from one country to another), scientific (research papers published abroad, scientific exchange or cooperation), and intelligence (diplomacy reports, international espionage, etc.).\n\nTypically the study of international communication includes a deep attention to the circulation of news among different countries (and the resulting imbalances, from which came the concept of news flow), the power of media organizations (such as conglomerates and news agencies), issues such as cultural imperialism and media imperialism, and the political role that international cooperation can have in enhancing the media industry (and society as a whole) in a given region, such as proposed by development communication or communication for development.\n\nSome renowned scholars in international communication include Wilbur Schramm, Ithiel de Sola Pool, Johan Galtung, Anthony Smith, Robert Stevenson, Jeremy Tunstall, Armand Mattelart, Oliver Boyd-Barrett, Ali Mohammadi, Annabelle Sreberny, Cees J. Hamelink, Daya Kishan Thussu and Chris Paterson. The \"International Communication Gazette\" and the \"Journal of International Communication\" are reference journals in this field.\n\n\n"}
{"id": "5024020", "url": "https://en.wikipedia.org/wiki?curid=5024020", "title": "Inverse Faraday effect", "text": "Inverse Faraday effect\n\nThe inverse Faraday effect is the effect opposite to the Faraday effect. A static magnetization formula_1 is induced by an external oscillating electrical field with the frequency formula_2, which can be achieved with a high intensity laser pulse for example. The induced magnetization is proportional to the vector product of formula_3 and formula_4:\n\nformula_5\n\nFrom this equation we see that the circularly polarized light with the frequency formula_2 should induce a magnetization along the wave vector formula_7. Because formula_3 is in the vector product, left- and right-handed polarization waves should induce magnetization of opposite signs. \n\nThe induced magnetization is comparable to the saturated magnetization of the media.\n\n"}
{"id": "18963787", "url": "https://en.wikipedia.org/wiki?curid=18963787", "title": "Ion", "text": "Ion\n\nAn ion () is an atom or molecule that has a non-zero net electrical charge. Since the charge of the electron (considered \"negative\" by convention) is equal and opposite to that of the proton (considered \"positive\" by convention), the net charge of an ion is non-zero due to its total number of electrons being unequal to its total number of protons. A cation is a positively charged ion, with fewer electrons than protons, while an anion is negatively charged, with more electrons than protons. Because of their opposite electric currents, cations and anions attract each other and readily form ionic compounds. \n\nIons consisting of only a single atom are termed atomic or monatomic ions, while two or more atoms form molecular ions or polyatomic ions. In the case of physical ionization in a medium, such as a gas, \"ion pairs\" are created by ion collisions, where each generated pair consists of a free electron and a positive ion. Ions are also created by chemical interactions, such as the dissolution of a salt in liquids, or by other means, such as passing a direct current through a conducting solution, dissolving an anode via ionization.\n\nThe word \"ion\" comes from the Greek word ἰόν, \"ion\", \"going\", the present participle of ἰέναι, \"ienai\", \"to go\". This term was introduced by English physicist and chemist Michael Faraday in 1834 for the then-unknown species that \"goes\" from one electrode to the other through an aqueous medium. Faraday did not know the nature of these species, but he knew that since metals dissolved into and entered a solution at one electrode and new metal came forth from a solution at the other electrode; that some kind of substance has moved through the solution in a current. This conveys matter from one place to the other.\n\nFaraday also introduced the words \"anion\" for a negatively charged ion, and \"cation\" for a positively charged one. In Faraday's nomenclature, cations were named because they were attracted to the cathode in a galvanic device and anions were named due to their attraction to the anode.\n\nSvante Arrhenius put forth, in his 1884 dissertation, his explanation of the fact that solid crystalline salts dissociate into paired charged particles when dissolved, for which he would win the 1903 Nobel Prize in Chemistry. Arrhenius' explanation was that in forming a solution, the salt dissociates into Faraday's ions. Arrhenius proposed that ions formed even in the absence of an electric current.\n\nIons in their gas-like state are highly reactive and will rapidly interact with ions of opposite charge to give neutral molecules or ionic salts. Ions are also produced in the liquid or solid state when salts interact with solvents (for example, water) to produce \"solvated ions\", which are more stable, for reasons involving a combination of energy and entropy changes as the ions move away from each other to interact with the liquid. These stabilized species are more commonly found in the environment at low temperatures. A common example is the ions present in seawater, which are derived from dissolved salts.\n\nAs charged objects, ions are attracted to opposite electric charges (positive to negative, and vice versa) and repelled by like charges. When they move, their trajectories can be deflected by a magnetic field.\n\nElectrons, due to their smaller mass and thus larger space-filling properties as matter waves, determine the size of atoms and molecules that possess any electrons at all. Thus, anions (negatively charged ions) are larger than the parent molecule or atom, as the excess electron(s) repel each other and add to the physical size of the ion, because its size is determined by its electron cloud. Cations are smaller than the corresponding parent atom or molecule due to the smaller size of the electron cloud. One particular cation (that of hydrogen) contains no electrons, and thus consists of a single proton - \"very much smaller\" than the parent hydrogen atom.\n\nSince the electric charge on a proton is equal in magnitude to the charge on an electron, the net electric charge on an ion is equal to the number of protons in the ion minus the number of electrons.\n\nAn (−) (), from the Greek word ἄνω (\"ánō\"), meaning \"up\", is an ion with more electrons than protons, giving it a net negative charge (since electrons are negatively charged and protons are positively charged).\n\nA (+) (), from the Greek word κάτω (\"káto\"), meaning \"down\", is an ion with fewer electrons than protons, giving it a positive charge.\n\nThere are additional names used for ions with multiple charges. For example, an ion with a −2 charge is known as a dianion and an ion with a +2 charge is known as a dication. A zwitterion is a neutral molecule with positive and negative charges at different locations within that molecule.\n\nCations and anions are measured by their ionic radius and they differ in relative size: \"Cations are small, most of them less than 10 m (10 cm) in radius. But most anions are large, as is the most common Earth anion, oxygen. From this fact it is apparent that most of the space of a crystal is occupied by the anion and that the cations fit into the spaces between them.\"\n\nA cation has radius less than 0.8 × 10 m (0.8 Å) while an anion has radius greater than 1.3 × 10 m (1.3 Å).\n\nIons are ubiquitous in nature and are responsible for diverse phenomena from the luminescence of the Sun to the existence of the Earth's ionosphere. Atoms in their ionic state may have a different colour from neutral atoms, and thus light absorption by metal ions gives the colour of gemstones. In both inorganic and organic chemistry (including biochemistry), the interaction of water and ions is extremely important; an example is the energy that drives breakdown of adenosine triphosphate (ATP). The following sections describe contexts in which ions feature prominently; these are arranged in decreasing physical length-scale, from the astronomical to the microscopic.\n\nIons can be non-chemically prepared using various ion sources, usually involving high voltage or temperature. These are used in a multitude of devices such as mass spectrometers, optical emission spectrometers, particle accelerators, ion implanters, and ion engines.\n\nAs reactive charged particles, they are also used in air purification by disrupting microbes, and in household items such as smoke detectors.\n\nAs signalling and metabolism in organisms are controlled by a precise ionic gradient across membranes, the disruption of this gradient contributes to cell death. This is a common mechanism exploited by natural and artificial biocides, including the ion channels gramicidin and amphotericin (a fungicide).\n\nInorganic dissolved ions are a component of total dissolved solids, a widely-known indicator of water quality.\n\nThe ionizing effect of radiation on a gas is extensively used for the detection of radiation such as alpha, beta, gamma and X-rays. The original ionization event in these instruments results in the formation of an \"ion pair\"; a positive ion and a free electron, by ion impact by the radiation on the gas molecules. The ionization chamber is the simplest of these detectors, and collects all the charges created by \"direct ionization\" within the gas through the application of an electric field.\n\nThe Geiger–Müller tube and the proportional counter both use a phenomenon known as a Townsend avalanche to multiply the effect of the original ionizing event by means of a cascade effect whereby the free electrons are given sufficient energy by the electric field to release further electrons by ion impact.\n\nWhen writing the chemical formula for an ion, its net charge is written in superscript immediately after the chemical structure for the molecule/atom. The net charge is written with the magnitude \"before\" the sign; that is, a doubly charged cation is indicated as 2+ instead of +2. However, the magnitude of the charge is omitted for singly charged molecules/atoms; for example, the sodium cation is indicated as Na and \"not\" Na.\n\nAn alternative (and acceptable) way of showing a molecule/atom with multiple charges is by drawing out the signs multiple times, this is often seen with transition metals. Chemists sometimes circle the sign; this is merely ornamental and does not alter the chemical meaning. All three representations of shown in the figure, are thus equivalent.\n\nMonatomic ions are sometimes also denoted with Roman numerals; for example, the example seen above is occasionally referred to as Fe(II) or Fe. The Roman numeral designates the \"formal oxidation state\" of an element, whereas the superscripted numerals denote the net charge. The two notations are, therefore, exchangeable for monatomic ions, but the Roman numerals \"cannot\" be applied to polyatomic ions. However, it is possible to mix the notations for the individual metal centre with a polyatomic complex, as shown by the uranyl ion example.\n\nIf an ion contains unpaired electrons, it is called a \"radical\" ion. Just like uncharged radicals, radical ions are very reactive. Polyatomic ions containing oxygen, such as carbonate and sulfate, are called \"oxyanions\". Molecular ions that contain at least one carbon to hydrogen bond are called \"organic ions\". If the charge in an organic ion is formally centred on a carbon, it is termed a \"carbocation\" (if positively charged) or \"carbanion\" (if negatively charged).\n\nMonatomic ions are formed by the gain or loss of electrons to the valence shell (the outer-most electron shell) in an atom. The inner shells of an atom are filled with electrons that are tightly bound to the positively charged atomic nucleus, and so do not participate in this kind of chemical interaction. The process of gaining or losing electrons from a neutral atom or molecule is called \"ionization\".\n\nAtoms can be ionized by bombardment with radiation, but the more usual process of ionization encountered in chemistry is the transfer of electrons between atoms or molecules. This transfer is usually driven by the attaining of stable (\"closed shell\") electronic configurations. Atoms will gain or lose electrons depending on which action takes the least energy.\n\nFor example, a sodium atom, Na, has a single electron in its valence shell, surrounding 2 stable, filled inner shells of 2 and 8 electrons. Since these filled shells are very stable, a sodium atom tends to lose its extra electron and attain this stable configuration, becoming a sodium cation in the process\n\nOn the other hand, a chlorine atom, Cl, has 7 electrons in its valence shell, which is one short of the stable, filled shell with 8 electrons. Thus, a chlorine atom tends to \"gain\" an extra electron and attain a stable 8-electron configuration, becoming a chloride anion in the process:\n\nThis driving force is what causes sodium and chlorine to undergo a chemical reaction, wherein the \"extra\" electron is transferred from sodium to chlorine, forming sodium cations and chloride anions. Being oppositely charged, these cations and anions form ionic bonds and combine to form sodium chloride, NaCl, more commonly known as table salt.\n\nPolyatomic and molecular ions are often formed by the gaining or losing of elemental ions such as a proton, H, in neutral molecules. For example, when ammonia, NH, accepts a proton, H—a process called protonation—it forms the ammonium ion, . Ammonia and ammonium have the same number of electrons in essentially the same electronic configuration, but ammonium has an extra proton that gives it a net positive charge.\n\nAmmonia can also lose an electron to gain a positive charge, forming the ion . However, this ion is unstable, because it has an incomplete valence shell around the nitrogen atom, making it a very reactive radical ion.\n\nDue to the instability of radical ions, polyatomic and molecular ions are usually formed by gaining or losing elemental ions such as , rather than gaining or losing electrons. This allows the molecule to preserve its stable electronic configuration while acquiring an electrical charge.\n\nThe energy required to detach an electron in its lowest energy state from an atom or molecule of a gas with less net electric charge is called the \"ionization potential\", or \"ionization energy\". The \"n\"th ionization energy of an atom is the energy required to detach its \"n\"th electron after the first \"n − 1\" electrons have already been detached.\n\nEach successive ionization energy is markedly greater than the last. Particularly great increases occur after any given block of atomic orbitals is exhausted of electrons. For this reason, ions tend to form in ways that leave them with full orbital blocks. For example, sodium has one \"valence electron\" in its outermost shell, so in ionized form it is commonly found with one lost electron, as . On the other side of the periodic table, chlorine has seven valence electrons, so in ionized form it is commonly found with one gained electron, as . Caesium has the lowest measured ionization energy of all the elements and helium has the greatest. In general, the ionization energy of metals is much lower than the ionization energy of nonmetals, which is why, in general, metals will lose electrons to form positively charged ions and nonmetals will gain electrons to form negatively charged ions.\n\n\"Ionic bonding\" is a kind of chemical bonding that arises from the mutual attraction of oppositely charged ions. Ions of like charge repel each other, and ions of opposite charge attract each other. Therefore, ions do not usually exist on their own, but will bind with ions of opposite charge to form a crystal lattice. The resulting compound is called an \"ionic compound\", and is said to be held together by \"ionic bonding\". In ionic compounds there arise characteristic distances between ion neighbours from which the spatial extension and the ionic radius of individual ions may be derived.\n\nThe most common type of ionic bonding is seen in compounds of metals and nonmetals (except noble gases, which rarely form chemical compounds). Metals are characterized by having a small number of electrons in excess of a stable, closed-shell electronic configuration. As such, they have the tendency to lose these extra electrons in order to attain a stable configuration. This property is known as \"electropositivity\". Non-metals, on the other hand, are characterized by having an electron configuration just a few electrons short of a stable configuration. As such, they have the tendency to gain more electrons in order to achieve a stable configuration. This tendency is known as \"electronegativity\". When a highly electropositive metal is combined with a highly electronegative nonmetal, the extra electrons from the metal atoms are transferred to the electron-deficient nonmetal atoms. This reaction produces metal cations and nonmetal anions, which are attracted to each other to form a \"salt\".\n"}
{"id": "22964702", "url": "https://en.wikipedia.org/wiki?curid=22964702", "title": "Jacques l'Hermite", "text": "Jacques l'Hermite\n\nJacques l'Hermite (c. 1582 – June 2, 1624), sometimes also known as Jacques le Clerq , was a Dutch merchant, explorer and admiral known for his journey around the globe with the Nassau Fleet (1623–1626) and for his blockade and raid on Callao in 1624 during that same voyage in which he also died. He served the Dutch East India Company as chief merchant in Bantam and Ambon Island in the Dutch East Indies. The Chilean Hermite Islands near Cape Horn which his fleet charted in February 1624 are named after him.\n\nL'Hermite was born in Antwerp, Dutch Republic (present-day Belgium) around the year 1582. After the fall of Antwerp in 1585 in a battle with the Spanish Empire, the family moved north to Amsterdam and later settled in Rotterdam. He left the Republic in 1606 and spent the next six years working in the Dutch East Indies.\n\nIn 1606 l'Hermite set sail to the Dutch East Indies as a secretary on the fleet commanded by Admiral Cornelis Matelief Jr. where in 1607 he was appointed chief merchant on the \"Black Lion\" (). From 1607–1611 l'Hermite was chief merchant () for the Dutch East India Company in Bantam, Dutch East Indies. After six years working abroad he returned to Amsterdam in the Dutch Republic. There he married Theodora van Wely in March 1613.\n\nIn April 1623 l'Hermite was commissioned by Prince Maurice of Nassau and the Dutch State General to lead a fleet of eleven ships known as the \"Nassau Fleet\" () with its flagship \"Amsterdam\". The fleet set sail on a circumnavigational voyage westwards from Amsterdam to the western coast of South America with the objective to hunt down Spanish silver ships leaving Peru and to establish a Dutch colony in either Peru or Chile, at that time known as the Viceroyalty of Peru.\n\nAlthough commanded by Admiral l'Hermite, the fleet was \"de facto\" led by his Vice-Admiral Gheen Huygen Schapenham and Rear-Admiral Julius Wilhelm Van Verschoor after l'Hermite like most of its crew suffered from dysentery during the journey. Early 1624 the fleet passed Cape Horn through Lemaire Channel and explored and charted the Hermite Islands. In May 1624 the fleet blocked the port of Callao and raided the cities of Pisco and Guayaquil, but they were unsuccessful in establishing a colony and the fleet was forced to continue its voyage westwards towards the Dutch East Indies.\n\nL'Hermite died on June 2, 1624 during the blockade of Callao after suffering from dysentery and scurvy for months. He was buried on San Lorenzo Island off the coast of Callao, Peru.\n\n\n"}
{"id": "11008433", "url": "https://en.wikipedia.org/wiki?curid=11008433", "title": "Johns Hopkins Center for Health Security", "text": "Johns Hopkins Center for Health Security\n\nThe Johns Hopkins Center for Health Security (abbreviated CHS; previously the UPMC Center for Health Security, the Center for Biosecurity of UPMC, and the Johns Hopkins Center for Civilian Biodefense Strategies) is an independent, nonprofit organization of the Johns Hopkins Bloomberg School of Public Health that works in the area of health consequences from epidemics and disasters. It is a think tank that does policy research and gives policy recommendations to the United States government.\n\nThe Center for Health Security began as the Johns Hopkins Center for Civilian Biodefense Strategies (CCBS) in 1998 at the Johns Hopkins Bloomberg School of Public Health. D. A. Henderson served as the founding director. At that time, the Center was the first and only academic center focused on biosecurity policy and practice.\n\nAt one point around 2003, CHS had become part of a new umbrella organization called the Institute for Global Health and Security at the Johns Hopkins Bloomberg School of Public Health.\n\nIn November 2003, the leaders left Johns Hopkins to join the University of Pittsburgh Medical Center (UPMC), and relaunched the Center as the Center for Biosecurity of UPMC. This move apparently split the organization in two, and it is unclear what happened to the old organization.\n\nOn April 30, 2013, the Center changed its name from \"Center for Biosecurity of UPMC\" to \"UPMC Center for Health Security\". This name change reflected a broadening of the scope of CHS's work.\n\nIn January 2017, the Center became part of the Johns Hopkins Bloomberg School of Public Health. Its domain name changed from upmchealthsecurity.org to centerforhealthsecurity.org.\n\nIn 2002, the Center received a $1 million grant from the US federal government.\n\nBefore 2017, CHS was heavily reliant on government funding.\n\nIn January 2017, the Open Philanthropy Project awarded a $16 million grant over three years to the Center for Health Security.\n\nThe Center for Health Security publishes three online newsletters:\n\n\nIt also provides editorial oversight for the journal \"Health Security\", which was launched in 2003 and called \"Biosecurity and Bioterrorism: Biodefense Strategy, Practice, and Science\" until 2015.\n\nCHS also publishes the blog \"The Bifurcated Needle\".\n\nThe Open Philanthropy Project's grant writeup of CHS noted several publications:\n\nThe Center has published in journals including \"JAMA\" and \"The Lancet\". A full list of publications is available on the CHS website. , the list shows more than 400 publications.\n\nOn May 15, 2018, the Center hosted Clade X, a day-long pandemic tabletop exercise that simulated a series of National Security Council–convened meetings of 10 US government leaders, played by individuals prominent in the fields of national security or epidemic response.\n\nDrawing from actual events, Clade X identified important policy issues and preparedness challenges that could be solved with sufficient political will and attention. These issues were designed in a narrative to engage and educate the participants and the audience.\n\nClade X was livestreamed on Facebook and extensive materials from the exercise are available online.\n\nOn January 14, 2005, CHS helped to host Atlantic Storm, a table-top smallpox bioterrorism simulation.\n\nFrom June 22–23, 2001, CHS co-hosted Operation Dark Winter, a senior-level bioterrorism attack simulation involving a covert and widespread smallpox attack on the United States.\n\n"}
{"id": "10655554", "url": "https://en.wikipedia.org/wiki?curid=10655554", "title": "Lacewood", "text": "Lacewood\n\nLacewood is a common name for the wood produced from a number of different trees, with mostly a striking appearance of their „lace-wood“, which gets its name from the lacelike pattern: These include:\n\n"}
{"id": "1430367", "url": "https://en.wikipedia.org/wiki?curid=1430367", "title": "Lechatelierite", "text": "Lechatelierite\n\nLechatelierite is silica glass, amorphous SiO, non-crystalline mineraloid.\n\nLechatelierite is a mineraloid as it does not have a crystal structure. Although not a true mineral, it is often classified in the quartz mineral group.\n\nOne common way in which lechatelierite forms naturally is by very high temperature melting of quartz sand during a lightning strike. The result is an irregular, branching, often foamy hollow tube of silica glass called a fulgurite. Not all fulgurites are lechatelierite; the original sand must be nearly pure silica.\n\nLechatelierite also forms as the result of high pressure shock metamorphism during meteorite impact cratering and is a common component of a type of glassy ejecta called tektites. Most tektites are blobs of impure glassy material, but tektites from the Sahara Desert in Libya and Egypt, known as \"Libyan desert glass\", are composed of almost pure silica that is almost pure lechatelierite. High pressure experiments have shown that shock pressures of 85 GPa are needed to produce lechatelierite in quartz grains embedded in granite.\n\nLechatelierite was formed during the impact of a meteorite into a layer of Coconino Sandstone at Meteor Crater in Arizona. During the rapid pressure reduction following the impact, steam expanded the newly formed lechatelierite. The shattered and expanded glass has a density less than that of water.\n\nLechatelierite may also form artificially, a unique example being the \"trinitite\" produced by melting of quartz sand at the first nuclear bomb explosion at Trinity Flats, White Sands, New Mexico.\n\n"}
{"id": "27885903", "url": "https://en.wikipedia.org/wiki?curid=27885903", "title": "List of volcanic settlements", "text": "List of volcanic settlements\n\nSettlements built on volcanoes of recent origin or somewhere on their volcanic complexes or volcanic fields. These volcanoes may be active, dormant or otherwise may not be entirely extinct. Such settlements are subject to potential future volcanic hazards, including but not limited to volcanic eruption, hydrothermal activity or volcanic gases in their vicinity. This category does not include settlements merely \"threatened\" by volcanic activity but which are located at a distance away, and not located on the volcanic structure itself. It does include settlements which have volcano evacuation routes/procedures or warning systems.\n\n\n"}
{"id": "38785187", "url": "https://en.wikipedia.org/wiki?curid=38785187", "title": "Lockheed Martin Compact Fusion Reactor", "text": "Lockheed Martin Compact Fusion Reactor\n\nThe Lockheed Martin Compact Fusion Reactor is a proposed nuclear fusion reactor. It is the goal of a project at Lockheed Martin’s Skunk Works. It is a high-beta fusion reactor.\n\nThe high-beta configuration allows a compact fusion reactor (CFR) design and expedited development. \n\n\"High beta\" implies that the ratio of plasma pressure to magnetic pressure is greater than or equal to 1. By contrast tokamak designs reach only 0.05.\n\nThe CFR chief designer and technical team lead is Thomas McGuire. McGuire studied fusion as a source of space propulsion in response to a NASA desire to improve travel times to Mars.\n\nThe project began in 2010, and was publicly presented at the Google Solve for X forum on February 7, 2013.\n\nIn October 2014 Lockheed Martin announced a plan to \"build and test a compact fusion reactor in less than a year with a prototype to follow within five years\". \n\nIn May 2016, Rob Weiss announced that Lockheed Martin continued to support the project and would increase its investment in it.\n\nCFR plans to achieve high beta (the ratio of plasma pressure to the magnetic pressure) by combining cusp confinement and magnetic mirrors to confine the plasma. Cusps are sharply bent magnetic fields. Ideally, the plasma forms a sheath along the surface of the cusps and plasma leaks out along the axis and edges of the sharply bent field. The plasma lost along the edges recycles back into the cusps.\n\nCFR uses two mirror sets. A pair of ring mirrors is placed inside the cylindrical reactor vessel at either end. The other mirror set encircles the reactor cylinder. The ring magnets produce a type of magnetic field known as a diamagnetic cusp, in which magnetic forces rapidly change direction and push the nuclei towards the midpoint between the two rings. The fields from the external magnets push the nuclei back towards the vessel ends.\n\nMagnetic field strength is an increasing function of distance from the center. This implies that as the plasma pressure causes the plasma to expand, the magnetic field becomes stronger at the plasma edge, increasing containment.\n\nCFR employs superconducting magnets. These allow strong magnetic fields to be created with less energy than conventional magnets. The CFR has no net current, which Lockheed claimed eliminates the prime source of plasma instabilities. The plasma has a favorable surface-to-volume ratio, which improves confinement. The plasma's small volume reduces the energy needed to achieve fusion.\n\nThe project plans to replace the microwave emitters that heat the plasma in their prototypes with neutral beam injection, in which electrically neutral deuterium atoms transfer their energy to the plasma. Once initiated, the energy from fusion maintains the necessary temperature for subsequent fusion events.\n\nThe eventual device may reach in width. The company claims that each design iteration is shorter and far lower cost than large-scale projects such as the Joint European Torus, ITER or NIF.\n\nA P reactor, long by in diameter, produces about a reactor, similar in size to an A5W nuclear submarine fission reactor.\n\nRing magnets require protection from the plasma's neutron radiation. Plasma temperatures must reach many millions of kelvins. Superconducting magnets must be kept just above absolute zero to maintain superconductivity.\n\nThe \"blanket\" component that lines the reactor vessel has two functions: it captures the neutrons and transfers their energy to a coolant, and forces the neutrons to collide with lithium atoms, transforming them into tritium to fuel the reactor. The blanket must be an estimated 80–150 cm thick and weigh 300–1000 tons.\n\nThe prototype was planned to be a 100-megawatt deuterium and tritium reactor measuring that could fit on the back of a large truck and would be about one tenth the size of current reactor prototypes. 100 megawatts is enough to provide power for 80,000 people. A series of prototypes were constructed to approach this goal.\n\nTechnical results presented on the T4 experiment in 2015 showed a cold, partially ionized plasma with the following parameters: peak electron temperature of 20 Electron volts, electron density, less than 1% ionization fraction and of input power. No confinement or fusion reaction rates were presented.\n\nMcGuire presented two theoretical reactor concepts in 2015. One was an ideal configuration weighing 200 metric tons with 1 meter of cryogenic radiation shielding and 15 tesla magnets. The other was a conservative configuration weighing 2,000 metric tons, with 2 meters of cryogenic radiation shielding and 5 Tesla magnets.\n\nThe T4B prototype was announced in 2016.\n\nParameters:\n\nParameters:\nIn February 2018, Lockheed Martin was granted one patent. It applied for three others.\n\nPhysics professor and director of the UK's national Fusion laboratory Steven Cowley called for more data, pointing out that the current thinking in fusion research is that \"bigger is better\". Other fusion reactors achieve 8 times improvement in heat confinement when machine size is doubled.\n\n\n"}
{"id": "14255345", "url": "https://en.wikipedia.org/wiki?curid=14255345", "title": "Lyonsite", "text": "Lyonsite\n\nLyonsite (CuFe(VO)) is a rare black vanadate mineral that is opaque with a metallic lustre. It crystallizes in the orthorhombic crystal system. Lyonsite often occurs as small tabular typically well formed crystals. Lyonsite has a good cleavage and a dark gray streak.\n\nLyonsite occurs as a sublimate in volcanic fumaroles. It is often associated with howardevansite and thenardite. It was first described in 1987 for an occurrence on the Izalco volcano, El Salvador. It was named for mineralogist John Bartholomew Lyons (1916–1998) of Dartmouth College. It has also been reported from a mine dump in the Lichtenberg Absetzer Mine of Thuringia, Germany.\n"}
{"id": "835139", "url": "https://en.wikipedia.org/wiki?curid=835139", "title": "Margaritasite", "text": "Margaritasite\n\nMargaritasite is a yellow, caesium-bearing mineral in the carnotite group. Its chemical formula is (Cs, K, HO)(UO)VO·HO and its crystal system is monoclinic.\n\nIt was first described in 1982 from the Margaritas uranium deposit in the Peña Blanca district of the municipality of Aldama, in the Mexican state of Chihuahua.\n\n\n"}
{"id": "12631416", "url": "https://en.wikipedia.org/wiki?curid=12631416", "title": "Mexican Plateau", "text": "Mexican Plateau\n\nThe Central Mexican Plateau, also known as the Mexican Altiplano (), is a large arid-to-semiarid plateau that occupies much of northern and central Mexico. Averaging above sea level, it extends from the United States border in the north to the Trans-Mexican Volcanic Belt in the south, and is bounded by the and to the west and east, respectively.\n\nA low east-west mountain range in the state of Zacatecas divides the plateau into northern and southern sections. These two sections, called the Northern Plateau () and Central Plateau (), are now generally regarded by geographers as sections of one plateau.\n\nThe Mexican Plateau is mostly covered by deserts and xeric shrublands, with pine-oak forests covering the surrounding mountain ranges and forming sky islands on some of the interior ranges. The Mexican Altiplano is one of six distinct physiographic sections of the Basin and Range Province, which in turn is part of the Intermontane Plateaus physiographic division.\n\nIn phytogeography, the Sonoran Desert is within the Sonoran Floristic Province of the Madrean Region in southwestern North America, part of the Holarctic Kingdom of the northern Western Hemisphere.\n\nWhile the plateau stretches from north to south, the southern east-west arc of the Central Mexican Plateau from Jalisco to Veracruz states historically as well as today has served as the population nexus of the Mexican nation, it is home to its biggest metro areas of Guadalajara, Leon, Querétaro, Morelia, Mexico City, Toluca, Cuernavaca, and Puebla.\n\nThe Mesa del Norte or northern plateau averages in elevation above mean sea level and extends south from the Rio Grande (Río Bravo del Norte) through the states of Chihuahua, Coahuila, Durango, Zacatecas and San Luis Potosí. Various narrow, isolated ridges cross the Mesa del Norte and numerous depressions also dot the region, the largest of which is the Bolsón de Mapimí. The Río Bravo del Norte and its tributary, the Río Conchos, drain portions of the northern plateau, and the Río Pánuco and its tributaries drain the southeastern corner. Both drain to the Gulf of Mexico. Much of the northern plateau comprises internal drainage basins that do not drain to the sea. The Chihuahuan Desert extends across the northern portion of the northern plateau, while the Meseta Central matorral covers the central portion, and the Central Mexican matorral extends from the southern portion of the northern plateau across the southern plateau.\n\nThe Mesa Central or southern plateau is higher than its northern counterpart, averaging in elevation. The southern plateau contains numerous valleys originally formed by ancient lakes. It extends across the states of Aguascalientes, Jalisco, Zacatecas, Guanajuato, Querétaro, Michoacán. Several of Mexico's most prominent cities, including Guadalajara, are located in the valleys of the southern plateau. Much of the southern plateau is drained by the Río Grande de Santiago and its tributaries, including the Río Lerma, which drain west into the Pacific Ocean. Tributaries of the Río Pánuco drain the eastern portion of the southern plateau. The Central Mexican matorral covers much of the southern plateau, with the subtropical Bajío dry forests occupying the lower portions of the Lerma–Río Grande de Santiago basin. Higher altitudes are covered by Mixed Forests, then Temperate Coniferous Forests, up to the snow line in the top of the volcanoes that surround the southern and western edges.\n\n"}
{"id": "57367053", "url": "https://en.wikipedia.org/wiki?curid=57367053", "title": "Mobile metering", "text": "Mobile metering\n\nMobile metering (recording of data using a mobile meter) is a technology which enables mobile recording of metering data. While railway companies such as the German Deutsche Bahn have been using this technology for years in their trains, it is now also being used for recording the charging transactions of electric vehicles (EVs). \n\nIn the latter case, a mobile electricity meter is integrated either into the vehicle itself or into the respective charging cable. This, together with the necessary communication technology (SIM card), makes it possible to transmit charging data (down to the kWh) to a matching backend. Lean, switchable system sockets suffice for charging – they serve as outlets for the power grid. These system sockets can be reduced to a technical minimum, as the vehicle or the cable, respectively, already carry the necessary billing and communication technology. This makes these sockets especially affordable and avoids running costs compared to conventional charging infrastructure, such as costs for maintenance or meter point operation. \n\nAs a result, precise metering, secure data transmission and efficient billing fulfill all preconditions for a comprehensive and future-proof charging and billing solution for electric mobility. \n\nThe mobile meter was developed in the projects „On Board Metering I & II“, that kicked off in March 2003, sponsored by the German ministry for economic affairs and technology. Participants of the project were:\n\n\nThe project’s approach to electric mobility was not tackling it as a singular challenge. The aim was rather to make a significant contribution to the energy transition by taking electric mobility one step further.\n\nFor this to happen, the EV was to become a system-relevant factor as an energy storage device. The goal was to create as much power grid connection points as possible while at the same time safeguarding exact metering and billing of electricity. This way, the vehicle could gain access to the grid anytime it is parked (ratio of EVs to grid connection points greater than 1). Up to that date, charging infrastructure for electric cars was thought of as stationary, similar to conventional gas stations for combustion cars. \n\nFor this, a shift of technology from the infrastructure to the vehicle side (or the cable, respectively) was needed. Such a network of ubiquitous charging spots was only to be realized with charging infrastructure that would cause comparably low costs over longer periods of time. \n\nThe disruptive approach of mobile metering has by now opened up new possibilities and business models for electric mobility. \n\n"}
{"id": "10189501", "url": "https://en.wikipedia.org/wiki?curid=10189501", "title": "Multifunction platform", "text": "Multifunction platform\n\nThe Multifunction Platform (MFP) is a concept and a structure developed by UNDP and deployed in a number of West African countries, and Tanzania and Zambia. The idea has been to place an MFP in a village which, driven by a diesel engine, powers devices such as pumps and grain mills and generators. The UNDP has produced a number of reports on this project in Mali which can be accessed here. There are few independent analyses of the development impact of the concept of the multifunctional platform, but recently an article in the journal Energy Policy,which can be accessed here, provide a more nuanced view of programme achievements than impact assessments made public at earlier stages of the project.\n\nThe primary impact of the MFP has been on women's work (on reducing daily drudgery and opening up new opportunities in life) and the UNDP's deployment has been to women's organisations, with part local funding and part local grant.\n\nIn places where Jatropha is grown, a device, powered by the MFP, can crush the Jatropha seed. The oil produced is suitable for running the diesel engine, allowing the MFP to produce fuel for its own operation. The company Mali Biocarburant has carried out a pilot with in a number of villages in Mali, as reported here. Also, The Mali Folke Centre has reported on its success in this field.\n\nMore recently, FACT Foundation has implemented a pilot programme in which biogas is used for co-fuelling MFP diesel engines. In five villages in Mali, biogas systems were installed and connected to the MFP diesel engines. As reported here, first results look promising, although further work is required to improve the management of the biogas systems.\n\nVolunteers from Engineers Without Borders have been involved with assessment of some MFP projects, as reported here.\n\nWhereas the UNDP's projects have involved substantial injection of foreign expert advice, a low budget self-initiated approach was in recent times taken in Kiliba, DRC at Farm of Hope, a project of Fondation chirezi. However, that project failed to materialise.\n"}
{"id": "6639878", "url": "https://en.wikipedia.org/wiki?curid=6639878", "title": "National Museums of Kenya", "text": "National Museums of Kenya\n\nThe National Museums of Kenya (NMK) is a state corporation that manages museums, sites and monuments in Kenya. It carries out heritage research, and has expertise in subjects ranging from palaeontology, ethnography and biodiversity research and conservation. Its headquarters and the National Museum (Nairobi National Museum) are located on Museum Hill, near Uhuru Highway between Central Business District and Westlands in Nairobi. The National Museum of Kenya was founded by the East Africa Natural History Society (E.A.N.H.S.) in 1910; the Society's main goal has always been to conduct an ongoing critical scientific examination of the natural attributes of the East African habitat. The museum houses collections, and temporary and permanent exhibits. Today the National Museum of Kenya manages over 22 regional museums, many sites, and monuments across the country.\n\nThe East Africa and Uganda Natural History Society was founded in 1910–11 by persons with an interest in nature in British East Africa. The group included two canons of the Church Missionary Society: The Rev. Harry Leakey (father of Louis Leakey) and The Rev. Kenneth St. Aubyn Rogers; some government officials: C. W. Hobley and John Ainsworth, doctors, dentists, big-game hunters and plantation owners. In 1911 they established the Natural History Museum and library with an honorary curator. Aladina Visram put up the money for a one-story, two-room building.\n\nIn 1914 they could afford a paid curator. They brought in Arthur Loveridge, a herpetologist, who arrived in March 1914. Loveridge concentrated on collections, with the members volunteering to contribute specimens, labour and funds. They also ran the museum while Loveridge fought for the British in German East Africa. He returned for a brief stay after the war, only to go to America, where he eventually became a Harvard University professor.\n\nThe next curator was A. F. J. Gedye. The museum moved to a new building at the corner of Government Road and Kirk Road. Among the new volunteers for the society were Sir Robert Coryndon, Governor of Kenya. At his unexpected death in 1927, Lady Coryndon established the Coryndon Memorial Fund to build a better museum for the society in memory of her husband. The government offered matching funds for public donations and in 1928 construction began.\n\nThe building was ready in 1929. Unfortunately no workrooms or storage space had been provided and therefore the Natural History Society declined to move in. The government then bought the old museum and the society used the money to add three rooms, gave its collections to the museum trustees, but retained the library. Everything was moved to the museum. Lady Coryndon donated Sir Robert's books to it.\n\nThe museum was officially opened on 22 September 1930, as Coryndon Museum, with Victor Gurney Logan Van Someren, a member, as curator. He was given a house on the grounds. In 1930 Evelyn Molony, née Napier was appointed the Museum's first botanist after a grant was given to the Museum by Ernest Carr to fund her employment. During her tenure she established within the Museum a herbarium on East African plants as well as publishing a series of scientific papers on East African flora.\n\nThe relationship between the museum trustees and the society became problematic, and as a result the two organisations appointed a committee including Sir Charles Belcher, a Kenyan jurist, to stabilise it. The committee turned everything over to the museum except for the library in exchange for annual payments for 15 years to the society.\n\nThe museum now had a staff. Mary Leakey became part of it and then Louis Leakey, as unpaid curator, in 1941. He stepped in when Dr. van Someren resigned after the board (including Louis) refused to dismiss Peter Bally in a personality conflict. The museum was a center for Leakey operations. In 1945 Louis was hired as paid curator with a new house, as the old one had become run-down. He built up the exhibitions and opened them to Africans and Asians by lowering the admission fee. Until then the museum had been \"for whites only.\"\n\nThe museum was a base for Leakey operations until 1961, when Louis founded the Centre for Prehistory and Paleontology on the grounds nearby and moved himself and his collections to it. He resigned in favour of the next director, Robert Carcasson.\n\nKenya became independent in 1963. The Coryndon Museum was renamed \"National Museum\" in 1964 and was included in a new system, the \"National Museums of Kenya.\" In 1967 Richard Leakey was having irreconcilable differences with Louis Leakey, his employer in the Centre, and decided to improve the National Museum. His main objection was that it had not been Kenyanized. He and supporters formed the Kenya Museum Associates, which obtained an observer's seat for Richard on the board from Carcasson in exchange for a 5000-pound contribution. Richard did not do much observing, as he departed for the first Omo expedition.\n\nThe Kenya Museum Associates included Joel Ojal, the museum overseer in the government. On his return from Omo Richard gave his ideas for improvement directly to Joel, who asked the chairman, Sir Ferdinand Cavendish-Bentinck, to place Richard in a senior position and begin replacing the board with Kenyans of Kenyan extraction, as there were only two out of 16 in that category. The penalty for inaction would be removal of government funding.\n\nRichard was at first offered a part-time executive position, which he turned down. Over the next few months much of the board was replaced and in May 1968 the new board offered Richard a permanent post as administrative director, with Carcasson to be retained as scientific director. However, Carcasson resigned and Richard became director.\n\nThis gallery contains artwork by Joy Adamson featuring various Kenyan communities in traditional attire.\n\nOn 15 October 2005 Nairobi Museum Galleries closed until December 2007 for an extensive rebuilding program. This was the first major renovation of Nairobi Museum since 1930. A new administration block and commercial center were built, and NMK's physical planning was improved.\n\nThe Museum re-opened in June 2008. It houses both temporary and permanent exhibitions.\n\nWithin the grounds are also the Nairobi Snake Park and the Botanic Garden and nature trail. The museum's commercial wing has restaurants and shops.\n\nOther museums, sites and monuments operated by the NMK, including eco-tourist attractions are:\n\n\n\n"}
{"id": "13360851", "url": "https://en.wikipedia.org/wiki?curid=13360851", "title": "Nature center", "text": "Nature center\n\nA nature center (or nature centre) is an organization with a visitor center or interpretive center designed to educate people about nature and the environment. Usually located within a protected open space, nature centers often have trails through their property. Some are located within a state or city park, and some have special gardens or an arboretum. Their properties can be characterized as nature preserves and wildlife sanctuaries. Nature centers generally display small live animals, such as reptiles, rodents, insects, or fish. There are often museum exhibits and displays about natural history, or preserved mounted animals or nature dioramas. Nature centers are staffed by paid or volunteer naturalists and most offer educational programs to the general public, as well as summer camp, after-school and school group programs.\n\nSome nature centers allow free admission but collect voluntary donations in order to help offset expenses. They usually rely on support from dedicated volunteers.\n\nEnvironmental education centers differ from nature centers in that their museum exhibits and education programs are available mostly by appointment, although casual visitors may be allowed to walk on their grounds.\n\nSome city, state and national parks have facilities similar to nature centers, such as museum exhibits, dioramas and trails, and some offer park nature education programs, usually presented by a park ranger.\n\n"}
{"id": "34071703", "url": "https://en.wikipedia.org/wiki?curid=34071703", "title": "Ocean governance", "text": "Ocean governance\n\nOcean governance is the conduct of the policy, actions and affairs regarding the world's oceans. Within governance, it incorporates the influence of non-state actors, i.e. stakeholders, NGOs and so forth, therefore the state is not the only acting power in policy making. However, in terms of the ocean, this is a complex issue because it is a commons that is not ‘owned’ by any single nation/state. The consequences of this has resulted in humankind abusing the oceans’ resources, by treating them as shared resources, but not taking equal and collective responsibilities in caring for them. This means that rules on the conduct of the ocean can only be implemented through international agreements. Therefore, there is a need for some form of governance to maintain the ocean for its various uses, preferably in a sustainable manner.\n\nThere are two major international legal organisations that exist that are involved in ocean governance on a global scale. The International Maritime Organization (IMO), which was ratified in 1958 is responsible mainly for maritime safety, liability and compensation and they have held some conventions on marine pollution related to shipping incidents.\n\n\nThe IMO sees the regulation of marine pollution as one its most important aspects of governance and in particular, the MARPOL convention is regarded as one of its greatest successes. The result of MARPOL has meant that oil pollution has decreased due to a change in equipment standards of oil tankers to prevent operational discharge of oil.\nHowever, the main organisation concerned with the economic, environmental, ethical, peace and security issues is the United Nations Convention on the Law of the Sea (UNCLOS).\n\nIt was first established under the Third UNCLOS in 1973 and fully ratified in 1982. The main aim was to adopt a regime of national seas and international waters on a global scale. What was agreed was that the jurisdictional boundaries of individual states were to be enlarged to 200 nautical miles off a state’s coastline. Coastal states were given greater rights to control these areas for protective purposes and the exploitation of natural resources. In total 38 million square nautical miles of ocean space was put under jurisdiction under the Exclusive Economic Zones (EEZ) and the legalities concerning the continental shelf and territorial sea were altered.\n\nHowever, the Convention did not come into full effect despite a number of implementations being carried out between 1973 and 1982. This was profoundly due to a dispute over mineral resources, particularly manganese nodules in the deep-oceans. Developing countries preferred treating these minerals as “common heritage,” that via an international organisation, they could benefit from a sharing of these resources. However, the developed world, in particular the United States, was not in favour of this and preferring a first-come, first-served basis, due to self-economic interest. Only in 1994 did the United States renounce their objections so that the Convention could be enacted.\n\n\nDespite an obvious need for international bodies to control and manage the resources, it has been argued that for sustainable governance of the ocean to occur, it needs to be participatory. The idea of the potential importance of participatory governance was first truly highlighted in Agenda 21, signed at the UNCED (United Nations Conference on Environment and Development) in Rio de Janeiro in 1992. It was stated that:\n\nWhat this means is that the public needs to be more actively involved along with the other stakeholders and authoritative bodies. This stems from democratic principles where the people are empowered to choose their own government, so with environmental concerns perhaps they should be involved in these in a similar manner. With the inclusion of people in a decision making process, it immediately secures legitimacy for these decisions.\n\nIt is said that expert scientific knowledge is important, but often enough “lay-knowledge” is equally credible concerning certain resources (in this case an example would be a fishery) and uncertainties within science itself. By engaging public participation, the benefit is that it is able to frame the environmental decision with the community in mind and how it will impact them. This would significantly be useful for ocean governance at a local and national level, in particular areas with coastal communities.\n\nDue to the nature of participatory governance being resource intensive, it would be harder to expand this to regional levels and beyond. It is also important to not necessarily enrol too many stakeholders, as there may be the ‘participation paradox’ that essentially means the greater the number of stakeholders, the fewer the responsibilities and involvement of each one and their effectiveness is reduced in the process.\n\nIn 2002, Canada introduced the Oceans Strategy, which was heavily based on participatory governance principles: \n\nThe aim of the ESSIM Initiative is to create integrated and adaptive management plans that are a collaborative effort for ecosystem, social, economic and institutional sustainability of the Eastern Scotian Shelf. It incorporates maintaining existing jurisdictional responsibilities, inclusion, consensus, accountability, dispute resolution, networking, evolution, and learning by doing, which are all part of the governance principles in the Oceans Strategy.\n\nHow it works is that the Stakeholders Roundtable (lead stakeholders and government) and the Planning Office draft up a management plan and this is then reviewed at the ESSIM Forum (an annual stakeholders’ meeting), community meetings and the general public. Overall, an agreement then must be reached with the Stakeholder Roundtable and a final plan given to appropriate federal and provincial government agencies, before acquiring final approval under the Oceans Act. It has been seen as fairly successful in improving communication and cooperation within government agencies, but there is room for greater inclusion of coastal community participation to fully fulfil the participatory theory.\n\nOcean governance can be carried out at a local, national, regional or international level. However, there needs to be a link between all levels of implementation for “good” ocean governance to build on the theory of participatory governance at these different levels. Yet one of the greatest problems with ocean governance is that the organisations lack authority and instruments to guarantee compliance and enforcement of these laws.\n\nCommunity-based management features in local implementation as stated in Principle 22 at Rio. What this means is that the state needs to recognise the importance that indigenous and local communities play in sustainable environmental policy making and how they can be beneficial. Also the stakeholders should take a role of responsibility with the government in a form of co-management to manage ocean resources.\n\nAt a national level, ocean governance is seen as an integrated management process within one state. This means there is a focus on almost all ministries of a government that have a function or authority related to ocean sectors and they collaborate. Due to ocean issues usually being low on a political agenda, it is said that to be successful integrated ocean policy is in need of the highest political direction and oversight to succeed.\n\nAt this scale, the scope of management and challenges are greater, so governance requires more organisations. It comprises Regional Seas Programme of UNEP creating programmes to be managed and coordinated by countries that share a common body of water. These Action Plans range from chemical waste to conservation of marine ecosystems. These however need to be strengthened along with The Global Programme of Action for the Protection of the Marine Environment from Land-based Activities (GPA).\n\nTo be effective Regional Development Banks (RDBs) and Regional Governmental Organizations (RGOs) are necessary to participate and provide reinforcement of the former organisations. Therefore, a mandate is required for the implementation of sub-regional programs, agreements and conventions to ensure consensus-based decision-making.\n\nThe General Assembly of the United Nations is seen as the proficient international body to implement ocean governance. It functions by the Secretary General producing reports on the recommendations by the Consultative Process of ocean matters and the law of the sea, which are then annually reviewed by The General Assembly.\n\nThe direct use of the ocean inherently comes from fishing for food resources, mainly for human consumption. In 2009, 79.9 million tonnes of fish were caught from a marine environment. While the FAO have stated that over half (53%) of fish stocks were said to be at full exploitation and thus their current catches are close to the maximum sustainable production levels. Therefore, it is important for an international agreement and policies to be in place, or nations will feel they are welcome to exploit the fish stocks beyond a sustainable level if there are no rules guiding fishing practices. Yet under UNCLOS, it is said that almost 99% of all fisheries are within a nation’s jurisdiction, however this is not preventing problems of exploitation.\n\nSince the mid-1980s, numerous fishery organisations appeared, which are narrowly focused but struggle to prevent global overfishing. As a result, there are problems with illegal fishing vessels, violating the laws of a fishery in which they may misreport their catches to authorities or are vessels that are not certified to be within an area of water. The reason for this is usually due to a certain fish species with a high economic value, for example Bluefin Tuna.\n\nOvercoming the problem of poor fishery management may be work best by transitioning to rights-based fishing and self-governance, which incorporates participatory governance theory. For it to work, there still needs to be a financial incentive that will fund it and ‘shares’ are distributed between the shareholders (individual/corporation, community or fishers’ collective) that are linked directly to the productivity and value of the resource, which consequently will make the shareholders appreciate the resource better and prevent overfishing. The theory is that when shareholders have an individual share, it reduces the competition between them since they are not allowed more than their share.\n\nThere is a focus on rights-based approaches in current development programmes, which have an emphasis on creating (or recreating) and supporting local institutions to the fishery. While a freedom of rights will result in economic benefits, there is a possibility of a monopolisation by more larger and powerful shareholders that will squeeze out small-scaled operations. The problem though with fisher folk having more rights, is they subsequently lack the transfer of skills to do with information, assessment, management and negotiation and they also lack sufficient funding due to their self-governing nature.\n\nAn alternative approach has been introducing market incentives to encourage sustainable fishing. The Marine Stewardship Council (MSC) introduced this through a fishery certification programme, with the incentive that the consumer will buy fish only caught by sustainable fisheries. This in turn creates a cycle that encourages the producer to abide by sustainable practices to gain the custom. To date (December 2011) there are currently 135 certified fisheries in the MSC Program.\n\n"}
{"id": "351564", "url": "https://en.wikipedia.org/wiki?curid=351564", "title": "Pileated woodpecker", "text": "Pileated woodpecker\n\nThe pileated woodpecker (\"Dryocopus pileatus\") is a woodpecker native to North America. This insectivorous bird is a mostly sedentary inhabitant of deciduous forests in eastern North America, the Great Lakes, the boreal forests of Canada, and parts of the Pacific coast. It is the second-largest woodpecker on the continent, after the critically endangered — and possibly extinct — ivory-billed woodpecker. The term \"pileated\" refers to the bird's prominent red crest, with the term from the Latin \"pileatus\" meaning \"capped\".\n\nAdults are long, span across the wings, and weigh , with an average weight of . Each wing measures , the tail measures , the bill is and the tarsus measures . They are mainly black with a red crest, and have a white line down the sides of the throat. They show white on the wings in flight. The flight of these birds is strong and direct, but undulates in the way characteristic of woodpeckers. Adult males have a red line from the bill to the throat, in adult females these are black. Two species found in the Old World, the white-bellied and black woodpeckers, are closely related and occupy the same ecological niche in their respective ranges that the pileated occupies in North America. The only North American birds of similar plumage and size are the ivory-billed woodpecker of the southeastern United States and Cuba, and the related imperial woodpecker of Mexico. However, unlike the pileated, both of those species are extremely rare, if not extinct. Most reports of the ivory-billed woodpecker are believed to be erroneous reports of the far more common pileated.\n\nUp to four subspecies of pileated woodpeckers have been recognized historically. However, many authors only recognize two subspecies, the southernly \"D. p.\" subsp. \"pileatus\" and the northerly \"D. p.\" subsp. \"abieticola\", and the differences between the other two subspecies other than range are not well described. Generally, pileated woodpeckers follow Bergmann's rule being smaller and more compact in the south (\"D. p.\" subsp. \"pileatus\") and larger in the north (\"D. p.\" subsp. \"abieticola\"). The ranges of the potential four subspecies are described below.\n\n\nTheir breeding habitat is forested areas across Canada, the eastern United States, and parts of the Pacific coast. This bird favors mature forests and heavily wooded parks. They specifically prefer mesic habitats with large, mature hardwood trees, often being found in large tracts of forest. However, they also inhabit smaller woodlots as long as they have a scattering of tall trees. Efforts to restore woodland by removing invasive honeysuckle and buckthorn seem to benefit them, as the removal of brush and shrubbery facilitates their foraging on the ground and in the lower stratum.\n\nThese birds mainly eat insects, especially carpenter ants and wood-boring beetle larvae. They also eat fruits, nuts, and berries, including poison ivy berries. Pileated woodpeckers often chip out large and roughly rectangular holes in trees while searching out insects, especially ant colonies. They also lap up ants by reaching with their long tongues into crevices. They are self-assured on the vertical surfaces of large trees, but can seem awkward while feeding on small branches and vines. Pileated woodpeckers may also forage on or near the ground, especially around fallen, dead trees, which can contain a variety of insect life. They may forage around the sides of human homes or even cars, and can occasionally be attracted to suet-type feeders. Although they are less likely feeder visitors than smaller woodpeckers, pileateds may regularly be attracted to them in areas experiencing harsh winter conditions.\n\nUsually, pileated woodpeckers excavate their large nests in the cavities of dead trees. Woodpeckers make such large holes in dead trees that the holes can cause a small tree to break in half. The roost of a pileated woodpecker usually has multiple entrance holes. Pileated woodpeckers raise their young every year in a hole in a tree. In April, the hole made by the male attracts a female for mating and raising their young. Once the brood is raised, the pileated woodpeckers abandon the hole and do not use it the next year. When abandoned, these holes — made similarly by all woodpeckers — provide good homes in future years for many forest song birds and a wide variety of other animals. Owls and tree-nesting ducks may largely rely on holes made by pileateds in which to lay their nests. Even mammals such as raccoons may use them. Other woodpeckers and smaller birds such as wrens may be attracted to pileated holes to feed on the insects found in them. Ecologically, the entire woodpecker family is important to the well being of many other bird species. The pileated woodpecker also nests in boxes about off the ground.\nA pileated woodpecker pair stays together on its territory all year round and is not migratory. They defend the territory in all seasons, but tolerate floaters during the winter. When clashing with conspecifics, they engage in much chasing, calling, striking with the wings, and jabbing with the bill. Drumming is most commonly to proclaim a territory, and hollow trees are often used to make the most resonant sound possible. The display drum consists of a burst of 11 to 30 taps delivered in less than a second.\n\nPileated woodpeckers have been observed to move to another site if any eggs have fallen out of the nest — a rare habit in birds. The cavity is unlined except for wood chips. Both parents incubate three to five eggs for 12 to 16 days. The average clutch size is four per nest. The young may take a month to fledge. The oldest known pileated woodpecker was 12 years and 11 months old. Predators at the nest can include American martens, weasels, squirrels, rat snakes, and gray foxes. Free-flying adults have fewer predators, but can be taken in some numbers by Cooper's hawks, northern goshawks, red-tailed hawks, great horned owls, and barred owls.\n\nThe pileated woodpecker occupies a large range and is quite adaptable. Its ability to survive in many wooded habitat types has allowed the species to survive human habitation of North America much better than the more specialized ivory-billed woodpecker. Pileated woodpeckers have a large population size, and despite being nonmigratory, are protected under the U.S. Migratory Bird Act. While the large birds control many insect populations, especially tree beetles, that may otherwise experience outbreaks, some people may consider them harmful if found on their property due to the considerable damage that pileated woodpeckers can do to trees and homes.\n\n\n"}
{"id": "8404649", "url": "https://en.wikipedia.org/wiki?curid=8404649", "title": "Pressure–volume diagram", "text": "Pressure–volume diagram\n\nA pressure–volume diagram (or PV diagram, or volume–pressure loop) is used to describe corresponding changes in volume and pressure in a system. They are commonly used in thermodynamics, cardiovascular physiology, and respiratory physiology.\n\nPV diagrams, originally called indicator diagrams, were developed in the 18th century as tools for understanding the efficiency of steam engines.\n\nA PV diagram plots the change in pressure \"P\" with respect to volume \"V\" for some process or processes. Typically in thermodynamics, the set of processes forms a cycle, so that upon completion of the cycle there has been no net change in state of the system; i.e. the device returns to the starting pressure and volume.\n\nThe figure shows the features of a typical PV diagram. A series of numbered states (1 through 4) are noted. The path between each state consists of some process (A through D) which alters the pressure or volume of the system (or both).\n\nA key feature of the diagram is that the amount of energy expended or received by the system as work can be estimated as the area under the curve on the chart. For a cyclic diagram, the net work is that \"enclosed\" by the curve. In the example given in the figure, the processes 1-2-3 produce a work output, but processes from 3-4-1 require a smaller energy input to return to the starting position / state; thus the net work is the difference between the two.\n\nNote that this figure is highly idealized, and a diagram showing the processes in a real device would tend to depict a more complex shape of the PV curve. (See section Applications, below).\n\nThe PV diagram, then called an indicator diagram, was developed by James Watt and his employee John Southern (1758–1815) to improve the efficiency of engines. In 1796, Southern developed the simple, but critical, technique to generate the diagram by fixing a board so as to move with the piston, thereby tracing the \"volume\" axis, while a pencil, attached to a pressure gauge, moved at right angles to the piston, tracing \"pressure\".\n\nThe gauge enabled Watt to calculate the work done by the steam while ensuring that its pressure had dropped to zero by the end of the stroke, thereby ensuring that all useful energy had been extracted. The total work could be calculated from the area between the \"volume\" axis and the traced line. The latter fact had been realised by Davies Gilbert as early as 1792 and used by Jonathan Hornblower in litigation against Watt over patents on various designs. Daniel Bernoulli had also had the insight about how to calculate work.\n\nWatt used the diagram to make radical improvements to steam engine performance and long kept it a trade secret. Though it was made public in a letter to the \"Quarterly Journal of Science\" in 1822, it remained somewhat obscure. John Farey, Jr. only learned of it on seeing it used, probably by Watt's men, when he visited Russia in 1826.\n\nIn 1834, Émile Clapeyron used a diagram of pressure against volume to illustrate and elucidate the Carnot cycle, elevating it to a central position in the study of thermodynamics.\n\nLater instruments (\"illus.\") used paper wrapped around a cylindrical barrel with a pressure piston inside it, the rotation of the barrel coupled to the piston crosshead by a weight- or spring-tensioned wire.\n\nIn 1869 the British marine engineer Nicholas Procter Burgh wrote a full book on the indicator diagram explaining the device step by step. He had noticed that \"a very large proportion of the young members of the engineering profession look at an indicator diagram as a mysterious production.\"\n\nPV diagrams can be used to estimate the net work performed by a thermodynamic cycle. The net work is the area enclosed by the PV curve in the diagram. This usage derived from the development of indicator diagrams which were used to estimate the performance of a steam engine. Specifically, the diagram records the pressure of steam versus the volume of steam in a cylinder, throughout a piston's cycle of motion in a steam engine. The diagram enables calculation of the work performed and thus can provide a measure of the power produced by the engine.\nTo exactly calculate the work done by the system it is necessary to calculate the integral of the pressure with respect to volume. One can often quickly calculate this using the PV diagram as it is simply the area enclosed by the cycle.\n\nNote that in some cases specific volume will be plotted on the x-axis instead of volume, in which case the area under the curve represents work per unit mass of the working fluid (i.e. J/kg).\n\nIn cardiovascular physiology, the diagram is often applied to the left ventricle, and it can be mapped to specific events of the cardiac cycle. PV loop studies are widely used in basic research and preclinical testing, to characterize the intact heart's performance under various situations (effect of drugs, disease, characterization of mouse strains),\n\nThe sequence of events occurring in every heart cycle is as follows. The left figure shows a PV loop from a real experiment; letters refer to points.\n\n\nAs it can be seen, the PV loop forms a roughly rectangular shape and each loop is formed in an anti-clockwise direction.\n\nVery useful information can be derived by examination and analysis of individual loops or series of loops, for example: \n\nSee external links for a much more precise representation.\n\n\n\n"}
{"id": "35574369", "url": "https://en.wikipedia.org/wiki?curid=35574369", "title": "Products of conception", "text": "Products of conception\n\nProducts of conception, abbreviated POC, is a medical term used for the tissue derived from the union of an egg and a sperm. It encompasses anembryonic gestation (blighted ovum) which does not have a viable embryo.\n\nIn the context of tissue from a dilation and curettage, the presence of POC essentially excludes an ectopic pregnancy.\n\n\"Retained products of conception\" is where products of conception remain in the uterus after childbirth, medical abortion or spontaneous abortion (colloquially known as miscarriage). Miscarriage with retained products of conception is termed \"delayed\" when no or very little products of conception have been passed, and \"incomplete\" when some products have been passed but some still remain \"in utero\".\n\nThe diagnosis is based on clinical presentation, quantitative HCG, ultrasound, and pathologic evaluation. A solid, heterogeneous, echogenic mass has a positive predictive value of 80%, but is present in only a minority of cases. A thickened endometrium of > 10 mm is usually considered abnormal, though no consensus exists on the appropriate cutoff. A cut-off of 8 mm or more has 34% positive rate, while a cut off of 14 mm or more has 85% sensitivity, 64% specificity for the diagnosis. Color Doppler flow in the endometrial canal can increased confidence in the diagnosis, though its absence does not exclude it, as 40% of cases of retained products have little or no flow. The differential in suspected cases includes uterine atony, blood clot, gestational trophoblastic disease, and normal post partum appearance of the uterus. Post partum blood clot is more common, reported in up to 24% of postpartum patients, and tends to be more hypoechoic than retained products with absent color flow on Doppler, and resolving spontaneously on follow up scans. The presence of gas raises the possibility of post partum endometritis, though this can also be seen in up to 21% of normal post pregnancy states. The normal post partum uterus is usually less than 2 cm in thickness, and continues to involute on follow up scans to 7 mm or less over time. Retained products are not uncommon, occurring in approximately 1% of all pregnancies, though it more common following abortions, either elective or spontaneous. There is significant overlap between appearance of a normal post partum uterus and retained products. If there is no endometrial canal mass or fluid, and endometrial thickness is less than 10 mm and without increased flow, retained products are statistically unlikely.\n\nAccording to the 2006 WHO \"Frequently asked clinical questions about medical abortion\", the presence of remaining products of conception in the uterus (as detected by obstetric ultrasonography) after a medical abortion is \"not\" an indication for surgical intervention (that is, vacuum aspiration or dilation and curettage). Remaining products of conception will be expelled during subsequent vaginal bleeding. Still, surgical intervention may be carried out on the woman's request, if the bleeding is heavy or prolonged, or causes anemia, or if there is evidence of endometritis.\n\nIn delayed miscarriage (also called missed abortion), the Royal Women's Hospital recommendations of management depend on the findings in ultrasonography:\n\nIn incomplete miscarriage, the Royal Women's Hospital recommendations of management depend on the findings in ultrasonography:\n\n"}
{"id": "18605319", "url": "https://en.wikipedia.org/wiki?curid=18605319", "title": "Quark–gluon plasma", "text": "Quark–gluon plasma\n\nA quark–gluon plasma (QGP) or quark soup is a state of matter in quantum chromodynamics (QCD) which exists at extremely high temperature and/or density. This state is thought to consist of asymptotically free strong-interacting quarks and gluons, which are ordinarily confined by color confinement inside atomic nuclei or other hadrons. This is in analogy with the conventional plasma where nuclei and electrons, confined inside atoms by electrostatic forces at ambient conditions, can move freely. Artificial quark matter, which has been produced at Brookhaven National Laboratory’s Relativistic Heavy Ion Collider and CERN's Large Hadron Collider, can only be produced in minute quantities and is unstable and impossible to contain, and will radioactively decay within a fraction of a second into stable particles through hadronization; the produced hadrons or their decay products and gamma rays can then be detected. In the quark matter phase diagram, QGP is placed in the high-temperature, high-density regime, whereas ordinary matter is a cold and rarefied mixture of nuclei and vacuum, and the hypothetical quark stars would consist of relatively cold, but dense quark matter. It is believed that up to a few milliseconds after the Big Bang, known as the quark epoch, the Universe was in a quark–gluon plasma state.\n\nThe strength of the color force means that unlike the gas-like plasma, quark–gluon plasma behaves as a near-ideal Fermi liquid, although research on flow characteristics is ongoing. Liquid or even near-perfect liquid flow with almost no frictional resistance or viscosity was claimed by research teams at RHIC and LHC's Compact Muon Solenoid detector. QGP differs from a \"free\" collision event by several features; for example, its particle content is indicative of a temporary chemical equilibrium producing an excess of middle-energy strange quarks vs. a nonequilibrium distribution mixing light and heavy quarks (\"strangeness production\"), and it does not allow particle jets to pass through (\"jet quenching\").\n\nExperiments at CERN's Super Proton Synchrotron (SPS) first tried to create the QGP in the 1980s and 1990s: the results led CERN to announce indirect evidence for a \"new state of matter\" in 2000. In 2010, scientists at Brookhaven National Laboratory’s Relativistic Heavy Ion Collider announced they had created quark–gluon plasma by colliding gold ions at nearly the speed of light, reaching temperatures of 4 trillion degrees Celsius. Current experiments (2017) at the Brookhaven National Laboratory's Relativistic Heavy Ion Collider (RHIC) on Long Island (NY, USA) and at CERN's recent Large Hadron Collider near Geneva (Switzerland) are continuing this effort, by colliding relativistically accelerated gold and other ion species (at RHIC) or lead (at LHC) with each other or with protons. Three experiments running on CERN's Large Hadron Collider (LHC), on the spectrometers ALICE, ATLAS and CMS, have continued studying the properties of QGP. CERN temporarily ceased colliding protons, and began colliding lead ions for the ALICE experiment in 2011, in order to create a QGP. A new record breaking temperature was set by at CERN on August, 2012 in the ranges of 5.5 trillion (5.5×10) kelvin as claimed in their Nature PR.\n\nQuark–gluon plasma is a state of matter in which the elementary particles that make up the hadrons of baryonic matter are freed of their strong attraction for one another under extremely high energy densities. These particles are the quarks and gluons that compose baryonic matter. In normal matter quarks are \"confined\"; in the QGP quarks are \"deconfined\". In classical QCD quarks are the fermionic components of hadrons (mesons and baryons) while the gluons are considered the bosonic components of such particles. The gluons are the force carriers, or bosons, of the QCD color force, while the quarks by themselves are their fermionic matter counterparts.\n\nAlthough the experimental high temperatures and densities predicted as producing a quark–gluon plasma have been realized in the laboratory, the resulting matter does \"not\" behave as a quasi-ideal state of free quarks and gluons, but, rather, as an almost perfect dense fluid. Actually, the fact that the quark–gluon plasma will not yet be \"free\" at temperatures realized at present accelerators was predicted in 1984 as a consequence of the remnant effects of confinement.\n\nA plasma is matter in which charges are screened due to the presence of other mobile charges. For example: Coulomb's Law is suppressed by the screening to yield a distance-dependent charge, formula_1, i.e., the charge Q is reduced exponentially with the distance divided by a screening length α. In a QGP, the color charge of the quarks and gluons is screened. The QGP has other analogies with a normal plasma. There are also dissimilarities because the color charge is non-abelian, whereas the electric charge is abelian. Outside a finite volume of QGP the color-electric field is not screened, so that a volume of QGP must still be color-neutral. It will therefore, like a nucleus, have integer electric charge.\n\nBecause of the extremely high energies involved, quark-antiquark pairs are produced by pair production and thus QGP is a roughly equal mixture of quarks and antiquarks of various flavors, with only a slight excess of quarks. This property is not a general feature of conventional plasmas, which may be too cool for pair production (see however pair instability supernova).\n\nOne consequence of this difference is that the color charge is too large for perturbative computations which are the mainstay of QED. As a result, the main theoretical tools to explore the theory of the QGP is lattice gauge theory. The transition temperature (approximately ) was first predicted by lattice gauge theory. Since then lattice gauge theory has been used to predict many other properties of this kind of matter. The AdS/CFT correspondence conjecture may provide insights in QGP, moreover the ultimate goal of the fluid/gravity correspondence is to understand QGP. The QGP is believed to be a phase of QCD which is completely locally thermalized and thus suitable for an effective fluid dynamic description.\n\nThe QGP can be created by heating matter up to a temperature of , which amounts to per particle. This can be accomplished by colliding two large nuclei at high energy (note that is not the energy of the colliding beam). Lead and gold nuclei have been used for such collisions at CERN SPS and BNL RHIC, respectively. The nuclei are accelerated to ultrarelativistic speeds (contracting their length) and directed towards each other, creating a \"fireball\", in the rare event of a collision. Hydrodynamic simulation predicts this fireball will expand under its own pressure, and cool while expanding. By carefully studying the spherical and elliptic flow, experimentalists put the theory to test.\n\nQCD is one part of the modern theory of particle physics called the Standard Model. Other parts of this theory deal with electroweak interactions and neutrinos. The theory of electrodynamics has been tested and found correct to a few parts in a billion. The theory of weak interactions has been tested and found correct to a few parts in a thousand. Perturbative forms of QCD have been tested to a few percent. Perturbative models assume relatively small changes from the ground state, i.e. relatively low temperatures and densities, which simplifies calculations at the cost of generality. In contrast, non-perturbative forms of QCD have barely been tested. The study of the QGP, which has both a high temperature and density, is part of this effort to consolidate the grand theory of particle physics.\n\nThe study of the QGP is also a testing ground for finite temperature field theory, a branch of theoretical physics which seeks to understand particle physics under conditions of high temperature. Such studies are important to understand the early evolution of our universe: the first hundred microseconds or so. It is crucial to the physics goals of a new generation of observations of the universe (WMAP and its successors). It is also of relevance to Grand Unification Theories which seek to unify the three fundamental forces of nature (excluding gravity).\n\nThe cross-over temperature from the normal hadronic to the QGP phase is about . This \"crossover\" may actually \"not\" be only a qualitative feature, but instead one may have to do with a true (second order) phase transition, e.g. of the universality class of the three-dimensional Ising model. The phenomena involved correspond to an energy density of a little less than . For relativistic matter, pressure and temperature are not independent variables, so the equation of state is a relation between the energy density and the pressure. This has been found through lattice computations, and compared to both perturbation theory and string theory. This is still a matter of active research. Response functions such as the specific heat and various quark number susceptibilities are currently being computed.\n\nThe equation of state is an important input into the flow equations. The speed of sound is currently under investigation in lattice computations. The mean free path of quarks and gluons has been computed using perturbation theory as well as string theory. Lattice computations have been slower here, although the first computations of transport coefficients have recently been concluded. These indicate that the mean free time of quarks and gluons in the QGP may be comparable to the average interparticle spacing: hence the QGP is a liquid as far as its flow properties go. This is very much an active field of research, and these conclusions may evolve rapidly. The incorporation of dissipative phenomena into hydrodynamics is another recent development that is still in an active stage.\n\nThe study of thermodynamic and flow properties indicate that the assumption of QGP consisting almost entirely of free quarks and gluons is an over-simplification. Many ideas are currently being developed and will be put to test in the near future. It has been hypothesized recently that some mesons built from heavy quarks do not dissolve until the temperature reaches about . This has led to speculation that many other kinds of bound states may exist in the plasma. Some static properties of the plasma (similar to the Debye screening length) constrain the excitation spectrum.\n\nSince 2008, there is a discussion about a hypothetical precursor state of the Quark–gluon plasma, the so-called \"Glasma\", where the dressed particles are condensed into some kind of glassy (or amorphous) state, below the genuine transition between the confined state and the plasma liquid. This would be analogous to the formation of metallic glasses, or amorphous alloys of them, below the genuine onset of the liquid metallic state.\n\nThose forms of the QGP that are easiest to compute are not those that are easiest to verify experimentally. While the balance of evidence points towards the QGP being the origin of the detailed properties of the fireball produced at SPS (CERN), in the RHIC and at LHC, this is the main barrier which prevents experimentalists from declaring a sighting of the QGP.\n\nThe important classes of experimental observations are\n\nIn short, a quark–gluon plasma flows like a splat of liquid, and because it's not \"transparent\" with respect to quarks, it can attenuate jets emitted by collisions. Furthermore, once formed, a ball of quark–gluon plasma, like any hot object, transfers heat internally by radiation. However, unlike in everyday objects, there is enough energy available that gluons (particles mediating the strong force) collide and produce an excess of the heavy (i.e. high-energy) strange quarks. Whereas, if the QGP didn't exist and there was a pure collision, the same energy would be converted into a nonequilibrium mixture containing even heavier quarks such as charm quarks or bottom quarks.\n\nIn April 2005, formation of quark matter was tentatively confirmed by results obtained at Brookhaven National Laboratory's Relativistic Heavy Ion Collider (RHIC). The consensus of the four RHIC research groups was that they had created a quark–gluon liquid of very low viscosity. However, contrary to what was at that time still the widespread assumption, it is yet unknown from theoretical predictions whether the QCD \"plasma\", especially close to the transition temperature, should behave like a gas or liquid. Authors favoring the weakly interacting interpretation derive their assumptions from the lattice QCD calculation, where the entropy density of quark–gluon plasma approaches the weakly interacting limit. However, since both energy density and correlation shows significant deviation from the weakly interacting limit, it has been pointed out by many authors that there is in fact no reason to assume a QCD \"plasma\" close to the transition point should be weakly interacting, like electromagnetic plasma (see, e.g.,). That being said, systematically improvable perturbative QCD quasiparticle models do a very good job of reproducing the lattice data for thermodynamical observables (pressure, entropy, quark susceptibility), including the aforementioned \"significant deviation from the weakly interacting limit\", down to temperatures on the order of 2 to 3 times the critical temperature for the transition.\n\n"}
{"id": "1104116", "url": "https://en.wikipedia.org/wiki?curid=1104116", "title": "Sikhote-Alin meteorite", "text": "Sikhote-Alin meteorite\n\nAn iron meteorite fell on the Sikhote-Alin Mountains, in southeastern Russia, in 1947. Though large iron meteorite falls had been witnessed previously and fragments recovered, never before in recorded history had a fall of this magnitude been observed. An estimated 70 tonnes (metric tons) of material survived the fiery passage through the atmosphere and reached the Earth.\n\nAt around 10:30 on 12 February 1947, eyewitnesses in the Sikhote-Alin Mountains, Primorye, Soviet Union, observed a large bolide brighter than the sun that came out of the north and descended at an angle of about 41 degrees. The bright flash and the deafening sound of the fall were observed for around the point of impact not far from Luchegorsk and approximately northeast of Vladivostok. A smoke trail, estimated at long, remained in the sky for several hours.\n\nAs the meteor, traveling at a speed of about , entered the atmosphere, it began to break apart, and the fragments fell together. At an altitude of about , the largest mass apparently broke up in a violent explosion called an air burst.\n\nOn November 20, 1957 the Soviet Union issued a stamp for the 10th anniversary of the Sikhote-Alin meteorite shower. It reproduces a painting by P. I. Medvedev, a Soviet artist who witnessed the fall: he was sitting in his window starting a sketch when the fireball appeared, so he immediately began drawing what he saw.\n\nBecause the meteor fell during daytime, it was observed by many eyewitnesses. Evaluation of this observational data allowed V. G. Fesenkov, then chairman of the meteorite committee of the USSR Academy of Science, to estimate the meteoroid's orbit before it encountered the Earth. This orbit was ellipse-shaped, with its point of greatest distance from the sun situated within the asteroid belt, similar to many other small bodies crossing the orbit of the Earth. Such an orbit was probably created by collisions within the asteroid belt.\n\nSikhote-Alin is a massive fall with the overall size of the meteoroid estimated at approximately . A more recent estimate by Tsvetkov (and others) puts the mass at around .\n\nKrinov had estimated the post-atmospheric mass of the meteoroid at some .\n\nThe strewn field for this meteorite covered an elliptical area of about . Some of the fragments made impact craters, the largest of which was about across and deep. Fragments of the meteorite were also driven into the surrounding trees.\n\nThe Sikhote-Alin meteorite is classified as an iron meteorite belonging to the meteorite group IIAB and with a coarse octahedrite structure. It is composed of approximately 93% iron, 5.9% nickel, 0.42% cobalt, 0.46% phosphorus, and 0.28% sulfur, with trace amounts of germanium and iridium. Minerals present include taenite, plessite, troilite, chromite, kamacite, and schreibersite.\n\nSpecimens of the Sikhote-Alin Meteorite are basically of two types:\nThe first type probably broke off the main object early in the descent. These pieces are characterized by regmaglypts (cavities resembling thumb prints) in the surface of each specimen. The second type are fragments which were either torn apart during the atmospheric explosions or blasted apart upon impact on the frozen ground. Most were probably the result of the explosion at altitude.\n\nA large specimen is on display in Moscow. Many other specimens are held by Russian Academy of Science and a great number of smaller specimens have made their way into the collector's market.\n\n\n"}
{"id": "39958236", "url": "https://en.wikipedia.org/wiki?curid=39958236", "title": "Super-spreader", "text": "Super-spreader\n\nA super-spreader is a host—an organism infected with a disease—that infects, disproportionally, more secondary contacts than other hosts who are, also, infected with the same disease. A sick human can be a super-spreader; they would be more likely to infect others than most people with the disease. Super-spreaders are thus of high concern in epidemiology (the study of the spread of diseases).\n\nSome cases of super-spreading conform to the 20/80 rule, where, approximately, 20% of infected individuals are responsible for 80% of transmissions, although super-spreading can still be said to occur when super-spreaders account for a higher or lower percentage of transmissions. In epidemics with super-spreading, the majority of individuals infect relatively few secondary contacts.\n\nSuper-spreading events are shaped by multiple factors including a decline in herd immunity, nosocomial infections, virulence, viral load, misdiagnosis, airflow dynamics, immune suppression, and co-infection with another pathogen.\n\nAlthough loose definitions of super-spreading exist, some effort has been made at defining what qualifies as a super-spreading event (SSE) more explicit. Lloyd-Smith et al. (2005) define a protocol to identify a super-spreading event as follows:\n\nThis protocol defines a 99th-percentile SSE as a case which causes more infections than would occur in 99% of infectious histories in a homogeneous population.\n\nDuring the 2003 SARS outbreak in Beijing, China, epidemiologists defined a super-spreader as an individual with transmission of SARS to at least eight contacts.\n\nSuper-spreaders may or may not show any symptoms of the disease.\n\nSuper-spreaders have been identified who excrete a higher than normal number of pathogens during the time they are infectious. This causes their contacts to be exposed to higher viral/bacterial loads than would be seen in the contacts of non-superspreaders with the same duration of exposure.\n\nThe basic reproduction number R is the average number of secondary infections caused by a typical infective person in a totally susceptible population. The basic reproductive number is found by multiplying the average number of contacts by the average probability that a susceptible individual will become infected, which is called the shedding potential. R = Number of contacts X Shedding potential \n\nThe individual reproductive number represents the number of secondary infections caused by a specific individual during the time that individual is infectious. Some individuals have significantly higher than average individual reproductive numbers and are known as super-spreaders. Through contact tracing, epidemiologists have identified super-spreaders in measles, tuberculosis, rubella, monkeypox, smallpox, Ebola hemorrhagic fever and SARS.\n\nMen with HIV who were co-infected with at least one other sexually transmitted disease, such as gonorrhea, hepatitis C, and herpes simplex 2 virus, were found to have an eight-fold higher HIV shedding rate than men without co-infection. This shedding rate was calculated in men with similar HIV viral loads. Once treatment for the co-infection had been completed, the HIV shedding rate returned to levels comparable to men without co-infection.\n\nHerd immunity, or herd effect, refers to the indirect protection that immunized community members provide to non-immunized members in preventing the spread of contagious disease. The greater the number of immunized individuals, the less likely an outbreak can occur because there are fewer susceptible contacts. In epidemiology, herd immunity is known as a \"dependent happening\" because it influences transmission over time. As a pathogen that confers immunity to the survivors moves through a susceptible population, the number of susceptible contacts declines. Even if susceptible individuals remain, their contacts are likely to be immunized, preventing any further spread of the infection. The proportion of immune individuals in a population above which a disease may no longer persist is the \"herd immunity threshold\". Its value varies with the virulence of the disease, the efficacy of the vaccine, and the contact parameter for the population. That is not to say that an outbreak can't occur, but it will be limited. \n\nThe first cases of SARS occurred in mid-November 2002 in the Guangdong Province of China. This was followed by an outbreak in Hong Kong in February, 2003. A Guangdong Province doctor, Liu Jianlun, who had treated SARS cases there, had contracted the virus and was symptomatic. Despite his symptoms, he traveled to Hong Kong to attend a family wedding. He stayed on the ninth floor of the Metropole Hotel in Kowloon, infecting 16 other hotel guests also staying on that floor (pictured above). The guests then traveled to Canada, Singapore, Taiwan, and Vietnam, spreading SARS to those locations and transmitting what became a global epidemic.\n\nIn another case during this same outbreak, a 54-year-old male was admitted to a hospital with coronary heart disease, chronic renal failure and type two diabetes. He had been in contact with a patient known to have SARS. Shortly after his admission he developed fever, cough, myalgia and sore throat. The admitting physician suspected SARS. The patient was transferred to another hospital for treatment of his coronary artery disease. While there, his SARS symptoms became more pronounced. Later, it was discovered he had transmitted SARS to 33 other patients in just two days. He was transferred back to the original hospital where he died of SARS.\n\nThe SARS pandemic was eventually contained, but not before it caused 8,273 cases and 775 deaths. Within two weeks of the original outbreak in Guangdong Province, SARS had spread to 37 countries.\n\nMeasles is a highly contagious, air-borne virus that reappears even among vaccinated populations. In one Finnish town in 1989, an explosive school-based outbreak resulted in 51 cases, several of whom had been previously vaccinated. One child alone, infected 22 others. It was noted during this outbreak that when vaccinated siblings shared a bedroom with an infected sibling, seven out of nine became infected as well.\n\nTyphoid fever is a human-specific disease caused by the bacterium \"Salmonella typhi\". It is highly contagious and becoming resistant to antibiotics. S. typhi is susceptible to creating asymptomatic carriers. The most famous carriers are Mary Mallon, known as Typhoid Mary, from New York City, and Mr. N. the Milker, from Folkstone, England. Both were active around the same time. Mallon infected 51 people from 1902 to 1909. Mr. N. infected more than 200 people over 14 years from 1901 to 1915. At the request of health officials, Mr. N. gave up working in food service. Mallon refused to give up working in food service and eventually was involuntarily quarantined at Brothers Island in New York, where she stayed until she died in November 1938, aged 69.\n\nIt has been found that \"Salmonella typhi\" persists in infected mice macrophages that have cycled from an inflammatory state to a non-inflammatory state. The bacteria remain and reproduce without causing further symptoms in the mice, and that this explains why carriers are asymptomatic.\n\n\n"}
{"id": "7777698", "url": "https://en.wikipedia.org/wiki?curid=7777698", "title": "Synopses of the British Fauna", "text": "Synopses of the British Fauna\n\nSynopses of the British Fauna is a series of identification guides, published by The Linnean Society and The Estuarine and Coastal Sciences Association. Each volume in the series provides and in-depth analysis of a group of animals and is designed to bridge the gap between the standard field guide and more specialised monograph or treatise. The series is now published by The Field Studies Council on behalf of The Linnean Society and The Estuarine and Coastal Sciences Association. \n\nThe series is designed for use in the field and is kept as user friendly as possible with technical terminology kept to a minimum and a glossary of terms provided, although the complexity of the subject matter makes the books more suitable for the more experienced practitioner.\n\nOn 11 March 1943, at a meeting of The Linnean Society in Burlington House, TH Savoy presented his \"Synopsis of the Opiliones\" (Harvestmen). It was so well received that a decision was made there and then to publish it as the first of a series of \"ecological fauna lists\".\n\nRe-launched by Dr Doris Kermack in the mid-1960s, the New Series of \"Synopses of the British Fauna\" went from strength to strength. From number 13, the series had been jointly sponsored by The Estuarine and Coastal Sciences Association and Dr RSK Barnes became co-editor.\n\nFrom 1993, the series has been published by The Field Studies Council and benefits from association with the extensive testing undertaken as part of the AIDGAP project.\n\nThe series contains the following volumes, many of which are out of print. Many of the volumes have been updated and reprinted under slightly different names to reflect either taxonomic changes or advances in the understanding of a group.\n\n\n"}
{"id": "18966340", "url": "https://en.wikipedia.org/wiki?curid=18966340", "title": "Turnoff point", "text": "Turnoff point\n\nThe turnoff point for a star refers to the point on the Hertzsprung-Russell diagram where it leaves the main sequence after the exhaustion of its main fuel. It is often referred to as the main sequence turnoff.\n\nBy plotting the turnoff point of the stars in star clusters, one can estimate the cluster's age.\n\nRed dwarfs are stars of 0.08-0.4 solar masses and are also referred to as class M stars. Red dwarfs have sufficient hydrogen mass to sustain hydrogen fusion to helium via the proton-proton chain reaction, but do not have sufficient mass to create the temperatures and pressures necessary to fuse helium to carbon, nitrogen or oxygen (see CNO cycle). However, all their hydrogen is available for fusion, and the low temperatures and pressures mean the lifetimes of these stars on the main sequence from zero point to turn off point is measured in trillions of years. For example, the lifespan of a star of 0.1 solar masses is 6 trillion years. This lifespan greatly exceeds the current age of the universe, therefore all red dwarfs are main sequence stars. Even though extremely long lived, those stars will eventually run out of fuel. Once all of the available hydrogen has been fused stellar nucleosynthesis stops and the remaining heated helium slowly cools by radiation. Gravity will contract the star from lack of expansive pressure from fusion until electron degeneracy pressure compensates. The cooling star is now off the main sequence and is known as a helium white dwarf.\n"}
{"id": "36677484", "url": "https://en.wikipedia.org/wiki?curid=36677484", "title": "Vineyard Power Co-operative", "text": "Vineyard Power Co-operative\n\nVineyard Power Co-operative is \"a community owned renewable energy co-operative based on the island of Martha's Vineyard, Massachusetts.\" \n\nVineyard Power develops local renewable energy projects and plans to assist the island of Martha's Vineyard to meet the energy goals of the Island Plan, and work towards a 100% renewable energy supply. The founding of Vineyard Power was inspired by the renewable energy vision of the people of Samso island in Denmark. \n\nOffshore wind development is another of the motivations for creating the Vineyard Power Co-op, with the recognition this energy source would need community support. The National Renewable Energy Lab (NREL) classifies the sound and ocean around Martha's Vineyard to be \"Excellent\" (between 7.5 and 8 meters per second (m/s) in regard to energy generation potential, at 50 meters altitude) or \"Outstanding\" (between 8.0 and 8.8 m/s). The first proposal for offshore wind in the area, known as Cape Wind, created significant opposition focused on the lack of community involvement, and visual impacts. https://www.nrel.gov/news/program/2011/1437.html That project obtained a lease for development in Federal waters six miles from land in Nantucket Sound. \n\nVineyard Power organized public input into the planning wind farm development off the southern shores of the island, with OffshoreMW, and Bureau of Ocean Energy Management, Regulation and Enforcement (BOEMRE). This collaboration began as the Martha's Vineyard Offshore Wind Alliance (MVOWA) in 2011. The Vineyard Power Co-op signed a Community Benefits Agreement with that one developer to formalize this arrangement for mutual benefit, and to pursue better community input and economic development for the people of Martha's Vineyard. The development is now called Vineyard Wind. The permitting process for this offshore windfarm is underway.\n\nDuring the years when the Federal process for offshore wind development moved slowly, the Co-op identified opportunities to develop solar projects on the island. The first of these were solar canopies in the parking lot at the locally-owned supermaket. Benefits for more citizens came through building solar on town-owned landfills. Town of Aquinnah chose the company, after an RFP, for a solar plant. Construction was started and completed in 2012. The most recent solar project completed is on the roof of the Boys and Girls Club.\n\n\nThe Co-op promotes planning and practices that reduce fossil fuel use and the replacement with renewable electricity. The first solar canopies built by the Co-op include electric vehicle chargers. The Co-op continues to support electric vehicle adoption. Leaders of island organizations met in September 2017 at the initiative of the Co-op and Environment Massachusetts to develop a vision of 100% renewable energy use for the island.\n\n"}
{"id": "8236995", "url": "https://en.wikipedia.org/wiki?curid=8236995", "title": "Vintage amateur radio", "text": "Vintage amateur radio\n\nVintage amateur radio is a subset of amateur radio activity and is considered a form of nostalgia or hobby much like antique car collecting, where enthusiasts collect, restore, preserve, build, and operate amateur radio equipment from bygone years, most notably those using vacuum tube technology.\n\nPopular modes of operation include voice communication using amplitude modulation (AM), and Morse code using continuous wave (CW) radiotelegraphy. Among enthusiasts, there is considerable interest in owning, restoring and operating vintage military and commercial radio equipment, much of it more than 40 years old. Some undertake to construct their own gear, known in ham slang as homebrewing, using vintage parts and designs. A number of amateur radio clubs and organizations sponsor contests, events, and swap meets that cater to this specialized aspect of the hobby.\n\nMany amateurs prefer the relatively precise digital frequency displays and stability of modern, state-of-the-art, microprocessor based amateur radios. Vintage radio enthusiasts contend that modern amateur equipment lacks the aesthetic appeal and \"soul\" of amateur electronic gear from the vacuum tube era. Additionally, many find satisfaction in taking commercially-made amateur equipment from the 1930s-1970s (affectionately called boat anchors by US amateurs because of their large size and weight) and carefully restoring it.\n\nEnthusiasts feel that the spacious electrical and mechanical designs of boat anchor radios are more easily worked on than the miniaturized layouts of modern Japanese gear. Fixing a modern transceiver often involves nothing more than a VLSI chip replacement. Vintage amateur radio devotees enjoy the more primitive experience that boat anchor radios offer, calling it \"real radio\". Enthusiasts claim that boat anchors sound better than modern equipment, saying that the tube audio from vintage gear is \"warmer\" and more aesthetically pleasing. Some hobbyists see vintage radio operation as a valuable asset to help preserve the history and heritage of radio for future generations, and may assist in the restoration and operation of vintage radio equipment for historical exhibits, museums and museum ships.\n\nAmplitude modulation (AM) was once the main voice mode in amateur radio before being superseded by Single-sideband modulation (SSB). But AM has recently become a nostalgic specialty interest on the shortwave ham bands. Vintage radio operation has drawn a wide range of amateur radio enthusiasts from rock star Joe Walsh, WB6ACU, to former Federal Communications Commission attorney Riley Hollingsworth, K4ZDH.\nA majority of \"AM'ers\" stations consist of vintage transmitters and receivers housed in separate cabinets. Some operators have even obtained old AM broadcast transmitters from radio stations that have upgraded their equipment. Others build their equipment from scratch (called homebrewing) using both modern and vintage-era components.\n\nIn the United States, amateur radio AM activity can be found on mediumwave, MF and shortwave, HF frequencies (in MHz) which include \"1.885, 1.930, 1.985, 3.870-3.885, 7.290-7.295, 7.160, 14.286, 14.330, 21.425, and 29.000-29.200,\" and sometimes feature \"special event\" stations using unique call signs. In the United Kingdom, AM activity can be found almost every day on frequencies between \"3.615\" and \"3.625\" MHz. The French AM activity can be found almost every day the morning between 6h30 and 8h00 the frequency \"3.550\" MHz. In Australia, AM activity can be found most days on \"7.125\" MHz. In New Zealand, AM nets are conducted on Friday evenings on 3.850 MHz at 8:00 pm or 8.30 pm, and on Sunday and Wednesday afternoons on 7.125 MHz at 4:00 pm. Conversations (QSO's in ham slang) are typically configured as \"roundtables\" consisting of several participants. Interested newcomers are usually encouraged to switch their modern transceivers to AM mode, introduce themselves, and join the conversation.\n\nVintage operating activity is not limited to the AM mode. Many devotees use their \"classic\" amateur gear from vintage-era American manufacturers like Eico, EF Johnson, National, Heathkit, Hammarlund, Drake, Collins, WRL, Swan, Signal/One, Lafayette and Hallicrafters, to make radiotelegraphy (CW), SSB, FM and RTTY two-way contacts. Although 1930s through 1970s gear is considered \"vintage\", collectors may differ on the cutoff dates.\n\nSome even sub-specialize in military radio collecting and undertake to restore and operate surplus communications equipment, much of it dating back to World War II, from the ubiquitous AN/ARC-5 command sets and US Signal Corps SCR-300 and SCR-536 walkie talkies to exotic gear like the British Paraset, a small espionage transceiver supplied to Resistance forces in France, Belgium and the Netherlands.\nThere is considerable interest in vintage military and commercial radio equipment among EU amateur radio operators, especially gear from British manufacturers such as Marconi, Racal, Eddystone, Pye, and a variety of Russian, German, Canadian, British RAF and British Army equipment, such as the well known Wireless Set No. 19.\n\n\"Glowbugs\" are a related aspect of vintage radio and harken back to the early days of amateur radio, when the majority of hams hand-crafted their own equipment. Smaller in size than \"boat anchors\", \"glowbug\" is a term used by US amateurs to describe a simple home-made tube-type radio set, reminiscent of the shortwave radio-building craze of the 1920s and 1930s. \"Glow\" refers to the glow of the vacuum tubes and \"bug\" to the gear's relatively diminutive size. The Doerle regenerative receiver and Hartley transmitter circuits are considered \"classic\" glowbug designs. Generally, any small, home-built tube-type transmitter or receiver may be referred to as a glowbug. The majority of glowbug transmitters are designed to be used in the CW radiotelegraphy mode.\n\nAs late as the 1960s, glowbugs were part of many beginner ham stations, and the ARRL Radio Amateur Handbook for those years exhibited a number of such simple, tube-based designs. Today, glowbug operators are enjoying a resurgence of interest among QRP enthusiasts and others with a penchant for constructing their own equipment. Many hams are assembling \"glowbug rigs\" on improvised chassis such as tin cakepans and wooden boards, and putting them on the air between 7040 – 7050 kHz and 7114 – 7125 kHz. Amateur radio Glowbug enthusiasts can often be heard communicating on the shortwave bands via CW using Morse code. Popular frequencies to hear glowbug contacts are around 3560 kHz and also 3579.5 kHz, chosen because crystals for this frequency can be salvaged from discarded color TV sets, along with other transmitter components.\n\nMany vintage radio clubs sponsor special events and contests, such as the \"AM QSO Party\" sponsored by the \"Antique Wireless Association\", the \"Heavy Metal Rally\" sponsored by \"Electric Radio Magazine\", and the \"Classic Radio Exchange\". Such operating events are not traditional ham radio contests inasmuch as they are a night of friendly QSO’s using home-built, restored commercial ham, broadcast or military equipment.\n\n\"The Amateur Radio Lighthouse Society\" and \"The AM Radio Network's\" \"Expedition to Thomas Point Shoal Lighthouse\" in Chesapeake Bay, MD commemorated the history of lighthouses with a vintage special event station using the call sign K3L.\n\nBritain's \"Vintage and Military Amateur Radio Society\" (affiliated with the Radio Society of Great Britain) coordinates regular on-air \"nets\" where enthusiasts gather as well as massive technical files for the benefit of members. \"The Surplus Radio Society\", a Dutch society of collectors of old ex-military radio equipment and other nostalgic receivers and transmitters holds weekly radio activity nets every Sunday on 3.575 MHz CW / 3.705 MHz AM and sponsors several flea markets and exchange fairs each year.\n\nThe Canada-based \"The Wireless Set No. 19 Group\", with members virtually worldwide, caters to those who collect, restore and/or operate vintage military communications equipment, with emphasis on the World War II Wireless Set No. 19 radio. Many members are Amateur Radio operators who use the equipment for on-air contacts with others.\n\nThe ARRL publishes \"Vintage Radio\", a collection of articles from QST magazine describing vintage equipment and restoration, and CQ Amateur Radio magazine releases a yearly \"Classic Radio Calendar\" featuring full-color vintage radio images.\n\nThe Antique Wireless Association of Southern Africa is devoted to the \"maintenance and preservation of our amateur heritage\" for enthusiasts of older types of short wave radios and amateur equipment, and maintains a museum exhibit in Johannesburg.\n\nThose accustomed to making repairs on solid state equipment are cautioned that vacuum tube gear contain \"potentially lethal voltages\". The practice of discharging power-supply capacitors and keeping one hand in your pocket when working on powered-up gear are essential safety measures. Some older equipment directly connects the metal chassis to one side of the incoming AC line, a practice which results in the entire unit becoming electrified if the wall plug is inserted backwards. Many older radios, such as vintage receivers, are not safety-fused. Restorers generally replace the AC line cord with a more modern 3 wire plug and install an in-line or chassis mount fuseholder. The use of a common station ground connection to all equipment is encouraged. Those who collect, restore or otherwise use vintage radio equipment should also be aware of possible radioactive substances, PCBs, and asbestos.\n\n\n"}
{"id": "1460862", "url": "https://en.wikipedia.org/wiki?curid=1460862", "title": "Yakov Perelman", "text": "Yakov Perelman\n\nYakov Isidorovich Perelman (; December 4, 1882 – March 16, 1942) was a Russian and Soviet science writer and author of many popular science books, including \"Physics Can Be Fun\" and \"Mathematics Can Be Fun\" (both translated from Russian into English).\n\nPerelman was born in 1882 in the town of Białystok, Congress Poland. He obtained the Diploma in Forestry from the Imperial Forestry Institute (Now Saint Petersburg State Forest Technical University) in Saint Petersburg, in 1909. He was influenced by Ernst Mach and probably the Russian Machist Alexander Bogdanov in his pedagogical approach to popularising science. After the success of \"Physics for Entertainment\", Perelman set out to produce other books, in which he showed himself to be an imaginative populariser of science. Especially popular were \"\"Arithmetic for entertainment\", \"Mechanics for entertainment\", \"Geometry for Entertainment\", \"Astronomy for entertainment\", \"Lively Mathematics\", \" Physics Everywhere\", and \"Tricks and Amusements\".\n\nHis famous books on physics and astronomy were translated into various languages by the erstwhile Soviet Union.\n\nThe scientist Konstantin Tsiolkovsky thought highly of Perelman's talents and creative genius, writing of him in the preface of \"Interplanetary Journeys\": \"The author has long been known by his popular, witty and quite scientific works on physics, astronomy and mathematics, which are, moreover written in a marvelous language and are very readable.\"\n\nPerelman has also authored a number of textbooks and articles in Soviet popular science magazines.\n\nIn addition to his educational and scientific writings, he also worked as an editor of science magazines, including \"Nature and People\" and \"In the Workshop of Nature\".\n\nPerelman died from starvation in 1942, during the German Siege of Leningrad. The siege started at 9 September 1941 and lasted 872 days, until \n27 January 1944. The Siege of Leningrad was one of the longest, most destructive sieges of a major city in modern history and one of the costliest in terms of casualties (1,117,000).\n\nHis older brother Yosif was a writer who published under the pseudonym Osip Dymov. He is not related to the Russian mathematician Grigori Perelman, who was born in 1966 to a different Yakov Perelman. However, Grigori Perelman told The New Yorker that his father gave him \"Physics for Entertainment\", and it inspired his interest in mathematics.\n\n\nHe has also written several books on interplanetary travel (\"Interplanetary Journeys, On a Rocket to Stars, and World Expanses\")\n\nIn 1913, Russian bookshops began carrying \"Physics for Entertainment\". The educationalist's new book attracted young readers seeking answers to scientific questions.\n\n\"Physics for Entertainment\" had a unique layout as well as an instructive style. In the preface (11th ed.) Perelman wrote: \"The main objective of \"Physics for entertainment\" is to arouse the activity of scientific imagination, to teach the reader to think in the spirit of the science of physics and to create in his mind a wide variety of associations of physical knowledge with the widely differing facts of life, with all that he normally comes into contact with.\"\n\nIn the foreword, Perelman describes the contents as “conundrums, brain-teasers, entertaining anecdotes, and unexpected comparisons,” adding, “I have quoted extensively from Jules Verne, H. G. Wells, Mark Twain and other writers, because, besides providing entertainment, the fantastic experiments these writers describe may well serve as instructive illustrations at physics classes.” The 13th edition (1936) would be the last published during the author's lifetime. Among the book's notable topics was the idea of a perpetual machine: a hypothetical machine which could run incessantly performing useful work. The author discusses perpetual motion, highlighting many attempts to build such a machine, and explains why they failed. Other topics included how to jump from a moving car, and why, “according to the law of buoyancy, we would never drown in the Dead Sea.”\n\nRandall Munroe, the creator of the web comic xkcd and author of his own popular science books, wrote: \nThe book is a series of a few hundred examples, no more than one or two pages each, asking a question that illustrates some idea in basic physics.\n\nIt’s neat to see what has and hasn’t changed in the last century or so. Many of the examples he uses seem to be straight out of a modern high school physics textbook, while others were totally new to me. And some of the answers to the questions he poses seem obvious, but others made me stop and think. [This] diagram ... shows a design for a fountain with no pump — it took me a while to get why it works... Later in the book, he explains the physics of that drinking bird toy.\nIt’s written in a fun, engaging, conversational style, as if he’s in the room chatting with you about these neat ideas.\n\n\n"}
{"id": "2282254", "url": "https://en.wikipedia.org/wiki?curid=2282254", "title": "Yama-no-Kami", "text": "Yama-no-Kami\n\nYama-no-Kami (山の神) is the name given to a kami of the mountains of the Shinto religion of Japan. These can be of two different types. The first type is a god of the mountains who is worshipped by hunters, woodcutters, and charcoal burners. The second is a god of agriculture who comes down from the mountains and is worshipped by farmers. This kami is generally considered as a goddess, or a female deity.\n\nYama-no-Kami appearing in Japanese mythology include:\n\n"}
{"id": "37564976", "url": "https://en.wikipedia.org/wiki?curid=37564976", "title": "Ájtte", "text": "Ájtte\n\nÁjtte, the Swedish Mountain and Sami Museum (), is a cultural and natural history museum in Jokkmokk in Lapland, Sweden. \n\nÁjtte is a museum, which specializes in the culture and nature of the mountainous area of Northern Sweden, and which is also the main museum and archive for the Sami culture of Sweden. Ájtte is also an information centre for tourism in Lapland. The word \"ájtte\" is a Lule Sami language one, meaning storage hut and referring to the museum as an archive for artifacts of the Sami cultural heritage.\n\nÁjtte was inaugurated in June 1989 and has a staff of about 25 employees. The museum is owned and managed by a foundation, which was established in 1983 by the Swedish Government, the Norrbotten Region, the Jokkmokk Municipality and the two national Sami organizations Svenska Samernas Riksförbund (National Union of Swedish Sami people) and Same Ätnam (Sami land). According to an agreement on financing of the museum, which was entered into the same year, the Government bodies commit themselves to a long term financial contribution to the museum. Such funds are the result of a court decision regarding compensation after rivers in Lapland have been exploited for electric power generation. The Swedish government appoints the chairman and three of the members of the board of the foundation. Thus, government funds cover around half of the current budget of the museum.\n\nSince 1995 Ájtte has established an alpine botanical garden at the valley of Kvarnbäcken in Jokkmokk with plants from different environments of the mountain range of Northern Scandinavia. One of the century old researcher cottages from Sarek National Park, designed and used by the pioneering scientist Axel Hamberg, has been dismantled and moved from Sarek and reerected in the botanical garden.\n\n"}
