{"id": "10273778", "url": "https://en.wikipedia.org/wiki?curid=10273778", "title": "ADA Signs", "text": "ADA Signs\n\nThe term \"ADA Signs\" has come into common use in the architectural, construction and signage industries with the advent of the Americans With Disabilities Act, or ADA. The Americans with Disabilities Act regulates accessibility; and includes requirements for signage that is conveniently located and easy to read both visually and through tactile touch. \n\n\"ADA Signs\" are sometimes misunderstood as being synonymous with braille signs. Signs with braille and raised characters are the most visible manifestation of the law requiring access to the built environment, but the sign standards in the ADA Accessibility Guidelines (or ADAAG) require much more than just braille and raised characters on some signs. In general, almost every sign that would be considered an \"architectural\" sign must comply with one or another of the ADA Guidelines. If a sign identifies a permanent room or space of a facility (including exits), directs or informs about functional spaces of the facility, or identifies, directs to, or informs about accessible features of the facility, it must comply. Signs for advertising and marketing purposes, temporary signs, company logos and names are examples of signs or sections of signs that do not have to comply.\n\nBecause of the rules requiring Braille on some signs, the signage section is most often considered as benefiting persons who are blind and visually impaired. Some of the sign guidelines are also designed to benefit persons with mobility or hearing impairments. In addition, it is generally considered that easy to read and well thought out signage systems are of benefit to deaf people, people who have problems speaking, and people with cognitive disabilities or psychiatric conditions that cause them to avoid speaking to strangers.\n\nThe general rules for signage covered by the law are these:\n\nAll signs (except for reflective parking and other traffic signs) must have non-glare backgrounds and characters. Glare and reflection are a major problem for persons with vision impairments, and particularly for the elderly.\n\nAll signs that contain visual characters must have a high dark to light (or vice versa) contrast between characters and their background. The important issue is not color, but lightness and darkness: a sign with very light gray letters on a charcoal gray background would be acceptable, but a sign with red letters on a black background would not.\n\nAll signs must have \"easy to read\" typefaces. There are different rules for signs that identify rooms and spaces, versus signs that direct and inform. This is because persons who are \"functionally blind\" (have no usable vision) are able to locate doors, and therefore can locate signs adjacent to doors that identify them; but have no consistent way to find directional and information signs that could be located anywhere along corridors.\n\nDirectional and informational signs can use upper and lower case letters (recommended by many experts for visual readability) and \"simple\" serif typefaces of a non-decorative nature. Condensed or extended typefaces are not allowed. Strokes are of medium weight, not too bold or too thin. The size of the letters is dictated by the distance of the sign from the expected position of the sign reader. Character size on these signs is to be determined by a chart in the 2010 ADA Standards for Accessible Design that uses a combination of the height of the text above the floor and the distance the reader has to stand from the sign.\n\nADA signs that identify rooms and spaces are to be located adjacent to the door they identify so they can be located by persons who are functionally blind. For the most part, one sign is used by both tactile and visual readers, so there are compromises to assist tactile readers. However, it is possible to use two separate signs with the same information. Tactile signs require uppercase characters in sans-serif typefaces. (Helvetica is most common; however other sans-serif typefaces can be used.) The characters can be between 5/8 inch and 2 inches high. The Braille must accompany the characters (below the characters) and must be Contracted Braille (formerly called Grade 2 Braille). The signs are installed 48 inches minimum from the baseline of the lowest raised character and 60 inches maximum from the baseline of the highest raised character. (Although the definition of \"character\" doesn't include Braille cells, the Access Board has stated that the 48 inch rule applies to the base of the lowest line of Braille cells.) If pictograms are used to identify the space (for example, restroom signs with gender pictograms), they must be in a six inch high clear field and accompanied by a tactile character and Braille label below the field.\n\nThere are four symbols that stand for accessibility. One is the familiar International Symbol of Access, or \"wheelchair symbol.\" This is used generally to show that persons with mobility impairments can access entrances, restrooms, or pathways. Three are specifically for persons with hearing impairments: the \"ear\" symbol is the International Symbol of Access for Hearing Loss, and is used to show the availability of an assistive listening system. The \"keyboard\" symbol stands for a TTY or text telephone. The \"phone\" symbol with sound waves stands for the availability of a volume controlled phone.\n\nThe standards for the signs (and most other standards used in ADA regulations and state building codes) are the product of the ANSI (American National Standards Institute) A117.1 Committee. This large committee is made up of a balanced group of representatives from industry, the government, disability organizations, designers, code officials, and experts. The committee meets in five year cycles to revise the last published standard. The standard is then used by the International Code Council for its model building code, and has formed the basis of the new version of the ADA Guidelines, now called the 2004 ADA/ABA. (However, with the final publication of the standards by the Department of Justice, we now generally refer to the Guidelines as the 2010 ADA Standards for Accessible Design.)\n\nThe standards had been already adopted by several federal agencies, and had been approved by the Department of Justice and were awaiting final review by the OMB when the Obama Administration came in. Although they are actually a product of the Clinton Administration and had taken eight years to make it through the Bush Administration, the Obama Administration considered them Bush Administration regulations, and held them up for review. They were approved by the Department of Justice for publication on September 15, 2010, and made legally enforceable on March 15, 2012.\n\nInternational Symbol of Access (Wheelchair Symbol)\n"}
{"id": "27970690", "url": "https://en.wikipedia.org/wiki?curid=27970690", "title": "Action for Healthy Kids", "text": "Action for Healthy Kids\n\nAction for Healthy Kids is a public-private partnership of more than 75 organizations dedicated to promoting school health. Founded in 2002, the organization addresses the epidemic of overweight, sedentary, and undernourished youth by focusing on changes in schools to improve nutrition and increase physical activity. Former U.S. Surgeon General David Satcher is the founding chair of the organization.\n\nThe organization has active volunteers in all 50 states and the District of Columbia. Their activities are supported by a variety of organizations, including the Kellogg Company, the National Dairy Council, and General Mills. Working with Kellogg, the organization provided financial support to schools to support National School Breakfast Week. They are partners with the National Dairy Council and the National Football League in a program called \"Fuel Up To Play 60.\"\n\n"}
{"id": "5111982", "url": "https://en.wikipedia.org/wiki?curid=5111982", "title": "Agricultural chemistry", "text": "Agricultural chemistry\n\nAgricultural chemistry is the study of both chemistry and biochemistry which are important in agricultural production, the processing of raw products into foods and beverages, and in environmental monitoring and remediation. These studies emphasize the relationships between plants, animals and bacteria and their environment.\nThe science of chemical compositions and changes involved in the production, protection, and use of crops and livestock. As a basic science, it embraces, in addition to test-tube chemistry, all the life processes through which humans obtain food and fiber for themselves and feed for their animals. As an applied science or technology, it is directed toward control of those processes to increase yields, improve quality, and reduce costs. One important branch of it, chemurgy, is concerned chiefly with utilization of agricultural products as chemical raw materials. Etc.\n\nThe goals of agricultural chemistry are to expand understanding of the causes and effects of biochemical reactions related to plant and animal growth, to reveal opportunities for controlling those reactions, and to develop chemical products that will provide the desired assistance or control. Every scientific discipline that contributes to agricultural progress depends in some way on chemistry. Hence agricultural chemistry is not a distinct discipline, but a common thread that ties together genetics, physiology, microbiology, entomology, and numerous other sciences that impinge on agriculture.\n\nChemical materials developed to assist in the production of food, feed, and fiber include scores of herbicides, insecticides, fungicides, and other pesticides, plant growth regulators, fertilizers, and animal feed supplements. Chief among these groups from the commercial point of view are manufactured fertilizers, synthetic pesticides (including herbicides), and supplements for feeds. The latter include both nutritional supplements (for example, mineral nutrients) and medicinal compounds for the prevention or control of disease.\n\nAgricultural chemistry often aims at preserving or increasing the fertility of soil, maintaining or improving the agricultural yield, and improving the quality of the crop.\n\nWhen agriculture is considered with ecology, the sustainablility of an operation is considered. Modern agrochemical industry has gained a reputation for maximising profits while violating sustainable and ecologically viable agricultural principles. Eutrophication, the prevalence of genetically modified crops and the increasing concentration of chemicals in the food chain (e.g. persistent organic pollutants) are only a few consequences of naive industrial agriculture.\n\n\n"}
{"id": "20646507", "url": "https://en.wikipedia.org/wiki?curid=20646507", "title": "Agricultural marketing", "text": "Agricultural marketing\n\nEfforts to develop agricultural marketing have, particularly in developing countries, tended to concentrate on a number of areas, specifically infrastructure development; information provision; training of farmers and traders in marketing and post-harvest issues; and support to the development of an appropriate policy environment. In the past, efforts were made to develop government-run marketing bodies but these have tended to become less prominent over the years.\n\nEfficient marketing infrastructure such as wholesale, retail and assembly markets and storage facilities is essential for cost-effective marketing, to minimize post-harvest losses and to reduce health risks. Markets play an important role in rural development, income generation, food security, and developing rural-market linkages. Experience shows that planners need to be aware of how to design markets that meet a community's social and economic needs and how to choose a suitable site for a new market. In many cases sites are chosen that are inappropriate and result in under-use or even no use of the infrastructure constructed. It is also not sufficient just to build a market: attention needs to be paid to how that market will be managed, operated and maintained.\n\nRural assembly markets are located in production areas and primarily serve as places where farmers can meet with traders to sell their products. These may be occasional (perhaps weekly) markets, such as haat bazaars in India and Nepal, or permanent. Terminal wholesale markets are located in major metropolitan areas, where produce is finally channelled to consumers through trade between wholesalers and retailers, caterers, etc. The characteristics of wholesale markets have changed considerably as retailing changes in response to urban growth, the increasing role of supermarkets and increased consumer spending capacity. These changes may require responses in the way in which traditional wholesale markets are organized and managed.\n\nRetail marketing systems in western countries have broadly evolved from traditional street markets through to the modern hypermarket or out-of-town shopping center. In developing countries, there remains scope to improve agricultural marketing by constructing new retail markets, despite the growth of supermarkets, although municipalities often view markets primarily as sources of revenue rather than infrastructure requiring development. Effective regulation of markets is essential. Inside a market, both hygiene rules and revenue collection activities have to be enforced. Of equal importance, however, is the maintenance of order outside the market. Licensed traders in a market will not be willing to cooperate in raising standards if they face competition from unlicensed operators outside who do not pay any of the costs involved in providing a proper service.\n\nEfficient market information can be shown to have positive benefits for farmers and traders. Up-to-date information on prices and other market factors enables farmers to negotiate with traders and also facilitates spatial distribution of products from rural areas to towns and between markets. Most governments in developing countries have tried to provide market information services to farmers, but these have tended to experience problems of sustainability. Moreover, even when they function, the service provided is often insufficient to allow commercial decisions to be made because of time lags between data collection and dissemination. Modern communications technologies open up the possibility for market information services to improve information delivery through SMS on cell phones and the rapid growth of FM radio stations in many developing countries offers the possibility of more localised information services. In the longer run, the internet may become an effective way of delivering information to farmers. However, problems associated with the cost and accuracy of data collection still remain to be addressed. Even when they have access to market information, farmers often require assistance in interpreting that information. For example, the market price quoted on the radio may refer to a wholesale selling price and farmers may have difficulty in translating this into a realistic price at their local assembly market. Various attempts have been made in developing countries to introduce commercial market information services but these have largely been targeted at traders, commercial farmers or exporters. It is not easy to see how small, poor farmers can generate sufficient income for a commercial service to be profitable although in India a new service introduced by Thomson Reuters was reportedly used by over 100,000 farmers in its first year of operation. Esoko in West Africa attempts to subsidize the cost of such services to farmers by charging access to a more advanced feature set of mobile-based tools to businesses.\n\nFarmers frequently consider marketing as being their major problem. However, while they are able to identify such problems as poor prices, lack of transport and high post-harvest losses, they are often poorly equipped to identify potential solutions. Successful marketing requires learning new skills, new techniques and new ways of obtaining information. Extension officers working with ministries of agriculture or NGOs are often well-trained in agricultural production techniques but usually lack knowledge of marketing or post-harvest handling.\n\nAgricultural marketing needs to be conducted within a supportive policy, legal, institutional, macro-economic, infrastructural and bureaucratic environment. Traders and others are generally reluctant to make investments in an uncertain policy climate, such as those that restrict imports and exports or internal produce movement. Businesses have difficulty functioning when their trading activities are hampered by excessive bureaucracy. Inappropriate law can distort and reduce the efficiency of the market, increase the costs of doing business and retard the development of a competitive private sector. Poor support institutions, such as agricultural extension services, municipalities that operate markets inefficiently and inadequate export promotion bodies, can be particularly damaging. Poor roads increase the cost of doing business, reduce payments to farmers and increase prices to consumers. Finally, corruption can increase the transaction costs faced by those in the marketing chain.\n\nMost governments have at some stage made efforts to promote agricultural marketing improvements. In the United States the Agricultural Marketing Service (AMS) is a division of USDA and has programs that provide testing, support standardization and grading and offer market news services. AMS oversees marketing agreements and orders research and promotion programs. It also purchases commodities for federal food programs. USDA also provides support to agricultural marketing work at various universities. In the United Kingdom, support for marketing of some commodities was provided before and after the Second World War by boards such as the Milk Marketing Board and the Egg Marketing Board. These boards were closed down in the 1970s. As a colonial power, Britain established marketing boards in many countries, particularly in Africa. Some continue to exist although many were closed at the time of the introduction of structural adjustment measures in the 1990s.\n\nSeveral developing countries have established government-sponsored marketing or agribusiness units. South Africa, for example, started the National Agricultural Marketing Council (NAMC) as a response to the deregulation of the agriculture industry and closure of marketing boards in the country. India has the long-established National Institute of Agricultural Marketing. These are primarily research and policy organizations, but other agencies provide facilitating services for marketing channels, such as the provision of infrastructure, market information and documentation support. Examples from the Caribbean include the National Agricultural Marketing Development Corporation (NAMDEVCO) in Trinidad and Tobago and the New Guyana Marketing Corporation in Guyana.\n\nNew marketing linkages between agribusiness, large retailers and farmers are gradually being developed, e.g. through contract farming, group marketing and other forms of collective action.\nDonors and NGOs are paying increasing attention to ways of promoting direct linkages between farmers and buyers within a value chain context. More attention is now being paid to the development of regional markets (e.g. East Africa) and to structured trading systems that should facilitate such developments. The growth of supermarkets, particularly in Latin America and East and South East Asia, is having a significant impact on marketing channels for horticultural, dairy and livestock products. Nevertheless, “spot” markets will continue to be important for many years, necessitating attention to infrastructure improvement such as for retail and wholesale markets.\n\n\n"}
{"id": "40345875", "url": "https://en.wikipedia.org/wiki?curid=40345875", "title": "Alcohol and Native Americans", "text": "Alcohol and Native Americans\n\nNative Americans in the United States have historically had extreme difficulty with the use of alcohol. Problems continue among contemporary Native Americans; 82% of the deaths among Native Americans and Alaska Natives are alcohol-related. Use of alcohol varies by age, gender and tribe with women, and older women in particular, being least likely to be regular drinkers. Native Americans, particularly women, are more likely to abstain entirely from alcohol than the general US population. Frequency of use among Native Americans is generally less than the general population, but the quantity consumed when it is consumed is generally greater.\n\nA survey of death certificates over a four-year period showed that deaths among Native Americans due to alcohol are about four times as common as in the general US population and are often due to traffic collisions and liver disease with homicide, suicide, and falls also contributing. Deaths due to alcohol among Native Americans are more common in men and among Northern Plains Indians. Alaska Natives showed the least incidence of death. Alcohol abuse by Native Americans has been shown to be associated with development of disease, including sprains and muscle strains, hearing and vision problems, kidney and bladder problems, head injuries, pneumonia, tuberculosis, dental problems, liver problems, and pancreatitis. In some tribes, the rate of fetal alcohol spectrum disorder is as high as 1.5 to 2.5 per 1000 live births, more than seven times the national average, while among Alaska natives, the rate of fetal alcohol spectrum disorder is 5.6 per 1000 live births.\n\nNative American youth are far more likely to experiment with alcohol than other youth with 80% alcohol use reported. Low self-esteem is thought to be one cause. Active efforts are underway to build self-esteem among youth and to combat alcoholism among Native Americans.\n\nPrior to contact with colonists, alcohol use and production was mainly concentrated in the southwestern United States. Some tribes produced weak beers, wine and other fermented beverages, but they had low alcohol concentrations (8–14%) and were to be used only for ceremonial purposes. The distillation technique required to make stronger, potent forms of alcohol were unknown. It was well documented that Mexican Native Americans prepared over forty different alcoholic beverages from a variety of plant substances, such as honey, palm sap, wild plum, and pineapple. In the Southwestern U.S., the Papago, Piman, Apache and Maricopa all used the saguaro cactus to produce a wine, sometimes called \"haren a pitahaya\". The Coahuiltecan in Texas combined mountain laurel with the Agave plant to create an alcoholic drink, and the Pueblos and Zunis were believed to have made fermented beverages from aloe, maguey, corn, prickly pear, pitahaya and even grapes. To the east, the Creek of Georgia and Cherokee of the Carolinas used berries and other fruits to make alcoholic beverages, and in the Northeast, there is some evidence that the Huron made a mild beer made from corn. In addition, despite the fact that they had little to no agriculture, both the Aleuts and Yuit of Alaska were believed to have made alcoholic drinks from fermented berries.\n\nWhen Europeans began making a large quantity of distilled spirits and wine available to American Indians, the tribes had very little time to adapt and develop social, legal, or moral guidelines to regulate alcohol use. Early traders built a large demand for alcohol by using it as a means to trade, using it in exchange for highly sought after animal skins and other materials and resources. Traders also discovered that giving free alcohol to the Native Americans during trading sessions made the likelihood of trading much higher. Extreme intoxication was common among the colonists, contrary to the inexperienced native populations. Numerous historical accounts describe extremely violent bouts of drinking among native tribes during trading sessions and on other occasions, but at least as many accounts exist of similar behavior among the colonizing traders, military personnel, and civilians. Such modeling was not limited to the early colonial era but continued as the land was colonized from east to west; trappers, miners, soldiers, and lumbermen were notorious for their heavy drinking sessions. History may have therefore sown the seeds for the prevalence of alcohol abuse in North American indigenous populations. Early demand, with no regulation and strong encouragement, may have contributed to a heavy alcohol use. It was then passed down from generation to generation, which has led to the current high level of alcohol-related problems.\n\nThe use of alcohol originated in Middle America but rapidly diffused to Northern Mexico and from there to the Southwestern United States. The majority of aboriginal production and use of alcoholic beverages was in this region. However, there was a surprising number of scattered accounts of intoxicating beverage use throughout the United States prior to White contact. For the most part, the use of alcoholic drinks required an agricultural base but not in all instances. The reason for this is primarily that alcoholic beverages were made from domesticated plants, but there are examples of liquor being derived from wild plants. Aboriginal use generally did not involve excessive drunkenness but controlled and supervised use often in highly ritualized occasions. Further, accounts of American Indians' initial encounters with alcoholic beverages did not describe reckless or disinhibited behavior.\n\nRather than infatuation, most Native peoples initially responded to alcohol with distaste and suspicion. They considered drunkenness \"degrading to free men\" and questioned the motives of those who would offer a substance that was so offensive to the senses and that made men foolish. Most Native people who did drink alcohol were reported to show \"remarkable restraint while in their cups\". Most drank alcohol only during social or trading contact with whites. Although drinking patterns since colonization grew almost exponentially, since 1975, drinking patterns among Native Americans have remained constant. Around the world, since 1975, Native Americans can be found more commonly among other US citizens in places that serve alcohol.\n\nAfter colonial contact, white drunkenness was interpreted by whites as the misbehavior of an individual. Native drunkenness was interpreted in terms of the inferiority of a race. What emerged was a set of beliefs known as firewater myths that misrepresented the history, nature, sources and potential solutions to Native alcohol problems. These myths proclaimed that Indian people:\n\nThe scientific literature has refuted the claims to many of these myths by documenting the wide variability of alcohol problems across and within Native tribes and the very different response that certain individuals have to alcohol opposed to others. Another important way that scientific literature has refuted these myths is by identifying that there are no current discovered genetic or other biological anomalies that render Native peoples particularly vulnerable to alcoholism.\n\nThere are long standing beliefs that date back to colonial times when it was thought native people all over the world were particularly vulnerable to addiction, but there is no evidence of this. \nIt has been found that the incidence of alcohol abuse varies with gender, age, and tribal culture and history. While little detailed genetic research has been done, it has been shown that alcoholism tends to run in families with possible involvement of differences in alcohol metabolism and the genotype of alcohol-metabolizing enzymes. There is no evidence, however, that these genetic factors are more prevalent in Native Americans than other ethnic groups. According to one 2013 review of academic literature on the issue, there is a \"substantial genetic component in Native Americans\" and that \"most Native Americans lack protective variants seen in other populations.\" Many scientists have provided evidence of the genetic component of alcoholism by the biopsychosocial model of alcoholism, but the molecular genetics research currently has not found one specific gene that is responsible for the rates of alcoholism among Native Americans, implying the phenomenon may be due to an interplay of multiple genes and environmental factors.\n\nHigh concentrations of thiamin found in beans may buffer symptoms of alcoholism while the preparation of maize using “lime water” in the traditional preparation of tortillas may free folate for human biological use. The agricultural food practices in Mesoamerica differed from the dietary food preparation of North American indigenous people. Because of the differences in diet, the effects of tequila on Mesoamerican Native Americans with regards to macrocytic anemia and alcohol induced beri-beri disease and may be less pronounced than the effects of whisky or other ethanol beverages in North American tribes that do not pre-treat maize with alkaline solutions prior to eating.\n\nThe National Institute on Alcohol Abuse and Alcoholism, or NIAAA, defines binge drinking as a pattern of drinking that brings blood alcohol concentration (BAC) levels to 0.08 g/dL. This typically occurs after 4 drinks for women and 5 drinks for men, in about 2 hours.\n\nAnastasia M. Shkilnyk, who conducted an observational study of the Asubpeeschoseewagong First Nation of Northwestern Ontario in the late 1970s, when they were demoralized by Ontario Minamata disease, has observed that heavy Native American drinkers may not be physiologically dependent on alcohol, but they abuse it by engaging in binge drinking, a practice associated with child neglect, violence, and impoverishment.\n\nResearchers from the Institute of Psychiatry and Stanford School of Medicine outlined the following stages of alcohol dependence: \n\nIndian youth become socialized into the culture of alcohol at an early age this pattern of testing alcohol limits persists until early adulthood. Approximately 20 percent of Indian youth between 7th and 12th grade belong in this category. Other youth exhibit an experimental pattern of drinking through adolescence and this is noted as one of the biggest identifiers of binge drinking later in life. Given the high rates of alcohol and substance abuse on reservations, researchers have seen higher rates of academic failure, delinquency, violent criminal behavior, suicidality, and alcohol-related mortality among Native American youth, which is far greater than the rest of the United States population.\n\nCompared with the US population in general, the American Indian population is much more susceptible to alcoholism and related diseases and deaths. According to IHS records on alcohol-related illness, the mortality due to alcohol was as much as 5.6 times higher among the Indian population than among the U.S. population in general. The rate was 7.1 times higher in 1980. Disproportionately more males are affected by alcohol related conditions than females. The highest risk of alcohol-related deaths is between 45 and 64. Chronic liver disease and cirrhosis are 3.9 times as prevalent in the Indian population as in the general U.S. population. Alcohol-related fatal car accidents are three times as prevalent. Alcohol was shown to be a factor in 69% of all suicides of American Indians. This number has grown larger since 1975.\n\nDuring the past twenty years, there has been growing recognition among health care professionals that domestic violence is a highly prevalent public health problem with devastating effects on individuals, families, and communities. A risk factor is a characteristic that has a high correlation with levels of domestic violence. They include the offender and victim both being under the age of 40, substance abuse, receiving public assistance, and the offender and/or the victim witnessing domestic violence between their parents as a child. For abuse victims, the health care setting offers a critical opportunity for early identification and even primary prevention of abuse. Alcohol and drugs use is attributed higher rates of domestic violence among Native Americans compared to many other demographics. Over two-thirds (68%) of American Indian and Alaska Native sexual assault victims attribute their attacker's actions to drinking and/or taking drugs before the offense\n\nNative Americans have one of the highest rates of fetal alcohol syndrome recorded. According to the Centers for Disease Control and Prevention, from 1981 to 1991, the prevalence of FAS in the overall US population per 10,000 births was 2.1. Among American Indians, that number was 31.0. The significant difference between the FAS rates of the US population and American Indians has been attributed to a lack of healthcare, high poverty levels, and a young average population. Healthcare spending for an average American on Medicare is about $11,762 whereas average spending on healthcare for a Native American is $2,782. In a 2007 document, \"Fetal Alcohol Spectrum Disorders among Native Americans,\" the US Department of Health and Human Services reported that the prevalence of fetal alcohol syndrome in Alaska was 1.5 per 1,000 live births but, among American Indians and Alaska Natives, the rate was 5.6.\n\nThe Alcohol and Substance Abuse Program (ASAP) is a program for American Indian and Alaska Native individuals to reduce the incidence and prevalence of alcohol and substance abuse. These programs are administered in tribal communities, including emergency, inpatient and outpatient treatment and rehabilitation services for individuals covered under Indian Health Services. It addresses and treats alcoholism from a disease model perspective.\n\nThe Indian Alcohol and Substance Abuse Prevention and Treatment Act of 1986 was updated to make requirements that the Office of Indian Alcohol and Substance Abuse (OIASA) is to work with federal agencies to assist Native American communities in developing a Tribal Action Plan (TAP). The TAP coordinates resources and funding required to help mitigates levels of alcohol and substance abuse among the Native American population.\n\n\n"}
{"id": "35830787", "url": "https://en.wikipedia.org/wiki?curid=35830787", "title": "Alvin J. Siteman Cancer Center", "text": "Alvin J. Siteman Cancer Center\n\nThe Alvin J. Siteman Cancer Center at Barnes-Jewish Hospital and Washington University School of Medicine is a cancer treatment, research and education institution with six locations in the St. Louis area. Siteman is the only cancer center in Missouri and within 240 miles of St. Louis to be designated a Comprehensive Cancer Center by the National Cancer Institute (NCI). Siteman is also the only area member of the National Comprehensive Cancer Network, a nonprofit alliance of 28 cancer centers dedicated to improving the quality and effectiveness of cancer care.\n\nIn 2015, Siteman received the highest rating possible - exceptional - by the NCI for cancer research. At Siteman's review leading to the rating, researchers presented their findings in genomics, cancer imaging, cancer prevention and disparities, and the use of the body's immune system to fight cancer.\n\nIn 2018, Siteman was named a top 15 U.S. cancer center by U.S. News & World Report. The recognition is part of the overall ranking of its parent institutions, Barnes-Jewish Hospital, which is tied for No. 11 on the newsmagazine's “The Best Hospitals 2018-19 Honor Roll,” and Washington University School of Medicine, which is No. 8 on the newsmagazine's \"2019 Best Medical Schools: Research\" list.\n\nIn 2014, Siteman treated nearly 9,000 newly diagnosed cancer patients and every year provides continuing care to about 40,000 people, making it one of the largest cancer centers in the United States.\n\nSiteman’s main facility is at Washington University Medical Center in St. Louis’ Central West End neighborhood. Five other St. Louis-area sites offer specialized cancer care in suburban locations:\n\nIn 1999, Alvin J. and Ruth Siteman committed $35 million to the development of the Siteman Cancer Center at Washington University School of Medicine and Barnes-Jewish Hospital. The commitment was the largest gift ever received by Barnes-Jewish and Washington University in support of cancer research, patient care and services, education and community outreach.\n\nTimothy Eberlein, M.D., has been director of the center since its inception. John DiPersio, M.D., Ph.D., is deputy director.\n\nIn 2001, the NCI designated Siteman as a Cancer Center, which signaled that the institution had demonstrated significant scope and quality in its cancer research programs. The designation came with $850,000 per year in federal research grants. The NCI named Siteman a Comprehensive Cancer Center in 2005, recognizing its broad-based research, outreach and education activities, and awarded the center a five-year, $21 million support grant. The NCI renewed the designation in 2010 and awarded another five-year grant, totaling $23 million. The grants fund programs and specialized services that promote multidisciplinary research, as well as shared scientific resources and seed awards that enable investigators to develop and pursue new research opportunities.\n\nAlvin J. Siteman announced in 2010 that he would donate $1 million annually to an endowment fund at the center to advance cancer prevention, diagnosis and treatment programs that might not receive federal funding.\n\nMore than 350 Washington University research scientists and physicians provide inpatient and outpatient care at Siteman. The center also offers patient and family support services, including discussion and education groups.\n\nScientists and physicians affiliated with Siteman hold more than $145 million in cancer research and related training grants. The results of basic laboratory research are rapidly incorporated into treatment advances. This process is enhanced by patient access to more than 500 therapeutic clinical studies, including many collaborative efforts with other leading cancer centers throughout the country.\n\nIn 2013, three scientists affiliated with Siteman, Washington University School of Medicine and the McDonnell Genome Institute were included on the Thomson Reuters list of “Hottest Scientific Researchers of 2012”: Richard K. Wilson, Ph.D.; Elaine Mardis, Ph.D.; and Li Ding, Ph.D. The list recognized the 21 most-cited researchers of 2012. Robert Fulton, a fourth scientist from Washington University School of Medicine and the McDonnell Genome Institute, also made the list.\n\nResearchers affiliated with Siteman and/or Washington University School of Medicine have pioneered important advances in cancer research, prevention, education and treatment. Highlights and ongoing studies include these projects:\n\n2018 — Personalized brain cancer vaccines\n\n\n2017 — CAR-T cell therapy and using Zika virus to fight brain cancer\n\n\n2016 — Chemotherapy for brain tumors\n\n\n2015 — Melanoma vaccine and urine test for kidney cancer\n\n\n2014 — Breast cancer vaccine and cancer goggles\n\n\n2013 — Endometrial cancer and leukemia\n\n\n2012 — Leukemia, breast cancer research and cancer prevention\n\n\n2011 — Blood-related cancers\n\n\n2010 — Pediatric cancers\n\n\n2008 — Genetic sequencing\n\n\n2007 — Nanotechnology and radiation therapy\n\n\n2006 — Photoacoustic imaging\n\n\n2003 — Breast cancer\n\n\n2001 — Imaging and the immune system’s role in controlling cancer\n\n\n1998 — Biopsies\n\n\n1994 — Genetic screening test for thyroid cancer\n\n\n1979 — Bone marrow transplants\n\n\nMid-1970s — Imaging\n\n1954 — Growth factors and cancer\n\n\n1946 — Radiocarbon in cancer research\n\n\n1941 — Cyclotron\n\n\n1933 — Lung cancer surgery and the disease’s link to smoking\n\n\nSiteman and Washington University School of Medicine are actively engaged in many projects to prevent cancer in the St. Louis region and across the United States. These efforts include:\n\nIn addition to treatment and research programs, Siteman is involved with community outreach, education and screening. Efforts include:\n\n\nIn 2017, Siteman Cancer Center launched the Siteman Cancer Network, an affiliation with regional medical centers that is aimed at improving the health of individuals and communities through cancer research, treatment and prevention. Network members are Boone Hospital Center's Stewart Cancer Center in Columbia, Missouri and Phelps County Regional Medical Center's Delbert Day Cancer Institute in Rolla, Missouri.\n\n"}
{"id": "46695301", "url": "https://en.wikipedia.org/wiki?curid=46695301", "title": "American Anti Drug Council", "text": "American Anti Drug Council\n\nAmerican Anti-Drug Council originated as an anti-marijuana campaign. The mission of this council is to help teens stand up to negative choices and aims to educate about the dangers of Illegal drugs. The council tries to keep pages on social network sites from posting illegal drug content.\nThe council has even blasted some celebrities for posting smoking selfies. \n\nThe AADC is located in the state of Tennessee and was established in 2008. \nThe American Anti-Drug Council has run many anti-drug campaigns. In 2014 it ran a campaign called \"College 101\" which looked at how college kids are using illegal drugs to stay up longer to study. Another was the \"Forget Pot\" campaign, which focused on how smoking marijuana has caused a lot of teens to fail classes or even drop out of high school. Its 2015 campaign is called \"Pick Healthy.\" This is not focused on a particular drug, but tries to get people to pick healthy fruits instead of using deadly drugs. This campaign wants people to use the \"#pickhealthy\" hastag when someone is eating a healthy treat.\n\nThe American Anti-Drug Council has blasted many celebrities for posting smoking selfies or drug content images. The AADC requests that if these celebrities post this kind of content they should have +18 in their bio which means no one under 18 should view their page. The AADC has posted comments on Snoop Dogg's Instagram claiming that he is \"targeting the youth to smoke marijuana.\"\n"}
{"id": "7550634", "url": "https://en.wikipedia.org/wiki?curid=7550634", "title": "Arseculeratne v. Priyani Soysa", "text": "Arseculeratne v. Priyani Soysa\n\nArseculeratne v. Priyani Soysa is a landmark and controversial case of alleged medical malpractice in Sri Lanka. Apart from being the first such case in recent times, it is also unique because the principal parties to the case were well known professionals of the country - lawyer Rienzie Arseculeratne (Plaintiff) and Emeritus Professor of Paediatrics, Priyani Soysa (Defendant).\n\nThe plaintiff's three-year-old daughter Suhani, was shown to the defendant with the complaint of dragging of feet. The defendant examined the child and observed involuntary movements of the upper limbs based on which she made a diagnosis of Rheumatic Chorea and admitted the child to hospital for further management. As there was no improvement in the child's condition after several days of hospital stay, the plaintiff discharged his daughter from the care of the defendant and entrusted her to Professor Sanath Lamabathusooriya, Professor of Paediatrics of the Faculty of Medicine, University of Colombo.\n\nProf. Lamabadusuriya initially concurred with the diagnosis of Rheumatic Chorea, making an entry in the Bed Head Ticket- \"All features of Rheumatic Chorea Seen\". He however noted that, unlike in Rheumatic Chorea, the tendon reflexes were brisk. Two days later after re-examining the child, Prof. Lamabadusuriya ordered a CT scan of the brain, since he could not exclude the possibility of a Space Occupying Lesion in the Brain. \n\nThe CT Scan revealed a Brain Stem Glioma (BSG), which is a growth of certain nerve cells in the proximal part of the brain which houses most of the vital structures necessary for life - hence it is associated with a very poor prognosis . The local neurosurgeons consulted were unanimous in their opinion that the tumour was inoperable. The distressed parents thereafter took the child to the United Kingdom for review by Neurosurgeon Dr. Srilal Dias, who also stated that no surgery was possible in the present state.\n\nThe child died a few days after returning to Sri Lanka.\n\nThe plaintiff's case was that the defendant was negligent in not diagnosing Brain Stem Glioma and in the misdiagnosis of Rheumatic Chorea. Had a timely diagnosis been made, it was argued by the plaintiff, survival or prolongation of life would have been possible.\n\nAlthough the plaintiff listed several specialist doctors including Prof. Lamabadusuriya as witnesses, only Dr. Srilal Dias was called to give evidence. It was alleged by his counsel that doctors in the country were reluctant to testify against the defendant in view of her esteemed standing in the medical profession. \n\nDr. Dias claimed that had the BSG been diagnosed earlier curative radiotherapy or surgery would have been possible. His claim was disputed by all expert witnesses who testified for the defendant, including the eminent neurosurgeon Dr. Shelton Cabraal. Since the tumour was found in the brain stem region it is extremely unlikely that any form of treatment would have had prospect of cure.\n\nThe defendant argued that since Prof. Lamabadusuriya too made an initial diagnosis of Rheumatic Chorea and ordered a CT scan only two days later, she was not negligent in her care. This argument was controversially rejected by all courts including the Supreme Court, which held that the defendant was negligent in not ordering a CT Scan.\n\nSince Brain Stem Glioma was a terminal condition with no prospect of effective treatment, it was also argued for the defendant that even if her negligence was established, causation had not been proved and as such the plaintiff's action should fail. It was on this ground that the Supreme Court allowed her appeal.\n\nThe case lasted almost a decade traversing the full extent of litigation in the country - from the District Court of Colombo to the Court of Appeal to finally the Supreme Court. The District Court upheld the Plaintiff's case and awarded him Rs. 5,000,000 (around US$ 125,000 at the time) and costs. \n\nArseculeratne was represented by eminent civil lawyer President's Counsel Romesh de Silva throughout the proceedings while Prof. Soysa was defended by Queen's Counsel Vernon Wijetunga at the District Court, and later by President's Counsel H.L. de Silva and senior lawyer R.K.W. Goonesekere at the appellate courts.\n\nProf. Soysa's appeal to the Court of Appeal was dismissed, however the quantum of damages was reduced as under the Common Law of Sri Lanka which is Roman Dutch law, damages could only be awarded for patrimonial loss. The Supreme Court allowed Prof. Soysa's appeal setting aside judgments of both lower courts. In a landmark judgment heavily critical of the decisions of the lower courts, the Supreme Court held that causation was not established on a balance of probabilities by the plaintiff. The defendant was also allowed costs of action, which she declined to accept.\n\n"}
{"id": "424015", "url": "https://en.wikipedia.org/wiki?curid=424015", "title": "Asbestosis", "text": "Asbestosis\n\nAsbestosis is long term inflammation and scarring of the lungs due to asbestos. Symptoms may include shortness of breath, cough, wheezing, and chest pain. Complications may include lung cancer, mesothelioma, and pulmonary heart disease.\nAsbestosis is caused by breathing in asbestos fibers. Generally it requires a relatively large exposure over a long period of time. Such levels of exposure typically only occur in those who work with the material. All types of asbestos fibers are associated with concerns. It is generally recommended that currently existing asbestos be left undisturbed. Diagnosis is based upon a history of exposure together with medical imaging. Asbestosis is a type of interstitial pulmonary fibrosis.\nThere is no specific treatment. Recommendations may include influenza vaccination, pneumococcal vaccination, oxygen therapy, and stopping smoking. Asbestosis affected about 157,000 people and resulted in 3,600 deaths in 2015. Asbestos use has been banned in a number of countries in an effort to prevent disease.\n\nThe signs and symptoms of asbestosis typically manifest after a significant amount of time has passed following asbestos exposure, often several decades under current conditions in the US. The primary symptom of asbestosis is generally the slow onset of shortness of breath, especially with physical activity. Clinically advanced cases of asbestosis may lead to respiratory failure. When a physician listens with a stethoscope to the lungs of a person with asbestosis, they may hear inspiratory crackles.\n\nThe characteristic pulmonary function finding in asbestosis is a restrictive ventilatory defect. This manifests as a reduction in lung volumes, particularly the vital capacity (VC) and total lung capacity (TLC). The TLC may be reduced through alveolar wall thickening; however, this is not always the case. Large airway function, as reflected by FEV/FVC, is generally well preserved. In severe cases, the drastic reduction in lung function due to the stiffening of the lungs and reduced TLC may induce right-sided heart failure (cor pulmonale). In addition to a restrictive defect, asbestosis may produce reduction in diffusion capacity and a low amount of oxygen in the blood of the arteries.\n\nThe cause of asbestosis is the inhalation of microscopic asbestos mineral fibers suspended in the air. In the 1930s, E. R. A. Merewether found that greater exposure resulted in greater risk.\n\nAsbestosis is the scarring of lung tissue (beginning around terminal bronchioles and alveolar ducts and extending into the alveolar walls) resulting from the inhalation of asbestos fibers. There are two types of fibers: amphibole (thin and straight) and serpentine (curly). All forms of asbestos fibers are responsible for human disease as they are able to penetrate deeply into the lungs. When such fibers reach the alveoli (air sacs) in the lung, where oxygen is transferred into the blood, the foreign bodies (asbestos fibers) cause the activation of the lungs' local immune system and provoke an inflammatory reaction dominated by lung macrophages that respond to chemotactic factors activated by the fibers. This inflammatory reaction can be described as chronic rather than acute, with a slow ongoing progression of the immune system attempting to eliminate the foreign fibers. Macrophages phagocytose (ingest) the fibers and stimulate fibroblasts to deposit connective tissue. Due to the asbestos fibers' natural resistance to digestion, some macrophages are killed and others release inflammatory chemical signals, attracting further lung macrophages and fibrolastic cells that synthesize fibrous scar tissue, which eventually becomes diffuse and can progress in heavily exposed individuals. This tissue can be seen microscopically soon after exposure in animal models. Some asbestos fibers become layered by an iron-containing proteinaceous material (ferruginous body) in cases of heavy exposure where about 10% of the fibers become coated. Most inhaled asbestos fibers remain uncoated. About 20% of the inhaled fibers are transported by cytoskeletal components of the alveolar epithelium to the interstitial compartment of the lung where they interact with macrophages and mesenchymal cells. The cytokines, transforming growth factor beta and tumor necrosis factor alpha, appear to play major roles in the development of scarring inasmuch as the process can be blocked in animal models by preventing the expression of the growth factors. The result is fibrosis in the interstitial space, thus asbestosis. This fibrotic scarring causes alveolar walls to thicken, which reduces elasticity and gas diffusion, reducing oxygen transfer to the blood as well as the removal of carbon dioxide. This can result in shortness of breath, a common symptom exhibited by individuals with asbestosis.\n\nAccording to the American Thoracic Society (ATS), the general diagnostic criteria for asbestosis are:\n\nThe abnormal chest x-ray and its interpretation remain the most important factors in establishing the presence of pulmonary fibrosis. The findings usually appear as small, irregular parenchymal opacities, primarily in the lung bases. Using the ILO Classification system, \"s\", \"t\", and/or \"u\" opacities predominate. CT or high-resolution CT (HRCT) are more sensitive than plain radiography at detecting pulmonary fibrosis (as well as any underlying pleural changes). More than 50% of people affected with asbestosis develop plaques in the parietal pleura, the space between the chest wall and lungs. Once apparent, the radiographic findings in asbestosis may slowly progress or remain static, even in the absence of further asbestos exposure. Rapid progression suggests an alternative diagnosis.\n\nAsbestosis resembles many other diffuse interstitial lung diseases, including other pneumoconiosis. The differential diagnosis includes idiopathic pulmonary fibrosis (IPF), hypersensitivity pneumonitis, sarcoidosis, and others. The presence of pleural plaquing may provide supportive evidence of causation by asbestos. Although lung biopsy is usually not necessary, the presence of asbestos bodies in association with pulmonary fibrosis establishes the diagnosis. Conversely, interstitial pulmonary fibrosis in the absence of asbestos bodies is most likely not asbestosis. Asbestos bodies in the absence of fibrosis indicate exposure, not disease.\n\nThere is no cure available for asbestosis. Oxygen therapy at home is often necessary to relieve the shortness of breath and correct underlying low blood oxygen levels. Supportive treatment of symptoms includes respiratory physiotherapy to remove secretions from the lungs by postural drainage, chest percussion, and vibration. Nebulized medications may be prescribed in order to loosen secretions or treat underlying chronic obstructive pulmonary disease. Immunization against pneumococcal pneumonia and annual influenza vaccination is administered due to increased sensitivity to the diseases. Those with asbestosis are at increased risk for certain cancers. If the person smokes, quitting the habit reduces further damage. Periodic pulmonary function tests, chest x-rays, and clinical evaluations, including cancer screening/evaluations, are given to detect additional hazards.\n\nThe death of English textile worker Nellie Kershaw in 1924 from pulmonary asbestosis was the first case to be described in medical literature, and the first published account of disease attributed to occupational asbestos exposure. However, her former employers (Turner Brothers Asbestos) denied that asbestosis even existed because the medical condition was not officially recognised at the time. As a result, they accepted no liability for her injuries and paid no compensation, either to Kershaw during her final illness or to her bereaved family after she had died. Even so, the findings of the inquest into her death were highly influential insofar as they led to a parliamentary enquiry by the British Parliament. The enquiry formally acknowledged the existence of asbestosis, recognised that it was hazardous to health and concluded that it was irrefutably linked to the prolonged inhalation of asbestos dust. Having established the existence of asbestosis on a medical and judicial basis, the report resulted in the first Asbestos Industry Regulations being published in 1931, which came into effect on 1 March 1932.\n\nThe first lawsuits against asbestos manufacturers occurred in 1929. Since then, many lawsuits have been filed against asbestos manufacturers and employers, for neglecting to implement safety measures after the link between asbestos, asbestosis and mesothelioma became known (some reports seem to place this as early as 1898 in modern times). The liability resulting from the sheer number of lawsuits and people affected has reached billions of dollars. The amounts and method of allocating compensation have been the source of many court cases, and government attempts at resolution of existing and future cases.\n\nTo date, about 100 companies have declared bankruptcy at least partially due to asbestos-related liability. In accordance with Chapter 11 and § 524(g) of the federal bankruptcy code, a company may transfer its liabilities and certain assets to an asbestos personal injury trust, which is then responsible for compensating present and future claimants. Since 1988, 60 trusts have been established to pay claims with about $37 billion in total assets. From 1988 through 2010, analysis from the United States Government Accountability Office indicates that trusts have paid about 3.3 million claims valued at about $17.5 billion.\n\nSome notable persons who have died from lung fibrosis associated with asbestos include:\n\n\n"}
{"id": "22523849", "url": "https://en.wikipedia.org/wiki?curid=22523849", "title": "Benzopyrene", "text": "Benzopyrene\n\nA benzopyrene is an organic compound with the formula CH. Structurally speaking, the colorless isomers of benzopyrene are pentacyclic hydrocarbons and are fusion products of pyrene and a phenylene group. Two isomeric species of benzopyrene are benzo[\"a\"]pyrene and the less common benzo[\"e\"]pyrene. They belong to the chemical class of polycyclic aromatic hydrocarbons.\n\nRelated compounds include cyclopentapyrenes, dibenzopyrenes, indenopyrenes and naphthopyrenes. Benzopyrene is a component of pitch and occurs together with other related pentacyclic aromatic species such as picene, benzofluoranthenes, and perylene. It is naturally emitted by forest fires and volcanic eruptions and can also be found in coal tar, cigarette smoke, wood smoke, and burnt foods such as coffee. Fumes that develop from fat dripping on blistering charcoal are rich in benzopyrene, which can condense on grilled goods.\n\nBenzopyrenes are harmful because they form carcinogenic and mutagenic metabolites (such as (+)-benzo[\"a\"]pyrene-7,8-dihydrodiol-9,10-epoxide from benzo[\"a\"]pyrene) which intercalate into DNA, interfering with transcription. They are considered pollutants and carcinogens. The mechanism of action of benzo[a]pyrene-related DNA modification has been investigated extensively and relates to the activity of cytochrome P450 subclass 1A1 (CYP1A1). Seemingly, the high activity of CYP1A1 in the intestinal mucosa prevents major amounts of ingested benzo[a]pyrene from entering portal blood and systemic circulation. The intestinal (but not hepatic) detoxification mechanism seems to depend on receptors that recognize bacterial surface components (TLR2).\n\nEvidence exists to link benzo[\"a\"]pyrene to the formation of lung cancer.\n\nIn February 2014, NASA announced a greatly upgraded database for tracking polycyclic aromatic hydrocarbons (PAHs), including benzopyrene, in the universe. According to scientists, more than 20% of the carbon in the universe may be associated with PAHs, possible starting materials for the formation of life. PAHs seem to have been formed shortly after the Big Bang, are widespread throughout the universe, and are associated with new stars and exoplanets.\n\n"}
{"id": "4463960", "url": "https://en.wikipedia.org/wiki?curid=4463960", "title": "Botswana Private Medical &amp; Health Services Workers' Union", "text": "Botswana Private Medical &amp; Health Services Workers' Union\n\nThe Botswana Private Medical & Health Services Workers' Union (BPM&HSWU) is a trade union affiliate of the Botswana Federation of Trade Unions in Botswana.\n\n"}
{"id": "36777018", "url": "https://en.wikipedia.org/wiki?curid=36777018", "title": "Causes of cancer", "text": "Causes of cancer\n\nCancer is a disease caused by genetic changes leading to uncontrolled cell growth and tumor formation. The basic cause of sporadic (non-familial) cancers is DNA damage and genomic instability. A minority of cancers are due to inherited genetic mutations. Most cancers are related to environmental, lifestyle, or behavioral exposures. Cancer is generally not contagious in humans, though it can be caused by oncoviruses and cancer bacteria. The term \"environmental\", as used by cancer researchers, refers to everything outside the body that interacts with humans. The environment is not limited to the biophysical environment (e.g. exposure to factors such as air pollution or sunlight), but also includes lifestyle and behavioral factors. Over one third of cancer deaths worldwide (and about 75-80% in the United States) are potentially avoidable by reducing exposure to known factors. Common environmental factors that contribute to cancer death include exposure to different chemical and physical agents (tobacco use accounts for 25–30% of cancer deaths), environmental pollutants, diet and obesity (30–35%), infections (15–20%), and radiation (both ionizing and non-ionizing, up to 10%). These factors act, at least partly, by altering the function of genes within cells. Typically many such genetic changes are required before cancer develops. Aging has been repeatedly and consistently regarded as an important aspect to consider when evaluating the risk factors for the development of particular cancers. Many molecular and cellular changes involved in the development of cancer accumulate during the aging process and eventually manifest as cancer.\n\nAlthough there are over 50 identifiable hereditary forms of cancer, less than 0.3% of the population are carriers of a cancer-related genetic mutation and these make up less than 3–10% of all cancer cases. The vast majority of cancers are non-hereditary (\"sporadic cancers\"). Hereditary cancers are primarily caused by an inherited genetic defect. A cancer syndrome or family cancer syndrome is a genetic disorder in which inherited genetic mutations in one or more genes predisposes the affected individuals to the development of cancers and may also cause the early onset of these cancers. Although cancer syndromes exhibit an increased risk of cancer, the risk varies. For some of these diseases, cancer is not the primary feature and is a rare consequence.\n\nMany of these syndromes are caused by mutations in tumor suppressor genes that regulate cell growth. Other common mutations alter the function of DNA repair genes, oncogenes and genes involved in the production of blood vessels. Certain inherited mutations in the genes \"BRCA1\" and \"BRCA2\" with a more than 75% risk of breast cancer and ovarian cancer. Some of the inherited genetic disorders that can cause colorectal cancer include familial adenomatous polyposis and hereditary non-polyposis colon cancer; however, these represent less than 5% of colon cancer cases. In many cases, genetic testing can be used to identify mutated genes or chromosomes that are passed through generations. \n\n\nParticular substances, known as carcinogens, have been linked to specific types of cancer. Common examples of non-radioactive carcinogens are inhaled asbestos, certain dioxins, and tobacco smoke. Although the public generally associates carcinogenicity with synthetic chemicals, it is equally likely to arise in both natural and synthetic substances. It is estimated that approximately 20,000 cancer deaths and 40,000 new cases of cancer each year in the U.S. are attributable to occupation. Every year, at least 200,000 people die worldwide from cancer related to their workplace. Millions of workers run the risk of developing cancers such as lung cancer and mesothelioma from inhaling asbestos fibers and tobacco smoke, or leukemia from exposure to benzene at their workplaces. Cancer related to one's occupation is believed to represent between 2–20% of all cases. Most cancer deaths caused by occupational risk factors occur in the developed world. Job stress does not appear to be a significant factor at least in lung, colorectal, breast and prostate cancers.\n\nTobacco smoking is associated with many forms of cancer, and causes 80% of lung cancer. Decades of research has demonstrated the link between tobacco use and cancer in the lung, larynx, head, neck, stomach, bladder, kidney, esophagus and pancreas. There is some evidence suggesting a small increased risk of developing myeloid leukemia, squamous cell sinonasal cancer, liver cancer, colorectal cancer, cancers of the gallbladder, the adrenal gland, the small intestine, and various childhood cancers. Tobacco smoke contains over fifty known carcinogens, including nitrosamines and polycyclic aromatic hydrocarbons. Tobacco is responsible for about one in three of all cancer deaths in the developed world, and about one in five worldwide. Lung cancer death rates in the United States have mirrored smoking patterns, with increases in smoking followed by dramatic increases in lung cancer death rates and, more recently, decreases in smoking rates since the 1950s followed by decreases in lung cancer death rates in men since 1990. However, the numbers of smokers worldwide is still rising, leading to what some organizations have described as the \"tobacco epidemic\".\n\nElectronic cigarettes or e-cigarettes are handheld electronic devices that simulate the feeling of tobacco smoking. Daily long-term use of high voltage (5.0 V) electronic cigarettes may generate formaldehyde-forming chemicals at a greater level than smoking, which was determined to be a lifetime cancer risk of approximately 5 to 15 times greater than smoking. However, the overall safety and long-term health effects of electronic cigarettes is still uncertain.\n\nSome substances cause cancer primarily through their physical, rather than chemical, effects on cells. A prominent example of this is prolonged exposure to asbestos, naturally occurring mineral fibers which are a major cause of mesothelioma, which is a cancer of the serous membrane, usually the serous membrane surrounding the lungs. Other substances in this category, including both naturally occurring and synthetic asbestos-like fibers such as wollastonite, attapulgite, glass wool, and rock wool, are believed to have similar effects. Non-fibrous particulate materials that cause cancer include powdered metallic cobalt and nickel, and crystalline silica (quartz, cristobalite, and tridymite). Usually, physical carcinogens must get inside the body (such as through inhaling tiny pieces) and require years of exposure to develop cancer. Common occupational carcinogens include:\n\nMany different lifestyle factors contribute to increasing cancer risk. Together, diet and obesity are related to approximately 30–35% of cancer deaths. Dietary recommendations for cancer prevention typically include an emphasis on vegetables, fruit, whole grains, and fish, and avoidance of processed meat, red meat, animal fats, and refined carbohydrates. The evidence to support these dietary changes is not definitive.\n\nAlcohol is an example of a chemical carcinogen. The World Health Organization has classified alcohol as a Group 1 carcinogen. In Western Europe 10% of cancers in males and 3% of cancers in females are attributed to alcohol. Worldwide, 3.6% of all cancer cases and 3.5% of cancer deaths are attributable to alcohol. In particular, alcohol use has been shown to increase the risk of developing cancers of the mouth, esophagus, pharynx, larynx, stomach, liver, ovaries, and colon. The main mechanism of cancer development involves increased exposure to acetaldehyde, a carcinogen and breakdown product of ethanol. Other mechanisms have been proposed, including alcohol-related nutritional deficiencies, changes in DNA methylation, and induction of oxidative stress in tissues.\n\nSome specific foods have been linked to specific cancers. Studies have shown that individuals that eat red or processed meat have a higher risk of developing breast cancer, prostate cancer, and pancreatic cancer. This may be partially explained by the presence of carcinogens in food cooked at high temperatures. Several risk factors for the development of colorectal cancer include high intake of fat, alcohol, red and processed meats, obesity, and lack of physical exercise. A high-salt diet is linked to gastric cancer. Aflatoxin B1, a frequent food contaminate, is associated with liver cancer. Betel nut chewing has been shown to cause oral cancers.\n\nThe relationship between diet and the development of particular cancers may partly explain differences in cancer incidence in different countries. For example, gastric cancer is more common in Japan due to the frequency of high-salt diets and colon cancer is more common in the United States due to the increased intake of processed and red meats. Immigrant communities tend to develop the cancer risk profile of their new country, often within one to two generations, suggesting a substantial link between diet and cancer.\n\nIn the United States, excess body weight is associated with the development of many types of cancer and is a factor in 14–20% of all cancer deaths. Every year, nearly 85,000 new cancer diagnoses in the United States are related to obesity. Individuals who underwent bariatric surgery for weight loss have reduced cancer incidence and mortality.\n\nThere is an associated between obesity and colon cancer, post-menopausal breast cancer, endometrial cancer, kidney cancer, and esophageal cancer. Obesity has also been linked with the development of liver cancer. The current understanding regarding the mechanism of cancer development in obesity relates to abnormal levels of metabolic proteins (including insulin-like growth factors) and sex hormones (estrogens, androgens and progestogens). Adipose tissue also creates an inflammatory environment which may contribute to the development of cancers.\n\nPhysical inactivity is believed to contribute to cancer risk not only through its effect on body weight but also through negative effects on immune system and endocrine system. More than half of the effect from diet is due to overnutrition rather than from eating too little healthy foods.\n\nSome hormones play a role in the development of cancer by promoting cell proliferation. Insulin-like growth factors and their binding proteins play a key role in cancer cell growth, differentiation and apoptosis, suggesting possible involvement in carcinogenesis.\n\nHormones are important agents in sex-related cancers such as cancer of the breast, endometrium, prostate, ovary, and testis, and also of thyroid cancer and bone cancer. For example, the daughters of women who have breast cancer have significantly higher levels of estrogen and progesterone than the daughters of women without breast cancer. These higher hormone levels may explain why these women have higher risk of breast cancer, even in the absence of a breast-cancer gene. Similarly, men of African ancestry have significantly higher levels of testosterone than men of European ancestry, and have a correspondingly much higher level of prostate cancer. Men of Asian ancestry, with the lowest levels of testosterone-activating androstanediol glucuronide, have the lowest levels of prostate cancer.\n\nOther factors are also relevant: obese people have higher levels of some hormones associated with cancer and a higher rate of those cancers. Women who take hormone replacement therapy have a higher risk of developing cancers associated with those hormones. On the other hand, people who exercise far more than average have lower levels of these hormones, and lower risk of cancer. Osteosarcoma may be promoted by growth hormones.\n\nSome treatments and prevention approaches leverage this cause by artificially reducing hormone levels, and thus discouraging hormone-sensitive cancers. Because steroid hormones are powerful drivers of gene expression in certain cancer cells, changing the levels or activity of certain hormones can cause certain cancers to cease growing or even undergo cell death. Perhaps the most familiar example of hormonal therapy in oncology is the use of the selective estrogen-receptor modulator tamoxifen for the treatment of breast cancer. Another class of hormonal agents, aromatase inhibitors, now have an expanding role in the treatment of breast cancer.\n\nWorldwide, approximately 18% of cancer cases are related to infectious diseases. This proportion varies in different regions of the world from a high of 25% in Africa to less than 10% in the developed world. Viruses are the usual infectious agents that cause cancer but bacteria and parasites also contribute. Infectious organisms that increase the risk of cancer are frequently a source of DNA damage or genomic instability.\n\nViral infection is a major risk factor for cervical and liver cancer. A virus that can cause cancer is called an \"oncovirus\". These include human papillomavirus (cervical carcinoma), Epstein–Barr virus (B-cell lymphoproliferative disease and nasopharyngeal carcinoma), Kaposi's sarcoma herpesvirus (Kaposi's sarcoma and primary effusion lymphomas), hepatitis B and hepatitis C viruses (hepatocellular carcinoma), and Human T-cell leukemia virus-1 (T-cell leukemias).\n\nIn Western developed countries, human papillomavirus (HPV), hepatitis B virus (HBV) and hepatitis C virus (HCV) are the most common oncoviruses. In the United States, HPV causes most cervical cancers, as well as some cancers of the vagina, vulva, penis, anus, rectum, throat, tongue and tonsils. Among high-risk HPV viruses, the HPV E6 and E7 oncoproteins inactivate tumor suppressor genes when infecting cells. In addition, the oncoproteins independently induce genomic instability in normal human cells, leading to an increased risk of cancer development. Individuals with chronic hepatitis B virus infection are more than 200 times more likely to develop liver cancer than uninfected individuals. Liver cirrhosis, whether from chronic viral hepatitis infection or alcohol abuse, is independently associated with the development of liver cancer, but the combination of cirrhosis and viral hepatitis presents the highest risk of liver cancer development.\n\nCertain bacterial infections also increase the risk of cancer, as seen in Helicobacter pylori-induced gastric carcinoma. The mechanism by which \"H. pylori\" causes cancer may involve chronic inflammation or the direct action of some of the bacteria's virulence factors. Parasitic infections strongly associated with cancer include \"Schistosoma haematobium\" (squamous cell carcinoma of the bladder) and the liver flukes, \"Opisthorchis viverrini\" and \"Clonorchis sinensis\" (cholangiocarcinoma). Inflammation triggered by the worm's eggs appears to be the cancer-causing mechanism. Certain parasitic infections can also increase the presence of carcinogenic compounds in the body, leading to the development of cancers. Tuberculosis infection, caused by the mycobacterium \"M. tuberculosis\", has also been linked with the development of lung cancer.\n\nThere is evidence that inflammation itself plays an important role in the development and progression of cancer. Chronic inflammation can lead to DNA damage over time and the accumulation of random genetic alterations in cancer cells. Inflammation can contribute to proliferation, survival, angiogensis and migration of cancer cells by influencing tumor microenvironment. Individuals with inflammatory bowel disease are at increased risk of developing colorectal cancers.\n\nUp to 10% of invasive cancers are related to radiation exposure, including both non-ionizing radiation and ionizing radiation. Unlike chemical or physical triggers for cancer, ionizing radiation hits molecules within cells randomly. If it happens to strike a chromosome, it can break the chromosome, result in an abnormal number of chromosomes, inactivate one or more genes in the part of the chromosome that it hit, delete parts of the DNA sequence, cause chromosome translocations, or cause other types of chromosome abnormalities. Major damage normally results in the cell dying, but smaller damage may leave a stable, partly functional cell that may be capable of proliferating and developing into cancer, especially if tumor suppressor genes were damaged by the radiation. Three independent stages appear to be involved in the creation of cancer with ionizing radiation: morphological changes to the cell, acquiring cellular immortality (losing normal, life-limiting cell regulatory processes), and adaptations that favor formation of a tumor. Even if the radiation particle does not strike the DNA directly, it triggers responses from cells that indirectly increase the likelihood of mutations.\n\nNot all types of electromagnetic radiation are carcinogenic. Low-energy waves on the electromagnetic spectrum including radio waves, microwaves, infrared radiation and visible light are thought not to be because they have insufficient energey to break chemical bonds. Non-ionizing radio frequency radiation from mobile phones, electric power transmission, and other similar sources have been described as a possible carcinogen by the World Health Organization's International Agency for Research on Cancer. However, studies have not found a consistent link between cell phone radiation and cancer risk.\n\nHigher-energy radiation, including ultraviolet radiation (present in sunlight), x-rays, and gamma radiation, generally is carcinogenic, if received in sufficient doses. Prolonged exposure to ultraviolet radiation from the sun can lead to melanoma and other skin malignancies. The vast majority of non-invasive cancers are non-melanoma skin cancers caused by non-ionizing ultraviolet radiation. Clear evidence establishes ultraviolet radiation, especially the non-ionizing medium wave UVB, as the cause of most non-melanoma skin cancers, which are the most common forms of cancer in the world.\n\nSources of ionizing radiation include medical imaging, and radon gas. Ionizing radiation is not a particularly strong mutagen. Medical use of ionizing radiation is a growing source of radiation-induced cancers. Ionizing radiation may be used to treat other cancers, but this may, in some cases, induce a second form of cancer. Radiation can cause cancer in most parts of the body, in all animals, and at any age, although radiation-induced solid tumors usually take 10–15 years, and can take up to 40 years, to become clinically manifest, and radiation-induced leukemias typically require 2–10 years to appear. Radiation-induced meningiomas are an uncommon complication of cranial irradiation. Some people, such as those with nevoid basal cell carcinoma syndrome or retinoblastoma, are more susceptible than average to developing cancer from radiation exposure. Children and adolescents are twice as likely to develop radiation-induced leukemia as adults; radiation exposure before birth has ten times the effect.\n\nIonizing radiation is also used in some kinds of medical imaging. In industrialized countries, medical imaging contributes almost as much radiation dose to the public as natural background radiation. Nuclear medicine techniques involve the injection of radioactive pharmaceuticals directly into the bloodstream. Radiotherapy deliberately deliver high doses of radiation to tumors and surrounding tissues as a form of disease treatment. It is estimated that 0.4% of cancers in 2007 in the United States are due to CTs performed in the past and that this may increase to as high as 1.5–2% with rates of CT usage during this same time period.\n\nResidential exposure to radon gas has similar cancer risks as passive smoking. Low-dose exposures, such as living near a nuclear power plant, are generally believed to have no or very little effect on cancer development. Radiation is a more potent source of cancer when it is combined with other cancer-causing agents, such as radon gas exposure plus smoking tobacco.\n\nThe development of donor-derived tumors from organ transplants is exceedingly rare. The main cause of organ transplant associated tumors seems to be malignant melanoma, that was undetected at the time of organ harvest. There have also been reports of Kaposi's sarcoma occurring after transplantation due to tumorous outgrowth of virus-infected donor cells.\n\nPhysical trauma resulting in cancer is relatively rare. Claims that breaking bones resulted in bone cancer, for example, have never been proven. Similarly, physical trauma is not accepted as a cause for cervical cancer, breast cancer, or brain cancer. One accepted source is frequent, long-term application of hot objects to the body. It is possible that repeated burns on the same part of the body, such as those produced by kanger and kairo heaters (charcoal hand warmers), may produce skin cancer, especially if carcinogenic chemicals are also present. Frequently drinking scalding hot tea may produce esophageal cancer. Generally, it is believed that the cancer arises, or a pre-existing cancer is encouraged, during the process of repairing the trauma, rather than the cancer being caused directly by the trauma. However, repeated injuries to the same tissues might promote excessive cell proliferation, which could then increase the odds of a cancerous mutation.\n\nIn the United States, approximately 3,500 pregnant women have a malignancy annually, and transplacental transmission of acute leukemia, lymphoma, melanoma and carcinoma from mother to fetus has been observed. Excepting the rare transmissions that occur with pregnancies and only a marginal few organ donors, cancer is generally not a transmissible disease. The main reason for this is tissue graft rejection caused by MHC incompatibility. In humans and other vertebrates, the immune system uses MHC antigens to differentiate between \"self\" and \"non-self\" cells because these antigens are different from person to person. When non-self antigens are encountered, the immune system reacts against the appropriate cell. Such reactions may protect against tumor cell engraftment by eliminating implanted cells.\n"}
{"id": "33266021", "url": "https://en.wikipedia.org/wiki?curid=33266021", "title": "Clarifying agent", "text": "Clarifying agent\n\nClarifying agents are used to remove suspended solids from liquids by inducing flocculation (the solids begin to aggregate forming flakes, which either precipitate to the bottom or float to the surface of the liquid, and then they can be removed or collected).\n\nParticles finer than 0.1 µm (10m) in water remains continuously in motion due to electrostatic charge (often negative) which causes them to repel each other. Once their electrostatic charge is neutralized by the use of a coagulant chemical, the finer particles start to collide and agglomerate (collect together) under the influence of Van der Waals's forces. These larger and heavier particles are called flocs.\n\nFlocculants, or flocculating agents (also known as flocking agents), are chemicals that promote flocculation by causing colloids and other suspended particles in liquids to aggregate, forming a floc. Flocculants are used in water treatment processes to improve the sedimentation or filterability of small particles. For example, a flocculant may be used in swimming pool or drinking water filtration to aid removal of microscopic particles which would otherwise cause the water to be turbid (cloudy) and which would be difficult or impossible to remove by filtration alone.\n\nMany flocculants are multivalent cations such as aluminium, iron, calcium or magnesium. These positively charged molecules interact with negatively charged particles and molecules to reduce the barriers to aggregation. In addition, many of these chemicals, under appropriate pH and other conditions such as temperature and salinity, react with water to form insoluble hydroxides which, upon precipitating, link together to form long chains or meshes, physically trapping small particles into the larger floc.\n\nLong-chain polymer flocculants, such as modified polyacrylamides, are manufactured and sold by the flocculant producing business. These can be supplied in dry or liquid form for use in the flocculation process. The most common liquid polyacrylamide is supplied as an emulsion with 10-40% actives and the rest is a non-aqueous carrier fluid, surfactants and latex. This form allows easy handling of viscous polymers at high concentrations. These emulsion polymers require \"activation\" - inversion of the emulsion so that the polymers molecules form an aqueous solution.\n\n\nThe following natural products are used as flocculants: \n\n"}
{"id": "958031", "url": "https://en.wikipedia.org/wiki?curid=958031", "title": "Compartmental models in epidemiology", "text": "Compartmental models in epidemiology\n\nCompartmental models are a technique used to simplify the mathematical modelling of infectious disease. The population is divided into compartments, with the assumption that every individual in the same compartment has the same characteristics. Its origin is in the early 20th century, with an important early work being that of Kermack and McKendrick in 1927.\n\nThe models are usually investigated through ordinary differential equations (which are deterministic), but can also be viewed in a stochastic framework, which is more realistic but also more complicated to analyse.\n\nCompartmental models may be used to predict properties of how a disease spreads, for example the prevalence (total number of infected) or the duration of an epidemic. Also, the model allows for understanding how different situations may affect the outcome of the epidemic, e.g., what the most efficient technique is for issuing a limited number of vaccines in a given population.\n\nThe SIR model is one of the simplest compartmental models, and many models are derivations of this basic form. The model consists of three compartments– S for the number susceptible, I for the number of infectious, and R for the number recovered (or immune). This model is reasonably predictive for infectious diseases which are transmitted from human to human, and where recovery confers lasting resistance, such as measles, mumps and rubella.\nThese variables (S, I, and R) represent the number of people in each compartment at a particular time. To represent that the number of susceptible, infected and recovered individuals may vary over time (even if the total population size remains constant), we make the precise numbers a function of \"t\" (time): S(\"t\"), I(\"t\") and R(\"t\"). For a specific disease in a specific population, these functions may be worked out in order to predict possible outbreaks and bring them under control.\n\nAs implied by the variable function of \"t\", the model is dynamic in that the numbers in each compartment may fluctuate over time. The importance of this dynamic aspect is most obvious in an endemic disease with a short infectious period, such as measles in the UK prior to the introduction of a vaccine in 1968. Such diseases tend to occur in cycles of outbreaks due to the variation in number of susceptibles (S(\"t\")) over time. During an epidemic, the number of susceptible individuals falls rapidly as more of them are infected and thus enter the infectious and recovered compartments. The disease cannot break out again until the number of susceptibles has built back up as a result of offspring being born into the susceptible compartment.\n\nEach member of the population typically progresses from susceptible to infectious to recovered. This can be shown as a flow diagram in which the boxes represent the different compartments and the arrows the transition between compartments, i.e.\n\nFor the full specification of the model, the arrows should be labeled with the transition rates between compartments. Between S and I, the transition rate is βI, where β is the contact rate, which takes into account the probability of getting the disease in a contact between a susceptible and an infectious subject.\n\nBetween I and R, the transition rate is γ (simply the rate of recovery or death). If the duration of the infection is denoted \"D\", then γ = 1/\"D\", since an individual experiences one recovery in \"D\" units of time.\n\nIt's assumed that the permanence of each single subject in the epidemic states is a random variable with exponential distribution. More complex and realistic distributions (such as Erlang distribution) can be equally used with few modifications.\n\nThe dynamics of an epidemic, for example the flu, are often much faster than the dynamics of birth and death, therefore, birth and death are often omitted in simple compartmental models. The SIR system without so-called vital dynamics (birth and death, sometimes called demography) described above can be expressed by the following set of ordinary differential equations:\n\nThis model was for the first time proposed by O. Kermack and Anderson Gray McKendrick as a special case of what we now call Kermack-McKendrick theory, and followed work McKendrick had done with Ronald Ross.\n\nThis system is non-linear, and does not admit a generic analytic solution. Nevertheless, significant results can be derived analytically, and with Monte Carlo methods, such as the Gillespie algorithm.\n\nFirstly note that from:\n\nit follows that:\n\nexpressing in mathematical terms the constancy of population formula_6. Note that the above relationship implies that one need only study the equation for two of the three variables.\n\nSecondly, we note that the dynamics of the infectious class depends on the following ratio:\nthe so-called basic reproduction number (also called basic reproduction ratio). This ratio is derived as the expected number of new infections (these new infections are sometimes called secondary infections) from a single infection in a population where all subjects are susceptible. This idea can probably be more readily seen if we say that the typical time between contacts is formula_8, and the typical time until recovery is formula_9. From here it follows that, on average, the number of contacts by an infected individual with others \"before\" the infected has recovered is: formula_10\n\nBy dividing the first differential equation by the third, separating the variables and integrating we get\n(where formula_12 and formula_13 are the initial numbers of, respectively, susceptible and removed subjects). Thus, in the limit formula_14, the proportion of recovered individuals obeys the following transcendental equation\nThis equation shows that at the end of an epidemic, unless formula_16, not all individuals of the population have recovered, so some must remain susceptible. This means that the end of an epidemic is caused by the decline in the number of infected individuals rather than an absolute lack of susceptible subjects.\nThe role of the basic reproduction number is extremely important. In fact, upon rewriting the equation for infectious individuals as follows:\nit yields that if:\nthen:\ni.e., there will be a proper epidemic outbreak with an increase of the number of the infectious (which can reach a considerable fraction of the population). On the contrary, if\nthen\ni.e., independently from the initial size of the susceptible population the disease can never cause a proper epidemic outbreak. As a consequence, it is clear that the basic reproduction number is extremely important.\n\nNote that in the above model the function:\n\nmodels the transition rate from the compartment of susceptible individuals to the compartment of infectious individuals, so that it is called the force of infection. However, for large classes of communicable diseases it is more realistic to consider a force of infection that does not depend on the absolute number of infectious subjects, but on their fraction (with respect to the total constant population formula_23):\n\nCapasso and, afterwards, other authors have proposed nonlinear forces of infection to model more realistically the contagion process.\n\nIn 2014, Harko T. et al. derived an exact analytical solution to the SIR model. In the without vital dynamics setup, for formula_25, etc., it corresponds to the following time parametrization\n\nfor formula_29, formula_30, with initial conditions formula_31 formula_32 satisfies formula_33. By the transcendental equation for formula_34 above, it follows that formula_35, if formula_36 and formula_37.\n\nAn equivalent analytical solution was found by Miller yields\n\nHere formula_42 can be interpreted as the expected number of transmissions an individual has received by time formula_43. The two solutions are related by formula_44.\n\nEffectively the same result can be found in the original work by Kermack and McKendrick .\nConsidering a population characterized by a death rate formula_45 and birth rate formula_46, and where a communicable disease is spreading. The model with mass-action transmission is:\n\nfor which the disease-free equilibrium (DFE) is:\n\nIn this case, we can derive a basic reproduction number :\nwhich has threshold properties. In fact, independently from biologically meaningful initial values, one can show that:\n\nThe point EE is called the Endemic Equilibrium. With heuristic arguments, one may show that formula_54 may be read as the average number of infections caused by a single infectious subject in a wholly susceptible population, the above relationship biologically means that if this number is less or equal than one the disease goes extinct, whereas if this number is greater than one the disease will remain permanently endemic in the population.\n\nSome infections, for example those from the common cold and influenza, do not confer any long lasting immunity. Such infections do not give immunization upon recovery from infection, and individuals become susceptible again.\n\nWe have the model:\n\nNote that denoting with N the total population it holds that:\nformula_57\nit follows that:\ni.e. the dynamics of infectious is ruled by a logistic equation, so that formula_59:\n\nFortunately, it is possible to find an analytical solution to this model (by making a transformation of variables: formula_62 and substituting this into the mean-field equations), such that the basic reproduction rate is greater than unity. The solution is given as\nwhere formula_64 is the endemic infected population, formula_65, and formula_66. As the system is assumed to be closed, the susceptible population is then formula_67.\n\nFor many infections, including measles, babies are not born into the susceptible compartment but are immune to the disease for the first few months of life due to protection from maternal antibodies (passed across the placenta and additionally through colostrum). This is called passive immunity. This added detail can be shown by including an M class (for maternally derived immunity) at the beginning of the model\n\nTo indicate this mathematically, an additional compartment is added, M(t), which results in the following differential equations:\n\nSome people who have had an infectious disease such as tuberculosis never completely recover and continue to carry the infection, whilst not suffering the disease themselves. They may then move back into the infectious compartment and suffer symptoms (as in tuberculosis) or they may continue to infect others in their carrier state, while not suffering symptoms. The most famous example of this is probably Mary Mallon, who infected 22 people with typhoid fever. The carrier compartment is labelled C.\n\nFor many important infections there is a significant incubation period during which the individual has been infected but is not yet infectious themselves. During this period the individual is in compartment E (for exposed).\n\nAssuming that the incubation period is a random variable with exponential distribution with\nparameter a (i.e. the average incubation period is formula_72), and also assuming the presence of vital dynamics with birth rate equal to death rate, we have the model:\n\nWe have formula_77, but this is only constant because of the (degenerate) assumption that birth and death rates are equal; in general formula_23 is a variable.\n\nFor this model, the basic reproduction number is:\n\nformula_79\n\nSimilarly to the SIR model, also in this case we have a Disease-Free-Equilibrium (N,0,0,0) and an Endemic Equilibrium EE, and one can show that, independently form biologically meaningful initial conditions\n\nit holds that:\n\nIn case of periodically varying contact rate formula_83 the condition for the global attractiveness of DFE is that the following linear system with periodic coefficients:\n\nis stable (i.e. it has its Floquet's eigenvalues inside the unit circle in the complex plane).\n\nThe SEIS model takes into consideration the exposed or latent period of the disease, giving an additional compartment, E(t).\nIn this model an infection does not leave any immunity thus individuals that have recovered return to being susceptible again, moving back into the \"S\"(\"t\") compartment. The following differential equations describe this model:\n\nFor the case of a disease, with the factors of passive immunity, and a latency period there is the MSEIR model.\n\nAn MSEIRS model is similar to the MSEIR, but the immunity in the R class would be temporary, so that individuals would regain their susceptibility when the temporary immunity ended.\n\nIt is well known that the probability of getting a disease is not constant in the time. Some diseases are seasonal, such as the common cold viruses, which are more prevalent during winter. With childhood diseases, such as measles, mumps, and rubella, there is a strong correlation with the school calendar, so that during the school holidays the probability of getting such a disease dramatically decreases.\n\nAs a consequence, for many classes of diseases one should consider a force of infection with periodically ('seasonal') varying contact rate\nwith period T equal to one year.\n\nThus, our model becomes\n\n(the dynamics of recovered easily follows from formula_100), i.e. a nonlinear set of differential equations with periodically varying parameters. It is well known that this class of dynamical systems may undergo very interesting and complex phenomena of nonlinear parametric resonance. It is easy to see that if:\n\nwhereas if the integral is greater than one the disease will not die out and there may be such resonances. For example, considering the periodically varying contact rate as the 'input' of the system one has that the output is a periodic function whose period is a multiple of the period of the input.\nThis allowed to give a contribution to explain the poly-annual (typically biennial) epidemic outbreaks of some infectious diseases as interplay between the period of the contact rate oscillations and the pseudo-period of the damped oscillations near the endemic equilibrium.\nRemarkably, in some cases the behavior may also be quasi-periodic or even chaotic.\n\nThe SIR model can be modified to model vaccination. Typically these introduce an additional compartment to the SIR model, formula_102, for vaccinated individuals. Below are some examples.\n\nIn presence of a communicable diseases, one of main tasks is that of eradicating it via prevention measures and, if possible, via the establishment of a mass vaccination program. Let us consider a disease for which the newborn are vaccinated (with a vaccine giving lifelong immunity) at a rate formula_103:\n\nwhere formula_102 is the class of vaccinated subjects. It is immediate to show that:\n\nthus we shall deal with the long term behavior of formula_109 and formula_110, for which it holds that:\n\nIn other words, if\nthe vaccination program is not successful in eradicating the disease, on the contrary it will remain endemic, although at lower levels than the case of absence of vaccinations. This means that the mathematical model suggests that for a disease whose basic reproduction number may be as high as 18 one should have to vaccinate 94.4% of newborns in order to eradicate the disease.\n\nModern societies are facing the challenge of \"rational\" exemption, i.e. the family's decision to not vaccinate children as a consequence of a \"rational\" comparison between the perceived risk from infection and that from getting damages from the vaccine. In order to assess whether this behavior is really rational, i.e. if it can equally lead to the eradication of the disease, one may simply assume that the vaccination rate is an increasing function of the number of infectious subjects:\nIn such a case the eradication condition becomes:\ni.e. the baseline vaccination rate should be greater than the \"mandatory vaccination\" threshold, which, in case of exemption, cannot hold. Thus, \"rational\" exemption might be myopic since it is based only on the current low incidence due to high vaccine coverage, instead taking into account future resurgence of infection due to coverage decline.\n\nIn case there also are vaccinations of non newborn at a rate ρ the equation for the susceptible and vaccinated subject has to be modified as follows:\n\nleading to the following eradication condition:\n\nThis strategy repeatedly vaccinates a defined age-cohort (such as young children or the elderly) in a susceptible population over time. Using this strategy, the block of susceptible individuals is then immediately removed, making it possible to eliminate an infectious disease, (such as measles), from the entire population. Every T time units a constant fraction p of susceptible subjects is vaccinated in a relatively short (with respect to the dynamics of the disease) time. This leads to the following impulsive differential equations for the susceptible and vaccinated subjects:\n\nIt is easy to see that by setting one obtains that the dynamics of the susceptible subjects is given by:\n\nand that the eradication condition is:\n\nAge has a deep influence on the disease spread rate in a population, especially the contact rate. This rate summarizes the effectiveness of contacts between susceptible and infectious subjects. Taking into account the ages of the epidemic classes formula_123 (to limit ourselves to the susceptible-infectious-removed scheme) such that:\n\n(where formula_127 is the maximum admissible age)and their dynamics is not described, as one might think, by \"simple\" partial differential equations, but by integro-differential equations:\n\nwhere:\n\nis the force of infection, which, of course, will depend, though the contact kernel formula_132 on the interactions between the ages.\n\nComplexity is added by the initial conditions for newborns (i.e. for a=0), that are straightforward for infectious and removed:\n\nbut that are nonlocal for the density of susceptible newborns:\n\nwhere formula_135 are the fertilities of the adults.\n\nMoreover, defining now the density of the total population formula_136 one obtains:\n\nIn the simplest case of equal fertilities in the three epidemic classes, we have that in oder to have demographic equilibrium the following necessary and sufficient condition linking the fertility formula_138 with the mortality formula_139 must hold:\n\nand the demographic equilibrium is\n\nautomatically ensuring the existence of the disease-free solution:\n\nA basic reproduction number can be calculated as the spectral radius of an appropriate functional operator.\n\nIn the case of some diseases such as AIDS and Hepatitis B, it is possible for the offspring of infected parents to be born infected. This transmission of the disease down from the mother is called Vertical Transmission. The influx of additional members into the infected category can be considered within the model by including a fraction of the newborn members in the infected compartment.\n\nDiseases transmitted from human to human indirectly, i.e. malaria spread by way of mosquitoes, are transmitted through a vector. In these cases, the infection transfers from human to insect and an epidemic model must include both species, generally requiring many more compartments than a model for direct transmission. For more information on this type of model see the reference \"Population Dynamics of Infectious Diseases: Theory and Applications\", by R. M. Anderson.\n\nOther occurrences which may need to be considered when modeling an epidemic include things such as the following:\n\nIt is important to stress that the deterministic models presented here are valid only in case of sufficiently large populations, and as such should be used cautiously.\n\nTo be more precise, these models are only valid in the thermodynamic limit, where the population is effectively infinite. In stochastic models, the long-time endemic equilibrium derived above, does not hold, as there is a finite probability that the number of infected individuals drops below one in a system. In a true system then, the pathogen may not propagate, as no host will be infected. But, in deterministic mean-field models, the number of infecteds can take on real, namely, non-integer values of infected hosts, and the pathogen may still persist in the system with a finite number of infected hosts, less than one but greater than zero.\n\n\n\n"}
{"id": "33367356", "url": "https://en.wikipedia.org/wiki?curid=33367356", "title": "Cost sharing", "text": "Cost sharing\n\nIn health care, cost sharing occurs when patients pay for a portion of health care costs not covered by health insurance. The \"out-of-pocket\" payment varies among healthcare plans and depends on whether or not the patient chooses to use a healthcare provider who is contracted with the healthcare plan's network. Examples of out-of-pocket payments involved in cost sharing include copays, deductibles, and coinsurance.\n\nIn accounting, cost sharing or matching means that portion of project or program costs not borne by the funding agency. It includes all contributions, including cash and in-kind, that a recipient makes to an award. If the award is federal, only acceptable non-federal costs qualify as cost sharing and must conform to other necessary and reasonable provisions to accomplish the program objectives. Cost sharing effort is included in the calculation of total committed effort. Effort is defined as the portion of time spent on a particular activity expressed as a percentage of the individual's total activity for the institution. Cost sharing can be audited and must be allowable under cost principles and verifiable to records.\n\nA cost-sharing mechanism is a truthful mechanism for deciding what agents should be served by a public project, and how much each of them should pay.\n\n"}
{"id": "44093530", "url": "https://en.wikipedia.org/wiki?curid=44093530", "title": "Daniel Danielopolu", "text": "Daniel Danielopolu\n\nDaniel Danielopolu (12 April 1884 – 29 April 1955) was a Romanian physiologist, clinician and pharmacologist. In 1938, he was elected an honorary member of the Romanian Academy.\n"}
{"id": "30872841", "url": "https://en.wikipedia.org/wiki?curid=30872841", "title": "Doctorandus", "text": "Doctorandus\n\nDoctorandus (; ), abbreviated drs., is a Dutch academic title according to the pre-Bachelor–Master system. The female form is 'doctoranda' (abbreviated dra., though this abbreviation is no longer used). The title is acquired by passing the \"doctoraalexamen\", the exam which usually concludes university study. Some students will continue to do research under the supervision of a professor, which eventually allows them to obtain the title of doctor.\n\nIn Dutch, the words \"doctoraal\" and \"doctoraat\" have different meanings, the first referring to the doctorandus, the second word referring to the doctorate phase or title. The word 'doctorandus' is based on the traditional principle that this degree is a prerequisite and intermediate step for obtaining a doctorate title. However, in the twentieth century the doctorandi have become considered to be graduates and when they can choose a scientific career, the do usually as a paid \"promovendus\" and not as research students. An exception are medical students, where the doctoral exam is an intermediate step after which the students have to follow internships, in order to obtain the full medical degree of physician (\"arts\" in Dutch).\n\nAccording to Dutch legislation, the Dutch doctorandus degree is equivalent to the MA or MSc degree in English-speaking countries, with the difference that the coursework and comprehensive exams for a doctorate are included in the academic study. After being graduated to \"drs.\", the candidate can start with PhD-level research and writing the dissertation without any further exams.\n\nThe abbreviation is drs. This means that Dutch graduates who received the doctorandus title may sign like drs. A. Jansen. After the Bologna process, the title doctorandus has been replaced by the degrees MA and MSc, and those who receive such Dutch degrees may choose: they may use MA/MSc behind their name, or continue to use drs., mr. or ir., reflecting the field in which they graduated. According to Art. 7.20 of the Dutch law on higher education and scientific research, a graduate of a master's degree granted through scientific education (i.e. by a Dutch research university) may sign as ir. (ir. stems from the dutch 'ingenieur' (engineer)) for those who graduated in an academic study of agriculture, natural environment or technical field, mr. (mr. stems from the dutch 'meester' (master) at law) for those who graduated in law and drs. by those who graduated in other fields. According to Art. 7.19a and the Dutch customs the degrees granted to such graduates are MSc for engineers, MA or MSc for doctoranduses and established by ministerial regulation for jurists (actually LLM). This means that two situations can be discerned:\n\n\nIn Belgium the title drs is only used for graduates working on a PhD, which will then eventually change their title from Drs. to Dr. The title Lic (which stems from 'licentiaat') is used for those who have finished a masters' degree, but it is rarely used.\n\nThe degree was also used in Indonesia until 1994 (because of Indonesia being a former colony of the Netherlands), where it was given to all bachelors except law, agriculture, natural environment, and engineering. The title ir. for academic engineers is still used in Indonesia by those who obtained their degree before 1994.\n\nIn Sweden and Estonia the term is used in the form: \"doktorand\", with the same etymology: \"he who should become a doctor\" (but dropping the gender qualifier) to refer to a student reading for his or her doctorate. However, it is not a formal title and is never written abbreviated. Similarly, in Romania it is \"doctorand\", abbreviated \"drd.\"\n\n"}
{"id": "24119474", "url": "https://en.wikipedia.org/wiki?curid=24119474", "title": "EHealth Ontario", "text": "EHealth Ontario\n\neHealth Ontario is the agency tasked with facilitating the development of Ontario's proposed public Electronic Health Record system. \nHealth Informatics in Canada is run provincially, with different provinces creating different systems, albeit sometimes under voluntary Pan-Canadian guidelines published by the federal body Canada Health Infoway.\neHealth Ontario was created in September 2008 out of a merger between the Ontario Ministry of Health's electronic health program and the Smart Systems for Health Agency (SSHA), with a mandate to create electronic health records for all patients in the province by 2015. \nIt has been plagued by delays and its CEO was fired over a multimillion-dollar contracts scandal in 2009. Today eHealth employs approximately 700 people.\n\nThe Drug Profile Viewer System tracks the prescription drug claims information of 2.5 million Ontario Drug Benefit Program and Trillium Drug Program recipients. This system is in use in hospitals throughout Ontario and access is being expanded to health care providers outside of the hospital setting.\n\neHealth Ontario created a pilot project through which some doctors are now able to electronically send prescriptions to participating local pharmacies instead of having to manually write down the prescriptions on paper. That pilot project is ongoing.\n\nIn May 2009, there were opposition calls for Ontario Health Minister David Caplan's resignation after it was revealed that eHealth Ontario CEO Sarah Kramer had approved about $4.8 million in no-bid contracts during the first four months of the agency's operation, while also spending $50,000 to refurnish her office, and paying consultants up to $300 an hour. One consultant earned about $192,000 in five months. Additionally, nine senior eHealth employees had been fired in a four-month period, some reportedly for challenging the agency's tendering practices.\n\nKramer was later forced to resign in June 2009, amid questions surrounding a $114,000 bonus paid to her. She received a $317,000 severance package with benefits for 10 months.\n\neHealth Ontario argued that the no-bid contracts were necessary due to the rapid transition process to eHealth from its predecessor Smart Systems for Health Agency, while Caplan defended Kramer's bonus as part of her move from another agency. The opposition argued that the government of Premier Dalton McGuinty spent five years and $647 million on the forerunner of eHealth Ontario: the Smart Systems for Health Agency, which used 15 per cent of its $225-million annual budget on consultants despite employing 166 people with annual salaries exceeding $100,000, before the project was shut down and restarted as eHealth Ontario.\n\nIn a public statement, Kramer argued that when she took over as CEO of eHealth Ontario, she \"was charged with turning around a failing behemoth - SSHA - which had already run through more than $600 million dollars with hardly anything to show for it in terms of moving Ontario closer to the goal of eHealth, and modernizing and improving the quality and safety of health care for Ontarians.\"\n\nJournalists have argued that Sarah Kramer received a \"trial by media\" and that the province of Ontario will be at a loss with her departure, as delivery of eHealth initiatives will be slowed. Marcus Gee from the Globe and Mail writes, \"what happened at eHealth may or may not qualify as scandalous. What happened to Ms. Kramer certainly does. This was media lynching. A good woman and a first-rate civil servant has been hounded from public life, and all of us will suffer for it.\"\n\nJournalists and former health policy advisors have noted that this media attention detracted from the organization’s mandate and ability to deliver on much needed eHealth initiatives and healthcare reform in the province of Ontario. One past policy director for a former Ontario Minister of Health, argued that the public’s focus should be on holding the government accountable for mitigating the problems that have resulted and demanding progress on eHealth.\n\nAndre Picard, a Canadian public health reporter, argued that the public’s focus should be on the delivery of electronic health records and not \"disingenuous tsk-tsking about the hiring of consultants.\" He writes, \"the true scandal in Ontario is the utter failure of the Ministry of Health to create electronic health records, which will ultimately lead to better and more efficient patient care.\" Picard argued that Ministry of Health bureaucrats are powerless when it comes to making real change in healthcare, as their political boss’s only vision for healthcare is not irritating the public so they can be re-elected. The result is change and innovation can, seemingly, only come from independent agencies or outside consultants.\n\nTwo inquiries were launched, but in August 2009, the independent review of eHealth Ontario had been dropped, with Caplan saying it would duplicate the work of Ontario's auditor general.\n\nOn October 6, 2009, David Caplan resigned, one day before the release of the report into spending scandals.\n"}
{"id": "2471621", "url": "https://en.wikipedia.org/wiki?curid=2471621", "title": "Eloise (psychiatric hospital)", "text": "Eloise (psychiatric hospital)\n\nEloise was a large complex located in Westland, Michigan. The name came from the post office on the grounds which opened July 20, 1894 and was named after Eloise Dickerson Davock, daughter of Detroit's postmaster.\n\nIt operated from 1839 to early 1982 and started out as a poor house and farm but developed into an asylum and hospital.\nIn 1832 it was called the Wayne County Poorhouse; in 1872 it was the Wayne County Alms House; in 1886 it was referred to simply as the Wayne County House. In 1913 there were three divisions: The Eloise Hospital (Mental Hospital), the Eloise Infirmary (Poorhouse) and the Eloise Sanitarium (T.B. Hospital) which were collectively called Eloise. In 1945 it was named Wayne County General Hospital and Infirmary at Eloise, Michigan. In 1974 it had two divisions - the Wayne County General Hospital and the Wayne County Psychiatric Hospital. The psychiatric division closed in 1977 and in 1979 it was officially called Wayne County General Hospital. At its prime, Eloise consisted of 78 buildings and 902 acres of land. Now only ruins, sewer lids with \"Eloise Hospital\" engraved on them, 4 of the 78 buildings and the Eloise Cemetery remain.\n\nThe Wayne County Poor House was founded in 1832. It was located at Gratiot and Mt. Elliott Avenues in Hamtramck Township two miles from the Detroit city limits. By 1834 the poorhouse was in bad condition and in Nankin Township were purchased. The Black Horse Tavern which served as a stagecoach stop between Detroit and Chicago was located on the property. In those days it was a two-day stagecoach ride from Hamtramck Township to Nankin Township. The register shows that on April 11, 1839 35 people were transferred from the poorhouse in Hamtramck Township to the new one in Nankin Township. 111 apparently refused to go to the \"awful wilderness.\" Many were children and homes among the residents of the city may have been found for them. The log cabin which was formerly the Black Horse Tavern became the keeper's quarters and in 1838-9 a frame building was put up to house the inmates. A frame cookhouse was erected in the back of the log building and was used for cooking by both inmates and the keeper's family.\n\nThe complex was almost self-sufficient. It had its own police and fire department, railroad and trolley stations, bakery, amusement hall, laundries, and a powerhouse. It also had many farm buildings including a dairy herd and dairy barns, a piggery, a root cellar, a Tobacco curing building, formed by Dr. Albarran. Patients came from Detroit and other communities to have x-rays done. It also housed the first kidney dialysis unit in the State of Michigan and pioneered in the field of Music Therapy.\n\nAs the years went on the institution grew larger and larger, a reflection in the increases in the population of the Detroit area. From only 35 residents in 1839 the complex grew to about 10,000 residents at its peak during the Great Depression and then started to decrease. The farm operations ceased in 1958 and some of the large psychiatric buildings were vacated in 1973.\nThe psychiatric division started closing in 1977 when the State of Michigan took over the psychiatric division. The general hospital closed in 1986.\n\nInventor Elijah McCoy may be its most famous former resident. He spent a year prior to his death as a patient in the Eloise Infirmary. There were other well-known people who died at Eloise including several baseball players. Among them are Jul Kustus (died April 27, 1916), Larry LeJeune (died April 21, 1952), Charlie Krause (died March 30, 1948) and Marty Kavanagh (died 1960) Musician Horace Flinders was also a patient, and received music therapy.\n\nToday the land that once was Eloise has been developed into a strip mall, a golf course, and condominiums. There are only two buildings currently in use. One is \"D\" Building or the Kay Beard Building. At one time this was an administration building and it was also used for psychiatric admissions and apartments for some employees like the Catholic chaplain. The old commissary building is currently being used as a family homeless shelter.\n\nThe old fire hall (former psychiatric facility laundry), and the power house are still standing in ruins. The old bakery was lost due to arson in April 2016, with charred ruins still standing as of May 16, 2016 (however, there are plans to raze and remove the remainder). The Eloise smokestack emblazoned \"Eloise\" in brick was deemed to be a hazard and was demolished in 2006.\n\nIn 1979, the Walter P. Reuther Psychiatric Hospital, located near the northwest end of the former Eloise property, just southeast of the intersection of Merriman and Palmer Roads, was opened. The facility is currently operated by the Michigan Department of Community Health. In 1996, Oakwood Health System opened an outpatient facility, the Adams Child & Adolescent Health Center, on the corner of Merriman and Palmer near Reuther Hospital. Both facilities have Palmer Road addresses.\n\nEloise is featured in the book \"Annie's Ghosts: A Journey Into a Family Secret\" by Steve Luxenberg, which is about Luxenberg's secret aunt who was committed to the Eloise psychiatric hospital in the 1940s.\n\nThe site is marked by a Michigan Historic Marker.\n\nPresently the site and cemetery are maintained by the Friends of Eloise.\n\nThe site and the adjoining Eloise Cemetery are reputed to be haunted.\n\nEloise is featured in the book \"Detroit Breakdown\" part of D.E. Johnson's Detroit mysteries series, a fictional story about the murder of patients at Eloise.\n\nIt also inspired the direct-to-video thriller Eloise (2017 film).\n\nCitations\nReferences\n\n"}
{"id": "37931075", "url": "https://en.wikipedia.org/wiki?curid=37931075", "title": "European Union of Medical Specialists", "text": "European Union of Medical Specialists\n\nThe European Union of Medical Specialists (UEMS: Union Européenne des Médecins Spécialistes) is a professional organisation of doctors other than general practitioners in the European Union, which was founded in 1958. This union represents about 1.4 million specialists.\n\nThe National Medical Associations of the countries in the European Union and European Economic Area are full members of the European Union of Medical Specialists, and provide delegates to the governing council. Fifty medical disciplines are represented, of which the most important are covered by 43 specialist sections for areas such as Allergology, Anaesthesiology, Cardiology, Cardiothoracic Surgery, and more. The European Union of Medical Specialists represents medical specialists to the EU authorities. It promotes high levels of medical training, practice, and health care.\n\nThe European Union of Medical Specialists is responsible for accreditation of Continuing Medical Education through the EACCME.\n\n"}
{"id": "12283723", "url": "https://en.wikipedia.org/wiki?curid=12283723", "title": "Fiji School of Medicine", "text": "Fiji School of Medicine\n\nThe Fiji School of Medicine is a tertiary institution based in Suva, Fiji. Originally established in 1885 as the \"Suva Medical School\". FSM became the College of Medicine, Nursing & Health Sciences as part of Fiji National University in 2010. It is located on the main island of Viti Levu in the Fiji Islands.\n\nFSM has been educating Health Care Professionals since its establishment in 1885 as Suva Medical School.\n\nThe school now provides training in most health science disciplines including medicine, dentistry, pharmacy, physiotherapy, radiography, laboratory technology, public health, health services management, dietetics and environmental health.\n\nDean: Dr William May\n\nVice Chancellor: Professor Nigel Healey\n\n\n\n"}
{"id": "1763544", "url": "https://en.wikipedia.org/wiki?curid=1763544", "title": "Five Tibetan Rites", "text": "Five Tibetan Rites\n\nThe Five Tibetan Rites is a system of exercises reported to be more than 2,500 years old which were first publicized by Peter Kelder in a 1939 publication titled \"The Eye of Revelation\".\n\nThe Rites are said to be a form of Tibetan yoga similar to the yoga series that originated in India. However, the Five Rites and traditional Tibetan yoga both emphasize \"a continuous sequence of movement\" (Sanskrit: vinyasa), whereas Indian forms focus on \"static positions\". Although the Rites have circulated amongst yogis for decades, skeptics say that Tibetans have never recognized them as being authentic Tibetan practices.\n\nThe Five Tibetan Rites are also referred to as \"The Five Rites\", \"The Five Tibetans\" and \"The Five Rites of Rejuvenation\".\n\nAlthough practically nothing is known about Kelder, one source reports that he was raised as an adopted child in the mid-western United States and left home while in his teens in search of adventure. In the 1930s, Kelder claims to have met, in southern California, a retired British army colonel who shared with him stories of travel and the subsequent discovery of the Rites.\nOriginally written as a 32-page booklet, the publication is the result of Kelder's conversations with the colonel.\n\nIn his booklet, Kelder claims that while stationed in India, British army officer Colonel Bradford (a pseudonym) heard a story about a group of lamas who had apparently discovered a \"Fountain of Youth\". The \"wandering natives\", as he called them, told him of old men who inexplicably became healthy, strong, and full of \"vigor and virility\" after entering a particular lamasery. After retiring, Kelder's Colonel Bradford went on to discover the lamasery and lived with the lamas, where they taught him five exercises, which they called \"rites\". According to the booklet, the lamas describe seven spinning, \"psychic vortexes\" within the body: two of these are in the brain, one at the base of the throat, one on the right side of the body in the vicinity of the liver, one in the reproductive anatomy, and one in each knee. As we grow older, the spin rate of the \"vortexes\" diminishes, resulting in \"ill-health\". However, the spin rate of these \"vortexes\" can be restored by performing the Five Rites daily, resulting in improved health.\n\nBradford was also instructed in how to perform a sixth rite, which the lamas recommended only for those willing to \"lead a more or less continent (celibate) life\". Additionally, Bradford reveals information on the importance of what foods one should eat, proper food combinations, and the correct method of eating.\n\nAlthough the origin of the Five Rites before the publication of \"The Eye of Revelation\" is disputed between practitioners and skeptics, a comparison of illustrations of the postures shows a remarkable similarity between the Rites and authentic Tibetan \"'phrul 'khor\" exercises from a system rendered into English as Vajra Body Magical Wheel Sun and Moon Union (). It has been noted, however, that even though these comparisons are compelling, a closer examination reveals that these similarities are misleading. Chris Kilham, whose 1994 book \"The Five Tibetans\" helped respark the Rites' popularity, says, \"As the story has it, they were shared by Tibetan lamas; beyond that I know nothing of their history.\" Even though the historic lineage of the Rites before the publication of Kelder's booklet remains to be ascertained, the Rites have nevertheless been affirmed by a lama and scholar of the Sa skya tradition of Tibetan Buddhism as being \"a genuine form of yoga and were originally taken from an authentic Indo-Tibetan tantric lineage, namely a cycle of yantra-yoga associated with the Sadnadapadadharma.\" However, it has been argued that the Five Rites predate yoga as we know it today by as much as seven hundred years or more and, therefore, could not have derived from either Tibetan or Indian forms of yoga. Moreover, it has been suggested that the Rites are more likely to have originated from a system of Kum Nye which, like the Rites, date back 2,500 years. Nevertheless, Kilham states that \"[t]he issue at hand, though, is not the lineage of the Five Tibetans. The point is their immense potential value for those who will clear 10 minutes a day to practice.\"\n\nIn the original \"The Eye of Revelation\" booklet, Kelder suggests standing erect between each of the Five Rites with hands on hips and taking one or two deep breaths; he neither implies nor suggests that specific breathing patterns should be adopted while performing the movements. Nevertheless, subsequent publications pertaining to the Rites contain edits by others which recommend and detail specific instructions for breathing while performing the exercises. Some practitioners also recommend taking caution before performing the Rites due to the possibility of aggravating certain health conditions.\n\nKelder cautions that when performing the First Rite, spinning must always be performed in a clockwise direction. He also states that Bradford clearly recalled that the Maulawiyah, otherwise known as \"Whirling Dervishes\", always spun from left to right, in a clockwise direction. No mention is made of the orientation of the palms, although the original illustration of the Rite in the 1939 edition of \"The Eye of Revelation\" clearly depicts both palms as facing toward the ground. Here arises a point of contention: the Whirling Dervishes spin in the \"counter\"-clockwise direction, with the left palm facing down, towards the earth, and the right palm facing up, towards heaven. However, this discrepancy may find partial resolution in the fact that Tibetan Buddhist yoga regards clockwise rotation to be favorable, whereas counter-clockwise rotation is considered to be unfavorable.\n\n\"Stand erect with arms outstretched, horizontal with the shoulders. Now spin around until you become slightly dizzy. There is only one caution: you must turn from left to right if you are in the northern hemisphere and counterclockwise if you are in the South hemisphere.\" A tip for this is to look at the end of your right hand as a reference point .\n\n\"Lie full length on rug or bed. Place the hands flat down alongside of the hips. Fingers should be kept close together with the finger-tips of each hand turned slightly toward one another. Raise the feet until the legs are straight up. If possible, let the feet extend back a bit over the body toward the head, but do not let the knees bend. Hold this position for a moment or two and then slowly lower the feet to the floor, and for the next several moments allow all of the muscles in the entire body to relax completely. Then perform the Rite all over again.\"\n\n\"While the feet and legs are being raised it is a good idea also to raise the head, then while the feet and legs are being lowered to the floor lower the head at the same time.\"\n\n\"Kneel on a rug or mat with hands at sides, palms flat against the side of legs. Then lean forward as far as possible, bending at the waist, with head well forward—chin on chest. The second position of this Rite is to lean backward as far as possible. Cause the head to move still further backward. The toes will prevent you from falling over backward. The hands are always kept against the side of the legs. Next come to an erect (kneeling) position, relax as much as possible for a moment, and perform Rite all over again.\"\n\n\"Sit erect on rug or carpet with feet stretched out in front. The legs must be perfectly straight -- back of knees must be well down or close to the rug. Place the hands flat on the rug, fingers together, and the hands pointing outward slightly. Chin should be on chest -- head forward.\"\n\n\"Now gently raise the body, at the same time bend the knees so that the legs from the knees down are practically straight up and down. The arms, too, will also be vertical while the body from shoulders to knees will be horizontal. As the body is raised upward allow the head gently to fall backward so that the head hangs backward as far as possible when the body is fully horizontal. Hold this position for a few moments, return to first position, and RELAX for a few moments before performing the Rite again.\"\n\n\"When the body is pressed up to complete horizontal position, tense every muscle in the body.\"\n\n\"Place the hands on the floor about two feet apart. Then, with the legs stretched out to the rear with the feet also about two feet apart, push the body, and especially the hips, up as far as possible, rising on the toes and hands. At the same time the head should be brought so far down that the chin comes up against the chest. Next, allow the body to come slowly down to a ‘sagging’ position. Bring the head up, causing it to be drawn as far back as possible.\"\n\n\"The muscles should be tensed for a moment when the body is at the highest point, and again at the lowest point.\"\n\n\"It should be practiced only when you feel an excess of sexual energy...\"\n\"Stand straight up and slowly let all of the air out of your lungs...bend over and put your hands on your knees...with the lungs empty, return to a straight up posture. Place your hands on your hips, and press down on them... As you do this, pull in the abdomen as much as possible, and at the same time raise the chest.\nhold this position as long as you possibly can.\ntake air into your empty lungs, let the air flow in through the nose. When the lungs are full, exhale through the mouth.\nAs you exhale, relax your arms... Then take several deep breaths through the mouth or nose, allowing them to escape through either the mouth or nose.\"\n\nAccording to Kelder, Bradford's stay in the lamasery transformed him from a stooped, old gentleman with a cane to a tall and straight young man in the prime of his life. Additionally, he reported that Bradford's hair had grown back, without a trace of gray. The revised publications of \"The Eye of Revelation\" titled \"Ancient Secret of the Fountain of Youth\" also contain numerous testimonials by practitioners of the Rites, claiming that they yield positive medical effects such as improved eyesight, memory, potency, hair growth, restoration of full color to completely gray hair, and anti-aging. However, claims as to the benefits of the Rites are often exaggerated, resulting in unrealistic expectations. The benefits most likely to be achieved are increased energy, stress reduction, and an enhanced sense of calm, clarity of thought, increased strength and flexibility, resulting in an overall improvement in health and well-being.\n\n\n"}
{"id": "38736209", "url": "https://en.wikipedia.org/wiki?curid=38736209", "title": "Food and dining in the Roman Empire", "text": "Food and dining in the Roman Empire\n\nFood and dining in the Roman Empire reflect both the variety of foodstuffs available through the expanded trade networks of the Roman Empire and the traditions of conviviality from ancient Rome's earliest times, inherited in part from the Greeks and Etruscans. In contrast to the Greek symposium, which was primarily a drinking party, the equivalent social institution of the Roman \"convivium\" (dinner party) was focused on food. Banqueting played a major role in Rome's communal religion. Maintaining the food supply to the city of Rome had become a major political issue in the late Republic, and continued to be one of the main ways the emperor expressed his relationship to the Roman people and established his role as a benefactor. Roman food vendors and farmers' markets sold meats, fish, cheeses, produce, olive oil and spices and pubs, bars, inns and food stalls sold prepared food.\n\nBread was a meaty food for Romans, with more well-to-do people eating wheat bread and poorer people eating barley bread. Fresh produce such as vegetables and legumes were important to Romans, as farming was a valued activity. A variety of olives and nuts were eaten. While there were prominent Romans who discouraged meat eating, a variety of meat products were prepared, including blood puddings, sausages, cured ham and bacon. The milk of goats or sheep was thought superior to that of cows; milk was used to make many types of cheese, as this was a way of storing and trading milk products. While olive oil was fundamental to Roman cooking, butter was viewed as an undesirable Gallic foodstuff. Sweet foods such as pastries typically used honey and wine-must syrup as a sweetener. A variety of dried fruits (figs, dates and plums) and fresh berries were also eaten.\n\nSalt–which in its pure form was an expensive commodity in Rome–was the fundamental seasoning and the most common salty condiment was a fermented fish sauce known as garum. Locally available seasonings included garden herbs, cumin, coriander, and juniper berries. Imported spices included pepper, saffron, cinnamon, and fennel. While wine was an important beverage, Romans looked down on drinking to excess and drank their wine mixed with water; drinking wine \"straight\" was viewed as a barbarian custom.\n\nFive main Roman ingredients in dishes were: Wheat, wine, meat and fish, bread, and sauces and spices. The richer Romans had very luxurious lives, and sometimes hosted banquets or feasts.\n\nMost people would have consumed at least 70 percent of their daily calories in the form of cereals and legumes. Grains included several varieties of wheat—emmer, rivet wheat, einkorn, spelt, and common wheat \"(Triticum aestivum)\"—as well as the less desirable barley, millet, and oats.\n\nLegumes included the lentil, chickpea, bitter vetch, broad bean, garden pea, and grass pea; Pliny names varieties such as the Venus pea, and poets praise Egyptian lentils imported from Pelusium. Legumes were planted in rotation with cereals to enrich the soil, and were stockpiled in case of famine. The agricultural writer Columella gives detailed instructions on curing lentils, and Pliny says they had health benefits. Although usually thought of as modest fare, legumes also appear among the dishes at banquets.\n\n\"Puls\" (pottage) was considered the aboriginal food of the Romans, and played a role in some archaic religious rituals that continued to be observed during the Empire. The basic grain pottage could be elaborated with chopped vegetables, bits of meat, cheese, or herbs to produce dishes similar to polenta or risotto. \"Julian stew\" \"(Pultes Iulianae)\" was made from spelt to which was added two kinds of ground meat, pepper, lovage, fennel, hard bread, and a wine reduction; according to tradition, it was eaten by the soldiers of Julius Caesar and was a \"quintessential Roman dish.\"\n\nUrban populations and the military preferred to consume their grain in the form of bread. The lower classes ate coarse brown bread made from emmer or barley. Fine white loaves were leavened by wild yeasts and sourdough cultures. The beer-drinking Celts of Spain and Gaul were known for the quality of their breads risen with barm or brewers' yeast. The poem \"Moretum\" describes a \"ploughman's lunch\", a flatbread prepared on a griddle and topped with cheese and a pesto-like preparation, somewhat similar to pizza or focaccia.\n\nMaintaining a bread oven is labor-intensive and requires space, so apartment dwellers probably prepared their dough at home, then had it baked in a communal oven. Mills and commercial ovens, usually combined in a bakery complex, were considered so vital to the wellbeing of Rome that several religious festivals honored the deities who furthered these processes—and even the donkeys who toiled in the mills. Vesta, the goddess of the hearth, was seen as complementary to Ceres, the goddess of grain, and donkeys were garlanded and given a rest on the Festival of Vesta. The Fornacalia was the \"Festival of Ovens\". Lateranus was a deity of brick ovens.\n\nBecause of the importance of landowning in the formation of the Roman cultural elite, Romans idealized farming and took a great deal of pride in serving produce. Leafy greens and herbs were eaten as salads with vinegar dressings. Cooked vegetables such as beets, leeks, and gourds were prepared with sauces as first courses or served with bread as a simple meal. The Romans had over 20 kind of vegetables and greens. Cured olives were available in wide variety even to those on a limited budget. Truffles and wild mushrooms, while not everyday fare, were perhaps more commonly foraged than today.\n\nProvinces exported regional dried fruits such as Carian figs and Theban dates, and fruit trees from the East were propagated throughout the Western empire: the cherry from Pontus (present-day Turkey); peach \"(persica)\" from Persia (Iran), along with the lemon and other citrus; the apricot from Armenia; the \"Damascan\" or damson plum from Syria; and what the Romans called the \"Punic apple\", the pomegranate from North Africa. The Romans ate cherries, blackberries, currants, elderberries, dates, pomegranates, peaches, apricots, quinces, melons, plums, figs, grapes, apples and pears, Around the Roman Table, p. 239. \n\nBerries were cultivated or gathered wild. Familiar nuts included almonds, hazelnuts, walnuts, pistachios, pine nuts, and chestnuts. Fruit and nut trees could be grafted with multiple varieties.\n\nWhile there were prominent Romans who discouraged meat eating– the Emperors Didius Julianus and Septimius Severus disdained meat–Roman butchers sold a variety of fresh meats, including pork, beef, and mutton or lamb. Due to the lack of refrigeration, techniques of preservation for meat, fish, and dairy were developed. No portion of the animal was allowed to go to waste, resulting in blood puddings, meatballs \"(isicia)\", sausages, and stews. Rural people cured ham and bacon, and regional specialties such as the fine salted hams of Gaul were items of trade. The sausages of Lucania were made from a mixture of ground meats, herbs, and nuts, with eggs as a binding ingredient, and then aged in a smoker.\n\nFresh milk was used in medicinal and cosmetic preparations, or for cooking. The milk of goats or sheep was thought superior to that of cows. Cheese was easier to store and transport to market, and literary sources describe cheesemaking in detail, including fresh and hard cheeses, regional specialties, and smoked cheeses.\n\nOlive oil was fundamental not only to cooking, but to the Roman way of life, as it was used also in lamps and preparations for bathing and grooming. The Romans invented the \"trapetum\" for extracting olive oil. The olive orchards of Roman Africa attracted major investment and were highly productive, with trees larger than those of Mediterranean Europe; massive lever presses were developed for efficient extraction. Spain was also a major exporter of olive oil, but the Romans regarded oil from central Italy as the finest. Specialty blends were created from Spanish olive oil; Liburnian Oil \"(Oleum Liburnicum)\" was flavored with elecampane, cyperus root, bay laurel and salt.\n\nButter was mostly disdained by the Romans, but was a distinguishing feature of the Gallic diet. Lard was used for baking pastries and seasoning some dishes.\n\nSalt was the fundamental seasoning: Pliny the Elder remarked that \"Civilized life cannot proceed without salt: it is so necessary an ingredient that it has become a metaphor for intense mental pleasure.\" In Latin literature, salt \"(sal)\" was a synonym for \"wit\". It was an important item of trade, but pure salt was relatively expensive. The most common salty condiment was \"garum,\" the fermented fish sauce that added the flavor dimension now called \"umami\". Major exporters of garum were located in the provinces of Spain.\nLocally available seasonings included garden herbs, cumin, coriander, and juniper berries. Pepper was so vital to the cuisine that ornamental pots \"(piperatoria)\" were created to hold it. \"Piper longum\" was imported from India, as was spikenard, used to season game birds and sea urchins.\n\nOther imported spices were saffron, cinnamon, and the silphium of Cyrene, a type of pungent fennel that was over-harvested into extinction during the reign of Nero, after which time it was replaced with \"laserpicium\", asafoetida exported from present-day Afghanistan. Pliny estimated that Romans spent 100 million sesterces a year on spices and perfumes from India, China, and the Arabian peninsula.\n\nSweeteners were limited mostly to honey and wine-must syrup \"(defrutum)\". Cane sugar was an exotic ingredient used as a garnish or flavoring agent, or in medicines.\n\nThe central government took an active interest in supporting agriculture. Producing food was the top priority of land use. Larger farms \"(latifundia)\" achieved an economy of scale that sustained urban life and its more specialized division of labor. The Empire's transportation network of roads and shipping lines benefitted small farmers by opening up access to local and regional markets in towns and trade centers. Agricultural techniques such as crop rotation and selective breeding were disseminated throughout the Empire, and new crops were introduced from one province to another, such as peas and cabbage to Britain.\n\nFood vendors are depicted in art throughout the Empire. In the city of Rome, the Forum Holitorium was an ancient farmers' market, and the Vicus Tuscus was famous for its fresh produce. Throughout the city, meats, fish, cheeses, produce, olive oil, spices, and the ubiquitous condiment \"garum\" (fish sauce) were sold at \"macella\", Roman indoor markets, and at marketplaces throughout the provinces.\n\nMaintaining an affordable food supply to the city of Rome had become a major political issue in the late Republic, when the state began to provide a grain dole \"(annona)\" to citizens who registered for it. About 200,000–250,000 adult males in Rome received the dole, amounting to about 33 kg per month, for a per annum total of about 100,000 tonnes of wheat primarily from Sicily, Northern Africa, and Egypt.\n\nThe dole cost at least 34 percent of state revenues, but improved living conditions and family life among the lower classes, and subsidized the rich by allowing workers to spend more of their earnings on the wine and olive oil produced on the estates of the landowning class.\n\nThe grain dole also had symbolic value: it affirmed both the emperor's position as universal benefactor, and the right of all citizens to share in \"the fruits of conquest\". The \"annona\", public facilities, and spectacular entertainments mitigated the otherwise dreary living conditions of lower-class Romans, and kept social unrest in check. The satirist Juvenal, however, saw \"bread and circuses\" \"(panem et circenses)\" as emblematic of the loss of republican political liberty:\n\nThe public has long since cast off its cares: the people that once bestowed commands, consulships, legions and all else, now meddles no more and longs eagerly for just two things: bread and circuses.\n\nRomans who received the dole took it to a mill to have it ground into flour. By the reign of Aurelian, the state had begun to distribute the \"annona\" as a daily ration of bread baked in state factories, and added olive oil, wine, and pork to the dole.\n\nMost people in the city of Rome lived in apartment buildings \"(insulae)\" that lacked kitchens, though shared cooking facilities might be available in ground-level commons areas. A charcoal brazier could be used for rudimentary cookery such as grilling and stewing in a pot (\"olla)\", but ventilation was poor and braziers were fire hazards.\n\nPrepared food was sold at pubs and bars, inns, and food stalls \"(tabernae, cauponae, popinae)\". Some establishments had countertops fitted with openings for pots that may have kept food warm over a heat source \"(thermopolium)\" or simply served as storage vessels \"(dolia)\".\n\nCarryout and restaurant dining were for the lower classes. Frequenting taverns, where prostitutes sometimes worked, was among the moral failings that louche emperors and other public figures might be accused of.\nMills and commercial ovens were usually combined in a bakery complex.\n\nThe importance of a good diet to health was recognized by medical writers such as Galen (2nd century AD), whose treatises included one \"On Barley Soup\". Views on nutrition were influenced by schools of thought such as humoral theory. Digestion of food within the body was thought to be a process analogous to cooking.\n\nThe Latin expression for a full-course dinner was \"ab ovo usque mala\", \"from the egg to the apples,\" equivalent to the English \"from soup to nuts.\" A multicourse dinner began with the \"gustatio\" (\"tasting\" or \"appetizer\"), often a salad or other minimally cooked composed dish, with ingredients to promote good digestion. The \"cena\" proper centered on meat, a practice that evokes the tradition of communal banquets following animal sacrifice. A meal concluded with fruits and nuts, or with deliberately superfluous desserts \"(secundae mensae)\".\n\nRoman literature focuses on the dining habits of the upper classes, and the most famous description of a Roman meal is probably Trimalchio's dinner party in the \"Satyricon\", a fictional extravaganza that bears little resemblance to reality even among the most wealthy. The poet Martial describes serving a more plausible dinner, beginning with the \"gustatio\", which was a composed salad of mallow leaves, lettuce, chopped leeks, mint, arugula, mackerel garnished with rue, sliced eggs, and marinated sow udder. The main course was succulent cuts of kid, beans, greens, a chicken, and leftover ham, followed by a dessert of fresh fruit and vintage wine.\n\nRoman books on agriculture include a few recipes. A book-length collection of Roman recipes is attributed to Apicius, a name for several figures in antiquity that became synonymous with \"gourmet\": \"the recipes are written haphazardly, as if someone familiar with the workings of a kitchen was jotting down notes for a colleague.\" Although often imprecise, particularly with measurements, Apicius uses eight different verbs for techniques for incorporating eggs into a dish, including one that might produce a soufflé.\n\nRecipes include regional specialties such as \"Ofellas Ostiensis\", an hors d'oeuvre made from \"choice squares of marinated pork cooked in a spicy sauce of typically Roman flavors: lovage, fennel, cumin, and anise.\" The signature dish \"Patina Apiciana\" required a complex forcemeat layered with egg and crepes, to be presented on a silver platter.\n\nRoman \"foodies\" indulged in wild game, fowl such as peacock and flamingo, large fish (mullet was especially prized), and shellfish. Oysters were farmed at Baiae, a resort town on the Campanian coast known for a regional shellfish stew made from oysters, mussels, sea urchins, celery and coriander.\n\nThe favorite dish of the emperor Vitellius was supposed to be the \"Shield of Minerva\", composed of pike liver, brains of pheasant and peacock, flamingo tongue, and lamprey milt. The description given by Suetonius emphasizes that these luxury ingredients were brought by the fleet from the far reaches of empire, from the Parthian frontier to the Straits of Gibraltar. The Augustan historian Livy explicitly links the development of gourmet cuisine to Roman territorial expansion, dating the introduction of the first chefs to 187 BC, following the Galatian War.\n\nAlthough food shortages were a constant concern, Italian viticulture produced an abundance of cheap wine that was shipped to Rome. Most provinces were capable of producing wine, but regional varietals were desirable, and wine was a central item of trade. Shortages of \"vin ordinaire\" were rare. Regional varieties such as Alban, Caecuban, and Falernian were prized. Opimian was the most prestigious vintage.\n\nThe major suppliers for the city of Rome were the west coast of Italy, southern Gaul, the Tarraconensis region of Spain, and Crete. Alexandria, the second-largest city in the Empire, imported wine from Laodicea in Syria and the Aegean. At the retail level, taverns or specialty wine shops \"(vinaria)\" sold wine by the jug for carryout and by the drink on premises, with price ranges reflecting quality.\n\nIn addition to regular consumption with meals, wine was a part of everyday religious observances. Before a meal, a libation was offered to the household gods. When Romans made their regular visits to burial sites to care for the dead, they poured a libation, facilitated at some tombs with a feeding tube into the grave.\n\nRomans drank their wine mixed with water, or in \"mixed drinks\" with flavorings. \"Mulsum\" was a mulled sweet wine, and \"apsinthium\" was a wormwood-flavored forerunner of absinthe. Although wine was enjoyed regularly, and the Augustan poet Horace coined the expression \"truth in wine\" \"(in vino veritas)\", drunkenness was disparaged. It was a Roman stereotype that Gauls had an excessive love of wine, and drinking wine \"straight\" (\"purum\" or \"merum\", unmixed) was a mark of the \"barbarian\". The Gauls also brewed various forms of beer.\n\nSince restaurants catered to the lower classes, fine dining could be sought only at private dinner parties in houses, or at banquets hosted by social clubs \"(collegia)\". The private home \"(domus)\" of an elite family would have had a kitchen, a kitchen garden, and a trained staff with a chef \"(archimagirus)\", a sous chef \"(vicarius supra cocos)\", and kitchen assistants \"(coci\", singular \"cocus\" or \"coquus\", from which the English \"cook\" derives).\n\nIn upperclass households, the evening meal \"(cena)\" had important social functions. Guests were entertained in a finely decorated dining room \"(triclinium)\", often with a view of the peristyle garden. Diners lounged on couches, leaning on the left elbow. The ideal number of guests for a dinner party \"(convivium\", \"life-sharing\" or \"a living together\") was nine.\n\nBy the late Republic, if not earlier, women dined, reclined, and drank wine along with men.  On at least some occasions, children attended, so they could acquire social skills. Multicourse meals were served by the household slaves, who appear prominently in the art of late antiquity as images of hospitality and luxury.\n\nOne of the chief logistical concerns of the Roman military was feeding the men, cavalry horses, and pack animals, usually mules. Wheat and barley were the primary food sources. Meat, olive oil, wine, and vinegar were also provided. An army of 40,000, including soldiers and other personnel such as slaves, would have about 4,000 horses and 3,500 pack animals. An army of this size would consume about 60 tonnes of grain and 240 amphorae of wine and olive oil each day.\n\nEach man received a ration of about of wheat per day in the form of unmilled grain, which is less perishable than flour. Handmills were used to grind it. The supply of all these foodstuffs depended on availability, and was hard to guarantee during times of war or other adverse conditions. The military attracted sutlers who sold various items, including foodstuffs with which the soldier might supplement his diet.\n\nDuring the expansionism of the Republic, the army usually had combined living off the land and organized supply lines (the \"frumentatio\") to ensure its food supply. Under the Empire, provinces might pay in-kind taxes in the form of grain to provision the permanent garrisons.\n\nRefined cuisine could be moralized as a sign of either civilized progress or decadent decline. The early Imperial historian Tacitus contrasted the indulgent luxuries of the Roman table in his day with the simplicity of the Germanic diet of fresh wild meat, foraged fruit, and cheese, unadulterated by imported seasonings and elaborate sauces.\n\nBecause of the importance of landowning in Roman culture, produce—cereals, legumes, vegetables, and fruit—was most often considered a more civilized form of food than meat. The Stoic philosopher Musonius Rufus, a vegetarian, regarded meat-eaters as not only less civilized but \"slower in intellect.\"\n\n\"Barbarians\" might be stereotyped as ravenous carnivores. The \"Historia Augusta\" describes the emperors Didius Julianus and Septimius Severus as disdaining meat in favor of vegetables, while the first emperor born of two barbarian parents, Maximinus Thrax, is said to have devoured mounds of meat.\n\nFor Pliny, the making of pastries was a sign of civilized countries at peace. The Mediterranean staples of bread, wine, and oil were sacralized by Roman Christianity, while Germanic meat consumption became a mark of paganism, as it might be the product of animal sacrifice.\n\nSome philosophers and Christians resisted the demands of the body and the pleasures of food, and adopted fasting as an ideal. Food became simpler in general as urban life in the West diminished, trade routes were disrupted, and the rich retreated to the more limited self-sufficiency of their country estates. As an urban lifestyle came to be associated with decadence, the Church formally discouraged gluttony, and hunting and pastoralism were seen as simple but virtuous ways of life.\n\n"}
{"id": "46433", "url": "https://en.wikipedia.org/wiki?curid=46433", "title": "Fun", "text": "Fun\n\nFun is the enjoyment of pleasure, particularly in leisure activities. Fun is an experience often unexpected, informal or purposeless. It is an enjoyable distraction, diverting the mind and body from any serious task or contributing an extra dimension to it. Although particularly associated with recreation and play, fun may be encountered during work, social functions, and even seemingly mundane activities of daily living. It may often have little to no logical basis, and opinions on whether an activity is fun may differ from person to person. A distinction between enjoyment and fun is difficult but possible to articulate, fun being a more spontaneous, playful, or active event. There are psychological and physiological implications to the experience of fun.\n\nThe word \"fun\" is associated with sports, sex, entertaining media, high merriment, and amusement. Although its etymology is uncertain, it may be derived from \"fonne\" (fool) and \"fonnen\" (the one fooling the other). Its meaning in 1727 was \"cheat, trick, hoax\", a meaning still retained in the phrase \"to make fun of\".\n\nThe way the word \"fun\" is used demonstrates its distinctive elusiveness and happiness. Expressions such as \"Have fun!\" and \"That was fun!\" indicate that fun is pleasant, personal, and to some extent unpredictable. Expressions such as \"I was making fun of myself\" convey the sense that fun is something that can be amusing and not to be taken seriously. The adjective \"funny\" has two meanings which often need to be clarified between a speaker and listener. One meaning is \"amusing, jocular, droll\" and the other meaning is \"odd, quirky, peculiar\". These differences indicate the evanescent and experiential nature of fun and the difficulty of distinguishing \"fun\" from \"enjoyment\".\n\nFun's evanescence can be seen when an activity regarded as fun becomes goal-oriented. Many physical activities and individual sports are regarded as fun until the participant seeks to win a competition, at which point, much of the fun may disappear as the individual's focus tightens. Surfing is an example. If you are a \"mellow soul\" (not in a competition or engaging in extreme sport) \"once you're riding waves, you're guaranteed to be having ... fun\".\n\nThe pleasure of fun can be seen by the numerous efforts to harness its positive associations. For example, there are many books on serious subjects, about skills such as music, mathematics and languages, normally quite difficult to master, which have \"fun\" added to the title.\n\nMany physical activities provide opportunities to play and have fun. Not only can these activities be fun but can also improve physical and mental states.\n\nAccording to Johan Huizinga, fun is \"an absolutely primary category of life, familiar to everybody at a glance right down to the animal level.\" Psychological studies reveal both the importance of fun and its effect on the perception of time, which is sometimes said to be shortened when one is having fun. As the adage states: \"Time flies when you're having fun\".\n\nIt has been suggested that games, toys, and activities perceived as fun are often challenging in some way. When a person is challenged to think consciously, overcome challenge and learn something new, they are more likely to enjoy a new experience and view it as fun. A change from routine activities appears to be at the core of this perception, since people spend much of a typical day engaged in activities that are routine and require limited conscious thinking. Routine information is processed by the brain as a \"chunked pattern\": \"We rarely look at the real world\", according to game designer Raph Koster, \"we instead recognize something we have chunked, and leave it at that. [...] One might argue that the essence of much of art is in forcing us to see things as they really are rather than as we assume them to be\". Since it helps people to relax, fun is sometimes regarded as a \"social lubricant\", important in adding \"to one's pleasure in life\" and helping to \"act as a buffer against stress\".\n\nFor children, fun is strongly related to play and they have great capacity to extract the fun from it in a spontaneous and inventive way. Play \"involves the capacity to have fun – to be able to return, at least for a little while, to never-never land and enjoy it.\"\n\nSome scientists have identified areas of the brain associated with the perception of novelty, which are stimulated when faced with \"unusual or surprising circumstances\". Information is initially received in the hippocampus, the site of long-term memory consolidation, where the brain attempts to match the new information with recognizable patterns stored in long-term memory. When it is unable to do this, the brain releases dopamine, a chemical which stimulates the amygdala, the site of emotion, and creates a pleasurable feeling that is associated with the new memory. In other words, fun is created by stimulating the brain with novelty.\n\nIn the modern world, fun is sold as a consumer product in the form of games, novelties, television, toys and other amusements. Marxist sociologists such as the Frankfurt School criticise mass-manufactured fun as too calculated and empty to be fully satisfying. Bill Griffith satirises this dysphoria when his cartoon character Zippy the Pinhead asks mechanically, \"Are we having fun yet?\" In the Beatles song \"She's Leaving Home\" fun is called \"the one thing that money can't buy.\"\n\n\n"}
{"id": "168639", "url": "https://en.wikipedia.org/wiki?curid=168639", "title": "Girl, Interrupted", "text": "Girl, Interrupted\n\nGirl, Interrupted is a best-selling 1993 memoir by American author Susanna Kaysen, relating her experiences as a young woman in a psychiatric hospital in the 1960s after being diagnosed with borderline personality disorder. The memoir's title is a reference to the Vermeer painting \"Girl Interrupted at Her Music\".\n\nWhile writing the novel \"Far Afield\", Kaysen began to recall her almost two years at McLean Hospital. She obtained her file from the hospital with the help of a lawyer.\n\nIn 1999, the memoir was adapted into a film of the same name starring Winona Ryder, Angelina Jolie and Brittany Murphy. It was directed by James Mangold.\n\nIn April 1967, 18-year-old Susanna Kaysen is admitted to McLean Hospital, in Belmont, Massachusetts, after attempting suicide by overdosing on pills. She denies that it was a suicide attempt to a psychiatrist, who suggests she take time to regroup in McLean, a private mental hospital. Susanna is diagnosed with borderline personality disorder, and her stay extends to 18 months rather than the proposed couple of weeks.\n\nFellow patients Polly, Cynthia, Lisa Rowe, Lisa Cody, Georgina and Daisy contribute to Susanna's experiences at McLean as she describes their personal issues and how they come to cope with the time they must spend in the hospital. Susanna also introduces the reader to particular staff members, including Valerie, Dr. Wick and Mrs. McWeeney. Susanna and the other girls are eventually informed that the recently released Daisy committed suicide on her birthday. Daisy's death deeply saddens the girls and they hold a prolonged moment of silence in her memory.\n\nSusanna reflects on the nature of her illness, including difficulty making sense of visual patterns, and suggests that sanity is a falsehood constructed to help the \"healthy\" feel \"normal\" in comparison. She also questions how doctors treat mental illness, and whether they are treating the brain or the mind. During her stay in the ward, Susanna also undergoes a period of depersonalization, where she bites open the flesh on her hand after she becomes terrified that she has \"lost her bones\". She develops a frantic obsession with the verification of this proposed reality and even insists on seeing an X-ray of herself to make sure. This hectic moment is described with shorter, choppy sentences that show Kaysen's state of mind and thought processes as she went through them. Also, during a trip to the dentist with Valerie, Susanna becomes frantic after she wakes from the general anesthesia, when no one will tell her how long she was unconscious, and she fears that she has lost time. Like the incident with her bones, Kaysen here also rapidly spirals into a panicky and obsessive state that is only ultimately calmed with medication.\n\nAfter leaving McLean, Susanna mentions that she kept in touch with Georgina and eventually saw Lisa, now a single mother who was about to board the subway with her toddler son and seemed, although quirky, to be sane.\n\n\"Girl, Interrupted\" does not follow a linear storyline, but instead the author provides personal stories through a series of short descriptions of events and personal reflections on why she was placed in the hospital. She begins by talking about the concept of a parallel universe and how easy it is to slip into one, comparing insanity to an alternate world. She discusses how some people fall into insanity gradually and others just snap. Kaysen also details the doctor's visit before first going to the hospital and the taxi ride there at the beginning of the book before launching into the chronicles of her time at the hospital.\n\nThere are two main groups of characters, the patients and the staff. In addition to those there are her parents, her boyfriend and various other minor characters such as her former boss.\n\n\n\n\n"}
{"id": "5191271", "url": "https://en.wikipedia.org/wiki?curid=5191271", "title": "Health Education Exhibition and Resource Centre", "text": "Health Education Exhibition and Resource Centre\n\nThe Health Education Exhibition and Resources Centre opened on 17May 1997 in Kowloon Park, Tsim Sha Tsui, Hong Kong. It is under the management of the Food and Environmental Hygiene Department of the Government of Hong Kong.\n\nThe Health Education Exhibition and Resource Centre is housed in a Grade I historic building (Block S4 of the former Whitfield Barracks) in Kowloon Park. In September 1993 the former Urban Council endorsed a proposal to use the building to house an Urban Council Health Education Exhibition and Resource Centre.\n\nThe centre comprises an exhibition area on two floors and an outdoor health education garden. \n\nThe exhibition on the ground floor introduce the general information of Food and Environmental Hygiene Department and then focuses on various aspects of food and environmental hygiene - high-risk foods, GM foods and food additives. The topics are thoroughly explained through computer games, videos and photos, etc. \n\nThe ground floor lecture room holds regular seminars and visitors can also enjoy a walk in the Health Education Garden while browsing the history and development of the Keep Hong Kong Clean Campaign.\n\nThere is also a souvenir counter on the ground floor.\n\nThe first floor hosts temporary exhibitions in the centre of the hall and permanent exhibitions on environmental hygiene, kitchen hygiene, public toilets and pest control. \n\nAn additional resources center is located on the second floor. The resource center boasts a collection of over 6,000 publications, a lecture room and a small conference room. It is equipped with a great variety of books and audio-visual materials.\n\nThe centre organises extra-curricular programmes such as school and group visits, a volunteer scheme and a health troop membership scheme. \n\n"}
{"id": "11017965", "url": "https://en.wikipedia.org/wiki?curid=11017965", "title": "Health in Scotland", "text": "Health in Scotland\n\nThe health of the Scottish population is, and has been for many years, worse than that of the English. Life expectancy is the lowest in the UK, at 77.1 for men and 81.1 for women, and one of the lowest in the OECD. The gap between Scotland and England has grown since 1980. Some of this is clearly attributable to economic disadvantage, but the differences in health status are more pronounced that would be expected on that basis. It has often been suggested that the Scottish diet is to blame. This is particularly so in Glasgow and the Glasgow effect has been the subject of some academic study.\n\nThe Health and Sport Committee has called for more action to tackle Scotland’s “obesogenic environment”.\n\nThe tobacco control strategy has had a \"positive impact\". Scottish smoking rates fell from 31% in 2003 to 21% in 2015. There is a socio-economic gradient with 35% of people living in the most deprived areas smoking compared to 10% in the most affluent areas.\n\nThere is some evidence that Scottish patients more often seek medical help with stress, anxiety and depression than English patients. To help combat this, Scotland has put in place a Mental Health Strategy. The strategy began in 2016 and will last for ten years. It aims to increase accessibility of mental healthcare towards children and adolescents, improve attitudes towards mental illness, and educate the community. The overall goal is to improve how people in Scotland live, grow, work, and age.\n\n"}
{"id": "58186656", "url": "https://en.wikipedia.org/wiki?curid=58186656", "title": "Infant Welfare Society of Chicago", "text": "Infant Welfare Society of Chicago\n\nThe Infant Welfare Society of Chicago promotes the welfare of infants in the Chicago area of the United States. It was established in 1911 by the staff and volunteers of the Chicago Milk Commission.\n\n"}
{"id": "3261530", "url": "https://en.wikipedia.org/wiki?curid=3261530", "title": "Jehovah's Witnesses and blood transfusions", "text": "Jehovah's Witnesses and blood transfusions\n\nJehovah's Witnesses believe that the Bible prohibits ingesting blood and that Christians should not accept blood transfusions or donate or store their own blood for transfusion. The belief is based on an interpretation of scripture that differs from that of other Christian denominations. It is one of the doctrines for which Jehovah's Witnesses are best known.\n\nJehovah's Witnesses' literature teaches that their refusal of transfusions of whole blood or its four primary components—red cells, white cells, platelets and plasma—is a non-negotiable religious stand and that those who respect life as a gift from God do not try to sustain life by taking in blood, even in an emergency. Witnesses are taught that the use of fractions such as albumin, immunoglobulins and hemophiliac preparations are \"not absolutely prohibited\", and are instead a matter of personal choice.\n\nThe doctrine was introduced in 1945, and has undergone some changes since then. Members of the group who voluntarily accept a transfusion and are not deemed repentant are regarded as having disassociated themselves from the group by abandoning its doctrines and are subsequently shunned by members of the organization. Although accepted by the majority of Jehovah's Witnesses, a minority does not endorse this doctrine.\n\nThe Watch Tower Society has established Hospital Information Services to provide education and facilitate bloodless surgery. This service also maintains Hospital Liaison Committees, whose function is to provide support to adherents.\n\nOn the basis of various biblical texts, including , , and , Jehovah's Witnesses believe:\n\nCertain medical procedures involving blood are specifically prohibited by Jehovah's Witnesses' blood doctrine. This includes the use of red blood cells, white blood cells, platelets and blood plasma. Other fractions derived from blood are not prohibited. Watch Tower publications state that some products derived from one of the four primary components may be so similar to the function of the whole component and carry on such a life-sustaining role in the body that \"most Christians would find them objectionable\". For procedures where there is no specific doctrinal prohibition, individuals are to obtain details from medical personnel and then make a personal decision.\n\nThe following medical procedures are prohibited:\n\nThe following procedures and products are not prohibited, and are left to the decision of individual members:\n\nA variety of bloodless surgical techniques have been developed for use on patients who refuse blood transfusions for reasons that include concern about AIDS, hepatitis, and other blood-borne infections, or immune system reactions. Many physicians have expressed a willingness to respect patients' preferences and provide bloodless treatment and about 200 hospitals offer bloodless medicine and surgery programs for patients who wish to avoid or limit blood transfusions. Bloodless surgery has been successfully performed in significant procedures including open-heart surgery and total hip replacements. A 2012 study in \"JAMA Internal Medicine\" concluded that \"Witnesses do not appear to be at increased risk for surgical complications or long-term mortality when comparisons are properly made by transfusion status. Thus, current extreme blood management strategies do not appear to place patients at heightened risk for reduced long-term survival.\" The study also stated that \"Survival estimates of Witnesses were 86%, 69%, 51%, and 34% at 5, 10, 15, and 20 years after surgery, respectively, vs 74%, 53%, 35%, and 23% among non-Witnesses who received transfusions.\"\n\nBloodless medical and surgical techniques have limitations, and surgeons say the use of various allogeneic blood products and pre-operative autologous blood transfusion are appropriate standards of care for certain patient presentations. The Watch Tower Society states that in medical emergencies where blood transfusions seem to be the only available way to save a life, Jehovah's Witnesses request that doctors provide the best alternative care possible under the circumstances with respect for their personal conviction. The Watch Tower Society has acknowledged that some members have died after refusing blood.\n\nIn some countries, including Canada and the United Kingdom, a parent or guardian's decision can be legally overruled by medical staff. In this case, medical staff may act without consent, by obtaining a court order in a non-emergency situation, or without such in an emergency. In Japan, a doctor must respect the wish of an adult but can override the wishes of a child and their parents if the child is under 15. If a child is aged 15 to 17, a doctor will not perform a transfusion if the parents and the child refuse the transfusion. If a child aged from 15 to 17 objects to a transfusion but the parents demand the transfusion, then a doctor can override the child's wish. In the United States, the American Academy of Pediatrics recommends that in cases of \"an imminent threat to a child's life\", physicians in some cases may \"intervene over parental objections\".\n\nIn 1988, the Watch Tower Society formed \"Hospital Information Services\", a department to help locate doctors or surgical teams who are willing to perform medical procedures on Witnesses without blood transfusions. The department was given oversight of each branch office's \"Hospital Information Desk\", and of one hundred \"Hospital Liaison Committees\" established throughout the United States. As of 2003, about 200 hospitals worldwide provide bloodless medical programs. As of 2006, there are 1,535 Hospital Liaison Committees worldwide coordinating communication between 110,000 physicians.\n\nHospital Information Services researches medical journals to locate information on the availability and effectiveness of bloodless surgery methods. It disseminates information about treatment options to local Hospital Liaison Committees, and to doctors and hospitals.\n\nAnnually since 2004, Jehovah's Witnesses in the United States have been informed that \"with your consent, the law allows for the elders to learn of your admission [to hospital] and provide spiritual encouragement\", but that \"elders serving on a \"Patient Visitation Group\" [could] have access to your name\" only if patients made their wishes known according to the Health Insurance Portability and Accountability Act (HIPAA).\n\nJehovah's Witnesses' branch offices communicate directly with congregations regarding \"ways to benefit from the activities of the Hospital Liaison Committee (HLC) and the Patient Visitation Group (PVG).\" A Jehovah's Witnesses publication in 2000 reported that Argentina had fewer than a hundred HLC committeemen \"giving vital information to the medical community\", adding that \"their work is complemented by hundreds of other self-sacrificing elders who make up Patient Visitation Groups that call on Witness patients to help and encourage them\". Each branch office appoints PVG committeemen, who serve as volunteers.\n\nSince the elaboration of the blood doctrine to the point of prohibiting transfusion, the majority of Jehovah's Witnesses have adopted the organization's position. Those Jehovah's Witnesses who accept the blood doctrine typically hold strongly to their conviction. In the August 1998 issue of \"Academic Emergency Medicine\", Donald Ridley, a Jehovah's Witness and organization staff attorney, argued that carrying an up-to-date Medical Directive card issued by the organization indicates that an individual personally agrees with the established religious position of Jehovah's Witness.\n\nIn 1958, \"The Watchtower\" reported on a particular member of Jehovah's Witnesses who voluntarily accepted blood transfusion, contrary to Watchtower doctrine. The organization confirms that members have accepted blood transfusions, despite the imposition in 1961 of a communal shunning policy for willful acceptance.\n\nIn 1982, a peer-reviewed case study of a congregation of Jehovah's Witnesses was undertaken by Drs. Larry J. Findley and Paul M. Redstone to evaluate individual belief in respect to blood among Jehovah's Witnesses. The study showed that 12% were willing to accept transfusion therapy forbidden by Jehovah's Witness doctrine. One peer-reviewed study examining medical records indicated a similar percentage of Jehovah's Witnesses willing to accept blood transfusions for their children. Young adults also showed a willingness to accept blood transfusions. In another study, Jehovah's Witness patients presented for labor and delivery showed a willingness to accept some form of blood or blood products. Of these patients, 10 percent accepted whole blood transfusion.\n\nWatch Tower publications have noted that within religions, the personal beliefs of members often differ from official doctrine. Regarding Jehovah's Witnesses acceptance of the organization's official position on blood, Drs Cynthia Gyamfi and Richard Berkowitz state, \"It is naïve to assume that all people in any religious group share the exact same beliefs, regardless of doctrine. It is well known that Muslims, Jews and Christians have significant individual variations in their beliefs. Why should that not also be true of Jehovah's Witnesses?\"\n\nAmbivalence and rejection of the blood doctrine dates back to at least the 1940s. After the Watch Tower Society established the doctrine, teaching that blood should not be eaten (circa 1927-31), Margaret Buber, who was never a member of the denomination, offered a firsthand eyewitness account of Jehovah's Witnesses in the Nazi Ravensbrück concentration camp. She relates that an overwhelming majority were willing to eat blood sausage despite having alternate food to choose from, and specifically after considering biblical statements regarding blood.\n\nFrom 1931, when the name \"Jehovah's witnesses\" was adopted, Watch Tower Society publications maintained the view of Society founder Charles Taze Russell that the reference to abstaining from the eating of blood in the Apostolic Decree of Acts 15:19–29 was a \"suggestion\" to be given to Gentile converts. Watch Tower publications during the presidency of Joseph Franklin Rutherford commended the commercial and emergency uses of blood. A 1925 issue of \"The Golden Age\" commended a man for donating blood 45 times without payment. In 1927, \"The Watchtower\" noted, without elaboration, that in Genesis 9, God decreed that Noah and his offspring \"must not eat the blood, because the life is in the blood\". In 1940 \"Consolation\" magazine reported on a woman who accidentally shot herself with a revolver in her heart and survived a major surgical procedure during which an attending physician donated a quart of his own blood for transfusion.\n\nIn 1944, with the Watch Tower Society under the administration of president Nathan Homer Knorr, \"The Watchtower\" asserted that the decrees contained in Genesis 9:4 and Leviticus 17:10–14 forbade the eating or drinking of blood in biblical times \"whether by transfusion or by the mouth\" and that this applied \"in a spiritual way to the consecrated persons of good-will today, otherwise known as 'Jonadabs' of the Lord's 'other sheep'.\"\n\nIn September 1945, representatives of the Watch Tower Society in the Netherlands commented on blood transfusion in the Dutch edition of \"Consolation\". A translation of their comments into English reads: According to sociologist Richard Singelenbreg, the statement appearing in the Dutch edition of \"Consolation\" may have been published without knowledge of the doctrinal position published in the English July 1945 issue of \"Consolation\" by the Watch Tower Society's headquarters in the United States.\n\nIn 1945, the application of the doctrine on blood was expanded to prohibit blood transfusions of whole blood, whether allogeneic or autologous. The prohibition did not specify any punitive measures for accepting a transfusion, but by January 1961—in what was later described as an application of \"increased strictness\"—it was ruled that it was a disfellowshipping offense to conscientiously accept a blood transfusion. Watch Tower publications warned that accepting a blood transfusion could prevent Witnesses from living eternally in God's new world, the hope held by members: \"It may result in the immediate and very temporary prolongation of life, but that at the cost of eternal life for a dedicated Christian.\"\n\nIn September 1956, \"Awake!\" stated, \"certain blood fractions ... also come under the Scriptural ban\". A position against \"the various blood fractions\" was reiterated in September 1961. In November of the same year, the doctrine was modified to allow individual members to decide whether they could conscientiously accept fractions used from blood for purposes such as vaccination. This position has been expanded on since; the pre-formatted Durable Power of Attorney form provided by the Watch Tower Society includes an option for Jehovah's Witnesses to \"accept all fractions derived from any primary component of blood.\"\n\nIn 1964, Jehovah's Witnesses were prohibited from obtaining transfusions for pets, from using fertilizer containing blood, and were even advised (if their conscience troubled them) to write to dog food manufacturers to verify that their products were blood-free. Later that year, it was stated that doctors or nurses who are Jehovah's Witness would not administer blood transfusions to fellow dedicated members. As to administering transfusions to non-members, \"The Watchtower\" stated that such a decision is \"left to the Christian doctor's own conscience.\"\n\nIn 1982, a \"Watchtower\" article declared that it would be wrong for a Witness to allow a leech to feed on his/her blood as part of a medical procedure, due to the sacredness of blood.\n\nIn 1989 \"The Watchtower\" stated, \"Each individual must decide\" whether to accept hemodilution and autologous blood salvage (cell saver) procedures. In 1990, a brochure entitled \"How Can Blood Save Your Life?\" was released, outlining Jehovah's Witnesses' general doctrine on blood.\n\nIn 2000, the Watch Tower Society's stand on blood fractions was clearly stated. Members were instructed to personally decide if accepting a fraction would violate the doctrine on blood. In a later article, members were reminded that Jehovah's Witnesses do not donate blood or store their own blood prior to surgery.\n\nIn May 2001, the Watch Tower Society revised its medical directives and identity cards addressing its doctrinal position on blood; the revised materials were distributed from May 3, 2001. These revised documents specified that \"allogeneic blood transfusions\" were unacceptable whereas the former document (dated 1999) stated that \"blood transfusions\" were unacceptable. The revised 2001 documents were active until December 20, 2001. The Watch Tower Society then rescinded the revised document, stating, \"After further review, it has been determined that the cards dated \"md-E 6/01\" and \"ic-E 6/01\" should not be used. Please destroy these items and make sure that they are not distributed to the publishers.\" Elders were instructed to revert to the older 1999 edition of the medical directives and identity cards.\n\nWatch Tower Society publications frequently claim negative consequences of blood transfusions:\n\nOpposition to the Watch Tower doctrines on blood transfusions has come from both members and non-members. A group of dissident Witnesses known as Associated Jehovah's Witnesses for Reform on Blood (AJWRB) states that there is no biblical basis for the prohibition of blood transfusions and seeks to have some policies changed. In a series of articles in the \"Journal of Medical Ethics\" US neurologist Osamu Muramoto, who is a medical adviser to the AJWRB, has raised issues including what he claims is coercion to refuse transfusions, doctrinal inconsistency, selective use of information by the Watch Tower Society to exaggerate the dangers of transfusions and the use of outdated medical beliefs.\n\nDissident Witnesses say the Society's use of Leviticus 17:12 to support its opposition to blood transfusions conflicts with its own teachings that Christians are not under the Mosaic law. Theologian Anthony Hoekema claims the blood prohibited in Levitical laws was not human, but animal. He cites other authors who support his view that the direction at Acts 15 to abstain from blood was intended not as an everlasting covenant but a means of maintaining a peaceful relationship between Jewish and Gentile Christians. He has described as \"absurd literalism\" the Witnesses' use of a scriptural prohibition on eating blood to prohibit the medical transfusion of human blood.\n\nOsamu Muramoto has argued that the refusal by Jehovah's Witnesses of \"life-saving\" blood treatment creates serious bio-medical ethical issues. He has criticized the \"controlling intervention\" of the Watch Tower Society by means of what he claims is information control and its policy of penalising members who accept blood transfusions or advocate freedom to choose blood-based treatment. He says the threat of being classified as a disassociated Witness and subsequently shunned by friends and relatives who are members coerces Jehovah's Witnesses to accept and obey the prohibition on blood transfusions. In one particular case involving a Russian district court decision, however, the European Court of Human Rights (ECHR) found nothing in the judgments to suggest that any form of improper pressure or undue influence was applied. It noted: \"On the contrary, it appears that many Jehovah's Witnesses have made a deliberate choice to refuse blood transfusions in advance, free from time constraints of an emergency situation.\" The court said: \"The freedom to accept or refuse specific medical treatment, or to select an alternative form of treatment, is vital to the principles of self-determination and personal autonomy. A competent adult patient is free to decide ... not to have a blood transfusion. However, for this freedom to be meaningful, patients must have the right to make choices that accord with their own views and values, regardless of how irrational, unwise or imprudent such choices may appear to others.\"\n\nMuramoto has claimed the intervention of Hospital Liaison Committees can add to \"organisational pressure\" applied by family members, friends and congregation members on Witness patients to refuse blood-based treatment. He notes that while HLC members, who are church elders, \"may give the patient 'moral support', the influence of their presence on the patient is known to be tremendous. Case reports reveal JW patients have changed their earlier decision to accept blood treatment after a visit from the elders.\" He claims such organizational pressure compromises the autonomy of Witness patients and interferes with their privacy and confidentiality. He has advocated a policy in which the Watch Tower organization and congregation elders would not question patients on the details of their medical care and patients would not disclose such information. He says the Society adopted such a policy in 1983 regarding details of sexual activity between married couples.\n\nWatch Tower spokesman Donald T. Ridley says neither elders nor HLC members are instructed or encouraged to probe into the health care decisions of Witness patients and do not involve themselves in patient hospitalisations unless patients request their assistance. Yet Watchtower HLC representative David Malyon says he would respond to \"sin\" of Witnesses he is privy to by effectively saying \"Are you going to tell them or shall I!\" Nevertheless Ridley says Muramoto's suggestion that Witnesses should be free to disregard Watch Tower scriptural teachings and standards is preposterous. He says loving God means obeying commandments, not disobeying them and hiding one's disobedience from others.\n\nMuramoto recommends doctors have a private meeting with patients to discuss their wishes, and that church elders and family members not be present, enabling patients to feel free of church pressure. He suggests doctors question patients on (a) whether they have considered that the Watch Tower Society might soon approve some medical practices they currently find objectionable, in the same manner that it has previously abandoned its opposition to vaccination and organ transplants; (b) whether Witness patients know which blood components are allowed and which are prohibited, and whether they acknowledge that those rulings are organizational policy rather than biblical teachings; and (c) whether they realize that although some Bible scriptures proscribe the eating of blood, eating and transfusing blood have entirely different effects on the body. English HLC representative David Malyon has responded that Muramoto's suggested questions are an affront to coerce Jehovah's Witnesses with \"complicated philosophical inquisition\" and, if used by doctors, would be \"an abusive transformation of the medical role of succour and care into that of devil's advocate and trickster\".\n\nMuramoto has claimed many Watch Tower Society publications employ exaggeration and emotionalism to emphasize the dangers of transfusions and the advantages of alternative treatments, but present a distorted picture by failing to report any benefits of blood-based treatment. Nor do its publications acknowledge that in some situations, including rapid and massive haemorrhage, there are no alternatives to blood transfusions. He claims Watch Tower Society publications often discuss the risk of death as a result of refusing blood transfusions, but give little consideration to the prolonged suffering and disability, producing an added burden on family and society, that can result from refusal. Attorney and former Witness Kerry Louderback-Wood also claims that Witness publications exaggerate the medical risks of taking blood and the efficiency of non-blood medical therapies in critical situations.\n\nDouglas E. Cowan, an academic in the sociology of religion, has claimed that members of the Christian countercult movement who criticize the Watch Tower Society, make selective use of information themselves. For example, Christian apologist Richard Abanes wrote that their ban on blood transfusions, \"has led to countless Witness deaths over the years, including many children.\" Cowan wrote: \"When the careful reader checks [Abanes' footnote], however, looking perhaps for some statistical substantiation, he or she finds only a statistical conjecture based on 1980 Red Cross blood use figures.\" Cowan also says Abanes omits \"critical issues\" in an attempt to \"present the most negative face possible.\" Cowan wrote that \"the reader is left with the impression that the Watchtower Society knowingly presides over a substantial number of preventable deaths each year.\"\n\nOsamu Muramoto says the Watch Tower Society relies on discarded, centuries-old medical beliefs to support its assertion that blood transfusions are the same as eating blood. A 1990 Watch Tower brochure on blood quoted a 17th-century anatomist to support its view. Muramoto says the view that blood is nourishment—still espoused in Watch Tower publications—was abandoned by modern medicine many decades ago. He has criticized an analogy commonly used by the Society in which it states: \"Consider a man who is told by the doctor that he must abstain from alcohol. Would he be obedient if he quit drinking alcohol but had it put directly into his veins?\" Muramoto says the analogy is false, explaining: \"Orally ingested alcohol is absorbed as alcohol and circulated as such in the blood, whereas orally eaten blood is digested and does not enter the circulation as blood. Blood introduced directly into the veins circulates and functions as blood, not as nutrition. Hence, blood transfusion is a form of cellular organ transplantation. And ... organ transplants are now permitted by the WTS.\" He says the objection to blood transfusions on the basis of biblical proscriptions against eating blood is similar to the refusal of a heart transplant on the basis that a doctor warned a patient to abstain from eating meat because of his high cholesterol level.\n\nDavid Malyon, chairman of the English Hospital Liaison Committee in Luton, England, has claimed that Muramoto's discussion of the differences between consuming blood and alcohol is pedantic and says blood laws in the Bible are based upon the reverence for life and its association with blood, and that laws should be kept in the spirit as much as in the letter.\n\nMuramoto has described as peculiar and inconsistent the Watch Tower policy of acceptance of all the individual components of blood plasma as long as they are not taken at the same time. He says the Society offers no biblical explanation for differentiating between prohibited treatments and those considered a \"matter of conscience\", explaining the distinction is based entirely on arbitrary decisions of the Governing Body, to which Witnesses must adhere strictly on the premise of them being Bible-based \"truth\". He has questioned why white blood cells (1 per cent of blood volume) and platelets (0.17 per cent) are forbidden, yet albumin (2.2 per cent of blood volume) is permitted. He has questioned why donating blood and storing blood for autologous transfusion is deemed wrong, but the Watch Tower Society permits the use of blood components that must be donated and stored before Witnesses use them. He has questioned why Witnesses, although viewing blood as sacred and symbolizing life, are prepared to let a person die by placing more importance on the symbol than the reality it symbolizes.\n\nKerry Louderback-Wood alleges that by labeling the currently acceptable blood fractions as \"minute\" in relation to whole blood, the Watch Tower organization causes followers to misunderstand the scope and extent of allowed fractions.\n\nWitnesses respond that the real issue is not of the fluid \"per se\", but of respect and obedience to God. They say their principle of abstaining from blood as a display of respect is demonstrated by the fact that members are allowed to eat meat that still contains some blood. As soon as blood is drained from an animal, the \"respect\" has been shown to God, and then a person can eat the meat even though it may contain a small amount of blood. Jehovah's Witnesses' view of meat and blood is different from that of kosher Jewish adherents, who go to great lengths to remove minor traces of blood.\n\n\n\n"}
{"id": "3636272", "url": "https://en.wikipedia.org/wiki?curid=3636272", "title": "Lactational amenorrhea", "text": "Lactational amenorrhea\n\nLactational amenorrhea is the temporary postnatal infertility that occurs when a woman is amenorrheic (not menstruating) and fully breastfeeding.\n\nFor women who follow the suggestions and meet the criteria (listed below), LAM is >98% effective during the first six months postpartum.\n\nIf not combined with barrier contraceptives, spermicides, hormonal contraceptives, or intrauterine devices, lactational amenorrhea method (LAM) may be considered natural family planning by the Roman Catholic Church.\n\nBreastfeeding delays the resumption of normal ovarian cycles by disrupting the pattern of pulsatile release of GnRH from the hypothalamus and hence LH from the pituitary. The plasma concentrations of FSH during lactation are sufficient to induce follicle growth, but the inadequate pulsatile LH signal results in a reduced estradiol production by these follicles. When follicle growth and estradiol secretion does increase to normal, the suckling stimulus prevents the generation of a normal preovulatory LH surge and follicles either fail to rupture, or become atretic or cystic. Only when the suckling stimulus declines sufficiently to allow generation of a normal preovulatory LH surge to occur will ovulation take place with the formation of a corpus luteum of variable normality. Thus suckling delays the resumption of normal ovarian cyclicity by disrupting but not totally inhibiting, the normal pattern of release of GnRH by the hypothalamus. The mechanism of suckling-induced disruption of GnRH release remains unknown. \n\nIn women, hyperprolactinemia is often associated with amenorrhea, a condition that resembles the physiological situation during lactation (lactational amenorrhea). Mechanical detection of suckling increases prolactin levels in the body to increase milk synthesis. Excess prolactin may inhibit the menstrual cycle directly, by a suppressive effect on the ovary, or indirectly, by decreasing the release of GnRH.\n\nReturn of menstruation following childbirth varies widely among individuals. A strong relationship has been observed between the amount of suckling and the contraceptive effect, such that the combination of feeding on demand rather than on a schedule and feeding only breast milk rather than supplementing the diet with other foods will greatly extend the period of effective contraception. The closer a woman's behavior is to the Seven Standards of ecological breastfeeding, the later (on average) her cycles will return. Average return of menses for women following all seven criteria is 14 months after childbirth, with some reports being as soon as 2 months while others are as late as 42 months. Couples who desire spacing of 18 to 30 months between children can often achieve this through breastfeeding alone, though this is not a foolproof method as return of menses is unpredictable and conception can occur in the weeks preceding the first menses. \n\nAlthough the first post-partum cycle is sometimes anovulatory (reducing the likelihood of becoming pregnant again before having a post-partum period), subsequent cycles are almost always ovulatory and therefore must be considered fertile. For women exclusive breastfeeding ovulation tends to return after their first menses after the 56 days postpartum time period. Supplementing nutritional intake can lead to an earlier return of menses and ovulation then exclusive breastfeeding. Nursing more frequently for a shorter amount of time was shown to be more successful in prolonging amenorrhea then nursing longer but less frequently. The continuing of breastfeeding, while introducing solids after 6 months, to 12 months were shown to have an efficiency rate of 92.6 – 96.3 percent in pregnancy prevention. Because of this some women find that breastfeeding interferes with fertility even after ovulation has resumed.\n\nThe Seven Standards: Phase 1 of Ecological Breastfeeding\n\n\nPhase 1 is the time of exclusive breastfeeding and thus usually lasts six to eight months.\n\n"}
{"id": "354076", "url": "https://en.wikipedia.org/wiki?curid=354076", "title": "Maternal death", "text": "Maternal death\n\nMaternal death or maternal mortality is defined by the World Health Organization (WHO) as \"the death of a woman while pregnant or within 42 days of termination of pregnancy, irrespective of the duration and site of the pregnancy, from any cause related to or aggravated by the pregnancy or its management but not from accidental or incidental causes.\"\n\nThere are two performance indicators that are sometimes used interchangeably: maternal mortality ratio and maternal mortality rate, which confusingly both are abbreviated \"MMR\". By 2017, the world maternal mortality rate had declined 44% since 1990, but still every day 830 women die from pregnancy or childbirth related causes. According to the United Nations Population Fund (UNFPA) 2017 report, this is equivalent to \"about one woman every two minutes and for every woman who dies, 20 or 30 encounter complications with serious or long-lasting consequences. Most of these deaths and injuries are entirely preventable.\"\n\nUNFPA estimated that 303,000 women died of pregnancy or childbirth related causes in 2015. These causes range from severe bleeding to obstructed labour, all of which have highly effective interventions . As women have gained access to family planning and skilled birth attendance with backup emergency obstetric care, the global maternal mortality ratio has fallen from 385 maternal deaths per 100,000 live births in 1990 to 216 deaths per 100,000 live births in 2015, and many countries halved their maternal death rates in the last 10 years.\n\nAlthough attempts have been made in reducing maternal mortality, there is much room for improvement, particularly in impoverished regions. Over 85% of maternal deaths are from impoverished communities in Africa and Asia. The effect of a mother's death results in vulnerable families. Their infants, if they survive childbirth, are more likely to die before reaching their second birthday.\n\nAccording to a 2003 article in the \"British Medical Bulletin\", maternal death was first defined as \"the death of a woman while pregnant or within 42 days of termination of pregnancy, irrespective of the duration and site of the pregnancy, from any cause related to or aggravated by the pregnancy or its management but not from accidental or incidental causes \" in the tenth revision of the International Classification of Diseases (ICD-10) which was completed in 1992. It is the definition still in use by the World Health Organization (WHO), which defines maternal mortality as \"the death of a woman while pregnant or within 42 days of termination of pregnancy, irrespective of the duration and site of the pregnancy, from any cause related to or aggravated by the pregnancy or its management but not from accidental or incidental causes.\"\n\nThe 2003 article \"Global burden of maternal death and disability\" noted that the definition leaves out a segment of the population. According to the Centers for Disease Control, during the period 1974-75 in Georgia, US, 29% of maternal deaths \"occurred after 42 days of pregnancy termination and 6% occurred after 90 days post-partum.\" This may explain the CDC’s definition, extending the period of consideration “within 1 year of the end of pregnancy.” Adding to the WHO definition, the CDC also mentions that this death can be irrespective of the outcome of the pregnancy.\n\nSevere maternal morbidity or SMM, is an unanticipated acute or chronic health outcome after labor and delivery that detrimentally affects a woman's health. Severe Maternal Morbidity (SMM) includes any unexpected outcomes from labor or delivery that cause both short and long-term consequences to the mother’s overall health. There are nineteen total indicators used by the CDC to help identify SMM, with the most prevalent indicator being a blood transfusion.  Other indicators include an acute myocardial infarction (\"heart attack\"), aneurysm, and kidney failure. All of this identification is done by using ICD-10 codes, which are disease identification codes found in hospital discharge data. Using these definitions that rely on these codes should be used with careful consideration since some may miss some cases, have a low predictive value, or may be difficult for different facilities to operationalize. There are certain screening criteria that may be helpful and are recommended through the American \"College\" of Obstetricians and Gynecologists as well as the Society for Maternal-Fetal Medicine (SMFM). These screening criteria for SMM are for transfusions of four or more units of blood and admission of a pregnant woman or a postpartum woman to an ICU facility or unit.\n\nThe greatest proportion of women with SMM are those who require a blood transfusion during delivery, mostly due to excessive bleeding. Blood transfusions given during delivery due to excessive bleeding has increased the rate of mothers with SMM. The rate of SMM has increased almost 200% between 1993 (49.5 per 100,000 live births) and 2014 (144.0 per 100,000 live births). This can be seen with the increased rate of blood transfusions given during delivery, which increased from 1993 (24.5 per 100,000 live births) to 2014 (122.3 per 100,000 live births).\n\nIn the United States, severe maternal morbidity has increased over the last several years, impacting greater than 50,000 women in 2014 alone. There is no conclusive reason for this dramatic increase. It is thought that the overall state of health for pregnant women is impacting these rates. For example, complications can derive from underlying chronic medical conditions like diabetes, obesity, HIV/AIDs, and high blood pressure. These underlying conditions are also thought to lead to increased risk of maternal mortality.\n\nThe increased rate for SMM can also be indicative of potentially increased rates for maternal mortality, since without identification and treatment of SMM, these conditions would lead to increased maternal death rates. Therefore, diagnosis of SMM can be considered a “near miss” for maternal mortality. With this consideration, several different expert groups have urged obstetric hospitals to review SMM cases for opportunities that can lead to improved care, which in turn would lead to improvements with maternal health and a decrease in the number of maternal deaths.\n\nFactors that increase maternal death can be direct or indirect. In a 2009 article on maternal morbidity, the authors said, that generally, there is a distinction between a direct maternal death that is the result of a complication of the pregnancy, delivery, or management of the two, and an indirect maternal death, that is a pregnancy-related death in a patient with a preexisting or newly developed health problem unrelated to pregnancy. Fatalities during but unrelated to a pregnancy are termed \"accidental\", \"incidental\", or nonobstetrical maternal deaths.\n\nAccording to a study published in the \"Lancet\" which covered the period from 1990 to 2013, the most common causes are postpartum bleeding (15%), complications from unsafe abortion (15%), hypertensive disorders of pregnancy (10%), postpartum infections (8%), and obstructed labour (6%). Other causes include blood clots (3%) and pre-existing conditions (28%). Maternal mortality caused by severe bleeding and infections are mostly after childbirth. Indirect causes are malaria, anaemia, HIV/AIDS, and cardiovascular disease, all of which may complicate pregnancy or be aggravated by it . Risk factors associated with increased maternal death include the age of the mother, obesity before becoming pregnant, other pre-existing chronic medical conditions, and cesarean delivery.\n\nPregnancy-related deaths between 2011 and 2014 in the United States have been shown to have major contributions from non-communicable diseases and conditions, and the following are some of the more common causes related to maternal death: cardiovascular diseases (15.2%.), non-cardiovascular diseases (14.7%), infection or sepsis (12.8%), hemorrhage (11.5%), cardiomyopathy (10.3%), thrombotic pulmonary embolism (9.1%), cerebrovascular accidents (7.4%), hypertensive disorders of pregnancy (6.8%), amniotic fluid embolism (5.5%), and anesthesia complications (0.3%).\n\nAccording to a 2004 WHO publication, sociodemographic factors such as age, access to resources and income level are significant indicators of maternal outcomes. Young mothers face higher risks of complications and death during pregnancy than older mothers, especially adolescents aged 15 years or younger. Adolescents have higher risks for postpartum hemorrhage, puerperal endometritis, operative vaginal delivery, episiotomy, low birth weight, preterm delivery, and small-for-gestational-age infants, all of which can lead to maternal death. The leading cause of death for girls at the age of 15 in developing countries is complication through pregnancy and childbirth. They have more pregnancies, on average, than women in developed countries and it has been shown that 1 in 180 fifteen year old girls in developing countries who become pregnant will die due to complications during pregnancy or childbirth. This is compared to women in developed countries, where the likelihood is 1 in 4900 live births. However, in the United States, as many women of older age continue to have children, trends have seen the maternal mortality rate to rise in some states, especially among women over 40 years old.\n\nStructural support and family support influences maternal outcomes . Furthermore, social disadvantage and social isolation adversely affects maternal health which can lead to increases in maternal death. Additionally, lack of access to skilled medical care during childbirth, the travel distance to the nearest clinic to receive proper care, number of prior births, barriers to accessing prenatal medical care and poor infrastructure all increase maternal deaths.\n\nUnsafe abortion is another major cause of maternal death. According to the World Health Organization in 2009, every eight minutes a woman died from complications arising from unsafe abortions. Complications include hemorrhage, infection, sepsis and genital trauma.\n\nBy 2007, globally, preventable deaths from improperly performed procedures constitute 13% of maternal mortality, and 25% or more in some countries where maternal mortality from other causes is relatively low, making unsafe abortion the leading single cause of maternal mortality worldwide.\n\nAbortions are more common in developed regions than developing regions of the world. It is estimated that 26% of all pregnancies that occur in the world are terminated by induced abortions. Out of these, 41% occur in developed regions and 23% of them occur in developing regions.\n\nUnsafe abortion practices are defined by the WHO as procedures that are “carried out by persons either lacking the necessary skills or in an environment that does not conform to minimal medical standards, or both.\" Using this definition, the WHO estimates that out of the 45 million abortions that are performed each year globally, 19 million of these are considered unsafe. Also, 97% of these unsafe abortions occur in developing countries.\n\nMaternal deaths caused by improperly performed procedures are preventable and contribute 13% to the maternal mortality rate worldwide. This number is increased to 25% in countries where other causes of maternal mortality are low, such as in Eastern European and South American countries. This makes unsafe abortion practices the leading cause of maternal death worldwide.\n\nSocial factors impact a woman’s decision to seek abortion services, and these can include fear of abandonment from the partner, family rejection and lack of employment. Social factors such as these can lead to the consequence of undergoing an abortion that is considered unsafe.\n\nOne proposal for measuring trends and variations in risks to maternal death associated with maternal death is to measure the percentage of induced abortions that are defined unsafe (by the WHO) and by the ratio of deaths per 100,000 procedures, which would be defined as the abortion mortality ratio.\n\nThere are four primary types of data sources that are used to collect abortion-related maternal mortality rates. These four sources are confidential enquiries, registration data, verbal autopsy, and facility-based data sources. A verbal autopsy is a systematic tool that is used to collect information on the cause of death from lay-people and not medical professionals.\n\nConfidential enquires for maternal deaths do not occur very often on a national level in most countries. Registration systems are usually considered the “gold-standard” method for mortality measurements. However, they have been shown to miss anywhere between 30-50% of all maternal deaths. Another concern for registration systems is that 75% of all global births occur in countries where vital registration systems do not exist, meaning that many maternal deaths occurring during these pregnancies and deliveries may not be properly record through these methods. There are also issues with using verbal autopsies and other forms of survey in recording maternal death rates. For example, the family’s willingness to participate after the loss of a loved one, misclassification of the cause of death, and under-reporting all present obstacles to the proper reporting of maternal mortality causes. Finally, an potential issue with facility-based data collection on maternal mortality is the likelihood that women who experience abortion-related complications to seek care in medical facilities. This is due to fear of social repercussions or legal activity in countries where unsafe abortion is common since it is more likely to be legally restrictive and/or more highly stigmatizing. Another concern for issues related to errors in proper reporting for accurate understanding of maternal mortality is the fact that global estimates of maternal deaths related to a specific cause present those related to abortion as a proportion of the total mortality rate. Therefore, any change, whether positive or negative, in the abortion-related mortality rate is only compared relative to other causes, and this does not allow for proper implications of whether abortions are becoming more safe or less safe with respect to the overall mortality of women.\n\nProviding safe services for pregnant women within family planning facilities is applicable to all regions. This is an important fact to consider since abortion is legal in some way in 189 out of 193 countries worldwide. Promoting effective contraceptive use and information distributed to a wider population, with access to high-quality care, can significantly make strides towards reducing the number of unsafe abortions. However, this alone will not eliminate the demand for safe services.\n\nThe four measures of maternal death are the maternal mortality ratio (MMR), maternal mortality rate, lifetime risk of maternal death and proportion of maternal deaths among deaths of women of reproductive years (PM).\n\nMaternal mortality ratio (MMR): the ratio of the number of maternal deaths during a given time period per 100,000 live births during the same time-period. The MMR is used as a measure of the quality of a health care system.\n\nMaternal mortality rate (MMRate): the number of maternal deaths in a population divided by the number of women of reproductive age, usually expressed per 1,000 women.\nLifetime risk of maternal death: refers to the probability that a 15-year-old female will die eventually from a maternal cause if she experiences throughout her lifetime the risks of maternal death and the overall levels of fertility and mortality that are observed for a given population. The adult lifetime risk of maternal mortality can be derived using either the maternal mortality ratio (MMR), or the maternal mortality rate (MMRate). \nProportion of maternal deaths among deaths of women of reproductive age (PM):\nthe number of maternal deaths in a given time period divided by the total deaths among \nwomen aged 15–49 years.\n\nApproaches to measuring maternal mortality includes civil registration system, household surveys, census, reproductive age mortality studies (RAMOS) and verbal autopsies.\n\nThe United Nations Population Fund (UNFPA; formerly known as the United Nations Fund for Population Activities) have established programs that support efforts in reducing maternal death. These efforts include education and training for midwives, supporting access to emergency services in obstetric and newborn care networks, and providing essential drugs and family planning services to pregnant women or those planning to become pregnant. They also support efforts for review and response systems regarding maternal deaths.\n\nAccording to the 2010 United Nations Population Fund report, developing nations account for ninety-nine percent of maternal deaths with the majority of those deaths occurring in Sub-Saharan Africa and Southern Asia. \nGlobally, high and middle income countries experience lower maternal deaths than low income countries. The Human Development Index (HDI) accounts for between 82 and 85 percent of the maternal mortality rates among countries. In most cases, high rates of maternal deaths occur in the same countries that have high rates of infant mortality. These trends are a reflection that higher income countries have stronger healthcare infrastructure, medical and healthcare personnel, use more advanced medical technologies and have fewer barriers to accessing care than low income countries. Therefore, in low income countries, the most common cause of maternal death is obstetrical hemorrhage, followed by hypertensive disorders of pregnancy, in contrast to high income countries, for which the most common cause is thromboembolism.\n\nBetween 1990 and 2015, the maternal mortality ratio has decreased from 385 deaths per 100,000 live births to 216 maternal deaths per 100,000 live births. Some factors that have attributed to the decreased maternal deaths seen between this period are in part to the access that women have gained to family planning services and skilled birth attendance, meaning a midwife, doctor, or trained nurse), with back-up obstetric care for emergency situations that may occur during the process of labor. This can be examined further by looking at statistics in some areas of the world where inequities in women’s access to health care services reflect an increased number of maternal deaths. The high maternal death rates also reflect access to health services between the poor communities compared to women who are rich.\n\nAt a country level, India (19% or 56,000) and Nigeria (14% or 40,000) accounted for roughly one third of the maternal deaths in 2010 . Democratic Republic of the Congo, Pakistan, Sudan, Indonesia, Ethiopia, United Republic of Tanzania, Bangladesh and Afghanistan accounted for between 3 and 5 percent of maternal deaths each. These ten countries combined accounted for 60% of all the maternal deaths in 2010 according to the United Nations Population Fund report. Countries with the lowest maternal deaths were Greece, Iceland, Poland, and Finland.\n\nUntil the early 20th century developed and developing countries had similar rates of maternal mortality. Since most maternal deaths and injuries are preventable, they have been largely eradicated in the developed world.\n\nA lot of progress has been made since the United Nations made the reduction of maternal mortality part of the Millennium Development Goals (MDGs) in 2000. Bangladesh, for example, cut the number of deaths per live births by almost two thirds from 1990 to 2015. However, the MDG was to reduce it by 75%. According to government data, the figure for 2015 was 181 maternal deaths per 100,000 births. The MDG mark was 143 per 100,000. A further reduction of maternal mortality is now part of the Agenda 2030 for sustainable development. The United Nations has more recently developed a list of goals termed the Sustainable Development Goals. The target of the third Sustainable Development Goal (SDG) is to reduce the global maternal mortality rate (MMR) to less than 70 per 100,000 live births by 2030. Some of the specific aims of the Sustainable Development Goals are to prevent unintended pregnancies by ensuring more women have access to contraceptives, as well as providing women who become pregnant with a safe environment for delivery with respectful and skilled care during delivery. This also includes providing women with complications during delivery timely access to emergency services through obstetric care.\n\nThe WHO has also developed a global strategy and goal to end preventable death related to maternal mortality. A major goal of this strategy is to identify and address the causes of maternal and reproductive morbidities and mortalities, as well as disabilities related to maternal health outcomes. The collaborations that this strategy introduces are to address the inequalities that are shown with access to reproductive, maternal, and newborn services, as well as the quality of that care. They also ensure that universal health coverage is essential for comprehensive health care services related to maternal and newborn health. The WHO strategy also implements strengthening health care systems to ensure quality data collection to better respond to the needs of women and girls, as well as ensuring responsibility and accountability to improve the equity and quality of care provided to women.\n\nThere are significant maternal mortality intracountry variations, especially in nations with large equality gaps in income and education and high healthcare disparities. Women living in rural areas experience higher maternal mortality than women living in urban and sub-urban centers because those living in wealthier households, having higher education, or living in urban areas, have higher use of healthcare services than their poorer, less-educated, or rural counterparts. There are also racial and ethnic disparities in maternal health outcomes which increases maternal mortality in marginalized groups.\n\nThe US has the \"highest rate of maternal mortality in the industrialized world.\" In the United States, the maternal death rate averaged 9.1 maternal deaths per 100,000 live births during the years 1979–1986, but then rose rapidly to 14 per 100,000 in 2000 and 17.8 per 100,000 in 2009. In 2013 the rate was 18.5 deaths per 100,000 live births. It has been suggested that the rise in maternal death in the United States may be due to improved identification and misclassification resulting in false positives. The rate has steadily increased to 18.0 deaths per 100,000 live births in 2014. Between 2011 and 2014, there were 7,208 deaths that were reported to the CDC that occurred for women within a year of the end of their pregnancy. Out of this there were 2,726 that were found to be pregnancy-related deaths.\n\nSince 2016, ProPublica and NPR investigated factors that led to the increase in maternal mortality in the United States. They reported that the \"rate of life-threatening complications for new mothers in the U.S. has more than doubled in two decades due to pre-existing conditions, medical errors and unequal access to care.\" According to the Centers for Disease Control and Prevention, c. 4 million women who give birth in the US annually, over 50,000 a year, experience \"dangerous and even life-threatening complications.\"\n\nAccording to a report by the United States Centers for Disease Control and Prevention, in 1993 the rate of Severe Maternal Morbidity, rose from 49.5 to 144 \"per 10,000 delivery hospitalizations\" in 2014, an increase of almost 200 percent. Blood transfusions also increased during the same period with \"from 24.5 in 1993 to 122.3 in 2014 and are considered to be the major driver of the increase in SMM. After excluding blood transfusions, the rate of SMM increased by about 20% over time, from 28.6 in 1993 to 35.0 in 2014.\"\n\nThe past 60 years have consistently shown considerable racial disparities in pregnancy-related deaths. Between 2011 and 2014, the mortality ratio for different racial populations based on pregnancy-related deaths were as follows: 12.4 deaths per 100,000 live births for white women, 40,0 for black women, and 17.8 for women of other races. This shows that black women have between three and four times greater chance of dying from pregnancy-related issues. It has also been shown that one of the major contributors to maternal health disparities within the United States is the growing rate of non-communicable diseases.\n\nIt is unclear why pregnancy-related deaths in the United States have increased. It seems that the use of computerized data servers by the states and changes in the way deaths are coded, with a pregnancy checkbox added to death certificates in many states, have been shown to improve the identification of these pregnancy-related deaths. However, this does not contribute to decreasing the actual number of deaths. Also, errors in reporting of pregnancy status have been seen, which most likely leads to overestimation of the number of pregnancy-related deaths. Again, this does not contribute to explaining why the death rate has increased, but does show complications between reporting and actual contributions to the overall rate of maternal mortality.\n\nEven though 99% of births in the United States are attended by some form of skilled health professional, the maternal mortality ratio in 215 was 14 deaths per 100,000 live births and it has been shown that the maternal mortality rate has been increasing. Also, the United States is not as efficient at preventing pregnancy-related deaths when compared to most of the other developed nations.\n\nThe United States took part in the Millennium Development Goals (MDGs) set forth from the United Nations. The MDGs ended in 2015 but were followed-up in the form of the Sustainable Development Goals starting in 2016. The MDGs had several tasks, one of which was to improve maternal mortality rates globally. Despite their participation in this program as well as spending more than any other country on hospital-based maternal care, however, the United States has still seen increased rates of maternal mortality. This increased maternal mortality rate was especially pronounced in relation to other countries who participated in the program, where during the same period, the global maternal mortality rate decreased by 44%. Also, the United States is not currently on track to meet the Healthy People 2020 goal of decreasing maternal mortality by 10% by the year 2020, and continues to fail in meeting national goals in maternal death reduction. Only 23 states have some form of policy that establishes review boards specific to maternal mortality as of the year 2010.\n\nIn an effort to respond to the maternal mortality rate in the United States, the CDC requests that the 52 reporting regions (all states and New York City and Washington DC) to send death certificates for all those women who have died and may fit their definition of a pregnancy-related death, as well as copies of the matching birth or death records for the infant. However, this request is voluntary and some states may not have the ability to abide by this effort.\n\nThe Affordable Care Act (ACA) provided additional access to maternity care by expanding opportunities to obtain health insurance for the uninsured and mandating that certain health benefits have coverage. It also expanded the coverage for women who have private insurance. This expansion allowed them better access to primary and preventative health care services, including for screening and management of chronic diseases. An additional benefit for family planning services was the requirement that most insurance plans cover contraception without cost sharing. However, more employers are able to claim exemptions for religious or moral reasons under the current administration. Also under the current administration, the Department of Health and Human Services (HHS) has decreased funding for pregnancy prevention programs for adolescent girls.\n\nThose women covered under Medicaid are covered when they receive prenatal care, care received during childbirth, and postpartum care. These services are provided to nearly half of the women who give birth in the United States. Currently, Medicaid is required to provide coverage for women whose incomes are at 133% of the federal poverty level in the United States.\n\nThe death rate for women giving birth plummeted in the twentieth century. The historical level of maternal deaths is probably around 1 in 100 births. Mortality rates reached very high levels in maternity institutions in the 1800s, sometimes climbing to 40 percent of patients (see Historical mortality rates of puerperal fever). At the beginning of the 1900s, maternal death rates were around 1 in 100 for live births. Currently, there are an estimated 275,000 maternal deaths each year. Public health, technological and policy approaches are steps that can be taken to drastically reduce the global maternal death burden. For developing regions, where it has been shown that maternal mortality is greater than in developed nations, antenatal care has increased from 65% in 1990 to 83% in 2012.\n\nIt was estimated that in 2015, a total of 303,000 women died due to causes related to pregnancy or childbirth. The majority of these causes were either severe bleeding, sepsis, eclampsia, labor that had some type of obstruction, and consequences from unsafe abortions. All of these causes are either preventable or have highly effective interventions. Another factor that contributes to the maternal mortality rate that have opportunities for prevention are access to prenatal care for women who are pregnant. Women who do not receive prenatal care are between three and four times more likely to die from complications resulting from pregnancy or delivery than those who receive prenatal care. For women in the United States, 25% do not receive the recommended number of prenatal visits, and this number increases for women among specific demographic populations: 32% for African American women and 41% for American Indian and Alaska Native women.\n\nFour elements are essential to maternal death prevention, according to UNFPA. First, prenatal care. It is recommended that expectant mothers receive at least four antenatal visits to check and monitor the health of mother and fetus. Second, skilled birth attendance with emergency backup such as doctors, nurses and midwives who have the skills to manage normal deliveries and recognize the onset of complications. Third, emergency obstetric care to address the major causes of maternal death which are hemorrhage, sepsis, unsafe abortion, hypertensive disorders and obstructed labour. Lastly, postnatal care which is the six weeks following delivery. During this time, bleeding, sepsis and hypertensive disorders can occur, and newborns are extremely vulnerable in the immediate aftermath of birth. Therefore, follow-up visits by a health worker to assess the health of both mother and child in the postnatal period is strongly recommended.\n\nWomen who have unwanted pregnancies who have access to reliable information as well as compassionate counseling and quality services for the management of any issues that arise from abortions (whether safe or unsafe) can be beneficial in reducing the number of maternal deaths. Also, in regions where abortion is not against the law, then abortion practices need to be safe in order to effectively reduce the number of maternal deaths related to abortion.\n\nMaternal Death Surveillance and Response is another strategy that has been used to prevent maternal death. This is one of the interventions proposed to reduce maternal mortality where maternal deaths are continuously reviewed to learn the causes and factors that led to the death. The information from the reviews is used to make recommendations for action to prevent future similar deaths. Maternal and perinatal death reviews have been in practice for a long time worldwide, and the World Health Organization (WHO) introduced the Maternal and Perinatal Death Surveillance and Response (MPDSR) with a guideline in 2013. Studies have shown that acting on recommendations from MPDSR can reduce maternal and perinatal mortality by improving quality of care in the community and health facilities.\n\nThe decline in maternal deaths has been due largely to improved asepsis, fluid management and blood transfusion, and better prenatal care.\n\nTechnologies have been designed for resource poor settings that have been effective in reducing maternal deaths as well. The non-pneumatic anti-shock garment is a low-technology pressure device that decreases blood loss, restores vital signs and helps buy time in delay of women receiving adequate emergency care during obstetric hemorrhage. It has proven to be a valuable resource. Condoms used as uterine tamponades have also been effective in stopping post-partum hemorrhage.\n\nA public health approach to addressing maternal mortality includes gathering information on the scope of the problem, identifying key causes, and implementing interventions, both prior to pregnancy and during pregnancy, to combat those causes.\n\nPublic health has a role to play in the analysis of maternal death. One important aspect in the review of maternal death and its causes are Maternal Mortality Review Committees or Boards. The goal of these review committees are to analyze each maternal death and determine its cause. After this analysis, the information can be combined in order to determine specific interventions that could lead to preventing future maternal deaths. These review boards are generally comprehensive in their analysis of maternal deaths, examining details that include mental health factors, public transportation, chronic illnesses, and substance use disorders. All of this information can be combined to give a detailed picture of what is causing maternal mortality and help to determine recommendations to reduce their impact. \n\nMany states within the US are taking Maternal Mortality Review Committees a step further and are collaborating with various professional organizations to improve quality of perinatal care. These teams of organizations form a \"perinatal quality collaborative,\" or PQC, and include state health departments, the state hospital association and clinical professionals such as doctors and nurses. These PQCs can also involve community health organizations, Medicaid representatives, Maternal Mortality Review Committees and patient advocacy groups. By involving all of these major players within maternal health, the goal is to collaborate and determine opportunities to improve quality of care. Through this collaborative effort, PQCs can aim to make impacts on quality both at the direct patient care level and through larger system devices like policy. It is thought that the institution of PQCs in California was the main contributor to the maternal mortality rate decreasing by 50% in the years following. The PQC developed review guides and quality improvement initiatives aimed at the most preventable and prevalent maternal deaths: those due to bleeding and high blood pressure. Success has also been observed with PQCs in Illinois and Florida.\n\nSeveral interventions prior to pregnancy have been recommended in efforts to reduce maternal mortality. Increasing access to reproductive healthcare services, such as family planning services and safe abortion practices, is recommended in order to prevent unintended pregnancies. Several countries, including India, Brazil, and Mexico, have seen some success in efforts to promote the use of reproductive healthcare services. Other interventions include high quality sex education, which includes pregnancy prevention and sexually-transmitted infection (STI) prevention and treatment. By addressing STIs, this not only reduces perinatal infections, but can also help reduce ectopic pregnancy caused by STIs. Adolescents are between two and five times more likely to suffer from maternal mortality than a female twenty years or older. Access to reproductive services and sex education could make a large impact, specifically on adolescents, who are generally uneducated in regards to carrying a healthy pregnancy. Education level is a strong predictor of maternal health as it gives women the knowledge to seek care when it is needed.\nPublic health efforts can also intervene during pregnancy to improve maternal outcomes. Areas for intervention have been identified in access to care, public knowledge about signs and symptoms of pregnancy complications, and improving relationships between healthcare professionals and expecting mothers.\n\nAccess to care during pregnancy is a significant issue in the face of maternal mortality. \"Access\" encompasses a wide range of potential difficulties including costs, location of healthcare services, availability of appointments, transportation services, and cultural or language barriers that could inhibit a woman from receiving proper care. For women carrying a pregnancy to term, access to necessary antenatal (prior to delivery) healthcare visits is crucial to ensuring healthy outcomes. These antenatal visits allow for early recognition and treatment of complications, treatment of infections and the opportunity to educate the expecting mother on how to manage her current pregnancy and the health advantages of spacing pregnancies apart. Access to birth at a facility with a skilled healthcare provider present has been associated with safer deliveries and better outcomes. The two areas bearing the largest burden of maternal mortality, Sub-Saharan Africa and South Asia, also had the lowest percentage of births attended by a skilled provider, at just 45% and 41% respectively. Emergency obstetric care is also crucial in preventing maternal mortality by offering services like emergency cesarean sections, blood transfusions, antibiotics for infections and assisted vaginal delivery with forceps or vacuum. In addition to physical barriers that restrict access to healthcare, financial barriers also exist. Close to one out of seven women of child-bearing age have no health insurance. This lack of insurance impacts access to pregnancy prevention, treatment of complications, as well as perinatal care visits. \n\nBy increasing public knowledge about pregnancy, including signs of complications that need addressed by a healthcare provider, this will increase the likelihood of an expecting mother to seek help when it is necessary. Higher levels of education have been associated with increased use of contraception and family planning services as well as antenatal care. Addressing complications at the earliest sign of a problem can improve outcomes for expecting mothers, which makes it extremely important for a pregnant woman to be knowledgeable enough to seek healthcare for potential complications. Improving the relationships between patients and the healthcare system as a whole will make it easier for a pregnant woman to feel comfortable seeking help. Good communication between patients and providers, as well as cultural competence of the providers, could also assist in increasing compliance with recommended treatments.\n\nThe biggest global policy initiative for maternal health came from the United Nations' Millennium Declaration which created the Millennium Development Goals. In 2012, this evolved at the United Nations Conference on Sustainable Development to become the Sustainable Development Goals (SDGs) with a target year of 2030. The SDGs are 17 goals that call for global collaboration to tackle a wide variety of recognized problems. Goal 3 is focused on ensuring health and well-being for people of all ages. A specific target is to achieve a global maternal mortality ratio of less than 70 per 100,000 live births. So far, specific progress has been made in births attended by a skilled provider, now at 80% of births worldwide compared with 62% in 2005. \n\nCountries and local governments have taken political steps in reducing maternal deaths. Researchers at the Overseas Development Institute studied maternal health systems in four apparently similar countries: Rwanda, Malawi, Niger, and Uganda. In comparison to the other three countries, Rwanda has an excellent recent record of improving maternal death rates. Based on their investigation of these varying country case studies, the researchers conclude that improving maternal health depends on three key factors: 1. reviewing all maternal health-related policies frequently to ensure that they are internally coherent; 2. enforcing standards on providers of maternal health services; 3. any local solutions to problems discovered should be promoted, not discouraged.\n\nIn terms of aid policy, proportionally, aid given to improve maternal mortality rates has shrunken as other public health issues, such as HIV/AIDS, have become major international concerns. Maternal health aid contributions tend to be lumped together with newborn and child health, so it is difficult to assess how much aid is given directly to maternal health to help lower the rates of maternal mortality. Regardless, there has been progress in reducing maternal mortality rates internationally.\n\nIn countries where abortion practices are not considered legal, it is necessary to look at the access that women have to high-quality family planning services, since some of the restrictive policies around abortion could impede access to these services. These policies may also affect the proper collection of information for monitoring maternal health around the world.\n\nMaternal deaths and disabilities are leading contributors in women's disease burden with an estimated 275,000 women killed each year in childbirth and pregnancy worldwide. In 2011, there were approximately 273,500 maternal deaths (uncertainty range, 256,300 to 291,700). Forty-five percent of postpartum deaths occur within 24 hours. Ninety-nine percent of maternal deaths occur in developing countries.\n\n\n"}
{"id": "51267644", "url": "https://en.wikipedia.org/wiki?curid=51267644", "title": "Meal", "text": "Meal\n\nA meal is an eating occasion that takes place at a certain time and includes prepared food. The names used for specific meals in English vary greatly, depending on the speaker's culture, the time of day, or the size of the meal.\n\nMeals occur primarily at homes, restaurants, and cafeterias, but may occur anywhere. Regular meals occur on a daily basis, typically several times a day. Special meals are usually held in conjunction with such occasions as birthdays, weddings, anniversaries, events and holidays. A meal is different from a snack in that meals are generally larger, more varied, and more filling than snacks.\n\nThe type of meal served or eaten at any given time varies by custom and location. In most modern cultures, three main meals are eaten: in the morning, early afternoon, and evening. Further, the names of meals are often interchangeable by custom as well. Some serve dinner as the main meal at midday, with supper as the late afternoon/early evening meal; while others may call their midday meal lunch and their early evening meal supper. Except for \"breakfast\", these names can vary from region to region or even from family to family.\n\nA study in 2016 by Toluna found that 47% of parents in the United States share fewer meals with their families than when growing up, and 58% wished they could do it more frequently, including 66% of dads.\n\nBreakfast is the first meal of a day, most often eaten in the early morning before undertaking the day's work. Some believe it to be the most important meal of the day. The word \"breakfast\" literally refers to breaking the fasting period of the prior night.\n\nBreakfast foods vary widely from place to place, but often include a carbohydrate such as grains or cereals, fruit, vegetables, a protein food such as eggs, meat or fish, and a beverage such as tea, coffee, milk, or fruit juice. Coffee, milk, tea, juice, breakfast cereals, pancakes, waffles, sausages, French toast, bacon, , fresh fruits, vegetables, eggs, baked beans, muffins, crumpets and toast with butter, margarine, jam or marmalade are common examples of Western breakfast foods, though a large range of preparations and ingredients are associated with breakfast globally.\n\nA full breakfast is a breakfast meal, usually including bacon, sausages, eggs, and a variety of other cooked foods, with a beverage such as coffee or tea. It is especially popular in the UK and Ireland, to the extent that many cafés and pubs offer the meal at any time of day as an \"all-day breakfast\". It is also popular in other English-speaking countries.\n\nIn England it is usually referred to as a 'full English breakfast' (often shortened to 'full English') or 'fry-up'. Other regional names and variants include the 'full Scottish', 'full Welsh', 'full Irish' and the 'Ulster fry'.\n\nThe full breakfast is among the most internationally recognised British dishes, along with such staples as bangers & mash, shepherd's pie, fish and chips and the Christmas dinner. The full breakfast became popular in the British Isles during the Victorian era, and appeared as one among many suggested breakfasts in the home economist Isabella Beeton's \"The Book of Household Management\" (1861). A full breakfast is often contrasted (e.g. on hotel menus) with the lighter alternative of a Continental breakfast, traditionally consisting of tea, milk or coffee and fruit juices with bread, croissants, or pastries.\n\n\"Instant breakfast\" typically refers to breakfast food products that are manufactured in a powdered form, which are generally prepared with the addition of milk and then consumed as a beverage. Some instant breakfasts are produced and marketed in liquid form, being pre-mixed. The target market for instant breakfast products includes consumers who tend to be busy, such as working adults.\n\nA champagne breakfast is a breakfast served with champagne or sparkling wine. It is a new concept in some countries and is not typical of the role of a breakfast.\n\nIt may be part of any day or outing considered particularly luxurious or indulgent. The accompanying breakfast is sometimes of a similarly high standard and include rich foods such as salmon, caviar, chocolate or pastries, which would not ordinarily be eaten at breakfast or more courses. \nInstead of as a formal meal the breakfast can be given to the recipient in a basket or hamper.\n\nLunch, the abbreviation for \"luncheon\", is a light meal typically eaten at midday. The origin of the words \"lunch\" and \"luncheon\" relate to a small snack originally eaten at any time of the day or night. During the 20th century the meaning gradually narrowed to a small or mid-sized meal eaten at midday. Lunch is commonly the second meal of the day after breakfast. The meal varies in size depending on the culture, and significant variations exist in different areas of the world.\n\nA packed lunch (also called pack lunch, sack lunch or bag lunch in North America, or packed lunch in the United Kingdom, as well as the regional variations: bagging in Lancashire, Merseyside and Yorkshire,) is a lunch prepared at home and carried to be eaten somewhere else, such as school, a workplace, or at an outing. The food is usually wrapped in plastic, aluminum foil, or paper and can be carried (\"packed\") in a lunch box, paper bag (a \"sack\"), or plastic bag. While packed lunches are usually taken from home by the people who are going to eat them, in Mumbai, India, tiffin boxes are most often picked up from the home and brought to workplaces later in the day by so-called dabbawallas. It is also possible to buy packed lunches from stores in several countries. Lunch boxes made out of metal, plastic or vinyl are now popular with today's youth. Lunch boxes provide a way to take heavier lunches in a sturdier box or bag. It is also environmentally friendly.\n\nDinner usually refers to the most significant and important meal of the day, which can be the noon or the evening meal. However, the term \"dinner\" can have many different meanings depending on the culture; it may mean a meal of any size eaten at any time of the day. Historically, it referred to the first meal of the day, eaten around noon, and is still sometimes used for a noon-time meal, particularly if it is a large or main meal. The meaning as the evening meal, generally the largest of the day, is becoming a standard in many parts of the English-speaking world.\n\nA full course dinner is a dinner consisting of multiple dishes, or courses. In its simplest form, it can consist of three to five courses, such as appetizers, fish course, entrée, main course and dessert.\n\nMeal preparation, sometimes called \"meal prep,\" is the process of planning and preparing meals. It generally involves food preparation, including cooking.\n\nPreparing food for eating generally requires selection, measurement and combination of ingredients in an ordered procedure so as to achieve desired results. Food preparation includes but is not limited to cooking.\n\nCooking or \"cookery\" is the art, technology and craft of preparing food for consumption with the use of heat. Cooking techniques and ingredients vary widely across the world, from grilling food over an open fire to using electric stoves, to baking in various types of ovens, reflecting unique environmental, economic, and cultural traditions and trends. The ways or types of cooking also depend on the skill and type of training an individual cook has. Cooking is done both by people in their own dwellings and by professional cooks and chefs in restaurants and other food establishments. Cooking can also occur through chemical reactions without the presence of heat, most notably with ceviche, a traditional South American dish where fish is cooked with the acids in lemon or lime juice.\n\n\n\n"}
{"id": "25572706", "url": "https://en.wikipedia.org/wiki?curid=25572706", "title": "Miracle Mineral Supplement", "text": "Miracle Mineral Supplement\n\nMiracle Mineral Supplement, often referred to as Miracle Mineral Solution, Master Mineral Solution, MMS or the CD protocol, is chlorine dioxide, an industrial bleach. It is made by mixing 28 percent sodium chlorite solution with an acid such as citrus juice. This mixture produces chlorine dioxide, a potent industrial bleach and industrial water cleaner. High oral doses of this bleach, such as those recommended in the labeling, can cause nausea, vomiting, diarrhea, symptoms of severe dehydration and other life threatening conditions. The name was coined by former Scientologist in his 2006 self-published book, \"The Miracle Mineral Solution of the 21st Century\". A more dilute version is marketed as Chlorine Dioxide Solution (CDS).\n\nMMS is falsely promoted as a cure for HIV, malaria, hepatitis viruses, the H1N1 flu virus, common colds, autism, acne, cancer, and much more. There have been no clinical trials to test these claims, which come only from anecdotal reports and Humble's book. In January 2010, the \"Sydney Morning Herald\" reported that one vendor admitted that they do not repeat any of Humble's claims in writing to circumvent regulations against using it as a medicine. Sellers sometimes describe MMS as a water purifier so as to circumvent medical regulations. The International Federation of Red Cross and Red Crescent Societies rejected \"in the strongest terms\" reports by promoters of MMS that they had used the product to fight malaria.\n\nSodium chlorite, the main constituent of MMS, is a toxic chemical that can cause acute renal failure if ingested. Small amounts of about 1 gram can be expected to cause nausea, vomiting, shedding of internal mucous membranes such as those of the small and large intestine and even life-threatening hemolysis in persons who are deficient in glucose-6-phosphate dehydrogenase. When citric acid or other food acid is used to \"activate\" MMS as described in its instructions, the mixture produces an aqueous solution containing chlorine dioxide, a toxin and a potent oxidizing agent used in the treatment of water and in bleaching. The United States Environmental Protection Agency has set a maximum level of 0.8 mg/L for chlorine dioxide in drinking water. Naren Gunja, director of the New South Wales Poisons Information Centre, has stated that using the product is \"a bit like drinking concentrated bleach\" and that users have displayed symptoms consistent with corrosive injuries, such as vomiting, stomach pains, and diarrhea.\n\n\"The Guardian\" has described MMS as \"extremely nasty stuff, and the medical advice given is that anyone who has this product should stop using it immediately and throw it away. In Canada it was banned after causing a life-threatening reaction.\" In August 2009, a Mexican woman travelling with her American husband on their yacht in Vanuatu took MMS as a preventative for malaria. Within 15 minutes she was ill, and within twelve hours she was dead. The island nation's public prosecutor, Kayleen Tavoa, did not press any charges as there were no specific laws banning the importation of MMS, but advised, \"While every case is assessed on its own merits, I advise that any person who misuses MMS in Vanuatu in the future would be likely to face prosecution for potentially serious criminal offences. No person should ever give MMS to another person to drink without advising them of what it is they are drinking and of the serious risks to health that may arise if they decide to drink the mixture.\"\n\nIn 2008, a 60-year-old Canadian man was hospitalized after a life-threatening response to MMS. Following a May 2010 advisory which indicated that MMS exceeds tolerable levels of sodium chlorite by a factor of 200, a Calgary-based supplier briefly stopped distribution. A February 2012 warning, which resulted in one website shutting down, advised: \"There are no therapeutic products containing sodium chlorite authorized for oral consumption by humans in Canada.\" In the UK, the Food Standards Agency has also released a warning, following the initial warning from Health Canada and a similar warning from the U.S. Food and Drug Administration, in which they stated that \"MMS is a 28% sodium chlorite solution which is equivalent to industrial-strength bleach. When taken as directed it could cause severe nausea, vomiting and diarrhea, potentially leading to dehydration and reduced blood pressure. If the solution is diluted less than instructed, it could cause damage to the gut and red blood cells, potentially resulting in respiratory failure.\" More dilute versions have potential to do harm, although it is less likely. The Food Standards Agency has since reiterated their warning on MMS and extended it to include CDS.\n\nSellers attribute the vomiting, nausea, and diarrhea to the product working, but it is simply the product's toxicity.\n\nIn December 2009, an alert was issued by the Belgian Poison Control Centre to the European Association of Poisons Centres and Clinical Toxicologists. In response, an evaluation was performed by the French \"Comité de coordination de toxicovigilance\" in March 2010, warning about a dose dependent irritation and possible toxic effects. They also warned that patients affected by serious diseases could be tempted to stop their treatments in favour of this alternative treatment.\nA similar notice was released in July 2010 by the U.S. Food and Drug Administration warning that the instructions for preparing the solution by mixing it with an acidic solution, or even orange juice, would produce chlorine dioxide, \"a potent bleach used for stripping textiles and industrial water treatment.\" Because of reports including nausea, vomiting, and dangerously low blood pressure as a result of dehydration following instructed use, the FDA has advised consumers to dispose of the product immediately.\n\nMMS is not approved for the treatment of any disease and according to the United States Environmental Protection Agency, chronic exposure to small doses of chlorine dioxide could cause reproductive and neurodevelopmental damage. While studies of chlorine dioxide effects in humans are rare, studies on animal subjects are more common; chlorine dioxide has been shown to impair thyroid function and reduce CD4 helper T cell count in grivet monkeys after 6 months. Another study in rats resulted in reduced red blood cells count when exposed to 100 mg/L of chlorine dioxide concentration in their drinking water, after 3 months. The United States Department of Labor restricts occupational exposure through inhalation of chlorine dioxide to 0.1 ppm since concentrations at 10 ppm resulted in deaths in rats, after 10 days while a case where a worker was accidentally exposed to 19 ppm resulted in death. According to the same organisation, \"chlorine dioxide is a severe respiratory and eye irritant in humans\".\n\nIn 2015, BBC London conducted an undercover investigation into MMS, with a reporter posing as a family member of a person with autism. The BBC reporter was sold two bottles containing sodium chlorite and hydrochloric acid by a self-styled \"reverend\" named Leon Edwards linked to the Genesis II Church in the United States. Edwards told the reporter that the solutions would cure nearly all illnesses and conditions, including cancer, HIV, malaria, autism and Alzheimer's. He recommended 27 drops per day for a baby. Laboratory analysis later showed that the concentration of both bottled solutions was stronger than advertised. Edwards told the reporter:\n\nEyewitness News and ABC News investigated the MMS phenomenon in 2016, and uncovered an \"underground network\" centred around southern California which was promoting the substance as a cure for conditions including cancer, Parkinson's and childhood autism.\n\nIt was reported in January 2018 that at least six police forces have investigated the use of MMS in the UK, where it continues to be available. Spokespersons for the UK's Medicines and Healthcare products Regulatory Agency and the Food Standards Agency have repeatedly warned of the dangers of using such a product.\n\nFormer Chicagoan Kerri Rivera, who now resides in Mexico, was required by the Illinois Attorney General to sign a document stating that she would no longer promote the use of toxic chlorine dioxide, or \"CD,\" in the state of Illinois. The agreement, which Rivera signed, says, \"Respondent [Rivera] makes unsubstantiated health and medical claims regarding the use of chlorine dioxide in the treatment of autism. In truth and fact, Respondent lacks competent and reliable scientific evidence to support her claims that chlorine dioxide can treat autism. Respondent's act of promoting unsubstantiated health and medical claims regarding the use of chlorine dioxide in the treatment of autism constitutes a violation of Section 2 of the Consumer Fraud Act.\" The agreement continued to bar Rivera from speaking at seminars and selling chlorine dioxide or similar substances for the treatment of autism. However, Rivera operates a Facebook page and website that promote injecting autistic children with a toxic chlorine dioxide formula via enema, and claims that the intestinal lining and membranes children expel as a result are parasites, which is patently false. Then-Attorney General Lisa Madigan described the case by saying, \"You have a situation where there are people, complete quacks, that are out there promoting a very dangerous chemical being given to young children... Ingesting what amounts to a toxic chemical - bleach - is not going to cure your child.\" Rivera advocates treating infants and toddlers, as well as older children, with chlorine dioxide enemas, requiring that children also drink the solution and bathe in it.\n\nMMS was a cure touted by an Australian couple targeting the Seattle area. They ran websites using fake testimonials, photographs, and Seattle addresses, to promote downloadable books touted as containing secret cures as well as selling bottles labeled \"water purification drops\" with a brand name of \"MMS Professional\". The Washington State Attorney General's Office filed suit, and in conjunction with the Australian Competition and Consumer Commission (ACCC), secured a settlement of more than US$40,000, roughly $25,000 for state legal fees and $14,000 to be divided among 200 consumers. In the ACCC legal action, the presiding judge described the cures as quack medicine and found the claims on the websites \"false, misleading or deceptive\".\n\nA woman from the city of Mackay in Australia, without qualifications to practice, charged up to A$2,000 to inject patients with MMS in her garage which lacked proper facilities for sterilization, and went as far as advising a person to avoid chemotherapy while \"dishonestly promoting its benefits with no scientific basis for her claims\". The Queensland Office of Fair Trading handed down a court order prohibiting her from \"making any claims she is able to treat, cure, or benefit any person suffering from cancer\" and she was charged court costs of A$12,000.\n\nOn May 28, 2015, a US federal jury found Louis Daniel Smith guilty of conspiracy, smuggling, selling misbranded drugs and defrauding the United States in relation to the sale of MMS. According to the evidence presented at trial, Smith created phony “water purification” and “wastewater treatment” businesses in order to obtain sodium chlorite and ship Miracle Mineral Supplement without being detected by the government . On October 28, 2015 Smith was sentenced to serve four years and three months in federal prison to be followed by three years of supervised release . Despite Smith's sentence the Genesis II Church of Health & Healing continue to promote the sale and use of MMS in many countries including the US. In a Washington Post article Floyd Jerred, a bishop in the Genesis II Church of Health & Healing was quoted as saying of MMS “As long as I’m just telling you about it, it’s just education,” and of Smith's conviction “And if they do lock me up, I know how to do out-of-body travel. I can go anywhere, see anything I want to see anyway.” . \n\n"}
{"id": "25841464", "url": "https://en.wikipedia.org/wiki?curid=25841464", "title": "Nutrition International (Organization)", "text": "Nutrition International (Organization)\n\nNutrition International, formerly the Micronutrient Initiative (MI), is an international not for profit agency based in Canada that works to eliminate vitamin and mineral deficiencies in developing countries. Although often only required by the body in very small amounts, vitamin and minerals – also known as micronutrients – support an array of critical biological functions including growth, immune function and eye function, as well as foetal development of the brain, the nervous system, and the skeletal system. Micronutrient deficiency is a form of malnutrition and is a recognized health problem in many developing countries. Globally, more than two billion people live with vitamin and mineral deficiencies.\n\nIn 1990, leaders attending the World Summit for Children set the goal of virtually eliminating micronutrient deficiencies. In 1992, \"Micronutrient Initiative\" was established as a secretariat within the International Development Research Centre (IDRC), based in Ottawa, Ontario, Canada, to support progress toward that goal. In 1993 IDRC recruited an executive director for the secretariat and transferred 2–3 of its staff to the new organization. Venkatesh Mannar took over in June 1994 as the executive director. Until 2000, MI was governed by a steering committee composed of the Canadian International Development Agency (CIDA), the International Development Research Centre, UNICEF, The World Bank, and USAID. In 2000, MI became an independent not-for-profit organization. Joel Spicer became President in February 2014. In 2017, the organization formally rebranded to Nutrition International to more accurately reflect their expanding scope, reach and impact.\n\nThe organization advocates for, and provides funding and technical assistance for salt iodisation, the distribution of multi-micronutrient powders, the fortification of staple foods such as wheat flour with vitamin A, iron and folic acid, and dietary supplementation with vitamin A, iron, zinc and folic acid. Since 1997, with funding support from the Government of Canada, Nutrition International has provided more than eight billion doses of vitamin A for use by UNICEF and national governments.\n\nNutrition International is credited with playing a pioneering role in engaging corporations and trade associations at both the global and national level in partnerships to improve nutrition.\n\nThe staff of Nutrition International includes scientists, nutritionists, and policy and development experts. In addition to its headquarters in Ottawa, the organization has offices in Ethiopia, Kenya, Nigeria, Senegal, Afghanistan, Bangladesh, India, Indonesia, the Philippines and Pakistan.\n\n\n"}
{"id": "32304375", "url": "https://en.wikipedia.org/wiki?curid=32304375", "title": "Partner notification", "text": "Partner notification\n\nPartner notification is the practice of notifying the sexual partners of a person, known as the \"index case\", who has been newly diagnosed with a sexually transmitted infection that they may have been exposed to the infection. It is a kind of contact tracing and is considered a partner service.\n\nIn the UK, partner notification has not played a significant part in HIV prevention and testing efforts despite proving to be highly effective in diagnosing people with HIV. Some audits show up to 37% of partners traced and tested through partner notification were newly diagnosed HIV positive as a result. Notifying sexual partners of people diagnosed with HIV is usually voluntary but a healthcare worker may decide to contact partners without consent if they feel there is a risk of HIV transmission. This is permitted under General Medical Council guidance. \n\nA 2002 survey in the United States showed that with regards to STIs, healthcare providers conduct screenings with less frequency than recommended by health department guidelines. Furthermore, when a person was found to have a sexually transmitted infection, it was much more common for the physician to ask the person to notify their partners rather than for the physician to arrange for this to be done on behalf of the patient.\n\nA study in San Francisco conducted between 2004–6 showed that many new diagnoses of HIV infection were made as a result of one person getting HIV tested, and then contacting that person's partner if the test was positive. It was sometimes the case that a positive person also had a positive partner. Patients rarely refused partner services, and their partners rarely refused to present themselves for healthcare when asked by the clinic.\n\nIn the past, the CDC has recommended that physicians and health departments should inform the sexual partners of people known to be infected with HIV of their risk for contracting the disease when the diagnosed individual refused to do so. However, the CDC currently recommends that all partner services, including partner notification, should be \"voluntary and non-coercive\" for both the individual initially diagnosed with the STD and his or her sexual partners. Despite the change, as of 1998, several states in the US have laws that codify involuntary or coercive partner notification. In Michigan and Indiana, individuals who test positive for HIV are legally obligated to notify past sexual contacts. In Texas and other states, doctors and public health officials are either required or authorized to inform known sexual contacts of patients who have tested positive for HIV.\n\nIn New South Wales, Australia, if a health worker believes that the behavior of the index case poses a risk to the health of the public, then the worker is obligated to inform the Director-General of NSW health, who has the sole authority to inform partners of the index case without his or her consent.\n"}
{"id": "28280653", "url": "https://en.wikipedia.org/wiki?curid=28280653", "title": "Paul Grundy", "text": "Paul Grundy\n\nPaul Grundy MD, MPH, FACOEM, FACPM, known as the \"Godfather\" of the Patient Centered Medical Home, member of the Institute of Medicine and recipient of the prestigious Barbara Starfield Primary Care Leadership Award in 2016 and the 2012 National Committee for Quality Assurance(NCQA) Quality Award, is IBM's Global Director of Healthcare Transformation.\n\nIn this role, Dr. Grundy develops and executes strategies that support IBM’s healthcare-industry transformation initiatives. Part of his work is directed towards shifting healthcare delivery around the world towards consumer-focused, primary-care based systems through the adoption of new philosophies, primary-care pilot programs, new incentives systems, and the information technology required to implement such change. He is one of only 38 IBMers and the only physician selected into IBM’s senior \nindustry leadership forum known as the IBM Industry Academy.\n\nDr. Grundy is also the Founding President of the Patient Centered Primary Care Collaborative and is an Adjunct Professor, University of Utah School of Medicine, Department of Family and Preventive Medicine.\n\nAn active social entrepreneur and speaker on global healthcare transformation, Dr. Grundy is driving comprehensive, linked, and integrated healthcare and the concept of the Patient Centered Medical Home. His work has been reported widely in the New York Times, BusinessWeek, Forbes, the Economist, the Huffington Post, New England Journal of Medicine and newspapers, radio and television around the country. Dr. Grundy is also invited frequently as a thought leader to conferences such as TED conference, Smarter healthcare by smarter use of data, or NHS Confederation conference, Foundation for Healthcare Transformation.\n\nDr. Grundy is also a Co-Author of the book \"Lost and Found: A Consumer's Guide to Healthcare\" , written together with Dr. Peter B. Anderson.\n\nPaul Grundy spent his early life in West Africa, the son of Quaker missionaries. He attended medical school at the University of California San Francisco and earned his Master's Degree in Public Health at the University of California Berkeley. Dr. Grundy performed his Residency training at Johns Hopkins in Preventive Medicine. He also completed a post doctoral fellowship at Johns Hopkins in Occupational Health in the International Environment.\n\nFrom 1997 to 2000, he was the Corporate Occupational Medical Director for International SOS, the largest medical service company in the world, providing and coordinating care and medical assistance for multinational corporations.\n\nFrom 1994 to 1997, he was the Medical Director of Adventist Health Systems, Pennsylvania, the second largest not-for-profit medical system operating in the state of Pennsylvania, and the Medical Director for the largest occupational medicine program in Berks County, Pennsylvania.\n\nFrom 1985 to 1994 he was a Regional Medical Officer and Counselor of Embassy for Medical Affairs, U.S. Department of State. In this role, Dr. Grundy was responsible for leading the interactions between health and diplomacy, successfully organizing such activities as the Clinton/Yeltsin health initiative, a $157 million bilateral initiative in Russia. In his role with the Department of State, he was responsible for advising United States Ambassadors on health-care programs for diplomatic posts. He set up the first U.S. policy and program addressing the HIV/AIDS epidemic in Africa. He worked to organize a Congressional fact-finding mission on the extent of the HIV/AIDS problem and drafted the first bill in congress dealing with the HIV/AIDS epidemic in Africa. Dr. Grundy finished his career in the Department of State as a Minister Counselor, the highest rank for a doctor and the equivalent of a three-star flag rank in the military.\n\nFrom 1979 to 1985 Dr. Grundy was a medical officer in the U.S. Air Force, where he taught at the School of Aerospace Medicine. He also served as a flight surgeon and Chief of Hospital Services in Korea.\n\nPrior to joining IBM, Dr Grundy worked as a senior diplomat in the US State Department supporting the intersection of health and diplomacy. He was also a Medical Director for the International SOS, the world's largest medical assistance company and for Adventist Health Systems, the second-largest not-for-profit medical system in the world.\n\nDr. Grundy attended medical school at the University of California San Francisco and trained at Johns Hopkins University. He has worked extensively in International AIDS Pandemic, including writing the United States' first piece of legislation addressing AIDS Education in Africa.\n\nDr Grundy serves as a Director of the Accreditation Council for Graduate Medical Education (ACGME), the body responsible for accrediting graduate medical training programs, Member National Advisory Board of the National Center for Interprofessional Education and the Medical Education Futures Study. Paul is the President of the Patient Centered Primary Care Collaborative (PCPCC), a coalition he led IBM in creating in early 2006. The PCPCC is dedicated to advancing a new primary-care model called the Patient Centered Medical Home as a means of fundamentally reforming healthcare delivery, which in turn is essential to maintaining US international competitiveness.Today, the PCPCC represents employers of some 50 million people across the United States as well as physician groups representing more than 330,000 medical doctors, leading consumer groups and the top seven US health-benefits companies. Dr. Paul Grundy is regarded as the \"godfather\" of the Patient Centered Medical Home (PCMH) concept because of his advocacy for PCMH-level care and in creation of the PCPCC. More information about his amazing advocacy for medical home can be found in the book \"The Familiar Physician: Saving Your Doctor In the Era of Obamacare\"\n\nDoctor Grundy has won numerous awards including the 2016 Barbara Starfield Primary Care Leadership Award for his life's work and dedication to the medical home model of care, Department of State Superior Honor Award for handling the crisis surrounding the two attempted coups in Russia, Department of State Superior Honor Award for work done in opening up all the new embassies after the fall of the Soviet Union, and Department of State Superior Honor Award for work on the HIV/AIDS epidemic in Africa. He also won four Department of State Meritorious Service awards for outstanding performance in the Middle East and Africa, and the Tim Ferriss Globetrotter Award 2010 from VT. He received the Defense Superior Service Award for outstanding service addressing HIV/AIDS and The Defense Meritorious Service Medal.\n\nDr. Paul Grundy retired with the rank of Minister-Counselor from the Foreign Services, U.S. State Department.\n\nDr. Paul Grundy has been named an Ambassador for Healthcare DENMARK in October 2014, a role in which he will share best practices from the Danish Healthcare system with doctors in the United States and in other parts of the world.\n"}
{"id": "58937837", "url": "https://en.wikipedia.org/wiki?curid=58937837", "title": "Peter Attia", "text": "Peter Attia\n\nPeter Attia is a Canadian-American physician known for his medical practice that focuses on the science of longevity.\n\nAfter medical school, Attia spent five years at the Johns Hopkins Hospital in Baltimore, Maryland as a general surgery resident.He spent two years at the National Cancer Institute (NCI) at National Institutes of Health (NIH), in Bethesda, Maryland as a Surgical Oncology Fellow under Dr. Steve Rosenberg.As his residency drew to a close, Attia joined the consulting firm McKinsey & Company in the Palo Alto office as a Member of the \"Corporate Risk Practice and Healthcare Practice\".Attia co-founded and served as President of Nutrition Science Initiative (NuSI) with Gary Taubes in 2012.\nIn 2014, Attia founded \"Attia Medical, PC\", a medical practice focusing on the applied science of longevity and optimal performance.\nAttia is the host of the podcast \"The Peter Attia Drive\". He was one of the speakers at TEDMED 2013.\n\n"}
{"id": "149973", "url": "https://en.wikipedia.org/wiki?curid=149973", "title": "Procrastination", "text": "Procrastination\n\nProcrastination is defined as the avoidance of doing a task that needs to be accomplished. It could be further stated as a habitual/intentional delay of starting or finishing a task despite its negative consequences. It is a common human experience involving delay in everyday chores or even putting off salient tasks such as attending an appointment, submitting a job report or academic assignment, or broaching a stressful issue with a partner. Although typically perceived as a negative trait due to its hindering effect on one's productivity often associated with depression, low self-esteem, guilt and inadequacy , it can also be considered a wise response to certain demands that could present risky or negative outcomes or require waiting for new information to arrive. Various types of procrastination (such as academic/non academic or behavioural/ indecisive) have their own underlying causes and effects. The most prominent explanation in present literature draws upon \"Intemporal discounting, task averseness and certain personality traits such as indecisiveness and distractibility\" as the common causes of procrastination.\n\nA study of behavioural patterns of pigeons through delayed reward suggests that procrastination is not unique to humans, but can also be observed in some animals.\n\nLatin: \"procrastinare\", \"pro-\", 'forward', with \"-crastinus\", 'till next day' from \"cras\", 'tomorrow'\n\nIn a study of academic procrastination from the University of Vermont, published in 1984, 46% of the subjects reported that they \"always\" or \"nearly always\" procrastinate writing papers, while approximately 30% reported procrastinating studying for exams and reading weekly assignments (by 28% and 30% respectively). Nearly a quarter of the subjects reported that procrastination was a problem for them regarding the same tasks. However, as many as 65% indicated that they would like to reduce their procrastination when writing papers, and approximately 62% indicated the same for studying for exams and 55% for reading weekly assignments.\n\nA 1992 study showed that \"52% of surveyed students indicated having a moderate to high need for help concerning procrastination.\" It is estimated that 80–95% of college students engage in procrastination, and approximately 75% consider themselves procrastinators.\n\nIn a study performed on university students, procrastination was shown to be greater on tasks that were perceived as unpleasant or as impositions than on tasks for which the student believed they lacked the required skills for accomplishing the task.\n\nGregory Schraw, Theresa Wadkins, and Lori Olafson in 2007 proposed three criteria for a behavior to be classified as academic procrastination: it must be counterproductive, needless, and delaying. Steel reviewed all previous attempts to define procrastination, and concluded in a 2007 study that procrastination is \"to voluntarily delay an intended course of action despite expecting to be worse off for the delay.\" Sabini & Silver argued that postponement and irrationality are the two key features of procrastination. Delaying a task is not deemed as procrastination, they argue, if there are rational reasons behind the delay.\n\nAn approach that integrates several core theories of motivation as well as meta-analytic research on procrastination is the temporal motivation theory. It summarizes key predictors of procrastination (expectancy, value, and impulsiveness) into a mathematical equation.\n\nThe pleasure principle may be responsible for procrastination; one may prefer to avoid negative emotions by delaying stressful tasks. As the deadline for their target of procrastination grows closer, they are more stressed and may, thus, decide to procrastinate more to avoid this stress. Some psychologists cite such behavior as a mechanism for coping with the anxiety associated with starting or completing any task or decision.\nPiers Steel indicated in 2010 that anxiety is just as likely to induce people to start working early as late, and that the focus of studies on procrastination should be impulsiveness. That is, anxiety will cause people to delay only if they are impulsive.\n\nNegative coping responses of procrastination tend to be avoidant or emotional rather than task-oriented or focused on problem-solving. Emotional and avoidant coping is employed to reduce stress (and cognitive dissonance) associated with delaying intended and important personal goals. This option provides immediate pleasure and is consequently very attractive to impulsive procrastinators, at the point of discovery of the achievable goals at hand. There are several emotion-oriented strategies, similar to Freudian defense mechanisms, coping styles and self-handicapping. \nCoping responses of procrastinators include the following.\n\n\nTask- or problem-solving measures are taxing from a procrastinator's outlook. If such measures are pursued, it is less likely the procrastinator would remain a procrastinator. However, pursuing such measures requires actively changing one's behavior or situation to prevent and minimize the re-occurrence of procrastination.\n\nIn 2006, it was suggested that neuroticism has no direct links to procrastination and that any relationship is fully mediated by conscientiousness.\nIn 1982, it had been suggested that irrationality was an inherent feature of procrastination. \"Putting things off even until the last moment isn't procrastination if there is a reason to believe that they will take only that moment\". Steel \"et al.\" explained in 2001, \"actions must be postponed and this postponement must represent poor, inadequate, or inefficient planning\".\n\nTo a certain degree it is normal to procrastinate and it can be regarded as a useful way to prioritize between tasks, due to a lower tendency of procrastination on truly valued tasks (for most people). On the other hand, excessive procrastination can become a problem and impede normal functioning. When this happens, procrastination has been found to result in health problems, stress, anxiety, sense of guilt and crisis as well as loss of personal productivity and social disapproval for not meeting responsibilities or commitments. Together these feelings may promote further procrastination and for some individuals procrastination becomes almost chronic. Such procrastinators may have difficulties seeking support due to procrastination itself, but also social stigma and the belief that task-aversion is caused by laziness, lack of willpower or low ambition. In some cases problematic procrastination might be a sign of some underlying psychological disorder, but not necessarily.\n\nResearch on the physiological roots of procrastination have been concerned with the role of the prefrontal cortex, the area of the brain that is responsible for executive brain functions such as impulse control, attention and planning. This is consistent with the notion that procrastination is strongly related to such functions, or a lack thereof. The prefrontal cortex also acts as a filter, decreasing distracting stimuli from other brain regions. Damage or low activation in this area can reduce one's ability to avert diversions, which results in poorer organization, a loss of attention, and increased procrastination. This is similar to the prefrontal lobe's role in attention-deficit hyperactivity disorder, where it is commonly underactivated.\n\nIn a 2014 U.S. study surveying procrastination and impulsiveness in fraternal- and identical twin pairs, both traits were found to be \"moderately heritable\". The two traits were not separable at the genetic level (r = 1.0), meaning no unique genetic influences of either trait alone was found. The authors confirmed three constructs developed from the evolutionary hypothesis that procrastination arose as a by-product of impulsivity: \"(a) Procrastination is heritable, (b) the two traits share considerable genetic variation, and (c) goal-management ability is an important component of this shared variation.\"\n\nPsychologist William J. Knaus estimated that more than 90% of college students procrastinate. Of these students, 25% are chronic procrastinators and typically abandon higher education (college dropouts).\n\nPerfectionism is a prime cause for procrastination because pursuing unattainable goals (perfection) usually results in failure. Unrealistic expectations destroy self-esteem and lead to self-repudiation, self-contempt, and widespread unhappiness. To overcome procrastination, it is essential to recognize and accept the power of failure without condemning, to stop focusing on faults and flaws and to set goals that are easier to achieve.\n\nBehaviors and practices that reduce procrastination:\n\nMaking a plan to complete tasks in a rigid schedule format might not work for everyone. There is no hard-and-fast rule to follow such a process if it turns out to be counter-productive. Instead of scheduling, it may be better to execute tasks in a flexible, unstructured schedule which has time slots for only necessary activities.\n\nPiers Steel suggests that better time management is a key to overcoming procrastination, including being aware of and using one's \"power hours\" (being a \"morning person\" or \"night owl\"). A good approach is to creatively utilize one's internal circadian rhythms that are best suited for the most challenging and productive work. Steel states that it is essential to have realistic goals, to tackle one problem at a time and to cherish the \"small successes\". Brian O'Leary supports that \"finding a work-life balance...may actually help us find ways to be more productive\", suggesting that dedicating leisure activities as motivation can increase one's efficiency at handling tasks.\n\nAfter contemplating his own procrastination habits, philosopher John Perry authored an essay entitled \"Structured Procrastination\", wherein he proposes a \"cheat\" method as a safer approach for tackling procrastination: using a pyramid scheme to reinforce the unpleasant tasks needed to be completed in a quasi-prioritized order. In other words, the procrastinator should postpone tasks with a mental note that one feels to do while engaged in a work that requires their current attentional focus.\n\nFor some people, procrastination can be persistent and tremendously disruptive to everyday life. For these individuals, procrastination may be symptomatic of a psychological disorder. Procrastination has been linked to a number of negative associations, such as depression, irrational behaviour, low self-esteem, anxiety and neurological disorders such as ADHD. Others have found relationships with guilt and stress. Therefore, it is important for people whose procrastination has become chronic and is perceived to be debilitating to seek out a trained therapist or psychiatrist to investigate whether an underlying mental health issue may be present.\n\nWith a distant deadline, procrastinators report significantly less stress and physical illness than do non-procrastinators. However, as the deadline approaches, this relationship is reversed. Procrastinators report more stress, more symptoms of physical illness, and more medical visits, to the extent that, overall, procrastinators suffer more stress and health problems.\n\nProcrastination has been linked to the complex arrangement of cognitive, affective and behavioral relationships from task desirability to low self esteem and anxiety to depression. A study found that procrastinators were less future-oriented than their non-procrastinator counterparts. This result was hypothesized to be in association with hedonistic perspectives on the present; instead it was found procrastination was better predicted by a fatalistic and hopeless attitude towards life.\n\nA correlation between procrastination and eveningness was observed where individuals who had later sleeping and waking patterns were more likely to procrastinate. It has been shown that Morningness increases across lifespan and procrastination decreases with age., \n\nTraditionally, procrastination has been associated with perfectionism: a tendency to negatively evaluate outcomes and one's own performance, intense fear and avoidance of evaluation of one's abilities by others, heightened social self-consciousness and anxiety, recurrent low mood, and \"workaholism\". However, adaptive perfectionists—egosyntonic perfectionism—were \"less\" likely to procrastinate than non-perfectionists, while maladaptive perfectionists, who saw their perfectionism as a problem—egodystonic perfectionism—had high levels of procrastination and anxiety.\nIn a regression analysis study of Steel, from 2007, it is found that mild to moderate perfectionists typically procrastinate slightly less than others, with \"the exception being perfectionists who were also seeking clinical counseling\".\n\nAccording to an Educational Science Professor, Hatice Odaci, academic procrastination is a significant problem during college years in part because many college students lack efficient time management skills in using the Internet. Also, Odaci notes that most colleges provide free and fast twenty-four-hour Internet service which some students are not usually accustomed to, and as a result of irresponsible use or lack of firewalls these students become engulfed in distractions, and thus in procrastination.\n\n\"Student syndrome\" refers to the phenomenon where a student will begin to fully apply themself to a task only immediately before a deadline. This negates the usefulness of any buffers built into individual task duration estimates. Results from a 2002 study indicate that many students are aware of procrastination and accordingly set binding deadlines long before the date for which a task is due. These self-imposed binding deadlines are correlated with a better performance than without binding deadlines though performance is best for evenly spaced external binding deadlines. Finally, students have difficulties optimally setting self-imposed deadlines, with results suggesting a lack of spacing before the date at which results are due.\nIn one experiment, participation in online exercises was found to be five times higher in the final week before a deadline than in the summed total of the first three weeks for which the exercises were available. Procrastinators end up being the ones doing most of the work in the final week before a deadline.\n\nOther reasons cited on why students procrastinate include fear of failure and success, perfectionist expectations, as well as legitimate activities that may take precedence over school work, such as a job.\n\nProcrastinators have been found to receive worse grades than non-procrastinators. Tice et al. (1997) report that more than one-third of the variation in final exam scores could be attributed to procrastination. The negative association between procrastination and academic performance is recurring and consistent. Howell et al. (2006) found that, though scores on two widely used procrastination scales were not significantly associated with the grade received for an assignment, self-report measures of procrastination on the assessment itself were negatively associated with grade.\n\nIn 2005, a study conducted by Angela Chu and Jin Nam Choi and published in the \"Journal of Social Psychology\" intended to understand task performance among procrastinators with the definition of procrastination as the absence of self-regulated performance, from the 1977 work of Ellis & Knaus. In their study they identified two types of procrastination: the traditional procrastination which they denote as passive, and active procrastination where the person finds enjoyment of a goal-oriented activity only under pressure. The study calls this active procrastination positive procrastination, as it is a functioning state in a self-handicapping environment. In addition, it was observed that active procrastinators have more realistic perceptions of time and perceive more control over their time than passive procrastinators, which is considered a major differentiator between the two types. But surprisingly, active and passive procrastinators showed similar levels of academic performance. The population of the study was college students and the majority of the sample size were women and Asian in origin. Comparisons with chronic pathological procrastination traits were avoided.\n\nDifferent findings emerge when observed and self-reported procrastination are compared. Steel et al. constructed their own scales based on Silver and Sabini’s \"irrational\" and \"postponement\" criteria. They also sought to measure this behavior objectively. During a course, students could complete exam practice computer exercises at their own pace, and during the supervised class time could also complete chapter quizzes. A weighted average of the times at which each chapter quiz was finished formed the measure of observed procrastination, whilst observed irrationality was quantified with the number of practice exercises that were left uncompleted. Researchers found that there was only a moderate correlation between observed and self-reported procrastination (r = 0.35). There was a very strong inverse relationship between the number of exercises completed and the measure of postponement (r = −0.78). Observed procrastination was very strongly negatively correlated with course grade (r = −0.87), as was self-reported procrastination (though less so, r = −0.36). As such, self-reported measures of procrastination, on which the majority of the literature is based, may not be the most appropriate measure to use in all cases. It was also found that procrastination itself may not have contributed significantly to poorer grades. Steel et al. noted that those students who completed all of the practice exercises \"tended to perform well on the final exam no matter how much they delayed.\"\n\nProcrastination is considerably more widespread in students than in the general population, with over 70 percent of students reporting procrastination for assignments at some point. A 2014 panel study from Germany among several thousand university students found that increasing academic procrastination increases the frequency of seven different forms of academic misconduct, i.e., using fraudulent excuses, plagiarism, copying from someone else in exams, using forbidden means in exams, carrying forbidden means into exams, copying parts of homework from others, fabrication or falsification of data and the variety of academic misconduct. This study argues that academic misconduct can be seen as a means to cope with the negative consequences of academic procrastination such as performance impairment.\n\n\n\n"}
{"id": "1270907", "url": "https://en.wikipedia.org/wiki?curid=1270907", "title": "Quad helix", "text": "Quad helix\n\nA quad helix (or quadhelix) is an orthodontic appliance for the upper teeth that is cemented in the mouth. It is attached to the molars by 2 bands and has two or four active helix springs that widen the arch of the mouth to make room for crowded teeth, or correct a posterior cross-bite, where lower teeth are buccal (outer) than upper teeth. It is usually made from 38 mil stainless steel wire and is primarily indicated in mixed dentition, cleft patients and those that have performed the act of thumbsucking. A variety of this appliance is inserted into attachments that are welded to the bands. In this way the orthodontist can adjust the appliance without removing the bands.\n\nThe precursor to the quad-helix was the coffin spring. Similar devices known as tri-helices and bi-helices were later developed, with three and two helix springs, respectively.\n\nThe expander works by gently pushing the teeth outwards to eventually widen the upper arch. A quad helix expander is usually given to those who have a narrow top jaw, a cross bite and/or crowded teeth. \n"}
{"id": "58730172", "url": "https://en.wikipedia.org/wiki?curid=58730172", "title": "Sara Howard (speech therapist)", "text": "Sara Howard (speech therapist)\n\nSara Howard FRCSLT is a British speech therapist and Professor Emerita of Clinical Phonetics at the University of Sheffield.\n\nHoward earned a BA in English and an MA in Linguistics at the University of Leeds before receiving a BSc in Speech & Language Therapy at Leeds Metropolitan University and a PhD in Clinical Phonetics at the University of Sheffield. She works in the area of the phonetics/phonology interface in developmental speech impairments (especially cleft lip and palate). \n\nBetween 2010 and 2012 Howard completed an ESRC Research Fellowship on \"Connected speech and word juncture in typical and atypical speech development\".\n\nHoward was elected a Fellow of the Royal College of Speech and Language Therapists in 2015, and as a life member of the International Clinical Phonetics and Linguistics Association in 2016 of which she was president from 2006 to 2014.\n\n"}
{"id": "7188776", "url": "https://en.wikipedia.org/wiki?curid=7188776", "title": "South Bay Pumping Plant", "text": "South Bay Pumping Plant\n\nThe South Bay Pumping Plant is located 4 miles (6 km) southwest of the Clifton Court Forebay and 10.3 miles (17 km) northeast of Livermore, CA. The plant is the only main line pumping plant for the 42.9 mile (69 km) long South Bay Aqueduct. The plant pumps from the Bethany Reservoir which is part of the California Aqueduct.\n\n\nDel Valle Pumping Plant is another pumping plant on the South Bay Aqueduct which pumps water into Lake Del Valle for storage.\n"}
{"id": "316612", "url": "https://en.wikipedia.org/wiki?curid=316612", "title": "Spring (hydrology)", "text": "Spring (hydrology)\n\nA spring is a point at which water flows from an aquifer to the Earth's surface. It is a component of the hydrosphere.\n\nA spring may be the result of karst topography where surface water has infiltrated the Earth's surface (recharge area), becoming part of the area groundwater. The groundwater then travels through a network of cracks and fissure—openings ranging from intergranular spaces to large caves. The water eventually emerges from below the surface, in the form of a karst spring.\n\nThe forcing of the spring to the surface can be the result of a confined aquifer in which the recharge area of the spring water table rests at a higher elevation than that of the outlet. Spring water forced to the surface by elevated sources are artesian wells. This is possible even if the outlet is in the form of a cave. In this case the cave is used like a hose by the higher elevated recharge area of groundwater to exit through the lower elevation opening.\n\nNon-artesian springs may simply flow from a higher elevation through the earth to a lower elevation and exit in the form of a spring, using the ground like a drainage pipe.\n\nStill other springs are the result of pressure from an underground source in the earth, in the form of volcanic activity. The result can be water at elevated temperature such as a hot spring.\nThe action of the groundwater continually dissolves permeable bedrock such as limestone and dolomite, creating vast cave systems.\n\n\nSpring discharge, or resurgence, is determined by the spring's recharge basin. Factors that affect the recharge include the size of the area in which groundwater is captured, the amount of precipitation, the size of capture points, and the size of the spring outlet. Water may leak into the underground system from many sources including permeable earth, sinkholes, and losing streams. In some cases entire creeks seemingly disappear as the water sinks into the ground via the stream bed. Grand Gulf State Park in Missouri is an example of an entire creek vanishing into the groundwater system. The water emerges away, forming some of the discharge of Mammoth Spring in Arkansas. Human activity may also affect a spring's discharge—withdrawal of groundwater reduces the water pressure in an aquifer, decreasing the volume of flow.\n\nSprings are often classified by the volume of the water they discharge. The largest springs are called \"first-magnitude\", defined as springs that discharge water at a rate of at least 2800 liters or of water per second. Some locations contain many first-magnitude springs, such as Florida where there are at least 27 known to be that size; the Missouri and Arkansas Ozarks, which contain 10 known of first-magnitude; and 11 more in the Thousand Springs area along the Snake River in Idaho. The scale for spring flow is as follows:\n\nMinerals become dissolved in the water as it moves through the underground rocks. This may give the water flavor and even carbon dioxide bubbles, depending on the nature of the geology through which it passes. This is why spring water is often bottled and sold as mineral water, although the term is often the subject of deceptive advertising. Springs that contain significant amounts of minerals are sometimes called 'mineral springs'. (Springs without such mineral content, meanwhile, are sometimes distinguished as 'sweet springs'.) Springs that contain large amounts of dissolved sodium salts, mostly sodium carbonate, are called 'soda springs'. Many resorts have developed around mineral springs and are known as spa towns.\n\nWater from springs is usually clear. However some springs may be colored by the minerals that are dissolved in the water. For instance, water heavy with iron or tannins will have an orange color. \n\nIn parts of the United States a stream carrying the outflow of a spring to a nearby primary stream may be called a spring branch or run. Groundwater tends to maintain a relatively long-term average temperature of its aquifer; so flow from a spring may be cooler than a summer day, but remain unfrozen in the winter. The cool water of a spring and its branch may harbor species such as certain trout that are otherwise ill-suited to a warmer local climate.\n\nSprings have been used for a variety of human needs including drinking water, domestic water supply, irrigation, mills, navigation, and electricity generation. Other modern uses include recreational activities such as fishing, swimming, and floating; therapy; water for livestock; fish hatcheries; and supply for bottled mineral water.\n\nA sacred spring, or holy well, is a small body of water emerging from underground and revered either in a Christian, pagan or other religious context, sometimes both. The lore and mythology of ancient Greece was replete with sacred and storied springs—notably, the Corycian, Pierian and Castalian. In medieval Europe, holy wells were frequently pagan sacred sites that later became Christianized. The term \"holy well\" is commonly employed to refer to any water source of limited size (i.e. not a lake or river, but including pools and natural springs and seeps), which has some significance in local folklore. This can take the form of a particular name, an associated legend, the attribution of healing qualities to the water through the numinous presence of its guardian spirit or Christian saint, or a ceremony or ritual centred on the well site. In Christian legend, the spring water is often said to have been made to flow by the action of a saint, a familiar theme especially in the hagiography of Celtic saints.\n\nAsia:\n\nEurope:\n\nNorth America:\n\nOceania:\n\nSouth America:\n\n\n\n"}
{"id": "30284", "url": "https://en.wikipedia.org/wiki?curid=30284", "title": "Statistical hypothesis testing", "text": "Statistical hypothesis testing\n\nA statistical hypothesis, sometimes called confirmatory data analysis, is an hypothesis that is testable on the basis of observing a process that is modeled via a set of random variables. A statistical hypothesis test is a method of statistical inference. Commonly, two statistical data sets are compared, or a data set obtained by sampling is compared against a synthetic data set from an idealized model. A hypothesis is proposed for the statistical relationship between the two data sets, and this is compared as an alternative to an idealized null hypothesis that proposes no relationship between two data sets. The comparison is deemed \"statistically significant\" if the relationship between the data sets would be an unlikely realization of the null hypothesis according to a threshold probability—the significance level. Hypothesis tests are used in determining what outcomes of a study would lead to a rejection of the null hypothesis for a pre-specified level of significance. The process of distinguishing between the null hypothesis and the alternative hypothesis is aided by identifying two conceptual types of errors. The first type occurs when the null hypothesis is falsely rejected. The second type of error occurs when the null hypothesis is falsely assumed to be true (type 1 and type 2 errors). By specifying a threshold probability ('alpha') on, e.g., the admissible risk of making a type 1 error, the statistical decision process can be controlled.\n\nAn alternative framework for statistical hypothesis testing is to specify a set of statistical models, one for each candidate hypothesis, and then use model selection techniques to choose the most appropriate model. The most common selection techniques are based on either Akaike information criterion or Bayes factor.\n\nConfirmatory data analysis can be contrasted with exploratory data analysis, which may not have pre-specified hypotheses.\n\nStatistical hypothesis testing is a key technique of both frequentist inference and Bayesian inference, although the two types of inference have notable differences. Statistical hypothesis tests define a procedure that controls (fixes) the probability of incorrectly \"deciding\" that a default position (null hypothesis) is incorrect. The procedure is based on how likely it would be for a set of observations to occur if the null hypothesis were true. Note that this probability of making an incorrect decision is \"not\" the probability that the null hypothesis is true, nor whether any specific alternative hypothesis is true. This contrasts with other possible techniques of decision theory in which the null and alternative hypothesis are treated on a more equal basis.\n\nOne naïve Bayesian approach to hypothesis testing is to base decisions on the posterior probability, but this fails when comparing point and continuous hypotheses. Other approaches to decision making, such as Bayesian decision theory, attempt to balance the consequences of incorrect decisions across all possibilities, rather than concentrating on a single null hypothesis. A number of other approaches to reaching a decision based on data are available via decision theory and optimal decisions, some of which have desirable properties. Hypothesis testing, though, is a dominant approach to data analysis in many fields of science. Extensions to the theory of hypothesis testing include the study of the power of tests, i.e. the probability of correctly rejecting the null hypothesis given that it is false. Such considerations can be used for the purpose of sample size determination prior to the collection of data.\n\nIn the statistics literature, statistical hypothesis testing plays a fundamental role. The usual line of reasoning is as follows:\n\nAn alternative process is commonly used:\n\nThe two processes are equivalent. The former process was advantageous in the past when only tables of test statistics at common probability thresholds were available. It allowed a decision to be made without the calculation of a probability. It was adequate for classwork and for operational use, but it was deficient for reporting results.\n\nThe latter process relied on extensive tables or on computational support not always available. The explicit calculation of a\nprobability is useful for reporting. The calculations are now trivially performed with appropriate software.\n\nThe difference in the two processes applied to the Radioactive suitcase example (below):\nThe former report is adequate, the latter gives a more detailed explanation of the data and the reason why the suitcase is being checked.\n\nIt is important to note the difference between accepting the null hypothesis and simply failing to reject it. The \"fail to reject\" terminology highlights the fact that the null hypothesis is assumed to be true from the start of the test; if there is a lack of evidence against it, it simply continues to be assumed true. The phrase \"accept the null hypothesis\" may suggest it has been proved simply because it has not been disproved, a logical fallacy known as the argument from ignorance. Unless a test with particularly high power is used, the idea of \"accepting\" the null hypothesis may be dangerous. Nonetheless the terminology is prevalent throughout statistics, where the meaning actually intended is well understood.\n\nThe processes described here are perfectly adequate for computation. They seriously neglect the design of experiments considerations.\n\nIt is particularly critical that appropriate sample sizes be estimated before conducting the experiment.\n\nThe phrase \"test of significance\" was coined by statistician Ronald Fisher.\n\nThe \"p\"-value is the probability that a given result (or a more significant result) would occur under the null hypothesis. For example, say that a fair coin is tested for fairness (the null hypothesis). At a significance level of 0.05, the fair coin would be expected to (incorrectly) reject the null hypothesis in about 1 out of every 20 tests. The \"p\"-value does not provide the probability that either hypothesis is correct (a common source of confusion).\n\nIf the \"p\"-value is less than the chosen significance threshold (equivalently, if the observed test statistic is in the\ncritical region), then we say the null hypothesis is rejected at the chosen level of significance. Rejection of the null hypothesis is a conclusion. This is like a \"guilty\" verdict in a criminal trial: the evidence is sufficient to reject innocence, thus proving guilt. We might accept the alternative hypothesis (and the research hypothesis).\n\nIf the \"p\"-value is \"not\" less than the chosen significance threshold (equivalently, if the observed test statistic is outside the critical region), then the evidence is insufficient to support a conclusion. (This is similar to a \"not guilty\" verdict.) The researcher typically gives extra consideration to those cases where the \"p\"-value is close to the significance level.\n\nSome people find it helpful to think of the hypothesis testing framework as analogous to a mathematical proof by contradiction.\n\nIn the Lady tasting tea example (below), Fisher required the Lady to properly categorize all of the cups of tea to justify the conclusion that the result was unlikely to result from chance. His test revealed that if the lady was effectively guessing at random (the null hypothesis), there was a 1.4% chance that the observed results (perfectly ordered tea) would occur.\n\nWhether rejection of the null hypothesis truly justifies acceptance of the research hypothesis depends on the structure of the hypotheses. Rejecting the hypothesis that a large paw print originated from a bear does not immediately prove the existence of Bigfoot. Hypothesis testing emphasizes the rejection, which is based on a probability, rather than the acceptance, which requires extra steps of logic.\n\n\"The probability of rejecting the null hypothesis is a function of five factors: whether the test is one- or two tailed, the level of significance, the standard deviation, the amount of deviation from the null hypothesis, and the number of observations.\" These factors are a source of criticism; factors under the control of the experimenter/analyst give the results an appearance of subjectivity.\n\nStatistics are helpful in analyzing most collections of data. This is equally true of hypothesis testing which can justify conclusions even when no scientific theory exists. In the Lady tasting tea example, it was \"obvious\" that no difference existed between (milk poured into tea) and (tea poured into milk). The data contradicted the \"obvious\".\n\nReal world applications of hypothesis testing include:\n\nStatistical hypothesis testing plays an important role in the whole of statistics and in statistical inference. For example, Lehmann (1992) in a review of the fundamental paper by Neyman and Pearson (1933) says: \"Nevertheless, despite their shortcomings, the new paradigm formulated in the 1933 paper, and the many developments carried out within its framework continue to play a central role in both the theory and practice of statistics and can be expected to do so in the foreseeable future\".\n\nSignificance testing has been the favored statistical tool in some experimental social sciences (over 90% of articles in the \"Journal of Applied Psychology\" during the early 1990s). Other fields have favored the estimation of parameters (e.g., effect size). Significance testing is used as a substitute for the traditional comparison of predicted value and experimental result at the core of the scientific method. When theory is only capable of predicting the sign of a relationship, a directional (one-sided) hypothesis test can be configured so that only a statistically significant result supports theory. This form of theory appraisal is the most heavily criticized application of hypothesis testing.\n\n\"If the government required statistical procedures to carry warning labels like those on drugs, most inference methods would have long labels indeed.\" This caution applies to hypothesis tests and alternatives to them.\n\nThe successful hypothesis test is associated with a probability and a type-I error rate. The conclusion \"might\" be wrong.\n\nThe conclusion of the test is only as solid as the sample upon which it is based. The design of the experiment is critical. A number of unexpected effects have been observed including:\nA statistical analysis of misleading data produces misleading conclusions. The issue of data quality can be more subtle. In forecasting for example, there is no agreement on a measure of forecast accuracy. In the absence of a consensus measurement, no decision based on measurements will be without controversy.\n\nThe book \"How to Lie with Statistics\" is the most popular book on statistics ever published. It does not much consider hypothesis\ntesting, but its cautions are applicable, including: Many claims are made on the basis of samples too small to convince. If a report does not mention sample size, be doubtful.\n\nHypothesis testing acts as a filter of statistical conclusions; only those results meeting a probability threshold are publishable. Economics also acts as a publication filter; only those results favorable to the author and funding source may be submitted for publication. The impact of filtering on publication is termed publication bias. A related problem is that of multiple testing (sometimes linked to data mining), in which a variety of tests for a variety of possible effects are applied to a single data set and only those yielding a significant result are reported. These are often dealt with by using multiplicity correction procedures that control the family wise error rate (FWER) or the false discovery rate (FDR).\n\nThose making critical decisions based on the results of a hypothesis test are prudent to look at the details rather than the conclusion alone. In the physical sciences most results are fully accepted only when independently confirmed. The general advice concerning statistics is, \"Figures never lie, but liars figure\" (anonymous).\n\nThe earliest use of statistical hypothesis testing is generally credited to the question of whether male and female births are equally likely (null hypothesis), which was addressed in the 1700s by John Arbuthnot (1710), and later by Pierre-Simon Laplace (1770s).\n\nArbuthnot examined birth records in London for each of the 82 years from 1629 to 1710, and applied the sign test, a simple non-parametric test. In every year, the number of males born in London exceeded the number of females. Considering more male or more female births as equally likely, the probability of the observed outcome is 0.5, or about 1 in 4,8360,0000,0000,0000,0000,0000; in modern terms, this is the \"p\"-value. This is vanishingly small, leading Arbuthnot that this was not due to chance, but to divine providence: \"From whence it follows, that it is Art, not Chance, that governs.\" In modern terms, he rejected the null hypothesis of equally likely male and female births at the \"p\" = 1/2 significance level.\n\nLaplace considered the statistics of almost half a million births. The statistics showed an excess of boys compared to girls. He concluded by calculation of a \"p\"-value that the excess was a real, but unexplained, effect.\n\nIn a famous example of hypothesis testing, known as the \"Lady tasting tea\", Dr. Muriel Bristol, a female colleague of Fisher claimed to be able to tell whether the tea or the milk was added first to a cup. Fisher proposed to give her eight cups, four of each variety, in random order. One could then ask what the probability was for her getting the number she got correct, but just by chance. The null hypothesis was that the Lady had no such ability. The test statistic was a simple count of the number of successes in selecting the 4 cups. The critical region was the single case of 4 successes of 4 possible based on a conventional probability criterion (< 5%). A pattern of 4 successes corresponds to 1 out of 70 possible combinations (p≈ 1.4%). Fisher asserted that no alternative hypothesis was (ever) required. The lady correctly identified every cup, which would be considered a statistically significant result.\n\nA statistical test procedure is comparable to a criminal trial; a defendant is considered not guilty as long as his or her guilt is not proven. The prosecutor tries to prove the guilt of the defendant. Only when there is enough evidence for the prosecution is the defendant convicted.\n\nIn the start of the procedure, there are two hypotheses formula_1: \"the defendant is not guilty\", and formula_2: \"the defendant is guilty\". The first one, formula_1, is called the \"null hypothesis\", and is for the time being accepted. The second one, formula_2, is called the \"alternative hypothesis\". It is the alternative hypothesis that one hopes to support.\n\nThe hypothesis of innocence is only rejected when an error is very unlikely, because one doesn't want to convict an innocent defendant. Such an error is called \"error of the first kind\" (i.e., the conviction of an innocent person), and the occurrence of this error is controlled to be rare. As a consequence of this asymmetric behaviour, an \"error of the second kind\" (acquitting a person who committed the crime), is more common.\n\nA criminal trial can be regarded as either or both of two decision processes: guilty vs not guilty or evidence vs a threshold (\"beyond a reasonable doubt\"). In one view, the defendant is judged; in the other view the performance of the prosecution (which bears the burden of proof) is judged. A hypothesis test can be regarded as either a judgment of a hypothesis or as a judgment of evidence.\n\nThe following example was produced by a philosopher describing scientific methods generations before hypothesis testing was\nformalized and popularized.\nFew beans of this handful are white.\nMost beans in this bag are white.\nTherefore: Probably, these beans were taken from another bag.\nThis is an hypothetical inference.\nThe beans in the bag are the population. The handful are the sample. The null hypothesis is that the sample originated from the population. The criterion for rejecting the null-hypothesis is the \"obvious\" difference in appearance (an informal difference in the mean). The interesting result is that consideration of a real population and a real sample produced an imaginary bag. The philosopher was considering logic rather than probability. To be a real statistical hypothesis test, this example requires the formalities of a probability calculation and a comparison of that probability to a standard.\n\nA simple generalization of the example considers a mixed bag of beans and a handful that contain either very few or very many white beans. The generalization considers both extremes. It requires more calculations and more comparisons to arrive at a formal answer, but the core philosophy is unchanged; If the composition of the handful is greatly different from that of the bag, then the sample probably originated from another bag. The original example is termed a one-sided or a one-tailed test while the generalization is termed a two-sided or two-tailed test.\n\nThe statement also relies on the inference that the sampling was random. If someone had been picking through the bag to find white beans, then it would explain why the handful had so many white beans, and also explain why the number of white beans in the bag was depleted (although the bag is probably intended to be assumed much larger than one's hand).\n\nA person (the subject) is tested for clairvoyance. They are shown the reverse of a randomly chosen playing card 25 times and asked which of the four suits it belongs to. The number of hits, or correct answers, is called \"X\".\n\nAs we try to find evidence of their clairvoyance, for the time being the null hypothesis is that the person is not clairvoyant. The alternative is: the person is (more or less) clairvoyant.\n\nIf the null hypothesis is valid, the only thing the test person can do is guess. For every card, the probability (relative frequency) of any single suit appearing is 1/4. If the alternative is valid, the test subject will predict the suit correctly with probability greater than 1/4. We will call the probability of guessing correctly \"p\". The hypotheses, then, are:\nand\n\nWhen the test subject correctly predicts all 25 cards, we will consider them clairvoyant, and reject the null hypothesis. Thus also with 24 or 23 hits. With only 5 or 6 hits, on the other hand, there is no cause to consider them so. But what about 12 hits, or 17 hits? What is the critical number, \"c\", of hits, at which point we consider the subject to be clairvoyant? How do we determine the critical value \"c\"? With the choice \"c\"=25 (i.e. we only accept clairvoyance when all cards are predicted correctly) we're more critical than with \"c\"=10. In the first case almost no test subjects will be recognized to be clairvoyant, in the second case, a certain number will pass the test. In practice, one decides how critical one will be. That is, one decides how often one accepts an error of the first kind – a false positive, or Type I error. With \"c\" = 25 the probability of such an error is:\n\nand hence, very small. The probability of a false positive is the probability of randomly guessing correctly all 25 times.\n\nBeing less critical, with \"c\"=10, gives:\n\nThus, \"c\" = 10 yields a much greater probability of false positive.\n\nBefore the test is actually performed, the maximum acceptable probability of a Type I error (\"α\") is determined. Typically, values in the range of 1% to 5% are selected. (If the maximum acceptable error rate is zero, an infinite number of correct guesses is required.) Depending on this Type 1 error rate, the critical value \"c\" is calculated. For example, if we select an error rate of 1%, \"c\" is calculated thus:\n\nFrom all the numbers c, with this property, we choose the smallest, in order to minimize the probability of a Type II error, a false negative. For the above example, we select: formula_10.\nAs an example, consider determining whether a suitcase contains some radioactive material. Placed under a Geiger counter, it produces 10 counts per minute. The null hypothesis is that no radioactive material is in the suitcase and that all measured counts are due to ambient radioactivity typical of the surrounding air and harmless objects. We can then calculate how likely it is that we would observe 10 counts per minute if the null hypothesis were true. If the null hypothesis predicts (say) on average 9 counts per minute, then according to the Poisson distribution typical for radioactive decay there is about 41% chance of recording 10 or more counts. Thus we can say that the suitcase is compatible with the null hypothesis (this does not guarantee that there is no radioactive material, just that we don't have enough evidence to suggest there is). On the other hand, if the null hypothesis predicts 3 counts per minute (for which the Poisson distribution predicts only 0.1% chance of recording 10 or more counts) then the suitcase is not compatible with the null hypothesis, and there are likely other factors responsible to produce the measurements.\n\nThe test does not directly assert the presence of radioactive material. A \"successful\" test asserts that the claim of no radioactive material present is unlikely given the reading (and therefore ...). The double negative (disproving the null hypothesis) of the method is confusing, but using a counter-example to disprove is standard mathematical practice. The attraction of the method is its practicality. We know (from experience) the expected range of counts with only ambient radioactivity present, so we can say that a measurement is \"unusually\" large. Statistics just formalizes the intuitive by using numbers instead of adjectives. We probably do not know the characteristics of the radioactive suitcases; We just assume\nthat they produce larger readings.\n\nTo slightly formalize intuition: radioactivity is suspected if the Geiger-count with the suitcase is among or exceeds the greatest (5% or 1%) of the Geiger-counts made with ambient radiation alone. This makes no assumptions about the distribution of counts. Many ambient radiation observations are required to obtain good probability estimates for rare events.\n\nThe test described here is more fully the null-hypothesis statistical significance test. The null hypothesis represents what we would believe by default, before seeing any evidence. Statistical significance is a possible finding of the test, declared when the observed sample is unlikely to have occurred by chance if the null hypothesis were true. The name of the test describes its formulation and its possible outcome. One characteristic of the test is its crisp decision: to reject or not reject the null hypothesis. A calculated value is compared to a threshold, which is determined from the tolerable risk of error.\n\nThe following definitions are mainly based on the exposition in the book by Lehmann and Romano:\n\n\nA statistical hypothesis test compares a test statistic (\"z\" or \"t\" for examples) to a threshold. The test statistic (the formula found in the table below) is based on optimality. For a fixed level of Type I error rate, use of these statistics minimizes Type II error rates (equivalent to maximizing power). The following terms describe tests in terms of such optimality:\n\n\nWhile hypothesis testing was popularized early in the 20th century, early forms were used in the 1700s. The first use is credited to John Arbuthnot (1710), followed by Pierre-Simon Laplace (1770s), in analyzing the human sex ratio at birth; see .\n\nModern significance testing is largely the product of Karl Pearson (p-value, Pearson's chi-squared test), William Sealy Gosset (Student's t-distribution), and Ronald Fisher (\"null hypothesis\", analysis of variance, \"significance test\"), while hypothesis testing was developed by Jerzy Neyman and Egon Pearson (son of Karl). Ronald Fisher began his life in statistics as a Bayesian (Zabell 1992), but Fisher soon grew disenchanted with the subjectivity involved (namely use of the principle of indifference when determining prior probabilities), and sought to provide a more \"objective\" approach to inductive inference.\n\nFisher was an agricultural statistician who emphasized rigorous experimental design and methods to extract a result from few samples assuming Gaussian distributions. Neyman (who teamed with the younger Pearson) emphasized mathematical rigor and methods to obtain more results from many samples and a wider range of distributions. Modern hypothesis testing is an inconsistent hybrid of the Fisher vs Neyman/Pearson formulation, methods and terminology developed in the early 20th century.\n\nFisher popularized the \"significance test\". He required a null-hypothesis (corresponding to a population frequency distribution) and a sample. His (now familiar) calculations determined whether to reject the null-hypothesis or not. Significance testing did not utilize an alternative hypothesis so there was no concept of a Type II error.\n\nThe p-value was devised as an informal, but objective, index meant to help a researcher determine (based on other knowledge) whether to modify future experiments or strengthen one's faith in the null hypothesis. Hypothesis testing (and Type I/II errors) was devised by Neyman and Pearson as a more objective alternative to Fisher's p-value, also meant to determine researcher behaviour, but without requiring any inductive inference by the researcher.\n\nNeyman & Pearson considered a different problem (which they called \"hypothesis testing\"). They initially considered two simple hypotheses (both with frequency distributions). They calculated two probabilities and typically selected the hypothesis associated with the higher probability (the hypothesis more likely to have generated the sample). Their method always selected a hypothesis. It also allowed the calculation of both types of error probabilities.\n\nFisher and Neyman/Pearson clashed bitterly. Neyman/Pearson considered their formulation to be an improved generalization of significance testing.(The defining paper was abstract. Mathematicians have generalized and refined the theory for decades.) Fisher thought that it was not applicable to scientific research because often, during the course of the experiment, it is discovered that the initial assumptions about the null hypothesis are questionable due to unexpected sources of error. He believed that the use of rigid reject/accept decisions based on models formulated before data is collected was incompatible with this common scenario faced by scientists and attempts to apply this method to scientific research would lead to mass confusion.\n\nThe dispute between Fisher and Neyman–Pearson was waged on philosophical grounds, characterized by a philosopher as a dispute over the proper role of models in statistical inference.\n\nEvents intervened: Neyman accepted a position in the western hemisphere, breaking his partnership with Pearson and separating disputants (who had occupied the same building) by much of the planetary diameter. World War II provided an intermission in the debate. The dispute between Fisher and Neyman terminated (unresolved after 27 years) with Fisher's death in 1962. Neyman wrote a well-regarded eulogy. Some of Neyman's later publications reported p-values and significance levels.\n\nThe modern version of hypothesis testing is a hybrid of the two approaches that resulted from confusion by writers of statistical textbooks (as predicted by Fisher) beginning in the 1940s. (But signal detection, for example, still uses the Neyman/Pearson formulation.) Great conceptual differences and many caveats in addition to those mentioned above were ignored. Neyman and Pearson provided the stronger terminology, the more rigorous mathematics and the more consistent philosophy, but the subject taught today in introductory statistics has more similarities with Fisher's method than theirs. This history explains the inconsistent terminology (example: the null hypothesis is never accepted, but there is a region of acceptance).\n\nSometime around 1940, in an apparent effort to provide researchers with a \"non-controversial\" way to have their cake and eat it too, the authors of statistical text books began anonymously combining these two strategies by using the p-value in place of the test statistic (or data) to test against the Neyman–Pearson \"significance level\". Thus, researchers were encouraged to infer the strength of their data against some null hypothesis using p-values, while also thinking they are retaining the post-data collection objectivity provided by hypothesis testing. It then became customary for the null hypothesis, which was originally some realistic research hypothesis, to be used almost solely as a strawman \"nil\" hypothesis (one where a treatment has no effect, regardless of the context).\n\nA comparison between Fisherian, frequentist (Neyman–Pearson)\nPaul Meehl has argued that the epistemological importance of the choice of null hypothesis has gone largely unacknowledged. When the null hypothesis is predicted by theory, a more precise experiment will be a more severe test of the underlying theory. When the null hypothesis defaults to \"no difference\" or \"no effect\", a more precise experiment is a less severe test of the theory that motivated performing the experiment. An examination of the origins of the latter practice may therefore be useful:\n\n1778: Pierre Laplace compares the birthrates of boys and girls in multiple European cities. He states: \"it is natural to conclude that these possibilities are very nearly in the same ratio\". Thus Laplace's null hypothesis that the birthrates of boys and girls should be equal given \"conventional wisdom\".\n\n1900: Karl Pearson develops the chi squared test to determine \"whether a given form of frequency curve will effectively describe the samples drawn from a given population.\" Thus the null hypothesis is that a population is described by some distribution predicted by theory. He uses as an example the numbers of five and sixes in the Weldon dice throw data.\n\n1904: Karl Pearson develops the concept of \"contingency\" in order to determine whether outcomes are independent of a given categorical factor. Here the null hypothesis is by default that two things are unrelated (e.g. scar formation and death rates from smallpox). The null hypothesis in this case is no longer predicted by theory or conventional wisdom, but is instead the principle of indifference that led Fisher and others to dismiss the use of \"inverse probabilities\".\n\nAn example of Neyman–Pearson hypothesis testing can be made by a change to the radioactive suitcase example. If the \"suitcase\" is actually a shielded container for the transportation of radioactive material, then a test might be used to select among three hypotheses: no radioactive source present, one present, two (all) present. The test could be required for safety, with actions required in each case. The Neyman–Pearson lemma of hypothesis testing says that a good criterion for the selection of hypotheses is the ratio of their probabilities (a likelihood ratio). A simple method of solution is to select the hypothesis with the highest probability for the Geiger counts observed. The typical result matches intuition: few counts imply no source, many counts imply two sources and intermediate counts imply one source. Notice also that usually there are problems for proving a negative. Null hypotheses should be at least falsifiable.\n\nNeyman–Pearson theory can accommodate both prior probabilities and the costs of actions resulting from decisions. The former allows each test to consider the results of earlier tests (unlike Fisher's significance tests). The latter allows the consideration of economic issues (for example) as well as probabilities. A likelihood ratio remains a good criterion for selecting among hypotheses.\n\nThe two forms of hypothesis testing are based on different problem formulations. The original test is analogous to a true/false question; the Neyman–Pearson test is more like multiple choice. In the view of Tukey the former produces a conclusion on the basis of only strong evidence while the latter produces a decision on the basis of available evidence. While the two tests seem quite different both mathematically and philosophically, later developments lead to the opposite claim. Consider many tiny radioactive sources. The hypotheses become 0,1,2,3... grains of radioactive sand. There is little distinction between none or some radiation (Fisher) and 0 grains of radioactive sand versus all of the alternatives (Neyman–Pearson). The major Neyman–Pearson paper of 1933 also considered composite hypotheses (ones whose distribution includes an unknown parameter). An example proved the optimality of the (Student's) \"t\"-test, \"there can be no better test for the hypothesis under consideration\" (p 321). Neyman–Pearson theory was proving the optimality of Fisherian methods from its inception.\n\nFisher's significance testing has proven a popular flexible statistical tool in application with little mathematical growth potential. Neyman–Pearson hypothesis testing is claimed as a pillar of mathematical statistics, creating a new paradigm for the field. It also stimulated new applications in statistical process control, detection theory, decision theory and game theory. Both formulations have been successful, but the successes have been of a different character.\n\nThe dispute over formulations is unresolved. Science primarily uses Fisher's (slightly modified) formulation as taught in introductory statistics. Statisticians study Neyman–Pearson theory in graduate school. Mathematicians are proud of uniting the formulations. Philosophers consider them separately. Learned opinions deem the formulations variously competitive (Fisher vs Neyman), incompatible or complementary. The dispute has become more complex since Bayesian inference has achieved respectability.\n\nThe terminology is inconsistent. Hypothesis testing can mean any mixture of two formulations that both changed with time. Any discussion of significance testing vs hypothesis testing is doubly vulnerable to confusion.\n\nFisher thought that hypothesis testing was a useful strategy for performing industrial quality control, however, he strongly disagreed that hypothesis testing could be useful for scientists.\nHypothesis testing provides a means of finding test statistics used in significance testing. The concept of power is useful in explaining the consequences of adjusting the significance level and is heavily used in sample size determination. The two methods remain philosophically distinct. They usually (but \"not always\") produce the same mathematical answer. The preferred answer is context dependent. While the existing merger of Fisher and Neyman–Pearson theories has been heavily criticized, modifying the merger to achieve Bayesian goals has been considered.\n\nCriticism of statistical hypothesis testing fills volumes citing 300–400 primary references. Much of the criticism can\nbe summarized by the following issues:\n\nCritics and supporters are largely in factual agreement regarding the characteristics of null hypothesis significance testing (NHST): While it can provide critical information, it is \"inadequate as the sole tool for statistical analysis\". \"Successfully rejecting the null hypothesis may offer no support for the research hypothesis.\" The continuing controversy concerns the selection of the best statistical practices for the near-term future given the (often poor) existing practices. Critics would prefer to ban NHST completely, forcing a complete departure from those practices, while supporters suggest a less absolute change.\n\nControversy over significance testing, and its effects on publication bias in particular, has produced several results. The American Psychological Association has strengthened its statistical reporting requirements after review, medical journal publishers have recognized the obligation to publish some results that are not statistically significant to combat publication bias and a journal (\"Journal of Articles in Support of the Null Hypothesis\") has been created to publish such results exclusively. Textbooks have added some cautions and increased coverage of the tools necessary to estimate the size of the sample required to produce significant results. Major organizations have not abandoned use of significance tests although some have discussed doing so.\n\nThe numerous criticisms of significance testing do not lead to a single alternative. A unifying position of critics is that statistics should not lead to a conclusion or a decision but to a probability or to an estimated value with a confidence interval rather than to an accept-reject decision regarding a particular hypothesis. It is unlikely that the controversy surrounding significance testing will be resolved in the near future. Its supposed flaws and unpopularity do not eliminate the need for an objective and transparent means of reaching conclusions regarding studies that produce statistical results. Critics have not unified around an alternative. Other forms of reporting confidence or uncertainty could probably grow in popularity. One strong critic of significance testing suggested a list of reporting alternatives: effect sizes for importance, prediction intervals for confidence, replications and extensions for replicability, meta-analyses for generality. None of these suggested alternatives produces a conclusion/decision. Lehmann said that hypothesis testing theory can be presented in terms of conclusions/decisions, probabilities, or confidence intervals. \"The distinction between the ... approaches is largely one of reporting and interpretation.\"\n\nOn one \"alternative\" there is no disagreement: Fisher himself said, \"In relation to the test of significance, we may say that a phenomenon is experimentally demonstrable when we know how to conduct an experiment which will rarely fail to give us a statistically significant result.\" Cohen, an influential critic of significance testing, concurred, \"... don't look for a magic alternative to NHST \"[null hypothesis significance testing]\" ... It doesn't exist.\" \"... given the problems of statistical induction, we must finally rely, as have the older sciences, on replication.\" The \"alternative\" to significance testing is repeated testing. The easiest way to decrease statistical uncertainty is by obtaining more data, whether by increased sample size or by repeated tests. Nickerson claimed to have never seen the publication of a literally replicated experiment in psychology. An indirect approach to replication is meta-analysis.\n\nBayesian inference is one proposed alternative to significance testing. (Nickerson cited 10 sources suggesting it, including Rozeboom (1960)). For example, Bayesian parameter estimation can provide rich information about the data from which researchers can draw inferences, while using uncertain priors that exert only minimal influence on the results when enough data is available. Psychologist John K. Kruschke has suggested Bayesian estimation as an alternative for the \"t\"-test. Alternatively two competing models/hypothesis can be compared using Bayes factors. Bayesian methods could be criticized for requiring information that is seldom available in the cases where significance testing is most heavily used. Neither the prior probabilities nor the probability distribution of the test statistic under the alternative hypothesis are often available in the social sciences.\n\nAdvocates of a Bayesian approach sometimes claim that the goal of a researcher is most often to objectively assess the probability that a hypothesis is true based on the data they have collected. Neither Fisher's significance testing, nor Neyman–Pearson hypothesis testing can provide this information, and do not claim to. The probability a hypothesis is true can only be derived from use of Bayes' Theorem, which was unsatisfactory to both the Fisher and Neyman–Pearson camps due to the explicit use of subjectivity in the form of the prior probability. Fisher's strategy is to sidestep this with the p-value (an objective \"index\" based on the data alone) followed by \"inductive inference\", while Neyman–Pearson devised their approach of \"inductive behaviour\".\n\nHypothesis testing and philosophy intersect. Inferential statistics, which includes hypothesis testing, is applied probability. Both probability and its application are intertwined with philosophy. Philosopher David Hume wrote, \"All knowledge degenerates into probability.\" Competing practical definitions of probability reflect philosophical differences. The most common application of hypothesis testing is in the scientific interpretation of experimental data, which is naturally studied by the philosophy of science.\n\nFisher and Neyman opposed the subjectivity of probability. Their views contributed to the objective definitions. The core of their historical disagreement was philosophical.\n\nMany of the philosophical criticisms of hypothesis testing are discussed by statisticians in other contexts, particularly correlation does not imply causation and the design of experiments.\nHypothesis testing is of continuing interest to philosophers.<ref name=\"doi10.1093/bjps/axl003\">\n</ref>\n\nStatistics is increasingly being taught in schools with hypothesis testing being one of the elements taught. Many conclusions reported in the popular press (political opinion polls to medical studies) are based on statistics. Some writers have stated that statistical analysis of this kind allows for thinking clearly about problems involving mass data, as well as the effective reporting of trends and inferences from said data, but caution that writers for a broad public should have a solid understanding of the field in order to use the terms and concepts correctly. An introductory college statistics class places much emphasis on hypothesis testing – perhaps half of the course. Such fields as literature and divinity now include findings based on statistical analysis (see the Bible Analyzer). An introductory statistics class teaches hypothesis testing as a cookbook process. Hypothesis testing is also taught at the postgraduate level. Statisticians learn how to create good statistical test procedures (like \"z\", Student's \"t\", \"F\" and chi-squared). Statistical hypothesis testing is considered a mature area within statistics, but a limited amount of development continues.\n\nAn academic study states that the cookbook method of teaching introductory statistics leaves no time for history, philosophy or controversy. Hypothesis testing has been taught as received unified method. Surveys showed that graduates of the class were filled with philosophical misconceptions (on all aspects of statistical inference) that persisted among instructors. While the problem was addressed more than a decade ago, and calls for educational reform continue, students still graduate from statistics classes holding fundamental misconceptions about hypothesis testing. Ideas for improving the teaching of hypothesis testing include encouraging students to search for statistical errors in published papers, teaching the history of statistics and emphasizing the controversy in a generally dry subject.\n\n\n\n"}
{"id": "2061975", "url": "https://en.wikipedia.org/wiki?curid=2061975", "title": "Stomatitis", "text": "Stomatitis\n\nStomatitis is inflammation of the mouth and lips. It refers to any inflammatory process affecting the mucous membranes of the mouth and lips, with or without oral ulceration.\n\nIn its widest meaning, stomatitis can have a multitude of different causes and appearances. Common causes include infections, nutritional deficiencies, allergic reactions, radiotherapy, and many others.\n\nWhen inflammation of the gums and the mouth generally presents itself, sometimes the term \"gingivostomatitis\" is used, though this is also sometimes used as a synonym for herpetic gingivostomatitis.\n\nThe term is derived from the Greek \"stoma\" (), meaning \"mouth\", and the suffix \"-itis\" (), meaning \"inflammation\".\n\nMalnutrition (improper dietary intake) or malabsorption (poor absorption of nutrients into the body) can lead to nutritional deficiency states, several of which can lead to stomatitis. For example, deficiencies of iron, vitamin B2 (riboflavin), vitamin B3 (niacin), vitamin B6 (pyridoxine), vitamin B9 (folic acid) or vitamin B12 (cobalamine) may all manifest as stomatitis. Iron is necessary for the upregulation of transcriptional elements for cell replication and repair. Lack of iron can cause genetic downregulation of these elements, leading to ineffective repair and regeneration of epithelial cells, especially in the mouth and lips. Many disorders which cause malabsorption can cause deficiencies, which in turn causes stomatitis. Examples include tropical sprue.\n\nAphthous stomatitis (canker sores) is the recurrent appearance of mouth ulcers in otherwise healthy individuals. The cause is not completely understood, but it is thought that the condition represents a T cell mediated immune response which is triggered by a variety of factors. The individual ulcers (aphthae) recur periodically and heal completely, although in the more severe forms new ulcers may appear in other parts of the mouth before the old ones have finished healing. Aphthous stomatitis is one of the most common diseases of the oral mucosa, and is thought to affect about 20% of the general population to some degree. The symptoms range from a minor nuisance to being disabling in their impact on eating, swallowing and talking, and the severe forms can cause people to lose weight. There is no cure for aphthous stomatitis, and therapies are aimed at alleviating the pain, reducing the inflammation and promoting healing of the ulcers, but there is little evidence of efficacy for any treatment that has been used.\n\nInflammation of the corners (angles) of the lips is termed angular stomatitis or angular cheilitis. In children a frequent cause is repeated lip-licking, and in adults it may be a sign of underlying iron deficiency anemia, or vitamin B deficiencies (\"e.g.\", B-riboflavin, B-folate, or B-cobalamin, which in turn may be evidence of poor diets or malnutrition such as celiac disease).\n\nAlso, angular cheilitis can be caused by a patient's jaws at rest being 'overclosed' due to edentulousness or tooth wear, causing the jaws to come to rest closer together than if the complete/unaffected dentition were present. This causes skin folds around the angle of the mouth which are kept moist by saliva, which in turn favours infection; mostly by \"Candida albicans\" or similar species. Treatment usually involves the administration of topical nystatin or similar antifungal agents. Another treatment can be to correct the jaw relationship with dental treatment (\"e.g.\", dentures or occlusal adjustment).\n\nThis is a common condition present in denture wearers. It appears as reddened but painless mucosa beneath the denture. 90% of cases are associated with \"Candida\" species, and it is the most common form of oral candidiasis. Treatment is by antifungal medication and improved dental hygiene, such as not wearing the denture during sleep.\n\nAllergic contact stomatitis (also termed \"allergic gingivostomatitis\" or \"allergic contact gingivostomatitis\") is a type IV (delayed) hypersensitivity reaction that occurs in susceptible atopic individuals when allergens penetrate the skin or mucosa.\n\nAllergens, which may be different for different individuals, combine with epithelial-derived proteins, forming haptens which bind with Langerhans cells in the mucosa, which in turn present the antigen on their surface to T lymphocytes, sensitizing them to that antigen and causing them to produce many specific clones. The second time that specific antigen is encountered, an inflammatory reaction is triggered at the site of exposure. Allergic contact stomatitis is less common than allergic contact dermatitis because the mouth is coated in saliva, which washes away antigens and acts as a barrier. The oral mucosa is also more vascular (has a better blood supply) than skin, meaning that any antigens are more quickly removed from the area by the circulation. Finally, there is substantially less keratin in oral mucosa, meaning that there is less likelihood that haptens will form.\n\nAllergic contact stomatitis appears as non-specific inflammation, so it may be mistaken for chronic physical irritation. There may be burning or soreness of the mouth and ulceration. Chronic exposure to the allergen may result in a lichenoid lesion. Plasma cell gingivitis may also occur, which may be accompanied by glossitis and cheilitis.\nAllergens that may cause allergic contact stomatitis in some individuals include cinnamaldehyde, Balsam of Peru, peppermint, mercury, gold, pyrophosphates, zinc citrate, free acrylic monomer, nickel, fluoride, and sodium lauryl sulfate. These allergens may originate from many sources, including various foods and drink, chewing gum, toothpaste, mouthwash, dental floss, dental fillings, dentures, orthodontic bands or wires, and many other sources. If the substance containing the allergen comes into contact with the lips, allergic contact cheilitis can occur, together with allergic contact stomatitis.\n\nThe diagnosis is confirmed by patch test, and management is by avoidance of exposure to the allergen.\n\nMigratory stomatitis (or geographic stomatitis) is an atypical presentation of a condition which normally presents on the tongue, termed geographic tongue. Geographic tongue is so named because there are atrophic, erythematous areas of depapillation that migrate over time, giving a map-like appearance.\n\nIn migratory stomatitis, other mucosal sites in the mouth, such as the ventral surface (undersurface) of the tongue, buccal mucosa, labial mucosa, soft palate, or floor of mouth may be afflicted with identical lesions, usually in addition to the tongue. Apart from not being restricted to the tongue, migratory stomatitis is an identical condition in every regard to geographic tongue. Another synonym for geographic tongue which uses the term stomatitis is \"stomatitis areata migrans\".\n\nThis is inflammation of the mouth caused by herpes simplex virus.\n\nStomatitis may also be caused by chemotherapy, or radiation therapy of the oropharyngeal area. The term \"mucositis\" is sometimes used synonymously with stomatitis, however the former usually refers to mucosal reactions to radiotherapy or chemotherapy, and may occur anywhere in the gastrointestinal tract and not just in the mouth.\n\nThe term \"necrotizing ulcerative gingivostomatitis\" is sometimes used as a synonym of the necrotizing periodontal disease more commonly termed necrotizing ulcerative gingivitis, or a more severe form (also termed necrotizing stomatitis). The term \"necrotizing gingivostomatitis\" is also sometimes used.\n\nAlso called smoker's palatal keratosis, this condition may occur in smokers, especially pipe smokers. The palate appears dry and cracked, and white from keratosis. The minor salivary glands appear as small, red and swollen bumps. It is not a premalignant condition, and the appearance reverses if the smoking is stopped.\n\nChronic ulcerative stomatitis is a recently discovered condition with specific immunopathologic features. It is characterized by erosions and ulcerations which relapse and remit. Lesions are located on the buccal mucosa (inside of the cheeks) or on the gingiva (gums). The condition resembles Oral lichen planus when biopsied.\n\nThe diagnosis is made with Immunofluorescence techniques, which shows circulating and tissue-bound autoantibodies (particulate stratified squamous-epithelium-specific antinuclear antibody) to DeltaNp63alpha protein, a normal component of the epithelium. Treatment is with hydroxychloroquine.\n\nTerms such as \"plasma cell gingivostomatitis\", \"atypical gingivostomatitis\" and \"idiopathic gingivostomatitis\" are sometimes a synonym for plasma cell gingivitis, or specifically to refer to a severe form of plasma cell gingivitis.\n\n\n"}
{"id": "1517921", "url": "https://en.wikipedia.org/wiki?curid=1517921", "title": "Time for Timer", "text": "Time for Timer\n\nTime for Timer is a title for a short series of public service announcements broadcast on Saturday mornings on the ABC television network starting in the early 1970s. The animated spots featured Timer, a tiny cartoon character who represents the sense of \"time\" in the human body. Timer is in charge of when a person felt it was time to eat, time to sleep, etc. He carries a large pocket watch inside of him, which would often set off an alarm whenever something was about to happen. \n\nUsually wearing a bow tie and top hat, Timer looks somewhat like a little yellow blob with long arms and legs, and a face. Timer also has limited magical powers, such as instant transportation, which he often uses to exit his host body from time to time if things got too exhausting. A wise-cracker as well as a song-and-dance man, Timer promotes healthy eating and personal hygiene for children, using clever songs and animation.\n\nLike ABC's \"The Bod Squad\" series, the segments never carried official titles, but are referred to by memorable catch phrases in the songs' lyrics. Perhaps most memorable was \"I Hanker for a Hunk of Cheese\" in which Timer, recast as a cowboy with a thick Western accent, suggests \"wagon wheels,\" sandwiches made with cheese slices and crackers as an easy and nutritious snack. (When Timer prepares one on a kitchen counter, he rolls it down the counter on its edge and tells us, \"Look! A wagon wheel!\") Others included \"You Are What You Eat\" (a simplified explanation of nutrients and how the body uses them), \"Quickie Breakfast\" (leftovers and other premade foods as an alternative for kids who do not have time, or are unable, to cook breakfast), and \"Sunshine on a Stick\" (how to make ice pops with fruit juice, an ice tray, and toothpicks).\n\n\"Time for Timer\" ran concurrently and interchangeably for many years with ABC's other educational spots, primarily \"The Bod Squad\" and \"Schoolhouse Rock!\". They generally appeared between cartoon programs on the hour and half hour marks interspersed with regular commercials.\n\nTimer's voice was provided by actor Lennie Weinrib. The spots themselves were produced by the cartoon studio DePatie-Freleng Enterprises. During the final five seconds or so of each segment (sometimes the first 5 seconds), the following end credit appeared on the bottom of the screen:\n\nTimer first appeared in the 1973 \"ABC Afterschool Special\" \"The Incredible, Indelible, Magical, Physical Mystery Trip\", where he was voiced by Len Maxwell. Timer also appeared in the 1974 \"ABC Afterschool Special\" \"The Magical Mystery Trip Through Little Red's Head\" (now voiced by Lennie Weinrib). In Mystery Trip, he was working inside the body of a man named \"Uncle Carl\". In Little Red, he was working inside the (Teenaged) Red Riding Hood.\n\n"}
{"id": "17395241", "url": "https://en.wikipedia.org/wiki?curid=17395241", "title": "Tomas J. Philipson", "text": "Tomas J. Philipson\n\nTomas J. Philipson is the Daniel Levin Chair in Public Policy at the University of Chicago with posts in the Harris School of Public Policy Studies, department of economics, and the Law School. He also is the Director of the health economics program at the Becker Friedman Institute at The University.\n\nPhilipson is a co-founder of Precision Heath Economics, a healthcare consulting firm with headquarters in Los Angeles and locations throughout the US, Canada, and Europe. In August 2017 he was appointed by President Donald Trump to become a member of the Council of Economic Advisers.\n\nPhilipson was born and raised in Sweden where he obtained his undergraduate degree in mathematics at Uppsala University. After earning his MA in mathematics at the Claremont Graduate University, he went on to receive his MA and PhD in economics from the Wharton School and the University of Pennsylvania.\n\nHe then joined the University of Chicago as a postdoctoral fellow in 1989 and thereafter joined the faculty. He was a visiting faculty member at Yale University in the academic year 1994-95 and a visiting fellow at the World Bank in the winter of 2003.\n\nPhilipson has served in several public sector positions. He served in the second Bush Administration as the senior economic advisor to the head of the Food and Drug Administration(FDA) during 2003-04 and subsequently as the senior economic advisor to the head of the Centers for Medicare and Medicaid Services (CMS) in 2004-05. He served as a senior health care advisor to Senator John McCain during his 2008 campaign for President of the United States. In December 2010, he was appointed by the Speaker of the US House of Representatives to the Key Indicator Commission created by the Affordable Care Act. He has served as scientific advisor to Congress on the 21st Century Cures Act and to Vice President Biden’s Cancer Moonshot Initiative.\n\nPhilipson is the recipient of numerous international and national research awards. He has twice (in 2000 and 2006) been the recipient of the highest honor of his field: the Kenneth Arrow Award of the International Health Economics Association (for best paper in the field of health economics). In addition, he was awarded the Garfield Award by Research America in 2007, The Prêmio Haralambos Simeonidisand from the Brazilian Economic Association in 2006, and the Distinguished Economic Research Award from the Milken Institute in 2003. Philipson has been awarded numerous grants and awards from both public and private agencies, including the National Institutes of Health, the National Science Foundation, the Rockefeller Foundation, the Alfred P. Sloan Foundation, the John M. Olin Foundation, and the Royal Swedish Academy of Sciences.\n\nPhilipson is a founding editor of the journal Forums for Health Economics & Policy of Berkeley Electronic Press and has been on the editorial board of the journal Health Economics and The European Journal of Health Economics. His research has been published widely in all leading academic journals of economics such as the American Economic Review, Journal of Political Economy, Quarterly Journal of Economics, Journal of Economic Theory, Journal of Health Economics, Health Affairs, and Econometrica.\n\nPhilipson is a fellow, board member, or associate of a number of other organizations outside the University, including the National Bureau of Economic Research, the American Enterprise Institute, the Manhattan Institute (where he is chairman of Project FDA), the Heartland Institute, the Milken Institute, the RAND Corporation, and the USC Shaeffer Center for Health Economics and Policy at the University of Chicago, he is affiliated with the John M. Olin Program of Law & Economics, the George J. Stigler Center for the Study of the Economy and the State, the Population Research Center, and the National Opinion Research Center (NORC). He was a member of the University-wide Council on Research in 2000-02 and is currently a member of the Advisory Committee to the University's Office of Intellectual Property and Technology Transfer (UCTech). Philipson has done an extensive amount of executive consulting in the United States and abroad. He has consulted for both private corporations, including many U.S. Fortune 100 companies, as well as government organizations domestically and internationally. This has included work for the President's Council on Science and Technology, the National Academy of Sciences, and reforming the payment system of the UK National Health Service. It has also included work for multi-lateral organizations such as the World Bank, the World Intellectual Property Organization, and the OECD.\n\nPhilipson is a contributor to a monthly column at Forbes Magazine and coverage of Philipson's research has appeared in numerous popular media outlets such as CNN, CBS, FOX News, Bloomberg TV, National Public Radio, New York Times, Wall Street Journal, New York Times Book Review, Business Week, The Economist, Washington Post, Investor's Business Daily, USA Today. He is a frequent keynote speaker at many domestic-and international health care events and conferences.\n\n"}
{"id": "2513680", "url": "https://en.wikipedia.org/wiki?curid=2513680", "title": "WHO disease staging system for HIV infection and disease", "text": "WHO disease staging system for HIV infection and disease\n\nWHO Disease Staging System for HIV Infection and Disease was first produced in 1990 by the World Health Organization and updated in September 2005. It is an approach for use in resource limited settings and is widely used in Africa and Asia and has been a useful research tool in studies of progression to symptomatic HIV disease. Most of these conditions are opportunistic infections that can be easily treated in healthy people. The staging system is different for adults and adolescents and children.\n\nStage I: HIV disease is asymptomatic and not categorized as AIDS.\n\nStage II: include minor mucocutaneous manifestations and recurrent upper respiratory tract infections.\n\nStage III: includes unexplained chronic diarrhea for longer than a month, severe bacterial infections and pulmonary tuberculosis.\n\nStage IV: includes toxoplasmosis of the brain, candidiasis of the esophagus, trachea, bronchi or lungs and Kaposi's sarcoma; these diseases are used as indicators of AIDS.\n"}
{"id": "29559391", "url": "https://en.wikipedia.org/wiki?curid=29559391", "title": "World Breastfeeding Week", "text": "World Breastfeeding Week\n\nWorld Breastfeeding Week (WBW) is an annual celebration which is being held every year from 1 to 7 August in more than 120 countries. According to the 26 August data of \"WBW\" website, 540 events have been held worldwide by more than 79 countries with 488 organizations and 406,620 participants for the \"World Breastfeeding Week 2010\". See WBW pledges for the complete list.\n\nBeing organized by WABA, WHO and UNICEF, \"WBW\" came up with the goal to promote exclusive breastfeeding for the first six months of life which yields tremendous health benefits, providing critical nutrients, protection from deadly diseases such as pneumonia and fostering growth and development for the first time in 1991.\n\nWorld Breastfeeding Week was first celebrated in 1992 by World Alliance for Breastfeeding Action (WABA) and is now observed in over 120 countries by UNICEF, WHO and their partners including individuals, organizations, and governments. WABA itself have been formed on 14 February 1991 with the goal to re-establish a global breastfeeding culture and provide support for breastfeeding everywhere.\n\nThe World Health Organization (WHO) and the American Academy of Pediatrics (AAP) emphasize the value of breastfeeding for mothers as well as children. Both recommend exclusive breastfeeding for the first six months of life and then supplemented breastfeeding for at least one year and up to two years or more. WBW commemorates the Innocenti Declaration made by WHO and UNICEF in August 1990 to protect and support breastfeeding.\n\n"}
