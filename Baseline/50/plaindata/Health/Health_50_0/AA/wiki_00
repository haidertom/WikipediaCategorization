{"id": "38493462", "url": "https://en.wikipedia.org/wiki?curid=38493462", "title": "Adoption of Electronic Medical Records in U.S. Hospitals", "text": "Adoption of Electronic Medical Records in U.S. Hospitals\n\nThe move to electronic medical records (EMRs) is becoming increasingly prevalent in health care delivery systems in the United States.\n\nDue to the enactment of the American Recovery and Reinvestment Act of 2009, there has been a rise in the number of federal investments in programs that increase EMR adoption. The Health Information Technology for Economic and Clinical Health Act portion of this stimulus law provides payments for providers that show they have reached the standard for “meaningful use”. This has led more hospitals to adopt EMR, though they have had different experiences in adopting electronic medical records.\n\nThere are several steps that need to be taken in order to adopt electronic medical records. \nHospitals have been using different suppliers of health data systems in order to adopt electronic medical records. The key suppliers of health data systems are Epic Systems, Allscripts, Meditech, Cerner, IBM, McKesson, Siemens, Healthland, CPSI, and GE Healthcare.\n\nThe decision of choosing an EMR vendor like Epic or Meditech can fall on either hospital leadership or the corporate level based on the size of the system. If an organization only has one hospital, the hospital leaders select the vendor. If the system is larger, the decision is made at the corporate level, though several people from the hospitals remain involved. There are several criteria for deciding the vendor. Sentara Healthcare was able to make their selection by looking for a vendor with high outpatient care integration, technical support at all levels of integration, and one that customized a system to their needs.\n\nThe length of time it takes to implement electronic medical records can vary but usually takes two to three years. The first stage of implementation is called “design, build, validate,” where the vendor is selected and the plans are put in place. This is followed by a “big bang” implementation, which means all the departments transfer to the system at once, due to the interconnectedness of hospitals this is necessary.\n\nThough the adoption of electronic medical records is increasing, there is a range in the level of implementation. The HIMSS Analytics Database shows the eight stages of adoption in their United States EMR Adoption Model. Stage 0 implies “All three ancillaries not installed,” while in Stage 7 there is a shift to complete EMR. The EMR Adoption Model shows that in 2011, the number of acute care hospitals achieving Stage 5 or Stage 6 increased by more than 80 percent. Meanwhile, the number of hospitals at Stages 0, 1, 2, and 3 has decreased. Furthermore, adoption has increased most among large hospitals and those in urban areas.\n\nEMR implementation experiences among hospitals and healthcare delivery systems vary. Some systems have successful experiences, while others do not have as seamless of a transition. For example, in 2002, Cedars-Sinai Medical Center in Los Angeles, CA attempted to implement a new EMR system, but the US$34 million system failed due to numerous factors. The physicians were unhappy because of the new physician order entry system for medications, labs and procedures was more time consuming than doing the orders by hand. Physicians often found themselves spending extra time to avoid the system's warnings because there was not room for flexibility within the EMR system. Not only was the new system more time consuming, but it also alerted physicians with numerous electronic reminders and alerts that the physicians felt were excessive. The system was implemented with numerous decision support mechanisms, which created alerts that could not be overridden by physicians. This flaw was a result of not obtaining adequate physician input for the EMR system and not enough prior testing. These problems then made it very difficult to obtain buy-in from the physicians, which ultimately caused several hundred physicians employed at Cedars to refuse to use the system after only 3 months.\n\nThe failure was not only due to technology design and inefficiencies, but also to poor training and implementation. The hospital had implemented the system with very little prior testing, and did not gradually phase in the system but rather implemented it all at once. There was also not full buy-in from the staff at all levels of the organization, and insufficient training to ensure that the staff was ready for implementation. The example of failed EMR implementation at Cedars shows the complexities that health systems face in EMR implementation. However, there are numerous positive EMR experiences as well.\n\nBoth Geisinger Health System and New York-Presbyterian Hospital have been cited as having successful implementations of EMR systems. Both of these systems utilized key strategies that ultimately led to successful implementation. To create success, both Geisinger and NY Presbyterian utilized clinical teams to develop the EMR system, and therefore gained staff and physician buy-in. Another strategy used to reach successful implementation is for the organization to focus on training. At Geisinger, for example, the hospital provided “shadowers” for physicians and nurses throughout the day during the implementation phase. Then, as physicians and nurses gradually become proficient with the new system, the number of trainers needed gradually decreased. Both hospitals also clearly emphasized that training was mandatory for all staff and that all staff must pass a proficiency test before they can access the EMR system. Another strategy for successful EMR implementation used by these two hospitals is to use the EMR system for performance improvement through standardized care protocols. To successfully do this, NY Presbyterian gradually utilized more hard stops within their protocols over time to ensure that protocols were being followed. However, these hard stops were created collaboratively to achieve physician buy-in and ensure that they were appropriate for care delivery. Allowing tailoring of the EMR system is essential to both before implementation and afterwards, and physicians and clinical staff must be used in this development process to create success.\n\nCurrently, there has been little research and evaluation on the post-implementation of commercial electronic medical records (EMR). MetroHealth Medical Center in Cleveland, OH, has published an evaluation of the EpicCare EMR by Epic Systems five years after implementation. MetroHealth’s methods included a web-based survey for primary care providers to measure their level of utilization of the EpicCare EMR. The physicians surveyed were those in the internal medicine, family medicine, and the pediatric units of the hospital. Prior to the survey, physicians were given eight hours of training by consultants on how to navigate the EMR system. The survey subsequently taken showed a response rate of 35% or 59 of 154 providers. Questions included whether providers have a computer at home, whether they access EpicCare from home, and their level of expertise on using a computer. The results showed that 97% had a computer at home, while 53% accessed EpicCare from home, and 11% were computer novice, 76% intermediate, and 13% were advance users. \n\nThe survey showed that in terms of the overall level of satisfaction with the EMR, 6% of respondents felt that EpicCare was inferior to a paper-based system, 81% felt that it was superior, while 13% were indifferent. In the area of training, 46% of respondents felt that their training was inadequate, and 75% would like to receive additional training. The implementation of the EpicCare EMR has impacted the way physicians treated their patients as evidenced by 12% of respondents reporting that they had changed medications based on the medication alerts and 15% reporting that they followed the EMR’s best practices alerts. The survey results revealed several areas that can benefit from improvement, such as ongoing training and support as well as the optimal utilization of all EMR functions.\n\nJKL Healthcare System, a nonprofit organization that employs a 450-employee physician group and operates three acute care hospitals, implemented an EMR system in 2001. The cost of \"Epic\", the new system, was $35 million. Their goal is for every physician in their organization to adapt the EMR at a 100% utilization rate in order to increase efficiency and reduce adverse patient outcomes. 450 physicians were required to attend a 16-hour training session in order to obtain a certification that would allow them to admit patients. 1,700 Non-physician employees were required to attend the same training to receive their certification as well.\n\nA physician survey was conducted two months after implementation which showed that out of 73 respondents, 90% of physicians felt that the Epic system made their jobs easier and more efficient. Admissions for the hospitals have risen since implementation as well as physician support, which indicated that the EMR has been implemented successfully. During the May – August 2004 period of post implementation, outpatient visits increased by 3%, medication errors were eliminated completely, and patient satisfaction surveys showed that overall satisfaction increased to their highest level since 2000. EMR utilization has saved JKL Healthcare approximately $50,000 on office supplies and has significantly reduced the number of medical records staff.\n\nThe JKL Healthcare System was one of the first successful implementers of EMR using the Epic System, and they became a model site for other healthcare systems. JKL Healthcare received the Davies Award in September 2004, which is the most prestigious award in the IT industry for its implementation of the most comprehensive EMR system in the US.\n"}
{"id": "52157367", "url": "https://en.wikipedia.org/wiki?curid=52157367", "title": "Asbestos and the law (United States)", "text": "Asbestos and the law (United States)\n\nWithin the United States, the use of asbestos is limited by state and federal regulations and legislation. Improper use of asbestos and injury from exposure is addressed through administrative action, litigation, and criminal prosecution. Injury claims arising from asbestos exposure may be tried as mass torts.\n\nAsbestos litigation is the longest, most expensive mass tort in U.S. history, involving more than 8,000 defendants and 700,000 claimants. By the early 1990s, \"more than half of the 25 largest asbestos manufacturers in the US, including Amatex, Carey-Canada, Celotex, Eagle-Picher, Forty-Eight Insulations, Manville Corporation, National Gypsum, Standard Insulation, Unarco, and UNR Industries had declared bankruptcy. Analysts have estimated that the total costs of asbestos litigation in the U.S. alone will eventually reach $200 to $275 billion. The amounts and method of allocating compensation have been the source of many court cases, and government attempts at resolution of existing and future cases.\n\nClaims made against employers by injured workers will typically be in the form of a workers compensation claim, although the long onset for diseases such as mesothelioma may make it impossible for a worker to pursue workers' compensation benefits. However, it is possible for an injured worker to also bring a product liability claim against a third party that is responsible for introducing asbestos into the workplace. Asbestos lawsuits in the U.S. have included the following as defendants:\n\nManufacturers of machinery in which asbestos-containing parts were used have contested liability on the grounds that nearly all of them either did not ship asbestos-containing parts with their products at all (that is, asbestos was installed only by end users) or did not sell replacement parts for their own products (in cases where the plaintiff was allegedly exposed well after any factory-original asbestos-containing parts would have been replaced), and either way cannot be responsible for toxic third-party parts that they did not manufacture, distribute, or sell. In 2008, the Washington Supreme Court, the first state supreme court to reach the issue, decided in favor of the defense. On January 12, 2012, the Supreme Court of California also decided in favor of the defense in \"O'Neil v. Crane Co.\"\n\nAnother area of dispute remains the so-called chrysotile-defense. Manufacturers of some products containing only chrysotile fibers claim that these are not as harmful as amphibole-containing products. As 95% of the products used in the United States historically were mostly chrysotile, this claim is widely disputed by health officials and medical professionals. The World Health Organization recognizes that exposure to all types of asbestos fibers, including chrysotile, can cause cancer of the lung, larynx, and ovary, mesothelioma, and asbestosis.\n\nDefendants in asbestos litigation have accused the lawyers who represent plaintiffs of unethical conduct, but those allegations have not been successful in stopping the litigation, nor have courts been sufficiently convinced by the allegations to sanction the law firms against whom the allegations have been raised.\n\n\nSince the bankruptcy filing of Johns-Manville in 1984, many U.S. and U.K. asbestos manufacturers have avoided litigation by filing bankruptcy. Asbestos bankruptcy trusts are personal injury trusts established by firms that have filed for reorganization under Chapter 11 of the United States Bankruptcy Code to pay personal injury claims caused by exposure to asbestos. At least fifty-six trusts were established from the mid-1970s through 2011. The largest 26 of these trusts paid about 2.4 million claims totaling about $10.9 billion up to 2008. The trusts are governed by trust advisory committees that are generally controlled by lawyers from a few prominent law firms such as Baron & Budd, P.C. and Weitz & Luxenberg P.C.\n\nBankruptcy trusts may pay pennies on the dollar to people injured as a result of asbestos exposure. At the same time, these trusts may permit larger numbers of claimants to receive some kind of compensation, even if greatly reduced from potential recoveries in the tort system.\n\nThe federal Medicare Secondary Payer law imposes penalties for paying settlements directly to claimants without repaying the government for medical costs covered under the same programs under the legal doctrine of subrogation. In late 2016, attorneys general from 13 states sent demand letters to bankruptcy trusts for Armstrong World Industries, Babcock & Wilcox, DII, and Owens Corning. The purpose of the demand letters was to determine if the funds are reimbursing states for medical treatment received under Medicaid and Medicare.\n\nThe pursuit of compensation for asbestos injuries often involves both litigation against solvent defendants and filing claims against asbestos bankruptcy trusts.\n\nThe amount of compensation recovered by an injured plaintiff may depend on whether evidence of exposure to products from bankrupt firms is introduced at trial. If no evidence of exposure from bankrupt firms is presented then increased financial responsibility is likely to be assigned to solvent defendants. Researchers from RAND Corporation found that if a company filed for bankruptcy plaintiffs claimed exposure to their products in interrogatories and depositions at significantly reduced rates.\n\nWhen a plaintiff claiming an asbestos injury has filed a claim against a solvent defendant, courts may extend or reopen discovery when it is discovered that the plaintiff failed to disclose a trust claims. For example, in the 2008 case of \"Edwards v. John Crane-Houdaille, Inc\" production of claim forms was delayed until two weeks before trial. In the 2004 case of \"Stoeckler v. American Oil Co.\" the defendants discovered that the plaintiff did not disclose trust claims only three days after the start of the trial, resulting in the re-opening of discovery. To help avoid this type of issue, Judges will often adopt mandatory disclosure obligations for bankruptcy trust claims.\n\nAccording to a September 2004 of the \"American Journal of Respiratory and Critical Care Medicine\", asbestos is still a hazard for 1.3 million US workers in the construction industry and for workers involved in the maintenance of buildings and equipment.\n\nAsbestos is not part of an ASTM (American Society for Testing and Materials) E 1527-05 Phase I Environmental Site Assessment (ESA). A building survey for asbestos is considered an out-of-scope consideration under the industry standard ASTM 1527-05 Phase I ESA (see ASTM E 1527-05). ASTM Standard E 2356-04 should be consulted by the owner or owner's agent to determine which type of asbestos building survey is appropriate, typically either a baseline survey or a design survey of functional areas. Both types of surveys are explained in detail under ASTM Standard E 2356-04. Typically, a baseline survey is performed by an EPA (or state) licensed asbestos inspector. The baseline survey provides the buyer with sufficient information on presumed asbestos at the facility, often which leads to reduction in the assessed value of the building (due primarily to forthcoming abatement costs). Note: EPA NESHAP (National Emissions Standards for Hazardous Air Pollutants) and OSHA (Occupational Safety and Health Administration) Regulations must be consulted in addition to ASTM Standard E 2356-04 to ensure all statutory requirements are satisfied, ex. notification requirements for renovation/demolition. Asbestos is not a material covered under CERCLA (Comprehensive Environmental Response, Compensation, and Liability Act ) innocent purchaser defense. In some instances, the U.S. EPA includes asbestos contaminated facilities on the NPL (Superfund). Buyers should be careful not to purchase facilities, even with an ASTM E 1527-05 Phase I ESA completed, without a full understanding of all the hazards in a building or at a property, without evaluating non-scope ASTM E 1527-05 materials, such as asbestos, lead, PCBs, mercury, radon, et al. A standard ASTM E 1527-05 does not include asbestos surveys as standard practice.\n\nIn 1988, the United States Environmental Protection Agency (USEPA) issued regulations requiring certain U.S. companies to report the asbestos used in their products.\n\nA Senate subcommittee of the Health Education Labor and Pensions Committee heard testimony on July 31, 2001, regarding the health effects of asbestos. Members of the public, doctors, and scientists called for the United States to join other countries in a ban on the product.\n\nSeveral legislative remedies have been considered by the U.S. Congress but each time rejected for a variety of reasons. In 2005, Congress considered but did not pass legislation entitled the \"Fairness in Asbestos Injury Resolution Act of 2005\". The act would have established a $140 billion trust fund in lieu of litigation, but as it would have proactively taken funds held in reserve by bankruptcy trusts, manufacturers and insurance companies, it was not widely supported either by victims or corporations.\n\nOn April 26, 2005, Dr. Philip J. Landrigan, professor and chair of the Department of Community and Preventive Medicine at Mount Sinai Medical Center in New York City, testified before the US Senate Committee on the Judiciary against this proposed legislation. He testified that many of the bill's provisions were unsupported by medicine and would unfairly exclude a large number of people who had become ill or died from asbestos: \"The approach to the diagnosis of disease caused by asbestos that is set forth in this bill is not consistent with the diagnostic criteria established by the American Thoracic Society. If the bill is to deliver on its promise of fairness, these criteria will need to be revised.\" Also opposing the bill were the American Public Health Association and the Asbestos Workers' Union.\n\nOn June 14, 2006, the Senate Judiciary Committee approved an amendment to the act which would have allowed victims of mesothelioma $1.1M within 30 days of their claim's approval. This version would have also expanded eligible claimants to people exposed to asbestos from the September 11, 2001 attacks on the World Trade Center, and to construction debris in hurricanes Katrina and Rita. Ultimately, the bill's reliance on funding from private entities large and small, as well as debate over a sunset provision and the impact on the U.S. budgetary process caused the bill to fail to leave committee.\nAccording to the Environmental Working Group Action Fund, 10,000 people die each year from asbestos-caused diseases in the United States, including one out of every 125 American men who die over the age of 50. The Environmental Protection Agency (EPA) has no general ban on the use of asbestos. However, asbestos was one of the first hazardous air pollutants regulated under Section 112 of the Clean Air Act of 1970, and many applications have been forbidden by the Toxic Substances Control Act (TSCA).\n\nIn 2010, Washington state passed a ban on hazardous materials in automotive brakes, phasing out asbestos in vehicle brakes, starting in 2014.\n\nIn 2013, Ohio passed became the first state to pass a law requiring transparency in asbestos bankruptcy trust claims. The same year Oklahoma passed a similar law called The Personal Injury Trust Fund Transparency Act. This law applies to all personal injury trusts. It requires plaintiffs to disclose all previously filed and anticipated trust claims for personal injuries within 90 days of filing a personal injury tort but not until at least 180 days before the assigned court date. If the plaintiff anticipates filing a trust claim all proceedings are stayed until their filing is complete. Filing new claims or amending claims after the initial disclosure triggers a new disclosure requirement. The law also allows defendants to stay proceedings by showing that the plaintiff could make a good faith filing with a trust. The law gives plaintiffs ten days to either file such a claim or show that it would probably be unsuccessful.\n\nIn South Carolina in 2015, State Senator Shane Massey introduced Senate Bill 281, \"The Court Transparency Act.\" S.281 would prohibit the state of South Carolina from hiring outside lawyers. Similar bills have been passed into law by 18 states. The bill would also prevent juries from awarding damages that exceed actual out of pocket costs incurred by plaintiffs.\n\nIn June 2015, Texas Governor Greg Abbott, a Republican, signed Texas House Bill 1492 into law. The law was written to end so-called asbestos \"double dipping\" in Texas. This law requires asbestos victims to perform more actions before proceeding to trial, and lowers the standard of proof of asbestos exposure for manufacturers to shift the blame onto other bankrupt companies A year earlier, Wisconsin Governor Scott Walker signed a similar bill into law.\n\nIn June 2016, President Obama signed into law the Frank R. Lautenburg Chemical Safety for the 21st Century Act (H.R. 2576). It serves to reform the TSCA of 1976 and aims to make federal safety regulations on toxic substances and chemicals effective.\n\nIn 2017, Iowa, Mississippi, North Dakota, and South Dakota all passed asbestos trust claims transparency laws.\n\nAsbestos abatement (removal of asbestos) has become a thriving industry in the United States. Strict removal and disposal laws have been enacted to protect the public from airborne asbestos. The Clean Air Act requires that asbestos be wetted during removal and strictly contained, and that workers wear safety gear and masks. The federal government has prosecuted dozens of violations of the act and violations of Racketeer Influenced and Corrupt Organizations Act (RICO) related to the operations. Often these involve contractors who hire undocumented workers without proper training or protection to illegally remove asbestos.\n\nW. R. Grace and Company faces fines of up to $280 million for polluting the town of Libby, Montana. Libby was declared a Superfund disaster area in 2002, and the EPA has spent $54 million in cleanup. Grace was ordered by a court to reimburse the EPA for cleanup costs, but the bankruptcy court must approve any payments.\n\nThe U.S. Supreme Court has dealt with several asbestos-related cases since 1986. Two large class action settlements, designed to limit liability, came before the court in 1997 and 1999. Both settlements were ultimately rejected by the court because they would exclude future claimants, or those who later developed asbestos-related illnesses. These rulings addressed the 20-50 year latency period of serious asbestos-related illnesses.\n\nIn this case a federal appeals court ruled that an insulation installer from Texas could sue asbestos manufactures for failure to warn. Borel's lawyers argued that had warning labels been affixed to Fiberboard's products he would have been able to protect himself more effectively.\n\nThe Manville Corporation, formerly the Johns-Manville Corporation, filed for reorganization and protection under the United States Bankruptcy Code in August 1982. At the time, it was the largest company ever to file bankruptcy, and was one of the richest. Manville was then 181st on the Fortune 500, but was the defendant of 16,500 lawsuits related to the health effects of asbestos. The company was described by Ron Motley, a South Carolina attorney, as \"the greatest corporate mass murderer in history.\" Court documents show that the corporation had a long history of hiding evidence of the ill effects of asbestos from its workers and the public. One of many examples is a memo from Manville's medical director to corporate headquarters:\n\nIn a decision from January 2014, \"Gray v. Garlock Sealing Technologies\" had entered into bankruptcy proceedings, and discovery in the case uncovered evidence of fraud that led to a reduction in estimated future liability to a tenth of what was estimated.\n\nA number of lawsuits have been filed under the Racketeer Influenced and Corrupt Organizations Act (RICO) in response to what defendants claim to be fraudulent asbestos-related lawsuits. RICO suits are civil in nature and brought by private parties. They typically allege that the suits themselves are forms of racketeering or that lawyers and experts had to engage in racketeering activities in order to bring them.\n\nFor example,\n\nSome companies and their executives have faced criminal prosecution for their actions in exposing workers to the dangers of asbestos, or their improper handling of asbestos waste.\n\nOn February 20, 1973 a federal grand jury in Detroit, Michigan indicted Adamo Wrecking Company (\"Adamo\") for violating provisions of the Clean Air Act by knowingly causing the emission of asbestos by failure to wet and remove friable asbestos materials from demolitions.\n\nAdamo was one of a number of demolition contractors indicted throughout the country for the alleged violation of the Clean Air Act. The United States District Court for the Eastern District of Michigan dismissed the criminal indictment on the ground that it was not an \"emission standard,\" but a \"work practice standard,\" which under the terms of the statute, did not carry criminal liability.\n\nThe government appealed and the Sixth Circuit Court of Appeals reversed the decision of the trial court, stating that it erred in determining that it had jurisdiction to review the validity of the standard in a criminal proceeding. Adamo's attorneys appealed to the Supreme Court.\n\nOn January 10, 1978, the Supreme Court ruled in favor of Adamo when it held that the trial court did have jurisdiction to review the standard in a criminal proceeding and also agreed with the trial court that the requirements in the act were \"not standards\" but \"procedures\" and therefore the proceedings were properly dismissed.\n\nA federal grand jury indicted W. R. Grace and Company and seven top executives on February 5, 2005, for its operations of a vermiculite mine in Libby, Montana. The indictment accused Grace of wire fraud, knowing endangerment of residents by concealing air monitoring results, obstruction of justice by interfering with an Environmental Protection Agency (EPA) investigation, violation of the Clean Air Act, providing asbestos materials to schools and local residents, and conspiracy to release asbestos and cover up health problems from asbestos contamination. The Department of Justice said 1,200 residents had developed asbestos-related diseases and some had died, and there could be many more injuries and deaths.\n\nOn June 8, 2006, a federal judge dismissed the conspiracy charge of \"knowing endangerment\" because some of the defendant officials had left the company before the five-year statute of limitations had begun to run. The wire fraud charge was dropped by prosecutors in March.\n\nOn April 2, 1998, three men were indicted in a conspiracy to use homeless men for illegal asbestos removal from an aging Wisconsin manufacturing plant. Then-US Attorney General Janet Reno said, \"Knowingly removing asbestos improperly is criminal. Exploiting the homeless to do this work is cruel.\"\n\nOn December 12, 2004, owners of New York asbestos abatement companies were sentenced to the longest federal jail sentences for environmental crimes in U.S. history, after they were convicted on 18 counts of conspiracy to violate the Clean Air Act and the Toxic Substances Control Act, and actual violations of the Clean Air Act and Racketeer-Influenced and Corrupt Organizations Act. The crimes involved a 10-year scheme to illegally remove asbestos. The RICO counts included obstruction of justice, money laundering, mail fraud and bid rigging, all related to the asbestos cleanup.\n\nOn January 11, 2006, San Diego Gas & Electric Co., two of its employees, and a contractor were indicted by a federal grand jury on charges that they violated safety standards while removing asbestos from pipes in Lemon Grove, California. The defendants were charged with five counts of conspiracy, violating asbestos work practice standards and making false statements.\n"}
{"id": "57944211", "url": "https://en.wikipedia.org/wiki?curid=57944211", "title": "Belgian Health Care Knowledge Centre", "text": "Belgian Health Care Knowledge Centre\n\nThe Belgian Health Care Knowledge Centre (abbreviated to KCE) is an independent federal research institute in Belgium that provides multidisciplinary scientific advice to relevant persons and authorities on topics related to health care. More specifically, the KCE carries out research on the organisation and financing of the health system (health services research), assesses health technologies and drugs (health technology assessment), establishes guidelines for clinical practice and updates them according to the latest scientific evidence (good clinical practices), and establishes validated methods for research in the field of health care and public health. Aside from producing scientific reports on these topics, the KCE also maintains a clinical research programme in which it selects and funds non-commercial clinical trials. Lastly, the KCE critically reviews, analyses and summarizes publications from foreign research institutes and peer reviewed journals that could be of interest to Belgian clinicians and authorities.\nThe KCE is a so-called 'organism of public interest' (parastatal body) type B according to Belgian public law. It was founded in 2003 and operates under the administrative authority of the federal minister of Public Health of Belgium. Its annual budget is approximately 10,000,000 euros and originates for about 75% from the National Institute for Health and Disability Insurance (NIHDI) and for about 25% from the Federal Public Service Health and the Federal Public Service Social Security. Additionally, the KCE disposes over a budget of approximately 10,000,000 euros for its clinical trials programme which originates fully from the NIHDI. It also receives European subsidies for its participation in certain international projects. As of 2017, it has an in-house research staff of about 45 researchers, including clinicians, health economists, human scientists, data analysts and statisticians. The KCE can also rely on a network of external experts and institutions, such as universities, to carry out studies and research through public tenders and provide external validation of its reports.\n\n"}
{"id": "24707867", "url": "https://en.wikipedia.org/wiki?curid=24707867", "title": "Canadian health claims for food", "text": "Canadian health claims for food\n\nCanadian health claims by Health Canada, the department of the Government of Canada responsible for national health, has allowed five scientifically verified disease risk reduction claims to be used on food labels and on food advertising. Other countries, including the United States and Great Britain, have approved similar health claims on food labels.\n\nThe Food Directorate of Health Canada is responsible for the development of policies, regulations and standards that relate to the use of health claims on foods. They assess whether health claims are truthful and not misleading by reviewing mandatory and voluntary pre-market submissions. Health Claims are regulated under the Food and Drugs Act and the Food and Drug Regulations. The Section 5(1) of the Food and Drugs Act requires that all health claims be truthful and not misleading or deceptive. The regulatory requirements permitting the use of claims vary significantly depending on the nature and type of the claim. Some claims may be made without pre-market approval provided they are truthful and not misleading or deceptive, whereas other claims, such as disease risk reduction or therapeutic claims are only allowed once a regulatory amendment specifying the conditions for their use has been completed \n\nManufacturers are responsible for the accuracy of all information on the labels and advertisements for food and for compliance with all relevant food legislation and policies, including those pertaining to health claims. The Canadian Food Inspection Agency is responsible for ensuring that industry complies with these requirements \n\nFood claims express the composition, quality, quantity or origin of a food product. Examples of Food Claims are \"made in Canada\" \"Home-style Chilli\" and \"Fresh Pasta.\"\n\nHealth claims is any representation in labeling or advertising that states, suggests, or implies that a relationship exists between consumption of a food or an ingredient in the food and a person's health.\n\nThere are two categories of health claims: General Health Claims and Specific Health Claims.\n\nGeneral health Claims: These claims are broad general claims that promote health through healthy eating or that provide dietary guidance. These claims do not refer to a specific or general health effect, disease, or health condition.\n\nExample: The claim \"Include low fat product \"x\" as part of healthy eating\" \"healthy for you\" or \"healthy choice\"\n\nDisease risk reduction and therapeutic claims:\n\nSince 2003, Health Canada has allowed certain disease risk reduction claims to be used on food labels or in advertisements. These claims are used to describe the link between the characteristics of a diet, a food or food constituent and the risk reduction of a disease or the therapeutic effect of a food or food constituent or diet, (including restoring, correcting, or modifying body functions). Example: The claim \"(naming the diet characteristics, food or food constituent) reduces the risk of heart disease\" or \"lowers blood cholesterol\" can be used when the food carrying the claim meets conditions for use set out in the food regulations.\n\nFunction claims:\n\nThese claims are used to describe the specific physiological effects of foods and food constituents associated with health or performance. Example: The claim \"(naming the food or food constituent) promotes regularity or laxation\" can be used for coarse wheat bran providing a minimum of 7 grams of dietary fibre in a reasonable daily intake of the food.\n\nNutrient function claims (formerly known as biological role claims), are a type of function claim that describe the well-established functions of nutrients or energy necessary for the maintenance of good health, normal growth and development\nExample: The claim \"Calcium aids in the formation and maintenance of bones and teeth\" may be used for foods providing a minimum of 5% of the Recommended Daily Intake of the nutrient per serving of stated size and reference amount of the food.\n\nDietary sodium can be measured in two ways. Total dietary sodium measures the amount (in grams or milligrams) of sodium in the food. Sodium ratio refers to the amount of sodium per amount of food eaten, usually in grams per kilocalorie, or milligrams per kilocalorie.\n\nDietary potassium can be measured as the total amount of potassium in the diet (usually in milligrams). It can also be measured in relation to sodium intake as the potassium-sodium ratio, in mg K/mg Na, or mg K/g Na. Only this last measurement shows the effect of dietary potassium as part of the equation; the rest measure only sodium intake, which is less important overall than the combined effects of potassium and sodium.\n\nDietary Approaches to Stop Hypertension (DASH) is a dietary intervention designed to reduce blood pressure in patients with hypertension. It emphasizes fruits and vegetables, low-fat dairy products, whole grains, poultry, nuts and fish, and limits red meats, sweets and sweetened drinks. It has been found to reduce hypertension in patients even without weight loss or reduction of sodium intake. However, it is usually used in combination with a sodium-controlled diet.\n\nMost research has shown that reducing sodium intake reduces the risk of cardiovascular disease(CVD) and all-cause mortality rate.\n\nA study by Langford in 1983 observed that populations that consume less sodium also tend to have relatively high potassium intake, and a lower rate of CVD. Within the USA, he also noticed racial and class differences in CVD, which he suggests may be due to sodium being cheaper to acquire in the diet than potassium, since many products contain added salt. Reddy and Katan recommend a salt intake below 5 g/day (5000 mg/day), and to increase potassium intake by 2 - 3 g/day. The study emphasizes that this should be done through dietary changes, rather than by taking a dietary supplement containing potassium.\n\nHe \"et al.\" observed in 1999 that increased salt consumption had a direct relationship to increased risk of CVD and all-cause mortality in overweight people.\n\nCook \"et al.\" observed patients 10–15 years after they had received treatments for prehypertension. They found that reducing sodium intake by 1.2 g/day reduced the number of people needing anti-hypertension treatment by 50%. Subjects who had no hypertension when they began the study 10 – 15 years earlier were also 50% less likely to require treatment for hypertension.\n\nTheir results suggest that increased sodium intake can cause CVD independently of hypertension - that is, even if the patient continues to have normal blood pressure. High sodium intake is associated with increased vascular reactivity and growth, and myocardial fibrosis, which is associated with myocardial disarray. They also noticed a direct relationship between sodium intake and ventricular hypertrophy, an increase in the mass of the left ventricle of the heart. In lay terms, this means an enlargement of the heart chamber that pumps blood to body tissues, including the cardiac muscle itself. In order to reduce blood pressure, and prevent hypertension and CVD, Cook \"et al.\" recommend reducing sodium intake by 25 - 30% from current levels.\n\nIncreasing dietary potassium intake has been shown to have a significant effect on blood pressure in populations with high sodium intake. It is not apparent from the study performed by Khaw \"et al.\" whether there is a difference for populations consuming low amounts of sodium; however, this is not particularly relevant in Canada, where people are more likely to consume excess sodium.\n\nIn 2007, Joffres \"et al.\" reported that in the typical Canadian diet, 11% of sodium occurs naturally, 12% is added during cooking and at the table, and 77% is added by industry during processing. Their study was to determine whether regulations to limit the amount of salt added by food manufacturers could reduce the prevalence of hypertension by 30%, which would substantially reduce Canadian health care costs.\n\nSome Studies do not recommend that the general healthy population should reduce their sodium intake because they feel that there is no conclusive evidence that this will guarantee a lower incidence of hypertension. A major study with this recommendation is the National Health and Nutrition Examination Survey (NHANES I). They did observe that people with a lower-sodium diet had a lower all-cause mortality and cardiovascular disease (CVD) -related mortality rate. However, because they could not be certain that reducing sodium intake could reduce hypertension and CVD, and it was not just one factor in the lifestyles of people who naturally ate a lower-sodium diet, they did not feel a need to recommend that the general population should reduce sodium intake.\n\nThe Developers of the DASH diet do not recommend reducing dietary sodium, because they found that the overall DASH diet is effective even without reducing sodium intake. That is, an overall healthy diet is more important than only reducing sodium intake, even in regulating hypertension. A 1996 study by Midgley \"et al.\" recommends a reduction in sodium intake for older patients with hypertension, but does not support recommendations for the general population to reduce their sodium intake.\n\nAs mentioned above, sodium intake can have cardiovascular effects without hypertension. It is therefore prudent to reduce sodium intake even if blood pressure is normal. It is important to consume an overall healthy diet that essentially follows the Canada Food Guide. Reducing sodium intake to recommended levels can reduce the risk of future hypertension and cardiovascular problems, and reduction of sodium intake carries no inherent risk. Because most of the sodium we consume is added during processing, preparing healthy meals at home and adding salt during cooking, rather than purchasing ready meals and snacks, is an easy way to reduce sodium intake without compromising the flavour and texture that salt provides in food.\n\nMost studies to date have shown that reducing sodium intake and increasing potassium intake, to levels recommended in Canadian nutritional guidelines such as RDA, can reduce the risk of hypertension, cardiovascular disease, and all-cause mortality. Most dietary sodium in Canada is added by food manufacturers during processing, and regulations controlling the amount of salt that can be added by food manufacturers may help reduce the prevalence of these illnesses in Canada.\n\nIn order to make the health claim \"A healthy diet adequate in calcium and vitamin D, and regular physical activity, help to achieve strong bones and may reduce the risk of osteoporosis, the food must be:\n\n\nThe two main calcium centers in the body are found in bones and blood. Homeostatic controls of the body ensure that the blood maintains a constant proportion of Ca. If there is a decrease in serum calcium levels, the body responds by secreting Parathormone (PTH) from the parathyroid gland into the blood which;\n\n1)Increases re-absorption of Ca from the kidneys and gastrointestinal tract(GI)\n\n2)Releases calcium from bones into the blood\n\nVitamin D also plays a crucial role in maintaining serum calcium levels (Ca in the blood). Vitamin D stimulates absorption of calcium from the GI tract through its interaction with receptors in the enterocyte. Vitamin D also increases transcription of genes that code for Calbindin. As the name implies, Calbindin functions as a calcium-binding protein thereby enhancing calcium absorption. As a result of the body scavenging for calcium from bones; the bones may become fragile, brittle and weak, which in prolonged states can lead to osteoporosis and or bone fractures.\n\nConversely, if there is an adequate level of Ca in the blood, PTH will be inhibited. Similarly, in the case where there is an excess of Ca in the blood, it will be stored in bones as this is the nutritional reserve for Calcium and Phosphorus \n\nBone depends upon dietary intake to supply the bulk materials needed for synthesis of the extracellular material, which composes more than 95% of bone. These bulk materials are mainly calcium, phosphorus and protein. Roughly half the volume of the extracellular material of bone consists of protein and the other half of calcium phosphate crystals. It is self-evident that a growing organism cannot amass this structural material if the bulk components of bone are not present in adequate amounts in the diet. It is for this reason that bone growth is stunted during general malnutrition and specific bone abnormalities develop with deficiencies in protein, ascorbic acid, vitamin D, magnesium, zinc, copper and manganese to name only a few \n\nThere is a significant body of evidence which establishes that high calcium intakes augment bone gain during growth, retards age-related bone loss, and reduces osteoporotic fracture risk. A meta-analysis study in 2007 assessed whether calcium supplementation can reduce osteoporotic fractures. The meta-analysis included all the randomized trials in which calcium, or calcium in combination with vitamin D, was used to prevent fracture and osteoporotic bone loss.\n\nIn total, 63 897 individuals were analysed, most of whom were women (n=58 785 [92%]) with a mean age of 67.8 years (SD 9.7). In trials that reported fracture as an outcome (17 trials, n=52 625), treatment was associated with a 12% risk reduction in fractures of all types. In trials that reported bone-mineral density as an outcome (23 trials, n=41 419), the treatment was associated with a reduced rate of bone loss of 0.54% (0.35–0.73; p<0.0001) at the hip and 1.19% (0.76–1.61%; p<0.0001) in the spine. The fracture risk reduction was significantly greater (24%) in trials in which the compliance rate was high (p<0.0001). The treatment effect was better with calcium doses of 1200 mg or more than with doses less than 1200 mg (0.80 vs 0.94; p=0.006), and with vitamin D doses of 800 IU or more than with doses less than 800 IU (0·84 vs 0·87; p=0·03).\n\nEvidence supports the use of calcium, or calcium in combination with vitamin D supplementation, in the preventive treatment of osteoporosis in people aged 50 years or older. For best therapeutic effect, minimum doses of 1200 mg of calcium, and 800 IU of vitamin D (for combined calcium plus vitamin D supplementation is recommended.\n\nCurrent recommendations for Calcium are:\n\nThe Upper Tolerable intake is 2.5 g/day \n\nThe Canadian health claim \"A healthy diet low in saturated and trans fat and reduced risk of heart disease\" is commonly accepted and correlated. Saturated fatty acids do not contain any carbon-to-carbon double bonds in the fatty acid chain. Trans fatty acids contain carbon-to-carbon double bonds in the trans confirmation.\n\nDuring the past several decades, reduction of fat intake has been one of the main focuses from a dietary perspective. During these decades, fats have been progressively gaining a greater correlation with health complications especially heart disease. Subsequently, the food industry had been taking notice of this, more labels like \"fat-free\" or \"low-fat\" appeared on food packaging.\n\n80,082 women who were between 34 and 59 years of age having no known stroke, cancer, coronary heart disease, hypercholesterolemia, or diabetes in 1980 were studied regarding dietary fat intake and its correlation with coronary heart disease. They found that with each 5% increase of energy intake from saturated fat with the same energy intake from carbohydrates was associated with a 17% increase in the risk of coronary heart disease.\n\nA related study found that the types of fat consumed is much more important than the amount of fat which is consumed with regards to coronary heart disease. Controlled clinical trials have proved that replacing saturated fat with polyunsaturated fat was much more effective in lowering serum cholesterol and reducing risk of CHD compared to reducing total fat consumption.\n\nWhen compared to saturated fatty acids, trans-fatty acids have a much greater ability to turn High-density lipoprotein (HDL) into Low-density lipoprotein (LDL). HDL commonly referred to as a \"good cholesterol\" due to its ability to remove cholesterol from clogs in arteries. LDL subsequently is known as a \"bad cholesterol\" since high levels of it is usually an indication of heart disease. The loss of HDL and creation of LDL is highly likely to lead to CHD complications.\n\nDespite the fact that trans-fatty acids are not saturated, does not correlate to all unsaturated fats lead to CHD. Omega-3 fatty acids are polyunsaturated cis-fatty acids which are an essential fatty acid which the body cannot produce. They are found to prevent and manage cardiovascular disease in clinical interventions. They do not change serum lipid concentrations but reduce blood clotting in vessel walls.\n\n\"Canada's Food Guide\" recommends 30–45 mL consumption of unsaturated fat each day to get the fat you require for the day. This amount includes oil used for cooking, salad dressings, margarine and mayonnaise.\n\nExamples of unsaturated vegetable oils they included were:\n\n\"Canada's Food Guide\" also recommends limiting the consumption of food and beverages high in saturated and trans fat.\n\nScientific literature shows that consuming a \"variety\" of fruits and vegetables is linked with reduction of some cancers. There is insufficient evidence to support any one fruit/vegetable or food constituent with a reduction of cancer occurrence. Fruits and vegetables have a wide range of nutrients and phytochemicals, thus to achieve optimal nutrient levels a variety is recommended. \"Some\" types of cancer signifies that not all types of cancers are diet related and thus not all types of cancer can be reduced by a dietary change. The statement \"may help\" eliminates consumer confusion that diet is the only factors in reducing risk of some types of cancers. The wording of this disease reduction health claim cannot be modified in any way. However, words, numbers or signs can be added before or after the claim, provided that they do not change the nature of the claim.\n\nThis claim is only allowed on certain fruits and vegetables listed below:\n\n\nThis claim is not allowed on some fruits and vegetables:\n\n\n<nowiki>*Mature seeds of legumes such as kidney beans are excluded from this claim, however young and immature pods of legumes such as edible podded peas. Additionally, the food cannot contain 0.5% alcohol or less.The claim is not extended to those foods because there is insufficient scientific evidence about any beneficial properties.</nowiki>\n\nFruits and vegetables may reduce the risk of some types of cancer due to:\n\n1)Beneficial properties of the micronutrients in fruits and vegetables such as vitamins, minerals, antioxidants\n2)Consumption of fruits and vegetables may decrease the consumption of other less healthy alternatives\n\nAccording to Statistics Canada during 2008 Canadians ate an average of 79.5 kg vegetables per person, about 4 kg lower than in 2005 when Canadians consumed 83.5 kg per person. Potatoes were the most consumed vegetable with 44%, followed by carrots, lettuce onions and tomatoes which marked a cumulative 29% of all vegetables consumed. During 2008 fruit consumption in Canada increased to a record high 47.5 kg per person. Fresh fruit consumption remains the same as in previous years; however, processed fruit consumption increased by 7%. Berries such as strawberries, cranberries, blackberries and blueberries have had a substantial consumption increase to 5 kg by Canadians in 2008. Oranges also remain a major part of fruit consumption at 4.9 kg per person.\n\nThe recommended number of daily servings in healthy adults aged 19–50 is 7-8 servings for women and 8-10 servings for men. Consumption of a \"variety\" of fruits and vegetables is emphasized as well as consumption of whole foods rather than their juices. Also recommended is to have at least one dark green and one orange vegetable per day. Moreover, the Canada Food Guide suggests that Canadians should consume fruits and vegetables with little or no added salt, sugar or fat.\n\nScientific studies have found a relationship between some cancers and fruit and vegetable intake and are the basis for making it one of Canada's five Health Claims. Some studies have looked at overall intake of fruits and vegetables and its relationship with certain types of cancer. Other studies have looked at specific nutrients found in certain fruits and vegetables, such as Vitamins, Minerals and antioxidants and their relationship with cancer. A review conducted by the World Cancer Research Fund and the American Institute for Cancer Research has concluded that there is a considerable amount of convincing evidence to support the claim suggesting a protective effect of fruits and vegetables against some cancers.\n\nA diet rich in fruits and vegetables are found to have a protective effect and reduce the occurrence of breast cancer. An analysis of 12-case control studies has been conducted in Oxford and found that fruits and vegetables have a consistent protective effect against breast cancer. In particular scholars found that Vitamin C intake had the most statistically significant inverse association with breast cancer. The study concludes that if this relationship represents causality then it is estimated that breast cancer might be prevented in 24% of postmenopausal women and 16% of pre-menopausal women.\n\nThere is an established relationship between cancer of the upper aero-digestive tract (oral cavity, pharynx, larynx and esophagus) and fruit and vegetable consumption. An EPIC (European Prospective Investigation into Cancer and Nutrition) study was conducted during 1992-1998 in which 345,904 people were studied using a dietary questionnaire. During 1998 data was collected and an inverse association was found between fruit and vegetable intake and upper aero-digestive tract cancer occurrence. The large study recommends an increase of fruit and vegetable consumption in order to reduce the risk of cancers of the upper aero-digestive tract.\n\nSome fruits and vegetables contain antioxidants which are also linked with cancer reduction risks. Micronutrient antioxidants (Vitamin E, Vitamin C, Vitamin A, B-carotene, lycopene) neutralize free radicals in the body and thus prevent cell damage and oxidative damage to DNA. The Journal of Internal medicine published a paper in 2007 which reviews 41 studies conducted concerning Vitamin C and Vitamin E and their role in cancer prevention and treatment. Thirty eight studies did show statistically insufficient beneficial effects of Vitamin E and C on cancer patients. Three of the studies show that there are statistically significant beneficial results. Overall, the systemic review of literature does not support the hypothesis that increased intake of Vitamin E and C (in the form of supplements) can help prevent or treat cancer and that more studies need to be done.\n\nA study published in the Journal of the National Cancer Institute was done on 1300 prostate cancer patients. Overall the risk of prostate cancer was unaffected by receiving dietary supplemental antioxidants. However, the results in smokers show that an increase in Vitamin E and B-carotene are statistically significant and are associated with reduced risk of the disease.\n\nLycopene is a strong antioxidant found in large quantities in tomatoes as well as other red fruits and vegetables. It has recently been a subject of great research concerning many potential beneficial properties in the human body. There has been some research suggesting that it may reduce the risk of some types of cancer including colorectal, lung and cervical cancer. However, a review done by the U.S. Food and Drug Administration (FDA) in 2007 looks at 168 research studies and concludes that there is insufficient evidence to support a relationship of lycopene and a reduction of any cancer.\n\nOverall, no single nutrient has been found to reduce the risk of cancer. However, a definite correlation has been found in fruit and vegetable intake and reduced risk of cancer, therefore, consumption of a variety of fruits and vegetables is recommended. Due to an extensive and conclusive research done in this area, Canada has officially made this a Disease Reduction Health Claim in 1997. However, much more research needs to be done in order to identify which chemicals in fruits and vegetables are responsible for a reduced risk of cancer.\n\nAccording to Health Canada, the \"chewing gums, confectionery, or breath freshening products\" are the only types of products that may contain this health claim. As follows, the following phrases are allowed to be printed on the mentioned product packaging.\n\nAllowed phrases:\n\n\nIn addition to the product type specificity, the products must contain one or more of these non-cariogenic sweeteners also known as non-fermentable carbohydrates.\n\nAllowed sweeteners:\n\n\nThe main culprits in creation of dental caries are cariogenic microorganisms such as streptococcus mutans and sugar (see dental caries for detailed explanation of tooth cavity formation).\n\nThere are several studies that show the positive effect of non-fermentable carbohydrates such as xylitol and sorbitol, the most commonly used sugar substitutes in gums and candies, on the reduction of dental caries.\n\nIn one study in Belize done on 1277 school children of the mean age of 10.2 years, which were given gums with the contents of either xylitol, xylitol-sorbitol, sorbitol, or sucrose to chew under teachers' supervision daily. After a dental exam after 16, 28, and 40 months, the results showed the most significant reduction in dental caries by the consumption of the xylitol gum (Relative Risk = 0.27). For the sorbitol gum the RR = 0.74 while the sucrose gum increased the incidence of dental caries (RR = 1.20).\n\nIn another study in Belize done on six-year-old children given xylitol or sorbitol pellets to chew, the results showed the Relative Risk to be 0.35 and 0.44 relative. This study shows that although both xylitol and sorbitol are effective in reduction of incidence of dental caries, xylitol is more effective.\n\nIn yet another study done on school children in Estonia, that were either given xylitol gums or hard candies to chew/eat for three and two years respectively, the results showed overall results in reduction caries rates versus the control group to be 53.5% for gums and 33-59% for candies.\n\nOverall, all these studies and more, show that non-fermentable carbohydrates can reduce dental caries, but let's look at one study that didn't get quite the same results.\n\nThis one study in Madagascar done on school children in grades 1 and 4 were given a \"school-based oral health education program.\" In addition to that all children had to undergo supervised toothbrushing daily. The group was then split into control group and test group where the test group received two types of gum to chew 3-5 times a day. One type of gum was polyol while the other contained a mix of sorbitol (55.5%), xylitol (4.3%), and carbamide (2.3%). After 3 years of this program, the results of the dental exams showed no significant difference between the test and control groups. It is thought that due to the daily supervised toothbrushing, the groups were similar in their overall oral hygiene and therefore the significant difference between the results was reduced.\n\n\nA diet rich in folate along with a daily folic acid supplement and reduced risk of having a baby with a birth defect of the brain or spinal cord\n\nFor the past decades, lack of folate (vitamin B9) had been linked to risk of having a baby with a birth defect of the brain or spinal cord (specifically neural tube defect). Neural tube defect is the most common brain and spinal cord related defect in Canada today. In a randomized controlled double-blind trial in South Wales, 44 women who had one child with neural tube defect, took 4 mg. of folic acid a day before and during pregnancy. This resulted in no recurrences amongst these who received supplementation. Concluding that folic acid supplementation might be an effective method of neural tube defect prevention.\n\nIn another related study, it was stated that folic acid prevents 70 percent of neural tube defects but its mode of action is unclear. Not fully understanding the mechanism of how folate prevents neural tube defects may be a concern that is preventing Canada from allowing it to be a health claim. Also, if this becomes a health claim it will be the first Canadian health claim recommending a natural health product supplement. Thus, more research on this topic is required to insure safety to Canadians.\n\n"}
{"id": "23804755", "url": "https://en.wikipedia.org/wiki?curid=23804755", "title": "Capitation (healthcare)", "text": "Capitation (healthcare)\n\nCapitation is a payment arrangement for health care service providers such as physicians, physician assistants or nurse practitioners. It pays a physician or group of physicians a set amount for each enrolled person assigned to them, per period of time, whether or not that person seeks care. These providers generally are contracted with a type of health maintenance organization (HMO) known as an independent practice association (IPA), which enlists the providers to care for HMO-enrolled patients. The amount of remuneration is based on the average expected health care utilization of that patient, with greater payment for patients with significant medical history.\n\nPrimary capitation is a relationship between a managed care organization (MCO) and primary care physician (PCP), in which the PCP is paid directly by the MCO for those enrolled members who have selected the physician as their provider.\n\nSecondary capitation is a relationship arranged by the MCO between a PCP and a secondary or specialist provider, such as an X-ray facility or ancillary facility such as a durable medical equipment supplier whose secondary provider is also paid capitation based on that PCP’s enrolled membership.\n\nGlobal capitation is a relationship based on a provider who provides services and is reimbursed per-member per-month (PMPM) for the entire network population.\n\nUnder capitation, physicians are given an incentive to consider the cost of treatment. Pure capitation pays a set fee per patient, regardless of their degree of infirmity, giving physicians an incentive to avoid the most costly patients.\n\nProviders who work under such plans focus on preventive health care, as there is a greater financial reward in the prevention of illness than in the treatment of the ill. Such plans divert providers from the use of expensive treatment options.\n\nThe financial risks providers accept in capitation are traditional insurance risks. Provider revenues are fixed, and each enrolled patient makes a claims against the full resources of the provider. In exchange for the fixed payment, physicians essentially become the enrolled clients' insurers, who resolve their patients' claims at the point of care and assume the responsibility for their unknown future health care costs. Large providers tend to manage the risk better than do smaller providers because they are better prepared for variations in service demand and costs, but even large providers are inefficient risk managers in comparison to large insurers. Providers tend to be small in comparison to insurers and so are more like individual consumers, whose annual costs as a percentage of their annual cash flow vary far more than do those of large insurers. For example, a capitated eye care program for 25,000 patients is more viable than a capitated eye program for 10,000 patients. The smaller the roster of patients, the greater the variation in annual costs and the more likely that the costs may exceed the resources of the provider. In very small capitation portfolios, a small number of costly patients can dramatically affect a provider's overall costs and increase the provider's risk of insolvency.\n\nPhysicians and other health care providers lack the necessary actuarial, underwriting, accounting and finance skills for insurance risk management, but their most severe problem is the greater variation in their estimates of the average patient cost, which leaves them at a financial disadvantage as compared to insurers whose estimates are far more accurate. Because their risks are a function of portfolio size, providers can reduce their risks only by increasing the numbers of patients they carry on their rosters, but their inefficiency relative to that of the insurers' is far greater than can be mitigated by these increases. To manage risk as efficiently as an insurer, a provider would have to assume 100% of the insurer's portfolio. HMOs and insurers manage their costs better than risk-assuming healthcare providers and cannot make risk-adjusted capitation payments without sacrificing profitability. Risk-transferring entities will enter into such agreements only if they can maintain the levels of profits they achieve by retaining risks.\n\nProviders cannot afford reinsurance, which would further deplete their inadequate capitation payments, as the reinsurer's expected loss costs, expenses, profits and risk loads must be paid by the providers. The goal of reinsurance is to offload risk and reward to the reinsurer in return for more stable operating results, but the provider's additional costs make that impractical. Reinsurance assumes that the insurance-risk-transferring entities do not create inefficiencies when they shift insurance risks to providers.\n\nWithout any induced inefficiencies, providers would be able to pass on a portion of their risk premiums to reinsurers, but the premiums that providers would have to receive would exceed the premiums that risk-transferring entities could charge in competitive insurance markets. Reinsurers are wary of contracting with physicians, as they believe that if providers think they can collect more than they pay in premiums, they would tend to revert to the same excesses encouraged by fee-for-service payment systems.\n\n\n"}
{"id": "15807148", "url": "https://en.wikipedia.org/wiki?curid=15807148", "title": "Chelates in animal nutrition", "text": "Chelates in animal nutrition\n\nChelates ( che·late ) [kee-leyt] in animal feed are organic forms of essential trace minerals such as copper, iron, manganese and zinc.\n\nAnimals absorb, digest and use mineral chelates better than inorganic minerals. This means that lower concentrations can be used in animal feeds. In addition, animals fed chelated sources of essential trace minerals excrete lower amounts in their faeces, and so there is less environmental contamination. Mineral chelates also offers health and welfare benefits in animal nutrition\n\nSince the 1950s, animal feeds have been supplemented with essential trace minerals such as copper (Cu), iron (Fe), iodine (I), manganese (Mn), molybdenum (Mo), selenium (Se) and zinc (Zn). Initially, such supplementation was by means of inorganic salts of essential trace elements. From the 1960s onwards, genetic improvement of farm livestock resulted in increased nutritional requirements for these nutrients. Chelated minerals were developed in the 1980s and 1990s. Trace mineral chelates have proven to be better than inorganic minerals in meeting the nutritional needs of modern farm animals.\n\nThe objective of supplementation with trace minerals is to avoid a variety of deficiency diseases. Trace minerals carry out key functions in relation to many metabolic processes, most notably as catalysts for enzymes and hormones, and are essential for optimum health, growth and productivity. For example, supplementary minerals help ensure good growth, bone development, feathering in birds, hoof, skin and hair quality in mammals, enzyme structure and functions, and appetite. Deficiency of trace minerals affect many metabolic processes and so may be manifested by different symptoms, such as poor growth and appetite, reproductive failures, impaired immune responses, and general ill-thrift. From the 1950s to the 1990s most trace mineral supplementation of animal diets was in the form of inorganic minerals, and these largely eradicated associated deficiency diseases in farm animals.\n\nThe role in fertility and reproductive diseases of dairy cattle highlights that organic forms of Zn are retained better than inorganic sources and so may provide greater benefit in disease prevention, notably mastitis and lameness\n\nIn recent decades, global food animal production has intensified and genetic potential for growth and yields has improved. As a result, commercial tendencies have been to increase trace mineral supplementation,in order to allow for the greater mineral requirements of superior stock reared under industrial conditions. Increasing the concentration of inorganic minerals in animal diets has led to several problems.\n\nThe use of high Cu in swine and poultry rations has caused accidental Cu poisoning in more sensitive animals, such as sheep grazing pastures fertilised with pig or poultry manure SCAN (2003a) Opinion of the Scientific Committee for Animal Nutrition on the use of copper in feedingstuffs. Secondly, inorganic minerals may form insoluble complexes with other dietary agents resulting in low absorption. In addition, it is thought that the positive charge of many inorganic minerals reduces access to the enterocytes due to repulsion by the negatively charged mucin layer and competition for binding sites.\n\nFinally, the poor retention and high excretion rates of inorganic minerals led to environmental concerns during the 1980s and 1990s, especially in Europe .Opinion of the Scientific Committee for Animal Nutrition on the use of zinc in feedingstuffs]. The European Union is concerned about possible detrimental effects of excess supplementation with trace minerals on the environment or human and animal health, and so in 2003 legislated a reduction in permitted feed concentrations of several trace metals (Co, Cu, Fe, Mn and Zn).\n\nResearch in trace element nutrition has led to the development of more bioavailable organic minerals, including trace minerals derived from chelates. Chelates allow a lower supplementation rate of trace minerals with an equivalent or improved effect on animal health, growth and productivity . Some samples of natural minerals at the Wikimedia Commons:\n\n\nSome notice concluded that the utilisation of organic Cu from a copper chelate or copper lysine were higher than that of inorganic Cu sulfate when fed to rats in the presence and absence of elemental Zn or Fe. The data suggest that, unlike inorganic Cu, organic Cu chelates exhibit absorption and excretion mechanisms that do not interfere with Fe. Copper chelate also achieved higher liver Zn, suggesting less interference at gut absorption sites in comparison with the other forms of Cu.\n\nEffect of organic zinc sources on performance, zinc status and carcass, meat and claw quality in fattening bulls. Livestock Prod. compared a Zn chelate, a Zn polysaccharide complex and ZnO (inorganic zinc oxide) in bull beef cattle, and concluded that the organic forms resulted in some improvement in hoof claw quality.\n\nCompared the bioavailability of Cu and Zn chelates in sheep with the inorganic sulfate forms, at \"low\" and \"high\" supplementation rates. Copper and Zn chelates at the lower rates caused significantly greater increases in blood plasma concentrations than the corresponding treatments with Zn sulfate (p<0.05) and Cu sulfate (p<0.01).\nIn addition, Zinc chelate supplementation resulted in significantly greater hoof horn Zn content than did Zn sulfate (p<0.05). At the \"low\" supplementation rate Zinc chelate achieved better hoof quality than Zn sulfate (p<0.05). The data suggest that Cu and Zn chelates are more readily absorbed and more easily deposited in key tissues such as hooves, in comparison with inorganic Zn forms.\n\nIn weaned piglets evaluated various supplementation rates of organic Zn in the form of a chelate or as a polysaccharide complex and compared these with ZnO, zinc oxide, at 2,000 ppm. Feeding lower concentrations of organic Zn greatly decreased the amount of Zn excreted in comparison with inorganic Zn, without loss of growth performance.\n\nStudied a Copper chelate in weaned pigs in comparison with inorganic Cu and sulfate. Piglet performance was consistently better with organic Cu at 50 to 100 ppm, in comparison with inorganic Cu at 250 ppm. In addition, organic Cu increased Cu absorption and retention, and decreased Cu excretion 77% and 61% respectively, compared with 250 ppm inorganic Cu.\n\nThe effects of an Mg chelate in broiler chickens in comparison with magnesium oxide and an unsupplemented control group. Diets for fattening chicken are not normally supplemented with Mg, but this study indicated positive effects on performance and meat quality. During the first 3 weeks of life, the Mg chelate improved feed efficiency significantly in comparison with both the inorganic MgO and the negative control group (p<0.05). Thigh meat pH and oxidative deterioration during storage were also studied. The Mg chelate increased thigh meat pH in comparison with the negative control (p<0.05). Mg supplementation significantly reduced chemical indicators (TBARS) of oxidative deterioration in liver and thigh muscle (p<0.01), with Mg chelate significantly more efficient than MgO (p<0.01). The data suggest that organic Mg in the form of a chelate is capable of reducing oxidation, and so improve chicken meat quality.\n\nA Zn chelate supplement was compared with Zn sulfate in broiler chickens.Weight gain and feed intake increased quadratically (p<0.05) with increasing Zn concentrations from the chelate and linearly with Zn sulfate. The relative bioavailability of the Zn chelate was 183% and 157% of Zn sulfate for weight gain and tibia Zn, respectively. The authors concluded that the supplemental concentration of Zn required in corn-soy diets for broilers from 1–21 days of age would be 9.8 mg/kg diet as Zn chelate and 20.1 mg/kg diet as Zn sulfate,respectively.\n\nThe effect of replacing inorganic minerals with organic minerals in broiler chickens. One group of chickens received inorganic sulfates of Cu (12 ppm), Fe (45 ppm), Mn (70 ppm) and Zn (37 ppm) and their performance was compared to a similar group supplemented with chelates of Cu (2.5 ppm), Fe, Mn, and Zn (all at 10 ppm). \nThere were no differences in performance between the birds fed the high inorganic minerals and the birds fed the low organic chelates. Faecal concentrations of Cu, Fe, Mn and Zn were 55%, 73%, 46% and 63%, respectively, of control birds fed inorganic minerals.\n\nA broiler study reported also compared inorganic and organic mineral supplementation in broiler chickens. Control birds were fed Cu, Fe, Mn Se and Zn in inorganic forms (15 ppm Cu 15 from sulfate; 60 ppm Fe from sulfate etc.),and compared with three treatment groups supplemented with organic forms. Apart from improved feathering, most likely associated with the presence of organic Se, there were no significant performance differences between birds fed inorganic and organic minerals. The authors concluded that the use of organic trace minerals permits a reduction of at least 33% in supplement rates in comparison with inorganic minerals, without compromising performance.\n\n\n"}
{"id": "7694809", "url": "https://en.wikipedia.org/wiki?curid=7694809", "title": "Comprehensive sex education", "text": "Comprehensive sex education\n\nComprehensive sex education (CSE) is a sex education instruction method based on-curriculum that aims to give students the knowledge, attitudes, skills and values to make appropriate and healthy choices in their sexual lives. The intention is that this understanding will prevent students from contracting sexually transmitted infections in the future, including HIV and HPV. CSE is also designed with the intention of reducing teenage and unwanted pregnancies, as well as lowering rates of domestic and sexual violence, thus contributing to a healthier society, both physically and mentally.\n\nComprehensive sex education ultimately promotes sexual abstinence as the safest sexual choice for young people. However, CSE curriculums and teachers are still committed to teaching students about topics connected to future sexual activity, such as age of consent, safe sex, contraception such as: birth control, abortion, and use of condoms. This also includes discussions which promote safe behaviors, such as communicating with partners and seeking testing for sexually transmitted infections. Additionally, comprehensive sex education curricula may include discussions surrounding pregnancy outcomes such as parenting, adoption, and abortion. The most widely agreed benefit of using comprehensive sex education over abstinence-only sex education is that CSE acknowledges the student population will be sexually active in their future. By acknowledging this, CSE can encourage students to plan ahead to make the healthiest possible sexual decisions. This ideology of arming students to most successfully survive their future sexual experiences underlies the majority of topics within CSE, including condoms, contraception, and refusal skills.\n\nStudies have found that comprehensive sex education is more effective than receiving no instruction and/or those who receive abstinence-only instruction. Acknowledging that people may engage in premarital sex rather than ignoring it (which abstinence-only is often criticized for) allows educators to give the students the necessary information to safely navigate their future sexual lives. \n\nCSE advocates argue that promoting abstinence without accompanied information regarding safe sex practices is a disregard of reality, and is ultimately putting the student at risk. For example, programs funded under AEGP are reviewed for compliance with the 8 standards (listed below in \"Abstinence Education Grant Program (AGEP) Requirements), but are not screened for medical accuracy. Therefore, critics believe that students under these educational programs are put at a disadvantage because it prevents them from making informed choices about their sexual health. Additionally, under these AEGP programs, health educators have referred to those that engage in sex, especially females, as \"dirty\" and \"used.\" They have also used phrases such as \"stay like a new toothbrush, wrapped up and unused\" and \"chewed-up gum\" to teach abstinence. Under a CSE model, language would be more sensitive.\n\nThere is clear evidence that CSE has a positive impact on sexual and reproductive health (SRH), notably in contributing to reducing STIs, HIV and unintended pregnancy. Sexuality education does not hasten sexual activity but has a positive impact on safer sexual behaviours and can delay sexual debut. A 2014 review of school-based sexuality education programmes has demonstrated increased HIV knowledge, increased self-efficacy related to condom use and refusing sex, increased contraception and condom use, a reduced number of sexual partners and later initiation of first sexual intercourse. A Cochrane review of 41 randomized controlled trials in Europe, the United States, Nigeria and Mexico also confirmed that CSE prevents unintended adolescent pregnancies. CSE is very beneficial in regards to teen pregnancy because studies show that, teen pregnancy and childbearing have a significant negative impact on high school success and completion, as well as future job prospects. A study in Kenya, involving more than 6,000 students who had received sexuality education led to delayed sexual initiation, and increased condom use among those who were sexually active once these students reached secondary school compared to more than 6,000 students who did not receive sexuality education. CSE also reduces the frequency of sex and the number of partners which in turn also reduces the rates of sexually transmitted infections.\n\nUNAIDS and the African Union have recognized CSE’s impact on increasing condom use, voluntary HIV testing and reducing pregnancy among adolescent girls and have included comprehensive, age-appropriate sexuality education as one of the key recommendations to fast track the HIV response and end the AIDS epidemic among young women and girls in Africa.\n\nAs the field of sexuality education develops, there is increasing focus on addressing gender, power relations and human rights in order to improve the impact on SRH outcomes. Integrating content on gender and rights makes sexuality education even more effective. A review of 22 curriculum- based sexuality education programmes found that 80 per cent of programmes that addressed gender or power relations were associated with a significant decrease in pregnancy, childbearing or STIs. These programmes were five times as effective as those programmes that did not address gender or power. CSE empowers young people to reflect critically on their environment and behaviours, and promotes gender equality and equitable social norms, which are important contributing factors for improving health outcomes, including HIV infection rates. The impact of CSE also increases when delivered together with efforts to expand access to a full range of high- quality, youth-friendly services and commodities, particularly in relation to contraceptive choice.\n\nA global review of evidence in the education sector also found that teaching sexuality education builds confidence, a necessary skill for delaying the age that young people first engage in sexual intercourse, and for using contraception, including condoms. CSE has a demonstrated impact on improving knowledge, self-esteem, changing attitudes, gender and social norms, and building self-efficacy.\n\nWhile CSE implementation is on the rise in the United States, it remains difficult for state officials to regulate what is and is not taught in the classroom. This is due in large part to the undefinability of CSE; CSE has the potential to comprise such a wide range of sexual information, and over-all focus varies widely between curriculums. Educators have also accused CSE as fundamentally operating as a form of \"abstinence-plus,\" due to the reality that CSE often involves minimal body related information and excessive promotions of abstinence. \"So-called Comprehensive Sex Ed\" says Sharon Lamb, a professor at the University of Massachusetts Boston, \"has been made less comprehensive as curricula are revised to meet current federal, state, and local requirements.\"\n\nThe term \"comprehensive\" is also often misleading because some comprehensive programs do not show the holistic picture of human sexuality. LGBTQIA+ advocates have long been critical of the ways in which comprehensive sex education generally promotes marriage as the end goal for students. Even when curriculums claim to be inclusive of LGBT experiences, they often promote heteronormative lifestyles as \"normal.\" Inclusion of LGBT identities and health topics is necessary for LGBTQIA+ students to feel safe and seen in their sex ed classrooms. When these students do not have access to or an interest in marriage they are practically erased from the CSE narrative. \n\nA cross sectional study done in New York City analyzed the sexual behaviors of high school girls. Studies found that, \"high school girls who identified as LGBTQIA+ were more likely to report substance use such as: alcohol, marijuana, cocaine, heroin, meth, ecstasy and prescription drugs. They also had higher rates of contemplating and/or attempting suicide.\" Another study found that \"the LGBTQIA+ youth accesses health information online five times more than the heterosexual population, and these rates are even higher for LGBTQIA+ youth that identify as a person of color which stems from the fact that they lack health resources. \n\nIn fact, as of May 2018, only 12 states require discussion of sexual orientation and of these, only 9 states require that discussion of sexual orientation be inclusive. Additionally, several states have passed legislation that bans teachers from discussing gay and transgender issues, such as sexual health and HIV/AIDS awareness, in a positive light. Furthermore, three states require that teachers only portray LGBTQIA+ people in a negative light.\n\nIn a Canada, a federal report showed that LGBTQIA+ community has less access to health services and faces more comprehensive health challenges compared to the general population. As a result of lack of support for the LGBTQIA+ population, the Comprehensive Health Education Workers (CHEW) Project emerged in October 2014. Their goal is to educate the LGBTQIA+ community about topics such sexual and gender identity, sexually transmitted infections (STIs), healthy social relationships, and depression. They do this though workshops, arts‐based projects, and one‐on‐one meetings. \n\nMany people regard health education as a moral or religious issue, and therefore should be taught not in schools. \"Before the late 1800s, delivering sex education in the United States and Canada was primarily seen as a parent’s responsibility. Today, programs under the Sexuality Information and Education Council of the United States (SIECUS) begin comprehensive sex education in pre-kindergarten, which many people believe is not age appropriate. Many of those in favor of abstinence only education, usually fear any type of sexual education that encourages sexual behavior at a young age.\n\nAlthough CSE is seen as the polar opposite of abstinence only education, some critics believe that they are very similar. They both aim at preventing STIs and teen pregnancy. The only way in which they differ is through their primary goal. Abstinence only education aims at reducing premarital sex while comprehensive sex education acknowledges that premarital sex may happen and therefore seeks to reduce the unintended consequences of premarital sex through education. Studies in developing counties surrounding sex education have also shown that comprehensive sex education is not needed. General education, such as literacy skills, was seen to delay sexual initiation and reduced the likelihood of pregnancy. Therefore, some people believe general education is of more importance.\n\nAlthough there is no federal mandate that requires states to teach sexual education, there is federal funding available to assist with sexual education programs.\n\nHistorically, funding for abstinence education has always been favored over CSE. In 1996, during Bill Clinton's presidency, legislation was passed to promote abstinence in education programs. Under Title V Section 510 of the Social Security Act, the Abstinence Education Grant Program (AGEP), was passed. AEGP has always been renewed before its expiration date, and each time funds gradually increase from fifty million dollars per year to seventy-five and as high as $6.75 million per state grant in 2015. The way the funds are disbursed are based on the proportion of low-income children in each state. So far, thirty-six states have been given AEGP funds.\n\nPart of Section 510(b) of Title V of the Social Security Act, contains the \"A-H guidelines,\" which are the eight criteria that programs must abide by order to be eligible to receive federal funding. They are as follows:\n\nA. Has as its exclusive purpose teaching the social, psychological, and health gains to be realized by abstaining from sexual activity;\n\nB. Teaches abstinence from sexual activity outside marriage as the expected standard for all school-age children;\n\nC. Teaches that abstinence from sexual activity is the only certain way to avoid out-of-wedlock pregnancy, sexually transmitted diseases, and other associated health problems;D. Teaches that a mutually faithful, monogamous relationship in the context of marriage is the expected standard of sexual activity;E. Teaches that sexual activity outside the context of marriage is likely to have harmful psychological and physical effects;\n\nF. Teaches that bearing children out of wedlock is likely to have harmful consequences for the child, the child's parents, and society;G. Teaches young people how to reject sexual advances and how alcohol and drug use increase vulnerability to sexual advances; andH. Teaches the importance of attaining self-sufficiency before engaging in sexual activity;\n\nIn addition to abiding by these 8 conditions, AGEP compliant programs cannot discuss contraception, STIs, or methods for protecting against STIs, except only when describing failure rates.\n\nMore recently legislation has pushed for funding that goes beyond abstinence only education. In 2010, President Obama introduced the Teen Pregnancy Prevention Program (TPP), which provides a total of $114.5 million annually to sex education programs that are \"medically accurate and age-appropriate.\" TPP falls under a subsection of United States Department of Health and Human Services (\"HHS\") which is overseen by the Office of Adolescent Health. Funding for TPP is dispersed if \"they emulate specific evidence-based programs promulgated under TPP.\" \n\nIn January 2016, the California Healthy Youth Act, amended the California Comprehensive Sexual Health and HIV/AIDS Prevention Education Act to include minority groups and expand health education. Before it authorized schools to provide comprehensive sex education and required that all materials are made accessible to students with a variety of needs. It also focused solely on marital relationships. It now mandates that schools provide comprehensive sex education and states that \"materials cannot be biased and must be appropriate for students of all races, genders, sexual orientations, and ethnic and cultural backgrounds, as well as those with disabilities and English language learners.\" Additionally, education must now include \"instruction about forming healthy and respectful committed relationships,\" regardless if marital status. Furthermore, it is now required to have discussions about all FDA-approved contraceptive methods in preventing pregnancy, including the morning after pill.\n\nIn conclusion now requires that all sex education programs promulgated in the state should\n\n\nSome critics state that young people’s access to CSE is grounded in internationally recognized human rights, which require governments to guarantee the overall protection of health, well-being and dignity, as per the Universal Declaration on Human Rights, and specifically to guarantee the provision of unbiased, scientifically accurate sexuality education.\n\nThese rights are protected by internationally ratified treaties, and lack of access to sexual and reproductive health (SRH) education remains a barrier to complying with the obligations to ensure the rights to life, health, non-discrimination and information, a view that has been supported by the Statements of the Committee on the Rights of the Child, the Convention on the Elimination of all Forms of Discrimination Against Women (CEDAW) Committee, and the Committee on Economic, Social and Cultural Rights.\n\nThe commitment of individual states to realizing these rights has been reaffirmed by the international community, in particular the Commission on Population and Development (CPD), which – in its resolutions 2009/12 and 2012/13 – called on governments to provide young people with comprehensive education on human sexuality, SRH and gender equality.\n\nOther analysis show that comprehensive sex education is not an international right nor a human right because it not clearly stated in either a treaty nor custom. By international law, states are required to provide access to information and education about reproductive health, but this does not require a sex education curriculum. It may take different forms such as mandating that local school districts create a system for providing information to students, or mandating that health clinics and practitioners dispense information to patients.\n\nAs CSE gains momentum and interest at international, regional and national levels, governments are increasingly putting in place measures to scale-up their delivery of some form of life skills-based sexuality education, as well as seeking guidance on best practice, particularly regarding placement within the school curriculum. Sexuality education may be delivered as a stand-alone subject or integrated across relevant subjects within the school curricula. These options have direct implications for implementation, including teacher training, the ease of evaluating and revising curricula, the likelihood of curricula being delivered, and the methods through which it is delivered.\n\nWithin countries, choices about implementing integrated or stand-alone sexuality education are typically linked to national policies and overall organization of the curricula. The evidence base on the effectiveness of stand-alone vs. integrated sexuality education programming is still limited. However, there are discernible differences for policy-makers to consider when deciding the position of CSE within the curriculum.\n\nAs a stand-alone subject, sexuality education is set apart from the rest of the curriculum, whether on its own or within a broader stand-alone health and life skills curriculum. This makes it more vulnerable to potentially being sacrificed due to time and budget constraints, since school curricula are typically overcrowded.\n\nHowever, a stand-alone curriculum also presents opportunities for specialized teacher training pathways, and the use of non-formal teaching methodologies that aim to build learners’ critical thinking skills. The pedagogical approaches promoted through sexuality education – such as learner-centred methodologies, development of skills and values, group learning and peer engagement – are increasingly being recognized as transformative approaches that impact on learning and education more widely. As a standalone subject, it is also significantly easier to monitor, which is crucial in terms of evaluating the effectiveness of programming, and revising curricula where it is not delivering the desired learning outcomes.\n\nWhen sexuality education is integrated or infused, it is mainstreamed across a number of subject areas, such as biology, social studies, home economics or religious studies. While this model may reduce pressure on an overcrowded curriculum, it is difficult to monitor or evaluate, and may limit teaching methodologies to traditional approaches.\n\nApart from the different teaching methods, termiology also differs. Abortion, homosexuality, abstinence have connotations and definitions that vary state. For example, the word \"abstinence\" may refer to disengaging from all forms of sexual activities until marriage or may refer to only disengaging from sexual intercourse. Furthermore, the degree of sexual activity that \"abstinence\" connotates is often unclear, because sexual behavior that is not sexual intercourse may or may not be included in its definition. As a result, students are left confused about what activities are risky and teachers do not know what they can and cannot teach.\n\nThe term \"comprehensive,\" is also falls on spectrum, therefore can be considered an umbrella term. CSE means something radical for some institutions while it can mean something moderate and even conservative for others.\n\nAccording to the Sexuality Information and Education Council of the United States (SIECUS), the guidelines for comprehensive sexuality education are as follows:\n\nJust as teaching methods and curricula vary by state, excusal from sex education also varies by state. States may have with an opt out or opt in produce. In some states, students can opt out of receiving sexual education without specifying a particular reason. In other states, students can only opt out for religious or moral reasons. In an opt-in provision, parents must actively agree to allow their children to receive sex education prior to the start of the sexual education.\n\nSince 1997, the amount of sexual content on TV has nearly doubled in the United States. Additionally, a study done in 2008 showed that nearly 40% of popular music lyrics contained sexual references which were often sexually degrading. These lyrics were also often accompanied with mentions of other risk behaviors, such as substance use and violence. \n\nTeens (ages 13-15) in the United States, use entertainment media as their top source for education in regards to sexuality and sexual health. Additionally, a study found that 15-19 year olds in the U.S use media far more than parents or schools to obtain information about birth control. Some studies have found that, \"very few teen television shows mention any of the responsibilities or risks (e.g., using contraception, pregnancy, STIs) associated with sex and almost none of the shows with sexual content include precaution, prevention, or negative outcomes as the primary theme.\" Television shows 16 and Pregnant and its spin-off, Teen Mom, which first aired on MTV in 2009 received major disapproval from some parents as they thought the shows glamorized teen pregnancy and motherhood. However, 16 and Pregnant actually led to a 4.3 percent reduction in teen pregnancy, mostly as a result of increased contraceptive use. In contrast, other data shows that exposure to high levels of sexual content on the television causes adolescents to have twice the risk of becoming pregnant in the following 3 years, compared to those who were exposed to low levels.\n\nThe film Mean Girls, directed by Mark Waters shed light on the state sex education in some parts of the United States. In the film the health instructor states, \"At your age, you're going to have a lot of urges. You're going to want to take off your clothes and touch each other. But if you do touch each other, you will get chlamydia and die.\" This line is meant to be satirical, but it illustrates common flaws within sex education in the U.S. It depicts simplistic descriptions of sexual activity and implementation of fear without any legitimate basis.\n\nComprehensive sex education is the main topic in the documentary \"The Education of Shelby Knox\" released in 2005 about Lubbock, Texas, which has one of the highest teen pregnancy and STD rates in the nation. The \"solution\" to which is a strict abstinence-only sex education curriculum in the public schools and a conservative preacher who urges kids to pledge abstinence until marriage.\n\nIn 2013, \"How to Lose Your Virginity\" was released, a documentary that questioned the effectiveness of the abstinence-only sex education movement and observed how sexuality continues to define a young woman's morality and self-worth. The meaning and necessity of virginity as a social construct is also examined through narration and interviews with notable sexuality experts, such as former Surgeon General Dr. Joycelyn Elders, \"Scarleteen\" creator and editor Heather Corinna, historian Hanne Blank, author Jessica Valenti, and comprehensive sex education advocate Shelby Knox.\n\nNot only have films portrayed sex education, but so has social media. Platforms such as YouTube, Facebook, Vine, and others are used as a tool to uplift the narratives of marginalized communities such as persons of color and LGBTQIA+ persons in hopes to \"strengthen sexual health equity for all.\"\n\nAs a result of the mass amount of sex content in media, media literacy education (MLE) has emerged. It was created to address the influence of unhealthy media messages on risky health decisions, such as intention to use substances, body image issues, and eating disorders. A study analyzed the effectiveness of a teacher-led MLE program, called Media Aware Sexual Health (MASH), which provides students with accurate health information and teaches them how to apply that information to critical analysis of media messages. This comprehensive sex education resulted in increased intentions to talk to a parent, partner and medical professional prior to sexual activity, and intentions for condom use.\n\nDue to knowledge gaps in most sex education curricula for teens, free online resources like Sex, Etc. and teensource.org have been created to promote comprehensive, inclusive sex education for teenagers.\n\n"}
{"id": "41216691", "url": "https://en.wikipedia.org/wiki?curid=41216691", "title": "David Vanderpool", "text": "David Vanderpool\n\nDavid Vanderpool (born February 18, 1960) is an American medical missionary and the CEO and founder of Live Beyond, which has provided medical, spiritual and logistical support to disaster ridden countries.\n\nVanderpool's work is unusual in that he combines his medical training with an explicit effort to convert his patients to Christianity.\n\nVanderpool was born in Dallas, Texas, graduated from St. Mark's School of Texas, and received his undergraduate degree from Abilene Christian University in 1982. He then attended the School of Medicine at Texas Tech University Health Sciences Center. After medical school, Vanderpool completed two surgical residencies at Baylor University Medical Center where he trained as a vascular surgeon.\n\nVanderpool wife, Laurie, is a speaker for Women's Retreats and Bible Studies, and speaks frequently for Down Syndrome organizations.\n\nVanderpool remained in Texas after residency and practiced as a vascular surgeon before moving to Brentwood, Tennessee in 2001 and opening his private practice Lave MD in 2003.\n\nDr. Vanderpool created Lave MD to act as both a medical facility and a spa. \nAfter the establishment of his international organization in 2005, Dr. Vanderpool used much of Lave MD's proceeds to fund the organization's efforts abroad.\n\nIn 2005, after Hurricane Katrina hit the southeastern part of the United States, Dr. Vanderpool delivered healthcare across the Mississippi Coast out of a trailer. His goal was to provide as much free healthcare as possible while the medical infrastructure could recover. Months later, Dr. Vanderpool established his organization, Medical Mobile Disaster Relief, as a 501(c)(3) non-profit organization with goals to provide disaster relief through medical clinics, clean water projects, and micro-finance projects to areas hit by disasters. \nVanderpool and the Mobile Medical Disaster Relief team began working in Mozambique in 2006. His goal was to provide healthcare to the indigenous people of the country in addition to enhancing the economy by implementing micro-finance projects among widows living in the communities. By 2008, Dr. Vanderpool teamed up with the Belmont School of Nursing to construct a nursing curriculum that could teach the Mozambique women to be self-sufficient in caring for themselves and their children.\n\nVanderpool and his Mobile Medical Disaster Relief partnered with PACODEP (Partners in Community Development) in Ghana in order to provide medical care, educate the locals on water purification and distribute water purifiers. PACODEP works to free enslaved children who are trafficked through Ghana for purposes of fishing work. Dr. Vanderpool has also partnered with local hospitals in Ghana in order to provide free invasive surgeries to these rescued children.\n\nDr. Vanderpool partnered with Mission Lazarus in 2009 to build a sustainable medical clinic in Cedeño, Honduras. Given sufficient medical supplies and equipment, Vanderpool allowed Mission Lazarus to take over the clinic and provision of healthcare for the people of Cedeño.\n\nDr. Vanderpool shifted his focus in the aftermath of the earthquake that devastated the entire country of Haiti in 2010. In 2010, Vanderpool officially changed the name of his organization from Mobile Medical Disaster Relief to Live Beyond with a stated mission to be \"an organization that chooses to Live Beyond...ourselves, our culture, our borders and this life so that others can Live Beyond…disease, hunger, poverty and despair.\"\n\nThe initial aid Vanderpool and his team brought to Haiti was primarily mobile medical care to relieve the thousands devastated and injured by the earthquake. Since then however, Vanderpool has grounded his missionary work in Thomazeau, Haiti where his organization began building a base. Vanderpool continued to provide free medical care, establishing a surgical hospital and clinic. In addition, clean water projects, orphanages, and other widow and orphan advocacy projects were begun.\n\nDr. Vanderpool is a Christian, who combines religion and the spread of his faith with his medical work. In Haiti, he has made it one of his objectives to bring Haiti away from its traditional voodoo culture and provide \"spiritual guidance\" to the Haitians in the role of Christianity, with the belief that Christianity will lead to a better Haiti. Prior to moving to Haiti, each medical outreach trip made by Vanderpool and other Live Beyond participants included prayer and Christian ministry along with healthcare to the voodoo priests, island chiefs, idol worshipers and the sick and dying in Haiti. On the Live Beyond site, Vanderpool's religious impact since being in Haiti has been characterized as leading to the baptism of dozens, the saving of tribes through \"Bibles being read in their own languages\" and \"the Kingdom is being expanded.\" Dr. Vanderpool promotes religious missionary work in tandem with his medical relief and sustainable development efforts. A worship center is being built in Thomazeau, Haiti and monthly mission trips are promoted and scheduled for Americans to travel to Thomazeau and volunteer.\n\n"}
{"id": "4637216", "url": "https://en.wikipedia.org/wiki?curid=4637216", "title": "Diseases of poverty", "text": "Diseases of poverty\n\nDiseases of poverty (also known as poverty related diseases) is the term used to describe diseases that are more prevalent in the low-income population. It includes infectious diseases as well as diseases related to malnutrition and poor health behaviors. Poverty is one of the social determinants of health. The World Health Report, 2002 states that diseases of poverty account for 45% of the disease burden in the countries with high poverty rate which are preventable or treatable with exciting interventions. Diseases of poverty are often co-morbid and ubiquitous with malnutrition.\n\nPoverty and diseases are a ramification of each other. Poverty increases chances of having these diseases as the deprivation of safe shelter, drinking water and food, poor sanitation, lack of knowledge and access to health services contributes towards poor health behaviors which often results into diseases of poverty. At the same time, these diseases act as a barrier for economic growth to affected people and families caring for them which in turn results into increased poverty in the community.\n\nThese diseases triggered in part by poverty are in contrast to so-called \"diseases of affluence\", which are diseases thought to be a result of increasing wealth in a society.\n\nFor many environmental and social factors, including poor housing conditions and poor working conditions, inadequate sanitation, and disproportionate occupation as sex workers, the poor are more likely to be exposed to infectious diseases. Malnutrition, mental stress, overwork, inadequate knowledge, and minimal health care can hinder recovery and exacerbate the disease. Malnutrition is associated with 54% of childhood deaths from diseases of poverty, and lack of skilled attendants during childbirth is primarily responsible for the high maternal and infant death rates among the poor.\n\nEach year many children and adults die as a result of a lack of access to clean drinking water and poor sanitation. Many poverty related diseases such as diarrhea acquire and spread as a result of inadequate access to clean drinking water. According to UNICEF, 3,000 children die every day, worldwide due to contaminated drinking water and poor sanitation.\n\nAlthough the Millennium Development Goal (MDG) of halving the number of people who did not have access to clean water by 2015, was reached five years ahead of schedule in 2010, there are still 783 million people who rely on unimproved water sources. In 2010 the United Nations declared access to clean water a fundamental human right, integral to the achievement of other rights. This made it enforceable and justifiable to permit governments to ensure their populations access to clean water. Though access to water has improved for some, it continues to be especially difficult for women and children. Women and girls bear most of the burden for accessing water and supplying it to their households.\n\nIn India, Sub-Saharan Africa, and parts of Latin America, women are required to travel long distances in order to access a clean water source and then bring some water home. This has a significant impact on girls’ educational attainment.\n\nThere have been further efforts to improve water quality using new technology which allows water to be disinfected immediately upon collection and during the storage process. Clean water is necessary for cooking, cleaning, and laundry because many people come into contact with disease causing pathogens through their food, or while bathing or washing.\n\nAn ongoing issue of contaminated water in the United States has been taking place in Flint, Michigan. On September 4, 2018, evidence of E Coli and other organisms that can cause disease were found in the water. The issue of contaminated water in Flint, Michigan started when the source for drinking water in Flint was changed from the Lake Huron and the Detroit River to the very cheap Flint River.\n\nContaminated water and inadequate sanitation are related to diseases of poverty such as malaria, parasitic diseases, and schistosomiasis. These infections act as cofactors that increase the risk of HIV transmission.\n\nStandpipes and sanitation are provided in most developing areas, but the death rates are not significantly reduced. One of the reasons that water-related diseases are still occurring is because water supplies can be contacted by contaminated surface water. To effectively decrease the morbidity and mortality of diseases, the population should get access to water from home instead from outside. Therefore, in addition to the installation of standpipes, water supplies and sanitation should be provided within houses.\n\nMalnutrition disproportionately affect those in sub-Saharan Africa. Over 35 percent of children under the age of 5 in sub-Saharan Africa show physical signs of malnutrition. Malnutrition, the immune system, and infectious diseases operate in a cyclical manner: infectious diseases have deleterious effects on nutritional status, and nutritional deficiencies can lower the strength of the immune system which affects the body’s ability to resist infections. Similarly, malnutrition of both macronutrients (such as protein and energy) and micronutrients (such as iron, zinc, and vitamins) increase susceptibility to HIV infections by interfering with the immune system and through other biological mechanisms. Depletion of macro-nutrients and micro-nutrients promotes viral replication that contributes to greater risks of HIV transmission from mother-to-child as well as those through sexual transmission. Increased mother-to-child transmission is related to specific deficiencies in micro-nutrients such as vitamin A. Further, anemia, a decrease in the number of red blood cells, increases viral shedding in the birth canal, which also increases risk of mother-to-child transmission. Without these vital nutrients, the body lacks the defense mechanisms to resist infections. At the same time, HIV lowers the body’s ability to intake essential nutrients. HIV infection can affect the production of hormones that interfere with the metabolism of carbohydrates, proteins, and fats.\n\nIn the United States, 11.1 percent of households struggle with food insecurity. Food insecurity refers to the lack of access to quality food for a healthy lifestyle. The rate of hunger and malnutrition in female headed households was three times the national average at 30.2 percent. According to the Food and Agriculture Organization of the United Nations, 10 percent of the population in Latin America and the Caribbean are affected by hunger and malnutrition.\n\nQuality and affordability of housing are one of the major concerns in public health. Poor housing conditions can be described as leaks, molds, indoor air pollutant, overcrowding, hazardous structures, affordability of home heating, and poor ergonomics. Housing insecurities are very common among the poor. It is often associated with infectious diseases, lead exposure, injuries, and mental health.\n\nAccording to WHO, medical strategies report, approximately 30% of the global population does not have regular access to exciting medicines. In the poorest parts of Africa and Asia, this percent goes up to 50%. The population below the poverty line lacks access due to higher retail price and unavailability of the medicines. The higher cost can be due to the higher manufacturing price or due to local or regional tax and Value Added Tax. There is a significant disparity in the research conducted in the health sector. It is claimed that only 10% of the health research conducted globally focuses on 90% disease burden. However, diseases such as cancer, cardiovascular diseases etc that traditionally were associated with the wealthier community are now becoming more prevalent in the poor communities as well. Hence, the research conducted now is relevant to poor population. Political priority is also one of the contributing factors of inaccessibility. The government of poor countries may allocate less funding to public health due to the scarcity of resources.\n\nTogether, diseases of poverty kill approximately 14 million people annually. Gastroenteritis with its associated diarrhea results in about 1.8 million deaths in children yearly with most of these in the world's poorest nations.\n\nAt the global level, the three primary PRDs are tuberculosis, AIDS/HIV and malaria. Developing countries account for 95% of the global AIDS prevalence and 98% of active tuberculosis infections. Furthermore, 90% of malaria deaths occur in sub-Saharan Africa. Together, these three diseases account for 10% of global mortality.\n\nTreatable childhood diseases are another set which have disproportionately higher rates in poor countries despite the availability of cures for decades. These include measles, pertussis and polio.<ref name=\"IPN 10/90\"></ref> The largest three poverty-related diseases (PRDs) — AIDS, malaria, and tuberculosis — account for 18% of diseases in poor countries. The disease burden of treatable childhood diseases in high-mortality, poor countries is 5.2% in terms of disability-adjusted life years but just 0.2% in the case of advanced countries.\n\nIn addition, infant mortality and maternal mortality are far more prevalent among the poor. For example, 98% of the 11,600 daily maternal and neonatal deaths occur in developing countries.\n\nThree other diseases, measles, pneumonia, and diarrheal diseases, are also closely associated with poverty, and are often included with AIDS, malaria, and tuberculosis in broader definitions and discussions of diseases of poverty.\n\nBased upon the spread of research in cures for diseases, certain diseases are identified and referred to as \"neglected diseases\". These include the following diseases:\n\nTropical diseases such as these tend to be neglected in research and development efforts. Of 1393 new drugs brought into use over a period of 25 years (1975–1999), only a total of thirteen, less than 1%, related to these diseases. Of 20 MNC drug companies surveyed for research on PRDs, only two had projects targeted towards these neglected PRDs. However, the combined total number of deaths due to these diseases is dwarfed by the enormous number of patients affected by PRDs such as respiratory infections, HIV/AIDS, diarrhea and tuberculosis, besides many others.\nSimilar to the spread of tropical neglected diseases in developing nations, these neglected infections disproportionately affect poor and minority populations in the United States. These diseases have been identified by the Centers for Disease Control and Prevention, as priorities for public health action based on the number of people infected, the severity of the illnesses, and the ability to prevent and treat them.\n\nTrichomoniasis is the most common sexually transmitted infection affecting more than 200 million people worldwide. It is especially prevalent among young, poor and African American women. This infection is also common in poor communities in Sub-Saharan Africa and impoverished parts of Asia. This neglected infection is one of special concern because it is associated with a heightened risk for contracting HIV and pre-term deliveries.\n\nIn addition, availability of cures and recent advances in medicine have led to only three diseases being considered neglected diseases, namely, African trypanosomiasis, Chagas disease and Leishmaniasis.\n\nAfrica accounts for a majority of malaria infections and deaths worldwide. Over 80 percent of the 300 to 500 million malaria infections occurring annually worldwide are in Africa. Each year, about one million children under the age of five die from malaria. Children who are poor, have mothers with little to no education, and live in rural areas are more susceptible to malaria and more likely to die from it. Malaria is directly related to the spread of HIV in sub-Saharan Africa. It increases viral load seven to ten times, which increases the chances of transmission of HIV through sexual intercourse from a patient with malaria to an uninfected partner. After the first pregnancy, HIV can also decrease the immunity to malaria. This contributes to the increase of the vulnerability to HIV and higher mortality from HIV, especially for women and infants. HIV and malaria interact in a cyclical manner—being infected with malaria increases susceptibility to HIV infection, and HIV infections increase malarial episodes. The co-existence of HIV and malaria infections helps spread both diseases, particularly in Sub-Saharan Africa. Malaria vaccines are an area of intensive research.\n\nIntestinal parasites are extremely prevalent in tropical areas. These include hookworms, roundworms, and other amoebas. They can aggravate malnutrition by depleting essential nutrients through intestinal blood loss and chronic diarrhea. Chronic worm infections can further burden the immune system. At the same time, chronic worm infections can cause immune activation that increases susceptibility of HIV infection and vulnerability to HIV replication once infected.\n\nSchistosomiasis (bilharzia) is a parasitic disease caused by the parasitic flatworm trematodes. Moreover, more than 80 percent of the 200 million people worldwide who have schistosomiasis live in sub-Saharan Africa. Infections often occur in contaminated water where freshwater snails release larval forms of the parasite. After penetrating the skin and eventually traveling to the intestines or the urinary tract, the parasite lays eggs and infects those organs. It damages the intestines, bladder, and other organs and can lead to anemia and protein-energy deficiency. Along with malaria, schistosomiasis is one of the most important parasitic co-factors aiding in HIV transmission. Epidemiological data shows schistosome-endemic areas coincide with areas of high HIV prevalence, suggesting that parasitic infections such as schistosomiasis increase risk of HIV transmission.\n\nTuberculosis is the leading cause of death around the world for an infectious disease. This disease is especially prevalent in sub-Saharan Africa, and the Latin American and Caribbean region. While the tuberculosis rate is decreasing in the rest of the world, it is increasing by rate of 6 percent per year in Sub-Saharan Africa. It is the leading cause of death for people with HIV in Africa. Tuberculosis (TB) is closely related to lifestyles of poverty, overcrowded conditions, alcoholism, stress, drug addiction and malnutrition. This disease spreads quickly among people who are undernourished. According to the Center for Disease Control and Prevention, in the United States, tuberculosis is more prevalent among foreign born persons, and ethnic minorities. The rates are especially high among Hispanics, Blacks and Asians.\nHIV infection and TB are also closely tied. Being infected with HIV increases the rate of activation of latent TB infections, and having TB, increases the rate of HIV replication, therefore accelerating the progression of AIDS.\n\nAIDS is a disease of the human immune system caused by the human immunodeficiency virus (HIV). Primary modes of HIV transmission in sub-Saharan Africa are sexual intercourse, mother-to-child transmission (vertical transmission), and through HIV-infected blood. Since rate of HIV transmission via heterosexual intercourse is so low, it is insufficient to cause AIDS disparities between countries. Critics of AIDS policies promoting safe sexual behaviors believe that these policies miss the biological mechanisms and social risk factors that contribute to the high HIV rates in poorer countries. In these developing countries, especially those in sub-Saharan Africa, certain health factors predispose the population to HIV infections.\n\nMany of the countries in Sub-Saharan Africa are ravaged with poverty and many people live on less than one United States dollar a day. The poverty in these countries gives rise to many other factors that explain the high prevalence of AIDS. The poorest people in most African countries suffer from malnutrition, lack of access to clean water, and have improper sanitation. Because of a lack of clean water many people are plagued by intestinal parasites that significantly increase their chances of contracting HIV due to compromised immune system. Malaria, a disease still rampant in Africa also increases the risk of contracting HIV. These parasitic diseases, affect the body’s immune response to HIV, making people more susceptible to contracting the disease once exposed. Genital schistosomiasis, also prevalent in the topical areas of Sub-Saharan Africa and many countries worldwide, produces genital lesions and attract CD4 cells to the genital region which promotes HIV infection. All these factors contribute to the high rate of HIV in Sub-Saharan Africa. Many of the factors seen in Africa are also present in Latin America and the Caribbean and contribute to the high rates of infections seen in those regions. In the United States, poverty is a contributing factor to HIV infections. There is also a large racial disparity, with African Americans having a significantly higher rate of infection than their white counterparts.\n\nMore than 300 million people worldwide have asthma. The rate of asthma increases as countries become more urbanized and in many parts of the world those who develop asthma do not have access to medication and medical care. Within the United States, African Americans and Latinos are four times more likely to suffer from severe asthma than whites. The disease is closely tied to poverty and poor living conditions. Asthma is also prevalent in children in low income countries. Homes with roaches and mice, as well as mold and mildew put children at risk for developing asthma as well as exposure to cigarette smoke.\n\nUnlike many other Western countries, the mortality rate for asthma has steadily risen in the United States over the last two decades. Mortality rates for African American children due to asthma are also far higher than that of other racial groups. For African Americans, the rate of visits to the emergency room is 330 percent higher than their white counterparts. The hospitalization rate is 220 percent higher and the death rate is 190 percent higher. Among Hispanics, Puerto Ricans are disporpotionatly affected by asthma with a disease rate that is 113 percent higher than non-Hispanic Whites and 50 percent higher than non-Hispanic Blacks. Studies have shown that asthma morbidity and mortality are concentrated in inner city neighborhoods characterized by poverty and large minority populations and this affects both genders at all ages. Asthma continues to have an adverse effects on the health of the poor and school attendance rates among poor children. 10.5 million days of school are missed each year due to asthma.\n\nThough heart disease is not exclusive to the poor, there are aspects of a life of poverty that contribute to its development. This category includes coronary heart disease, stroke and heart attack. Heart disease is the leading cause of death worldwide and there are disparities of morbidity between the rich and poor. Studies from around the world link heart disease to poverty. Low neighborhood income and education were associated with higher risk factors. Poor diet, lack of exercise and limited (or no) access to a specialist were all factors related to poverty, though to contribute to heart disease.\nBoth low income and low education were predictors of coronary heart disease, a subset of cardiovascular disease. Of those admitted to hospital in the United States for heart failure, women and African Americans were more likely to reside in lower income neighborhoods. In the developing world, there is a 10 fold increase in cardiac events in the black and urban populations.\n\nObstetric fistula or vaginal fistula is a medical condition in which a fistula (hole) develops between either the rectum and vagina (see rectovaginal fistula) or between the bladder and vagina (see vesicovaginal fistula) after severe or failed childbirth, when adequate medical care is not available. It is considered a disease of poverty because of its tendency to occur women in poor countries who do not have health resources comparable to developed nations.\n\nDental decay or dental caries is the gradual destruction of tooth enamel. Poverty is a significant determinant for oral health. Dental caries is one of the most common chronic diseases worldwide. In the United States it is the most common chronic disease of childhood. Risk factors for dental caries includes living in poverty, poor education, low socioeconomic status, being part of an ethnic minority group, having a developmental disability, recent immigrants and people infected with HIV/AIDS. In Peru, poverty was found to be positively correlated with dental caries among children. According to a report by U.S health surveillance, tooth decay peaks earlier in life and is more severe in children with families living below the poverty line. Tooth decay is also strongly linked to dietary behaviors, and in poor rural areas where nutrient dense foods, fruits and vegetables are unavailable, the consumption of sugary and fatty food increases the risk of dental decay. Because the mouth is a gateway to the respiratory and digestive tracts, oral health has a significant impact on other health outcomes. Gum disease has been linked to diseases such as cardiovascular disease.\n\nDiseases of poverty reflect the dynamic relationship between poverty and poor health; while such diseases result directly from poverty, they also perpetuate and deepen impoverishment by sapping personal and national health and financial resources. For example, malaria decreases GDP growth by up to 1.3% in some developing nations, and by killing tens of millions in sub-Saharan Africa, AIDS alone threatens “the economies, social structures, and political stability of entire societies”.\n\nWomen and children are often put at a high risk of being infected by schistosomiasis, which in turn puts them at a higher risk of acquiring HIV. Since the mode of schistosomiasis transmission is usually through contaminated water in streams and lakes, women and children who do their household chores by the water are more likely to acquire the disease. Activities that women and children often do around waterfront include washing clothes, collecting water, bathing, and swimming. Women who have schistosomiasis lesions are three times more likely to be infected with HIV.\n\nWomen also have a higher risk of HIV transmission through the use of medical equipment such as needles. Because more women than men use health services, especially during pregnancy, they are more likely to come across unsterilized needles for injections. Although statistics estimate that unsterilized needles only account for 5 to 10 percent of primary HIV infections, studies show this mode of HIV transmission may be higher than reported. This increased risk of contracting HIV through non-sexual means has social consequences for women as well. Over half of the husbands of HIV-positive women in Africa tested HIV-negative. When HIV-positive women reveal their HIV status to their HIV-negative husbands, they are often accused of infidelity and face violence and abandonment from their family and community.\n\nMalnutrition associated with HIV impacts people’s ability to provide for themselves and their dependents, thus limiting the human capabilities of both themselves and their dependents. HIV can negatively affect work output, which impacts the ability to generate income. This is crucial in parts of Africa where farming is the primary occupation and obtaining food is dependent on the agricultural outcome. Without adequate food production, malnutrition becomes more prevalent. Children are often collateral damage in the AIDS crisis. As dependents, they can be burdened by the illness and eventual death of one or both parents due to HIV/AIDS. Studies have shown that orphaned children are more likely to display physical symptoms of malnutrition than children whose parents are both alive.\n\nThere are a number of proposals for reducing the diseases of poverty and eliminating health disparities within and between countries. The World Health Organization proposes closing the gaps by acting on social determinants. \nTheir first recommendation is to improve daily living conditions. This area involves improving the lives of women and girls so that their children are born in healthy environments and placing an emphasis on early childhood health. \nTheir second recommendation is to tackle the inequitable distribution of money, power and resources. This would involve building stronger public sectors and changing the way in which society is organized. \nTheir third recommendation is to measure and understand the problem and assess the impact of action. This would involve training policy makers and healthcare practitioners to recognize problems and form policy solutions. \n\nThe 8th Global Conference on Health Promotion held in Helsinki in June 2013 has proposed an approach termed Health in All Policies. Health inequalities are shaped by many powerful forces and social, political, and economic determinants. Governments have a responsibility to ensure that their people are able to live healthy lives and have equitable access to achieving a reasonable state of good health. Policies that governments craft and implement in all sectors have a significant and ongoing impact on public health, health equity, and the lives of their citizens. Increases in technology, medical innovation, and living conditions have led to the disappearance of diseases and other factors contributing to poor health. However, there are many diseases of poverty that still persist in developed and developing countries. Tackling these health inequalities and diseases of poverty requires a willingness to engage the whole government in health. The Helskinki Statement lays out a framework of action for countries and calls on governments to make a commitment to building health equity within their country. \n\nHealth in All Policies (HiAP) is an approach to public policies across all sectors of government that takes into account the health implications of all government and policy decisions to improve health equity across all populations residing within the borders of a country. This concept is built upon principles in line with the Universal Declaration of Human Rights, The United Nations Millennium Development Declaration, and principles of good governance : legitimacy given by national and international law, accountability of government, transparency of policy making, participation of citizens, sustainability ensuring policies meet the needs of both present and future generations, and collaboration across sectors and levels of government. \n\nFinally the Framework lists and expands upon six steps for implementation that may be undertaken by a country in taking action towards Health in All Policies. These are components of action and not a rigid checklist of steps to adhere to. The most important aspect of this policy is that governments should adapt the policy to suit the needs of their citizens, their socioeconomic situation, and their governance system.\n\n\n\n"}
{"id": "20611030", "url": "https://en.wikipedia.org/wiki?curid=20611030", "title": "Ejaculation", "text": "Ejaculation\n\nEjaculation is the discharge of semen (normally containing sperm) from the male reproductory tract, usually accompanied by orgasm. It is the final stage and natural objective of male sexual stimulation, and an essential component of natural conception. In rare cases, ejaculation occurs because of prostatic disease. Ejaculation may also occur spontaneously during sleep (a nocturnal emission or \"wet dream\"). \"Anejaculation\" is the condition of being unable to ejaculate. \"Dysejaculation\" is an ejaculation that is painful or uncomfortable. Retrograde ejaculation is the condition where semen travels backwards into the bladder rather than out the urethra.\n\nA usual precursor to ejaculation is the sexual arousal of the male, leading to the erection of the penis, though not every arousal nor erection leads to ejaculation. Penile sexual stimulation during masturbation or vaginal, anal, oral, or non-penetrative sexual activity may provide the necessary stimulus for a man to achieve orgasm and ejaculation. With regard to intravaginal ejaculation latency time, men typically reach orgasm 5–7 minutes after the start of penile-vaginal intercourse, taking into account their desires and those of their partners, but 10 minutes is also a common intravaginal ejaculation latency time. A prolonged stimulation either through foreplay (kissing, petting and direct stimulation of erogenous zones before penetration during intercourse) or stroking (during masturbation) leads to an adequate amount of arousal and production of pre-ejaculatory fluid. While the presence of sperm in pre-ejaculatory fluid is thought to be rare, sperm from an earlier ejaculation, still present in the urethra, may be picked up by pre-ejaculatory fluid. In addition, infectious agents (including HIV) can often be present in pre-ejaculate.\n\nPremature ejaculation is when ejaculation occurs before the desired time. If a man is unable to ejaculate in a timely manner after prolonged sexual stimulation, in spite of his desire to do so, it is called delayed ejaculation or anorgasmia. An orgasm that is not accompanied by ejaculation is known as a dry orgasm.\n\nWhen a man has achieved a sufficient level of stimulation, the orgasm and ejaculation begins. At that point, under the control of the sympathetic nervous system, semen containing sperm is produced (emission). The semen is ejected through the urethra with rhythmic contractions. These rhythmic contractions are part of the male orgasm. They are generated by the bulbospongiosus and pubococcygeus muscles under the control of a spinal reflex at the level of the spinal nerves S2–4 via the pudendal nerve. The typical male orgasm lasts several seconds.\n\nAfter the start of orgasm, pulses of semen begin to flow from the urethra, reach a peak discharge and then diminish in flow. The typical orgasm consists of 10 to 15 contractions, although the man is unlikely to be consciously aware of that many. Once the first contraction has taken place, ejaculation will continue to completion as an involuntary process. At this stage, ejaculation cannot be stopped. The rate of contractions gradually slows during the orgasm. Initial contractions occur at an average interval of 0.6 seconds with an increasing increment of 0.1 seconds per contraction. Contractions of most men proceed at regular rhythmic intervals for the duration of the orgasm. Many men also experience additional irregular contractions at the conclusion of the orgasm.\n\nEjaculation usually begins during the first or second contraction of orgasm. For most men, the first ejection of semen occurs during the second contraction, while the second is typically the largest expelling 40% or more of total semen discharge. After this peak, the magnitude of semen the penis emits diminishes as the contractions begin to lessen in intensity. The muscle contractions of the orgasm can continue after ejaculation with no additional semen discharge occurring. A small sample study of seven men showed an average of 7 spurts of semen followed by an average of 10 more contractions with no semen expelled. This study also found a high correlation between number of spurts of semen and total ejaculate volume, i.e., larger semen volumes resulted from additional pulses of semen rather than larger individual spurts.\n\nAlfred Kinsey measured the distance of ejaculation, in \"some hundreds\" of men. In three-quarters of men tested, ejaculate \"is propelled with so little force that the liquid is not carried more than a minute distance beyond the tip of the penis.\" In contrast to those test subjects, Kinsey noted \"In other males the semen may be propelled from a matter of some inches to a foot or two, or even as far as five or six and (rarely) eight feet\". Masters and Johnson report ejaculation distance to be no greater than . During the series of contractions that accompany ejaculation, semen is propelled from the urethra at , close to .\n\nMost men experience a refractory period immediately following an orgasm, during which time they are unable to achieve another erection, and a longer period again before they are capable of achieving another ejaculation. During this time a male feels a deep and often pleasurable sense of relaxation, usually felt in the groin and thighs. The duration of the refractory period varies considerably, even for a given individual. Age affects the recovery time, with younger men typically recovering faster than older men, though not universally so.\n\nWhereas some men may have refractory periods of 15 minutes or more, some men are able to experience sexual arousal immediately after ejaculation. A short recovery period may allow partners to continue sexual play relatively uninterrupted by ejaculation. Some men may experience their penis becoming hypersensitive to stimulation after ejaculation, which can make sexual stimulation unpleasant even while they may be sexually aroused.\n\nThere are men who are able to achieve multiple orgasms, with or without the typical sequence of ejaculation and refractory period. Some of those men report not noticing refractory periods, or are able to maintain erection by \"sustaining sexual activity with a full erection until they passed their refractory time for orgasm when they proceeded to have a second or third orgasm\".\n\nThe force and amount of semen that will be ejected during an ejaculation will vary widely between men and may contain between 0.1 and 10 milliliters (by way of comparison, note that a teaspoon is 5 ml and a tablespoon holds 15 ml). Adult semen volume is affected by the time that has passed since the previous ejaculation; larger semen volumes are seen with greater durations of abstinence. The duration of the stimulation leading up to the ejaculation can affect the volume. Abnormally low semen volume is known as hypospermia. One of the possible underlying causes of low volume or complete lack of semen is ejaculatory duct obstruction. It is normal for the amount of semen to diminish with age.\n\nThe number of sperm in an ejaculation also varies widely, depending on many factors, including the time since the last ejaculation, age, stress levels, and testosterone. Greater lengths of sexual stimulation immediately preceding ejaculation can result in higher concentrations of sperm. An unusually low sperm count, not the same as low semen volume, is known as oligospermia, and the absence of any sperm from the semen is termed azoospermia.\n\nThe first ejaculation in males often occurs about 12 months after the onset of puberty, generally through masturbation or nocturnal emission (wet dreams). This first semen volume is small. The typical ejaculation over the following three months produces less than 1 ml of semen. The semen produced during early puberty is also typically clear. After ejaculation this early semen remains jellylike and, unlike semen from mature males, fails to liquefy. A summary of semen development is shown in Table 1.\n\nMost first ejaculations (90 percent) lack sperm. Of the few early ejaculations that do contain sperm, the majority of sperm (97%) lack motion. The remaining sperm (3%) have abnormal motion.\n\nAs the male proceeds through puberty, the semen develops mature characteristics with increasing quantities of normal sperm. Semen produced 12 to 14 months after the first ejaculation liquefies after a short period of time. Within 24 months of the first ejaculation, the semen volume and the quantity and characteristics of the sperm match that of adult male semen.\nEjaculate is jellylike and fails to liquefy.\nMost samples liquefy. Some remain jellylike.\nEjaculate liquefies within an hour.\n\nThere is a central pattern generator in the spinal cord, made up of groups of spinal interneurons, that is involved in the rhythmic response of ejaculation. This is known as the \"spinal generator for ejaculation\".\n\nTo map the neuronal activation of the brain during the ejaculatory response, researchers have studied the expression of c-Fos, a proto-oncogene expressed in neurons in response to stimulation by hormones and neurotransmitters. Expression of c-Fos in the following areas has been observed:\n\nAlthough uncommon, some men can achieve ejaculations during masturbation without any manual stimulation. Such men usually do it by tensing and flexing their abdominal and buttocks muscles along with vigorous fantasising. Others may do it by relaxing the area around the penis, which may result in harder erections especially when hyperaroused.\n\nPerineum pressing results in an ejaculation which is purposefully held back by pressing on either the perineum or the urethra to force the seminal fluid to remain inside. In such a scenario, the seminal fluid stays inside the body and goes to the bladder. Some people do this to avoid making a mess by keeping all the semen inside. As a medical condition, it is called retrograde ejaculation.\n\nFor most men, no detrimental health effects have been determined from ejaculation itself or from frequent ejaculations, though sexual activity in general can have health or psychological consequences. A small fraction of men have a disease called postorgasmic illness syndrome (POIS), which causes severe muscle pain throughout the body and other symptoms immediately following ejaculation. The symptoms last for up to a week. Some doctors speculate that the frequency of POIS \"in the population may be greater than has been reported in the academic literature\", and that many POIS sufferers are undiagnosed.\n\nIt is not clear whether frequent ejaculation increases, reduces or has no effect on the risk of prostate cancer. Two large studies: \"Ejaculation Frequency and Subsequent Risk of Prostate Cancer\" and \"Sexual Factors and Prostate Cancer\" suggest that frequent ejaculation over a lifetime offers some protection against prostate cancer. The US study involving \"29,342 US men aged 46 to 81 years\" suggest that \"high ejaculation frequency was related to decreased risk of total prostate cancer\". An Australian study involving \"1,079 men with prostate cancer and 1,259 healthy men\" found that \"there is evidence that the more frequently men ejaculate between the ages of 20 and 50, the less likely they are to develop prostate cancer\":\n\nIn mammals and birds, multiple ejaculation is commonplace. During copulation, each side of a short-beaked echidna's penis is used alternately, with the other half being shut down between ejaculations.\n\nIn stallions, ejaculation is accompanied by a motion of the tail known as \"tail flagging\". When a male wolf ejaculates, his final pelvic thrust may be slightly prolonged. A male rhesus monkey usually ejaculates less than 15 seconds after sexual penetration. The first report and footage of spontaneous ejaculation in an aquatic mammal was recorded in a wild Indo-Pacific bottlenose dolphin near Mikura Island, Japan in 2012.\n\nIn horses, sheep, and cattle, ejaculation lasts for several seconds or fractions of a second, but in boars, it can last for 10–30 minutes or 5–10 minutes. Ejaculation in boars is stimulated when the spiral-shaped glans penis interlocks with the female's cervix. A mature boar can produce of semen during one ejaculation. In llamas and alpacas, ejaculation occurs continuously during copulation.\n\nThe semen of male dogs is ejaculated in three separate fractions. The third fraction is produced during the copulatory tie, and consists mainly of prostatic fluid.\n\n\n"}
{"id": "51959628", "url": "https://en.wikipedia.org/wiki?curid=51959628", "title": "Eleanor Jane Taylor Calverley", "text": "Eleanor Jane Taylor Calverley\n\nEleanor Calverley (1887–1968) was the first medical missionary in Kuwait to gain the trust of Arab women who were forbidden to see male physicians.\n\nBorn in Woodstock, New Jersey, on March 24, 1887 to William Lewis and Jane Long Hillman Taylor, Calverley was educated in public schools of New Haven, Connecticut. She later graduated in 1908 after pursuing a medical education at the Women's Medical College of Pennsylvania. On September 6, 1909, Eleanor married Edwin E. Calverley, a missionary and preacher, with whom she trained for work in the Arabian Peninsula. They traveled together to Kuwait in 1911, and worked there for many years. In time, their family grew to include three daughters: Grace, Elisabeth and Eleanor.\n\nTo provide medical care to the general population and the Kuwaiti women in particular, she opened a small dispensary connected to her home. In 1919, under Elanor's leadership, a women's hospital was opened.\n\n"}
{"id": "4161179", "url": "https://en.wikipedia.org/wiki?curid=4161179", "title": "Electronic common technical document", "text": "Electronic common technical document\n\nThe electronic common technical document (eCTD) is an interface and international specification for the pharmaceutical industry to agency transfer of regulatory information.\nThe specification is based on the Common Technical Document (CTD) format and was developed by the International Conference on Harmonisation (ICH) Multidisciplinary Group 2 Expert Working Group (ICH M2 EWG).\n\nVersion 2.0 of eCTD – an upgrade over the original CTD – was finalized on February 12, 2002, and version 3.0 was finalized on October 8 of the same year. , the most current version is 3.2.2, released on July 16, 2008.\n\nA Draft Implementation Guide for version 4.0 of eCTD was released in August 2012. However, work stalled on the project. An additional Draft Implementation Guide was released in February 2015 Draft specifications and guides were issued in April 2016 by the ICH and the FDA, followed by a May 13 ICH \"teleconference to discuss the guidance and any questions and clarifications needed.\"\n\nOn May 5, 2015, the U.S. Food & Drug Administration published a final, binding guidance document requiring certain submissions in electronic (eCTD) format within 24 months. The projected date for mandatory electronic submissions is May 5, 2017 for New Drug Applications (NDAs), Biologic License Applications (BLAs), Abbreviated New Drug Applications (ANDAs) and Drug Master Files (DMFs).\n\nThe E.U. and its European Medicines Agency began accepting eCTD submissions in 2003. However, in February 2015, the \"EMA announced it would no longer accept paper application forms for products applying to the centralized procedure beginning 1 July 2015.\" The EMA verified on that date that it would no longer accept \"human and veterinary centralised procedure applications\" and that all electronic application forms would have to be eCTD by January 2016.\n\nIn Nov 2017, China Food and Drug Administration (CFDA) publishes draft eCTD structure for drug registration for public consultations. This is a big transition for China to move from paper submission to eCTD submissions. \n\nAn eCTD submission's structure is largely defined by the primary standard created by the ICH, the Electronic Common Technical Document Specification. However, additional specifications may be applied in national and continental contexts. In the United States, the Food and Drug Administration (FDA) layers additional specifications onto its requirements for eCTD submissions, including PDF, transmission, file format, and supportive file specifications. In the European Union, the European Medicines Agency's EU Module 1 specification as well as other QA documents lay out additional requirements for eCTD submissions.\n\nThe eCTD has five modules:\n\n\nA full table of contents could be quite large.\n\nThere are two categories of modules:\n\n\nThe CTD defines the content only of the common modules. The contents of the Regional Module 1 are defined by each of the ICH \"regions\" (USA, Europe and Japan).\n\nThe eCTD is a message specification for the transfer of files and metadata from a submitter to a receiver. The primary technical components are:\n\n\nEach submission message constitutes one \"sequence\". A cumulative eCTD consists of one or more sequences. While a single sequence may be viewed with web browser and the ICH stylesheet provided, viewing a cumulative eCTD requires specialized eCTD viewers.\n\nThe top part of the directory structure is as follows:\nThe string \"ctd-123456/0000\" is just an example.\n\nThis is the file codice_1 in the \"submission sequence number folder\".\nFor example:\nThe purpose of this file is twofold:\n\n\nStylesheets that support the presentation and navigation should be included. They must be placed in the directory:\n\nSee entry 377 in Appendix 4.\n\nDTDs must be placed in the directory:\n\nSee entries 371–76 in Appendix 4.\nThey must follow a naming convention.\n\nThe DTD of the backbone is in Appendix 8.\nIt must be placed in the above directory.\n\nThe business process to be supported can be described as follows:\n\nThe \"lifecycle management\" is composed at least of:\n\n\n"}
{"id": "50337", "url": "https://en.wikipedia.org/wiki?curid=50337", "title": "Fructose", "text": "Fructose\n\nFructose, or fruit sugar, is a simple ketonic monosaccharide found in many plants, where it is often bonded to glucose to form the disaccharide sucrose. It is one of the three dietary monosaccharides, along with glucose and galactose, that are absorbed directly into blood during digestion. Fructose was discovered by French chemist Augustin-Pierre Dubrunfaut in 1847. The name \"fructose\" was coined in 1857 by the English chemist William Allen Miller. Pure, dry fructose is a sweet, white, odorless, crystalline solid, and is the most water-soluble of all the sugars.\nFructose is found in honey, tree and vine fruits, flowers, berries, and most root vegetables.\n\nCommercially, fructose is derived from sugar cane, sugar beets, and maize. Crystalline fructose is the monosaccharide, dried, ground, and of high purity. High-fructose corn syrup is a mixture of glucose and fructose as monosaccharides. Sucrose is a compound with one molecule of glucose covalently linked to one molecule of fructose. All forms of fructose, including fruits and juices, are commonly added to foods and drinks for palatability and taste enhancement, and for browning of some foods, such as baked goods. About 240,000 tonnes of crystalline fructose are produced annually.\n\nExcessive consumption of fructose may contribute to insulin resistance, obesity, elevated LDL cholesterol and triglycerides, leading to metabolic syndrome, type 2 diabetes and cardiovascular disease. The European Food Safety Authority stated that fructose is preferable over sucrose and glucose in sugar-sweetened foods and beverages because of its lower effect on postprandial blood sugar levels, and also noted that \"high intakes of fructose may lead to metabolic complications such as dyslipidaemia, insulin resistance, and increased visceral adiposity\". Further, the UK’s Scientific Advisory Committee on Nutrition in 2015 disputed the claims of fructose causing metabolic disorders, stating that \"there is insufficient evidence to demonstrate that fructose intake leads to adverse health outcomes independent of any effects related to its presence as a component of total and free sugars.\"\n\nThe word \"fructose\" was coined in 1857 from the Latin for \"fructus\" (fruit) and the generic chemical suffix for sugars, \"-ose\". It is also called fruit sugar and levulose.\n\nFructose is a 6-carbon polyhydroxyketone. Crystalline fructose adopts a cyclic six-membered structure owing to the stability of its hemiketal and internal hydrogen-bonding. This form is formally called -fructopyranose. In solution, fructose exists as an equilibrium mixture of 70% fructopyranose and about 22% fructofuranose, as well as small amounts of three other forms, including the acyclic structure.\n\nFructose may be anaerobically fermented by yeast or bacteria. Yeast enzymes convert sugar (glucose, or fructose) to ethanol and carbon dioxide. The carbon dioxide released during fermentation will remain dissolved in water, where it will reach equilibrium with carbonic acid, unless the fermentation chamber is left open to the air. The dissolved carbon dioxide and carbonic acid produce the carbonation in bottled fermented beverages.\n\nFructose undergoes the Maillard reaction, non-enzymatic browning, with amino acids. Because fructose exists to a greater extent in the open-chain form than does glucose, the initial stages of the Maillard reaction occur more rapidly than with glucose. Therefore, fructose has potential to contribute to changes in food palatability, as well as other nutritional effects, such as excessive browning, volume and tenderness reduction during cake preparation, and formation of mutagenic compounds.\n\nFructose readily dehydrates to give hydroxymethylfurfural (\"HMF\").\nThis process, in the future, may become part of a low-cost, carbon-neutral system to produce replacements for petrol and diesel from plants.\n\nThe primary reason that fructose is used commercially in foods and beverages, besides its low cost, is its high relative sweetness. It is the sweetest of all naturally occurring carbohydrates. The relative sweetness of fructose has been reported in the range of 1.2–1.8 times that of sucrose. However, it is the 6-membered ring form of fructose that is sweeter; the 5-membered ring form tastes about the same as usual table sugar. Warming fructose leads to formation of the 5-membered ring form. Therefore, the relative sweetness decreases with increasing temperature. However it has been observed that the absolute sweetness of fructose is identical at 5 °C as 50 °C and thus the relative sweetness to sucrose is not due to anomeric distribution but a decrease in the absolute sweetness of sucrose at lower temperatures.\n\nThe sweetness of fructose is perceived earlier than that of sucrose or glucose, and the taste sensation reaches a peak (higher than that of sucrose) and diminishes more quickly than that of sucrose. Fructose can also enhance other flavors in the system.\n\nFructose exhibits a sweetness synergy effect when used in combination with other sweeteners. The relative sweetness of fructose blended with sucrose, aspartame, or saccharin is perceived to be greater than the sweetness calculated from individual components.\n\nFructose has higher solubility than other sugars as well as other sugar alcohols. Fructose is, therefore, difficult to crystallize from an aqueous solution. Sugar mixes containing fructose, such as candies, are softer than those containing other sugars because of the greater solubility of fructose.\n\nFructose is quicker to absorb moisture and slower to release it to the environment than sucrose, glucose, or other nutritive sweeteners. Fructose is an excellent humectant and retains moisture for a long period of time even at low relative humidity (RH). Therefore, fructose can contribute a more palatable texture, and longer shelf life to the food products in which it is used.\n\nFructose has a greater effect on freezing point depression than disaccharides or oligosaccharides, which may protect the integrity of cell walls of fruit by reducing ice crystal formation. However, this characteristic may be undesirable in soft-serve or hard-frozen dairy desserts.\n\nFructose increases starch viscosity more rapidly and achieves a higher final viscosity than sucrose because fructose lowers the temperature required during gelatinizing of starch, causing a greater final viscosity.\n\nAlthough some artificial sweeteners are not suitable for home-baking, many traditional recipes use fructose.\n\nNatural sources of fructose include fruits, vegetables (including sugar cane), and honey. Fructose is often further concentrated from these sources. The highest dietary sources of fructose, besides pure crystalline fructose, are foods containing table sugar (sucrose), high-fructose corn syrup, agave nectar, honey, molasses, maple syrup, fruit and fruit juices, as these have the highest percentages of fructose (including fructose in sucrose) per serving compared to other common foods and ingredients. Fructose exists in foods either as a free monosaccharide or bound to glucose as sucrose, a disaccharide. Fructose, glucose, and sucrose may all be present in a food; however, different foods will have varying levels of each of these three sugars.\n\nThe sugar contents of common fruits and vegetables are presented in Table 1. In general, in foods that contain free fructose, the ratio of fructose to glucose is approximately 1:1; that is, foods with fructose usually contain about an equal amount of free glucose. A value that is above 1 indicates a higher proportion of fructose to glucose, and below 1 a lower proportion. Some fruits have larger proportions of fructose to glucose compared to others. For example, apples and pears contain more than twice as much free fructose as glucose, while for apricots the proportion is less than half as much fructose as glucose.\n\nApple and pear juices are of particular interest to pediatricians because the high concentrations of free fructose in these juices can cause diarrhea in children. The cells (enterocytes) that line children's small intestines have less affinity for fructose absorption than for glucose and sucrose. Unabsorbed fructose creates higher osmolarity in the small intestine, which draws water into the gastrointestinal tract, resulting in osmotic diarrhea. This phenomenon is discussed in greater detail in the Health Effects section.\n\nTable 1 also shows the amount of sucrose found in common fruits and vegetables. Sugarcane and sugar beet have a high concentration of sucrose, and are used for commercial preparation of pure sucrose. Extracted cane or beet juice is clarified, removing impurities; and concentrated by removing excess water. The end-product is 99.9%-pure sucrose. Sucrose-containing sugars include common table white granulated sugar and powdered sugar, as well as brown sugar.\n\nAll data with a unit of g (gram) are based on 100 g of a food item.\nThe fructose/glucose ratio is calculated by dividing the sum of free fructose plus half sucrose by the sum of free glucose plus half sucrose.\n\nFructose is also found in the manufactured sweetener, high-fructose corn syrup (HFCS), which is produced by treating corn syrup with enzymes, converting glucose into fructose. The common designations for fructose content, HFCS-42 and HFCS-55, indicate the percentage of fructose present in HFCS. HFCS-55 is commonly used as a sweetener for soft drinks, whereas HFCS-42 is used to sweeten processed foods, breakfast cereals, bakery foods, and some soft drinks.\n\nData obtained from Kretchmer, N. & Hollenbeck, CB (1991). Sugars and Sweeteners, Boca Raton, FL: CRC Press, Inc. for HFCS, and USDA for fruits and vegetables and the other refined sugars.\n\nCane and beet sugars have been used as the major sweetener in food manufacturing for centuries. However, with the development of HFCS, a significant shift occurred in the type of sweetener consumption in certain countries, particularly the United States. Contrary to the popular belief, however, with the increase of HFCS consumption, the total fructose intake relative to the total glucose intake has not dramatically changed. Granulated sugar is 99.9%-pure sucrose, which means that it has equal ratio of fructose to glucose. The most commonly used forms of HFCS, HFCS-42, and HFCS-55, have a roughly equal ratio of fructose to glucose, with minor differences. HFCS has simply replaced sucrose as a sweetener. Therefore, despite the changes in the sweetener consumption, the ratio of glucose to fructose intake has remained relatively constant.\n\nProviding 368 kcal per 100 grams of dry powder (table), fructose has 95% the caloric value of sucrose by weight. Fructose powder is 100% carbohydrates and supplies no nutrients in significant content (table).\n\nFructose exists in foods either as a monosaccharide (free fructose) or as a unit of a disaccharide (sucrose). Free fructose is absorbed directly by the intestine. When fructose is consumed in the form of sucrose, it is digested (broken down) and then absorbed as free fructose. As sucrose comes into contact with the membrane of the small intestine, the enzyme sucrase catalyzes the cleavage of sucrose to yield one glucose unit and one fructose unit, which are then each absorbed. After absorption, it enters the hepatic portal vein and is directed toward the liver.\n\nThe mechanism of fructose absorption in the small intestine is not completely understood. Some evidence suggests active transport, because fructose uptake has been shown to occur against a concentration gradient. However, the majority of research supports the claim that fructose absorption occurs on the mucosal membrane via facilitated transport involving GLUT5 transport proteins. Since the concentration of fructose is higher in the lumen, fructose is able to flow down a concentration gradient into the enterocytes, assisted by transport proteins. Fructose may be transported out of the enterocyte across the basolateral membrane by either GLUT2 or GLUT5, although the GLUT2 transporter has a greater capacity for transporting fructose, and, therefore, the majority of fructose is transported out of the enterocyte through GLUT2.\n\nThe absorption capacity for fructose in monosaccharide form ranges from less than 5 g to 50 g (per individual serving) and adapts with changes in dietary fructose intake. Studies show the greatest absorption rate occurs when glucose and fructose are administered in equal quantities. When fructose is ingested as part of the disaccharide sucrose, absorption capacity is much higher because fructose exists in a 1:1 ratio with glucose. It appears that the GLUT5 transfer rate may be saturated at low levels, and absorption is increased through joint absorption with glucose. One proposed mechanism for this phenomenon is a glucose-dependent cotransport of fructose.\nIn addition, fructose transfer activity increases with dietary fructose intake. The presence of fructose in the lumen causes increased mRNA transcription of GLUT5, leading to increased transport proteins. High-fructose diets (>2.4 g/kg body wt) increase transport proteins within three days of intake.\n\nSeveral studies have measured the intestinal absorption of fructose using the hydrogen breath test. These studies indicate that fructose is not completely absorbed in the small intestine. When fructose is not absorbed in the small intestine, it is transported into the large intestine, where it is fermented by the colonic flora. Hydrogen is produced during the fermentation process and dissolves into the blood of the portal vein. This hydrogen is transported to the lungs, where it is exchanged across the lungs and is measurable by the hydrogen breath test. The colonic flora also produces carbon dioxide, short-chain fatty acids, organic acids, and trace gases in the presence of unabsorbed fructose. The presence of gases and organic acids in the large intestine causes gastrointestinal symptoms such as bloating, diarrhea, flatulence, and gastrointestial pain Exercise immediately after consumption can exacerbate these symptoms by decreasing transit time in the small intestine, resulting in a greater amount of fructose emptied into the large intestine.\n\nAll three dietary monosaccharides are transported into the liver by the GLUT2 transporter. Fructose and galactose are phosphorylated in the liver by fructokinase (K= 0.5 mM) and galactokinase (K = 0.8 mM), respectively. By contrast, glucose tends to pass through the liver (K of hepatic glucokinase = 10 mM) and can be metabolised anywhere in the body. Uptake of fructose by the liver is not regulated by insulin. However, insulin is capable of increasing the abundance and functional activity of GLUT5 in skeletal muscle cells.\n\nThe initial catabolism of fructose is sometimes referred to as fructolysis, in analogy with glycolysis, the catabolism of glucose. In fructolysis, the enzyme fructokinase initially produces fructose 1-phosphate, which is split by aldolase B to produce the trioses dihydroxyacetone phosphate (DHAP) and glyceraldehyde . Unlike glycolysis, in fructolysis the triose glyceraldehyde lacks a phosphate group. A third enzyme, triokinase, is therefore required to phosphorylate glyceraldehyde, producing glyceraldehyde 3-phosphate. The resulting trioses are identical to those obtained in glycolysis and can enter the gluconeogenic pathway for glucose or glycogen synthesis, or be further catabolized through the lower glycolytic pathway to pyruvate.\n\nThe first step in the metabolism of fructose is the phosphorylation of fructose to fructose 1-phosphate by fructokinase, thus trapping fructose for metabolism in the liver. Fructose 1-phosphate then undergoes hydrolysis by aldolase B to form DHAP and glyceraldehydes; DHAP can either be isomerized to glyceraldehyde 3-phosphate by triosephosphate isomerase or undergo reduction to glycerol 3-phosphate by glycerol 3-phosphate dehydrogenase. The glyceraldehyde produced may also be converted to glyceraldehyde 3-phosphate by glyceraldehyde kinase or further converted to glycerol 3-phosphate by glycerol 3-phosphate dehydrogenase. The metabolism of fructose at this point yields intermediates in the gluconeogenic pathway leading to glycogen synthesis as well as fatty acid and triglyceride synthesis.\n\nThe resultant glyceraldehyde formed by aldolase B then undergoes phosphorylation to glyceraldehyde 3-phosphate. Increased concentrations of DHAP and glyceraldehyde 3-phosphate in the liver drive the gluconeogenic pathway toward glucose and subsequent glycogen synthesis. It appears that fructose is a better substrate for glycogen synthesis than glucose and that glycogen replenishment takes precedence over triglyceride formation. Once liver glycogen is replenished, the intermediates of fructose metabolism are primarily directed toward triglyceride synthesis.\n\nCarbons from dietary fructose are found in both the free fatty acid and glycerol moieties of plasma triglycerides. High fructose consumption can lead to excess pyruvate production, causing a buildup of Krebs cycle intermediates. Accumulated citrate can be transported from the mitochondria into the cytosol of hepatocytes, converted to acetyl CoA by citrate lyase and directed toward fatty acid synthesis. In addition, DHAP can be converted to glycerol 3-phosphate, providing the glycerol backbone for the triglyceride molecule. Triglycerides are incorporated into very-low-density lipoproteins (VLDL), which are released from the liver destined toward peripheral tissues for storage in both fat and muscle cells.\n\nIn a meta-analysis of clinical trials with controlled feeding — where test subjects were fed a fixed amount of energy rather than being allowed to choose the amount they ate — fructose was not an independent factor for weight gain; however, fructose consumption was associated with weight gain when the fructose provided excess calories.\n\nAn expert panel of the European Food Safety Authority concluded that fructose is preferred in food and beverage manufacturing to replace sucrose and glucose due to the lower effect of fructose on blood glucose levels following a meal. However, as a common sweetening agent for foods and beverages, fructose has been associated with increased risk of obesity, diabetes, and cardiovascular disorders that are part of metabolic syndrome. Clinical research has provided no or only limited direct evidence that fructose itself is associated with elevated LDL cholesterol and triglycerides leading to metabolic syndrome, but rather indicates that excessive consumption of sugar-sweetened foods and beverages, and the concurrent increase in calorie intake, underlies metabolic syndrome. Similarly, increased consumption of sweetened foods and beverages raises risk of cardiovascular disease, including hypertension, but there is no direct cause and effect relationship in humans showing that fructose is the causative factor.\n\nFructose is often recommended for diabetics because it does not trigger the production of insulin by pancreatic β cells, probably because β cells have low levels of GLUT5, For a 50 gram reference amount, fructose has a glycemic index of 23, compared with 100 for glucose and 60 for sucrose. Fructose is also 73% sweeter than sucrose at room temperature, allowing diabetics to use less of it per serving. Fructose consumed before a meal may reduce the glycemic response of the meal. Fructose-sweetened food and beverage products cause less of a rise in blood glucose levels than do those manufactured with sucrose or glucose.\n\n"}
{"id": "22180696", "url": "https://en.wikipedia.org/wiki?curid=22180696", "title": "GIS and public health", "text": "GIS and public health\n\nGeographic information systems (GISs) and geographic information science (GIScience) combine computer-mapping capabilities with additional database management and data analysis tools. Commercial GIS systems are very powerful and have touched many applications and industries, including environmental science, urban planning, agricultural applications, and others.\n\nPublic health is another focus area that has made increasing use of GIS techniques. A strict definition of public health is difficult to pin down, as it is used in different ways by different groups. In general, public health differs from personal health in that it is (1) focused on the health of populations rather than of individuals, (2) focused more on prevention than on treatment, and (3) operates in a mainly governmental (rather than private) context. These efforts fall naturally within the domain of problems requiring use of spatial analysis as part of the solution, and GIS and other spatial analysis tools are therefore recognized as providing potentially transformational capabilities for public health efforts.\n\nThis article presents some history of use of geographic information and geographic information systems in public health application areas, provides some examples showing the utilization of GIS techniques in solving specific public health problems, and finally addresses several potential issues arising from increased use of these GIS techniques in the public health arena.\n\nPublic health efforts have been based on analysis and use of spatial data for many years. Dr. John Snow (physician), often credited as the father of epidemiology, is arguably the most famous of those examples. Dr. Snow used a hand-drawn map to analyze the geographic locations of deaths related to cholera in London in the mid-1850s. His map, which superimposed the locations of cholera deaths with those of public water supplies, pinpointed the Broad Street pump as the most likely source of the cholera outbreak. Removal of the pump handle led to a rapid decline in the incidence of cholera, helping the medical community to eventually conclude that cholera was a water-borne disease. \n\nDr. Snow's work provides an indication of how a GIS could benefit public health investigations and other research. He continued to analyze his data, eventually showing that the incidence rate of cholera was also related to local elevation as well as soil type and alkalinity. Low-lying areas, particularly those with poorly draining soil, were found to have higher incidence rates for cholera, which Dr. Snow attributed to the pools of water that tended to collect there, again showing evidence that cholera was in fact a water-borne disease (rather than one borne by 'miasma' as was commonly believed at the time.\n\nThis is an early example of what has come to be known as disease diffusion mapping, an area of study based on the idea that a disease starts from some source or central point and then spreads throughout the local area according to patterns and conditions there. This is another area of research where the capabilities of a GIS have been shown to be of help to practitioners.\n\nToday’s public health problems are much larger in scope than those Dr. Snow faced, and researchers today depend on modern GIS and other computer mapping applications to assist in their analyses. For example, see the map to the right depicting death rates from heart disease among white males above age 35 in the US between 2000 and 2004. \n\nPublic health informatics (PHI) is an emerging specialty which focuses on the application of information science and technology to public health practice and research. As part of that effort, a GIS – or more generally a spatial decision support system (SDSS) – offers improved geographic visualization techniques, leading to faster, better, and more robust understanding and decision-making capabilities in the public health arena.\n\nFor example, GIS displays have been used to show a clear relationship between clusters of emergent Hepatitis C cases and those of known intravenous drug users in Connecticut. Causality is difficult to prove conclusively – collocation does not establish causation – but confirmation of previously established causal relationships (like intravenous drug use and Hepatitis C) can strengthen acceptance of those relationships, as well as help to demonstrate the utility and reliability of GIS-related solution techniques. Conversely, showing the coincidence of potential causal factors with the ultimate effect can help suggest a potential causal relationship, thereby driving further investigation and analysis (source needed?).\n\nAlternately, GIS techniques have been used to show a lack of correlation between causes and effects or between different effects. For example, the distributions of both birth defects and infant mortality in Iowa were studied, and the researchers found no relationship in those data. This led to the conclusion that birth defects and infant mortality are likely unrelated, and are likely due to different causes and risk factors.\n\nGIS can support public health in different ways as well. First and foremost, GIS displays can help inform proper understanding and drive better decisions. For example, elimination of health disparities is one of two primary goals of Healthy People 2010, one of the preeminent public health programs in existence today in the US. GIS can play a significant role in that effort, helping public health practitioners identify areas of disparities or inequities, and ideally helping them identify and develop solutions to address those shortcomings. GIS can also help researchers integrate disparate data from a wide variety of sources, and can even be used to enforce quality control measures on those data. Much public health data is still manually generated, and is therefore subject to human-generated mistakes and miscoding. For example, geographic analysis of health care data from North Carolina showed that just over 40% of the records contained errors of some sort in the geographic information (city, county, or zip code), errors that would have gone undetected without the visual displays provided by GIS. Correction of these errors led not only to more correct GIS displays, but also improved ALL analyses using those data.\n\nThere are also concerns or issues with use of GIS tools for public health efforts. Chief among those is a concern for privacy and confidentiality of individuals. Public health is concerned about the health of the population as a whole, but must use data on the health of individuals to make many of those assessments, and protecting the privacy and confidentiality of those individuals is of paramount importance. Use of GIS displays and related databases raises the potential of compromising those privacy standards, so some precautions are necessary to avoid pinpointing individuals based on spatial data. For example, data may need to be aggregated to cover larger areas such as a zip code or county, helping to mask individual identities. Maps can also be constructed at smaller scales so that less detail is revealed. Alternately, key identifying features (such as the road and street network) can be left off the maps to mask exact location, or it may even be advisable to intentionally offset the location markers by some random amount if deemed necessary. \nIt is well established in the literature that statistical inference based on aggregated data can lead researchers to erroneous conclusions, suggesting relationships that in fact do not exist or obscuring relationships that do in fact exist. This issue is known as the modifiable areal unit problem. For example, New York public health officials worried that cancer clusters and causes would be misidentified after they were forced to post maps showing cancer cases by ZIP code on the internet. Their assertion was that ZIP codes were designed for a purpose unrelated to public health issues, and so use of these arbitrary boundaries might lead to inappropriate groupings and then to incorrect conclusions.\n\nUse of GIS in public health is an application area still in its infancy. Like most new applications, there is a lot of promise, but also a lot of pitfalls that must be avoided along the way. Many researchers and practitioners are concentrating of this effort, hoping that the benefits outweigh the risks and the costs associated with this emerging application area for modern GIS techniques.\n\n\n"}
{"id": "1660162", "url": "https://en.wikipedia.org/wiki?curid=1660162", "title": "Group purchasing organization", "text": "Group purchasing organization\n\nIn the United States, a group purchasing organization (GPO) is an entity that is created to leverage the purchasing power of a group of businesses to obtain discounts from vendors based on the collective buying power of the GPO members.\n\nMany GPOs are funded by administrative fees that are paid by the vendors that GPOs oversee. Some GPOs are funded by fees paid by the buying members. Some GPOs are funded by a combination of both of these methods. These fees can be set as a percentage of the purchase or set as an annual flat rate. Some GPOs set mandatory participation levels for their members, while others are completely voluntary. Members participate based on their purchasing needs and their level of confidence in what should be competitive pricing negotiated by their GPOs.\n\nGroup purchasing is used in many industries to purchase raw materials and supplies, but it is common practice in the grocery industry, health care, electronics, industrial manufacturing and agricultural industries. As the electrical industry has started deregulating, it has also become an increasing trend there. Also, in recent years, group purchasing has begun to take root in the nonprofit community. Group purchasing amongst nonprofits is still relatively new, but is quickly becoming common place as nonprofits aim to find ways to reduce overhead expenses. In the healthcare field, GPOs have most commonly been accessed by acute-care organizations, but non-profit Community Clinics and Health Centers throughout the U.S. have also been engaging in group purchasing.\n\nThe first healthcare GPO was established in 1910 by the Hospital Bureau of New York. For many decades, healthcare GPOs grew slowly in number, to only 10 in 1962.\n\nMedicare and Medicaid stimulated growth in the number of GPOs to 40 in 1974. That number tripled between 1974 and 1977. The institution of the Medicare Prospective Payment System (PPS) in 1983 focused greater scrutiny on costs and fostered further rapid GPO expansion. In 1986, Congress granted GPOs in healthcare \"Safe Harbor\" from federal anti-kickback statutes after successful lobbying efforts.\nBy 2007, there were hundreds of healthcare GPOs, \"affiliates\" and cooperatives in the United States that were availing themselves of substantial revenues obtained from vendors in the form of administrative fees, or \"remuneration.\" 96 percent of all acute-care hospitals and 98 percent of all community hospitals held at least one GPO membership. Importantly, 97 percent of all not-for-profit, non-governmental hospitals participated in some form of group purchasing.\n\nWith healthcare costs rising sharply in the early 1980s, the federal government revised Medicare from a system of fee-for-service (FFS) payments to PPS, under which hospitals receive a fixed amount for each patient with a given diagnosis. Other insurers also limited what hospitals could charge. The result was a financial squeeze on hospitals, compelling them to seek new ways to manage their costs.\n\nIn specifically exempting GPOs from the Federal Anti-Kickback Law, many healthcare providers interpreted the act as an encouragement to the expansion of GPOs.\nCongress did not specify any limit on contract administration fees, but required the United States Department of Health and Human Services (HHS) to monitor such fees for possible abuse – particularly with respect to fees in excess of 3.0 percent.\n\nIn 1991, HHS promulgated safe harbor regulations, reflecting Congress’ intent to permit contract administration fees and creating the additional safeguard that GPOs inform members of administrative fees in excess of 3.0 percent. Despite these safeguards, the Government Accounting Office (GAO) published a study in 2002 indicating that GPOs did not always in fact reduce the cost of supplies and equipment for hospitals, but in some cases increased these costs by as much as 37%. Further examining the practices of GPOs, the Federal Trade Commission (FTC) clarified that \"safety zone thresholds do not prevent and should not be appropriately read as preventing antitrust challenges to any of the alleged anticompetitive contracting practices...\" of GPOs.\n\nIn 2002, the Senate Judiciary Committee's Antitrust Subcommittee imposed stricter standards on GPOs in healthcare, requiring the adoption of a Code of Conduct to which GPOs must subscribe.\n\nCritics of GPOs charge that, as long as GPOs receive fees from the vendors they are charged with policing, the industry has anti-competitive contracting potential that should be subjected to further scrutiny and/or regulation.\n\nA vertical GPO assists companies and organizations within a specific industry or market segment.\n\nA healthcare group purchasing organization (GPO) assists in promoting quality healthcare relief and assists diverse providers in effectively managing expenses. A GPO aggregates the purchasing volume of its members for various goods and services and develops contracts with suppliers through which members may buy at group price and terms if they choose to. GPOs typically provide contracted discounts on medical supplies, nutrition, pharmacy and laboratory. Some of the large GPOs have expanded contract portfolios to also offer discounts on office supplies and non-medical related services.\nA GPO's earnings come from an \"Administrative\" fee. GPOs may collect an \"Administrative\" fee up to 3.0% of all sales volumes from the vendors that they negotiate a contract from, upon selling products to their member hospitals. These fees do not influence the prices negotiated. They are used to cover the GPO's operating expenses. If there is a remainder it is distributed back to the GPO owners; thus, GPO owners achieve cost-savings on the goods they choose to buy through group contracts, and also receive distributions back from the GPO. General GPO members may also receive some fee share as part of their member agreement, though this practice is no longer typical; thus the primary benefit to a GPO member is the access to deeply discounted pricing.\n\nGPOs submit that their services allow for improved operating margins for healthcare providers, and that members enjoy value added benefits like clinical support, benchmarking data, supply chain support and comprehensive portfolios of products and services to address specific needs.\n\nGPOs vary in their strategy for negotiating discounts with suppliers - from requiring that its members not join other GPOs (exclusivity) to requiring compliance to awarding single source contracts. As the healthcare industry becomes saturated with GPOs, pricing is one way for GPOs to bring in new members or convince members of another GPO to switch.\n\nA foodservice or grocery GPO focuses exclusively on the $600 billion foodservice marketplace, including food and food-related purchasing for multi-unit foodservice operators, contract negotiation and supply chain services. \nThese negotiations are made with supplier/manufacturing agreements and purchasing contracts. Categories for grocer purchases include: poultry, fresh produce, frozen food products, fresh and frozen meats, candy and snacks, dairy and bakery, dry goods, disposables and beverages.\n\nA manufacturer’s GPO succeeds in solving procurement and sourcing concerns by aggregating the demand for products and services used in the manufacturing and production process and delivering deep savings on raw materials, services and components by issuing rebates, discounts, and preferred pricing to its members. \nThe combined buying power helps manufacturers save money on their purchases and more effectively compete against the largest global manufacturers.\n\nA GPO in the electrical industry attempts to help energy consumers and businesses solve issues with the rising cost of electricity by pooling the electrical demand of multiple consumers, grouping them together, and through this, delivering savings to its members by obtaining more favorable electrical rates. The difference between residential and commercial rates can be quite large in many regions, and are at least 7% cheaper in every US state. The buying power of the resulting groups allows companies and individuals alike to save greatly on their electrical bills.\n\nNon-profit GPOs are similar to regular GPOs, but they are typically made of up a board of non-profit, community-based organizations working together to help lower their costs.\n\nWhereas a vertical GPO assists organizations in specific industries, such as health care, food service, legal, dairy, and industrial manufacturing, a horizontal GPO assists companies across a broad spectrum of industries.\n\nA horizontal indirect spend GPO succeeds in reducing procurement costs by aggregating the demand for non-strategic, or indirect cost supplies and services used by a broad horizontal market spectrum of member client organizations by consolidating purchasing power and establishing contracts to achieve preferred pricing, terms, and service standards.\n\nThe resulting combined buying power helps the usually mid-size and larger member client organizations save money on their purchases of categories such as temporary labor services, office products, safety supplies, office equipment, packaging supplies, uniform & laundry services, pest control, and expedited parcel delivery. The consolidation of purchasing effort with the GPO also allows member clients savings from reduced staffing overhead costs. According to an analysis by SpendMatters, from an adoption perspective, 15-20% of the Fortune 1000 currently use buying consortiums and 85% of the time, they’re seeing 10%+ of savings in the categories in which they put through a consortium model.\n\nThe suppliers to this type of GPO offer preferred pricing, terms, and service standards because they experience lower overall selling expenses and the increased volume usually associated with the addition of a single very large customer.\n\n\n"}
{"id": "28695746", "url": "https://en.wikipedia.org/wiki?curid=28695746", "title": "Health effects of pesticides", "text": "Health effects of pesticides\n\nHealth effects of pesticides may be acute or delayed in those who are exposed. A 2007 systematic review found that \"most studies on non-Hodgkin lymphoma and leukemia showed positive associations with pesticide exposure\" and thus concluded that cosmetic use of pesticides should be decreased. Strong evidence also exists for other negative outcomes from pesticide exposure including neurological problems, birth defects, fetal death, and neurodevelopmental disorder.\n\nAccording to The Stockholm Convention on Persistent Organic Pollutants (2001), 9 of the 12 most dangerous and persistent chemicals were pesticides, so many have now been withdrawn from use.\n\nAcute health problems may occur in workers that handle pesticides, such as abdominal pain, dizziness, headaches, nausea, vomiting, as well as skin and eye problems. In China, an estimated half million people are poisoned by pesticides each year, 500 of whom die. Pyrethrins, insecticides commonly used in common bug killers, can cause a potentially deadly condition if breathed in.\n\nMany studies have examined the effects of pesticide exposure on the risk of cancer. Associations have been found with: leukemia, lymphoma, brain, kidney, breast, prostate, pancreas, liver, lung, and skin cancers. This increased risk occurs with both residential and occupational exposures. Increased rates of cancer have been found among farm workers who apply these chemicals. A mother's occupational exposure to pesticides during pregnancy is associated with an increases in her child's risk of leukemia, Wilms' tumor, and brain cancer. Exposure to insecticides within the home and herbicides outside is associated with blood cancers in children.\n\nEvidence links pesticide exposure to worsened neurological outcomes.\n\nThe United States Environmental Protection Agency finished a 10-year review of the organophosphate pesticides following the 1996 Food Quality Protection Act, but did little to account for developmental neurotoxic effects, drawing strong criticism from within the agency and from outside researchers. Comparable studies have not been done with newer pesticides that are replacing organophosphates.\n\nStrong evidence links pesticide exposure to birth defects, fetal death and altered fetal growth. Agent Orange, a 50:50 mixture of 2,4,5-T and 2,4-D, has been associated with bad health and genetic effects in Malaya and Vietnam. It was also found that offspring that were at some point exposed to pesticides had a low birth weight and had developmental defects.\n\nA number of pesticides including dibromochlorophane and 2,4-D has been associated with impaired fertility in males.\nPesticide exposure resulted in reduced fertility in males, genetic alterations in sperm, a reduced number of sperm, damage to germinal epithelium and altered hormone function.\n\nSome studies have found increased risks of dermatitis in those exposed.\n\nAdditionally, studies have indicated that pesticide exposure is associated with long-term respiratory problems. Summaries of peer-reviewed research have examined the link between pesticide exposure and neurologic outcomes and cancer, perhaps the two most significant things resulting in organophosphate-exposed workers.\n\nAccording to researchers from the National Institutes of Health (NIH), licensed pesticide applicators who used chlorinated pesticides on more than 100 days in their lifetime were at greater risk of diabetes. One study found that associations between specific pesticides and incident diabetes ranged from a 20 percent to a 200 percent increase in risk. New cases of diabetes were reported by 3.4 percent of those in the lowest pesticide use category compared with 4.6 percent of those in the highest category. Risks were greater when users of specific pesticides were compared with applicators who never applied that chemical.\n\nPeople can be exposed to pesticides by a number of different routes including: occupation, in the home, at school and in their food.\n\nThere are concerns that pesticides used to control pests on food crops are dangerous to people who consume those foods. These concerns are one reason for the organic food movement. Many food crops, including fruits and vegetables, contain pesticide residues after being washed or peeled. Chemicals that are no longer used but that are resistant to breakdown for long periods may remain in soil and water and thus in food.\n\nThe United Nations Codex Alimentarius Commission has recommended international standards for maximum residue limits (MRLs), for individual pesticides in food.\n\nIn the EU, MRLs are set by DG-SANCO.\n\nIn the United States, levels of residues that remain on foods are limited to tolerance levels that are established by the U.S. Environmental Protection Agency and are considered safe. The EPA sets the tolerances based on the toxicity of the pesticide and its breakdown products, the amount and frequency of pesticide application, and how much of the pesticide (i.e., the residue) remains in or on food by the time it is marketed and prepared. Tolerance levels are obtained using scientific risk assessments that pesticide manufacturers are required to produce by conducting toxicological studies, exposure modeling and residue studies before a particular pesticide can be registered, however, the effects are tested for single pesticides, and there is little information on possible synergistic effects of exposure to multiple pesticide traces in the air, food and water.\n\nStrawberries and tomatoes are the two crops with the most intensive use of soil fumigants. They are particularly vulnerable to several type of diseases, insects, mites, and parasitic worms. In 2003, in California alone, 3.7 million pounds (1,700 metric tons) of metham sodium were used on tomatoes. In recent years other farmers have demonstrated that it is possible to produce strawberries and tomatoes without the use of harmful chemicals and in a cost-effective way.\n\nExposure routes other than consuming food that contains residues, in particular pesticide drift, are potentially significant to the general public.\n\nSome pesticides can remain in the environment for prolonged periods of time. For example, most people in the United States still have detectable levels of DDT in their bodies even though it was banned in the US in 1972.\n\nPesticides exposure cannot be studied in placebo controlled trials as this would be unethical. A definitive cause effect relationship therefore cannot be established. Consistent evidence can and has been gathered through other study designs. The precautionary principle is thus frequently used in environmental law such that absolute proof is not required before efforts to decrease exposure to potential toxins are enacted.\n\nThe American Medical Association recommend limiting exposure to pesticides. They came to this conclusion due to the fact that surveillance systems currently in place are inadequate to determine problems related to exposure. The utility of applicator certification and public notification programs are also of unknown value in their ability to prevent adverse outcomes.\n\nThe World Health Organization and the UN Environment Programme estimate that each year, 3 million workers in agriculture in the developing world experience severe poisoning from pesticides, about 18,000 of whom die. According to one study, as many as 25 million workers in developing countries may suffer mild pesticide poisoning yearly. Detectable levels of 50 different pesticides were found in the blood of a representative sample of the U.S. population.\n\nConcerns regarding conflict of interests regarding the research base have been raised. After his death Richard Doll of the Imperial Cancer Research Fund in England was found to have undisclosed ties to industry funding.\n\nA number of pesticides including the neonicotinoids clothianidin, dinotefuran, imidacloprid are toxic to bees. Exposure to pesticides may be one of the contributory factors to colony collapse disorder.\nA study in North Carolina indicated that more than 30 percent of the quail tested were made sick by one aerial insecticide application. Once sick, wild birds may neglect their young, abandon their nests, and become more susceptible to predators or disease.\n\n"}
{"id": "46750253", "url": "https://en.wikipedia.org/wiki?curid=46750253", "title": "Hepatitis Testing Day", "text": "Hepatitis Testing Day\n\nHepatitis Testing Day is May 19 in the United States.\n"}
{"id": "31974016", "url": "https://en.wikipedia.org/wiki?curid=31974016", "title": "History of USDA nutrition guides", "text": "History of USDA nutrition guides\n\nThe history of USDA nutrition guides includes over 100 years of American nutrition advice. The guides have been updated over time, to adopt new scientific findings and new public health marketing techniques. The current guidelines are the Dietary Guidelines for Americans 2015 - 2020. Over time they have described from 4 to 11 food groups. Various guides have been criticized as not accurately representing scientific information about optimal nutrition, and as being overly influenced by the agricultural industries the USDA promotes.\n\nThe USDA's first nutrition guidelines were published in 1894 by Dr. Wilbur Olin Atwater as a farmers' bulletin. In Atwater's 1904 publication titled \"Principles of Nutrition and Nutritive Value of Food,\" he advocated variety, proportionality and moderation; measuring calories; and an efficient, affordable diet that focused on nutrient-rich foods and less fat, sugar and starch. This information preceded the discovery of individual vitamins beginning in 1910.\n\nA new guide in 1916, \"Food for Young Children\" by nutritionist Caroline Hunt, categorized foods into milk and meat; cereals; vegetables and fruits; fats and fatty foods; and sugars and sugary foods. \"How to Select Food\" in 1917 promoted these five food groups to adults, and the guidelines remained in place through the 1920s. In 1933, the USDA introduced food plans at four different cost levels in response to the Great Depression.\n\nIn 1941, the first Recommended Dietary Allowances were created, listing specific intakes for calories, protein, iron, calcium, and vitamins A, B, B B, C and D.\n\nIn 1943, during World War II, The USDA introduced a nutrition guide promoting the \"Basic 7\" food groups to help maintain nutritional standards under wartime food rationing. The Basic 7 food groups were:\n\n\nFrom 1956 until 1992 the United States Department of Agriculture recommended its \"Basic Four\" food groups. These food groups were:\n\n\n\"Other foods\" were said to round out meals and satisfy appetites. These included additional servings from the Basic Four, or foods such as butter, margarine, salad dressing and cooking oil, sauces, jellies and syrups.\n\nThe Basic Four guide was omnipresent in nutrition education in the United States. A notable example is the 1972 series Mulligan Stew, providing nutrition education for schoolchildren in reruns until 1981.\n\nThe introduction of the USDA's food guide pyramid in 1992 attempted to express the recommended servings of each food group, which previous guides did not do. 6 to 11 servings of bread, cereal, rice and pasta occupied the large base of the pyramid; followed by 3 to 5 servings of vegetables; then fruits (2 to 4); then milk, yogurt and cheese (2 to 3); followed by meat, poultry, fish, dry beans, eggs, and nuts (2 to 3); and finally fats, oils and sweets in the small apex (to be used sparingly). Inside each group were several images of representative foods, as well as symbols representing the fat and sugar contents of the foods.\n\nA modified food pyramid was proposed for adults aged over 70. This \"Modified Food Pyramid for 70+ Adults\" accounted for changing diets with age by emphasizing water consumption as well as nutrient-dense and high-fiber foods.\n\nThe first chart suggested to the USDA by nutritional experts in 1992 featured fruits and vegetables as the biggest group, not breads. This chart was overturned at the hand of special interests in the grain, meat, and dairy industries, all of which are heavily subsidized by the USDA.\n\n\"The 'Pyramid' emphasized eating more vegetables and fruits, less meat, salt, sugary foods, bad fat, and additive-rich factory foods. USDA censored that research-based version of the food guide and altered it to include more refined grains, meat, commercial snacks and fast foods, only releasing their revamped version 12 years after it was originally scheduled for release. \" \n\nIn 2005, the USDA updated its guide with MyPyramid, which replaced the hierarchical levels of the Food Guide Pyramid with colorful vertical wedges, often displayed without images of foods, creating a more abstract design. Stairs were added up the left side of the pyramid with an image of a climber to represent a push for exercise. The share of the pyramid allotted to grains now only narrowly edged out vegetables and milk, which were of equal proportions. Fruits were next in size, followed by a narrower wedge for protein and a small sliver for oils. An unmarked white tip represented discretionary calories for items such as candy, alcohol, or additional food from any other group.\n\nMyPlate is the current nutrition guide published by the United States Department of Agriculture, consisting of a diagram of a plate and glass divided into five food groups. It replaced the USDA's MyPyramid diagram on June 2, 2011, ending 19 years of food pyramid iconography. The guide will be displayed on food packaging and used in nutritional education in the United States.\n\nThe Center for Nutrition Policy and Promotion in the USDA and the United States Department of Health and Human Services jointly released a longer textual document called \"Dietary Guidelines for Americans 2015 - 2020\", to be updated in 2020. The first edition was published in 1980, and since 1985 has been updated every five years by the Dietary Guidelines Advisory Committee. Like the USDA Food Pyramid, these guidelines have been criticized as being overly influenced by the agriculture industry.\nThese criticisms of the Dietary Guidelines arose due to the omission of high-quality evidence that the Public Health Service decided to exclude. The phrasing of recommendations was extremely important and widely affected everyone who read it. The wording had to be changed constantly as there were protests due to comments such as “cut down on fatty meats”, which led to the U.S Department of Agriculture having to stop the publication of the USDA Food Book. Slight alterations of various dietary guidelines had to be made throughout the 1970s and 1980s in an attempt to calm down the protests emerged. As a compromise, the phrase was changed to “choose lean meat” but didn’t result in a better situation. In 2015 the committee factored in environmental sustainability for the first time in its recommendations. The committee's 2015 report found that a healthy diet should comprise higher plant based foods and lower animal based foods. It also found that a plant food based diet was better for the environment than one based on meat and dairy.\n\nIn 2013 and again in 2015, Edward Archer and colleagues published a series of research articles in PlosOne and Mayo Clinic Proceedings demonstrating that the dietary data used to develop the US Dietary Guidelines were physiologically implausible (i.e., incompatible with survival) and therefore these data were \"inadmissible\" as scientific evidence and should not be used to inform public policy.\n\nIn 2016, Nina Teicholz authored a critique of the US Dietary Guidelines in the British Medical Journal titled The scientific report guiding the US dietary guidelines: is it scientific? Teicholz suggested that \"the scientific committee advising the US government has not used standard methods for most of its analyses and instead relies heavily on systematic reviews from professional bodies such as the American Heart Association and the American College of Cardiology, which are heavily supported by food and drug companies.\"\n\n"}
{"id": "15430615", "url": "https://en.wikipedia.org/wiki?curid=15430615", "title": "Holozoic nutrition", "text": "Holozoic nutrition\n\nHolozoic nutrition (Greek: \"holo\"-whole ; \"zoikos\"-of animals) is a type of heterotrophic nutrition that is characterized by the internalization (ingestion) and internal processing of gaseous, liquid or solid food particles. Protozoa, such as amoebas, and most of the free living animals,such as animals, exhibit this type of nutrition. \n\nIn Holozoic nutrition the energy and organic building blocks are obtained by ingesting and then digesting other organisms or pieces of other organisms, including blood and decaying organic matter. This contrasts with holophytic nutrition, in which energy and organic building blocks are obtained through photosynthesis or chemosynthesis, and with saprozoic nutrition, in which digestive enzymes are released externally and the resulting monomers (small organic molecules) are absorbed directly from the environment.\n\nThere are several stages of holozoic nutrition, which often occur in separate compartments within an organism (such as the stomach and intestines):\n"}
{"id": "39948152", "url": "https://en.wikipedia.org/wiki?curid=39948152", "title": "Impairment rating", "text": "Impairment rating\n\nAn impairment rating is a percentage intended to represent the degree of an individual's impairment, which is a deviation away from one's normal health status and functionality. Impairment is distinct from disability. An individual's impairment rating is based on the restrictive impact of an impairment, whereas disability is broadly the consequences one's impairment. Impairment ratings given to an individual by different medical examiners are sometimes problematically inconsistent with each other.\n"}
{"id": "33334901", "url": "https://en.wikipedia.org/wiki?curid=33334901", "title": "Institut national de recherche et de sécurité", "text": "Institut national de recherche et de sécurité\n\nThe French National Research and Safety Institute for the Prevention of Occupational Accidents and Diseases (French: \"Institut national de recherche et de sécurité\", INRS) is a French association. It works under the auspices of the Caisse nationale de l’assurance maladie des travailleurs salariés (National Health Insurance Fund). Its board is composed of equal parts of representatives employers and representatives of the unions.\n\nThe main tasks of the INRS are:\n\nIt produces and distributes many information media such as magazines (Travail et sécurité), forms with the professional world. It also has a role of expertise and training to improve safety conditions.\n\n\n"}
{"id": "570749", "url": "https://en.wikipedia.org/wiki?curid=570749", "title": "Laboratory information management system", "text": "Laboratory information management system\n\nA laboratory information management system (LIMS), sometimes referred to as a laboratory information system (LIS) or laboratory management system (LMS), is a software-based solution with features that support a modern laboratory's operations. Key features include—but are not limited to—workflow and data tracking support, flexible architecture, and data exchange interfaces, which fully \"support its use in regulated environments\". The features and uses of a LIMS have evolved over the years from simple sample tracking to an enterprise resource planning tool that manages multiple aspects of laboratory informatics.\n\nThe definition of a LIMS is somewhat controversial: LIMSs are dynamic because the laboratory's requirements are rapidly evolving and different labs often have different needs. Therefore, a working definition of a LIMS ultimately depends on the interpretation by the individuals or groups involved.\n\nHistorically the LIMS, LIS, and process development execution system (PDES) have all performed similar functions. The term \"LIMS\" has tended to refer to informatics systems targeted for environmental, research, or commercial analysis such as pharmaceutical or petrochemical work. \"LIS\" has tended to refer to laboratory informatics systems in the forensics and clinical markets, which often required special case management tools. \"PDES\" has generally applied to a wider scope, including, for example, virtual manufacturing techniques, while not necessarily integrating with laboratory equipment.\n\nIn recent times LIMS functionality has spread even farther beyond its original purpose of sample management. Assay data management, data mining, data analysis, and electronic laboratory notebook (ELN) integration have been added to many LIMS, enabling the realization of translational medicine completely within a single software solution. Additionally, the distinction between LIMS and LIS has blurred, as many LIMS now also fully support comprehensive case-centric clinical data.\n\nUp until the late 1970s, the management of laboratory samples and the associated analysis and reporting were time-consuming manual processes often riddled with transcription errors. This gave some organizations impetus to streamline the collection of data and how it was reported. Custom in-house solutions were developed by a few individual laboratories, while some enterprising entities at the same time sought to develop a more commercial reporting solution in the form of special instrument-based systems.\n\nIn 1982 the first generation of LIMS was introduced in the form of a single centralized minicomputer, which offered laboratories the first opportunity to utilize automated reporting tools. As the interest in these early LIMS grew, industry leaders like Gerst Gibbon of the Federal Energy Technology Center in Pittsburgh began planting the seeds through LIMS-related conferences. By 1988 the second-generation commercial offerings were tapping into relational databases to expand LIMS into more application-specific territory, and International LIMS Conferences were in full swing. As personal computers became more powerful and prominent, a third generation of LIMS emerged in the early 1990s. These new LIMS took advantage of client/server architecture, allowing laboratories to implement better data processing and exchanges.\n\nBy 1995 the client/server tools had developed to the point of allowing processing of data anywhere on the network. Web-enabled LIMS were introduced the following year, enabling researchers to extend operations outside the confines of the laboratory. From 1996 to 2002 additional functionality was included in LIMS, from wireless networking capabilities and georeferencing of samples, to the adoption of XML standards and the development of Internet purchasing.\n\nAs of 2012, some LIMS have added additional characteristics that continue to shape how a LIMS is defined. Additions include clinical functionality, electronic laboratory notebook (ELN) functionality, as well a rise in the software as a service (SaaS) distribution model.\n\nThe LIMS is an evolving concept, with new features and functionality being added often. As laboratory demands change and technological progress continues, the functions of a LIMS will likely also change. Despite these changes, a LIMS tends to have a base set of functionality that defines it. That functionality can roughly be divided into five laboratory processing phases, with numerous software functions falling under each:\n(1) the reception and log in of a sample and its associated customer data, \n(2) the assignment, scheduling, and tracking of the sample and the associated analytical workload, \n(3) the processing and quality control associated with the sample and the utilized equipment and inventory, \n(4) the storage of data associated with the sample analysis, \n(5) the inspection, approval, and compilation of the sample data for reporting and/or further analysis.\n\nThere are several pieces of core functionality associated with these laboratory processing phases that tend to appear in most LIMS:\n\nThe core function of LIMS has traditionally been the management of samples. This typically is initiated when a sample is received in the laboratory, at which point the sample will be registered in the LIMS. Some LIMS will allow the customer to place an \"order\" for a sample directly to the LIMS at which point the sample is generated in an \"unreceived\" state. The processing could then include a step where the sample container is registered and sent to the customer for the sample to be taken and then returned to the lab. The registration process may involve accessioning the sample and producing barcodes to affix to the sample container. Various other parameters such as clinical or phenotypic information corresponding with the sample are also often recorded. The LIMS then tracks chain of custody as well as sample location. Location tracking usually involves assigning the sample to a particular freezer location, often down to the granular level of shelf, rack, box, row, and column. Other event tracking such as freeze and thaw cycles that a sample undergoes in the laboratory may be required.\n\nModern LIMS have implemented extensive configurability as each laboratory's needs for tracking additional data points can vary widely. LIMS vendors cannot typically make assumptions about what these data tracking needs are, and therefore vendors must create LIMS that are adaptable to individual environments. LIMS users may also have regulatory concerns to comply with such as CLIA, HIPAA, GLP, and FDA specifications, affecting certain aspects of sample management in a LIMS solution. One key to compliance with many of these standards is audit logging of all changes to LIMS data, and in some cases a full electronic signature system is required for rigorous tracking of field-level changes to LIMS data.\n\nModern LIMS offer an increasing amount of integration with laboratory instruments and applications. A LIMS may create control files that are \"fed\" into the instrument and direct its operation on some physical item such as a sample tube or sample plate. The LIMS may then import instrument results files to extract data for quality control assessment of the operation on the sample. Access to the instrument data can sometimes be regulated based on chain of custody assignments or other security features if need be.\n\nModern LIMS products now also allow for the import and management of raw assay data results. Modern targeted assays such as qPCR and deep sequencing can produce tens of thousands of data points per sample. Furthermore, in the case of drug and diagnostic development as many as 12 or more assays may be run for each sample. In order to track this data, a LIMS solution needs to be adaptable to many different assay formats at both the data layer and import creation layer, while maintaining a high level of overall performance. Some LIMS products address this by simply attaching assay data as BLOBs to samples, but this limits the utility of that data in data mining and downstream analysis.\n\nThe exponentially growing volume of data created in laboratories, coupled with increased business demands and focus on profitability, have pushed LIMS vendors to increase attention to how their LIMS handles electronic data exchanges. Attention must be paid to how an instrument's input and output data is managed, how remote sample collection data is imported and exported, and how mobile technology integrates with the LIMS. The successful transfer of data files in spreadsheets and other formats is a pivotal aspect of the modern LIMS. In fact, the transition \"from proprietary databases to standardized database management systems such as MySQL\" has arguably had one of the biggest impacts on how data is managed and exchanged in laboratories. In addition to mobile and database electronic data exchange, many LIMS support real-time data exchange with Electronic Health Records used in core hospital or clinic operations.\n\nAside from the key functions of sample management, instrument and application integration, and electronic data exchange, there are numerous additional operations that can be managed in a LIMS. This includes but is not limited to:\n\n\nA LIMS has utilized many architectures and distribution models over the years. As technology has changed, how a LIMS is installed, managed, and utilized has also changed with it. The following represents architectures which have been utilized at one point or another.\n\nA thick-client LIMS is a more traditional client/server architecture, with some of the system residing on the computer or workstation of the user (the client) and the rest on the server. The LIMS software is installed on the client computer, which does all of the data processing. Later it passes information to the server, which has the primary purpose of data storage. Most changes, upgrades, and other modifications will happen on the client side.\n\nThis was one of the first architectures implemented into a LIMS, having the advantage of providing higher processing speeds (because processing is done on the client and not the server). Additionally, thick-client systems have also provided more interactivity and customization, though often at a greater learning curve. The disadvantages of client-side LIMS include the need for more robust client computers and more time-consuming upgrades, as well as a lack of base functionality through a web browser. The thick-client LIMS can become web-enabled through an add-on component.\n\nAlthough there is a claim of improved security through the use of a thick-client LIMS, this is based on the misconception that \"only users with the client application installed on their PC can access server side information\". This secrecy-of-design reliance is known as security through obscurity and ignores an adversary's ability to mimic client-server interaction through, for example, reverse engineering, network traffic interception, or simply purchasing a thick-client license. Such a view is in contradiction of the \"Open Design\" principle of the National Institute of Standards and Technology's \"Guide to General Server Security\" which states that \"system security should not depend on the secrecy of the implementation or its components\", which can be considered as a reiteration of Kerckhoffs's principle.\n\nA thin-client LIMS is a more modern architecture which offers full application functionality accessed through a device's web browser. The actual LIMS software resides on a server (host) which feeds and processes information without saving it to the user's hard disk. Any necessary changes, upgrades, and other modifications are handled by the entity hosting the server-side LIMS software, meaning all end-users see all changes made. To this end, a true thin-client LIMS will leave no \"footprint\" on the client's computer, and only the integrity of the web browser need be maintained by the user. The advantages of this system include significantly lower cost of ownership and fewer network and client-side maintenance expenses. However, this architecture has the disadvantage of requiring real-time server access, a need for increased network throughput, and slightly less functionality. A sort of hybrid architecture that incorporates the features of thin-client browser usage with a thick client installation exists in the form of a web-based LIMS.\n\nSome LIMS vendors are beginning to rent hosted, thin-client solutions as \"software as a service\" (SaaS). These solutions tend to be less configurable than on-premises solutions and are therefore considered for less demanding implementations such as laboratories with few users and limited sample processing volumes.\n\nAnother implementation of the thin client architecture is the maintenance, warranty, and support (MSW) agreement. Pricing levels are typically based on a percentage of the license fee, with a standard level of service for 10 concurrent users being approximately 10 hours of support and additional customer service, at a roughly $200 per hour rate. Though some may choose to opt out of an MSW after the first year, it's often more economical to continue the plan in order to receive updates to the LIMS, giving it a longer life span in the laboratory.\n\nA web-enabled LIMS architecture is essentially a thick-client architecture with an added web browser component. In this setup, the client-side software has additional functionality that allows users to interface with the software through their device's browser. This functionality is typically limited only to certain functions of the web client. The primary advantage of a web-enabled LIMS is the end-user can access data both on the client side and the server side of the configuration. As in a thick-client architecture, updates in the software must be propagated to every client machine. However, the added disadvantages of requiring always-on access to the host server and the need for cross-platform functionality mean that additional overhead costs may arise.\n\nA web-based LIMS architecture is a hybrid of the thick- and thin-client architectures. While much of the client-side work is done through a web browser, the LIMS may also require the support of desktop software installed on the client device. The end result is a process that is apparent to the end-user through a web browser, but perhaps not so apparent as it runs thick-client-like processing in the background. In this case, web-based architecture has the advantage of providing more functionality through a more friendly web interface. The disadvantages of this setup are more sunk costs in system administration and reduced functionality on mobile platforms.\n\nThe disadvantage of a thick client is in the installation and update phases of the applications. Users who want the security, high speed and functionality of a thick client may use Microsoft ClickOnce Technology. This enables the user to install and run a Windows-based smart client application by clicking a link in a web page. The software does not need to be installed at each user workstation one by one. ClickOnce applications can be self-updating; they can check for newer versions as they become available and automatically replace any updated files.\n\nLIMS implementations are notorious for often being lengthy and costly. This is due in part to the diversity of requirements within each lab, but also to the inflexible nature of most LIMS products for adapting to these widely varying requirements. Newer LIMS solutions are beginning to emerge that take advantage of modern techniques in software design that are inherently more configurable and adaptable — particularly at the data layer — than prior solutions. This means not only that implementations are much faster, but also that the costs are lower and the risk of obsolescence is minimized.\n\nUntil recently, the LIMS and Laboratory Information System (LIS) have exhibited a few key differences, making them noticeably separate entities.\n\nA LIMS traditionally has been designed to process and report data related to batches of samples from biology labs, water treatment facilities, drug trials, and other entities that handle complex batches of data. A LIS has been designed primarily for processing and reporting data related to individual patients in a clinical setting.\n\nA LIMS may need to satisfy good manufacturing practice (GMP) and meet the reporting and audit needs of the regulatory bodies and research scientists in many different industries. A LIS, however, must satisfy the reporting and auditing needs of health service agencies e.g. the hospital accreditation agency, HIPAA in the US, or other clinical medical practitioners.\n\nA LIMS is most competitive in group-centric settings (dealing with \"batches\" and \"samples\") that often deal with mostly anonymous research-specific laboratory data, whereas a LIS is usually most competitive in patient-centric settings (dealing with \"subjects\" and \"specimens\") and clinical labs. An LIS is regulated as a medical device by the FDA, and the companies that produce the software are therefore liable for defects. Due to this, an LIS can not be customized by the client.\n\nA LIMS covers standards such as 21 CFR Part 11 from the Food and Drug Administration (United States), ISO/IEC 17025, \nISO 15189, \ngood laboratory practice, and \nGood Automated Manufacturing Practice (GAMP).\n\n\nCloudLIMS, Blog (19 November 2018). \"What is a LIMS and How does a LIMS work?. Cloudlims.com. Retrieved 27 November 2018.\n"}
{"id": "12612736", "url": "https://en.wikipedia.org/wiki?curid=12612736", "title": "Lactivism", "text": "Lactivism\n\nLactivism (a portmanteau of \"lactation\" and \"activism\") is the doctrine or practice of vigorous action or involvement as a means of achieving a breastfeeding culture, sometimes by demonstrations, protests, etc. of breastfeeding. Supporters, referred to as \"lactivists\", seek to protest the violation of International Code of Marketing of Breast-milk Substitutes by formula companies and industry. \n\nOne form that lactivism takes is the staging of a \"nurse-in\" (a play on \"sit-in\"), which involves women gathering in public to nurse their children, usually to protest incidents in which a nursing mother was asked to cover up or leave a location because she was nursing.\n\nDuring nurse-ins, nursing mothers often wear clothing with the International Breastfeeding Symbol on it, to show their solidarity.\n\nAnother form of lactivism is acting as support for mothers that wish to nurse. Lactivists provide information and share resources on successful nursing.\n\nMany lactivists choose to breastfeed their children over bottle feeding, seeing this as the natural way to provide nutrition. It is claimed that breastfeeding provides a bonding experience superior to that or bottle feeding. Lactivists may also argue that bottle feeding is costlier than breastfeeding as it requires a multitude of items, and the money saved from breastfeeding can be spent on other useful items for the child. Multiple health organizations recommend breast milk as the primary source of nutrition for babies, including the American Academy of Pediatrics, the American Medical Association, and the World Health Organization.\n\n\n"}
{"id": "17747", "url": "https://en.wikipedia.org/wiki?curid=17747", "title": "Lead", "text": "Lead\n\nLead is a chemical element with symbol Pb (from the Latin \"plumbum\") and atomic number 82. It is a heavy metal that is denser than most common materials. Lead is soft and malleable, and has a relatively low melting point. When freshly cut, lead is silvery with a hint of blue; it tarnishes to a dull gray color when exposed to air. Lead has the highest atomic number of any stable element and three of its isotopes each conclude a major decay chain of heavier elements.\n\nLead is a relatively unreactive post-transition metal. Its weak metallic character is illustrated by its amphoteric nature; lead and lead oxides react with acids and bases, and it tends to form covalent bonds. Compounds of lead are usually found in the +2 oxidation state rather than the +4 state common with lighter members of the carbon group. Exceptions are mostly limited to organolead compounds. Like the lighter members of the group, lead tends to bond with itself; it can form chains, rings and polyhedral structures.\n\nLead is easily extracted from its ores; prehistoric people in Western Asia knew of it. Galena, a principal ore of lead, often bears silver, interest in which helped initiate widespread extraction and use of lead in ancient Rome. Lead production declined after the fall of Rome and did not reach comparable levels until the Industrial Revolution. In 2014, annual global production of lead was about ten million tonnes, over half of which was from recycling. Lead's high density, low melting point, ductility and relative inertness to oxidation make it useful. These properties, combined with its relative abundance and low cost, resulted in its extensive use in construction, plumbing, batteries, bullets and shot, weights, solders, pewters, fusible alloys, white paints, leaded gasoline, and radiation shielding.\n\nIn the late 19th century, lead's toxicity was recognized, and its use has since been phased out of many applications. Lead is a toxin that accumulates in soft tissues and bones, it acts as a neurotoxin damaging the nervous system and interfering with the function of biological enzymes. It is particularly problematic in children: even if blood levels are promptly normalized with treatment, neurological disorders, such as brain damage and behavioral problems, may result.\n\nA lead atom has 82 electrons, arranged in an electron configuration of [Xe]4f5d6s6p. The sum of lead's first and second ionization energies—the total energy required to remove the two 6p electrons—is close to that of tin, lead's upper neighbor in the carbon group. This is unusual; ionization energies generally fall going down a group, as an element's outer electrons become more distant from the nucleus, and more shielded by smaller orbitals. The similarity of ionization energies is caused by the lanthanide contraction—the decrease in element radii from lanthanum (atomic number 57) to lutetium (71), and the relatively small radii of the elements from hafnium (72) onwards. This is due to poor shielding of the nucleus by the lanthanide 4f electrons. The sum of the first four ionization energies of lead exceeds that of tin, contrary to what periodic trends would predict. Relativistic effects, which become significant in heavier atoms, contribute to this behavior. One such effect is the inert pair effect: the 6s electrons of lead become reluctant to participate in bonding, making the distance between nearest atoms in crystalline lead unusually long.\n\nLead's lighter carbon group congeners form stable or metastable allotropes with the tetrahedrally coordinated and covalently bonded diamond cubic structure. The energy levels of their outer s- and p-orbitals are close enough to allow mixing into four hybrid sp orbitals. In lead, the inert pair effect increases the separation between its s- and p-orbitals, and the gap cannot be overcome by the energy that would be released by extra bonds following hybridization. Rather than having a diamond cubic structure, lead forms metallic bonds in which only the p-electrons are delocalized and shared between the Pb ions. Lead consequently has a face-centered cubic structure like the similarly sized divalent metals calcium and strontium.\n\nPure lead has a bright, silvery appearance with a hint of blue. It tarnishes on contact with moist air, and takes on a dull appearance, the hue of which depends on the prevailing conditions. Characteristic properties of lead include high density, malleability, ductility, and high resistance to corrosion due to passivation.\nLead's close-packed face-centered cubic structure and high atomic weight result in a density of 11.34 g/cm, which is greater than that of common metals such as iron (7.87 g/cm), copper (8.93 g/cm), and zinc (7.14 g/cm). This density is the origin of the idiom \"to go over like a lead balloon\". Some rarer metals are denser: tungsten and gold are both at 19.3 g/cm, and osmium—the densest metal known—has a density of 22.59 g/cm, almost twice that of lead.\n\nLead is a very soft metal with a Mohs hardness of 1.5; it can be scratched with a fingernail. It is quite malleable and somewhat ductile. The bulk modulus of lead—a measure of its ease of compressibility—is 45.8 GPa. In comparison, that of aluminium is 75.2 GPa; copper 137.8 GPa; and mild steel 160–169 GPa. Lead's tensile strength, at 12–17 MPa, is low (that of aluminium is 6 times higher, copper 10 times, and mild steel 15 times higher); it can be strengthened by adding small amounts of copper or antimony.\n\nThe melting point of lead—at 327.5 °C (621.5 °F)—is very low compared to most metals. Its boiling point of 1749 °C (3180 °F) is the lowest among the carbon group elements. The electrical resistivity of lead at 20 °C is 192 nanoohm-meters, almost an order of magnitude higher than those of other industrial metals (copper at 15.43 nΩ·m; gold 20.51 nΩ·m; and aluminium at 24.15 nΩ·m). Lead is a superconductor at temperatures lower than 7.19 K; this is the highest critical temperature of all type-I superconductors and the third highest of the elemental superconductors.\n\nNatural lead consists of four stable isotopes with mass numbers of 204, 206, 207, and 208, and traces of five short-lived radioisotopes. The high number of isotopes is consistent with lead's atomic number being even. Lead has a magic number of protons (82), for which the nuclear shell model accurately predicts an especially stable nucleus. Lead-208 has 126 neutrons, another magic number, which may explain why lead-208 is extraordinarily stable.\n\nWith its high atomic number, lead is the heaviest element whose natural isotopes are regarded as stable; lead-208 is the heaviest stable nucleus. (This distinction formerly fell to bismuth, with an atomic number of 83, until its only primordial isotope, bismuth-209, was found in 2003 to decay very slowly.) The four stable isotopes of lead could theoretically undergo alpha decay to isotopes of mercury with a release of energy, but this has not been observed for any of them; their predicted half-lives range from 10 to 10 years (at least 10 times the current age of the universe).\n\nThree of the stable isotopes are found in three of the four major decay chains: lead-206, lead-207, and lead-208 are the final decay products of uranium-238, uranium-235, and thorium-232, respectively. These decay chains are called the uranium chain, the actinium chain, and the thorium chain. Their isotopic concentrations in a natural rock sample depends greatly on the presence of these three parent uranium and thorium isotopes. For example, the relative abundance of lead-208 can range from 52% in normal samples to 90% in thorium ores; for this reason, the standard atomic weight of lead is given to only one decimal place. As time passes, the ratio of lead-206 and lead-207 to lead-204 increases, since the former two are supplemented by radioactive decay of heavier elements while the latter is not; this allows for lead–lead dating. As uranium decays into lead, their relative amounts change; this is the basis for uranium–lead dating. Lead-207 exhibits nuclear magnetic resonance, a property that has been used to study its compounds in solution and solid state, including in human body.\nApart from the stable isotopes, which make up almost all lead that exists naturally, there are trace quantities of a few radioactive isotopes. One of them is lead-210; although it has a half-life of only 22.3 years, small quantities occur in nature because lead-210 is produced by a long decay series that starts with uranium-238 (which has been present for billions of years on Earth). Lead-211, -212, and -214 are present in the decay chains of uranium-235, thorium-232, and uranium-238, respectively, so traces of all three of these lead isotopes are found naturally. Minute traces of lead-209 arise from the very rare cluster decay of radium-223, one of the daughter products of natural uranium-235, and the decay chain of neptunium-237, traces of which are produced by neutron capture in uranium ores. Lead-210 is particularly useful for helping to identify the ages of samples by measuring its ratio to lead-206 (both isotopes are present in a single decay chain).\n\nIn total, 43 lead isotopes have been synthesized, with mass numbers 178–220. Lead-205 is the most stable radioisotope, with a half-life of around 1.5 years. The second-most stable is lead-202, which has a half-life of about 53,000 years, longer than any of the natural trace radioisotopes.\n\nBulk lead exposed to moist air forms a protective layer of varying composition. Lead(II) carbonate is a common constituent; the sulfate or chloride may also be present in urban or maritime settings. This layer makes bulk lead effectively chemically inert in the air. Finely powdered lead, as with many metals, is pyrophoric, and burns with a bluish-white flame.\n\nFluorine reacts with lead at room temperature, forming lead(II) fluoride. The reaction with chlorine is similar but requires heating, as the resulting chloride layer diminishes the reactivity of the elements. Molten lead reacts with the chalcogens to give lead(II) chalcogenides.\n\nLead metal resists sulfuric and phosphoric acid but not hydrochloric or nitric acid; the outcome depends on insolubility and subsequent passivation of the product salt. Organic acids, such as acetic acid, dissolve lead in the presence of oxygen. Concentrated alkalis will dissolve lead and form plumbites.\n\nLead shows two main oxidation states: +4 and +2. The tetravalent state is common for the carbon group. The divalent state is rare for carbon and silicon, minor for germanium, important (but not prevailing) for tin, and is the more important of the two oxidation states for lead. This is attributable to relativistic effects, specifically the inert pair effect, which manifests itself when there is a large difference in electronegativity between lead and oxide, halide, or nitride anions, leading to a significant partial positive charge on lead. The result is a stronger contraction of the lead 6s orbital than is the case for the 6p orbital, making it rather inert in ionic compounds. The inert pair effect is less applicable to compounds in which lead forms covalent bonds with elements of similar electronegativity, such as carbon in organolead compounds. In these, the 6s and 6p orbitals remain similarly sized and sp hybridization is still energetically favorable. Lead, like carbon, is predominantly tetravalent in such compounds.\n\nThere is a relatively large difference in the electronegativity of lead(II) at 1.87 and lead(IV) at 2.33. This difference marks the reversal in the trend of increasing stability of the +4 oxidation state going down the carbon group; tin, by comparison, has values of 1.80 in the +2 oxidation state and 1.96 in the +4 state.\n\nLead(II) compounds are characteristic of the inorganic chemistry of lead. Even strong oxidizing agents like fluorine and chlorine react with lead to give only PbF and PbCl. Lead(II) ions are usually colorless in solution, and partially hydrolyze to form Pb(OH) and finally [Pb(OH)] (in which the hydroxyl ions act as bridging ligands), but are not reducing agents as tin(II) ions are. Techniques for identifying the presence of the Pb ion in water generally rely on the precipitation of lead(II) chloride using dilute hydrochloric acid. As the chloride salt is sparingly soluble in water, in very dilute solutions the precipitation of lead(II) sulfide is achieved by bubbling hydrogen sulfide through the solution.\n\nLead monoxide exists in two polymorphs, litharge α-PbO (red) and massicot β-PbO (yellow), the latter being stable only above around 488 °C. Litharge is the most commonly used inorganic compound of lead. There is no lead(II) hydroxide; increasing the pH of solutions of lead(II) salts leads to hydrolysis and condensation.\nLead commonly reacts with heavier chalcogens. Lead sulfide is a semiconductor, a photoconductor, and an extremely sensitive infrared radiation detector. The other two chalcogenides, lead selenide and lead telluride, are likewise photoconducting. They are unusual in that their color becomes lighter going down the group.\nLead dihalides are well-characterized; this includes the diastatide, and mixed halides, such as PbFCl. The relative insolubility of the latter forms a useful basis for the gravimetric determination of fluorine. The difluoride was the first solid ionically conducting compound to be discovered (in 1834, by Michael Faraday). The other dihalides decompose on exposure to ultraviolet or visible light, especially the diiodide. Many lead(II) pseudohalides are known, such as the cyanide, cyanate, and thiocyanate. Lead(II) forms an extensive variety of halide coordination complexes, such as [PbCl], [PbCl], and the [PbCl] chain anion.\n\nLead(II) sulfate is insoluble in water, like the sulfates of other heavy divalent cations. Lead(II) nitrate and lead(II) acetate are very soluble, and this is exploited in the synthesis of other lead compounds.\n\nFew inorganic lead(IV) compounds are known. They are only formed in highly oxidizing solutions and do not normally exist under standard conditions. Lead(II) oxide gives a mixed oxide on further oxidation, PbO. It is described as lead(II,IV) oxide, or structurally 2PbO·PbO, and is the best-known mixed valence lead compound. Lead dioxide is a strong oxidizing agent, capable of oxidizing hydrochloric acid to chlorine gas. This is because the expected PbCl that would be produced is unstable and spontaneously decomposes to PbCl and Cl. Analogously to lead monoxide, lead dioxide is capable of forming plumbate anions. Lead disulfide and lead diselenide are only stable at high pressures. Lead tetrafluoride, a yellow crystalline powder, is stable, but less so than the difluoride. Lead tetrachloride (a yellow oil) decomposes at room temperature, lead tetrabromide is less stable still, and the existence of lead tetraiodide is questionable.\n\nSome lead compounds exist in formal oxidation states other than +4 or +2. Lead(III) may be obtained, as an intermediate between lead(II) and lead(IV), in larger organolead complexes; this oxidation state is not stable, as both the lead(III) ion and the larger complexes containing it are radicals. The same applies for lead(I), which can be found in such radical species.\n\nNumerous mixed lead(II,IV) oxides are known. When PbO is heated in air, it becomes PbO at 293 °C, PbO at 351 °C, PbO at 374 °C, and finally PbO at 605 °C. A further sesquioxide, PbO, can be obtained at high pressure, along with several non-stoichiometric phases. Many of them show defective fluorite structures in which some oxygen atoms are replaced by vacancies: PbO can be considered as having such a structure, with every alternate layer of oxygen atoms absent.\n\nNegative oxidation states can occur as Zintl phases, as either free lead anions, as in BaPb, with lead formally being lead(−IV), or in oxygen-sensitive ring-shaped or polyhedral cluster ions such as the trigonal bipyramidal Pb ion, where two lead atoms are lead(−I) and three are lead(0). In such anions, each atom is at a polyhedral vertex and contributes two electrons to each covalent bond along an edge from their sp hybrid orbitals, the other two being an external lone pair. They may be made in liquid ammonia via the reduction of lead by sodium.\n\nLead can form multiply-bonded chains, a property it shares with its lighter homologs in the carbon group. Its capacity to do so is much less because the Pb–Pb bond energy is over three and a half times lower than that of the C–C bond. With itself, lead can build metal–metal bonds of an order up to three. With carbon, lead forms organolead compounds similar to, but generally less stable than, typical organic compounds (due to the Pb–C bond being rather weak). This makes the organometallic chemistry of lead far less wide-ranging than that of tin. Lead predominantly forms organolead(IV) compounds, even when starting with inorganic lead(II) reactants; very few organolead(II) compounds are known. The most well-characterized exceptions are Pb[CH(SiMe)] and Pb(\"η\"-CH).\n\nThe lead analog of the simplest organic compound, methane, is plumbane. Plumbane may be obtained in a reaction between metallic lead and atomic hydrogen. Two simple derivatives, tetramethyllead and tetraethyllead, are the best-known organolead compounds. These compounds are relatively stable: tetraethyllead only starts to decompose if heated or if exposed to sunlight or ultraviolet light. (Tetraphenyllead is even more thermally stable, decomposing at 270 °C.) With sodium metal, lead readily forms an equimolar alloy that reacts with alkyl halides to form organometallic compounds such as tetraethyllead. The oxidizing nature of many organolead compounds is usefully exploited: lead tetraacetate is an important laboratory reagent for oxidation in organic synthesis, and tetraethyllead was once produced in larger quantities than any other organometallic compound. Other organolead compounds are less chemically stable. For many organic compounds, a lead analog does not exist.\n\nLead's per-particle abundance in the Solar System is 0.121 ppb (parts per billion). This figure is two and a half times higher than that of platinum, eight times more than mercury, and seventeen times more than gold. The amount of lead in the universe is slowly increasing as most heavier atoms (all of which are unstable) gradually decay to lead. The abundance of lead in the Solar System since its formation 4.5 billion years ago has increased by about 0.75%. The solar system abundances table shows that lead, despite its relatively high atomic number, is more prevalent than most other elements with atomic numbers greater than 40.\n\nPrimordial lead—which comprises the isotopes lead-204, lead-206, lead-207, and lead-208—was mostly created as a result of repetitive neutron capture processes occurring in stars. The two main modes of capture are the s- and r-processes.\n\nIn the s-process (s is for \"slow\"), captures are separated by years or decades, allowing less stable nuclei to undergo beta decay. A stable thallium-203 nucleus can capture a neutron and become thallium-204; this undergoes beta decay to give stable lead-204; on capturing another neutron, it becomes lead-205, which has a half-life of around 15 million years. Further captures result in lead-206, lead-207, and lead-208. On capturing another neutron, lead-208 becomes lead-209, which quickly decays into bismuth-209. On capturing another neutron, bismuth-209 becomes bismuth-210, and this beta decays to polonium-210, which alpha decays to lead-206. The cycle hence ends at lead-206, lead-207, lead-208, and bismuth-209.\nIn the r-process (r is for \"rapid\"), captures happen faster than nuclei can decay. This occurs in environments with a high neutron density, such as a supernova or the merger of two neutron stars. The neutron flux involved may be on the order of 10 neutrons per square centimeter per second. The r-process does not form as much lead as the s-process. It tends to stop once neutron-rich nuclei reach 126 neutrons. At this point, the neutrons are arranged in complete shells in the atomic nucleus, and it becomes harder to energetically accommodate more of them. When the neutron flux subsides, these nuclei beta decay into stable isotopes of osmium, iridium, and platinum.\n\nLead is classified as a chalcophile under the Goldschmidt classification, meaning it is generally found combined with sulfur. It rarely occurs in its native, metallic form. Many lead minerals are relatively light and, over the course of the Earth's history, have remained in the crust instead of sinking deeper into the Earth's interior. This accounts for lead's relatively high crustal abundance of 14 ppm; it is the 38th most abundant element in the crust.\n\nThe main lead-bearing mineral is galena (PbS), which is mostly found with zinc ores. Most other lead minerals are related to galena in some way; boulangerite, PbSbS, is a mixed sulfide derived from galena; anglesite, PbSO, is a product of galena oxidation; and cerussite or white lead ore, PbCO, is a decomposition product of galena. Arsenic, tin, antimony, silver, gold, copper, and bismuth are common impurities in lead minerals.\nWorld lead resources exceed two billion tons. Significant deposits are located in Australia, China, Ireland, Mexico, Peru, Portugal, Russia, and the United States. Global reserves—resources that are economically feasible to extract—totaled 88 million tons in 2016, of which Australia had 35 million, China 17 million, and Russia 6.4 million.\n\nTypical background concentrations of lead do not exceed 0.1 μg/m in the atmosphere; 100 mg/kg in soil; and 5 μg/L in freshwater and seawater.\n\nThe modern English word \"lead\" is of Germanic origin; it comes from the Middle English \"leed\" and Old English \"lēad\" (with the macron above the \"e\" signifying that the vowel sound of that letter is long). The Old English word is derived from the hypothetical reconstructed Proto-Germanic \"*lauda-\" (\"lead\"). According to linguistic theory, this word bore descendants in multiple Germanic languages of exactly the same meaning.\n\nThe origin of the Proto-Germanic \"*lauda-\" is not agreed in the linguistic community. One hypothesis suggests it is derived from Proto-Indo-European \"*lAudh-\" (\"lead\"; capitalization of the vowel is equivalent to the macron). Another hypothesis suggests it is borrowed from Proto-Celtic \"*ɸloud-io-\" (\"lead\"). This word is related to the Latin \"plumbum\", which gave the element its chemical symbol \"Pb\". The word \"*ɸloud-io-\" is thought to be the origin of Proto-Germanic \"*bliwa-\" (which also means \"lead\"), from which stemmed the German \"Blei\".\n\nThe name of the chemical element is not related to the verb of the same spelling, which is derived from Proto-Germanic \"*laidijan-\" (\"to lead\").\n\nMetallic lead beads dating back to 7000–6500 BCE have been found in Asia Minor and may represent the first example of metal smelting. At that time lead had few (if any) applications due to its softness and dull appearance. The major reason for the spread of lead production was its association with silver, which may be obtained by burning galena (a common lead mineral). The Ancient Egyptians were the first to use lead minerals in cosmetics, an application that spread to Ancient Greece and beyond; the Egyptians may have used lead for sinkers in fishing nets, glazes, glasses, enamels, and for ornaments. Various civilizations of the Fertile Crescent used lead as a writing material, as currency, and for construction. Lead was used in the Ancient Chinese royal court as a stimulant, as currency, and as a contraceptive; the Indus Valley civilization and the Mesoamericans used it for making amulets; and the eastern and southern African peoples used lead in wire drawing.\n\nBecause silver was extensively used as a decorative material and an exchange medium, lead deposits came to be worked in Asia Minor since 3000 BCE; later, lead deposits were developed in the Aegean and Laurion. These three regions collectively dominated production of mined lead until c. 1200 BCE. Since 2000 BCE, the Phoenicians worked deposits in the Iberian peninsula; by 1600 BCE, lead mining existed in Cyprus, Greece, and Sardinia.\n\nRome's territorial expansion in Europe and across the Mediterranean, and its development of mining, led to it becoming the greatest producer of lead during the classical era, with an estimated annual output peaking at 80,000 tonnes. Like their predecessors, the Romans obtained lead mostly as a by-product of silver smelting. Lead mining occurred in Central Europe, Britain, the Balkans, Greece, Anatolia, and Hispania, the latter accounting for 40% of world production.\n\nLead tablets were commonly used as a material for letters. Lead coffins, cast in flat sand forms, with interchangeable motifs to suit the faith of the deceased were used in ancient Judea.\n\nLead was used for making water pipes in the Roman Empire; the Latin word for the metal, \"plumbum\", is the origin of the English word \"plumbing\". Its ease of working and resistance to corrosion ensured its widespread use in other applications including pharmaceuticals, roofing, currency, and warfare. Writers of the time, such as Cato the Elder, Columella, and Pliny the Elder, recommended lead (or lead-coated) vessels for the preparation of sweeteners and preservatives added to wine and food. The lead conferred an agreeable taste due to the formation of \"sugar of lead\" (lead(II) acetate), whereas copper or bronze vessels could impart a bitter flavor through verdigris formation.\n\nThe Roman author Vitruvius reported the health dangers of lead and modern writers have suggested that lead poisoning played a major role in the decline of the Roman Empire. Other researchers have criticized such claims, pointing out, for instance, that not all abdominal pain is caused by lead poisoning. According to archaeological research, Roman lead pipes increased lead levels in tap water but such an effect was \"unlikely to have been truly harmful\". When lead poisoning did occur, victims were called \"saturnine\", dark and cynical, after the ghoulish father of the gods, Saturn. By association, lead was considered the father of all metals. Its status in Roman society was low as it was readily available and cheap.\n\nDuring the classical era (and even up to the 17th century), tin was often not distinguished from lead: Romans called lead \"plumbum nigrum\" (\"black lead\"), and tin \"plumbum candidum\" (\"bright lead\"). The association of lead and tin can be seen in other languages: the word \"olovo\" in Czech translates to \"lead\", but in Russian the cognate \"олово\" (\"olovo\") means \"tin\". To add to the confusion, lead bore a close relation to antimony: both elements commonly occur as sulfides (galena and stibnite), often together. Pliny incorrectly wrote that stibnite would give lead on heating, instead of antimony. In countries such as Turkey and India, the originally Persian name \"surma\" came to refer to either antimony sulfide or lead sulfide, and in some languages, such as Russian, gave its name to antimony \"(сурьма).\"\n\nLead mining in Western Europe declined after the fall of the Western Roman Empire, with Arabian Iberia being the only region having a significant output. The largest production of lead occurred in South and East Asia, especially China and India, where lead mining grew rapidly.\n\nIn Europe, lead production began to increase in the 11th and 12th centuries, when it was again used for roofing and piping. Starting in the 13th century, lead was used to create stained glass. In the European and Arabian traditions of alchemy, lead (symbol in the European tradition) was considered an impure base metal which, by the separation, purification and balancing of its constituent essences, could be transformed to pure and incorruptible gold. During the period, lead was used increasingly for adulterating wine. The use of such wine was forbidden for use in Christian rites by a papal bull in 1498, but it continued to be imbibed and resulted in mass poisonings up to the late 18th century. Lead was a key material in parts of the printing press, which was invented around 1440; lead dust was commonly inhaled by print workers, causing lead poisoning. Firearms were invented at around the same time, and lead, despite being more expensive than iron, became the chief material for making bullets. It was less damaging to iron gun barrels, had a higher density (which allowed for better retention of velocity), and its lower melting point made the production of bullets easier as they could be made using a wood fire. Lead, in the form of Venetian ceruse, was extensively used in cosmetics by Western European aristocracy as whitened faces were regarded as a sign of modesty. This practice later expanded to white wigs and eyeliners, and only faded out with the French Revolution in the late 18th century. A similar fashion appeared in Japan in the 18th century with the emergence of the geishas, a practice that continued long into the 20th century. The white faces of women \"came to represent their feminine virtue as Japanese women\", with lead commonly used in the whitener.\n\nIn the New World, lead was produced soon after the arrival of European settlers. The earliest recorded lead production dates to 1621 in the English Colony of Virginia, fourteen years after its foundation. In Australia, the first mine opened by colonists on the continent was a lead mine, in 1841. In Africa, lead mining and smelting were known in the Benue Trough and the lower Congo Basin, where lead was used for trade with Europeans, and as a currency by the 17th century, well before the scramble for Africa.\n\nIn the second half of the 18th century, Britain, and later continental Europe and the United States, experienced the Industrial Revolution. This was the first time during which lead production rates exceeded those of Rome. Britain was the leading producer, losing this status by the mid-19th century with the depletion of its mines and the development of lead mining in Germany, Spain, and the United States. By 1900, the United States was the leader in global lead production, and other non-European nations—Canada, Mexico, and Australia—had begun significant production; production outside Europe exceeded that within. A great share of the demand for lead came from plumbing and painting—lead paints were in regular use. At this time, more (working class) people were exposed to the metal and lead poisoning cases escalated. This led to research into the effects of lead intake. Lead was proven to be more dangerous in its fume form than as a solid metal. Lead poisoning and gout were linked; British physician Alfred Baring Garrod noted a third of his gout patients were plumbers and painters. The effects of chronic ingestion of lead, including mental disorders, were also studied in the 19th century. The first laws aimed at decreasing lead poisoning in factories were enacted during the 1870s and 1880s in the United Kingdom.\n\nFurther evidence of the threat that lead posed to humans was discovered in the late 19th and early 20th centuries. Mechanisms of harm were better understood, lead blindness was documented, and the element was phased out of public use in the United States and Europe. The United Kingdom introduced mandatory factory inspections in 1878 and appointed the first Medical Inspector of Factories in 1898; as a result, a 25-fold decrease in lead poisoning incidents from 1900 to 1944 was reported. The last major human exposure to lead was the addition of tetraethyllead to gasoline as an antiknock agent, a practice that originated in the United States in 1921. It was phased out in the United States and the European Union by 2000. Most European countries banned lead paint—commonly used because of its opacity and water resistance—for interiors by 1930.\n\nIn the 1970s, the United States and Western European countries introduced legislation to reduce lead air pollution. The impact was significant: while a study conducted by the Centers for Disease Control and Prevention in the United States in 1976–1980 showed that 77.8% of the population had elevated blood lead levels, in 1991–1994, a study by the same institute showed the share of people with such high levels dropped to 2.2%. The main product made of lead by the end of the 20th century was the lead–acid battery, which posed no direct threat to humans. From 1960 to 1990, lead output in the Western Bloc grew by a third. The share of the world's lead production by the Eastern Bloc increased from 10% to 30%, from 1950 to 1990, with the Soviet Union being the world's largest producer during the mid-1970s and the 1980s, and China starting major lead production in the late 20th century. Unlike the European communist countries, China was largely unindustrialized by the mid-20th century; in 2004, China surpassed Australia as the largest producer of lead. As was the case during European industrialization, lead has had a negative effect on health in China.\n\nProduction of lead is increasing worldwide due to its use in lead–acid batteries. There are two major categories of production: primary from mined ores, and secondary from scrap. In 2014, 4.58 million metric tons came from primary production and 5.64 million from secondary production. The top three producers of mined lead concentrate in that year were China, Australia, and the United States. The top three producers of refined lead were China, the United States, and South Korea. According to the International Resource Panel's Metal Stocks in Society report of 2010, the total amount of lead in use, stockpiled, discarded, or dissipated into the environment, on a global basis, is 8 kg per capita. Much of this is in more developed countries (20–150 kg per capita) rather than less developed ones (1–4 kg per capita).\n\nThe primary and secondary lead production processes are similar. Some primary production plants now supplement their operations with scrap lead, and this trend is likely to increase in the future. Given adequate techniques, lead obtained via secondary processes is indistinguishable from lead obtained via primary processes. Scrap lead from the building trade is usually fairly clean and is re-melted without the need for smelting, though refining is sometimes needed. Secondary lead production is therefore cheaper, in terms of energy requirements, than is primary production, often by 50% or more.\n\nMost lead ores contain a low percentage of lead (rich ores have a typical content of 3–8%) which must be concentrated for extraction. During initial processing, ores typically undergo crushing, dense-medium separation, grinding, froth flotation, and drying. The resulting concentrate, which has a lead content of 30–80% by mass (regularly 50–60%), is then turned into (impure) lead metal.\n\nThere are two main ways of doing this: a two-stage process involving roasting followed by blast furnace extraction, carried out in separate vessels; or a direct process in which the extraction of the concentrate occurs in a single vessel. The latter has become the most common route, though the former is still significant.\n\nFirst, the sulfide concentrate is roasted in air to oxidize the lead sulfide:\n\nAs the original concentrate was not pure lead sulfide, roasting yields not only the desired lead(II) oxide, but a mixture of oxides, sulfates, and silicates of lead and of the other metals contained in the ore. This impure lead oxide is reduced in a coke-fired blast furnace to the (again, impure) metal:\n\nImpurities are mostly arsenic, antimony, bismuth, zinc, copper, silver, and gold. The melt is treated in a reverberatory furnace with air, steam, and sulfur, which oxidizes the impurities except for silver, gold, and bismuth. Oxidized contaminants float to the top of the melt and are skimmed off. Metallic silver and gold are removed and recovered economically by means of the Parkes process, in which zinc is added to lead. Zinc, which is immiscible in lead, dissolves the silver and gold. The zinc solution can be separated from the lead, and the silver and gold retrieved. De-silvered lead is freed of bismuth by the Betterton–Kroll process, treating it with metallic calcium and magnesium. The resulting bismuth dross can be skimmed off.\n\nVery pure lead can be obtained by processing smelted lead electrolytically using the Betts process. Anodes of impure lead and cathodes of pure lead are placed in an electrolyte of lead fluorosilicate (PbSiF). Once electrical potential is applied, impure lead at the anode dissolves and plates onto the cathode, leaving the majority of the impurities in solution. This is a high-cost process and thus mostly reserved for refining bullion containing high percentages of impurities.\n\nIn this process, lead bullion and slag is obtained directly from lead concentrates. The lead sulfide concentrate is melted in a furnace and oxidized, forming lead monoxide. Carbon (as coke or coal gas) is added to the molten charge along with fluxing agents. The lead monoxide is thereby reduced to metallic lead, in the midst of a slag rich in lead monoxide.\n\nAs much as 80% of the lead in very high-content initial concentrates can be obtained as bullion; the remaining 20% forms a slag rich in lead monoxide. For a low-grade feed, all of the lead can be oxidized to a high-lead slag. Metallic lead is further obtained from the high-lead (25–40%) slags via submerged fuel combustion or injection, reduction assisted by an electric furnace, or a combination of both.\n\nResearch on a cleaner, less energy-intensive lead extraction process continues; a major drawback is that either too much lead is lost as waste, or the alternatives result in a high sulfur content in the resulting lead metal. Hydrometallurgical extraction, in which anodes of impure lead are immersed into an electrolyte and pure lead is deposited onto a cathode, is a technique that may have potential.\n\nSmelting, which is an essential part of the primary production, is often skipped during secondary production. It is only performed when metallic lead has undergone significant oxidation. The process is similar to that of primary production in either a blast furnace or a rotary furnace, with the essential difference being the greater variability of yields: blast furnaces produce hard lead (10% antimony) while reverberatory and rotary kiln furnaces produced semisoft lead (3–4% antimony). The Isasmelt process is a more recent method that may act as an extension to primary production; battery paste from spent lead–acid batteries has sulfur removed by treating it with alkali, and is then treated in a coal-fueled furnace in the presence of oxygen, which yields impure lead, with antimony the most common impurity. Refining of secondary lead is similar to that of primary lead; some refining processes may be skipped depending on the material recycled and its potential contamination.\n\nOf the sources of lead for recycling, lead–acid batteries are the most important; lead pipe, sheet, and cable sheathing are also significant.\n\nContrary to popular belief, pencil leads in wooden pencils have never been made from lead. When the pencil originated as a wrapped graphite writing tool, the particular type of graphite used was named \"plumbago\" (literally, \"act for lead\" or \"lead mockup\").\n\nLead metal has several useful mechanical properties, including high density, low melting point, ductility, and relative inertness. Many metals are superior to lead in some of these aspects but are generally less common and more difficult to extract from parent ores. Lead's toxicity has led to its phasing out for some uses.\n\nLead has been used for bullets since their invention in the Middle Ages. It is inexpensive; its low melting point means small arms ammunition and shotgun pellets can be cast with minimal technical equipment; and it is denser than other common metals, which allows for better retention of velocity. Concerns have been raised that lead bullets used for hunting can damage the environment.\n\nLead's high density and resistance to corrosion have been exploited in a number of related applications. It is used as ballast in sailboat keels; its density allows it to take up a small volume and minimize water resistance, thus counterbalancing the heeling effect of wind on the sails. It is used in scuba diving weight belts to counteract the diver's buoyancy. In 1993, the base of the Leaning Tower of Pisa was stabilized with 600 tonnes of lead. Because of its corrosion resistance, lead is used as a protective sheath for underwater cables.\nLead has many uses in the construction industry; lead sheets are used as architectural metals in roofing material, cladding, flashing, gutters and gutter joints, and on roof parapets. Detailed lead moldings are used as decorative motifs to fix lead sheet. Lead is still used in statues and sculptures, including for armatures. In the past it was often used to balance the wheels of cars; for environmental reasons this use is being phased out in favor of other materials.\n\nLead is added to copper alloys, such as brass and bronze, to improve machinability and for its lubricating qualities. Being practically insoluble in copper the lead forms solid globules in imperfections throughout the alloy, such as grain boundaries. In low concentrations, as well as acting as a lubricant, the globules hinder the formation of swarf as the alloy is worked, thereby improving machinability. Copper alloys with larger concentrations of lead are used in bearings. The lead provides lubrication, and the copper provides the load-bearing support.\n\nLead's high density, atomic number, and formability form the basis for use of lead as a barrier that absorbs sound, vibration, and radiation. Lead has no natural resonance frequencies; as a result, sheet-lead is used as a sound deadening layer in the walls, floors, and ceilings of sound studios. Organ pipes are often made from a lead alloy, mixed with various amounts of tin to control the tone of each pipe. Lead is an established shielding material from radiation in nuclear science and in X-ray rooms due to its denseness and high attenuation coefficient. Molten lead has been used as a coolant for lead-cooled fast reactors.\n\nThe largest use of lead in the early 21st century is in lead–acid batteries. The reactions in the battery between lead, lead dioxide, and sulfuric acid provide a reliable source of voltage. The lead in batteries undergoes no direct contact with humans, so there are fewer toxicity concerns. Supercapacitors incorporating lead–acid batteries have been installed in kilowatt and megawatt scale applications in Australia, Japan, and the United States in frequency regulation, solar smoothing and shifting, wind smoothing, and other applications. These batteries have lower energy density and charge-discharge efficiency than lithium-ion batteries, but are significantly cheaper.\n\nLead is used in high voltage power cables as sheathing material to prevent water diffusion into insulation; this use is decreasing as lead is being phased out. Its use in solder for electronics is also being phased out by some countries to reduce the amount of environmentally hazardous waste. Lead is one of three metals used in the Oddy test for museum materials, helping detect organic acids, aldehydes, and acidic gases.\n\nIn addition to being the main application for lead metal, lead-acid batteries are also the main consumer of lead compounds. The energy storage/release reaction used in these devices involves lead sulfate and lead dioxide:\n\nOther applications of lead compounds are very specialized and often fading. Lead-based coloring agents are used in ceramic glazes and glass, especially for red and yellow shades. While lead paints are phased out in Europe and North America, they remain in use in less developed countries such as China or India. Lead tetraacetate and lead dioxide are used as oxidizing agents in organic chemistry. Lead is frequently used in the polyvinyl chloride coating of electrical cords. It can be used to treat candle wicks to ensure a longer, more even burn. Because of its toxicity, European and North American manufacturers use alternatives such as zinc. Lead glass is composed of 12–28% lead oxide, changing its optical characteristics and reducing the transmission of ionizing radiation. Lead-based semiconductors such as lead telluride and lead selenide are used in photovoltaic cells and infrared detectors.\n\nLead has no confirmed biological role. Its prevalence in the human body—at an adult average of 120 mg—is nevertheless exceeded only by zinc (2500 mg) and iron (4000 mg) among the heavy metals. Lead salts are very efficiently absorbed by the body. A small amount of lead (1%) is stored in bones; the rest is excreted in urine and feces within a few weeks of exposure. Only about a third of lead is excreted by a child. Continual exposure may result in the bioaccumulation of lead.\n\nLead is a highly poisonous metal (whether inhaled or swallowed), affecting almost every organ and system in the human body. At airborne levels of 100 mg/m, it is immediately dangerous to life and health. Most ingested lead is absorbed into the bloodstream. The primary cause of its toxicity is its predilection for interfering with the proper functioning of enzymes. It does so by binding to the sulfhydryl groups found on many enzymes, or mimicking and displacing other metals which act as cofactors in many enzymatic reactions. Among the essential metals that lead interacts with are calcium, iron, and zinc. High levels of calcium and iron tend to provide some protection from lead poisoning; low levels cause increased susceptibility.\n\nLead can cause severe damage to the brain and kidneys and, ultimately, death. By mimicking calcium, lead can cross the blood–brain barrier. It degrades the myelin sheaths of neurons, reduces their numbers, interferes with neurotransmission routes, and decreases neuronal growth. In the human body, lead inhibits porphobilinogen synthase and ferrochelatase, preventing both porphobilinogen formation and the incorporation of iron into protoporphyrin IX, the final step in heme synthesis. This causes ineffective heme synthesis and microcytic anemia.\n\nSymptoms of lead poisoning include nephropathy, colic-like abdominal pains, and possibly weakness in the fingers, wrists, or ankles. Small blood pressure increases, particularly in middle-aged and older people, may be apparent and can cause anemia. Several studies, mostly cross-sectional, found an association between increased lead exposure and decreased heart rate variability. In pregnant women, high levels of exposure to lead may cause miscarriage. Chronic, high-level exposure has been shown to reduce fertility in males.\n\nIn a child's developing brain, lead interferes with synapse formation in the cerebral cortex, neurochemical development (including that of neurotransmitters), and the organization of ion channels. Early childhood exposure has been linked with an increased risk of sleep disturbances and excessive daytime drowsiness in later childhood. High blood levels are associated with delayed puberty in girls. The rise and fall in exposure to airborne lead from the combustion of tetraethyl lead in gasoline during the 20th century has been linked with historical increases and decreases in crime levels, a hypothesis which is not universally accepted.\n\nLead exposure is a global issue since lead mining and smelting, and battery manufacturing/disposal/recycling, are common in many countries. Lead enters the body via inhalation, ingestion, or skin absorption. Almost all inhaled lead is absorbed into the body; for ingestion, the rate is 20–70%, with children absorbing a higher percentage than adults.\n\nPoisoning typically results from ingestion of food or water contaminated with lead, and less commonly after accidental ingestion of contaminated soil, dust, or lead-based paint. Seawater products can contain lead if affected by nearby industrial waters. Fruit and vegetables can be contaminated by high levels of lead in the soils they were grown in. Soil can be contaminated through particulate accumulation from lead in pipes, lead paint, and residual emissions from leaded gasoline.\n\nThe use of lead for water pipes is problematic in areas with soft or acidic water. Hard water forms insoluble layers in the pipes whereas soft and acidic water dissolves the lead pipes. Dissolved carbon dioxide in the carried water may result in the formation of soluble lead bicarbonate; oxygenated water may similarly dissolve lead as lead(II) hydroxide. Drinking such water, over time, can cause health problems due to the toxicity of the dissolved lead. The harder the water the more calcium bicarbonate and sulfate it will contain, and the more the inside of the pipes will be coated with a protective layer of lead carbonate or lead sulfate.\n\nIngestion of applied lead-based paint is the major source of exposure for children:\na direct source is chewing on old painted window sills. Alternatively, as the applied dry paint deteriorates, it peels, is pulverized into dust and then enters the body through hand-to-mouth contact or contaminated food, water, or alcohol. Ingesting certain home remedies may result in exposure to lead or its compounds.\n\nInhalation is the second major exposure pathway, affecting smokers and especially workers in lead-related occupations. Cigarette smoke contains, among other toxic substances, radioactive lead-210.\n\nSkin exposure may be significant for people working with organic lead compounds. The rate of skin absorption is lower for inorganic lead.\n\nTreatment for lead poisoning normally involves the administration of dimercaprol and succimer. Acute cases may require the use of disodium calcium edetate, the calcium chelate, and the disodium salt of ethylenediaminetetraacetic acid (EDTA). It has a greater affinity for lead than calcium, with the result that lead chelate is formed by exchange and excreted in the urine, leaving behind harmless calcium.\n\nThe extraction, production, use, and disposal of lead and its products have caused significant contamination of the Earth's soils and waters. Atmospheric emissions of lead were at their peak during the Industrial Revolution, and the leaded gasoline period in the second half of the twentieth century. Lead releases originate from natural sources (i.e., concentration of the naturally occurring lead), industrial production, incineration and recycling, and mobilization of previously buried lead. Elevated concentrations of lead persist in soils and sediments in post-industrial and urban areas; industrial emissions, including those arising from coal burning, continue in many parts of the world, particularly in the developing countries.\n\nLead can accumulate in soils, especially those with a high organic content, where it remains for hundreds to thousands of years. Environmental lead can compete with other metals found in and on plants surfaces potentially inhibiting photosynthesis and at high enough concentrations, negatively affecting plant growth and survival. Contamination of soils and plants can allow lead to ascend the food chain affecting microorganisms and animals. In animals, lead exhibits toxicity in many organs, damaging the nervous, renal, reproductive, hematopoietic, and cardiovascular systems after ingestion, inhalation, or skin absorption. Fish uptake lead from both water and sediment; bioaccumulation in the food chain poses a hazard to fish, birds, and sea mammals.\n\nAntropogenic lead includes lead from shot and sinkers. These are among the most potent sources of lead contamination along with lead production sites. Lead was banned for shot and sinkers in the United States in 2017, although that ban was only effective for a month, and a similar ban is being considered in the European Union.\n\nAnalytical methods for the determination of lead in the environment include spectrophotometry, X-ray fluorescence, atomic spectroscopy and electrochemical methods. A specific ion-selective electrode has been developed based on the ionophore S,S'-methylenebis(N,N-diisobutyldithiocarbamate). An important biomarker assay for lead poisoning is δ-aminolevulinic acid levels in plasma, serum, and urine.\n\nBy the mid-1980s, there was significant decline in the use of lead in industry. In the United States, environmental regulations reduced or eliminated the use of lead in non-battery products, including gasoline, paints, solders, and water systems. Particulate control devices were installed in coal-fired power plants to capture lead emissions. Lead use was further curtailed by the European Union's 2003 Restriction of Hazardous Substances Directive. A large drop in lead deposition occurred in the Netherlands after the 1993 national ban on use of lead shot for hunting and sport shooting: from 230 tonnes in 1990 to 47.5 tonnes in 1995.\n\nIn the United States, the permissible exposure limit for lead in the workplace, comprising metallic lead, inorganic lead compounds, and lead soaps, was set at 50 μg/m over an 8-hour workday, and the blood lead level limit at 5 μg per 100 g of blood in 2012. Lead may still be found in harmful quantities in stoneware, vinyl (such as that used for tubing and the insulation of electrical cords), and Chinese brass. Old houses may still contain lead paint. White lead paint has been withdrawn from sale in industrialized countries, but specialized uses of other pigments such as yellow lead chromate remain. Stripping old paint by sanding produces dust which can be inhaled. Lead abatement programs have been mandated by some authorities in properties where young children live.\n\nLead waste, depending on the jurisdiction and the nature of the waste, may be treated as household waste (in order to facilitate lead abatement activities), or potentially hazardous waste requiring specialized treatment or storage. Lead is released to the wildlife in shooting places and a number of lead management practices, such as stewardship of the environment and reduced public scrutiny, have been developed to counter the lead contamination. Lead migration can be enhanced in acidic soils; to counter that, it is advised soils be treated with lime to neutralize the soils and prevent leaching of lead.\n\nResearch has been conducted on how to remove lead from biosystems by biological means: Fish bones are being researched for their ability to bioremediate lead in contaminated soil. The fungus \"Aspergillus versicolor\" is effective at removing lead ions. Several bacteria have been researched for their ability to remove lead from the environment, including the sulfate-reducing bacteria \"Desulfovibrio\" and \"Desulfotomaculum\", both of which are highly effective in aqueous solutions.\n\n\n\n"}
{"id": "6003061", "url": "https://en.wikipedia.org/wiki?curid=6003061", "title": "List of additives in cigarettes", "text": "List of additives in cigarettes\n\nThis is the list of 599 additives in cigarettes submitted to the United States Department of Health and Human Services in April 1994. It applies, as documented, only to American manufactured cigarettes intended for distribution within the United States by the listed companies. The five major tobacco companies that reported the information were:\n\nOne significant issue is that while all these chemical compounds have been approved as additives to food, they were not tested by burning. Burning changes the properties of chemicals. According to the U.S. National Cancer Institute: \"Of the more than 7,000 chemicals in tobacco smoke, at least 250 are known to be harmful, including hydrogen cyanide, carbon monoxide, and ammonia. Among the 250 known harmful chemicals in tobacco smoke, at least 69 can cause cancer.\"\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "349340", "url": "https://en.wikipedia.org/wiki?curid=349340", "title": "Men's Health", "text": "Men's Health\n\nMen's Health (MH), published by Rodale Inc. in Emmaus, Pennsylvania, United States, is the world's largest men's magazine brand, with 35 editions in 59 countries. It is also the best-selling men's magazine on U.S. newsstands. Although originally started as a men's health magazine, it currently covers various men's lifestyle topics such as fitness, nutrition, fashion, and sexuality. The magazine's website, MensHealth.com, averages over 118 million page views a month.\n\nStarted by Mark Bricklin in 1986 as a health magazine, \"Men's Health\" evolved into a lifestyle magazine, covering fitness, nutrition, relationships, travel, technology, fashion, and finance. Bricklin, Rodale editors Larry Stains and Stefan Bechtel produced three newsstand test issues. The results led Rodale to start \"Men's Health\" as a quarterly magazine in 1988 and begin to sell subscriptions. Bricklin, who was editor-in-chief of \"Prevention\" magazine, appointed Mike Lafavore as editor of \"Men's Health\" that year. In his 12 years as editor-in-chief, Lafavore increased the circulation from 100,000 to over 1.5 million, increased publication to ten 10 times a year, and expanded the magazine to Australia, France, Germany, Mexico, Russia, South Africa, and the UK.\n\nHe created the editorial formula, hired Steven Slon from service journalism and Greg Gutfeld from \"Prevention\". He worked with longtime staff editor Denis Boyles, a former \"Playboy\" contributing editor, to develop the magazine's voice. Lafavore left \"Men's Health\" in 2000, the same year Capell's Circulation Report named the magazine Circulation Performer of the Decade. He named Gutfeld his successor. After one year, Gutfeld was replaced by David Zinczenko.\n\nZinczenko became editor-in-chief in 2000. Circulation increased 30 percent, ad pages by 80 percent from 700 to 1150. In 2000, the brand had 21 international editions. In 2001 the title was consistently selling 400,000 copies at newsstands and circulation was 1.6 million. In 2001, the magazine started the annual list of cities with the healthiest men, based on twenty \"live-long parameters, including death rates (both homicide and disease); illness rates (high blood pressure, heart disease, stroke, etc.); body-mass index; fitness training; even environmental factors like number of parks, golf courses, etc.\" In 2003, the circulation was 1.7 million. In 2006, the circulation was close to 1.8 million.\n\n\"Men's Health\" magazine has been criticized for its focus on physical health, which can increase men's anxieties about their bodies, making them more prone to eating disorders and compulsive over-exercising. \"The New York Times\" stated, \"Since its debut in the late 1980's, the magazine has surpassed traditional men's books like \"Esquire\" and \"GQ\" by following the formula of best-selling women's magazines—by catering to men's anxieties about their bodies and sexual performance.\" \"Columbia Journalism Review\" stated the magazine \"deals overwhelmingly with self-care and, in fact, exaggerates the possibilities for autonomous personal transformation.\" Editor-In-Chief Zinczenko argued that the magazine worked toward \"overcoming the resistance of the 86-percent male audience to health as a subject\" and redefining health as \"inclusive of everything that could improve a man's life. Great sex. Great food. Endorphin-boosting exercise. Looking and feeling your best. We turned health into a concept every guy would want to embrace, starting with the healthy guy on the cover.\"\n\n\"Men's Health\" has been criticized for reusing cover taglines. Zinczenko replied that 80 percent of magazine sales are by subscription, and those covers differ from the newsstand version. \"Twenty years of \"Men's Health\" has certainly produced several lines that have proven themselves effective at newsstand, which makes up about 20 percent of our print run. We plan to keep using the most effective marketing tools to reach the largest market we possibly can.\" In July 2010, the magazine was criticized for including tiny credit lines on the cover rather than inside as a possible quid-pro-quo for advertisers. Zinczenko said the lines saved readers from having to dig for information and that \"Men's Health\" had been including the lines for over a year regardless of advertiser status. A spokesperson for the American Society of Magazine Editors said that no rules were broken. The director for print strategy at a media firm said the mention was \"too small of a plug to get brands excited.\"\n\nIn 2004, \"Men's Health\" began putting celebrities and athletes on the cover, and with their shirts on—a departure from the covers of the 1990s. In 2004, Rodale filed suit against \"Men's Fitness\" for its redesign, \"a copycat version—one that is obviously intended to confuse consumers.\" In May 2006, the magazine published a limited edition color cover of Josh Holloway. In the first half of 2006, newsstand sales for \"Men's Health\" rose from 492,000 to 544,000 during a price increase from $3.95 to $4.50. In 2006 Rodale's properties, including \"Men's Health\", tried to increase online content by adding video to each section, telling section editors to write blogs, and hiring an online ad sales director.\n\nIn 2008, the magazine partnered with Google to make back issues available. In July 2008, \"Men's Health\" became the first to \"create the first fully interactive advertising magazine in America,\" where readers could take a picture of an ad, and a promotional \"bounce-back\" was sent to their phone. For its 20th anniversary issue in November 2008, \"Men's Health\" included an interview and photo shoot with president-elect Barack Obama. In 2010, Obama was again featured about health care and his plans.\n\nIn 2009, \"Men's Health\" published \"Belly Off! Diet\" based on the weight-loss testimonial column in the magazine. The column \"Eat This, Not That!\" became a book series in 2007, written by Zinczenko and Matt Goulding), and was turned into different versions (children, supermarket, restaurant, diet book) and free iPhone applications. EatThis.MensHealth.com was the most highly trafficked section of MensHealth.com in 2009 with 1 million unique visitors and 15 million page views a month.\n\nEditor-in-chief Matt Bean led the magazine in developing over 40 mobile apps for the iPhone, Android, and BlackBerry. \"Eat This, Not That! The Game\" won an American Society of Magazine Editors award for Best Interactive Tool and was downloaded 500,000 times in two weeks. The magazine's first application, \"Men's Health Workouts\", was in the top 10 in the Health and Fitness category. In September 2009, the column \"Ask Jimmy the Bartender\" was turned into an iPhone and iPad application, which was downloaded 50,000 times in its first month. In 2010, \"Men's Health\" became one of the first consumer magazines to enter the iPad market.\n\nIn 2011, David Zinczenko was replaced by Bill Phillips, who was the executive editor of the magazine and editor of MensHealth.com.\n\nIn November 2014, \"Men's Health\" featured a reader on the cover for the first time with amputee and veteran Noah Galloway, the winner of the first Ultimate \"Men's Health\" Guy Search.\n\nIn February 2015, \"Men's Health\" won the National Magazine Award for General Excellence.\n\nIn 2016, Matt Bean became editor-in-chief. He redesigned the magazine with visual updates inspired by media, such as auto repair guides, hiking maps, and military field manuals, added \"The Exchange\", \"Unfiltered\", \"Field Guide\", and a column by Tim Ferriss. He introduced the digital franchise MH Longform. In October 2017 \"Men's Health\" began the cross-platform series \"The Adventurist\" in partnership with Fitbit.\n\nIn 2000 \"MH-18\", a youth-oriented version of \"Men's Health\" covering teen lifestyle, was spun off but ceased publication in November 2001.\n\nIn 2004 under Zinczenko's direction, \"Men's Health\" spun off \"Best Life.\" May 2009 was \"Best Life\"'s last issue. \"Best Life\" was published 10 times a year and had a circulation of more than 500,000. Stephen Perrine, the former editorial creative director at \"Men's Health\", was the editor-in-chief. David Zinczenko was editorial director. In March 2008, \"Best Life\" finished #2 on Adweek's prestigious \"10 under 50\" Hot List, which recognizes magazines with fewer than $50 million in ad revenue.\n\nIn 2005, \"Men's Health\" spun off \"Women's Health\". The test-issue team was headed by Bill Stump, a former \"Men's Health\" editor who was then the head of Rodale Inc.'s New Product Development department, and included former director of new product development Andréa Mallard. Within a year the circulation was at 750,000. \"Women's Health\" magazine is now published 10 times a year. In January 2009, Michele Promaulayko was named editor-in-chief of \"Women's Health.\" In March 2008, \"Women's Health\" finished #1 on Adweek's \"10 under 50\" Hot List. The magazine was named #2 on Advertising Age's 2008 A List. \"Women's Health\" has a circulation of 1.1 million.\n\nIn 2007, \"Men's Health\" spun off \"Men's Health Living\", a newsstand special which was named one of the 30 most notable launches of 2007 by Samir Husni. Samir Husni stated that \"Men's Health Living\" is a \"new genre of men's magazines that cater to non-woman related issues in a man's life - that has gone unfulfilled for years: interior design and home that meets the needs of the affluent man.\" The test issue of \"Men's Health Living\" was edited by Bill Phillips, executive editor of \"Men's Health\", and Matt Bean. The first issue sold around 200,000 copies at $4.99 each out of 375,000 sent to newsstands. In January 2009, a second \"Men's Health Living\" issue was at newsstands, 450,000 copies at $5.99 each.\n\nIn 2007, they also spun off \"Men's Health on Campus\" as a test with a goal for quarterly publication thereafter.\n\nIn 2009, \"Men's Health\" spun off \"Children's Health\", a special issue that was part of a Rodale publishing idea to work with President and First Lady Obama to show support for the Patient Protection and Affordable Care Act. The magazine published how-to stories about fitness and nutrition for children.\n\nIn 2013, \"Men's Health\" launched the radio show \"Men's Health Live\" in partnership with Entertainment Radio Network.\n\nIn April 2017, under Matt Bean, \"Men's Health\" released an online video franchise, MH Films, which has featured people such as Hafþór Björnsson, Erik Weihenmayer and Sam Calagione. In June 2017, the magazine launched MH Rec Room, specializing in shorter videos for social media featuring various fitness trainers, lifestyle influencers and authors.\n\nIn March 1994, \"Advertising Age\" magazine named Mike Lafavore Editor of the Year. [March 6, 1994]. Four years later he won the International Herald Tribune Award for International Editor of the Year for his work on \"Men's Health\" foreign editions. The magazine was nominated for several National Magazine Awards, including General Excellence. Since 2000, \"Men's Health\" has been nominated for 17 National Magazine Awards, or \"Ellies,\" which are administrated by Columbia University's Graduate School of Journalism and presented by the American Society of Magazine Editors.\n\n\"Men's Health\" won the category of Personal Service in 2004, the first win for the magazine and Rodale. In 2010, \"Men's Health\" received the General Excellence award. Menshealth.com's \"Eat This, Not That!\" portion of their Web site won the 2010 Digital Ellies award, also sponsored by the ASME, for best Interactive Tool, an award honoring the outstanding use of interactive tools that enable readers to create or share content, participate in communities, improve the quality of their lives, or enjoy recreational activities. In 2010, Minonline.com deemed menshealth.com's personal trainer channel, the \"Best Premium Site,\" an award recognizing subscription sites oriented around service. In 2011, \"Men's Health\" won an \"Ad Age\" Media Vanguard Award in the Print-to-Digital Best Reader-Service Website category, a Society of Publication Designers Award for design and photography, and an ASME Ellie in the category of Personal Service for \"I Want My Prostate Back\" by Larry Stains. It was also a finalist in the 2012 Ellies.\n\nIn 2012 \"Men's Health\" won the Digital Magazine Awards' Magazine Launch of the Year for its iPhone edition and a 2012 National Magazine Award in the Personal Service, Digital Media category for \"Skin Cancer Center\".\n\nIn 2013 \"Men's Health\" won the James Beard Foundation Book, Broadcast and Journalism Awards for Cooking/Recipes/Instruction and Food Coverage in General Interest Publication categories. The brand was recognized as one of min’s Top 20 Magazines on Twitter and the \"Men's Health\" \"Guy Gourmet\" Twitter account was included in the \"TIME\" 140 Best Twitter Feeds of 2013. \"Men's Health\" won min's Best of the Web award for Overall Digital Excellence and FAME's Best Series of Events Award for its URBANATHALON series. The brand was included in iMonitor's Best Magazine Apps for iPad list.\n\nIn 2015 \"Men's Health\" won first place in the Service category for the American Society of Journalists and Authors (ASJA)'s Writing Awards for the article \"Clucked\" by Rachael Moeller Gorman, a min Best of the Web & Digital Award in the “Integration with Print” category, and a FOLIO: Marketing Award in the “Integrated Program” category for the Men’s Health Next Top Trainer Program. The magazine also won in the \"Lifestyle\" category for the American Society of Magazine Editors' Best Cover Awards, for its November 2014 cover. It was named Reader’s Choice for men's health/fitness magazines in \"Adweek's\" 2013, 2014, and 2015 Hot List, and both Editor's Choice and Reader's Choice for the 2016 Hot List.\n\nIt was also recognized in 2017 as an \"Ad Age\" magazine of the year. In March 2017 \"Men's Health\" was named a Print Medal Finalist for the Society of Publication Designers' Annual Design Competition Awards; it was also nominated in 2015 and 2016.\n\nAlthough \"Men's Health\" was founded in the U.S., its international editions have made it the world's largest men's magazine brand. These magazines reach over 71 million readers worldwide. \"Men's Health\" is published in 35 editions.\n\nInternational editions account for over 80% of the magazine's trade volume. In each market, local editors commission or purchase articles for their own market and share content with US and other editions. The selected articles are then translated and edited by local staffers to make them match the style of the American edition. Usually, these editions started out as translations of the US version of the magazine, but over time many non-US editions became unique, providing material more pertinent to local readers.\n\n"}
{"id": "55223808", "url": "https://en.wikipedia.org/wiki?curid=55223808", "title": "Miscarriage and grief", "text": "Miscarriage and grief\n\nMiscarriage and grief are both an event and subsequent process of grieving that develops in response to a miscarriage. Almost all those experiencing a miscarriage experience grief. This event is often considered to be identical to the loss of a child and has been described as traumatic. \"Devastation\" is another descriptor of miscarriage. Grief differs from the emotion sadness. Sadness is an emotion along with grief, on the other hand, is a response to the loss a of the bond or affection was formed and is a process rather than one single emotional response. Grief is not equivalent to depression. Grief also has physical, cognitive, behavioral, social, cultural, and philosophical dimensions. Bereavement and mourning refer to the ongoing state of loss, and grief is the reaction to that loss. Emotional responses may be bitterness, anxiety, anger, surprise, fear, and disgust and blaming others; these responses may persist for months. Self-esteem can be diminished as another response to miscarriage. Not only does miscarriage tend to be a traumatic event, women describe their treatment afterwards to be worse than the miscarriage itself.\n\nA miscarriage can often be \"heart-breaking\". A miscarriage can effect the women, husband, partner, siblings, grandparents, the whole family system and friends. Almost all those experiencing a miscarriage go through a grieving process. Serious emotional impact is usually experienced immediately after the miscarriage. Some may go through the same loss when an ectopic pregnancy is terminated. In some, the realization of the loss can take weeks. Providing family support to those experiencing the loss can be challenging because some find comfort in talking about the miscarriage while others may find the event painful to discuss. The father of the baby can have the same sense of loss. Expressing feelings of grief and loss can sometimes be harder for men. Some women are able to begin planning their next pregnancy after a few weeks of having the miscarriage. For others, planning another pregnancy can be difficult. Organizations exist that provide information and counselling to help those who have had a miscarriage. Some women have a higher risk of developing prolonged grief and complicated grief than others.\n\nMiscarriage has an emotional effect and can also lead to psychological disorders. One discorder that can develop is primary maternal preoccupation. This is defined as a \" ...'special psychiatric condition' in which the pregnant woman identifies with her baby, highlights the crisis a woman faces when the baby with whom she is preoccupied and identified dies...\" Grieving manifests itself differently for each woman after miscarriage. It may often go unrecognized. The grief that follows a miscarriage resembles, but is not the same as, the grief experienced after the loss of a family member. Disbelief, depression, anger, and yearning, are described as being a part of the normal grieving process. These reactions remain from three to nine months after the loss. Forty-one percent of parents experience a normal, expected decline in grief in the first two years while 59% were delayed in the resolution of their grief.\n\nGrieving can create feelings of loneliness.\nThis grieving has been called a type of psychological trauma. Other serious consequences can develop including depression, anxiety disorder, post-traumatic stress disorder, and somatoform disorder. These responses all are associated with grieving after a miscarriage. Some women are able to complete the grieving process a few weeks after the miscarriage and start anticipating their next pregnancy. Planning another pregnancy is traumatic for others. The impact of a miscarriage can be \"crippling\" psychologically. Anger can be directed toward those who have had successful pregnancies and children. A woman can grieve the \"loss of a future child\" and question her own role as a mother. They may blame themselves or their partner for the miscarriage.\n\nUnsuccessful attempts to become pregnant through in vitro fertilization (IVF) can also illicit a similar grief response in women. Those experiencing a late miscarriage may have more significant distress compared to those who have experienced a miscarriage in the first trimester. Even depression can occur.\n\n\"Women today...are aught in a unique historical moment: technology encourages them to form emotional attachments to their pregnancies, but society has not developed traditions to cushion the shock when those attachmets are shattered.\"\n\nDescriptions of the miscarriage are expressed in non-clinical terms by those who have experienced the event.\n\nMiscarriage has been found to be a traumatic event and a major loss for women. Pregnancy loss, including induced abortion is a risk factor for mental illness. The impact of miscarriage can be underestimated. The trauma can be compounded if the miscarriage was accompanied by visible and relatively large amounts of blood loss.\nThe trauma of miscarriage can be compounded if the miscarriage was accompanied by visible and relatively large amounts of blood loss.\nBipolar disorders are associated with miscarriage. Depression and bilpolar disorder becomes evident after a miscarriage in 43% of women. Some women are more likely to experience complicated and prolonged grief than others.\n\nWomen experiencing miscarriage are at risk for grief reactions, anxiety or depression. Obsessiveness regarding the miscarriage can develop. Primary maternal preoccupation is also considered a consequence of miscarriage. This condition can occur if a woman who develop a close bond \"with her baby\" experiences the loss of the pregnancy.\n\nDifferent grieving \"styles\" can exist and vary between individuals. There can be a complete avoidance of dealing with the memories of the miscarriage and there can be an \"obsessive\" concentration on an event in the miscarriage. This is in contrast with the expected ability to \"reminisce about the loss of a loved one\". Complicated grief differs from the more common form of grief that occurs after a miscarriage. The grieving process associated with other events such as the loss of a spouse or parent is expected to decline in predictable and steady rate. This not true for those experiencing grief after a miscarriage because only 41% follow the expected decline in grief while most, 59% do not fit this pattern.\n\nMiscarriage is associated with post traumatic stress disorder.\nRisks for developing PTSD after miscarriage are: emotional pain, expressions of emotion, and low levels of social support. Even if relatively low levels of stress occur after the miscarriage, symptoms of PTSD including flashbacks, intrusive thoughts, dissociation and hyperarousal can later develop. The effects of stress can complicate miscarriage. Miscarriage is a stressful event and because stress is a risk factor for subsequent miscarriage, its presence can become part of cycle that continues. Lower stress levels are associated with more favorable outcomes in future pregnancies while higher stress levels increase the risk.\n\nPhysical recovery from miscarriage can have an effect on emotional disturbances. The body has to recover from the sudden pregnancy loss. In some instances ffatigue is present. Insomnia can be a problem. The miscarriage is very upsetting to the family and can generate very strong emotions. Some women may feel that the miscarriage occurred because they somehow had caused it. Others may blame the father or partner for the miscarriage. Coping with a miscarriage can very greatly between women and families. Some find it difficult to talk about the miscarriage. The narratives of women tend to coincide with quantified and measurable effects. Some women engage in activities that are believed to aid in recovery such as therapy, religion and art.\nCounseling can be offered but effective interventions to assist in recovery have been difficult to identify due to the reports of efficacy and ineffective counseling. Comparisons are hard to make. Despite the lack of studies that describe effective interventions for those with grief after a miscarriage, some clinicians still offer counselling and follow-up to help women recover and adapt to the loss.\n\nRecommendations to help recover from the event include:\n\nGenerally, the impact of experiencing miscarriage is underestimated. Other methods used to promote recovery are be relaxation techniques, guided imagery, and \"thought-stopping\". Even Gestalt role-playing has been used. Some women can \"emotionally relocate the child\", redefine a relationship \"with the missing child\", and engage in \"continuing the bond\" to incorporate the loss into their life experiences.\n\nWomen who have miscarried report that they were dissatisfied with the care they received from physicians and nurses. One observer highlights the insensitivity of some health care providers when they approach the grieving mother \"...by playing down her emotion as somehow an irrational response...\" Clinicians may not recognize the psychological impact of the miscarriage and can \"expect parents to move on with their lives.\"\n\nSince the experiences of women can vary so widely, sensitive nursing care afterward is appropriate.\n\nOne emotional response to miscarriage is the strong apprehension that can develop anticipating a subsequent pregnancy. Procreation abilities may also be questioned by the woman. Significant distress can develop in the other children in the family when they think a sibling has died. They may regard this as a baby they did not get to meet. They can also experience grief and guilt find it difficult to express these emotions to their parents. The siblings may feel a need to act as if everything is the same and that they are unaffected in an attempt to protect their parents from their own feelings. Children can also need help, understanding and the ability to make sense of the event.\n\nRituals that recognize the loss can be important in coping. Family and friends often conduct a memorial or burial service. Hospitals also can provide support and help memorialize the event. Depending on locale others desire to have a private ceremony. The religious faith of the woman and family impacts the grief process. Conversely, the lack of recognition that the miscarriage has occurred by family and friends can be troubling and add to the trauma of the event.\nGrieving after the loss of a child through miscarriage in other cultures can vary from western culture. An individual’s culture plays a large role in determining an inappropriate pattern of grief, and it is appropriate to take into account cultural norms before reaching a complicated grief diagnosis. There are cultural differences in emotional levels, how these are expressed and how long they are expressed. External symptoms of grief differ in non-Western cultures, presenting increased somatization. Narratives by Swedish women include their own perception of losing a child. Investigations describe their grief over their miscarriage: \"When miscarriage occurs it is not a gore, an embryo, or a fetus they lose, it is their child. They feel that they are the cause of the miscarriage through something they have done, eaten, or thought. They feel abandonment and they grieve for their profound loss; they are actually in bereavement.\" Native American women have cut their long hair following the death of a family member. The narratives of women tend to coincide with quantified and measurable effects. In women who are induced to have an abortion, an identical grieving process can occur. The emotional responses to a spontaneous abortion (miscarriage) and an elective abortion are sometimes identical. Spanish women experience grief in much the same way in the rest of Western culture. Some women find online forums helpful.\n\n\n\n"}
{"id": "25678466", "url": "https://en.wikipedia.org/wiki?curid=25678466", "title": "Monthly nurse", "text": "Monthly nurse\n\nA monthly nurse is a woman who looks after a mother and her baby during the postpartum or postnatal period.\n\nHistorically, women were confined to their beds or their homes for extensive periods after giving birth; care was provided either by her female relatives (mother or mother-in-law), or, for those who could afford it, by the monthly nurse. These weeks were called confinement or lying-in, and ended with the re-introduction of the mother to the community in the Christian ceremony of the churching of women. The term \"monthly nurse\" was most common in 18th and 19th century England.\n\nThe job still exists, although it now might be described as \"postnatal doula\" or \"maternity nurse\" or \"newborn care specialist\" - all specialist sorts of nannies. A modern version of this rest period has evolved, to give maximum support to the new mother, especially if she is recovering from a difficult labour and delivery. It is especially popular in China and its diaspora, where postpartum confinement is known as \"sitting the month\".\n\nFrom long ago, the delivery of children and care of the mothers was a profession often handed down from mother to daughter, with the daughter spending many years as the pupil or apprentice. The Church supported that by a system of licensing, which required midwives to swear to certain rules relating to contraception, abortion and concealment of births and also to deliver the newborn infants for baptism or, in extreme cases, to perform the ceremony themselves.\n\nIn the mid-18th century the legal status of midwives was withdrawn and the responsibility for delivery was vested in the surgeon. The work of the nurse element had to be covered, as \"who was to look after the baby?\" Clearly, the first thought that would naturally occur to a mother was that the best person to look after her baby was a woman who had had one herself. Often, the task was allotted to motherly or grandmotherly hands and, from that requirement for postnatal care, the monthly nurse originated. \"The Nursing Record\" reported that \"there was little or no attempt at knowledge or instruction, and we know as a fact that ignorance, prejudice and neglect resulted in a goodly crop of errors, wrongs, and woes as regards the hapless infant\".\n\nThe term \"monthly nurse\" is one that is frequently used to describe the nurse who cares for lying-in cases, certainly because such a nurse frequently remains with the patient for four weeks. The term \"monthly\" is somewhat inaccurate, as there is no reason for the nurse's services to be dispensed with after ten days or retained for much longer, but it is entirely a matter of arrangement.\n\n\"The Nursing Record\" reported that \"nurses who attend the 'artisan' classes in their confinements as a rule pay a visit daily for ten days and then give up the case, as few working class mothers can afford to lie up for longer\".\n\nA monthly nurse could earn more than a midwife, as the monthly nurse was employed for periods between 10 days and often much longer and might attend several women on a part time basis. She often \"lived in\". The midwife's only duty was perceived as \"being trained to assist the parturient woman while nature does her own work and able to call upon a surgeon who could step in where nature fails and skill and science are required\". Many certified midwives transferred to the ranks of monthly nurses to benefit from an increased income.\n\nAlthough 'registration' was not available for women to act as midwives or monthly nurses a system of 'certification' was in being in the late 19th century and continued into the early 20th century. To qualify, a candidate monthly nurse would attend a course in a lying-in hospital for four or five weeks and a midwife for up to three months. The prospective midwives and monthly nurses, as a rule, paid their own charges in respect of hospital expenses and then entered practice on their own responsibility. In 1893, a Miss Gosling reported that \"although the certificated monthly nurse could be relied upon as being trustworthy and efficient, there were a number of women who attend lectures for a short time and through one cause or another fail to pass their examination and obtain a certificate nevertheless enter a 'Nurses Home' or open one for themselves\".\n\nAs might be expected rogue institutions issued certificates and diplomas “for a price”. Another that reporting on a lying in hospital and signed herself a ‘victim of the system’ said that she “witnessed the first phase of the system which turns out yearly hundreds of midwives and monthly nurses on an unsuspecting public. These would be nurses represented almost every grade of the lower classes and every degree of lack of education, and one woman, I remember could not write. Personally I found many to be dishonest, untruthful, indescribably dirty in their habits and persons, utterly unprincipled, shockingly coarse and deficient intelligence, and with not the faintest idea of discipline”’\n\nIn the late 19th century, reformers were calling not only for registration and recognition of the profession of midwife but also for the two functions of midwife and monthly nurse to be amalgamated: \"The work of midwives lies, for the most part, amongst the poor and the poor lying-in woman needs not only to be delivered, but to be visited for some ten days subsequent to her confinement\". The registration of midwives was opposed by members of the House of Lords and Parliament for many years, who argued that the delivery of infants was the responsibility of trained doctors and to allow women to do the job, even in straightforward cases, would take away doctors' income. It was not until the Midwives Act 1902, following 12 years of representation by women, that midwives were \"registered\", but it would still take several years for it to be accepted. The professional training and formal qualification of midwives, and eventually, the postnatal care offered by the National Health Service, saw the end of the monthly nurse.\n\n\n"}
{"id": "2241254", "url": "https://en.wikipedia.org/wiki?curid=2241254", "title": "OpenEHR", "text": "OpenEHR\n\nopenEHR is an open standard specification in health informatics that describes the management and storage, retrieval and exchange of health data in electronic health records (EHRs). In openEHR, all health data for a person is stored in a \"one lifetime\", vendor-independent, person-centred EHR. The openEHR specifications include an EHR Extract specification but are otherwise not primarily concerned with the exchange of data between EHR-systems as this is the focus of other standards such as EN 13606 and HL7.\n\nThe openEHR specifications are maintained by the openEHR Foundation, a not for profit foundation supporting the open research, development, and implementation of openEHR EHRs. The specifications are based on a combination of 15 years of European and Australian research and development into EHRs and new paradigms, including what has become known as the archetype methodology for specification of content.\n\nThe openEHR specifications include information and service models for the EHR, demographics, clinical workflow and archetypes. They are designed to be the basis of a medico-legally sound, distributed, versioned EHR infrastructure.\n\nThe architecture of the openEHR specifications as a whole consists of the following key elements:\n\n\nThe use of the first two enable the development of 'archetypes' and 'templates', which are formal models of clinical and related content, and constitute a layer of \"de facto\" standards of their own, far more numerous than the base specifications on which they are built. The query language enables queries to be built based on the archetypes, rather than physical database schemata, thus decoupling queries from physical persistence details. The service models define access to key back-end services, including the EHR Service and Demographics Service, while a growing set of lightweight REST-based APIs based on archetype paths are used for application access.\n\nThe openEHR Architecture Overview provides a summary of the architecture and the detailed specifications.\n\nA central part of the openEHR specifications is the set of information models, known in openEHR as 'reference models'. The models constitute the base information models for openEHR systems, and define the invariant semantics of the Electronic Health Record (EHR), EHR Extract, and Demographics model, as well as supporting data types, data structures, identifiers and useful design patterns.\n\nSome of the key classes in the EHR component are the ENTRY classes, whose subtypes include OBSERVATION, EVALUATION, INSTRUCTION, ACTION and ADMIN_ENTRY, as well as the Instruction State Machine, a state machine defining a standard model of the lifecycle of interventions, including medication orders, surgery and other therapies.\n\nA key innovation in the openEHR framework is to leave all specification of clinical information out of the information model (also known as \"reference model\") and instead to provide a powerful means of expressing definitions of the content clinicians and patients need to record that can be directly consumed at runtime by systems built on the Reference Model. This is justified by the need to deal scalably with the generic problem in health of a very large, growing, and ever-changing set of information types.\n\nClinical content is specified in terms of two types of artefact which exist outside the information model. The first, known as \"archetypes\" provides a place to formally define re-usable data point and data group definitions, i.e. content items that will be re-used in numerous contexts. Typical examples include \"systemic arterial blood pressure measurement\" and \"serum sodium\". Many such data points occur in logical groups, e.g. the group of data items to document an allergic reaction, or the analytes in a liver function test result. Some archetypes contain numerous data points, e.g. 50, although a more common number is 10-20. A collection of archetypes can be understood as a \"library\" of re-usable domain content definitions, with each archetype functioning as a \"governance unit\", whose contents are co-designed, reviewed and published.\n\nThe second kind of artefact is known in openEHR as a \"template\", and is used to logically represent a use case-specific data-set, such as the data items making up a patient discharge summary, or a radiology report. A template is constructed by referencing relevant items from a number of archetypes. A template might only require one or two data points or groups from each archetype. In terms of the technical representation, openEHR templates cannot violate the semantics of the archetypes from which they are constructed. Templates are almost always developed for local use by software developers and clinical analysts. Templates are typically defined for GUI screen forms, message definitions and document definitions, and as such, correspond to \"operational\" content definitions.\n\nThe justification for the two layers of models over and above the information model is that if data set definitions consist of pre-defined data points from a library of such definitions, then all recorded data (i.e. instances of templates) will ultimately just be instances of the standard content definitions. This provides a basis for standardised querying to work. Without the archetype \"library\" level, every data set (i.e. chunk of operational content) is uniquely defined and a standard approach to querying is difficult.\n\nAccordingly, openEHR defines a method of querying based on archetypes, known as AQL (Archetype Querying Language).\n\nNotably, openEHR has been used to model shared care plan. The archetypes have been designed to accommodate the concepts of the shared care plan.\n\nWhile individual health records may be vastly different in content, the core information in openEHR data instances always complies to archetypes. The way this works is by creating archetypes which express clinical information in a way that is highly reusable, even universal in some cases.\n\nopenEHR archetypes are expressed in \"Archetype Definition Language\", an openEHR public specification. Two versions are available: ADL 1.4, and ADL 2, a new release with better support for specialisation, redefinition and annotations, among other improvements. The 1.4 release of ADL and its \"object model\" counterpart Archetype Object Model (AOM) are the basis for the CEN and ISO \"Archetype Definition Language\" standard (ISO standard 13606-2).\n\nTemplates have historically been developed in a simple, de facto industry-developed XML format, known as \".oet\", after the file extension. ADL 2 defines a way to express templates seamlessly with archetypes, using extensions of the ADL language.\n\nVarious principles for developing archetypes have been identified. For example, a set of openEHR archetypes needs to be quality managed to conform to a number of axioms such as being mutually exclusive. The archetypes can be managed independently from software implementations and infrastructure, in the hands of clinician groups to ensure they meet the real needs on the ground. Archetypes are designed to allow the specification of clinical knowledge to evolve and develop over time. Challenges in implementation of information designs expressed in openEHR centre on the extent to which actual system constraints are in harmony with the information design.\n\nIn the field of Electronic health records there are a number of existing information models with overlaps in their scope which are difficult to manage, such as between HL7 V3 and SNOMED CT. The openEHR approach faces harmonisation challenges unless used in isolation.\n\nFollowing the openEHR approach, the use of shared and governed archetypes globally would ensure openEHR health data could be consistently manipulated and viewed, regardless of the technical, organisational and cultural context. This approach also means the actual data models used by any EHR are flexible, given that new archetypes may be defined to meet future needs of clinical record keeping. Recently work in Australia has demonstrated how archetypes and templates may be used to facilitate the use of legacy health record and message data in an openEHR health record system, and output standardised messages and CDA documents.\n\nThe prospect of gaining agreement on design and on forms of governance at the international level remains speculative, with influences ranging from the diverse medico-legal environments to cultural variations, to technical variations such as the extent to which a reference clinical terminology is to be integral.\n\nThe openEHR Framework is consistent with the Electronic Health Record Communication Standard (ISO 13606), and the Archetype Object Model 2 (AOM2) has been officially accepted by ISO TC 215 as the draft specification for the 2017 revision of ISO 13606:2.\n\nopenEHR archetypes are being used by the National e-Health Transition Authority of Australia, the UK NHS Health and Social Care Information Centre (HSCIC), the Norwegian Nasjonal IKT organisation, and the Slovenian Ministry of Health.\n\nopenEHR has been selected as the basis for the standardised EHR in Brazil .\n\nIt is beginning to be utilised in commercial solutions throughout the world, including those produced by the openEHR Industry Partners.\n\nOne of the outcomes of openEHR modelling approach is the open development of archetypes, templates and terminology subsets to represent health data. Due to the open nature of openEHR, these structures are publicly available to be used and implemented in health information systems. Community users are able to share, discuss and approve these structures in a collaborative repository known as the Clinical Knowledge Manager (CKM). Some currently used openEHR CKMs:\n\n\n"}
{"id": "28236488", "url": "https://en.wikipedia.org/wiki?curid=28236488", "title": "Ovarian drilling", "text": "Ovarian drilling\n\nOvarian drilling, also known as multiperforation or laparoscopic ovarian diathermy, is a surgical technique of puncturing the membranes surrounding the ovary with a laser beam or a surgical needle using minimally invasive laparoscopic procedures. It differs from ovarian wedge resection, because resection involves the cutting of tissue. Minimally invasive ovarian drilling procedures have replaced wedge resections. Ovarian drilling is preferred to wedge resection because cutting in to the ovary can cause adhesions which may complicate postoperative outcomes. Ovarian drilling and ovarian wedge resection are treatment options to reduce the amount of androgen producing tissue in women with Polycystic Ovarian Syndrome (PCOS). PCOS is the primary cause of anovulation, which results in female infertility. The induction of mono-ovulatory cycles can restore fertility.\n\nLaparoscopic ovarian drilling (LOD) may improve the effectiveness of other ovulation induction treatments and results in lower multiple pregnancy rates than other treatment options like gonadotropins. The oral drug, clomiphene citrate (CC), is the first-line treatment for PCOS, yet, one fifth of women are resistant to the drug and fail to ovulate. Patients are considered resistant if the treatment fails for 6 months at the appropriate dosage. Ovarian drilling is a surgical alternative to CC treatment or recommended for women with WHO Group II ovulation disorders. Other non-surgical medical options in the treatment of PCOS include the oestrogen receptor modulator, tamoxifen, aromatase inhibitors, insulin sensitising drugs, and hormonal ovarian stimulation. The effectiveness of the surgical procedure is similar to CC or gonadotropin treatment for induced ovulation for PCOS patients, but results in fewer multiple pregnancies per ongoing pregnancy regardless if the technique is unilaterally or bilaterally performed.\n\nIf patients do not become pregnant 6 months after ovulation has been reestablished from ovarian drilling treatment, drug treatments may be reintroduced or in vitro fertilization (IVF) may be considered.\n\nPart of the criteria of PCOS diagnosis includes elevated levels of androgens in the bloodstream or other signs of androgen excess (hyperandrogenism). The procedure causes a drop in serum androgen levels and possibly in estrogen levels. After ovarian follicles and stroma are destroyed, there is a reduction in these hormone levels. The procedure results in a decrease in plasma luteinizing hormone (LH) and in pulsations as well as a periodic drop in inhibin B levels. The most plausible theory states that the reduction of these hormone concentrations leads to an increase in the secretion of follicle-stimulating hormone (FSH) and sex hormone-binding globulin, leading to effective follicular maturation and ovulation. Low serum oestradiol concentrations are associated with decreased aromatase activity. Inflammatory growth factors such as insulin-like growth factor-1 are produced due to injury and aid the effects of FSH through greater blood flow and gonadotropin delivery. Circulating and intrafollicular levels of anti-mullerian hormone (AMH), which can help quantify recruitable ovarian follicle activity, are reduced after laparoscopic ovarian drilling in women with PCOS.\n\nWhen the clinician determines that ovarian drilling is appropriate and the woman decides to undergo this treatment, consent is obtained. The risks are communicated to the woman.\n\nThe most commonly performed method is with a monopolar needle or hook because of the equipment's availability and simple installation. Other common instrumentation consists of the use of a bipolar electrical surgical electrodes or a CO, argon, or ND-YAG laser. This instrumentation has the ability to produce the intended results with a very focal approach. Typically, a 100 W electrical cautery dissector is first used to cross the ovarian cortex, then electrocoagulation is performed at 40 W, however rates range from 30 to 400 W. The surgical punctures are performed on the ovarian cortex and are usually 4–10 mm deep and 3 mm wide. The number of punctures is related to subsequent ability to conceive—it has been found that five to ten punctures are more likely to produce the intended conception. Ovarian drilling is performed laparoscopically and either transumbilical (culdoscopy) or transvaginal (fertiloscopy).\n\nThough preferable to creating incisions on the ovary, ovarian drilling does have some risks. These are: pelvic adhesion formation, hemorrhage, gas embolism, pneumothorax, premature ovarian failure, long-term ovarian function, developing hyperstimulation syndrome, adhesion formation, infertility and multiple births. Transvaginal hydrolaparoscopy (THL) ovarian drilling may minimize the risk of iatrogenic adhesion formation and decreased ovarian reserve (DOR), which can impinge upon fertility. LOD does not contribute to the risk of decreased ovarian reserve. There is risk of electrical accidents with monopoly current. A rare complication of LOD is major vascular injury, mostly on the small vessels in the anterior abdominal wall when the Veress needle and trocar are inserted at the beginning of the procedure. \n\nOvarian drilling has lower rates of ovarian hyperstimulation syndrome and of multi-fetal gestation. The advantages of the procedure also include its singular treatment, as opposed to several trials of ovulation inductions. Other benefits of this technique include cost-effectiveness and that it can be performed as an outpatient procedure.\n\nOvarian drilling was first used in the treatment of PCOS in 1984 and has evolved as a safe and effective surgery. After performing laparoscopic electrosurgical ovarian drilling in CC-resistant patients in 1984, Gjönnaess found that this technique increased ovulation rates to 45 percent and pregnancy rates to 42 percent. In 1988, laparoscopic multiple punch resection of ovaries on the hypothalamo-pituitary axis, slightly modified from Gjönnaess's operation, caused a reduction in LH pulsation and pituitary responsiveness in the treatment of PCOS. In 1989, ovarian drilling was conducted with argon, carbon dioxide (CO2) or potassium-titanyl-phosphate (KTP) laser vaporization causing spontaneous ovulation in 71 percent of those treated. The procedure has been modified and popularized in the treatment of patients with CC-resistance.\n"}
{"id": "51013265", "url": "https://en.wikipedia.org/wiki?curid=51013265", "title": "Patient-initiated violence", "text": "Patient-initiated violence\n\nPatient-initiated violence is a specific form of workplace violence that affects healthcare workers that is the result of verbal, physical, or emotional abuse from a patient or family members of whom they have assumed care. Nurses represent the highest percentage of affected workers; however, other roles include physicians, therapists, technicians, home care workers, and social workers. The Occupational Safety and Health Administration used 2013 Bureau of Labor Statistics and reported that healthcare workplace violence requiring days absent from work from patients represented 80% of cases. In 2014, a survey by the American Nurses Association of 3,765 21% of nurses and nursing students reported physical abuse and over 50% reported verbal abuse within a 12-month period. Causes for patient outbursts vary, including psychiatric diagnosis, under the influence of drugs or alcohol, or subject to a long wait time. Certain areas are more at risk for this kind of violence including healthcare workers in psychiatric settings, emergency or critical care, or long term care and dementia units.\n\nThe American College of Emergency Physicians found that greater than 75% of emergency physicians were the victim of one or more violent incidents in the workplace, noting that the majority was by patients or their families. Causes for the increased presence of violence, especially in urban settings, are related to gang activity, lengthened waiting periods to see a doctor, a failure of community medical resources, and unavailable acute psychiatric treatment. In 2011 the Emergency Nurses Association studies the occurrence of physical violence at 54.4% and verbal violence at 42.5% among emergency room nurses. Within this study, 55.7% perpetrators of physical violence were under the influence of alcohol. 46.8% were under the influence of illegal or prescription drugs, and 45.2% were being treated for psychiatric reasons. A majority of the violent behavior that occurred was during the triage process at 40.2%.\n\nWorkers in departments that specialize in mental health are particularly at risk for patient abuse due to the psychiatric disease states, high rates of substance abuse, and previous violent patient behavior. A study of Canadian psychiatric nurses noted that social stigma of psychiatric disorders plays a significant role in how nurses perceive violence. Caregivers were uncomfortable with the notion that patient violence is part of the job but also that it is unfair to believe that those with mental illness should be seen as more violent in nature. Osha reported that violent injuries in psychiatric aids was 10 times higher than any other healthcare worker at 590 injuries per 10,000 full-time employees.\n\nUnderreporting of patient initiated violence is common with professionals claiming that assault is a part of the job. A report from the National Center for the Analysis of Violent Crime dedicates underreporting is likely due to a fear of retaliation, or belief that it will not lead to any change. There is also a commonly held belief that violence is a part of the job. The Massachusetts Nurses Association followed up on this common belief through a survey of three New England hospitals, finding that only 39% of participants reported all incidences of violence. The same report found that 70% of those who reported an incident found that management was supportive, however a majority noted that nothing was done to solve the problem. A study of Canadian psychiatric nurses reported that not only was violence a part of the job, but its occurrence no longer warranted a reporting. The same report noted that underreporting drastically affects the ability of the Occupational Safety and Health Administration to track these occurrences. Many qualitative studies done on nurses suggest that there is frequent discouragement by hospital officials and legal officers to not press charges against abusive patients or their families related to an understanding that violence is a part of the job.\n\nThe effects of patient initiated violence has been found correlate to lasting symptoms of post traumatic stress disorder, acute stress disorder, and high rates of burnout. Nurses who experience a lack of support from public officials after the event reported feelings of anxiety and frustration. OSHA sampled one hospital who paid for medical treatment of 30 staff members subject to patient initiated violence over a one-year period costing $94,146. It was also estimated that the costs of separation, recruitment, hiring, and training of new staff to be anywhere from 25,000 to 103,000.\n\nA study in Orebro Reginal Hospital in Sweden suggested a link between patient initiated violence, burnout, and decreased care outcomes. It was reported that the highest indicator of care quality outcomes is a positive or negative association with an individuals work environment.\n\nIn November 2014 Charles Logan, a 68-year-old patient at St John's Hospital in Mapplewood Minnesota attacked nurses using a bar from his stretcher.\n\nSolutions to this issues range dependent on facility and location. A common suggestions from nursing staff is for additional trainings specifically on the de-escalation of high risk situations and health professional legal rights 55% of participants of workers in New England Hospitals stated they were aware of their legal rights relating to workplace violence. The national institute of occupational safety and health (NOSHA) created a free online training module that went live in 2013. The Veterans Health Administration has reduced occurrences of assaults by flagging high risk based on previous documentation of attacks on caregivers. The American Nurses Association has modeled a state bill for a “Violence Prevention in Health Care Facilities Act” that would call for the creation of violence prevention committees, annual violence prevention training, and sufficient record keeping of violence acts.\n\n"}
{"id": "17577602", "url": "https://en.wikipedia.org/wiki?curid=17577602", "title": "Pesticide formulation", "text": "Pesticide formulation\n\nThe biological activity of a pesticide, be it chemical or biological in nature, is determined by its active ingredient (AI - also called the \"active substance\"). Pesticide products very rarely consist of pure technical material. The AI is usually formulated with other materials and this is the product as sold, but it may be further diluted in use. Formulations improves the properties of a chemical for handling, storage, application and may substantially influence effectiveness and safety.\n\nFormulation terminology follows a 2-letter convention: (\"e.g.\" GR: granules) listed by CropLife International (formerly GIFAP then GCPF) in the \"Catalogue of Pesticide Formulation Types\" (Monograph 2); see: download page . Some manufacturers do not follow these industry standards, which can cause confusion for users.\n\nBy far the most frequently used products are formulations for mixing with water then applying as sprays.\nWater miscible, older formulations include:\n\nNewer, non-powdery formulations with reduced or no use of hazardous solvents and improved stability include:\n\nOther common formulations include granules (GR) and dusts (DP), although for improved safety the latter have been replaced by microgranules (MG \"e.g\". for rice farmers in Japan). Specialist formulations are available for ultra-low volume spraying, fogging, fumigation, \"etc\". Very occasionally, some pesticides (\"e.g\". malathion) may be sold as technical material (TC - which is mostly AI, but also contains small quantities of, usually non-active, by-products of the manufacturing process).\n\nA particularly efficient form of pesticide dose transfer is seed treatment and specific formulations have been developed for this purpose. A number of pesticide bait formulations are available for rodent pest control, \"etc\".\n\nIn reality many formulation codes are used: AB, AE, AL, AP, BB, BR, CB, CF, CG, CL, CP, CS, DC, DL, DP, DS, DT, EC, ED, EG, EO, ES, EW, FD, FG, FK, FP, FR, FS, FT, FU, FW, GA, GB, GE, GF, GG, GL, GP, GR, GS, GW, HN, KK, KL, KN, KP, LA, LS, LV, MC, ME, MG, MV, OD, OF, OL, OP, PA, PB, PC, PO, PR, PS, RB, SA, SB, SC, SD, SE, SG, SL, SO, SP, SS, ST, SU, TB, TC, TK, TP, UL, VP, WG, WP, WS, WT, XX, ZC, ZE and ZW.\n\n"}
{"id": "29786555", "url": "https://en.wikipedia.org/wiki?curid=29786555", "title": "Placental expulsion", "text": "Placental expulsion\n\nPlacental expulsion (also called afterbirth) occurs when the placenta comes out of the birth canal after childbirth. The period from just after the baby is expelled until just after the placenta is expelled is called the third stage of labor.\n\nThe third stage of labor can be managed actively with several standard procedures, or it can be managed expectantly (also known as \"physiological management\" or \"passive management\"), the latter allowing the placenta to be expelled without medical assistance.\n\nAlthough uncommon, in some cultures the placenta is kept and consumed by the mother over the weeks following the birth. This practice is termed placentophagy.\n\nAs the fetal hypothalmus matures, activation of the HPA axis (hypothalmic-pituitary-adrenal axis) initiates labour though two hormonal mechanisms. The end pathway of both mechanisms leads to myometrial contractions a mechanical cause of placental separation, from the shear force, contractile and involution changes in the uterus distorting the placentome.\n\n\nAs the HPA axis activates the posterior pituitary of the fetus begins to increase production of oxytocin, which stimulates the maternal myometrium to contract.\n\nIn the seventh month of pregnancy the MHC-I complexes increase in the interplacentomal arcade reducs the bi- and tri-nucleate cells, a source of immune suppression in pregnancy. By the ninth month the endometrial lining has thinned (due to loss of trophoblast giant cells) which exposes the endometrium directly to the fetal trophoblast epithelium. With this exposure and the increase in maternal MHC-I, T-helper 1 (Th1) cells, and macrophages induce apoptosis of trophoblast cells and endometrial epithelial cells, facilitating placental release. Th1 cells attract an influx of phagocytic leukocytes into the placentome at separation, allowing further degration of the extracellular matrix.\n\nAfter delivery, loss of fetal blood return to the placenta allows for shrinkage and collapse of the cotyledonary villi with subsequent fetal membrane separation\n\nMethods of active management include umbilical cord clamping, stimulation of uterine contraction and cord traction.\n\nActive management routinely involves clamping of the umbilical cord, often within seconds or minutes of birth.\n\nUterine contraction assists in delivering the placenta. Uterine contraction reduces the placental surface area, often forming a temporary hematoma at their former interface. Myometrial contractions can be induced with medication, usually oxytocin via intramuscular injection. The use of ergometrine, on the other hand, is associated with nausea or vomiting and hypertension.\n\nBreastfeeding soon after birth stimulates oxytocin which increases uterine tone, and through physical mechanisms uterine massage (the fundus) also causes uterine contractions.\n\nControlled cord traction (CCT) consists of pulling on the umbilical cord while applying counter pressure to help deliver the placenta. It may be uncomfortable for the mother. Its performance requires specific training. Premature cord traction can pull the placenta before it has naturally detached from the uterine wall, resulting in hemorrhage. Controlled cord traction requires the immediate clamping of the umbilical cord.\n\nA Cochrane review came to the results that controlled cord traction does not clearly reduce severe postpartum hemorrhage (defined as blood loss >1000 mL) but overall resulted in a small reduction in postpartum hemorrhage (defined as blood loss >500 mL) and mean blood loss. It did reduce the risk of manual placenta removal. The review concluded that use of controlled cord traction should be recommended if the care provider has the skills to administer it safely.\n\nManual placenta removal is the evacuation of the placenta from the uterus by hand. It is usually carried out under anesthesia or more rarely, under sedation and analgesia. A hand is inserted through the vagina into the uterine cavity and the placenta is detached from the uterine wall and then removed manually. A placenta that does not separate easily from the uterine surface indicates the presence of placenta accreta.\n\nA Cochrane database study suggests that blood loss and the risk of postpartum bleeding will be reduced in women offered active management of the third stage of labour. A summary of the Cochrane study came to the results that active management of the third stage of labour, consisting of controlled cord traction, early cord clamping plus drainage, and a prophylactic oxytocic agent, reduced postpartum haemorrhage by 500 or 1000 mL or greater, as well as related morbidities including mean blood loss, incidences of postpartum haemoglobin becoming less than 9 g/dL, blood transfusion, need for supplemental iron postpartum, and length of third stage of labour. Although active management increased adverse effects such as nausea, vomiting, and headache, women were less likely to be dissatisfied.\n\nA retained placenta is a placenta that doesn't undergo expulsion within a normal time limit. Risks of retained placenta include hemorrhage and infection. If the placenta fails to deliver in 30 minutes in a hospital environment, manual extraction may be required if heavy ongoing bleeding occurs, and very rarely a curettage is necessary to ensure that no remnants of the placenta remain (in rare conditions with very adherent placenta, placenta accreta). However, in birth centers and attended home birth environments, it is common for licensed care providers to wait for the placenta's birth up to 2 hours in some instances.\n\nIn most mammalian species, the mother bites through the cord and consumes the placenta, primarily for the benefit of prostaglandin on the uterus after birth. This is known as placentophagy. However, it has been observed in zoology that chimpanzees apply themselves to nurturing their offspring, and keep the fetus, cord, and placenta intact until the cord dries and detaches the next day.\n\nThe placenta exists in most mammals and some reptiles. It is probably polyphyletic, having arisen separately in evolution rather than being inherited from one distant common ancestor.\n\nStudies on pigs indicate that the duration of placenta expulsion increases significantly with increased duration of farrowing.\n"}
{"id": "48676301", "url": "https://en.wikipedia.org/wiki?curid=48676301", "title": "Population informatics", "text": "Population informatics\n\nThe field of population informatics is the systematic study of populations via secondary analysis of massive data collections (termed \"big data\") about people. Scientists in the field refer to this massive data collection as the social genome, denoting the collective digital footprint of our society. Population informatics applies data science to social genome data to answer fundamental questions about human society and population health much like bioinformatics applies data science to human genome data to answer questions about individual health. It is an emerging research area at the intersection of SBEH (Social, Behavioral, Economic, & Health) sciences, computer science, and statistics in which quantitative methods and computational tools are used to answer fundamental questions about our society.\n\nThe term was first used in August 2012 when the Population Informatics Research Group was founded at the University of North Carolina at Chapel Hill. The term was first defined in a peer reviewed article in 2013 and further elaborated on in another article in 2014. The first Workshop on Population Informatics for Big Data was held at the ACM SIGKDD conference in Sydney, Australia, in August 2015.\n\nTo study social, behavioral, economic, and health sciences using the massive data collections, aka social genome data, about people. The primary goal of population informatics is to increase the understanding of social processes by developing and applying computationally intensive techniques to the social genome data.\n\nSome of the important sub-disciplines are :\n\nRecord Linkage, the task of finding records in a dataset that refer to the same entity across different data sources, is a major activity in the population informatics field because most of the digital traces about people are fragmented in many heterogeneous databases that need to be linked before analysis can be done.\n\nOnce relevant datasets are linked, the next task is usually to develop valid meaningful measures to answer the research question. Often developing measures involves iterating between inductive and deductive approaches with the data and research question until usable measures are developed because the data were collected for other purposes with no intended use to answer the question at hand. Developing meaningful and useful measures from existing data is a major challenge in many research projects. In computation fields, these measures are often called features.\n\nFinally, with the datasets linked and required measures developed, the analytic dataset is ready for analysis. Common analysis methods include traditional hypothesis driven research as well more inductive approaches such as data science and predictive analytics.\n\nComputational social science refers to the academic sub-disciplines concerned with computational approaches to the social sciences. This means that computers are used to model, simulate, and analyze social phenomena. Fields include computational economics and computational sociology. The seminal article on computational social science is by Lazer et al. 2009 which was a summary of a workshop held at Harvard with the same title. However, the article does not define the term computational social science precisely.\n\nIn general, computational social science is a broader field and encompasses population informatics. Besides population informatics, it also includes complex simulations of social phenomena. Often complex simulation models use results from population informatics to configure with real world parameters.\n\nData Science for Social Good (DSSG) is another similar field coming about. But again, DSSG is a bigger field applying data science to any social problem that includes study of human populations but also many problems that do not use any data about people.\n\nPopulation reconstruction is the multi-disciplinary field to reconstruct specific (historical) populations by linking data from diverse sources, leading to rich novel resources for study by social scientists.\n\nThe first Workshop on Population Informatics for Big Data was held at the ACM SIGKDD conference in Sydney, Australia, in 2015. The workshop brought together computer science researchers, as well as public health practitioners and researchers. This Wikipedia page started at the workshop.\n\nThe International Population Data Linkage Network (IPDLN) facilitates communication between centres that specialize in data linkage and users of the linked data. The producers and users alike are committed to the systematic application of data linkage to produce community benefit in the population and health-related domains.\n\nThree major challenges specific to population informatics are: \n\n\n"}
{"id": "54711372", "url": "https://en.wikipedia.org/wiki?curid=54711372", "title": "Postpartum physiological changes", "text": "Postpartum physiological changes\n\nThe postpartum physiological changes are those expected changes that occur to the woman's body after childbirth, in the postpartum period. These changes mark the beginning of the return of pre-pregnancy physiology and of breastfeeding. Most of the time these postnatal changes are normal and can be managed with medication and comfort measures, but in a few situations complications may develop. Postpartum physiological changes may be different for women delivering by cesarean section. Other postpartum changes, may indicate developing complications such as, postpartum bleeding, engorged breasts, postpartum infections.\n\nThe physiological changes that occurred earlier in the pregnancy that prepared the breast for lactation become altered after the birth. About 2 to 5 days after the birth the breasts begin to produce milk. This sometimes is described as \"the milk coming in\". Information can be provided to the mother before the birth to enhance the understanding of breastfeeding and the support that will be available to make it successful. The mother is encouraged to breastfeed and spend time bonding with her infant immediately after the birth. Sucking causes the pituitary to release oxytocin that to contract the uterus and prevent bleeding. Progesterone is the hormone that influences the growth of breast tissue before the birth. The postpartum changes that occur in the endocrine system after the birth shift from hormones that prevent lactation to hormones that trigger milk production. This can be felt by the mother in the breasts. The crying of the infant can induce the release of oxytocin from the pituitary gland. Cracked nipples can develop at this time and is managed with pharmacological and nonpharmacological treatment.\n\nThe most drastic change in the uterus is the contraction from an organ weighing one kilogram and a volume of about 10-L down to a 60 gram organ that only contains 5ml of fluid. Immediately after delivery, the fundus of the uterus begins to contract. This is to deliver the placenta which can take up to 30 minutes and may involve a feeling of chills. In a normal and healthy response it should feel firm and not boggy. It begins to involute with contractions of the smooth muscle of the uterus. It will contract midline with the umbilicus. It begins its contractions and by twelve hours after the birth it can be felt to be at the level of the umbilicus. The uterus changes in size from one kilogram to 60 -80 grams in the space of six weeks. After birth, the fundus contracts downward into the pelvis one centimeter each day. After two weeks the uterus will have contracted and return into the pelvis. The sensation and strength of postpartum uterine contractions can be stronger in women who have previously delivered a child or children.\n\nThe cervix remains soft after birth. The vagina contracts and begins to return to the size before pregnancy. For four to six weeks of the postpartum period the vagina will discharge lochia, a discharge containing blood, mucus, and uterine tissue.\n\nDuring pregnancy the normal inflammatory immune response is shut down in order to allow the acceptance of the embryo to the body. In the postpartum period this needs to be quickly reversed back to normal. This immune reconstitution can result in the symptomatic expression of infections that were present but previously not responded to, especially infections with an autoimmune basis.\n\nEducation and discussion before the birth can alleviate some of the fear of the unknown and the anxiety when treatments are experienced for the first time. Providing continuous updates on the status of the infant is beneficial.\n\nPerineal pain after childbirth has immediate and long-term negative effects for women and their babies. These effects can interfere with breastfeeding and the care of the infant. The pain from injection sites and possible episiotomy is managed by the frequent assessment of the report of pain from the mother. Pain can come from possible lacerations, incisions, uterine contractions and sore nipples. Appropriate medications are usually administered. Routine episiotomies have not been found to reduce the level of pain after the birth. Comfort is enhanced with changing linens, urination, the cleaning of the perineum and ice packs. Privacy also in implemented to promote comfort.\n\nHemorrhoid pain can be managed with a variety of methods. Some recommendations for reducing the pain of hemorrhoids include: cleansing with warm water, hemorrhoid creams, increasing fluids, lying on the site and sitz baths.\n\nMedications controlling pain will begin to wear off. This also true when an epidural or spinal block was given. Uterine contractions are sometimes painful and comfort can be promoted by suggesting that different positions can be tried. Walking around, with assistance, can decrease pain. Since uterine cramping may become more painful during breastfeeding, medications can be given a half an hour before nursing. Pain control and comfort can be managed by anticipating the return of sensation and bodily reactions to bruises, tears, incisions and puncture sites.\n\nImmediately after the birth, on going assessments are performed with recommendations from the American Academy of Pediatrics and American College of Obstetricians and Gynecologists. They have identified that vital signs of blood pressure, and pulse, uterine position, and bleeding should be assessed every 15 minutes for the first two hours after birth. The temperature is then measured twice, four hours and eight hours after birth.\n\nThe caloric needs will changed based upon the production of milk for the infant. The caloric requirement for a non-breastfeeding, non-pregnant woman changes from 1,000-2,000 kcal/day to 2,300 to 2500 kcal/day for the breastfeeding woman. Nutritional supplementation is often prescribed and recommended. In some instances women are encouraged to continue to take pre-natal vitamins. Increasing the intake of fluids is discussed. The need for additional levels of minerals are mostly likely due to lactation. Calcium and iron needs increase postpartum. Calories may need to increase by 333 kcal/day during the first four to six weeks postpartum and then by 400 kcal/day 6 months postpartum.\n\nOther foods or substances are not recommended postpartum if breastfeeding because they may have affects on the baby via breastmilk. Some clinicians discourage the use of caffeine. This could produce fussiness in the baby. Alcohol use is strongly discouraged. Consuming fish is healthy and provides vitamins, minerals and proteins. Consumption of oily fish like haddock, herring, sardines, grouper,and tuna may need to be limited due to pollutants.\n\nWeight loss should be monitored to ensure recovery. Quick weight loss can reduce milk supply. Low carb and high protein diets may not be appropriate. A realistic weight loss goal is one pound per week.\n\nA urinary catheter is usually put in place before the cesarean section to prevent urinary retention. The abdominal incision will be another site for pain and possible infection. Moving out of bed may be delayed. As with any surgical procedure, the risk is higher for the development of blood clots in the legs. In these cases intermittent a pneumatic pressure device may be used or more simple compression stockings. Leg exercise will also be effective in promoting blood circulation in the legs. Higher levels of pain medication may be needed related to abdominal incisions. If the cesarean was not planned, some women will be disappointed and may benefit from encouraging counsel from clinicians.\n\n"}
{"id": "35709604", "url": "https://en.wikipedia.org/wiki?curid=35709604", "title": "Public health nursing", "text": "Public health nursing\n\nPublic health nursing, a term coined by Lillian Wald of the Henry Street Settlement, or community health nursing, is a nursing specialty focused on public health. Public health nurses (PHNs) or community health nurses \"integrate community involvement and knowledge about the entire population with personal, clinical understandings of the health and illness experiences of individuals and families within the population.\" Public health nursing in the United States traces back to a nurse named Lillian Wald who, in 1893, established the Henry Street Settlement in New York City and coined the expression \"public health nurse\".\n\nPublic health nurses work within communities and focus on different areas to improve the overall health of the people within that community. Some areas of employment for public health nurses are school districts, county or state health departments, and departments of correction. The public health nurse looks for areas of concern within the community and assesses and plans ways through which the concerns can be resolved or minimized. Some health concerns a public health nurse may work on are infection control, health maintenance, health coaching, as well as home care visits for welfare and to provide care to certain members of the community who may need it.\n\n"}
{"id": "5953219", "url": "https://en.wikipedia.org/wiki?curid=5953219", "title": "Refeeding syndrome", "text": "Refeeding syndrome\n\nRefeeding syndrome is a syndrome consisting of metabolic disturbances that occur as a result of reinstitution of nutrition to patients who are starved, severely malnourished or metabolically stressed due to severe illness. When too much food and/or liquid nutrition supplement is consumed during the initial four to seven days of refeeding, this triggers synthesis of glycogen, fat and protein in cells, to the detriment of serum (blood) concentrations of potassium, magnesium, and phosphorus. Cardiac, pulmonary and neurological symptoms can be signs of refeeding syndrome. The low serum minerals, if severe enough, can be fatal.\n\nAny individual who has had negligible nutrient intake for many consecutive days and/or is metabolically stressed from a critical illness or major surgery is at risk of refeeding syndrome. Refeeding syndrome usually occurs within four days of starting to re-feed. Patients can develop fluid and electrolyte disorders, especially hypophosphatemia, along with neurologic, pulmonary, cardiac, neuromuscular, and hematologic complications.\n\nDuring fasting the body switches its main fuel source from carbohydrates to fat tissue fatty acids and amino acids as the main energy sources. The spleen decreases its rate of red blood cell breakdown thus conserving red blood cells. Many intracellular minerals become severely depleted during this period, although serum levels remain normal. Importantly, insulin secretion is suppressed in this fasted state and glucagon secretion is increased.\n\nDuring refeeding, insulin secretion resumes in response to increased blood sugar, resulting in increased glycogen, fat and protein synthesis. This process requires phosphates, magnesium and potassium which are already depleted and the stores rapidly become used up. Formation of phosphorylated carbohydrate compounds in the liver and skeletal muscle depletes intracellular ATP and 2,3-diphosphoglycerate in red blood cells, leading to cellular dysfunction and inadequate oxygen delivery to the body's organs. Refeeding increases the basal metabolic rate. Intracellular movement of electrolytes occurs along with a fall in the serum electrolytes, including phosphorus and magnesium. Levels of serum glucose may rise and the B vitamin thiamine may fall. Abnormal heart rhythms are the most common cause of death from refeeding syndrome, with other significant risks including confusion, coma and convulsions and cardiac failure.\n\nThis syndrome can occur at the beginning of treatment for anorexia nervosa when patients have an increase in calorie intake and can be fatal. It can also occur after the onset of a severe illness or major surgery. The shifting of electrolytes and fluid balance increases cardiac workload and heart rate. This can lead to acute heart failure. Oxygen consumption is also increased which strains the respiratory system and can make weaning from ventilation more difficult.\n\nRefeeding syndrome can be fatal if not recognized and treated properly. An awareness of the condition and a high index of suspicion are required in order to make the diagnosis. The electrolyte disturbances of the refeeding syndrome can occur within the first few days of refeeding. Close monitoring of blood biochemistry is therefore necessary in the early refeeding period. \n\nIn critically ill patients admitted to an intensive care unit, if phosphate drops to below 0.65 mmol/L (2.0 mg/dL) from a previously normal level within three days of starting enteral or parenteral nutrition, caloric intake should be reduced to 480 kcals per day for at least two days whilst electrolytes are replaced. Prescribing thiamine, vitamin B complex (strong) and a multivitamin and mineral preparation is recommended. Biochemistry should be monitored regularly until it is stable. Although clinical trials are lacking in patients other than those admitted to an intensive care, it is commonly recommended that energy intake should remain lower than that normally required for the first 3–5 days of treatment of refeeding syndrome.\n\nSee NICE Clinical guideline CG32, section 6.6. On first aid and preliminary medical management, see for example the guidance by HMS Monmouth medical officer.\nA common error, repeated in multiple papers, is that \"The syndrome was first \ndescribed after World War II in Americans who, held by the Japanese as prisoners of war, had become malnourished during captivity and who were then released to the care of United States personnel in the Philippines.\" However, closer inspection of the 1951 paper by Schnitker reveals the prisoners under study were not American POWs but Japanese soldiers who, already malnourished, surrendered in the Philippines during 1945, after the war was over. \n\nIt is difficult to ascertain when the syndrome was first discovered and named, but it is likely the associated electrolyte disturbances were identified well before 1951, perhaps in Holland during the closing months of World War II, before Victory Day in Europe. There are also anecdotal eyewitness reports from a still earlier time of Polish prisoners in Iran who were freed from Soviet camps in 1941–1942 under an amnesty to form an army under General Anders and were given food whilst in a state of starvation which caused many to die.\n\nThe Roman Historian Flavius Josephus writing in first century the described classic symptoms of the syndrome among survivors of the siege of Jerusalem. He described the death of those who over indulged in food after famine, whereas those who ate a more restrained pace survived.\n\nIn his 5th century BCE work 'On Fleshes' (De Carnibus), Hippocrates writes, \"if a person goes\nseven days without eating or drinking anything, in this period most die; but there are some who survive that time\nbut still die, and others are persuaded not to starve themselves to death but to eat and drink: however, the cavity\nno longer admits anything because the jejunum (nêstis) has grown together in that many days, and these people\ntoo die.\" Though Hippocrates misidentifies the exact cause of death, this passage likely represents an early description of Refeeding Syndrome.\n\n"}
{"id": "267488", "url": "https://en.wikipedia.org/wiki?curid=267488", "title": "Reference Daily Intake", "text": "Reference Daily Intake\n\nThe Reference Daily Intake (RDI) is the daily intake level of a nutrient that is considered to be sufficient to meet the requirements of 97–98% of healthy individuals in every demographic in the United States. While developed for the US population, it has been adopted by other countries, though not universally.\n\nThe RDI is used to determine the Daily Value (DV) of foods, which is printed on nutrition facts labels (as % DV) in the United States and Canada, and is regulated by the Food and Drug Administration (FDA) and Health Canada. The labels \"high\", \"rich in\", or \"excellent source of\" may be used for a food if it contains 20% or more of the RDI. The labels \"good source\", \"contains\", or \"provides\" may be used on a food if it contains between 10% and 20% of the RDI. \n\nThe Recommended Dietary Allowances (RDAs) were a set of nutrition recommendations that evolved into both the Dietary Reference Intake (DRI) system of nutrition recommendations (which still defines RDA values) and the RDIs used for food labelling. The first regulations governing U.S. nutrition labels specified a \"% U.S. RDA\" declaration based on the current RDA values, which had been published in 1968. Later, the % U.S. RDA was renamed the %DV and the RDA values that the %DVs were based on became the RDIs.\n\nThe RDAs (and later the RDA values within the DRI) were regularly revised to reflect the latest scientific information, but although the nutrition labeling regulations were occasionally updated, the existing RDI values were not changed, so that until 2016 many of the DVs used on nutrition facts labels were still based on the outdated RDAs from 1968. In 2016, the Food and Drug Administration published changes to the regulations including updated RDIs and DVs based primarily on the RDAs in the current DRI.\n\nDaily Values used by the FDA for the following macronutrients are Daily Reference Values.\n\nFDA issued a Final Rule on changes to facts panel on May 27, 2016. The new values were published in the Federal Register. New values can be used on labels now. The original deadline to be in compliance was July 28, 2018, but on September 29, 2017 the FDA released a proposed rule that extended the deadline to January 1, 2020 for large companies and January 1, 2021 for small companies. In the interim, products with old or new facts panel content will be on market shelves at same time.\n\nThe following table lists the old and new DVs based on a caloric intake of 2000 kcal (8400 kJ), for adults and children four or more years of age.\n\nFor vitamins and minerals, the old RDIs and new RDIs (old and new adult 100% Daily Values) are given in the following table, along with the more recent RDAs or AIs of the Dietary Reference Intakes (maximized over sex and age groups, excluding women who are pregnant or lactating):\n\nThe RDI is derived from the RDAs, which were first developed during World War II by Lydia J. Roberts, Hazel Stiebeling and Helen S. Mitchell, all part of a committee established by the U.S. National Academy of Sciences to investigate issues of nutrition that might \"affect national defense\" (Nestle, 35). The committee was renamed the Food and Nutrition Board in 1941, after which they began to deliberate on a set of recommendations of a standard daily allowance for each type of nutrient. The standards would be used for nutrition recommendations for the armed forces, for civilians, and for overseas population who might need food relief. Roberts, Stiebeling, and Mitchell surveyed all available data, created a tentative set of allowances for \"energy and eight nutrients\", and submitted them to experts for review (Nestle, 35). The final set of guidelines, called RDAs for Recommended Dietary Allowances, were accepted in 1941. The allowances were meant to provide superior nutrition for civilians and military personnel, so they included a \"margin of safety\". Because of food rationing during the war, the food guides created by government agencies to direct citizens' nutritional intake also took food availability into account.\n\nThe Food and Nutrition Board subsequently revised the RDAs every five to ten years. In 1973, the FDA introduced regulations to specify the format of nutrition labels when present, although the inclusion of such labels was largely voluntary, only being required if nutrition claims were made or if nutritional supplements were added to the food. The nutrition labels were to include percent U.S. RDA based on the 1968 RDAs in effect at the time. The RDAs continued to be updated (in 1974, 1980 and 1989) but the values specified for nutrition labelling remained unchanged.\n\nIn 1993 the FDA published new regulations mandating the inclusion of a nutrition facts label on most packaged foods. Originally the FDA had proposed replacing the percent U.S. RDAs with percent daily values based on the 1989 RDAs but the Dietary Supplement Act of 1992 prevented it from doing so. Instead it introduced the RDI to be the basis of the new daily values. The RDI consisted of the existing U.S. RDA values (still based on the 1968 RDAs as the FDA was not allowed to change them at the time) and new values for additional nutrients not included in the 1968 RDAs.\n\nIn 1997, at the suggestion of the Institute of Medicine of the National Academy, the RDAs became one part of a broader set of dietary guidelines called the Dietary Reference Intake used by both the United States and Canada. As part of the DRI, the RDAs continued to be updated.\n\nOn May 27, 2016, the FDA updated the regulations to change the RDI and Daily Values to reflect current scientific information. Until this time, the Daily Values were still largely based on the 1968 RDAs. The new regulations make several other changes to the nutrition facts label to facilitate consumer understanding of the calorie and nutrient contents of their foods, emphasizing nutrients of current concern, such as vitamin D and potassium. The revision to the regulations came into effect on 26 July 2016 and initially stipulated that larger manufacturers must comply within two years while smaller manufacturers had an additional year. On September 29, 2017 the FDA released a proposed rule that extended the deadline to January 1, 2020 for large companies and January 1, 2021 for small companies.\n\nIn 2010, the U.S. Institute of Medicine determined that the government should establish new consumption standards for salt to reduce the amount of sodium in the typical American diet below levels associated with higher risk of several cardiovascular diseases, yet maintain consumer preferences for salt-flavored food. The daily maximum for sodium in the United States had been above estimated minimums for decades. For instance, the National Research Council found that 500 milligrams of sodium per day (approximately 1,250 milligrams of table salt) is a safe minimum level. In the United Kingdom, the daily allowance for salt is 6 g (approximately 2.5 teaspoons, about the upper limit in the U.S.), an amount considered \"too high\".\n\nThe Institute of Medicine advisory stated (daily intake basis): \"Americans consume more than 3,400 milligrams of sodium – the amount in about 1.5 teaspoons of salt (8.7 g) – each day. The recommended maximum daily intake of sodium – the amount above which health problems appear – is 2,300 milligrams per day for adults, about 1 teaspoon of salt (5.9 g). The recommended adequate intake of sodium is 1,500 milligrams (3.9 g salt) per day, and people over 50 need even less.\"\n\n\n"}
{"id": "50066385", "url": "https://en.wikipedia.org/wiki?curid=50066385", "title": "Reproductive health care for incarcerated women in the United States", "text": "Reproductive health care for incarcerated women in the United States\n\nIn the United States, prisons are obligated to provide health care to prisoners. Such health care is sometimes called \"correctional medicine\". In women's prisons, correctional medicine includes attention to reproductive health.\n\nThe number of women incarcerated in the United States has increased greatly over the last few decades, and at a faster rate than the number of incarcerated men. Many of the same factors that increase women's likelihood of incarceration also put them at a higher risk for contracting HIV/AIDS and other sexually transmitted infections, and for having high-risk pregnancies. The majority of incarcerated women are economically disadvantaged and poorly educated, and have not had adequate access to preventive healthcare prior to their imprisonment, such as Pap tests, STI screening, and pregnancy counseling.\n\nEstelle v. Gamble obligates prisons to provide for the serious medical needs of their inmates. However, it also requires that an incarcerated person must demonstrate that they had a serious medical need, and that they didn't receive adequate medical care because officials showed \"deliberate indifference.\" Due to the difficulty inherent in proving that a prison official knew about a medical condition, yet failed to respond to it, this standard makes it difficult to hold correctional facilities accountable for their mistakes.\n\nTodaro v. Ward argued that women within a New York prison did not have adequate, constitutional access to healthcare. Since Todaro v. Ward was the first major court case that called into question incarcerated women's actual access to health care, it spurred organizations such as the American Medical Association, American Correctional Association, and the American Public Health Association to start creating standards for health care within prisons.\n\nWith this ruling, the court determined that the medical care provided to prisoners must only be \"reasonable,\" not necessarily \"perfect, the best obtainable, or even very good.\"\n\nCurrently, no national agency monitors the treatment of prisoners, although a few governmental and non-governmental agencies do provide monitoring standards that facilities may use if they wish to become accredited (about thirty percent of prisons are accredited), and some federal legislation has been passed regarding correctional health care.\n\nThis legislation required that incarcerated individuals pay for some of their health care bill while in prison.\n\nThe Prison Litigation Reform Act made it much more difficult for prisoners to file class action and individual lawsuits against a prison. The Prison Litigation Reform Act (PRLA) requires that prisoners exhaust a facility's own administrative resources and solutions before attempting to file a lawsuit. This can be detrimental because prisoners' health problems are often time-sensitive. PRLA also includes a cap on attorney fees, which has made fewer attorneys willing to represent prisoners. Although PRLA applies to all lawsuits filed by incarcerated people, it is particularly relevant for women's health because most lawsuits filed by incarcerated women concern substandard health care.\n\nAs of 2005, about five to ten percent of incarcerated women were pregnant (most upon intake), and about 2,000 incarcerated women give birth every year. Incarcerated women already tend to have more high-risk pregnancies, due to possible complications by drug and alcohol abuse and STIs, histories of victimization and abuse, and poor support networks, so they are particularly in need of quality prenatal care as compared to non-incarcerated women.\n\nStudies have found mixed results when it comes to the effects of incarceration on pregnancy outcomes. Some studies have found that incarceration correlates to lower birth weight, and an increased chance of complications in the pregnancy; others have found the opposite: that incarceration is associated with higher birth weight and a decreased likelihood of premature delivery. However, the latter finding may simply speak to the low level of medical care available to women in poverty in the free world.\n\nA 1996 study conducted by the National Council on Crime and Delinquency (NCCD) on women in California, Connecticut, and Florida state prisons found a lack of adequate prenatal and postnatal medical care, prenatal nutrition, level of methadone maintenance for opiate-dependent pregnant inmates, education regarding childbirth and parenting, and preparation for the mother's separation from her child. Often, inmates report receiving no regular pelvic exams or sonograms, and little to no information about proper prenatal care and nutrition. Prison diets often lack proper nutrition, particularly for the changing and special dietary needs of pregnant women, and many women prisoners report not being allowed to alter their diets during their pregnancy.\n\nDespite the lack or low quality of prenatal care in most institutions, many correctional facilities have made strides to provide adequate care. For example, Washington State has a program called the Birth Attendants Prison Doula project, which provides support to incarcerated pregnant and postpartum women. When asked about her prison's response to pregnancy, Boo, an inmate in an Arizona correctional facility, said:\n\nIn general, pregnant inmates are transported to outside medical facilities to give birth, because most correctional facilities are not medically equipped to provide such services. These transports often result in complications due to the risk of injury to both the mother and the child, and due to the additional stress.\n\nThe practice of shackling inmates, both in transit to the hospital and during labor, is also common at many facilities. Forty-one states permit the use of restraints during transport to the hospital, and twenty-three states and the federal government permit the use of restraints during labor. These restraints may include belly chains, shackles, handcuffs, or nylon \"soft restraints.\" The use of restraints on women so far into their pregnancy and during labor poses many health risks. Restraints inhibit the movement of a woman, something that aids the progression of the labor and alleviates some of her discomfort. They may also hinder the ability of health care professionals to respond quickly to emergencies during the labor. Organizations such as Amnesty International have pressured correctional facilities to stop the use of shackles on pregnant women, pointing out the fact that most pregnant women are incarcerated for nonviolent offenses and pose no risk (especially during childbirth), so the restraints are unnecessary. Maria Jones, a pregnant inmate, described her experience being shackled during labor:\n\nIncarcerated women are 15 times more likely to be infected with HIV than are women who are free, and are also more than twice as likely than incarcerated men to be infected with HIV. For example, in New York, a state which does blind testing, 14.6% of incarcerated women and 7.3% of incarcerated men tested positive for HIV. The 1996 NCCD study found that female African American and Hispanic/Latina inmates were significantly more likely than their White counterparts to report testing positive for HIV. Many of the same social factors that increase the likelihood of incarceration for women, such as poverty, race, gender, and a history of victimization, are also correlated to HIV infection.\n\nA 2000 study by the American Correctional Association found that mandatory HIV testing is conducted upon intake in 23 states, and that a few also provide 6-month follow-up testing. Most prisons test inmates who either ask to be tested, or display symptoms of HIV (this is the policy in 44 out of 51 jurisdictions), and fifteen states also specifically test inmates that are in high-risk groups. Three states, the District of Columbia, and the Federal Bureau of Prisons test inmates upon release. The same American Correctional Association study also found that most prisons provide HIV-positive individuals with medications while they are in the prison, may provide them with a small supply after they are released, and may also direct them towards community resources where they can receive more medication. Treatment of inmates who have tested positive for HIV varies greatly from state to state. For example, in New Jersey, female HIV positive prisoners were shackled to their beds for up to six months after being diagnosed. However, Ohio and New York, among other states, have infirmaries specifically adapted to attend to the needs of inmates with HIV/AIDS, and allow some inmates to reside in a hospital for treatment. In addition to this, New York State's Division of Health Services department does regular evaluations of the state prisons' services for AIDS patients, and also provides space for support groups to meet, and patients to be counseled on their illness.\n\nWomen's symptoms and treatment needs for HIV infection are quite different from those of men, but the treatment resources for incarcerated women are often limited. The treatment of HIV requires a specialist, and generally, prison doctors don't have adequate training to treat women effectively. Or, prisons often do not have adequate facilities, staff, or a follow-up treatment system. For example, the 1996 NCCD study also found that within the entire California women's prison system, one of the largest in the nation, there was only one full-time specialist able to provide \"gender-specific treatment\" to women with HIV. Male prisoners, however, did have access to gender-specific HIV treatment.\n\nFederal courts have established, according to constitutional law, that prisoners retain the right to have an abortion once they are incarcerated. However, state standards regarding abortion for incarcerated women are unclear - actions are often left up to the discretion of prison officials on a case-by-case basis.\n\nMost state abortion policies are written and approved without going through the administrative process which other policies are generally subject to; thus, they are frequently incomplete. In fact, fourteen state DOCs have no official written abortion policies, and others simply will not release or publish their policies. Alaska has a provision which bans abortion funding in its policy on general medical services, but no specific policy guidelines on abortion. Other states may not define the exact type of abortions they pay for. Most states require that women pay for all of the costs of an abortion procedure, including transportation to the clinic (which is often far due to prisons' rural locations, and the urban locations of most abortion clinics), security, and the actual surgery. Minnesota and Wisconsin are the only two states which explicitly mention in their policies that they pay for abortions if a woman has been raped. The policy of the Federal Bureau of Prisons since 1987 has been to pay for abortion only in the cases of rape or life endangerment (although they do pay for transportation to the clinic). Women's experiences and perceptions of their own access to abortion reflect these policies. In a nationwide survey of correctional health care and inmates' access to abortion, 68% of respondents said that women within their prison were allowed to have an abortion if they requested one. However, many respondents also stated that although their prison did allow access to abortion, women receive little to no help with arranging an appointment, paying for the procedure, and getting themselves to the clinic.\n\nAccording to the U.S. Department of Justice Bureau of Justice Statistics, cancer is one of the leading causes of death for women (both inside and outside prison), and one-quarter of deaths of incarcerated women due to cancer are from breast, cervical, ovarian, and uterine cancer. In particular, women in prison are at a high risk for contracting cervical cancer due to their high rates of substance abuse and diseases such as hepatitis C, HIV, and other STIs. However, reproductive health care reform in prisons generally focuses on pregnancy or HIV.\n\nIncarcerated women have higher rates of STIs and gynecologic infections than non-incarcerated women. In fact, one study estimated that about 9% of incarcerated women tested positive for gonococcus infection, and between 11 and 17% of incarcerated women are infected with chlamydia. However, not every infected woman will be diagnosed and given treatment, because many prisons only test women who request to be tested, or show symptoms.\n\nStates often have their own legislation regarding correctional health care, but it doesn't always completely take into consideration the complexities of women's reproductive health. For example, the New York State Department of Corrections and Community Supervision (DOCCS) has issued two main documents which outline their institutional health care policies: the Health Services Policy Manual, and the Women's Health Primary Care Practice Guideline (first published in 2000, and updated in 2008 and 2011). Within these documents, the Patient Bill of Rights includes the right of a patient to respectful care, the right to refuse treatment, and the right to complete information regarding a diagnosis; the Professional Code of Ethics outlines the standards that staff must follow, including having a respect for human dignity, and a professional relationship with the patient. The DOCCS does not monitor how well facilities actually adhere to these standards, and there are no consequences for those who do not. Also, many women are not informed of their patient's rights, so are unaware when standards have been breached.\n\nThe DOCCS has no written policies on pregnancy tests, pregnancy options counseling, abortion, ectopic pregnancy, miscarriage, stillbirth, nutrition for pregnant and nursing women, or hysterectomies, and has incomplete policies regarding menopause, vitamins, health care for pregnant women, women in labor, or women who have recently given birth. The DOCCS policies stray from community standards in the areas of the starting age for yearly GYN check-ups, the frequency of breast exams and Pap smears, follow-up for abnormal Pap smears, frequency of prenatal visits and ultrasounds, and the time frame for postpartum check-ups for women who have a Caesarean section.\n\nThe Federal Bureau of Prisons' policy is to provide each inmate with a complete medical exam (which includes gynecological and obstetrical history) within 30 days of admission. The BOP currently adheres to the standards for yearly exams put forth by the American College of Obstetrics and Gynecology. According to a 1997 survey, approximately 90 percent of the inmates in women's state prisons reported having received a gynecologic exam from their institution upon intake. Additionally, an American Correctional Association study found that most women's correctional institutions provided OB/GYN services, prenatal and postpartum care, mammography, and Pap smears upon request. Fewer provided counseling for inmates regarding their reproductive health, many correctional facilities do not provide follow-up exams, and screenings often do not continue on the recommended schedule. In a New York city juvenile detention facility, the 5,000 youth who went through the system annually were all served by a single physician. Less than one-third of the inmates received a Pap smear test, and one in five were tested for gonorrhea, chlamydia, and syphilis.\n\nWomen often report long delays and wait times in signing up for appointments with a GYN doctor, and in getting treated. For example, more than half of respondents in a survey of incarcerated women in New York state prisons said that they were not able to see the GYN when necessary, and 47% of respondents stated that their problems became worse in the time that they had to wait. In one extreme case, Sara, a woman incarcerated in New York, had to wait seven months before she was finally diagnosed with an aggressive cancer. Another woman had to wait four months to receive a colposcopy, a follow-up to her Pap test. Another woman at the same prison wrote:\n\nIn a qualitative study on the experiences of incarcerated women with the Papanicolaou test (a test which screens for cervical cancer) in California state prisons, researchers found a lack of communication between medical providers within the prison, which tended to result in long delays or cancellations of treatment, no standardized process for scheduling a Pap test, and a lack of education and explanation regarding both the test itself, and how to fill out the medical forms associated with it.\n\nIn addition to this, women who aren't called in for a Pap test are sometimes required to pay a $5 fee if they request one. This can be a significant barrier to requesting a test, considering an average prisoner's wage is around 7 to 13 cents per hour.\n\nIn the same study, women brought attention to the discomfort suffered by inmates with personal histories of sexual abuse and victimization when they were forced to be examined by a male physician. Women interviewed in New York State prisons had similar complaints about their physicians' seeming lack of awareness of female prisoners' past traumas, and the lack of explanation of the procedures. One woman stated:\n\nAnother consideration in correctional reproductive health care is the relationship between the prisoner-patients and the physicians. Studies have shown that female inmates often report distrust of physicians and disappointment with their interactions with providers.\n\nResearchers have found that most of the women interviewed in the 2005 California state prison study had negative perceptions of their gynecologic tests and treatment. According to the women interviewed, the prison doctors who performed their tests were often unprofessional and disrespectful. One woman described her experience, saying: \"They expect us to give them respect, but they don't respect us. They treat us like we are animals just because we are incarcerated.\" The California prison system employs corrections officers who are also trained to be licensed nurses (called medical technical assistants). Similarly, in Chicago, the county jails train corrections officers as doulas and birth attendants. Inmates generally must request medical treatment through either these officers, or through other, non-medically trained correctional staff, and studies have shown that female patients' complaints and requests for medical assistance are often not taken as seriously as those of male patients. The simultaneous positions that these officers hold as both security personnel and medical caretakers or advocates may contribute to female inmates' distrust of them.\n\nA 1999 study found that female inmates were more likely than their male counterparts to think that their access to health care was inadequate and that their quality of care was low compared to the care male prisoners were given. Women in this study also visited the health care facility more often than their male counterparts and reported being healthy less often than men did.\n\nA lack of resources, specifically of adequate staff, within facilities is largely what contributes to the substandard care of inmates. For example, although California's prison system is the largest in the country, an obstetrician/gynecologist was not hired at the California Institution for Women (CIW) until pregnant prisoners in California filed a class action lawsuit (Harris v. McCarthy) against CIW. The Valley State Prison for Women (VSPW), another prison in California, had just two OB/GYN doctors on staff in 2000 (one of whom was indicted on four counts of sexual misconduct and eventually fired). Albion, a women's prison in New York State, has 1,000 prisoners, but only one GYN doctor, who works only 16 hours per week. Taconic, another facility in New York, which holds about 370 women, has no GYN doctor on site. In these cases, routine GYN care falls to the general nurse practitioner or Medical Director. In addition to the lack of staff, medical doctors are often not on duty on the weekends and during evening hours. This can be dangerous if medical emergencies arise during non-business hours. For example, a prisoner in CIW in 1997 went into labor over the weekend, when there was only one nurse on duty. The nurse strapped the woman into a gurney, but refused to help with her labor. When the baby was born not breathing, the nurse had to call paramedics, because she was unable to activate the breathing apparatus. By the time the newborn was transported to the hospital, he was declared brain dead.\n\nThe difficulty with recruiting qualified staff can be attributed to both the physical isolation of women's prisons (often placed in areas that are considered undesirable to live in, where there are few available medical professionals), and to the relative lack of medical resources and low salaries that prisons offer to medical staff. Dr. Valda Chijide, a former HIV doctor in an Alabama prison, for instance, resigned from her position due to inadequate support. In this case, the HIV unit was rat-infested, with broken windows covered in plastic.\n\nThere is also a trend towards privatizing health care by hiring outside, for-profit health care companies to provide medical care to inmates. According to a 1996 survey by the National Institute of Corrections (NIC), forty-four state Departments of Corrections contract out at least some of their medical care to private vendors - in 1996 this amounted to $706 million.\n\nThis can result in a lack of accountability, as proper monitoring makes a contract more expensive. Oversight is often lacking. In Alabama, the same official who previously held a high level position at Prison Health Services, one of the nation's largest private vendors, now works in a position within the Alabama Department of Corrections which requires her to oversee Prison Health's compliance to the contract. Prison Health Services, which has cared for 237,000 inmates, has had to pay millions of dollars in fines and settlements due to inadequate care. In another example, according to a former supervisory nurse of a jail operated by Correctional Medical Services (CMS), a large private contractor, the jail would often release pregnant women when they went into labor, and then arrest them again after they gave birth, in order to avoid having to pay for the inmates' medical expenses.\n\nLack of accountability also results due to varying state and federal laws on private contractors' liability for medical abuse and neglect. Private contractors can be challenged under the same federal civil rights law that applies to state and local governments, but these corporations cannot be sued for unconstitutional medical practices.\n\n"}
{"id": "23184143", "url": "https://en.wikipedia.org/wiki?curid=23184143", "title": "Sugary drink tax", "text": "Sugary drink tax\n\nA sugary drink tax or soda tax is a tax or surcharge designed to reduce consumption of drinks with added sugar. Drinks covered under a soda tax often include carbonated soft drinks, sports drinks and energy drinks.\n\nThe tax is a matter of public debate in many countries and beverage producers like Coca-Cola often oppose it. Advocates such as national medical associations and the World Health Organization promote the tax as an example of Pigovian taxation, aimed to discourage unhealthy diets and offset the growing economic costs of obesity.\n\nType II diabetes is a growing health concern in many developed and developing countries around the world, with 1.6 million deaths directly due to this disease in 2015 alone. Unlike sugar from food, the sugar from drinks enters the body so quickly that it can overload the pancreas and the liver, leading to diabetes and heart disease over time. A 2010 study said that consuming one to two sugary drinks a day increases your risk of developing diabetes by 26%.\n\nHeart disease is responsible for 31% of all global deaths and although one sugary drink has minimal effects on the heart, consuming sugary drinks daily are associated with long term consequences. A study found that men, for every added serving per day of sugar-sweetened beverages, each serving was associated with a 19% increased risk of developing heart disease. Another study also found increased risks for heart disease in women who drank sugary drinks daily.\n\nObesity is also a global public and health policy concern, with the percentage of overweight and obese people in many developed and middle income countries rising rapidly. Consumption of added sugar in sugar-sweetened beverages has been positively correlated with high calorie intake, and through it, with excess weight and obesity. The addition of one sugar-sweetened beverage per day to the normal US diet can amount to 15 pounds of weight gain over the course of 1 year. Added sugar is a common feature of many processed and convenience foods such as breakfast cereals, chocolate, ice cream, cookies, yogurts and drinks produced by retailers. The ubiquity of sugar-sweetened beverages and their appeal to younger consumers has made their consumption a subject of particular concern by public health professionals. In both the United States and the United Kingdom, sugar sweetened drinks are the top calorie source in teenager's diets.\n\nTrends indicate that traditional soda consumption is declining in many developed economies, but growing rapidly in middle income economies such as Vietnam and India. In the United States, the single biggest market for carbonated soft drinks, consumers annual average per capita purchase of soda was 154 liters.\n\nDenmark began taxing soft drinks and juices in the 1930s. More recently, Finland reintroduced an earlier soft drink tax in 2011, while Hungary taxes sugary drinks as part of its 2011 public health product tax, which covers all food products with unhealthy levels of sugar. France introduced a targeted sugar tax on soft drinks in 2012. At a national level similar measures have also been announced in Mexico in 2013 and in the United Kingdom in 2016. In November 2014, Berkeley, California was the first city in the U.S. to pass a targeted tax on surgary drinks.\n\nProponents of soda taxes cite the success of tobacco taxes worldwide when explaining why they think a soda tax will work to lower soda consumption. Where the main concern with tobacco is cancer, the main concerns with soda are diabetes and obesity. The tactics used to oppose soda taxes by soda companies mimic those of tobacco companies, including funding research that downplays the health risks of its products.\n\nThe U.S. Department of Health and Human Services reports that a targeted tax on sugar in soda could generate $14.9 billion in the first year alone. The Congressional Budget Office (CBO) estimates that three-cent-per-ounce tax would generate over $24 billion over four years. Some tax measures call for using the revenue collected to pay for relevant health needs: improving diet, increasing physical activity, obesity prevention, nutrition education, advancing healthcare reform, etc. Another area to which the revenue raised by a soda tax might go, as suggested by Mike Rayner of the United Kingdom, is to subsidize healthier foods like fruits and vegetables.\n\nThe imposition of a sugar tax means that sellers of sugary drinks would have to increase the price of their goods by an amount P2 from the original price X, and then take on the rest of the tax themselves (P1) in the form of lower profit per unit sold. The tax burden on consumers (P2) makes it more expensive for consumers to buy sugary drinks and hence a higher proportion of their incomes would have to be spent to buy the same amount of sugary drinks. This decreases the equilibrium quantity of sugary drinks that will be sold. Whether the sugary drinks tax is imposed on the seller or consumer, in both cases the tax burden is shared between both.\n\nThe way that the tax burden is divided upon the consumer and seller depends on the price elasticity for sugary drinks. The tax burden will fall more on sellers when the price elasticity of demand is greater than the price elasticity of supply while on buyers when the price elasticity of supply is greater than the price elasticity of demand. The price elasticity for sugary drinks is different from country to country. For instance, the price elasticity of demand for sugary drinks was found to be -1.37 in Chile while -1.16 in Mexico. Hence if both of those results were realistic and the price elasticity of supply would be the same for both, the tax burden on consumers would be higher in Mexico than in Chile.\n\nThe reasons for a sugar tax are the negative externalities of consuming sugar. As over-consumption of sugar causes health problems (external costs) such as obesity, type 2 diabetes and other diseases, and lost productivity, the third party impacted by this is the ‘public health system’ that will need to deal with those issues. More demand for health services leads to higher costs for health care and hence this increased stress on the public health system is a negative consumption externality of sugar consumption.\n\nIn economics terms, the marginal social benefit (MSB) of sugar consumption is less than the marginal private benefit (MB). This can also be illustrated in the following equation. MSB = MB – Marginal External Cost (MXC). This is the case due to the fact that consumers think only of the benefit of sugar consumption to them (MB) and not the negative externalities to third parties (MXC) and so want to consume at the unregulated market equilibrium to maximize their utility. This means that there is overconsumption of sugar and a welfare loss is created.\n\nThe sugary drinks tax, a Pigovian tax, is a way to correct the negative externality by regulating the consumption of sugary drinks. Without a sugary drink tax, taxpayer money is used to pay for higher health care costs incurred from high consumption of sugar. Although this solution corrects the negative consumption externality, taxpayers that consume sugary drinks moderately and hence do not contribute to higher health care costs, still need to pay for this negative externality. Hence a sugary drinks tax may be a more appropriate solution as tax revenue that is collected from the sugar tax can be used to create childhood nutrition programs or obesity-prevention programs. This is a solution that could also correct the negative externality of sugar consumption as well as is a way to make the parties that cause the negative externality pay their fair share.\n\nThe Australian Beverages Council announced in June 2018 that the industry would cut sugar content by 10% by 2020, and by another 10% by 2025. This was seen as an attempt to stave off a sugar tax. There were no plans to reduce the sugar content in the high sugar drinks. The plan is primarily to increase consumption of low-sugar or no-sugar drinks. sales of Coca-Cola Amatil's fizzy drinks have fallen 8.1% by volume from 2016 to 2018. The Australian Medical Association continued to press for a sugar tax.\n\nA 2016 proposal for a 20% sugary drink tax, campaigned by Educar Consumidores, was turned down by the Colombian legislature despite popular support for it. Soda is often less expensive than bottled water in Colombia.\n\nDenmark instituted a soft drink tax in the 1930s (it amounted to 1.64 Danish krone per liter), but announced in 2013 that they were going to abolish it along with an equally unpopular fat tax, with the goal of creating jobs and helping the local economy. Critics claimed that the taxes were notably ineffective; to avoid the fat and sugar taxes, local retailers had complained that Danes simply went to Sweden and Germany, where prices were lower to buy butter, ice cream and soda. Denmark repealed the fat tax in January 2013 and repealed the tax on soft drinks in 2014.\n\nFrance first introduced a targeted tax on sugary drinks at a national level in 2012; following introduction, soft drinks are estimated to be up to 3.5% more expensive. Analysis by the market research firm Canadean found that sales of soft drinks declined in the year following the introduction of the tax, following several years of annual growth. However, the tax applies to both drinks with added sugars and drinks with artificial sweeteners, possibly limiting its effects on the healthfulness of soda products.\n\nA 2016 study by Mazzochi has shown that the sugary drinks tax saw a 19 euro-cent per liter increase in price of non-pure fruit juices, a 16 euro-cent per liter increase for diet sodas and little impact on regular soft drinks prices. The study also estimated that the quantity consumed of the taxed drinks has decreased by 9 centiliters per week per person after the tax has been implemented.\n\nHungary's tax, which came into effect in September 2011, is a 4-cent tax on foods and drinks that contain large quantities of sugar and salt, such as soft drinks, confectionery, salty snacks, condiments, and fruit jams. In 2016, the tax has resulted in a 22% reduction in energy drink consumption and 19% of people reduced their intake of sugary soft drinks.\n\nSoda tax introduced on May 1st 2018. The tax will see 30 cent per litre added to the price of popular sweetened drinks containing more than 8g of sugar per 100ml.\n\nIn September 2013, Mexico's president Enrique Peña Nieto, on his fiscal bill package, proposed a 10% tax on all soft drinks, especially carbonated drinks, with the intention of reducing the number of patients with diabetes and other cardiovascular diseases in Mexico, which has one of the world's highest rates of obesity. According to Mexican government data, in 2011, the treatment for each patient with diabetes cost the Mexican public health care system (the largest of Latin America) around 708 USD per year, with a total cost of 778,427,475 USD in 2010, and with each patient paying only 30 MXN (around 2.31 USD).\n\nIn September 2013, soda companies launched a media campaign to discourage the Mexican Chamber of Deputies and Senate from approving the 10% soda tax. They argued that such measure would not help reduce the obesity in Mexico and would leave hundreds of Mexicans working in the sugar cane industry jobless. They also publicly accused New York City Mayor Michael Bloomberg of orchestrating the controversial bill from overseas. In late October 2013, the Mexican Senate approved a 1 MXN per litre tax (around 0.08 USD) on sodas, along with a 5% tax on junk food.\n\nResearch has shown that Mexico's sugary drinks tax reduced soft drink consumption. According to a 2016 study published in \"BMJ\", annual sales of sodas in Mexico declined 6% in 2014 after the introduction of the soda tax. Monthly sales figures for December 2014 were down 12% on the previous two years. Households with the fewest resources had an average reduction in purchases of 9% in 2014, increasing to 17% by December. Furthermore, purchases of water and non-taxed beverages increased by about 4% on average. Whether the imposition of the tax and the resulting 6% decline in sales of soft drinks will have any measurable impact on long-term obesity or diabetes trends in Mexico has yet to be determined. The authors of the study urged the Mexican authorities to double the tax to further reduce consumption.\n\nA 2016 study published in \"PLoS Medicine\" suggested that a 10% excise tax on soda \"could prevent 189,300 new cases of Type 2 diabetes, 20,400 strokes and heart attacks, and 18,900 deaths among adults 35 to 94 years old\" over a ten-year period. The study also included that \"the reductions in diabetes alone could yield savings in projected healthcare costs of $983 million.\"\n\nA 2017 study in the \"Journal of Nutrition\" found a 6.3% reduction in soft drink consumption, with the greatest reductions \"among lower-income households, residents living in urban areas, and households with children. We also found a 16.2% increase in water purchases that was higher in low- and middle-income households, in urban areas, and among households with adults only.\"\n\nNorway has had a generalized sugar tax measure on refined sugar products since 1922, introduced to boost state income rather than reducing sugar consumption. Non-alcoholic beverages have since been separated from the general tax, and in 2017, the tax for sugary drinks was set to 3.34 kroner per litre.\n\nIn January 2018, the Norwegian government increased the sugar tax level by 83% for general sugar-containing ready-to-eat products, and 42% for beverages. The sugar tax per litre was bumped up to 4.75 kroner, and applies to beverages which are either naturally or artificially sweetened.\n\nIn the taxation reform law dubbed as the Tax Reform for Acceleration and Inclusion Law (TRAIN) signed by Philippine President Rodrigo Duterte in December 2017. It includes taxation on sugar-sweetened drinks which will be implemented the following year, as an effort to increase revenue and to fight obesity. Drinks with caloric and non-caloric sweeteners will be taxed ₱6.00 per liter, while those using high-fructose corn syrup, a cheap sugar substitute, will be taxed at ₱12 per liter.\n\nExempted from the sugar tax are all kinds of milk, whether powdered or in liquid form, ground and 3-in-1 coffee packs, and 100-percent natural fruit and vegetable juices, meal replacements and medically indicated drinks, as well as beverages sweetened with stevia or coco sugar. These drinks, especially 3-in-1 coffee drinks which are popular especially among lower-income families, are to be taxed as initially proposed by the House of Representatives version of the bill, but were exempted in the Senate version.\n\nSouth Africa proposed a sugar-sweetened beverages tax in the 2016 South African national government budget. South Africa introduced a sugar tax on 1 April 2018. The levy was fixed at 2.1 cents per gram of sugar, for each gram above 4g per 100ml of sweetened beverage. The levy excludes fruit juices.\n\nOn October 2017, the United Arab Emirates introduced a 50% tax on soft drinks and a 100% tax on energy drinks, to curb unhealthy consumption of sugary drinks that can lead to diabetes; it also added a 100% tax on cigarettes.\n\nIn the 2016 United Kingdom budget, the UK Government announced the introduction of a sugar tax, officially named the \"Soft Drinks Industry Levy\". The tax came into effect on 6 April 2018. Beverage manufacturers are taxed according to the volume of sugar-sweetened beverages they produce or import. The tax is imposed at the point of production or importation, in two bands. Drinks with total sugar content above 5g per 100 millilitres are taxed at 18p per litre and drinks above 8g per 100 millilitres at 24p per litre. The measure is estimated to generate an additional £1 billion a year in tax revenue which will be spent on funding for sport in UK schools.\n\nIt was proposed that pure fruit juices, milk-based drinks and the smallest producers would not be taxed. For other beverages there was an expectation that some manufacturers would reduce sugar content in order to avoid the taxation. Indeed, manufacturer A.G. Barr significantly cut sugar content in their primary product Irn-Bru in advance of the tax.\n\nNotable research on effect of excess sugar in modern diets in the United Kingdom includes the work of Professor John Yudkin with his book called, \"Pure, White and Deadly: The Problem of Sugar\" first published in 1972. With regard to a proposed tax on sugar-sweetened beverages, a study published in the British Medical Journal on 31 October 2013, postulated that a 20% tax on sugar-sweetened beverages would reduce obesity in the United Kingdom rates by about 1.3%, and concluded that taxing sugar-sweetened beverages was \"a promising population measure to target population obesity, particularly among younger adults.\"\n\nThe tax has been criticised on several grounds, including its likely efficacy and its narrow base. UK Member of Parliament Will Quince as, \"patronising, regressive and the nanny state at its worst.\" In addition a study by the University of Glasgow, which sampled 132,000 adults, found that focusing on sugar in isolation misleads consumers as reducing fat intake is also crucial to reducing obesity.\n\nFrom an opposing standpoint, Professor Robert Lustig of the University of California, San Francisco School of Medicine, has argued that the UK tax measure may not go far enough and that, \"juice should be taxed the same way as soda because from a metabolic standpoint juice is the same as soda.\" Campaigners have since called for the soft drinks tax to be extended to include confectionery and sweets to help tackle childhood obesity.\n\nThe United States does not have a nationwide soda tax, but a few of its cities have passed their own tax and the U.S. has seen a growing debate around taxing soda in various cities, states and even in Congress in recent years. A few states impose excise taxes on bottled soft drinks or on wholesalers, manufacturers, or distributors of soft drinks.\n\nMedical costs related to obesity in the United States alone were estimated to be $147 billion a year in 2009. In the same year, the American Heart Association reported that the soft drinks and sugar sweetened beverages are the largest contributors of added sugars in Americans' diets. Added sugars are sugars and syrups added to foods during processing or preparation and sugars and syrups added after preparation. Excessive intake of added sugars, as opposed to naturally occurring sugars, is implicated in the rise in obesity.\n\nPhiladelphia and Berkeley are the first two cities to pass a tax on sugary drinks in the U.S. Berkeley's tax of 1 cent/oz of sugary drink has seen a decline in soda consumption by more than 20 percent. Philadelphia's tax of 1.5 cents/oz took effect on January 1, 2017.\n\nThe Measure D soda tax was approved by 76% of Berkeley voters on 4 November 2014, and took effect on 1 January 2015 as the first such tax in the United States. The measure imposes a tax of one cent per ounce on the distributors of specified sugar-sweetened beverages such as soda, sports drinks, energy drinks, and sweetened ice teas but excluding milk-based beverages, meal replacement drink, diet sodas, fruit juice, and alcohol. The revenue generated will enter the general fund of the City of Berkeley. A similar measure in neighboring San Francisco received 54% of the vote, but fell short of the supermajority required to pass. In August 2015, researchers found that average prices for beverages covered under the law rose by less than half of the tax amount. For Coke and Pepsi, 22 percent of the tax was passed on to consumers, with the balance paid by vendors. UC Berkeley researchers found a higher pass-through rate for the tax: 47% of the tax was passed-through to higher prices of sugar-sweetened beverages overall with 69% being passed-through to higher soda prices. In August 2016, a UC Berkeley study showed a 21% drop in the drinking of soda and sugary beverages in low-income neighborhoods in its city.\n\nA study from 2016 compared the changing intake of sugar sweetened beverages and water in Berkeley versus San Francisco and Oakland (which did not have a sugary drink tax passed) before and after Berkeley passed its sugary drink tax. This analysis showed a 26% decrease of soda consumption in Berkeley and 10% increase in San Francisco and Oakland while water intake increased by 63% in Berkeley and 19% in the two neighboring cities. A 2017 before and after study has concluded that one year after the tax was introduced in Berkeley, sugary drink sales decreased by 9.6% when compared to a scenario where the tax was not in place. This same study was also able to show that overall consumer spending did not increase, contradicting the argument of opponents of the Sugary Drink Tax. Another 2017 study results were that purchases of healthier drinks went up and sales of sugary drinks went down, without overall grocery bills increasing or the local food sector losing money.\n\nDemocratic Philadelphia mayor Jim Kenney proposed a citywide soda tax that would raise the price of soda at three cents per ounce. At the time, it was the biggest soda tax proposal in the United States. Kenney promoted using tax revenue to fund universal pre-K, jobs, and development projects, which he predicted would raise $400 million over five years, all the while reducing sugar intake by decreasing the demand for sugary beverages. Kenney's soda tax proposal was brought to the national spotlight and divided key members of the Democratic Party. Presidential hopeful Bernie Sanders argued in an op-ed that the tax would hurt the poor. His opponent, Hillary Clinton, on the other hand, said that she was \"very supportive\" of the idea. The American Beverage Association (ABA), funded by soda companies and distributors, ran local television, radio, and newspaper advertisements against the idea, claiming that the tax would disproportionately hurt the poor. The ABA spent $10.6 million in 2016 in its effort against the tax. The American Medical Association, American Heart Association, and other medical and public health groups support the tax.\n\nThe Philadelphia City Council approved a 1.5-cents-per-ounce tax on 16 June 2016. As part of the compromise legislation that passed, the tax is also imposed on artificially sweetened beverages, such as diet soda. The law became effective on 1 January 2017. It was reported after two months of the tax that Philadelphia supermarkets and beverage distributors are planning layoffs because sugary beverage sales are down between 30 and 50 percent.\n\nAfter the tax took effect, Kenney said it was \"wrong\" and \"misleading\" for businesses to pass the tax on to their customers in the form of higher soda prices. In February 2017, soda manufacturers and retailers announced sales declines of 30-50% in Philadelphia and announced job cuts and layoffs. Kenny characterized the layoffs as evidence of greed among manufacturers. In the first four months of the soda tax $25.6 million was collected, which is lower than predicted. The revenue is intended to pay for a pre-K program (49% of tax revenue), government employee benefits and city programs (20%), and rebuilding city parks and recreation centers. A recent study from 2017 found that Philadelphia's tax has decreased sugary beverage consumption in impoverished youth by 1.3 drinks/week. Langellier et al. also found that when paired with the pre-K program, attendance increases significantly, a finding that is likely to have longer term positive effects than a sugary drink tax alone.\n\nIn March 2017, Pepsi laid off between 80 and 100 employees at two distribution plants in Philadelphia and one plant in nearby Wilmington, Delaware. The company blamed the layoffs on the tax, an assertion rejected by the city government.\n\nIn September 2016, the American Beverage Association, Philadelphia business owners, and other plaintiffs filed a lawsuit against the soda tax, alleging that the tax violated the \"Tax Uniformity Clause\" of the state constitution. The legal challenge was dismissed by the Court of Common Pleas in December 2016, and in June 2017 the Commonwealth Court of Pennsylvania (in a 5-2 decision) affirmed that ruling. The ABA is appealing the decision to the Pennsylvania Supreme Court.\n\nA one-cent-per-ounce soda tax (Prop V) passed with over 61% of the vote on 8 November 2016 and applies to distributors of sugary beverages on 1 January 2018. Exemptions for the tax include infant formulas, milk products, supplements, drinks used for medical reasons, and 100% fruit and vegetable juices. The soda industry spent almost $20 million in its unsuccessful push to defeat the soda tax initiative, a record-breaking amount for a San Francisco ballot initiative.\n\nIn 2014, the first referendum on a soda tax, Proposition E, was voted down by San Francisco; the 2014 referendum received the support of 55 percent of voters, short of the two-thirds required for a referendum directing money to a specific item (the referendum proposed directing the revenue raised to children's physical education and nutrition programs, and in San Francisco such earmarking requires a two-thirds vote to pass). In that referendum campaign, the soda industry spent about $10 million in opposition to the proposed tax.\n\nA one-cent-per-ounce soda tax (Measure HH) passed with over 60% of the vote on 8 November 2016. The tax went into effect on 1 July 2017.\n\nA one-cent-per-ounce soda tax (Prop O1) passed with over 70% of the vote on 8 November 2016. The tax went into effect on April 1, 2017\n\nA two-cents-per-ounce soda tax (Measure 2H) passed with 54% of the vote on 8 November 2016. The tax took effect on July 1, 2017, and revenue will be spent on health promotion, general wellness programs and chronic disease prevention that improve health equity, and other health programs especially for residents with low income and those most affected by chronic disease linked to sugary drink consumption. The tax is exempted at the University of Colorado, Boulder, campus as school officials survey what types of drinks students wish to have. The University was not aware it would be involved in the soda tax, and would have to pay as much as $1 million a year to purchase it.\n\nA one-cent-per-ounce soda tax passed on November 10, 2016, by a 9-8 vote, with Cook County Board of Commissioners President Toni Preckwinkle breaking the 8-8 tie. Cook County includes Chicago and has a population of nearly 5.2 million. This was the most populous jurisdiction with a soda tax in the U.S. The campaign to introduce the tax was heavily funded by Mike Bloomberg.\n\nOn June 30, 2017, a Cook County judge granted a temporary restraining order filed by the Illinois Retail Merchants Association and several Cook County-based grocers that prohibited the tax from being put into effect until at least July 12. The tax eventually went into effect on August 2. Due to a conflict with the Supplemental Nutrition Assistance Program, this soda tax did not apply to any soda purchases made with food stamps, which were used by over 870,000 people. Controversially, the tax affected diet drinks but not sugar-packed fruit juices.\n\nOn October 10, 2017, the Board of Commissioners voted to repeal the tax in a 15-1 vote. The tax stayed in effect up until December 1. The tax was highly unpopular and seen mainly as an attempt to plug the county’s $1.8 billion budget deficit, rather than a public health measure.\n\nThe Coalition for Healthy Kids and Education is currently campaigning to get a soda tax on the May 2018 ballot. Their aim is to implement a 1.15 cents per ounce tax on sugary drinks. There are 18,000 signatures required by December 15, 2017 in order for the tax to be voted on in May.\n\nOn June 5, 2017, Seattle's City Council voted 7-1 to pass a 1.75 cents per ounce tax on sugary drinks; the tax does not include diet soda drinks or fruit drinks and it started on January 1, 2018. After the tax was implemented, people were surprised that the tax made a case (24 cans) of Coke become $7.35 more expensive when compared to a case of Diet Coke or Coke Zero. The $15 million Seattle assumes will be collected from the tax will be used for programs that give access to more fruits and vegetables for low-income families, adding education programs and studying the tax on how it impacts behavior. Seattle collected $4 million in the first four months of the tax.\n\nCoca-Cola has been under fire since 2015 when emails revealed that funding for scientific studies sought to influence research to be more favorable to soda. Research funded by soda companies are 34 times more likely to find soda has no significant health impacts on obesity or diabetes.\n\nTaxing soda can lead to a reduction in overall consumption, according to a scientific study published in the \"Archives of Internal Medicine\" in March 2010. The study found that a 10 percent tax on soda led to a 7 percent reduction in calories from soft drinks. These researchers believe that an 18 percent tax on these foods could cut daily intake by 56 calories per person, resulting in a weight loss of 5 pounds (2 kg) per person per year. The study followed 5,115 young adults ages 18 to 30 from 1985 to 2006.\n\nA 2010 study published in the medical journal \"Health Affairs\" found that if taxes were about 18 cents on the dollar, they would make a significant difference in consumption.\n\nResearch from Duke University and the National University of Singapore released in December 2010 tested larger taxes and determined that a 20 percent and 40 percent taxes on sugar-sweetened beverages would largely not affect calorie intake because people switch to untaxed, but equally caloric, beverages. Kelly Brownell, a proponent of soda taxes, reacted by stating that “[t]he fact is that nobody has been able to see how people will really respond under these conditions.” Similarly, a 2010 study concluded that while people would drink less soda as a result of a soda tax, they would also compensate for this reduction by switching to other high-calorie beverages. In response to these arguments, the American Public Health Association released a statement in 2012 in which they argued that \"Even if individuals switch to 100% juice or chocolate milk, this would be an improvement, as those beverages contribute some nutrients to the diet.\"\n\nA 2011 study in the journal \"Preventive Medicine\" concluded that \"a modest tax on sugar-sweetened beverages could both raise significant revenues and improve public health by reducing obesity\". It has been used by the Rudd Center for Food Policy and Obesity at Yale to estimate revenue from a soda tax, depending on the state, year and tax rate.\n\nA 2012 study by Y. Claire Wang, also in the journal \"Health Affairs\", estimates that a penny per ounce tax on sugared beverages could prevent 2.4 million cases of diabetes per year, 8,000 strokes, and 26,000 premature deaths over 10 years.\n\nIn 2012, just before the city of Richmond began voting on a soda tax, a study was presented at a conference held by the American Public Health Association regarding the potential effects of such a tax in California. The study concluded that, given that soda's price elasticity is such that taxing it would reduce consumption by 10–20 percent, that this reduction \"...is projected to reduce diabetes incidence by 2.9–5.6% and CHD by 0.6–1.2%.\"\n\nA 2013 study in the \"American Journal of Agricultural Economics\" concluded that a 0.5-cent-per-ounce tax on soft drinks would reduce consumption, but \"increase sodium and fat intakes as a result of product substitution,\" in line with the Duke University study mentioned above.\n\nA 2014 study published in the \"American Journal of Public Health\" concluded that Sugar-Sweetened Beverages (SSBs) don’t have a negative impact on employment. Even though job losses in the taxed industry occurred, they were offset by new employment in other sectors of the economy.\n\nA 2016 modelling study estimated that a 20% tax on SSBs would decrease the consumption of SSBs in Australia by 12.6%. The tax could decline the prevalence of obesity in the Australian population, which could lead to gains in health-adjusted life years. The results showed an increase of 7.6 days in full health for a 20-24-year-old male and a 3.7 day increase in longevity for their female peers.\n\nThere have been a number of proposed taxes on sugary beverages, including:\n\n\nA 2016 poll by Morning Consult-Vox finds Americans split on their support of a soda tax. Attitudes seem to have shifted a lot since 2013 when a poll concluded that \"respondents were opposed to government taxes on sugary drinks and candy by a more than 2-to-1 margin.\" In California, however, support for a tax has been high for a few years. According to a Field Poll conducted in 2012, \"Nearly 3 out of 5 California voters would support a special fee on soft drinks to fight childhood obesity.\" \nSupport for a soda tax in New York was higher when pollsters say the money will go towards health care. A Quinnipiac University poll released in April 2010 found that New Yorkers opposed a state tax on soda of one penny per ounce by a 35-point margin, but opposition dropped to a margin of one point when respondents were told the money would go towards health care. A Thompson Reuters poll released in the same month found that 51 percent of Americans opposed a soda tax, while 33 percent supported one.\n\nFighting the creation of soft drink taxes, the American Beverage Association, the largest U.S. trade organization for soft drink bottlers, has spent considerable money lobbying Congress. The Association's annual lobbying spending rose from about $391,000 to more than $690,000 from 2003 to 2008. And, in the 2010 election cycle, its lobbying grew to $8.67 million. These funds helped to pay for 25 lobbyists at seven different lobbying firms.\n\nAn industry group called \"Americans Against Food Taxes,\" backed by juice maker Welch's, soft drink maker PepsiCo Inc, the American Beverage Association, the Corn Refiners Association, McDonald's Corporation and Burger King Holdings Inc used national advertising and conducted lobbying to oppose these taxes. The group has characterized the soda tax as a regressive tax, which would unfairly burden the poor\n\n\nIsland nations and territories have been successful in passing soda taxes. Just like with tobacco taxes, smaller communities are often the first to pass a new type of tax.\n\nBarbados passed a soda tax in September 2015, applied as an excise of 10%.\n\nFiji has an import tax and an excise tax on soda.\n\nFrench Polynesia implemented taxes on soft drinks in 2002.\n\nMauritius passed a soda tax in 2013.\n\nNauru implemented a soda tax in 2007.\n\nSamoa passed a soda tax in 1984.\n\nIn March 2014, the government of the island of St Helena, a British Overseas Territory in the South Atlantic, announced that it would be introducing an additional import duty of 75 pence per litre on sugar-sweetened carbonated drinks with more than 15 grams of sugar per litre. The measure was introduced in May 2014 as part of a number of measures to tackle obesity on the island and the resulting high incidence of type 2 diabetes.\n\nTonga has a soda tax.\n\n\n"}
{"id": "38526066", "url": "https://en.wikipedia.org/wiki?curid=38526066", "title": "Systems medicine", "text": "Systems medicine\n\nSystems medicine is an interdisciplinary field of study that looks at the systems of the human body as part of an integrated whole, incorporating biochemical, physiological, and environment interactions. Systems medicine draws on systems science and systems biology, and considers complex interactions within the human body in light of a patient's genomics, behavior and environment.\n\nThe earliest uses of the term \"systems medicine\" appeared in 1992, in an article on systems medicine and pharmacology by B.J. Zeng and in a paper on systems biomedicine by T. Kamada.\n\nAn important topic in systems medicine and systems biomedicine is the development of computational models that describe disease progression and the effect of therapeutic interventions. \n\nMore recent approaches include the redefinition of disease phenotypes based on common mechanisms rather than symptoms. These provide then therapeutic targets including network pharmacology and drug repurposing. Since 2018, there is a dedicated scientific journal, Systems Medicine, published by Marie-Ann Liebert and with Jan Baumbach and Harald Schmidt as co-editors in chief. \n"}
{"id": "1150541", "url": "https://en.wikipedia.org/wiki?curid=1150541", "title": "Testicular atrophy", "text": "Testicular atrophy\n\nTesticular atrophy is a medical condition in which the male reproductive organs (the testes, which in humans are located in the scrotum) diminish in size and may be accompanied by loss of function. This does not refer to temporary changes, such as those brought on by cold.\n\nTesticular atrophy may be brought on by surgical repairs to certain types of hernias.\n\nResearch has found testicular atrophy to be caused by scrotal trauma, inguinal hernia repair (rarely, and more often in very young children), anabolic steroid use, and oestrogen therapy, varicocele, and ischaemia (most often secondary to testicular torsion).\n\nPhthalate esters, and Zika virus infection have been found to cause testicular atrophy in animals.\n\n"}
{"id": "17475776", "url": "https://en.wikipedia.org/wiki?curid=17475776", "title": "The Heart Truth", "text": "The Heart Truth\n\nThe Heart Truth is a campaign meant to raise awareness of the risk of heart disease in women. The campaign is sponsored in the United States by the National Heart, Lung, and Blood Institute, an organization of the United States Department of Health and Human Services; a similar campaign is promoted in Canada by the Heart and Stroke Foundation of Canada. It focuses mainly on educating women aged forty to sixty, as that is the time when the risk of heart disease begins to increase.\n\nThe campaign began in March 2001 on recommendation from over seventy experts on the health of women. The research stressed the need to communicate to women about the risk of heart disease, and endorsed The Heart Truth as a means of doing so.\n\nThe logo of the campaign is a red dress. It came into being as a way to attract attention to the Heart Truth, and eliminate perceptions that heart disease is an issue only for men. The dress reminds women to focus on their \"outerselves\", as well as their \"innerselves\", especially heart health.\n\nThe campaign has also conjured a National Wear Red Day, meant to take place on the first Friday of February annually.\n\nThe Heart Truth has joined with the United States Federal Government and fashion industries, in an attempt to appeal to female audiences. Red dresses have been displayed across the country, primarily at New York's Fashion Week. The first Red Dress Collection Fashion Week took place in 2003 when nineteen designers, including Vera Wang, Oscar de la Renta, and Carmen Marc Valvo contributed dresses that were displayed in the Byrant Park Tents. Many fashion shows have been put on in recent years during the Fashion Week festivities; many famous celebrities have participated in walking the aisle, including Jenna Fischer, Sheryl Crow, Natalie Morales, Kelly Ripa, Deborah Harry, Venus Williams, Angela Bassett, Rachael Ray, Valerie Bertinelli, Christie Brinkley, Thalía, Vanessa L. Williams, Raven-Symoné, Allison Janney, Sara Ramirez, Billie Jean King, Katie Couric, Sarah, Duchess of York, Lindsay Lohan, LeAnn Rimes, Christina Milian, Fergie, Jordin Sparks, Ashanti, Hilary Duff, Mary Lynn Rajskub, Rose McGowan and Eartha Kitt.\n\nLaureen Harper, the Wife of Canadian Prime Minister Stephen Harper has been a great supporter and has served as guest of honour at the event at Nathan Phillips Square in Toronto, Ontario for many consecutive years.\n\nFormer First Lady Laura Bush has been the ambassador for the Heart Truth since 2003. She has led the federal government in giving women more information relating to heart disease. Bush has coordinated many events relating to the Heart Truth, including a White House ceremony in 2004, the Kennedy Center exhibit, the Reagan Library exhibit, and has participated in all Fashion Week events dating to 2003.\n\nA signature component of Mrs. Bush's involvement is her communication with women at hospital events featuring those living with heart disease. She promotes the campaign through various media interviews as well.\n\nIn May 2005, the Heart Truth constructed a special exhibition at the John F. Kennedy Center for the Performing Arts in Washington, D.C., known as the First Ladies Red Dress Collection. The collection featured seven red dresses worn by America's first ladies Lady Bird Johnson, Betty Ford, Rosalynn Carter, Nancy Reagan, Barbara Bush, Hillary Clinton and Laura Bush. The exhibit was unveiled by Laura Bush, in the presence of many Congressional spouses and Cabinet secretaries.\n\nIn February 2007, the Heart Truth moved that exhibit to the Ronald Reagan Presidential Library and Museum in Simi Valley, California. There, the exhibit was opened by former First Lady Nancy Reagan along with television personality Larry King and Laura Bush. A conference was held at the library with leaders of the heart disease awareness movement as well as Bush and Reagan.\n\nIn February 2013, the Heart Truth presented a fashion show at Manhattan's Hammerstein Ballroom. Celebrities who walked the runway included Minka Kelly, Soledad O'Brien, Wendy Williams, Brenda Strong, Kris Jenner, Jamie Chung, Toni Braxton, Kelly Osbourne and others.\n\n"}
{"id": "40667954", "url": "https://en.wikipedia.org/wiki?curid=40667954", "title": "The Rheumatoid Arthritis Quality of Life Questionnaire", "text": "The Rheumatoid Arthritis Quality of Life Questionnaire\n\nThe Rheumatoid Arthritis Quality of Life Questionnaire (RAQoL) is a disease specific patient-reported outcome measure which determines the effect rheumatoid arthritis has on a patient’s quality of life. The RAQoL has 30 items with a yes and no response format and takes about six minutes to complete.\n\nScores on the RAQoL are a sum of all the individual item scores with a range from 0-30, with a lower score indicating better quality of life. The RAQoL is a self-assessment questionnaire, meaning patients fill out the survey themselves in order to avoid experimental error.\n\nThe RAQoL was developed by Galen Research, the University of Leeds and the Academic Hospital Maastricht, and was first published in 1997. It was the first patient completed quality of life questionnaire that focused on rheumatoid arthritis and is distinct from other questionnaires as it includes physical contact as a dimension of quality of life. Other dimensions include activities of daily living, social interaction/function, emotions, mood and recreation and pastimes.\n\nSince its development, the RAQOL has been translated into 33 languages other than Dutch and UK English. Validation studies have been performed in countries such as Sweden, Argentina and Australia in order to confirm the responsiveness and validity of the language adaptations.\n\nThe RAQoL has been used in clinical studies in order to confirm the efficacy of proposed treatments of rheumatoid arthritis. It has been utilized in order to confirm the efficacy of tocilizumab and infliximab.\n"}
{"id": "30321101", "url": "https://en.wikipedia.org/wiki?curid=30321101", "title": "Transmission-based precautions", "text": "Transmission-based precautions\n\nTransmission-based precautions are additional infection control precautions in health care, and the latest routine infection prevention and control practices applied for patients who are known or suspected to be infected or colonized with infectious agents, including certain epidemiologically important pathogens. The latter require additional control measures to effectively prevent transmission.\n\nCommunicable diseases occur as a result of the interaction between:\nThe control of communicable diseases may involve changing one or more of these components, the first three of which are influenced by the environment. These diseases can have a wide range of effects, varying from silent infection – with no signs or symptoms – to severe illness and death. According to its nature, a certain infectious agent may demonstrate one or more following modes of transmission:\nTransmission-based precautions are used when the route(s) of transmission is (are) not completely interrupted using Standard Precautions alone.\nThree categories of transmission-based precautions are designed with respect to the modes of transmission:\nFor some diseases that have multiple routes of transmission (e.g., SARS), more than one transmission-based precautions category may be used. When used either singly or in combination, they are always used in addition to Standard Precautions.\n\nWhen transmission-based precautions are indicated, efforts must be made to counteract possible adverse effects on patients (i.e., anxiety, depression and other mood disturbances, perceptions of stigma, reduced contact with clinical staff, and increases in preventable adverse events in order to improve acceptance by the patients and adherence by health care workers.\n\nContact precautions are intended to prevent transmission of infectious agents, including epidemiologically important microorganisms, which are spread by direct or indirect contact with the patient or the patient’s environment. The specific agents and circumstance for which contact precautions are indicated are found in Appendix A \nof the Guidance. The application of contact precautions for patients infected or colonized with MDROs is described in the 2006 HICPAC/CDC MDRO guideline. Contact precautions also apply where the presence of excessive wound drainage, fecal incontinence, or other discharges from the body suggest an increased potential for extensive environmental contamination and risk of transmission. A single-patient room is preferred for patients who require contact precautions. When a single-patient room is not available, consultation with infection control personnel is recommended to assess the various risks associated with other patient placement options (e.g., cohorting, keeping the patient with an existing roommate). In multi-patient rooms, >3 feet spatial separation between beds is advised to reduce the opportunities for inadvertent sharing of items between the infected/colonized patient and other patients. Healthcare personnel caring for patients on contact precautions wear a gown and gloves for all interactions that may involve contact with the patient or potentially contaminated areas in the patient's environment. Donning PPE upon room entry and discarding before exiting the patient room is done to contain pathogens, especially those that have been implicated in transmission through environmental contamination (e.g., VRE, C. difficile, noroviruses and other intestinal tract pathogens; RSV)\n\nDroplet precautions are intended to prevent transmission of pathogens spread through close respiratory or mucous membrane contact with respiratory secretions. Because these pathogens do not remain infectious over long distances in a healthcare facility, special air handling and ventilation are not required to prevent droplet transmission. Infectious agents for which droplet precautions are indicated are found in Appendix A and include \"B. pertussis\", influenza virus, adenovirus, rhinovirus, \"N. meningitides\", and group A streptococcus (for the first 24 hours of antimicrobial therapy). A single patient room is preferred for patients who require droplet precautions. When a single-patient room is not available, consultation with infection control personnel is recommended to assess the various risks associated with other patient placement options (e.g., cohorting, keeping the patient with an existing roommate). Spatial separation of > 3 feet and drawing the curtain between patient beds is especially important for patients in multi-bed rooms with infections transmitted by the droplet route. Healthcare personnel wear a mask (a respirator is not necessary) for close contact with infectious patient; the mask is generally donned upon room entry. Patients on droplet precautions who must be transported outside of the room should wear a mask if tolerated and follow Respiratory Hygiene/Cough Etiquette.\n\nAirborne precautions prevent transmission of infectious agents that remain infectious over long distances when suspended in the air (e.g., rubeola virus [measles], varicella virus [chickenpox], \"M. tuberculosis\", and possibly SARS-CoV). The preferred placement for patients who require airborne precautions is in an airborne infection isolation room (AIIR). An AIIR is a single-patient room that is equipped with special air handling and ventilation capacity that meet the American Institute of Architects/Facility Guidelines Institute (AIA/FGI) standards for AIIRs (i.e., monitored negative pressure relative to the surrounding area, air exchanges per hour for new construction and renovation and 6 air exchanges per hour for existing facilities, air exhausted directly to the outside or recirculated through HEPA filtration before return). Some states require the availability of such rooms in hospitals, emergency departments, and nursing homes that care for patients with \"M. tuberculosis\". A respiratory protection program that includes education about use of respirators, fit-testing, and user seal checks is required in any facility with AIIRs. In settings where airborne precautions cannot be implemented due to limited engineering resources (e.g., physician offices), masking the patient, placing the patient in a private room (e.g., office examination room) with the door closed, and providing N95 or higher level respirators or masks if respirators are not available for healthcare personnel will reduce the likelihood of airborne transmission until the patient is either transferred to a facility with an AIIR or returned to the home environment, as deemed medically appropriate. Healthcare personnel caring for patients on airborne precautions wear a mask or respirator, depending on the disease-specific recommendations (Appendix A), that is donned prior to room entry. Whenever possible, non-immune HCWs should not care for patients with vaccine-preventable airborne diseases (e.g., measles, chickenpox, and smallpox).\n\nSince the infecting agent often is not known at the time of admission to a healthcare facility, transmission-based precautions are used empirically, according to the clinical syndrome and the likely etiologic agents at the time, and then modified when the pathogen is identified or a transmissible infectious etiology is ruled out.\nDiagnosis of many infections requires laboratory confirmation. Since laboratory tests, especially those that depend on culture techniques, often require two or more days for completion, transmission-based precautions must be implemented while test results are pending based on the clinical presentation and likely pathogens. Use of appropriate transmission-based precautions at the time a patient develops symptoms or signs of transmissible infection, or arrives at a healthcare facility for care, reduces transmission opportunities. While it is not possible to identify prospectively all patients needing transmission-based precautions, certain clinical syndromes and conditions carry a sufficiently high risk to warrant their use empirically while confirmatory tests are pending.\n¹ Patients with the syndromes or conditions listed below may present with atypical signs or symptoms (e.g.neonates and adults with pertussis may not have paroxysmal or severe cough). The clinician's index of suspicion should be guided by the prevalence of specific conditions in the community, as well as clinical judgment.\n\n² The organisms listed under the column \"Potential Pathogens\" are not intended to represent the complete, or even most likely, diagnoses, but rather possible etiologic agents that require additional precautions beyond Standard Precautions until they can be ruled out.\n\nDetails one may find in Appendix A of the Guidance\n1 Type of precautions: A, Airborne precautions; C, Contact; D, Droplet; S, Standard; when A, C, and D are specified, also use S.\n\n² Duration of precautions: CN, until off antimicrobial treatment and culture-negative; DI, duration of illness (with wound lesions, DI means until wounds stop draining); DE, until environment completely decontaminated; U, until time specified in hours (hrs) after initiation of effective therapy; Unknown: criteria for establishing eradication of pathogen has not been determined\n\nTransmission-based precautions remain in effect for limited periods of time (i.e., while the risk for transmission of the infectious agent persists or for the duration of the illness (Appendix A). For most infectious diseases, this duration reflects known patterns of persistence and shedding of infectious agents associated with the natural history of the infectious process and its treatment. For some diseases (e.g., pharyngeal or cutaneous diphtheria, RSV), transmission-based precautions remain in effect until culture or antigen-detection test results document eradication of the pathogen and, for RSV, symptomatic disease is resolved. For other diseases, (e.g., \"M. tuberculosis\") state laws and regulations, and healthcare facility policies, may dictate the duration of precautions 12). In immunocompromised patients, viral shedding can persist for prolonged periods of time (many weeks to months) and transmission to others may occur during that time; therefore, the duration of contact and/or droplet precautions may be prolonged for many weeks. The duration of contact precautions for patients who are colonized or infected with MDROs remains undefined. MRSA is the only MDRO for which effective decolonization regimens are available. However, carriers of MRSA who have negative nasal cultures after a course of systemic or topical therapy may resume shedding MRSA in the weeks that follow therapy. Although early guidelines for VRE suggested discontinuation of contact precautions after three stool cultures obtained at weekly intervals proved negative, subsequent experiences have indicated that such screening may fail to detect colonization that can persist for >1 year. Likewise, available data indicate that colonization with VRE, MRSA, and possibly MDR-GNB, can persist for many months, especially in the presence of severe underlying disease, invasive devices, and recurrent courses of antimicrobial agents. It may be prudent to assume that MDRO carriers are colonized permanently and manage them accordingly. Alternatively, an interval free of hospitalizations, antimicrobial therapy, and invasive devices (e.g., 6 or 12 months) before reculturing patients to document clearance of carriage may be used. Determination of the best strategy awaits the results of additional studies. See the 2006 HICPAC/CDC MDRO guideline for discussion of possible criteria to discontinue contact precautions for patients colonized or infected with MDROs.\n\nAlthough transmission-based precautions generally apply in all healthcare settings, exceptions exist. For example, in home care, AIIRs are not available. Furthermore, family members already exposed to diseases such as varicella and tuberculosis would not use masks or respiratory protection, but visiting HCWs would need to use such protection. Similarly, management of patients colonized or infected with MDROs may necessitate contact precautions in acute care hospitals and in some LTCFs when there is continued transmission, but the risk of transmission in ambulatory care and home care, has not been defined. Consistent use of Standard Precautions may suffice in these settings, but more information is needed.\n"}
{"id": "584416", "url": "https://en.wikipedia.org/wiki?curid=584416", "title": "Uterine contraction", "text": "Uterine contraction\n\nA uterine contraction is a muscle contraction of the uterine smooth muscle.\n\nThe uterus frequently contracts throughout the entire menstrual cycle, and these contractions have been termed \"endometrial waves\" or \"contractile waves\". These appear to involve only the sub-endometrial layer of the myometrium. In the early follicular phase, these contractions occur once or twice per minute and last 10–15 seconds with a low amplitude of usually 30 mmHg. The frequency increases to 3–4 per minute towards ovulation. During the luteal phase, the frequency and amplitude decrease, possibly to facilitate any implantation.\n\nIf implantation does not occur, the frequency remains low, but the amplitude increases dramatically to between 50 and 200 mmHg producing labor-like contractions at the time of menstruation. These contractions are sometimes termed \"menstrual cramps\", although that term is often used for menstrual pain in general. These contractions may be uncomfortable or even painful, but they are generally significantly less painful than contractions during labour. A hot water bottle or exercising has been found to help.\n\nA shift in the myosin expression of the uterine smooth muscle has been hypothesized to avail for changes in the directions of uterine contractions that are seen during the menstrual cycle.\n\nA contraction refers specifically to the motion of the uterus as part of the process of childbirth. Contractions, and labour in general, is one condition that releases the hormone oxytocin into the body. Contractions become longer as labour intensifies.\n\nPrior to actual labour, women may experience Braxton Hicks contractions, sometimes known as \"false labour.\"\n\nSince every pregnancy is different, a doctor, midwife or other competent professional should always be consulted before any action is taken to reduce the pain. Some popular methods may be harmful to the mother and/or the baby, or may actually worsen the pain or lengthen the labour.\n\nUterine contractions during childbirth can be monitored by cardiotocography, in which a device is fixated to the skin of the mother or directly to the fetal scalp. The pressure required to flatten a section of the uterine wall correlates with the internal pressure, thereby providing an estimate of it. \n\nA type of monitoring technology under development at Drexel University embeds conductive threads in the knitted fabric of a bellyband. When the fibers stretch in response to a contraction, the threads function like an antenna, and send the signals they pick up to an embedded RFID (radio frequency identification device) chip that reports the data.\n\nThe uterus and vagina contract during female orgasm. These contractions may not be noticed by all women; pregnant women are more likely to notice these contractions by late 2nd and 3rd trimesters.\n\n"}
{"id": "56746554", "url": "https://en.wikipedia.org/wiki?curid=56746554", "title": "Vaginitis emphysematosa", "text": "Vaginitis emphysematosa\n\nVaginitis emphysematosa is a rare, benign vaginal cyst known from 173 cases. Those women affected were 42 to 65 years. The cysts appear grouped but defined from one another, smooth, and can be as large as 2 cm. Symptoms included: vaginal discharge, itching, sensation of pressure, appearance of nodules, and sometimes a \"popping sound\". The cause is unknown. Histological examination showed the cysts contained pink hyalin-like material, foreign body-type giant cells in the cyst's wall, with chronic inflammatory cell fluid. The gas-filled cysts are identified with CT imaging. The gas contained in the cysts has been analysed and consists of nitrogen, oxygen, argon, carbon dioxide, and sulfur dioxide. Treatment may not be required and no complications follow the resolution of the cysts. It may be associated with immunosuppresion, trichomonsis, or \"Haemophilus vaginalis\" infection. Vaginitis emphysemotosa is characterized by gas-filled cysts in the vaginal wall. The first mention of the cyst was in 1877 by Zweifel.\n\n"}
{"id": "4580454", "url": "https://en.wikipedia.org/wiki?curid=4580454", "title": "Virtual Physiological Human", "text": "Virtual Physiological Human\n\nThe Virtual Physiological Human (VPH) is a European initiative that focuses on a methodological and technological framework that, once established, will enable collaborative investigation of the human body as a single complex system. The collective framework will make it possible to share resources and observations formed by institutions and organizations, creating disparate but integrated computer models of the mechanical, physical and biochemical functions of a living human body.\n\nVPH is a framework which aims to be descriptive, integrative and predictive. Clapworthy \"et al.\" state that the framework should be descriptive by allowing laboratory and healthcare observations around the world \"to be collected, catalogued, organized, shared and combined in any possible way.\" It should be integrative by enabling those observations to be collaboratively analyzed by related professionals in order to create \"systemic hypotheses.\" Finally, it should be predictive by encouraging interconnections between extensible and scalable predictive models and \"systemic networks that solidify those systemic hypotheses\" while allowing observational comparison.\n\nThe framework is formed by large collections of anatomical, physiological, and pathological data stored in digital format, typically by predictive simulations developed from these collections and by services intended to support researchers in the creation and maintenance of these models, as well as in the creation of end-user technologies to be used in the clinical practice. VPH models aim to integrate physiological processes across different length and time scales (multi-scale modelling). These models make possible the combination of patient-specific data with population-based representations. The objective is to develop a systemic approach which avoids a reductionist approach and seeks not to subdivide biological systems in any particular way by dimensional scale (body, organ, tissue, cells, molecules), by scientific discipline (biology, physiology, biophysics, biochemistry, molecular biology, bioengineering) or anatomical sub-system (cardiovascular, musculoskeletal, gastrointestinal, etc.).\n\nThe initial concepts that led to the Virtual Physiological Human initiative came from the IUPS Physiome Project. The project was started in 1997 and represented the first worldwide effort to define the physiome through the development of databases and models which facilitated the understanding of the integrative function of cells, organs, and organisms. The project focused on compiling and providing a central repository of databases that would link experimental information and computational models from many laboratories into a single, self-consistent framework.\n\nFollowing the launch of the Physiome Project, there were many other worldwide initiatives of loosely coupled actions all focusing on the development of methods for modelling and simulation of human pathophysiology. In 2005, an expert workshop of the Physiome was held as part of the Functional Imaging and Modelling of the Heart Conference in Barcelona where a white paper entitled \"Towards Virtual Physiological Human: Multilevel modelling and simulation of the human anatomy and physiology\" was presented. The goal of this paper was to shape a clear overview of on-going relevant VPH activities, to build a consensus on how they can be complemented by new initiatives for researchers in the EU and to identify possible mid-term and long term research challenges.\n\nIn 2006, the European Commission funded a coordination and support action entitled \"STEP: Structuring The EuroPhysiome\". The STEP consortium promoted a significant consensus process that involved more than 300 stakeholders including researchers, industry experts, policy makers, clinicians, etc. The prime result of this process was a booklet entitled \"Seeding the EuroPhysiome: A Roadmap to the Virtual Physiological Human\". The STEP action and the resulting research roadmap were instrumental in the development of the VHP concept and in the initiation of much larger process that involves significant research funding, large collaborative projects, and a number of connected initiatives, not only in Europe but also in the United States, Japan, and China.\n\nVPH now forms a core target of the 7th Framework Programme of the European Commission, and aims to support the development of patient-specific computer models and their application in personalised and predictive healthcare. The Virtual Physiological Human Network of Excellence (VPH NoE) aims to connect the various VPH projects within the 7th Framework Programme.\n\nVPH-related projects have received substantial funding from the European Commission in order to further scientific progress in this area. The European Commission is insistent that VPH-related projects demonstrate strong industrial participation and clearly indicate a route from basic science into clinical practice. In the future, it is hoped that the VPH will eventually lead to a better healthcare system which aims to produce the following benefits:\n\n\nPersonalized care solutions are a key aim of the VPH, with new modelling environments for predictive, individualized healthcare to result in better patient safety and drug efficacy. It is anticipated that the VPH could also result in healthcare improvement through greater understanding of pathophysiological processes. The use of biomedical data from a patient to simulate potential treatments and outcomes could prevent the patient from experiencing unnecessary or ineffective treatments. The use of in silico (by computer simulation) modelling and testing of drugs could also reduce the need for experiments on animals.\n\nA future goal is that there will be also be a more holistic approach to medicine with the body treated as a single multi-organ system rather than as a collection of individual organs. Advanced integrative tools should further help to improve the European healthcare system on a number of different levels that include diagnosis, treatment and care of patients and in particular quality of life.\n\n\n\n\n"}
{"id": "57165508", "url": "https://en.wikipedia.org/wiki?curid=57165508", "title": "Weathering hypothesis", "text": "Weathering hypothesis\n\nThe weathering hypothesis was first proposed by Arline Geronimus in 1992. It holds that African American women's health deteriorates in early adulthood as a result of their cumulative exposure to socioeconomic disadvantage. Subsequent studies by her and others have found support for the hypothesis. In recent years, the hypothesis has been examined with regard to phenomena such as allostatic load, epigenetics, and telomere shortening.\nWhile working part-time at a school for pregnant teenagers in Trenton, New Jersey, Geronimus first noticed that the teens who came to the school tended to have far more health problems than her classmates at Princeton University. She thus began to wonder whether the health conditions of the teens at that clinic may have been caused by their environment. While in graduate school, she proposed the \"weathering hypothesis\". She chose the term weathering as a metaphor for the effects she perceived that exposure to stress was having on the health of marginalized people.\n\n"}
