{"id": "28231080", "url": "https://en.wikipedia.org/wiki?curid=28231080", "title": "Archiv für Begriffsgeschichte", "text": "Archiv für Begriffsgeschichte\n\nArchiv für Begriffsgeschichte ('Archive for Conceptual History') is a German peer-reviewed academic journal. It was founded by Erich Rothacker, and is published by Christian Bermes, Ulrich Dierse and Michael Erler. The editor is Annika Hand.\n\nThe journal publishes works on concepts of the history of philosophy and sciences, both from the European and from non-European traditions, on mythological and religious concepts, and on concepts of common parlance which have a characteristic significance for a special era or culture. The journal also embraces articles on revealing metaphors, on problems at translating concepts, as well as on theory and criticism of the method of conceptual history.\n\n\n"}
{"id": "33944016", "url": "https://en.wikipedia.org/wiki?curid=33944016", "title": "Canon (basic principle)", "text": "Canon (basic principle)\n\nThe concept of canon is very broad; in a general sense it refers to being a rule or a body of rules.\n\nThere are definitions that state it as: “the body of rules, principles, or standards accepted as axiomatic and universally binding in a field of study or art”. This can be related to such topics as literary canons or the canons of rhetoric, which is a topic within itself that describes the rules of giving a speech. There are five key principles, and when grouped together, are the principles set for giving speeches as seen with regard to Rhetoric. This is one such example of how the term canon is used in regard to rhetoric.\n\n"}
{"id": "29358535", "url": "https://en.wikipedia.org/wiki?curid=29358535", "title": "Comply or explain", "text": "Comply or explain\n\nComply or explain is a regulatory approach used in the United Kingdom, Germany, the Netherlands and other countries in the field of corporate governance and financial supervision. Rather than setting out binding laws, government regulators (in the UK, the Financial Reporting Council, in Germany, under the Aktiengesetz) set out a code, which listed companies may either comply with, or if they do not comply, explain publicly why they do not. The UK Corporate Governance Code, the German Corporate Governance Code (or Deutscher Corporate Governance Kodex) and the Dutch Corporate Governance Code 'Code Tabaksblat' () use this approach in setting minimum standards for companies in their audit committees, remuneration committees and recommendations for how good companies should divide authority on their boards.\n\nThe purpose of \"comply or explain\" is to \"let the market decide\" whether a set of standards is appropriate for individual companies. Since a company may deviate from the standard, this approach rejects the view that \"one size fits all\", but because of the requirement of disclosure of explanations to market investors, anticipates that if investors do not accept a company's explanations, then investors will sell their shares, hence creating a \"market sanction\", rather than a legal one. The concept was first introduced after the recommendations of the Cadbury Report of 1992.\n\n"}
{"id": "36797", "url": "https://en.wikipedia.org/wiki?curid=36797", "title": "Occam's razor", "text": "Occam's razor\n\nOccam's razor (also Ockham's razor or Ocham's razor; Latin: \"lex parsimoniae\" \"law of parsimony\") is the problem-solving principle that the simplest solution tends to be the correct one. When presented with competing hypotheses to solve a problem, one should select the solution with the fewest assumptions. The idea is attributed to English Franciscan friar William of Ockham (c. 1287–1347), a scholastic philosopher and theologian.\n\nIn science, Occam's razor is used as an abductive heuristic in the development of theoretical models, rather than as a rigorous arbiter between candidate models. In the scientific method, Occam's razor is not considered an irrefutable principle of logic or a scientific result; the preference for simplicity in the scientific method is based on the falsifiability criterion. For each accepted explanation of a phenomenon, there may be an extremely large, perhaps even incomprehensible, number of possible and more complex alternatives. Since one can always burden failing explanations with \"ad hoc\" hypotheses to prevent them from being falsified, simpler theories are preferable to more complex ones because they are more testable.\n\nThe term \"Occam's razor\" did not appear until a few centuries after William of Ockham's death in 1347. Libert Froidmont, in his \"On Christian Philosophy of the Soul\", takes credit for the phrase, speaking of \"novacula occami\". Ockham did not invent this principle, but the \"razor\"—and its association with him—may be due to the frequency and effectiveness with which he used it. Ockham stated the principle in various ways, but the most popular version, \"Entities are not to be multiplied without necessity\" () was formulated by the Irish Franciscan philosopher John Punch in his 1639 commentary on the works of Duns Scotus.\n\nThe origins of what has come to be known as Occam's razor are traceable to the works of earlier philosophers such as John Duns Scotus (1265–1308), Robert Grosseteste (1175–1253), Maimonides (Moses ben-Maimon, 1138–1204), and even Aristotle (384–322 BC). Aristotle writes in his \"Posterior Analytics\", \"We may assume the superiority [other things being equal] of the demonstration which derives from fewer postulates or hypotheses.\" Ptolemy () stated, \"We consider it a good principle to explain the phenomena by the simplest hypothesis possible.\"\n\nPhrases such as \"It is vain to do with more what can be done with fewer\" and \"A plurality is not to be posited without necessity\" were commonplace in 13th-century scholastic writing. Robert Grosseteste, in \"Commentary on\" [Aristotle's] \"the Posterior Analytics Books\" (\"Commentarius in Posteriorum Analyticorum Libros\") (c. 1217–1220), declares: \"That is better and more valuable which requires fewer, other circumstances being equal... For if one thing were demonstrated from many and another thing from fewer equally known premises, clearly that is better which is from fewer because it makes us know quickly, just as a universal demonstration is better than particular because it produces knowledge from fewer premises. Similarly in natural science, in moral science, and in metaphysics the best is that which needs no premises and the better that which needs the fewer, other circumstances being equal.\"\n\nThe \"Summa Theologica\" of Thomas Aquinas (1225–1274) states that \"it is superfluous to suppose that what can be accounted for by a few principles has been produced by many.\" Aquinas uses this principle to construct an objection to God's existence, an objection that he in turn answers and refutes generally (cf. \"quinque viae\"), and specifically, through an argument based on causality. Hence, Aquinas acknowledges the principle that today is known as Occam's razor, but prefers causal explanations to other simple explanations (cf. also Correlation does not imply causation).\n\nWilliam of Ockham (\"circa\" 1287–1347) was an English Franciscan friar and theologian, an influential medieval philosopher and a nominalist. His popular fame as a great logician rests chiefly on the maxim attributed to him and known as Occam's razor. The term \"razor\" refers to distinguishing between two hypotheses either by \"shaving away\" unnecessary assumptions or cutting apart two similar conclusions.\n\nWhile it has been claimed that Occam's razor is not found in any of William's writings, one can cite statements such as (\"Plurality must never be posited without necessity\"), which occurs in his theological work on the \"Sentences of Peter Lombard\" (\"Quaestiones et decisiones in quattuor libros Sententiarum Petri Lombardi\"; ed. Lugd., 1495, i, dist. 27, qu. 2, K).\n\nNevertheless, the precise words sometimes attributed to William of Ockham, (Entities must not be multiplied beyond necessity), are absent in his extant works; this particular phrasing comes from John Punch, who described the principle as a \"common axiom\" (\"axioma vulgare\") of the Scholastics. William of Ockham's contribution seems to restrict the operation of this principle in matters pertaining to miracles and God's power; so, in the Eucharist, a plurality of miracles is possible, simply because it pleases God.\n\nThis principle is sometimes phrased as (\"Plurality should not be posited without necessity\"). In his \"Summa Totius Logicae\", i. 12, William of Ockham cites the principle of economy, (\"It is futile to do with more things that which can be done with fewer\"; Thorburn, 1918, pp. 352–53; Kneale and Kneale, 1962, p. 243.)\n\nTo quote Isaac Newton, \"We are to admit no more causes of natural things than such as are both true and sufficient to explain their appearances. Therefore, to the same natural effects we must, as far as possible, assign the same causes.\"\n\nBertrand Russell offers a particular version of Occam's razor: \"Whenever possible, substitute constructions out of known entities for inferences to unknown entities.\"\n\nAround 1960, Ray Solomonoff founded the theory of universal inductive inference, the theory of prediction based on observations; for example, predicting the next symbol based upon a given series of symbols. The only assumption is that the environment follows some unknown but computable probability distribution. This theory is a mathematical formalization of Occam's razor.\n\nAnother technical approach to Occam's razor is ontological parsimony. Parsimony means spareness and is also referred to as the Rule of Simplicity. This is considered a strong version of Occam's razor. A variation used in medicine is called the \"Zebra\": a doctor should reject an exotic medical diagnosis when a more commonplace explanation is more likely, derived from Theodore Woodward's dictum \"When you hear hoofbeats, think of horses not zebras\".\n\nErnst Mach formulated the stronger version of Occam's razor into physics, which he called the Principle of Economy stating: \"Scientists must use the simplest means of arriving at their results and exclude everything not perceived by the senses.\"\n\nThis principle goes back at least as far as Aristotle, who wrote \"Nature operates in the shortest way possible.\" The idea of parsimony or simplicity in deciding between theories, though not the intent of the original expression of Occam's razor, has been assimilated into our culture as the widespread layman's formulation that \"the simplest explanation is usually the correct one.\"\n\nPrior to the 20th century, it was a commonly held belief that nature itself was simple and that simpler hypotheses about nature were thus more likely to be true. This notion was deeply rooted in the aesthetic value that simplicity holds for human thought and the justifications presented for it often drew from theology. Thomas Aquinas made this argument in the 13th century, writing, \"If a thing can be done adequately by means of one, it is superfluous to do it by means of several; for we observe that nature does not employ two instruments [if] one suffices.\"\n\nBeginning in the 20th century, epistemological justifications based on induction, logic, pragmatism, and especially probability theory have become more popular among philosophers.\n\nOccam's razor has gained strong empirical support in helping to converge on better theories (see \"Applications\" section below for some examples).\n\nIn the related concept of overfitting, excessively complex models are affected by statistical noise (a problem also known as the bias-variance trade-off), whereas simpler models may capture the underlying structure better and may thus have better predictive performance. It is, however, often difficult to deduce which part of the data is noise (cf. model selection, test set, minimum description length, Bayesian inference, etc.).\n\nThe razor's statement that \"other things being equal, simpler explanations are generally better than more complex ones\" is amenable to empirical testing. Another interpretation of the razor's statement would be that \"simpler hypotheses are generally better than the complex ones\". The procedure to test the former interpretation would compare the track records of simple and comparatively complex explanations. If one accepts the first interpretation, the validity of Occam's razor as a tool would then have to be rejected if the more complex explanations were more often correct than the less complex ones (while the converse would lend support to its use). If the latter interpretation is accepted, the validity of Occam's razor as a tool could possibly be accepted if the simpler hypotheses led to correct conclusions more often than not.\n\nSome increases in complexity are sometimes necessary. So there remains a justified general bias toward the simpler of two competing explanations. To understand why, consider that for each accepted explanation of a phenomenon, there is always an infinite number of possible, more complex, and ultimately incorrect, alternatives. This is so because one can always burden a failing explanation with an ad hoc hypothesis. Ad hoc hypotheses are justifications that prevent theories from being falsified. Even other empirical criteria, such as consilience, can never truly eliminate such explanations as competition. Each true explanation, then, may have had many alternatives that were simpler and false, but also an infinite number of alternatives that were more complex and false. But if an alternative ad hoc hypothesis were indeed justifiable, its implicit conclusions would be empirically verifiable. On a commonly accepted repeatability principle, these alternative theories have never been observed and continue to escape observation. In addition, one does not say an explanation is true if it has not withstood this principle.\n\nPut another way, any new, and even more complex, theory can still possibly be true. For example, if an individual makes supernatural claims that leprechauns were responsible for breaking a vase, the simpler explanation would be that he is mistaken, but ongoing ad hoc justifications (e.g. \"... and that's not me on the film; they tampered with that, too\") successfully prevent outright falsification. This endless supply of elaborate competing explanations, called saving hypotheses, cannot be ruled out—except by using Occam's razor. A study of the predictive validity of Occam's razor found 32 published papers that included 97 comparisons of economic forecasts from simple and complex forecasting methods. None of the papers provided a balance of evidence that complexity of method improved forecast accuracy. In the 25 papers with quantitative comparisons, complexity increased forecast errors by an average of 27 percent.\n\nOne justification of Occam's razor is a direct result of basic probability theory. By definition, all assumptions introduce possibilities for error; if an assumption does not improve the accuracy of a theory, its only effect is to increase the probability that the overall theory is wrong.\n\nThere have also been other attempts to derive Occam's razor from probability theory, including notable attempts made by Harold Jeffreys and E. T. Jaynes. The probabilistic (Bayesian) basis for Occam's razor is elaborated by David J. C. MacKay in chapter 28 of his book \"Information Theory, Inference, and Learning Algorithms\", where he emphasizes that a prior bias in favour of simpler models is not required.\n\nWilliam H. Jefferys and James O. Berger (1991) generalize and quantify the original formulation's \"assumptions\" concept as the degree to which a proposition is unnecessarily accommodating to possible observable data. They state, \"A hypothesis with fewer adjustable parameters will automatically have an enhanced posterior probability, due to the fact that the predictions it makes are sharp.\" The model they propose balances the precision of a theory's predictions against their sharpness—preferring theories that sharply make correct predictions over theories that accommodate a wide range of other possible results. This, again, reflects the mathematical relationship between key concepts in Bayesian inference (namely marginal probability, conditional probability, and posterior probability).\n\nThe bias–variance tradeoff is a framework that incorporates the Occam's razor principal in its balance between overfitting (i.e. variance minimization) and underfitting (i.e. bias minimization).\n\nKarl Popper argues that a preference for simple theories need not appeal to practical or aesthetic considerations. Our preference for simplicity may be justified by its falsifiability criterion: we prefer simpler theories to more complex ones \"because their empirical content is greater; and because they are better testable\" (Popper 1992). The idea here is that a simple theory applies to more cases than a more complex one, and is thus more easily falsifiable. This is again comparing a simple theory to a more complex theory where both explain the data equally well.\n\nThe philosopher of science Elliott Sober once argued along the same lines as Popper, tying simplicity with \"informativeness\": The simplest theory is the more informative, in the sense that it requires less information to a question. He has since rejected this account of simplicity, purportedly because it fails to provide an epistemic justification for simplicity. He now believes that simplicity considerations (and considerations of parsimony in particular) do not count unless they reflect something more fundamental. Philosophers, he suggests, may have made the error of hypostatizing simplicity (i.e., endowed it with a \"sui generis\" existence), when it has meaning only when embedded in a specific context (Sober 1992). If we fail to justify simplicity considerations on the basis of the context in which we use them, we may have no non-circular justification: \"Just as the question 'why be rational?' may have no non-circular answer, the same may be true of the question 'why should simplicity be considered in evaluating the plausibility of hypotheses?'\"\n\nRichard Swinburne argues for simplicity on logical grounds:\n\nAccording to Swinburne, since our choice of theory cannot be determined by data (see Underdetermination and Duhem-Quine thesis), we must rely on some criterion to determine which theory to use. Since it is absurd to have no logical method for settling on one hypothesis amongst an infinite number of equally data-compliant hypotheses, we should choose the simplest theory: \"Either science is irrational [in the way it judges theories and predictions probable] or the principle of simplicity is a fundamental synthetic a priori truth.\" (Swinburne 1997).\n\nFrom the \"Tractatus Logico-Philosophicus\":\n\n\nand on the related concept of \"simplicity\":\n\n\nIn science, Occam's razor is used as a heuristic to guide scientists in developing theoretical models rather than as an arbiter between published models. In physics, parsimony was an important heuristic in Albert Einstein's formulation of special relativity, in the development and application of the principle of least action by Pierre Louis Maupertuis and Leonhard Euler, and in the development of quantum mechanics by Max Planck, Werner Heisenberg and Louis de Broglie.\n\nIn chemistry, Occam's razor is often an important heuristic when developing a model of a reaction mechanism. Although it is useful as a heuristic in developing models of reaction mechanisms, it has been shown to fail as a criterion for selecting among some selected published models. In this context, Einstein himself expressed caution when he formulated Einstein's Constraint: \"It can scarcely be denied that the supreme goal of all theory is to make the irreducible basic elements as simple and as few as possible without having to surrender the adequate representation of a single datum of experience\". An often-quoted version of this constraint (which cannot be verified as posited by Einstein himself) says \"Everything should be kept as simple as possible, but not simpler.\"\n\nIn the scientific method, parsimony is an epistemological, metaphysical or heuristic preference, not an irrefutable principle of logic or a scientific result. As a logical principle, Occam's razor would demand that scientists accept the simplest possible theoretical explanation for existing data. However, science has shown repeatedly that future data often support more complex theories than do existing data. Science prefers the simplest explanation that is consistent with the data available at a given time, but the simplest explanation may be ruled out as new data become available. That is, science is open to the possibility that future experiments might support more complex theories than demanded by current data and is more interested in designing experiments to discriminate between competing theories than favoring one theory over another based merely on philosophical principles.\n\nWhen scientists use the idea of parsimony, it has meaning only in a very specific context of inquiry. Several background assumptions are required for parsimony to connect with plausibility in a particular research problem. The reasonableness of parsimony in one research context may have nothing to do with its reasonableness in another. It is a mistake to think that there is a single global principle that spans diverse subject matter.\n\nIt has been suggested that Occam's razor is a widely accepted example of extraevidential consideration, even though it is entirely a metaphysical assumption. There is little empirical evidence that the world is actually simple or that simple accounts are more likely to be true than complex ones.\n\nMost of the time, Occam's razor is a conservative tool, cutting out \"crazy, complicated constructions\" and assuring \"that hypotheses are grounded in the science of the day\", thus yielding \"normal\" science: models of explanation and prediction. There are, however, notable exceptions where Occam's razor turns a conservative scientist into a reluctant revolutionary. For example, Max Planck interpolated between the Wien and Jeans radiation laws and used Occam's razor logic to formulate the quantum hypothesis, even resisting that hypothesis as it became more obvious that it was correct.\n\nAppeals to simplicity were used to argue against the phenomena of meteorites, ball lightning, continental drift, and reverse transcriptase. One can argue for atomic building blocks for matter, because it provides a simpler explanation for the observed reversibility of both mixing and chemical reactions as simple separation and rearrangements of atomic building blocks. At the time, however, the atomic theory was considered more complex because it implied the existence of invisible particles that had not been directly detected. Ernst Mach and the logical positivists rejected John Dalton's atomic theory until the reality of atoms was more evident in Brownian motion, as shown by Albert Einstein.\n\nIn the same way, postulating the aether is more complex than transmission of light through a vacuum. At the time, however, all known waves propagated through a physical medium, and it seemed simpler to postulate the existence of a medium than to theorize about wave propagation without a medium. Likewise, Newton's idea of light particles seemed simpler than Christiaan Huygens's idea of waves, so many favored it. In this case, as it turned out, neither the wave—nor the particle—explanation alone suffices, as light behaves like waves and like particles.\n\nThree axioms presupposed by the scientific method are realism (the existence of objective reality), the existence of natural laws, and the constancy of natural law. Rather than depend on provability of these axioms, science depends on the fact that they have not been objectively falsified. Occam's razor and parsimony support, but do not prove, these axioms of science. The general principle of science is that theories (or models) of natural law must be consistent with repeatable experimental observations. This ultimate arbiter (selection criterion) rests upon the axioms mentioned above.\n\nThere are examples where Occam's razor would have favored the wrong theory given the available data. Simplicity principles are useful philosophical preferences for choosing a more likely theory from among several possibilities that are all consistent with available data. A single instance of Occam's razor favoring a wrong theory falsifies the razor as a general principle. Michael Lee and others provide cases in which a parsimonious approach does not guarantee a correct conclusion and, if based on incorrect working hypotheses or interpretations of incomplete data, may even strongly support a false conclusion.\n\nIf multiple models of natural law make exactly the same testable predictions, they are equivalent and there is no need for parsimony to choose a preferred one. For example, Newtonian, Hamiltonian and Lagrangian classical mechanics are equivalent. Physicists have no interest in using Occam's razor to say the other two are wrong. Likewise, there is no demand for simplicity principles to arbitrate between wave and matrix formulations of quantum mechanics. Science often does not demand arbitration or selection criteria between models that make the same testable predictions.\n\nBiologists or philosophers of biology use Occam's razor in either of two contexts both in evolutionary biology: the units of selection controversy and systematics. George C. Williams in his book \"Adaptation and Natural Selection\" (1966) argues that the best way to explain altruism among animals is based on low-level (i.e., individual) selection as opposed to high-level group selection. Altruism is defined by some evolutionary biologists (e.g., R. Alexander, 1987; W. D. Hamilton, 1964) as behavior that is beneficial to others (or to the group) at a cost to the individual, and many posit individual selection as the mechanism that explains altruism solely in terms of the behaviors of individual organisms acting in their own self-interest (or in the interest of their genes, via kin selection). Williams was arguing against the perspective of others who propose selection at the level of the group as an evolutionary mechanism that selects for altruistic traits (e.g., D. S. Wilson & E. O. Wilson, 2007). The basis for Williams' contention is that of the two, individual selection is the more parsimonious theory. In doing so he is invoking a variant of Occam's razor known as Morgan's Canon: \"In no case is an animal activity to be interpreted in terms of higher psychological processes, if it can be fairly interpreted in terms of processes which stand lower in the scale of psychological evolution and development.\" (Morgan 1903).\n\nHowever, more recent biological analyses, such as Richard Dawkins' \"The Selfish Gene\", have contended that Morgan's Canon is not the simplest and most basic explanation. Dawkins argues the way evolution works is that the genes propagated in most copies end up determining the development of that particular species, i.e., natural selection turns out to select specific genes, and this is really the fundamental underlying principle that automatically gives individual and group selection as emergent features of evolution.\n\nZoology provides an example. Muskoxen, when threatened by wolves, form a circle with the males on the outside and the females and young on the inside. This is an example of a behavior by the males that seems to be altruistic. The behavior is disadvantageous to them individually but beneficial to the group as a whole and was thus seen by some to support the group selection theory. Another interpretation is kin selection: if the males are protecting their offspring, they are protecting copies of their own alleles. Engaging in this behavior would be favored by individual selection if the cost to the male musk ox is less than half of the benefit received by his calf – which could easily be the case if wolves have an easier time killing calves than adult males. It could also be the case that male musk oxen would be individually less likely to be killed by wolves if they stood in a circle with their horns pointing out, regardless of whether they were protecting the females and offspring. That would be an example of regular natural selection – a phenomenon called \"the selfish herd\".\n\nSystematics is the branch of biology that attempts to establish patterns of genealogical relationship among biological taxa. It is also concerned with their classification. There are three primary camps in systematics: cladists, pheneticists, and evolutionary taxonomists. The cladists hold that genealogy alone should determine classification, pheneticists contend that overall similarity is the determining criterion, while evolutionary taxonomists say that both genealogy and similarity count in classification.\n\nIt is among the cladists that Occam's razor is to be found, although their term for it is \"cladistic parsimony\". Cladistic parsimony (or maximum parsimony) is a method of phylogenetic inference in the construction of types of phylogenetic trees (more specifically, cladograms). Cladograms are branching, tree-like structures used to represent hypotheses of relative degree of relationship, based on shared, derived character states. Cladistic parsimony is used to select as the preferred hypothesis of relationships the cladogram that requires the fewest implied character state transformations. Critics of the cladistic approach often observe that for some types of tree, parsimony consistently produces the wrong results, regardless of how much data is collected (this is called statistical inconsistency, or long branch attraction). However, this criticism is also potentially true for any type of phylogenetic inference, unless the model used to estimate the tree reflects the way that evolution actually happened. Because this information is not empirically accessible, the criticism of statistical inconsistency against parsimony holds no force. For a book-length treatment of cladistic parsimony, see Elliott Sober's \"Reconstructing the Past: Parsimony, Evolution, and Inference\" (1988). For a discussion of both uses of Occam's razor in biology, see Sober's article \"Let's Razor Ockham's Razor\" (1990).\n\nOther methods for inferring evolutionary relationships use parsimony in a more traditional way. Likelihood methods for phylogeny use parsimony as they do for all likelihood tests, with hypotheses requiring few differing parameters (i.e., numbers of different rates of character change or different frequencies of character state transitions) being treated as null hypotheses relative to hypotheses requiring many differing parameters. Thus, complex hypotheses must predict data much better than do simple hypotheses before researchers reject the simple hypotheses. Recent advances employ information theory, a close cousin of likelihood, which uses Occam's razor in the same way.\n\nFrancis Crick has commented on potential limitations of Occam's razor in biology. He advances the argument that because biological systems are the products of (an ongoing) natural selection, the mechanisms are not necessarily optimal in an obvious sense. He cautions: \"While Ockham's razor is a useful tool in the physical sciences, it can be a very dangerous implement in biology. It is thus very rash to use simplicity and elegance as a guide in biological research.\"\n\nIn biogeography, parsimony is used to infer ancient migrations of species or populations by observing the geographic distribution and relationships of existing organisms. Given the phylogenetic tree, ancestral migrations are inferred to be those that require the minimum amount of total movement.\n\nIn the philosophy of religion, Occam's razor is sometimes applied to the existence of God. William of Ockham himself was a Christian. He believed in God, and in the authority of Scripture; he writes that \"nothing ought to be posited without a reason given, unless it is self-evident (literally, known through itself) or known by experience or proved by the authority of Sacred Scripture.\" Ockham believed that an explanation has no sufficient basis in reality when it does not harmonize with reason, experience, or the Bible. However, unlike many theologians of his time, Ockham did not believe God could be logically proven with arguments. To Ockham, science was a matter of discovery, but theology was a matter of revelation and faith. He states: \"only faith gives us access to theological truths. The ways of God are not open to reason, for God has freely chosen to create a world and establish a way of salvation within it apart from any necessary laws that human logic or rationality can uncover.\"\n\nSt. Thomas Aquinas, in the \"Summa Theologica\", uses a formulation of Occam's razor to construct an objection to the idea that God exists, which he refutes directly with a counterargument:\n\nFurther, it is superfluous to suppose that what can be accounted for by a few principles has been produced by many. But it seems that everything we see in the world can be accounted for by other principles, supposing God did not exist. For all natural things can be reduced to one principle which is nature; and all voluntary things can be reduced to one principle which is human reason, or will. Therefore there is no need to suppose God's existence.\n\nIn turn, Aquinas answers this with the \"quinque viae\", and addresses the particular objection above with the following answer:\n\nSince nature works for a determinate end under the direction of a higher agent, whatever is done by nature must needs be traced back to God, as to its first cause. So also whatever is done voluntarily must also be traced back to some higher cause other than human reason or will, since these can change or fail; for all things that are changeable and capable of defect must be traced back to an immovable and self-necessary first principle, as was shown in the body of the Article.\n\nRather than argue for the necessity of a god, some theists base their belief upon grounds independent of, or prior to, reason, making Occam's razor irrelevant. This was the stance of Søren Kierkegaard, who viewed belief in God as a leap of faith that sometimes directly opposed reason. This is also the doctrine of Gordon Clark's presuppositional apologetics, with the exception that Clark never thought the leap of faith was contrary to reason (see also Fideism).\n\nVarious arguments in favor of God establish God as a useful or even necessary assumption. Contrastingly some anti-theists hold firmly to the belief that assuming the existence of God introduces unnecessary complexity (Schmitt 2005, e.g., the Ultimate Boeing 747 gambit).\n\nAnother application of the principle is to be found in the work of George Berkeley (1685–1753). Berkeley was an idealist who believed that all of reality could be explained in terms of the mind alone. He invoked Occam's razor against materialism, stating that matter was not required by his metaphysic and was thus eliminable. One potential problem with this belief is that it's possible, given Berkeley's position, to find solipsism itself more in line with the razor than a God-mediated world beyond a single thinker.\n\nOccam's razor may also be recognized in the apocryphal story about an exchange between Pierre-Simon Laplace and Napoleon. It is said that in praising Laplace for one of his recent publications, the emperor asked how it was that the name of God, which featured so frequently in the writings of Lagrange, appeared nowhere in Laplace's. At that, he is said to have replied, \"It's because I had no need of that hypothesis.\" Though some point to this story as illustrating Laplace's atheism, more careful consideration suggests that he may instead have intended merely to illustrate the power of methodological naturalism, or even simply that the fewer logical premises one assumes, the stronger is one's conclusion.\n\nIn his article \"Sensations and Brain Processes\" (1959), J. J. C. Smart invoked Occam's razor with the aim to justify his preference of the mind-brain identity theory over spirit-body dualism. Dualists state that there are two kinds of substances in the universe: physical (including the body) and spiritual, which is non-physical. In contrast, identity theorists state that everything is physical, including consciousness, and that there is nothing nonphysical. Though it is impossible to appreciate the spiritual when limiting oneself to the physical, Smart maintained that identity theory explains all phenomena by assuming only a physical reality. Subsequently, Smart has been severely criticized for his use (or misuse) of Occam's razor and ultimately retracted his advocacy of it in this context. Paul Churchland (1984) states that by itself Occam's razor is inconclusive regarding duality. In a similar way, Dale Jacquette (1994) stated that Occam's razor has been used in attempts to justify eliminativism and reductionism in the philosophy of mind. Eliminativism is the thesis that the ontology of folk psychology including such entities as \"pain\", \"joy\", \"desire\", \"fear\", etc., are eliminable in favor of an ontology of a completed neuroscience.\n\nIn penal theory and the philosophy of punishment, parsimony refers specifically to taking care in the distribution of punishment in order to avoid excessive punishment. In the utilitarian approach to the philosophy of punishment, Jeremy Bentham's \"parsimony principle\" states that any punishment greater than is required to achieve its end is unjust. The concept is related but not identical to the legal concept of proportionality. Parsimony is a key consideration of the modern restorative justice, and is a component of utilitarian approaches to punishment, as well as the prison abolition movement. Bentham believed that true parsimony would require punishment to be individualised to take account of the sensibility of the individual—an individual more sensitive to punishment should be given a proportionately lesser one, since otherwise needless pain would be inflicted. Later utilitarian writers have tended to abandon this idea, in large part due to the impracticality of determining each alleged criminal's relative sensitivity to specific punishments.\n\nMarcus Hutter's universal artificial intelligence builds upon Solomonoff's mathematical formalization of the razor to calculate the expected value of an action.\n\nThere are various papers in scholarly journals deriving formal versions of Occam's razor from probability theory, applying it in statistical inference, and using it to come up with criteria for penalizing complexity in statistical inference. Papers have suggested a connection between Occam's razor and Kolmogorov complexity.\n\nOne of the problems with the original formulation of the razor is that it only applies to models with the same explanatory power (i.e., it only tells us to prefer the simplest of equally good models). A more general form of the razor can be derived from Bayesian model comparison, which is based on Bayes factors and can be used to compare models that don't fit the observations equally well. These methods can sometimes optimally balance the complexity and power of a model. Generally, the exact Occam factor is intractable, but approximations such as Akaike information criterion, Bayesian information criterion, Variational Bayesian methods, false discovery rate, and Laplace's method are used. Many artificial intelligence researchers are now employing such techniques, for instance through work on Occam Learning or more generally on the Free energy principle.\n\nStatistical versions of Occam's razor have a more rigorous formulation than what philosophical discussions produce. In particular, they must have a specific definition of the term \"simplicity\", and that definition can vary. For example, in the Kolmogorov–Chaitin minimum description length approach, the subject must pick a Turing machine whose operations describe the basic operations \"believed\" to represent \"simplicity\" by the subject. However, one could always choose a Turing machine with a simple operation that happened to construct one's entire theory and would hence score highly under the razor. This has led to two opposing camps: one that believes Occam's razor is objective, and one that believes it is subjective.\n\nThe minimum instruction set of a universal Turing machine requires approximately the same length description across different formulations, and is small compared to the Kolmogorov complexity of most practical theories. Marcus Hutter has used this consistency to define a \"natural\" Turing machine of small size as the proper basis for excluding arbitrarily complex instruction sets in the formulation of razors. Describing the program for the universal program as the \"hypothesis\", and the representation of the evidence as program data, it has been formally proven under Zermelo–Fraenkel set theory that \"the sum of the log universal probability of the model plus the log of the probability of the data given the model should be minimized.\" Interpreting this as minimising the total length of a two-part message encoding model followed by data given model gives us the minimum message length (MML) principle.\n\nOne possible conclusion from mixing the concepts of Kolmogorov complexity and Occam's razor is that an ideal data compressor would also be a scientific explanation/formulation generator. Some attempts have been made to re-derive known laws from considerations of simplicity or compressibility.\n\nAccording to Jürgen Schmidhuber, the appropriate mathematical theory of Occam's razor already exists, namely, Solomonoff's theory of optimal inductive inference and its extensions. See discussions in David L. Dowe's \"Foreword re C. S. Wallace\" for the subtle distinctions between the algorithmic probability work of Solomonoff and the MML work of Chris Wallace, and see Dowe's \"MML, hybrid Bayesian network graphical models, statistical consistency, invariance and uniqueness\" both for such discussions and for (in section 4) discussions of MML and Occam's razor. For a specific example of MML as Occam's razor in the problem of decision tree induction, see Dowe and Needham's \"Message Length as an Effective Ockham's Razor in Decision Tree Induction\".\n\nOccam's razor is not an embargo against the positing of any kind of entity, or a recommendation of the simplest theory come what may. Occam's razor is used to adjudicate between theories that have already passed \"theoretical scrutiny\" tests and are equally well-supported by evidence. Furthermore, it may be used to prioritize empirical testing between two equally plausible but unequally testable hypotheses; thereby minimizing costs and wastes while increasing chances of falsification of the simpler-to-test hypothesis.\n\nAnother contentious aspect of the razor is that a theory can become more complex in terms of its structure (or syntax), while its ontology (or semantics) becomes simpler, or vice versa. Quine, in a discussion on definition, referred to these two perspectives as \"economy of practical expression\" and \"economy in grammar and vocabulary\", respectively.\n\nGalileo Galilei lampooned the \"misuse\" of Occam's razor in his \"Dialogue\". The principle is represented in the dialogue by Simplicio. The telling point that Galileo presented ironically was that if one really wanted to start from a small number of entities, one could always consider the letters of the alphabet as the fundamental entities, since one could construct the whole of human knowledge out of them.\n\nOccam's razor has met some opposition from people who have considered it too extreme or rash. Walter Chatton (c. 1290–1343) was a contemporary of William of Ockham (c. 1287–1347) who took exception to Occam's razor and Ockham's use of it. In response he devised his own \"anti-razor:\" \"If three things are not enough to verify an affirmative proposition about things, a fourth must be added, and so on.\" Although there have been a number of philosophers who have formulated similar anti-razors since Chatton's time, no one anti-razor has perpetuated in as much notability as Chatton's anti-razor, although this could be the case of the Late Renaissance Italian motto of unknown attribution \"Se non è vero, è ben trovato\" (\"Even if it is not true, it is well conceived\") when referred to a particularly artful explanation.\n\nAnti-razors have also been created by Gottfried Wilhelm Leibniz (1646–1716), Immanuel Kant (1724–1804), and Karl Menger (1902–1985). Leibniz's version took the form of a principle of plenitude, as Arthur Lovejoy has called it: the idea being that God created the most varied and populous of possible worlds. Kant felt a need to moderate the effects of Occam's razor and thus created his own counter-razor: \"The variety of beings should not rashly be diminished.\"\n\nKarl Menger found mathematicians to be too parsimonious with regard to variables, so he formulated his Law Against Miserliness, which took one of two forms: \"Entities must not be reduced to the point of inadequacy\" and \"It is vain to do with fewer what requires more.\" A less serious but (some might say) even more extremist anti-razor is 'Pataphysics, the \"science of imaginary solutions\" developed by Alfred Jarry (1873–1907). Perhaps the ultimate in anti-reductionism, \"'Pataphysics seeks no less than to view each event in the universe as completely unique, subject to no laws but its own.\" Variations on this theme were subsequently explored by the Argentine writer Jorge Luis Borges in his story/mock-essay \"Tlön, Uqbar, Orbis Tertius\". There is also Crabtree's Bludgeon, which cynically states that \"[n]o set of mutually inconsistent observations can exist for which some human intellect cannot conceive a coherent explanation, however complicated.\"\n\n\n"}
{"id": "19777249", "url": "https://en.wikipedia.org/wiki?curid=19777249", "title": "Organizing principle", "text": "Organizing principle\n\nAn organizing principle is a core assumption from which everything else by proximity can derive a classification or a value. It is like a central reference point that allows all other objects to be located, often used in a conceptual framework. Having an organizing principle might help one simplify and get a handle on a particularly complicated domain or phenomenon. On the other hand, it might create a deceptive prism that colors one's judgment.\n\n\n\n"}
{"id": "1005874", "url": "https://en.wikipedia.org/wiki?curid=1005874", "title": "Principle", "text": "Principle\n\nA principle is a concept or value that is a guide for behavior or evaluation. In law, it is a rule that has to be or usually is to be followed, or can be desirably followed, or is an inevitable consequence of something, such as the laws observed in nature or the way that a system is constructed. The principles of such a system are understood by its users as the essential characteristics of the system, or reflecting system's designed purpose, and the effective operation or use of which would be impossible if any one of the principles was to be ignored. A system may be explicitly based on and implemented from a document of principles as was done in IBM's 360/370 \"Principles of Operation\".\n\nExamples of principles are, entropy in a number of fields, least action in physics, those in descriptive comprehensive and fundamental law: doctrines or assumptions forming normative rules of conduct, separation of church and state in statecraft, the central dogma of molecular biology, fairness in ethics, etc.\n\nIn common English, it is a substantive and collective term referring to rule governance, the absence of which, being \"unprincipled\", is considered a character defect. It may also be used to declare that a reality has diverged from some ideal or norm as when something is said to be true only \"in principle\" but not in fact.\n\nA principle represents values that orient and rule the conduct of persons in a particular society. To \"act on principle\" is to act in accordance with one's moral ideals. Principles are absorbed in childhood through a process of socialization. There is a presumption of liberty of individuals that is restrained. Exemplary principles include First, do no harm, the golden rule and the doctrine of the mean.\n\nIt represents a set of values that inspire the written norms that organize the life of a society submitting to the powers of an authority, generally the State. The law establishes a legal obligation, in a coercive way; it therefore acts as principle conditioning of the action that limits the liberty of the individuals. See, for examples, the territorial principle, homestead principle, and precautionary principle.\n\nArchimedes principle, relating buoyancy to the weight of displaced water, is an early example of a law in science. Another early one developed by Malthus is the \"population principle\", now called the Malthusian principle. Freud also wrote on principles, especially the reality principle necessary to keep the id and pleasure principle in check. Biologists use the principle of priority and principle of Binominal nomenclature for precision in naming species. There are many principles observed in physics, notably in cosmology which observes the mediocrity principle, the anthropic principle, the principle of relativity and the cosmological principle. Other well-known principles include the uncertainty principle in quantum mechanics and the pigeonhole principle and superposition principle in mathematics.\n\nThe principle states that every event has a rational explanation. The principle has a variety of expressions, all of which are perhaps best summarized by the following:\n\nHowever, one realizes that in every sentence there is a direct relation between the predicate and the subject. To say that \"the Earth is round\", corresponds to a direct relation between the subject and the predicate.\n\nAccording to Aristotle, “It is impossible for the same thing to belong and not to belong at the same time to the same thing and in the same respect.” For example, it is not possible that in exactly the same moment and place, it rains and doesn't rain.\n\nThe principle of the excluding third or \"principium tertium exclusum\" is a principle of the traditional logic formulated canonically by Leibniz as: either \"A\" is \"B\" or \"A\" isn't \"B\". It is read the following way: either \"P\" is true, or its denial ¬\"P\" is.\nIt is also known as \"tertium non datur\" ('A third (thing) is not). Classically it is considered to be one of the most important fundamental principles or laws of thought (along with the principles of identity, no contradiction and sufficient reason).\n"}
{"id": "6394087", "url": "https://en.wikipedia.org/wiki?curid=6394087", "title": "Revelation principle", "text": "Revelation principle\n\nThe revelation principle is a fundamental principle in mechanism design. It states that if a social choice function can be implemented by an arbitrary mechanism (i.e. if that mechanism has an equilibrium outcome that corresponds to the outcome of the social choice function), then the same function can be implemented by an incentive-compatible-direct-mechanism (i.e. in which players truthfully report type) with the same equilibrium outcome (payoffs).\n\nIn mechanism design, the revelation principle is of utmost importance in finding solutions. The researcher need only look at the set of equilibrium characterized by incentive compatibility. That is, if the mechanism designer wants to implement some outcome or property, he can restrict his search to mechanisms in which agents are willing to reveal their private information to the mechanism designer that has that outcome or property. If no such direct and truthful mechanism exists, no mechanism can implement this outcome/property. By narrowing the area needed to be searched, the problem of finding a mechanism becomes much easier.\n\nThe principle comes in two variants corresponding to the two flavors of incentive-compatibility:\n\nConsider the following example. There is a certain item that Alice values as formula_1 and Bob values as formula_2. The government needs to decide who will receive that item and in what terms. \n\nSuppose we have an arbitrary mechanism Mech that implements Soc.\n\nWe construct a direct mechanism Mech' that is truthful and implements Soc.\n\nMech' simply simulates the equilibrium strategies of the players in Game(Mech). I.e:\n\nReporting the true valuations in Mech' is like playing the equilibrium strategies in Mech. Hence, reporting the true valuations is a Nash equilibrium in Mech', as desired. Moreover, the equilibrium payoffs are the same, as desired.\n\nThe revelation principle says that for every arbitrary \"coordinating device\" a.k.a. correlating there exists another direct device for which the state space equals the action space of each player. Then the coordination is done by directly informing each player of his action.\n\n"}
{"id": "39396833", "url": "https://en.wikipedia.org/wiki?curid=39396833", "title": "Self-consistency principle in high energy Physics", "text": "Self-consistency principle in high energy Physics\n\nThe self-consistency principle was established by Rolf Hagedorn in 1965 to explain the thermodynamics of fireballs in high energy physics collisions. A thermodynamical approach to the high energy collisions first proposed by E. Fermi.\n\nThe partition function of the fireballs can be written in two forms, one in terms of its density of states, formula_1, and the other in terms of its mass spectrum, formula_2.\n\nThe self-consistency principle says that both forms must be asymptotically equivalent for energies or masses sufficiently high (asymptotic limit). Also, the density of states and the mass spectrum must be asymptotically equivalent in the sense of the weak constraint proposed by Hagedorn as\n\nThese two conditions are known as the \"self-consistency principle\" or \"bootstrap-idea\". After a long mathematical analysis Hagedorn was able to prove that there is in fact formula_4 and formula_5 satisfying the above conditions, resulting in\n\nand\n\nwith formula_8 and formula_9 related by\n\nThen the asymptotic partition function is given by\n\nwhere a singularity is clearly observed for formula_12 →formula_13. This singularity determines the limiting temperature formula_14 in Hagedorn's theory, which is also known as Hagedorn temperature.\n\nHagedorn was able not only to give a simple explanation for the thermodynamical aspect of high energy particle production, but also worked out a formula for the hadronic mass spectrum and predicted the limiting temperature for hot hadronic systems.\n\nAfter some time this limiting temperature was shown by N. Cabibbo and G. Parisi to be related to a phase transition, which characterizes by the deconfinement of quarks at high energies. The mass spectrum was further analyzed by Steven Frautschi.\n\nThe Hagedorn theory was able to describe correctly the experimental data from collision with center-of-mass energies up to approximately 10 GeV, but above this region it failed. In 2000 I. Bediaga, E. M. F. Curado and J. M. de Miranda proposed a phenomenological generalization of Hagedorn's theory by replacing the exponential function that appears in the partition function by the q-exponential function from the Tsallis non-extensive statistics. With this modification the generalized theory was able again to describe the extended experimental data.\n\nIn 2012 A. Deppman proposed a non-extensive self-consistent thermodynamical theory that includes the self-consistency principle and the non-extensive statistics. This theory gives as result the same formula proposed by Bediaga et al., which describes correctly the high energy data, but also new formulas for the mass spectrum and density of states of fireball. It also predicts a new limiting temperature and a limiting entropic index.\n\n"}
{"id": "244755", "url": "https://en.wikipedia.org/wiki?curid=244755", "title": "Sense and reference", "text": "Sense and reference\n\nIn the philosophy of language, the distinction between sense and reference was an innovation of the German philosopher and mathematician Gottlob Frege in 1892 (in his paper \"On Sense and Reference\"; German: \"Über Sinn und Bedeutung\"), reflecting the two ways he believed a singular term may have meaning.\n\nThe reference (or \"referent\"; \"Bedeutung\") of a proper name is the object it means or indicates (\"bedeuten\"), its sense (\"Sinn\") is what the name expresses. The reference of a sentence is its truth value, its sense is the thought that it expresses. Frege justified the distinction in a number of ways.\n\nMuch of analytic philosophy is traceable to Frege's philosophy of language. Frege's views on logic (i.e., his idea that some parts of speech are complete by themselves, and are analogous to the arguments of a mathematical function) led to his views on a theory of reference.\n\nFrege developed his original theory of meaning in early works like \"Begriffsschrift\" ('concept script') of 1879 and \"Grundlagen\" ('foundations of arithmetic') of 1884. On this theory, the meaning of a complete sentence consists in its being true or false, and the meaning of each significant expression in the sentence is an extralinguistic entity which Frege called its \"Bedeutung\", literally 'meaning' or 'significance', but rendered by Frege's translators as 'reference', 'referent', \"'M\"eaning', 'nominatum', etc. Frege supposed that some parts of speech are complete by themselves, and are analogous to the arguments of a mathematical function, but that other parts are incomplete, and contain an empty place, by analogy with the function itself. Thus 'Caesar conquered Gaul' divides into the complete term 'Caesar', whose reference is Caesar himself, and the incomplete term '—conquered Gaul', whose reference is a Concept. Only when the empty place is filled by a proper name does the reference of the completed sentence – its truth value – appear. This early theory of meaning explains how the significance or reference of a sentence (its truth value) depends on the significance or reference of its parts.\n\nFrege introduced the notion of \"sense\" (German: \"Sinn\") to accommodate difficulties in his early theory of meaning.\n\nFirst, if the entire significance of a sentence consists of its truth value, it follows that the sentence will have the same significance if we replace a word of the sentence with one having an identical reference, as this will not change its truth value. The reference of the whole is determined by the reference of the parts. If \"the evening star\" has the same reference as \"the morning star\", it follows that \"the evening star is a body illuminated by the Sun\" has the same truth value as \"the morning star is a body illuminated by the Sun\". But it is possible for someone to think that the first sentence is true while also thinking that the second is false. Therefore, the thought corresponding to each sentence cannot be its reference, but something else, which Frege called its \"sense\".\n\nSecond, sentences that contain proper names with no reference cannot have a truth value at all. Yet the sentence 'Odysseus was set ashore at Ithaca while sound asleep' obviously has a sense, even though 'Odysseus' has no reference. The thought remains the same whether or not 'Odysseus' has a reference. Furthermore, a thought cannot contain the objects that it is about. For example, Mont Blanc, 'with its snowfields', cannot be a component of the thought that Mont Blanc is more than 4,000 metres high. Nor can a thought about Etna contain lumps of solidified lava.\n\nFrege's notion of sense is somewhat obscure, and neo-Fregeans have come up with different candidates for its role. Accounts based on the work of Carnap and Church treat sense as an intension, or a function from possible worlds to extensions. For example, the intension of ‘number of planets’ is a function that maps any possible world to the number of planets in that world. John McDowell supplies cognitive and reference-determining roles. Devitt treats senses as causal-historical chains connecting names to referents.\n\nIn his theory of descriptions, Bertrand Russell held the view that most proper names in ordinary language are in fact disguised definite descriptions. For example, 'Aristotle' can be understood as \"The pupil of Plato and teacher of Alexander,\" or by some other uniquely applying description. This is known as the descriptivist theory of names. Because Frege used definite descriptions in many of his examples, he is often taken to have endorsed the descriptivist theory. Thus Russell's theory of descriptions was conflated with Frege's theory of sense, and for most of the twentieth century this 'Frege-Russell' view was the orthodox view of proper name semantics. However, Saul Kripke argued compellingly against the descriptivist theory. According to Kripke, proper names are rigid designators which designate the same object in every possible world. Descriptions such as 'the President of the U.S. in 1970' do not designate the same in every possible world. For example, someone other than Richard Nixon, e.g. Hubert Humphrey, might have been the President in 1970. Hence a description (or cluster of descriptions) cannot be a rigid designator, and thus a proper name cannot \"mean\" the same as a description.\n\nHowever, the Russellian descriptivist reading of Frege has been rejected by many scholars, in particular by Gareth Evans in \"The Varieties of Reference\" and by John McDowell in \"The Sense and Reference of a Proper Name,\" following Michael Dummett, who argued that Frege's notion of sense should not be equated with a description. Evans further developed this line, arguing that a sense without a referent was not possible. He and McDowell both take the line that Frege's discussion of empty names, and of the idea of sense without reference, are inconsistent, and that his apparent endorsement of descriptivism rests only on a small number of imprecise and perhaps offhand remarks. And both point to the power that the sense-reference distinction \"does\" have (i.e., to solve at least the first two problems), even if it is not given a descriptivist reading.\n\nAs noted above, translators of Frege have rendered the German \"Bedeutung\" in various ways. The term 'reference' has been the most widely adopted, but this fails to capture the meaning of the original German ('meaning' or 'significance'), and does not reflect the decision to standardise key terms across different editions of Frege's works published by Blackwell. The decision was based on the principle of exegetical neutrality, namely that 'if at any point in a text there is a passage that raises for the native speaker legitimate questions of exegesis, then, if at all possible, a translator should strive to confront the reader of his version with the same questions of exegesis and not produce a version which in his mind resolves those questions'. The term 'meaning' best captures the standard German meaning of \"Bedeutung\", and Frege's own use of the term sounds as odd when translated into English as it does in German. Moreover, 'meaning' captures Frege's early use of \"Bedeutung\" well, and it would be problematic to translate Frege's early use as 'meaning' and his later use as 'reference', suggesting a change in terminology not evident in the original German.\n\nThe Greek philosopher Antisthenes, a pupil of Socrates, apparently distinguished \"a general object that can be aligned with the meaning of the utterance” from “a particular object of extensional reference.\" This \"suggests that he makes a distinction between sense and reference.\" \nThe principal basis of this claim is a quotation in Alexander of Aphrodisias's “Comments on Aristotle's 'Topics'” with a three-way distinction: \n\nThe sense-reference distinction is commonly confused with that between connotation and denotation, which originates with John Stuart Mill. According to Mill, a common term like 'white' \"denotes\" all white things, as snow, paper. But according to Frege, a common term does not refer to any individual white thing, but rather to an abstract Concept (\"Begriff\"). We must distinguish between the relation of reference, which holds between a proper name and the object it refers to, such as between the name 'Earth', and the planet Earth, and the relation of 'falling under', such as when the Earth falls under the concept \"planet\". The relation of a proper name to the object it designates is direct, whereas a word like 'planet' has no such direct relation at all to the Earth at all, but only to a concept that the Earth falls under. Moreover, judging \"of\" anything that it falls under this concept is not in any way part of our knowledge of what the word 'planet' means. The distinction between connotation and denotation is closer to that between Concept and Object, than to that between 'sense' and 'reference'.\n\n"}
