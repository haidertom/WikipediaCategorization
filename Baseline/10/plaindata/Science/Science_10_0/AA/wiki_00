{"id": "838846", "url": "https://en.wikipedia.org/wiki?curid=838846", "title": "Concept inventory", "text": "Concept inventory\n\nA concept inventory is a criterion-referenced test designed to help determine whether a student has an accurate working knowledge of a specific set of concepts. Historically, concept inventories have been in the form of multiple-choice tests in order to aid interpretability and facilitate administration in large classes. Unlike a typical, teacher-authored multiple-choice test, questions and response choices on concept inventories are the subject of extensive research. The aims of the research include ascertaining (a) the range of what individuals think a particular question is asking and (b) the most common responses to the questions. Concept inventories are evaluated to ensure test reliability and validity. In its final form, each question includes one correct answer and several distractors.\n\nIdeally, a score on a criterion-referenced test reflects the amount of content knowledge a student has mastered. Criterion-referenced tests differ from norm-referenced tests in that (in theory) the former is not used to compare an individual's score to the scores of the group. Ordinarily, the purpose of a criterion-referenced test is to ascertain whether a student mastered a predetermined amount of content knowledge; upon obtaining a test score that is at or above a cutoff score, the student can move on to study a body of content knowledge that follows next in a learning sequence. In general, item difficulty values ranging between 30% and 70% are best able to provide information about student understanding.\n\nThe distractors are incorrect or irrelevant answers that are usually (but not always) based on students' commonly held misconceptions. Test developers often research student misconceptions by examining students' responses to open-ended essay questions and conducting \"think-aloud\" interviews with students. The distractors chosen by students help researchers understand student thinking and give instructors insights into students' prior knowledge (and, sometimes, firmly held beliefs). This foundation in research underlies instrument construction and design, and plays a role in helping educators obtain clues about students' ideas, scientific misconceptions, and didaskalogenic (\"teacher-induced\" or \"teaching-induced\") confusions and conceptual lacunae that interfere with learning.\n\nConcept inventories are education-related diagnostic tests. In 1985 Halloun and Hestenes introduced a \"multiple-choice mechanics diagnostic test\" to examine students' concepts about motion. It evaluates student understanding of basic concepts in classical (macroscopic) mechanics. A little later, the Force Concept Inventory (FCI), another concept inventory, was developed. The FCI was designed to assess student understanding of the Newtonian concepts of force. Hestenes (1998) found that while \"nearly 80% of the [students completing introductory college physics courses] could state Newton's Third Law at the beginning of the course. FCI data showed that less than 15% of them fully understood it at the end\".These results have been replicated in a number of studies involving students at a range of institutions (see sources section below). That said, there remains questions as what exactly the FCI measures. Results from Hake (1998) using the FCI have led to greater recognition in the science education community of the importance of students' \"interactive engagement\" with the materials to be mastered. .\n\nSince the development of the FCI, other physics instruments have been developed. These include the Force and Motion Conceptual Evaluation developed by Thornton and Sokoloff and the Brief Electricity and Magnetism Assessment developed by Ding et al. For a discussion of how a number of concept inventories were developed see Beichner. Information about physics concept tests can be found at the NC State Physics Education Research Group website (see the external links below).\n\nIn addition to physics, concept inventories have been developed in statistics, chemistry, astronomy, basic biology, natural selection, genetics, engineering, geoscience. and computer science.\n\nIn many areas, foundational scientific concepts transcend disciplinary boundaries. An example of an inventory that assesses knowledge of such concepts is an instrument developed by Odom and Barrow (1995) to evaluate understanding of diffusion and osmosis. In addition, there are non-multiple choice conceptual instruments, such as the essay-based approach suggested by Wright et al. (1998) and the essay and oral exams used by Nehm and Schonfeld (2008). and Cooper et al to measure student understanding of Lewis structures in chemistry.\n\nSome concept inventories are problematic. The concepts tested may not be fundamental or important in a particular discipline, the concepts involved may not be explicitly taught in a class or curriculum, or answering a question correctly may require only a superficial understanding of a topic. It is therefore possible to either over-estimate or under-estimate student content mastery. While concept inventories designed to identify trends in student thinking may not be useful in monitoring learning gains as a result of pedagogical interventions, disciplinary mastery may not be the variable measured by a particular instrument. Users should be careful to ensure that concept inventories are actually testing conceptual understanding, rather than test-taking ability, language skills, or other abilities that can influence test performance.\n\nThe use of multiple-choice exams as concept inventories is not without controversy. The very structure of multiple-choice type concept inventories raises questions involving the extent to which complex, and often nuanced situations and ideas must be simplified or clarified to produce unambiguous responses. For example, a multiple-choice exam designed to assess knowledge of key concepts in natural selection does not meet a number of standards of quality control. One problem with the exam is that the two members of each of several pairs of parallel items, with each pair designed to measure exactly one key concept in natural selection, sometimes have very different levels of difficulty. Another problem is that the multiple-choice exam overestimates knowledge of natural selection as reflected in student performance on a diagnostic essay exam and a diagnostic oral exam, two instruments with reasonably good construct validity. Although scoring concept inventories in the form of essay or oral exams is labor-intensive, costly, and difficult to implement with large numbers of students, such exams can offer a more realistic appraisal of the actual levels of students' conceptual mastery as well as their misconceptions. Recently, however, computer technology has been developed that can score essay responses on concept inventories in biology and other domains (Nehm, Ha, & Mayfield, 2011), promising to facilitate the scoring of concept inventories organized as (transcribed) oral exams as well as essays.\n\n"}
{"id": "14765778", "url": "https://en.wikipedia.org/wiki?curid=14765778", "title": "El Sueño de Arquímedes", "text": "El Sueño de Arquímedes\n\nEl Sueño de Arquímedes was a Spanish science podcast and radio program which was broadcast by Radio Nacional de España (RNE) from September 2006 until June, 2007. The program was created by Ángel Rodríguez Lozano. A total of 35 programs are still available for download. In addition to El Sueño de Arquímedes, Ángel Rodríguez Lozano also hosted Vanguardia de la Ciencia, which was broadcast weekly without interruption from April 1995 until June 2007. The name of the program means \"the dream of Archimedes\", and alludes to Archimedes' statement that given a lever and a fixed point, he could move the world. To Ángel Rodríguez Lozano, the dream was moving the world by popularizing and sharing knowledge.\n\nBefore the startup of El Sueño de Arquímedes, Ángel Rodríguez Lozano had been hosting Vanguardia de la Ciencia for more than a decade. When RNE\nasked if he would host a second popular science program, Ángel Rodríguez Lozano realised that the scheduled timing, Sundays between 3 and 4 p.m, meant that it would be sandwiched between sports broadcasts. Therefore, he would have to capture the attention of listeners who were not looking for this type of program. The intention was therefore to make the program even more accessible than Vanguardia de la Ciencia, with shorter interviews and more music. Ángel Rodríguez Lozano stated that it had been a marvellous experience, and that the response had been tremendous.\n\nThe program included science news, interviews, and biographies of great scientists, written by Carmen Buergo. In the final, humorous section of the program, Ángel Rodríguez Lozano paid a visit to the archetypical mad scientist\nAlejandro Laguna, who supposedly lived and worked in a hidden laboratory in the basements of Radio Nacional de España, seven floors below ground level. Alejandro would demonstrate one of his latest inventions, which usually defied the laws of physics, and Ángel played the role of a rather gullible spectator. Alejandro then explained the physics of the corresponding real-world device. Finally, the demonstration of his invention usually had some highly unpleasant consequence for Ángel.\n\nIn June 2007, El Sueño de Arquímedes and Vanguardia de la Ciencia were abruptly terminated. In the correspondence section of one of the last programs of Vangurardia de la Ciencia, Ángel Rodríguez Lozano explained, in response to a letter from an outraged listener, that the decision to terminate the program was made due to a re-structuration of RNE, and that he was but one of 4,150 employees who had to leave.\nIn the previously referenced interview, he explained that everyone older than 52 years had to retire early, and that he was 54 years old at the time.\n\nThe decision to terminate the programs was widely criticized in Spanish-speaking blogs.\n\nAt the web-site of RNE, 36 programs are still available.\n\n"}
{"id": "2929560", "url": "https://en.wikipedia.org/wiki?curid=2929560", "title": "Hyle", "text": "Hyle\n\nIn philosophy, hyle (; from ) refers to matter or stuff. It can also be the material cause underlying a change in Aristotelian philosophy. The Greeks originally had no word for matter in general, as opposed to raw material suitable for some specific purpose or other, so Aristotle adapted the word for \"wood\" to this purpose. The idea that everything physical is made of the same basic substance holds up well under modern science, although it may be thought of more in terms of energy or matter/energy.\n\nThe matter of hyle is closely related to that of substance, in so far as both endure a change in form, or transformation. Aristotle defined primary substance as that which can neither be predicated nor attributed to something else, and he explained the transformation between the four terrestrial elements in terms of an abstract primary matter that underlies each element due to the four combinations of two properties: hot or cold and wet or dry. He stipulated that transformations between opposing elements, where both properties differ, must be analyzed as two discrete steps wherein one of the two properties changes to its contrary while the other remains unchanged (see essence and hylomorphism).\n\nModern substance theory differs, for example Kant's \"Ding an sich\", or \"thing in itself\", is generally described as whatever is its own cause, or alternatively as a thing whose only property is that it is that thing (or, in other words, that it has only that property). However, this notion is subject to the criticism, as by Nietzsche, that there is no way to \"directly\" prove the existence of any thing which has no properties, since such a thing could not possibly interact with other things and thus would be unobservable and indeterminate.\n\nOn the other hand, we may need to postulate a substance that endures through change in order to explain the nature of change—without an enduring factor that persists through change, there is no change but only a succession of unrelated events. The existence of change is hard to deny, and if we have to postulate something \"unobserved\" in order to explain what \"is\" observed, that is a valid \"indirect\" demonstration (by abductive reasoning). Moreover, something like a prime substance is posited by physics in the form of matter/energy.\n\n"}
{"id": "813041", "url": "https://en.wikipedia.org/wiki?curid=813041", "title": "International HapMap Project", "text": "International HapMap Project\n\nThe International HapMap Project was an organization that aimed to develop a haplotype map (HapMap) of the human genome, to describe the common patterns of human genetic variation. HapMap is used to find genetic variants affecting health, disease and responses to drugs and environmental factors. The information produced by the project is made freely available for research.\n\nThe International HapMap Project is a collaboration among researchers at academic centers, non-profit biomedical research groups and private companies in Canada, China, Japan, Nigeria, the United Kingdom, and the United States. It officially started with a meeting on October 27 to 29, 2002, and was expected to take about three years. It comprises two phases; the complete data obtained in Phase I were published on 27 October 2005. The analysis of the Phase II dataset was published in October 2007. The Phase III dataset was released in spring 2009.\n\nUnlike with the rarer Mendelian diseases, combinations of different genes and the environment play a role in the development and progression of common diseases (such as diabetes, cancer, heart disease, stroke, depression, and asthma), or in the individual response to pharmacological agents. To find the genetic factors involved in these diseases, one could in principle do a genome-wide association study: obtain the complete genetic sequence of several individuals, some with the disease and some without, and then search for differences between the two sets of genomes. At the time, this approach was not feasible because of the cost of full genome sequencing. The HapMap project proposed a shortcut.\n\nAlthough any two unrelated people share about 99.5% of their DNA sequence, their genomes differ at specific nucleotide locations. Such sites are known as single nucleotide polymorphisms (SNPs), and each of the possible resulting gene forms is called an allele. The HapMap project focuses only on common SNPs, those where each allele occurs in at least 1% of the population.\n\nEach person has two copies of all chromosomes, except the sex chromosomes in males. For each SNP, the combination of alleles a person has is called a genotype. Genotyping refers to uncovering what genotype a person has at a particular site. The HapMap project chose a sample of 269 individuals and selected several million well-defined SNPs, genotyped the individuals for these SNPs, and published the results.\n\nThe alleles of nearby SNPs on a single chromosome are correlated. Specifically, if the allele of one SNP for a given individual is known, the alleles of nearby SNPs can often be predicted. This is because each SNP arose in evolutionary history as a single point mutation, and was then passed down on the chromosome surrounded by other, earlier, point mutations. SNPs that are separated by a large distance on the chromosome are typically not very well correlated, because recombination occurs in each generation and mixes the allele sequences of the two chromosomes. A sequence of consecutive alleles on a particular chromosome is known as a haplotype.\n\nTo find the genetic factors involved in a particular disease, one can proceed as follows. First a certain region of interest in the genome is identified, possibly from earlier inheritance studies. In this region one locates a set of tag SNPs from the HapMap data; these are SNPs that are very well correlated with all the other SNPs in the region. Thus, learning the alleles of the tag SNPs in an individual will determine the individual's haplotype with high probability. Next, one determines the genotype for these tag SNPs in several individuals, some with the disease and some without. By comparing the two groups, one determines the likely locations and haplotypes that are involved in the disease.\n\nHaplotypes are generally shared between populations, but their frequency can differ widely. Four populations were selected for inclusion in the HapMap: 30 adult-and-both-parents Yoruba trios from Ibadan, Nigeria (YRI), 30 trios of Utah residents of northern and western European ancestry (CEU), 44 unrelated Japanese individuals from Tokyo, Japan (JPT) and 45 unrelated Han Chinese individuals from Beijing, China (CHB). Although the haplotypes revealed from these populations should be useful for studying many other populations, parallel studies are currently examining the usefulness of including additional populations in the project.\n\nAll samples were collected through a community engagement process with appropriate informed consent. The community engagement process was designed to identify and attempt to respond to culturally specific concerns and give participating communities input into the informed consent and sample collection processes.\n\nIn phase III, 11 global ancestry groups have been assembled: ASW (African ancestry in Southwest USA); CEU (Utah residents with Northern and Western European ancestry from the CEPH collection); CHB (Han Chinese in Beijing, China); CHD (Chinese in Metropolitan Denver, Colorado); GIH (Gujarati Indians in Houston, Texas); JPT (Japanese in Tokyo, Japan); LWK (Luhya in Webuye, Kenya); MEX (Mexican ancestry in Los Angeles, California); MKK (Maasai in Kinyawa, Kenya); TSI (Tuscans in Italy); YRI (Yoruba in Ibadan, Nigeria).\n\nThree combined panels have also been created, which allow better identification of SNPs in groups outside the nine homogenous samples: CEU+TSI (Combined panel of Utah residents with Northern and Western European ancestry from the CEPH collection and Tuscans in Italy); JPT+CHB (Combined panel of Japanese in Tokyo, Japan and Han Chinese in Beijing, China) and JPT+CHB+CHD (Combined panel of Japanese in Tokyo, Japan, Han Chinese in Beijing, China and Chinese in Metropolitan Denver, Colorado). CEU+TSI, for instance, is a better model of UK British individuals than is CEU alone.\n\nFor the Phase I, one common SNP was genotyped every 5,000 bases. Overall, more than one million SNPs were genotyped. The genotyping was carried out by 10 centres using five different genotyping technologies. Genotyping quality was assessed by using duplicate or related samples and by having periodic quality checks where centres had to genotype common sets of SNPs.\n\nThe Canadian team was led by Thomas J. Hudson at McGill University in Montreal and focused on chromosomes 2 and 4p. The Chinese team was led by Huanming Yang with centres in Beijing, Shanghai and Hong Kong and focused on chromosomes 3, 8p and 21. The Japanese team was led by Yusuke Nakamura at the University of Tokyo and focused on chromosomes 5, 11, 14, 15, 16, 17 and 19. The British team was led by David R. Bentley at the Sanger Institute and focused on chromosomes 1, 6, 10, 13 and 20. There were four United States' genotyping centres: a team led by Mark Chee and Arnold Oliphant at Illumina Inc. in San Diego (studying chromosomes 8q, 9, 18q, 22 and X), a team led by David Altshuler and Mark Daly at the Broad Institute in Cambridge, USA (chromosomes 4q, 7q, 18p, Y and mitochondrion), a team led by Richard Gibbs at the Baylor College of Medicine in Houston (chromosome 12), and a team led by Pui-Yan Kwok at the University of California, San Francisco (chromosome 7p).\n\nTo obtain enough SNPs to create the Map, the Consortium funded a large re-sequencing project to discover millions of additional SNPs. These were submitted to the public dbSNP database. As a result, by August 2006, the database included more than ten million SNPs, and more than 40% of them were known to be polymorphic. By comparison, at the start of the project, fewer than 3 million SNPs were identified, and no more than 10% of them were known to be polymorphic.\n\nDuring Phase II, more than two million additional SNPs have been genotyped throughout the genome by the company Perlegen Sciences and 500,000 by the company Affymetrix.\n\nAll of the data generated by the project, including SNP frequencies, genotypes and haplotypes, were placed in the public domain and are available for download. This website also contains a genome browser which allows to find SNPs in any region of interest, their allele frequencies and their association to nearby SNPs. A tool that can determine tag SNPs for a given region of interest is also provided. These data can also be directly accessed from the widely used Haploview program.\n\n\n\n"}
{"id": "33298861", "url": "https://en.wikipedia.org/wiki?curid=33298861", "title": "Iron Science Teacher", "text": "Iron Science Teacher\n\nThe Iron Science Teacher is a national competition that celebrates innovation and creativity in science teaching. The competition originated at the Exploratorium in San Francisco. Parodying the cult Japanese TV program, “Iron Chef,” this competition showcases science teachers as they devise classroom activities using a particular ingredient — an everyday item such as a plastic bag, milk carton, or nail. Contestants are currently or formally part of the Exploratorium's Teacher Institute and compete before a live audience for the title of \"Iron Science Teacher.\" Shows are also archived on the Exploratorium's site.\n\nAstrophysicist Dr. Linda Shore, Director of the Exploratorium Teacher Institute and host of the competition, says one goal of the Iron Science Teacher is to \"provide teachers with ideas about how to teach multimillion dollar state and national science teaching standards using, trash, recyclables, and inexpensive materials\" as well as \"to allow teachers to receive applause for great teaching.\"\n\nBack in 1997, the Exploratorium's Phyllis C. Wattis Webcast Studio was looking for new shows. During a staff brainstorming session, a fan of the popular Food Network television show, The Iron Chef, suggested naming a secret ingredient for science teachers to use in an experiment to present to the audience. \"It was honestly and truly a joke,\" Shore says. \"We thought we'd do one show.\"\n\nNow 10 to 12 shows are produced annually for the Exploratorium's website. \"Secret\" ingredients, which are revealed in advance to participants so they can practice, have included everything from ordinary baking soda and food coloring to Marshmallow Peeps and pantyhose.\n\nThe Canadian Iron Science Teacher also parodies the popular TV series Iron Chef and is hosted by Jay Ingram of Daily Planet on Discovery Channel. Unlike the Exploratorium version, where championship comes with no tangible prize, in the Canadian version, five \"finalist\" teachers, with their support teams, are selected to compete in the Iron Science Teacher finals at the University of Calgary in order to win a variety of cash prizes. \n\nColorado Springs, CO initiated their own CoOL Iron Science Teacher Competition as part of their What If: A Festival of Creativity & Innovation on September 11, 2010.\n\n\"(plus links to the webcasts)\"\n"}
{"id": "24533329", "url": "https://en.wikipedia.org/wiki?curid=24533329", "title": "MovAlyzeR", "text": "MovAlyzeR\n\nMovAlyzeR is a software package for handwriting movement analysis for research and professional applications. Handwriting movements are recorded using a digitizing tablet connected to a computer. MovAlyzeR is used in many different fields ranging from research in kinesiology, psychology, education, geriatrics, neurology, psychiatry, occupational therapy, forensic document examination or questioned document examination, computer science, to educational demonstrations or student projects in these fields.\n\nMovAlyzeR can be customized for many different pen-movement tests, including goal-directed movements, drawing and handwriting up to a full page of text. It can also process scanned handwriting images for use in, e.g., forensic document examination. Immediately after each trial, consistency with the required pen-movement task is verified so that the user can decide to correct or redo a trial. MovAlyzeR can generate animated audiovisual stimuli which can be edited using its Stimulus Editor.\n\nMovAlyzeRx has the same capability as MovAlyzeR except altering a test. It is designed for medical professionals (hence \"Rx\"). The user interface is as simple as possible. No left-clicks are required. The screen layout can be customized. To start testing, just type the patient or participant code.\n\nScriptAlyzeR handwriting analysis software, is a sub-package of MovAlyzeR excluding visual stimuli and sub-movement analysis.\n\nGripAlyzeR is another flavor of MovAlyzeR for bi-manual force coordination using dedicated hardware: Two grip-force units connected via a magnet of programmable force.\n\nThe original code of the software was the result of many years of research in handwriting movements. At the core of the software, the signal analysis algorithms that are used have been developed since 1976 when Dr. Hans-Leo Teulings conducted research into the development of handwriting motor control in children in comparison to children with developmental disorders at the Department of Experimental Psychology at the University of Nijmegen (KUN), in The Netherlands. The department became part of the Nijmegen Institute for Cognition and Information (NICI) and eventually the Centre for Cognition at the Radboud University Nijmegen (RU).\n\nThese signal analysis algorithms were originally coded in Fortran on Digital's PDP11/34 laboratory computers with 54kB of memory. The algorithms use a complex Fast Fourier Transform (FFT) to transform both the x and y signals into frequency domain. This allows low-pass filtering and differentiation with zero-phase and ripple free filtering of both x and y signals simultaneously (Teulings & Maarse, 1984).\n\nThe software was expanded and transcribed into plain C during the European ESPRIT projects: P419 \"Image and Movement Understanding -- IMU\" on Cursive-script recognition (1985–1988) and ESPRIT project P5204 \"Pen And Paper Input Recognition Using Script – PAPYRUS (1991–1992).\n\nThe software was developed further at the Motor Control Laboratory of Arizona State University, USA (1993–1997) for research on Parkinson's disease and aging.\n\nIn 1997, NeuroScript was founded by Dr. Hans-Leo Teulings and Prof. George Stelmach (who has since retired) as a result of a National Institutes of Health (NIH) Small Business and Innovation Research (SBIR) Phase I grant (R43 RR11683: Analysis system for fine motor control) to conduct a feasibility study into the feasibility of a general purpose handwriting movement analysis system for research.\n\nIn 1999, an SBIR Phase II grant was awarded to NeuroScript (R44 RR11683 Analysis system for fine motor control). The aim was to develop this system into a usable product. The result: the MovAlyzeR software was born - named, designed and implemented by Gregory M. Baker who joined NeuroScript in 1999.\n\nIn 2002, NeuroScript received an SBIR Phase II grant (R44 NS39212 Force measurement and analysis system). This enabled NeuroScript to generalize MovAlyzeR for bi-manual force coordination: GripAlyzeR. Instead of x and y movement components and axial pen pressure, GripAlyzeR used left and right grip forces and a lift force.\n\nIn 2002 NeuroScript received another Phase II grant (R44 NS38793: Optimization software for goal-directed movements). MovAlyzeR was expanded with interactive and animated audio-visual stimuli and sub-movement analysis.\n\nIn 2006, a Phase II grant was awarded (R44 MH073192: Movement Analysis to Monitor Medication). MovAlyzeR was tested in several major clinics where hundreds of patients were tested for movement side effects due to schizophrenia medication in addition to conventional clinical evaluation (SAEPS - Simpson-Angus Scale for Extrapyramidal Symptoms and parkinsonism, AIMS – Abnormal Involuntary Movement Scale for tardive dyskinesia, BARS – Barnes Akathisia Rating Scale). Results demonstrated that MovAlyzeR measurements were more sensitive for dosage and medication type than the conventional clinical evaluations.\n\n\nCSWIN by Science and Motion, OASIS by KikoSoft, Pullman Spiral Acquisition and Analysis by Lafayette, NeuroSkill by Verifax, COMpet by University of Haifa, MedDraw by Universities of Kent and Rouen.\n\n\n"}
{"id": "12108220", "url": "https://en.wikipedia.org/wiki?curid=12108220", "title": "On Nature (Anaximander)", "text": "On Nature (Anaximander)\n\nOn Nature was a philosophical poem which details Anaximander's theories about the evolution of the Earth, plants, animals and humankind. Anaximander described his theory that humans and other animals descended from fish once the world's oceans began to dry up. Also he described a theory of abiogenesis in his book in the way that he believed that the first life forms formed from mist. We know little about his book because it has been lost or destroyed, however it still remains important today because it describes one of the world's earliest theories of evolution.\n\n\n"}
{"id": "27006", "url": "https://en.wikipedia.org/wiki?curid=27006", "title": "Serendipity", "text": "Serendipity\n\nSerendipity means an unplanned, fortunate discovery.\n\nThe notion of serendipity is a common occurrence throughout the history of scientific innovation. Examples are Alexander Fleming's accidental discovery of penicillin in 1928, the invention of the microwave oven by Percy Spencer in 1945, and the invention of the Post-it note by Spencer Silver in 1968.\n\nIn June 2004, a British translation company voted the word to be one of the ten English words hardest to translate. However, due to its sociological use, the word has since been exported into many other languages.\n\nThe first noted use of \"serendipity\" in the English language was by Horace Walpole in 1754. In a letter he wrote to his friend Horace Mann, Walpole explained an unexpected discovery he had made about a (lost) painting of Bianca Cappello by Giorgio Vasari by reference to a Persian fairy tale, \"The Three Princes of Serendip\". The princes, he told his correspondent, were \"always making discoveries, by accidents and sagacity, of things which they were not in quest of\". The name comes from \"Serendip\", an old name for Sri Lanka (Ceylon), hence \"Sarandib\" by Arab traders. It is derived from the Sanskrit \"Siṃhaladvīpaḥ\" (Siṃhalaḥ, Sri Lanka + dvīpaḥ, island).\n\nM. E. Graebner describes serendipitous value in the context of the acquisition of a business as \"windfalls that were not anticipated by the buyer prior to the deal\": i.e., unexpected advantages or benefits incurred due to positive synergy effects of the merger.\n\nIkujiro Nonaka points out that the serendipitous quality of innovation is highly recognized by managers and links the success of Japanese enterprises to their ability to create knowledge not by processing information but rather by \"tapping the tacit and often highly subjective insights, intuitions, and hunches of individual employees and making those insights available for testing and use by the company as a whole\".\n\nSerendipity is postulated by Napier and Vuong (2013) as a 'strategic advantage' with which a firm can tap its potential creativity.\n\nSerendipity is a key concept in competitive intelligence because it is one of the tools for avoiding blind spots (see Blindspots analysis).\n\nSerendipity is used as a sociological method in Anselm L. Strauss' and Barney G. Glaser's \"grounded theory\", building on ideas by sociologist Robert K. Merton, who in \"Social Theory and Social Structure\" (1949) referred to the \"serendipity pattern\" as the fairly common experience of observing an unanticipated, anomalous and strategic datum which becomes the occasion for developing a new theory or for extending an existing theory. Merton also co-authored (with Elinor Barber) \"The Travels and Adventures of Serendipity\", which traces the origins and uses of the word \"serendipity\" since it was coined. The book is \"a study in sociological semantics and the sociology of science\", as the subtitle of the book declares. It further develops the idea of serendipity as scientific \"method\" (as juxtaposed with purposeful discovery by experiment or retrospective prophecy).\n\nWilliam Boyd coined the term zemblanity in the late twentieth century to mean somewhat the opposite of serendipity: \"making unhappy, unlucky and unexpected discoveries occurring by design\". A zemblanity is, effectively, an \"unpleasant unsurprise\". It derives from Novaya Zemlya (or Nova Zembla), a cold, barren land with many features opposite to the lush Sri Lanka (Serendip). On this island Willem Barents and his crew were stranded while searching for a new route to the east.\n\nBahramdipity is derived directly from Bahram Gur as characterized in \"The Three Princes of Serendip\". It describes the \"suppression\" of serendipitous discoveries or research results by powerful individuals.\n\n\n\n"}
{"id": "48311968", "url": "https://en.wikipedia.org/wiki?curid=48311968", "title": "Solomon Four Group Design", "text": "Solomon Four Group Design\n\nThe Solomon Four Group Design is a research method that is sometimes used in social science, psychology and medicine. It can be used if there are concerns that the treatment might be sensitized by the pre-test. The four groups have four different experiences:\n\nThe effectiveness of the treatment can be evaluated by comparisons between groups 1 and 2 and between groups 3 and 4.\n"}
{"id": "20612276", "url": "https://en.wikipedia.org/wiki?curid=20612276", "title": "Total balance", "text": "Total balance\n\nTotal balance refers to whole-system scientific methods for adding up the whole effect of things, starting from the concepts of life-cycle analysis (LCA) and using scientific methods to mix physical measures with economic measures and the whole system learning curves of their environmental connections. A balance of future compensations assuming a goal of sustainability is the objective. It is aimed at providing people a true measure of the meaning of their choices. The start is using the energy value of money as a measure of the whole economic system, and its widely distributed average effects.\n\nThe untraceable portion of whole system impacts may be shown to be many times the scale of the traceable portion when considered that way. It's useful because so much of the impacts distributed in complex systems are unaccountable. The major unaccountable part of economic impacts, for example, is the free use of the money given others for their goods and services. There are no data for what they do with it, making it a perfect hiding place for many times the impacts you readily see.\n\nFor whole energy impact of a choice the simple first step is to add up the accountable BTUs used, like electric bills and LCA accounts, and then add the country's GDP for the cost. The global average spent per BTU works out to roughly 6000 BTU per dollar in 2008 based on United States Department of Energy trends. Then there are ways to proportionally adjust that for the details of other things. The effect is to add the impacts of the technology and the impacts of the commerce.\n"}
