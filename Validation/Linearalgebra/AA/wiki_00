{"id": "18422", "url": "https://en.wikipedia.org/wiki?curid=18422", "title": "Linear algebra", "text": "Linear algebra\n\nLinear algebra is the branch of mathematics concerning linear equations such as \nlinear functions such as\nand their representations through matrices and vector spaces.\n\nLinear algebra is central to almost all areas of mathematics. For instance, linear algebra is fundamental in modern presentations of geometry, including for defining basic objects such as lines, planes and rotations. Also, functional analysis may be basically viewed as the application of linear algebra to spaces of functions. Linear algebra is also used in most sciences and engineering areas, because it allows modeling many natural phenomena, and efficiently computing with such models. For nonlinear systems, which cannot be modeled with linear algebra, linear algebra is often used as a first-order approximation.\n\nThe procedure for solving simultaneous linear equations now called Gaussian elimination appears in the ancient Chinese mathematical text Chapter Eight: \"Rectangular Arrays\" of \"The Nine Chapters on the Mathematical Art\". Its use is illustrated in eighteen problems, with two to five equations. \n\nSystems of linear equations arose in Europe with the introduction in 1637 by René Descartes of coordinates in geometry. In fact, in this new geometry, now called Cartesian geometry, lines and planes are represented by linear equations, and computing their intersections amounts to solving systems of linear equations.\n\nThe first systematic methods for solving linear systems used determinants, first considered by Leibniz in 1693. In 1750, Gabriel Cramer used them for giving explicit solutions of linear systems, now called Cramer's Rule. Later, Gauss further described the method of elimination, which was initially listed as an advancement in geodesy.\n\nIn 1844 Hermann Grassmann published his \"Theory of Extension\" which included foundational new topics of what is today called linear algebra. In 1848, James Joseph Sylvester introduced the term \"matrix\", which is Latin for \"womb\". \n\nLinear algebra grew with ideas noted in the complex plane. For instance, two numbers \"w\" and \"z\" in ℂ have a difference \"w\" – \"z\", and the line segments formula_3 are of the same length and direction. The segments are equipollent. The four-dimensional system ℍ of quaternions was started in 1843. The term \"vector\" was introduced as \"v\" = \"x\" i + \"y\" j + \"z\" k representing a point in space. The quaternion difference \"p\" – \"q\" also produces a segment equipollent to formula_4 \nOther hypercomplex number systems also used the idea of a linear space with a basis.\n\nArthur Cayley introduced matrix multiplication and the inverse matrix in 1856, making possible the general linear group. The mechanism of group representation became available for describing complex and hypercomplex numbers. Crucially, Cayley used a single letter to denote a matrix, thus treating a matrix as an aggregate object. He also realized the connection between matrices and determinants, and wrote \"There would be many things to say about this theory of matrices which should, it seems to me, precede the theory of determinants\".\n\nBenjamin Peirce published his \"Linear Associative Algebra\" (1872), and his son Charles Sanders Peirce extended the work later.\n\nThe telegraph required an explanatory system, and the 1873 publication of A Treatise on Electricity and Magnetism instituted a field theory of forces and required differential geometry for expression. Linear algebra is flat differential geometry and serves in tangent spaces to manifolds. Electromagnetic symmetries of spacetime are expressed by the Lorentz transformations, and much of the history of linear algebra is the history of Lorentz transformations.\n\nThe first modern and more precise definition of a vector space was introduced by Peano in 1888; by 1900, a theory of linear transformations of finite-dimensional vector spaces had emerged. Linear algebra took its modern form in the first half of the twentieth century, when many ideas and methods of previous centuries were generalized as abstract algebra. The development of computers led to increased research in efficient algorithms for Gaussian elimination and matrix decompositions, and linear algebra became an essential tool for modelling and simulations.\n\nSee also and .\n\nUntil 19th century, linear algebra was introduced through systems of linear equations and matrices. In modern mathematics, the presentation through \"vector spaces\" is generally preferred, since it is more synthetic, more general (not limited to the finite-dimensional case), and conceptually simpler, although more abstract.\n\nA vector space over a field (often the field of the real numbers) is a set equipped with two binary operations satisfying the following axioms. \nElements of are called \"vectors\", and elements of \"F\" are called \"scalars\". The first operation, \"vector addition\", takes any two vectors and and outputs a third vector . The second operation, \"scalar multiplication\", takes any scalar and any vector and outputs a new . The axioms that addition and scalar multiplication must satisfy are the following (in the list below, and are arbitrary elements of , and and are arbitrary scalars in the field .\n\nThe first four axioms mean that is an abelian group under addition.\n\nElements of a vector space may have various nature; for example, they can be sequences, functions, polynomials or matrices. Linear algebra is concerned with properties common to all vector spaces.\n\nLinear maps are mappings between vector spaces that preserve the vector-space structure. Given two vector spaces and over a field , a linear map (also called, in some contexts, linear transformation, linear mapping or linear operator) is a map\n\nthat is compatible with addition and scalar multiplication, that is\n\nfor any vectors in and scalar in .\n\nThis implies that for any vectors in and scalars in , one has\n\nWhen a bijective linear map exists between two vector spaces (that is, every vector from the second space is associated with exactly one in the first), the two spaces are isomorphic. Because an isomorphism preserves linear structure, two isomorphic vector spaces are \"essentially the same\" from the linear algebra point of view, in the sense that they cannot be distinguished by using vector space properties. An essential question in linear algebra is testing whether a linear map is an isomorphism or not, and, if it is not an isomorphism, finding its range (or image) and the set of elements that are mapped to the zero vector, called the kernel of the map. All these questions can be solved by using Gaussian elimination or some variant of this algorithm.\n\nThe study of subsets of vector spaces that are themselves vector spaces for the induced operations is fundamental, similarly as for many mathematical structures. These subsets are called linear subspaces. More precisely, a linear subspace of a vector space over a field is a subset of such that and are in , for every , in , and every in . (These conditions suffices for implying that is a vector space.)\n\nFor example, the image of a linear map, and the inverse image of 0 by a linear map (called kernel or null space) are linear subspaces.\n\nAnother important way of forming a subspace is to consider linear combinations of a set of vectors: the set of all sums \nwhere are in , and are in form a linear subspace called the span of . The span of is also the intersection of all linear subspaces containing . In other words, it is the (smallest for the inclusion relation) linear subspace containing .\n\nA set of vectors is linearly independent if none is in the span of the others. Equivalently, a set of vector is linearly independent if the only way to express the zero vector as a linear combination of elements of is to take zero for every coefficient formula_9\n\nA set of vectors that spans a vector space is called a spanning set or generating set. If a spanning set is \"linearly dependent\" (that is not linearly independent), then some element of is in the span of the other elements of , and the span would remain the same if one remove from . One may continue to remove elements of until getting a \"linearly independent spanning set\". Such a linearly independent set that spans a vector space is called a basis of . The importance of bases lies in the fact that there are together minimal generating sets and maximal independent sets. More precisely, if is a linearly independent set, and is a spanning set such that formula_10 then there is a basis such that formula_11\n\nAny two bases of a vector space have the same cardinality, which is called the dimension of ; this is the dimension theorem for vector spaces. Moreover, two vector spaces over the same field are isomorphic if and only if they have the same dimension.\n\nIf any basis of (and therefore every basis) has a finite number of elements, is a \"finite-dimensional vector space\". If is a subspace of , then . In the case where is finite-dimensional, the equality of the dimensions implies .\n\nIf \"U\" and \"U\" are subspaces of \"V\", then\n\nwhere formula_13denotes the span of formula_14\n\nMatrices allow explicit manipulation of finite-dimensional vector spaces and linear maps. Their theory is thus an essential part of linear algebra.\n\nLet be a finite-dimensional vector space over a field , and be a basis of (thus is the dimension of ). By definition of a basis, the map\nis a bijection from formula_16 the set of the sequences of elements of , onto . This is an isomorphism of vector spaces, if formula_17 is equipped of its standard structure of vector space, where vector addition and scalar multiplication are done component by component.\n\nThis isomorphism allows representing a vector by its inverse image under this isomorphism, that is by the coordinates vector formula_18 or by the column matrix\n\nIf is another finite dimensional vector space (possibly the same), with a basis formula_20 a linear map from to is well defined by its values on the basis elements, that is formula_21 Thus, is well represented by the list of the corresponding column matrices. That is, if \nfor , then is represented by the matrix\nwith rows and columns.\n\nMatrix multiplication is defined in such a way that the product of two matrices is the matrix of the composition of the corresponding linear maps, and the product of a matrix and a column matrix is the column matrix representing the result of applying the represented linear map to the represented vector. It follows that the theory of finite-dimensional vector spaces and the theory of matrices are two different languages for expressing exactly the same concepts.\n\nTwo matrices that encode the same linear transformation in different bases are called similar. Equivalently, two matrices are similar if one can transform one in the other by elementary row and column operations. For a matrix representing a linear map from to , the row operations correspond to change of bases in and the column operations correspond to change of bases in . Every matrix is similar to an identity matrix possibly bordered by zero rows and zero columns. In terms of vector space, this means that, for any linear map from to , there are bases such that a part of the basis of is mapped bijectively on a part of the basis of , and that the remaining basis elements of , if any, are mapped to zero (this is a way of expressing the fundamental theorem of linear algebra). Gaussian elimination is the basic algorithm for finding these elementary operations, and proving this theorem.\n\nSystems of linear equations form a fundamental part of linear algebra. Historically, linear algebra and matrix theory has been developed for solving such systems. In the modern presentation of linear algebra through vector spaces and matrices, many problems may be interpreted in terms of linear systems.\n\nFor example, let\nbe a linear system.\n\nTo such a system, one may associate its matrix \nand its right member vector\n\nLet be the linear transformation associated to the matrix . A solution of the system is a vector \nsuch that \nthat is an element of the preimage of by .\n\nLet be the associated homogeneous system, where the right-hand sides of the equations are put to zero. The solutions of are exactly the elements of the kernel of or, equivalently, .\n\nThe Gaussian-elimination consists of performing elementary row operations on the augmented matrix\nfor putting it in reduced row echelon form. These row operations do not change the set of solutions of the system of equations. In the example, the reduced echelon form is \nshowing that the system has the unique solution\n\nIt follows from this matrix interpretation of linear systems that the same methods can be applied for solving linear systems and for many operations on matrices and linear transformations, which include the computation of the ranks, kernels, matrix inverses.\n\nA linear endomorphism is a linear map that maps a vector space to itself. \nIf has a basis of elements, such an endomorphism is represented by a square matrix of size .\n\nWith respect to general linear maps, linear endomorphisms and square matrices have some specific properties that make their study an important part of linear algebra, which is used in many parts of mathematics, including geometric transformations, coordinate changes, quadratic forms, and many other part of mathematics.\n\nThe \"determinant\" of a square matrix is a polynomial function of the entries of the matrix, such that the matrix is invertible if and only if the determinant is not zero. This results from the fact that the determinant of a product of matrices is the product of the determinants, and thus that a matrix is invertible if and only if its determinant is invertible.\n\nCramer's rule is a closed-form expression, in terms of determinants, of the solution of a system of linear equations in unknowns. Cramer's rule is useful for reasoning about the solution, but, except for or , it is rarely used for computing a solution, since Gaussian elimination is a faster algorithm.\n\nThe \"determinant of an endomorphism\" is the determinant of the matrix representing the endomorphism in terms of some ordered basis. This definition makes sense, since this determinant is independent of the choice of the basis.\n\nIf is a linear endomorphism of a vector space over a field , an eigenvector of is a nonzero vector of such that for some scalar in . This scalar is an eigenvalue of .\n\nIf the dimension of is finite, and a basis has been chosen, and may be represented, respectively, by a square matrix and a column matrix and ; the equation defining eigenvectors and eigenvalues becomes\nUsing the identity matrix , whose all entries are zero, except those of the main diagonal, which are equal to one, this may be rewritten\nAs is supposed to be nonzero, this means that is a singular matrix, and thus that its determinant formula_34 equals zero. The eigenvalues are thus the roots of the polynomial\nIf is of dimension , this is a monic polynomial of degree , called the characteristic polynomial of the matrix (or of the endomorphism), and there are, at most, eigenvalues.\n\nIf a basis exists that consists only of eigenvectors, the matrix of on this basis has a very simple structure: it is a diagonal matrix such that the entries on the main diagonal are eigenvalues, and the other entries are zero. In this case, the endomorphism and the matrix are said diagonalizable. More generally, an endomorphism and a matrix are also said diagonalizable, if they become diagonalizable after extending the field of scalars. In this extended sense, if the characteristic polynomial is square-free, then the matrix is diagonalizable.\n\nA symmetric matrix is always diagonalizable. There are non-diagonizable matrices, the simplest being\n(it cannot be diagonalizable since its square is the zero matrix, and the square of a nonzero diagonal matrix is never zero).\n\nWhen an endomorphism is not diagonalizable, there are bases on which it has a simple form, although not as simple as the diagonal form. The Frobenius normal form does not need of extending the field of scalars and makes the characteristic polynomial immediately readable on the matrix. The Jordan normal form requires to extend the field of scalar for containing all eigenvalues, and differs from the diagonal form only by some entries that are just above the main diagonal and are equal to 1.\n\nA linear form is a linear map from a vector space over a field to the field of scalars , viewed as a vector space over itself. Equipped by pointwise addition and multiplication by a scalar, the linear forms form a vector space, called the dual space of , and usually denoted formula_37\n\nIf formula_38 is a basis of (this implies that is finite-dimensional), then one can define, for , a linear map formula_39 such that formula_40 and formula_41 if . These linear maps form a basis of formula_42 called the dual basis of formula_43 (If is not finite-dimensional, the formula_44 may be defined similarly; they are linearly independent, but do not form a basis.)\n\nFor in , the map\nis a linear form on formula_37 This defines the canonical linear map from into formula_47, the dual of formula_42 called the bidual of . This canonical map is an isomorphism if is finite-dimensional, and this allows identifying with its bidual. (In the infinite dimensional case, the canonical map is injective, but not surjective.)\n\nThere is thus a complete symmetry between a finite-dimensional vector space and its dual. This motivates the frequent use, in this context, of the bra–ket notation\nfor denoting .\n\nLet \nbe a linear map. For every linear form on , the composite function is a linear form on . This defines a linear map\nbetween the dual spaces, which is called the dual or the transpose of .\n\nIf and are finite dimensional, and is the matrix of in terms of some ordered bases, then the matrix of formula_52 over the dual bases is the transpose formula_53 of , obtained by exchanging rows and columns.\n\nIf elements of vector spaces and their duals are represented by column vectors, this duality may be expressed in bra–ket notation by \nFor highlighting this symmetry, the two members of this equality are sometimes written \n\nBesides these basic concepts, linear algebra also studies vector spaces with additional structure, such as an inner product. The inner product is an example of a bilinear form, and it gives the vector space a geometric structure by allowing for the definition of length and angles. Formally, an \"inner product\" is a map\n\nthat satisfies the following three axioms for all vectors \"u\", \"v\", \"w\" in \"V\" and all scalars \"a\" in \"F\":\n\nNote that in R, it is symmetric.\n\n\nWe can define the length of a vector \"v\" in \"V\" by\nand we can prove the Cauchy–Schwarz inequality:\n\nIn particular, the quantity\nand so we can call this quantity the cosine of the angle between the two vectors.\n\nTwo vectors are orthogonal if formula_64. An orthonormal basis is a basis where all basis vectors have length 1 and are orthogonal to each other. Given any finite-dimensional vector space, an orthonormal basis could be found by the Gram–Schmidt procedure. Orthonormal bases are particularly easy to deal with, since if \"v\" = \"a\" \"v\" + ... + \"a v\", then formula_65.\n\nThe inner product facilitates the construction of many useful concepts. For instance, given a transform \"T\", we can define its Hermitian conjugate \"T*\" as the linear transform satisfying\nIf \"T\" satisfies \"TT*\" = \"T*T\", we call \"T\" normal. It turns out that normal matrices are precisely the matrices that have an orthonormal system of eigenvectors that span \"V\".\n\nThere is a strong relationship between linear algebra and geometry, which started with the introduction by René Descartes, in 1637, of Cartesian coordinates. In this new (at that time) geometry, now called Cartesian geometry, points are represented by Cartesian coordinates, which are sequences of three real numbers (in the case of the usual three-dimensional space). The basic objects of geometry, which are lines and planes are represented by linear equations. Thus, computing intersections of lines and planes amounts solving systems of linear equations. This was one of the main motivations for developing linear algebra.\n\nMost geometric transformation, such as translations, rotations, reflections, rigid motions, isometries, and projections transform lines into lines. It follows that they can be defined, specified and studied in terms of linear maps. This is also the case of homographies and Möbius transformations, when considered as transformations of a projective space. \n\nUntil the end of 19th century, geometric spaces were defined by axioms relating points, lines and planes (synthetic geometry). Around this date, it appeared that one may also define geometric spaces by constructions involving vector spaces (see, for example, Projective space and Affine space) It has been shown that the two approaches are essentially equivalent. In classical geometry, the involved vector spaces are vector spaces over the reals, but the constructions may be extended to vector spaces over any field, allowing considering geometry over arbitrary fields, including finite fields. \n\nPresently, most textbooks, introduce geometric spaces from linear algebra, and geometry is often presented, at elementary level, as a subfield of linear algebra.\n\nLinear algebra is used in almost all areas of mathematics, and therefore in almost all scientific domains that use mathematics. These applications may be divided into several wide categories.\n\nThe modeling of our ambient space is based on geometry. Sciences concerned with this space use geometry widely. This is the case with mechanics and robotics, for describing rigid body dynamics; geodesy for describing Earth shape; perspectivity, computer vision, and computer graphics, for describing the relationship between a scene and its plane representation; and many other scientific domains.\n\nIn all these applications, synthetic geometry is often used for general descriptions and a qualitative approach, but for the study of explicit situations, one must compute with coordinates. This requires the heavy use of linear algebra.\n\nFunctional analysis studies function spaces. These are vector spaces with additional structure, such as Hilbert spaces. Linear algebra is thus a fundamental part of functional analysis and its applications, which include, in particular, quantum mechanics (wave functions).\n\nMost physical phenomena are modeled by partial differential equations. To solve them, one usually decomposes the space in which the solutions are searched into small, mutually interacting cells. For linear systems this interaction involves linear functions. For nonlinear systems, this interaction is often approximated by linear functions. In both cases, very large matrices are generally involved. Weather forecasting is a typical example, where the whole Earth atmosphere is divided in cells of, say, 100 km of width and 100 m of height.\n\nNearly all scientific computations involve linear algebra. Consequently, linear algebra algorithms have been highly optimized. BLAS and LAPACK are the best known implementations. For improving efficiency, some of them configure the algorithms automatically, at run time, for adapting them to the specificities of the computer (cache size, number of available cores, ...).\n\nSome processors, typically graphics processing units (GPU), are designed with a matrix structure, for optimizing the operations of linear algebra.\n\nThis section presents several related topics that do not appear generally in elementary textbooks on linear algebra, but are commonly considered, in advanced mathematics, as parts of linear algebra.\n\nThe existence of multiplicative inverses in fields is not involved in the axioms defining a vector space. One may thus replace the field of scalars by a ring , and this gives a structure called module over , or -module.\n\nThe concepts of linear independence, span, basis, and linear maps (also called module homomorphisms) are defined for modules exactly as for vector spaces, with the essential difference that, if is not a field, there are modules that do not have any basis. The modules that have a basis are the free modules, and those that are spanned by a finite set are the finitely generated modules. Module homomorphisms between finitely generated free modules may be represented by matrices. The theory of matrices over a ring is similar to that of matrices over a field, except that determinants exist only if the ring is commutative, and that a square matrix over a commutative ring is invertible only if its determinant has a multiplicative inverse in the ring.\n\nVector spaces are completely characterized by their dimension (up to an isomorphism). In general, there is not such a complete classification for modules, even if one restricts oneself to finitely generated modules. However, every module is a cokernel of a homomorphism of free modules.\n\nModules over the integers can be identified with abelian groups, since the multiplication by an integer may identified to a repeated addition. Most of the theory of abelian groups may be extended to modules over a principal ideal domain. In particular, over a principal ideal domain, every submodule of a free module is free, and the fundamental theorem of finitely generated abelian groups may be extended straightforwardly to finitely generated modules over a principal ring.\n\nThere are many rings for which there are algorithms for solving linear equations and systems of linear equations. However, these algorithms have generally a computational complexity that is much higher than the similar algorithms over a field. For more details, see Linear equation over a ring.\n\nIn multilinear algebra, one considers multivariable linear transformations, that is, mappings that are linear in each of a number of different variables. This line of inquiry naturally leads to the idea of the dual space, the vector space \"V\" consisting of linear maps where \"F\" is the field of scalars. Multilinear maps can be described via tensor products of elements of \"V\".\n\nIf, in addition to vector addition and scalar multiplication, there is a bilinear vector product , the vector space is called an algebra; for instance, associative algebras are algebras with an associate vector product (like the algebra of square matrices, or the algebra of polynomials).\n\nFunctional analysis mixes the methods of linear algebra with those of mathematical analysis and studies various function spaces, such as L spaces.\n\n\n\n\n\n\n\n"}
