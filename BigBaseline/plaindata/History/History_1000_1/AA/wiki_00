{"id": "47412391", "url": "https://en.wikipedia.org/wiki?curid=47412391", "title": "1987 in Sri Lanka", "text": "1987 in Sri Lanka\n\nThe following lists events that happened during 1987 in Sri Lanka.\n\n\n"}
{"id": "47489976", "url": "https://en.wikipedia.org/wiki?curid=47489976", "title": "1995 in Sri Lanka", "text": "1995 in Sri Lanka\n\nThe following lists events that happened during 1995 in Sri Lanka.\n\n"}
{"id": "47508669", "url": "https://en.wikipedia.org/wiki?curid=47508669", "title": "1999 in Sri Lanka", "text": "1999 in Sri Lanka\n\nThe following lists events that happened during 1999 in Sri Lanka.\n\n\n"}
{"id": "47766805", "url": "https://en.wikipedia.org/wiki?curid=47766805", "title": "2001 in Sri Lanka", "text": "2001 in Sri Lanka\n\nThe following lists events that happened during 2001 in Sri Lanka.\n\n\n"}
{"id": "33316520", "url": "https://en.wikipedia.org/wiki?curid=33316520", "title": "2012 World Monuments Watch", "text": "2012 World Monuments Watch\n\nThe World Monuments Watch is a flagship advocacy program of the New York-based private non-profit organization World Monuments Fund (WMF) that calls international attention to cultural heritage around the world that is threatened by neglect, vandalism, conflict, or disaster.\n\nEvery two years, it publishes a select list known as the Watch List of Endangered Sites that are in urgent need of preservation funding and protection. The sites are nominated by governments, conservation professionals, site caretakers, non-government organizations (NGOs), concerned individuals, and others working in the field. An independent panel of international experts then select 100 candidates from these entries to be part of the Watch List, based on the significance of the sites, the urgency of the threat, and the viability of both advocacy and conservation solutions. For the succeeding two-year period until a new Watch List is published, these 100 sites can avail grants and funds from the WMF, as well as from other foundations, private donors, and corporations by capitalizing on the publicity and attention gained from the inclusion on the Watch List.\n\nThe 2012 Watch List was published on 5 October 2011.\nThe following countries/territories have multiple sites entered on the 2012 Watch List, listed by the number of sites:\n"}
{"id": "54729261", "url": "https://en.wikipedia.org/wiki?curid=54729261", "title": "2017 Bojangles' Southern 500", "text": "2017 Bojangles' Southern 500\n\nThe 2017 Bojangles' Southern 500, the 68th running of the event, was a Monster Energy NASCAR Cup Series race held on September 3, 2017 at Darlington Raceway in Darlington, South Carolina. Contested over 367 laps on the egg-shaped oval, it was the 25th race of the 2017 Monster Energy NASCAR Cup Series season.\n\nDarlington Raceway is a race track built for NASCAR racing located near Darlington, South Carolina. It is nicknamed \"The Lady in Black\" and \"The Track Too Tough to Tame\" by many NASCAR fans and drivers and advertised as \"A NASCAR Tradition.\" It is of a unique, somewhat egg-shaped design, an oval with the ends of very different configurations, a condition which supposedly arose from the proximity of one end of the track to a minnow pond the owner refused to relocate. This situation makes it very challenging for the crews to set up their cars' handling in a way that is effective at both ends.\n\nKyle Larson was the fastest in the first practice session with a time of 28.415 seconds and a speed of .\n\nKyle Busch was the fastest in the final practice session with a time of 28.373 seconds and a speed of .\n\nKevin Harvick scored the pole for the race with a time of 27.669 and a speed of .\n\nStage 1\n\"Laps:\" 100\n\nStage 2\n\"Laps:\" 100\n\nStage 3\n\"Laps:\" 167\n\n\nNBC Sports covered the race on the television side. Rick Allen, two–time Darlington winner Jeff Burton and Steve Letarte had the call in the booth for the race. As part of the throwback weekend, Ken Squier, Ned Jarrett and Dale Jarrett also called a portion of the race. Dave Burns, Marty Snider and Kelli Stavast reported from pit lane during the race.\n\nThe Motor Racing Network had the radio call for the race, which was simulcast on Sirius XM NASCAR Radio. Dave Moody called the race from a Billboard outside of turn when the field raced through turns 1 and 2, and Mike Bagley had the call of the race atop of the Darlington Raceway Club outside of turn 3 when the field raced through turns 3 and 4 \n\n"}
{"id": "31222359", "url": "https://en.wikipedia.org/wiki?curid=31222359", "title": "7 Billion Others", "text": "7 Billion Others\n\n7 billion Others () is a series of videos by Yann Arthus-Bertrand. Each of the videos feature a single person telling the viewer about their life, often in their native language. The project's homepage states that since the project's beginning in 2003, over 5000 individuals have had their testimonials recorded for the project. According to Arthus-Bertrand, the project's aim is to help different people to understand each other through listening.\nThe project was previously called 6 billion Others, but is meanwhile renamed to 7 billion others.\n"}
{"id": "4973119", "url": "https://en.wikipedia.org/wiki?curid=4973119", "title": "AP World History", "text": "AP World History\n\nAdvanced Placement World History (also known as AP World History, WHAP, AP World or APWH) is a college-level course and examination offered to high school students through the College Board's Advanced Placement Program designed to help students develop a greater understanding of the evolution of global processes and contacts as well as interactions between different types of human society. The course advances this understanding through a combination of selective factual knowledge and appropriate analytical skills. Students study all prehistory and history, especially from 8000 BCE to the present day. It was announced in July 2018 that the test would be changed to an AP World History: Modern exam that only contains content since 1200 CE, starting in the 2019-2020 school year. The AP World History exam was first administered in 2002. The course has undergone multiple changes with the latest changes effective Fall 2017. Often times in the United States, students take the course in their sophomore year of high school, but this is not a requirement.\n\nThis is a high school course designed specifically for sophomores in high school and gives college credit. The course is organized around six eras/periods and nineteen \"Key Concepts\":\nKey Concept 1.1 Big Geography and the Peopling of the Earth\n\nKey Concept 1.2 The Neolithic Revolution and Early Agricultural Societies\n\nKey Concept 1.3 The Development and Interactions of Early Agricultural, Pastoral, and Urban Societies\nKey Concept 2.1 The Development and Codification of Religious and Cultural Traditions\n\nKey Concept 2.2 The Development of States and Empires\n\nKey Concept 2.3 Emergence of Transregional Networks of Communication and Exchange\nKey Concept 3.1 Expansion and Intensification of Communication and Exchange Networks\n\nKey Concept 3.2 Continuity and Innovation of State Forms and Their Interactions\n\nKey Concept 3.3 Increased Economic Productive Capacity and Its Consequences\nKey Concept 4.1 Globalizing Networks of Communication and Exchange\n\nKey Concept 4.2 New Forms of Social Organization and Modes of Production\n\nKey Concept 4.3 State Consolidation and Imperial Expansion\nKey Concept 5.1 Industrialization and Global Capitalism\n\nKey Concept 5.2 Imperialism and Nation-State Formation\n\nKey Concept 5.3 Nationalism, Revolution, and Reform\n\nKey Concept 5.4 Global Migration\nKey Concept 6.1 Science and the Environment\n\nKey Concept 6.2 Global Conflicts and their Consequences\n\nKey Concept 6.3 New Conceptualizations of Global Economy, Society, & Culture\n\nThe first section of the AP World History exam consists of 55 multiple choice questions with a 55-minute time limit. The questions are not divided up evenly between the six periods.\n\nThe multiple choice section is weighted as 40% of one's total score (Section I Part A). It consists of 55 questions to be answered in 55 minutes based on the accompanying sources.\n\nWhile previously the exam deducted 1/4 of a point for every incorrect answer, starting from 2011 on, the penalty for incorrect answers had been removed. It is to one's advantage to attempt every question possible within the time limit. Note also that the number of multiple choice options is being reduced from five to four at the same time.\n\nThis test underwent a major re-haul for the 2017 exam, however, due to the prodigious number of students that struggled with the free response section, the College Board decided to initiate yet another round of sweeping reform, to be put in effect in May 2018. Currently it has the same format as Advanced Placement United States History and Advanced Placement European History. The exam features a new section (Section I Part B) that requires three short answer questions, one of which is selected from two options. Each question has three parts, making for a total of 9 parts within the SAQ section. Students have forty minutes to answer these, and they count for twenty percent of the exam score.\n\nSection II lasts for a total of 100 minutes, and it includes a document-based question (DBQ) and a long essay question (LEQ). Students are allowed to work on either essay within this total time period. The section begins with a 15-minute reading period where students are advised to read both the documents for DBQ. However, students may begin writing during this time; most students take notes on the documents in order to plan out the DBQ. Students are advised to spend 45 minutes writing the DBQ and then 40 writing the LEQ, but there are no rules on when each essay must be worked on. There are three prompts for the LEQ, but only one needs to be chosen. Each LEQ prompt addresses a different period, with one addressing periods 1 & 2, another addressing periods 3 & 4, and a third addressing periods 5 & 6.\n\nThe DBQ counts for 25% of the total exam score, and the LEQ is 15%. The essays are out of seven points and six points, respectively. Students are required to analyze and synthesize the documents of the DBQ, but some outside information is still needed. The LEQ only provides a prompt and no sort of stimulus, so a large amount of outside information is necessary.\n\nAP World History Test Grade Distribution: \nIn 2012, the head of AP Grading, Trevor Packer, stated that the reason for the low percentages of 5s is that \"AP World History is a college-level course, & many sophomores aren't yet writing at that level.\" 10.44 percent of all seniors who took the exam in 2012 received a 5, while 6.62 percent of sophomores received a 5.\n\n"}
{"id": "54980087", "url": "https://en.wikipedia.org/wiki?curid=54980087", "title": "A View of Religions", "text": "A View of Religions\n\nA View of Religions is an 18th-century comprehensive survey of world religions by the American author, Hannah Adams. First published in Boston, Massachusetts in 1784, it was a pioneering work in that it represented denominations from the perspective of their adherents, without imposing Adams' own preferences. The book was divided into sections and passed through several editions, which included minor changes in the name of the work. It was reprinted in England. \"A View of Religions\" was Adams' first and principal literary work. It was the result of her dissatisfaction with the prejudice of most writers on the various religious sects. She began thinking on the subject after reading a manuscript from Thomas Broughton's \"Historical Dictionary of all Religions from the Creation of the World to the Present Times\" (1742).\n\nUntil she was twenty years of age, Adams's reading had been limited mostly to works of imagination and feeling. She had never been directed to those of a controversial nature, nor to a study of the disputed points between the various sects. Her curiosity being awakened by a small manuscript from Broughton's \"Dictionary\", giving an account of some of the most common of the religious denominations, she began to read all she could find on the subject. At about the age of 30, becoming disgusted with the intolerance and lack of candor of the authors, she decided to write in a blank book what seemed to her to be the truth. In doing this she had no idea of publication. The work grew upon her hands and became known to her friends. Needing money, she was urged to publish it. But to find a printer to take it without immediate pay was the difficulty. She obtained 395 subscribers, 28 of whom were women, including Rev. Nathaniel Emmons, Franklin, Massachusetts (six copies), Rev. Charles Chauncy, D. D., Boston, Dr. James Manning, president of Rhode Island College, Edmund Quincy, Esq., Hon. Oliver Wendell, Nathaniel Appleton, Esq., and Samuel Adams, all of Boston. The hard study and close reflection were so difficult, that she was attacked before she finished it by a severe fit of illness, and threatened with derangement.\n\nA bargain having been made with the printer, the book appeared in 1784. Its motto is, \"Prove all things, hold fast that which is good.\" In the Advertisement at the beginning, the author says she intends to \"avoid giving the least preference of one denomination above another, to give a few arguments of the principal sects from their own authors as far as possible, to endeavor to represent every sect, and be very careful to enter into the spirit of each author.\"\n\nThe condition of public opinion is seen when Thomas Prentiss, pastor of the Congregational Church in Medfield, Massachusetts, in the Preface, felt obliged to say:— \n\nIn 1791, appeared under the title of \"A View of Religions\", the second edition of her book first published seven years before. It was enlarged to 410 pages. Part First treated of nearly 300 different religious denominations which had appeared from the beginning of the Christian era; Part Second, of the worship of the Grand Lama, of Mahometans, Jews, Deists and Sceptics, followed by a short review of the religions of the people of the habitable world. A discriminating judgment was noticeable in the work. This second edition was dedicated to John Adams, then Vice President of the United States, a name, she said, \"which excites the veneration and gratitude of fellow-citizens, the admiration and esteem of foreign nations.\"\n\nOf the 400 subscribers to this second edition, 82 of whom were clergymen and 16 women, John Adams headed the list with three copies. Samuel Adams, lieutenant-governor of Massachusetts, followed. Among the new names gained were those of John Hancock, of Boston; Joseph Willard, D. D., president of Harvard College; Right Rev. John Caroll, D. D., Roman Catholic Bishop of Baltimore, Maryland; Hon. William Bradford, governor of Rhode Island; Rev. Henry Ware of Hingham; Christopher Gore, Esq., Boston; Harrison Gray Otis, Boston; Rev. Adoniram Judson, Sr., Malden; Hon. Benjamin Greenleaf, Newburyport; and many others. Mr. Moses Brown, of Providence, took fifteen copies, and seven of the subscribers took six each.\n\nSuch distinguished names spoke well of Adams's literary ability. The book was pronounced the best of the kind ever written, possibly the first. From its profits, Adams was enabled to pay the debts her sister's illness had contracted, and to put a small sum at interest. In its sale, her father, who was called by the townspeople \"Book Adams,\" was of much help. On horseback, with his saddle-bag full of the books, often reading a volume, he went from place to place, to sell or to distribute them. Of decided literary tastes himself, he was never happier than when making his frequent visits to the library of Harvard College. Once, upon entering it, he lifted up both hands and exclaimed in great earnestness, \"I'd rather be librarian of Harvard College than Emperor of all the Russias!\" Who knows but that if Thomas Adams could have followed out his early desires, he might have been librarian of Harvard College? Circumstances obliged him to go into business, and he made a failure of it.\n\nAfter the publication of the second edition of her book, Adams taught school.\n\nTwo years after the publication of Adams's \"History of New England from the first settlement at Plymouth to the acceptance of the Federal Constitution\", \"A view of religions, in two parts : Part I. Containing an alphabetical compedium of the various religious denominations, which have appeared in the world, from the beginning of the Christian era to the present day. Part II. Containing a brief account of the different schemes of religion now embraced among mankind \" was published, enlarged, and dedicated as before to John Adams. Through the continued kindness of Rev. James Freeman, a bargain was made with the printer whereby she was to receive five hundred dollars in yearly payments, covering a certain period, for the edition of two thousand copies. In 1804 appeared her book of nearly four hundred pages called \" Truth and Excellence of the Christian Religion.\" Her selection of authors and the extracts from their works reveal good judgment and a wide survey of general literature. Not being able to purchase or borrow all the books she needed for this effort, she spent some time in Boston, in order to visit the booksellers' shops. In such places much of the work, as well as the additions to the third edition of her first book, was written. Adams gave this work, with its four hundred subscribers, to the printer for one hundred dollars in books.\n\n\"A View of Religions, in three Parts : Part I. Containing an Alphabetical Compendium of the Denominations among Christians. Part II. Containing a brief Account of Paganism, Mahomedism, Judaism, and Deism. Part III. Containing a View of the Religious of the different Nations of the World. By Hannah Adams. A new Edition, with Corrections and Additions. To which is prefixed, An Essay on Truth, by Andrew Fuller.\" was published in 1805. Griffiths & Griffiths commented:— \n\nIn 1817 appeared \"A Dictionary of All Religions and Religious Denominations\", dedicated as before to John Adams. This was a popular book from the first. It was published in England with a preface and additions by Mr. Andrew Fuller; also in another form by Mr. Thomas Williams, who likewise made alterations. Adams acknowledged herself indebted to both these editions for some of the improvements in her fourth edition. It received the notice of Jared Sparks in the North American Review; he pronounced it the best manual of the kind he knew of. \"It has the peculiar merit,\" he added, \"of the strictest candor and impartiality; and so completely has the author divested herself of all in lividual prepossessions, that it may be doubted whether from a single passage in the whole work her own religious sentiments can be inferred. This freedom from personal bias, in exhibiting the views of others, especially on topics rarely touched without calling out private opinion, inspires confidence in her statements, as well as respect for her judgment and Christian charity.\"\n\n\n"}
{"id": "20688906", "url": "https://en.wikipedia.org/wiki?curid=20688906", "title": "Aftermath of the 2008 Mumbai attacks", "text": "Aftermath of the 2008 Mumbai attacks\n\nIn the aftermath of the 2008 Mumbai attacks, there were multiple and far-ranging events that were observed. Besides the immediate impact on the victims and their families, the attacks caused widespread anger among the Indian public, and condemnations from countries throughout the world.\n\nThe immediate impact was felt on Mumbai and Maharashtra state, and throughout urban India. There were also after-effects on the Indian government, centre-state relations within India, Indo-Pakistani relations, domestic impact within Pakistan, the United States's relationships with both countries, the US-led NATO war in Afghanistan, and on the Global War on Terror.\nThere was also impact on the region of Kashmir and this also led to the 2010 Kashmir Unrest.\n\nThe Leopold Cafe opened its doors to customers just four days after the attacks. The owners wanted to repair the damaged parts of the cafe, while retaining some of the damaged pieces as a tribute to those who lost their lives in the attacks.\n\nSecurity forces handed back control of the Taj Mahal Hotel to the Taj group on 1 December 2008, and work on its repairs began that same day. Celebrated artist M.F. Hussain, whose art was destroyed in the attacks, has agreed to replace the paintings with a series that will condemn the attack. Hussain plans this series as a tribute to the staff of the hotel, who laid down their lives to save other people.\n\nControl of the Trident has already been handed back to the management, while the Oberoi will take 3–4 months to resume operations. Both the Taj and Trident hotels reopened on 21 December 2008.\n\nNariman House will also reopen soon, but it is not known exactly when. Several young Chabad couples from all over the world have stepped forward to move to Mumbai and continue the movement's work.\n\nIn the aftermath of the November 2008 Mumbai attacks, all schools and colleges, and most offices were closed. The Bombay Stock Exchange and National Stock Exchange remained closed on 27 November 2008. Shooting of Bollywood films and TV series had also been halted in the city. Many international airlines temporarily discontinued operations to Mumbai, in the interest of passenger and crew safety.\n\nThe Indian Cricket League's ongoing tournament in Ahmedabad was canceled. The two remaining One Day Internationals of the seven match series between the visiting England cricket team and India were cancelled. The visiting team flew home, but returned to continue the test series. However, the venue of the second India-England test match, scheduled on 19–23 December, was shifted from Mumbai to Chennai. The inaugural Twenty20 Champions League, scheduled from 3 to 10 December, Mumbai being one of the host cities, was postponed. The attacks have brought into significance the issue of 379 Indian boats and 336 fishermen apprehended by the Pakistan marine agency, for entering their waters. Nearly 200 of the boats have reportedly been auctioned, now recognised as a national security issue for India. On 28 November, Pakistan released 99 fishermen who were apprehended, as part of confidence building measures with India. There were threats to blow up ITC Fortune Hotel in Navi Mumbai, after Mumbai police received a bomb threat from terrorists. Rumours about further shootings at Chhatrapati Shivaji Terminus were doing the rounds in Mumbai on 28 November, and were widely reported by the news channels. The Railway Police denied these rumours, but stopped trains approaching CST.\n\nAfter seeing the disparity between the quality of helmets and bulletproof vests used by NSG commandos and the police, the Police Commissioner of Pune, Satyapal Singh, said his police officers needed the same quality equipment as used by the NSG to reduce deaths and improve performance.\n\nMaharashtra government has planned to buy 36 speed boats to patrol the coastal areas, and several helicopters for the same purpose. It will also create an Anti-Terror force known as the Force One, and it will upgrade all the weapons that Mumbai Police currently have.\n\nThe CST railway station was upgraded with metal detectors, but reports say that these are ineffective because the attendants cannot hear their beeps in the crowd. Civilians have tested these using their own licensed weapons.\n\nMaharashtra's new Home Minister, Jayant Patil, was forced to defend the performance of the police in the legislative assembly, against demands from the opposition parties for resignations from the police chief and other officials.\n\nIn an attempt to take forward the 26/11 probe, the Mumbai Police will send a three members team to the United States in this regard, sources said. The team comprising Inspector Bharti, Inspector Kadam and Inspector Arun Chavan will share the findings of the 26 November 2008 Mumbai attacks, with apex US investigative agency FBI. Mumbai police have sent a proposal to the Home Ministry, stating the need to establish direct contact with the FBI, to facilitate investigation in the Mumbai terror attacks. It is believed that the team, if permitted to go, will share the evidence gathered by Mumbai police, which prove Pakistan's hand in the attacks on the nation's integrity.\n\nA Cabinet Committee on Security meeting was held on Tuesday, 2 December, to discuss expanding the National Security Guards (NSG) to cities outside Delhi. The aim is to have permanent presence of NSG anti-terrorist squads in cities such as Mumbai, Chennai, Bangalore, Hyderabad and Kolkata, to avoid wasting precious time travelling from Delhi.\n\nAll NSG commandos will now undergo a new module of training, to learn how to deal with future anti-siege operations, because the Taj terrorists were in a gun battle for 59 hours continuously.\n\nPrime Minister Dr. Manmohan Singh, on an all party conference, declared that legal framework will be strengthened in the battle against terrorism and a federal anti-terrorist intelligence and investigation agency, like the FBI, will soon be set up to co-ordinate actions against terrorism. On 17 December, the Lok Sabha approved two new anti-terror bills, which are expected to pass the upper house (Rajya Sabha) on the 19th. One sets up a National Investigation Agency, similar to the FBI, with sweeping powers of investigation. The second strengthens existing anti-terror laws, to allow suspects to be detained without bail for up to six months, on the orders of a judge.\n\nThe \"National Investigation Agency Bill, 2008\", sets up a central agency for investigating terrorism related crimes. However, law and order is a state subject in the Constitution of India, which had made such a law difficult to pass in the past.\n\nUnion Home Minister P. Chidambaram assured parliament that the National Investigation Agency (NIA) did not usurp the States' right in any manner. The central government would make use of its power only under \"extraordinary\" circumstances, and depending on the gravity of the situation, he said. \"The agency will also have the powers to return the investigations to the State, if it so thinks. We have struck a balance between the right of the States and duties of the Centre to investigate.\"\n\nThe attacks have damaged India's already strained relationship with Pakistan. India handed over two demarches to Pakistan—one was submitted at the Foreign Office in Islamabad by Indian High Commissioner Satyabrata Pal. The Indian Ministry of External Affairs also summoned Pakistan High Commissioner Shahid Malik on 1 December 2008, to lodge a formal protest over Pakistan's failure to curb terrorism emanating from its soil. According to the Indian External Affairs Minister, Pranab Mukherjee, India, in the demarches to Pakistan, asked for the arrest and handover of those 20 persons, including gangster Dawood Ibrahim, the founder of Jaish-e-Mohammad, terrorist leader Maulana Masood Azhar and Lashkar-e-Taiba, and chief Hafiz Muhammad Saeed, who are settled in Pakistan and who are fugitives of Indian law. The external affairs minister has also stated that India will await Pakistan’s response. He has not ruled out the option of military strikes against terrorist camps in Pakistan.\n\nOn 28 November, a hoax caller pretending to be the Indian Foreign Minister threatened Pakistan President Zardari with war, leading to the Pakistan military being put in high alert. Military aircraft with live ammunition were scrambled to patrol above Islamabad and Rawalpindi. \n\nIndia's claims that the perpetrators were Pakistan-based were consistently denied by Pakistan. Pakistan initially contested this attribution, but agreed this was the case on 7 January 2009. The Indian government supplied a dossier to Pakistan's high commission in Delhi, containing interrogations, weapons, and call records of conversations during the attacks. Shown to friendly governments and media, it provides a detailed sequence of training, supplying, and constant communications with handlers from Pakistan. The Pakistan government dismissed the dossier as \"not evidence\", but also announced that it had detained over a hundred members of Jamaat-ud-Dawa, a charity linked with Lashkar-e-Taiba. Moreover, Indian government officials said that the attacks were so sophisticated that they must have had official backing from Pakistani \"agencies\". an accusation denied by Pakistan.\n\nAt the request of Indian Prime Minister Manmohan Singh, the head of Pakistan's Inter-Services Intelligence (ISI), Ahmad Shuja Pasha, was reported to be coming to India to share intelligence and help the investigation, but later on it was decided by Pakistani authorities that instead of Director General of the ISI, his representative will visit India to help the Indian government in the investigations.\n\nIndia handed over two demarches to Pakistan – one was submitted at the Foreign Office in Islamabad by Indian High Commissioner Satyabrata Pal. The Indian Ministry of External Affairs also summoned Pakistan High Commissioner Shahid Malik on 1 December 2008, to lodge a formal protest against Pakistan's inaction against terrorist groups operating within the country.\n\nThe Indian foreign ministry released a statement, describing the actions it expects Islamabad to take. \"It was conveyed to the Pakistan high commissioner that Pakistan's actions needed to match the sentiments expressed by its leadership, that it wishes to have a qualitatively new relationship with India,\" the statement said. \"He was informed that the recent terrorist attack on Mumbai was carried out by elements from Pakistan. Government expects that strong action would be taken against those elements, whosoever they may be, responsible for this outrage,\" it said.\n\nThe CNN-IBN reported that India had asked Pakistan to hand over Mumbai Underworld Don Dawood Ibrahim, Lashkar-e-Toiba chief Hafiz Muhammad Saeed and Jaish-e-Mohammed leader Maulana Masood Azhar for their suspected involvement in the Mumbai terror attack.\n\nDawood, India’s most wanted criminal, is suspected to have helped the LeT terrorists who attacked Mumbai on 26 November. Azhar, founder of the terrorist group Jaish-e-Mohammad, is on India's most wanted list of people it accuses of terrorism. India freed Azhar from prison in exchange for passengers on a hijacked Indian Airlines Flight 814 in 1999.\n\n\"Times of India\", quoting Indian External Affairs Minister Pranab Mukherjee, reported: \"Now, we have in our demarche asked (for) the arrest and handover of those persons who are settled in Pakistan and who are fugitives of Indian law\"\n\n\"...there are lists of about 20 persons. (These) lists are sometimes altered and this exercise is going on and we have renewed it in our demarche\", Mukherjee said, adding that India \"will await\" Pakistan's response in the Indian-Arab forum.\n\nIn an interview with NDTV, Pranab Mukherjee had not ruled out the option of military strikes against terror camps in Pakistan. Mukherjee said that every country has the right to protect its territorial integrity, and take appropriate action when necessary. He also said that it has become difficult to continue the peace process with Pakistan in this.\n\nPakistan claimed that it had not received any letter from Mohammad Ajmal Amir, the lone terrorist arrested for the Mumbai attacks, seeking legal aid. It also repeated its denial on his Pakistani nationality, saying it needed \"incontrovertible\" evidence.\n\nThe Mumbai police had said that the captured terrorist Ajmal Amir had written a letter to the Pakistan High Commission in India, asking for help, and that the letter had been given to India's central government. But the Pakistan High Commissioner to India, Shahid Malik, told Karan Thapar in the \"India Tonight\" program of CNBC TV-18, that no such letter had been received.\n\nAccording to Pakistan's \"Dawn\" newspaper, Ajmal's father in Pakistan had admitted that the man seen in photographs at the CST railway station was indeed his son. But the High Commissioner said they needed \"something which is incontrovertible, which cannot be challenged in a court of law\".\n\nPakistan was also not contemplating giving Zaki-ur-Rahman Lakhvi, the alleged mastermind of the Mumbai attacks, access to the FBI for investigation.\n\nShortly after the Mumbai attacks, the Interpol team visited India and \"promised help in securing the details of the 10 gunmen who attacked several places in Mumbai, 26 November, leaving at least 173 people dead and more than 300 others injured\".\n\nInterpol Secretary General Ronald Noble arrived in Islamabad on Tuesday, 23 December, for talks over Indian allegations of involvement with Pakistan-based militant groups in the Mumbai terror attacks. Interpol secretary general Ronald Noble met Pakistan's security officials the same day.\n\nHowever, in a later press-conference, the Interpol chief said that India didn't share any specific information with the Interpol regarding the terror attacks. He said that Interpol had the same information that had been with media and the general public. Pakistan's Minister for Information and Broadcasting Sherry Rehman claimed that this statement somehow supported Pakistan government's stance that Indian government has yet to share substantial and credible evidence with them, and again offered help in due cooperation to India, for joint investigations into the Mumbai attacks.\n\nOn 10 March, India gave Interpol the DNA of the attackers.\n\nOn 7 December, US Senator John McCain relayed a message from Prime Minister Manmohan Singh, to a group of Pakistanis at a lunch in Lahore, that if Pakistan did not arrest those involved with the attacks, India would begin aerial attacks against Pakistan.\n\nPakistan Information Minister Sherry Rehman said that \"Our air force is on alert, and ready to face any eventuality\". Ali Abbas Rizvi quoted a source as saying, \"They [India] may have wanted to know whether the PAF was on five-minute alert or cockpit alert, and thereby find out the reaction time\".\n\nOn 19 December, private intelligence agency Stratfor, in its latest report, said, \"Indian military operations against targets in Pakistan have in fact been prepared, and await the signal to go forward\". They also wrote that, \"Indian military preparations, unlike previous cases, will be carried out in stealth\". India's Border Security Force (BSF) has been put on high alert on the western sector, as well as the eastern sector, in order to prevent terrorist infiltration.\n\nOn 22 December, Pakistan began combat air patrol (CAP) over several cities, including Islamabad, Lahore, and Rawalpindi, which began a panic among Pakistani civilians. Many Pakistani civilians \"started making frantic phone calls to media house[s] to enquire about whether a war has been declared\". Pakistani Foreign Minister Shah Mehmood Qureshi said, \"Pakistan defence forces and armed forces are ready to face any challenge, as Pak has the full right to defend itself\". Pakistani PM Yousuf Raza Gilani said, \"Pakistan remains united and is ready to fight anyone to defend itself\". Pakistani Defense Minister Ahmad Mukhtar Chaudhry said, \"If India tried to thrust war, then the armed forces of Pakistan have all the potential and right to defend [Pakistan]\".\n\nAccording to Pakistani media, India had started deploying troops along the Rajasthan border, and had tightened security in and around the defence airstrips. More radars and quick reaction teams were then deployed along the India-Pakistan border. Indian forces were on regular firing exercises at locations, like Lathi Firing Range in Jaisalmer, Mahsan in Bikaner, Suratgarh and Ganganagar.\n\nOn 23 December, Kamal Hyder, Al Jazeera's correspondent in Pakistan, wrote that the Pakistani \"navy, air force and army were on red alert\" and that \"the chiefs of Pakistan's three armed forces were holding what had been described as an emergency meeting at general headquarters in Rawalpindi\". He also wrote that \"[t]he Pakistani air force have been seen visibly in a number of locations flying close to the Pakistani-India border, in what is being described as an aggressive patrolling mode, following reports that India is planning pre-emptive strikes against locations in Pakistan\". A Pakistani airforce spokesperson said \"[i]n view of the current environment, the PAF has enhanced its vigilance\". Pakistani army chief General Ashfaq Parvez Kayani, said that Pakistan would mount an equal response \"within minutes\", to any Indian attack. Pakistan continued to combat air patrol over several cities.\n\nThe Taliban and affiliated groups openly declared their solidarity with Pakistan. The banned Tehrik-e-Taliban had proclaimed that they would send \"thousands of (their) well-armed militants\" in order to wage jihad against India if war should break out. Hundreds of would-be bombers were equipped with suicide jackets and explosive-laden vehicles.\n\nOn 24 December, P.K. Barbora, the air officer commanding-in-chief of Western Air Command, said \"[t]he IAF has earmarked 5,000 targets in Pakistan. But whether we will cross the LoC or the International Border to hit the enemy targets will have to be decided by the political leadership of the country\". India Today reported that \"Indian Air Force fighter planes are engaged in round the clock sorties. An unusual hectic activity of Indian Air Force has been visible along the border for past some days\". On the same day, Stratfor confirmed that \"the state government of Rajasthan has ordered residents of its border villages to be prepared for relocation\". President Asif Ali Zardari said \"We will defend the country till the last drop of our blood\", and \"[w]e will defend the country till our last breath\". Pakistan began deploying warplanes to forward air bases.\n\nOn 25 December, however, the ruling UPA government in India played down apprehensions of an imminent military conflagration. The Indian Prime Minister made it clear that \"nobody wanted war\". The Indian Air Force also downplayed the sorties by PAF fighter jets, saying it was an air defence exercise. Officials in New Delhi were amused at Pakistan air force's attempt to create war hysteria in the region. However, R. C. Dhyani, DIG of Rajasthan frontier BSF, said, \"[a] lot of military movement is being noticed in districts just across the international border for the last few days, which is not normal\" and \"Pakistan has deployed more troops across border\".\n\nLeader of the House Raza Rabbani, said that any surgical strike into its territory would be taken as an act of war and would be repulsed with \"full force\", and that \"[e]ach and every inch of the country will be safeguarded\". India moved MiG-29s to Hindon air base, located near New Delhi, in order \"to protect the capital from aerial threats\". The Pakistani city of Mianwali began a blackout.\n\nPakistan continued deployment, and moved the 10th Brigade to the outskirts of Lahore, and the 3rd Armored Brigade to Jhelum. The 10th Infantry Division and the 11th Infantry Division had been placed on high alert. The Indian Army deployed quick reaction teams (QRTs) along the border, which \"precede the movement of bridging equipment – to cross canals in Punjab – and of heavy guns\".\n\nAmir Mir of \"Daily News and Analysis\" wrote that \"Pakistan's military leadership has advised president Asif Ali Zardari to take back his statement made last month, that his country would not be the first to use nuclear weapons in the event of a conflict with India\".\n\nOn 26 December, Pakistan cancelled all military leave, and activated contacts with friendly countries and military partners. Pakistan deployed troops to \"protect vital points along the Line of Control (LoC) in Jammu and Kashmir and the international border with India\". Pakistani Foreign Minister Quresh said that, \"if war is imposed, we will respond to it like a brave, self-respecting nation\". Indian Prime Minister Manmohan Singh held a second meeting of the Nuclear Command Authority to \"discuss all the options available to India\".\n\nPakistan deployed the 14th Infantry Division to Kasur and Sialkot, close to the border. India advised its citizens not to travel to Pakistan. Indian Prime Minister Manmohan Singh met with the chiefs of the Indian air force, army, and navy.\n\nOn 27 December, India's largest opposition party, the Bharatiya Janata Party (BJP), called for all travel between India and Pakistan to be stopped, and for the recall of the Indian High Commissioner from Pakistan. The Pakistani Army alerted retired army personnel to be ready to be called back to active duty. On 28 December, Pakistan postponed all officer training courses.\n\nOn 29 December, the leaders of the Indian and Pakistani armies spoke over their red telephone, in order to avert an accidental nuclear war. The President of the BJP, Rajnath Singh, called for a joint India-US military action against Pakistan. John McCain said, \"The Indians are on the verge of some kind of attack on Pakistan\".\n\nOn 30 December, Pakistani media stated: \"The service chiefs of all of the branches of India's military were told to stay in the country in order to achieve 'complete readiness'. All units that are on exercises have been ordered to remain so indefinitely, and to indicate any equipment or ammunition they need\". However, this was not backed by Indian nor international media.\n\nThe Pakistani military had cancelled all leave. Elements of the Pakistani Airforce had been deployed to frontline bases. The IV Corp, with 60,000 troops, has been deployed to Lahore. Pakistan had deployed the 3rd Armored Brigade to Jhelum, and the 10th Brigade, with 5,000 troops, to Lahore. The 10th Division had been deployed to Ichogul and the 11th Division had been deployed to Tilla. Pakistani Army units had been deployed to Kashmir and the Jammu sector of the border. The 14th Division, with 20,000 troops, had been deployed to Kasur and Sialkot.\n\nIndia had put its Border Security Force, India's border patrol agency, on high alert. Mig-29s have been deployed to Hindon air base, in order to protect New Delhi. Later IAF sources claimed that the move was a result of intelligence inputs of an air attack on Delhi. The Indian Navy had moved six warships, including the INS \"Jalashwa\" and the INS \"Ranveer\", to the west coast.\n\nIndian and Pakistani police had exchanged DNA evidence, photographs, and items found with the attackers, to piece together a detailed portrait of the Mumbai plot. Police in Pakistan had arrested seven people, including Hammad Amin Sadiq, a homoeopathic pharmacist, who arranged bank accounts and secured supplies, and he and six others began their formal trial on 3 October 2009, in Pakistan, though Indian authorities say the prosecution stopped well short of top Lashkar leaders.\n\nUS officials feared that should the firm evidence emerge that the Mumbai terror attacks were planned and directed from within Pakistan, it would certainly escalate tension between the neighbouring countries, and could also provoke an Indian military response, even strikes against terrorists, a media report said on Saturday. US Secretary of State Condoleezza Rice, on Monday, urged Pakistan to give its \"absolute, total\" cooperation in finding those responsible for last week's attacks on Mumbai. Rice travelled to India on 4 December 2008, at the request of President George W. Bush, in the wake of the Mumbai attacks. She said that there is need for \"direct and tough action\" by Islamabad, even if the perpetrators of the Mumbai terror attacks were \"non-state actors\".\n\"Secretary Rice's visit to India is a further demonstration of the United States' commitment to stand in solidarity with the people of India, as we all work together to hold these extremists accountable\", White House spokeswoman Dana Perino said in a statement.\n\nAfter visiting India, Condoleezza Rice travelled to Pakistan on 4 December 2008, to talk with the Pakistani government. She is quoted as saying, \"We talked at length about the importance of Pakistan taking its responsibility to deal with those who may use Pakistan territory even if they are non-state actors\".\n\nHowever, some of the most wanted names on the list continue to operate openly in Pakistan. No action has been taken against them by the government of Pakistan.\n\nPolice forces in places like New York and Boston practised handling of similar situations, should they occur in the US.\n\nIn the wake of the attacks, a new system of compensation for UK citizens caught up in terrorist attacks is being considered. However, \"currently the UK government does not guarantee compensation for people injured in attacks abroad\".\n\nUK police feared similar attacks could occur in the country.\n\nThe Mumbai attack also triggered a chain of citizens' movements across India. People from all walks of life hit the streets with candles and placards to pay tributes to the victims of the tragedy almost every weekend after the 26/11 incident. While NSG commandos, Mumbai police officials, hotel staffers, etc. who participated in the operation to eliminate the terrorists became overnight heroes, the wrath of the citizens was directed towards the Government's inability to ensure adequate security to its citizens, and soon led to the chain of resignations, including that of the Indian Home Minister, Shivraj Patil. The gathering of citizens at the Gateway of India in Mumbai was unprecedented and historic. From India Gate and Jantar Mantar in New Delhi to marketplaces and street corners all across the country, India witnessed candlelight vigils by the common people. Some organised initiatives to carry forward the citizens' movement included a Hindustan Times-CNN-IBN Initiative, India Today Group's War Against Terror, and Hindustan Hamara Citizens' Initiative, besides a large number of blogs on the issue.\n\nOn 25 February 2009, new videos were released off of CCTV, displaying the inside of the hotel where the Mumbai siege took place.\n\n"}
{"id": "55127908", "url": "https://en.wikipedia.org/wiki?curid=55127908", "title": "American Association for the History of Medicine", "text": "American Association for the History of Medicine\n\nThe American Association for the History of Medicine is an American professional association dedicated to the study of medical history. It is the largest society dedicated to medical history in the United States, and the oldest such organization in North America. It was established in 1925 as the American Section of the International Society for the History of Medicine, and obtained its current name in 1958. Its first president was Fielding Hudson Garrison. Its official journal is the \"Bulletin of the History of Medicine\", which is published quarterly. Its current membership is in excess of 1,000 people.\n"}
{"id": "252507", "url": "https://en.wikipedia.org/wiki?curid=252507", "title": "American frontier", "text": "American frontier\n\nThe American frontier comprises the geography, history, folklore, and cultural expression of life in the forward wave of American expansion that began with English colonial settlements in the early 17th century and ended with the admission of the last mainland territories as states in 1912. \"Frontier\" refers to a contrasting region at the edge of a European–American line of settlement. American historians cover multiple frontiers but the folklore is focused primarily on the conquest and settlement of Native American lands west of the Mississippi River, in what is now the Midwest, Texas, the Great Plains, the Rocky Mountains, the Southwest, and the West Coast.\n\nIn 19th- and early 20th-century media, enormous popular attention was focused on the Western United States in the second half of the 19th century, a period sometimes called the \"Old West\" or the \"Wild West\". Such media typically exaggerated the romance, anarchy, and chaotic violence of the period for greater dramatic effect. This eventually inspired the Western genre of film, which spilled over into comic books, and children's toys, games and costumes. This era of massive migration and settlement was particularly encouraged by President Thomas Jefferson following the Louisiana Purchase, giving rise to the expansionist philosophy known as \"Manifest destiny\".\n\nAs defined by Hine and Faragher, \"frontier history tells the story of the creation and defense of communities, the use of the land, the development of markets, and the formation of states.\" They explain, \"It is a tale of conquest, but also one of survival, persistence, and the merging of peoples and cultures that gave birth and continuing life to America.\" Through treaties with foreign nations and native tribes; political compromise; military conquest; establishment of law and order; the building of farms, ranches, and towns; the marking of trails and digging of mines; and the pulling in of great migrations of foreigners, the United States expanded from coast to coast, fulfilling the dreams of Manifest Destiny. Historian Frederick Jackson Turner in his \"Frontier Thesis\" (1893) theorized that the frontier was a process that transformed Europeans into a new people, the Americans, whose values focused on equality, democracy, and optimism, as well as individualism, self-reliance, and even violence. Thus, Turner's Frontier Thesis proclaimed the westward frontier to be the defining process of American history.\n\nAs the American frontier passed into history, the myths of the West in fiction and film took a firm hold in the imagination of Americans and foreigners alike. In David Murdoch's view, America is \"exceptional\" in choosing its iconic self-image: \"No other nation has taken a time and place from its past and produced a construct of the imagination equal to America's creation of the West.\"\n\nThe frontier line was the outer line of European-American settlement. It moved steadily westward from the 1630s to the 1880s (with occasional movements north into Maine and Vermont, south into Florida, and east from California into Nevada). Turner favored the Census Bureau definition of the \"frontier line\" as a settlement density of two people per square mile. The \"West\" was the recently settled area near that boundary. Thus, parts of the Midwest and American South, though no longer considered \"western\", have a frontier heritage along with the modern western states. In the 21st century, however, the term \"American West\" is most often used for the area west of the Great Plains.\n\nIn the colonial era, before 1776, the west was of high priority for settlers and politicians. The American frontier began when Jamestown, Virginia was settled by the English in 1607. In the earliest days of European settlement of the Atlantic coast, until about 1680, the frontier was essentially any part of the interior of the continent beyond the fringe of existing settlements along the Atlantic coast. English, French, Spanish and Dutch patterns of expansion and settlement were quite different. Only a few thousand French migrated to Canada; these habitants settled in villages along the St. Lawrence River, building communities that remained stable for long stretches; they did not simply jump west the way the British did. Although French fur traders ranged widely through the Great Lakes and mid-west region they seldom settled down. French settlement was limited to a few very small villages such as Kaskaskia, Illinois as well as a larger settlement around New Orleans. Likewise, the Dutch set up fur trading posts in the Hudson River valley, followed by large grants of land to rich landowning patroons who brought in tenant farmers who created compact, permanent villages. They created a dense rural settlement in upstate New York, but they did not push westward.\n\nAreas in the north that were in the frontier stage by 1700 generally had poor transportation facilities, so the opportunity for commercial agriculture was low. These areas remained primarily in subsistence agriculture, and as a result by the 1760s these societies were highly egalitarian, as explained by historian Jackson Turner Main:\nIn the South, frontier areas that lacked transportation, such as the Appalachian Mountain region, remained based on subsistence farming and resembled the egalitarianism of their northern counterparts, although they had a larger upper-class of slaveowners. North Carolina was representative. However frontier areas of 1700 that had good river connections were increasingly transformed into plantation agriculture. Rich men came in, bought up the good land, and worked it with slaves. The area was no longer \"frontier\". It had a stratified society comprising a powerful upper-class white landowning gentry, a small middle-class, a fairly large group of landless or tenant white farmers, and a growing slave population at the bottom of the social pyramid. Unlike the North, where small towns and even cities were common, the South was overwhelmingly rural.\n\nThe seaboard colonial settlements gave priority to land ownership for individual farmers, and as the population grew they pushed westward for fresh farm land. Unlike Britain, where a small number of landlords owned most of the good land, ownership in America was cheap, easy and widespread. Land ownership brought a degree of independence as well as a vote for local and provincial offices. The typical New England settlements were quite compact and small—under a square mile. Conflict with the Native Americans arose out of political issues, namely who would rule. Early frontier areas east of the Appalachian Mountains included the Connecticut River valley, and northern New England (which was a move to the north, not the west).\n\nMost of the frontiers experienced Native wars, The \"French and Indian Wars\" were \"imperial wars\" between Britain and France, with the French making up for their small colonial population base by enlisting Indian war parties as allies. The series of large wars spilling over from European wars ended in a complete victory for the British in the worldwide Seven Years' War. In the peace treaty of 1763, France lost practically everything, as the lands west of the Mississippi river, in addition to Florida and New Orleans, went to Spain. Otherwise lands east of the Mississippi River and what is now Canada went to Britain.\n\nRegardless of wars Americans were moving across the Appalachians into western Pennsylvania, what is now West Virginia, and areas of the Ohio Country, Kentucky and Tennessee. In the southern settlements via the Cumberland Gap, their most famous leader was Daniel Boone, Young George Washington promoted settlements in West Virginia on lands awarded to him and his soldiers by the Royal government in payment for their wartime service in Virginia's militia. West of the mountains, settlements were curtailed briefly by a decree by the Royal Proclamation of 1763. However the Treaty of Fort Stanwix (1768) re-opened most of the western lands for frontiersmen to settle.\n\nThe first major movement west of the Appalachian mountains originated in Pennsylvania, Virginia, and North Carolina as soon as the Revolutionary War ended in 1781. Pioneers housed themselves in a rough lean-to or at most a one-room log cabin. The main food supply at first came from hunting deer, turkeys, and other abundant game.\nIn a few years, the pioneer added hogs, sheep, and cattle, and perhaps acquired a horse. Homespun clothing replaced the animal skins. The more restless pioneers grew dissatisfied with over civilized life, and uprooted themselves again to move 50 or hundred miles (80 or 160 km) further west.\n\nThe land policy of the new nation was conservative, paying special attention to the needs of the settled East. The goals sought by both parties in the 1790–1820 era were to grow the economy, avoid draining away the skilled workers needed in the East, distribute the land wisely, sell it at prices that were reasonable to settlers yet high enough to pay off the national debt, clear legal titles, and create a diversified Western economy that would be closely interconnected with the settled areas with minimal risk of a breakaway movement. By the 1830s, however, the West was filling up with squatters who had no legal deed, although they may have paid money to previous settlers. The Jacksonian Democrats favored the squatters by promising rapid access to cheap land. By contrast, Henry Clay was alarmed at the \"lawless rabble\" heading West who were undermining the utopian concept of a law-abiding, stable middle-class republican community. Rich southerners, meanwhile, looked for opportunities to buy high-quality land to set up slave plantations. The Free Soil movement of the 1840s called for low-cost land for free white farmers, a position enacted into law by the new Republican Party in 1862, offering free 160 acre (65 ha) homesteads to all adults, male and female, black and white, native-born or immigrant.\n\nAfter winning the Revolutionary War (1783), American settlers in large numbers poured into the west. In 1788, American pioneers to the Northwest Territory established Marietta, Ohio as the first permanent American settlement in the Northwest Territory.\n\nIn 1775, Daniel Boone blazed a trail for the Transylvania Company from Virginia through the Cumberland Gap into central Kentucky. It was later lengthened to reach the Falls of the Ohio at Louisville. The Wilderness Road was steep and rough, and it could only be traversed on foot or horseback, but it was the best route for thousands of settlers moving into Kentucky. In some areas they had to face Indian attacks. In 1784 alone, Indians killed over 100 travelers on the Wilderness Road. No Indians lived permanently in Kentucky but they sent raiding parties to stop the newcomers. One of those intercepted was Abraham Lincoln's grandfather, who was scalped in 1784 near Louisville.\n\nThe War of 1812 marked the final confrontation involving major British and Indian forces fighting to stop American expansion. The British war goal included the creation of an independent Indian state (under British auspices) in the Midwest. American frontier militiamen under General Andrew Jackson defeated the Creeks and opened the Southwest, while militia under Governor William Henry Harrison defeated the Indian-British alliance at the Battle of the Thames in Canada in 1813. The death in battle of the Indian leader Tecumseh dissolved the coalition of hostile Indian tribes. Meanwhile, General Andrew Jackson ended the Indian military threat in the Southeast at the Battle of Horseshoe Bend in 1814 in Alabama. In general the frontiersmen battled the Indians with little help from the U.S. Army or the federal government.\n\nTo end the War of 1812 American diplomats negotiated the Treaty of Ghent, signed in 1815, with Britain. They rejected the British plan to set up an Indian state in U.S. territory south of the Great Lakes. They explained the American policy toward acquisition of Indian lands:\n\nAs settlers poured in, the frontier districts first became territories, with an elected legislature and a governor appointed by the president. Then when population reached 100,000 the territory applied for statehood. Frontiersmen typically dropped the legalistic formalities and restrictive franchise favored by eastern upper classes, and adopting more democracy and more egalitarianism.\n\nIn 1800 the western frontier had reached the Mississippi River. St. Louis, Missouri was the largest town on the frontier, the gateway for travel westward, and a principal trading center for Mississippi River traffic and inland commerce but remained under Spanish control until 1803.\n\nThomas Jefferson thought of himself as a man of the frontier and was keenly interested in expanding and exploring the West. Jefferson's Louisiana Purchase of 1803 doubled the size of the nation at the cost of $15 million, or about $0.04 per acre ($ million in dollars, less than 42 cents per acre). Federalists opposed the expansion, but Jeffersonians hailed the opportunity to create millions of new farms to expand the domain of land-owning yeomen; the ownership would strengthen the ideal republican society, based on agriculture (not commerce), governed lightly, and promoting self-reliance and virtue, as well as form the political base for Jeffersonian Democracy.\n\nThe $15 million paid France for its sovereignty over the territory in terms of international law. Because of inflation, that $15 million is equivalent to about $294 million in 2012 dollars. Between 1803 and the 1870s, the federal government purchased the actual land from the Indian tribes then in possession of it. 20th century accountants and courts have calculated the value of the payments made to the Indians, which included future payments of cash, food, horses, cattle, supplies, buildings, schooling, and medical care. In cash terms, the total paid to the tribes in the area of the Louisiana Purchase amounted to about $2.6 billion in current dollars, or $8.5 billion in 2012 dollars (nearly $9 billion in 2016 dollars). Additional sums were paid to the Indians living east of the Mississippi for their lands, as well as payments to Indians living in parts of the west outside the Louisiana Purchase.\n\nEven before the purchase Jefferson was planning expeditions to explore and map the lands. He charged Lewis and Clark to \"explore the Missouri River, and such principal stream of it, as, by its course and communication with the waters of the Pacific Ocean; whether the Columbia, Oregon, Colorado or any other river may offer the most direct and practicable communication across the continent for the purposes of commerce\". Jefferson also instructed the expedition to study the region's native tribes (including their morals, language, and culture), weather, soil, rivers, commercial trading, and animal and plant life.\n\nEntrepreneurs, most notably John Jacob Astor quickly seized the opportunity and expanded fur trading operations into the Pacific Northwest. Astor's \"Fort Astoria\" (later Fort George), at the mouth of the Columbia River, became the first permanent white settlement in that area, although it was not profitable for Astor. He set up the American Fur Company in an attempt to break the hold that the Hudson's Bay Company monopoly had over the region. By 1820, Astor had taken over independent traders to create a profitable monopoly; he left the business as a multi-millionaire in 1834.\n\nAs the frontier moved west, trappers and hunters moved ahead of settlers, searching out new supplies of beaver and other skins for shipment to Europe. The hunters were the first Europeans in much of the Old West and they formed the first working relationships with the Native Americans in the West. They added extensive knowledge of the Northwest terrain, including the important South Pass through the central Rocky Mountains. Discovered about 1812, it later became a major route for settlers to Oregon and Washington. By 1820, however, a new \"brigade-rendezvous\" system sent company men in \"brigades\" cross-country on long expeditions, bypassing many tribes. It also encouraged \"free trappers\" to explore new regions on their own. At the end of the gathering season, the trappers would \"rendezvous\" and turn in their goods for pay at river ports along the Green River, the Upper Missouri, and the Upper Mississippi. St. Louis was the largest of the rendezvous towns. By 1830, however, fashions changed and beaver hats were replaced by silk hats, ending the demand for expensive American furs. Thus ended the era of the mountain men, trappers, and scouts such as Jedediah Smith, Hugh Glass, Davy Crockett, Jack Omohundro, and others. The trade in beaver fur virtually ceased by 1845.\n\nThere was wide agreement on the need to settle the new territories quickly, but the debate polarized over the price the government should charge. The conservatives and Whigs, typified by president John Quincy Adams, wanted a moderated pace that charged the newcomers enough to pay the costs of the federal government. The Democrats, however, tolerated a wild scramble for land at very low prices. The final resolution came in the Homestead Law of 1862, with a moderated pace that gave settlers 160 acres free after they worked on it for five years.\n\nThe private profit motive dominated the movement westward, but the Federal Government played a supporting role in securing land through treaties and setting up territorial governments, with governors appointed by the President. The federal government first acquired western territory through treaties with other nations or native tribes. Then it sent surveyors to map and document the land. By the 20th century Washington bureaucracies managed the federal lands such as the General Land Office in the Interior department, and after 1891 the Forest Service in the Department of Agriculture. After 1900 dam building and flood control became major concerns.\n\nTransportation was a key issue and the Army (especially the Army Corps of Engineers) was given full responsibility for facilitating navigation on the rivers. The steamboat, first used on the Ohio River in 1811, made possible inexpensive travel using the river systems, especially the Mississippi and Missouri rivers and their tributaries. Army expeditions up the Missouri River in 1818–25 allowed engineers to improve the technology. For example, the Army's steamboat \"Western Engineer\" of 1819 combined a very shallow draft with one of the earliest stern wheels. In 1819–25, Colonel Henry Atkinson developed keelboats with hand-powered paddle wheels.\n\nThe federal postal system played a crucial role in national expansion. It facilitated expansion into the West by creating an inexpensive, fast, convenient communication system. Letters from early settlers provided information and boosterism to encourage increased migration to the West, helped scattered families stay in touch and provide neutral help, assisted entrepreneurs to find business opportunities, and made possible regular commercial relationships between merchants and the West and wholesalers and factories back east. The postal service likewise assisted the Army in expanding control over the vast western territories. The widespread circulation of important newspapers by mail, such as the \"New York Weekly Tribune\", facilitated coordination among politicians in different states. The postal service helped integrated established areas with the frontier, creating a spirit of nationalism and providing a necessary infrastructure.\n\nGovernment and private enterprise sent many explorers to the West. In 1805–6, Army lieutenant Zebulon Pike (1779–1813) led a party of 20 soldiers to find the head waters of the Mississippi. He later explored the Red and Arkansas Rivers in Spanish territory, eventually reaching the Rio Grande. On his return, Pike sighted the peak in Colorado named after him. Major Stephen Harriman Long (1784–1864) led the Yellowstone and Missouri expeditions of 1819–1820, but his categorizing in 1823 of the Great Plains as arid and useless led to the region getting a bad reputation as the \"Great American Desert\", which discouraged settlement in that area for several decades.\n\nIn 1811, naturalists Thomas Nuttall (1786–1859) and John Bradbury (1768–1823) traveled up the Missouri River documenting and drawing plant and animal life. Artist George Catlin (1796–1872) painted accurate paintings of Native American culture. Swiss artist Karl Bodmer made compelling landscapes and portraits. John James Audubon (1785–1851) is famous for classifying and painting in minute details 500 species of birds, published in \"Birds of America.\"\n\nThe most famous of the explorers was John Charles Frémont (1813–1890), an Army officer in the Corps of Topographical Engineers. He displayed a talent for exploration and a genius at self-promotion that gave him the sobriquet of \"Pathmarker of the West\" and led him to the presidential nomination of the new Republican Party in 1856. He led a series of expeditions in the 1840s which answered many of the outstanding geographic questions about the little-known region. He crossed through the Rocky Mountains by five different routes, and mapped parts of Oregon and California. In 1846–7, he played a role in conquering California. In 1848–49, Frémont was assigned to locate a central route through the mountains for the proposed transcontinental railroad, but his expedition ended in near-disaster when it became lost and was trapped by heavy snow. His reports mixed narrative of exciting adventure with scientific data, and detailed practical information for travelers. It caught the public imagination and inspired many to head west. Goetzman says it was \"monumental in its breadth—a classic of exploring literature\".\n\nWhile colleges were springing up across the Northeast, there was little competition on the western frontier for Transylvania University, founded in Lexington, Kentucky, in 1780. It boasted of a law school in addition to its undergraduate and a medical programs. Transylvania attracted politically ambitious young men from across the Southwest, including 50 who became United States senators, 101 representatives, 36 governors, and 34 ambassadors, as well as Jefferson Davis, the president of the Confederacy.\n\nThe established Eastern churches were slow to meet the needs of the frontier. The Presbyterians and Congregationalists, since they depended on well-educated ministers, were shorthanded in evangelizing the frontier. They set up a Plan of Union of 1801 to combine resources on the frontier. Most frontiersmen showed little commitment to religion until traveling evangelists began to appear and to produce \"revivals\". The local pioneers responded enthusiastically to these events and, in effect, evolved their own populist religions, especially during the Second Great Awakening (1790–1840), which featured outdoor camp meetings lasting a week or more and which introduced many people to organized religion for the first time. One of the largest and most famous camp meetings took place at Cane Ridge, Kentucky in 1801.\n\nThe localistic Baptists set up small independent churches—Baptists abjured centralized authority; each local church was founded on the principle of independence of the local congregation. On the other hand, bishops of the well-organized, centralized Methodists assigned circuit riders to specific areas for several years at a time, then moved them to fresh territory. Several new denominations were formed, of which the largest was the Disciples of Christ.\n\nHistorian Mark Wyman calls Wisconsin a \"palimpsest\" of layer upon layer of peoples and forces, each imprinting permanent influences. He identified these layers as multiple \"frontiers\" over three centuries: Native American frontier, French frontier, English frontier, fur-trade frontier, mining frontier, and the logging frontier. Finally the coming of the railroad brought the end of the frontier.\n\nFrederick Jackson Turner grew up in Wisconsin during its last frontier stage, and in his travels around the state he could see the layers of social and political development. One of Turner's last students, Merle Curti used in-depth analysis of local Wisconsin history to test Turner's thesis about democracy. Turner's view was that American democracy, \"involved widespread participation in the making of decisions affecting the common life, the development of initiative and self-reliance, and equality of economic and cultural opportunity. It thus also involved Americanization of immigrant.\" Curti found that from 1840 to 1860 in Wisconsin the poorest groups gained rapidly in land ownership, and often rose to political leadership at the local level. He found that even landless young farmworkers were soon able to obtain their own farms. Free land on the frontier therefore created opportunity and democracy, for both European immigrants as well as old stock Yankees.\n\nFrom the 1770s to the 1830s, pioneers moved into the new lands that stretched from Kentucky to Alabama to Texas. Most were farmers who moved in family groups.\n\nHistorian Louis Hacker shows how wasteful the first generation of pioneers was; they were too ignorant to cultivate the land properly and when the natural fertility of virgin land was used up, they sold out and moved west to try again. Hacker describes that in Kentucky about 1812:\nHacker adds that the second wave of settlers reclaimed the land, repaired the damage, and practiced a more sustainable agriculture. Historian Frederick Jackson Turner explored the individualistic world view and values of the first generation:\n\nManifest Destiny was the belief that the United States was pre-ordained to expand from the Atlantic coast to the Pacific coast. The concept was expressed during Colonial times, but the term was coined in the 1840s by a popular magazine which editorialized, \"the fulfillment of our manifest destiny...to overspread the continent allotted by Providence for the free development of our yearly multiplying millions.\" As the nation grew, \"Manifest destiny\" became a rallying cry for expansionists in the Democratic Party. In the 1840s the Tyler and Polk administrations (1841–49) successfully promoted this nationalistic doctrine. However the Whig Party, which represented business and financial interests, stood opposed to Manifest Destiny. Whig leaders such as Henry Clay and Abraham Lincoln called for deepening the society through modernization and urbanization instead of simple horizontal- expansion. Starting with the annexation of Texas, the expansionists got the upper hand. John Quincy Adams, an anti-slavery Whig, felt the Texas annexation in 1845 to be \"the heaviest calamity that ever befell myself and my country\".\nHelping settlers move westward were the emigrant \"guide books\" of the 1840s featuring route information supplied by the fur traders and the Frémont expeditions, and promising fertile farm land beyond the Rockies.\n\nMexico became independent of Spain in 1821, and took over Spain's northern possessions stretching from Texas to California. Caravans began delivering goods to Mexico's Santa Fe along the Santa Fe Trail, over the journey which took 48 days from Kansas City, Missouri (then known as Westport). Santa Fe was also the trailhead for the \"El Camino Real\" (the King's Highway), a trade route which carried American manufactured goods southward deep into Mexico and returned silver, furs, and mules northward (not to be confused with another \"Camino Real\" which connected the missions in California). A branch also ran eastward near the Gulf (also called the Old San Antonio Road). Santa Fe connected to California via the Old Spanish Trail.\n\nThe Spanish and Mexican governments attracted American settlers to Texas with generous terms. Stephen F. Austin became an \"empresario\", receiving contracts from the Mexican officials to bring in immigrants. In doing so, he also became the \"de facto\" political and military commander of the area. Tensions rose, however, after an abortive attempt to establish the independent nation of Fredonia in 1826. William Travis, leading the \"war party\", advocated for independence from Mexico, while the \"peace party\" led by Austin attempted to get more autonomy within the current relationship. When Mexican president Santa Anna shifted alliances and joined the conservative Centralist party, he declared himself dictator and ordered soldiers into Texas to curtail new immigration and unrest. However, immigration continued and 30,000 Anglos with 3,000 slaves were settled in Texas by 1835. In 1836, the Texas Revolution erupted. Following losses at the Alamo and Goliad, the Texians won the decisive Battle of San Jacinto to secure independence. At San Jacinto, Sam Houston, commander-in-chief of the Texian Army and future President of the Republic of Texas famously shouted \"Remember the Alamo! Remember Goliad\". The U.S. Congress declined to annex Texas, stalemated by contentious arguments over slavery and regional power. Thus, the Republic of Texas remained an independent power for nearly a decade before it was annexed as the 28th state in 1845. The government of Mexico, however, viewed Texas as a runaway province and asserted its ownership.\n\nMexico refused to recognize the independence of Texas in 1836, but the U.S. and European powers did so. Mexico threatened war if Texas joined the U.S., which it did in 1845. American negotiators were turned away by a Mexican government in turmoil. When the Mexican army killed 16 American soldiers in disputed territory war was at hand. Whigs, such as Congressman Abraham Lincoln denounced the war, but it was quite popular outside New England.\n\nThe Mexican strategy was defensive; the American strategy was a three pronged offensive, using large numbers of volunteer soldiers. Overland forces seized New Mexico with little resistance and headed to California, which quickly fell to the American land and naval forces. From the main American base at New Orleans, General Zachary Taylor led forces into northern Mexico, winning a series of battles that ensued. The U.S. Navy transported General Winfield Scott to Veracruz. He then marched his 12,000-man force west to Mexico City, winning the final battle at Chapultepec. Talk of acquiring all of Mexico fell away when the army discovered the Mexican political and cultural values were so alien to America's. As the \"Cincinnati Herald\" asked, what would the U.S. do with eight million Mexicans \"with their idol worship, heathen superstition, and degraded mongrel races?\"\n\nThe Treaty of Guadalupe Hidalgo of 1848 ceded the territories of California and New Mexico to the United States for $18.5 million (which included the assumption of claims against Mexico by settlers). The Gadsden Purchase in 1853 added southern Arizona, which was needed for a railroad route to California. In all Mexico ceded half a million square miles (1.3 million km) and included the states-to-be of California, Utah, Arizona, Nevada, New Mexico, and parts of Colorado and Wyoming, in addition to Texas. Managing the new territories and dealing with the slavery issue caused intense controversy, particularly over the Wilmot Proviso, which would have outlawed slavery in the new territories. Congress never passed it, but rather temporarily resolved the issue of slavery in the West with the Compromise of 1850. California entered the Union in 1850 as a free state; the other areas remained territories for many years.\n\nThe new state grew rapidly as migrants poured into the fertile cotton lands of east Texas. German immigrants started to arrive in the early 1840s because of negative economic, social and political pressures in Germany. With their investments in cotton lands and slaves, planters established cotton plantations in the eastern districts. The central area of the state was developed more by subsistence farmers who seldom owned slaves.\n\nTexas in its Wild West days attracted men who could shoot straight and possessed the zest for adventure, \"for masculine renown, patriotic service, martial glory and meaningful deaths\".\n\nIn 1846 about 10,000 Californios (Hispanics) lived in California, primarily on cattle ranches in what is now the Los Angeles area. A few hundred foreigners were scattered in the northern districts, including some Americans. With the outbreak of war with Mexico in 1846 the U.S. sent in Frémont and a U.S. Army unit, as well as naval forces, and quickly took control. As the war was ending, gold was discovered in the north, and the word soon spread worldwide.\n\nThousands of \"Forty-Niners\" reached California, by sailing around South America (or taking a short-cut through disease-ridden Panama), or walked the California trail. The population soared to over 200,000 in 1852, mostly in the gold districts that stretched into the mountains east of San Francisco.\n\nHousing in San Francisco was at a premium, and abandoned ships whose crews had headed for the mines were often converted to temporary lodging. In the gold fields themselves living conditions were primitive, though the mild climate proved attractive. Supplies were expensive and food poor, typical diets consisting mostly of pork, beans, and whiskey. These highly male, transient communities with no established institutions were prone to high levels of violence, drunkenness, profanity, and greed-driven behavior. Without courts or law officers in the mining communities to enforce claims and justice, miners developed their own ad hoc legal system, based on the \"mining codes\" used in other mining communities abroad. Each camp had its own rules and often handed out justice by popular vote, sometimes acting fairly and at times exercising vigilantism—with Indians, Mexicans, and Chinese generally receiving the harshest sentences.\n\nThe gold rush radically changed the California economy and brought in an array of professionals, including precious metal specialists, merchants, doctors, and attorneys, who added to the population of miners, saloon keepers, gamblers, and prostitutes. A San Francisco newspaper stated, \"The whole country... resounds to the sordid cry of gold! Gold! \"Gold!\" while the field is left half planted, the house half built, and everything neglected but the manufacture of shovels and pick axes.\" Over 250,000 miners found a total of more than $200 million in gold in the five years of the California Gold Rush. As thousands arrived, however, fewer and fewer miners struck their fortune, and most ended exhausted and broke.\n\nViolent bandits often preyed upon the miners, such as the case of Jonathan R. Davis' killing of eleven bandits single-handedly. Camps spread out north and south of the American River and eastward into the Sierras. In a few years, nearly all of the independent miners were displaced as mines were purchased and run by mining companies, who then hired low-paid salaried miners. As gold became harder to find and more difficult to extract, individual prospectors gave way to paid work gangs, specialized skills, and mining machinery. Bigger mines, however, caused greater environmental damage. In the mountains, shaft mining predominated, producing large amounts of waste. Beginning in 1852, at the end of the '49 gold rush, through 1883, hydraulic mining was used. Despite huge profits being made, it fell into the hands of a few capitalists, displaced numerous miners, vast amounts of waste entered river systems, and did heavy ecological damage to the environment. Hydraulic mining ended when public outcry over the destruction of farmlands led to the outlawing of this practice.\n\nThe mountainous areas of the triangle from New Mexico to California to South Dakota contained hundreds of hard rock mining sites, where prospectors discovered gold, silver, copper and other minerals (as well as some soft-rock coal). Temporary mining camps sprang up overnight; most became ghost towns when the ores were depleted. Prospectors spread out and hunted for gold and silver along the Rockies and in the southwest. Soon gold was discovered in Colorado, Utah, Arizona, New Mexico, Idaho, Montana, and South Dakota (by 1864).\nThe discovery of the Comstock Lode, containing vast amounts of silver, resulted in the Nevada boomtowns of Virginia City, Carson City, and Silver City. The wealth from silver, more than from gold, fueled the maturation of San Francisco in the 1860s and helped the rise of some of its wealthiest families, such as that of George Hearst.\n\nTo get to the rich new lands of the West Coast, there were two options: some sailed around the southern tip of South America during a six-month voyage, but 400,000 others walked there on an overland route of more than 2,000 miles (3,000 km); their wagon trains usually left from Missouri. They moved in large groups under an experienced wagonmaster, bringing their clothing, farm supplies, weapons, and animals. These wagon trains followed major rivers, crossed prairies and mountains, and typically ended in Oregon and California. Pioneers generally attempted to complete the journey during a single warm season, usually over the course of six months. By 1836, when the first migrant wagon train was organized in Independence, Missouri, a wagon trail had been cleared to Fort Hall, Idaho. Trails were cleared further and further west, eventually reaching all the way to the Willamette Valley in Oregon. This network of wagon trails leading to the Pacific Northwest was later called the Oregon Trail. The eastern half of the route was also used by travelers on the California Trail (from 1843), Mormon Trail (from 1847), and Bozeman Trail (from 1863) before they turned off to their separate destinations.\n\nIn the \"Wagon Train of 1843\", some 700 to 1,000 emigrants headed for Oregon; missionary Marcus Whitman led the wagons on the last leg. In 1846, the Barlow Road was completed around Mount Hood, providing a rough but passable wagon trail from the Missouri River to the Willamette Valley: about 2,000 miles (3,000 km). Though the main direction of travel on the early wagon trails was westward, people also used the Oregon Trail to travel eastward. Some did so because they were discouraged and defeated. Some returned with bags of gold and silver. Most were returning to pick up their families and move them all back west. These \"gobacks\" were a major source of information and excitement about the wonders and promises—and dangers and disappointments—of the far West.\n\nNot all emigrants made it to their destination. The dangers of the overland route were numerous: snakebites, wagon accidents, violence from other travelers, suicide, malnutrition, stampedes, Indian attacks, a variety of diseases (dysentery, typhoid, and cholera were among the most common), exposure, avalanches, etc. One particularly well-known example of the treacherous nature of the journey is the story of the ill-fated Donner Party, which became trapped in the Sierra Nevada mountains during the winter of 1846–1847 in which nearly half of the 90 people traveling with the group died from starvation and exposure, and some resorted to cannibalism to survive. Another story of cannibalism featured Alfred Packer and his trek to Colorado in 1874. There were also frequent attacks from bandits and highwaymen, such as the infamous Harpe brothers who patrolled the frontier routes and targeted migrant groups.\n\nIn Missouri and Illinois, animosity between the Mormon settlers and locals grew, which would mirror those in other states such as Utah years later. Violence finally erupted on October 24, 1838 when militias from both sides clashed and a mass killing of Mormons in Livingston County occurred 6 days later. An executive order was filed during these conflicts, and the Mormons were forced to scatter. Brigham Young, seeking to leave American jurisdiction to escape religious persecution in Illinois and Missouri, led the Mormons to the valley of the Great Salt Lake, owned at the time by Mexico but not controlled by them. A hundred rural Mormon settlements sprang up in what Young called \"Deseret\", which he ruled as a theocracy. It later became Utah Territory. Young's Salt Lake City settlement served as the hub of their network, which reached into neighboring territories as well. The communalism and advanced farming practices of the Mormons enabled them to succeed. They sold goods to wagon trains passing through and came to terms with local Indian tribes because Young decided it was cheaper to feed the Indians than fight them. Education became a high priority to protect the beleaguered group, reduce heresy and maintain group solidarity.\n\nThe great threat to the Mormons in Utah was the U.S. government, which took ownership of Utah in 1848, and pushed by the Protestant churches, rejected theocracy and polygamy. The Republican Party swore to destroy polygamy, which it saw as an affront to religious, cultural and moral values of a modern civilization. Confrontations verged on open warfare in the late 1850s as President Buchanan sent in troops. Although there were no military battles fought, and negotiations led to a stand down, violence still escalated and there were a number of casualties. After the Civil War the federal government systematically took control of Utah away from the Mormons, and drove the church's leadership underground. Meanwhile, aggressive missionary work in the U.S. and Europe brought a flood of Mormon converts to Utah. Finally in 1890 the Church leadership announced polygamy was no longer a central tenet, and a compromise was reached, with Utah becoming a state and the Mormons dividing into Republicans and Democrats.\n\nThe federal government provided subsidies for the development of mail and freight delivery, and by 1856, Congress authorized road improvements and an overland mail service to California. The new commercial wagon trains service primarily hauled freight. In 1858 John Butterfield (1801–69) established a stage service that went from Saint Louis to San Francisco in 24 days along a southern route. This route was abandoned in 1861 after Texas joined the Confederacy, in favor of stagecoach services established via Fort Laramie and Salt Lake City, a 24-day journey, with Wells Fargo & Co. as the foremost provider (initially using the old \"Butterfield\" name).\n\nWilliam Russell, hoping to get a government contract for more rapid mail delivery service, started the Pony Express in 1860, cutting delivery time to ten days. He set up over 150 stations about apart.\n\nIn 1861 Congress passed the Land-Grant Telegraph Act which financed the construction of Western Union's transcontinental telegraph lines. Hiram Sibley, Western Union's head, negotiated exclusive agreements with railroads to run telegraph lines along their right-of-way. Eight years before the transcontinental railroad opened, the First Transcontinental Telegraph linked Omaha, Nebraska and San Francisco (and points in-between) on October 24, 1861. The Pony Express ended in just 18 months because it could not compete with the telegraph.\n\nConstitutionally, Congress could not deal with slavery in the states but it did have jurisdiction in the western territories. California unanimously rejected slavery in 1850 and became a free state. New Mexico allowed slavery, but it was rarely seen there. Kansas was off limits to slavery by the Compromise of 1820. Free Soil elements feared that if slavery were allowed rich planters would buy up the best lands and work them with gangs of slaves, leaving little opportunity for free white men to own farms. Few Southern planters were actually interested in Kansas, but the idea that slavery was illegal there implied they had a second-class status that was intolerable to their sense of honor, and seemed to violate the principle of state's rights. With the passage of the extremely controversial Kansas–Nebraska Act in 1854, Congress left the decision up to the voters on the ground in Kansas. Across the North a new major party was formed to fight slavery: the Republican Party, with numerous westerners in leadership positions, most notably Abraham Lincoln of Illinois. To influence the territorial decision, anti-slavery elements (also called \"Jayhawkers\" or \"Free-soilers\") financed the migration of politically determined settlers. But pro-slavery advocates fought back with pro-slavery settlers from Missouri. Violence on both sides was the result; in all 56 men were killed by the time the violence abated in 1859. By 1860 the pro-slavery forces were in control—but Kansas had only two slaves. The antislavery forces took over by 1861, as Kansas became a free state. The episode demonstrated that a democratic compromise between North and South over slavery was impossible and served to hasten the Civil War.\n\nDespite its large territory, the trans-Mississippi West had a small population and its wartime story has to a large extent been underplayed in the historiography of the American Civil War.\n\nThe Confederacy engaged in several important campaigns in the West. However, Kansas, a major area of conflict building up to the war, was the scene of only one battle, at Mine Creek. But its proximity to Confederate lines enabled pro-Confederate guerrillas, such as Quantrill's Raiders, to attack Union strongholds and massacre the residents.\n\nIn Texas, citizens voted to join the Confederacy; anti-war Germans were hanged. Local troops took over the federal arsenal in San Antonio, with plans to grab the territories of northern New Mexico, Utah, and Colorado, and possibly California. Confederate Arizona was created by Arizona citizens who wanted protection against Apache raids after the United States Army units were moved out. The Confederacy then sets its sight to gain control of the New Mexico Territory. General Henry Hopkins Sibley was tasked for the campaign, and together with his New Mexico Army, marched right up the Rio Grande in an attempt to take the mineral wealth of Colorado as well as California. The First Regiment of Volunteers discovered the rebels, and they immediately warned and joined the Yankees at Fort Union. The Battle of Glorieta Pass soon erupted, and the Union ended the Confederate campaign and the area west of Texas remained in Union hands.\n\nMissouri, a Union state where slavery was legal, became a battleground when the pro-secession governor, against the vote of the legislature, led troops to the federal arsenal at St. Louis; he was aided by Confederate forces from Arkansas and Louisiana. However Union General Samuel Curtis regained St. Louis and all of Missouri for the Union. The state was the scene of numerous raids and guerrilla warfare in the west.\n\nThe U.S. Army after 1850 established a series of military posts across the frontier, designed to stop warfare among Indian tribes or between Indians and settlers. Throughout the 19th century, Army officers typically served built their careers in peacekeeper roles moving from fort to fort until retirement. Actual combat experience was uncommon for any one soldier.\n\nThe most dramatic conflict was the Sioux war in Minnesota in 1862, when Dakota tribes systematically attacked German farms in an effort to drive out the settlers. Over a period of several days, Dakota attacks at the Lower Sioux Agency, New Ulm and Hutchinson, slaughtered 300 to 400 white settlers. The state militia fought back and Lincoln sent in federal troops. The ensuing battles at Fort Ridgely, Birch Coulee, Fort Abercrombie, and Wood Lake punctuated a six-week war, which ended in American victory. The federal government tried 425 Indians for murder, and 303 were convicted and sentenced to death. Lincoln pardoned the majority, but 38 leaders were hanged .\n\nThe decreased presence of Union troops in the West left behind untrained militias; hostile tribes used the opportunity to attack settlers. The militia struck back hard, most notably by attacking the winter quarters of the Cheyenne and Arapaho Indians, filled with women and children, at the Sand Creek massacre in eastern Colorado in late 1864.\n\nKit Carson and the U.S. Army in 1864 trapped the entire Navajo tribe in New Mexico, where they had been raiding settlers, and put them on a reservation. Within the Indian Territory, now Oklahoma, conflicts arose among the Five Civilized Tribes, most of which sided with the South being slaveholders themselves.\n\nIn 1862, Congress enacted two major laws to facilitate settlement of the West: the Homestead Act and the Pacific Railroad Act. The result by 1890 was millions of new farms in the Plains states, many operated by new immigrants from Germany and Scandinavia.\n\nWith the war over and slavery abolished, the federal government focused on improving the governance of the territories. It subdivided several territories, preparing them for statehood, following the precedents set by the Northwest Ordinance of 1787. It standardized procedures and the supervision of territorial governments, taking away some local powers, and imposing much \"red tape\", growing the federal bureaucracy significantly.\n\nFederal involvement in the territories was considerable. In addition to direct subsidies, the federal government maintained military posts, provided safety from Indian attacks, bankrolled treaty obligations, conducted surveys and land sales, built roads, staffed land offices, made harbour improvements, and subsidized overland mail delivery. Territorial citizens came to both decry federal power and local corruption, and at the same time, lament that more federal dollars were not sent their way.\n\nTerritorial governors were political appointees and beholden to Washington so they usually governed with a light hand, allowing the legislatures to deal with the local issues. In addition to his role as civil governor, a territorial governor was also a militia commander, a local superintendent of Indian affairs, and the state liaison with federal agencies. The legislatures, on the other hand, spoke for the local citizens and they were given considerable leeway by the federal government to make local law.\n\nThese improvements to governance still left plenty of room for profiteering. As Mark Twain wrote while working for his brother, the secretary of Nevada, \"The government of my country snubs honest simplicity, but fondles artistic villainy, and I think I might have developed into a very capable pickpocket if I had remained in the public service a year or two.\" \"Territorial rings\", corrupt associations of local politicians and business owners buttressed with federal patronage, embezzled from Indian tribes and local citizens, especially in the Dakota and New Mexico territories.\n\nIn acquiring, preparing, and distributing public land to private ownership, the federal government generally followed the system set forth by the Land Ordinance of 1785. Federal exploration and scientific teams would undertake reconnaissance of the land and determine Native American habitation. Through treaty, land title would be ceded by the resident tribes. Then surveyors would create detailed maps marking the land into squares of six miles (10 km) on each side, subdivided first into one square mile blocks, then into lots. Townships would be formed from the lots and sold at public auction. Unsold land could be purchased from the land office at a minimum price of $1.25 per acre.\n\nAs part of public policy, the government would award public land to certain groups such as veterans, through the use of \"land script\". The script traded in a financial market, often at below the $1.25 per acre minimum price set by law, which gave speculators, investors, and developers another way to acquire large tracts of land cheaply. Land policy became politicized by competing factions and interests, and the question of slavery on new lands was contentious. As a counter to land speculators, farmers formed \"claims clubs\" to enable them to buy larger tracts than the allotments by trading among themselves at controlled prices.\n\nIn 1862, Congress passed three important bills that transformed the land system. The Homestead Act granted free to each settler who improved the land for five years; citizens and non-citizens including squatters and women, were all eligible. The only cost was a modest filing fee. The law was especially important in the settling of the Plains states. Many took free homestead and others purchased their land from railroads at low rates.\n\nThe Pacific Railway Acts of 1862 provided for the land needed to build the transcontinental railroad. The land given the railroads alternated with government-owned tracts saved for free distribution to homesteaders. In an effort to be equitable, the federal government reduced each tract to because of its perceived higher value given its proximity to the rail line. Railroads had up to five years to sell or mortgage their land, after tracks were laid, after which unsold land could be purchased by anyone. Often railroads sold some of their government acquired land to homesteaders immediately to encourage settlement and the growth of markets the railroads would then be able to serve. Nebraska railroads in the 1870s were strong boosters of lands along their routes. They sent agents to Germany and Scandinavia with package deals that included cheap transportation for the family as well as its furniture and farm tools, and they offered long-term credit at low rates. Boosterism succeeded in attracting adventurous American and European families to Nebraska, helping them purchase land grant parcels on good terms. The selling price depended on such factors as soil quality, water, and distance from the railroad.\n\nThe Morrill Act of 1862 provided land grants to states to begin colleges of agriculture and mechanical arts (engineering). Black colleges became eligible for these land grants in 1890. The Act succeeded in its goals to open new universities and make farming more scientific and profitable.\n\nIn the 1850s government sponsored surveys to chart the remaining unexplored regions of the West, and to plan possible routes for a transcontinental railroad. Much of this work was undertaking by the Corps of Engineers, Corps of Topographical Engineers, and Bureau of Explorations and Surveys, and became known as \"The Great Reconnaissance\". Regionalism animated debates in Congress regarding the choice of a northern, central or southern route. Engineering requirements for the rail route were an adequate supply of water and wood, and as nearly-level route as possible, given the weak locomotives of the era.\n\nIn the 1850s, proposals to build a transcontinental failed because of Congressional disputes over slavery. With the secession of the Confederate states in 1861, the modernizers in the Republican party took over Congress and wanted a line to link to California. Private companies were to build and operate the line. Construction would be done by unskilled laborers who would live in temporary camps along the way. Immigrants from China and Ireland did most of the construction work. Theodore Judah, the chief engineer of the Central Pacific surveyed the route from San Francisco east. Judah's tireless lobbying efforts in Washington were largely responsible for the passage of the 1862 Pacific Railroad Act, which authorized construction of both the Central Pacific and the Union Pacific (which built west from Omaha). In 1862 four rich San Francisco merchants (Leland Stanford, Collis Huntington, Charles Crocker, and Mark Hopkins) took charge, with Crocker in charge of construction. The line was completed in May 1869. Coast-to-coast passenger travel in 8 days now replaced wagon trains or sea voyages that took 6 to 10 months and cost much more.\n\nThe road was built with mortgages from New York, Boston and London, backed by land grants. There were no federal cash subsidies, But there was a loan to the Central Pacific that was eventually repaid at six percent interest. The federal government offered land-grants in a checkerboard pattern. The railroad sold every-other square, with the government opening its half to homesteaders. The government also loaned money—later repaid—at $16,000 per mile on level stretches, and $32,000 to $48,000 in mountainous terrain. Local and state governments also aided the financing.\n\nMost of the manual laborers on the Central Pacific were new arrivals from China. Kraus shows how these men lived and worked, and how they managed their money. He concludes that senior officials quickly realized the high degree of cleanliness and reliability of the Chinese. The Central Pacific employed over 12,000 Chinese workers, 90% of its manual work force. Ong explores whether or not the Chinese Railroad Workers were exploited by the railroad, with whites in the better positions. He finds the railroad set different wage rates for whites and Chinese and used the latter in the more menial and dangerous jobs, such as the handling and the pouring of nitroglycerin. However the railroad also provided camps and food the Chinese wanted and protected the Chinese workers from threats from whites.\n\nBuilding the railroad required six main activities: surveying the route, blasting a right of way, building tunnels and bridges, clearing and laying the roadbed, laying the ties and rails, and maintaining and supplying the crews with food and tools. The work was highly physical, using horse-drawn plows and scrapers, and manual picks, axes, sledgehammers, and handcarts. A few steam-driven machines, such as shovels, were used. The rails were iron (steel came a few years later) and weighed . and required five men to lift. For blasting, they used black powder. The Union Pacific construction crews, mostly Irish Americans, averaged about two miles (3 km) of new track per day.\n\nSix transcontinental railroads were built in the Gilded Age (plus two in Canada); they opened up the West to farmers and ranchers. From north to south they were the Northern Pacific, Milwaukee Road, and Great Northern along the Canada–US border; the Union Pacific/Central Pacific in the middle, and to the south the Santa Fe, and the Southern Pacific. All but the Great Northern of James J. Hill relied on land grants. The financial stories were often complex. For example, the Northern Pacific received its major land grant in 1864. Financier Jay Cooke (1821–1905) was in charge until 1873, when he went bankrupt. Federal courts, however, kept bankrupt railroads in operation. In 1881 Henry Villard (1835–1900) took over and finally completed the line to Seattle. But the line went bankrupt in the Panic of 1893 and Hill took it over. He then merged several lines with financing from J.P. Morgan, but President Theodore Roosevelt broke them up in 1904.\n\nIn the first year of operation, 1869–70, 150,000 passengers made the long trip. Settlers were encouraged with promotions to come West on free scouting trips to buy railroad land on easy terms spread over several years. The railroads had \"Immigration Bureaus\" which advertised package low-cost deals including passage and land on easy terms for farmers in Germany and Scandinavia. The prairies, they were promised, did not mean backbreaking toil because \"settling on the prairie which is ready for the plow is different from plunging into a region covered with timber\". The settlers were customers of the railroads, shipping their crops and cattle out, and bringing in manufactured products. All manufacturers benefited from the lower costs of transportation and the much larger radius of business.\n\nWhite concludes with a mixed verdict. The transcontinentals did open up the West to settlement, brought in many thousands of high-tech, highly paid workers and managers, created thousands of towns and cities, oriented the nation onto an east–west axis, and proved highly valuable for the nation as a whole. On the other hand, too many were built, and they were built too far ahead of actual demand. The result was a bubble that left heavy losses to investors, and led to poor management practices. By contrast, as White notes, the lines in the Midwest and East supported by a very large population base, fostered farming, industry and mining while generating steady profits and receiving few government benefits.\n\nAfter the Civil War, many from the East Coast and Europe were lured west by reports from relatives and by extensive advertising campaigns promising \"the Best Prairie Lands\", \"Low Prices\", \"Large Discounts For Cash\", and \"Better Terms Than Ever!\". The new railroads provided the opportunity for migrants to go out and take a look, with special family tickets, the cost of which could be applied to land purchases offered by the railroads. Farming the plains was indeed more difficult than back east. Water management was more critical, lightning fires were more prevalent, the weather was more extreme, rainfall was less predictable.\n\nThe fearful stayed home. The actual migrants looked beyond fears of the unknown. Their chief motivation to move west was to find a better economic life than the one they had. Farmers sought larger, cheaper and more fertile land; merchants and tradesman sought new customers and new leadership opportunities. Laborers wanted higher paying work and better conditions. As settlers move West, they have to faced challenges along the way, such as the lack of wood for housing, bad weather like blizzards and droughts, and fearsome tornadoes. In the treeless prairies homesteaders built sod houses. One of the greatest plague that hit the homesteaders was the 1874 Locust Plague which devastated the Great Plains. These challenges hardened these settlers in taming the frontier.\n\nIn 1889, Washington opened of unoccupied lands in the Oklahoma territory. On April 22, over 100,000 settlers and cattlemen (known as \"boomers\") lined up at the border, and when the army's guns and bugles giving the signal, began a mad dash to stake their claims in the Land Run of 1889. A witness wrote, \"The horsemen had the best of it from the start. It was a fine race for a few minutes, but soon the riders began to spread out like a fan, and by the time they reached the horizon they were scattered about as far as the eye could see\". In a single day, the towns of Oklahoma City, Norman, and Guthrie came into existence. In the same manner, millions of acres of additional land was opened up and settled in the following four years.\n\nFearful of takeover of Alaska (then Russian America) from the British Army based in British North America and due to lack of economic interests, the Russian Empire, which had established a presence in Alaska in the mid-18th century, was eager to get rid of the territory. They first approached the United States about selling the territory in the late 1850s, but negotiations were stalled by the outbreak of the Civil War. After 1865, Secretary of State William Seward, a supporter of territorial expansion, was eager to acquire the tremendous landmass of Alaska, an area roughly one-fifth the size of the rest of the United States. On March 30, 1867, the U.S. purchased the territory from the Russians for $7.2 million ($118 million in today's dollars). The transfer ceremony was completed in Sitka on October 18 of that same year where Russian soldiers handed over the territory to the United States Army.\n\nCritics at the time decried the purchase as \"Seward's Folly\", reasoning that there was no natural resources in the new territory and no one can be bothered to live in such a cold, icy climate. Although the development and settlement of Alaska grew slow, the discovery of gold fields during the Klondike Gold Rush in 1896 and Nome Gold Rush in 1898 brought thousands of migrants and immigrants into the territory, thus propelling Alaska's prosperity for decades to come. Alaska is later known as \"America's Last Frontier\", as the U.S. completed westward in the North American continent.\n\nIndian wars have occurred throughout the United States though the conflicts are generally separated into two categories; the Indian wars east of the Mississippi River and the Indian wars west of the Mississippi. The U.S. Bureau of the Census (1894) provided an estimate of deaths:\nHistorian Russell Thornton estimates that from 1800 to 1890, the Indian population declined from 600,000 to as few as 250,000. The depopulation was principally caused by disease as well as warfare. Many tribes in Texas, such as the Karankawan, Akokisa, Bidui and others, were extinguished due to conflicts with settlers. The rapid depopulation of the American Indians after the Civil War alarmed the U.S. Government, and the Doolittle Committee was formed to investigate the causes as well recommendations to save the population. The solutions presented by the committee, such as the establishment of the five boards of inspection to prevent Indian abuses, had little effect as large Western migration commenced.\n\nThe expansion of migration into the Southeastern United States in the 1820s to the 1830s forced the federal government to deal with the \"Indian question\". The Indians were under federal control but were independent of state governments. State legislatures and state judges had no authority on their lands, and the states demanded control. Politically the new Democratic Party of President Andrew Jackson demanded removal of the Indians out of the southeastern states to new lands in the west, while the Whig Party and the Protestant churches were opposed to removal. The Jacksonian Democracy proved irresistible, as it won the presidential elections of 1828, 1832 and 1836. By 1837 the \"Indian Removal policy\" began, to implement the act of Congress signed by Andrew Jackson in 1830. Many historians have sharply attacked Jackson. The 1830 law theoretically provided for voluntary removal and had safeguards for the rights of Indians, but in reality the removal was involuntary, brutal and ignored safeguards. Jackson justified his actions by stating that Indians had \"neither the intelligence, the industry, the moral habits, nor the desire of improvements\".\n\nThe forced march of about twenty tribes included the \"Five Civilized Tribes\" (Creek, Choctaw, Cherokee, Chickasaw, and Seminole). To motivate natives reluctant to move, the federal government also promised rifles, blankets, tobacco, and cash. By 1835 the Cherokee, the last Indian nation in the South, had signed the removal treaty and relocated to Oklahoma. All the tribes were given new land in the \"Indian Territory\" (which later became Oklahoma). Of the approximate 70,000 Indians removed, about 18,000 died from disease, starvation, and exposure on the route. This exodus has become known as The Trail of Tears (in Cherokee \"Nunna dual Tsuny\", \"The Trail Where they Cried\"). The impact of the removals was severe. The transplanted tribes had considerable difficulty adapting to their new surroundings and sometimes clashed with the tribes native to the area.\n\nThe only way for an Indian to remain and avoid removal was to accept the federal offer of or more of land (depending on family size) in exchange for leaving the tribe and becoming a state citizen subject to state law and federal law. However, many natives who took the offer were defrauded by \"ravenous speculators\" who stole their claims and sold their land to whites. In Mississippi alone, fraudulent claims reached . Of the five tribes, the Seminole offered the most resistance, hiding out in the Florida swamps and waging a war which cost the U.S. Army 1,500 lives and $20 million.\n\nIndian warriors in the West, using their traditional style of limited, battle-oriented warfare, confronted the U.S. Army. The Indians emphasized bravery in combat while the Army put its emphasis not so much on individual combat as on building networks of forts, developing a logistics system, and using the telegraph and railroads to coordinate and concentrate its forces. Plains Indian intertribal warfare bore no resemblance to the \"modern\" warfare practiced by the Americans along European lines, using its vast advantages in population and resources. Many tribes avoided warfare and others supported the U.S. Army. The tribes hostile to the government continued to pursue their traditional brand of fighting and, therefore, were unable to have any permanent success against the Army.\n\nIndian wars were fought throughout the western regions, with more conflicts in the states bordering Mexico than in the interior states. Arizona ranked highest, with 310 known battles fought within the state's boundaries between Americans and the natives. Arizona ranked highest in war deaths, with 4,340 killed, including soldiers, civilians and Native Americans. That was more than twice as many as occurred in Texas, the second highest ranking state. Most of the deaths in Arizona were caused by the Apache. Michno also says that fifty-one percent of the Indian war battles between 1850 and 1890 took place in Arizona, Texas and New Mexico, as well as thirty-seven percent of the casualties in the county west of the Mississippi River.\n\nOne of the deadliest Indian wars fought was the Snake War in 1864–1868, which was conducted by a confederacy of Northern Paiute, Bannock and Shoshone Native Americans, called the \"Snake Indians\" against the United States Army in the states of Oregon, Nevada, California, and Idaho which ran along the Snake River. The war started when tension arose between the local Indians and the flooding pioneer trains encroaching through their lands, which resulted in competition for food and resources. Indians included in this group attacked and harassed emigrant parties and miners crossing the Snake River Valley, which resulted in further retaliation of the white settlements and the intervention of the United States army. The war resulted in a total of 1,762 men who have been killed, wounded, and captured from both sides. Unlike other Indian Wars, the Snake War was widely forgotten in United States history due to having only limited coverage of the war.\n\nThe Colorado War fought by Cheyenne, Arapaho and Sioux, was fought in the territories of Colorado to Nebraska. The conflict was fought in 1863–1865 while the American Civil War was still ongoing. Caused by dissolution between the Natives and the white settlers in the region, the war was infamous for the atrocities done between the two parties. White militias destroyed Native villages and killed Indian women and children such as the bloody Sand Creek massacre, and the Indians also raided ranches, farms and killed white families such as the American Ranch massacre and Raid on Godfrey Ranch.\n\nIn the Apache Wars, Colonel Christopher \"Kit\" Carson forced the Mescalero Apache onto a reservation in 1862. In 1863–1864, Carson used a scorched earth policy in the Navajo Campaign, burning Navajo fields and homes, and capturing or killing their livestock. He was aided by other Indian tribes with long-standing enmity toward the Navajos, chiefly the Utes. Another prominent conflict of this war was Geronimo's fight against settlements in Texas in the 1880s. The Apaches under his command conducted ambushes on US cavalries and forts, such as their attack on Cibecue Creek, while also raiding upon prominent farms and ranches, such as their infamous attack on the Empire Ranch that killed three cowboys. The U.S. finally induced the last hostile Apache band under Geronimo to surrender in 1886.\n\nDuring the Comanche Campaign, the Red River War was fought in 1874–75 in response to the Comanche's dwindling food supply of buffalo, as well as the refusal of a few bands to be inducted in reservations. Comanches started raiding small settlements in Texas, which led to the Battle of Buffalo Wallow and Second Battle of Adobe Walls fought by buffalo hunters, and the Battle of Lost Valley against the Texas Rangers. The war finally ended with a final confrontation between the Comanches and the U.S. Cavalry in Palo Duro Canyon. The last Comanche war chief, Quanah Parker, surrendered in June 1875, which would finally end the wars fought by Texans and Indians.\n\nRed Cloud's War was led by the Lakota chief Red Cloud against the military who were erecting forts along the Bozeman trail. It was the most successful campaign against the U.S. during the Indian Wars. By the Treaty of Fort Laramie (1868), the U.S. granted a large reservation to the Lakota, without military presence; it included the entire Black Hills. Captain Jack was a chief of the Native American Modoc tribe of California and Oregon, and was their leader during the Modoc War. With 53 Modoc warriors, Captain Jack held off 1,000 men of the U.S. Army for 7 months. Captain Jack killed Edward Canby.\nIn June 1877, in the Nez Perce War the Nez Perce under Chief Joseph, unwilling to give up their traditional lands and move to a reservation, undertook a 1,200-mile (2,000 km) fighting retreat from Oregon to near the Canada–US border in Montana. Numbering only 200 warriors, the Nez Perce \"battled some 2,000 American regulars and volunteers of different military units, together with their Indian auxiliaries of many tribes, in a total of eighteen engagements, including four major battles and at least four fiercely contested skirmishes.\" The Nez Perce were finally surrounded at the Battle of Bear Paw and surrendered. The Great Sioux War of 1876 was conducted by the Lakota under Sitting Bull and Crazy Horse. The conflict began after repeated violations of the Treaty of Fort Laramie (1868) once gold was discovered in the hills. One of its famous battles was the Battle of the Little Bighorn, in which combined Sioux and Cheyenne forces defeated the 7th Cavalry, led by General George Armstrong Custer. The Ute War, fought by the Ute people against settlers in Utah and Colorado, led to two battles; the Meeker massacre which killed 11 Indian agents, and the Pinhook massacre which killed 13 armed ranchers and cowboys. The Ute conflicts finally ended after the events of the Bluff War.\n\nThe end of the Indian wars came at the Wounded Knee massacre on December 29, 1890 where the 7th Cavalry attempted to disarm a Sioux man and precipitated an engagement in which about 150 Sioux men, women, and children were killed. Only thirteen days before, Sitting Bull had been killed with his son Crow Foot in a gun battle with a group of Indian police that had been sent by the American government to arrest him.\n\nAs the frontier moved westward, the establishment of U.S. military forts moved with it, representing and maintaining federal sovereignty over new territories. The military garrisons usually lacked defensible walls but were seldom attacked. They served as bases for troops at or near strategic areas, particularly for counteracting the Indian presence. For example, Fort Bowie protected Apache Pass in southern Arizona along the mail route between Tucson and El Paso and was used to launch attacks against Cochise and Geronimo. Fort Laramie and Fort Kearny helped protect immigrants crossing the Great Plains and a series of posts in California protected miners. Forts were constructed to launch attacks against the Sioux. As Indian reservations sprang up, the military set up forts to protect them. Forts also guarded the Union Pacific and other rail lines. Other important forts were Fort Sill, Oklahoma, Fort Smith, Arkansas, Fort Snelling, Minnesota, Fort Union, New Mexico, Fort Worth, Texas, and Fort Walla Walla in Washington. Fort Omaha, Nebraska was home to the Department of the Platte, and was responsible for outfitting most Western posts for more than 20 years after its founding in the late 1870s. Fort Huachuca in Arizona was also originally a frontier post and is still in use by the United States Army.\n\nSettlers on their way overland to Oregon and California became targets of Indian threats. Robert L. Munkres read 66 diaries of parties traveling the Oregon Trail between 1834 and 1860 to estimate the actual dangers they faced from Indian attacks in Nebraska and Wyoming. The vast majority of diarists reported no armed attacks at all. However many did report harassment by Indians who begged or demanded tolls, and stole horses and cattle. Madsen reports that the Shoshoni and Bannock tribes north and west of Utah were more aggressive toward wagon trains. The federal government attempted to reduce tensions and create new tribal boundaries in the Great Plains with two new treaties in the early 1850, The Treaty of Fort Laramie established tribal zones for the Sioux, Cheyennes, Arapahos, Crows, and others, and allowed for the building of roads and posts across the tribal lands. A second treaty secured safe passage along the Santa Fe Trail for wagon trains. In return, the tribes would receive, for ten years, annual compensation for damages caused by migrants. The Kansas and Nebraska territories also became contentious areas as the federal government sought those lands for the future transcontinental railroad. In the Far West settlers began to occupy land in Oregon and California before the federal government secured title from the native tribes, causing considerable friction. In Utah, the Mormons also moved in before federal ownership was obtained.\n\nA new policy of establishing reservations came gradually into shape after the boundaries of the \"Indian Territory\" began to be ignored. In providing for Indian reservations, Congress and the Office of Indian Affairs hoped to de-tribalize Native Americans and prepare them for integration with the rest of American society, the \"ultimate incorporation into the great body of our citizen population\". This allowed for the development of dozens of riverfront towns along the Missouri River in the new Nebraska Territory, which was carved from the remainder of the Louisiana Purchase after the Kansas–Nebraska Act. Influential pioneer towns included Omaha, Nebraska City and St. Joseph.\n\nAmerican attitudes towards Indians during this period ranged from malevolence (\"the only good Indian is a dead Indian\") to misdirected humanitarianism (Indians live in \"inferior\" societies and by assimilation into white society they can be redeemed) to somewhat realistic (Native Americans and settlers could co-exist in separate but equal societies, dividing up the remaining western land). Dealing with nomadic tribes complicated the reservation strategy and decentralized tribal power made treaty making difficult among the Plains Indians. Conflicts erupted in the 1850s, resulting in various Indian wars. In these times of conflict, Indians become more stringent about white men entering their territory. Such as in the case of Oliver Loving, they would sometimes attack cowboys and their cattle if ever caught crossing in the borders of their land. They would also prey upon livestock if food was scarce during hard times. However, relationship between cowboys and Native Americans were more mutual than they are portrayed, and the former would occasionally pay a fine of 10 cents per cow for the latter to allow them to travel through their land. Indians also preyed upon stagecoaches travelling in the frontier for its horses and valuables.\n\nAfter the Civil War, as the volunteer armies disbanded, the regular army cavalry regiments increased in number from six to ten, among them Custer's U.S. 7th Cavalry Regiment of Little Bighorn fame, and the African-American U.S. 9th Cavalry Regiment and U.S. 10th Cavalry Regiment. The black units, along with others (both cavalry and infantry), collectively became known as the Buffalo Soldiers. According to Robert M. Utley:\n\nWesterners were proud of their leadership in the movement for democracy and equality, a major theme for Frederick Jackson Turner. The new states of Kentucky, Tennessee, Alabama and Ohio were more democratic than the parent states back East in terms of politics and society. The Western states were the first to give women the right to vote. By 1900 the West, especially California and Oregon, led the Progressive movement.\n\nScholars have examined the social history of the west in search of the American character. The history of Kansas, argued historian Carl L. Becker a century ago, reflects American ideals. He wrote: \"The Kansas spirit is the American spirit double distilled. It is a new grafted product of American individualism, American idealism, American intolerance. Kansas is America in microcosm.\"\n\nScholars have compared the emergence of democracy in America with other countries, with reference to the frontier experience. Selwyn Troen has made the comparison with Israel. The American frontiersmen relied on individual effort, in the context of very large quantities of unsettled land with weak external enemies. Israel by contrast, operated in a very small geographical zone, surrounded by more powerful neighbors. The Jewish pioneer was not building an individual or family enterprise, but was a conscious participant in nation building, with a high priority on collective and cooperative planned settlements. The Israeli pioneers brought in American experts on irrigation and agriculture to provide technical advice. However they rejected the American frontier model in favor of a European model that supported their political and security concerns.\n\nThe cities played an essential role in the development of the frontier, as transportation hubs, financial and communications centers, and providers of merchandise, services, and entertainment. As the railroads pushed westward into unsettled territory after 1860, they build service towns to handle the needs of railroad construction crews, train crews, and passengers who ate meals at scheduled stops. In most of the South, there were very few cities of any size for miles around, and this pattern held for Texas as well, so railroads did not arrive until the 1880s. They then shipped the cattle out and cattle drives became short-distance affairs. However the passenger trains were often the targets of armed gangs.\n\nDenver's economy before 1870 had been rooted in mining; it then grew by expanding its role in railroads, wholesale trade, manufacturing, food processing, and servicing the growing agricultural and ranching hinterland. Between 1870 and 1890, manufacturing output soared from $600,000 to $40 million, and population grew by a factor of 20 times to 107,000. Denver had always attracted miners, workers, whores and travelers. Saloons and gambling dens sprung up overnight. The city fathers boasted of its fine theaters, and especially the Tabor Grand Opera House built in 1881. By 1890, Denver had grown to be the 26th largest city in America, and the fifth-largest city west of the Mississippi River. The boom times attracted millionaires and their mansions, as well as hustlers, poverty and crime. Denver gained regional notoriety with its range of bawdy houses, from the sumptuous quarters of renowned madams to the squalid \"cribs\" located a few blocks away. Business was good; visitors spent lavishly, then left town. As long as madams conducted their business discreetly, and \"crib girls\" did not advertise their availability too crudely, authorities took their bribes and looked the other way. Occasional cleanups and crack downs satisfied the demands for reform.\n\nWith its giant mountain of copper, Butte, Montana was the largest, richest and rowdiest mining camp on the frontier. It was an ethnic stronghold, with the Irish Catholics in control of politics and of the best jobs at the leading mining corporation Anaconda Copper. City boosters opened a public library in 1894. Ring argues that the library was originally a mechanism of social control, \"an antidote to the miners' proclivity for drinking, whoring, and gambling\". It was also designed to promote middle-class values and to convince Easterners that Butte was a cultivated city.\n\nEuropean immigrants often built communities of similar religious and ethnic backgrounds. For example, many Finns went to Minnesota and Michigan, Swedes and Norwegians to Minnesota and the Dakotas, Irish to railroad centers along the transcontinental lines, Volga Germans to North Dakota, and German Jews to Portland, Oregon.\n\nAfrican Americans moved West as soldiers, as well as cowboys, farm hands, saloon workers, cooks, and outlaws. The Buffalo Soldiers were soldiers in the all-black 9th and 10th Cavalry regiments, and 24th and 25th Infantry Regiments of the U.S. Army. They had white officers and served in numerous western forts.\n\nAbout 4,000 blacks came to California in Gold Rush days. In 1879, after the end of Reconstruction in the South, several thousand Freedmen moved from Southern states to Kansas. Known as the Exodusters, they were lured by the prospect of good, cheap Homestead Law land and better treatment. The all-black town of Nicodemus, Kansas, which was founded in 1877, was an organized settlement that predates the Exodusters but is often associated with them.\n\nThe California Gold Rush included thousands of Mexican and Chinese arrivals. Chinese migrants, many of whom were impoverished peasants, provided the major part of the workforce for the building of Central Pacific portion of the transcontinental railroad. Most of them went home by 1870 when the railroad was finished. Those who stayed on worked in mining, agriculture, and opened small shops such as groceries, laundries and restaurants. Hostility remained high as seen by the Chinese Massacre Cove episode and the Rock Springs massacre. The Chinese were generally forced into self-sufficient \"Chinatowns\" in cities such as San Francisco, Portland, and Seattle. In Los Angeles, the last major anti-Chinese riot took place in 1871, after which local law enforcement grew stronger. In the late 19th century, Chinatowns were squalid slums known for their vice, prostitution, drugs, and violent battles between \"tongs\". By the 1930s, however, Chinatowns had become clean, safe and attractive tourist destinations.\n\nIn the 1890–1907 era, thousands of Japanese permanently migrated to Hawaii, Alaska, and California as farm workers. Immigrants born in Asia were generally ineligible for U.S. citizenship until World War II. However, their children born in the U.S. automatically became citizens in accordance to the 14th Amendment to the United States Constitution.\n\nThe great majority of Hispanics who had been living in the former territories of New Spain remained and became American citizens in 1848. The 10,000 or so Californios lived in southern California and after 1880 were overshadowed by the hundreds of thousands of arrivals from the east. Those in New Mexico dominated towns and villages that changed little until well into the 20th century. New arrivals from Mexico arrived, especially after the Revolution of 1911 terrorized thousands of villages all across Mexico. Most refugees went to Texas or California, and soon poor barrios appeared in many border towns. Early on there was a criminal element as well. The California \"Robin Hood\", Joaquin Murieta, led a gang in the 1850s which burned houses, killed miners, and robbed stagecoaches. In Texas, Juan Cortina led a 20-year campaign against Anglos and the Texas Rangers, starting around 1859.\n\nOn the Great Plains very few single men attempted to operate a farm or ranch; farmers clearly understood the need for a hard-working wife, and numerous children, to handle the many chores, including child-rearing, feeding and clothing the family, managing the housework, and feeding the hired hands. During the early years of settlement, farm women played an integral role in assuring family survival by working outdoors. After a generation or so, women increasingly left the fields, thus redefining their roles within the family. New conveniences such as sewing and washing machines encouraged women to turn to domestic roles. The scientific housekeeping movement, promoted across the land by the media and government extension agents, as well as county fairs which featured achievements in home cookery and canning, advice columns for women in the farm papers, and home economics courses in the schools all contributed to this trend.\n\nAlthough the eastern image of farm life on the prairies emphasizes the isolation of the lonely farmer and farm life, in reality rural folk created a rich social life for themselves. They often sponsored activities that combined work, food, and entertainment such as barn raisings, corn huskings, quilting bees, Grange meetings, church activities, and school functions. The womenfolk organized shared meals and potluck events, as well as extended visits between families.\n\nChildhood on the American frontier is contested territory. One group of scholars, following the lead of novelists Willa Cather and Laura Ingalls Wilder, argue the rural environment was beneficial to the child's upbringing. Historians Katherine Harris and Elliott West write that rural upbringing allowed children to break loose from urban hierarchies of age and gender, promoted family interdependence, and in the end produced children who were more self-reliant, mobile, adaptable, responsible, independent and more in touch with nature than their urban or eastern counterparts. On the other hand, historians Elizabeth Hampsten and Lillian Schlissel offer a grim portrait of loneliness, privation, abuse, and demanding physical labor from an early age. Riney-Kehrberg takes a middle position.\n\nEntrepreneurs set up shops and businesses to cater to the miners. World-famous were the houses of prostitution found in every mining camp worldwide. Prostitution was a growth industry attracting sex workers from around the globe, pulled in by the money, despite the harsh and dangerous working conditions and low prestige. Chinese women were frequently sold by their families and taken to the camps as prostitutes; they had to send their earnings back to the family in China. In Virginia City, Nevada, a prostitute, Julia Bulette, was one of the few who achieved \"respectable\" status. She nursed victims of an influenza epidemic; this gave her acceptance in the community and the support of the sheriff. The townspeople were shocked when she was murdered in 1867; they gave her a lavish funeral and speedily tried and hanged her assailant. Until the 1890s, madams predominately ran the businesses, after which male pimps took over, and the treatment of the women generally declined. It was not uncommon for bordellos in Western towns to operate openly, without the stigma of East Coast cities. Gambling and prostitution were central to life in these western towns, and only later―as the female population increased, reformers moved in, and other civilizing influences arrived―did prostitution become less blatant and less common. After a decade or so the mining towns attracted respectable women who ran boarding houses, organized church societies, worked as laundresses and seamstresses, and strove for independent status.\n\nHistorian Waddy W. Moore uses court records to show that on the sparsely settled Arkansas frontier lawlessness was common. He distinguished two types of crimes: unprofessional (dueling, crimes of drunkenness, selling whiskey to the Indians, cutting trees on federal land) and professional (rustling, highway robbery, counterfeiting). Criminals found many opportunities to rob pioneer families of their possessions, while the few underfunded lawmen had great difficulty detecting, arresting, holding, and convicting wrongdoers. Bandits, typically in groups of two or three, rarely attacked stagecoaches with a guard carrying a sawed-off, double-barreled shotgun; it proved less risky to rob teamsters, people on foot, and solitary horsemen, while bank robberies themselves were harder to pull off due to the security of the establishment. According also to historian Brian Robb, the earliest form of organized crime in America was born from the gangs of the Old West.\n\nWhen criminals were convicted, punishment was severe. Aside from the occasional Western sheriff and Marshal, there were other various law enforcement agencies throughout the American frontier, such as the Texas Rangers and the North-West Mounted Police. These lawmen were not just instrumental in keeping peace, but also in protecting the locals from Indian and Mexican threats at the border. Law enforcement tended to be more stringent in towns than in rural areas. Law enforcement emphasized maintaining stability more than armed combat, focusing on drunkenness, disarming cowboys who violated gun-control edicts and dealing with flagrant breaches of gambling and prostitution ordinances.\n\nDykstra argues that the violent image of the cattle towns in film and fiction is largely myth. The real Dodge City, he says, was the headquarters for the buffalo-hide trade of the Southern Plains and one of the West's principal cattle towns, a sale and shipping point for cattle arriving from Texas. He states there is a \"second Dodge City\" that belongs to the popular imagination and thrives as a cultural metaphor for violence, chaos, and depravity. For the cowboy arriving with money in hand after two months on the trail, the town was exciting. A contemporary eyewitness of Hays City, Kansas paints a vivid image of this cattle town:\nIt has been acknowledged that the popular portrayal of Dodge City in film and fiction carries a note of truth, however, as gun crime was rampant in the city prior to the establishment of a local government. Soon after the city's residents officially established their first municipal government, however, a law banning concealed firearms was enacted and crime was reduced soon afterwards. Similar laws were passed in other frontier towns to reduce the rate of gun crime as well. As UCLA law professor Adam Wrinkler noted:\n\nTombstone, Arizona was a turbulent mining town that flourished longer than most, from 1877 to 1929. Silver was discovered in 1877, and by 1881 the town had a population of over 10,000. In 1879 the newly arrived Earp brothers bought shares in the Vizina mine, water rights, and gambling concessions, but Virgil, Wyatt, and Morgan Earp obtained positions at different times as federal and local lawmen. After more than a year of threats and feuding, they killed three outlaws in the Gunfight at the O.K. Corral, the most famous gunfight of the Old West. In the aftermath, Virgil Earp was maimed in an ambush and Morgan Earp was assassinated while playing billiards. Wyatt and others, including his brothers James Earp and Warren Earp, pursued those they believed responsible in an extra-legal vendetta and warrants were issued for their arrest in the murder of Frank Stilwell. The Cochise County Cowboys were one of the first organized crime syndicates in the United States, and their demise came at the hands of Wyatt Earp.\n\nWestern story tellers and film makers featured the gunfight in many Western productions. Walter Noble Burns's novel \"Tombstone\" (1927) made Earp famous. Hollywood celebrated Earp's Tombstone days with John Ford's \"My Darling Clementine\" (1946), John Sturges's \"Gunfight at the O.K. Corral\" (1957) and \"Hour of the Gun\" (1967), Frank Perry's \"Doc\" (1971), George Cosmatos's \"Tombstone\" (1993), and Lawrence Kasdan's \"Wyatt Earp\" (1994). They solidified Earp's modern reputation as the Old West's deadliest gunman.\n\nThe major type of banditry was conducted by the infamous outlaws of the West, including Jesse James, Billy the Kid, the Dalton Gang, Black Bart, Butch Cassidy and the Wild Bunch and hundreds of others who preyed on banks, trains, stagecoaches, and in some cases even armed government transports such as the Wham Paymaster Robbery and the Skeleton Canyon Robbery. Some of the outlaws, such as Jesse James, were products of the violence of the Civil War (James had ridden with Quantrill's Raiders) and others became outlaws during hard times in the cattle industry. Many were misfits and drifters who roamed the West avoiding the law. In rural areas Joaquin Murieta, Jack Powers, Augustine Chacon and other bandits terrorized the state. When outlaw gangs were near, towns would occasionally raise a posse to drive them out or capture them. Seeing that the need to combat the bandits was a growing business opportunity, Allan Pinkerton ordered his National Detective Agency, founded in 1850, to open branches out West, and they got into the business of pursuing and capturing outlaws. There was plenty of business thanks to the criminals such as the James Gang, Butch Cassidy, Sam Bass, and dozens of others. To take refuge from the law, outlaws would use the advantages of the open range, remote passes and badlands to hide. While some settlements and towns in the frontier also house outlaws and criminals, which were called \"outlaw towns\".\nBanditry was a major issue in California after 1849, as thousands of young men detached from family or community moved into a land with few law enforcement mechanisms. To combat this, the San Francisco Committee of Vigilance was established to give drumhead trials and death sentences to well-known offenders. As such, other earlier settlements created their own private agencies to protect communities due to the lack of peace-keeping establishments. These vigilance committees reflected different occupations in the frontier, such as land clubs, cattlemen's associations and mining camps. Similar vigilance committees also existed in Texas, and their main objective was to stamp out lawlessness and rid communities of desperadoes and rustlers. These committees would sometimes form mob rule for private vigilante groups, but usually were made up of responsible citizens who wanted only to maintain order. Criminals caught by these vigilance committees were treated cruelly; often hung or shot without any form of trial.\n\nCivilians also took arms to defend themselves in the Old West, sometimes siding with lawmen (Coffeyville Bank Robbery), or siding with outlaws (Battle of Ingalls). In the Post-Civil War frontier, over 523 whites, 34 blacks and 75 others were victims of lynching. However, cases of lynching in the Old West wasn't primarily caused by the absence of a legal system, but also because of social class. Historian Michael J. Pfeifer writes, \"Contrary to the popular understanding, early territorial lynching did not flow from an absence or distance of law enforcement but rather from the social instability of early communities and their contest for property, status, and the definition of social order.\"\n\nThe names and exploits of Western gunslingers took a major role in American folklore, fiction and film. Their guns and costumes became children's toys for make-believe shootouts. The stories became immensely popular in Germany and other European countries, which produced their own novels and films about the American frontier. The image of a Wild West filled with countless gunfights was a myth based on repeated exaggerations. The most notable and well-known took place in Arizona, New Mexico, Kansas, Oklahoma, and Texas. Actual gunfights in the Old West were more episodic than being a common thing, but when gunfights did occur, the cause for each varied. Some were simply the result of the heat of the moment, while others were longstanding feuds, or between bandits and lawmen. Although mostly romanticized, there were instances of \"quick draw\" that did occur though rarely, such as Wild Bill Hickok – Davis Tutt shootout and Luke Short-Jim Courtright Duel. Fatal duels were fought to uphold personal honor in the West. To prevent gunfights, towns such as Dodge City and Tombstone prohibited firearms in town.\nRange wars were infamous armed conflicts that took place in the \"open range\" of the American frontier. The subject of these conflicts was the control of lands freely used for farming and cattle grazing which gave the conflict its name. Range wars became more common by the end of the American Civil War, and numerous conflicts were fought such as the Pleasant Valley War, Mason County War, Johnson County War, Colorado Range War, Fence Cutting War, Colfax County War, Castaic Range War, Barber–Mizell feud, San Elizario Salt War and others. During a range war in Montana, a vigilante group called Stuart's Stranglers, which were made up of cattlemen and cowboys, killed up to 20 criminals and range squatters in 1884 alone. In Nebraska, stock grower Isom Olive led a range war in 1878 that killed a number of homesteaders from lynchings and shootouts before eventually leading to his own murder. Another infamous type of open range conflict were the Sheep Wars, which were fought between sheep ranchers and cattle ranchers over grazing rights and mainly occurred in Texas, Arizona and the border region of Wyoming and Colorado. In most cases, formal military involvement were used to quickly put an end to these conflicts. Other conflicts over land and territory were also fought such as the Regulator–Moderator War, Cortina Troubles, Las Cuevas War and the Bandit War.\n\nFeuds involving families and bloodlines also occurred much in the frontier. Since private agencies and vigilance committees were the substitute for proper courts, many families initially depended on themselves and their communities for their security and justice. These wars include the Lincoln County War, Tutt–Everett War, Flynn–Doran feud, Early–Hasley feud, Brooks-Baxter War, Sutton–Taylor feud, Horrell Brothers feud, Brooks–McFarland Feud, Reese–Townsend feud and the Earp Vendetta Ride.\n\nThe end of the bison herds opened up millions of acres for cattle ranching. Spanish cattlemen had introduced cattle ranching and longhorn cattle to the Southwest in the 17th century, and the men who worked the ranches, called \"vaqueros\", were the first \"cowboys\" in the West. After the Civil War, Texas ranchers raised large herds of longhorn cattle. The nearest railheads were 800 or more miles (130+ km) north in Kansas (Abilene, Kansas City, Dodge City, and Wichita). So once fattened the ranchers and their cowboys drove the herds north along the Western, Chisholm, and Shawnee trails. The cattle were shipped to Chicago, St. Louis, and points east for slaughter and consumption in the fast-growing cities. The Chisholm Trail, laid out by cattleman Joseph McCoy along an old trail marked by Jesse Chisholm, was the major artery of cattle commerce, carrying over 1.5 million head of cattle between 1867 and 1871 over the from south Texas to Abilene, Kansas. The long drives were treacherous, especially crossing water such as the Brazos and the Red River and when they had to fend off Indians and rustlers looking to make off with their cattle. A typical drive would take three to four months and contained two miles (3 km) of cattle six abreast. Despite the risks, a successful drive proved very profitable to everyone involved, as the price of one steer was $4 in Texas and $40 back East.\n\nBy the 1870s and 1880s, cattle ranches expanded further north into new grazing grounds and replaced the bison herds in Wyoming, Montana, Colorado, Nebraska and the Dakota territory, using the rails to ship to both coasts. Many of the largest ranches were owned by Scottish and English financiers. The single largest cattle ranch in the entire West was owned by American John W. Iliff, \"cattle king of the Plains\", operating in Colorado and Wyoming. Gradually, longhorns were replaced by the American breeds of Hereford and Angus, introduced by settlers from the Northwest. Though less hardy and more disease-prone, these breeds produced better tasting beef and matured faster.\n\nThe funding for the cattle industry came largely from British sources, as the European investors engaged in a speculative extravaganza—a \"bubble\". Graham concludes the mania was founded on genuine opportunity, as well as \"exaggeration, gullibility, inadequate communications, dishonesty, and incompetence\". A severe winter engulfed the plains toward the end of 1886 and well into 1887, locking the prairie grass under ice and crusted snow which starving herds could not penetrate. The British lost most of their money—as did eastern investors like Theodore Roosevelt, but their investments did create a large industry that continues to cycle through boom and bust periods.\n\nOn a much smaller scale sheep grazing was locally popular; sheep were easier to feed and needed less water. However, Americans did not eat mutton. As farmers moved in open range cattle ranching came to an end and was replaced by barbed wire spreads where water, breeding, feeding, and grazing could be controlled. This led to \"fence wars\" which erupted over disputes about water rights.\n\nCentral to the myth and the reality of the West is the American cowboy. His real life was a hard one and revolved around two annual roundups, spring and fall, the subsequent drives to market, and the time off in the cattle towns spending his hard earned money on food, clothing, gambling, and prostitution. During winter, many cowboys hired themselves out to ranches near the cattle towns, where they repaired and maintained equipment and buildings. Working the cattle was not just a routine job but also a lifestyle that exulted in the freedom of the wide unsettled outdoors on horseback. Long drives hired one cowboy for about 250 head of cattle. Saloons were ubiquitous (outside Mormondom), but on the trail the cowboys were forbidden to drink alcohol. Often, hired cowboys were trained and knowledgeable in their trade such as herding, ranching and protecting cattle. To protect their herd from wild animals, hostile Indians and rustlers, cowboys carried with them their iconic weaponry such as the Bowie knife, lasso, bullwhip, pistols, rifles and shotguns.\n\nMany of the cowboys were veterans of the Civil War; a diverse group, they included Blacks, Hispanics, Native Americans, and immigrants from many lands. The earliest cowboys in Texas learned their trade, adapted their clothing, and took their jargon from the Mexican vaqueros or \"buckaroos\", the heirs of Spanish cattlemen from middle-south of Spain. Chaps, the heavy protective leather trousers worn by cowboys, got their name from the Spanish \"chaparreras\", and the lariat, or rope, was derived from \"la reata\". All the distinct clothing of the cowboy—boots, saddles, hats, pants, chaps, slickers, bandannas, gloves, and collar-less shirts—were practical and adaptable, designed for protection and comfort. The cowboy hat quickly developed the capability, even in the early years, to identify its wearer as someone associated with the West; it came to symbolize the frontier. The most enduring fashion adapted from the cowboy, popular nearly worldwide today, are \"blue jeans\", originally made by Levi Strauss for miners in 1850.\n\nBefore a drive, a cowboy's duties included riding out on the range and bringing together the scattered cattle. The best cattle would be selected, roped, and branded, and most male cattle were castrated. The cattle also needed to be dehorned and examined and treated for infections. On the long drives, the cowboys had to keep the cattle moving and in line. The cattle had to be watched day and night as they were prone to stampedes and straying. While camping every night, cowboys would often sing to their herd to keep them calm. The work days often lasted fourteen hours, with just six hours of sleep. It was grueling, dusty work, with just a few minutes of relaxation before and at the end of a long day. On the trail, drinking, gambling, and brawling were often prohibited and fined, and sometimes cursing as well. It was monotonous and boring work, with food to match: bacon, beans, bread, coffee, dried fruit, and potatoes. On average, cowboys earned $30 to $40 per month, because of the heavy physical and emotional toll, it was unusual for a cowboy to spend more than seven years on the range. As open range ranching and the long drives gave way to fenced-in ranches in the 1880s, by the 1890s the glory days of the cowboy came to an end, and the myths about the \"free living\" cowboy began to emerge.\n\nAnchoring the booming cattle industry of the 1860s and 1870s were the cattle towns in Kansas and Missouri. Like the mining towns in California and Nevada, cattle towns such as Abilene, Dodge City, and Ellsworth experienced a short period of boom and bust lasting about five years. The cattle towns would spring up as land speculators would rush in ahead of a proposed rail line and build a town and the supporting services attractive to the cattlemen and the cowboys. If the railroads complied, the new grazing ground and supporting town would secure the cattle trade. However, unlike the mining towns which in many cases became ghost towns and ceased to exist after the ore played out, cattle towns often evolved from cattle to farming and continued on after the grazing lands were exhausted.\n\nConcern with the protection of the environment became a new issue in the late 19th century, pitting different interests. On the one side were the lumber and coal companies who called for maximum exploitation of natural resources to maximize jobs, economic growth, and their own profit.\n\nIn the center were the conservationists, led by Theodore Roosevelt and his coalition of outdoorsmen, sportsmen, bird watchers and scientists. They wanted to reduce waste; emphasized the value of natural beauty for tourism and ample wildlife for hunters; and argued that careful management would not only enhance these goals but also increase the long-term economic benefits to society by planned harvesting and environmental protections. Roosevelt worked his entire career to put the issue high on the national agenda. He was deeply committed to conserving natural resources. He worked closely with Gifford Pinchot and used the Newlands Reclamation Act of 1902 to promote federal construction of dams to irrigate small farms and placed 230 million acres (360,000 mi² or 930,000 km²) under federal protection. Roosevelt set aside more Federal land, national parks, and nature preserves than all of his predecessors combined.\n\nRoosevelt explained his position in 1910:\n\nThe third element, smallest at first but growing rapidly after 1870, were the environmentalists who honored nature for its own sake, and rejected the goal of maximizing human benefits. Their leader was John Muir (1838–1914), a widely read author and naturalist and pioneer advocate of preservation of wilderness for its own sake, and founder of the Sierra Club. Muir, based in California, in 1889 started organizing support to preserve the sequoias in the Yosemite Valley; Congress did pass the Yosemite National Park bill (1890). In 1897 President Grover Cleveland created thirteen protected forests but lumber interests had Congress cancel the move. Muir, taking the persona of an Old Testament prophet, crusaded against the lumberman, portraying it as a contest \"between landscape righteousness and the devil\". A master publicist, Muir's magazine articles, in \"Harper's Weekly\" (June 5, 1897) and the \"Atlantic Monthly\" turned the tide of public sentiment. He mobilized public opinion to support Roosevelt's program of setting aside national monuments, national forest reserves, and national parks. However Muir broke with Roosevelt and especially President William Howard Taft on the Hetch Hetchy dam, which was built in the Yosemite National Park to supply water to San Francisco. Biographer Donald Worster says, \"Saving the American soul from a total surrender to materialism was the cause for which he fought.\"\n\nThe rise of the cattle industry and the cowboy is directly tied to the demise of the huge herds of bison—usually called the \"buffalo\". Once numbering over 25 million on the Great Plains, the grass-eating herds were a vital resource animal for the Plains Indians, providing food, hides for clothing and shelter, and bones for implements. Loss of habitat, disease, and over-hunting steadily reduced the herds through the 19th century to the point of near extinction. The last 10–15 million died out in a decade 1872–1883; only 100 survived. The tribes that depended on the buffalo had little choice but to accept the government offer of reservations, where the government would feed and supply them on condition they did not go on the warpath. Conservationists founded the American Bison Society in 1905; it lobbied Congress to establish public bison herds. Several national parks in the U.S. and Canada were created, in part to provide a sanctuary for bison and other large wildlife, with no hunting allowed. The bison population reached 500,000 by 2003.\n\nThe exploration, settlement, exploitation, and conflicts of the \"American Old West\" form a unique tapestry of events, which has been celebrated by Americans and foreigners alike—in art, music, dance, novels, magazines, short stories, poetry, theater, video games, movies, radio, television, song, and oral tradition—which continues in the modern era. Levy argues that the physical and mythological West inspired composers Aaron Copland, Roy Harris, Virgil Thomson, Charles Wakefield Cadman, and Arthur Farwell.\n\nReligious themes have inspired many environmentalists as they contemplate the pristine West before the frontiersmen violated its spirituality. Actually, as historian William Cronon has demonstrated, the concept of \"wilderness\" was highly negative and the antithesis of religiosity before the romantic movement of the 19th century.\n\nThe Frontier Thesis of historian Frederick Jackson Turner, proclaimed in 1893, established the main lines of historiography which fashioned scholarship for three or four generations and appeared in the textbooks used by practically all American students.\n\nThe mythologizing of the West began with minstrel shows and popular music in the 1840s. During the same period, P. T. Barnum presented Indian chiefs, dances, and other Wild West exhibits in his museums. However, large scale awareness really took off when the dime novel appeared in 1859, the first being \"Malaeska, the Indian Wife of the White Hunter\". By simplifying reality and grossly exaggerating the truth, the novels captured the public's attention with sensational tales of violence and heroism, and fixed in the public's mind stereotypical images of heroes and villains—courageous cowboys and savage Indians, virtuous lawmen and ruthless outlaws, brave settlers and predatory cattlemen. Millions of copies and thousands of titles were sold. The novels relied on a series of predictable literary formulas appealing to mass tastes and were often written in as little as a few days. The most successful of all dime novels was Edward S. Ellis' \"Seth Jones\" (1860). Ned Buntline's stories glamorized Buffalo Bill Cody and Edward L. Wheeler created \"Deadwood Dick\", \"Hurricane Nell\", and \"Calamity Jane\".\n\nBuffalo Bill Cody was the most effective popularizer of the Old West in the U.S. and Europe. He presented the first \"Wild West\" show in 1883, featuring a recreation of famous battles (especially Custer's Last Stand), expert marksmanship, and dramatic demonstrations of horsemanship by cowboys and Indians, as well as sure-shooting Annie Oakley.\n\nElite Eastern writers and artists of the late 19th century promoted and celebrated western lore. Theodore Roosevelt, wearing his hats as historian, explorer, hunter, rancher and naturalist, was especially productive. Their work appeared in upscale national magazines such as \"Harper's Weekly\" featured illustrations by artists Frederic Remington, Charles M. Russell, and others. Readers bought action-filled stories by writers like Owen Wister, conveying vivid images of the Old West. Remington lamented the passing of an era he helped to chronicle when he wrote:\n\nIn the 20th century, both tourists to the West and avid readers enjoyed the visual imagery of the frontier. The Western movies provided the most famous examples, as in the numerous films of John Ford. He was especially enamored of Monument Valley. Critic Keith Phipps says, \"its have defined what decades of moviegoers think of when they imagine the American West.\" The heroic stories coming out of the building of the transcontinental railroad in the mid-1860s enlivened many dime novels, and illustrated many newspapers and magazines with the juxtaposition of traditional environment with the iron horse of modernity.\n\nThe cowboy has for over a century been an iconic American image both in the country and abroad; recognized worldwide and revered by Americans. The most famous popularizers of the image include part-time cowboy and \"Rough Rider\" President Theodore Roosevelt (1858–1919), who made \"cowboy\" internationally synonymous with the brash aggressive American, and Indian Territory-born trick roper Will Rogers (1879–1935), the leading humorist of the 1920s.\n\nRoosevelt conceptualized the herder (cowboy) as a stage of civilization distinct from the sedentary farmer—a theme well expressed in the 1944 Hollywood hit \"Oklahoma!\" that highlights the enduring conflict between cowboys and farmers. Roosevelt argued that the manhood typified by the cowboy—and outdoor activity and sports generally—was essential if American men were to avoid the softness and rot produced by an easy life in the city.\n\nWill Rogers, the son of a Cherokee judge in Oklahoma, started with rope tricks and fancy riding, but by 1919 discovered his audiences were even more enchanted with his wit in his representation of the wisdom of the common man.\n\nOthers who contributed to enhancing the romantic image of the American cowboy include Charles Siringo (1855–1928) and Andy Adams (1859–1935). Cowboy, Pinkerton detective, and western author, Siringo was the first authentic cowboy autobiographer. Adams spent the 1880s in the cattle industry in Texas and 1890s mining in the Rockies. When an 1898 play's portrayal of Texans outraged Adams, he started writing plays, short stories, and novels drawn from his own experiences. His \"The Log of a Cowboy\" (1903) became a classic novel about the cattle business, especially the cattle drive. It described a fictional drive of the Circle Dot herd from Texas to Montana in 1882, and became a leading source on cowboy life; historians retraced its path in the 1960s, confirming its basic accuracy. His writings are acclaimed and criticized for realistic fidelity to detail on the one hand and thin literary qualities on the other. Many regard \"Red River\" (1948), directed by Howard Hawks, and starring John Wayne and Montgomery Clift, as an authentic cattle drive depiction.\n\nThe unique skills of the cowboys are highlighted in the rodeo. It began in organized fashion in the West in the 1880s, when several Western cities followed up on touring Wild West shows and organized celebrations that included rodeo activities. The establishment of major cowboy competitions in the East in the 1920s led to the growth of rodeo sports. Trail cowboys who were also known as gunfighters like John Wesley Hardin, Luke Short and others, were known for their prowess, speed and skill with their pistols and other firearms. Their violent escapades and reputations morphed over time into the stereotypical image of violence endured by the \"cowboy hero\".\n\nHistorians of the American West have written about the mythic West; the west of western literature, art and of people's shared memories. The phenomenon is \"the Imagined West\". The \"Code of the West\" was an unwritten, socially agreed upon set of informal laws shaping the cowboy culture of the Old West. Over time, the cowboys developed a personal culture of their own, a blend of values that even retained vestiges of chivalry. Such hazardous work in isolated conditions also bred a tradition of self-dependence and individualism, with great value put on personal honesty, exemplified in songs and cowboy poetry. The code also included the Gunfighter, who sometimes followed a form of code duello adopted from the Old South, in order to solve disputes and duels. Extrajudicial justice seen during the frontier days such as lynching, vigilantism and gunfighting, in turn popularized by the Western genre, would later be known in modern times as examples of \"frontier justice\", as the West became a thing of imagination by the late 19th century.\n\nFollowing the eleventh U.S. Census taken in 1890 the superintendent announced that there was no longer a clear line of advancing settlement, and hence no longer a frontier in the continental United States. Historian Frederick Jackson Turner seized upon the statistic to announce the end of the era in which the frontier process shaped the American character.\n\nFresh farmland was increasingly hard to find after 1890—although the railroads advertised some in eastern Montana. Bicha shows that nearly 600,000 American farmers sought cheap land by moving to the Prairie frontier of the Canadian West from 1897 to 1914. However, about two-thirds of them grew disillusioned and returned to the U.S. The admission of Oklahoma as a state in 1907 upon the combination of the Oklahoma Territory and the last remaining Indian Territory, and the Arizona and New Mexico territories as states in 1912, did not end the frontier. These contained plenty of unoccupied land, as did the territory of Alaska. Nevertheless, the ethos and storyline of the \"American frontier\" had passed.\n\nScores of Turner students became professors in history departments in the western states, and taught courses on the frontier. Scholars have debunked many of the myths of the frontier, but they nevertheless live on in community traditions, folklore and fiction. In the 1970s a historiographical range war broke out between the traditional frontier studies, which stress the influence of the frontier on all of American history and culture, and the \"New Western History\" which narrows the geographical and time framework to concentrate on the trans-Mississippi West after 1850. It avoids the word \"frontier\" and stresses cultural interaction between white culture and groups such as Indians and Hispanics. History professor William Weeks of the University of San Diego argues that in this \"New Western History\" approach:\n\nHowever, by 2005, Aron argues, the two sides had \"reached an equilibrium in their rhetorical arguments and critiques\".\n\nMeanwhile, environmental history has emerged, in large part from the frontier historiography, hence its emphasis on wilderness. It plays an increasingly large role in frontier studies. Historians approached the environment from the point of view of the frontier or regionalism. The first group emphasizes human agency on the environment; the second looks at the influence of the environment. William Cronon has argued that Turner's famous 1893 essay was environmental history in an embryonic form. It emphasized the vast power of free land to attract and reshape settlers, making a transition from wilderness to civilization.\n\nJournalist Samuel Lubell saw similarities between the frontier's Americanization of immigrants that Turner described and the social climbing by later immigrants in large cities as they moved to wealthier neighborhoods. He compared the effects of the railroad opening up Western lands to urban transportation systems and the automobile, and Western settlers' \"land hunger\" to poor city residents seeking social status. Just as the Republican party benefited from support from \"old\" immigrant groups that settled on frontier farms, \"new\" urban immigrants formed an important part of the Democratic New Deal coalition that began with Franklin Delano Roosevelt's victory in the 1932 presidential election.\n\nSince the 1960s an active center is the history department at the University of New Mexico, along with the University of New Mexico Press. Leading historians there include Gerald D. Nash, Donald C. Cutter, Richard N. Ellis, Richard Etulain, Margaret Connell-Szasz, Paul Hutton, Virginia Scharff, and Samuel Truett. The department has collaborated with other departments and emphasizes Southwestern regionalism, minorities in the Southwest, and historiography.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "7493315", "url": "https://en.wikipedia.org/wiki?curid=7493315", "title": "Anacalypsis", "text": "Anacalypsis\n\nAnacalypsis (full title: Anacalypsis: An Attempt to Draw Aside the Veil of the Saitic Isis or an Inquiry into the Origin of Languages, Nations and Religions) is a lengthy two-volume treatise written by religious historian Godfrey Higgins, and published after his death in 1836. The book was published in two quarto volumes numbering 1,436 pages, and contains meticulous references to hundreds of references. Initially printed as a limited edition of 200 copies, it was partially reprinted in 1878, and completely reprinted in a limited edition of 350 copies in 1927. In 1965, University Books, Inc. published 500 sets for the United States and 500 sets for the British Commonwealth with Publisher's Note and a Postface.\n\nA problem that the reader may find in Higgins' book is that the meaning of the title is unexplained. In the original London edition of 1836, the word \"Anacalypsis\" appears only twice: once in the title, and once in a footnote on page 447, in which Higgins expresses his plan for a sequel titled \"Commentaries on the Anacalypsis and on Ancient History\".\n\nThe word \"anacalypsis\" comes from the Greek \"ανακάλυψης\", which can be translated as «discovery» or «find». \"Anacalypsis\" is the antonym of \"apocalypse.\" In the Higgins´ book, the meaning of the word is twofold:\n\n\nRegarding to the second meaning, it is very important to understand that, the Higgins´ book, \"anacalypsis\" does not refer to a teleological principle, but a regression towards the beginning that allows us to see through light how myths were created, precisely like the apocalypse in many religions. In fact, the title of the work speaks precisely about this unveiling from the Egyptian goddess Isis. The idea of \"anacalypsis\" as \"unveiling\" was discussed in depth by the Russian-born writer and theosophist Helena Blavatsky in her book \"Isis Unveiled\".\n\nThe work is the product of more than twenty years of research during which Higgins tried to uncover \"a most ancient and universal religion from which all later creeds and doctrines sprang.\" It includes several maps and lithographic plates of Druidical Monuments. The book itself details many of Higgins' beliefs and observations about the development of religion. Among these was his theory that a secret religious order, which he labeled \"Pandeism\" (from Pans- or Pandu- referring to a family of Gods, appending with -ism), had continued from ancient times to the present day, stretching at least from Greece to India, and possibly having once covered the entire world.\n\nAmong the many theories presented in this book is that both the Celtic Druids and the Jews originated in India – and that the name of the Biblical Abraham is really a variation of the word Brahma, created by shifting the last letter to the beginning: Abrahma. Higgins used the term \"Pandeism\" to describe the religious society that he purported had existed from ancient times, and at one time had been known throughout the entire world. Higgins believed this practice continued in secret until the time of his writing, in the 1830s in an area stretching from Greece to India.\n\nHis usage appears related to pantheism, but is distinctly different. While pantheism normally refers to one universal god, the Pandeism described by Higgins, refers to the worship of a family, a union, or a pantheon of gods which are collectively universal.\n\nHiggins was a follower of John Toland; although Toland had lived in an era when \"deism\" and \"theism\" were interchangeable, Higgins wrote during the 1820s and 1830s, a period several generations later when deism was popular and became distinct from theism. When coining \"Pandeism\", Higgins showed his awareness of the similarity between Pandeism and pantheism by directly contrasting his Pandeism with Toland's pantheism:\nHiggins was also aware of the similarity between his Pandeism and deism, and demonstrated familiarity with deism, as he mentions deism or deists at several other points in the same work. Higgins noted for example that \"the Rev. R. Taylor, A.M., the Deist, now in gaol, persecuted by the Whigs for his religious opinions, in his learned defense of Deism called the \"Diegesis\", has clearly proved all the hierarchical institutions of the Christians to be a close copy of those of the Essenians of Egypt.\" \n\nWhile more contemporary pandeism evokes both pantheism and deism and suggests their combination, Higgins' usage is removed from both. Whereas Toland's construction of pantheism was based on the Greek root words \"pan\", meaning \"all\" and \"Theos\", meaning \"God\", Higgins flips the construction around, stating:\nIn contrast to Toland, Higgins uses the word \"Pans\" or \"Pande\" to collect variations of named gods or godlike heroes – such as Pandu, Pandæa, the Pandavas, and Pandion – into a single system of worship called \"Pandeism\" as a sort of family name for a group of godlike individuals. Thus where Toland's term referred to pan- (all) and -theism (god), Higgins refers to Pande- (a root indicating this family of gods) and -ism, a wholly English construction indicating allegiance to an ideology. The term related by Higgins refers to a secret sect of worshipers of these \"Pans\", which was left in the wake of the collapse of an ancient empire that stretched from Greece (the home of Medea and Perseus) to India (where the Buddhists and the Brahmins coexist). Higgins concludes that his observations:\n\nWhile worthy of note, the above discussion is an example of what Higgins tries to present in his work: that religious scripture is written in a manner to confuse rather than clarify. The exhaustive discussion above comparing \"Pandeism\" and \"Pantheism\", while valid, fails to disclose the main emphasis of his effort, which is to show that all religions are the same and from a lost, antediluvian, original source in which all characters are allegoric representations of the zodiac with the primary deity being the sun. His theory is that this lost doctrine has been corrupted, by ignorance of allegory or by intentional purposes, from ancient times up and through Higgins' own time.\n\nHiggins died before he was able to complete the final chapter on Christianity. Higgins leaves clues, however, that there may be additional layers of meaning in his work, stating in the preface to Vol. I of 'Anacalypsis':\n\nDecades later, John Ballou Newbrough cited extensively to the \"Anacalypsis\", including Higgins' use of Pandeism, in the notes to Newbrough's 1882 \"Oahspe Bible\". Similar, possibly related coinings of Pandeism have occurred elsewhere. One author writes of a contemporary religious group in Bali (which is within the geographic realm of Pandeism described by Higgins):\n\nKersey Graves, author of the 1875 book \"The World's 16 Crucified Saviours,\" derived \"many of the most important facts collated in [his] work\" from the Anacalypsis.\n\nVsevolod Solovyov, author of the 1895 book, \"A Modern Priestess of Isis\", was alleged to have plagiarized extensively from Higgins, among others. The same investigator also alleged similar plagiarism in Madame Blavatsky's 1888, work, \"The Secret Doctrine\".\n\nTwentieth century author Alvin Boyd Kuhn and contemporary author Tom Harpur, both proponents of the Christ myth theory, reference Higgins in their writings, particularly in Harpur's 2004 best-seller, \"The Pagan Christ\".\n\n\n"}
{"id": "42841565", "url": "https://en.wikipedia.org/wiki?curid=42841565", "title": "Annales Petaviani", "text": "Annales Petaviani\n\nThe Annales Petaviani (\"AP\") is one of the so-called \"minor annals group\", three related \"Reichsannalen\", year-by-year histories of the Carolingian empire composed in Latin. They are named after the former owner of the manuscript, the French Jesuit Denis Pétau (1583–1652), whose name, in Latin, is Dionysius Petavius. The standard critical edition of the \"Annales\" is that of Georg Pertz in the \"Monumenta Germaniae Historica\". \n\nThe first entry in the \"Annales Petaviani\" is for the year 687 and records the Battle of Tertry. There is then a gap until 708, when the annals begin again and continue to 799 in chronological order. Those entries through to 771 were compiled from earlier annals, such as the \"Annales sancti Amandi\" and the \"Annales mosellani\", and do not comprise an independent source. Together with the \"Annales sancti Amandi\", the \"Annales Petaviani\" are the primary source of the entries for 741–88 in the \"Annales laurissenses maiores\". Both of these may have been based on an earlier exemplar originally compiled contemporaneously with events at the convent of Sankt Martin in Cologne. For the years 771–99 the \"Annales Petaviani\" are an independent and contemporary source.\n\nThey are the only source to date Charlemagne's birth to 747. They are also the only source to name either of Carloman I's two known sons, who fled to Italy with his widow in 771. The one born towards 770, whom Pope Stephen III offered to baptise himself, was named Pepin. Carloman's widow, Gisela, is also named in only one source: the \"Annales mettenses priores\". The \"Annales Petaviani\" also provide a unique explanation for the retirement of Carloman's uncle and namesake, Carloman, son of Charles Martel, who entered the Abbey of Montecassino in 747, leaving power in the hands of his brother, Pepin the Short. The \"Annales\" claim that Carloman's conversion to the religious life came about because his conscience was unsettled by his defeat in Alemannia, where he lost thousands of men: \"Karolomannus intravit Alamanniam ubi fertur quod multa hominum millia ceciderit. Unde compunctus regnum reliquit\" (\"Carloman enterred Alemannia where it is said that many thousands of men died. In remorse he relinquished the kingdom\"). The \"Annales\" also provide evidence of an Anglo-Saxon presence in Marseille, the great seaport of Merovingian Gaul, when they note under the year 790 the death of the son of Botto, an English \"negotiator\" in Marseille. \n\n"}
{"id": "209584", "url": "https://en.wikipedia.org/wiki?curid=209584", "title": "Atomic Age", "text": "Atomic Age\n\nThe Atomic Age, also known as the Atomic Era, is the period of history following the detonation of the first nuclear (\"atomic\") bomb, \"Trinity\", on July 16, 1945, during World War II. Although nuclear chain reactions had been hypothesized in 1933 and the first artificial self-sustaining nuclear chain reaction (Chicago Pile-1) had taken place in December 1942, the Trinity test and the ensuing bombings of Hiroshima and Nagasaki that ended World War II represented the first large-scale use of nuclear technology and ushered in profound changes in sociopolitical thinking and the course of technology development.\n\nWhile atomic power was promoted for a time as the epitome of progress and modernity, entering into the nuclear power era also entailed frightful implications of nuclear warfare, the Cold War, mutual assured destruction, nuclear proliferation, the risk of nuclear disaster (potentially as extreme as anthropogenic global nuclear winter), as well as beneficial civilian applications in nuclear medicine. It is no easy matter to fully segregate peaceful uses of nuclear technology from military or terrorist uses (such as the fabrication of dirty bombs from radioactive waste), which complicated the development of a global nuclear-power export industry right from the outset.\n\nIn 1973, concerning a flourishing nuclear power industry, the United States Atomic Energy Commission predicted that, by the turn of the 21st century, one thousand reactors would be producing electricity for homes and businesses across the U.S. However, the \"nuclear dream\" fell far short of what was promised because nuclear technology produced a range of social problems, from the nuclear arms race to nuclear meltdowns, and the unresolved difficulties of bomb plant cleanup and civilian plant waste disposal and decommissioning. Since 1973, reactor orders declined sharply as electricity demand fell and construction costs rose. Many orders and partially completed plants were cancelled.\n\nBy the late 1970s, nuclear power had suffered a remarkable international destabilization, as it was faced with economic difficulties and widespread public opposition, coming to a head with the Three Mile Island accident in 1979, and the Chernobyl disaster in 1986, both of which adversely affected the nuclear power industry for many decades.\n\nIn 1901, Frederick Soddy and Ernest Rutherford discovered that radioactivity was part of the process by which atoms changed from one kind to another, involving the release of energy. Soddy wrote in popular magazines that radioactivity was a potentially “inexhaustible” source of energy, and offered a vision of an atomic future where it would be possible to “transform a desert continent, thaw the frozen poles, and make the whole earth one smiling Garden of Eden.” The promise of an “atomic age,” with nuclear energy as the global, utopian technology for the satisfaction of human needs, has been a recurring theme ever since. But \"Soddy also saw that atomic energy could possibly be used to create terrible new weapons\".\n\nThe concept of a nuclear chain reaction was hypothesized in 1933, shortly after Chadwick's discovery of the neutron. Only a few years later, in December 1938 nuclear fission was discovered by Otto Hahn and his assistant Fritz Strassmann, and proved with Hahn's radiochemical methods. The first artificial self-sustaining nuclear chain reaction (Chicago Pile-1, or CP-1) took place in December 1942 under the leadership of Enrico Fermi.\n\nIn 1945, the pocketbook \"The Atomic Age\" heralded the untapped atomic power in everyday objects and depicted a future where fossil fuels would go unused. One science writer, David Dietz, wrote that instead of filling the gas tank of your car two or three times a week, you will travel for a year on a pellet of atomic energy the size of a vitamin pill. Glenn T. Seaborg, who chaired the Atomic Energy Commission, wrote \"there will be nuclear powered earth-to-moon shuttles, nuclear powered artificial hearts, plutonium heated swimming pools for SCUBA divers, and much more\".\n\nThe phrase \"Atomic Age\" was coined by William L. Laurence, a New York Times journalist who became the official journalist for the Manhattan Project which developed the first nuclear weapons. He witnessed both the Trinity test and the bombing of Nagasaki and went on to write a series of articles extolling the virtues of the new weapon. His reporting before and after the bombings helped to spur public awareness of the potential of nuclear technology and in part motivated development of the technology in the U.S. and in the Soviet Union. The Soviet Union would go on to test its first nuclear weapon in 1949.\n\nIn 1949, U.S. Atomic Energy Commission chairman, David Lilienthal stated that \"atomic energy is not simply a search for new energy, but more significantly a beginning of human history in which faith in knowledge can vitalize man's whole life\".\n\nThe phrase gained popularity as a feeling of nuclear optimism emerged in the 1950s in which it was believed that all power generators in the future would be atomic in nature. The atomic bomb would render all conventional explosives obsolete and nuclear power plants would do the same for power sources such as coal and oil. There was a general feeling that everything would use a nuclear power source of some sort, in a positive and productive way, from irradiating food to preserve it, to the development of nuclear medicine. There would be an age of peace and plenty in which atomic energy would \"provide the power needed to desalinate water for the thirsty, irrigate the deserts for the hungry, and fuel interstellar travel deep into outer space\". This use would render the Atomic Age as significant a step in technological progress as the first smelting of Bronze, of Iron, or the commencement of the Industrial Revolution.\n\nThis included even cars, leading Ford to display the Ford Nucleon concept car to the public in 1958. There was also the promise of golf balls which could always be found and nuclear-powered aircraft, which the US federal government even spent US $1.5 billion researching. Nuclear policymaking became almost a collective technocratic fantasy, or at least was driven by fantasy:\n\nThe very idea of splitting the atom had an almost magical grip on the imaginations of inventors and policymakers. As soon as someone said – in an even mildly credible way – that these things \"could\" be done, then people quickly convinced themselves ... that they \"would\" be done.\nIn the USA, military planners \"believed that demonstrating the civilian applications of the atom would also affirm the American system of private enterprise, showcase the expertise of scientists, increase personal living standards, and defend the democratic lifestyle against communism\".\n\nSome media reports predicted that thanks to the giant nuclear power stations of the near future electricity would soon become much cheaper and that electricity meters would be removed, because power would be \"too cheap to meter.\"\n\nWhen the Shippingport reactor went online in 1957 it produced electricity at a cost roughly ten times that of coal-fired generation. Scientists at the AEC's own Brookhaven Laboratory \"wrote a 1958 report describing accident scenarios in which 3,000 people would die immediately, with another 40,000 injured\".\n\nHowever Shippingport was an experimental reactor using highly enriched uranium (unlike most power reactors) and originally intended for a (cancelled) nuclear-powered aircraft carrier. Kenneth Nichols was a consultant for the Connecticut Yankee and Yankee Rowe nuclear power stations wrote that while considered \"experimental\" and not expected to be competitive with coal and oil, they \"became competitive because of inflation... and the large increase in price of coal and oil.\" He wrote that for nuclear power stations the capital cost is the major cost factor over the life of the plant, hence \"antinukes\" try to increase costs and building time with changing regulations and lengthly hearings, so that \"it takes almost twice as long to build a (US-designed boiling-water or pressurised water) atomic power plant in the United States as in France, Japan, Taiwan or South Korea.\" French pressurised-water nuclear plants produce 60% of their electric power, and have proven to be much cheaper than oil or coal. \n\nFear of possible atomic attack from the Soviet Union caused U.S. school children to participate in \"duck and cover\" civil defense drills.\n\nDuring the 1950s, Las Vegas, Nevada, earned the nickname \"Atomic City\" for becoming a hotspot where tourists would gather to watch above-ground nuclear weapons tests taking place at Nevada Test Site. Following the detonation of Able, one of the first atomic bombs dropped at the Nevada Test Site, the Las Vegas Chamber of Commerce began advertising the tests as an entertainment spectacle to tourists. \n\nThe detonations proved popular and casinos throughout the city capitalised on the tests by advertising hotel rooms or rooftops which offered views of the testing site or by planning \"Dawn Bomb Parties\" where people would come together to celebrate the detonations. Most parties started at midnight and musicians would perform at the venues until 4.00AM when the party would briefly stop so guests could silently watch the detonation. Some casinos capitalised on the tests further by creating atomic cocktails, a mixture of vodka, cognac, sherry and champagne. \n\nMeanwhile, groups of tourists would drive out into the desert with family or friends to watch the detonations. \n\nDespite the health risks associated with nuclear fallout, tourists and viewers were told to simply \"shower\". Later on, however, anyone who had worked at the testing site or lived in areas exposed to nuclear fallout fell ill and had higher chances of developing cancer or suffering pre-mature deaths.\n\nBy exploiting the peaceful uses of the \"friendly atom\" in medical applications, earth removal and, subsequently, in nuclear power plants, the nuclear industry and government sought to allay public fears about nuclear technology and promote the acceptance of nuclear weapons. At the peak of the Atomic Age, the United States government initiated Operation Plowshare, involving \"peaceful nuclear explosions\". The United States Atomic Energy Commission chairman announced that the Plowshares project was intended to \"highlight the peaceful applications of nuclear explosive devices and thereby create a climate of world opinion that is more favorable to weapons development and tests\".\n\nProject Plowshare “was named directly from the Bible itself, specifically Micah 4:3, which states that God will beat swords into ploughshares, and spears into pruning hooks, so that no country could lift up weapons against another”. Proposed uses included widening the Panama Canal, constructing a new sea-level waterway through Nicaragua nicknamed the Pan-Atomic Canal, cutting paths through mountainous areas for highways, and connecting inland river systems. Other proposals involved blasting underground caverns for water, natural gas, and petroleum storage. It was proposed to plant underground atomic bombs to extract shale oil in eastern Utah and western Colorado. Serious consideration was also given to using these explosives for various mining operations. One proposal suggested using nuclear blasts to connect underground aquifers in Arizona. Another plan involved surface blasting on the western slope of California's Sacramento Valley for a water transport project. However, there were many negative impacts from Project Plowshare’s 27 nuclear explosions. Consequences included blighted land, relocated communities, tritium-contaminated water, radioactivity, and fallout from debris being hurled high into the atmosphere. These were ignored and downplayed until the program was terminated in 1977, due in large part to public opposition, after $770 million had been spent on the project.\n\nIn the \"Thunderbirds\" TV series, a set of vehicles was presented that were imagined to be completely nuclear, as shown in cutaways presented in their comic-books.\n\nThe term \"atomic age\" was initially used in a positive, futuristic sense, but by the 1960s the threats posed by nuclear weapons had begun to edge out nuclear power as the dominant motif of the atom.\n\nFrench advocates of nuclear power developed an aesthetic vision of nuclear technology as art to bolster support for the technology. Leclerq compares the nuclear cooling tower to some of the grandest architectural monuments of western culture:\n\nThe age in which we live has, for the public, been marked by the nuclear engineer and the gigantic edifices he has created. For builders and visitors alike, nuclear power plants will be considered the cathedrals of the 20th century. Their syncretism mingles the conscious and the unconscious, religious fulfilment and industrial achievement, the limitations of uses of materials and boundless artistic inspiration, utopia come true and the continued search for harmony. \nIn 1973, the United States Atomic Energy Commission predicted that, by the turn of the 21st century, one thousand reactors would be producing electricity for homes and businesses across the USA. But after 1973, reactor orders declined sharply as electricity demand fell and construction costs rose. Many orders and partially completed plants were cancelled.\n\nNuclear power has proved controversial since the 1970s. Highly radioactive materials may overheat and escape from the reactor building. Nuclear waste (spent nuclear fuel) needs to be regularly removed from the reactors and disposed of safely for up to a million years, so that it does not pollute the environment. Recycling of nuclear waste has been discussed, but it creates plutonium which can be used in weapons, and in any case still leaves much unwanted waste to be stored and disposed of. Large, purpose-built facilities for long-term disposal of nuclear waste have been difficult to site, and have not yet reached fruition.\n\nBy the late 1970s, nuclear power suffered a remarkable international destabilization, as it was faced with economic difficulties and widespread public opposition, coming to a head with the Three Mile Island accident in 1979, and the Chernobyl disaster in 1986, both of which adversely affected the nuclear power industry for decades thereafter. A cover story in the February 11, 1985, issue of \"Forbes magazine\" commented on the overall management of the nuclear power program in the United States:\n\nThe failure of the U.S. nuclear power program ranks as the largest managerial disaster in business history, a disaster on a monumental scale … only the blind, or the biased, can now think that the money has been well spent. It is a defeat for the U.S. consumer and for the competitiveness of U.S. industry, for the utilities that undertook the program and for the private enterprise system that made it possible.\nSo, in a period just over 30 years, the early dramatic rise of nuclear power went into equally meteoric reverse. With no other energy technology has there been a conjunction of such rapid and revolutionary international emergence, followed so quickly by equally transformative demise.\n\nIn the 21st century, the label of the \"Atomic Age\" connotes either a sense of nostalgia or naïveté, and is considered by many to have ended with the fall of the Soviet Union in 1991, though the term continues to be used by many historians to describe the era following the conclusion of the Second World War. Atomic energy and weapons continue to have a strong effect on world politics in the 21st century. The term is used by some science fiction fans to describe not only the era following the conclusion of the Second World War but also contemporary history up to the present day.\n\nThe nuclear power industry has improved the safety and performance of reactors, and has proposed new safer (but generally untested) reactor designs but there is no guarantee that the reactors will be designed, built and operated correctly. Mistakes do occur and the designers of reactors at Fukushima in Japan did not anticipate that a tsunami generated by an earthquake would disable the backup systems that were supposed to stabilize the reactor after the earthquake. According to UBS AG, the Fukushima I nuclear accidents have cast doubt on whether even an advanced economy like Japan can master nuclear safety. Catastrophic scenarios involving terrorist attacks are also conceivable. An interdisciplinary team from MIT has estimated that if nuclear power use tripled from 2005–2055 (from 2% to 7%), at least four serious nuclear accidents would be expected in that period.\n\nIn September 2012, Japan announced that it would completely phase out nuclear power by 2030, although under the Abe administration this is now unlikely, with Germany, and other countries in reaction to the accident at Fukushima. Germany plans to completely phase-out nuclear energy by 2022.\n\nA large anti-nuclear demonstration was held on May 6, 1979, in Washington D.C., when 125,000 people including the Governor of California, attended a march and rally against nuclear power. In New York City on September 23, 1979, almost 200,000 people attended a protest against nuclear power. Anti-nuclear power protests preceded the shutdown of the Shoreham, Yankee Rowe, Millstone I, Rancho Seco, Maine Yankee, and about a dozen other nuclear power plants.\n\nOn June 12, 1982, one million people demonstrated in New York City's Central Park against nuclear weapons and for an end to the cold war arms race. It was the largest anti-nuclear protest and the largest political demonstration in American history. International Day of Nuclear Disarmament protests were held on June 20, 1983 at 50 sites across the United States.\nIn 1986, hundreds of people walked from Los Angeles to Washington DC in the Great Peace March for Global Nuclear Disarmament. There were many Nevada Desert Experience protests and peace camps at the Nevada Test Site during the 1980s and 1990s.\n\nOn May 1, 2005, 40,000 anti-nuclear/anti-war protesters marched past the United Nations in New York, 60 years after the atomic bombings of Hiroshima and Nagasaki. This was the largest anti-nuclear rally in the U.S. for several decades.\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "13776844", "url": "https://en.wikipedia.org/wiki?curid=13776844", "title": "Baby Scoop Era", "text": "Baby Scoop Era\n\nThe Baby Scoop Era was a period in anglosphere history starting after the end of World War II and ending in the early 1970s, characterized by an increased rate of pre-marital pregnancies over the preceding period, along with a higher rate of newborn adoption.\n\nFrom 1945 to 1973, it is estimated that up to 4 million parents in the United States had children placed for adoption, with 2 million during the 1960s alone. Annual numbers for non-relative adoptions increased from an estimated 33,800 in 1951 to a peak of 89,200 in 1970, then quickly declined to an estimated 47,700 in 1975. (This does not include the number of infants adopted and raised by relatives.) In contrast, the U.S. Department of Health and Human Services estimates that only 14,000 infants were placed for adoption in 2003.\n\nThis period of history has been documented in scholarly books such as \"Wake Up Little Susie\" and \"Beggars and Choosers\", both by historian Rickie Solinger, and social histories such as the book \"The Girls Who Went Away\" and the documentary, \"A Girl Like Her\", based on the book by Ann Fessler. Fessler is a professor of photography at the Rhode Island School of Design who exhibited an art installation titled \"The Girls Who Went Away\". It is also the theme of the documentary \"Gone To A Good Home\" by Film Australia.\n\nBeginning in the 1940s and 1950s, illegitimacy began to be defined in terms of psychological deficits on the part of the mother. At the same time, a liberalization of sexual mores combined with restrictions on access to birth control led to an increase in premarital pregnancies. The dominant psychological and social work view was that the large majority of unmarried mothers were better off being separated by adoption from their newborn babies. According to Mandell (2007), \"In most cases, adoption was presented to the mothers as the only option and little or no effort was made to help the mothers keep and raise the children\".\n\nSolinger describes the social pressures that led to this unusual trend, explaining that women who had no control over their reproductive lives were defined by psychological theory as \"not-mothers\", and that because they had no control over their reproductive lives, they were subject to the ideology of those who watched over them. As such, for unmarried pregnant girls and women in the pre-\"Roe\" era, the main chance for attaining home and marriage rested on their acknowledging their alleged shame and guilt, and this required relinquishing their children, with more than 80% of unwed mothers in maternity homes acting in essence as \"breeders\" for adoptive parents. According to Ellison, from 1960 to 1970, 27 percent of all births to married women between the ages of 15 and 29 were conceived premaritally. This problem was thought to be caused by female neurosis, and those who could not procure an abortion, legally or otherwise, were encouraged to put up their children for adoption.\n\nIn popular usage, singer Celeste Billhartz uses the term \"baby scoop era\" on her website to refer to the era covered by her work \"The Mothers Project.\" A letter on Senator Bill Finch's website uses the term as well. Writer Betty Mandell references the term in her article \"Adoption\". The term was also used in a 2004 edition of the \"Richmond Times-Dispatch\".\n\nInfant adoptions began declining in the early 1970s, a decline often attributed to the decreasing birth rate, but which also partially resulted from social and legal changes that enabled middle-class mothers to have an alternative single motherhood.\n\nThe decline in the fertility rate is associated with the introduction of the pill in 1960, the completion of legalization of artificial birth control methods, the introduction of federal funding to make family planning services more available to the young and low income, and the legalization of abortion.\n\nBrozinsky (1994) speaks of the decline in newborn adoptions as reflecting a freedom of choice embraced by youth and the women's movement of the 1960s and 1970s, resulting in an increase in the number of unmarried mothers who parented their babies as opposed to having them taken for adoption purposes. \"In 1970, approximately 80% of the infants born to single mothers were [...] [taken for adoption purposes], whereas by 1983 that figure had dropped to only 4%.\" \n\nIn contrast to numbers in the 1960s and 1970s, from 1989 to 1995 fewer than 1% of children born to never-married women were surrendered for adoption.\n\nA similar social development took place in the United Kingdom, New Zealand, Australia, and Canada.\n\nCanada's \"Baby Scoop Era\" refers to the postwar period from 1945 to 1988, when over 400,000 unmarried pregnant persons, mostly aged 15–19 (usually Caucasian), were targeted for their yet-to-be-born infants, simply because they were unmarried with a child. A large number of these young women were confined as inmates in maternity group homes, which were managed by religious orders, such as the Salvation Army, the Catholic Church, the United Church and the Anglican Church etc. These maternity \"homes\" were heavily funded by the Canadian government. There were over 70 maternity homes in Canada which housed between 20 and 200 pregnant women at a time. In Canadian maternity \"homes\" and hospitals, up to 100% of newborns were removed from their legal mothers after birth and given up for adoption purposes. These newborns were taken under a Health and Welfare protocol.\n\nSome professionals of the era considered that the punishment of the mother for her transgression was an important part of the process. Dr. Marion Hilliard of Women's College Hospital was quoted in 1956 saying: The father plays absolutely no part in this. That is part of her rehabilitation. When she renounces her child for its own good, the unwed mother has learned a lot. She has learned an important human value. She has learned to pay the price of her misdemeanor, and this alone, if punishment is needed, is punishment enough...We must go back to a primary set of values and the discipline that starts with the very small child.The term Baby Scoop Era is similar to the term Sixties Scoop, which was coined by Patrick Johnston, author of \"Native Children and the Child Welfare System\". \"Sixties Scoop\" refers to the Canadian practice, beginning in the 1960s and continuing until the late 1980s, of apprehending unusually high numbers of Native children over the age of 5 years old from their families and fostering or adopting them out. A similar event happened in Australia where Aboriginal children, sometimes referred to as the Stolen Generation, were removed from their families and placed into internment camps, orphanages and other institutions.\n\nA similar period of forced adoption, also known as the \"White Stolen Generations\", also occurred in Australia. It is generally understood that a decline of adoption during the 1970s was linked to a 1973 law providing for financial assistance to single parents.\n\n"}
{"id": "508383", "url": "https://en.wikipedia.org/wiki?curid=508383", "title": "Church history", "text": "Church history\n\nChurch history or ecclesiastical history as an academic discipline studies the history of Christianity and the way the Christian Church has developed since its inception. \n\nHenry Melvill Gwatkin defined church history as \"the spiritual side of the history of civilized people ever since our Master's coming\". A. M. Renwick, however, defines it as an account of the Church's success and failure in carrying out Christ's Great Commission. Renwick suggests a fourfold division of church history into missionary activity, church organization, doctrine and \"the effect on human life\".\n\nChurch history is often, but not always, studied from a Christian perspective. Writers from different Christian traditions will often highlight people and events particularly relevant to their own denominational history. Catholic and Orthodox writers often highlight the achievements of the ecumenical councils, while evangelical historians may focus on the Protestant Reformation and the Great Awakenings.\n\n\n\n"}
{"id": "40626908", "url": "https://en.wikipedia.org/wiki?curid=40626908", "title": "Classical Marxism", "text": "Classical Marxism\n\nClassical Marxism refers to the economic, philosophical and sociological theories expounded by Karl Marx and Friedrich Engels as contrasted with later developments in Marxism, especially Leninism and Marxism–Leninism.\n\nKarl Heinrich Marx (May 5, 1818, Trier, Germany – March 14, 1883, London) was an immensely influential German philosopher, sociologist, political economist and revolutionary socialist. Marx addressed a wide range of issues, including alienation and exploitation of the worker, the capitalist mode of production and historical materialism, although he is most famous for his analysis of history in terms of class struggles, summed up in the opening line of the introduction to \"The Communist Manifesto\": \"The history of all hitherto existing society is the history of class struggles\". The influence of his ideas, already popular during his life, was given added impetus by the victory of the Russian Bolsheviks in the 1917 October Revolution and there are few parts of the world which were not significantly touched by Marxian ideas in the course of the twentieth century.\n\nAs the American Marx scholar Hal Draper remarked: \"[T]here are few thinkers in modern history whose thought has been so badly misrepresented, by Marxists and anti-Marxists alike\".\n\nThe early influences on Marx are often grouped into three categories: German philosophy, English/Scottish political economy and French socialism.\n\nMain influences include Immanuel Kant; G. W. F. Hegel; and Ludwig Feuerbach. \n\nMarx studied under one of Hegel's pupils, Bruno Bauer, a leader of the circle of Young Hegelians to whom Marx attached himself. However, from 1841 he and Engels came to disagree with Bauer and the rest of the Young Hegelians about socialism and also about the usage of Hegel's dialectic and progressively broke away from German idealism and the Young Hegelians. Marx's early writings are thus a response towards Hegel, German idealism and a break with the rest of the Young Hegelians. Marx, \"stood Hegel on his head\", in his own view of his role by turning the idealistic dialectic into a materialistic one, in proposing that material circumstances shape ideas instead of the other way around. In this, Marx was following the lead of Feuerbach. His theory of alienation, developed in the \"Economic and Philosophical Manuscripts of 1844\" (published in 1932), inspired itself from Feuerbach's critique of the alienation of Man in God through the objectivation of all his inherent characteristics (thus man projected on God all qualities which are in fact man's own quality which defines the \"human nature\"). But Marx also criticized Feuerbach for being insufficiently materialistic.\n\nMain influences include Adam Smith and David Ricardo. \n\nMarx built on and critiqued the most well-known political economists of his day, the British classical political economists.\n\nMarx critiqued Smith and Ricardo for not realizing that their economic concepts reflected specifically capitalist institutions, not innate natural properties of human society and could not be applied unchanged to all societies. He proposed a systematic correlation between labour-values and money prices. He claimed that the source of profits under capitalism is value added by workers not paid out in wages. This mechanism operated through the distinction between \"labour power\", which workers freely exchanged for their wages; and \"labour\", over which asset-holding capitalists thereby gained control. This practical and theoretical distinction was Marx's primary insight, and allowed him to develop the concept of \"surplus value\", which distinguished his works from that of Smith and Ricardo.\n\nMain influences include Jean-Jacques Rousseau; Charles Fourier; Henri de Saint-Simon; Pierre-Joseph Proudhon; and Louis Blanc.\n\nRousseau was one of the first modern writers to seriously attack the institution of private property and therefore is sometimes considered a forebear of modern socialism and communism, though Marx rarely mentions Rousseau in his writings.\n\nIn 1833, France was experiencing a number of social problems arising out of the Industrial Revolution. A number of sweeping plans of reform were developed by thinkers on the political left. Among the more grandiose were the plans of Charles Fourier and the followers of Saint-Simon. Fourier wanted to replace modern cities with utopian communities while the Saint-Simonians advocated directing the economy by manipulating credit. Although these programs did not have much support, they did expand the political and social imagination of Marx.\n\nLouis Blanc is perhaps best known for originating the social principle, later adopted by Marx, of how labor and income should be distributed: “From each according to his abilities, to each according to his needs”.\n\nPierre-Joseph Proudhon participated in the February 1848 uprising and the composition of what he termed \"the first republican proclamation\" of the new republic, but he had misgivings about the new government because it was pursuing political reform at the expense of the socio-economic reform, which Proudhon considered basic. Proudhon published his own perspective for reform, \"Solution du problème social\", in which he laid out a program of mutual financial cooperation among workers. He believed this would transfer control of economic relations from capitalists and financiers to workers. It was Proudhon's book \"What Is Property?\" that convinced the young Karl Marx that private property should be abolished.\n\nMain influences includes Friedrich Engels; ancient Greek materialism; Giambattista Vico; and Lewis H. Morgan.\n\nMarx's revision of Hegelianism was also influenced by Engels' book \"The Condition of the Working Class in England\" in 1844, which led Marx to conceive of the historical dialectic in terms of class conflict and to see the modern working class as the most progressive force for revolution.\n\nMarx was influenced by Antique materialism, especially Epicurus (to whom Marx dedicated his thesis, \"The Difference Between the Democritean and Epicurean Philosophy of Nature\", 1841) for his materialism and theory of clinamen which opened up a realm of liberty.\n\nGiambattista Vico propounded a cyclical theory of history, according to which human societies progress through a series of stages from barbarism to civilization and then return to barbarism. In the first stage—called the Age of the Gods—religion, the family and other basic institutions emerge; in the succeeding Age of Heroes, the common people are kept in subjection by a dominant class of nobles; in the final stage—the Age of Men—the people rebel and win equality, but in the process society begins to disintegrate. Vico's influence on Marx is obvious.\n\nMarx drew on Lewis H. Morgan and his social evolution theory. He wrote a collection of notebooks from his reading of Lewis Morgan, but they are regarded as being quite obscure and only available in scholarly editions. (However Engels is much more noticeably influenced by Morgan than Marx).\n\nFriedrich Engels (November 28, 1820, Wuppertal – August 5, 1895, London) was a nineteenth-century German political philosopher. He developed communist theory alongside his better-known collaborator, Karl Marx.\n\nIn 1842, his father sent the young Engels to England to help manage his cotton factory in Manchester. Shocked by the widespread poverty, Engels began writing an account which he published in 1845 as \"The Condition of the Working Class in England in 1844\" ().\n\nIn July 1845, Engels went to England, where he met an Irish working-class woman named Mary Burns (Crosby), with whom he lived until her death in 1863 (Carver 2003:19). Later, Engels lived with her sister Lizzie, marrying her the day before she died in 1877 (Carver 2003:42). These women may have introduced him to the Chartist movement, of whose leaders he met several, including George Harney.\n\nEngels actively participated in the Revolution of 1848, taking part in the uprising at Elberfeld. Engels fought in the Baden campaign against the Prussians (June/July 1849) as the aide-de-camp of August Willich, who commanded a Free Corps in the Baden-Palatinate uprising. \n\nMarx and Engels first met in person in September 1844. They discovered that they had similar views on philosophy and on capitalism and decided to work together, producing a number of works including \"Die heilige Familie\" (\"The Holy Family\"). After the French authorities deported Marx from France in January 1845, Engels and Marx decided to move to Belgium, which then permitted greater freedom of expression than some other countries in Europe. Engels and Marx returned to Brussels in January 1846, where they set up the Communist Correspondence Committee.\n\nIn 1847, Engels and Marx began writing a pamphlet together, based on Engels' \"The Principles of Communism\". They completed the 12,000-word pamphlet in six weeks, writing it in such a manner as to make communism understandable to a wide audience and published it as \"The Communist Manifesto\" in February 1848. In March, Belgium expelled both Engels and Marx. They moved to Cologne, where they began to publish a radical newspaper, the \"Neue Rheinische Zeitung\". By 1849, both Engels and Marx had to leave Germany and moved to London. The Prussian authorities applied pressure on the British government to expel the two men, but Prime Minister Lord John Russell refused. With only the money that Engels could raise, the Marx family lived in extreme poverty. The contributions of Marx and Engels to the formation of Marxist theory have been described as inseparable.\n\nMarx's main ideas included:\n\nMarx believed that class identity was configured in the relations with the mode of production. In other words, a class is a collective of individuals who have a similar relationship with the means of production (as opposed to the more common-sense idea that class is determined by wealth alone, i.e. high class, middle class and poor class).\n\nMarx describes several social classes in capitalist societies, including primarily:\nThe bourgeoisie may be further subdivided into the very wealthy bourgeoisie and the petty bourgeoisie. The petty bourgeoisie are those who employ labor, but may also work themselves. These may be small proprietors, land-holding peasants, or trade workers. Marx predicted that the petty bourgeoisie would eventually be destroyed by the constant reinvention of the means of production and the result of this would be the forced movement of the vast majority of the petty bourgeoisie to the proletariat. Marx also identified the lumpenproletariat, a stratum of society completely disconnected from the means of production.\n\nMarx also describes the communists as separate from the oppressed proletariat. The communists were to be a unifying party among the proletariat; they were educated revolutionaries who could bring the proletariat to revolution and help them establish the democratic dictatorship of the proletariat. According to Marx, the communists would support any true revolution of the proletariat against the bourgeoisie. Thus the communists aide the proletariat in creating the inevitable classless society (Vladimir Lenin takes this concept a step further by stating that only \"professional revolutionaries\" can lead the revolution against the bourgeoisie).\n\nThe Marxist theory of historical materialism understands society as fundamentally determined by the material conditions at any given time—this means the relationships which people enter into with one another in order to fulfill their basic needs, for instance to feed and clothe themselves and their families. In general, Marx and Engels identified five successive stages of the development of these material conditions in Western Europe.\n\n"}
{"id": "48290414", "url": "https://en.wikipedia.org/wiki?curid=48290414", "title": "Diyarbakır Fortress", "text": "Diyarbakır Fortress\n\nDiyarbakır Fortress, is a historical fortress in Sur, Diyarbakır, Turkey. It consists of an inner fortress and an outer fortress. \n\nThe main gates of the fortress are: Dağ (Mountain) Gate, Urfa Gate, Mardin Gate and Yeni (New) Gate. The walls come from the old Roman city of Amida and were constructed in their present form in the mid-fourth century AD by the emperor Constantius II. They are the widest and longest complete defensive walls in the world after only the Great Wall of China (the Theodosian Walls for example are longer in length, but are not continuous)\n\nUNESCO added the building to their tentative list on 2000, and listed it as a World Heritage Site in 2015 along with Hevsel Gardens.\n\nThough the walls and fortress itself were once compared to the Great Wall of China, this started to change as wars broke out during 2015(&Al-monitor); walls from the fortress collapsed, along with a mosque, two churches, and the homes of many civilians, forcing some sections to be abandoned.\n\nAs the war continued, the government of Turkey and UNESCO jointly began a reconstruction and preservation effort, intending to complete it within two years, starting with the demolition of part of the city. Historic sites were spared, and in June 2015, UNESCO added the fortress and Hensel Gardens Cultural Landscape to UNESCO's World Heritage List. UNESCO's main focus was to protect the environment of the land itself, more than the heritage of the land. And the Prime Minister himself also spoke of plans to reconstruct the city walls as a great tourist attraction intended to resemble Paris; this provoked considerable controversy in Diyarbakir, with some locals arguing that they would lose their ancient culture heritage.\n\nDiyarbakir fortress is constructed with stone, black basalt, and adobe, and has gone through countless renovations; the basalt fortifications are exceptionally durable, one reason why the structure has remained relatively intact for over 2000 years. Diyarbakir fortress is among the best surviving examples of a castle or fort built with a natural feature like a cliffside or body of water on one side as a boundary. The walls have a symbolic function as well as a defensive purpose, with inscriptions on the inner city's walls (the fort) that testify to the city of Diyarbakir's history.\n\nToday, Diyarbakir Fortress can be divided into two categories, the bailey and the citadel. In the northeast, the citadel contains the first settlement inside Diyarbakir, and those walls stretch 598 meters long. The bailey houses a tower and the city walls, surrounding the much more urban walled city region of Diyarbakir. Most of these walls are constructed with traditional masonry and construction styles; the towers consist of 2-4 floors and are 4.4 meters thick on the ground floor and get thinner on higher floors.\n\nThe castle plans reveal the dominance of two different building forms, circular and tetragonal. The walls were divided into five groups, four of which contained the towers around the four main gates, while the fifth contained the citadel towers. It has been found that 65 of the original 82 towers still remain on the outside of the city's walls and 18 of the citadel's towers remain today. Due to cultural differences, the fort has undergone some modifications. The fort was reconstructed, repaired and or heightened over time. However, the overall typology has remained constant in the fort's renovations.\n\nDiyarbakir fortress was first built in 297 AD by Romans under the order of Constantius II. Over the next 1500+ years, these walls were expanded and fortified using volcanic rock from the surrounding region. There are four main gates and 82 watch towers on the walls. The Diyarbakir city walls have an ancient history dating back to the Romans. The towers at Diyarbakir were mainly built by the Romans and later reconstructed by the Ottomans when they took over the city in the 15th and 16th centuries. During the defeat of the Safavids at Diyarbakir, the Ottomans destroyed the walls with the use of cannons and therefore had to be rebuilt. Today, the walls are largely intact, and form a ring around the old city that is over 3 miles in circumference. The walls are over 33 feet high and are about 10-16 feet thick. They are the widest and longest complete defensive walls in the world after only the Great Wall of China.\n\nDiyarbakir Fortress was built, used, and rebuilt during the Roman and Ottoman periods, including the Diyarbakir city walls that measure up to 53 meters long. There are both inner and outer walls; the site also has several gates and towers. Included on the walls are about 63 inscriptions from various historical periods including remains from the Hurrians, Medes, Romans, Sassanians, Byzantines, Marwanids (Kurds), Ayyubids (Kurds), and Ottomans. The city is considered to be a “multi-cultural, multi-lingual, and multi-culture character\" Recently, the war between the Turkish army and Kurdish guerillas has resulted in damage to the fortress and surrounding monuments, disrupting government plans to conserve the historical fortress in hopes of attracting tourists to the Diyarbakir cultural area. About one-third of the historic Old Town was deliberaitely destroyed by the Turkish state after the clashes ended, damaging the ancient city irreversibly.\n\n"}
{"id": "87030", "url": "https://en.wikipedia.org/wiki?curid=87030", "title": "Glottochronology", "text": "Glottochronology\n\nGlottochronology (from Attic Greek γλῶττα \"tongue, language\" and χρóνος \"time\") is the part of lexicostatistics dealing with the chronological relationship between languages.\n\nThe idea was developed by Morris Swadesh under two assumptions: that there does, indeed, exist a relatively stable \"basic vocabulary\" (referred to as \"Swadesh lists\") in all languages of the world; and that any replacements happen in a way analogous to radioactive decay, in constant percentages per time elapsed. Meanwhile, there exist many different methods, partly extensions of the Swadesh method, now more and more under the biological assumptions of replacements in genes. However, Swadesh's technique is so well known that, for many people, 'glottochronology' refers to it alone.\n\nThe original method presumed that the core vocabulary of a language is replaced at a constant (or constant average) rate across all languages and cultures, and can therefore be used to measure the passage of time. The process makes use of a list of lexical terms. Lists were compiled by Morris Swadesh and assumed to be resistant against borrowing (originally designed in 1952 as a list of 200 items; however, the refined 100 word list in Swadesh (1955) is much more common among modern day linguists). This core vocabulary was designed to encompass concepts common to every human language (such as personal pronouns, body parts, heavenly bodies, verbs of basic actions, numerals 'one' and 'two', etc.), eliminating concepts that are specific to a particular culture or time. It has been found that this ideal is not in fact possible and that the meaning set may need to be tailored to the languages being compared. Many alternative word lists have been compiled by other linguists, often using fewer meaning slots.\n\nThe percentage of cognates (words that have a common origin) in these word lists is then measured. The larger the percentage of cognates, the more recently the two languages being compared are presumed to have separated.\n\nRobert Lees obtained a value for the \"glottochronological constant\" (r) of words by considering the known changes in 13 pairs of languages using the 200 word list. He obtained a value of 0.805 ± 0.0176 with 90% confidence. For the 100 word list Swadesh obtained a value of 0.86, the higher value reflecting the elimination of semantically unstable words. This constant may be related to the retention rate of words by:-\n\nwhere \"L\" is the rate of replacement, ln is the logarithm to base e, and \"r\" is the glottochronological constant\n\nThe basic formula of glottochronology in its shortest form is:-\n\nwhere \"t\" = a given period of time from one stage of the language to another, \"c\" = proportion of wordlist items retained at the end of that period, and \"L\" = rate of replacement for that word list.\n\nOne can also therefore formulate that:\n\nBy testing historically verifiable cases where we have knowledge of \"t\" through non-linguistic data (e. g. the approximate distance from Classical Latin to modern Romance languages), Swadesh arrived at the empirical value of approximately 0.14 for \"L\" (meaning that the rate of replacement constitutes around 14 words from the 100-wordlist per millennium).\n\nGlottochronology was found to work in the case of Indo-European, accounting for 87% of the variance. It is also postulated to work for Hamito-Semitic (Fleming 1973), Chinese (Munro 1978) and Amerind (Stark 1973; Baumhoff and Olmsted 1963). For the latter, correlations have been obtained with radiocarbon dating and blood groups as well as archaeology.\nNote that the approach of Gray and Atkinson, as they say, has nothing to do with \"glottochronology\".\n\nThe concept of language change is old, and its history is reviewed in Hymes (1973) and Wells (1973). Glottochronology itself dates back to the mid-20th century. An introduction to the subject is given in Embleton (1986) and in McMahon and McMahon (2005).\n\nGlottochronology has been controversial ever since, partly owing to issues of accuracy, as well as the question of whether its basis is sound (see e.g. Bergsland 1958; Bergsland and Vogt 1962; Fodor 1961; Chretien 1962; Guy 1980). These concerns have been addressed by Dobson et al. (1972), Dyen (1973) and Kruskal, Dyen and Black (1973). The assumption of a single-word replacement rate can distort the divergence-time estimate when borrowed words are included (Thomason and Kaufman 1988). Chrétien purported to disprove the mathematics of the Swadesh-model. At a conference at Yale in 1971 his criticisms were shown to be invalid. See the published proceedings under Dyen (1973) The same conference saw the application of the theory to Creole language (Wittmann 1973).\nAn overview of recent arguments can be obtained from the papers of a conference held at the McDonald Institute in 2000. These presentations vary from \"Why linguists don't do dates\" to the one by Starostin discussed above.\nSince its original inception, glottochronology has been rejected by many linguists, mostly Indo-Europeanists of the school of the traditional comparative method. Criticisms have been answered in particular around three points of discussion.\n\n\nSomewhere in between the original concept of Swadesh and the rejection of glottochronology in its entirety lies the idea that glottochronology as a formal method of linguistic analysis becomes valid with the help of several important modifications. Thus, inhomogeneities in the replacement rate were dealt with by Van der Merwe (1966) by splitting the word list into classes each with their own rate, while Dyen, James and Cole (1967) allowed each meaning to have its own rate. Simultaneous estimation of divergence time and replacement rate was studied by Kruskal, Dyen and Black.\n\nBrainard (1970) allowed for chance cognation and drift effects was introduced by Gleason (1959). Sankoff (1973) suggested introducing a borrowing parameter and allowed synonyms.\n\nA combination of these various improvements is given in Sankoff's \"Fully Parameterised Lexicostatistics\". In 1972 Sankoff in a biological context developed a model of genetic divergence of populations. Embleton (1981) derives a simplified version of this in a linguistic context. She carries out a number of simulations using this which are shown to give good results.\n\nImprovements in statistical methodology related to a completely different branch of science – changes in DNA over time – have sparked a recent renewed interest. These methods are more robust than the earlier ones because they calibrate points on the tree with known historical events and smooth the rates of change across these. As such, they no longer require the assumption of a constant rate of change (Gray & Atkinson 2003).\n\nAnother attempt to introduce such modifications was performed by the Russian linguist Sergei Starostin, who had proposed that\n\n\nThe resulting formula, taking into account both the time dependence and the individual stability quotients, looks as follows:\n\nIn this formula, −\"Lc\" reflects the gradual slowing down of the replacement process due to different individual rates (the less stable elements are the first and the quickest to be replaced), whereas the square root represents the reverse trend – acceleration of replacement as items in the original wordlist \"age\" and become more prone to shifting their meaning. The formula is obviously more complicated than Swadesh's original one, but, as shown in Starostin's work, yields more credible results than the former (and more or less agrees with all the cases of language separation that can be confirmed by historical knowledge). On the other hand, it shows that glottochronology can really only be used as a serious scientific tool on language families the historical phonology of which has been meticulously elaborated (at least to the point of being able to clearly distinguish between cognates and loanwords).\n\nThe McDonald Institute hosted a conference on the issue of time-depth estimation in 2000. The published papers give an idea of the views on glottochronology at that time. These vary from \"Why linguists don't do dates\" to the one by Starostin discussed above. Note that in the referenced Gray and Atkinson paper, they hold that their methods can not be called \"glottochronology\", by confining this term to its original method.\n\n\n"}
{"id": "12223", "url": "https://en.wikipedia.org/wiki?curid=12223", "title": "Great man theory", "text": "Great man theory\n\nThe great man theory is a 19th-century idea according to which history can be largely explained by the impact of great men, or heroes; highly influential individuals who, due to either their personal charisma, intelligence, wisdom, or political skill used their power in a way that had a decisive historical impact. The theory was popularized in the 1840s by Scottish writer Thomas Carlyle, but in 1860 Herbert Spencer formulated a counter-argument that has remained influential throughout the 20th century to the present: Spencer said that such great men are the products of their societies, and that their actions would be impossible without the social conditions built before their lifetimes.\n\nCarlyle stated that \"The history of the world is but the biography of great men\", reflecting his belief that heroes shape history through both their personal attributes and divine inspiration. In his book \"On Heroes, Hero-Worship and the Heroic in History\", Carlyle saw history as having turned on the decisions of \"heroes\", giving detailed analysis of the influence of several such men (including Muhammad, Shakespeare, Martin Luther, Rousseau, Pericles, Napoleon, and Richard Wagner). Carlyle also felt that the study of great men was \"profitable\" to one's own heroic side; that by examining the lives led by such heroes, one could not help but uncover something about one's true nature.\n\nAmerican scholar Frederick Adams Woods supported the great man theory in his work \"The Influence of Monarchs: Steps in a New Science of History\". Woods investigated 386 rulers in Western Europe from the 12th century until the French revolution in the late 18th century and their influence on the course of historical events.\n\nThis theory is usually contrasted with \"history from below\", which emphasizes the life of the masses over the leader. An overwhelming wave of smaller events causes certain developments to occur. The Great Man approach to history was most fashionable with professional historians in the 19th century; a popular work of this school is the \"Encyclopædia Britannica Eleventh Edition\" (1911) which contains lengthy and detailed biographies about the great men of history, but very few general or social histories. For example, all information on the post-Roman \"Migrations Period\" of European History is compiled under the biography of Attila the Hun. This heroic view of history was also strongly endorsed by some philosophers, such as Leon Bloy, Hegel, Kierkegaard, Nietzsche, Spengler and Max Weber, but it fell out of favor after World War II.\n\nIn \"Untimely Meditations\", Nietzsche writes that \"the goal of humanity lies in its highest specimens\".\n\nIn \"Fear and Trembling\", Kierkegaard writes that \"to be able to fall down in such a way that the same second it looks as if one were standing and walking, to transform the leap of life into a walk, absolutely to express the sublime and the pedestrian—that only these knights of faith can do—this is the one and only prodigy.\"\n\nHegel, proceeding from providentialist theory, argued that \"what is real is reasonable\" and World-Historical individuals are World-Spirit's agents. Hegel wrote: \"Such are great historical men—whose own particular aims involve those large issues which are the will of the World-Spirit.\" Thus, according to Hegel, a great man does not create historical reality himself but only uncovers the inevitable future.\n\nOne of the most forceful critics of Carlyle's formulation of the great man theory was Herbert Spencer, who believed that attributing historical events to the decisions of individuals was a hopelessly primitive, childish, and unscientific position. He believed that the men Carlyle called \"great men\" were merely products of their social environment:\n\nTolstoy's \"War and Peace\" features criticism of Great Man Theories as a recurring theme in the philosophical digressions. According to Tolstoy, the significance of great individuals is imaginary; as a matter of fact they are only \"history's slaves\" realizing the decree of Providence.\n\nWilliam James in his lecture \"Great Men and Their Environment\" underlined the importance of the Great Man's congruence with the surroundings (in the broad sense), though his ultimate point was that environments and individuals shape each other reciprocally, just as environments and individual members of animal species do according to Darwinian theory.\n\nAmong modern critics of the theory, one, Sidney Hook, is supportive of the idea; he gives credit to those who shape events through their actions, and his book \"The Hero in History\" is devoted to the role of the hero and in history and influence of the outstanding persons.\n\nLeonid Grinin defines a historical figure (a great man) thus:\n\n"}
{"id": "57726569", "url": "https://en.wikipedia.org/wiki?curid=57726569", "title": "Hans Werner Debrunner", "text": "Hans Werner Debrunner\n\nHans Werner Debrunner (1923 –1998) was a Swiss German historian and theologian whose work mainly covered mission history, West Africa and the African diaspora. He also carried out academic research on history relating to missiology in northern, eastern and southern Africa. Upon his death in 1998, his private library and archive were donated to the \"Carl Schlettwein Foundation\". The \"independent, self-contained collection\" comprises more than 3100 books and single journal issues on his area of specialty, published mostly in the first half to mid-twentieth century. Furthermore, Debrunner’s academic archives is a compilation of historiography and ethnography, particularly bio-bibliographies of Swiss missionaries and native African pastors and missionaries who worked with the Basel Mission in Africa. Debrunner also documented history, socio-political and intercultural relations between Africa and Europe. Hans Debrunner’s accounts explored the life and works of African saints, royalty, aristocrats, noblemen, political envoys, writers, intellectuals, scholars and artists who lived in or visited Europe from the early medieval period through the Enlightenment until the end of the World War I era.\n\n"}
{"id": "47499031", "url": "https://en.wikipedia.org/wiki?curid=47499031", "title": "Historical anthropology", "text": "Historical anthropology\n\nHistorical anthropology is a historiographical movement which applies methodologies and objectives from Social and Cultural Anthropology to the study of historical societies. Like most such movements, it is understood in different ways by different scholars, and to some may be synonymous with the history of mentalities, cultural history, ethnohistory, microhistory, history from below or \"Alltagsgeschichte\". Anthropologists whose work has been particularly inspirational to historical anthropology include Emile Durkheim, Clifford Geertz, Arnold van Gennep, Jack Goody, Lucien Lévy-Bruhl, Marcel Mauss and Victor Turner.\n\nPeter Burke has contrasted historical anthropology with Social History, finding that historical anthropology tends to focus on qualitative rather than quantitative data, smaller communities, and symbolic aspects of culture. Thus it reflects a turn away, in the 1960s, in Marxist historiography from 'the orthodox Marxist approach to human behaviour in which actors are seen as motivated in the first instance by economics, and only secondarily by culture or ideology', in the work of historians such as E. P. Thompson.\n\nHistorical anthropology was rooted in the Annales School, associated with a succession of major historians such as Fernand Braudel, Jacques Le Goff, Emmanuel Le Roy Ladurie and Pierre Nora, along with researchers from elsewhere on the Continent such as Carlo Ginzburg. The label \"historical anthropology\" has been actively promoted by some recent Annales School historians, such as Jean-Claude Schmitt. Established in 1929 by Marc Bloch and Lucien Febvre, the review Annales. Histoire, Sciences sociales is still among the most influential French publications for research in historical anthropology.\n\nHistorical anthropology has been open to similar criticisms to anthropology: 'as Bernard Cohn and John and Jean Comaroff have observed, studies in which societies were represented in this way were often partial, biased, and unwitting handmaidens to the domination of non-Western peoples by Europeans and Americans'. But since the Second World War, increasingly reflexive approaches have led to sophisticated developments of the field, and the banner of 'historical anthropology' has often attracted Anglo-American historians in ways that the Annales School did not: key figures have been Sidney Mintz, Jay O'Brien, William Roseberry, Marshall Sahlins, Jane Schneider, Peter Schneider, Eric Wolf, Peter Burke, and people from elsewhere in the world such as Aaron Gurevich.\n\n"}
{"id": "43349992", "url": "https://en.wikipedia.org/wiki?curid=43349992", "title": "Historical determinism", "text": "Historical determinism\n\nHistorical determinism is the stance that events are historically predetermined or currently constrained by various forces. Historical determinism can be understood in contrast to its negation, i.e. the rejection of historical determinism.\n\nSome political philosophies (e.g. Early and Stalinist Marxism) assert a historical materialism of either predetermination or constraint, or both.\n\nUsed as a pejorative, it is normally meant to designate an overdetermination of present possibilities by historical conditions.\n\n\n"}
{"id": "1923965", "url": "https://en.wikipedia.org/wiki?curid=1923965", "title": "Historical geography", "text": "Historical geography\n\nHistorical geography is the branch of geography that studies the ways in which geographic phenomena have changed over time. It is a synthesizing discipline which shares both topical and methodological similarities with history, anthropology, ecology, geology, environmental studies, literary studies, and other fields. Although the majority of work in historical geography is considered human geography, the field also encompasses studies of geographic change which are not primarily anthropogenic. Historical geography is often a major component of school and university curricula in geography and social studies. Current research in historical geography is being performed by scholars in more than forty countries. \n\nHistorical geography seeks to determine how cultural features of various societies across the planet emerged and evolved by understanding their interaction with their local environment and surroundings.\n\nIn its early days, historical geography was difficult to define as a subject. A textbook from the 1950s cites a previous definition as an 'unsound attempt by geographers to explain history'. Its author, J. B. Mitchell, came down firmly on the side of geography: 'the historical geographer is a geographer first last and all the time'. By 1975 the first number of the \"Journal of Historical Geography\" had widened the discipline to a broader church: 'the writings of scholars of any disciplinary provenance who have something to say about matters of geographical interest relating to past time'.\n\nFor some in the United States of America, the term \"historical geography\" has a more specialized meaning: the name given by Carl Ortwin Sauer of the University of California, Berkeley to his program of reorganizing cultural geography (some say all geography) along regional lines, beginning in the first decades of the 20th century. To Sauer, a landscape and the cultures in it could only be understood if all of its influences through history were taken into account: physical, cultural, economic, political, environmental. Sauer stressed regional specialization as the only means of gaining sufficient expertise on regions of the world. Sauer's philosophy was the principal shaper of American geographic thought in the mid-20th century. Regional specialists remain in academic geography departments to this day. Despite this, some geographers feel that it harmed the discipline; that too much effort was spent on data collection and classification, and too little on analysis and explanation. Studies became more and more area-specific as later geographers struggled to find places to make names for themselves. These factors may have led in turn to the 1950s crisis in geography, which raised serious questions about geography as an academic discipline in the USA.\n\nThis sub-branch of human geography is closely related to history, environmental history, and historical ecology. \n\n\n\n"}
{"id": "10772350", "url": "https://en.wikipedia.org/wiki?curid=10772350", "title": "History", "text": "History\n\nHistory (from Greek , \"historia\", meaning \"inquiry, knowledge acquired by investigation\") is the study of the past as it is described in written documents. Events occurring before written record are considered prehistory. It is an umbrella term that relates to past events as well as the memory, discovery, collection, organization, presentation, and interpretation of information about these events. Scholars who write about history are called historians.\n\nHistory can also refer to the academic discipline which uses a narrative to examine and analyse a sequence of past events, and objectively determine the patterns of cause and effect that determine them. Historians sometimes debate the nature of history and its usefulness by discussing the study of the discipline as an end in itself and as a way of providing \"perspective\" on the problems of the present.\n\nStories common to a particular culture, but not supported by external sources (such as the tales surrounding King Arthur), are usually classified as cultural heritage or legends, because they do not show the \"disinterested investigation\" required of the discipline of history. Herodotus, a 5th-century BC Greek historian is considered within the Western tradition to be the \"father of history\", and, along with his contemporary Thucydides, helped form the foundations for the modern study of human history. Their works continue to be read today, and the gap between the culture-focused Herodotus and the military-focused Thucydides remains a point of contention or approach in modern historical writing. In East Asia, a state chronicle, the Spring and Autumn Annals was known to be compiled from as early as 722 BC although only 2nd-century BC texts survived.\n\nAncient influences have helped spawn variant interpretations of the nature of history which have evolved over the centuries and continue to change today. The modern study of history is wide-ranging, and includes the study of specific regions and the study of certain topical or thematical elements of historical investigation. Often history is taught as part of primary and secondary education, and the academic study of history is a major discipline in university studies.\n\nThe word \"history\" comes ultimately from Ancient Greek ἱστορία (\"historía\"), meaning \"inquiry\", \"knowledge from inquiry\", or \"judge\". It was in that sense that Aristotle used the word in his (\"Perì Tà Zôa Ηistoríai\" \"Inquiries about Animals\"). The ancestor word is attested early on in Homeric Hymns, Heraclitus, the Athenian ephebes' oath, and in Boiotic inscriptions (in a legal sense, either \"judge\" or \"witness\", or similar).\n\nThe Greek word was borrowed into Classical Latin as \"historia\", meaning \"investigation, inquiry, research, account, description, written account of past events, writing of history, historical narrative, recorded knowledge of past events, story, narrative\". \"History\" was borrowed from Latin (possibly via Old Irish or Old Welsh) into Old English as \"stær\" ('history, narrative, story'), but this word fell out of use in the late Old English period.\n\nMeanwhile, as Latin became Old French (and Anglo-Norman), \"historia\" developed into forms such as \"istorie\", \"estoire\", and \"historie\", with new developments in the meaning: \"account of the events of a person's life (beginning of the 12th century), chronicle, account of events as relevant to a group of people or people in general (1155), dramatic or pictorial representation of historical events (c. 1240), body of knowledge relative to human evolution, science (c. 1265), narrative of real or imaginary events, story (c. 1462)\".\n\nIt was from Anglo-Norman that \"history\" was borrowed into Middle English, and this time the loan stuck. It appears in the thirteenth-century \"Ancrene Wisse\", but seems to have become a common word in the late fourteenth century, with an early attestation appearing in John Gower's \"Confessio Amantis\" of the 1390s (VI.1383): \"I finde in a bok compiled | To this matiere an old histoire, | The which comth nou to mi memoire\". In Middle English, the meaning of \"history\" was \"story\" in general. The restriction to the meaning \"the branch of knowledge that deals with past events; the formal record or study of past events, esp. human affairs\" arose in the mid-fifteenth century.\n\nWith the Renaissance, older senses of the word were revived, and it was in the Greek sense that Francis Bacon used the term in the late sixteenth century, when he wrote about \"Natural History\". For him, \"historia\" was \"the knowledge of objects determined by space and time\", that sort of knowledge provided by memory (while science was provided by reason, and poetry was provided by fantasy).\n\nIn an expression of the linguistic synthetic vs. analytic/isolating dichotomy, English like Chinese (史 vs. 诌) now designates separate words for human history and storytelling in general. In modern German, French, and most Germanic and Romance languages, which are solidly synthetic and highly inflected, the same word is still used to mean both \"history\" and \"story\".\n\nThe adjective \"historical\" is attested from 1661, and \"historic\" from 1669.\n\n\"Historian\" in the sense of a \"researcher of history\" is attested from 1531. In all European languages, the substantive \"history\" is still used to mean both \"what happened with men\", and \"the scholarly study of the happened\", the latter sense sometimes distinguished with a capital letter, \"History\", or the word \"historiography\".\n\nHistorians write in the context of their own time, and with due regard to the current dominant ideas of how to interpret the past, and sometimes write to provide lessons for their own society. In the words of Benedetto Croce, \"All history is contemporary history\". History is facilitated by the formation of a \"true discourse of past\" through the production of narrative and analysis of past events relating to the human race. The modern discipline of history is dedicated to the institutional production of this discourse.\n\nAll events that are remembered and preserved in some authentic form constitute the historical record. The task of historical discourse is to identify the sources which can most usefully contribute to the production of accurate accounts of past. Therefore, the constitution of the historian's archive is a result of circumscribing a more general archive by invalidating the usage of certain texts and documents (by falsifying their claims to represent the \"true past\").\n\nThe study of history has sometimes been classified as part of the humanities and at other times as part of the social sciences. It can also be seen as a bridge between those two broad areas, incorporating methodologies from both. Some individual historians strongly support one or the other classification. In the 20th century, French historian Fernand Braudel revolutionized the study of history, by using such outside disciplines as economics, anthropology, and geography in the study of global history.\n\nTraditionally, historians have recorded events of the past, either in writing or by passing on an oral tradition, and have attempted to answer historical questions through the study of written documents and oral accounts. From the beginning, historians have also used such sources as monuments, inscriptions, and pictures. In general, the sources of historical knowledge can be separated into three categories: what is written, what is said, and what is physically preserved, and historians often consult all three. But writing is the marker that separates history from what comes before.\n\nArchaeology is a discipline that is especially helpful in dealing with buried sites and objects, which, once unearthed, contribute to the study of history. But archaeology rarely stands alone. It uses narrative sources to complement its discoveries. However, archaeology is constituted by a range of methodologies and approaches which are independent from history; that is to say, archaeology does not \"fill the gaps\" within textual sources. Indeed, \"historical archaeology\" is a specific branch of archaeology, often contrasting its conclusions against those of contemporary textual sources. For example, Mark Leone, the excavator and interpreter of historical Annapolis, Maryland, USA; has sought to understand the contradiction between textual documents and the material record, demonstrating the possession of slaves and the inequalities of wealth apparent via the study of the total historical environment, despite the ideology of \"liberty\" inherent in written documents at this time.\n\nThere are varieties of ways in which history can be organized, including chronologically, culturally, territorially, and thematically. These divisions are not mutually exclusive, and significant overlaps are often present, as in \"The International Women's Movement in an Age of Transition, 1830–1975.\" It is possible for historians to concern themselves with both the very specific and the very general, although the modern trend has been toward specialization. The area called Big History resists this specialization, and searches for universal patterns or trends. History has often been studied with some practical or theoretical aim, but also may be studied out of simple intellectual curiosity.\n\nThe history of the world is the memory of the past experience of \"Homo sapiens sapiens\" around the world, as that experience has been preserved, largely in written records. By \"prehistory\", historians mean the recovery of knowledge of the past in an area where no written records exist, or where the writing of a culture is not understood. By studying painting, drawings, carvings, and other artifacts, some information can be recovered even in the absence of a written record. Since the 20th century, the study of prehistory is considered essential to avoid history's implicit exclusion of certain civilizations, such as those of Sub-Saharan Africa and pre-Columbian America. Historians in the West have been criticized for focusing disproportionately on the Western world. In 1961, British historian E. H. Carr wrote:\n\nThis definition includes within the scope of history the strong interests of peoples, such as Indigenous Australians and New Zealand Māori in the past, and the oral records maintained and transmitted to succeeding generations, even before their contact with European civilization.\n\nHistoriography has a number of related meanings. Firstly, it can refer to how history has been produced: the story of the development of methodology and practices (for example, the move from short-term biographical narrative towards long-term thematic analysis). Secondly, it can refer to what has been produced: a specific body of historical writing (for example, \"medieval historiography during the 1960s\" means \"Works of medieval history written during the 1960s\"). Thirdly, it may refer to why history is produced: the Philosophy of history. As a meta-level analysis of descriptions of the past, this third conception can relate to the first two in that the analysis usually focuses on the narratives, interpretations, world view, use of evidence, or method of presentation of other historians. Professional historians also debate the question of whether history can be taught as a single coherent narrative or a series of competing narratives.\n\nPhilosophy of history is a branch of philosophy concerning the eventual significance, if any, of human history. Furthermore, it speculates as to a possible teleological end to its development—that is, it asks if there is a design, purpose, directive principle, or finality in the processes of human history. Philosophy of history should not be confused with historiography, which is the study of history as an academic discipline, and thus concerns its methods and practices, and its development as a discipline over time. Nor should philosophy of history be confused with the history of philosophy, which is the study of the development of philosophical ideas through time.\n\nThe historical method comprises the techniques and guidelines by which historians use primary sources and other evidence to research and then to write history.\n\nHerodotus of Halicarnassus (484 BC – ca.425 BC) has generally been acclaimed as the \"father of history\". However, his contemporary Thucydides (c. 460 BC – ca. 400 BC) is credited with having first approached history with a well-developed historical method in his work the \"History of the Peloponnesian War\". Thucydides, unlike Herodotus, regarded history as being the product of the choices and actions of human beings, and looked at cause and effect, rather than as the result of divine intervention. In his historical method, Thucydides emphasized chronology, a neutral point of view, and that the human world was the result of the actions of human beings. Greek historians also viewed history as cyclical, with events regularly recurring.\n\nThere were historical traditions and sophisticated use of historical method in ancient and medieval China. The groundwork for professional historiography in East Asia was established by the Han dynasty court historian known as Sima Qian (145–90 BC), author of the \"Records of the Grand Historian\" (\"Shiji\"). For the quality of his written work, Sima Qian is posthumously known as the Father of Chinese historiography. Chinese historians of subsequent dynastic periods in China used his \"Shiji\" as the official format for historical texts, as well as for biographical literature.\n\nSaint Augustine was influential in Christian and Western thought at the beginning of the medieval period. Through the Medieval and Renaissance periods, history was often studied through a sacred or religious perspective. Around 1800, German philosopher and historian Georg Wilhelm Friedrich Hegel brought philosophy and a more secular approach in historical study.\n\nIn the preface to his book, the \"Muqaddimah\" (1377), the Arab historian and early sociologist, Ibn Khaldun, warned of seven mistakes that he thought that historians regularly committed. In this criticism, he approached the past as strange and in need of interpretation. The originality of Ibn Khaldun was to claim that the cultural difference of another age must govern the evaluation of relevant historical material, to distinguish the principles according to which it might be possible to attempt the evaluation, and lastly, to feel the need for experience, in addition to rational principles, in order to assess a culture of the past. Ibn Khaldun often criticized \"idle superstition and uncritical acceptance of historical data.\" As a result, he introduced a scientific method to the study of history, and he often referred to it as his \"new science\". His historical method also laid the groundwork for the observation of the role of state, communication, propaganda and systematic bias in history, and he is thus considered to be the \"father of historiography\" or the \"father of the philosophy of history\".\n\nIn the West, historians developed modern methods of historiography in the 17th and 18th centuries, especially in France and Germany. The 19th-century historian with greatest influence on methods was Leopold von Ranke in Germany.\n\nIn the 20th century, academic historians focused less on epic nationalistic narratives, which often tended to glorify the nation or great men, to more objective and complex analyses of social and intellectual forces. A major trend of historical methodology in the 20th century was a tendency to treat history more as a social science rather than as an art, which traditionally had been the case. Some of the leading advocates of history as a social science were a diverse collection of scholars which included Fernand Braudel, E. H. Carr, Fritz Fischer, Emmanuel Le Roy Ladurie, Hans-Ulrich Wehler, Bruce Trigger, Marc Bloch, Karl Dietrich Bracher, Peter Gay, Robert Fogel, Lucien Febvre and Lawrence Stone. Many of the advocates of history as a social science were or are noted for their multi-disciplinary approach. Braudel combined history with geography, Bracher history with political science, Fogel history with economics, Gay history with psychology, Trigger history with archaeology while Wehler, Bloch, Fischer, Stone, Febvre and Le Roy Ladurie have in varying and differing ways amalgamated history with sociology, geography, anthropology, and economics. More recently, the field of digital history has begun to address ways of using computer technology to pose new questions to historical data and generate digital scholarship.\n\nIn opposition to the claims of history as a social science, historians such as Hugh Trevor-Roper, John Lukacs, Donald Creighton, Gertrude Himmelfarb and Gerhard Ritter argued that the key to the historians' work was the power of the imagination, and hence contended that history should be understood as an art. French historians associated with the Annales School introduced quantitative history, using raw data to track the lives of typical individuals, and were prominent in the establishment of cultural history (cf. \"histoire des mentalités\"). Intellectual historians such as Herbert Butterfield, Ernst Nolte and George Mosse have argued for the significance of ideas in history. American historians, motivated by the civil rights era, focused on formerly overlooked ethnic, racial, and socio-economic groups. Another genre of social history to emerge in the post-WWII era was \"Alltagsgeschichte\" (History of Everyday Life). Scholars such as Martin Broszat, Ian Kershaw and Detlev Peukert sought to examine what everyday life was like for ordinary people in 20th-century Germany, especially in the Nazi period.\n\nMarxist historians such as Eric Hobsbawm, E. P. Thompson, Rodney Hilton, Georges Lefebvre, Eugene Genovese, Isaac Deutscher, C. L. R. James, Timothy Mason, Herbert Aptheker, Arno J. Mayer and Christopher Hill have sought to validate Karl Marx's theories by analyzing history from a Marxist perspective. In response to the Marxist interpretation of history, historians such as François Furet, Richard Pipes, J. C. D. Clark, Roland Mousnier, Henry Ashby Turner and Robert Conquest have offered anti-Marxist interpretations of history. Feminist historians such as Joan Wallach Scott, Claudia Koonz, Natalie Zemon Davis, Sheila Rowbotham, Gisela Bock, Gerda Lerner, Elizabeth Fox-Genovese, and Lynn Hunt have argued for the importance of studying the experience of women in the past. In recent years, postmodernists have challenged the validity and need for the study of history on the basis that all history is based on the personal interpretation of sources. In his 1997 book \"In Defence of History\", Richard J. Evans defended the worth of history. Another defence of history from post-modernist criticism was the Australian historian Keith Windschuttle's 1994 book, \"The Killing of History\".\n\nThe Marxist theory of historical materialism theorises that society is fundamentally determined by the \"material conditions\" at any given time – in other words, the relationships which people have with each other in order to fulfill basic needs such as feeding, clothing and housing themselves and their families. Overall, Marx and Engels claimed to have identified five successive stages of the development of these material conditions in Western Europe. Marxist historiography was once orthodoxy in the Soviet Union, but since the collapse of communism there in 1991, Mikhail Krom says it has been reduced to the margins of scholarship.\n\nHistorical study often focuses on events and developments that occur in particular blocks of time. Historians give these periods of time names in order to allow \"organising ideas and classificatory generalisations\" to be used by historians. The names given to a period can vary with geographical location, as can the dates of the beginning and end of a particular period. Centuries and decades are commonly used periods and the time they represent depends on the dating system used. Most periods are constructed retrospectively and so reflect value judgments made about the past. The way periods are constructed and the names given to them can affect the way they are viewed and studied.\n\nThe field of history generally leaves prehistory to the archaeologists, who have entirely different sets of tools and theories. The usual method for periodisation of the distant prehistoric past, in archaeology is to rely on changes in material culture and technology, such as the Stone Age, Bronze Age and Iron Age and their sub-divisions also based on different styles of material remains. Here prehistory is divided into a series of \"chapters\" so that periods in history could unfold not only in a relative chronology but also narrative chronology. This narrative content could be in the form of functional-economic interpretation. There are periodisation, however, that do not have this narrative aspect, relying largely on relative chronology and, thus, devoid of any specific meaning.\n\nDespite the development over recent decades of the ability through radiocarbon dating and other scientific methods to give actual dates for many sites or artefacts, these long-established schemes seem likely to remain in use. In many cases neighbouring cultures with writing have left some history of cultures without it, which may be used. Periodisation, however, is not viewed as a perfect framework with one account explaining that \"cultural changes do not conveniently start and stop (combinedly) at periodisation boundaries\" and that different trajectories of change are also needed to be studied in their own right before they get intertwined with cultural phenomena. \n\nParticular geographical locations can form the basis of historical study, for example, continents, countries, and cities. Understanding why historic events took place is important. To do this, historians often turn to geography. According to Jules Michelet in his book \"Histoire de France\" (1833), \"without geographical basis, the people, the makers of history, seem to be walking on air.\" Weather patterns, the water supply, and the landscape of a place all affect the lives of the people who live there. For example, to explain why the ancient Egyptians developed a successful civilization, studying the geography of Egypt is essential. Egyptian civilization was built on the banks of the Nile River, which flooded each year, depositing soil on its banks. The rich soil could help farmers grow enough crops to feed the people in the cities. That meant everyone did not have to farm, so some people could perform other jobs that helped develop the civilization. There is also the case of climate, which historians like Ellsworth Huntington and Allen Semple, cited as a crucial influence on the course of history and racial temperament.\n\n\nMilitary history concerns warfare, strategies, battles, weapons, and the psychology of combat. The \"new military history\" since the 1970s has been concerned with soldiers more than generals, with psychology more than tactics, and with the broader impact of warfare on society and culture.\n\nThe history of religion has been a main theme for both secular and religious historians for centuries, and continues to be taught in seminaries and academe. Leading journals include \"Church History\", \"The Catholic Historical Review\", and \"History of Religions\". Topics range widely from political and cultural and artistic dimensions, to theology and liturgy. This subject studies religions from all regions and areas of the world where humans have lived.\n\n\"Social history\", sometimes called the \"new social history\", is the field that includes history of ordinary people and their strategies and institutions for coping with life. In its \"golden age\" it was a major growth field in the 1960s and 1970s among scholars, and still is well represented in history departments. In two decades from 1975 to 1995, the proportion of professors of history in American universities identifying with social history rose from 31% to 41%, while the proportion of political historians fell from 40% to 30%. In the history departments of British universities in 2007, of the 5723 faculty members, 1644 (29%) identified themselves with social history while political history came next with 1425 (25%).\nThe \"old\" social history before the 1960s was a hodgepodge of topics without a central theme, and it often included political movements, like Populism, that were \"social\" in the sense of being outside the elite system. Social history was contrasted with political history, intellectual history and the history of great men. English historian G. M. Trevelyan saw it as the bridging point between economic and political history, reflecting that, \"Without social history, economic history is barren and political history unintelligible.\" While the field has often been viewed negatively as history with the politics left out, it has also been defended as \"history with the people put back in.\"\n\nThe chief subfields of social history include:\nSmaller specialties include:\n\nCultural history replaced social history as the dominant form in the 1980s and 1990s. It typically combines the approaches of anthropology and history to look at language, popular cultural traditions and cultural interpretations of historical experience. It examines the records and narrative descriptions of past knowledge, customs, and arts of a group of people. How peoples constructed their memory of the past is a major topic.\nCultural history includes the study of art in society as well is the study of images and human visual production (iconography).\n\nDiplomatic history focuses on the relationships between nations, primarily regarding diplomacy and the causes of wars. More recently it looks at the causes of peace and human rights. It typically presents the viewpoints of the foreign office, and long-term strategic values, as the driving force of continuity and change in history. This type of \"political history\" is the study of the conduct of international relations between states or across state boundaries over time. Historian Muriel Chamberlain notes that after the First World War, \"diplomatic history replaced constitutional history as the flagship of historical investigation, at once the most important, most exact and most sophisticated of historical studies.\" She adds that after 1945, the trend reversed, allowing social history to replace it.\n\nAlthough economic history has been well established since the late 19th century, in recent years academic studies have shifted more and more toward economics departments and away from traditional history departments. Business history deals with the history of individual business organizations, business methods, government regulation, labour relations, and impact on society. It also includes biographies of individual companies, executives, and entrepreneurs. It is related to economic history; Business history is most often taught in business schools.\n\nEnvironmental history is a new field that emerged in the 1980s to look at the history of the environment, especially in the long run, and the impact of human activities upon it.\n\nWorld history is the study of major civilizations over the last 3000 years or so. World history is primarily a teaching field, rather than a research field. It gained popularity in the United States, Japan and other countries after the 1980s with the realization that students need a broader exposure to the world as globalization proceeds.\n\nIt has led to highly controversial interpretations by Oswald Spengler and Arnold J. Toynbee, among others.\n\nThe World History Association publishes the \"Journal of World History\" every quarter since 1990. The H-World discussion list serves as a network of communication among practitioners of world history, with discussions among scholars, announcements, syllabi, bibliographies and book reviews.\n\nA people's history is a type of historical work which attempts to account for historical events from the perspective of common people. A people's history is the history of the world that is the story of mass movements and of the outsiders. Individuals or groups not included in the past in other type of writing about history are the primary focus, which includes the disenfranchised, the oppressed, the poor, the nonconformists, and the otherwise forgotten people. The authors are typically on the left and have a socialist model in mind, as in the approach of the History Workshop movement in Britain in the 1960s.\n\nIntellectual history and the history of ideas emerged in the mid-20th century, with the focus on the intellectuals and their books on the one hand, and on the other the study of ideas as disembodied objects with a career of their own.\n\nGender history is a sub-field of History and Gender studies, which looks at the past from the perspective of gender. It is in many ways, an outgrowth of women's history. Despite its relatively short life, Gender History (and its forerunner Women's History) has had a rather significant effect on the general study of history. Since the 1960s, when the initially small field first achieved a measure of acceptance, it has gone through a number of different phases, each with its own challenges and outcomes. Although some of the changes to the study of history have been quite obvious, such as increased numbers of books on famous women or simply the admission of greater numbers of women into the historical profession, other influences are more subtle.\n\nPublic history describes the broad range of activities undertaken by people with some training in the discipline of history who are generally working outside of specialized academic settings. Public history practice has quite deep roots in the areas of historic preservation, archival science, oral history, museum curatorship, and other related fields. The term itself began to be used in the U.S. and Canada in the late 1970s, and the field has become increasingly professionalized since that time. Some of the most common settings for public history are museums, historic homes and historic sites, parks, battlefields, archives, film and television companies, and all levels of government.\n\nProfessional and amateur historians discover, collect, organize, and present information about past events.They discover this information through archaeological evidence, written primary sources from the past and other various means such as place names. In lists of historians, historians can be grouped by order of the historical period in which they were writing, which is not necessarily the same as the period in which they specialized. Chroniclers and annalists, though they are not historians in the true sense, are also frequently included.\n\nSince the 20th century, Western historians have disavowed the aspiration to provide the \"judgement of history.\" The goals of historical judgements or interpretations are separate to those of legal judgements, that need to be formulated quickly after the events and be final. A related issue to that of the judgement of history is that of collective memory.\n\nPseudohistory is a term applied to texts which purport to be historical in nature but which depart from standard historiographical conventions in a way which undermines their conclusions.\nClosely related to deceptive historical revisionism, works which draw controversial conclusions from new, speculative, or disputed historical evidence, particularly in the fields of national, political, military, and religious affairs, are often rejected as pseudohistory.\n\nA major intellectual battle took place in Britain in the early twentieth century regarding the place of history teaching in the universities. At Oxford and Cambridge, scholarship was downplayed. Professor Charles Harding Firth, Oxford's Regius Professor of history in 1904 ridiculed the system as best suited to produce superficial journalists. The Oxford tutors, who had more votes than the professors, fought back in defence of their system saying that it successfully produced Britain's outstanding statesmen, administrators, prelates, and diplomats, and that mission was as valuable as training scholars. The tutors dominated the debate until after the Second World War. It forced aspiring young scholars to teach at outlying schools, such as Manchester University, where Thomas Frederick Tout was professionalizing the History undergraduate programme by introducing the study of original sources and requiring the writing of a thesis.\n\nIn the United States, scholarship was concentrated at the major PhD-producing universities, while the large number of other colleges and universities focused on undergraduate teaching. A tendency in the 21st century was for the latter schools to increasingly demand scholarly productivity of their younger tenure-track faculty. Furthermore, universities have increasingly relied on inexpensive part-time adjuncts to do most of the classroom teaching.\n\nFrom the origins of national school systems in the 19th century, the teaching of history to promote national sentiment has been a high priority. In the United States after World War I, a strong movement emerged at the university level to teach courses in Western Civilization, so as to give students a common heritage with Europe. In the U.S. after 1980, attention increasingly moved toward teaching world history or requiring students to take courses in non-western cultures, to prepare students for life in a globalized economy.\n\nAt the university level, historians debate the question of whether history belongs more to social science or to the humanities. Many view the field from both perspectives.\n\nThe teaching of history in French schools was influenced by the \"Nouvelle histoire\" as disseminated after the 1960s by \"Cahiers pédagogiques and Enseignement\" and other journals for teachers. Also influential was the Institut national de recherche et de documentation pédagogique, (INRDP). Joseph Leif, the Inspector-general of teacher training, said pupils children should learn about historians' approaches as well as facts and dates. Louis François, Dean of the History/Geography group in the Inspectorate of National Education advised that teachers should provide historic documents and promote \"active methods\" which would give pupils \"the immense happiness of discovery.\" Proponents said it was a reaction against the memorization of names and dates that characterized teaching and left the students bored. Traditionalists protested loudly it was a postmodern innovation that threatened to leave the youth ignorant of French patriotism and national identity.\n\nIn several countries history textbooks are tools to foster nationalism and patriotism, and give students the official line about national enemies.\n\nIn many countries, history textbooks are sponsored by the national government and are written to put the national heritage in the most favourable light. For example, in Japan, mention of the Nanking Massacre has been removed from textbooks and the entire Second World War is given cursory treatment. Other countries have complained. It was standard policy in communist countries to present only a rigid Marxist historiography.\n\nIn the United States, especially the southern part history about slavery and the American Civil War are controversial topics. McGraw-Hill Education for example, was criticised for describing Africans brought to American plantations as \"workers\" instead of slaves in a textbook.\n\nAcademic historians have often fought against the politicization of the textbooks, sometimes with success.\n\nIn 21st-century Germany, the history curriculum is controlled by the 16 states, and is characterized not by superpatriotism but rather by an \"almost pacifistic and deliberately unpatriotic undertone\" and reflects \"principles formulated by international organizations such as UNESCO or the Council of Europe, thus oriented towards human rights, democracy and peace.\" The result is that \"German textbooks usually downplay national pride and ambitions and aim to develop an understanding of citizenship centered on democracy, progress, human rights, peace, tolerance and Europeanness.\"\n\n\n\n\n\n"}
{"id": "17420714", "url": "https://en.wikipedia.org/wiki?curid=17420714", "title": "History, Classics and Archaeology Subject Centre", "text": "History, Classics and Archaeology Subject Centre\n\nThe Subject Centre for History, Classics and Archaeology was one of 24 Subject Centres funded within the Higher Education Academy to promote high quality learning and teaching in UK Higher Education (HE) by providing subject-based support for sharing innovations and good practices. Initially the only one of the 24 Subject Centres to be sited in Scotland, it was hosted originally by the University of Glasgow and later by the University of Liverpool. Its various Directors were Dr Andrew Roach, Dr Donald Spaeth, Colin Brooks and Dr Anthony Sinclair.\n\nThe Higher Education Academy itself came into being in 2004 through the amalgamation of the Learning and Teaching Support Network (LTSN), the Institute for the Learning and Teaching in Higher Education (ILTHE) and the Higher Education Staff Development Agency (HESDA) in order to enable comprehensive, knowledgeable support for practitioners and institutions with an aim to the greater professionalisation of HE teaching. \n\nThe Subject Centre for History, Classics and Archaeology supported teaching and learning in history, archaeology, and classics as broadly defined (the study of the languages and cultures of the Greek and Roman Worlds, including Byzantine Studies and Classical Reception Studies). \n\nThe key role of HCA was to be a \"one stop shop\" for the subject communities for matters relating to learning and teaching both at an individual subject level and with regard to common issues across these three Humanities disciplines. HCA did this by identifying, collecting and disseminating information and material on good practice in teaching and learning, enabling practitioners to share their experience with colleagues for mutual benefit. HCA both funded and published disciplinary specific pedagogical research (Ped Res) and development (Ped Dev) and scholarship of teaching and learning (SOTL).\n\nThe Subject Centre for History, Classics and Archaeology worked in conjunction with its various subject associations and other relevant national organisations to influence the development of policy and practice within History, Classics and Archaeology in UK higher education. It held several well-attended 3-day Teaching & Learning in History conferences annually at Lady Margaret Hall, University of Oxford.\n\n"}
{"id": "2612879", "url": "https://en.wikipedia.org/wiki?curid=2612879", "title": "History wars", "text": "History wars\n\nThe history wars in Australia are an ongoing public debate over the interpretation of the history of the British colonisation of Australia and development of contemporary Australian society (particularly with regard to the impact on Aboriginal Australians and Torres Strait Islanders).\n\nThe Australian debate often concerns the extent to which the history of European colonisation post-1788 and government administration since Federation in 1901 may be characterised as having been:\n\n\nThe history wars also relates to broader themes concerning national identity, as well as methodological questions concerning the historian and the craft of researching and writing history, including issues such as the value and reliability of written records (of the authorities and settlers) and the oral tradition (of the Indigenous Australians), along with the political or similar ideological biases of those who interpret them. One theme is how British or multicultural Australian identity has been in history and today.\n\nAt the same time the history wars were in play, professional history seemed in decline, and popular writers began reclaiming the field.\n\nIn 1968 Professor W. E. H. \"Bill\" Stanner, an Australian anthropologist, coined the term the \"Great Australian Silence\" in a Boyer Lecture entitled \"After the Dreaming\", where he argued that the writing of Australian history was incomplete. He asserted that Australian national history as documented up to that point had largely been presented in a positive light, but that Indigenous Australians had been virtually ignored. He saw this as a structural and deliberate process to omit \"several hundred thousand Aboriginal people who lived and died between 1788 and 1938 ... (who were but) ... negative facts of history and ... were in no way consequential for the modern period\". A new strand of Australian historiography subsequently emerged which gave much greater attention to the negative experiences of Indigenous Australians during the British settlement of Australia. In the 1970s and 1980s, historians such as Manning Clark and Henry Reynolds published work which they saw as correcting a selective historiography that had misrepresented or ignored Indigenous Australian history. The historian Geoffrey Blainey argued in the literary and political journal \"Quadrant\" in 1993 that the telling of Australian history had moved from an unduly positive rendition (the \"Three Cheers View\") to an unduly negative view (The \"'black armband'\") and Australian commentators and politicians have continued to debate this subject.\n\nInterpretations of Aboriginal history became part of the wider political debate sometimes called the \"'culture wars'\" during the tenure of the Coalition government from 1996–2007, with the Prime Minister of Australia John Howard publicly championing the views of some of those associated with \"Quadrant\".<ref name=\"Manne11/08\"> Robert Manne, \"What is Rudd’s Agenda?\", \"The Monthly\", November 2008.</ref> This debate extended into a controversy over the way history was presented in the National Museum of Australia and in high school history curricula. It also migrated into the general Australian media, with regular opinion pieces being published in major broadsheets such as \"The Australian\", \"The Sydney Morning Herald\" and \"The Age\". Marcia Langton has referred to much of this wider debate as 'war porn' and an 'intellectual dead end'.\n\nTwo Australian Prime Ministers, Paul Keating and John Howard, were major participants in the \"wars\". According to the analysis for the Australian Parliamentary Library of Dr Mark McKenna, Paul Keating (1991–1996) was believed by John Howard (1996–2007) to portray Australia pre-Whitlam in an unduly negative light; while Keating sought to distance the modern Labor movement from its historical support for the Monarchy and the White Australia policy by arguing that it was the Conservative Australian Parties who had been barriers to national progress and excessively loyal to the British Empire. He accused Britain of having abandoned Australia during World War II. Keating was a staunch advocate of a symbolic apology to indigenous people for the misdeeds of past governments, and outlined his view of the origins and potential solutions to contemporary Aboriginal disadvantage in his Redfern Park Speech (drafted with the assistance of historian Don Watson). In 1999, following the release of the 1998 Bringing Them Home Report, Howard passed a Parliamentary Motion of Reconciliation describing treatment of Aboriginal people as the \"most blemished chapter\" in Australian history, but he did not make a Parliamentary apology. Howard argued that an apology was inappropriate as it would imply \"intergeneration guilt\" and said that \"practical\" measures were a better response to contemporary Aboriginal disadvantage. Keating has argued for the eradication of remaining symbols linked to British origins: including deference for ANZAC Day, the Australian Flag and the Monarchy in Australia, while Howard was a supporter of these institutions. Unlike fellow Labor leaders and contemporaries, Bob Hawke and Kim Beazley, Keating never traveled to Gallipoli for ANZAC Day ceremonies. In 2008 he described those who gathered there as \"misguided\".\n\nIn 2006, John Howard said in a speech to mark the 50th anniversary of \"Quadrant\" that \"Political Correctness\" was dead in Australia but: \"we should not underestimate the degree to which the soft-left still holds sway, even dominance, especially in Australia's universities\"; and in 2006, \"Sydney Morning Herald\" Political Editor Peter Hartcher reported that Opposition foreign affairs spokesman Kevin Rudd was entering the philosophical debate by arguing in response that \"John Howard, is guilty of perpetrating 'a fraud' in his so-called culture wars ... designed not to make real change but to mask the damage inflicted by the Government's economic policies\".\n\nThe defeat of the Howard government in the Australian Federal election of 2007, and its replacement by the Rudd Labor government altered the dynamic of the debate. Rudd made an official apology to the \"Stolen Generation\" with bi-partisan support. Like Keating, Rudd supported an Australian Republic, but in contrast to Keating, Rudd declared support for the Australian flag and supported the commemoration of ANZAC Day and expressed admiration for Liberal Party founder Robert Menzies.\n\nFollowing the change of government and the passage, with support from all parties, of a Parliamentary apology to indigenous Australians, Professor of Australian Studies Richard Nile argued: \"the culture and history wars are over and with them should also go the adversarial nature of intellectual debate\", a view contested by others, including conservative commentator Janet Albrechtsen. However, an intention to reengage in the history wars was indicated by then-Federal Opposition member Christopher Pyne.\n\nThe black armband debate concerns whether or not accounts of Australian history gravitate towards an overly negative or an overly positive point of view. The \"black armband view of history\" was a phrase first used by Australian historian Geoffrey Blainey in his 1993 \"Sir John Latham Memorial Lecture\" to describe views of history which, he believed, posited that \"much of [pre-multicultural] Australian history had been a disgrace\" and which focused mainly on the treatment of minority groups (especially Aboriginal people). This he contrasted with the \" 'Three Cheers' \" view, according to which: \"nearly everything that came after [the convict era] was believed to be pretty good\". Blainey argued that both such accounts of Australian history were inaccurate: \"The Black Armband view of history might well represent the swing of the pendulum from a position that had been too favourable, too self congratulatory, to an opposite extreme that is even more unreal and decidedly jaundiced.\"\n\nThe lecture was subsequently published in the political and literary journal, \"Quadrant\", which at the time was edited by Robert Manne and is now edited by Keith Windschuttle, two of the leading \"history warriors\", albeit on opposing sides of the debate. The phrase then began to be used by some commentators pejoratively to describe historians viewed as writing excessively critical Australian history \"while wearing a black armband\" of \"mourning and grieving, or shame\". New interpretations of Australia's history since 1788 were contested for focussing almost exclusively on official and unofficial imperialism, exploitation, ill treatment, colonial dispossession and cultural genocide and ignoring positive aspects of Australia's history. Manning Clark was named by Blainey in his 1993 speech as having \"done much to spread the gloomy view and also the compassionate view with his powerful prose and Old Testament phrases\". \n\nThe Howard Government's responses to the question of how to recount Australian history were initially formulated in the context of Paul Keating's characterisation of the subject. John Howard argued in a 1996 Sir Robert Menzies Lecture that the \"balance sheet of Australian history\" had come to be misrepresented:\nIn 2009, Howard's successor Kevin Rudd also called for moving away from a \"black-arm view\":\n\nStephen Muecke, currently Professor of Writing at the University of New South Wales, contributed to the debate by arguing that black armband events bring people together in common remembrance and cited Anzac Day as an example; while Aboriginal lawyer Noel Pearson argued that whilst there was much that is worth preserving in the cultural heritage of non-Aboriginal Australia, \"To say that ordinary Australians who are part of the national community today do not have any connection with the shameful aspects of our past is at odds with our exhortations that they have connections to the prideful bits\".\n\nThe notion of the \"white blindfold\" view of history entered the debate as a pejorative counter-response to the notion of the \"black armband school\".\n\nIn his book \"Why Weren't We Told?\" in 1999, Henry Reynolds referred to Stanner's \"Great Australian Silence\", and to \"a 'mental block' which prevented Australians from coming to terms with the past\". He argued that the silence about Australia's history of frontier violence in much of the twentieth century stands in stark contrast with the openness with which violence was admitted and discussed in the nineteenth. Reynolds quotes many excerpts from the press, including an article written in the \"Townsville Herald\" in Queensland as late as 1907, by a \"pioneer\" who described his part in a massacre. Reynolds commented that violence against Aboriginals, far from being hushed up or denied, was openly talked about.\n\nThe nature of the debate began to change in 1999 with the publication of a book \"Massacre Myth\" by journalist, Rod Moran, who examined the 1926 Forrest River massacre in Western Australia. Moran concluded that the massacre was a myth inspired by the false claims of a missionary (possibly as a result of mental health issues). The principal historian of the Forrest River massacre, Neville Green, describes the massacre as probable but not able to be proven in court. Keith Windschuttle, an Australian historian, said that reviewing Moran's book inspired his own examination of the wider historical record. Windschuttle argues that much of Australian Aboriginal history, particularly as written since the late 1970s, was based on the use of questionable or unreliable evidence and on deliberate misrepresentation and fabrication of historical evidence. He based his conclusions on his examination of the evidence cited in previous historical accounts and reported incidences of non-existent documents being cited, misquoting and misleadingly selective quoting from documents and of documents being cited as evidence that certain events took place when his examination concluded that they do not support those claims. Windschuttle reported his conclusions in a number of articles published in Quadrant and in 2002, he published a book, \"The Fabrication of Aboriginal History\", Volume 1, Van Diemen’s Land 1803-1847, which focussed on Tasmanian colonial history.\n\nHistorian Geoffrey Blainey argued in a 2003 book review of Fabrication, that the number of instances when the source documents do not support the claims made and the fact that the divergences overwhelmingly tend to purport claims of violent conflict and massacres indicates that this is not a matter of mere error but bias.\n\nThe debate had therefore changed from an argument over whether there was an excessive focus on negative aspects of Australian history to one over to what extent, if at all, Australian Aboriginal history had been based on questionable evidence or had been falsified or fabricated and whether this had exaggerated the extent of violence against Aboriginal people. Particular historians and histories that are challenged include Lyndall Ryan and Henry Reynolds and the histories of massacres, particularly in Tasmania but also elsewhere in Australia. Windschuttle's naming of historians whom he accused of misrepresentation and fabrication of the historical evidence, created considerable controversy and produced a range of responses including condemnation of as well as support for his work.\n\nThe case for using the term \"Australian genocide\" rests on evidence from various sources that people argue proves some form of genocide. People cite the list of massacres of indigenous Australians by white settlers, mainly in the 19th century (cf. \"Blood on the Wattle\" by Bruce Elder or \"Frontier History Revisited\" by Robert Orsted-Jensen); only a few massacres were documented, and the evidence is strong that evidence of massacres was generally covered by secrecy and there are powerful signs that documents had been destroyed. Evidence is solid that Queensland's Native Police produced diaries, collision reports and monthly and quarterly enumerations of 'patrols' and 'collisions' with indigenous people, and that all of this material was stored in the Queensland police department. However, not one single sheet of information of this kind which is today available at the Queensland State archive originate from files delivered by the police department, the material left comes solely from other government offices. Only human interference can produce a total loss of the vast Native Police Force records once stored in the Queensland Police Department.\n\nOthers have pointed to the dramatic reduction in the Tasmanian Aboriginal population in the 19th century and the forced removal of generations of Aboriginal children from their parents during the 20th century as evidence of genocide. The evidence includes documentation of the wish, and sometimes intention, of a significant proportion of late 19th-century and early 20th-century white Australians to see the Aboriginal \"race\" eliminated. Documents include published letters to the editors of high-circulation newspapers. Certainly this was the case in Queensland, in terms of indigenous people the most populated section of Australia and certainly the colony with the most violent frontier. In June 1866 Sir Robert Herbert summing up his experience after little more than five years as the first Premier of this colony wrote:\n\nThe mentioned \"system\", for which Herbert was among the people personal responsible, was the so-called \"Native Police system\" which typically went about \"dispersing\" any sign of indigenous resistance at the frontier by use of deadly early morning attacks on Aboriginal camps. This semi-military force was allowed to go about its business, typically instigating large scale deadly retaliation without prior investigating of alleged crime. They generally took no prisoners at the frontier and there are no signs that they ever enforced any other \"law\" than \"might is right\". It was a force designed more in the manner of the recent times phenomenon known as the \"death-squad\" and the secrecy of its operations was ensured by the remoteness of its operations, added a system that denied the evidence from \"blacks\" while the force itself was instructed to ensure that there would always be only one white witness, the officer in charge of each detachment. Recently the first ever attempt to scientifically calculate the amount of Aboriginal people killed in encounters with the Native Police indicates that numbers may exceed 45,000.\n\nThe phrase \"useless race\" was commonly expressed in Queensland such as in 1877 when an editorial in the leading journal noted that,\n\nClassifying Aboriginal people as a useless or unimprovable race was common. Comprehensively debating the native police and the frontier in public in 1880 in the columns of the \"Queenslander\" (the weekly edition of the colony's leading journal), one could read the following statements from yet another prominent settler,\n\nRemarks which were followed up in October of that years by Boyd Dunlop Morehead, one of the leading landholders, manager of the Scottish Australian Investment Co.'s \"Bowen Downs\" in 1866-81 and a future Premier, could be heard making the following acknowledgement in a parliamentary speech, saying, yes settlers in the past did go\n\nAfter the introduction of the word \"genocide\" in the 1940s by Raphael Lemkin, Lemkin himself and most comparative scholars of genocide and many general historians, such as Robert Hughes, Ward Churchill, Leo Kuper and Jared Diamond, basing their analysis on previously published histories, present the extinction of the Tasmanian Aboriginal people as a text book example of a genocide. The Australian historian of genocide, Ben Kiernan, in his recent history of the concept and practice, \"Blood and soil: a world history of genocide and extermination from Sparta to Darfur\" (2007), treats the Australian evidence over the first century of colonization as an example of genocide.\n\nAmong scholars specializing in Australian history much recent debate has focused on whether indeed what happened to groups of Aboriginal people, and especially the Tasmanian Aboriginal people, during the European colonisation of Australia can be classified as genocide. According to Mark Levene, most Australian experts are now \"considerably more circumspect\". In the specific instance of the Tasmanian Aboriginal people, Henry Reynolds, who takes events in other regions of colonial Australia as marked by \"genocidal moments\", argues that the records show that British administrative policy in Tasmania was explicitly concerned to avoid extermination. However, in practice the events on the ground that lead to their virtual extinction worked out. Tony Barta, John Docker and Anne Curthoys however emphasize Lemkin's linkage between colonization and genocide. Barta, an Australian expert in German history, argued from Lemkin that, \"there is no dispute that the basic fact of Australian history is the appropriation of the continent by an invading people and the dispossession, with ruthless destructiveness, of another\". Docker argues that, \"(w)e ignore Lemkin's wide-ranging definition of genocide, inherently linked with colonialism, at our peril\". Curthoys argues that the separation between international and local Australian approaches has been deleterious. While calling for \"a more robust exchange between genocide and Tasmanian historical scholarship\", her own view is that the Tasmanian instance constitutes a \"case for genocide, though not of state planning, mass killing, or extinction\".\n\nMuch of the debate on whether European colonisation of Australia resulted in genocide, centres on whether \"the term 'genocide' only applies to cases of deliberate mass killings of Aboriginal people by European settlers, or ... might also apply to instances in which many Aboriginal people were killed by the reckless or unintended actions and omissions of settlers\". Historians such as Tony Barta argue that for the victim group it matters little if they were wiped out as part of a planned attack. If a group is decimated as a result of smallpox introduced to Australia by British settlers, or introduced European farming methods causing a group of Aboriginal people to starve to death, the result is, in his opinion, genocide.\n\nHenry Reynolds points out that European colonists and their descendants frequently use expressions that included \"extermination\", \"extinction\", and \"extirpation\" when discussing the treatment of Aboriginal people during the colonial period, and as in his opinion genocide \"can take many forms, not all of them violent\". Janine Roberts has argued that genocide was Australian policy, even if only by omission. She notes that despite contemporary newspapers regularly decrying \"the barbarous crop of exterminators\", and \"a system of native slaughter ... merciless and complete\", the government contended that \"no illegal acts were occurring\", with the worst incidents being described as merely \"indiscretions\".\n\nThe political scientist Kenneth Minogue and other historians such as Keith Windschuttle disagree and think that no genocide took place. Minogue does not try to define genocide but argues that its use is an extreme manifestation of the guilt felt by modern Australian society about the past misconduct of their society to Aboriginal people. In his opinion its use reflects the process by which Australian society is trying to come to terms with its past wrongs and in doing this Australians are stretching the meaning of genocide to fit within this internal debate.\n\nIn the April 2008 edition of \"The Monthly\", David Day wrote further on the topic of genocide. He wrote that Lemkin considered genocide to encompass more than mass killings but also acts like \"driv[ing] the original inhabitants off the land ... confin[ing] them in reserves, where policies of deliberate neglect may be used to reduce their numbers ... Tak[ing] indigenous children to absorb them within their own midst ... assimilation to detach the people from their culture, language and religion, and often their names.\"\n\nThe arrival of smallpox in Australia is of uncertain origin and is a major theme in the history wars. The lack of immunity among Aboriginal Australians to this introduced disease saw it inflict a devastating toll on the Aboriginal population. Though the First Fleet itself did not arrive with any known carriers of the disease, the observation of an epidemic among the Aboriginal population of Sydney around 16 months after the British arrived has led to speculation that the Fleet itself brought this disease to Australia. Some historians have suggested that the disease may have been either released by accident or theft of medicine stores or perhaps been deliberately employed as a form of \"germ warfare\" against indigenous Australians. Inoculation was thus commonly practised by surgeons decades before 1796 and the process of smallpox vaccination was introduced by Edward Jenner. Dried scab was commonly stored in glass containers as part of a surgeons remedies.\n\nEarly speculation on the origins of the disease is recorded in the writing of a First Fleet Captain of Marines, Watkin Tench, who noted an \"extraordinary calamity\" among the Aboriginal people of Sydney, beginning in April 1789. Repeated accounts of dead bodies marked with pustules consistent with smallpox began being reported around Sydney Harbour around this time. Tench wrote that the colonists' observations had led them to suppose that smallpox was not known in New South Wales and as no First Fleeters had suffered from the disease, its sudden existence among the Aboriginal people was \"inexplicable\". Tench speculated as to whether the disease might be indigenous to the country; or whether it had been brought to the colony by the French expedition of Lapérouse a year before; traversed the continent from the West where Europeans had previously landed; brought by expedition of James Cook; or indeed by the first British settlers at Sydney. \"Our surgeons brought out varioulous matter in bottles\", he wrote, \"but to infer that it was produced from this cause were a supposition so wild as to be unworthy of consideration\".\n\nSubsequently, and despite the lack of certainty over how or when the disease reached Australia, there has been a history war regarding the way that smallpox arrived in Australia, especially whether it was deliberately used as a crude biological weapon against indigenous peoples.\n\nMedical scientists such as Sir Edward Stirling and Sir John Cleland published a number of books and articles between 1911 and 1966 suggesting that smallpox arrived in Northern Australia from an Asian source.\n\nA rival theory, that smallpox was introduced to NSW in 1789 by British settlers, was put forward in 1914 by the director of the Australian Quarantine Service, Dr J. H. L. Cumpston.\n\nIn 1983, Professor Noel Butlin, an economic historian, suggested: \"it is possible and, in 1789, likely, that infection of the Aboriginal people was a deliberate extermination act\". Historians David Day and Henry Reynolds repeated Butlin’s claims and in 2001 Reynolds wrote: \"one possibility is that the epidemic was deliberately or accidentally let loose by someone in the settlement at Sydney Cove. Not surprisingly this is a highly contentious proposition. If true, it would clearly fall within the ambit of the Genocide Convention\". Butlin argued that while Macassan fishermen could possibly 'have landed the virus on the Australian mainland at some stage their ability to do so was limited'. It is furthermore highly unlikely, he argued, that this virus should have been brought down from the Gulf of Carpentaria to coincidence with the first major outbreak \"just fifteen months after the landing of the first fleet\". Besides the time factor connected to Macassans, 'over seven or eight weeks (or more)', the type of vessels, the limited potential for contact between Aboriginal people and fishermen, and the fact of clothing as carrier and virus is destroyed or seriously reduced in contact with salt water, makes the Macassan theory highly unlikely, he argued. Indeed, infected 'Macassans would be either dead or fully recovered long before reaching the Gulf of Carpentaria. Whereas transfer somehow, theft accident or the like, from scab originally stored in glass containers carried by just one of the seven medical officers on the first fleet seems the most likely cause.\n\nC. C. Macknight (1986) an authority on the centuries-old interaction between indigenous Australians and the people of Makassar (later part of Indonesia), revived the theory that smallpox was introduced to Australia by Macassan mariners visiting Arnhem Land.\n\nAustralian virologist Frank Fenner (1988) – who in 1977–80 led the successful World Health Organization (WHO) campaign to eradicate smallpox and was the principal author of a 1988 WHO report, \"Smallpox and its Eradication\" – pointed out that no cases of smallpox were reported amongst convicts, sailors, military personnel, or free settlers, on the First Fleet. The virus was also not reported among British or Aboriginal people at Port Jackson over the following 15 months. It was, therefore, unlikely that a person suffering from smallpox and travelling with the First Fleet had caused the 1789 outbreak.\n\nWhile there were cases of smallpox in Macassar during 1789, there are no reports of it occurring prior to that period. However, smallpox had long been present in island South East Asia – possibly as early the 4th century according to Frank Fenner. There were outbreaks of smallpox in Indonesia throughout the 18th century. These included, for example, major epidemics in the Sultanate of Tidore (in the Moluccas) during the 1720s, the Sultanate of Banjar (South Kalimantan), in 1734, 1750–51, 1764–65 and 1778–79; and in southern Sumatra during the 1750s, the 1770s, and in 1786. Macassans had contact with these areas both directly and indirectly (through foreign traders and invaders).\n\nDavid Day (2001) reiterated Butlin's argument and suggested that members of Sydney's garrison of Royal Marines may have attempted to use smallpox as a biological weapon in 1789. The following year, however, John Connor stated that Day's theory was \"unsustainable\".\n\nIn a 2002 book, \"Invisible Invaders\", historian Judy Campbell – advised by Fenner – reviewed reports of disease amongst Aboriginal people from 1780–1880, including the smallpox epidemics of 1789-90, the 1830s and the 1860s. Campbell argues that the evidence, including that contained in these reports shows that, while many diseases such as tuberculosis \"were\" introduced by British colonists, this was not so for smallpox and that the speculations of British responsibility made by other historians were based on tenuous evidence, largely on the mere coincidence that the 1789-90 epidemic was first observed afflicting the Aboriginal people not long after the establishment of the first British settlement. Campbell argues instead that the north-south route of transmission of the 1860s epidemics (which is generally agreed), also applied in the earlier ones. Campbell noted that the fleets of fast Macassan fishing vessels, propelled by monsoonal winds, reached Australia after being at sea for as little as ten to fifteen days, well within the incubation period of smallpox. The numbers of people travelling in the fleets were large enough to sustain smallpox for extended periods of time without it ‘burning out’. The Macassans spent up to six months fishing along the northern Australian coastline and Aboriginal people had \"day-to-day contact with the islanders. Aboriginals visited the praus and the camps the visitors set up on shore, they talked and traded…\" She also notes that Butlin, writing in 1983, \"did not recognize that Aboriginals were \"great travellers\", who spread infection over long distances….\" and that smallpox was spread through their extensive social and trading contacts as well as by Aboriginal people fleeing from the disease. Campbell also cited British historian Charles Wilson, who cited \"medical microbiology\" in disagreeing with Butlin about the origins of the 1789 outbreak, and \"doubted his estimates of its demographic impact\", as well as \"First Fleet historian Alan Frost [who] also disagreed with Butlin’s views\".\n\nChristopher Warren (2007) claimed that Fenner did not address the issue of variolous material brought in bottles by the First Fleet, for use as an inoculant. Warren argued that, even if the variolous material was degraded, it could still infect susceptible people. Smallpox spread by the inhalation of airborne droplets of virus in situations of personal contact or by contact with blankets, clothing or other objects that an infected person had recently used. This material was carried by First Fleet surgeons for inoculation purposes. Warren also suggested that Frost's view was based on a false premise: that the First Fleet's stocks of virus were sterilised by summer heat.\n\nCraig Mear (2008) and Michael J. Bennett (2009) have disputed Campbell's hypothesis that smallpox was introduced to Australia in 1789 through contact between Aboriginal people and mariners from Makassar.\n\nH. A. Willis (2010), in a survey of much of the literature discussed above, reiterated the argument made by Campbell. In response, Warren (2011) suggested that Willis had not taken into account research on how heat affects the smallpox virus, cited by the World Health Organization. In reply, Willis (2011) reiterated that his position was supported by a closer reading of Frank Fenner’s report to the World Health Organization (1988) and invited readers to consult that report online.\n\nMacknight re-entered the debate in 2011, declaring: \"The overwhelming probability must be that it [smallpox] was introduced, like the later epidemics, by [Macassan] trepangers on the north coast and spread across the continent to arrive in Sydney quite independently of the new settlement there.\"\n\nJohn Carmody, a professor of medicine, put forward an alternative theory (2013) suggesting that the 1789 epidemic may have been chickenpox rather than smallpox. Carmody pointed out that chickenpox could have taken a severe toll on a population with little hereditary or acquired immunological resistance. With regard to smallpox, Carmody said: \"There is absolutely no evidence to support any of the theories and some of them are fanciful and far-fetched.\" In response, Christopher Warren rejected suggestions that chickenpox has caused the 1789 epidemic.\n\nWarren (2014) subsequently rejected the theory that the 1789 epidemic had originated from Macassar. He claimed that there was no evidence of a major outbreak of smallpox in Macassar before 1789; there were no indigenous trade routes that would have enabled overland transmission from Arnhem Land to Port Jackson; the Makassan theory was contradicted by Aboriginal oral tradition, and 1829 was the earliest point at which there was possible evidence that Makassans had been the source of a smallpox outbreak.\n\nDr Seth Carus (2015) states: \"Ultimately, we have a strong circumstantial case supporting the theory that someone deliberately introduced smallpox in the Aboriginal population.\"\n\nDespite the lengthy and detailed findings set out in the 1997 \"Bringing Them Home\" report into the Stolen Generation, which documented the removal of Aboriginal children from their families by Australian State and Federal government agencies and church missions, the nature and extent of the removals have been disputed within Australia, with some commentators questioning the findings contained in the report and asserting that the Stolen Generation has been exaggerated. Sir Ronald Wilson, former President of the Human Rights and Equal Opportunities Commission and a Commissioner on the Inquiry, has stated that none of the more than 500 witnesses who appeared before the Inquiry were cross-examined. This has been the basis of criticism by the Coalition Government and by the anthropologist Ron Brunton in a booklet published by the Institute of Public Affairs that was criticised in turn by the lawyer Hal Wootten. An Australian Federal Government submission has questioned the conduct of the Commission which produced the report, arguing that the Commission failed to critically appraise or test the claims on which it based the report and failed to distinguish between those separated from their families \"with and without consent, and with and without good reason\". Not only has the number of children removed from their parents been questioned, but also the intent and effects of the government policy.\n\nSome critics, such as Andrew Bolt, have questioned the very existence of the Stolen Generation. Bolt stated that it is a \"preposterous and obscene\" myth and that there was actually no policy in any state or territory at any time for the systematic removal of \"half-caste\" Aboriginal children. Robert Manne responded that Bolt did not address the documentary evidence demonstrating the existence of the Stolen Generations and that this is a clear case of historical denialism. Bolt then challenged Manne to produce ten cases in which the evidence justified the claim that children were \"stolen\" as opposed to having been removed for reasons such as neglect, abuse, abandonment, etc. He argued that Manne did not respond and that this was an indication of unreliability of the claim that there was policy of systematic removal. In reply, Manne stated that he supplied a documented list of 250 names Bolt stated that prior to a debate, Manne provided him with a list of 12 names that he was able to show during the debate was \"a list of people abandoned, saved from abuse or voluntarily given up by their parents\"; and that during the actual debate, Manne produced a list of 250 names without any details or documentation as to their circumstances. Bolt also stated that he was subsequently able to identify and ascertain the history of some of those on the list and was unable to find a case where there was evidence to justify the term ‘stolen’. He stated that one of the names on the list of allegedly stolen children was 13-year-old Dolly, taken into the care of the State after being \"found seven months pregnant and penniless, working for nothing on a station\".\n\nThe Bolt/Manne debate is a fair sample of the adversarial debating style in the area. There is focus on individual examples as evidence for or against the existence of a policy, and little or no analysis of other documentary evidence such as legislative databases showing how the legal basis for removal varied over time and between jurisdictions, or testimony from those who were called on to implement the policies, which was also recorded in the \"Bringing Them Home\" report. A recent review of legal cases claims it is difficult for Stolen Generation claimants to challenge what was written about their situation at the time of removal.\n\nThe report also identified instances of official misrepresentation and deception, such as when caring and able parents were incorrectly described by Aboriginal Protection Officers as not being able to properly provide for their children, or when parents were told by government officials that their children had died, even though this was not the case.\n\nThe new Australian Government elected in 2007 issued an Apology similar to those that State Governments had issued at or about the time of the \"Bringing Them Home\" report ten years earlier. On 13 February 2008, Kevin Rudd, Prime Minister of Australia moved a formal apology in the House of Representatives, which was moved concurrently by the Leader of the Government in the Senate. It passed unanimously in the House of Representatives on 13 March 2008. In the Senate, the leader of the Australian Greens moved an amendment seeking to add compensation to the apology, which was defeated in a vote of 65 to 4, after which the motion was passed unanimously.\n\nIn 2002, historian Keith Windschuttle, in his book \"The Fabrication of Aboriginal History, Volume One: Van Diemen's Land 1803-1847\", questions the historical evidence used to identify the number of Aboriginal people deliberately killed during European colonisation, especially focusing on the Black War in Tasmania. He argues that there is credible evidence for the violent deaths of only 118 Tasmanian Aboriginal people, as having been directly killed by the British, although there were undoubtedly an unquantifiable number of other deaths for which no evidence exists. He argues that the Tasmanian Aboriginal population was devastated by a lethal cocktail of introduced diseases to which they had little or no resistance due to their isolation from the mainland and the rest of humanity for thousands of years. The deaths and infertility caused by these introduced diseases, combined with the deaths from what violent conflict there was, rapidly decimated the relatively small Aboriginal population. Windschuttle also examined the nature of those violent episodes that did occur and concluded that there is no credible evidence of warfare over territory. Windschuttle argues that the primary source of conflict between the British and the Aboriginal people was raids by Aboriginal people, often involving violent attacks on settlers, to acquire goods (such as blankets, metal implements and 'exotic' foods) from the British. With this and with a detailed examination of footnotes in and evidence cited by the earlier historical works, he criticises the claims by historians such as Henry Reynolds and Professor Lyndall Ryan that there was a campaign of guerrilla warfare against British settlement. Particular historians and histories that are challenged include Henry Reynolds and the histories of , particularly in Tasmania (such as in the Cape Grim massacre) but also elsewhere in Australia. Windschuttle's claims are based upon the argument that the 'orthodox' view of Australian history were founded on hearsay or the misleading use of evidence by historians.\n\nWindschuttle argues that, in order to advance the ‘deliberate genocide’ argument, Reynolds has misused source documentation, including that from British colonist sources, by quoting out of context. In particular, he accuses Reynolds of selectively quoting from responses to an 1830 survey in Tasmania in that Reynolds quoted only from those responses that could be construed as advocating \"extermination\", \"extinction\", and \"extirpation\" and failed to mention other responses to the survey, which indicated that a majority of respondents rejected genocide, were sympathetic to the plight of the Aboriginal people, feared that conflict arising from Aboriginal attacks upon settlers would result in the extinction of the Tasmanian Aboriginal people and advocated the adoption of courses of action to prevent this happening.\n\nWindschuttle's claims and research have been disputed by some historians, in \"Whitewash. On Keith Windschuttle's Fabrication of Aboriginal History\", an anthology including contributions from Henry Reynolds and Professor Lyndall Ryan, edited and introduced by Robert Manne, professor of politics at La Trobe University. This anthology has itself been the subject of examination by Melbourne businessman, freelance writer and Objectivist John Dawson, in \"Washout: On the academic response to The Fabrication of Aboriginal History\", which argues that \"Whitewash\" leaves Windschuttle's claims and research unrefuted.\n\nIn \"Contra Windschuttle\", an article published in the conservative publication \"Quadrant\", S.G. Foster examined some of the evidence that Windschuttle presented on one issue, Stanner's notion of the \"Great Australian Silence\". In Foster’s opinion, the evidence produced by Windschuttle did not prove his case that the \"Great Australian Silence\" was largely a myth. Windschuttle argues that, in the years prior to Stanner’s 1968 Boyer lecture, Australian historians had not been silent on the Aboriginal people although, in most cases, the historians’ \"discussions were not to Stanner’s taste\" and the Aboriginal people \"might not have been treated in the way Reynolds and his colleagues would have liked\". Foster argues that Windschuttle is \"merciless with those who get their facts wrong\" and that the fact that Windschuttle has also made a mistake means that he did not meet the criteria that he used to assess 'orthodox historians' he was arguing against and whom he accused of deliberately and extensively misrepresenting, misquoting, exaggerating and fabricating evidence relating to the level and nature of violent conflict between Aboriginal people and white settlers.\n\nAt the time of the publication of \"The Fabrication of Aboriginal History, Volume One\" it was announced that a second volume, to be published in 2003, would cover claims of frontier violence in New South Wales and Queensland, and a third, in 2004, would cover Western Australia.\nOn 9 February 2008, however, it was announced that the second volume, anticipated to be published later in 2008, would be entitled \"The Fabrication of Australian History, Volume 2: The \"Stolen Generations\"\" and would address the issue of the removal of Aboriginal children (the \"stolen generations\") from their families in the 20th century.\n\nThe new volume was released in January 2010, now listed as \"Volume 3\", with a statement that Volumes 2 and 4 would appear later. Announcing the publication, Windschuttle claimed that the film \"Rabbit-Proof Fence\" had misrepresented the child removal at the centre of the story, and offered inaccurate accounts of Molly's journey as it was recounted by her daughter, Doris Pilkington. These claims were subsequently rejected by the makers of the film.\n\nIn 2003 Australian historian Stuart Macintyre published \"The History Wars\", written with Anna Clark. This was a study of the background of, and arguments surrounding, recent developments in Australian historiography, and concluded that the History Wars had done damage to the nature of objective Australian history. At the launch of his book, historian Stuart Macintyre emphasised the political dimension of these arguments and said the Australian debate took its cue from the Enola Gay controversy in the United States. The book was launched by former Prime Minister Paul Keating, who took the opportunity to criticise conservative views of Australian history, and those who hold them (such as the then Prime Minister John Howard), saying that they suffered from \"a failure of imagination\", and said that \"The History Wars\" \"rolls out the canvas of this debate.\" Macintyre's critics, such as Greg Melluish (History Lecturer at the University of Wollongong), responded to the book by declaring that Macintyre was a partisan history warrior himself, and that \"its primary arguments are derived from the pro-Communist polemics of the Cold War.\" Keith Windschuttle said that Macintyre attempted to \"caricature the history debate.\" In a foreword to the book, former Chief Justice of Australia Sir Anthony Mason said that the book was \"a fascinating study of the recent endeavours to rewrite or reinterpret the history of European settlement in Australia.\"\n\nIn 2001, writing in \"Quadrant\", a conservative magazine, historian Keith Windschuttle argued that the then-new National Museum of Australia (NMA) was marred by \"political correctness\" and did not present a balanced view of the nation's history. In 2003 the Howard Government commissioned a review of the NMA. A potentially controversial issue was in assessing how well the NMA met the criterion that displays should: \"Cover darker historical episodes, and with a gravity that opens the possibility of collective self-accounting. The role here is in helping the nation to examine fully its own past, and the dynamic of its history—with truthfulness, sobriety and balance. This extends into covering present-day controversial issues.\" While the report concluded that there was no systemic bias, it recommended that there be more recognition in the exhibits of European achievements.\nThe report drew the ire of some historians in Australia, who claimed that it was a deliberate attempt on the part of the Government to politicise the museum and move it more towards a position which Geoffrey Blainey called the 'three cheers' view of Australian history, rather than the 'black armband' view. In 2006 columnist Miranda Devine described some of the Braille messages encoded on the external structure of the NMA, including \"sorry\" and \"forgive us our genocide\" and how they had been covered over by aluminium discs in 2001, and stated that under the new Director \"what he calls the 'black T-shirt' view of Australian culture\" is being replaced by \"systematically reworking the collections, with attention to 'scrupulous historical accuracy'\".\n\nAn example of the current approach at the NMA is the Bells Falls Gorge Interactive display, which presents Windschuttles's view of an alleged massacre alongside other views and contemporary documents and displays of weapons relating to colonial conflict around Bathurst in 1824 and invites visitors to make up their own minds.\n\nPublication in 2016 of \"Indigenous Terminology\" guidelines for the teaching and writing of history by the University of New South Wales created a brief media uproar. Amongst the advised language changes, they recommended \"settlement\" be replaced by \"invasion\", \"colonisation\" or \"occupation\". They also deemed that the generally accepted anthropological assumption that \"Aboriginal people have lived in Australia for 40,000 years\" should be dropped for \"... since the beginning of the Dreaming/s\" as it \"reflects the beliefs of many Indigenous Australians that they have always been in Australia, from the beginning of time\" and because \"many Indigenous Australians see this sort of measurement and quantifying as inappropriate.\" While some commentators considered the guidelines appropriate, others categorised them as political correctness that was an anathema to learning and scholarship.\nThe \"history wars\" are widely viewed, by external observers and participants on both sides as similar to the \"culture war\" underway in the United States. William D. Rubinstein, writing for the conservative British think tank the Social Affairs Unit, refers to the history wars as \"the Culture War down under\". Participants in the debate including Keith Windschuttle and Robert Manne are frequently described as \"culture warriors\" for their respective points of view.\n\n\n\n\n\n\n\n"}
{"id": "14914", "url": "https://en.wikipedia.org/wiki?curid=14914", "title": "Industrial Revolution", "text": "Industrial Revolution\n\nThe Industrial Revolution was the transition to new manufacturing processes in the period from about 1760 to sometime between 1820 and 1840. This transition included going from hand production methods to machines, new chemical manufacturing and iron production processes, the increasing use of steam power, the development of machine tools and the rise of the factory system.\n\nTextiles were the dominant industry of the Industrial Revolution in terms of employment, value of output and capital invested. The textile industry was also the first to use modern production methods.\n\nThe Industrial Revolution began in Great Britain, and many of the technological innovations were of British origin. By the mid-18th century Britain was the world's leading commercial nation, controlling a global trading empire with colonies in North America and the Caribbean, and with some political influence on the Indian subcontinent, through the activities of the East India Company. The development of trade and the rise of business were major causes of the Industrial Revolution.\n\nThe Industrial Revolution marks a major turning point in history; almost every aspect of daily life was influenced in some way. In particular, average income and population began to exhibit unprecedented sustained growth. Some economists say that the major impact of the Industrial Revolution was that the standard of living for the general population began to increase consistently for the first time in history, although others have said that it did not begin to meaningfully improve until the late 19th and 20th centuries.\n\nGDP per capita was broadly stable before the Industrial Revolution and the emergence of the modern capitalist economy, while the Industrial Revolution began an era of per-capita economic growth in capitalist economies. Economic historians are in agreement that the onset of the Industrial Revolution is the most important event in the history of humanity since the domestication of animals and plants.\n\nAlthough the structural change from agriculture to industry is widely associated with Industrial Revolution, in United Kingdom it was already almost complete by 1760.\n\nThe precise start and end of the Industrial Revolution is still debated among historians, as is the pace of economic and social changes. Eric Hobsbawm held that the Industrial Revolution began in Britain in the 1780s and was not fully felt until the 1830s or 1840s, while T. S. Ashton held that it occurred roughly between 1760 and 1830. Rapid industrialization first began in Britain, starting with mechanized spinning in the 1780s, with high rates of growth in steam power and iron production occurring after 1800. Mechanized textile production spread from Great Britain to continental Europe and the United States in the early 19th century, with important centres of textiles, iron and coal emerging in Belgium and the United States and later textiles in France.\n\nAn economic recession occurred from the late 1830s to the early 1840s when the adoption of the original innovations of the Industrial Revolution, such as mechanized spinning and weaving, slowed and their markets matured. Innovations developed late in the period, such as the increasing adoption of locomotives, steamboats and steamships, hot blast iron smelting and new technologies, such as the electrical telegraph, widely introduced in the 1840s and 1850s, were not powerful enough to drive high rates of growth. Rapid economic growth began to occur after 1870, springing from a new group of innovations in what has been called the Second Industrial Revolution. These new innovations included new steel making processes, the large-scale manufacture of machine tools and the use of increasingly advanced machinery in steam-powered factories.\n\nThe earliest recorded use of the term \"Industrial Revolution\" seems to have been in a letter from 6 July 1799 written by French envoy Louis-Guillaume Otto, announcing that France had entered the race to industrialise. In his 1976 book \"\", Raymond Williams states in the entry for \"Industry\": \"The idea of a new social order based on major industrial change was clear in Southey and Owen, between 1811 and 1818, and was implicit as early as Blake in the early 1790s and Wordsworth at the turn of the [19th] century.\" The term \"Industrial Revolution\" applied to technological change was becoming more common by the late 1830s, as in Jérôme-Adolphe Blanqui's description in 1837 of \"la révolution industrielle\". Friedrich Engels in \"The Condition of the Working Class in England\" in 1844 spoke of \"an industrial revolution, a revolution which at the same time changed the whole of civil society\". However, although Engels wrote in the 1840s, his book was not translated into English until the late 1800s, and his expression did not enter everyday language until then. Credit for popularising the term may be given to Arnold Toynbee, whose 1881 lectures gave a detailed account of the term.\n\nSome historians, such as John Clapham and Nicholas Crafts, have argued that the economic and social changes occurred gradually and the term \"revolution\" is a misnomer. This is still a subject of debate among some historians.\n\nThe commencement of the Industrial Revolution is closely linked to a small number of innovations, beginning in the second half of the 18th century. By the 1830s the following gains had been made in important technologies:\n\nIn 1750 Britain imported 2.5 million pounds of raw cotton, most of which was spun and woven by cottage industry in Lancashire. The work was done by hand in workers' homes or occasionally in shops of master weavers. In 1787 raw cotton consumption was 22 million pounds, most of which was cleaned, carded and spun on machines. The British textile industry used 52 million pounds of cotton in 1800, which increased to 588 million pounds in 1850.\n\nThe share of value added by the cotton textile industry in Britain was 2.6% in 1760, 17% in 1801 and 22.4% in 1831. Value added by the British woollen industry was 14.1% in 1801. Cotton factories in Britain numbered approximately 900 in 1797. In 1760 approximately one-third of cotton cloth manufactured in Britain was exported, rising to two-thirds by 1800. In 1781 cotton spun amounted to 5.1 million pounds, which increased to 56 million pounds by 1800. In 1800 less than 0.1% of world cotton cloth was produced on machinery invented in Britain. In 1788 there were 50,000 spindles in Britain, rising to 7 million over the next 30 years.\n\nWages in Lancashire, a core region for cottage industry and later factory spinning and weaving, were about six times those in India in 1770, when overall productivity in Britain was about three times higher than in India.\n\nParts of India, China, Central America, South America and the Middle-East have a long history of hand manufacturing cotton textiles, which became a major industry sometime after 1000 AD. In tropical and subtropical regions where it was grown, most was grown by small farmers alongside their food crops and was spun and woven in households, largely for domestic consumption. In the 15th century China began to require households to pay part of their taxes in cotton cloth. By the 17th century almost all Chinese wore cotton clothing. Almost everywhere cotton cloth could be used as a medium of exchange. In India a significant amount of cotton textiles were manufactured for distant markets, often produced by professional weavers. Some merchants also owned small weaving workshops. India produced a variety of cotton cloth, some of exceptionally fine quality.\n\nCotton was a difficult raw material for Europe to obtain before it was grown on colonial plantations in the Americas. The early Spanish explorers found Native Americans growing unknown species of excellent quality cotton: sea island cotton (\"Gossypium barbadense\") and upland green seeded cotton \"Gossypium hirsutum\". Sea island cotton grew in tropical areas and on barrier islands of Georgia and South Carolina, but did poorly inland. Sea island cotton began being exported from Barbados in the 1650s. Upland green seeded cotton grew well on inland areas of the southern U.S., but was not economical because of the difficulty of removing seed, a problem solved by the cotton gin. A strain of cotton seed brought from Mexico to Natchez, Mississippi, USA in 1806 became the parent genetic material for over 90% of world cotton production today; it produced bolls that were three to four times faster to pick.\n\nThe Age of Discovery was followed by a period of colonialism beginning around the 16th century. Following the discovery of a trade route to India around southern Africa by the Portuguese, the Dutch established the Verenigde Oostindische Compagnie (abbr. VOC) or Dutch East India Company and the British founded the East India Company, along with smaller companies of different nationalities which established trading posts and employed agents to engage in trade throughout the Indian Ocean region and between the Indian Ocean region and North Atlantic Europe. One of the largest segments of this trade was in cotton textiles, which were purchased in India and sold in Southeast Asia, including the Indonesian archipelago, where spices were purchased for sale to Southeast Asia and Europe. By the mid-1760s cloth was over three-quarters of the East India Company's exports. Indian textiles were in demand in North Atlantic region of Europe where previously only wool and linen were available; however, the amount of cotton goods consumed in Western Europe was minor until the early 19th century.\n\nBy 1600 Flemish refugees began weaving cotton cloth in English towns where cottage spinning and weaving of wool and linen was well established; however, they were left alone by the guilds who did not consider cotton a threat. Earlier European attempts at cotton spinning and weaving were in 12th century Italy and 15th century southern Germany, but these industries eventually ended when the supply of cotton was cut off. The Moors in Spain grew, spun and wove cotton beginning around the 10th century.\n\nBritish cloth could not compete with Indian cloth because India's labor cost was approximately one-fifth to one-sixth that of Britain's. In 1700 and 1721 the British government passed Calico Acts in order to protect the domestic woollen and linen industries from the increasing amounts of cotton fabric imported from India.\n\nThe demand for heavier fabric was met by a domestic industry based around Lancashire that produced fustian, a cloth with flax warp and cotton weft. Flax was used for the warp because wheel-spun cotton did not have sufficient strength, but the resulting blend was not as soft as 100% cotton and was more difficult to sew.\n\nOn the eve of the Industrial Revolution, spinning and weaving were done in households, for domestic consumption and as a cottage industry under the putting-out system. Occasionally the work was done in the workshop of a master weaver. Under the putting-out system, home-based workers produced under contract to merchant sellers, who often supplied the raw materials. In the off season the women, typically farmers' wives, did the spinning and the men did the weaving. Using the spinning wheel, it took anywhere from four to eight spinners to supply one hand loom weaver.\n\nThe flying shuttle, patented in 1733 by John Kay, with a number of subsequent improvements including an important one in 1747, doubled the output of a weaver, worsening the imbalance between spinning and weaving. It became widely used around Lancashire after 1760 when John's son, Robert, invented the drop box, which facilitated changing thread colors.\n\nLewis Paul patented the roller spinning frame and the flyer-and-bobbin system for drawing wool to a more even thickness. The technology was developed with the help of John Wyatt of Birmingham. Paul and Wyatt opened a mill in Birmingham which used their new rolling machine powered by a donkey. In 1743 a factory opened in Northampton with 50 spindles on each of five of Paul and Wyatt's machines. This operated until about 1764. A similar mill was built by Daniel Bourn in Leominster, but this burnt down. Both Lewis Paul and Daniel Bourn patented carding machines in 1748. Based on two sets of rollers that travelled at different speeds, it was later used in the first cotton spinning mill. Lewis's invention was later developed and improved by Richard Arkwright in his water frame and Samuel Crompton in his spinning mule.\n\nIn 1764 in the village of Stanhill, Lancashire, James Hargreaves invented the spinning jenny, which he patented in 1770. It was the first practical spinning frame with multiple spindles. The jenny worked in a similar manner to the spinning wheel, by first clamping down on the fibres, then by drawing them out, followed by twisting. It was a simple, wooden framed machine that only cost about £6 for a 40-spindle model in 1792, and was used mainly by home spinners. The jenny produced a lightly twisted yarn only suitable for weft, not warp.\n\nThe spinning frame or water frame was developed by Richard Arkwright who, along with two partners, patented it in 1769. The design was partly based on a spinning machine built for Thomas High by clockmaker John Kay, who was hired by Arkwright. For each spindle the water frame used a series of four pairs of rollers, each operating at a successively higher rotating speed, to draw out the fibre, which was then twisted by the spindle. The roller spacing was slightly longer than the fibre length. Too close a spacing caused the fibres to break while too distant a spacing caused uneven thread. The top rollers were leather-covered and loading on the rollers was applied by a weight. The weights kept the twist from backing up before the rollers. The bottom rollers were wood and metal, with fluting along the length. The water frame was able to produce a hard, medium count thread suitable for warp, finally allowing 100% cotton cloth to be made in Britain. A horse powered the first factory to use the spinning frame. Arkwright and his partners used water power at a factory in Cromford, Derbyshire in 1771, giving the invention its name.\n\nSamuel Crompton's Spinning Mule was introduced in 1779. Mule implies a hybrid because it was a combination of the spinning jenny and the water frame, in which the spindles were placed on a carriage, which went through an operational sequence during which the rollers stopped while the carriage moved away from the drawing roller to finish drawing out the fibres as the spindles started rotating. Crompton's mule was able to produce finer thread than hand spinning and at a lower cost. Mule spun thread was of suitable strength to be used as warp, and finally allowed Britain to produce highly competitive yarn in large quantities.\n\nRealising that the expiration of the Arkwright patent would greatly increase the supply of spun cotton and lead to a shortage of weavers, Edmund Cartwright developed a vertical power loom which he patented in 1785. In 1776 he patented a two-man operated loom which was more conventional. Cartwright built two factories; the first burned down and the second was sabotaged by his workers. Cartwright's loom design had several flaws, the most serious being thread breakage. Samuel Horrocks patented a fairly successful loom in 1813. Horock's loom was improved by Richard Roberts in 1822 and these were produced in large numbers by Roberts, Hill & Co.\n\nThe demand for cotton presented an opportunity to planters in the Southern United States, who thought upland cotton would be a profitable crop if a better way could be found to remove the seed. Eli Whitney responded to the challenge by inventing the inexpensive cotton gin. A man using a cotton gin could remove seed from as much upland cotton in one day as would previously, working at the rate of one pound of cotton per day, have taken a woman two months to process.\n\nThese advances were capitalised on by entrepreneurs, of whom the best known is Richard Arkwright. He is credited with a list of inventions, but these were actually developed by such people as Thomas Highs and John Kay; Arkwright nurtured the inventors, patented the ideas, financed the initiatives, and protected the machines. He created the cotton mill which brought the production processes together in a factory, and he developed the use of power – first horse power and then water powerwhich made cotton manufacture a mechanised industry. Other inventors increased the efficiency of the individual steps of spinning (carding, twisting and spinning, and rolling) so that the supply of yarn increased greatly. Before long steam power was applied to drive textile machinery. Manchester acquired the nickname Cottonopolis during the early 19th century owing to its sprawl of textile factories.\n\nAlthough mechanization dramatically decreased the cost of cotton cloth, by the mid-19th century machine-woven cloth still could not equal the quality of hand-woven Indian cloth, in part due to the fineness of thread made possible by the type of cotton used in India, which allowed high thread counts. However, the high productivity of British textile manufacturing allowed coarser grades of British cloth to undersell hand-spun and woven fabric in low-wage India, eventually destroying the industry.\n\nThe earliest European attempts at mechanized spinning were with wool; however, wool spinning proved more difficult to mechanize than cotton. Productivity improvement in wool spinning during the Industrial Revolution was significant but was far less than that of cotton.\n\nArguably the first highly mechanised factory was John Lombe's water-powered silk mill at Derby, operational by 1721. Lombe learned silk thread manufacturing by taking a job in Italy and acting as an industrial spy; however, because the Italian silk industry guarded its secrets closely, the state of the industry at that time is unknown. Although Lombe's factory was technically successful, the supply of raw silk from Italy was cut off to eliminate competition. In order to promote manufacturing the Crown paid for models of Lombe's machinery which were exhibited in the Tower of London.\n\nBar iron was the commodity form of iron used as the raw material for making hardware goods such as nails, wire, hinges, horse shoes, wagon tires, chains, etc. and for structural shapes. A small amount of bar iron was converted into steel. Cast iron was used for pots, stoves and other items where its brittleness was tolerable. Most cast iron was refined and converted to bar iron, with substantial losses. Bar iron was also made by the bloomery process, which was the predominant iron smelting process until the late 18th century.\n\nIn the UK in 1720 there were 20,500 tons of cast iron produced with charcoal and 400 tons with coke. In 1750 charcoal iron production was 24,500 and coke iron was 2,500 tons. In 1788 the production of charcoal cast iron was 14,000 tons while coke iron production was 54,000 tons. In 1806 charcoal cast iron production was 7,800 tons and coke cast iron was 250,000 tons.\n\nIn 1750 the UK imported 31,200 tons of bar iron and either refined from cast iron or directly produced 18,800 tons of bar iron using charcoal and 100 tons using coke. In 1796 the UK was making 125,000 tons of bar iron with coke and 6,400 tons with charcoal; imports were 38,000 tons and exports were 24,600 tons. In 1806 the UK did not import bar iron but exported 31,500 tons.\n\nA major change in the iron industries during the era of the Industrial Revolution was the replacement of wood and other bio-fuels with coal. For a given amount of heat, coal required much less labour to mine than cutting wood and converting it to charcoal, and coal was much more abundant than wood, supplies of which were becoming scarce before the enormous increase in iron production that took place in the late 18th century. By 1750 coke had generally replaced charcoal in smelting of copper and lead and was in widespread use in making glass. In the smelting and refining of iron, coal and coke produced inferior iron to that made with charcoal because of the coal's sulfur content. Low sulfur coals were known, but they still contained harmful amounts. Conversion of coal to coke only slightly reduces the sulfur content. A minority of coals are coking.\n\nAnother factor limiting the iron industry before the Industrial Revolution was the scarcity of water power to power blast bellows. This limitation was overcome by the steam engine.\n\nUse of coal in iron smelting started somewhat before the Industrial Revolution, based on innovations by Sir Clement Clerke and others from 1678, using coal reverberatory furnaces known as cupolas. These were operated by the flames playing on the ore and charcoal or coke mixture, reducing the oxide to metal. This has the advantage that impurities (such as sulphur ash) in the coal do not migrate into the metal. This technology was applied to lead from 1678 and to copper from 1687. It was also applied to iron foundry work in the 1690s, but in this case the reverberatory furnace was known as an air furnace. (The foundry cupola is a different, and later, innovation.)\n\nBy 1709 Abraham Darby made progress using coke to fuel his blast furnaces at Coalbrookdale. However, the coke pig iron he made was not suitable for making wrought iron and was used mostly for the production of cast iron goods, such as pots and kettles. He had the advantage over his rivals in that his pots, cast by his patented process, were thinner and cheaper than theirs.\n\nCoke pig iron was hardly used to produce wrought iron until 1755-56, when Darby's son Abraham Darby II built furnaces at Horsehay and Ketley where low sulfur coal was available (and not far from Coalbrookdale). These new furnaces were equipped with water-powered bellows, the water being pumped by Newcomen steam engines. The Newcomen engines were not attached directly to the blowing cylinders because the engines alone could not produce a steady air blast. Abraham Darby III installed similar steam-pumped, water-powered blowing cylinders at the Dale Company when he took control in 1768. The Dale Company used several Newcomen engines to drain its mines and made parts for engines which it sold throughout the country.\n\nSteam engines made the use of higher-pressure and volume blast practical; however, the leather used in bellows was expensive to replace. In 1757, iron master John Wilkinson patented a hydraulic powered blowing engine for blast furnaces. The blowing cylinder for blast furnaces was introduced in 1760 and the first blowing cylinder made of cast iron is believed to be the one used at Carrington in 1768 that was designed by John Smeaton. Cast iron cylinders for use with a piston were difficult to manufacture; the cylinders had to be free of holes and had to be machined smooth and straight to remove any warping. James Watt had great difficulty trying to have a cylinder made for his first steam engine. In 1774 John Wilkinson, who built a cast iron blowing cylinder for his iron works, invented a precision boring machine for boring cylinders. After Wilkinson bored the first successful cylinder for a Boulton and Watt steam engine in 1776, he was given an exclusive contract for providing cylinders. After Watt developed a rotary steam engine in 1782, they were widely applied to blowing, hammering, rolling and slitting.\n\nThe solutions to the sulfur problem were the addition of sufficient limestone to the furnace to force sulfur into the slag and the use of low sulfur coal. Use of lime or limestone required higher furnace temperatures to form a free-flowing slag. The increased furnace temperature made possible by improved blowing also increased the capacity of blast furnaces and allowed for increased furnace height. In addition to lower cost and greater availability, coke had other important advantages over charcoal in that it was harder and made the column of materials (iron ore, fuel, slag) flowing down the blast furnace more porous and did not crush in the much taller furnaces of the late 19th century.\n\nAs cast iron became cheaper and widely available, it began being a structural material for bridges and buildings. A famous early example was the Iron Bridge built in 1778 with cast iron produced by Abraham Darby III. However, most cast iron was converted to wrought iron.\n\nEurope relied on the bloomery for most of its wrought iron until the large scale production of cast iron. Conversion of cast iron was done in a finery forge, as it long had been. An improved refining process known as potting and stamping was developed, but this was superseded by Henry Cort's puddling process. Cort developed two significant iron manufacturing processes: rolling in 1783 and puddling in 1784. Puddling produced a structural grade iron at a relatively low cost.\n\nPuddling was a means of decarburizing molten pig iron by slow oxidation in a reverberatory furnace by manually stirring it with a long rod. The decarburized iron, having a higher melting point than cast iron, was raked into globs by the puddler. When the glob was large enough, the puddler would remove it. Puddling was backbreaking and extremely hot work. Few puddlers lived to be 40. Because puddling was done in a reverberatory furnace, coal or coke could be used as fuel. The puddling process continued to be used until the late 19th century when iron was being displaced by steel. Because puddling required human skill in sensing the iron globs, it was never successfully mechanised. Rolling was an important part of the puddling process because the grooved rollers expelled most of the molten slag and consolidated the mass of hot wrought iron. Rolling was 15 times faster at this than a trip hammer. A different use of rolling, which was done at lower temperatures than that for expelling slag, was in the production of iron sheets, and later structural shapes such as beams, angles and rails.\nThe puddling process was improved in 1818 by Baldwyn Rogers, who replaced some of the sand lining on the reverberatory furnace bottom with iron oxide. In 1838 John Hall patented the use of roasted tap cinder (iron silicate) for the furnace bottom, greatly reducing the loss of iron through increased slag caused by a sand lined bottom. The tap cinder also tied up some phosphorus, but this was not understood at the time. Hall's process also used iron scale or rust, which reacted with carbon in the molten iron. Hall's process, called \"wet puddling\", reduced losses of iron with the slag from almost 50% to around 8%.\n\nPuddling became widely used after 1800. Up to that time British iron manufacturers had used considerable amounts of iron imported from Sweden and Russia to supplement domestic supplies. Because of the increased British production, imports began to decline in 1785 and by the 1790s Britain eliminated imports and became a net exporter of bar iron.\n\nHot blast, patented by James Beaumont Neilson in 1828, was the most important development of the 19th century for saving energy in making pig iron. By using preheated combustion air, the amount of fuel to make a unit of pig iron was reduced at first by between one-third using coke or two-thirds using coal; however, the efficiency gains continued as the technology improved. Hot blast also raised the operating temperature of furnaces, increasing their capacity. Using less coal or coke meant introducing fewer impurities into the pig iron. This meant that lower quality coal or anthracite could be used in areas where coking coal was unavailable or too expensive; however, by the end of the 19th century transportation costs fell considerably.\n\nShortly before the Industrial Revolution an improvement was made in the production of steel, which was an expensive commodity and used only where iron would not do, such as for cutting edge tools and for springs. Benjamin Huntsman developed his crucible steel technique in the 1740s. The raw material for this was blister steel, made by the cementation process.\n\nThe supply of cheaper iron and steel aided a number of industries, such as those making nails, hinges, wire and other hardware items. The development of machine tools allowed better working of iron, causing it to be increasingly used in the rapidly growing machinery and engine industries.\n\nThe development of the stationary steam engine was an important element of the Industrial Revolution; however, during the early period of the Industrial Revolution, most industrial power was supplied by water and wind. In Britain by 1800 an estimated 10,000 horsepower was being supplied by steam. By 1815 steam power had grown to 210,000 hp.\n\nThe first commercially successful industrial use of steam power was due to Thomas Savery in 1698. He constructed and patented in London a low-lift combined vacuum and pressure water pump, that generated about one horsepower (hp) and was used in numerous water works and in a few mines (hence its \"brand name\", \"The Miner's Friend\"). Savery's pump was economical in small horsepower ranges, but was prone to boiler explosions in larger sizes. Savery pumps continued to be produced until the late 18th century.\n\nThe first successful piston steam engine was introduced by Thomas Newcomen before 1712. A number of Newcomen engines were installed in Britain for draining hitherto unworkable deep mines, with the engine on the surface; these were large machines, requiring a significant amount of capital to build, and produced upwards of . They were also used to power municipal water supply pumps. They were extremely inefficient by modern standards, but when located where coal was cheap at pit heads, opened up a great expansion in coal mining by allowing mines to go deeper. Despite their disadvantages, Newcomen engines were reliable and easy to maintain and continued to be used in the coalfields until the early decades of the 19th century. By 1729, when Newcomen died, his engines had spread (first) to Hungary in 1722, Germany, Austria, and Sweden. A total of 110 are known to have been built by 1733 when the joint patent expired, of which 14 were abroad. In the 1770s the engineer John Smeaton built some very large examples and introduced a number of improvements. A total of 1,454 engines had been built by 1800.\n\nA fundamental change in working principles was brought about by Scotsman James Watt. With financial support from his business partner Englishman Matthew Boulton, he had succeeded by 1778 in perfecting his steam engine, which incorporated a series of radical improvements, notably the closing off of the upper part of the cylinder, thereby making the low-pressure steam drive the top of the piston instead of the atmosphere, use of a steam jacket and the celebrated separate steam condenser chamber. The separate condenser did away with the cooling water that had been injected directly into the cylinder, which cooled the cylinder and wasted steam. Likewise, the steam jacket kept steam from condensing in the cylinder, also improving efficiency. These improvements increased engine efficiency so that Boulton & Watts engines used only 20–25% as much coal per horsepower-hour as Newcomen's. Boulton and Watt opened the Soho Foundry for the manufacture of such engines in 1795.\n\nBy 1783 the Watt steam engine had been fully developed into a double-acting rotative type, which meant that it could be used to directly drive the rotary machinery of a factory or mill. Both of Watt's basic engine types were commercially very successful, and by 1800, the firm Boulton & Watt had constructed 496 engines, with 164 driving reciprocating pumps, 24 serving blast furnaces, and 308 powering mill machinery; most of the engines generated from .\n\nUntil about 1800 the most common pattern of steam engine was the beam engine, built as an integral part of a stone or brick engine-house, but soon various patterns of self-contained rotative engines (readily removable, but not on wheels) were developed, such as the table engine. Around the start of the 19th century, at which time the Boulton and Watt patent expired, the Cornish engineer Richard Trevithick and the American Oliver Evans began to construct higher-pressure non-condensing steam engines, exhausting against the atmosphere. High pressure yielded an engine and boiler compact enough to be used on mobile road and rail locomotives and steam boats.\n\nThe development of machine tools, such as the engine lathe, planing, milling and shaping machines powered by these engines, enabled all the metal parts of the engines to be easily and accurately cut and in turn made it possible to build larger and more powerful engines.\n\nSmall industrial power requirements continued to be provided by animal and human muscle until widespread electrification in the early 20th century. These included crank-powered, treadle-powered and horse-powered workshop and light industrial machinery.\n\nPre-industrial machinery was built by various craftsmen – millwrights built water and windmills, carpenters made wooden framing, and smiths and turners made metal parts. Wooden components had the disadvantage of changing dimensions with temperature and humidity, and the various joints tended to rack (work loose) over time. As the Industrial Revolution progressed, machines with metal parts and frames became more common. Other important uses of metal parts were in firearms and threaded fasteners, such as machine screws, bolts and nuts. There was also the need for precision in making parts. Precision would allow better working machinery, interchangeability of parts and standardization of threaded fasteners.\n\nThe demand for metal parts led to the development of several machine tools. They have their origins in the tools developed in the 18th century by makers of clocks and watches and scientific instrument makers to enable them to batch-produce small mechanisms.\n\nBefore the advent of machine tools, metal was worked manually using the basic hand tools of hammers, files, scrapers, saws and chisels. Consequently, the use of metal machine parts was kept to a minimum. Hand methods of production were very laborious and costly and precision was difficult to achieve.\n\nThe first large precision machine tool was the cylinder boring machine invented by John Wilkinson in 1774. It used for boring the large-diameter cylinders on early steam engines. Wilkinson's boring machine differed from earlier cantilevered machines used for boring cannon in that the cutting tool was mounted on a beam that ran through the cylinder being bored and was supported outside on both ends.\n\nThe planing machine, the milling machine and the shaping machine were developed in the early decades of the 19th century. Although the milling machine was invented at this time, it was not developed as a serious workshop tool until somewhat later in the 19th century.\n\nHenry Maudslay, who trained a school of machine tool makers early in the 19th century, was a mechanic with superior ability who had been employed at the Royal Arsenal, Woolwich. He worked as an apprentice in the Royal Gun Foundry of Jan Verbruggen. In 1774 Jan Verbruggen had installed a horizontal boring machine in Woolwich which was the first industrial size Lathe in the UK. Maudslay was hired away by Joseph Bramah for the production of high-security metal locks that required precision craftsmanship. Bramah patented a lathe that had similarities to the slide rest lathe. Maudslay perfected the slide rest lathe, which could cut machine screws of different thread pitches by using changeable gears between the spindle and the lead screw. Before its invention screws could not be cut to any precision using various earlier lathe designs, some of which copied from a template. The slide rest lathe was called one of history's most important inventions. Although it was not entirely Maudslay's idea, he was the first person to build a functional lathe using a combination of known innovations of the lead screw, slide rest and change gears.\n\nMaudslay left Bramah's employment and set up his own shop. He was engaged to build the machinery for making ships' pulley blocks for the Royal Navy in the Portsmouth Block Mills. These machines were all-metal and were the first machines for mass production and making components with a degree of interchangeability. The lessons Maudslay learned about the need for stability and precision he adapted to the development of machine tools, and in his workshops he trained a generation of men to build on his work, such as Richard Roberts, Joseph Clement and Joseph Whitworth.\n\nJames Fox of Derby had a healthy export trade in machine tools for the first third of the century, as did Matthew Murray of Leeds. Roberts was a maker of high-quality machine tools and a pioneer of the use of jigs and gauges for precision workshop measurement.\n\nThe impact of machine tools during the Industrial Revolution was not that great because other than firearms, threaded fasteners and a few other industries there were few mass-produced metal parts. The techniques to make mass-produced metal parts made with sufficient precision to be interchangeable is largely attributed to a program of the U.S. Department of War which perfected interchangeable parts for firearms in the early 19th century.\n\nIn the half century following the invention of the fundamental machine tools the machine industry became the largest industrial sector of the U.S. economy, by value added.\n\nThe large-scale production of chemicals was an important development during the Industrial Revolution. The first of these was the production of sulphuric acid by the lead chamber process invented by the Englishman John Roebuck (James Watt's first partner) in 1746. He was able to greatly increase the scale of the manufacture by replacing the relatively expensive glass vessels formerly used with larger, less expensive chambers made of riveted sheets of lead. Instead of making a small amount each time, he was able to make around in each of the chambers, at least a tenfold increase.\n\nThe production of an alkali on a large scale became an important goal as well, and Nicolas Leblanc succeeded in 1791 in introducing a method for the production of sodium carbonate. The Leblanc process was a reaction of sulphuric acid with sodium chloride to give sodium sulphate and hydrochloric acid. The sodium sulphate was heated with limestone (calcium carbonate) and coal to give a mixture of sodium carbonate and calcium sulphide. Adding water separated the soluble sodium carbonate from the calcium sulphide. The process produced a large amount of pollution (the hydrochloric acid was initially vented to the air, and calcium sulphide was a useless waste product). Nonetheless, this synthetic soda ash proved economical compared to that from burning specific plants (barilla) or from kelp, which were the previously dominant sources of soda ash, and also to potash (potassium carbonate) produced from hardwood ashes.\n\nThese two chemicals were very important because they enabled the introduction of a host of other inventions, replacing many small-scale operations with more cost-effective and controllable processes. Sodium carbonate had many uses in the glass, textile, soap, and paper industries. Early uses for sulphuric acid included pickling (removing rust) iron and steel, and for bleaching cloth.\n\nThe development of bleaching powder (calcium hypochlorite) by Scottish chemist Charles Tennant in about 1800, based on the discoveries of French chemist Claude Louis Berthollet, revolutionised the bleaching processes in the textile industry by dramatically reducing the time required (from months to days) for the traditional process then in use, which required repeated exposure to the sun in bleach fields after soaking the textiles with alkali or sour milk. Tennant's factory at St Rollox, North Glasgow, became the largest chemical plant in the world.\n\nAfter 1860 the focus on chemical innovation was in dyestuffs, and Germany took world leadership, building a strong chemical industry. Aspiring chemists flocked to German universities in the 1860–1914 era to learn the latest techniques. British scientists by contrast, lacked research universities and did not train advanced students; instead, the practice was to hire German-trained chemists.\n\nIn 1824 Joseph Aspdin, a British bricklayer turned builder, patented a chemical process for making portland cement which was an important advance in the building trades. This process involves sintering a mixture of clay and limestone to about , then grinding it into a fine powder which is then mixed with water, sand and gravel to produce concrete. Portland cement was used by the famous English engineer Marc Isambard Brunel several years later when constructing the Thames Tunnel. Cement was used on a large scale in the construction of the London sewerage system a generation later.\n\nAnother major industry of the later Industrial Revolution was gas lighting. Though others made a similar innovation elsewhere, the large-scale introduction of this was the work of William Murdoch, an employee of Boulton & Watt, the Birmingham steam engine pioneers. The process consisted of the large-scale gasification of coal in furnaces, the purification of the gas (removal of sulphur, ammonia, and heavy hydrocarbons), and its storage and distribution. The first gas lighting utilities were established in London between 1812 and 1820. They soon became one of the major consumers of coal in the UK. Gas lighting affected social and industrial organisation because it allowed factories and stores to remain open longer than with tallow candles or oil. Its introduction allowed nightlife to flourish in cities and towns as interiors and streets could be lighted on a larger scale than before.\n\nA new method of producing glass, known as the cylinder process, was developed in Europe during the early 19th century. In 1832 this process was used by the Chance Brothers to create sheet glass. They became the leading producers of window and plate glass. This advancement allowed for larger panes of glass to be created without interruption, thus freeing up the space planning in interiors as well as the fenestration of buildings. The Crystal Palace is the supreme example of the use of sheet glass in a new and innovative structure.\n\nA machine for making a continuous sheet of paper on a loop of wire fabric was patented in 1798 by Nicholas Louis Robert who worked for Saint-Léger Didot family in France. The paper machine is known as a Fourdrinier after the financiers, brothers Sealy and Henry Fourdrinier, who were stationers in London. Although greatly improved and with many variations, the Fourdriner machine is the predominant means of paper production today.\n\nThe method of continuous production demonstrated by the paper machine influenced the development of continuous rolling of iron and later steel and other continuous production processes.\n\nThe British Agricultural Revolution is considered one of the causes of the Industrial Revolution because improved agricultural productivity freed up workers to work in other sectors of the economy. However, per-capita food supply in Europe was stagnant or declining and did not improve in some parts of Europe until the late 18th century.\n\nIndustrial technologies that affected farming included the seed drill, the Dutch plough, which contained iron parts, and the threshing machine.\n\nJethro Tull invented an improved seed drill in 1701. It was a mechanical seeder which distributed seeds evenly across a plot of land and planted them at the correct depth. This was important because the yield of seeds harvested to seeds planted at that time was around four or five. Tull's seed drill was very expensive and not very reliable and therefore did not have much of an impact. Good quality seed drills were not produced until the mid 18th century.\n\nJoseph Foljambe's \"Rotherham plough\" of 1730 was the first commercially successful iron plough. The threshing machine, invented by Andrew Meikle in 1784, displaced hand threshing with a flail, a laborious job that took about one-quarter of agricultural labour. It took several decades to diffuse and was the final straw for many farm labourers, who faced near starvation, leading to the 1830 agricultural rebellion of the Swing Riots.\n\nMachine tools and metalworking techniques developed during the Industrial Revolution eventually resulted in precision manufacturing techniques in the late 19th century for mass-producing agricultural equipment, such as reapers, binders and combine harvesters.\n\nCoal mining in Britain, particularly in South Wales, started early. Before the steam engine, pits were often shallow bell pits following a seam of coal along the surface, which were abandoned as the coal was extracted. In other cases, if the geology was favourable, the coal was mined by means of an adit or drift mine driven into the side of a hill. Shaft mining was done in some areas, but the limiting factor was the problem of removing water. It could be done by hauling buckets of water up the shaft or to a sough (a tunnel driven into a hill to drain a mine). In either case, the water had to be discharged into a stream or ditch at a level where it could flow away by gravity. The introduction of the steam pump by Thomas Savery in 1698 and the Newcomen steam engine in 1712 greatly facilitated the removal of water and enabled shafts to be made deeper, enabling more coal to be extracted. These were developments that had begun before the Industrial Revolution, but the adoption of John Smeaton's improvements to the Newcomen engine followed by James Watt's more efficient steam engines from the 1770s reduced the fuel costs of engines, making mines more profitable. The Cornish engine, developed in the 1810s, was much more efficient than the Watt steam engine.\n\nCoal mining was very dangerous owing to the presence of firedamp in many coal seams. Some degree of safety was provided by the safety lamp which was invented in 1816 by Sir Humphry Davy and independently by George Stephenson. However, the lamps proved a false dawn because they became unsafe very quickly and provided a weak light. Firedamp explosions continued, often setting off coal dust explosions, so casualties grew during the entire 19th century. Conditions of work were very poor, with a high casualty rate from rock falls.\n\nAt the beginning of the Industrial Revolution, inland transport was by navigable rivers and roads, with coastal vessels employed to move heavy goods by sea. Wagonways were used for conveying coal to rivers for further shipment, but canals had not yet been widely constructed. Animals supplied all of the motive power on land, with sails providing the motive power on the sea. The first horse railways were introduced toward the end of the 18th century, with steam locomotives being introduced in the early decades of the 19th century. Improving sailing technologies boosted average sailing speed 50% between 1750 and 1830.\n\nThe Industrial Revolution improved Britain's transport infrastructure with a turnpike road network, a canal and waterway network, and a railway network. Raw materials and finished products could be moved more quickly and cheaply than before. Improved transportation also allowed new ideas to spread quickly.\n\nBefore and during the Industrial Revolution navigation on several British rivers was improved by removing obstructions, straightening curves, widening and deepening and building navigation locks. Britain had over 1000 miles of navigable rivers and streams by 1750.\n\nCanals and waterways allowed bulk materials to be economically transported long distances inland. This was because a horse could pull a barge with a load dozens of times larger than the load that could be drawn in a cart.\n\nBuilding of canals dates to ancient times. The Grand Canal in China, \"the world's largest artificial waterway and oldest canal still in existence,\" parts of which were started between the 6th and 4th centuries BC, is long and links Hangzhou with Beijing.\n\nIn the UK, canals began to be built in the late 18th century to link the major manufacturing centres across the country. Known for its huge commercial success, the Bridgewater Canal in North West England, which opened in 1761 and was mostly funded by The 3rd Duke of Bridgewater. From Worsley to the rapidly growing town of Manchester its construction cost £168,000 (£ ), but its advantages over land and river transport meant that within a year of its opening in 1761, the price of coal in Manchester fell by about half. This success helped inspire a period of intense canal building, known as Canal Mania. New canals were hastily built in the aim of replicating the commercial success of the Bridgewater Canal, the most notable being the Leeds and Liverpool Canal and the Thames and Severn Canal which opened in 1774 and 1789 respectively.\n\nBy the 1820s a national network was in existence. Canal construction served as a model for the organisation and methods later used to construct the railways. They were eventually largely superseded as profitable commercial enterprises by the spread of the railways from the 1840s on. The last major canal to be built in the United Kingdom was the Manchester Ship Canal, which upon opening in 1894 was the largest ship canal in the world, and opened Manchester as a port. However it never achieved the commercial success its sponsors had hoped for and signalled canals as a dying mode of transport in an age dominated by railways, which were quicker and often cheaper.\n\nBritain's canal network, together with its surviving mill buildings, is one of the most enduring features of the early Industrial Revolution to be seen in Britain.\n\nFrance was known for having an excellent system of roads at the time of the Industrial Revolution; however, most of the roads on the European Continent and in the U.K. were in bad condition and dangerously rutted.\n\nMuch of the original British road system was poorly maintained by thousands of local parishes, but from the 1720s (and occasionally earlier) turnpike trusts were set up to charge tolls and maintain some roads. Increasing numbers of main roads were turnpiked from the 1750s to the extent that almost every main road in England and Wales was the responsibility of a turnpike trust. New engineered roads were built by John Metcalf, Thomas Telford and most notably John McAdam, with the first 'macadamised' stretch of road being Marsh Road at Ashton Gate, Bristol in 1816. The major turnpikes radiated from London and were the means by which the Royal Mail was able to reach the rest of the country. Heavy goods transport on these roads was by means of slow, broad wheeled, carts hauled by teams of horses. Lighter goods were conveyed by smaller carts or by teams of pack horse. Stagecoaches carried the rich, and the less wealthy could pay to ride on carriers carts.\n\nReducing friction was one of the major reasons for the success of railroads compared to wagons. This was demonstrated on an iron plate covered wooden tramway in 1805 at Croydon, England.\n“A good horse on an ordinary turnpike road can draw two thousand pounds, or one ton. A party of gentlemen were invited to witness the experiment, that the superiority of the new road might be established by ocular demonstration. Twelve wagons were loaded with stones, till each wagon weighed three tons, and the wagons were fastened together. A horse was then attached, which drew the wagons with ease, six miles in two hours, having stopped four times, in order to show he had the power of starting, as well as drawing his great load.”\n\nRailways were made practical by the widespread introduction of inexpensive puddled iron after 1800, the rolling mill for making rails, and the development of the high-pressure steam engine also around 1800.\n\nWagonways for moving coal in the mining areas had started in the 17th century and were often associated with canal or river systems for the further movement of coal. These were all horse drawn or relied on gravity, with a stationary steam engine to haul the wagons back to the top of the incline. The first applications of the steam locomotive were on wagon or plate ways (as they were then often called from the cast-iron plates used). Horse-drawn public railways did not begin until the early years of the 19th century when improvements to pig and wrought iron production were lowering costs. See: Metallurgy\n\nSteam locomotives began being built after the introduction of high-pressure steam engines after the expiration of the Boulton and Watt patent in 1800. High-pressure engines exhausted used steam to the atmosphere, doing away with the condenser and cooling water. They were also much lighter weight and smaller in size for a given horsepower than the stationary condensing engines. A few of these early locomotives were used in mines. Steam-hauled public railways began with the Stockton and Darlington Railway in 1825.\n\nThe rapid introduction of railways followed the 1829 Rainhill Trials, which demonstrated Robert Stephenson's successful locomotive design and the 1828 development of Hot blast, which dramatically reduced the fuel consumption of making iron and increased the capacity the blast furnace.\n\nOn 15 September 1830, the Liverpool and Manchester Railway was opened, the first inter-city railway in the world and was attended by Prime Minister, the Duke of Wellington. The railway was engineered by Joseph Locke and George Stephenson, linked the rapidly expanding industrial town of Manchester with the port town of Liverpool. The opening was marred by problems, due to the primitive nature of the technology being employed, however problems were gradually ironed out and the railway became highly successful, transporting passengers and freight. The success of the inter-city railway, particularly in the transport of freight and commodities, led to Railway Mania.\n\nConstruction of major railways connecting the larger cities and towns began in the 1830s but only gained momentum at the very end of the first Industrial Revolution. After many of the workers had completed the railways, they did not return to their rural lifestyles but instead remained in the cities, providing additional workers for the factories.\n\nOther developments included more efficient water wheels, based on experiments conducted by the British engineer John Smeaton the beginnings of a machine industry and the rediscovery of concrete (based on hydraulic lime mortar) by John Smeaton, which had been lost for 1300 years.\n\nPrior to the Industrial Revolution, most of the workforce was employed in agriculture, either as self-employed farmers as landowners or tenants, or as landless agricultural labourers. It was common for families in various parts of the world to spin yarn, weave cloth and make their own clothing. Households also spun and wove for market production. At the beginning of the Industrial Revolution India, China and regions of Iraq and elsewhere in Asia and the Middle East produced most of the world's cotton cloth while Europeans produced wool and linen goods.\n\nIn Britain by the 16th century the putting-out system, by which farmers and townspeople produced goods for market in their homes, often described as \"cottage industry\", was being practiced. Typical putting out system goods included spinning and weaving. Merchant capitalist typically provided the raw materials, paid workers by the piece, and were responsible for the sale of the goods. Embezzlement of supplies by workers and poor quality were common problems. The logistical effort in procuring and distributing raw materials and picking up finished goods were also limitations of the putting out system.\n\nSome early spinning and weaving machinery, such as a 40 spindle jenny for about six pounds in 1792, was affordable for cottagers. Later machinery such as spinning frames, spinning mules and power looms were expensive (especially if water powered), giving rise to capitalist ownership of factories.\n\nThe majority of textile factory workers during the Industrial Revolution were unmarried women and children, including many orphans. They typically worked for 12 to 14 hours per day with only Sundays off. It was common for women take factory jobs seasonally during slack periods of farm work. Lack of adequate transportation, long hours and poor pay made it difficult to recruit and maintain workers. Many workers, such as displaced farmers and agricultural workers, who had nothing but their labour to sell, became factory workers out of necessity. (See: British Agricultural Revolution, Threshing machine)\n\nThe change in the social relationship of the factory worker compared to farmers and cottagers was viewed unfavourably by Karl Marx, however, he recognized the increase in productivity made possible by technology.\n\nSome economists, such as Robert E. Lucas, Jr., say that the real impact of the Industrial Revolution was that \"for the first time in history, the living standards of the masses of ordinary people have begun to undergo sustained growth ... Nothing remotely like this economic behaviour is mentioned by the classical economists, even as a theoretical possibility.\" Others, however, argue that while growth of the economy's overall productive powers was unprecedented during the Industrial Revolution, living standards for the majority of the population did not grow meaningfully until the late 19th and 20th centuries, and that in many ways workers' living standards declined under early capitalism: for instance, studies have shown that real wages in Britain only increased 15% between the 1780s and 1850s, and that life expectancy in Britain did not begin to dramatically increase until the 1870s. Similarly, the average height of the population declined during the Industrial Revolution, implying that their nutritional status was also decreasing. Real wages were not keeping up with the price of food.\n\nDuring the Industrial Revolution, the life expectancy of children increased dramatically. The percentage of the children born in London who died before the age of five decreased from 74.5% in 1730–1749 to 31.8% in 1810–1829.\n\nThe effects on living conditions the industrial revolution have been very controversial, and were hotly debated by economic and social historians from the 1950s to the 1980s. A series of 1950s essays by Henry Phelps Brown and Sheila V. Hopkins later set the academic consensus that the bulk of the population, that was at the bottom of the social ladder, suffered severe reductions in their living standards. During 1813–1913, there was a significant increase in worker wages.\n\nChronic hunger and malnutrition were the norm for the majority of the population of the world including Britain and France, until the late 19th century. Until about 1750, in large part due to malnutrition, life expectancy in France was about 35 years and about 40 years in Britain. The United States population of the time was adequately fed, much taller on average and had life expectancy of 45–50 years although U.S. life expectancy declined by a few years by the mid 19th century. Food consumption per capita also declined during an episode known as the Antebellum Puzzle.\n\nFood supply in Great Britain was adversely affected by the Corn Laws (1815-1846). The Corn Laws, which imposed tariffs on imported grain, were enacted to keep prices high in order to benefit domestic producers. The Corn Laws were repealed in the early years of the Great Irish Famine.\n\nThe initial technologies of the Industrial Revolution, such as mechanized textiles, iron and coal, did little, if anything, to lower food prices. In Britain and the Netherlands, food supply increased before the Industrial Revolution due to better agricultural practices; however, population grew too, as noted by Thomas Malthus. This condition is called the Malthusian trap, and it finally started to overcome by transportation improvements, such as canals, improved roads and steamships. Railroads and steamships were introduced near the end of the Industrial Revolution.\n\nThe very rapid growth in population in the 19th century in the cities included the new industrial and manufacturing cities, as well as service centers such as Edinburgh and London. The critical factor was financing, which was handled by building societies that dealt directly with large contracting firms. Private renting from housing landlords was the dominant tenure. P. Kemp says this was usually of advantage to tenants. People moved in so rapidly that there was not enough capital to build adequate housing for everyone, so low-income newcomers squeezed into increasingly overcrowded slums. Clean water, sanitation, and public health facilities were inadequate; the death rate was high, especially infant mortality, and tuberculosis among young adults. Cholera from polluted water and typhoid were endemic. Unlike rural areas, there were no famines such as devastated Ireland in the 1840s.\n\nA large exposé literature grew up condemning the unhealthy conditions. By far the most famous publication was by one of the founders of the Socialist movement, \"The Condition of the Working Class in England\" in 1844 Friedrich Engels described backstreet sections of Manchester and other mill towns, where people lived in crude shanties and shacks, some not completely enclosed, some with dirt floors. These shanty towns had narrow walkways between irregularly shaped lots and dwellings. There were no sanitary facilities. Population density was extremely high. Not everyone lived in such poor conditions. The Industrial Revolution also created a middle class of businessmen, clerks, foremen and engineers who lived in much better conditions.\n\nConditions improved over the course of the 19th century due to new public health acts regulating things such as sewage, hygiene and home construction. In the introduction of his 1892 edition, Engels notes that most of the conditions he wrote about in 1844 had been greatly improved. For example, the Public Health Act 1875 led to the more sanitary byelaw terraced house.\n\nIn \"The Condition of the Working Class in England\" in 1844 Friedrich Engels described how untreated sewage created awful odors and turned the rivers green in industrial cities.\n\nIn 1854 John Snow traced a cholera outbreak in Soho to fecal contamination of a public water well by a home cesspit. Snow's findings that cholera could be spread by contaminated water took some years to be accepted, but his work led to fundamental changes in the design of public water and waste systems.\n\nPre-industrial water supply relied on gravity systems and pumping of water was done by water wheels. Pipes were typically made of wood. Steam powered pumps and iron pipes allowed the widespread piping of water to horse watering troughs and households.\n\nThe invention of the paper machine and the application of steam power to the industrial processes of printing supported a massive expansion of newspaper and popular book publishing, which contributed to rising literacy and demands for mass political participation.\n\nConsumers benefited from falling prices for clothing and household articles such as cast iron cooking utensils, and in the following decades, stoves for cooking and space heating. Coffee, tea, sugar, tobacco and chocolate became affordable to many in Europe. Watches and household clocks became popular consumer items.\n\nMeeting the demands of the consumer revolution and growth in wealth of the middle classes in Britain, potter and entrepreneur Josiah Wedgwood, founder of Wedgwood fine china and porcelain, created goods such as tableware, which was starting to become a common feature on dining tables.\n\nThe Industrial Revolution was the first period in history during which there was a simultaneous increase in both population and per capita income.\n\nAccording to Robert Hughes in \"The Fatal Shore\", the population of England and Wales, which had remained steady at six million from 1700 to 1740, rose dramatically after 1740. The population of England had more than doubled from 8.3 million in 1801 to 16.8 million in 1850 and, by 1901, had nearly doubled again to 30.5 million. Improved conditions led to the population of Britain increasing from 10 million to 40 million in the 1800s. Europe's population increased from about 100 million in 1700 to 400 million by 1900.\n\nThe growth of modern industry since the late 18th century led to massive urbanisation and the rise of new great cities, first in Europe and then in other regions, as new opportunities brought huge numbers of migrants from rural communities into urban areas. In 1800, only 3% of the world's population lived in cities, compared to nearly 50% today (the beginning of the 21st century). Manchester had a population of 10,000 in 1717, but by 1911 it had burgeoned to 2.3 million.\n\nWomen's historians have debated the effect of the Industrial Revolution and capitalism generally on the status of women. Taking a pessimistic side, Alice Clark argued that when capitalism arrived in 17th century England, it lowered the status of women as they lost much of their economic importance. Clark argues that in 16th-century England, women were engaged in many aspects of industry and agriculture. The home was a central unit of production and women played a vital role in running farms, and in some trades and landed estates. Their useful economic roles gave them a sort of equality with their husbands. However, Clark argues, as capitalism expanded in the 17th century, there was more and more division of labour with the husband taking paid labour jobs outside the home, and the wife reduced to unpaid household work. Middle- and upper-class women were confined to an idle domestic existence, supervising servants; lower-class women were forced to take poorly paid jobs. Capitalism, therefore, had a negative effect on powerful women.\n\nIn a more positive interpretation, Ivy Pinchbeck argues that capitalism created the conditions for women's emancipation. Tilly and Scott have emphasised the continuity in the status of women, finding three stages in English history. In the pre-industrial era, production was mostly for home use and women produce much of the needs of the households. The second stage was the \"family wage economy\" of early industrialisation; the entire family depended on the collective wages of its members, including husband, wife and older children. The third or modern stage is the \"family consumer economy,\" in which the family is the site of consumption, and women are employed in large numbers in retail and clerical jobs to support rising standards of consumption.\n\nIdeas of thrift and hard work characterized middle-class families as the Industrial Revolution swept Europe. These values were displayed in Samuel Smiles' book \"Self-Help\", in which he states that the misery of the poorer classes was \"voluntary and self-imposed - the results of idleness, thriftlessness, intemperance, and misconduct.\"\n\nIn terms of social structure, the Industrial Revolution witnessed the triumph of a middle class of industrialists and businessmen over a landed class of nobility and gentry. Ordinary working people found increased opportunities for employment in the new mills and factories, but these were often under strict working conditions with long hours of labour dominated by a pace set by machines. As late as the year 1900, most industrial workers in the United States still worked a 10-hour day (12 hours in the steel industry), yet earned from 20% to 40% less than the minimum deemed necessary for a decent life; however, most workers in textiles, which was by far the leading industry in terms of employment, were women and children. For workers of the laboring classes, industrial life \"was a stony desert, which they had to make habitable by their own efforts.\" Also, harsh working conditions were prevalent long before the Industrial Revolution took place. Pre-industrial society was very static and often cruel – child labour, dirty living conditions, and long working hours were just as prevalent before the Industrial Revolution.\n\nIndustrialisation led to the creation of the factory. The factory system contributed to the growth of urban areas, as large numbers of workers migrated into the cities in search of work in the factories. Nowhere was this better illustrated than the mills and associated industries of Manchester, nicknamed \"Cottonopolis\", and the world's first industrial city. Manchester experienced a six-times increase in its population between 1771 and 1831. Bradford grew by 50% every ten years between 1811 and 1851 and by 1851 only 50% of the population of Bradford was actually born there.\n\nIn addition, between 1815 and 1939, 20 percent of Europe's population left home, pushed by poverty, a rapidly growing population, and the displacement of peasant farming and artisan manufacturing. They were pulled abroad by the enormous demand for labor overseas, the ready availability of land, and cheap transportation. Still, many did not find a satisfactory life in their new homes, leading 7 million of them to return to Europe. This mass migration had large demographic impacts: in 1800, less than one percent of the world population consisted of overseas Europeans and their descendants; by 1930, they represented 11 percent. The Americas felt the brunt of this huge emigration, largely concentrated in the United States.\n\nFor much of the 19th century, production was done in small mills, which were typically water-powered and built to serve local needs. Later, each factory would have its own steam engine and a chimney to give an efficient draft through its boiler.\n\nIn other industries, the transition to factory production was not so divisive. Some industrialists themselves tried to improve factory and living conditions for their workers. One of the earliest such reformers was Robert Owen, known for his pioneering efforts in improving conditions for workers at the New Lanark mills, and often regarded as one of the key thinkers of the early socialist movement.\n\nBy 1746 an integrated brass mill was working at Warmley near Bristol. Raw material went in at one end, was smelted into brass and was turned into pans, pins, wire, and other goods. Housing was provided for workers on site. Josiah Wedgwood and Matthew Boulton (whose Soho Manufactory was completed in 1766) were other prominent early industrialists, who employed the factory system.\n\nThe Industrial Revolution led to a population increase but the chances of surviving childhood did not improve throughout the Industrial Revolution, although \"infant\" mortality rates were reduced markedly. There was still limited opportunity for education and children were expected to work. Employers could pay a child less than an adult even though their productivity was comparable; there was no need for strength to operate an industrial machine, and since the industrial system was completely new, there were no experienced adult labourers. This made child labour the labour of choice for manufacturing in the early phases of the Industrial Revolution between the 18th and 19th centuries. In England and Scotland in 1788, two-thirds of the workers in 143 water-powered cotton mills were described as children.\n\nChild labour existed before the Industrial Revolution but with the increase in population and education it became more visible. Many children were forced to work in relatively bad conditions for much lower pay than their elders, 10–20% of an adult male's wage. Children as young as four were employed. Beatings and long hours were common, with some child coal miners and hurriers working from 4 am until 5 pm. Conditions were dangerous, with some children killed when they dozed off and fell into the path of the carts, while others died from gas explosions. Many children developed lung cancer and other diseases and died before the age of 25. Workhouses would sell orphans and abandoned children as \"pauper apprentices\", working without wages for board and lodging. Those who ran away would be whipped and returned to their masters, with some masters shackling them to prevent escape. Children employed as mule scavengers by cotton mills would crawl under machinery to pick up cotton, working 14 hours a day, six days a week. Some lost hands or limbs, others were crushed under the machines, and some were decapitated. Young girls worked at match factories, where phosphorus fumes would cause many to develop phossy jaw. Children employed at glassworks were regularly burned and blinded, and those working at potteries were vulnerable to poisonous clay dust.\n\nReports were written detailing some of the abuses, particularly in the coal mines and textile factories, and these helped to popularise the children's plight. The public outcry, especially among the upper and middle classes, helped stir change in the young workers' welfare.\n\nPoliticians and the government tried to limit child labour by law but factory owners resisted; some felt that they were aiding the poor by giving their children money to buy food to avoid starvation, and others simply welcomed the cheap labour. In 1833 and 1844, the first general laws against child labour, the Factory Acts, were passed in Britain: Children younger than nine were not allowed to work, children were not permitted to work at night, and the work day of youth under the age of 18 was limited to twelve hours. Factory inspectors supervised the execution of the law, however, their scarcity made enforcement difficult. About ten years later, the employment of children and women in mining was forbidden. Although laws such as these decreased the number of child labourers, child labour remained significantly present in Europe and the United States until the 20th century.\n\nThe Industrial Revolution concentrated labour into mills, factories and mines, thus facilitating the organisation of \"combinations\" or trade unions to help advance the interests of working people. The power of a union could demand better terms by withdrawing all labour and causing a consequent cessation of production. Employers had to decide between giving in to the union demands at a cost to themselves or suffering the cost of the lost production. Skilled workers were hard to replace, and these were the first groups to successfully advance their conditions through this kind of bargaining.\n\nThe main method the unions used to effect change was strike action. Many strikes were painful events for both sides, the unions and the management. In Britain, the Combination Act 1799 forbade workers to form any kind of trade union until its repeal in 1824. Even after this, unions were still severely restricted. One British newspaper in 1834 described unions as \"the most dangerous institutions that were ever permitted to take root, under shelter of law, in any country...\"\n\nIn 1832, the Reform Act extended the vote in Britain but did not grant universal suffrage. That year six men from Tolpuddle in Dorset founded the Friendly Society of Agricultural Labourers to protest against the gradual lowering of wages in the 1830s. They refused to work for less than ten shillings a week, although by this time wages had been reduced to seven shillings a week and were due to be further reduced to six. In 1834 James Frampton, a local landowner, wrote to the Prime Minister, Lord Melbourne, to complain about the union, invoking an obscure law from 1797 prohibiting people from swearing oaths to each other, which the members of the Friendly Society had done. James Brine, James Hammett, George Loveless, George's brother James Loveless, George's brother in-law Thomas Standfield, and Thomas's son John Standfield were arrested, found guilty, and transported to Australia. They became known as the Tolpuddle Martyrs. In the 1830s and 1840s, the Chartist movement was the first large-scale organised working class political movement which campaigned for political equality and social justice. Its \"Charter\" of reforms received over three million signatures but was rejected by Parliament without consideration.\n\nWorking people also formed friendly societies and co-operative societies as mutual support groups against times of economic hardship. Enlightened industrialists, such as Robert Owen also supported these organisations to improve the conditions of the working class.\n\nUnions slowly overcame the legal restrictions on the right to strike. In 1842, a general strike involving cotton workers and colliers was organised through the Chartist movement which stopped production across Great Britain.\n\nEventually, effective political organisation for working people was achieved through the trades unions who, after the extensions of the franchise in 1867 and 1885, began to support socialist political parties that later merged to become the British Labour Party.\n\nThe rapid industrialisation of the English economy cost many craft workers their jobs. The movement started first with lace and hosiery workers near Nottingham and spread to other areas of the textile industry owing to early industrialisation. Many weavers also found themselves suddenly unemployed since they could no longer compete with machines which only required relatively limited (and unskilled) labour to produce more cloth than a single weaver. Many such unemployed workers, weavers, and others, turned their animosity towards the machines that had taken their jobs and began destroying factories and machinery. These attackers became known as Luddites, supposedly followers of Ned Ludd, a folklore figure. The first attacks of the Luddite movement began in 1811. The Luddites rapidly gained popularity, and the British government took drastic measures, using the militia or army to protect industry. Those rioters who were caught were tried and hanged, or transported for life.\n\nUnrest continued in other sectors as they industrialised, such as with agricultural labourers in the 1830s when large parts of southern Britain were affected by the Captain Swing disturbances. Threshing machines were a particular target, and hayrick burning was a popular activity. However, the riots led to the first formation of trade unions, and further pressure for reform.\n\nThe traditional centers of hand textile production such as India, parts of the Middle East and later China could not withstand the competition from machine-made textiles, which over a period of decades destroyed the hand made textile industries and left millions of people without work, many of whom starved.\n\nThe Industrial Revolution also generated an enormous and unprecedented economic division in the world, as measured by the share of manufacturing output.\nCheap cotton textiles increased the demand for raw cotton; previously, it had primarily been consumed in subtropical regions where it was grown, with little raw cotton available for export. Consequently, prices of raw cotton rose. Some cotton had been grown in the West Indies, particularly in Hispaniola, but Haitian cotton production was halted by the Haitian Revolution in 1791. The invention of the cotton gin in 1792 allowed Georgia green seeded cotton to be profitable, leading to the widespread growth of cotton plantations in the United States and Brazil. In 1791 world cotton production was estimated to be 490,000,000 pounds with U.S. production accounting to 2,000,000 pounds. By 1800 U.S. production was 35,000,000 pounds, of which 17,790,000 were exported. In 1945 the U.S. produced seven-eights of the 1,169,600,000 pounds of world production.\n\nThe Americas, particularly the U.S., had labor shortages and high priced labor, which made slavery attractive. America's cotton plantations were highly efficient and profitable, and able to keep up with demand. The U.S. Civil war created a \"cotton famine\" that lead to increased production in other areas of the world, including new colonies in Africa.\n\nThe origins of the environmental movement lay in the response to increasing levels of smoke pollution in the atmosphere during the Industrial Revolution. The emergence of great factories and the concomitant immense growth in coal consumption gave rise to an unprecedented level of air pollution in industrial centers; after 1900 the large volume of industrial chemical discharges added to the growing load of untreated human waste. The first large-scale, modern environmental laws came in the form of Britain's Alkali Acts, passed in 1863, to regulate the deleterious air pollution (gaseous hydrochloric acid) given off by the Leblanc process, used to produce soda ash. An Alkali inspector and four sub-inspectors were appointed to curb this pollution. The responsibilities of the inspectorate were gradually expanded, culminating in the Alkali Order 1958 which placed all major heavy industries that emitted smoke, grit, dust and fumes under supervision.\n\nThe manufactured gas industry began in British cities in 1812–1820. The technique used produced highly toxic effluent that was dumped into sewers and rivers. The gas companies were repeatedly sued in nuisance lawsuits. They usually lost and modified the worst practices. The City of London repeatedly indicted gas companies in the 1820s for polluting the Thames and poisoning its fish. Finally, Parliament wrote company charters to regulate toxicity. The industry reached the US around 1850 causing pollution and lawsuits.\n\nIn industrial cities local experts and reformers, especially after 1890, took the lead in identifying environmental degradation and pollution, and initiating grass-roots movements to demand and achieve reforms. Typically the highest priority went to water and air pollution. The Coal Smoke Abatement Society was formed in Britain in 1898 making it one of the oldest environmental NGOs. It was founded by artist Sir William Blake Richmond, frustrated with the pall cast by coal smoke. Although there were earlier pieces of legislation, the Public Health Act 1875 required all furnaces and fireplaces to consume their own smoke. It also provided for sanctions against factories that emitted large amounts of black smoke. The provisions of this law were extended in 1926 with the Smoke Abatement Act to include other emissions, such as soot, ash, and gritty particles and to empower local authorities to impose their own regulations.\n\nThe Industrial Revolution on Continental Europe came a little later than in Great Britain. In many industries, this involved the application of technology developed in Britain in new places. Often the technology was purchased from Britain or British engineers and entrepreneurs moved abroad in search of new opportunities. By 1809, part of the Ruhr Valley in Westphalia was called 'Miniature England' because of its similarities to the industrial areas of England. The German, Russian and Belgian governments all provided state funding to the new industries. In some cases (such as iron), the different availability of resources locally meant that only some aspects of the British technology were adopted.\n\nBelgium was the second country, after Britain, in which the Industrial Revolution took place and the first in continental Europe: Wallonia (French speaking southern Belgium) was the first region to follow the British model successfully. Starting in the middle of the 1820s, and especially after Belgium became an independent nation in 1830, numerous works comprising coke blast furnaces as well as puddling and rolling mills were built in the coal mining areas around Liège and Charleroi. The leader was a transplanted Englishman John Cockerill. His factories at Seraing integrated all stages of production, from engineering to the supply of raw materials, as early as 1825.\n\nWallonia exemplified the radical evolution of industrial expansion. Thanks to coal (the French word \"houille\" was coined in Wallonia), the region geared up to become the 2nd industrial power in the world after Britain. But it is also pointed out by many researchers, with its \"Sillon industriel\", 'Especially in the Haine, Sambre and Meuse valleys, between the Borinage and Liège, [...] there was a huge industrial development based on coal-mining and iron-making...'. Philippe Raxhon wrote about the period after 1830: \"It was not propaganda but a reality the Walloon regions were becoming the second industrial power all over the world after Britain.\" \"The sole industrial centre outside the collieries and blast furnaces of Walloon was the old cloth making town of Ghent.\" Michel De Coster, Professor at the Université de Liège wrote also: \"The historians and the economists say that Belgium was the second industrial power of the world, in proportion to its population and its territory [...] But this rank is the one of Wallonia where the coal-mines, the blast furnaces, the iron and zinc factories, the wool industry, the glass industry, the weapons industry... were concentrated.\" \n\nWallonia was also the birthplace of a strong Socialist party and strong trade-unions in a particular sociological landscape. At the left, the \"Sillon industriel\", which runs from Mons in the west, to Verviers in the east (except part of North Flanders, in another period of the industrial revolution, after 1920). Even if Belgium is the second industrial country after Britain, the effect of the industrial revolution there was very different. In 'Breaking stereotypes', Muriel Neven and Isabelle Devious say:\n\nThe industrial revolution changed a mainly rural society into an urban one, but with a strong contrast between northern and southern Belgium. During the Middle Ages and the Early Modern Period, Flanders was characterised by the presence of large urban centres [...] at the beginning of the nineteenth century this region (Flanders), with an urbanisation degree of more than 30 per cent, remained one of the most urbanised in the world. By comparison, this proportion reached only 17 per cent in Wallonia, barely 10 per cent in most West European countries, 16 per cent in France and 25 per cent in Britain. Nineteenth century industrialisation did not affect the traditional urban infrastructure, except in Ghent [...] Also, in Wallonia the traditional urban network was largely unaffected by the industrialisation process, even though the proportion of city-dwellers rose from 17 to 45 per cent between 1831 and 1910. Especially in the Haine, Sambre and Meuse valleys, between the Borinage and Liège, where there was a huge industrial development based on coal-mining and iron-making, urbanisation was fast. During these eighty years the number of municipalities with more than 5,000 inhabitants increased from only 21 to more than one hundred, concentrating nearly half of the Walloon population in this region. Nevertheless, industrialisation remained quite traditional in the sense that it did not lead to the growth of modern and large urban centres, but to a conurbation of industrial villages and towns developed around a coal-mine or a factory. Communication routes between these small centres only became populated later and created a much less dense urban morphology than, for instance, the area around Liège where the old town was there to direct migratory flows.\n\nThe industrial revolution in France followed a particular course as it did not correspond to the main model followed by other countries. Notably, most French historians argue France did not go through a clear \"take-off\". Instead, France's economic growth and industrialisation process was slow and steady through the 18th and 19th centuries. However, some stages were identified by Maurice Lévy-Leboyer:\n\nBased on its leadership in chemical research in the universities and industrial laboratories, Germany, which was unified in 1871, became dominant in the world's chemical industry in the late 19th century. At first the production of dyes based on aniline was critical.\n\nGermany's political disunity – with three dozen states – and a pervasive conservatism made it difficult to build railways in the 1830s. However, by the 1840s, trunk lines linked the major cities; each German state was responsible for the lines within its own borders. Lacking a technological base at first, the Germans imported their engineering and hardware from Britain, but quickly learned the skills needed to operate and expand the railways. In many cities, the new railway shops were the centres of technological awareness and training, so that by 1850, Germany was self-sufficient in meeting the demands of railroad construction, and the railways were a major impetus for the growth of the new steel industry. Observers found that even as late as 1890, their engineering was inferior to Britain's. However, German unification in 1870 stimulated consolidation, nationalisation into state-owned companies, and further rapid growth. Unlike the situation in France, the goal was support of industrialisation, and so heavy lines crisscrossed the Ruhr and other industrial districts, and provided good connections to the major ports of Hamburg and Bremen. By 1880, Germany had 9,400 locomotives pulling 43,000 passengers and 30,000 tons of freight, and pulled ahead of France\n\nDuring the period 1790–1815 Sweden experienced two parallel economic movements: an \"agricultural revolution\" with larger agricultural estates, new crops and farming tools and a commercialisation of farming, and a \"protoindustrialisation\", with small industries being established in the countryside and with workers switching between agricultural work in summer and industrial production in winter. This led to economic growth benefiting large sections of the population and leading up to a \"consumption revolution\" starting in the 1820s.\n\nDuring 1815–1850 the protoindustries developed into more specialised and larger industries. This period witnessed increasing regional specialisation with mining in Bergslagen, textile mills in Sjuhäradsbygden and forestry in Norrland. Several important institutional changes took place in this period, such as free and mandatory schooling introduced 1842 (as first country in the world), the abolition of the national monopoly on trade in handicrafts in 1846, and a stock company law in 1848.\n\nDuring 1850–1890, Sweden experienced a veritable explosion in export, dominated by crops, wood and steel. Sweden abolished most tariffs and other barriers to free trade in the 1850s and joined the gold standard in 1873.\n\nDuring 1890–1930, Sweden experienced the second industrial revolution. New industries developed with their focus on the domestic market: mechanical engineering, power utilities, papermaking and textile.\n\nThe industrial revolution began about 1870 as Meiji period leaders decided to catch up with the West. The government built railroads, improved roads, and inaugurated a land reform programme to prepare the country for further development. It inaugurated a new Western-based education system for all young people, sent thousands of students to the United States and Europe, and hired more than 3,000 Westerners to teach modern science, mathematics, technology, and foreign languages in Japan (Foreign government advisors in Meiji Japan).\n\nIn 1871, a group of Japanese politicians known as the Iwakura Mission toured Europe and the United States to learn western ways. The result was a deliberate state-led industrialisation policy to enable Japan to quickly catch up. The Bank of Japan, founded in 1882, used taxes to fund model steel and textile factories. Education was expanded and Japanese students were sent to study in the west.\n\nModern industry first appeared in textiles, including cotton and especially silk, which was based in home workshops in rural areas.\n\nDuring the late 18th an early 19th centuries when the UK and parts of Western Europe began to industrialise, the US was primarily an agricultural and natural resource producing and processing economy. The building of roads and canals, the introduction of steamboats and the building of railroads were important for handling agricultural and natural resource products in the large and sparsely populated country of the period.\n\nImportant American technological contributions during the period of the Industrial Revolution were the cotton gin and the development of a system for making interchangeable parts, the latter aided by the development of the milling machine in the US. The development of machine tools and the system of interchangeable parts were the basis for the rise of the US as the world's leading industrial nation in the late 19th century.\n\nOliver Evans invented an automated flour mill in the mid-1780s that used control mechanisms and conveyors so that no labour was needed from the time grain was loaded into the elevator buckets until flour was discharged into a wagon. This is considered to be the first modern materials handling system an important advance in the progress toward mass production.\n\nThe United States originally used horse-powered machinery for small scale applications such as grain milling, but eventually switched to water power after textile factories began being built in the 1790s. As a result, industrialisation was concentrated in New England and the Northeastern United States, which has fast-moving rivers. The newer water-powered production lines proved more economical than horse-drawn production. In the late 19th century steam-powered manufacturing overtook water-powered manufacturing, allowing the industry to spread to the Midwest.\n\nThomas Somers and the Cabot Brothers founded the Beverly Cotton Manufactory in 1787, the first cotton mill in America, the largest cotton mill of its era, and a significant milestone in the research and development of cotton mills in the future. This mill was designed to use horse power, but the operators quickly learned that the horse-drawn platform was economically unstable, and had economic losses for years. Despite the losses, the Manufactory served as a playground of innovation, both in turning a large amount of cotton, but also developing the water-powered milling structure used in Slater's Mill.\n\nIn 1793, Samuel Slater (1768–1835) founded the Slater Mill at Pawtucket, Rhode Island. He had learned of the new textile technologies as a boy apprentice in Derbyshire, England, and defied laws against the emigration of skilled workers by leaving for New York in 1789, hoping to make money with his knowledge. After founding Slater's Mill, he went on to own 13 textile mills. Daniel Day established a wool carding mill in the Blackstone Valley at Uxbridge, Massachusetts in 1809, the third woollen mill established in the US (The first was in Hartford, Connecticut, and the second at Watertown, Massachusetts.) The John H. Chafee Blackstone River Valley National Heritage Corridor retraces the history of \"America's Hardest-Working River', the Blackstone. The Blackstone River and its tributaries, which cover more than from Worcester, Massachusetts to Providence, Rhode Island, was the birthplace of America's Industrial Revolution. At its peak over 1100 mills operated in this valley, including Slater's mill, and with it the earliest beginnings of America's Industrial and Technological Development.\n\nMerchant Francis Cabot Lowell from Newburyport, Massachusetts memorised the design of textile machines on his tour of British factories in 1810. Realising that the War of 1812 had ruined his import business but that a demand for domestic finished cloth was emerging in America, on his return to the United States, he set up the Boston Manufacturing Company. Lowell and his partners built America's second cotton-to-cloth textile mill at Waltham, Massachusetts, second to the Beverly Cotton Manufactory. After his death in 1817, his associates built America's first planned factory town, which they named after him. This enterprise was capitalised in a public stock offering, one of the first uses of it in the United States. Lowell, Massachusetts, using of canals and 10,000 horsepower delivered by the Merrimack River, is considered by some as a major contributor to the success of the American Industrial Revolution. The short-lived utopia-like Waltham-Lowell system was formed, as a direct response to the poor working conditions in Britain. However, by 1850, especially following the Irish Potato Famine, the system had been replaced by poor immigrant labour.\n\nA major U.S. contribution to industrialization was the development of techniques to make interchangeable parts from metal. Precision metal machining techniques were developed by the U.S. Department of War to make interchangeable parts for small firearms. The development work took place at the Federal Arsenals at Springfield Armory and Harpers Ferry Armory. Techniques for precision machining using machine tools included using fixtures to hold the parts in proper position, jigs to guide the cutting tools and precision blocks and gauges to measure the accuracy. The milling machine, a fundamental machine tool, is believed to have been invented by Eli Whitney, who was a government contractor who built firearms as part of this program. Another important invention was the Blanchard lathe, invented by Thomas Blanchard. The Blanchard lathe, or pattern tracing lathe, was actually a shaper that could produce copies of wooden gun stocks. The use of machinery and the techniques for producing standardized and interchangeable parts became known as the American system of manufacturing.\n\nPrecision manufacturing techniques made it possible to build machines that mechanized the shoe industry. and the watch industry. The industrialisation of the watch industry started 1854 also in Waltham, Massachusetts, at the Waltham Watch Company, with the development of machine tools, gauges and assembling methods adapted to the micro precision required for watches.\n\n \nSteel is often cited as the first of several new areas for industrial mass-production, which are said to characterise a \"Second Industrial Revolution\", beginning around 1850, although a method for mass manufacture of steel was not invented until the 1860s, when Sir Henry Bessemer invented a new furnace which could convert molten pig iron into steel in large quantities. However, it only became widely available in the 1870s after the process was modified to produce more uniform quality. Bessemer steel was being displaced by the open hearth furnace near the end of the 19th century.\n\nThis Second Industrial Revolution gradually grew to include chemicals, mainly the chemical industries, petroleum (refining and distribution), and, in the 20th century, the automotive industry, and was marked by a transition of technological leadership from Britain to the United States and Germany.\n\nThe increasing availability of economical petroleum products also reduced the importance of coal and further widened the potential for industrialisation.\n\nA new revolution began with electricity and electrification in the electrical industries. The introduction of hydroelectric power generation in the Alps enabled the rapid industrialisation of coal-deprived northern Italy, beginning in the 1890s.\n\nBy the 1890s, industrialisation in these areas had created the first giant industrial corporations with burgeoning global interests, as companies like U.S. Steel, General Electric, Standard Oil and Bayer AG joined the railroad and ship companies on the world's stock markets.\n\nThe causes of the Industrial Revolution were complicated and remain a topic for debate, with some historians believing the Industrial Revolution was an outgrowth of social and institutional changes brought by the end of feudalism in Britain after the English Civil War in the 17th century. The Enclosure movement and the British Agricultural Revolution made food production more efficient and less labour-intensive, forcing the farmers who could no longer be self-sufficient in agriculture into cottage industry, for example weaving, and in the longer term into the cities and the newly developed factories. The colonial expansion of the 17th century with the accompanying development of international trade, creation of financial markets and accumulation of capital are also cited as factors, as is the scientific revolution of the 17th century. A change in marrying patterns to getting married later made people able to accumulate more human capital during their youth, thereby encouraging economic development.\n\nUntil the 1980s, it was universally believed by academic historians that technological innovation was the heart of the Industrial Revolution and the key enabling technology was the invention and improvement of the steam engine. However, recent research into the Marketing Era has challenged the traditional, supply-oriented interpretation of the Industrial Revolution.\n\nLewis Mumford has proposed that the Industrial Revolution had its origins in the Early Middle Ages, much earlier than most estimates. He explains that the model for standardised mass production was the printing press and that \"the archetypal model for the industrial era was the clock\". He also cites the monastic emphasis on order and time-keeping, as well as the fact that medieval cities had at their centre a church with bell ringing at regular intervals as being necessary precursors to a greater synchronisation necessary for later, more physical, manifestations such as the steam engine.\n\nThe presence of a large domestic market should also be considered an important driver of the Industrial Revolution, particularly explaining why it occurred in Britain. In other nations, such as France, markets were split up by local regions, which often imposed tolls and tariffs on goods traded among them. Internal tariffs were abolished by Henry VIII of England, they survived in Russia until 1753, 1789 in France and 1839 in Spain.\n\nGovernments' grant of limited monopolies to inventors under a developing patent system (the Statute of Monopolies in 1623) is considered an influential factor. The effects of patents, both good and ill, on the development of industrialisation are clearly illustrated in the history of the steam engine, the key enabling technology. In return for publicly revealing the workings of an invention the patent system rewarded inventors such as James Watt by allowing them to monopolise the production of the first steam engines, thereby rewarding inventors and increasing the pace of technological development. However, monopolies bring with them their own inefficiencies which may counterbalance, or even overbalance, the beneficial effects of publicising ingenuity and rewarding inventors. Watt's monopoly prevented other inventors, such as Richard Trevithick, William Murdoch, or Jonathan Hornblower, whom Boulton and Watt sued, from introducing improved steam engines, thereby retarding the spread of steam power.\n\nOne question of active interest to historians is why the Industrial Revolution occurred in Europe and not in other parts of the world in the 18th century, particularly China, India, and the Middle East (which pioneered in shipbuilding, textile production, water mills, and much more in the period between 750 and 1100), or at other times like in Classical Antiquity or the Middle Ages. A recent account argued that Europeans have been characterized for thousands of years by a freedom-loving culture originating from the aristocratic societies of early Indo-European invaders. Many historians, however, have challenged this explanation as being not only Eurocentric, but also ignoring historical context. In fact, before the Industrial Revolution, \"there existed something of a global economic parity between the most advanced regions in the world economy.\" These historians have suggested a number of other factors, including education, technological changes (see Scientific Revolution in Europe), \"modern\" government, \"modern\" work attitudes, ecology, and culture.\n\nChina was the world's most technologically advanced country for many centuries; however, China stagnated economically and technologically and was surpassed by Western Europe before the Age of Exploration, by which time China banned imports and denied entry to foreigners. China was also a totalitarian society. Modern estimates of per capita income in Western Europe in the late 18th century are of roughly 1,500 dollars in purchasing power parity (and Britain had a per capita income of nearly 2,000 dollars) whereas China, by comparison, had only 450 dollars. India was essentially feudal, politically fragmented and not as economically advanced as Western Europe.\n\nHistorians such as David Landes and Max Weber credit the different belief systems in Asia and Europe with dictating where the revolution occurred. The religion and beliefs of Europe were largely products of Judaeo-Christianity and Greek thought. Conversely, Chinese society was founded on men like Confucius, Mencius, Han Feizi (Legalism), Lao Tzu (Taoism), and Buddha (Buddhism), resulting in very different worldviews. Other factors include the considerable distance of China's coal deposits, though large, from its cities as well as the then unnavigable Yellow River that connects these deposits to the sea.\n\nRegarding India, the Marxist historian Rajani Palme Dutt said: \"The capital to finance the Industrial Revolution in India instead went into financing the Industrial Revolution in Britain.\" In contrast to China, India was split up into many competing kingdoms after the decline of the Mughal Empire, with the major ones in its aftermath including the Marathas, Sikhs, Bengal Subah, and Kingdom of Mysore. In addition, the economy was highly dependent on two sectors – agriculture of subsistence and cotton, and there appears to have been little technical innovation. It is believed that the vast amounts of wealth were largely stored away in palace treasuries by totalitarian monarchs prior to the British take over.\n\nEconomic historian Joel Mokyr has argued that political fragmentation (the presence of a large number of European states) made it possible for heterodox ideas to thrive, as entrepreneurs, innovators, ideologues and heretics could easily flee to a neighboring state in the event that the one state would try to suppress their ideas and activities. This is what set Europe apart from the technologically advanced, large unitary empires such as China and India by providing \"an insurance against economic and technological stagnation\". China had both a printing press and movable type, and India had similar levels scientific and technological achievement as Europe in 1700, yet the Industrial Revolution would occur in Europe, not China or India. In Europe, political fragmentation was coupled with an \"integrated market for ideas\" where Europe's intellectuals used the lingua franca of Latin, had a shared intellectual basis in Europe's classical heritage and the pan-European institution of the Republic of Letters.\n\nIn addition, Europe's monarchs desperately needed revenue, pushing them into alliances with their merchant classes. Small groups of merchants were granted monopolies and tax-collecting responsibilities in exchange for payments to the state. Located in a region \"at the hub of the largest and most varied network of exchange in history,\" Europe advanced as the leader of the Industrial Revolution. In the Americas, Europeans found a windfall of silver, timber, fish, and maize, leading historian Peter Stearns to conclude that \"Europe's Industrial Revolution stemmed in great part from Europe's ability to draw disproportionately on world resources.\"\n\nGreat Britain provided the legal and cultural foundations that enabled entrepreneurs to pioneer the Industrial Revolution. Key factors fostering this environment were: (1) The period of peace and stability which followed the unification of England and Scotland; (2) no trade barriers between England and Scotland; (3) the rule of law (enforcing property rights and respecting the sanctity of contracts); (4) a straightforward legal system that allowed the formation of joint-stock companies (corporations); (5) absence of tolls, which had largely disappeared from Britain by the 15th century, but were an extreme burden on goods elsewhere in the world, and (6) a free market (capitalism).\n\nGeographical and natural resource advantages of Great Britain were the fact that it had extensive coastlines and many navigable rivers in an age where water was the easiest means of transportation and having the highest quality coal in Europe.\n\nThere were two main values that really drove the Industrial Revolution in Britain. These values were self-interest and an entrepreneurial spirit. Because of these interests, many industrial advances were made that resulted in a huge increase in personal wealth and a consumer revolution. These advancements also greatly benefitted the British society as a whole. Countries around the world started to recognise the changes and advancements in Britain and use them as an example to begin their own Industrial Revolutions.\n\nThe debate about the start of the Industrial Revolution also concerns the massive lead that Great Britain had over other countries. Some have stressed the importance of natural or financial resources that Britain received from its many overseas colonies or that profits from the British slave trade between Africa and the Caribbean helped fuel industrial investment. However, it has been pointed out that slave trade and West Indian plantations provided only 5% of the British national income during the years of the Industrial Revolution. Even though slavery accounted for so little, Caribbean-based demand accounted for 12% of Britain's industrial output.\nInstead, the greater liberalisation of trade from a large merchant base may have allowed Britain to produce and use emerging scientific and technological developments more effectively than countries with stronger monarchies, particularly China and Russia. Britain emerged from the Napoleonic Wars as the only European nation not ravaged by financial plunder and economic collapse, and having the only merchant fleet of any useful size (European merchant fleets were destroyed during the war by the Royal Navy). Britain's extensive exporting cottage industries also ensured markets were already available for many early forms of manufactured goods. The conflict resulted in most British warfare being conducted overseas, reducing the devastating effects of territorial conquest that affected much of Europe. This was further aided by Britain's geographical position – an island separated from the rest of mainland Europe.\nAnother theory is that Britain was able to succeed in the Industrial Revolution due to the availability of key resources it possessed. It had a dense population for its small geographical size. Enclosure of common land and the related agricultural revolution made a supply of this labour readily available. There was also a local coincidence of natural resources in the North of England, the English Midlands, South Wales and the Scottish Lowlands. Local supplies of coal, iron, lead, copper, tin, limestone and water power, resulted in excellent conditions for the development and expansion of industry. Also, the damp, mild weather conditions of the North West of England provided ideal conditions for the spinning of cotton, providing a natural starting point for the birth of the textiles industry.\n\nThe stable political situation in Britain from around 1688 following the Glorious Revolution, and British society's greater receptiveness to change (compared with other European countries) can also be said to be factors favouring the Industrial Revolution. Peasant resistance to industrialisation was largely eliminated by the Enclosure movement, and the landed upper classes developed commercial interests that made them pioneers in removing obstacles to the growth of capitalism. (This point is also made in Hilaire Belloc's \"The Servile State\".)\n\nThe French philosopher Voltaire wrote about capitalism and religious tolerance in his book on English society, \"Letters on the English\" (1733), noting why England at that time was more prosperous in comparison to the country's less religiously tolerant European neighbours. \"Take a view of the Royal Exchange in London, a place more venerable than many courts of justice, where the representatives of all nations meet for the benefit of mankind. There the Jew, the Mahometan [Muslim], and the Christian transact together, as though they all professed the same religion, and give the name of infidel to none but bankrupts. There the Presbyterian confides in the Anabaptist, and the Churchman depends on the Quaker's word. If one religion only were allowed in England, the Government would very possibly become arbitrary; if there were but two, the people would cut one another's throats; but as there are such a multitude, they all live happy and in peace.\"\n\nBritain's population grew 280% 1550–1820, while the rest of Western Europe grew 50–80%. Seventy percent of European urbanisation happened in Britain 1750–1800. By 1800, only the Netherlands was more urbanised than Britain. This was only possible because coal, coke, imported cotton, brick and slate had replaced wood, charcoal, flax, peat and thatch. The latter compete with land grown to feed people while mined materials do not. Yet more land would be freed when chemical fertilisers replaced manure and horse's work was mechanised. A workhorse needs for fodder while even early steam engines produced four times more mechanical energy.\n\nIn 1700, 5/6 of coal mined worldwide was in Britain, while the Netherlands had none; so despite having Europe's best transport, most urbanised, well paid, literate people and lowest taxes, it failed to industrialise. In the 18th century, it was the only European country whose cities and population shrank. Without coal, Britain would have run out of suitable river sites for mills by the 1830s.\n\nEconomic historian Robert Allen has argued that high wages, cheap capital and very cheap energy in Britain made it the ideal place for the industrial revolution to occur. These factors made it vastly more profitable to invest in research and development, and to put technology to use in Britain than other societies. However, two 2018 studies in \"The Economic History Review\" showed that wages were not particularly high in the British spinning sector or the construction sector, casting doubt on Allen's explanation.\n\nKnowledge of innovation was spread by several means. Workers who were trained in the technique might move to another employer or might be poached. A common method was for someone to make a study tour, gathering information where he could. During the whole of the Industrial Revolution and for the century before, all European countries and America engaged in study-touring; some nations, like Sweden and France, even trained civil servants or technicians to undertake it as a matter of state policy. In other countries, notably Britain and America, this practice was carried out by individual manufacturers eager to improve their own methods. Study tours were common then, as now, as was the keeping of travel diaries. Records made by industrialists and technicians of the period are an incomparable source of information about their methods.\n\nAnother means for the spread of innovation was by the network of informal philosophical societies, like the Lunar Society of Birmingham, in which members met to discuss 'natural philosophy' (\"i.e.\" science) and often its application to manufacturing. The Lunar Society flourished from 1765 to 1809, and it has been said of them, \"They were, if you like, the revolutionary committee of that most far reaching of all the eighteenth century revolutions, the Industrial Revolution\". Other such societies published volumes of proceedings and transactions. For example, the London-based Royal Society of Arts published an illustrated volume of new inventions, as well as papers about them in its annual \"Transactions\".\n\nThere were publications describing technology. Encyclopaedias such as Harris's \"Lexicon Technicum\" (1704) and Abraham Rees's \"Cyclopaedia\" (1802–1819) contain much of value. \"Cyclopaedia\" contains an enormous amount of information about the science and technology of the first half of the Industrial Revolution, very well illustrated by fine engravings. Foreign printed sources such as the \"Descriptions des Arts et Métiers\" and Diderot's \"Encyclopédie\" explained foreign methods with fine engraved plates.\n\nPeriodical publications about manufacturing and technology began to appear in the last decade of the 18th century, and many regularly included notice of the latest patents. Foreign periodicals, such as the \"Annales des Mines\", published accounts of travels made by French engineers who observed British methods on study tours.\n\nAnother theory is that the British advance was due to the presence of an entrepreneurial class which believed in progress, technology and hard work. The existence of this class is often linked to the Protestant work ethic (see Max Weber) and the particular status of the Baptists and the dissenting Protestant sects, such as the Quakers and Presbyterians that had flourished with the English Civil War. Reinforcement of confidence in the rule of law, which followed establishment of the prototype of constitutional monarchy in Britain in the Glorious Revolution of 1688, and the emergence of a stable financial market there based on the management of the national debt by the Bank of England, contributed to the capacity for, and interest in, private financial investment in industrial ventures.\n\nDissenters found themselves barred or discouraged from almost all public offices, as well as education at England's only two universities at the time (although dissenters were still free to study at Scotland's four universities). When the restoration of the monarchy took place and membership in the official Anglican Church became mandatory due to the Test Act, they thereupon became active in banking, manufacturing and education. The Unitarians, in particular, were very involved in education, by running Dissenting Academies, where, in contrast to the universities of Oxford and Cambridge and schools such as Eton and Harrow, much attention was given to mathematics and the sciences – areas of scholarship vital to the development of manufacturing technologies.\n\nHistorians sometimes consider this social factor to be extremely important, along with the nature of the national economies involved. While members of these sects were excluded from certain circles of the government, they were considered fellow Protestants, to a limited extent, by many in the middle class, such as traditional financiers or other businessmen. Given this relative tolerance and the supply of capital, the natural outlet for the more enterprising members of these sects would be to seek new opportunities in the technologies created in the wake of the scientific revolution of the 17th century.\n\nDuring the Industrial Revolution an intellectual and artistic hostility towards the new industrialisation developed, associated with the Romantic movement. Romanticism revered the traditionalism of rural life and recoiled against the upheavals caused by industrialization, urbanization and the wretchedness of the working classes. Its major exponents in English included the artist and poet William Blake and poets William Wordsworth, Samuel Taylor Coleridge, John Keats, Lord Byron and Percy Bysshe Shelley. The movement stressed the importance of \"nature\" in art and language, in contrast to \"monstrous\" machines and factories; the \"Dark satanic mills\" of Blake's poem \"And did those feet in ancient time\". Mary Shelley's novel \"Frankenstein\" reflected concerns that scientific progress might be two-edged. French Romanticism likewise was highly critical of industry.\n\n\n\n"}
{"id": "1204141", "url": "https://en.wikipedia.org/wiki?curid=1204141", "title": "Industrial civilization", "text": "Industrial civilization\n\nIndustrial civilization refers to the state of civilization following the Industrial Revolution, characterised by widespread use of powered machines. The transition of an individual region from pre-industrial society into an industrial society is referred to as the process of industrialisation, which may occur in different regions of the world at different times. Individual regions may specialise further as the civilisation continues to advance, resulting in some regions transitioning to a service economy, or information society, or post-industrial society (these are still dependent on industry, but allow individuals to move out of manufacturing jobs). The present era is sometimes referred to as the Information Age . De-industrialization of a region may occur for a range of reasons.\n\nIndustrial civilization has allowed a significant growth both in world population, thanks to mechanised agriculture and advances in modern medicine, and in the standard of living.\n\nSuch a civilization is mostly dependent on fossil fuel, with efforts underway to find alternatives for energy production. Some areas have exhibited de-industrialization as certain industries go into decline, or are superseded.\n\n\"Industrial civilization\" refers to the broader state of civilization, which spans multiple societies; industrial society just to specific segments (within the civilization) dependant on manufacturing jobs, whilst industrial civilisation as a whole involves many regions interdependent (via international trade) specialized in different ways, including information society and service economy. Note that these societies are still dependant on industrial civilization for their goods, and food imports coming from mechanised agriculture.\n\nThe \"industrial revolution\" is the historical event that ushered in industrial civilization. The modern world has evolved further following development in mass production and information technology (allowing service economy, and information society).\n\n\"Industrialisation\" is the process of any individual area being transformed. Industrial civilisation as a whole may have regions that still benefit from industrial societies, without being industrialised themselves, or having specialised in other ways (e.g. service economies).\n"}
{"id": "2329664", "url": "https://en.wikipedia.org/wiki?curid=2329664", "title": "Jean Leclerc (theologian)", "text": "Jean Leclerc (theologian)\n\nJean Le Clerc, also Johannes Clericus (March 19, 1657 in Geneva – January 8, 1736 in Amsterdam), was a Genevan theologian and biblical scholar. He was famous for promoting exegesis, or critical interpretation of the Bible, and was a radical of his age. He parted with Calvinism over his interpretations and left Geneva for that reason.\n\nHis father, Stephen Le Clerc, was professor of Greek in Geneva. The family originally belonged to the neighborhood of Beauvais in France, and several of its members acquired some name in literature. Jean Le Clerc applied himself to the study of philosophy under Jean-Robert Chouet (1642-1731) the Cartesian, and attended the theological lectures of Philippe Mestrezat, François Turrettini and Louis Tronchin () (1629-1705). In 1678-1679 he spent some time in Grenoble as tutor in a private family; on his return to Geneva he passed his examinations and received ordination. Soon afterwards he went to Saumur.\n\nIn 1682 he went to London, where he remained for six months, preaching on alternate Sundays in the Walloon church and in the Savoy Chapel. Due to political instability, he moved to Amsterdam, where he was introduced to John Locke and to Philipp van Limborch, professor at the Remonstrant college. He later included Locke in the journals he edited; and the acquaintance with Limborch soon ripened into a close friendship, which strengthened his preference for the Remonstrant theology, already favorably known to him by the writings of his grand-uncle, Stephan Curcellaeus (d. 1645) and by those of Simon Episcopius.\n\nA last attempt to live at Geneva, made at the request of relatives there, satisfied him that the theological atmosphere was uncongenial, and in 1684 he finally settled in Amsterdam, first as a moderately successful preacher, until ecclesiastical jealousy reportedly shut him out from that career, and afterwards as professor of philosophy, belles-lettres and Hebrew in the Remonstrant seminary. This appointment, which he owed to Limborch, he held from 1684, and in 1725 on the death of his friend he was called to occupy the chair of church history also.\n\nApart from literary work, Le Clerc's life at Amsterdam was uneventful. In 1691 he married a daughter of Gregorio Leti. From 1728 onward he was subject to repeated strokes of paralysis, and he died 8 years later, on 8 January.\n\nHis suspected Socinianism was the cause, it is said, of his exclusion from the chair of dogmatic theology.\n\nIn 1679 in Saumur were published \"Liberii de Sancto Amore Epistolae Theologicae\" (Irenopoli: Typis Philalethianis), usually attributed to Leclerc. They deal with the doctrine of the Trinity, the Hypostatic union of the two natures in Jesus Christ, original sin, and other topics, in a manner unorthodox for the period. In 1685 he published with Charles Le Cène \"Entretiens sur diverses matières de théologie\".\n\nIn 1685 he published \"Sentimens de quelques theologiens de Hollande sur l'histoire critique du Vieux Testament composée par le P. Richard Simon\", in which, while pointing out what he believed to be Richard Simon's faults, he advanced views of his own. These included: arguments against the Mosaic authorship of the Pentateuch; his views as to the manner in which the five books were composed; and his opinions on the subject of divine inspiration in general, in particular on the Book of Job, Book of Proverbs, Ecclesiastes, and Canticles. Simon's \"Réponse\" (1686) drew from Le Clerc a \"Defence des sentimens\" in the same year, which was followed by a new \"Réponse\" (1687).\n\nIn 1692 appeared his \"Logica sive Ars Ratiocinandi\", and also \"Ontologia et Pneumatologia\"; these, with the \"Physica sive de rebus corporeis\" (1696), are incorporated with the \"Opera Philosophica\", which have passed through several editions. In his \"Logica\", Le Clerc rewrites the Catholic Port-Royal \"Logique\" from a protestant Remonstrant perspective and supplements the \"Logique\" with analyses taken from \"Essay\" of his friend, John Locke.\nIn turn, Charles Gildon published a partial and unattributed translation of Le Clerc's \"Logica\" as the treatise \"Logic; or, The Art of Reasoning\" in the second (1712) and subsequent editions of John Brightland's \"Grammar of the English Tongue\".\nIn 1728, Ephraim Chambers used Gildon's translation of Le Clerc's version of the Port-Royal \"Logique\" as one of his sources when he compiled his \"Cyclopaedia\". John Mills and Gottfried Sellius later translated Chambers's \"Cyclopaedia\" into French. Their translation was appropriated by Denis Diderot and Jean le Rond d'Alembert as the starting point for their Encyclopédie. \nIn particular, the article on définition (1754) in the \"Encyclopédie\" can be traced through this chain of writers, editors, translators, and compilers to the Port-Royal \"Logique\" through the \"Logica\" of Jean Le Clerc.\n\nIn 1693 his series of Biblical commentaries began with that on the Book of Genesis; the series was not completed until 1731. The portion relating to the New Testament books included the paraphrase and notes of Henry Hammond. Le Clerc's commentary challenged traditional views and argued the case for inquiry into the origin and meaning of the biblical books, It was hotly attacked on all sides.\n\nHis \"Ars Critica\" appeared in 1696, and, in continuation, \"Epistolae Criticae et Ecclesiasticae\" in 1700. Le Clerc produced a new edition of the Apostolic Fathers of Cotelerius (Jean-Baptiste Cotelier, 1627-1686), published in 1698. He also edited journals of book notices and reviews: the \"Bibliothèque universelle et historique\" (Amsterdam, 25 vols, 1686-1693), begun with J. C. de la Croze; the \"Bibliothèque choisie\" (Amsterdam, 28 vols, 1703-1713); and the \"Bibliothèque ancienne et moderne\", (29 vols, 1714-1726).\n\nOther works were Le Clerc's \"Parrhasiana ou, Pensées diverses sur des matiéres de critique, d'histoire, de morale et de politique avec la défense de divers ouvrages de M. L. C. par Théodore Parrhase\" (Amsterdam, 1699); and \"Vita et opera ad annum MDCCXI, amici ejus opusculum, philosophicis Clerici operibus subjiciendum\", also attributed to himself. The supplement to Hammond's notes was translated into English in 1699, \"Parrhasiana, or Thoughts on Several Subjects\", in 1700, the Harmony of the Gospels in 1701, and \"Twelve Dissertations\" out of 211. Other works include the collected works of Erasmus, begun in 1703, and \"Harmonia evangelica\", 1700. One of his final works was his three-volume \"Histoire des Provinces-Unies des Pays Bas\", covering the history of the Dutch Republic up to the 1713 Treaty of Utrecht and published between 1723 and 1728.\n\n\n"}
{"id": "57205935", "url": "https://en.wikipedia.org/wiki?curid=57205935", "title": "Josephine Talamantez", "text": "Josephine Talamantez\n\nJosephine \"Josie\" Talamantez is a historian from San Diego, California. She co-founded Chicano Park in 1970 and helped develop it into a cultural National Historic Landmark containing the largest collection of artistic murals in the United States. Talamantez was also the Chief of Programs for the California Arts Council, served as the director of the Centro Cultural de la Raza, and was on the board of the National Association of Latino Arts and Culture. \n\nJosephine Talamantez was born and raised in the Logan Heights neighborhood of San Diego. She attended San Diego High School and received a B.A. in Sociology from University of California, Berkeley. She received a master's degree in public history from California State University, Sacramento. Her grandmother moved to Logan Heights in the early 20th century, and Talamantez is the third generation to live in that area. \n\nTalamantez was the Chief of Programs for the California Arts Council from 1987-2011. She also served as the executive director of the Centro Cultural de la Raza, and was on the board of the National Association of Latino Arts and Culture.\n\nIn 1970, the California Highway Patrol announced they were planning to build a substation below the Coronado bridge, replacing a potential park area that was there. Local residents, including Talamantez, responded by staging a 12-day occupation and successfully demanded that the space be allowed to remain a park. Talamantez, who was 18 years old and a student at San Diego City College at the time, helped found the Chicano Park Steering Committee to negotiate with officials on behalf of the park.\n\nIn 1973, Talamantez and others in the Steering Committee started discussing the addition of Mexican-American artwork to the park. Over the next 20 years, many pieces of artwork were added, including more than 70 murals by Chicano artists from across California, making the park the largest collection of murals in the United States. Other works of art in the park include sculptures, earthworks, and an architectural piece dedicated to the cultural heritage of the community.\n\nTalamantez is currently working towards opening a Chicano Park Museum and Cultural Center inside a nearby city-owned building that used to house the Cesar Chavez Continuing Education Center.\n\nTalamantez and Manny Galaviz submitted the proposal that successfully added Chicano Park to the National Register of Historic Places in 2013 due to its association with the Chicano Movement. \n\nIn 1997, Talamantez began the process of placing Chicano Park with its artwork and murals on the National Register in order to prevent the city from damaging the murals while retrofitting Coronado Bridge. Years later, Talamantez traveled to Washington, D.C. to make a case before a national review and advisory committee, and Chicano Park was successfully designated as a National Historic Landmark in December 2016.\n"}
{"id": "10046963", "url": "https://en.wikipedia.org/wiki?curid=10046963", "title": "Landscape history", "text": "Landscape history\n\nLandscape history is the study of the way in which humanity has changed the physical appearance of the environment – both present and past. It is sometimes referred to as landscape archaeology. It was first recognised as a separate area of study during the 20th century and uses evidence and approaches from other disciplines including archaeology, architecture, ecology, aerial photography, local history and historical geography.\n\nIn England, landscape history emerged as an academic discipline following the publication of \"The Making of the English Landscape\" by W. G. Hoskins in 1955, although some topics that are now considered part of landscape history had been identified earlier. Darby, for example, gives many early examples of regional characterisation of landscapes.\n\nFollowing Hoskins, landscape history expanded in various directions. There are published landscape histories of a number of English counties. Other authors have studied the landscape at earlier periods. One productive avenue has been the study of specific landscape features such as fields, villages, and so on. Managed woodland has been extensively studied by Oliver Rackham.\n\nThe scope of landscape history ranges from specific individual features to areas covering hundreds of square miles. Topics studied by landscape historians include: \n\nTwo complementary approaches can be used to study landscape history - fieldwork and desk research. Fieldwork involves physical inspection of the landscape to identify earthworks and other potential features. Documentary desk research involves finding references to landscape features in primary and secondary sources. Among the most useful documentary sources are maps. Modern aerial photographs are useful for identifying large-scale features; earlier aerial photographs may show features that have now been lost.\n\nThe origin of features can often be related to the geology and ecology of the area being studied - for example the importance of springs and the suitability of the soil for different forms of agriculture.\n\nThe presence of indicator species can be used to identify previous land use, for example bluebells suggesting ancient woodland, particularly in the East of England and Lincolnshire.\n\nLandscape features can also indicate earlier land usage. For example, a red hill in a coastal area is an indication of salt production.\n\nThe historic landscape characterisation programme initiated by English Heritage provides a framework for standardising and recording information about landscape history, particularly to support the planning authorities.\n\nFew universities have a department of landscape history. Academic landscape historians are typically found within departments of archaeology, history, local history or continuing education. For example, Nick Higham at Manchester has the title Professor in Early Medieval and Landscape History in the school of history. Landscape history courses are typically post-graduate or extra mural. As a result, much of the work in landscape history is undertaken by amateurs (although often supervised by professionals in landscape studies).\n\n\"Landscape History\" is the name of a refereed journal published by the Society for Landscape Studies.\n\n"}
{"id": "4285595", "url": "https://en.wikipedia.org/wiki?curid=4285595", "title": "List of Antarctic expeditions", "text": "List of Antarctic expeditions\n\nThis list of Antarctic expeditions is a chronological list of expeditions involving Antarctica. Although the existence of a southern continent had been hypothesized as early as the writings of Ptolemy in the 1st century AD, the South Pole was not reached until 1911.\n\n\n\n\n\n\n\n\n\n"}
{"id": "9547941", "url": "https://en.wikipedia.org/wiki?curid=9547941", "title": "List of Muslim states and dynasties", "text": "List of Muslim states and dynasties\n\n(This article lists some of the states, empires, or dynasties that were ruled by a Muslim elite, or which were in some way central to or a part of a Muslim empire.) \n\nAll the disparate Islamic Empires can all be traced back to Muhammad, as the founder not only of the Islamic faith but also the first leader of the Muslim people. \n\nScholars debate what exactly constitutes an empire. One definition defines an empire as a state that extends dominion over areas and populations culturally and ethnically distinct from the culture/ethnicity at the center of power. \n\nThe early Muslim conquests began in the lifetime of Muhammad. His successors conquered large swaths of the Middle East and North Africa, in addition to parts of southern Europe and the Indian subcontinent, in the decades after his death. The caliphate founded by his earliest successors, called the Rashidun caliphate, was succeeded by the Umayyad caliphate and later the Abbasid caliphate.\n\nWhile the caliphates gradually fractured and fell, other Muslim dynasties rose; some of these dynasties grew into \"Islamic empires\", with some of the most notable being the Safavid, Ottoman, and Mughal Empires.\n\n\n\"Grouped by capital or core region\"\n\n\n\n\n\n\nSouthern Europe\nEastern Europe\n\n\n\n\nMalay Archipelago (East Indies) (Indonesia,Malaysia and Brunei) \n\n\n"}
{"id": "5634", "url": "https://en.wikipedia.org/wiki?curid=5634", "title": "List of centuries", "text": "List of centuries\n\nThe pages listed below contain information about trends and events in particular centuries and millennia.\n\n"}
{"id": "13316", "url": "https://en.wikipedia.org/wiki?curid=13316", "title": "List of historical anniversaries", "text": "List of historical anniversaries\n\nCondensed list of historical anniversaries.\n\n"}
{"id": "23563446", "url": "https://en.wikipedia.org/wiki?curid=23563446", "title": "List of largest funerals", "text": "List of largest funerals\n\n\n"}
{"id": "1072855", "url": "https://en.wikipedia.org/wiki?curid=1072855", "title": "Moorish Science Temple of America", "text": "Moorish Science Temple of America\n\nThe Moorish Science Temple of America is an American national and religious organization founded by Noble Drew Ali. He based it on the premise that African Americans are descendants of the Moabites and thus are Moorish by nationality, and Islamic by faith. Ali put together elements of major traditions to develop a message of personal transformation through historical education, racial pride and spiritual uplift. His doctrine was also intended to provide African Americans with a sense of identity in the world and to promote civic involvement.\n\nOne primary tenet of the Moorish Science Temple is the belief that African Americans are of Moorish ancestry, specifically from the \"Moroccan Empire.\" According to Ali, this area included other countries that today surround Morocco. To join the movement, individuals had to proclaim their \"Moorish nationality.\" They were given \"nationality cards.\" In religious texts, adherents refer to themselves racially as \"Asiatics,\" as the Middle East is also western Asia. Adherents of this movement are known as \"Moorish-American Moslems\" and are called \"Moorish Scientists\" in some circles.\n\nThe Moorish Science Temple of America was incorporated under the Illinois Religious Corporation Act 805 ILCS 110. Timothy Drew, known to its members as Prophet Noble Drew Ali, founded the Moorish Science Temple of America in 1913 in Newark, New Jersey, a booming industrial city. After some difficulties, Ali moved to Chicago, establishing a center there, as well as temples in other major cities. The movement expanded rapidly during the late 1920s. The quick expansion of the Moorish Science Temple arose in large part from the search for identity and context among black Americans at the time of the Great Migration to northern and midwestern cities, as they were becoming an urbanized people.\n\nCompeting factions developed among the congregations and leaders, especially after the death of the charismatic Ali. Three independent organizations developed from this ferment. The founding of the Nation of Islam by Wallace Fard Muhammad in 1930 also created competition for members. In the 1930s membership was estimated at 30,000, with one third in Chicago. During the postwar years, the Moorish Science Temple of America continued to increase in membership, albeit at a slower rate.\n\nTimothy Drew was believed to have been born on January 8, 1886 in North Carolina, United States. Sources differ as to his background and upbringing: one reports he was the son of two former slaves who was adopted by a tribe of Cherokee; another describes Drew as the son of a Moroccan Muslim father and a Cherokee mother. In 2014 an article in the online \"Journal of Race Ethnicity and Religion\" attempted to link Timothy Drew to one Thomas Drew, born January 8, 1886, using census records, a World War I draft card, and street directory records.\n\nDrew Ali reported that during his travels, he met with a high priest of Egyptian magic. In one version of Drew Ali's biography, the leader saw him as a reincarnation of the founder. In others, he claims that the priest considered him a reincarnation of Jesus, the Buddha, Muhammad and other religious prophets. According to the biography, the high priest trained Ali in mysticism and gave him a \"lost section\" of the Quran.\n\nThis text came to be known as the \"Holy Koran of the Moorish Science Temple of America\" (not to be confused with the Islamic \"Quran\"). It is also known as the \"Circle Seven Koran\" because of its cover, which features a red \"7\" surrounded by a blue circle. The first 19 chapters are from \"The Aquarian Gospel of Jesus the Christ,\" published in 1908 by esoteric Ohio preacher Levi Dowling. In \"The Aquarian Gospel\", Dowling described Jesus's supposed travels in India, Egypt, and Palestine during the years of his life which are not accounted for by the New Testament.\n\nChapters 20 through 45 are borrowed from the Rosicrucian work, \"Unto Thee I Grant\" with minor changes in style and wording. They are instructions on how to live, and the education and duties of adherents.\n\nDrew Ali wrote the last four chapters of the Circle Seven Koran himself. In these he wrote:\n\nDrew Ali and his followers used this material to claim, \"Jesus and his followers were Asiatic.\" (\"Asiatic\" was the term Drew Ali used for all dark or olive-colored people; he labeled all whites as European. He suggested that all Asiatics should be allied.)\n\nDrew Ali crafted Moorish Science from a variety of sources, a \"network of alternative spiritualities that focused on the power of the individual to bring about personal transformation through mystical knowledge of the divine within\". In the inter-war years in Chicago and other major cities, he used these concepts to preach racial pride and uplift. His approach appealed to thousands of African Americans who had left severely oppressive conditions in the South through the Great Migration and faced struggles in new urban environments.\n\nAli believed that African Americans were all Moors, whom he claimed were descended from the ancient Moabites (describing them as belonging to Northwest Africa as opposed to Moab as the name suggests). He claimed that Islam and its teachings are more beneficial to their earthly salvation, and that their 'true nature' had been 'withheld' from them. In the traditions he founded, male members of the Temple wear a fez or turban as head covering; women wear a turban.\n\nThey added the suffixes Bey or El to their surnames, to signify Moorish heritage as well as their taking on the new life as Moorish Americans. It was also a way to claim and proclaim a new identity over that lost to the slavery of their ancestors. These suffixes were a clear sign to others that while one's African tribal name may never be known to him/her, acceptance of European names given by slave masters were not theirs, either.\n\nAs Drew Ali began his version of teaching the Moorish-Americans to become better citizens, he made speeches like, \"A Divine Warning By the Prophet for the Nations\", in which he urged them to reject derogatory labels, such as \"Black\", \"colored\", and \"Negro\". He urged Americans of all races to reject hate and embrace love. He believed that Chicago would become a second Mecca.\n\nThe ushers of the Temple wore black fezzes. The leader of a particular temple was known as a Grand Sheik, or Governor. Noble Drew Ali had several wives. According to the \"Chicago Defender\", he claimed the power to marry and divorce at will.\n\nIn 1913, Drew Ali formed the Canaanite Temple in Newark, New Jersey. He left the city after agitating people with his views on race. Drew Ali and his followers migrated, while planting congregations in Philadelphia; Washington, D.C., and Detroit. Finally, Drew Ali settled in Chicago in 1925, saying the Midwest was \"closer to Islam.\" The following year he officially registered Temple No. 9.\n\nThere he instructed followers not to be confrontational but to build up their people to be respected. In this way, they might take their place in the United States by developing a cultural identity that was congruent with Drew Ali's beliefs on personhood. In the late 1920s, journalists estimated the Moorish Science Temple had 35,000 members in 17 temples in cities across the Midwest and upper South. It was reportedly studied and watched by the Chicago police.\n\nBuilding Moorish-American businesses was part of their program, and in that was similar to Marcus Garvey's Universal Negro Improvement Association and African Communities League and the later Nation of Islam. By 1928, members of the Moorish Science Temple of America had obtained some respectability within Chicago and Illinois, as they were featured prominently and favorably in the pages of the \"Chicago Defender\", an African American newspaper, and conspicuously collaborated with African American politician and businessman Daniel Jackson.\n\nDrew Ali attended the January 1929 inauguration of Louis L. Emmerson, as 27th Governor of Illinois in the state capital of Springfield. The \"Chicago Defender\" stated that his trip included \"interviews with many distinguished citizens from Chicago, who greeted him on every hand.\" With the growth in its population and membership, Chicago was established as the center of the movement.\n\nIn early 1929, following a conflict over funds, Claude Green-Bey, the business manager of Chicago Temple No. 1 split from the Moorish Science Temple of America. He declared himself Grand Sheik and took a number of members with him. On March 15, Green-Bey was stabbed to death at the Unity Hall of the Moorish Science Temple, on Indiana Avenue in Chicago.\n\nDrew Ali was out of town at the time, as he was dealing with former Supreme Grand Governor Lomax Bey (professor Ezaldine Muhammad), who had supported Green-Bey's attempted coup. When Drew Ali returned to Chicago, the police arrested him and other members of the community on suspicion of having instigated the killing. No indictment was sworn for Drew Ali at that time.\n\nShortly after his release by the police, Drew Ali died at age 43 at his home in Chicago on July 20, 1929. Although the exact circumstances of his death are unknown, the Certificate of Death stated that Noble Drew Ali died from \"tuberculosis broncho-pneumonia\". Despite the official report, many of his followers speculated that his death was caused by injuries from the police or from other members of the faith. Others thought it was due to pneumonia. One Moor told the \"Chicago Defender\", \"The Prophet was not ill; his work was done and he laid his head upon the lap of one of his followers and passed out.\"\n\nThe death of Drew Ali brought out a number of candidates to succeed him. Brother Edward Mealy El stated that he had been declared Drew Ali's successor by Drew Ali himself. In August, within a month of Drew Ali's death, John Givens El, Drew Ali's chauffeur, declared that he was Drew Ali reincarnated. He is said to have fainted while working on Drew Ali's automobile and \"the sign of the star and crescent [appeared] in his eyes\".\n\nAt the September Unity Conference, Givens again made his claim of reincarnation. However, the governors of the Moorish Science Temple of America declared Charles Kirkman Bey to be the successor to Drew Ali and named him Grand Advisor.\n\nWith the support of several temples each, Mealy El and Givens El both went on to lead separate factions of the Moorish Science Temple. All three factions (Kirkman Bey, Mealy El, and Givens El) are active today.\n\nOn September 25, 1929, Kirkman Bey's wife reported to the Chicago police his apparent kidnapping by one Ira Johnson. Accompanied by two Moorish Science members, the police visited the home of Johnson, when they were met by gunfire. The attack escalated into a shoot-out that spilled into the surrounding neighborhood. In the end, a policeman as well as a member were killed in the gun battle, and a second policeman later died of his wounds. The police took 60 people into police custody, and a reported 1000 police officers patrolled the Chicago South Side that evening. Johnson and two others were later convicted of murder.\n\nKirkman Bey went on to serve as Grand Advisor of one of the most important factions until 1959, when the reins were given to F. Nelson-Bey.\n\nThe community was further split when Wallace Fard Muhammad, known within the temple as David Ford El, also claimed (or was taken by some) to be the reincarnation of Drew Ali. When his leadership was rejected, Ford El broke away from the Moorish Science Temple. He moved to Detroit, where he formed his own group, an organization that would become the Nation of Islam. The Nation of Islam denied any historical connection with the Moorish Science Temple until February 26, 2014, when Louis Farrakhan acknowledged the contribution(s) of Noble Drew Ali to the Nation of Islam and their founding principles.\n\nDespite the turmoil and defections, the movement continued to grow in the 1930s. It is estimated that membership in the 1930s reached 30,000. There were major congregations in Philadelphia, Detroit, and Chicago.\n\nOne-third of the members, or 10,000, lived in Chicago, the center of the movement. There were congregations in numerous other cities where African Americans had migrated in the early 20th century. The group published several magazines: one was the \"Moorish Guide National\". During the 1930s and 1940s, continued surveillance by police (and later the FBI) caused the Moors to become more withdrawn and critical of the government.\n\nDuring the 1940s, the Moorish Science Temple (specifically the Kirkman Bey faction) came to the attention of the FBI, who investigated claims of members committing subversive activities by adhering to and spreading of Japanese propaganda. The investigation failed to find any substantial evidence, and the investigations were dropped. The federal agency later investigated the organization in 1953 for violation of the Selective Service Act of 1948 and sedition. In September 1953, the Department of Justice determined that prosecution was not warranted for the alleged violations. The file that the FBI created on the temple grew to 3,117 pages during its lifetime. They never found any evidence of any connection or much sympathy of the temple's members for Japan.\n\nIn 1976 Jeff Fort, leader of Chicago's Black P Stone Nation, announced at his parole from prison in 1976 that he had converted to Islam. Moving to Milwaukee, Fort associated himself with the Moorish Science Temple of America. It is unclear whether he officially joined or was instead rejected by its members.\n\nIn 1978, Fort returned to Chicago and changed the name of his gang to \"El Rukn\" (\"the foundation\" in Arabic), also known as \"Circle Seven El Rukn Moorish Science Temple of America\" and the \"Moorish Science Temple, El Rukn tribe\". Scholars are divided over the nature of the relationship, if any, between \"El Rukn\" and the Moorish Science Temple of America. Fort reportedly hoped that an apparent affiliation with a religious organization would discourage law enforcement.\n\nIn 1984 the Chicago congregation bought a building from Buddhist monks in Ukrainian Village, which continues to be used for Temple No. 9. Demographic and cultural changes have decreased the attraction of young people to the Moorish Science Temple. Only about 200 members attended a convention in 2007, rather than the thousands of the past. In the early 2000s, the temples in Chicago, Philadelphia, Detroit, and Washington, D.C. had about 200 members each, and many were older people.\n\nAn increasing number of people claiming to follow Moorish Science have filed false legal documents in various municipalities around the United States. The documents include fake liens, deeds, and property claims. The Moorish Science Temple has disavowed any affiliation with those filing the false documents, calling them \"radical and subversive fringe groups\".\n\n\n"}
{"id": "3402732", "url": "https://en.wikipedia.org/wiki?curid=3402732", "title": "Outline of history", "text": "Outline of history\n\nThe following outline is provided as an overview of and topical guide to history:\n\nHistory – discovery, collection, organization, and presentation of information about past events. History can also mean the period of time after writing was invented (the beginning of recorded history).\nHistory can be described as all of the following:\n\n\n\nAuxiliary sciences of history – scholarly disciplines which help evaluate and use historical sources and are seen as auxiliary for historical research. Auxiliary sciences of history include, but are not limited to:\n\n\nHistory by period\n\n\n\n\n\nRegional history\n\n\nEra\n\n\n\n\n\n\n\n\n\n\n\n\nHistorian \n\n\n\n"}
{"id": "2687740", "url": "https://en.wikipedia.org/wiki?curid=2687740", "title": "Presentism (literary and historical analysis)", "text": "Presentism (literary and historical analysis)\n\nIn literary and historical analysis, presentism is the anachronistic introduction of present-day ideas and perspectives into depictions or interpretations of the past. Some modern historians seek to avoid presentism in their work because they consider it a form of cultural bias, and believe it creates a distorted understanding of their subject matter. The practice of presentism is regarded by some as a common fallacy in historical writing. \n\nThe \"Oxford English Dictionary\" gives the first citation for \"presentism\" in its historiographic sense from 1916, and the word may have been used in this meaning as early as the 1870s. The historian David Hackett Fischer identifies presentism as a fallacy also known as the \"fallacy of \"nunc pro tunc\"\". He has written that the \"classic example\" of presentism was the so-called \"Whig history\", in which certain 18th- and 19th-century British historians wrote history in a way that used the past to validate their own political beliefs. This interpretation was presentist because it did not depict the past in objective historical context but instead viewed history only through the lens of contemporary Whig beliefs. In this kind of approach, which emphasizes the relevance of history to the present, things that do not seem relevant receive little attention, which results in a misleading portrayal of the past. \"Whig history\" or \"whiggishness\" are often used as synonyms for \"presentism\" particularly when the historical depiction in question is teleological or triumphalist.\n\nPresentism has a shorter history in sociological analysis, where it has been used to describe technological determinists who interpret a change in behavior as starting with the introduction of a new technology. For example, scholars such as Frances Cairncross proclaimed that the Internet had led to \"the death of distance\", but most community ties and many business ties had been transcontinental and even intercontinental for many years.\n\nPresentism is also a factor in the problematic question of history and moral judgments. Among historians, the orthodox view may be that reading modern notions of morality into the past is to commit the error of presentism. To avoid this, historians restrict themselves to describing what happened and attempt to refrain from using language that passes judgment. For example, when writing history about slavery in an era when the practice was widely accepted, letting that fact influence judgment about a group or individual would be presentist and thus should be avoided.\n\nCritics respond that to avoid moral judgments is to practice moral relativism, a controversial idea. Some religious historians argue that morality is timeless, having been established by God; they say it is not anachronistic to apply timeless standards to the past. (In this view, while mores may change, morality does not.)\n\nOthers argue that application of religious standards has varied over time as well. Saint Augustine, for example, holds that there exist timeless moral principles, but contends that certain practices (such as polygamy) were acceptable in the past because they were customary but now are neither customary nor acceptable.\n\nFischer, for his part, writes that while historians might not always manage to avoid the fallacy completely, they should at least try to be aware of their biases and write history in such a way that they do not create a distorted depiction of the past.\n\n\n"}
{"id": "768542", "url": "https://en.wikipedia.org/wiki?curid=768542", "title": "Restoration (TV series)", "text": "Restoration (TV series)\n\nRestoration was a set of BBC television series where viewers decided on which listed building that was in immediate need of remedial works was to win a grant from Heritage Lottery Fund. It first aired in 2003.\n\nThe host of all three series is Griff Rhys Jones, whilst investigating each building in the heats are the show's resident \"ruin detectives\", Marianne Suhr and Ptolemy Dean.\n\nThirty buildings featured in ten regional heats in 2003, with money raised from the telephone vote being added to the prize fund. Viewers chose which of a selection of the United Kingdom's most important, but neglected, buildings should be awarded a Heritage Lottery Grant of £3m. The winning building was the turkish-bath section of the Victoria Baths in Manchester; however, bureaucratic and technical hurdles meant that the money raised could not be spent immediately, and final planning-approval to begin a restoration process did not go through until September 2005. The first phase of restoration work finally began on 19 March 2007.\n\nKate Humble co-hosted the 2003 live grand final.\n\nA second series, featuring 21 buildings in 7 regional heats, appeared on BBC Two in the summer of 2004. The winner was the Old Grammar School and Saracen's Head in Kings Norton, Birmingham. Both buildings closed to the public in July 2006 for archeological investigation, restoration work began in February 2007. Both buildings were officially reopened on 13 June 2008.\n\nThe 2004 live Grand Final was co-hosted by Natasha Kaplinsky.\n\n\nOn 4 September 2005, Rhys Jones presented a programme, updating viewers as to the progress made by the featured buildings, or otherwise.\n\nA third series of nine programmes, presented by Griff Rhys Jones, began on BBC Two in August 2006. Entitled Restoration Village, the series focused on buildings in smaller settlements, using the same format and voting as before, featuring 21 buildings in 7 regional heats. Updates about previously featured buildings were also included. The winner of Restoration Village was Chedham's Yard, an early 19th-century blacksmith's yard.\n\nOn 22 April 2009, Rhys Jones presented \"Restoration Revisited\", a 60-minute programme updating viewers as to the progress made by some of the 72 featured buildings throughout the three TV series.\n\nThe Perfect Village was a companion series of architectural travelogues presented by Ptolemy Dean, and shown on BBC Four in 2006. The show chose twelve villages from all around the United Kingdom as illustrations of village life. In the final show Heighington in County Durham was chosen as the UK's \"perfect village\".\n\n\nThe Channel 4 programme, \"Demolition\", broadcast in December 2005, was an \"answer\" to \"Restoration\"; instead of voting for a building to be saved, viewers were asked to vote on which eyesore should be demolished.\n\n\n\n"}
{"id": "35122793", "url": "https://en.wikipedia.org/wiki?curid=35122793", "title": "Smihula waves", "text": "Smihula waves\n\nSmihula waves (or Smihula cycles, Smihula waves of technological revolutions, economic waves of technological revolutions) are long-term waves of technological progress which are reflected also in long-term economic waves. They are a crucial notion of Daniel Šmihula's theory of technological progress.\n\nThe Smihula's theory of waves of technological revolutions is based on the idea that the main technological innovations are introduced in society and the economy not continually but in specific waves, and the time spans of these waves is shortening due to technological progress.\n\nThe time period with the highest concentration of technological innovations is labeled as a \"technological revolution\"\nA period of technological revolution (an innovation phase) is associated with economic revival. When new but also already-proven and reliable technologies are available, the interest in new technological development temporary declines and investments are diverted from research to their maximal practical utilization. This period we can designate as an application phase. It is also associated with economic growth and perhaps even an economic boom. However, at a certain moment profitability (profit/price ratio) from new innovations and new sectors declines to the level acquired from older traditional sectors. Markets are saturated by technological products – (market saturation – everybody has a mobile phone, every small town has a railway station) and new capital investment in this originally new sector will not bring any above-average profit (e.g. the first railways connected the biggest cities with many potential passengers, later ones had ever smaller and smaller customer potential, and the level of profit from each new railway was therefore lower than from the previous one). At this moment economic stagnation and crisis begin – but a will to risk and to try something new emerges. The stagnation and crisis are therefore overcome by a new technological revolution with new innovations which will revitalize the economy. And this new technological revolution is the beginning of a new wave.\n\nThe internal structure of each long wave of technological innovations with economic implications is as follows:\n\na) innovation phase – technological revolution (an economic revival after the crisis from the end of a previous wave)\n\nb) application phase (an economic boom)\n\nc) saturation of economy and society with innovations, impossibility of further extensive growth (an economic crisis)\n\nIn Smihula theory technological revolutions are the main engine of economic development, and hence long-term economic cycles are dependent on these waves of technological innovation.\nSmihula identified during the modern age in society six waves of technological innovations begun by technological revolutions (one of them is a hypothetical revolution in the near future).\nUnlike other scholars he believed that it is possible to find similar technological revolutions and long-term economic waves dependant on them even in pre-modern ages. (This is the most original part of the Smihula's theory.)\n\nPre-modern technological waves:\n\nModern technological waves:\nTheory of Smihula waves of technological revolutions is popular among supporters of the long economic waves (e.g. Kondratieff cycles) and among scholars who believs that the economic crisis in 2007–2012 was a result of the technological stagnation.\n\nRussian sociologist A.A. Davydov believes that he even identified a specific mathematical formula for the lengths of Smihula waves which is based on the Fibonacci sequence.\n\nAs Smihula published his theory in the time of revived interest in long economic cycles and when a link between economic cycles and technological revolutions was generally accepted (e.g. in works of Carlota Perez), it did not evoke strong criticism or opposition.\nOn the other side it has the same problem as the other long-cycles theories – it is sometime hard to support them by exact data and the potential curve of a long time development is always modified by other short-time factors – therefore its course is always only a rather abstract reconstruction. Also the idea of concentration of the most important innovation in certain bordered periods seems to be very logical, but its verification depends on a very subjective definition of the \"most important\" innovations.\nSmihula's theory of long waves of technological innovations and economic cycles dependent on them is more popular in Russia, Brazil and India than in Europe.\n"}
{"id": "30862857", "url": "https://en.wikipedia.org/wiki?curid=30862857", "title": "Technology and society", "text": "Technology and society\n\nTechnology society and life or technology and culture refers to cyclical co-dependence, co-influence, and co-production of technology and society upon the other (technology upon culture, and vice versa). This synergistic relationship occurred from the dawn of humankind, with the invention of simple tools and continues into modern technologies such as the printing press and computers. The academic discipline studying the impacts of science, technology, and society, and vice versa is called science and technology studies.\n\nThe importance of stone tools, circa 2.5 million years ago, is considered fundamental in the human development in the hunting hypothesis.\n\nPrimatologist, Richard Wrangham, theorizes that the control of fire by early humans and the associated development of cooking was the spark that radically changed human evolution. Texts such as \"Guns, Germs, and Steel\" suggest that early advances in plant agriculture and husbandry fundamentally shifted the way that collective groups of individuals, and eventually societies, developed.\n\nTechnology has become a huge part in society and day-to-day life. When societies know more about the development in a technology, they become able to take advantage of it. When an innovation achieves a certain point after it has been presented and promoted, this technology becomes part of the society.The use of technology in education provides students with technology literacy, information literacy, capacity for life-long learning and other skills necessary for the 21st century workplace. Digital technology has entered each process and activity made by the social system. In fact, it constructed another worldwide communication system in addition to its origin.\n\nA 1982 study by \"The New York Times\" described a technology assessment study by the Institute for the Future, \"peering into the future of an electronic world.\" The study focused on the emerging videotex industry, formed by the marriage of two older technologies, communications and computing. It estimated that 40 percent of American households will have two-way videotex service by the end of the century. By comparison, it took television 16 years to penetrate 90 percent of households from the time commercial service was begun.\n\nSince the creation of computers achieved an entire better approach to transmit and store data. Digital technology became commonly used for downloading music and watching movies at home either by DVDs or purchasing it online.\nDigital music records are not quite the same as traditional recording media. Obviously, because digital ones are reproducible, portable and free.\n\nSeveral states started to implement education technology in schools, universities and colleges. According to the statistics, in the early beginnings of 1990s the use of Internet in schools was ,on average, 2-3%. Continuously, by the end of 1990s the evolution of technology increases rapidly and reaches to 60%, and by the year of 2008 nearly 100% of schools use Internet on educational form. According to ISTE researchers, technological improvements can lead to numerous achievements in classrooms. E-learning system, collaboration of students on project based learning, and technological skills for future results in motivation of students. \n\nAlthough these previous examples only show a few of the positive aspects of technology in society, there are negative side effects as well. Within this virtual realm, social media platforms such as Instagram, Facebook, and Snapchat have altered the way Generation Y culture is understanding the world and thus how they view themselves. In recent years, there has been more research on the development of social media depression in users of sites like these. \"Facebook Depression\" is when users are so affected by their friends' posts and lives that their own jealousy depletes their sense of self-worth. They compare themselves to the posts made by their peers and feel unworthy or monotonous because they feel like their lives are not nearly as exciting as the lives of others.\n\nAnother instance of the negative effects of technology in society, is how quickly it is pushing younger generations into maturity. With the world at their fingertips, children can learn anything they wish to. But with the uncensored sources from the internet, without proper supervision, children can be exposed to explicit material at inappropriate ages. This comes in the forms of premature interests in experimenting with makeup or opening an email account or social media page—all of which can become a window for predators and other dangerous entities that threaten a child's innocence. Technology has a serious effect on youth's health. The overuse of technology is said to be associated with sleep deprivation which is linked to obesity and poor academic performance in the lives of adolescents.\n\nIn ancient history, economics began when spontaneous exchange of goods and services was replaced over time by deliberate trade structures. Makers of arrowheads, for example, might have realized they could do better by concentrating on making arrowheads and barter for other needs. Regardless of goods and services bartered, some amount of technology was involved—if no more than in the making of shell and bead jewelry. Even the shaman's potions and sacred objects can be said to have involved some technology. So, from the very beginnings, technology can be said to have spurred the development of more elaborate economies.Technology is seen as primary source in economic development.\n\nTechnology advancement and economic growth are related to each other.The level of technology is important to determine the economic growth.It is the technological process which keeps the economy moving.\n\nIn the modern world, superior technologies, resources, geography, and history give rise to robust economies; and in a well-functioning, robust economy, economic excess naturally flows into greater use of technology. Moreover, because technology is such an inseparable part of human society, especially in its economic aspects, funding sources for (new) technological endeavors are virtually illimitable. However, while in the beginning, technological investment involved little more than the time, efforts, and skills of one or a few men, today, such investment may involve the collective labor and skills of many millions.\n\nConsequently, the sources of funding for large technological efforts have dramatically narrowed, since few have ready access to the collective labor of a whole society, or even a large part. It is conventional to divide up funding sources into governmental (involving whole, or nearly whole, social enterprises) and private (involving more limited, but generally more sharply focused) business or individual enterprises.\n\nThe government is a major contributor to the development of new technology in many ways. In the United States alone, many government agencies specifically invest billions of dollars in new technology.\n\n[In 1980, the UK government invested just over six million pounds in a four-year program, later extended to six years, called the Microelectronics Education Programme (MEP), which was intended to give every school in Britain at least one computer, software, training materials, and extensive teacher training. Similar programs have been instituted by governments around the world.]\n\nTechnology has frequently been driven by the military, with many modern applications developed for the military before they were adapted for civilian use. However, this has always been a two-way flow, with industry often developing and adopting a technology only later adopted by the military.\n\nEntire government agencies are specifically dedicated to research, such as America's National Science Foundation, the United Kingdom's scientific research institutes, America's Small Business Innovative Research effort. Many other government agencies dedicate a major portion of their budget to research and development.\n\nResearch and development is one of the smallest areas of investments made by corporations toward new and innovative technology.\nMany foundations and other nonprofit organizations contribute to the development of technology. In the OECD, about two-thirds of research and development in scientific and technical fields is carried out by industry, and 98 percent and 10 percent, respectively, by universities and government. But in poorer countries such as Portugal and Mexico the industry contribution is significantly less. The U.S. government spends more than other countries on military research and development, although the proportion has fallen from about 30 percent in the 1980s to less than 10 percent.\n\nThe 2009 founding of Kickstarter allows individuals to receive funding via crowdsourcing for many technology related products including both new physical creations as well as documentaries, films, and webseries that focus on technology management. This circumvents the corporate or government oversight most inventors and artists struggle against but leaves the accountability of the project completely with the individual receiving the funds.\n\n\nThe implementation of technology influences the values of a society by changing expectations and realities. The implementation of technology is also influenced by values. There are (at least) three major, interrelated values that inform, and are informed by, technological innovations:\n\nTechnology often enables organizational and bureaucratic group structures that otherwise and heretofore were simply not possible. Examples of this might include:\n\nTechnology enables greater knowledge of international issues, values, and cultures. Due mostly to mass transportation and mass media, the world seems to be a much smaller place, due to the following:\n\nTechnology provides an understanding, and an appreciation for the world around us.\n\nMost modern technological processes produce unwanted by products in addition to the desired products, which is known as industrial waste and pollution. While most material waste is re-used in the industrial process, many forms are released into the environment, with negative environmental side effects, such as pollution and lack of sustainability. Different social and political systems establish different balances between the value they place on additional goods versus the disvalues of waste products and pollution. Some technologies are designed specifically with the environment in mind, but most are designed first for economic or ergonomic effects. Historically, the value of a clean environment and more efficient productive processes has been the result of an increase in the wealth of society, because once people are able to provide for their basic needs, they are able to focus on less tangible goods such as clean air and water.\n\nThe effects of technology on the environment are both obvious and subtle. The more obvious effects include the depletion of nonrenewable natural resources (such as petroleum, coal, ores), and the added pollution of air, water, and land. The more subtle effects include debates over long-term effects (e.g., global warming, deforestation, natural habitat destruction, coastal wetland loss.)\n\nEach wave of technology creates a set of waste previously unknown by humans: toxic waste, radioactive waste, electronic waste.\n\nOne of the main problems is the lack of an effective way to remove these pollutants on a large scale expediently. In nature, organisms \"recycle\" the wastes of other organisms, for example, plants produce oxygen as a by-product of photosynthesis, oxygen-breathing organisms use oxygen to metabolize food, producing carbon dioxide as a by-product, which plants use in a process to make sugar, with oxygen as a waste in the first place. No such mechanism exists for the removal of technological wastes.\n\nSociety also controls technology through the choices it makes. These choices not only include consumer demands; they also include:\n\nAccording to Williams and Edge, the construction and shaping of technology includes the concept of choice (and not necessarily conscious choice). Choice is inherent in both the design of individual artifacts and systems, and in the making of those artifacts and systems.\n\nThe idea here is that a single technology may not emerge from the unfolding of a predetermined logic or a single determinant, technology could be a garden of forking paths, with different paths potentially leading to different technological outcomes. This is a position that has been developed in detail by Judy Wajcman. Therefore, choices could have differing implications for society and for particular social groups.\n\nIn one line of thought, technology develops autonomously, in other words, technology seems to feed on itself, moving forward with a force irresistible by humans. To these individuals, technology is \"inherently dynamic and self-augmenting.\"\n\nJacques Ellul is one proponent of the irresistibleness of technology to humans. He espouses the idea that humanity cannot resist the temptation of expanding our knowledge and our technological abilities. However, he does not believe that this seeming autonomy of technology is inherent. But the perceived autonomy is because humans do not adequately consider the responsibility that is inherent in technological processes.\n\nLangdon Winner critiques the idea that technological evolution is essentially beyond the control of individuals or society in his book Autonomous Technology. He argues instead that the apparent autonomy of technology is a result of \"technological somnambulism,\" the tendency of people to uncritically and unreflectively embrace and utilize new technologies without regard for their broader social and political effects.\n\nIndividuals rely on governmental assistance to control the side effects and negative consequences of technology.\n\nRecently, the social shaping of technology has had new influence in the fields of e-science and e-social science in the United Kingdom, which has made centers focusing on the social shaping of science and technology a central part of their funding programs.\n\n\n"}
{"id": "2311313", "url": "https://en.wikipedia.org/wiki?curid=2311313", "title": "The Jew of Linz", "text": "The Jew of Linz\n\nThe Jew of Linz is a 1998 book by Australian writer Kimberley Cornish, in which the author alleges that the Austrian philosopher Ludwig Wittgenstein had a profound effect on Adolf Hitler when they were both pupils at the Realschule (lower secondary school) in Linz, Austria, in the early 1900s. Cornish also alleges that Wittgenstein was involved in the Cambridge Five Soviet spy ring during the Second World War.\n\n\nCornish used a school photograph from the \"Realschule\" (lower secondary school) in Linz, Austria, on his book cover. That boy in the top-right corner is undisputedly Hitler (see above right). Cornish alleges that Wittgenstein is the boy on the bottom left; he says the Victoria Police photographic evidence unit in Australia examined the photograph and confirmed that it was \"highly probable\" the boy is Wittgenstein. German government\nand U.S. sources\ndate the photograph to 1901, slightly after Hitler's arrival at the school, but two years prior to Wittgenstein's enrollment. \n\nWittgenstein and Hitler both attended the Linz \"Realschule\", a state school of about 300 students, and were there at the same time only from 1903 to 1904, according to Wittgenstein's biographers. While Hitler was just six days older than Wittgenstein, they were two grades apart at the school—Hitler was repeating a year and Wittgenstein had been advanced a year. Cornish's thesis is not only that Hitler knew the young Wittgenstein, but that he hated him, and that Wittgenstein was specifically the one Jewish boy from his school days referred to in \"Mein Kampf\". The last claim referred to the following quote:\n\nCornish argues further that Hitler's anti-Semitism involved a projection of the young Wittgenstein's traits onto the whole Jewish people. It should be noted that Wittgenstein did have three Jewish grandparents but Wittgenstein himself, and his mother and father, were Roman Catholics.\n\nCornish also argues that Wittgenstein is the most likely suspect as recruiter of the \"Cambridge Five\" spy ring. The author suggests that Wittgenstein was responsible for British decryption technology for the German Enigma code reaching the Red Army and that he thereby enabled the Red Army victories on the Eastern Front that liberated the camps and ultimately overthrew the Reich.\n\nHe writes that the Soviet government offered Wittgenstein the chair in philosophy at what had been Lenin's university (Kazan) at a time (during the Great Purge) when ideological conformity was at a premium amongst Soviet academics and enforced by the very harshest penalties. Wittgenstein wanted to emigrate to Russia, first in the twenties, as he wrote in a letter to Paul Engelmann, and again in the thirties, either to work as a labourer or as a philosophy lecturer. Cornish argues that given the nature of the Soviet regime, the possibility that a non-Marxist philosopher (or even one over whom the government could exert no ideological control) would be offered such a post, is unlikely in the extreme.\n\nOther sections of the book deal with Cornish's theories about what he claims are the common roots of Wittgenstein's and Hitler's philosophies in mysticism, magic, and the \"no-ownership\" theory of mind. Cornish sees this as Wittgenstein's generalisation of Arthur Schopenhauer's account of the Unity of the Will, in which despite appearances, there is only a single Will acting through the bodies of all creatures. This doctrine, generalized to other mental faculties such as thinking, is presented in Ralph Waldo Emerson's \"Essays\". The doctrine, writes Cornish, was also held by the Oxford philosopher R. G. Collingwood who was one of Wittgenstein's electors to his Cambridge chair. Cornish tries to tie this to Wittgenstein's arguments against the idea of \"mental privacy\" and in conclusion says \"I have attempted to locate the source of the Holocaust in a perversion of early Aryan religious doctrines about the ultimate nature of man\". Cornish also suggests that Hitler's oratorical powers in addressing the group mind of crowds and Wittgenstein's philosophy of language and denial of mental privacy, are the practical and theoretical consequences of this doctrine.\n\nThe book proved controversial, with reviewers criticizing it for drawing unwarranted connections between disparate events. The main criticisms were that:\n\n\nOne of the main issues of contention is the claim that Wittgenstein triggered or substantially contributed to Hitler's antisemitism while they were at school together. It is a view that has some support. British professor Laurence Goldstein, in his \"Clear and Queer Thinking: Wittgenstein's Development and His Relevance to Modern Thought\" (1999), called Cornish's book important, writing: \"For one thing, at the K.u.k. Realschule in Linz, Wittgenstein met Hitler and may have inspired in him a hatred of Jews which led, ultimately, to the Holocaust. This, naturally enough, weighed heavily on Wittgenstein's conscience in his later years ... It is overwhelmingly probable that Hitler and Wittgenstein did meet, and with dire consequences for the history of the world.\"\n\nReviewing Goldstein's own book, Mary McGinn called it a sloppy and irresponsible argument: \" [O]ne is amazed at the sheer looseness of thought that allows him to assert that 'at certain points in \"Mein Kampf\" where Hitler seems to be raging against Jews in general it is the individual young Ludwig Wittgenstein whom he has in mind', and to suggest that Wittgenstein 'may have inspired … (the) hatred of Jews which led, ultimately, to the Holocaust'. It is exactly this sort of sloppy, irresponsible, 'plausible' style of thought that Wittgenstein's philosophy, by its careful attention to the particular and to not saying more or less than is warranted, is directed against.\"\n\nRay Monk, one of Wittgenstein's biographers, concentrates on the inconsistencies in Cornish's theory that Wittgenstein was the head of the Cambridge spy ring, asking why Cornish has apparently not bothered to verify any of his theories by checking the KGB archives. Ultimately, Monk says \"As I read \"The Jew of Linz\", I found myself wondering how on earth Cornish had confected so strange a piece of work. I found it by turns puzzling, funny, challenging and outrageously nutty... Cornish calls his book 'pioneer detective work', but I think it is really pioneer detective fiction.\"\n\nDaniel Johnson viewed \"The Jew of Linz\" as a \"revisionist tract masquerading as psycho-history\". He wrote, \"Cornish correctly identifies 'the twist of the investigation' as the thesis that 'Nazi metaphysics, as discernible in Hitler's writings... is nothing but Wittgenstein's theory of the mind modified so as to exclude the race of its inventor'. So the \"Jew of Linz\" was indirectly responsible, at least in part, for the Holocaust. Cornish tries to deflect the implications of his argument thus: 'Whatever 'the Jews' may have done, nothing humanly justifies what was done to them.' But he then offers 'a thought that might occur to a Hassidic Jew, and that is more fittingly a matter for Jewish, as opposed to gentile, reflection: the very engine that drove Hitler's acquisition of the magical powers that made his ascent and the Holocaust possible was the Wittgenstein Covenant violation'. At this point, the nonsensical shades into the downright sinister.\n\nSean French wrote in the \"New Statesman\": \"There is something heroic about this argument and it would be a good subject for a novel about the dangers of creating theories out of nothing. Vladimir Nabokov should have written it. It is not just that there are weak links in the theory. There are no links in the theory. No evidence that Hitler, in his final unhappy year, even knew a boy two years above him. If they did know each other, there is no evidence that he was the boy Hitler distrusted, no evidence that Hitler's remarks on snitching related to specific incidents at the Linz Realschule, no evidence that Wittgenstein informed on his fellow pupils.\" In the same journal, Roz Kaveney calls it \"a stupid and dishonest book\", and says \"[Cornish's] intention is to claim Wittgenstein for his own brand of contemplative mysticism, which he defines as the great insight that IndoEuropeans (or, as he unregenerately terms them, Aryans) brought to Hinduism and Buddhism.\"\n\nAntony Flew offers a mixed review: \"Mr Cornish contends that the reason why the government of the USSR treated Wittgenstein with such peculiar generosity was that he had been the recruiter of all the Cambridge spies. The question whether or not this hypothesis is true or false can be definitively settled only if and when the relevant Soviet archives are examined. But I am myself as confident as without such knock-down decisive verification it is possible to be that Mr Cornish is right. On the other hand, 'On the very first page of Part III, Mr Cornish explains that the essence of this doctrine was expressed by Emerson in his restatement of the original Aryan doctrine of consciousness: '… the act of seeing and the thing seen, the see-er and the spectacle, the subject and the object is one'. I confess, not very shamefacedly, that confronted with such doctrines I want to quote Groucho Marx: 'It appears absurd. But don't be misled. It is absurd.'\"\n\nGerman historian \"Michael Rissmann\" argues that Cornish overestimates Hitler's intellectual capacities and uses fraudulent talks Hermann Rauschning claims to have had with Hitler to prove Hitler's alleged occultist interest.\" In \"Philosophy Now\", John Mann argues that the contentions that so riled up the book's many critics were simply a clever ruse by Cornish designed to attract more readers. Mann writes: \"Cornish is clever enough to know if he wrote a book on his 'no ownership' theory of language it would not have a wide readership. If he says this 'no ownership' theory was taught by Wittgenstein, learned and twisted for his own ends by Hitler, and actually needs Cornish to explain it all in great detail for the rest of the book he has the book reviewed in every paper and even serialised in the Sunday Times. ... If you’re looking for a book which offers history, politics, magic and philosophy, try \"The Jew of Linz\".\"\n\n"}
{"id": "23915750", "url": "https://en.wikipedia.org/wiki?curid=23915750", "title": "The Other Side: The Secret Relationship Between Nazism and Zionism", "text": "The Other Side: The Secret Relationship Between Nazism and Zionism\n\nThe Other Side: the Secret Relationship Between Nazism and Zionism (Arabic: \"al-Wajh al-Akhar: al-'Alaqat as-Sirriya bayna an-Naziya wa's-Sihyuniya\") is a book by Mahmoud Abbas, published in 1984 in Arabic. It is based on his CandSc thesis, completed in 1982 at Patrice Lumumba University (now the Peoples' Friendship University of Russia) under the title \"The Connection between the Nazis and the Leaders of the Zionist Movement\", and defended at the Institute of Oriental Studies of the Soviet Academy of Sciences.\n\nIn the book, Abbas argues that the Nazi-perpetrated Holocaust had been exaggerated and that Zionists created \"the myth\" of six million murdered Jews, which he called a \"fantastic lie\". He further claimed that those Jews who were killed by the Nazis were actually the victims of a Zionist-Nazi plot aimed to fuel vengeance against Jews and to expand their mass extermination. The book also discussed topics such as the Haavara Agreement, in which the Third Reich agreed with the Jewish Agency to facilitate Jewish emigration from Germany to Mandate Palestine.\n\nPortions of \"The Other Side\" have been considered as Holocaust denial by some critics, especially the parts disputing the accepted number of deaths in the Holocaust as well as the accusations that Zionist agitation was the cause of the Holocaust, a charge that Abbas denies.\n\nWhen Abbas was appointed the Palestinian prime minister in 2003, he wrote that the \"Holocaust was a terrible, unforgivable crime against the Jewish nation, a crime against humanity that cannot be accepted by humankind\" and that he does not deny it, and said that \"When I wrote \"The Other Side\" … we were at war with Israel. Today I would not have made such remarks\". In 2014, he stated the Holocaust was the \"most heinous crime in the modern era\".\n\nIn 2013 Abbas reasserted part of his thesis to the extend that \"the Zionist movement had ties with the Nazis\".\n\nAbbas attended at Patrice Lumumba University to prepare and present his doctoral thesis. The institute's director at the time, Yevgeny Primakov, appointed a Soviet specialist on Palestine, Vladimir Ivanovich Kisilev as Abbas' dissertation adviser. They communicated mostly in English and Arabic. In an interview with the magazine \"Kommersant\" 20 years later, Kisilev remembers Abbas as a well-prepared graduate student, who came to Moscow with an already chosen research topic and a large amount of already prepared material.\n\nThe title of Abbas' thesis is \"The Connection between the Nazis and the Leaders of the Zionist Movement\" or, in Russian, \"Связи между сионизмом и нацизмом. 1933–1945\". In 1984, a book based on Abbas' doctoral dissertation was published in Arabic by Dar Ibn Rushd publishers in Amman, Jordan under the title \"al-Wajh al-akhar : al-`alaqat al-sirriyah bayna al-Naziyah wa-al-Sihyuniyah\".\n\nIn the doctoral thesis, Abbas describes the number of Jews murdered in the Nazi Holocaust as agreed upon by mainstream historians, six millions, as a \"fantastic lie\". In the book, he wrote:\nIn the book, he wrote:\nAbbas quotes historian Raul Hilberg to support his allegations that fewer than one million Jews were killed. However, Rafael Medoff of the David S. Wyman Institute for Holocaust Studies denied the assertion that \"The historian and author, Raoul Hilberg, thinks that the figure does not exceed 890,000\", and said this is \"utterly false\". He wrote that \"Professor Hilberg, a distinguished historian and author of the classic study \"The Destruction of the European Jews\", has never said or written any such thing.\"\n\nAbbas raised doubts regarding the existence of the gas chambers, quoting Robert Faurisson, on the nonexistence of gas chambers.\n\nAdditionally, he stated that the much smaller number of Jews which he reportedly admitted that the Germans did massacre were actually the victims of a Zionist-Nazi plot:\nThe thesis also discussed topics such as the Haavara Agreement of 1933, in which the Third Reich agreed with the Jewish Agency to enable Jews to emigrate from Germany directly to Mandate Palestine, which he sees as evidence of collaboration.\n\nA global survey of Holocaust denial, published by David S. Wyman Institute for Holocaust Studies in 2004, describes the book as \"denying the Holocaust\".\n\nAfter Abbas was appointed prime minister of the Palestinian Authority in 2003, the Israel Defense Forces removed excerpts from the Abbas book from its website, including quotes questioning the use of gas chambers and talking of less than one million victims.\n\nAccording to the Anti-Defamation League, the Simon Wiesenthal Center called for Abbas to clarify his position on the Holocaust in 1995, but he did not do so at that time. Abbas' reported defence when asked about the book was telling: \"When I wrote \"The Other Side\"… we were at war with Israel. Today I would not have made such remarks… Today there is peace and what I write from now on must help advance the peace process.\"\n\nIn his May 2003 interview with \"Haaretz,\" Abbas stated:\n\nAccording to the Ma'an News Agency, in an interview in 2013, Abbas defended his doctoral thesis regarding the relationship between the Zionists and the Nazis and said he \"challenges anyone who can deny that the Zionist movement had ties with the Nazis before World War II.\"\n"}
{"id": "364584", "url": "https://en.wikipedia.org/wiki?curid=364584", "title": "Translatio studii", "text": "Translatio studii\n\nTranslatio studii (Latin for \"transfer of learning\") is a historiographical concept, originating in the Middle Ages, in which history is viewed as a linear succession of transfers of knowledge or learning from one geographical place and time to another. The concept is closely linked to \"translatio imperii\", which similarly describes the movement of imperial dominance. Both terms are thought to have their origins in the second chapter of the Book of Daniel in the Hebrew Bible (verses 39–40).\n\nIt is a celebrated topos in medieval literature, most notably articulated in the prologue to Chrétien de Troyes's \"Cligès\", composed ca. 1170. There, Chrétien explains that Greece was first the seat of all knowledge, then it came to Rome, and now it has come to France, where, by the grace of God, it shall remain forever more. \n\nIn the Renaissance and later, historians saw the metaphorical light of learning as moving much as the light of the sun did: westward. According to this notion, the first center of learning was Eden, followed by Jerusalem, and Babylon. From there, the light of learning moved westward to Athens, and then west to Rome. After Rome, learning moved west to Paris. From there, enlightenment purportedly moved west to London, though other nations laid claim to the mantle, most notably Russia, which would involve a retrograde motion and rupture in the westerly direction. The metaphor of \"translatio studii\" went out of fashion in the 18th century, but such English Renaissance authors as George Herbert were already predicting that learning would move next to America. \n\nA pessimistic corollary metaphor is the \"translatio stultitiae\". As learning moves west, as the earth turns and light falls ever westward, so night follows and claims the places learning has departed from. The metaphor of the \"translatio stultitiae\" informs Alexander Pope's \"Dunciad,\" and particularly book IV of the \"Greater Dunciad\" of 1741, which opens with the nihilistic invocation:\n\n<poem>\nYet, yet a moment, one dim Ray of Light\nIndulge, dread Chaos, and eternal Night!\" (\"B\" IV 1-2)\n\nSuspend a while your Force inertly strong,\nThen take at once the Poet, and the Song. (ibid. 7-8)\n</poem>\n\nWhile the term \"translatio studii\" literally means in English the translation of studies, there is an implication within the concept that the transmission of learning also carried with it cultural ideals and information. That being said, there is a lot more to \"translatio studii\" than the simple movement of common concepts from the Mediterranean westward.\n\nAccording to Karlheinz Stierle, English is what we might consider the current language of this sort of transmission, but \"what English is today...Latin was in the first centuries after Christ.\" In the way that politics and social issues move circulate around the world very often in English, these same concepts traveled along the developing roads from Greece and Italy to England during Medieval times. As religion spread from Rome to Londinium (or present day Britain) it brought with it other concepts that can still be seen in the Romance languages.\n\nAn interesting example of this is the term \"translatio\" itself. In Ancient times \"translatio\" in Latin meant both \"translation\" and \"transfer\". As time went on, \"translatio\" was designated to only mean transmission and \"traductio\" took on the meaning of what we know as translation. This carried over to the developing Romance languages as time went on. The words \"translation\" in French and \"traslazione\" in Italian mean the displacement of physical objects, and these languages still use other words to mean \"translation\" in the English sense. This varies remarkably from the common meaning used in English, which is solely a linguistic concept rather than a spatial one. In this way, it is clear that historically the significance of \"translatio studii\" concerns the transfer of ideas that hold cultural value.\n\n\"Translatio imperii\" often served as a precedent or coordinate to \"translatio studii\". A transferral of rule assisted a transferral of culture, and vice versa: \"The transferal of power also conveys the phoenix-like reestablishment of culture - as fictionalized in and transmitted by literature - which establishes each new imperial power as the new stronghold of the culturally elite.\"\n\nAs it is concerned with the progress of learning, \"translatio studii\" provides an overview of intellectual heritage. Although it may be considered from various angles (e.g., history, linguistics, and literature) the concept of \"translatio studii\" is fundamentally concerned with texts. \"Reading, translating, commenting, interpreting, rewriting — all are common intertextual activities of the \"translatio studii\".\"\n\n\"Translatio studii\" is based on the assumption that human learning and the potential for human learning originated in Greece from whence it spread westward to Rome and then France.\n\nChrétien de Troyes, a French poet of the late 12th century, writes of \"translatio studii\" in the opening of Cligès:\n\n<poem>\n\"Par les livres que nous avons\"\n\"Les fez des anciiens savons\"\n\"Et del siecle qui fu jadis.\"\n\"Ce nos ont nostre livre apris,\"\n\"Que Grece ot de chevalerie\"\n\"Le premier los et de clergie.\"\n\"Puis vint chevalerie a Rome\"\n\"Et de la clergie la some,\"\n\"Qui or est en France venue.\"\n\"Deus doint qu'ele i soit retenue\"\n\"Et que li leus li abelisse\"\n\"Tant que ja mes de France n'isse.\"\n</poem>\n\nThat is: \"Through the books which we have, we know the deeds of the ancients and of times long passed. Our books have taught us that Greece had the first fame of chivalry and learning. Then came chivalry to Rome, and the sum of learning, which now is come to France. God grant that it remain there, and that it find the place so pleasant that it will never depart from France.\"\n\nAll Roman comedy stems from Greek New Comedy but rewritten in Latin with slight adjustments to local taste and the long, narrow stage of Roman theatre. It keeps the characteristics of conventional situations from domestic life and stock character-masks that were traditional in the Greek model.\n\nRoman theatre in turn influenced theatre of the Renaissance. \"The nine Greek-style tragedies of Seneca (c. 4 B.C.E. -65 C.E.) are especially noteworthy, partly because they were to have a more profound influence on Renaissance tragedians than their Greek originals.\" Conventions commonly associated with Renaissance tragedies, most popularly Shakespeare, that are owed to Seneca, are revenge tragedies, structure of five acts, use of elaborate speeches, soliloquies, and asides, violence and horror performed on stage (as opposed to Greek tragedies in which all such actions occurred off stage), and an interest in the human condition, morality of nobility, and the supernatural, specifically with its human connection.\n\nRome also used the Greek language as a model on which to aid the expansion of their power and secure a language for their empire. According to L. G. Kelly, author of \"The True Interpreter: A History of Translation Theory and Practice in the West\" (1979), \"Western Europe owes its civilization to its translators.\"\n\n\"Cortoisie\" is a synthesis of the superiority of French knighthood and learning. As a new distinction of the French knight, \"cortoisie\" implies not only a new style of communication and mastery of language, but a new style of communicative attitude, especially when regarding women. From \"cortoisie\" comes courtly love, a highly disciplined, self-denying, and respectful social form. Ideally, in this form, the knight honors his lady as something sacred. This new ideal of love called for a new ideal of language, according to Chrétien, and so, translations from old, dead Latin into French, or \"romanz\", began. It is this language that replaces Latin as a new and lasting period of high culture, and, in so doing, becomes the real language or medium of \"translatio studii\".\n\n\n"}
{"id": "43698260", "url": "https://en.wikipedia.org/wiki?curid=43698260", "title": "University of Edinburgh School of History, Classics and Archaeology", "text": "University of Edinburgh School of History, Classics and Archaeology\n\nThe School of History, Classics and Archaeology at the University of Edinburgh is a school within the College of Humanities and Social Science. \n\nThe school is located in the William Robertson Wing of the Old Medical School buildings on Teviot Place.\n\nClassics have been taught at the university since its foundation in 1583. The school has the oldest established Chair in Scottish History. Several well-known archaeologists have graduated and taught at the school.\n\nNotable members of Edinburgh University's School of History, Classics and Archaeology: \n\n\nThe School of History, Classics and Archaeology currently publishes the Journal of Lithic Studies.\n\n"}
{"id": "46814110", "url": "https://en.wikipedia.org/wiki?curid=46814110", "title": "Watumull Prize", "text": "Watumull Prize\n\nThe Watumull Prize (1945–82) was established in 1944 to recognize \"the best book on the history of India originally published in the United States\".\n"}
