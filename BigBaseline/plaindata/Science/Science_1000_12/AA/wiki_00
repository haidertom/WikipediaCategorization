{"id": "5654189", "url": "https://en.wikipedia.org/wiki?curid=5654189", "title": "American Academy of Anti-Aging Medicine", "text": "American Academy of Anti-Aging Medicine\n\nThe American Academy of Anti-Aging Medicine (A4M) is a United States registered 501(c)(3) nonprofit organization that promotes the questionable field of anti-aging medicine and trains and certifies physicians in this specialty. As of 2011, approximately 26,000 practitioners had been given certificates. The field of anti-aging medicine is not recognized by established medical organizations, such as the American Board of Medical Specialties (ABMS) and the American Medical Association (AMA). The Academy's activities include lobbying and public relations. The A4M was founded in 1993 by osteopathic physicians Robert Goldman and Ronald Klatz, and has grown to 26,000 members from 110 countries. The organization sponsors several conferences, including the Annual World Congress on Anti-Aging Medicine.\n\nSeveral of the anti-aging methods recommended by the Academy have wide support among experts in the field, such as exercise and a healthy diet, but others, such as hormone treatments, do not have support from a consensus of the wider medical community. Many scientists studying aging dissociate themselves from the claims of A4M, and critics have accused the group of using misleading marketing to sell expensive and ineffective products. The A4M's founders and merchants who promote products through the organization have been involved in legal and professional disputes.\n\nThe activities of the A4M are controversial: in 2003 a commentary on the response of the scientific community to the promotion of anti-aging medicine noted that the activities of the A4M were seen as a threat to the credibility of serious scientific research on aging. According to MSNBC, anti-aging advocates have responded to such criticism by describing it as censorship perpetrated by a conspiracy of the US government, notably the Food and Drug Administration, the AMA, and the mainstream media, motivated by competing commercial interests. Thomas Perls of the Boston University School of Medicine, a prominent critic of the organization, has stated that claims of censorship and suppression are a common theme in what he calls \"anti-aging quackery\".\n\nAccording to \"The New York Times\", their co-founder and president Ronald Klatz stated that \"We're not about growing old gracefully. We're about never growing old.\" With Klatz being quoted in 2004 as stating that:\nWriting in the 2001 issue of the journal \"Generations\", historian Carole Haber of the University of Delaware, states that Klatz' aspirations and the rhetoric of the A4M \"reflect well-worn ideas and the often-enunciated hopes of the past\", drawing parallels with the ideas of the 19th century physiologists Charles-Édouard Brown-Séquard, Serge Voronoff and Eugen Steinach. Haber states that the current resurgence of these ideas may be due to their appeal to the aging Baby Boom Generation, in a culture that is focused on the ideal of youth. Haber has also discussed the strong continuities within the philosophy of the anti-aging movement, writing that \"For Steinach and Voronoff, as for the members of the A4M, old age was a \"grotesque\" disease that could be scientifically eradicated through the correct combination of hormones, diet, and surgery.\"\n\nThe chairman of the A4M is Robert Goldman and the president is Ronald Klatz. The senior vice president is Joseph Maroon of the University of Pittsburgh and Nicholas DiNubile of the University of Pennsylvania is the vice president. The Academy states that it has over 20,000 members from over 100 countries, and that this membership is made up of physicians, scientists, researchers, health practitioners and members of the public. In 2007, the organization reported just over seven million dollars in assets. However, a 2006 review of anti-aging medicine notes that of the researchers who are interested in this topic, the \"vast majority dissociate themselves from the A4M.\" The \"Los Angeles Times\" states that \"Many physicians, researchers and scientists, delving into the physiological aspects of human aging, view the Academy's activities with disdain, saying that the organization is an inappropriate blend of scientific and commercial interests.\"\n\nThe main activity of the A4M is PR and advocacy for its brand of anti-aging medicine. It does this through publications, on-line activity and sponsoring conferences including the World Anti-Aging Congress and Exposition and the Annual World Congress on Anti-Aging Medicine. Some of these conferences are in conjunction with the World Anti-Aging Academy of Medicine, an umbrella group for several national anti-aging organizations that is also headed by Goldman. The \"LA Times\" stated that the 2004 annual conference of the A4M at Las Vegas presented a mix of \"scientific and technical presentations\" and exhibitors selling \"wrinkle creams, hair-growing potions, sexual enhancement pills and hormone treatments\".\n\nAccording to a review of the anti-aging movement published in 2005, the A4M is one of the most prominent organizations that are making \"attempts at legitimizing anti-aging as a medical specialty\". The review notes that these efforts at legitimization are contentious and have been rebuffed by some academic scientists who work on aging, who instead attempt to portray the A4M as \"charlatans whose main goal is making money\". In a review of the history of anti-aging medicine published in 2004, Robert Binstock of Case Western Reserve University noted that A4M \"actively solicits and displays numerous advertisements on its website for products and services (such as cosmetics and alternative medicines and therapies), anti-aging clinics, and anti-aging physicians and practitioners.\" \"The Times\" reported in 2004 that Klatz professes outrage at suggestions that he is motivated by money, quoting him as insisting that \"The only thing that I sell are books...my website is non-commercial – we’re just trying to advance science.\" \"The Times\" went on to note a partnership between Klatz and Goldman and a business named Market America, which sells products that promise to \"slow the ageing process\". However, according to a 2005 article in the \"Chicago Tribune\", the company later pulled out of this contract.\n\nThe A4M's American Board of Anti-Aging Medicine (ABAAM) states that it offers anti-aging medicine as a specialty and gives educational credits to those who attend A4M conferences. \"The New York Times\" has reported that the American Board of Medical Specialties does not recognize this body as having professional standing. MSNBC noted that \"as far as the American Medical Association or the American Board of Medical Specialties is concerned, there is no such thing as an anti-aging specialty.\" Robert Binstock stated in a 2004 review article in \"The Gerontologist\" that \"Although the organization is not recognized by the American Medical Association, A4M has established three board-certification programs under its auspices—for physicians, chiropractors, dentists, naturopaths, podiatrists, pharmacists, registered nurses, nurse practitioners, nutritionists, dieticians, sports trainers and fitness consultants, and PhDs.\"\n\nThe A4M publishes \"Anti Aging Medical News\", a trade periodical which is their official magazine, as well as proceedings of its anti-aging conferences in a periodical called \"Anti-Aging Therapeutics\", this is edited by Klatz and Goldman.\n\nThe \"International Journal of Anti-Aging Medicine\" (IJAAM) was another periodical published by the A4M. Describing the intended scope of this publication, Klatz is quoted as stating, \"We hope to cover the waterfront of the entire field of anti-aging medicine, with a clinical focus.\" As of 2009, the A4M recommend this publication on their website as a good way of keeping up with recent developments in anti-aging medicine, stating that it \"report(s) on the latest anti-aging findings\". According to Ulrich's Periodicals Directory, IJAAM was published by Total Health Holdings, LLC from 1998 to 2001, on behalf of the A4M.\n\nThe contents of the \"International Journal of Anti-Aging Medicine\" have been strongly criticised. In a 2002 letter published in \"Science\", Aubrey de Grey described them as consisting of a set of advertisements for a \"pseudoscientific anti-aging industry\". According to Bruce Carnes of the University of Oklahoma:\nLeonard Hayflick of the University of California, San Francisco, a former editor of \"Experimental Gerontology\", writes:\n\nIn 2009 the A4M stated that it is no longer associated with the journal and that it had sold its interests in this publication in 1999. They also defended the scientific quality of its contents, writing that almost all of its articles were reviewed by an editorial board before publication. Robert Binstock of Case Western Reserve University stated in 2004 that this periodical is a \"nonrefereed publication\".\n\nAccording to a 2002 article in the \"Seattle Times\", there are two opposing viewpoints of anti-aging products. The article states that the first view is represented by scientists who publish their findings in the scientific literature and who believe that no currently available intervention can slow or prevent aging. The alternative viewpoint is represented by people who the article states have \"fewer credentials\" and who promote a range of products that claim to have anti-aging properties. A similar observation was made by \"Business Week\" in 2006, when they stated that although anti-aging medicine is increasingly popular, there is \"precious little scientific data to back up their claims that the potions extend life.\"\n\nAs an example of the first viewpoint, a 2004 review in \"Trends in Biotechnology\" written by Leigh Turner of the Institute for Advanced Study in Princeton, New Jersey stated that the products promoted by the A4M have \"no credible scientific basis\" and that \"there are no proven, scientifically established ‘anti-aging’ medications\". A 2006 review published in the \"Cleveland Clinic Journal of Medicine\" of the antioxidants and hormones that are promoted as anti-aging products by the A4M and the Life Extension Institute concluded that these products have \"minimal to no effect on improving longevity or functional abilities.\" In an editorial accompanying this study, Thomas Perls stated that although many unjustified claims were made about anti-ageing products, no substance had yet been shown to halt or slow the aging process. Similarly, the National Institute on Aging, who are part of the National Institutes of Health, published a general warning in 2009 against businesses that claim anti-aging benefits for their products, describing these as \"health scams\" and stating that \"no treatments have been proven to slow or reverse the aging process\".\n\nThe alternative view is held by the A4M, who argue that anti-aging medicine is \"evidence-based, clinically sound health care.\" and state that \"only those diagnostic and treatment elements which prove their validity through independent evaluations are embraced by the A4M.\" The \"Seattle Times\" quotes Klatz as describing those who doubt the validity of anti-aging medicine as \"flat-earthers\" who make unjustified criticisms that are not backed by scientific evidence, the article also states that Klatz \"sees the science and medical establishments as out to get him.\" Though he has been quoted as saying, \"I’m not against the AMA and I’m not against the establishment, I’m really for the establishment, I’m for technology I’m for science-based medicine. But the innovators are always 30 years ahead of the mainstream and that’s just the way it is with anti-aging medicine. We’re just ahead of the curve.\"\n\nThe American Academy of Anti-Aging Medicine was formed following a 1990 study on human growth hormone (hGH) that was published in the \"New England Journal of Medicine\". The study was performed by Daniel Rudman and colleagues at the Medical College of Wisconsin. Rudman had treated twelve men over 60 years of age with human growth hormone; after six months, these men had an increase in lean body mass and a decrease in adipose tissue mass when compared with a group of nine men who did not receive hormone. Members of the anti-aging movement have interpreted these results to support a role for growth hormone in slowing or reversing aging. A review in \"The Journal of Urology\" noted that this promotion of growth hormone as an anti-aging remedy is \"arguably similar\" to ideas that date back to the late 19th century, when the physiologist Charles-Édouard Brown-Séquard advocated rejuvenating hormone products prepared from animal testicles and stated that \"the injections have taken 30 years off my life\".\n\n\"The New York Times\" reports that the idea that growth hormone can improve \"health, energy level and sense of well-being.\" is a core belief of the A4M, with Klatz writing a book in 1998 entitled \"Grow Young with HGH: The Amazing Medically Proven Plan to Reverse Aging\" where he states \"The ‘Fountain of Youth’ lies within the cells of each of us. All you need to do is release it\". A 2005 review in the \"Journal of Endocrinological Investigation\" noted the long history of these ideas, but stated that the \"concept of a 'hormonal fountain of youth' is predominantly mythological.\" Nevertheless, Klatz maintains that growth hormone reverses aging as a physical process and has described growth hormone as \"the first medically proven age-reversal therapy.\" However, MSNBC reports that Daniel Rudman, the author of the 1990 study that sparked the movement, \"issued many caveats and cautions about using HGH and never recommended its use to delay aging. In fact, he was horrified his study was being used to support the industry especially since heavy use of growth hormone can have unwanted side effects\".\n\n\"The New York Times\" states that medical authorities not affiliated with the A4M question the safety and efficacy of the use of growth hormone in anti-aging medicine, quoting Michael Fossell of Michigan State University who stated that \"hormone therapies are the new patent medicines – cure-alls embraced by a too-trusting public.\" A 2003 review that was published in the \"Annual Review of Medicine\" noted that the long-term risks or benefits of this treatment are uncertain, that \"neither the benefits nor the dangers have been defined\" and advising that a \"prudent physician should not condone the use of GH for normal aging\".\n\nAs a result of the reactions to the 1990 article and its frequent citation by proponents of HGH as an anti-aging agent, in 2003 the \"New England Journal of Medicine\" published two articles that strongly and clearly stated that there was insufficient medical and scientific evidence to support use of HGH as anti-aging drug. One article was written by the \"Journal\"'s then-editor in chief, Jeffrey M. Drazen, M.D. and was entitled, \"Inappropriate Advertising of Dietary Supplements\". It focused mostly on the advertising of dietary supplements. The other article was written by the editor-in-chief at the time the 1990 article was published, Mary Lee Vance, M.D., and was entitled, \"Can Growth Hormone Prevent Aging?\"; it focused more on the medical issues around whether there was sufficient evidence to use HGH as an anti-aging agent.\n\nA 2007 review on the use of human growth hormone as an anti-aging treatment in healthy elderly people published in the \"Annals of Internal Medicine\" concluded the risks of HGH significantly outweigh the benefits, noted soft tissue edema as a common side effect and found no evidence that the hormone prolongs life. \"ABC News\" interviewed Hau Liu of Stanford University and lead author of the paper, who stated that people are paying thousands of dollars a year for a treatment that has not been proved to be beneficial and has many side effects. \"ABC News\" also reported that the A4A disputed the conclusions of this review, quoting from an A4A statement which maintained that growth hormone supplementation is beneficial in healthy adults and which described arguments against the use of the hormone as a \"heinous act of malpractice\".\n\nSome small studies have shown that low-dose GH treatment for adults with severe GH deficiency, such as that produced after surgical removal of the pituitary gland, produces positive changes in body composition by increasing muscle mass, decreasing fat mass, increasing bone density and muscle strength; improves cardiovascular parameters (i.e. decrease of LDL cholesterol), and improves quality of life without significant side effects. The extension of this approach to healthy elderly people is an area of current research, with a 2000 review in \"Hormone Research\" commenting that \"Clearly more studies are needed before GH replacement for the elderly becomes established.\" and noting that \"safety issues will require close scrutiny\".\n\nA 2008 review of the controversy surrounding the use of growth hormone in anti-aging medicine which published in \"Clinical Interventions in Aging\" noted the opinions of the A4A on this topic, but suggested that high levels of growth hormone might actually accelerate aging. This concern was repeated by the United States National Institute on Aging who stated in 2009 that:\n\nThe \"Clinical Interventions in Aging\" review also stated that although the decreasing levels of the hormone seen in the elderly might reduce quality of life, this change could protect from age-related diseases and cited evidence linking GH to cancer. This concern was mirrored in a 2008 review published in \"Clinical Endocrinology\", which stated that the risk of increasing the incidence of cancer was a strong argument against the use of this hormone as an \"elixir of youth\" in healthy adults.\n\nThe Academy's co-founders include Klatz and Goldman, who are licensed osteopathic physicians and have Doctor of Osteopathic Medicine degrees (D.O.). However, according to \"The New York Times\", they also received M.D. degrees as doctors of medicine from a university in Belize in 1988, although the paper notes that they had not studied in Belize. In 2009 Klatz and Goldman stated that these degrees involved eight years of medical and surgical training and a year of clinical rotations. \"The New York Times\" reported that the Illinois State Board of Medical Registration did not recognize these MD degrees, and stated that the Board fined the men for using MD after their names. Writing in 2004, \"The Times\" stated that Klatz and Goldman \"agreed to pay $5,000 penalties for allegedly identifying themselves as doctors of medicine in the state without being \"properly licensed\".\" The Illinois Division of Professional Regulation disciplinary records state that Klatz and Goldman \"agreed to cease and desist using the designation \"M.D.\" in addition to the appropriate \"D.O.\" title and fined $5,000. Both physicians did receive degrees as doctors of medicine, but were never properly licensed to use the title \"M.D.\" in Illinois\" In 2009, Klatz and Goldman stated that Illinois Department of Financial & Professional Regulation had determined that they are currently:\n\nThey go on to state that they have \"valid M.D. degrees from a recognized medical school\". Writing in 2004, the historian Carole Haber put this dispute into context, noting that \"like the gland doctors before them, the leaders of the A4M have had their practices and credentials assailed by the medical and legal communities\".\n\nTwo articles in the Journal of the American Medical Association have stated that the use of growth hormone as an anti-aging product is illegal. However, Klatz and Goldman dispute this, arguing that this use of growth hormone is legal. The United States Department of Justice states that growth hormone is a potentially dangerous drug and its supply \"for any use . . . other than the treatment of a disease or other recognized medical condition, where such use has been authorized by the Secretary of Human Services\" is a felony under the 1990 Anabolic Steroids Control Act. Similarly, the FDA has stated in a Warning Letter that no growth hormone products have been approved as anti-aging treatments and supply for this use is therefore illegal and an \"offense punishable by not more than 5 years in prison\". In 2007 \"The New York Times\" discussed ongoing federal and state investigations into illegal trafficking of human growth hormone and anabolic steroids, noting that \"many of the individuals and companies cited in the indictments have been involved with the academy and its conventions over the years\". However, the paper notes that the Academy is not accused of any wrongdoing as part of these investigations and quotes Klatz and Goldman as stating that \"they barely knew the suspects or the nature of their businesses\". A May 2000 article in the \"Los Angeles Times\" suggested that, from an examination of the disciplinary records of doctors in California, members of the A4M in this state were approximately ten times more likely to be disciplined than the national average. In the article, Klatz is quoted as commenting that:\n\nAccording to lawyers claiming to act for A4M and one or more people involved with it, their clients had initiated \"defamation actions in New York and Massachusetts\" against Wikipedia editors in 2009.\nAccording to Courthouse News Service, the A4M co-founders Ronald Klatz and Robert Goldman are pursuing legal action against the online encyclopedia Wikipedia in New York County Court, seeking damages for alleged defamation.\n\nIn 2002, A4M was a co-recipient of the first \"Silver Fleece Award,\" created to publicize \"the most ridiculous claims about antiaging medicine\" according to the award's inventor, S. Jay Olshansky. Heated legal and academic controversies ensued. Olshansky, a biodemographer at the University of Illinois at Chicago, described it as \"a lighthearted attempt to make the public aware of...anti-aging quackery\". This \"award\" was presented by Olshansky, who stated that in his opinion, a \"suite of anti-aging substances created by Ronald Klatz and Robert Goldman...and sold on the Internet by Market America, Inc.\" had made \"outrageous or exaggerated claims about slowing or reversing human aging\". Writing in \"Biogerontology\", anthropologist Courtney Mykytyn of the University of Southern California states that this award appears to have been an attempt by Olshansky to protect what he saw as \"'real' science from the taint of swindle.\" Mykytyn states that this involved Olshansky \"tagging the A4M as fraudulent and its principals as profiteers\". In response, the Academy filed defamation lawsuits, demanding $150 million in damages, with Klatz stating \"We take great exception to Mr Olshansky and his tactics which have finally compelled us to file suit for various unprofessional and improper actions\". Klatz and Goldman described this action as \"part of a larger campaign of disparagement by Olshansky and Perls aimed at discrediting A4M and its founders\". The \"Chicago Tribune\" quoted experts on libel law who stated that the action was an \"almost unheard-of attempt to punish academics for comments made in their professional capacity\". \"CNN\" states that Olshansky countersued and that \"both sides eventually agreed to drop their cases\". The \"Chicago Tribune\" states that the case \"ended in a settlement, with neither side paying damages or the other's costs.\"\n\nIn 2002, Olshansky, Hayflick, and Carnes published a position paper, endorsed by 51 scientists in the field of aging, stating that \"no currently marketed intervention has yet been proved to slow, stop or reverse human aging...The entrepreneurs, physicians and other health care practitioners who make these claims are taking advantage of consumers who cannot easily distinguish between the hype and reality of interventions designed to influence the aging process and age-related diseases,\". The A4M responded by publishing a critique of what it argued were biased statements in this paper.\n\nIn 2009, Imre Zs-Nagy of the University of Debrecen, Hungary, defended A4M from what he called the \"gerontological establishment\" in an editorial published in \"Archives of Gerontology and Geriatrics\", a journal Zs-Nagy founded and of which he is editor-in-chief. Zs-Nagy defended therapies promoted by A4M, which he states are related to his own \"membrane hypothesis of aging\", as theoretically feasible. He described the conflict between the scientific community and the Academy as one pitting government funds, \"personal gain\" and \"intellectual dishonesty\" against the \"independent, open-minded approach\" of A4M, calling the conflict one of the \"biggest scandals of the recent history of medicine\".\n\n\n\n"}
{"id": "52218622", "url": "https://en.wikipedia.org/wiki?curid=52218622", "title": "Arthur Good", "text": "Arthur Good\n\nArthur Good (16 or 26 August 1853 – 30 March 1928) was a French engineer, science educator, author and caricaturist who used the pen name Tom Tit. He wrote a series of weekly articles, \"La Science Amusante\", or \"Amusing Science\", that were collected in book form and have been translated and republished in more than 130 editions in several languages. The illustrations for his do-it-yourself scientific apparatuses have been described as surrealist collages, and were an inspiration for surrealist artists such as Max Ernst and Joseph Cornell.\n\nArthur Good was born in Montivilliers, Seine-Maritime, France on 16 or 26 August 1853. He was the son of Protestant pastor Gustave Frédéric Good (1823–1896) and Louise Stéphanie Monod (1827–1909).\n\nGood graduated from the École centrale des arts et manufacture in Paris, where he studied engineering.\n\nHe married Jeanne Valon (1857–1910) in Paris on 6 April 1881. They had four children.\n\nUnder the pen name Tom Tit, Arthur Good wrote a series of weekly articles, \"La Science Amusante\", or \"Amusing Science\", for the French magazine \"L’Illustration\". Good presented a range of physical experiments, from \"simple games meant to amuse the family\" to experiments \"of a truly scientific character\". They introduce a range of physical and scientific principles including magnetism and surface tension. Good's articles include geometrical demonstrations, craft projects, and physics experiments which can be carried out with everyday household materials. In books such as \"La Récréation En Famille\" he emphasized that scientific education could be a common activity and amusement for the entire family. He dedicated \"La Science Amusante\" to one of his children, saying \"In dedicating this volume today, I would like it to be a souvenir for you of the happy moments we have spent together trying the experiments and constructing the apparatuses\".\n\nGood created improvised scientific apparatuses like his \"Soap-bubble Chandelier\" using common items such as bottles, eggs, corks, candles, and soap. His constructions have an imaginative charm that has led to them being compared to surrealistic collages. Nonetheless, his drawings were seriously and carefully rendered by scientific engraver Louis Poyet (1846–1913) and his assistants.\n\nThe original columns from \"La Science Amusante\" were collected and published in a three-volume series in France. Each volume contained 100 amusements. Beginning in 1889, they have been reprinted in over 130 editions. Collections of amusements were translated and published in English, Italian, and Spanish. In the United States they appeared as \"Magical Experiments, or Science in Play\", and in England as \"Scientific Amusements\". A selection has also been republished as \"100 Amazing Magic Tricks\". Good's books are considered to have laid the foundations for modern approaches to science education in their introduction of \"kitchen science\" and hands-on experiments for children.\n\nGood also published instructions for DIY entertainments in \"Pour Amuser Les Petits ou les joujoux qu’on peut faire soi-même\" (To Amuse the Little Ones, or Do-It-Yourself Small Toys), \"La Récréation En Famille\" (Family Recreations), \"Les Bons Jeudis\" (Fun Thursdays – in Good's day in France there were no classes on Thursdays), and \"Joujoux en Papier\" (Paper Toys). From 1885 to 1888, he was the editor of a periodical, \"Le Chercheur\", that featured new inventions. He also wrote for \"La Nature\".\n\nIn addition to his science education publications, Good published a set of \"Caricatures\" of famous Britons in London in 1913. He received a medal of honor from the National Society for the Development of Good (\"Société nationale d'encouragement au bien\").\n\nDuring the 1920s and 1930s, surrealist artists such as Max Ernst and\nJoseph Cornell were intrigued by the Tom Tit illustrations, and incorporated them into their own works.\n\n"}
{"id": "13074924", "url": "https://en.wikipedia.org/wiki?curid=13074924", "title": "Bioindustry Park Silvano Fumero", "text": "Bioindustry Park Silvano Fumero\n\nThe Bioindustry Park Silvano Fumero (BiPCa) is a Science and Technology Park located in Canavese near Turin in the north-west of Italy.\nThe park was founded to promote and develop biotechnological research, to host companies operating research and development, and pilot production in the life science sector. Companies such as Merck Serono and Bracco chose the Park as the location for their research and development activities.\n\nThe Park offers a complete set of support services to R&D activities, not only offering research facilities and scientific services, but also flexible and attractive financing solutions, sound project management and results evaluation. \n\nThe park is the owner of the Laboratory for Advanced Methodologies (LIMA) which performs exploitation of scientific results in the fields of Chemistry, Molecular Biology, Proteomics and Bioinformatics. LIMA, thanks to its solid co-operation with the University of Turin and CNR ISPA (Consiglio Nazionale delle Ricerche), is also a permanent training site for graduates and researchers. \n\nA University Center in Imaging Technologies (CEIP) managed by University of Torino is also active and is focused on the use of different technologies (MRI, Ultrasound, X-ray, etc.) in order to evaluate new solutions in the diagnosis and assessment of the efficacy and efficiency of drugs.\n\nIn 2005 BiPCa has launched, with the support of Regione Piemonte, the Discovery initiative to identify and select innovative entrepreneurial ideas in the Biotech field. Eight start-ups have already been created from the beginning of the project in partnership with Eporgen venture, a Seed capital company focused on investment in new innovative entrepreneurial projects in the biotech field.\n\nBioindustry Park is also acting as system integrator for the development of a local cluster, bioPmed (http://www.biopmed.eu).\nSince 2009 Bioindustry Park acts as cluster management company for the development of the regional cluster bioPmed (www.biopmed.eu) on biotech and medtech. Created thanks to Regione Piemonte, it gathers more than 60 companies, research centres and three academic institutions (University of Turin, Università del Piemonte Orientale and Turin Polythecnic), who signed an agreement to create, build, support and animate the local cluster. \n\nAll the activities are carried out in cooperation with strong local actors, such as Turin Province and Turin Chamber of Commerce, CEIPiemonte, ALPS EEN and Patlib network and synergies with other local R&D initiatives are in place. The cluster is also founding member of the AlpsBioCluster (www.alpsbiocluster.eu) together with Rhône-Alpes Region (France), Lombardy, South Tirol, Bavaria and Western Switzerland that intends to promote and foster transalpine competitiveness in the biotech and medical sciences sectors. \n\n"}
{"id": "31950201", "url": "https://en.wikipedia.org/wiki?curid=31950201", "title": "Biomedical sciences", "text": "Biomedical sciences\n\nBiomedical sciences are a set of applied sciences applying portions of natural science or formal science, or both, to knowledge, interventions, or technology that are of use in healthcare or public health. Such disciplines as medical microbiology, clinical virology, clinical epidemiology, genetic epidemiology, and biomedical engineering are medical sciences. In explaining physiological mechanisms operating in pathological processes, however, pathophysiology can be regarded as basic science.\n\nThere are at least 45 different specialisms within healthcare science, which are traditionally grouped into three main divisions:\n\nThe healthcare science workforce is an important part of the UK's National Health Service. While people working in healthcare science are only 5% of the staff of the NHS, 80% of all diagnoses can be attributed to their work.\n\nThe volume of specialist healthcare science work is a significant part of the work of the NHS. Every year, NHS healthcare scientists carry out:\n\nThe four governments of the UK have recognised the importance of healthcare science to the NHS, introducing the Modernising Scientific Careers initiative to make certain that the education and training for healthcare scientists ensures there is the flexibility to meet patient needs while keeping up to date with scientific developments.\n\n"}
{"id": "16011340", "url": "https://en.wikipedia.org/wiki?curid=16011340", "title": "Comparison of Asian national space programs", "text": "Comparison of Asian national space programs\n\nSeveral Asian countries have space programs and are actively competing to achieve scientific and technological advancements in space, a situation sometimes referred to as the Asian space race in the popular media as a reference to the earlier Space Race between the United States and the Soviet Union. Like the previous space race, issues involved in the current push to space include national security, which has spurred many countries to send artificial satellites as well as humans into Earth orbit and beyond. A number of Asian countries are seen as contenders in the ongoing race to be the pre-eminent power in space.\n\nOf the ten countries that have independently successfully launched a satellite into orbit, six are Asian: China, India, Iran, Israel, Japan and North Korea.\n\nChina's first manned spacecraft entered orbit in October 2003, making China the first Asian nation to send a human into space.\n\nIndia is expected to demonstrate independent human spaceflight by 2020 and human landing on the moon by 2030, and Iran and Japan have plans for independent manned spaceflights around 2020. China is also still predicting manned mission to the Earth moon by 2025 and to Mars by 2050.\n\nWhile the achievements of space programs run by the main Asian space players (China, India, and Japan) pale in comparison to the milestones set by the former Soviet Union and the United States, some experts believe Asia may soon lead the world in space exploration. China has been the leader of Asia's space race since the beginning of the 21st century. The first Chinese manned spaceflight, in 2003, marked the beginning of a space race in the region. At the same time, the existence of a space race in Asia is still debated due to the non-concurrence of space milestone events like there was for the United States and the Soviet Union. Japan for example was the first power on Earth to get a sample return mission from an asteroid. There was however some concurrence between China and India to see which of those two could be the first to launch a probe to the Earth's moon back in the late 2000s decade. China, for example, denies that there is an Asian space race.\nIn January 2007 China became the first Asian military-space power to send an anti-satellite missile into orbit, to destroy an aging Chinese Feng Yun 1C weather satellite in polar orbit. The resulting explosion sent a wave of debris hurtling through space at more than 6 miles per second. A month later, Japan's space agency launched an experimental communications satellite designed to enable super high-speed data transmission in remote areas.\n\nAfter successful achievement of geostationary technology, India's ISRO launched its first Moon mission, Chandrayaan-1 in October 2008, which discovered ice water on the Moon. India then launched on 5 November 2013 its maiden interplanetary mission, the Mars Orbiter Mission. The primary objective is to determine Mars' atmospheric composition and attempt to detect methane. The spacecraft completed its journey on 24 September 2014 when it entered its intended orbit around Mars, making India the first Asian country to successfully place a Mars orbiter and the only country in history to do so in the first attempt. ISRO became the fourth space agency in the world to send a spacecraft to Mars, only behind NASA, ROSMOS, and ESA.\n\nIn addition to increasing national pride, countries are commercially motivated to operate in space. Commercial satellites are launched for communications, weather forecasting, and atmospheric research. According to a report by the Space Frontier Foundation released in 2006, the \"space economy\" is estimated to be worth about $180 billion, with more than 60% of space-related economic activity coming from commercial goods and services. China and India propose the initiation of a commercial launch service.\n\nChina has a space program with an independent human spaceflight capability. It has developed a sizable family of successful Long March rockets. It has launched two lunar orbiters, Chang'e 1 and Chang'e 2. On 2 December 2013, China launched a modified Long March 3B rocket, with Chang'e 3 Moon lander and its rover Yutu on-board toward the Moon and successfully performed soft landing and rover operations, becoming the third country to do so. It also has plans to retrieve samples by late 2017. In 2011, China embarked on a program to establish a manned space station, starting with the launch of Tiangong 1 and followed by Tiangong 2 in 2016. China attempted to send a Mars orbiter (Yinghuo-1) in 2011 on a joint mission with Russia, which failed to leave Earth orbit. Nevertheless, the 2020 Chinese Mars Mission with an orbiter, a lander and a rover has been approved by the government and is aiming a launch date in the year 2020. China has collaborative projects with Russia, ESA, and Brazil, and has launched commercial satellites for other countries. Some analysts suggest that the Chinese space program is linked to the nation's efforts at developing advanced military technology.\n\nChina's advanced technology is the result of the integration of various related technological experiences. Early Chinese satellites, such as the FSW series, have undergone many atmospheric reentry tests. In the 1990s China had commercial launches, resulting in more launch experiences and a high success rate after the 1990s. China has aimed to undertake scientific development in fields like Solar System exploration. China's Shenzhou 7 spacecraft successfully performed an EVA in September 2008. China's Shenzhou 9 spacecraft successfully performed a manned docking in June 2012. Furthermore, China's Chang'e 2 explorer became the first object to reach Sun-Earth Lagrangian point in August 2011 and also the first probe to explore both Moon and asteroid by making a flyby of the asteroid 4179 Toutatis. China has launched DAMPE, the most capable dark matter explorer to date in 2015, and world's first quantum communication satellite QUESS in 2016.\n\nIndia's interest in space travel began in the early 1960s, when scientists launched a Nike-Apache rocket from TERLS, Kerala. Under Vikram Sarabhai, the program focused on the practical uses of space in increasing the standard of living. Remote sensing and communications satellites were placed into orbit.\n\nThe first Indian to travel in space was Rakesh Sharma, who flew aboard Soyuz T-11, launched April 2, 1984, from erstwhile USSR.\n\nJust a few days after China said that it would send a human into orbit in the second half of 2003, Indian Prime Minister Atal Bihari Vajpayee publicly urged his country's scientists to work towards sending a man to the Moon. It successfully sent its probe to the Moon in October 2008 and is planning its second Moon mission, Chandrayaan-2 for 2019.\n\nISRO launched its Mars Orbiter Mission on November 5, 2013 (informally called \"Mangalyaan\") which successfully entered into the orbit around Mars on 24 September 2014. India is the first in Asia and fourth in the world to perform a successful Mars mission. It is also the only one to do so on the first attempt and at a record low cost of $74 million.\n\nISRO has demonstrated its re-entry technology and till date has launched as many as 175 foreign satellites belonging to global customers from 20 countries including US, Germany, France, Japan, Canada, U.K. All of these have been launched successfully by PSLVs so far, gaining significant expertise in space technologies. In June 2016, India set a record by launching 20 satellites simultaneously. The PSLVs are also one of world's most reliable launch vehicles which clocked its 35th successful mission (39 total) in a row as of February 2017, thus having success rate of nearly 90%.\n\nIndia broke the world record by successfully placing 104 satellites (almost tripling the Russian record of 37) in Earth Orbit on 15 February 2017 on a single rocket launch (PSLV-C37).\n\nRecent reports indicate that human spaceflight is planned with a spacecraft called \"Gaganyaan\" for December 2021 on a home-grown GSLV-III rocket. ISRO is also planning to send orbiters to Venus, Mars and Jupiter or comets and asteroids in the near future.\n\nJapan has been cooperating with the United States on missile defence since 1999. North Korean nuclear and Chinese military programs represent a serious issue for Japan's foreign relations.\nJapan is working on military and civilian space technologies, developing missile defence systems, new generations of military spy satellites, and planning for manned stations on the Moon.\nJapan started to construct spy satellites after North Korea test fired a Taepodong missile over Japan in 1998. The North Korean government claimed the missile was merely launching a satellite to space, and accused Japan of causing an arms race. The Japanese constitution adopted after World War II limits military activities to defensive operations. On May 2007 Prime Minister Shinzo Abe called for a bold review of the Japanese Constitution to allow the country to take a larger role in global security and foster a revival of national pride. Japan has not yet developed its own manned spacecraft and does not have a program in place to develop one. The Japanese space shuttle HOPE-X, to be launched by the conventional space launcher H-II, was developed but the program was postponed and eventually cancelled. Then the simpler manned capsule Fuji was proposed but not adopted. Pioneer projects of single-stage to orbit, reusable launch vehicle horizontal takeoff and landing ASSTS and vertical takeoff and landing Kankoh-maru were developed but have not been adopted. A more conservative new (JAXA manned spacecraft) project is proposed to launch by 2025 as part of the Japanese plan to send manned missions to the Moon. is doubtful about the Japanese manned Moon project, and suspects the project is a euphemism for participation in the American Constellation program. JAXA planned to send a humanoid robot (such as ASIMO) to the Moon.\n\nIran has developed its own satellite launch vehicle, named the Safir SLV, based on the Shahab series of IRBMs. On 2 February 2009, Iranian state television reported that Iran's first domestically made satellite Omid (from the Persian امید, meaning \"Hope\") had been successfully launched into low Earth orbit by a version of Iran's Safir rocket, the Safir-2. The launch coincided with the 30th anniversary of the Iranian Revolution. Iran is also developing a new launch vehicle Simorgh (rocket).\n\nIsrael became the tenth country in the world to build its own satellite and launch it with its own launcher on 19 September 1988. Israel launched its first satellite, Ofeq-1, using an Israeli-built Shavit three-stage launch vehicle. The launching was the high point of a process that began in 1983 with the establishment of the Israel Space Agency under the aegis of the Ministry of Science. Space research by university-based scientists began in the 1960s, providing a ready-made pool of experts for Israel's foray into space. Since then, local universities, research institutes, and private industry, backed by the Israel Space Agency, have made progress in space technology. The agency's role is to support \"private and academic space projects, coordinate their efforts, initiate and develop international relations and projects, head integrative projects involving different bodies, and create public awareness for the importance of space development.\"\n\nNorth Korea has many years of experience with rocket technology, which it has passed along to Pakistan and other countries. On December 12, 2012, North Korea placed its first satellite in orbit with the launch of Kwangmyŏngsŏng-3 Unit 2. On 12 March 2009 North Korea signed the Outer Space Treaty and the Registration Convention, after a previous declaration of preparations for the launch of Kwangmyongsong-2. North Korea twice announced satellite launches: Kwangmyŏngsŏng-1 on 31 August 1998 and Kwangmyŏngsŏng-2 on 5 April 2009. Neither of these claims were confirmed by the rest of the world, but the United States and South Korea believe there were tests of military ballistic missiles. The North Korean space agency is the Korean Committee of Space Technology, which operates the Musudan-ri and Tongch'ang-dong Space Launch Center rocket launching sites and has developed the Baekdusan-1 and Unha (Baekdusan-2) space launchers and Kwangmyŏngsŏng satellites. In 2009 North Korea announced several future space projects, including manned space flights and the development of a manned partially reusable launch vehicle. The successor to the Korean Committee of Space Technology, National Aerospace Development Administration (NADA) successfully launched an Unha-3 launch vehicle in February of 2016, placing the Kwangmyŏngsŏng-4 satellite in orbit.\n\nSouth Korea is a newer player in the Asian space race. In August 2006 South Korea launched its first military communications satellite, the Mugunghwa-5. The satellite was placed in geosynchronous orbit and collects surveillance information about North Korea. The South Korean government is spending hundreds of millions of dollars in space technology and was due to launch its first space launcher, the Korea Space Launch Vehicle, in 2008. South Korea's government justifies the cost for reasons of long-term commercial benefits and national pride. South Korea has long seen North Korea's significantly longer missile range as a serious threat to its national security. With the nation's first astronaut launched into space, Lee So-yeon, South Korea gained confidence in entering the Asian space race. They have completed the construction of Naro Space Center. South Korea is now attempting to build satellites and rockets with local technology. South Korea is pursuing a space program that could defend the peninsula while lessening their dependency on the United States.\n\nPakistan started pursuing space technology on 16 September 1961, when Pakistan's space agency, SUPARCO was created, with renowned physicist Abdus Salam as its first administrator. In its early days, SUPARCO researched on the development of solid-fuel sounding rockets with assistance provided by the United States. On 7 June 1962, with the launch of the \"Rehbar-I\" (\"lit. Teller of the Way\") rocket, Pakistan became the tenth country in the world to successfully conduct the launch of an unmanned spacecraft. This rocket had been developed by a team of scientists and engineers of the Pakistan Air Force, led by Air Commodore (Brigadier-General) Władysław Józef Marian Turowicz in collaboration with NASA and was launched from Sonmiani, Pakistan's first space launch facility. SUPARCO's unmanned space program continued till 1972, with nearly 20 successful launches. SUPARCO's unmanned space program suffered setbacks during the 1970s and the 1980s, delaying the development and launch of Pakistan's first satellite, Badr-I till 1990 when it was launched from China. SUPARCO launched Pakistan's second satellite, Badr-B in 2001 from Baikonur Cosmodrome using a Ukrainian Zenit-2 rocket, followed by Paksat-1R in 2011 which was contracted and actually built and launched by China, was Pakistan's first communication satellite. On 9 July, 2018, Pakistan launched two satellites from China's Jiuquan Satellite Launch Centre with Long March 2C Rocket. The Pakistan Remote Sensing Satellite (PRSS-1) was a dual-purpose Earth observational and optical satellite. It was designed and developed by SUPARCO’s engineers. It was weight satellite which will operate at an altitude of . With the launching of PRSS-1 , Pakistan has become one of the few countries to have its own remote sensing satellite in the orbit. Another flagship satellite, Pakistan Technology Evaluation Satellite (PakTES-1A) which was also designed and developed by SUPARCO’s engineers, has also co-launched with PRSS-1 by the same launch rocket, Long March 2C. It was a weight satellite with an optical payload commensurate with national needs. It will operate at an altitude of .\n\nPakistan intends to send its first national in space onboard a Chinese spacecraft by 2022, as announced by information minister Chaudhry. \n\nWith the planned launched of Bangabandhu-1 satellite purchased abroad, Bangladesh will operate its first communication satellite. Bangladesh Space Agency intends to launch satellites after 2020. Bangladesh's government has stressed that the country seeks an \"entirely peaceful and commercial\" role in space.\n\nIndonesia operates their own communication satellites purchased abroad, and intends to develop and use their own small space launch vehicle Pengorbitan (RPS-420).\n\nOther space players are Malaysia and Turkey, that announced multi-task space programs in 2006 and 2007. They intend to develop their own satellites and launchers in the near future, and manned space facilities. As of 2012 Turkey was developing its own military satellite. The first Göktürk satellite is planned to be launched in 2013. The Turkish satellite is planned to be capable of taking satellite images of greater than two meters per pixel resolution, thus making Turkey the second nation in the world capable of such a feat, after the United States.\n\n\"Also see the section: Comparison of key technologies\"\n\n\nRecords of each country are listed by chronological order unless otherwise noted.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n? : Date is assumed\n\"Only projects with under-development or above status have been listed\"\n\nSolar System exploration and manned spaceflights are major space technologies in the public eye. Since Sakigake, the first interplanetary probe in Asia, was launched in 1985, Japan has completed the most planetary exploration, but other nations are catching up.\n\nThe Moon is thought to be rich in Helium-3, which could one day be used in nuclear fusion power plants to fuel future energy demands in Asia. All three main Asian space powers plan to send men to the Moon in the distant future and have already sent lunar probes.\n\nJapan was the first Asian country to launch a lunar probe. The Hiten (Japanese: \"flying angel\") spacecraft (known before the launch as MUSES-A), built by the Institute of Space and Astronautical Science of Japan, was launched on 24 January 1990. In many ways, the mission did not go as was planned. Kaguya, the second Japanese lunar orbiter spacecraft, was launched on 14 September 2007.\n\nChina launched its first lunar probe, Chang'e-1, on 24 October 2007 and successfully entered lunar orbit on 5 November 2007.\n\nIndia launched its first lunar probe, Chandrayaan-1, on 22 October 2008 and successfully entered its final lunar orbit on 2 November 2008. The mission was considered a major success and the probe detected water on the lunar surface.\n\nThe first confirmed Moon landing from Asia was Hiten's mission in 1993. An intentional hard landing at the end of the mission, some pictures of the lunar surface were taken before impact. Hiten was not designed as a Moon lander and had few scientific instruments for lunar exploration. The next Japanese Moon landing program was the LUNAR-A, developed from 1992. Although the LUNAR-A orbiter was cancelled, its penetrators are integrated into the Russian Luna-Glob program, which was scheduled to launch in 2011. The penetrators are \"relatively\" hard landers, but they are not expected to be destroyed at impact.\n\nThe first Asian probe that was part of a lunar landing program was the Indian Moon Impact Probe (MIP) released from Chandrayaan-1 in 2008. MIP was a hard lander and was designed to move the ground under for research purposes. MIP was designed to be destroyed at impact. Its instruments performed lunar observations to within 25 minutes before impact. The landing test will be applied to future soft landings such as Chandrayaan-2, planned for 2016.\n\nThe Chinese Chang'e-1 spacecraft also achieved a systematic hard landing at the end of its mission in 2009, when China became the sixth country to reach the lunar surface. One purpose of the lander was to pre-test for future soft landings. A Chinese lunar soft lander is achieved with the Chang'e-3 mission.\n\nJapanese interplanetary probes have been mostly limited to Small Solar System bodies such as comets and asteroids. JAXA's Nozomi probe was launched in 1998, but contact was lost with the probe due to electrical failures before visiting the planet Mars. The second Japanese probe for the planet Venus, Akatsuki, was launched in 2010. Akatsuki entered orbit around Venus on December 7, 2015.\n\nChinese scientists expect that China will take 20 years to launch independent planetary probes. The Chinese manned Mars exploration program is planned for around 2050 by the Chinese Academy of Sciences.\n\nIndia has successfully launched Mars Orbiter Mission on November 5, 2013. It reached Mars on September 2014. India has become the only country to successfully insert a satellite into Martian orbit in its maiden attempt; it also became the first Asian country to achieve this feat.\n\n\n\n"}
{"id": "838846", "url": "https://en.wikipedia.org/wiki?curid=838846", "title": "Concept inventory", "text": "Concept inventory\n\nA concept inventory is a criterion-referenced test designed to help determine whether a student has an accurate working knowledge of a specific set of concepts. Historically, concept inventories have been in the form of multiple-choice tests in order to aid interpretability and facilitate administration in large classes. Unlike a typical, teacher-authored multiple-choice test, questions and response choices on concept inventories are the subject of extensive research. The aims of the research include ascertaining (a) the range of what individuals think a particular question is asking and (b) the most common responses to the questions. Concept inventories are evaluated to ensure test reliability and validity. In its final form, each question includes one correct answer and several distractors.\n\nIdeally, a score on a criterion-referenced test reflects the amount of content knowledge a student has mastered. Criterion-referenced tests differ from norm-referenced tests in that (in theory) the former is not used to compare an individual's score to the scores of the group. Ordinarily, the purpose of a criterion-referenced test is to ascertain whether a student mastered a predetermined amount of content knowledge; upon obtaining a test score that is at or above a cutoff score, the student can move on to study a body of content knowledge that follows next in a learning sequence. In general, item difficulty values ranging between 30% and 70% are best able to provide information about student understanding.\n\nThe distractors are incorrect or irrelevant answers that are usually (but not always) based on students' commonly held misconceptions. Test developers often research student misconceptions by examining students' responses to open-ended essay questions and conducting \"think-aloud\" interviews with students. The distractors chosen by students help researchers understand student thinking and give instructors insights into students' prior knowledge (and, sometimes, firmly held beliefs). This foundation in research underlies instrument construction and design, and plays a role in helping educators obtain clues about students' ideas, scientific misconceptions, and didaskalogenic (\"teacher-induced\" or \"teaching-induced\") confusions and conceptual lacunae that interfere with learning.\n\nConcept inventories are education-related diagnostic tests. In 1985 Halloun and Hestenes introduced a \"multiple-choice mechanics diagnostic test\" to examine students' concepts about motion. It evaluates student understanding of basic concepts in classical (macroscopic) mechanics. A little later, the Force Concept Inventory (FCI), another concept inventory, was developed. The FCI was designed to assess student understanding of the Newtonian concepts of force. Hestenes (1998) found that while \"nearly 80% of the [students completing introductory college physics courses] could state Newton's Third Law at the beginning of the course. FCI data showed that less than 15% of them fully understood it at the end\".These results have been replicated in a number of studies involving students at a range of institutions (see sources section below). That said, there remains questions as what exactly the FCI measures. Results from Hake (1998) using the FCI have led to greater recognition in the science education community of the importance of students' \"interactive engagement\" with the materials to be mastered. .\n\nSince the development of the FCI, other physics instruments have been developed. These include the Force and Motion Conceptual Evaluation developed by Thornton and Sokoloff and the Brief Electricity and Magnetism Assessment developed by Ding et al. For a discussion of how a number of concept inventories were developed see Beichner. Information about physics concept tests can be found at the NC State Physics Education Research Group website (see the external links below).\n\nIn addition to physics, concept inventories have been developed in statistics, chemistry, astronomy, basic biology, natural selection, genetics, engineering, geoscience. and computer science.\n\nIn many areas, foundational scientific concepts transcend disciplinary boundaries. An example of an inventory that assesses knowledge of such concepts is an instrument developed by Odom and Barrow (1995) to evaluate understanding of diffusion and osmosis. In addition, there are non-multiple choice conceptual instruments, such as the essay-based approach suggested by Wright et al. (1998) and the essay and oral exams used by Nehm and Schonfeld (2008). and Cooper et al to measure student understanding of Lewis structures in chemistry.\n\nSome concept inventories are problematic. The concepts tested may not be fundamental or important in a particular discipline, the concepts involved may not be explicitly taught in a class or curriculum, or answering a question correctly may require only a superficial understanding of a topic. It is therefore possible to either over-estimate or under-estimate student content mastery. While concept inventories designed to identify trends in student thinking may not be useful in monitoring learning gains as a result of pedagogical interventions, disciplinary mastery may not be the variable measured by a particular instrument. Users should be careful to ensure that concept inventories are actually testing conceptual understanding, rather than test-taking ability, language skills, or other abilities that can influence test performance.\n\nThe use of multiple-choice exams as concept inventories is not without controversy. The very structure of multiple-choice type concept inventories raises questions involving the extent to which complex, and often nuanced situations and ideas must be simplified or clarified to produce unambiguous responses. For example, a multiple-choice exam designed to assess knowledge of key concepts in natural selection does not meet a number of standards of quality control. One problem with the exam is that the two members of each of several pairs of parallel items, with each pair designed to measure exactly one key concept in natural selection, sometimes have very different levels of difficulty. Another problem is that the multiple-choice exam overestimates knowledge of natural selection as reflected in student performance on a diagnostic essay exam and a diagnostic oral exam, two instruments with reasonably good construct validity. Although scoring concept inventories in the form of essay or oral exams is labor-intensive, costly, and difficult to implement with large numbers of students, such exams can offer a more realistic appraisal of the actual levels of students' conceptual mastery as well as their misconceptions. Recently, however, computer technology has been developed that can score essay responses on concept inventories in biology and other domains (Nehm, Ha, & Mayfield, 2011), promising to facilitate the scoring of concept inventories organized as (transcribed) oral exams as well as essays.\n\n"}
{"id": "142910", "url": "https://en.wikipedia.org/wiki?curid=142910", "title": "Creativity", "text": "Creativity\n\nCreativity is a phenomenon whereby something new and somehow valuable is formed. The created item may be intangible (such as an idea, a scientific theory, a musical composition, or a joke) or a physical object (such as an invention, a literary work, or a painting).\n\nScholarly interest in creativity is found in a number of disciplines, primarily psychology, business studies, and cognitive science, but also education, technology, engineering, philosophy (particularly philosophy of science), theology, sociology, linguistics, and economics, covering the relations between creativity and general intelligence, personality type, mental and neurological processes, mental health, or artificial intelligence; the potential for fostering creativity through education and training; the fostering of creativity for national economic benefit, and the application of creative resources to improve the effectiveness of teaching and learning.\n\nThe lexeme in the English word \"creativity\" comes from the Latin term \"creō\" \"to create, make\": its derivational suffixes also come from Latin. The word \"create\" appeared in English as early as the 14th century, notably in Chaucer, to indicate divine creation (in The Parson's Tale). However, its modern meaning as an act of human creation did not emerge until after the Enlightenment.\n\nIn a summary of scientific research into creativity, Michael Mumford suggested: \"Over the course of the last decade, however, we seem to have reached a general agreement that creativity involves the production of novel, useful products\" (Mumford, 2003, p. 110), or, in Robert Sternberg's words, the production of \"something original and worthwhile\". Authors have diverged dramatically in their precise definitions beyond these general commonalities: Peter Meusburger reckons that over a hundred different analyses can be found in the literature. As an illustration, one definition given by Dr. E. Paul Torrance described it as \"a process of becoming sensitive to problems, deficiencies, gaps in knowledge, missing elements, disharmonies, and so on; identifying the difficulty; searching for solutions, making guesses, or formulating hypotheses about the deficiencies: testing and retesting these hypotheses and possibly modifying and retesting them; and finally communicating the results.\"\n\nTheories of creativity (particularly investigation of why some people are more creative than others) have focused on a variety of aspects. The dominant factors are usually identified as \"the four Ps\" — process, product, person, and place (according to Mel Rhodes). A focus on \"process\" is shown in cognitive approaches that try to describe thought mechanisms and techniques for creative thinking. Theories invoking divergent rather than convergent thinking (such as Guilford), or those describing the staging of the creative process (such as Wallas) are primarily theories of creative process. A focus on creative \"product\" usually appears in attempts to measure creativity (psychometrics, see below) and in creative ideas framed as successful memes. The psychometric approach to creativity reveals that it also involves the ability to produce more.\nA focus on the nature of the creative \"person\" considers more general intellectual habits, such as openness, levels of ideation, autonomy, expertise, exploratory behavior, and so on. A focus on \"place\" considers the circumstances in which creativity flourishes, such as degrees of autonomy, access to resources, and the nature of gatekeepers. Creative lifestyles are characterized by nonconforming attitudes and behaviors as well as flexibility.\n\nMost ancient cultures, including thinkers of Ancient Greece, Ancient China, and Ancient India, lacked the concept of creativity, seeing art as a form of discovery and not creation. The ancient Greeks had no terms corresponding to \"to create\" or \"creator\" except for the expression \"poiein\" (\"to make\"), which only applied to \"poiesis\" (poetry) and to the \"poietes\" (poet, or \"maker\") who made it. Plato did not believe in art as a form of creation. Asked in \"The Republic\", \"Will we say, of a painter, that he makes something?\", he answers, \"Certainly not, he merely imitates.\"\n\nIt is commonly argued that the notion of \"creativity\" originated in Western culture through Christianity, as a matter of divine inspiration. According to the historian Daniel J. Boorstin, \"the early Western conception of creativity was the Biblical story of creation given in the \"Genesis\".\" However, this is not creativity in the modern sense, which did not arise until the Renaissance. In the Judaeo-Christian tradition, creativity was the sole province of God; humans were not considered to have the ability to create something new except as an expression of God's work. A concept similar to that of Christianity existed in Greek culture, for instance, Muses were seen as mediating inspiration from the Gods. Romans and Greeks invoked the concept of an external creative \"daemon\" (Greek) or \"genius\" (Latin), linked to the sacred or the divine. However, none of these views are similar to the modern concept of creativity, and the individual was not seen as the cause of creation until the Renaissance. It was during the Renaissance that creativity was first seen, not as a conduit for the divine, but from the abilities of \"great men\".\n\nThe rejection of creativity in favor of discovery and the belief that individual creation was a conduit of the divine would dominate the West probably until the Renaissance and even later. The development of the modern concept of creativity begins in the Renaissance, when creation began to be perceived as having originated from the abilities of the individual, and not God. This could be attributed to the leading intellectual movement of the time, aptly named humanism, which developed an intensely human-centric outlook on the world, valuing the intellect and achievement of the individual. From this philosophy arose the Renaissance man (or polymath), an individual who embodies the principals of humanism in their ceaseless courtship with knowledge and creation. One of the most well-known and immensely accomplished examples is Leonardo da Vinci.\n\nHowever, this shift was gradual and would not become immediately apparent until the Enlightenment. By the 18th century and the Age of Enlightenment, mention of creativity (notably in aesthetics), linked with the concept of imagination, became more frequent. In the writing of Thomas Hobbes, imagination became a key element of human cognition; William Duff was one of the first to identify imagination as a quality of genius, typifying the separation being made between talent (productive, but breaking no new ground) and genius.\n\nAs a direct and independent topic of study, creativity effectively received no attention until the 19th century. Runco and Albert argue that creativity as the subject of proper study began seriously to emerge in the late 19th century with the increased interest in individual differences inspired by the arrival of Darwinism. In particular, they refer to the work of Francis Galton, who through his eugenicist outlook took a keen interest in the heritability of intelligence, with creativity taken as an aspect of genius.\n\nIn the late 19th and early 20th centuries, leading mathematicians and scientists such as Hermann von Helmholtz (1896) and Henri Poincaré (1908) began to reflect on and publicly discuss their creative processes.\n\nThe insights of Poincaré and von Helmholtz were built on in early accounts of the creative process by pioneering theorists such as Graham Wallas and Max Wertheimer. In his work \"Art of Thought\", published in 1926, Wallas presented one of the first models of the creative process. In the Wallas stage model, creative insights and illuminations may be explained by a process consisting of 5 stages:\nWallas' model is often treated as four stages, with \"intimation\" seen as a sub-stage.\n\nWallas considered creativity to be a legacy of the evolutionary process, which allowed humans to quickly adapt to rapidly changing environments. Simonton provides an updated perspective on this view in his book, \"Origins of genius: Darwinian perspectives on creativity\".\n\nIn 1927, Alfred North Whitehead gave the Gifford Lectures at the University of Edinburgh, later published as \"Process and Reality.\" He is credited with having coined the term \"creativity\" to serve as the ultimate category of his metaphysical scheme: \"Whitehead actually coined the term – our term, still the preferred currency of exchange among literature, science, and the arts. . . a term that quickly became so popular, so omnipresent, that its invention within living memory, and by Alfred North Whitehead of all people, quickly became occluded\".\n\nThe formal psychometric measurement of creativity, from the standpoint of orthodox psychological literature, is usually considered to have begun with J. P. Guilford's 1950 address to the American Psychological Association, which helped popularize the topic and focus attention on a scientific approach to conceptualizing creativity. (It should be noted that the London School of Psychology had instigated psychometric studies of creativity as early as 1927 with the work of H. L. Hargreaves into the Faculty of Imagination, but it did not have the same impact.) Statistical analysis led to the recognition of creativity (as measured) as a separate aspect of human cognition to IQ-type intelligence, into which it had previously been subsumed. Guilford's work suggested that above a threshold level of IQ, the relationship between creativity and classically measured intelligence broke down.\n\nJames C. Kaufman and Beghetto introduced a \"four C\" model of creativity; \"mini-c\" (\"transformative learning\" involving \"personally meaningful interpretations of experiences, actions, and insights\"), \"little-c\" (everyday problem solving and creative expression), \"Pro-C\" (exhibited by people who are professionally or vocationally creative though not necessarily eminent) and \"Big-C\" (creativity considered great in the given field). This model was intended to help accommodate models and theories of creativity that stressed competence as an essential component and the historical transformation of a creative domain as the highest mark of creativity. It also, the authors argued, made a useful framework for analyzing creative processes in individuals.\n\nThe contrast of terms \"Big C\" and \"Little c\" has been widely used. Kozbelt, Beghetto and Runco use a little-c/Big-C model to review major theories of creativity. Margaret Boden distinguishes between h-creativity (historical) and p-creativity (personal).\n\nRobinson and Anna Craft have focused on creativity in a general population, particularly with respect to education. Craft makes a similar distinction between \"high\" and \"little c\" creativity. and cites Ken Robinson as referring to \"high\" and \"democratic\" creativity. Mihaly Csikszentmihalyi has defined creativity in terms of those individuals judged to have made significant creative, perhaps domain-changing contributions. Simonton has analysed the career trajectories of eminent creative people in order to map patterns and predictors of creative productivity.\n\nThere has been much empirical study in psychology and cognitive science of the processes through which creativity occurs. Interpretation of the results of these studies has led to several possible explanations of the sources and methods of creativity.\n\nIncubation is a temporary break from creative problem solving that can result in insight. There has been some empirical research looking at whether, as the concept of \"incubation\" in Wallas' model implies, a period of interruption or rest from a problem may aid creative problem-solving. Ward lists various hypotheses that have been advanced to explain why incubation may aid creative problem-solving, and notes how some empirical evidence is consistent with the hypothesis that incubation aids creative problem-solving in that it enables \"forgetting\" of misleading clues. Absence of incubation may lead the problem solver to become fixated on inappropriate strategies of solving the problem. This work disputes the earlier hypothesis that creative solutions to problems arise mysteriously from the unconscious mind while the conscious mind is occupied on other tasks. \nThis earlier hypothesis is discussed in Csikszentmihalyi's five phase model of the creative process which describes incubation as a time that your unconscious takes over. This allows for unique connections to be made without your consciousness trying to make logical order out of the problem.\n\nJ. P. Guilford drew a distinction between convergent and divergent production (commonly renamed convergent and divergent thinking). Convergent thinking involves aiming for a single, correct solution to a problem, whereas divergent thinking involves creative generation of multiple answers to a set problem. Divergent thinking is sometimes used as a synonym for creativity in psychology literature. Other researchers have occasionally used the terms \"flexible\" thinking or fluid intelligence, which are roughly similar to (but not synonymous with) creativity.\n\nIn 1992, Finke et al. proposed the \"Geneplore\" model, in which creativity takes place in two phases: a generative phase, where an individual constructs mental representations called preinventive structures, and an exploratory phase where those structures are used to come up with creative ideas. Some evidence shows that when people use their imagination to develop new ideas, those ideas are heavily structured in predictable ways by the properties of existing categories and concepts. Weisberg argued, by contrast, that creativity only involves ordinary cognitive processes yielding extraordinary results.\n\nHelie and Sun recently proposed a unified framework for understanding creativity in problem solving, namely the Explicit–Implicit Interaction (EII) theory of creativity. This new theory constitutes an attempt at providing a more unified explanation of relevant phenomena (in part by reinterpreting/integrating various fragmentary existing theories of incubation and insight).\n\nThe EII theory relies mainly on five basic principles, namely:\nA computational implementation of the theory was developed based on the CLARION cognitive architecture and used to simulate relevant human data. This work represents an initial step in the development of process-based theories of creativity encompassing incubation, insight, and various other related phenomena.\n\nIn \"The Act of Creation\", Arthur Koestler introduced the concept of \"bisociation\" — that creativity arises as a result of the intersection of two quite different frames of reference. This idea was later developed into conceptual blending. In the 1990s, various approaches in cognitive science that dealt with metaphor, analogy, and structure mapping have been converging, and a new integrative approach to the study of creativity in science, art and humor has emerged under the label conceptual blending.\n\nHoning theory, developed principally by psychologist Liane Gabora, posits that creativity arises due to the self-organizing, self-mending nature of a worldview. The creative process is a way in which the individual hones (and re-hones) an integrated worldview. Honing theory places emphasis not only on the externally visible creative outcome but also the internal cognitive restructuring and repair of the worldview brought about by the creative process. When faced with a creatively demanding task, there is an interaction between the conception of the task and the worldview. The conception of the task changes through interaction with the worldview, and the worldview changes through interaction with the task. This interaction is reiterated until the task is complete, at which point not only is the task conceived of differently, but the worldview is subtly or drastically transformed as it follows the natural tendency of a worldview to attempt to resolve dissonance and seek internal consistency amongst its components, whether they be ideas, attitudes, or bits of knowledge.\n\nA central feature of honing theory is the notion of a potentiality state. Honing theory posits that creative thought proceeds not by searching through and randomly ‘mutating’ predefined possibilities, but by drawing upon associations that exist due to overlap in the distributed neural cell assemblies that participate in the encoding of experiences in memory. Midway through the creative process one may have made associations between the current task and previous experiences, but not yet disambiguated which aspects of those previous experiences are relevant to the current task. Thus the creative idea may feel ‘half-baked’. It is at that point that it can be said to be in a potentiality state, because how it will actualize depends on the different internally or externally generated contexts it interacts with.\n\nHoning theory is held to explain certain phenomena not dealt with by other theories of creativity, for example, how different works by the same creator are observed in studies to exhibit a recognizable style or 'voice' even through in different creative outlets. This is not predicted by theories of creativity that emphasize chance processes or the accumulation of expertise, but it is predicted by honing theory, according to which personal style reflects the creator's uniquely structured worldview. Another example is in the environmental stimulus for creativity. Creativity is commonly considered to be fostered by a supportive, nurturing, trustworthy environment conducive to self-actualization. However, research shows that creativity is also associated with childhood adversity, which would stimulate honing.\n\nIn everyday thought, people often spontaneously imagine alternatives to reality when they think \"if only...\". Their counterfactual thinking is viewed as an example of everyday creative processes. It has been proposed that the creation of counterfactual alternatives to reality depends on similar cognitive processes to rational thought.\n\nThere was a \"creativity quotient\" developed similar to the intelligence quotient (IQ). It makes use of the results of divergent thinking tests (see below) by processing them further. It gives more weight to ideas that are radically different from other ideas in the response.\n\nJ. P. Guilford's group, which pioneered the modern psychometric study of creativity, constructed several tests to measure creativity in 1967:\n\nBuilding on Guilford's work, Torrance developed the Torrance Tests of Creative Thinking in 1966. They involved simple tests of divergent thinking and other problem-solving skills, which were scored on:\n\nSuch tests, sometimes called \"Divergent Thinking (DT)\" tests have been both supported and criticized.<ref name=\"doi10.1080/10400419.2011.545713\"></ref>\n\nConsiderable progress has been made in automated scoring of divergent thinking tests using semantic approach. When compared to human raters, NLP techniques were shown to be reliable and valid in scoring the originality. The reported computer programs were able to achieve a correlation of 0.60 and 0.72 respectively to human graders.\n\nSemantic networks were also used to devise originality scores that yielded significant correlations with socio-personal measures. Most recently, an NSF-funded team of researchers led by James C. Kaufman and Mark A. Runco combined expertise in creativity research, natural language processing, computational linguistics, and statistical data analysis to devise a scalable system for computerized automated testing (SparcIt Creativity Index Testing system). This system enabled automated scoring of DT tests that is reliable, objective, and scalable, thus addressing most of the issues of DT tests that had been found and reported. The resultant computer system was able to achieve a correlation of 0.73 to human graders.\n\nSome researchers have taken a social-personality approach to the measurement of creativity. In these studies, personality traits such as independence of judgement, self-confidence, attraction to complexity, aesthetic orientation, and risk-taking are used as measures of the creativity of individuals. A meta-analysis by Gregory Feist showed that creative people tend to be \"more open to new experiences, less conventional and less conscientious, more self-confident, self-accepting, driven, ambitious, dominant, hostile, and impulsive.\" Openness, conscientiousness, self-acceptance, hostility, and impulsivity had the strongest effects of the traits listed. Within the framework of the Big Five model of personality, some consistent traits have emerged. Openness to experience has been shown to be consistently related to a whole host of different assessments of creativity. Among the other Big Five traits, research has demonstrated subtle differences between different domains of creativity. Compared to non-artists, artists tend to have higher levels of openness to experience and lower levels of conscientiousness, while scientists are more open to experience, conscientious, and higher in the confidence-dominance facets of extraversion compared to non-scientists.\n\nAn alternative are biographical methods. These methods use quantitative characteristics such as the number of publications, patents, or performances of a work. While this method was originally developed for highly creative personalities, today it is also available as self-report questionnaires supplemented with frequent, less outstanding creative behaviors such as writing a short story or creating your own recipes. For example, the Creative Achievement Questionnaire, a self-report test that measures creative achievement across 10 domains, was described in 2005 and shown to be reliable and valid when compared to other measures of creativity and to independent evaluation of creative output. Besides the English original, it was also used in a Chinese, French, and German-speaking version. It is the self-report questionnaire most frequently used in research.\n\nThe potential relationship between creativity and intelligence has been of interest since the late 1900s, when a multitude of influential studies – from Getzels & Jackson, Barron, Wallach & Kogan, and Guilford – focused not only on creativity, but also on intelligence. This joint focus highlights both the theoretical and practical importance of the relationship: researchers are interested not only if the constructs are related, but also how and why.\n\nThere are multiple theories accounting for their relationship, with the 3 main theories as follows:\n\n\nSternberg and O’Hara proposed a framework of 5 possible relationships between creativity and intelligence:\n\nA number of researchers include creativity, either explicitly or implicitly, as a key component of intelligence.\n\n\"Examples of theories that include creativity as a subset of intelligence\"\n\n\nIn this relationship model, intelligence is a key component in the development of creativity.\n\n\"Theories of creativity that include intelligence as a subset of creativity\"\n\n\nThis possible relationship concerns creativity and intelligence as distinct, but intersecting constructs.\n\n\"Theories that include Creativity and Intelligence as Overlapping Yet Distinct Constructs\"\n\n\nIn support of the TT, Barron reported finding a non-significant correlation between creativity and intelligence in a gifted sample; and a significant correlation in a non-gifted sample. Yamamoto in a sample of secondary school children, reported a significant correlation between creativity and intelligence of \"r\" = .3, and reported no significant correlation when the sample consisted of gifted children. Fuchs-Beauchamp et al. in a sample of preschoolers found that creativity and intelligence correlated from \"r\" = .19 to \"r\" = .49 in the group of children who had an IQ below the threshold; and in the group above the threshold, the correlations were \"r\" = <.12. Cho et al. reported a correlation of .40 between creativity and intelligence in the average IQ group of a sample of adolescents and adults; and a correlation of close to \"r\" = .0 for the high IQ group. Jauk et al. found support for the TT, but only for measures of creative potential; not creative performance.\n\nMuch modern day research reports findings against TT. Wai et al. in a study using data from the longitudinal Study of Mathematically Precocious Youth – a cohort of elite students from early adolescence into adulthood – found that differences in SAT scores at age 13 were predictive of creative real-life outcomes 20 years later. Kim’s meta-analysis of 21 studies did not find any supporting evidence for TT, and instead negligible correlations were reported between intelligence, creativity, and divergent thinking both below and above IQ's of 120. Preckel et al., investigating fluid intelligence and creativity, reported small correlations of \"r\" = .3 to \"r\" = .4 across all levels of cognitive ability.\n\nUnder this view, researchers posit that there are no differences in the mechanisms underlying creativity in those used in normal problem solving; and in normal problem solving, there is no need for creativity. Thus, creativity and Intelligence (problem solving) are the same thing. Perkins referred to this as the ‘nothing-special’ view.\n\nWeisberg & Alba examined problem solving by having participants complete the 9-dot problem (see Thinking outside the box#Nine dots puzzle) – where the participants are asked to connect all 9 dots in the 3 rows of 3 dots using 4 straight lines or less, without lifting their pen or tracing the same line twice. The problem can only be solved if the lines go outside the boundaries of the square of dots. Results demonstrated that even when participants were given this insight, they still found it difficult to solve the problem, thus showing that to successfully complete the task it is not just insight (or creativity) that is required.\n\nIn this view, creativity and intelligence are completely different, unrelated constructs.\n\nGetzels and Jackson administered 5 creativity measures to a group of 449 children from grades 6-12, and compared these test findings to results from previously administered (by the school) IQ tests. They found that the correlation between the creativity measures and IQ was \"r\" = .26. The high creativity group scored in the top 20% of the overall creativity measures, but were not included in the top 20% of IQ scorers. The high intelligence group scored the opposite: they scored in the top 20% for IQ, but were outside the top 20% scorers for creativity, thus showing that creativity and intelligence are distinct and unrelated.\n\nHowever, this work has been heavily criticised. Wallach and Kogan highlighted that the creativity measures were not only weakly related to one another (to the extent that they were no more related to one another than they were with IQ), but they seemed to also draw upon non-creative skills. McNemar noted that there were major measurement issues, in that the IQ scores were a mixture from 3 different IQ tests.\n\nWallach and Kogan administered 5 measures of creativity, each of which resulted in a score for originality and fluency; and 10 measures of general intelligence to 151 5th grade children. These tests were untimed, and given in a game-like manner (aiming to facilitate creativity). Inter-correlations between creativity tests were on average \"r\" = .41. Inter-correlations between intelligence measures were on average \"r\" = .51 with each other. Creativity tests and intelligence measures correlated \"r\" = .09.\n\nThe neuroscience of creativity looks at the operation of the brain during creative behaviour. It has been addressed in the article \"Creative Innovation: Possible Brain Mechanisms.\" The authors write that \"creative innovation might require coactivation and communication between regions of the brain that ordinarily are not strongly connected.\" Highly creative people who excel at creative innovation tend to differ from others in three ways:\n\nThus, the frontal lobe appears to be the part of the cortex that is most important for creativity.\n\nThis article also explored the links between creativity and sleep, mood and addiction disorders, and depression.\n\nIn 2005, Alice Flaherty presented a three-factor model of the creative drive. Drawing from evidence in brain imaging, drug studies and lesion analysis, she described the creative drive as resulting from an interaction of the frontal lobes, the temporal lobes, and dopamine from the limbic system. The frontal lobes can be seen as responsible for idea generation, and the temporal lobes for idea editing and evaluation. Abnormalities in the frontal lobe (such as depression or anxiety) generally decrease creativity, while abnormalities in the temporal lobe often increase creativity. High activity in the temporal lobe typically inhibits activity in the frontal lobe, and vice versa. High dopamine levels increase general arousal and goal directed behaviors and reduce latent inhibition, and all three effects increase the drive to generate ideas. A 2015 study on creativity found that it involves the interaction of multiple neural networks, including those that support associative thinking, along with other default mode network functions.\n\nVandervert described how the brain's frontal lobes and the cognitive functions of the cerebellum collaborate to produce creativity and innovation. Vandervert's explanation rests on considerable evidence that all processes of working memory (responsible for processing all thought) are adaptively modeled for increased efficiency by the cerebellum. The cerebellum (consisting of 100 billion neurons, which is more than the entirety of the rest of the brain) is also widely known to adaptively model all bodily movement for efficiency. The cerebellum's adaptive models of working memory processing are then fed back to especially frontal lobe working memory control processes where creative and innovative thoughts arise. (Apparently, creative insight or the \"aha\" experience is then triggered in the temporal lobe.)\n\nAccording to Vandervert, the details of creative adaptation begin in \"forward\" cerebellar models which are anticipatory/exploratory controls for movement and thought. These cerebellar processing and control architectures have been termed Hierarchical Modular Selection and Identification for Control (HMOSAIC). New, hierarchically arranged levels of the cerebellar control architecture (HMOSAIC) develop as mental mulling in working memory is extended over time. These new levels of the control architecture are fed forward to the frontal lobes. Since the cerebellum adaptively models all movement and all levels of thought and emotion, Vandervert's approach helps explain creativity and innovation in sports, art, music, the design of video games, technology, mathematics, the child prodigy, and thought in general.\n\nEssentially, Vandervert has argued that when a person is confronted with a challenging new situation, visual-spatial working memory and speech-related working memory are decomposed and re-composed (fractionated) by the cerebellum and then blended in the cerebral cortex in an attempt to deal with the new situation. With repeated attempts to deal with challenging situations, the cerebro-cerebellar blending process continues to optimize the efficiency of how working memory deals with the situation or problem. Most recently, he has argued that this is the same process (only involving visual-spatial working memory and pre-language vocalization) that led to the evolution of language in humans. Vandervert and Vandervert-Weathers have pointed out that this blending process, because it continuously optimizes efficiencies, constantly improves prototyping attempts toward the invention or innovation of new ideas, music, art, or technology. Prototyping, they argue, not only produces new products, it trains the cerebro-cerebellar pathways involved to become more efficient at prototyping itself. Further, Vandervert and Vandervert-Weathers believe that this repetitive \"mental prototyping\" or mental rehearsal involving the cerebellum and the cerebral cortex explains the success of the self-driven, individualized patterning of repetitions initiated by the teaching methods of the Khan Academy. The model proposed by Vandervert has, however, received incisive critique from several authors.\n\nCreativity involves the forming of associative elements into new combinations that are useful or meet some requirement. Sleep aids this process. REM rather than NREM sleep appears to be responsible. This has been suggested to be due to changes in cholinergic and noradrenergic neuromodulation that occurs during REM sleep. During this period of sleep, high levels of acetylcholine in the hippocampus suppress feedback from the hippocampus to the neocortex, and lower levels of acetylcholine and norepinephrine in the neocortex encourage the spread of associational activity within neocortical areas without control from the hippocampus. This is in contrast to waking consciousness, where higher levels of norepinephrine and acetylcholine inhibit recurrent connections in the neocortex. It is proposed that REM sleep adds creativity by allowing \"neocortical structures to reorganize associative hierarchies, in which information from the hippocampus would be reinterpreted in relation to previous semantic representations or nodes.\"\n\nSome theories suggest that creativity may be particularly susceptible to affective influence. As noted in voting behavior, the term \"affect\" in this context can refer to liking or disliking key aspects of the subject in question. This work largely follows from findings in psychology regarding the ways in which affective states are involved in human judgment and decision-making.\n\nAccording to Alice Isen, positive affect has three primary effects on cognitive activity:\n\nBarbara Fredrickson in her broaden-and-build model suggests that positive emotions such as joy and love broaden a person's available repertoire of cognitions and actions, thus enhancing creativity.\n\nAccording to these researchers, positive emotions increase the number of cognitive elements available for association (attention scope) and the number of elements that are relevant to the problem (cognitive scope).\n\nVarious meta-analyses, such as Baas et al. (2008) of 66 studies about creativity and affect support the link between creativity and positive affect.\n\nJürgen Schmidhuber's formal theory of creativity postulates that creativity, curiosity, and interestingness are by-products of a simple computational principle for measuring and optimizing learning progress. Consider an agent able to manipulate its environment and thus its own sensory inputs. The agent can use a black box optimization method such as reinforcement learning to learn (through informed trial and error) sequences of actions that maximize the expected sum of its future reward signals. There are extrinsic reward signals for achieving externally given goals, such as finding food when hungry. But Schmidhuber's objective function to be maximized also includes an additional, intrinsic term to model \"wow-effects.\" This non-standard term motivates purely creative behavior of the agent even when there are no external goals. A wow-effect is formally defined as follows. As the agent is creating and predicting and encoding the continually growing history of actions and sensory inputs, it keeps improving the predictor or encoder, which can be implemented as an artificial neural network or some other machine learning device that can exploit regularities in the data to improve its performance over time. The improvements can be measured precisely, by computing the difference in computational costs (storage size, number of required synapses, errors, time) needed to encode new observations before and after learning. This difference depends on the encoder's present subjective knowledge, which changes over time, but the theory formally takes this into account. The cost difference measures the strength of the present \"wow-effect\" due to sudden improvements in data compression or computational speed. It becomes an intrinsic reward signal for the action selector. The objective function thus motivates the action optimizer to create action sequences causing more wow-effects. Irregular, random data (or noise) do not permit any wow-effects or learning progress, and thus are \"boring\" by nature (providing no reward). Already known and predictable regularities also are boring. Temporarily interesting are only the initially unknown, novel, regular patterns in both actions and observations. This motivates the agent to perform continual, open-ended, active, creative exploration.\n\nAccording to Schmidhuber, his objective function explains the activities of scientists, artists, and comedians.\nFor example, physicists are motivated to create experiments leading to observations obeying previously unpublished physical laws permitting better data compression. Likewise, composers receive intrinsic reward for creating non-arbitrary melodies with unexpected but regular harmonies that permit wow-effects through data compression improvements.\nSimilarly, a comedian gets intrinsic reward for \"inventing a novel joke with an unexpected punch line, related to the beginning of the story in an initially unexpected but quickly learnable way that also allows for better compression of the perceived data.\"\nSchmidhuber argues that ongoing computer hardware advances will greatly scale up rudimentary artificial scientists and artists based on simple implementations of the basic principle since 1990.\nHe used the theory to create low-complexity art and an attractive human face.\n\nA study by psychologist J. Philippe Rushton found creativity to correlate with intelligence and psychoticism. Another study found creativity to be greater in schizotypal than in either normal or schizophrenic individuals. While divergent thinking was associated with bilateral activation of the prefrontal cortex, schizotypal individuals were found to have much greater activation of their \"right\" prefrontal cortex. This study hypothesizes that such individuals are better at accessing both hemispheres, allowing them to make novel associations at a faster rate. In agreement with this hypothesis, ambidexterity is also associated with schizotypal and schizophrenic individuals. Three recent studies by Mark Batey and Adrian Furnham have demonstrated the relationships between schizotypal and hypomanic personality and several different measures of creativity.\n\nParticularly strong links have been identified between creativity and mood disorders, particularly manic-depressive disorder (a.k.a. bipolar disorder) and depressive disorder (a.k.a. unipolar disorder). In \"Touched with Fire: Manic-Depressive Illness and the Artistic Temperament\", Kay Redfield Jamison summarizes studies of mood-disorder rates in writers, poets, and artists. She also explores research that identifies mood disorders in such famous writers and artists as Ernest Hemingway (who shot himself after electroconvulsive treatment), Virginia Woolf (who drowned herself when she felt a depressive episode coming on), composer Robert Schumann (who died in a mental institution), and even the famed visual artist Michelangelo. A different case study suggested for Schumann a difference between bipolar disorder and \"creative bipolarity\".\n\nA study looking at 300,000 persons with schizophrenia, bipolar disorder, or unipolar depression, and their relatives, found overrepresentation in creative professions for those with bipolar disorder as well as for undiagnosed siblings of those with schizophrenia or bipolar disorder. There was no overall overrepresentation, but overrepresentation for artistic occupations, among those diagnosed with schizophrenia. There was no association for those with unipolar depression or their relatives. Another case study suggested a difference between severe mood disorders and \"creative melancholy\", pointing out mild and moderate depression may inspire creative achievements while severe depression inhibits and may even destroy creative activity.\n\nAnother study involving more than one million people, conducted by Swedish researchers at the Karolinska Institute, reported a number of correlations between creative occupations and mental illnesses. Writers had a higher risk of anxiety and bipolar disorders, schizophrenia, unipolar depression, and substance abuse, and were almost twice as likely as the general population to kill themselves. Dancers and photographers were also more likely to have bipolar disorder.\n\nHowever, as a group, those in the creative professions were no more likely to suffer from psychiatric disorders than other people, although they were more likely to have a close relative with a disorder, including anorexia and, to some extent, autism, the Journal of Psychiatric Research reports.\n\nAccording to psychologist Robert Epstein, PhD, creativity can be obstructed through stress.\n\nCreativity can be expressed in a number of different forms, depending on unique people and environments. A number of different theorists have suggested models of the creative person. One model suggests that there are kinds to produce growth, innovation, speed, etc. These are referred to as the four \"Creativity Profiles\" that can help achieve such goals.\n\nResearch by Dr Mark Batey of the Psychometrics at Work Research Group at Manchester Business School has suggested that the creative profile can be explained by four primary creativity traits with narrow facets within each\n\nThis model was developed in a sample of 1000 working adults using the statistical techniques of Exploratory Factor Analysis followed by Confirmatory Factor Analysis by Structural Equation Modelling.\n\nAn important aspect of the creativity profiling approach is to account for the tension between predicting the creative profile of an individual, as characterised by the psychometric approach, and the evidence that team creativity is founded on diversity and difference.\n\nOne characteristic of creative people, as measured by some psychologists, is what is called \"divergent production\". \"Divergent production\" is the ability of a person to generate a diverse assortment, yet an appropriate amount of responses to a given situation. One way of measuring \"divergent production\" is by administering the Torrance Tests of Creative Thinking. The Torrance Tests of Creative Thinking assesses the diversity, quantity, and appropriateness of participants responses to a variety of open-ended questions.\n\nOther researchers of creativity see the difference in creative people as a cognitive process of dedication to problem solving and developing expertise in the field of their creative expression. Hard working people study the work of people before them and within their current area, become experts in their fields, and then have the ability to add to and build upon previous information in innovative and creative ways. In a study of projects by design students, students who had more knowledge on their subject on average had greater creativity within their projects.\n\nThe aspect of motivation within a person's personality may predict creativity levels in the person. Motivation stems from two different sources, intrinsic and extrinsic motivation. Intrinsic motivation is an internal drive within a person to participate or invest as a result of personal interest, desires, hopes, goals, etc. Extrinsic motivation is a drive from outside of a person and might take the form of payment, rewards, fame, approval from others, etc. Although extrinsic motivation and intrinsic motivation can both increase creativity in certain cases, strictly extrinsic motivation often impedes creativity in people.\n\nFrom a personality-traits perspective, there are a number of traits that are associated with creativity in people. Creative people tend to be more open to new experiences, are more self-confident, are more ambitious, self-accepting, impulsive, driven, dominant, and hostile, compared to people with less creativity.\n\nFrom an evolutionary perspective, creativity may be a result of the outcome of years of generating ideas. As ideas are continuously generated, the need to evolve produces a need for new ideas and developments. As a result, people have been creating and developing new, innovative, and creative ideas to build our progress as a society.\n\nIn studying exceptionally creative people in history, some common traits in lifestyle and environment are often found. Creative people in history usually had supportive parents, but rigid and non-nurturing. Most had an interest in their field at an early age, and most had a highly supportive and skilled mentor in their field of interest. Often the field they chose was relatively uncharted, allowing for their creativity to be expressed more in a field with less previous information. Most exceptionally creative people devoted almost all of their time and energy into their craft, and after about a decade had a creative breakthrough of fame. Their lives were marked with extreme dedication and a cycle of hard-work and breakthroughs as a result of their determination.\n\nAnother theory of creative people is the \"investment theory of creativity\". This approach suggest that there are many individual and environmental factors that must exist in precise ways for extremely high levels of creativity opposed to average levels of creativity. In the \"investment\" sense, a person with their particular characteristics in their particular environment may see an opportunity to devote their time and energy into something that has been overlooked by others. The creative person develops an undervalued or under-recognised idea to the point that it is established as a new and creative idea. Just like in the financial world, some investments are worth the buy in, while others are less productive and do not build to the extent that the investor expected. This \"investment theory of creativity\" views creativity in a unique perspective compared to others, by asserting that creativity might rely to some extent on the right investment of effort being added to a field at the right time in the right way.\n\nMalevolent creativity (MC) focuses on the \"darker side\" of creativity. This type of creativity is not typically accepted within society and is defined by the intention to cause harm to others through original and innovative means. MC should be distinguished from negative creativity in that negative creativity may unintentionally cause harm to others, whereas MC is explicitly malevolently motivated. MC is often a key contributor to crime and in its most destructive form can even manifest as terrorism. However, MC can also be observed in ordinary day-to-day life as lying, cheating and betrayal. Although everyone shows some levels of MC under certain conditions, those that have a higher propensity towards malevolent creativity have increased tendencies to deceive and manipulate others to their own gain. Although levels of MC appear to dramatically increase when an individual is placed under unfair conditions, personality is also a key predictor in anticipating levels of malevolent thinking. Researches Harris and Reiter-Palmon investigated the role of aggression in levels of MC, in particular levels of implicit aggression and the tendency to employ aggressive actions in response to problem solving. The personality traits of physical aggression, conscientiousness, emotional intelligence and implicit aggression all seem to be related with MC. Harris and Reiter-Palmon's research showed that when subjects were presented with a problem that triggered malevolent creativity, participants high in implicit aggression and low in premeditation expressed the largest number of malevolently-themed solutions. When presented with the more benign problem that triggered prosocial motives of helping others and cooperating, those high in implicit aggression, even if they were high in impulsiveness, were far less destructive in their imagined solutions. They concluded premeditation, more than implicit aggression controlled an individual’s expression of malevolent creativity.\n\nThe current measure for malevolent creativity is the 13 item test Malevolent Creativity Behaviour Scale (MCBS) \n\nMalevolent creativity has strong links with crime. As creativity requires deviating from the conventional, there is a permanent tension between being creative and producing products that go too far and in some cases to the point of breaking the law. Aggression is a key predictor of malevolent creativity, and studies have also shown that increased levels of aggression also correlates to a higher likelihood of committing crime.\n\nCreativity is viewed differently in different countries. For example, cross-cultural research centred on Hong Kong found that Westerners view creativity more in terms of the individual attributes of a creative person, such as their aesthetic taste, while Chinese people view creativity more in terms of the social influence of creative people e.g. what they can contribute to society. Mpofu et al. surveyed 28 African languages and found that 27 had no word which directly translated to 'creativity' (the exception being Arabic). The principle of linguistic relativity, i.e. that language can affect thought, suggests that the lack of an equivalent word for 'creativity' may affect the views of creativity among speakers of such languages. However, more research would be needed to establish this, and there is certainly no suggestion that this linguistic difference makes people any less (or more) creative; Africa has a rich heritage of creative pursuits such as music, art, and storytelling. Nevertheless, it is true that there has been very little research on creativity in Africa, and there has also been very little research on creativity in Latin America. Creativity has been more thoroughly researched in the northern hemisphere, but here again there are cultural differences, even between countries or groups of countries in close proximity. For example, in Scandinavian countries, creativity is seen as an individual attitude which helps in coping with life's challenges, while in Germany, creativity is seen more as a process that can be applied to help solve problems.\n\nIt has been the topic of various research studies to establish that organizational effectiveness depends on the creativity of the workforce to a large extent. For any given organization, measures of effectiveness vary, depending upon its mission, environmental context, nature of work, the product or service it produces, and customer demands. Thus, the first step in evaluating organizational effectiveness is to understand the organization itself — how it functions, how it is structured, and what it emphasizes.\n\nAmabile argued that to enhance creativity in business, three components were needed:\n\nThere are two types of motivation:\n\nSix managerial practices to encourage motivation are:\n\nNonaka, who examined several successful Japanese companies, similarly saw creativity and knowledge creation as being important to the success of organizations. In particular, he emphasized the role that tacit knowledge has to play in the creative process.\n\nIn business, originality is not enough. The idea must also be appropriate—useful and actionable. Creative competitive intelligence is a new solution to solve this problem. According to Reijo Siltala it links creativity to innovation process and competitive intelligence to creative workers.\n\nCreativity can be encouraged in people and professionals and in the workplace. It is essential for innovation, and is a factor affecting economic growth and businesses. In 2013, the sociologist Silvia Leal Martín, using the Innova 3DX method, suggested measuring the various parameters that encourage creativity and innovation: corporate culture, work environment, leadership and management, creativity, self-esteem and optimism, locus of control and learning orientation, motivation, and fear.\n\nSimilarly, social psychologists, organizational scientists, and management scientists who conduct extensive research on the factors that influence creativity and innovation in teams and organizations have developed integrative theoretical models that emphasize the roles of team composition, team processes, and organizational culture, as well as the mutually reinforcing relationships between them in promoting innovation.\n\nThe investigation by Loo (2017) on creative working in the knowledge economy brings together studies of creativity as delineated in this web page. It offers connections with the sections on the ‘”Four C” model’, ‘Theories of creative processes’, ‘Creativity as a subset of intelligence’, ‘Creativity and personality’, and ‘In organisations’ It is the last section that the investigation addresses.\n\nResearch studies of the knowledge economy may be classified into three levels: macro, meso and micro. Macro studies refer to investigations at a societal or transnational dimension. Meso studies focus on organisations. Micro investigations centre on the minutiae workings of workers. There is also an interdisciplinary dimension such as research from businesses (e.g. Burton-Jones, 1999; Drucker, 1999), economics (e.g. Cortada, 1998; Reich, 2001; Florida, 2003), education (e.g. Farrell and Fenwick, 2007; Brown, Lauder and Ashton, 2011), human resource management (e.g. Davenport, 2005), knowledge and organizational management (Alvesson, 2004; Defillippi, Arthur and Lindsay, 2006; Orr, Nutley, Russell, Bain, Hacking and Moran, 2016), sociology, psychology, and knowledge economy-related sectors – especially information technology (IT) software (e.g. O’Riain, 2004; Nerland, 2008) and advertising (e.g. Grabher, 2004; Lury, 2004) (Loo, 2017).\n\nLoo (2017) studies how individual workers in the knowledge economy use their creativity and know-how in the advertising and IT software sectors. It examines this phenomenon across three developed countries of England, Japan and Singapore to observe global perspectives. Specifically, the study uses qualitative data from semi-structured interviews of the related professionals in the roles of creative directing and copywriting (in advertising), and systems software developing and software programme managing.\n\nThe study offers a conceptual framework (Loo, 2017, p. 49) of a two-dimensional matrix of individual and collaborative working styles, and single and multi-contexts. The investigation draws on literature sources from the four disciplines of economics (e.g. Reich, 2001; Quah, 2002), management (e.g. ,Drucker, 1994; Nonaka and Takeuchi, 1995; von Hippel, 2006), sociology (e.g. Zuboff, 1988; Bell, 1973; Lash and Urry, 1994; Castells, 2000; Knorr Cetina, 2005), and psychology (e.g. Gardner, 1984; Csikszentmihalyi, 1988; Sternberg, Kaufman and Pretz, 2004). The themes arising from the analysis of knowledge work and creativity literature serve to create a distinct theoretical framework of creative knowledge work. These workers apply their cognitive abilities, creative personalities and skill sets in the areas of science, technology, or culture industries to invent or discover new possibilities – e.g. a medium, product or service. These work activities may be done individually or collectively. Education, training and ‘encultured environments’ are necessary for the performance of these creative activities. Acts of creativity are viewed as asking new questions over and above those questions asked by an intelligent person, seeking novelty when reviewing a situation (Gardner, 1993), and creating something that is different and novel, i.e. a ‘variation’ on the idea of existing ideas in a domain (Csikszentmihalyi, 1988). This framework is evidenced by the empirical chapters on the micro-workings of creative workers in the two knowledge economy sectors from global perspectives.\n\nThis investigation identifies a definition of creative work, three types of work and the necessary conditions for it to occur. These workers use a combination of creative applications including anticipatory imagination, problem-solving, problem seeking, and generating ideas and aesthetic sensibilities. Taking aesthetic sensibilities as an example, for a creative director in the advertising industry, it is a visual imagery whether still or moving via a camera lens, and for a software programmer, it is the innovative technical expertise in which the software is written. There are specific creative applications for each of the sectors such as emotional connection in the advertising sector, and the power of expression and sensitivity in the IT software sector. In addition to the creative applications, creative workers require abilities and aptitudes to carry out their roles. Passion for one’s job is generic. For copywriters, this passion is identified with fun, enjoyment and happiness alongside attributes such as honesty (regarding the product), confidence, and patience in finding the appropriate copy. Knowledge is also required in the disciplines of the humanities (e.g. literature), the creative arts (e.g. painting and music) and technical-related know-how (e.g. mathematics, computer sciences and physical sciences). In the IT software, technical knowledge of computer languages (e.g. C++) is especially significant for programmers whereas the degree of technical expertise may be less for a programme manager, as only knowledge of the relevant language is necessary to understand the issues for communicating with the team of developers and testers.\n\nThere are three types of work. One is intra-sectoral (e.g. ‘general sponge’ and ’in tune with the zeitgeist’ [advertising], and ‘power of expression’ and ‘sensitivity’ [IT software]). The second is inter-sectoral (e.g. ‘integration of advertising activities’ [advertising], and ‘autonomous decentralized systems’ [ADS] [IT software]). The third relates to changes in culture/practices in the sectors (e.g. ‘three-dimensional trust’ and ‘green credentials’ [advertising], and ‘collaboration with HEIs and industry’ and ‘ADS system in the Tokyo train operator’ [IT software]).\n\nThe necessary conditions for creative work to exist are a supportive environment such as supportive information, communications and electronic technologies (ICET) infrastructure, training, work environment and education.\n\nThis investigation has implications for lifelong learning of these workers informally and formally. Teaching institutions need to offer multi-disciplinary knowledge of humanities, arts and sciences and it has impacts on the programme structure, delivery approaches and assessments. At a macro level, governments need to offer a rich diet of cultural activities, outdoor activities and sports fixtures that inform potential creative workers in the areas of video gaming and advertising. This study has implications for work organisations that support and encourage collaborative working alongside individual working, offer opportunities to engage in continuous professional development (formally and informally), and foster an environment, which promotes experiential functioning and supports experimentation.\n\nTeam Composition\n\nDiversity between team members’ backgrounds and knowledge can increase team creativity by expanding the total collection of unique information that is available to the team and introducing different perspectives that can integrate in novel ways. However, under some conditions, diversity can also decrease team creativity by making it more difficult for team members to communicate about ideas and causing interpersonal conflicts between those with different perspectives. Thus, the potential advantages of diversity must be supported by appropriate team processes and organizational cultures in order to enhance creativity.\n\nTeam Processes\n\nTeam communication norms, such as respecting others’ expertise, paying attention to others’ ideas, expecting information sharing, tolerating disagreements, negotiating, remaining open to others’ ideas, learning from others, and building on each other’s ideas, increase team creativity by facilitating the social processes involved with brainstorming and problem solving. Through these processes, team members are able to access their collective pool of knowledge, reach shared understandings, identify new ways of understanding problems or tasks, and make new connections between ideas. Engaging in these social processes also promotes positive team affect, which facilitates collective creativity.\n\nOrganizational Culture\n\nSupportive and motivational environments that create psychological safety by encouraging risk taking and tolerating mistakes increase team creativity as well. Organizations in which help-seeking, help giving, and collaboration are rewarded promote innovation by providing opportunities and contexts in which team processes that lead to collective creativity can occur. Additionally, leadership styles that downplay status hierarchies or power differences within an organization and empower people to speak up about their ideas or opinions also help to create cultures that are conducive to creativity.\n\nEconomic approaches to creativity have focussed on three aspects — the impact of creativity on economic growth, methods of modelling markets for creativity, and the maximisation of economic creativity (innovation).\n\nIn the early 20th century, Joseph Schumpeter introduced the economic theory of \"creative destruction\", to describe the way in which old ways of doing things are endogenously destroyed and replaced by the new. Some economists (such as Paul Romer) view creativity as an important element in the recombination of elements to produce new technologies and products and, consequently, economic growth. Creativity leads to capital, and creative products are protected by intellectual property laws.\n\nMark A. Runco and Daniel Rubenson have tried to describe a \"psychoeconomic\" model of creativity. In such a model, creativity is the product of endowments and active investments in creativity; the costs and benefits of bringing creative activity to market determine the supply of creativity. Such an approach has been criticised for its view of creativity consumption as always having positive utility, and for the way it analyses the value of future innovations.\n\nThe \"creative class\" is seen by some to be an important driver of modern economies. In his 2002 book, \"The Rise of the Creative Class\", economist Richard Florida popularized the notion that regions with \"3 T's of economic development: Technology, Talent and Tolerance\" also have high concentrations of creative professionals and tend to have a higher level of economic development.\n\nSeveral different researchers have proposed methods of increasing the creativity of an individual. Such ideas range from the psychological-cognitive, such as Osborn-Parnes Creative Problem Solving Process, Synectics, science-based creative thinking, Purdue Creative Thinking Program, and Edward de Bono's lateral thinking; to the highly structured, such as TRIZ (the Theory of Inventive Problem-Solving) and its variant Algorithm of Inventive Problem Solving (developed by the Russian scientist Genrich Altshuller), and Computer-Aided morphological analysis.\n\nDaniel Pink, in his 2005 book \"A Whole New Mind\", repeating arguments posed throughout the 20th century, argues that we are entering a new age where creativity is becoming increasingly important. In this \"conceptual age\", we will need to foster and encourage \"right-directed thinking\" (representing creativity and emotion) over \"left-directed thinking\" (representing logical, analytical thought). However, this simplification of 'right' versus 'left' brain thinking is not supported by the research data.\n\nNickerson provides a summary of the various creativity techniques that have been proposed. These include approaches that have been developed by both academia and industry:\n\nSome see the conventional system of schooling as \"stifling\" of creativity and attempt (particularly in the preschool/kindergarten and early school years) to provide a creativity-friendly, rich, imagination-fostering environment for young children. Researchers have seen this as important because technology is advancing our society at an unprecedented rate and creative problem solving will be needed to cope with these challenges as they arise. In addition to helping with problem solving, creativity also helps students identify problems where others have failed to do so. See the Waldorf School as an example of an education program that promotes creative thought.\n\nPromoting intrinsic motivation and problem solving are two areas where educators can foster creativity in students. Students are more creative when they see a task as intrinsically motivating, valued for its own sake. To promote creative thinking, educators need to identify what motivates their students and structure teaching around it. Providing students with a choice of activities to complete allows them to become more intrinsically motivated and therefore creative in completing the tasks.\n\nTeaching students to solve problems that do not have well defined answers is another way to foster their creativity. This is accomplished by allowing students to explore problems and redefine them, possibly drawing on knowledge that at first may seem unrelated to the problem in order to solve it. In adults, mentoring individuals is another way to foster their creativiy. However, the benefits of mentoring creativity apply only to creative contributions considered great in a given field, not to everyday creative expression.\n\nIn the Scottish education system, creativity is identified as a core skillset for learning, life and work and is defined as “a process which generates ideas that have value to the individual. It involves looking at familiar things with a fresh eye, examining problems with an open mind, making connections, learning from mistakes and using imagination to explore new possibilities.” The need to develop a shared language and understanding of creativity and its role across every aspect of learning, teaching and continuous improvement was identified as a necessary aim and a set of four skills is used to allow educators to discuss and develop creativity skills across all subjects and sectors of education – curiosity, open—mindedness, imagination and problem solving. Distinctions are made between creative learning (when learners are using their creativity skills), creative teaching (when educators are using their creativity skills) and creative change (when creativity skills are applied to planning and improvement). Scotland’s national Creative Learning Plan supports the development of creativity skills in all learners and of educators’ expertise in developing creativity skills. A range of resources have been created to support and assess this including a national review of creativity across learning by Her Majesty’s Inspectorate for Education. \n\nCreativity has also been identified as one of the key 21st century skills and as one of the Four Cs of 21st century learning by educational leaders and theorists in the United States.\n\n\n"}
{"id": "9627438", "url": "https://en.wikipedia.org/wiki?curid=9627438", "title": "Daedeok Science Town", "text": "Daedeok Science Town\n\nDaedeok Innopolis, formerly known as Daedeok Science Town, is the research and development district in the Yuseong-gu district in Daejeon, South Korea. Daedeok Innopolis grew out of the research cluster established by President Park Chunghee in 1973 with the opening of the KAIST. Over 20 major research institutes and over 40 corporate research centers make up this science cluster. Over the last few years, a number of IT venture companies have sprung up in this region, which has a high concentration of Ph.Ds in the applied sciences. There are 232 research and educational institutions to be found in Daejeon, many in the Daedeok region, among them the Electronics and Telecommunications Research Institute and the Korea Aerospace Research Institute. The \"town\" will provide a core for the International Science and Business Belt.\n\nThe Daedeok Innopolis logo was created by the industrial design company INNO Design in Palo Alto, USA. \n\n\n\n\n"}
{"id": "30303286", "url": "https://en.wikipedia.org/wiki?curid=30303286", "title": "Decoding Reality", "text": "Decoding Reality\n\nDecoding Reality: The Universe as Quantum Information is a popular science book by Vlatko Vedral published by Oxford University Press in 2010. Vedral examines information theory and proposes information as the most fundamental building block of reality. He argues what a useful framework this is for viewing all natural and physical phenomena. In building out this framework the books touches upon the origin of information, the idea of entropy, the roots of this thinking in thermodynamics, the replication of DNA, development of social networks, quantum behaviour at the micro and macro level, and the very role of indeterminism in the universe. The book finishes by considering the answer to the ultimate question: where did all of the information in the Universe come from? The ideas address concepts related to the nature of particles, time, determinism, and of reality itself.\n\nVedral believes in the principle that information is physical. \"Creation ex nihilo\" comes from Catholic dogma, the idea being that God created the universe out of nothing. Vedral says that invoking a supernatural being as an explanation for creation does not explain reality because the supernatural being would have to come into existence itself too somehow presumably from nothing (or else from an infinite regression of supernatural beings), thus of course the reality can come from nothing without a supernatural being. Occam's razor principle favours the simplest explanation. Vedral believes information is the fundamental building block of reality as it occurs at the macro level (economics, human behaviour etc.) as well as the subatomic level. Vedral argues that information is the only candidate for such a building block that can explain its own existence as information generates additional information that needs to be compressed thus generating more information. 'Annihilation of everything' is a more fitting term than \"creation ex nihilo\" Vedral states, as compression of possibilities is the process of how new information is created.\n\nVedral uses an Italo Calvino philosophical story about a tarot-like card game as the kernel for his metaphor of conscious life arriving \"in medias res\" to a pre-existing contextual reality. In this game the individual observers/players (Vedral suggests: quantum physics, thermodynamics, biology, sociology, economics, philosophy) lay down cards with ambiguous meanings as an attempt to communicate messages to deduce meaning out of the other players' interactions. The results (information) of previous rounds establish contextual rules for observers/players in subsequent rounds. The point of this game is not established until the last card has been played as later cards can change the meaning of previous events, as in the case of the quantum explanation for the photoelectric effect instantly disproving classical physics. Vedral points out that in our reality there is no last card.\n\nShannon entropy or information content measured as the surprise value of a particular event, is essentially inversely proportional to the logarithm of the event's probability, i = log(1/p). Claude Shannon's information theory arose from research at Bell labs, building upon George Boole's digital logic. As information theory predicts common and easily predicted words tend to become shorter for optimal communication channel efficiency while less common words tend to be longer for redundancy and error correction. Vedral compares the process of life to John von Neumann's self replicating automata. These are enduring information carriers that will survive wear and tear of the individual by producing copies that can in turn go on to produce more copies.\n\nGenetic code as an efficient digital information store, containing built in codon redundancy for error correction in transcription.\n\nExamines the Second law of thermodynamics and the process of information increasing entropy. Maxwell's Demon was thought to be a way around this inevitability, however such a demon would run out of information storage space and have to delete unwanted data thus having to do work to do so, increasing entropy.\n\nBlackjack as controlled risk taking using Shannon's information theory probability formulas. Casino as a ′cool′ financial entropy source and the gambler as a ′hot′ financial source, once again the Second law of thermodynamics means the flow is almost always from hot to cold in the long run. For managed risk spread bets widely and in high risk high reward investments (assuming a known probability), this is the Log optimal portfolio approach.\n\nSix degrees of separation means well connected people tend to be more successful as their social networks expose them to more chances to make choices they want. Schelling precommitment as strategy in social and self-control, for example burning your bridges by buying gym membership to help motivated self win over lazy self. Mutual information resulting in phase transitions in social and political demography as well as physical systems, like water freezing into ice at a particular critical temperature or magnetic fields spontaneously aligning in certain atoms when cooling from a molten state.\n\nVedral examines the basis of quantum information, the qubit, and examines one-time pad quantum cryptography as the most secure form of encryption because of its uncomputability. Quantum entanglement demonstrates the importance of mutual information in defining outcomes in a reality.\n\nQuantum computers offer a search advantage over classical computers by searching many database elements at once as a result of quantum superpositions. A sufficiently advanced quantum computer would break current encryption methods by factorizing large numbers several orders of magnitude faster than any existing classical computer. Any computable problem may be expressed as a general quantum search algorithm although classical computers may have an advantage over quantum search when using more efficient tailored classical algorithms. The issue with quantum computers is that a measurement must be made to determine if the problem is solved which collapses the superposition. Vedral points out that unintentional interaction with the environment can be mitigated with redundancy, and this would be necessary if we were to scale up current quantum computers to achieve greater utility, i.e. to utilize 10 qubits have a 100 atom quantum system so that if one atom decoheres a consensus will still be held by the other 9 for the state of the same qubit.\n\nRandomness is key to generating new sources of surprise in a reality. Compression of these new sources to discard unimportant information is the deterministic element and organising principle.\n\nThe information content of the universe as measured in bits or qubits. Vedral uses the initial effort of Archimedes of Syracuse in calculating the amount of sand that could theoretically fit inside the universe and compares it to a modern-day attempt to calculate the bit content of the universe. Vedral however sees this content as ultimately limitless as possibly maximum entropy is never reached as compression of complexity is an open ended process and random events will continue to occur. As Vedral sees information as the ultimate building block of physical reality, he speculates that information originating at any scale can force outcomes in all other scales to abide where mutual information is shared. For example, a human performed macro-level scientific test in search of a behaviour in a quantum particle could set parameters for that type of particle in the future when subjected to a similar test.\n\nThe information basis for \"creation ex nihilo\". According to John von Neumann, starting trivially from an empty set of numbers an infinite sequence of numbers can bootstrap their way out. An empty set creates the number 1 by observing an empty set within itself which is enough of a basis for distinguishability. It creates the number 2 by observing an empty set within the second empty set and the number 1, and so on. Vedral sees this not as creation but as data compression, as every event of a reality breaks the symmetry of the pre-existing formlessness. Science is the process of describing a large amount of observed phenomena in a compressed programmatic way to predict future outcomes, and in this process of data compression science creates new information by eliminating all contrary possibilities to explain those phenomena.\n\nThe book explains the world as being made up of information. The Universe and its workings are the ebb and flow of information. We are all transient patterns of information, passing on the recipe for our basic forms to future generations using a four-letter digital code called DNA. In this engaging and mind-stretching account, Vlatko Vedral considers some of the deepest questions about the Universe and considers the implications of interpreting it in terms of information. He explains the nature of information, the idea of entropy, and the roots of this thinking in thermodynamics. He describes the bizarre effects of quantum behaviour - effects such as 'entanglement', which Einstein called 'spooky action at a distance' and explores cutting edge work on the harnessing quantum effects in hyperfast quantum computers, and how recent evidence suggests that the weirdness of the quantum world, once thought limited to the tiniest scales, may reach into the macro world. Vedral finishes by considering the answer to the ultimate question: where did all of the information in the Universe come from? The answers he considers are exhilarating, drawing upon the work of distinguished physicist John Wheeler and his concept of “it from bit”. The ideas challenge our concept of the nature of particles, of time, of determinism, and of reality itself.\n\n"}
{"id": "56104271", "url": "https://en.wikipedia.org/wiki?curid=56104271", "title": "Degg's Model", "text": "Degg's Model\n\nThe Degg's Model shows that a disaster only occurs if a vulnerable population is exposed to a hazard. It was devised in 1992 by Dr. Martin Degg, Head of Geography at the University of Chester.\n"}
{"id": "2736602", "url": "https://en.wikipedia.org/wiki?curid=2736602", "title": "El Perú (book)", "text": "El Perú (book)\n\nEl Perú: Itinerarios de Viajes is an expansive written work covering a variety of topics in the natural history of Peru, written by the prominent Italian-born Peruvian geographer and scientist Antonio Raimondi in the latter half of the 19th century. The work was compiled from extensive and detailed notes Raimondi took while criss-crossing the country, studying the nation's geography, geology, meteorology, botany, zoology, ethnography, and archaeology; \"El Perú\" focuses to some extent on each of these topics and others. The first volume was published in 1874; several more volumes were published both before Raimondi's death and posthumously from his notes, the last being released in 1913, making a five volume set. The volumes are a classic example of exploration scholarship, and form one of the earliest and broadest scientific reviews of Peru's natural and cultural heritage.\n\n\n"}
{"id": "5976199", "url": "https://en.wikipedia.org/wiki?curid=5976199", "title": "Energy and Power", "text": "Energy and Power\n\nEnergy and Power is a 1962 science book for children by L. Sprague de Camp, illustrated by Weimer Pursell and Fred Eng, published by Golden Press as part of The Golden Library of Knowledge Series. It has been translated into Portuguese and Spanish.\n\nThe title blurb summarizes the content as \"How man uses animals, wind, water, heat, electricity, chemistry, and atoms to perform work.\"\n"}
{"id": "20265741", "url": "https://en.wikipedia.org/wiki?curid=20265741", "title": "Fact-finding", "text": "Fact-finding\n\nFact-finding is the job of a person or group of persons in a judicial or administrative proceeding that has or have the responsibility of determining the facts relevant to decide a controversy. The term trier of fact generally denotes the same function. The process is an extremely important part of the communication process.\n\nFact-finding was first established during the Hague Convention of 1907, which dealt with international commissions of inquiry.\n\nOn 9 December 1991, the General Assembly of the United Nations approved the \"Declaration on Fact-finding by the United Nations in the Field of the Maintenance of International Peace and Security\". The resolution emphasized\n\n...that the ability of the United Nations to maintain international peace and security depends to a large extent on its acquiring detailed knowledge about the factual circumstances of any dispute or situation.\n\nand\n\nto encourage States to bear in mind the role that competent organs of the United Nations can play in ascertaining the facts in relation to disputes or situations.\nThere is also a range of fact-finding procedures in the United Nations relating to serious violations of human rights and humanitarian law, as well as fact-finding on the crimes of aggression, genocide, war crimes and crimes against humanity which is carried out by the International Criminal Court. Professor Lyal S. Sunga discusses how UN human rights special procedures fact-finding can help the International Criminal Court find facts and how the two should be made complementary.\n\nFact finders often have the job of determining what facts are available and their relevancy.\n\nThe position of fact finder is determined by the type of proceeding. In a jury trial, it is the role of a jury in a jury trial. In a non-jury trial, the judge sits both as a fact-finder and as the trier of law. In administrative proceedings it may be a hearing officer or a hearing body.\n\nAccording to the United Nations, fact-finding should be comprehensive, objective, impartial and timely.\n\n\n"}
{"id": "20116656", "url": "https://en.wikipedia.org/wiki?curid=20116656", "title": "Guangzhou Science City", "text": "Guangzhou Science City\n\nGuangzhou Science City (GSC; ) is a technology center developed with support from the Guangzhou government. The Guangzhou government is trying to make the Science City an important destination for IT R&D, outsourcing in China.\n\nGSC is the core of Guangzhou Hi-Tech Industrial Development Zone, in Tianhe District. Its area is 37.5 km sq, east of Dongsha road, north of Guangshen highway, south of Guangshan road and Luonan road, and west of the north the second city ring road.\n\n\n"}
{"id": "57104312", "url": "https://en.wikipedia.org/wiki?curid=57104312", "title": "Haplogroup A-P305", "text": "Haplogroup A-P305\n\nHaplogroup A-P305 also known as A1 is a Human Y-chromosome DNA haplogroup. Like its parent haplogroup haplogroup A0-T (A-L1085), A1 includes the vast majority of living human males. It emerged in Africa approximately 161,300 years ago. By comparison, members of its sole sibling subclade, haplogroup A0 – the only other primary subclade of haplogroup A0-T – are found mostly in \n\nBasal, undivergent A-P305* is largely restricted to populations native to Africa, though a handful of cases have been reported in Europe and Western Asia. A-P305* is found at its highest rates in Bakola Pygmies (South Cameroon) at 8.3% and Berbers from Algeria at 1.5% and in Ghana. The clade also achieves high frequencies in the Bushmen hunter-gatherer populations of Southern Africa, followed closely by many Nilotic groups in Eastern Africa. However, haplogroup A's oldest sub-clades are exclusively found in Central-Northwest Africa, where it, and consequently Y-chromosomal Adam, is believed to have originated about 140,000 years ago. The clade has also been observed at notable frequencies in certain populations in Ethiopia, as well as some Pygmy groups in Central Africa.\n"}
{"id": "14285", "url": "https://en.wikipedia.org/wiki?curid=14285", "title": "History of science and technology", "text": "History of science and technology\n\nThe history of science and technology (HST) is a field of history which examines how humanity's understanding of the natural world (science) and ability to manipulate it (technology) have changed over the centuries. This academic discipline also studies the cultural, economic, and political impacts of scientific innovation.\n\nHistories of science were originally written by practicing and retired scientists, starting primarily with William Whewell, as a way to communicate the virtues of science to the public. In the early 1930s, after a famous paper given by the Soviet historian Boris Hessen, was focused into looking at the ways in which scientific practices were allied with the needs and motivations of their context. After World War II, extensive resources were put into teaching and researching the discipline, with the hopes that it would help the public better understand both Science and Technology as they came to play an exceedingly prominent role in the world. In the 1960s, especially in the wake of the work done by Thomas Kuhn, the discipline began to serve a very different function, and began to be used as a way to critically examine the scientific enterprise. At the present time it is often closely aligned with the field of science studies.\n\nModern engineering as it is understood today took form during the scientific revolution, though much of the mathematics and science was built on the work of the Greeks, Egyptians, Mesopotamians, Chinese, Indians. See the main articles History of science and History of technology for these respective topics.\n\n\n\n\n\n\n\n\nHistory of science and technology is a well developed field in India. At least three generations of scholars can be identified.\nThe first generation includes D.D.Kosambi, Dharmpal, Debiprasad Chattopadhyay and Rahman. The second generation mainly consists of Ashis Nandy, Deepak Kumar, Dhruv Raina, S. Irfan Habib, Shiv Visvanathan, Gyan Prakash, Stan Lourdswamy, V.V. Krishna, Itty Abraham, Richard Grove, Kavita Philip, Mira Nanda and Rob Anderson. There is an emergent third generation that includes scholars like Abha Sur and Jahnavi Phalkey.\n\nDepartments and Programmes\n\nThe National Institute of Science, Technolology and Development Studies had a research group active in the 1990s which consolidated social history of science as a field of research in India. \nCurrently there are several institutes and university departments offering HST programmes.\n\n\n\n\n\n\n\n\nAcademic study of the History of Science as an independent discipline was launched by George Sarton at Harvard with his book \"Introduction to the History of Science\" (1927) and the \"Isis\" journal (founded in 1912). Sarton exemplified the early 20th century view of the history of science as the history of great men and great ideas. He shared with many of his contemporaries a Whiggish belief in history as a record of the advances and delays in the march of progress. The History of Science was not a recognized subfield of American history in this period, and most of the work was carried out by interested Scientists and Physicians rather than professional Historians. With the work of I. Bernard Cohen at Harvard, the history of Science became an established subdiscipline of history after 1945.\n\n\n\n\n\nHistoriography of science\n\nHistory of science as a discipline\n\n"}
{"id": "49054606", "url": "https://en.wikipedia.org/wiki?curid=49054606", "title": "Holism and Evolution", "text": "Holism and Evolution\n\nHolism and Evolution is a 1926 book by South African statesman Jan Smuts, in which he coined the word \"holism\", although Smuts' meaning is different from the modern concept of holism. Smuts defined holism as the \"fundamental factor operative towards the creation of wholes in the universe.\"\n\nThe book was part of a broader trend of interest in holism in European and colonial academia during the early twentieth century. Smuts' philosophy of holism was based in the thoughts behind his earlier book \"Walt Whitman: A Study in the Evolution of Personality\", written during his time in Cambridge. He described a \"process-orientated, hierarchical view of nature\" and has been influential among criticisms of reductionism. Smuts' formulation of holism has also been linked with his political-military activity, especially his aspiration to create a league of nations.\n"}
{"id": "43707763", "url": "https://en.wikipedia.org/wiki?curid=43707763", "title": "Hot hardness", "text": "Hot hardness\n\nIn materials engineering and metallurgy, hot hardness or red hardness (when a metal glows a dull red from the heat) corresponds to hardness of a material at high temperatures. As the temperature of material increases, hardness decreases and at some point a drastic change in hardness occurs. The hardness at this point is termed the \"hot\" or \"red\" hardness of that material. Such changes can be seen in materials such as heat treated alloys.\n"}
{"id": "7469344", "url": "https://en.wikipedia.org/wiki?curid=7469344", "title": "Hyperaccumulators table – 2 : Nickel", "text": "Hyperaccumulators table – 2 : Nickel\n\nThis list covers known nickel hyperaccumulators, accumulators or plant species tolerant to nickel.\n\nSee also:\n\nNotes\n\n\n"}
{"id": "7469508", "url": "https://en.wikipedia.org/wiki?curid=7469508", "title": "Hyperaccumulators table – 3", "text": "Hyperaccumulators table – 3\n\nThis list covers hyperaccumulators, plant species which accumulate, or are tolerant of radionuclides (Cd, Cs-137, Co, Pu-238, Ra, Sr, U-234, 235, 238), hydrocarbons and organic solvents (Benzene, BTEX, DDT, Dieldrin, Endosulfan, Fluoranthene, MTBE, PCB, PCNB, TCE and by-products), and inorganic solvents (Potassium ferrocyanide).\n\nSee also:\n\n\n"}
{"id": "636129", "url": "https://en.wikipedia.org/wiki?curid=636129", "title": "Ideation (creative process)", "text": "Ideation (creative process)\n\nIdeation is the creative process of generating, developing, and communicating new ideas, where an idea is understood as a basic element of thought that can be either visual, concrete, or abstract. Ideation comprises all stages of a thought cycle, from innovation, to development, to actualization. Ideation can be conducted by individuals, organizations, or crowds. As such, it is an essential part of the design process, both in education and practice. \n\nThe book \"Ideation: The Birth and Death of Ideas\" (Graham and Bachmann, 2004) proposes the following methods of innovation:\n\n\nThis list of methods is by no means comprehensive or necessarily accurate. Graham and Bachmann's examples of revolutionary ideas might better be described as evolutionary; both Marx and Copernicus having built upon pre-existing concepts within new or different contexts. Similarly, the description provided for artistic innovation represents one perspective.\n\nMore-nuanced understandings, such as that expressed by Stephen Nachmanovitch in \"Free Play: Improvisation in Life and Art\", recognize the generative force technical and perceptual limitations provide within specific arts practices. In painting, for example, technical limitations such as the frame, the surface and the palette, along with perceptual constraints like figure/ground relationships and perspective, provide creative frameworks for the painter. Similarly in music, harmonic scales, meter and time signatures work in tandem with choices of instrumentation and expression to both produce specific results and improvise novel outcomes.\n\nThe T.O.T.E. model, an iterative problem solving strategy based on feedback loops, provides an alternative approach to considering the process of ideation. Ideation may also be considered as a facet of other generative systems, such as Emergence.\n\nThe word \"ideation\" has come under informal criticism as being a term of meaningless jargon, as well as being inappropriately similar to the psychiatric term for suicidal ideation.\n\n\n"}
{"id": "8999017", "url": "https://en.wikipedia.org/wiki?curid=8999017", "title": "Laboratory specimen", "text": "Laboratory specimen\n\nIn medicine, a laboratory specimen is a biological specimen taken by sampling, that is, gathered matter of a medical patient's tissue, fluid, or other material derived from the patient used for laboratory analysis to assist differential diagnosis or staging of a disease process. Common examples include throat swabs, sputum, urine, blood, surgical drain fluids, and tissue biopsies.\n"}
{"id": "780025", "url": "https://en.wikipedia.org/wiki?curid=780025", "title": "Lexicon Technicum", "text": "Lexicon Technicum\n\nLexicon Technicum: or, Universal English Dictionary of Arts and Sciences: Explaining not only the Terms of Art, but the Arts Themselves was in many respects the first alphabetical encyclopedia written in English. Although the emphasis of the \"Lexicon Technicum\" was on mathematical subjects, its contents go beyond what would be called science or technology today, in conformity with the broad eighteenth-century understanding of the terms \"arts\" and \"science,\" and it includes entries on the humanities and fine arts, notably on law, commerce, music, and heraldry. However, the \"Lexicon Technicum\" neglects theology, antiquity, biography, and poetry. \n\nThe \"Lexicon Technicum\" was the work of a London clergyman, John Harris (1666-1719). Its professed advantage over French dictionaries of the arts and sciences was that it contained explanation not only of the terms used in the arts and sciences, but also of the arts and sciences themselves. Harris issued a three-page proposal for this work in 1702, and the first edition of the first volume was published in London in 1704. The first volume contains 1220 pages, 4 plates, and many additional diagrams and figures within the text. Like many early English encyclopedias, the pages are not numbered; numbering may have been thought unnecessary as readers could search by its alphabetical arrangement.\n\nVolume 2, which was also alphabetized from A through Z, was first published in 1710. The second volume contains 1419 pages and 4 plates, with a list of about 1300 subscribers. A previously unpublished treatise on acids by Sir Isaac Newton was included in its original Latin along with Harris's English translation, perhaps without the latter's permission or encouragement. A large part of the volume consists of mathematical and astronomical tables, since Harris intended his work to serve as a small mathematical library. He provided tables of logarithms, sines, tangents, and secants, a two-page list of books, and an index of the articles in both volumes under 26 heads, filling 50 pages. The longest lists are for law (1700 articles), surgery, anatomy, geometry, fortification, botany, and music.\n\nIn his preface, Harris stated that he got less help from previous dictionaries than one would expect. While acknowledging some borrowing, Harris insisted that \"much the greater part of what [the reader] will find here is collected from no Dictionaries, but from the best Original Authors I could procure.\" Harris's preface touted his coverage of mathematical subjects. He admitted the imperfection of his data on stars, noting that Flamsteed had refused to assist him, but he vaunted his coverage of astronomy, especially his full coverage of Newton's theories of the moon and of comets. In botany he claimed to have given \"a pretty exact botanick lexicon, which was what we really wanted before,\" using Dr John Ray's method. To describe the parts of a ship accurately, he supposedly \"often\" went on board himself. In law, he wrote, he abridged from the best writers and had the result \"carefully examined and corrected by a Gentleman of known Ability in that Profession.\"\n\nThe specified aims of the book did not prevent Harris from including some highly opinionated asides, for example this definition conveying the poor view he took of lawyers: \"Sollicitor, is a Man imploy'd to take care of, and follow Suits depending in Courts of Law, or Equity, formerly allowed only to Nobility, whose Menial Servants they were; but now too frequently used by others, to the damage of the People, and the increase of Champerty and Maintenance\".\n\nHarris wrote that he had wished to supply an index for each art and science as well as more plates on anatomy and ships, but the underwriters could not afford it, \"the Book having swelled so very much beyond the Expectation.\"\n\nA review of this work, extending to the unusual length of four pages, appeared in the \"Philosophical Transactions\" for 1704.\n\nThe \"Lexicon Technicum\" was long very popular, enduring through at least 1744 as the main rival of Ephraim Chambers's \"Cyclopaedia\". The final publication of the two volumes of the \"Lexicum Technicum\" was in 1736. An anonymous one-volume supplement did appear in 1744, with 996 pages and 6 plates, but this work was allegedly \"not well received,\" being perceived by contemporaries as a mere \"booksellers speculation.\" In any case, no new editions of the \"Lexicon Technicum\" were published thereafter.\n\nUnlike most multi-volume works, the several editions of volumes 1 and 2 were not synchronized until 1736. The first volume appeared in five editions, while the second volume appeared only in three editions. The first edition of volume 1 appeared in 1704; the second edition in 1708, the third edition in 1716, and the fourth edition in 1725. In contrast, the first edition of volume 2 appeared in 1710, and the second edition did not appear until 1723. The two volumes were not published together until 1736, constituting the fifth edition of the first volume and the third edition of the second volume. The supplement of 1744 was billed as volume 3.\n\nWe do not know the names of the editors who carried on Harris's work after his death in 1719.\n\n\n"}
{"id": "14047716", "url": "https://en.wikipedia.org/wiki?curid=14047716", "title": "Light ergonomics", "text": "Light ergonomics\n\nLight ergonomics is the relationship between the light source and the individual. Poor light can be divided into the following:\n\n\nThe effects of poor light can include the following:\n\n"}
{"id": "42335532", "url": "https://en.wikipedia.org/wiki?curid=42335532", "title": "List of regimes", "text": "List of regimes\n\nThis list of regimes lists the results of regime-classification schemes from political science literature, including Polity data series and the Democracy-Dictatorship Index.\n\n"}
{"id": "5721896", "url": "https://en.wikipedia.org/wiki?curid=5721896", "title": "List of research parks", "text": "List of research parks\n\nThe following is a list of science park, technology parks and biomedical parks of the world, organized by continent.\n\nASEAN Economic Community\nReport listing all the Economic Zones in the ASEAN Economic Community from UNIDO Viet Nam\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThere are approximately 170 university research parks in North America today.\n\nAlberta\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSilicon Mallee Adelaide, South Australia\n\n\n"}
{"id": "15428814", "url": "https://en.wikipedia.org/wiki?curid=15428814", "title": "List of scientific equations named after people", "text": "List of scientific equations named after people\n\nThis is a list of scientific equations named after people (eponymous equations).\n"}
{"id": "39127306", "url": "https://en.wikipedia.org/wiki?curid=39127306", "title": "Mechanism (philosophy)", "text": "Mechanism (philosophy)\n\nMechanism is the belief that natural wholes (principally living things) are like complicated machines or artifacts, composed of parts lacking any intrinsic relationship to each other. Thus, the source of an apparent thing's activities is not the whole itself, but its parts or an external influence on the parts.\n\nThe doctrine of mechanism in philosophy comes in two different flavors. They are both doctrines of metaphysics, but they are different in scope and ambitions: the first is a global doctrine about nature; the second is a local doctrine about humans and their minds, which is hotly contested. For clarity, we might distinguish these two doctrines as universal mechanism and anthropic mechanism.\n\nThe older doctrine, here called universal mechanism, is the ancient philosophies closely linked with materialism and reductionism, especially that of the atomists and to a large extent, stoic physics. They held that the universe is reducible to completely mechanical principles—that is, the motion and collision of matter. Later mechanists believed the achievements of the scientific revolution had shown that all phenomena could eventually be explained in terms of 'mechanical' laws, natural laws governing the motion and collision of matter that implied a thorough going determinism: if \"all\" phenomena could be explained \"entirely\" through the motion of matter under the laws of classical physics, then even more surely than the gears of a clock determine that it must strike 2:00 an hour after striking 1:00, \"all\" phenomena must be completely determined: whether past, present or future. (One of the philosophical implications of modern quantum mechanics is that this view of determinism is not defensible.)\n\nThe French mechanist and determinist Pierre Simon de Laplace formulated the sweeping implications of this thesis by saying:\n\nOne of the first and most famous expositions of universal mechanism is found in the opening passages of \"Leviathan\" by Thomas Hobbes (1651). What is less frequently appreciated is that René Descartes was a staunch mechanist, though today, in the philosophy of mind, he is remembered for introducing the mind–body problem in terms of dualism and physicalism.\n\nDescartes was a substance dualist, and argued that reality was composed of two radically different types of substance: extended matter, on the one hand, and immaterial mind, on the other. Descartes argued that one cannot explain the conscious mind in terms of the spatial dynamics of mechanistic bits of matter cannoning off each other. Nevertheless, his understanding of biology was thoroughly mechanistic in nature:\n\nHis scientific work was based on the traditional mechanistic understanding that animals and humans are completely mechanistic automata. Descartes' dualism was motivated by the seeming impossibility that mechanical dynamics could yield mental experiences.\n\nIsaac Newton ushered in a much weaker acceptation of mechanism that tolerated the antithetical, and as yet inexplicable, action at a distance of gravity. However, his work seemed to successfully predict the motion of both celestial and terrestrial bodies according to that principle, and the generation of philosophers who were inspired by Newton's example carried the mechanist banner nonetheless. Chief among them were French philosophers such as Julien Offray de La Mettrie and Denis Diderot (see also: French materialism).\n\nThe thesis in anthropic mechanism is not that everything can be completely explained in mechanical terms (although some anthropic mechanists may \"also\" believe that), but rather that everything \"about human beings\" can be completely explained in mechanical terms, as surely as can everything about clocks or the internal combustion engine.\n\nOne of the chief obstacles that all mechanistic theories have faced is providing a mechanistic explanation of the human mind; Descartes, for one, endorsed dualism in spite of endorsing a completely mechanistic conception of the material world because he argued that mechanism and the notion of a mind were logically incompatible. Hobbes, on the other hand, conceived of the mind and the will as purely mechanistic, completely explicable in terms of the effects of perception and the pursuit of desire, which in turn he held to be completely explicable in terms of the materialistic operations of the nervous system. Following Hobbes, other mechanists argued for a thoroughly mechanistic explanation of the mind, with one of the most influential and controversial expositions of the doctrine being offered by Julien Offray de La Mettrie in his \"Man a Machine\" (1748).\n\nToday, as in the past, the main points of debate between anthropic mechanists and anti-mechanists are mainly occupied with two topics: the mind — and consciousness, in particular — and free will. Anti-mechanists argue that anthropic mechanism is incompatible with our commonsense intuitions: in philosophy of mind they argue that unconscious matter cannot completely explain the phenomenon of consciousness, and in metaphysics they argue that anthropic mechanism implies determinism about human action, which (they argue) is incompatible with our understanding of ourselves as creatures with free will. Contemporary philosophers who have argued for this position include Norman Malcolm and\nDavid Chalmers.\n\nAnthropic mechanists typically respond in one of two ways. In the first, they agree with anti-mechanists that mechanism conflicts with some of our commonsense intuitions, but go on to argue that our commonsense intuitions are simply mistaken and need to be revised. Down this path lies eliminative materialism in philosophy of mind, and hard determinism on the question of free will. This option is accepted by the eliminative materialist philosopher Paul Churchland. Some have questioned how eliminative materialism is compatible with the freedom of will apparently required for anyone (including its adherents) to make truth claims. The second option, common amongst philosophers who adopt anthropic mechanism, is to argue that the arguments given for incompatibility are specious: whatever it is we mean by \"consciousness\" and \"free will,\" they urge, it is fully compatible with a mechanistic understanding of the human mind and will. As a result, they tend to argue for one or another non-eliminativist physicalist theories of mind, and for compatibilism on the question of free will. Contemporary philosophers who have argued for this sort of account include J. J. C. Smart and Daniel Dennett.\n\nSome scholars have debated over what, if anything, Gödel's incompleteness theorems imply about anthropic mechanism. Much of the debate centers on whether the human mind is equivalent to a Turing machine, or by the Church-Turing thesis, any finite machine at all. If it is, and if the machine is consistent, then Gödel's incompleteness theorems would apply to it.\n\nGödelian arguments claim that a system of human mathematicians (or some idealization of human mathematicians) is both consistent and powerful enough to recognize its own consistency. Since this is impossible for a Turing machine, the Gödelian concludes that human reasoning must be non-mechanical.\n\nHowever, the modern consensus in the scientific and mathematical community is that actual human reasoning is inconsistent; that any consistent \"idealized version\" \"H\" of human reasoning would logically be forced to adopt a healthy but counter-intuitive open-minded skepticism about the consistency of \"H\" (otherwise \"H\" is provably inconsistent); and that Gödel's theorems do not lead to any valid argument against mechanism. This consensus that Gödelian anti-mechanist arguments are doomed to failure is laid out strongly in \"Artificial Intelligence\": \"\"any\" attempt to utilize (Gödel's incompleteness results) to attack the computationalist thesis is bound to be illegitimate, since these results are quite consistent with the computationalist thesis.\"\n\nOne of the earliest attempts to use incompleteness to reason about human intelligence was by Gödel himself in his 1951 Gibbs Lecture entitled \"Some basic theorems on the foundations of mathematics and their philosophical implications\". In this lecture, Gödel uses the incompleteness theorem to arrive at the following disjunction: (a) the human mind is not a consistent finite machine, or (b) there exist Diophantine equations for which it cannot decide whether solutions exist. Gödel finds (b) implausible, and thus seems to have believed the human mind was not equivalent to a finite machine, i.e., its power exceeded that of any finite machine. He recognized that this was only a conjecture, since one could never disprove (b). Yet he considered the disjunctive conclusion to be a \"certain fact\".\n\nIn subsequent years, more direct anti-mechanist lines of reasoning were apparently floating around the intellectual atmosphere. In 1960, Hilary Putnam published a paper entitled \"Minds and Machines,\" in which he points out the flaws of a typical anti-mechanist argument. Informally, this is the argument that the (alleged) difference between \"what can be mechanically proven\" and \"what can be seen to be true by humans\" shows that human intelligence is not mechanical in nature. Or, as Putnam puts it:\n\nLet T be a Turing machine which \"represents\" me in the sense that T can prove just the mathematical statements I prove. Then using Gödel's technique I can discover a proposition that T cannot prove, and moreover I can prove this proposition. This refutes the assumption that T \"represents\" me, hence I am not a Turing machine.\n\nHilary Putnam objects that this argument ignores the issue of consistency. Gödel's technique can only be applied to consistent systems. It is conceivable, argues Putnam, that the human mind is inconsistent. If one is to use Gödel's technique to prove the proposition that T cannot prove, one must first prove (the mathematical statement representing) the consistency of T, a daunting and perhaps impossible task. Later Putnam suggested that while Gödel's theorems cannot be applied to humans, since they make mistakes and are therefore inconsistent, it may be applied to the human faculty of science or mathematics in general. If we are to believe that it is consistent, then either we cannot prove its consistency, or it cannot be represented by a Turing machine.\n\nJ. R. Lucas in \"Minds, Machines and Gödel\" (1961), and later in his book \"The Freedom of the Will\" (1970), lays out an anti-mechanist argument closely following the one described by Putnam, including reasons for why the human mind can be considered consistent. Lucas admits that, by Gödel's second theorem, a human mind cannot formally prove its own consistency, and even says (perhaps facetiously) that women and politicians are inconsistent. Nevertheless, he sets out arguments for why a male non-politician can be considered consistent. These arguments are philosophical in nature and are the subject of much debate; Lucas provides references to responses on his own website.\n\nAnother work was done by Judson Webb in his 1968 paper \"Metamathematics and the Philosophy of Mind\". Webb claims that previous attempts have glossed over whether one truly can see that the Gödelian statement \"p\" pertaining to oneself, is true. Using a different formulation of Gödel's theorems, namely, that of Raymond Smullyan and Emil Post, Webb shows one can derive convincing arguments for oneself of both the truth and falsity of \"p\". He furthermore argues that all arguments about the philosophical implications of Gödel's theorems are really arguments about whether the Church-Turing thesis is true.\n\nLater, Roger Penrose entered the fray, providing somewhat novel anti-mechanist arguments in his books, \"The Emperor's New Mind\" (1989) [ENM] and \"Shadows of the Mind\" (1994) [SM]. These books have proved highly controversial. Martin Davis responded to ENM in his paper \"Is Mathematical Insight Algorithmic?\" (ps), where he argues that Penrose ignores the issue of consistency. Solomon Feferman gives a critical examination of SM in his paper \"Penrose's Gödelian argument\" (pdf). The response of the scientific community to Penrose's arguments has been negative, with one group of scholars calling Penrose's repeated attempts to form a persuasive Gödelian argument \"a kind of intellectual shell game, in which a precisely defined notion to which a mathematical result applies... is switched for a vaguer notion\".\n\nA Gödel-based anti-mechanism argument can be found in Douglas Hofstadter's book \"\", though Hofstadter is widely viewed as a known skeptic of such arguments:\nLooked at this way, Gödel's proof suggests – though by no means does it prove! – that there could be some high-level way of viewing the mind/brain, involving concepts which do not appear on lower levels, and that this level might have explanatory power that does not exist – not even in principle – on lower levels. It would mean that some facts could be explained on the high level quite easily, but not on lower levels at all. No matter how long and cumbersome a low-level statement were made, it would not explain the phenomena in question.\nIt is analogous to the fact that, if you make derivation after derivation in Peano arithmetic, no matter how long and cumbersome you make them, you will never come up with one for G – despite the fact that on a higher level, you can see that the Gödel sentence is true.\n\nWhat might such high-level concepts be? It has been proposed for eons, by various holistically or \"soulistically\" inclined scientists and humanists that consciousness is a phenomenon that escapes explanation in terms of brain components; so here is a candidate at least. There is also the ever-puzzling notion of free will. So perhaps these qualities could be \"emergent\" in the sense of requiring explanations which cannot be furnished by the physiology alone\n\n\n"}
{"id": "24118168", "url": "https://en.wikipedia.org/wiki?curid=24118168", "title": "Nano spray dryer", "text": "Nano spray dryer\n\nNano spray dryers refer to using spray drying to create particles in the nanometer range. Spray drying is a gentle method for producing powders with a defined particle size out of solutions, dispersions, and emulsions which is widely used for pharmaceuticals, food, biotechnology, and other industrial materials synthesis. \n\nIn the past, the limitations of spray drying were the particle size (minimum 2 micrometres), the yield (maximum around 70%), and the sample volume (minimum 50 ml for devices in lab scale). Recently, minimum particle sizes have been reduced to 300 nm, yields up to 90% are possible, and the sample amount can be as small as 1 ml. These expanded limits are possible due to new technological developments to the spray head, the heating system, and the electrostatic particle collector. To emphasize the small particle sizes possible with this new technology, it has been described as \"nano\" spray drying. However, the smallest particles produced are in the sub-micrometre range common to fine particles rather than the nanometer scale of ultrafine particles.\nThe functional principle is basically the same as with normal spray dryers. There are just different technologies that are used to do similar things.\n\nThe drying gas enters the system via the heater. A new kind of heater system allows for laminar air flow. The spray head sprays the fine droplets with a narrow size distribution into the drying chamber. The droplets dry and become solid particles. The solid particles are separated in the electrostatic particle collector. The exhaust gas is filtered and sent to a fume hood or the environment. The inlet temperature is controlled by a temperature sensor.\n\nPharmaceuticals:\nThis technique is widely used in the pharma market. Because of the small sample amounts and the high yields it is ideal for spray drying expensive substances in basic research. The following list shows examples of what is possible:\n\n\nMaterials science:\nThis new technique offers new prospects in materials science, specially in the nanomaterial field. Now it is possible to spray dry fine particles. The following list shows examples of what is possible:\n\nFood:\nAlso in the field of food science this technology offers new possibilities. Especially in the currently vibrant field of functional food, the following list shows examples of what is possible:\nOne of the three new technologies that makes \"nano\" spray drying possible is the spray head. \nA piezoelectric system precisely vibrates a fine mesh. Vibration produces fine droplets with a narrow size distribution.\n\nIn the field of \"nano\" spray drying a new heating system is used to provide the drying gas to produce the particles. The gas flow in the system is laminar and not turbulent as in common spray drying. The advantage of a laminar flow is that the particles fall straight down from the spray head and do not stick to the glass wall.\n\nThe laminar flow is produced by pressing the air through a porous metal foam.\n\nTo collect the very fine particles a new technology is used in the field of \"nano\" spray drying. The reason is that common cyclone technology depends on the particle mass; particles smaller than 2 μm can’t be separated and instead exit the system along with the exhaust gas.\n\nThe electrostatic particle collector charges the dry particles' surface and deflects them with an electrical field. To produce the electrical field, a high voltage (16 kV) is applied to a round collector tube. The electrical field builds up between the inner wall of the collector tube and the tips of a grounded star electrode. To have a low level of energy in the system the current is very low.\n\nAfter getting deflected the particles stay at the inner wall of the particle collector tube and are completely uncharged. This separation method works fine for all kinds of materials.\n\nThe efficiency of the electrostatic particle collector is very high: 99% of all particles that enter the system are collected.\n"}
{"id": "905278", "url": "https://en.wikipedia.org/wiki?curid=905278", "title": "Nanotechnology education", "text": "Nanotechnology education\n\nNanotechnology education involves a multidisciplinary natural science education with courses such as physics, chemistry, mathematics and molecular biology. It is being offered by many universities around the world. The first program involving nanotechnology was offered by the University of Toronto's Engineering Science program, where nanotechnology could be taken as an option.\n\nHere is a partial list of universities offering nanotechnology education, and the degrees offered (Bachelor of Science, Master of Science, and/or Ph.D in Nanotechnology).\n\n\n\nA list of the master's programs is kept by the UK-based Institute of Nanotechnology in their Nano, Enabling, and Advanced Technologies (NEAT) Post-graduate Course Directory.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nImportant:\n\n\n\n\n\n\n\n\n\n\n\nIn recent years, there has been a growing interest in introducing nanoscience and nanotechnology in grade schools, especially at the high school level. \nIn the United States, although very few high schools officially offer a two-semester course in nanotechnology, “nano” concepts are bootstrapped and taught during traditional science classes using a number of educational resources and hands-on activities developed by dedicated non-profit organizations, such as:\n\n\nIn Egypt, in2nano is a high school outreach program aiming to increase scientific literacy and prepare students for the sweeping changes of nanotechnology.\n\n"}
{"id": "18700697", "url": "https://en.wikipedia.org/wiki?curid=18700697", "title": "Open peer review", "text": "Open peer review\n\nOpen peer review is a process in which names of peer reviewers of papers submitted to academic journals are disclosed to the authors of the papers in question. In some cases, as with the \"BMJ\" and BioMed Central, the process also involves posting the entire pre-publication history of the article online, including not only signed reviews of the article, but also its previous versions and author responses to the reviewers.\n\nThere is no single definition of open peer review, as it is implemented differently by different academic journals, but it has been broadly defined as \"any scholarly review mechanism providing disclosure of author and referee identities to one another at any point during the peer review or publication process\".\n\nPossible advantages to an open peer-review system include reviewers being \"more tactful and constructive\" than they would be if they could remain anonymous. It has also been argued that open review leads to more honest reviewing and prevents reviewers from following their individual agendas, as well as leading to the detection of reviewers' conflicts of interests. Some studies have also found that open peer review is associated with an increase in quality of reviews, although other studies have not found such an association. A study of BioMed Central medical journals, all of which use open peer review, found that reviewers usually did not notice problems or request changes in reporting of the results of randomized trials. The same study found most, but not all, of the requested changes had a positive effect on reporting.\n\nA 1999 study found that open peer review did not affect the quality of reviews or the recommendation regarding whether the paper being reviewed should be published, but that it \"significantly increased the likelihood of reviewers declining to review\". Open review of abstracts tended to lead to bias favoring authors from English-speaking countries and prestigious academic institutions. It has also been argued that open peer review could lead to authors accumulating enemies who try to keep their papers from being published or their grant applications from being successful.\n"}
{"id": "46531324", "url": "https://en.wikipedia.org/wiki?curid=46531324", "title": "Open synthetic biology", "text": "Open synthetic biology\n\nOpen Synthetic Biology (Open SynBio) is the idea that scientific knowledge and data should be openly accessible through common rights licensing to enable the rapid development of safe, effective and commercially viable synthetic biology applications.\n\nIts foundational concepts are:\n\n\nOpen SynBio is a theoretical framework supporting a global ecosystem of responsible and capable research scientists working collaboratively on synthetic biology application development projects to reduce cost, time, and risks of developing new synthetic biology applications (including Open SynBio therapeutics) from the inception of primary science to applications reaching market readiness and commercial viability.\n\nIts general principle is that participating research scientists agree to share research, data, findings and results with the Open Synthetic Biology community and the public generally. The Open SynBio community will set standards and expectations of the participants and their \"science to market\" process and the community will work collaboratively with downstream stakeholders (e.g., investors and business advisors) to ensure public safety and general availability of new synthetic biology applications.\n\nOne example of open synthetic biology is when DNA2.0 donated several artificial gene sequences into an open-access repository run by the BioBricks Foundation.\n\n"}
{"id": "7528885", "url": "https://en.wikipedia.org/wiki?curid=7528885", "title": "Outline of Big Science", "text": "Outline of Big Science\n\nThe following outline is provided as an overview of and topical guide to Big Science.\n\nBig Science – term used by scientists and historians of science to describe a series of changes in science which occurred in industrial nations during and after World War II.\n\n\n\n"}
{"id": "4624242", "url": "https://en.wikipedia.org/wiki?curid=4624242", "title": "Parascience", "text": "Parascience\n\nParascience is the study of subjects that are outside the scope of the natural and social sciences because they cannot be explained by accepted scientific theory or tested by conventional scientific methods. This study may be concerned with phenomena assumed to be beyond the scope of scientific inquiry or for which no scientific explanation exists. The parasciences include history, philosophy, art, and religion.\n\nParascience can also be defined as a subject, method, etc., purporting to be scientific but regarded as unorthodox or unacceptable by the scientific community; an \"alternative\" science.\n"}
{"id": "7972385", "url": "https://en.wikipedia.org/wiki?curid=7972385", "title": "Phantom of the Poles", "text": "Phantom of the Poles\n\nThe Phantom of the Poles is a book written by William Reed, and published in 1906. It attempts to explain certain mysterious phenomena reported by polar explorers by postulating that the Earth is in fact hollow, with holes at its poles.\n\nIn the General Summary chapter of \"The Phantom of the Poles\", Reed posed several questions that he claimed were explained by the Hollow Earth theory:\nAdmiral Peary claimed to have reached the North Pole on April 6, 1909. This would have invalidated Reed's premise that the poles cannot be reached. Although Peary's claim was, in its day, and continues to be controversial, on December 14, 1911, Roald Amundsen undisputedly reached the South Pole. Subsequent expeditions to and flights over the south pole have conclusively demonstrated that there are no large holes there.\n\n"}
{"id": "25183022", "url": "https://en.wikipedia.org/wiki?curid=25183022", "title": "Private-collective model of innovation", "text": "Private-collective model of innovation\n\nThe term private-collective model of innovation was coined by Eric von Hippel and Georg von Krogh in their 2003 publication in \"\". This innovation model represents a combination of the private investment model and the collective-action innovation model.\n\nIn the private investment model innovators appropriate financial returns from innovations through intellectual property rights such as patents, copyright, licenses, or trade secrets. Any knowledge spillover reduces the innovator's benefits, thus freely revealed knowledge is not in the interest of the innovator.\n\nThe collective-action innovation model explains the creation of public goods which are defined by the non-rivalry of benefits and non-excludable access to the good. In this case the innovators do not benefit more than any one else not investing into the public good, thus free-riding occurs. In response to this problem, the cost of innovation has to be distributed, therefore governments typically invest into public goods through public funding.\n\nAs combination of these two models, the private-collective model of innovation explains the creation of public goods through private funding. The model is based on the assumption that the innovators privately creating the public goods benefit more than the free-riders only consuming the public good. While the result of the investment is equally available to all, the innovators benefit through the \"process\" of creating the public good. Therefore, private-collective innovation occurs when the process-related rewards exceed the process-related costs.\n\nA laboratory study traced the initiation of private-collective innovation to the first decision to share knowledge in a two-person game with multiple equilibria. The results indicate fragility: when individuals face opportunity costs to sharing their knowledge with others they quickly turn away from the social optimum of mutual sharing. The opportunity costs of the \"second player\", the second person deciding whether to share, have a bigger (negative) impact on knowledge sharing than the opportunity costs of the first person to decide. Overall, the study also observed sharing behavior in situations where none was predicted.\n\nRecent work shows that a project will not \"take off\" unless the right incentives are in place for innovators to contribute their knowledge to open innovation from the beginning. The article explores social preferences in the initiation of PCI. It conducted a simulation study that elucidates how inequality aversion, reciprocity, and fairness affect the underlying conditions that lead to the initiation of Private-collective innovation.\n\nWhile firms increasingly seek to cooperate with outside individuals and organizations to tap into their ideas for new products and services, mechanisms that motivate innovators to \"open up\" are critical in achieving the benefits of open innovation.\n\nThe theory of private collective innovation has recently been extended by a study on the exclusion rights for technology in the competition between private-collective and other innovators. The authors argue that the investment in orphan exclusion rights for technology serves as a subtle coordination mechanism against alternative proprietary solutions.\n\nAdditionally, the research on private-collective innovation has been extended with theoretical explanations and empirical evidence of egoism and altruism as significant explanations for cooperation in private-collective innovation. Benbunan-Fich and Koufaris show that contributions to a social bookmarking site are a combination of intentional and unintentional contributions. The intentional public contribution of bookmarks is driven by an egoistic motivation to contribute valuable information and thus showing competence.\n\nThe development of open source software / Free Software (consequently named Free and Libre Open Source Software – FLOSS) is the most prominent example of private-collective innovation. By definition, FLOSS represents a public good. It is non-rival because copying and distributing software does not decrease its value. And it is non-excludable because FLOSS licenses enable everyone to use, change and redistribute the software without any restriction.\n\nWhile FLOSS is created by many unpaid individuals, it has been shown that technology firms invest substantially in the development of FLOSS. These companies release previously proprietary software under FLOSS licenses, employ programmers to work on established FLOSS projects, and fund entrepreneurial firms to develop certain features. In this way, private entities invest into the creation of public goods.\n"}
{"id": "50891749", "url": "https://en.wikipedia.org/wiki?curid=50891749", "title": "Research proposal", "text": "Research proposal\n\nA research proposal is a document proposing a research project, generally in the sciences or academia, and generally constitutes a request for sponsorship of that research. Proposals are evaluated on the cost and potential impact of the proposed research, and on the soundness of the proposed plan for carrying it out. Research proposals generally address several key points:\n\nResearch proposals may be \"solicited\", meaning that they are submitted in response to a request with specified requirements, such as a request for proposal, or they may be \"unsolicited\", meaning they are submitted without prior request. Other types of proposals include \"preproposals\", where a letter of intent or brief abstract is submitted for review prior to submission of a full proposal; continuation proposals, which re-iterate an original proposal and its funding requirements in order to ensure continued funding; and renewal proposals, which seek continued sponsorship of a project which would otherwise be terminated.\n\nAcademic research proposals are generally written as part of the initial requirements of writing a thesis, research paper, or dissertation. They generally follow the same format as a research paper, with an introduction, a literature review, a discussion of research methodology and goals, and a conclusion. This basic structure may vary between projects and between fields, each of which may have its own requirements.\n"}
{"id": "3437663", "url": "https://en.wikipedia.org/wiki?curid=3437663", "title": "Science, technology, engineering, and mathematics", "text": "Science, technology, engineering, and mathematics\n\nScience, Technology, Engineering and Mathematics (STEM), previously Science, Math, Engineering and Technology (SMET), is a term used to group together these academic disciplines. This term is typically used when addressing education policy and curriculum choices in schools to improve competitiveness in science and technology development. It has implications for workforce development, national security concerns and immigration policy.\n\nThe acronym came into common use shortly after an interagency meeting on science education held at the US National Science Foundation chaired by the then NSF director Rita Colwell.\nA director from the Office of Science division of Workforce Development for Teachers and Scientists, Peter Faletra, suggested the change from the older acronym SMET to STEM. Colwell, expressing some dislike for the older acronym, responded by suggesting NSF institute the change. However, the acronym STEM predates NSF and likely traces its origin to Charles Vela, the founder and director of the Center for the Advancement of Hispanics in Science and Engineering Education (CAHSEE). In the early 1990's CAHSEE started a summer program for talented under-represented students in the Washington, DC area called the STEM Institute. Based on the program's recognized success and his expertise in STEM education, Charles Vela was asked to serve on numerous NSF and Congressional panels in science, mathematics and engineering education; it is through this manner that NSF was first introduced to the acronym STEM. One of the first NSF projects to use the acronym was STEMTEC, the Science, Technology, Engineering and Math Teacher Education Collaborative at the University of Massachusetts Amherst, which was founded in 1998.\n\n\nIn the United States, the acronym began to be used in education and immigration debates in initiatives to begin to address the perceived lack of qualified candidates for high-tech jobs. It also addresses concern that the subjects are often taught in isolation, instead of as an integrated curriculum. Maintaining a citizenry that is well versed in the STEM fields is a key portion of the public education agenda of the United States. The acronym has been widely used in the immigration debate regarding access to United States work visas for immigrants who are skilled in these fields. It has also become commonplace in education discussions as a reference to the shortage of skilled workers and inadequate education in these areas. The term tends not to refer to the non-professional and less visible sectors of the fields, such as electronics assembly line work.\n\nMany organizations in the United States follow the guidelines of the National Science Foundation on what constitutes a STEM field. The NSF uses a broader definition of STEM subjects that includes subjects in the fields of chemistry, computer and information technology science, engineering, geosciences, life sciences, mathematical sciences, physics and astronomy, social sciences (anthropology, economics, psychology and sociology), and STEM education and learning research. Eligibility for scholarship programs such as the CSM STEM Scholars Program use the NSF definition.\n\nThe NSF is the only American federal agency whose mission includes support for all fields of fundamental science and engineering, except for medical sciences. Its disciplinary program areas include scholarships, grants, fellowships in fields such as biological sciences, computer and information science and engineering, education and human resources, engineering, environmental research and education, geosciences, international science and engineering, mathematical and physical sciences, social, behavioral and economic sciences, cyberinfrastructure, and polar programs.\n\nAlthough many organizations in the United States follow the guidelines of the National Science Foundation on what constitutes a STEM field, the United States Department of Homeland Security (DHS) has its own functional definition used for immigration policy. In 2012, DHS or ICE announced an expanded list of STEM designated-degree programs that qualify eligible graduates on student visas for an optional practical training (OPT) extension. Under the OPT program, international students who graduate from colleges and universities in the United States are able to remain in the country and receive training through work experience for up to 12 months. Students who graduate from a designated STEM degree program can remain for an additional 17 months on an OPT STEM extension.\n\nAn exhaustive list of STEM disciplines does not exist because the definition varies by organization. The U.S. Immigration and Customs Enforcement lists disciplines including physics, actuarial science, chemistry, biology, mathematics, applied mathematics, statistics, computer science, computational science, psychology, biochemistry, robotics, computer engineering, electrical engineering, electronics, mechanical engineering, industrial engineering, information science, information technology, civil engineering, aerospace engineering, chemical engineering, astrophysics, astronomy, optics, nanotechnology, nuclear physics, mathematical biology, operations research, neurobiology, biomechanics, bioinformatics, acoustical engineering, geographic information systems, atmospheric sciences, educational/instructional technology, software engineering, and educational research.\n\nBy cultivating an interest in the natural and social sciences in preschool or immediately following school entry, the chances of STEM success in high school can be greatly improved. School integration can help black, Hispanic and aboriginal students catch up with Asian and white students.\n\nSTEM supports broadening the study of engineering within each of the other subjects, and beginning engineering at younger grades, even elementary school. It also brings STEM education to all students rather than only the gifted programs. In his 2012 budget, President Barack Obama renamed and broadened the \"Mathematics and Science Partnership (MSP)\" to award block grants to states for improving teacher education in those subjects.\n\nIn the 2015 run of the international assessment test the Program for International Student Assessment (PISA), American students came out 35th in mathematics, 24th in reading and 25th in science, out of 109 countries. The United States also ranked 29th in the percentage of 24-year-olds with science or mathematics degrees. \n\nSTEM education often uses new technologies such as RepRap 3D printers to encourage interest in STEM fields.\n\nIn 2006 the United States National Academies expressed their concern about the declining state of STEM education in the United States. Its Committee on Science, Engineering, and Public Policy developed a list of 10 actions. Their top three recommendations were to:\n\nThe National Aeronautics and Space Administration also has implemented programs and curricula to advance STEM education in order to replenish the pool of scientists, engineers and mathematicians who will lead space exploration in the 21st century.\n\nIndividual states, such as California, have run pilot after-school STEM programs to learn what the most promising practices are and how to implement them to increase the chance of student success. Another state to invest in STEM education is Florida, where Florida Polytechnic University, Florida’s first public university for engineering and technology dedicated to science, technology, engineering and mathematics (STEM), was established. During school, STEM programs have been established for many districts throughout the U.S. Some states include New Jersey, Arizona, Virginia, North Carolina, Texas, and Ohio.\n\nContinuing STEM education has expanded to the post-secondary level through masters programs such as the University of Maryland's STEM Program as well as the University of Cincinnati.\n\nIn the United States, the National Science Foundation found that the average science score on the 2011 National Assessment of Educational Progress was lower for black and Hispanic students than white, Asian, and Pacific Islanders. In 2011, eleven percent of the U.S. workforce was black, while only six percent of STEM workers were black. Though STEM in the U.S. has typically been dominated by white males, there have been considerable efforts to create initiatives to make STEM a more racially and gender diverse field. Some evidence suggests that all students, including black and Hispanic students, have a better chance of earning a STEM degree if they attend a college or university at which their entering academic credentials are at least as high as the average student's.\n\nIn the State of the Union Address on January 31, 2006, President George W. Bush announced the American Competitiveness Initiative. Bush proposed the initiative to address shortfalls in federal government support of educational development and progress at all academic levels in the STEM fields. In detail, the initiative called for significant increases in federal funding for advanced R&D programs (including a doubling of federal funding support for advanced research in the physical sciences through DOE) and an increase in U.S. higher education graduates within STEM disciplines.\n\nThe \"NASA Means Business\" competition, sponsored by the Texas Space Grant Consortium, furthers that goal. College students compete to develop promotional plans to encourage students in middle and high school to study STEM subjects and to inspire professors in STEM fields to involve their students in outreach activities that support STEM education.\n\nThe National Science Foundation has numerous programs in STEM education, including some for K–12 students such as the ITEST Program that supports The Global Challenge Award ITEST Program. STEM programs have been implemented in some Arizona schools. They implement higher cognitive skills for students and enable them to inquire and use techniques used by professionals in the STEM fields.\n\nThe STEM Academy is a national nonprofit-status organization dedicated to improving STEM literacy for all students. It represents a recognized national next-generation high-impact academic model. The practices, strategies, and programming are built upon a foundation of identified national best practices which are designed to improve under-represented minority and low-income student growth, close achievement gaps, decrease dropout rates, increase high school graduation rates and improve teacher and principal effectiveness. The STEM Academy represents a flexible use academic model that targets all schools and is for all students.\n\nProject Lead The Way (PLTW) is a leading provider of STEM education curricular programs to middle and high schools in the United States. The national nonprofit organization has over 5,200 programs in over 4,700 schools in all 50 states. Programs include a high school engineering curriculum called \"Pathway To Engineering\", a high school biomedical sciences program, and a middle school engineering and technology program called \"Gateway To Technology\". PLTW provides the curriculum and the teacher professional development and ongoing support to create transformational programs in schools, districts, and communities. PLTW programs have been endorsed by President Barack Obama and United States Secretary of Education Arne Duncan as well as various state, national, and business leaders.\n\n\"The Science, Technology, Engineering, and Mathematics (STEM) Education Coalition\" works to support STEM programs for teachers and students at the U. S. Department of Education, the National Science Foundation, and other agencies that offer STEM-related programs. Activity of the STEM Coalition seems to have slowed since September 2008.\n\nIn 20102, the Boy Scouts of America began handing out awards, titled NOVA and SUPERNOVA, for completing specific requirements appropriate to scouts' program level in each of the four main STEM areas. The Girl Scouts of the USA has similarly incorporated STEM into their program through the introduction of merit badges such as \"Naturalist\" and \"Digital Art\".\n\nSAE is an international organization, solutions'provider specialized on supporting education, award and scholarship programs for STEM matters, from pre-K to the College degree. It also promotes scientific and technologic innovation.\n\nThe eCybermission is a free, web-based science, mathematics and technology competition for students in grades six through nine sponsored by the U.S. Army. Each webinar is focused on a different step of the scientific method and is presented by an experienced eCybermission CyberGuide. CyberGuides are military and civilian volunteers with a strong background in STEM and STEM education, who are able to provide valuable insight into science, technology, engineering, and mathematics to students and team advisers.\n\nSTARBASE is a premier educational program, sponsored by the Office of the Assistant Secretary of Defense for Reserve Affairs. Students interact with military personnel to explore careers and make connections with the \"real world.\" The program provides students with 20–25 hours of stimulating experiences at National Guard, Navy, Marines, Air Force Reserve and Air Force bases across the nation.\n\nSeaPerch is an innovative underwater robotics program that trains teachers to teach their students how to build an underwater remotely operated vehicle (ROV) in an in-school or out-of-school setting. Students build the ROV from a kit composed of low-cost, easily accessible parts, following a curriculum that teaches basic engineering and science concepts with a marine engineering theme.\n\nNASAStem is a program of the U.S. space agency NASA to increase diversity within its ranks, including age, disability, and gender as well as race/ethnicity.\n\nThe America COMPETES Act (P.L. 110-69) became law on August 9, 2007. It is intended to increase the nation's investment in science and engineering research and in STEM education from kindergarten to graduate school and postdoctoral education. The act authorizes funding increases for the National Science Foundation, National Institute of Standards and Technology laboratories, and the Department of Energy (DOE) Office of Science over FY2008–FY2010. Robert Gabrys, Director of Education at NASA's Goddard Space Flight Center, articulated success as increased student achievement, early expression of student interest in STEM subjects, and student preparedness to enter the workforce.\n\nIn November 2012 the White House announcement before congressional vote on the STEM Jobs Act put President Obama in opposition to many of the Silicon Valley firms and executives who bankrolled his re-election campaign. The Department of Labor identified 14 sectors that are \"projected to add substantial numbers of new jobs to the economy or affect the growth of other industries or are being transformed by technology and innovation requiring new sets of skills for workers.\" The identified sectors were as follows: advanced manufacturing, Automotive, construction, financial services, geospatial technology, homeland security, information technology, Transportation, Aerospace, Biotechnology, energy, healthcare, hospitality, and retail.\n\nThe Department of Commerce notes STEM fields careers are some of the best-paying and have the greatest potential for job growth in the early 21st century. The report also notes that STEM workers play a key role in the sustained growth and stability of the U.S. economy, and training in STEM fields generally results in higher wages, whether or not they work in a STEM field.\n\nIn 2015, there were around 9.0 million STEM jobs in the United States, representing 6.1% of American employment. STEM jobs were increasing around 9% percent per year.. Brookings Institution found that the demand for competent technology graduates will surpass the number of capable applicants by at least one million individuals.  The BLS noted that almost 100 percent of STEM jobs require postsecondary education, while only 36 percent of other jobs call for that same degree.\n\nPEW findings revealed that Americans identified several issues that hound STEM education which included unconcerned parents, disinterested students, obsolete curriculum materials, and too much focus on state parameters. 57 percent of survey respondents pointed out that one main problem of STEM is lack of students’ concentration in learning. \n\nThe recent National Assessment of Educational Progress (NAEP) report card made public technology as well as engineering literacy scores which determines whether students have the capability to apply technology and engineering proficiency to real-life scenarios. The report showed a gap of 28 points between low-income students and their high-income counterparts. The same report also indicated a 38-point difference between white and black students.\n\nThe 2019 fiscal budget proposal of the White House supported the funding plan in President Donald Trump’s Memorandum on STEM Education which allocated around $200 million (grant funding) on STEM education every year. This budget also supports STEM through a grant program worth $20 million for career as well as technical education programs. In September 2017, a number of large American technology firms collectively pledged to donate $300 million for computer science education in the U.S.\n\nCanada ranks 12th out of 16 peer countries in the percentage of its graduates who studied in STEM programs, with 21.2%, a number higher than the United States, but lower than France, Germany, and Austria. The peer country with the greatest proportion of STEM graduates, Finland, has over 30% of their university graduates coming from science, mathematics, computer science, and engineering programs.\n\nSHAD is an annual Canadian summer enrichment program for high-achieving high school students in July. The program focuses on academic learning particularly in STEAM fields.\n\nScouts Canada has taken similar measures to their American counterpart to promote STEM fields to youth. Their STEM program began in 2015.\n\nIn 2011 Canadian entrepreneur and philanthropist Seymour Schulich established the Schulich Leader Scholarships, $100 million in $60,000 scholarships for students beginning their university education in a STEM program at 20 institutions across Canada. Each year 40 Canadian students would be selected to receive the award, two at each institution, with the goal of attracting gifted youth into the STEM fields. The program also supplies STEM scholarships to five participating universities in Israel.\n\nSeveral European projects have promoted STEM education and careers in Europe. For instance, Scientix is a European cooperation of STEM teachers, education scientists, and policymakers. The SciChallenge project used a social media contest and the student-generated content to increase motivation of pre- university students for STEM education and careers.\n\nSTEM education has not been promoted among the local schools in Hong Kong until recent years. In November 2015, the Education Bureau of Hong Kong released a document entitled \"Promotion of STEM Education\", which proposes the strategies and recommendations on promoting STEM education.\n\nTurkish STEM Education Task Force (or FeTeMM—Fen Bilimleri, Teknoloji, Mühendislik ve Matematik) is a coalition of academicians and teachers who show an effort to increase the quality of education in STEM fields rather than focussing on increasing the number of STEM graduates.\n\nIn Qatar, AL-Bairaq is an outreach program to high-school students with a curriculum that focuses on STEM, run by the Center for Advanced Materials (CAM) at Qatar University. Each year around 946 students, from about 40 high schools, participate in AL-Bairaq competitions. AL-Bairaq make use of project-based learning, encourages students to solve authentic problems, and inquires them to work with each other as a team to build real solutions. Research has so far shown positive results for the program.\n\nIn Vietnam, beginning in 2012 many private education organizations has STEM education initiatives.\n\nIn 2015, the Ministry of Science and Technology and Liên minh STEM organized the first National STEM day, followed by many similar events across the country.\n\nin 2015, Ministry of Education and Training included STEM as an area needed to be encouraged in national school year program.\n\nIn May 2017, Prime Minister signed a Directive no. 16 stating: \"Dramatically change the policies, contents, education and vocational training methods to create a human resource capable of receiving new production technology trends, with a focus on promoting training in science, technology, engineering and mathematics (STEM), foreign languages, information technology in general education; \" and asking \"Ministry of Education and Training (to): Promote the deployment of science, technology, engineering and mathematics (STEM) education in general education program; Pilot organize in some high schools from 2017 to 2018.\n\nWomen constitute 47% of the U.S. workforce, and perform 24% of STEM-related jobs. In the UK women perform 13% of STEM-related jobs (2014). In the U.S. women with STEM degrees are more likely to work in education or healthcare rather than STEM fields compared with their male counterparts.\n\nThe gender ratio depends on field of study. For example, in the European Union in 2012 women made up 47.3% of the total, 51% of the social sciences, business and law, 42% of the science, mathematics and computing, 28% of engineering, manufacturing and construction, and 59% of PhD graduates in Health and Welfare.\n\nThe focus on increasing participation in STEM fields has attracted criticism. In the 2014 article \"The Myth of the Science and Engineering Shortage\" in \"The Atlantic\", demographer Michael S. Teitelbaum criticized the efforts of the U.S. government to increase the number of STEM graduates, saying that, among studies on the subject, \"No one has been able to find any evidence indicating current widespread labor market shortages or hiring difficulties in science and engineering occupations that require bachelor's degrees or higher\", and that \"Most studies report that real wages in many—but not all—science and engineering occupations have been flat or slow-growing, and unemployment as high or higher than in many comparably-skilled occupations.\" Teitelbaum also wrote that the then-current national fixation on increasing STEM participation paralleled previous U.S. government efforts since World War II to increase the number of scientists and engineers, all of which he stated ultimately ended up in \"mass layoffs, hiring freezes, and funding cuts\"; including one driven by the Space Race of the late 1950s and 1960s, which he wrote led to \"a bust of serious magnitude in the 1970s.\"\n\n\"IEEE Spectrum\" contributing editor Robert N. Charette echoed these sentiments in the 2013 article \"The STEM Crisis Is a Myth\", also noting that there was a \"mismatch between earning a STEM degree and having a STEM job\" in the United States, with only around ¼ of STEM graduates working in STEM fields, while less than half of workers in STEM fields have a STEM degree.\n\nEconomics writer Ben Casselman, in a 2014 study of post-graduation earnings for \"FiveThirtyEight\", wrote that, based on the data, science should not be grouped with the other three STEM categories, because, while the other three generally result in high-paying jobs, \"many sciences, particularly the life sciences, pay below the overall median for recent college graduates.\"\n\nEfforts to remedy the perceived domination of STEM subjects by men of Asian and non-Hispanic European backgrounds has led to intense efforts to diversify the STEM workforce. However, some critics feel that this practice in higher education, as opposed to a strict meritocracy, causes lower academic standards.\n\n\n\n"}
{"id": "55894093", "url": "https://en.wikipedia.org/wiki?curid=55894093", "title": "Science education in England", "text": "Science education in England\n\nScience education in England is generally regulated at all levels for assessments that are England's, from 'primary' to 'tertiary' (university). Below university-level, science education is the responsibility of three bodies: the Department for Education, Ofqual and the QAA, but at university-level, science education is regulated by various \"professional bodies\", and the Bologna Process via the QAA. The QAA also regulates science education for some qualifications that are not university degrees via various \"qualification boards\", but not content for GCSEs, and GCE AS and A levels. Ofqual on the other hand regulates science education for GCSEs and AS/A levels, as well as all other qualifications, except those covered by the QAA, also via qualification boards. The Department for Education prescribes the content for science education for GCSEs and AS/A levels, which is implemented by the qualification boards, who are then regulated by Ofqual. The Department for Education also regulates science education for students aged 16 years and under. The department's policies on science education (and indeed all subjects) are implemented by local government authorities on all state schools (also called \"publicly funded\" schools) in England. The content of the nationally organised science curriculum (along with other subjects) for England is published in the National Curriculum, which covers key stage 1 (KS1), key stage 2 (KS2), key stage 3 (KS3) and key stage 4 (KS4). The four key stages can be grouped a number of ways; how they are grouped significantly affects the way the science curriculum is delivered. In state schools, the four key stages are grouped into KS1–2 and KS3–4; KS1–2 covers primary education while KS3–4 covers secondary education. But in independent or public (which in the United Kingdom are historic independent) schools (not to be confused with 'publicly funded' schools), the key stage grouping is more variable, and rather than using the terms ‘primary’ and 'secondary’, the terms ‘prep’ and ‘senior’ are used instead. Science is a compulsory subject in the National Curriculum of England, Wales and Northern Ireland; state schools have to follow the National Curriculum while independent schools need not follow it. That said, science is compulsory in the Common Entrance Examination for entry into senior schools, so it does feature prominently in the curricula of independent schools. Beyond the National Curriculum and Common Entrance Examination, science is voluntary, but the government of the United Kingdom (comprising England, Wales, Scotland and Northern Ireland) provides incentives for students to continue studying science subjects. Science is regarded as vital to the economic growth of the United Kingdom (UK). For students aged 16 years (the upper limit of compulsory \"school age\" in England, but not compulsory education as a whole) and over, there is no \"compulsory\" nationally organised science curriculum for all state/publicly funded education providers in England to follow, and individual providers can set their own content, although they often (and in the case of England's state/publicly funded post-16 schools and colleges have to) get their science (and indeed all) courses \"accredited\" or made \"satisfactory\" (ultimately by either Ofqual or the QAA via the qualification boards). Universities do not need such approval, but there is a reason for them to seek accreditation regardless. Moreover, UK universities have obligations to the Bologna Process to ensure high standards. Science education in England has undergone significant changes over the centuries; facing challenges over that period, and still facing challenges to this day.\n\nGillard (2011) gives a documented account of science curriculum and education during this period. According to his work, the teaching of science in England dates back to at least Anglo-Saxon times. Gillard explains that the first schools in England (that are known of) were created by St Augustine when he brought Christianity to England around the end of the sixth century—there were almost certainly schools in Roman Britain before St Augustine, but they did not survive after the Romans left. It is thought the first grammar school was established at Canterbury in 598 during the reign of King Ethelbert. Gillard also mentions Bede's \"Ecclesiastical History\", here science (in the form of astronomy) was already part of the curriculum in the early schools of the 600s. As the founding of grammar schools spread from south to north of England, science education spread with it. Science as it is known today developed from two spheres of knowledge: natural philosophy and natural history. The former was associated with the reasoning and explanation of nature while the latter focused more on living things. Both strands of knowledge can be identified in a curriculum provided by a school in York run by Alcuin in the 770s and 780s. Subsequent Viking invasions of England interrupted the development of schools, but despite this, through the ages, education in England was provided by the church and grammar schools (which were linked to the church). The link between church and school started to change in the 1300s when schools independent of the church began to emerge. University education in England started in Oxford in the 1100s (although there is evidence that teaching began there in the 1000s). Like pre-university education, science at Oxford University was initially taught in the form of astronomy (as part of the quadrivium). The Renaissance spurred physical inquiry into nature which led to natural philosophy developing into physics and chemistry, and natural history developing into biology; these three disciplines form natural science, from which interdisciplinary fields (or at least their modern versions) that overlap two or all three branches of natural science develop. This emerging trend in physical inquiry do not appear to have been reflected in the science curriculum in schools at the time. Even in universities, the changes to science education that were necessary as a result of the Renaissance occurred very slowly. It was not till the 1800s that the science curriculum and education recognised in England today at all levels truly began to emerge.\n\nUp until the 1800s there were only two stages of education: elementary and university. However, in the nineteenth century, elementary education began to divide into primary (still called elementary) and secondary education. Elementary schools were defined in law in England through a series of acts of parliament which made education compulsory and free for children up to the age of 11 (later increased to 12). There were six (and later seven) standards for children to pass; science education did not feature in any of these standards, but for some schools it was an \"add-on\" especially at the higher standards (such as sixth and seventh—science subjects included physics, chemistry, mechanics). Promotion from one standard to the next was on merit and not age. Not all children completed all standards, which meant that by the age of 12, there were children that had not ‘completed’ their elementary education. Of course families that could afford (and wanted) to keep their children in school post-compulsory age to pass all standards did so. In fact some children stayed in school beyond the seventh standard. Schools that offered post-seventh standard education became known as \"higher grade schools\", of which science education was a recognised feature of their curricula.\n\nThis was by far the single most important development for science education in schools in England in the nineteenth century from a British parliament point of view. Ironically the original purpose of the committee that authored the 'Taunton' Report of 1868, or more formally, \"Volume II Miscellaneous Papers of the Schools Inquiry Commission\" (1868), was to examine how best endowed schools should be managed; something Parliament at the time thought was of utmost importance. The committee for the report was chaired by Lord Taunton (born Henry Labouchere). In heading the preparation for the report, Lord Taunton sent a circular letter listing four questions to a number of prominent people in different parts of England on 28 May 1866; the first three were endowment-related issues, but the fourth question was on how to encourage a due supply of qualified teachers. Apart from the contents page, the word \"science\" first appears on page 45 of the report in a reply by one of the recipients of the circular letter; that recipient was Reverend W C Lake. The reverend comments:\nOn page 77 of the report, Edward Twisleton, a member of the Schools Inquiry Commission, comments on the answers provided to the four questions set by the committee's chairman, Lord Taunton, based on feedback from the circular letter sent. To the first question, Twisleton writes:\nThere were noticeable opinions on the issue of science education from contributors that wrote to the committee to express their views. One by Robert Mosley of Holgate Seminary, York (pages 104 to 105 of the report), suggested the inclusion of physical sciences in a 'National education'; this national education being the best way to utilise educational endowment. Based on feedback from contributors, the Taunton Committee gave several arguments in favour of science education; two of them are:\nand\nThe committee subsequently made several recommendations; the first three on promoting scientific education in schools are listed below:\nThe issue of increased cost for fee payers played heavily on the minds of the committee, and although the committee felt that for \"a wealthy country like England\" (page 219 of the report), a slight increase in cost should not be a barrier to science education, it was left to individual schools to decide how to incorporate science into their curricula.\n\nBy the time of the Taunton Report there were four universities in England (Oxford, Cambridge, Durham and London), but from the 1880s, a new wave of universities / university colleges completely separate from the original four began to emerge; these universities were called red brick universities. The first of these universities was established in Manchester in 1880 and was called Victoria University. Over the subsequent 80 years, a further 11 universities outside London, Cambridge, Durham and Oxford were founded, significantly expanding the availability of university (science) education throughout England. All through the 1800s, science was becoming increasingly specialised into the different areas we know today.\n\nThe Education Act 1902 led to the higher grade schools (alluded to earlier) and fee-paying schools being absorbed into the legally defined “higher education” (meaning any education that was not elementary (as primary education was known at the time)). Despite science education in higher grade schools and the recommendations of the Taunton Report, as well as the British Association for the Advancement of Science’s campaign for a science curriculum, science was still seen as a minor subject by the most prestigious public schools. The problem was that most of these public schools had close relationships with Oxford and Cambridge universities which offered the majority of their scholarships in classics, and so science was regarded in low importance by the prestigious schools. Consequently, science education varied significantly across English schools. Numerous education-related acts were passed throughout the twentieth century, but the most important in the history of science education in England was the Education Reform Act 1988 (see next subsection). Another act of importance to the development of science education below university-level in England was the Education Act 1944. The 1944 act's contribution was indirect though—it raised the compulsory school age to 15, but made provisions for it to be raised to 16 at a future date—which happened in 1972 (which is still the case today). By raising the school leaving age to 16, this formed the basis for creating a nationally organised science curriculum and education in England. However, the 1944 Education Act did not stipulate that science be taught. For university-level science education, two significant developments were the expansion of distance learning science courses and the introduction of the World Wide Web (via the Internet) into the delivery of science teaching, although this has also been adopted below university-level.\n\nThis was the most important development in the history of science education in England. It was this act that established the National Curriculum and made science compulsory across both secondary and primary schools (alongside maths and English). The 1988 act in effect implemented the recommendation of the Taunton Committee made more than a century earlier. The act also established the now familiar “key stages”.\n\nThe most significant developments to the science curriculum and education in this period to date have been the expansion of the compulsory science content in the National Curriculum and the associated changes to its assessment. Another significant event was the passing of the Education and Skills Act 2008, which raised the education leaving age in England to 18. It is unclear whether this extension of compulsory education will result in more science learners as science is not compulsory after the age of 16—the school leaving age, which the 2008 act did not alter.\n\nCompulsory science content is provided by the National Curriculum and generally applies to children between the ages of 5 and 16. These eleven years of compulsory education are divided by the state into four key stages: KS1, KS2, KS3 and KS4. Regardless of key stage, the National Curriculum states two overarching aims of science education:\nA third aim is common to KS1–3:\nBut for KS4, the third aim is far more detailed, and there is also a fourth aim:\nThe need for mathematical skills is stressed by the National Curriculum across all key stages, but more so at KS3 and KS4.\n\nThe National Curriculum for science is a spiral curriculum; it is also prescriptive. Because of its spiral nature, this makes its learning essentially constructivist. These points are illustrated in the subsections that follow. In addition, the Science National Curriculum emphasises the need for active learning right from the child’s earliest exposure to the curriculum. Research on the value of active learning has been demonstrated and published. Experimentation by the child is underscored in the curriculum accompanied by careful discussion of what was observed. Despite these positive features, it has been argued that evaluating the effectiveness of the National Curriculum on learning is difficult to answer.\n\nKey stage 1 (KS1) covers the first two years of compulsory school education in the National Curriculum. As such, the years are referred to as years 1 and 2. Children are typically in the age range 5–7. The emphasis of science at this stage is observation and describing or drawing things that the child can see, either around them or from a book or photograph or video; the feel of materials is also an important feature of KS1 science. Abstract concepts in science are not introduced at this stage (at least not on the basis of the National Curriculum). As a result, the science curriculum at KS1 is more or less plants and animals, and materials, with the emphasis on what can easily be seen or described by feeling things.\n\nKey stage 2 (KS2) covers years 3, 4, 5 and 6 of compulsory school education in the National Curriculum. It is the longest stage of compulsory school education in England. Children are typically in the age range 7–11. The National Curriculum divides KS2 into lower KS2 (years 3 and 4) and upper KS2 (years 5 and 6). Year 3 continues from KS1, but more complex observations for the child to do on plants and animals, and materials—rocks, fossils and soils, are brought in. Setting up simple experiments and recording data become increasingly important at this stage. Hazards and dangers of certain scientific experiments (such as feeling things after they have been heated) are drilled into pupils; necessary precautions against such dangers/hazards are taught. New areas are introduced: light (and the dangers of looking directly at sunlight with necessary precautions), forces and magnets. In year 4, classification of living and non-living things come to the fore; additional areas introduced include:\nIn years 5 and 6 (upper KS2), the National Curriculum states that the emphasis should be on enabling pupils develop a deeper understanding of scientific ideas. The need to read, spell and pronounce scientific vocabulary correctly is emphasised by the National Curriculum. This emphasis probably reflects the fact that by the age of 9, 10 or 11, a child in England should be able to read and write properly. Year 5 continues on from year 4; studying increasingly more complex aspects of what was introduced in year 4. Also the pupil starts to learn about accepting or \"refuting\" ideas based on scientific evidence. Additional areas include:\nYear 6 not only continues on from year 5, adding more complex aspects of what was learnt in year 5, but also prepares the pupil for KS3 science; additional areas include:\n\nBetween the early 1990s and early 2010s, state school pupils had to take statutory SAT exams at the end of KS2 science although teacher assessments were also allowed. The KS2 SAT science exam consisted of two papers (forty-five minutes each). The scores from both papers were combined to give a final score. This score would then be converted into a \"numerical\" level, which would in turn be converted into an \"expectation\" level. The conversion scale for the levels at KS2 SAT science is shown in the table below.\n\n\"Science KS2 SATs\"\nLevel 6 (exceptional) was also available, but only in mathematics and English (reading); a separate test for level 6 assessment had to be taken, which had to be marked externally. Science KS2 SATs were discontinued in 2013 and replaced by teacher assessments (which were already allowed during the time of SATs). In addition to teacher assessments, a SAT replacement assessment called \"key stage 2 science sampling test\" is now offered to five randomly selected pupils in a school every two years. The test comprises three papers: ‘b’ for biology, ‘c’ for chemistry, and ‘p’ for physics (each twenty-five minutes). The aim of the tests is to assess how well children are getting on with the curriculum. The first test of this kind was in the summer of 2016.\n\nThis exam is taken by KS2 pupils wishing to be admitted into independent senior schools for KS3 study and higher (although not all senior schools admit 11-year-olds). The exam is typically taken while the pupil is in prep school, although some state school pupils use the exam to make the transition to an independent school. The syllabus for the 11+ CE science exam is based on the National Curriculum for KS2 science; one paper for science (one hour) is taken. In addition to the examinable syllabus for the 11+ CE, there is also prep-KS3 science material for the pupil to cover; this prep-KS3 science material is not examinable, but is required as preparation for KS3 science study in senior school if admitted.\n\nThe National Curriculum for KS3–4 science differs from KS1–2 not just in its complexity, but unlike the latter, the science curriculum is divided into three explicit parts: biology, chemistry and physics. Typically in a state secondary school there can be anything from one to up to three (or even more) teachers delivering science to a single class (depending on the breadth of knowledge of the teacher and staff resources of the school). Broadly speaking, similar areas are covered at both stages (that is KS3 and KS4), but at a more advanced level in KS4. Below is a broad (and simplified) summary of the curriculum of each part at KS3/4 level.\n\nDefined in the National Curriculum as:\nThe content for KS3/4 biology in the National Curriculum is broadly:\n\nDefined in the National Curriculum as:\nThe content for KS3/4 chemistry in the National Curriculum is broadly:\n\nDefined in the National Curriculum as:\nThe content for KS3/4 physics in the National Curriculum is broadly:\n\nKey stage 3 (KS3) covers years 7, 8 and 9 of compulsory school education in the National Curriculum. Pupils are typically in the age range 11–14.\n\nBetween the early 1990s and late 2000s (‘late noughties’) state school pupils had to take statutory SAT exams at the end of KS3 science (just like KS2), although teacher assessments were also allowed. The KS3 SAT science exam consisted of two papers (one hour each). The scores from both papers were combined to give a final score. This score would then be converted into a numerical level, which would in turn be converted into an expectation level. The conversion scale for the levels at KS3 SAT is shown below.\nThe conversion of the raw score from the two papers to a numerical level depended on the ‘tier’ taken by the student. For science KS3 SATs, two tiers were available: lower tier and higher tier. Levels 3–6 were available at the lower tier while levels 5–7 were available at the higher tier. The conversion scale for each tier’s scores are shown below.\n\n\"Science KS3 SATs: lower tier\"\n\"Science KS3 SATs: higher tier\"\nLevel 8 (exceptional) was not available to science KS3 SATs (not even at the higher tier); it was available to mathematics, but only at the highest tier (levels 6–8) out of four tiers that were available to mathematics KS3 SATs. Science KS3 SATs were discontinued in 2010 and replaced by teacher assessments (just like science KS2 SATs). Despite the discontinuation of statutory science KS3 SATs, the past papers are still used by schools today.\n\nLike the 11+ CEs, the 13+ CEs are taken by pupils wishing to be admitted to independent senior schools; in this case for KS4 study and higher. The exam is typically taken while the pupil is in prep school (some senior schools only admit from the age of 13). Some state school pupils use the exam to make the transition to an independent school. The syllabus for the 13+ CE science exam(s) is based on the National Curriculum for KS3 science, although not all of the KS3 science content is examinable in the CE, but the parts left out are recommended for teaching in year 9. For the exam the candidate can take either the simpler one paper in science (one hour) comprising biology, chemistry and physics parts, or three higher (and harder) papers (forty minutes each)—one in biology, one in chemistry, and one in physics. In addiion, individual senior schools may have exams for entry into other years; for example, 14+, 16+ (for post-16 or ‘KS5’ study); details of which they give on their websites.\n\nKey stage 4 (KS4) covers years 10 and 11 of compulsory school education, but it may start earlier for science (and mathematics) in some schools. Pupils are typically in the age range 14–16. At the end of KS4, students have to take statutory GCSE exams, which can be taken at either foundation tier or higher tier. Science GCSEs can be complicated in that they offer a vast array of ‘routes’ although this has simplified somewhat following recent changes to GCSEs. Today science GCSE can be taken either as a combined single subject (which is worth two GCSEs) or as the three separate subjects of physics, chemistry and biology, (each worth a single GCSE in its own right). When biology, chemistry and physics are taken as separate GCSE subjects the tiers can be mixed. So for instance, a student could take say, biology at higher tier, but chemistry at foundation tier. By contrast, tiers cannot be mixed in combined science (that is, all constituent parts must be taken at the same tier). Experiments (also called \"practicals\") are compulsory in the GCSE science course, but in different ways across the boards offering GCSE science to English schools. For most boards the results of the practicals do not count towards the final grade in the reformed GCSE (as this is determined entirely by the results of the written examination), but the school/college must submit a signed \"practical science statement\" to the board under which the science is being studied BEFORE the students can take the examination. The statement must declare that all students have completed all the required practicals. The skills and knowledge that should have been acquired from the practicals are subsequently assessed in the GCSE exams, which for most boards are entirely written (as alluded to earlier). For one board (CCEA) however, in addition to the examination of practical skills in the written papers, the results of some of the actual practicals do count towards the final grade in the reformed GCSE. Currently, GCSE sciences in England are available from five boards: AQA, OCR, Edexcel. WJEC-Eduqas and CCEA. Although all five boards provide GCSE science to English schools, not all of these boards are based in England: AQA, OCR and Edexcel are based in England, but WJEC-Eduqas is based in Wales, while CCEA is based in Northern Ireland. Schools are free to choose any board for their science, and where the three sciences of chemistry, physics and biology are being taken independently at GCSE level, all three sciences need not be taken from the same board. Some boards offer multiple routes for their combined science courses in the reformed GCSE in England.\n\nFollowing recent changes, a student can go for one of two routes if taking AQA combined science: \"trilogy\" or \"synergy\". In trilogy, science is delivered in the three traditional parts of biology, chemistry and physics. The trilogy specification document outlines topics for each science part and practicals are specified. The trilogy GCSE exam itself is made up of six papers (each one hour and fifteen minutes): two for biology, two for chemistry, and two for physics. In synergy, science is delivered in two parts: \"life and environmental sciences\" AND \"physical sciences\". Unlike trilogy, each of the two parts in the synergy specification document is broken down into ‘areas’ that enable biology, chemistry and physics to sit together. The synergy GCSE exam itself is made up of four papers (each one hour and forty-five minutes): two for life and environmental sciences and two for physical sciences.\n\nLike AQA combined science, following recent changes, a student can go for one of two routes if taking OCR combined science; in this case either \"combined science A\" or \"combined science B\". In combined science A, science is delivered in the three traditional parts of biology, chemistry and physics. Like AQA's trilogy, each science part is broken into topics in combined science A's specification document , but unlike AQA combined science, practicals are suggested rather than specified, although practicals are still compulsory (the same goes for combined science B). The GCSE combined science A exam is made up of six papers (each one hour and ten minutes): two each for biology, chemistry and physics respectively. In combined science B, the science curriculum is delivered in four parts: biology, chemistry, physics and combined science. Each part is broken into topics in the combined science B specification document . The exam itself is made up of four papers (each one hour and forty-five minutes): one each for biology, chemistry, physics and combined science respectively.\n\nFollowing the changes to GCSEs, only one route is available to the student that takes Edexcel or Eduqas combined science. In Edexcel's combined science specification document the curriculum is delivered in the three traditional disciplines of biology, chemistry and physics, but in Eduqas's , the science curriculum is divided into four parts: \"Concepts in Biology\", \" Concepts in Chemistry\", \"Concepts in Physics\" and \"Applications in Science\". The Eduqas combined science exam is made up of four papers (one hour and forty-five minutes each): one for each of the three 'Concepts in ...' and one for 'Applications in Science'. The Edexcel exam is made up of six papers (each one hour and ten minutes): two each for biology, chemistry and physics respectively.\n\nThe new combined science from CCEA since the GCSE reforms retains the same name as its predecessor. The specification document presents the science curriculum in the traditional disciplines of biology, chemistry and physics. The exam is the most extensive of the GCSE science boards; made up of nine papers and three practical exams. For each of biology, chemistry and physics there are three papers and one practical exam: Paper 1 is one hour long, Paper 2 is one hour and fifteen minutes, Paper 3 is a practical skills paper and is thirty minutes long, and the practical exam is one hour long.\n\nAs alluded to earlier, in the mid-2010s, the GCSE science courses of the GCSE exam boards underwent significant changes. This was in part due to changes in the National Curriculum, of which one of the areas affected the most was key stage 4 (KS4). The revised version of the National Curriculum covered more content; the one for KS4 science was published in December 2014 and a version specifically for GCSE combined science was published in June 2015, and implemented in September 2016. The increased content triggered a change in the GCSE grading system from A*–G to 9–1. Much more detail on the new grading system and how it differs from the previous can be read here. One consequence of the increased science content in the National Curriculum was that it helped simplify a bewildering array of GCSE science courses particularly from AQA, which are/were designed to accommodate students from the least able to the most able. AQA science courses such as core science, additional science, further additional science, science A, science B, additional applied science illustrate the variety. The new trilogy and synergy courses (which were developed from the recently expanded National Curriculum for science) have removed the need for the most able students to take multiple science courses unless the student decides to take chemistry, biology and physics individually. The content for GCSE physics as a stand-alone subject is more than the content for physics in GCSE combined science. For instance, in the National Curriculum for KS4 science, space physics is included, but not in the GCSE combined science version. AQA includes space physics in its GCSE specification, but only when GCSE physics is taken as an independent subject in its own right, and not when physics is taken as part of GCSE combined science.\n\nFor the ages of 16, 17 and 18 (and older for those that remain in education below university-level), students in England do what is sometimes loosely called ‘key stage 5’ or KS5; it has no legal meaning (unlike the other key stages). And unlike KS1–4 in which the levels of complexity of topics learnt at each stage are prescribed within relatively narrow limits, at KS5, the levels of complexity of topics cover a wide range, although the highest level of complexity at KS5 is RQF level 3. Whether or not a student actually studies at this level of complexity in KS5 depends on his/her GCSE results—crucially on what subjects the student obtained passes at RQF level 2 standard (including mathematics and English) as well as the actual grades themselves. In other words, unlike KS1–4, where a specific student studies at one RQF level, at KS5, a specific student may be studying at several RQF levels depending on what s/he obtained at GCSEs. Regardless of the RQF-level mix, a KS5 student can do his/her post-16 study in one of the following:\n\nThis can be done either full-time or part-time. If done part-time, the student also has to be working or volunteering for at least 20 hours a week. As already hinted, the science curriculum and education at KS5 is highly varied, often disparate and tends to be specialised as students in their late teens interested in science begin to study subjects that will prepare them for science careers. In KS5 study at RQF level 3, students are introduced to concepts they would never have heard of during their time from KS1 to KS4, which they will either study in much greater depth at university-level (if s/he continues to study the science in question) or apply at vocational placements or apprenticeships. Practical science at KS5–RQF level 3 can be more extensive. Individual A levels in chemistry, biology and physics are perhaps the best known KS5–RQF level 3 science subjects (and they take two years to complete when done full-time), but A level students may well choose only one or two of these subjects, and mix with mathematics or non-science A level subjects depending on what university degree the student wishes to study post-KS5 (typically A level students go straight to university on successful completion of A levels). Although A levels are probably the highest profile KS5 studies, there are other qualifications students can take as alternatives. KS5 science subjects (including laboratory science) can also be taken in BTECs, Cambridge Pre-Us, IBs, AQAs (non-A levels), OCRs (non-A levels). NVQs, university specific foundation year programmes (generally offered to students that have taken A levels, but not the correct ones; can also be offered to those that have failed their A levels), access to HEs (generally not available to students under 21). Although all these alternative non-A level qualifications (which are all available at RQF level 3) can offer content similar in complexity to their A/AS level counterparts (which are also RQF level 3), the make-up of their content can vary significantly depending on the subject, and the board offering it. A comprehensive list of most subjects at most levels and the boards offering them is kept by the National Careers Service and individual subjects and their boards can be searched for on their website . A search tool for only Ofqual approved list of subjects and their boards can be found at \"Ofqual: The Register\"; the list can also be downloaded from the site, while a search tool for only QAA approved access to HE subjects can be found at \"Access to Higher Education\". Both the National Careers Service and Ofqual lists include all A/AS levels, GCSEs (RQF levels 1–2) and most of the rest (RQF levels 1–8, and the RQF entry level (which is below RQF level 1)). With regards to universities in England accepting RQF level 3 science subjects for their science degrees, students with only non-A level science subjects may be accepted, or the student may require a mixture of some of these non-A level science subjects with one or two A/AS level science subjects. This all depends on the level 3 qualification in question, the university, and science degree the student wishes to study. Individual universities give details of their entry requirements for their various science (and obviously all) degrees on their websites. Some RQF level 3 students may use the KS5 science subjects they study for entry into higher/degree apprenticeships or university-level vocational training.\n\nBeyond 18 years of age, students that have already either left or finished their formal education, but return at later times in their lives to study science (having decided they do not have the appropriate level of knowledge), can do so on their return at RQF level 3 or lower. The level the student returns at will depend on his/her pre-enrollment level of knowledge of science, although science is generally not available below RQF level 1 (that is, the RQF entry (sub-1) level) to adult returners to education (but maths and English are). Typically, further education colleges admit adult returners, although some universities may offer distance learning courses. Further education and distance learning courses are often the ways these mature students can access science courses long after they have left education. Just like students that have neither left nor previously finished their education, satisfactorily passing the summative assessment at RQF level 3 is the crucial gateway into university-level education (that is RQF level 4 and higher) in England. In addition to satisfactory passes in science subjects at RQF level 3, the learner also has to have passed mathematics and English at RQF level 2 standard (typically GCSEs or equivalent with minimum (or equivalent minimum) grades of 'C' or '4'); providers of university-level education give details on their websites.\n\nLike post-16 or KS5, this is also highly varied, disparate and specialised, but more so, as a student may chose to study 'one' science, which s/he will subsequently study in depth for three or more years; the summative assessment leads to a degree (of which for science in England today is typically one of two levels—RQF level 6 or 7). Such education will enable students market themselves as (specialist) scientists to employers or postgraduate science degree programmes (although the choices available to the graduate are affected by the class of degree the graduate achieves—recruiters give details on their websites). Many concepts the student first encountered in A levels / RQF level 3 are dealt with in much greater detail. The biggest difference between A level / RQF level 3 science and university-level science occurs in physics, which at university-level becomes highly mathematical (and at times difficult to distinguish from mathematics). Practical science at university-level can be quite extensive and by the time of the dissertation project, the student may well be doing complex experiments lasting weeks or months unsupervised (although s/he will still have a supervisor on hand). Science degrees in England are offered by both universities and some further education colleges. University-level teachers (also referred to in England as \"lecturers\") will teach one area of the science the student is studying, but two notable differences between university-level science education in further education colleges and universities are that in universities, there is a close connection between teaching and research. In other words, it is common for a university teacher to be a researcher in the area s/he teaches—this applies not just to science, but to all areas; such connection between teaching and research does not occur in further education colleges in England. And the other difference is that further education colleges must have their degrees approved by universities. Although universities do not need approval for their science degrees and are free to set their own content, they generally get many of their science courses accredited by professional bodies. So for example, universities offering biology degrees commonly get these programmes accredited by the Royal Society of Biology; for chemistry degrees, it is the Royal Society of Chemistry; for physics degrees, it is the Institute of Physics; for geology degrees, it is the Geological Society, and so on. Accreditation of a science degree by a professional body is a precondition if the student studying the degree is to become a member of the body following graduation, and subsequently acquire chartered status. In addition, UK universities are obliged to ensure that their degrees meet the standards agreed to in the Bologna Process to which the UK is a co-signatory. The QAA certifies those British degrees that meet those standards. Not all university-level students studying science study for science degrees; many will study science as part of a vocational degree such as pharmacy, medicine, dentistry, nursing, veterinary medicine, allied health professions, and so on. And some will study science as part of a higher/degree apprenticeship.\n\nThe challenges of establishing a national curriculum for science below university-level in England over the last two centuries have been explored by Smith (2010) and others. In Smith's paper, she highlighted two potentially conflicting roles for science education below university-level: educating a public to be scientifically literate, and providing scientific training for aspiring science professionals. Smith further pointed out in her paper that even among the training of aspiring science professionals, three groups could be identified: those that sought science in pursuance of the truth and an abstract understanding of science; those that sought science for actual benefit to society—the applied scientists, and then the failures. The dilemma did not escape the committee led by J J Thomson (discoverer of the electron) in 1918, which is quite telling of the tension in trying to accommodate several very different groups of science learners:\nSuch tension has never really dissipated. In a report by the Royal Society from 2008, they state several challenges facing science education; the first two are reproduced here:\n\nThe first:\nand the second: \nA lack of good quality teachers has also been cited as a challenge. Difficulty recruiting science teachers, which is a current problem in England (and the UK as a whole) is certainly not new as the following extract from the report by the Thomson Committee in 1918 shows:\nSome interesting figures were quoted in the 1918 report; for instance on page 31 of the report: out of 72 schools that had 200–400 girls of all ages, only 39 had the services of two science teachers (mistresses). The report went on state that these figures had contributed to long hours and inadequate salaries. This sounds strikingly similar to the situation facing science (and indeed all) school teachers in England today; a hundred years later. Another challenge was that there was not an appreciation by the political elite on the value of a science education to the wider public; despite the fact that England was producing some of the greatest scientists in the world. Yet another challenge was that public schools were slow to respond to the needs of developing a science curriculum. For example, William Sharp was the first science teacher for Rugby School, a prestigious public school in England, which only happened for the first time in 1847; nearly 300 years after the college was established and more than 100 years after England had lost one of the world’s greatest scientists—Isaac Newton. Despite these challenges, a science curriculum and education developed through the twentieth century and eventually became a compulsory part of the new National Curriculum in 1988 (phased in from 1989 to 1992). Even at the time of the deliberations in the mid-1980s prior to the creation of the National Curriculum, there was disagreement over how much time science should occupy in the curriculum. There was pressure for science to be made to occupy 20% of curriculum time for 14–16-year-olds, but not everyone agreed with this; certainly not the then Secretary of State for Education and Science Kenneth Baker. The then Department for Education and Science settled for 12.5% of curriculum time, but schools were free to increase this. The result was the emergence of single science (which occupied 10% of curriculum time and was the minimum requirement—also called \"core\" science), double science (which occupied 20% of curriculum time, and was so called because it involved studying core science and \"additional\" science), and there was the option of doing the sciences of physics, chemistry and biology separately (also known as 'triple' science). Following the changes to the National Curriculum in the 2010s, single science has effectively been removed, and the two components of double science have been combined to form 'combined science', which is now the minimum requirement. One challenge that ties in with England's shortage of science teachers is the number of science undergraduates in higher education, which provides the pool for future trainee science teachers, but undergraduate numbers affect the three sciences differently: the number of students that study physical sciences in higher education (93050 in the year 2012/13) are less than half the students that study biological sciences (201520 in the year 2012/13). This has had a direct impact on government policy in England; for example, the UK government offers bursaries of £30000 to graduates with first class honours degrees wishing to train as physics teachers in secondary schools in England; for chemistry, the top bursary is £25000, and for biology it is £15000. For students with lower honours degrees in these subjects, correspondingly lower bursaries are offered, but they are still considerable for physics graduates (compared to bursaries offered to trainee teachers of other subjects). For instance, a physics graduate with a lower second class honours degree can still attract a bursary of £25000. But the government has also implemented a policy to increase the number of science graduates from UK universities: normally a student in England wishing to study for a first degree including an honours degree can get a UK-government-backed student loan as long as s/he does not already possess an honours degree. Exceptions are permitted, but prior to September 2017 (and in the case of postgraduate master's degrees, September 2016), these UK-government-backed loans for those in England that already had honours degrees were only available for them if the courses they were going to study led to professional qualifications such as medicine, dentistry, social care, architecture or teaching. However the range of subjects for which a student in England already in possession of an honours degree could get a second UK-government-backed student loan to study a second honours degree was expanded to include science subjects (as well as technology, engineering and mathematics), which took effect from 1 September 2017. Like before, the student has to meet both England and UK residency requirements . The inclusion of science, technology, engineering and mathematics (collectively called \"STEM\" subjects) to the list appears to have been triggered not just by teacher shortages in those subjects, but also by a general skills shortage (in those subjects) UK-wide. It remains to be seen whether the direct interventions by the UK government help alleviate the general skills shortages in STEM subjects, as well as the challenges of delivering a science curriculum and education in the long-term. As for universities, several challenges have been identified by Grove (2015); the summaries of those challenges have been reproduced below:\n\nThese challenges apply not just to the university provision of science education, but to all areas of university education.\n\n"}
{"id": "51015112", "url": "https://en.wikipedia.org/wiki?curid=51015112", "title": "Scientific dissent", "text": "Scientific dissent\n\nScientific dissent is dissent from scientific consensus. Disagreements can be useful for finding problems in underlying assumptions, methodologies, and reasoning, as well as for generating and testing new ways of tackling the unknown. In modern times, with the increased role of science on the society and the politicization of science, a new aspect gained prominence: effects of scientific dissent on public policies.\n\nScientific dissent is distinct from denialism, which is a deliberate rejection of scientific consensus usually for commercial or ideological reasons.\n\nMiriam Solomon in her book \"Social Empiricism\" argues that scientific dissent is the normal state of scientific inquiry, rather than a conflict situation that needs resolution. She argues that disagreements of individual scientists about the proper direction of research are not cause for concern, because scientific rationality must be assessed at the level of the scientific community. As long as all theories being pursued yield some unique empirical successes, Solomon argues that their pursuit is worthwhile and even consistent with the common view that science aims at truth. In Solomon's view, competing scientific theories can even be inconsistent with one another while each containing some degree of truth. Empirical evidence may not be sufficient to distinguish between competing theories, and successful theories often have core assumptions that are incorrect.\n\nA number of famous scientists have been sceptical of what were, or came to be, mainstream scientific positions. For example, Ernst Mach famously declared in 1897: \"I don't believe that atoms exist!\" Wilhelm Ostwald expressed a similar scepticism about atoms, but changed his mind in 1908.\n\nIn the early 20th century, peptic ulcers were believed to be caused by stress and dietary factors. The physicians Robin Warren and Barry Marshall showed in 1982 that the bacterium \"Helicobacter pylori\" was responsible, but the medical community was slow to make appropriate changes in ulcer treatment.\n\nScientific debate is a healthy and necessary part of science, but scientific debate may collide with power dynamics within the academic world. Suppression of legitimate scientific debate can be considered as a breach of academic integrity. Examples of suppression include journal editors rejecting a paper for political reasons prior to peer-review, refusing access to data for research which might draw negative conclusions about the safety of some commercial product, and putting pressure on a university to fire a dissenting researcher.\n\nIn modern times proponents of science denialism, pseudoscience, and conspiracy theories often try to disguise their viewpoints as \"scientific dissent\" to take an advantage of the benefit of doubt. Such cases are typically recognized by lack of crucial elements of scientific approach: insufficient evidence base, lack of rigor and control, etc.\n\nLack of discussion of claims coming from fringe science may be presented as suppression by mainstream science. This was described as \"manufacturing dissent\" and discussed in the context of neo-creationism.\n\nDavid Harker in the introduction to his book \"Creating Scientific Controversies\", summarizes the history how tobacco industry worked towards manufacturing a controversy regarding the health effects of tobacco.\n\nIn what is sometimes known as the \"Galileo gambit,\" pseudoscientists will sometimes compare themselves to Galileo, arguing that opposition from established scientists is actually a point in favour of their ideas. Jean Paul Van Bendegem writes that \"No doubt the most famous example of mistaken analogy is the abuse of Galileo Galilei's case resulting in his conviction by the Holy Inquisition. The basic strategy consists of equating Galileo with the poor astrologer or parapsychologist and equating the Inquisition with the scientific establishment.\"\n\nViews which disagree with scientific consensus may have an adverse effect on the perception of science by general public and affect decision making in various policies. When prominently promoted without due proportion, dissenting views can create an impression of uncertainty to laypeople. Common examples of such situation include global warming controversy and issues of public health and genetically modified organisms. Therefore, scientists treat scientific dissent as problematic when it may have a significant impact on public and policy-making, and try to mitigate it.\n\nInmaculada de Melo-Martín and Kristen Intemann criticize three major strategies in battling allegedly dangerous scientific dissent: masking the dissent, silencing the dissent, and discrediting the dissenters. Melo-Martin and Intermann argue that these strategies come from a misdiagnosis: the real problem is not dissent, but public scientific illiteracy. Rather than focusing on dissent, scientists must concentrate on educating the general public, so that people could make educated opinions and recognize false claims and invalid arguments. They further argue that silencing dissent rather than promoting literacy incurs the risk of undermining the public trust in science.\n\nSheila Jasanoff, in the context of climate change, mentions a common argument that public opinion is poorly informed because petroleum industry manufactures uncertainties and the media exaggerate the dissent, but argues that it is insufficient for the understanding of the problem. She writes that studies of scientific controversies show that credibility of science depend not only on strong scientific consensus, but also on the persuasive power of those who speak for science, especially in the situations of controversy.\n\n\n"}
{"id": "244629", "url": "https://en.wikipedia.org/wiki?curid=244629", "title": "Scientific law", "text": "Scientific law\n\nThe laws of science, also called scientific laws or scientific principles, are statements that describe or predict a range of natural phenomena. Each scientific law is a statement based on repeated experimental observations that describes some aspect of the Universe. The term \"law\" has diverse usage in many cases (approximate, accurate, broad, or narrow theories) across all fields of natural science (physics, chemistry, biology, geology, astronomy, etc.). Scientific laws summarize and explain a large collection of facts determined by experiment, and are tested based on their ability to predict the results of future experiments. They are developed either from facts or through mathematics, and are strongly supported by empirical evidence. It is generally understood that they reflect causal relationships fundamental to reality, and are discovered rather than invented.\n\nLaws reflect scientific knowledge that experiments have repeatedly verified (and never falsified). Their accuracy does not change when new theories are worked out, but rather the scope of application, since the equation (if any) representing the law does not change. As with other scientific knowledge, they do not have absolute certainty (as mathematical theorems or identities do), and it is always possible for a law to be overturned by future observations. A law can usually be formulated as one or several statements or equations, so that it can be used to predict the outcome of an experiment, given the circumstances of the processes taking place.\n\nLaws differ from hypotheses and postulates, which are proposed during the scientific process before and during validation by experiment and observation. Hypotheses and postulates are not laws since they have not been verified to the same degree and may not be sufficiently general, although they may lead to the formulation of laws. A law is a more solidified and formal statement, distilled from repeated experiment. Laws are narrower in scope than scientific theories, which may contain one or several laws. Science distinguishes a law or theory from facts. Calling a law a fact is ambiguous, an overstatement, or an equivocation. Although the nature of a scientific law is a question in philosophy and although scientific laws describe nature mathematically, scientific laws are practical conclusions reached by the scientific method; they are intended to be neither laden with ontological commitments nor statements of logical absolutes.\n\nAccording to the unity of science thesis, \"all\" scientific laws follow fundamentally from physics. Laws which occur in other sciences ultimately follow from physical laws. Often, from mathematically fundamental viewpoints, universal constants emerge from a scientific law.\n\nA scientific law always applies under the same conditions, and implies that there is a causal relationship involving its elements. Factual and well-confirmed statements like \"Mercury is liquid at standard temperature and pressure\" are considered too specific to qualify as scientific laws. A central problem in the philosophy of science, going back to David Hume, is that of distinguishing causal relationships (such as those implied by laws) from principles that arise due to constant conjunction.\n\nLaws differ from scientific theories in that they do not posit a mechanism or explanation of phenomena: they are merely distillations of the results of repeated observation. As such, a law is limited in applicability to circumstances resembling those already observed, and may be found false when extrapolated. Ohm's law only applies to linear networks, Newton's law of universal gravitation only applies in weak gravitational fields, the early laws of aerodynamics such as Bernoulli's principle do not apply in case of compressible flow such as occurs in transonic and supersonic flight, Hooke's law only applies to strain below the elastic limit, etc. These laws remain useful, but only under the conditions where they apply.\n\nMany laws take mathematical forms, and thus can be stated as an equation; for example, the law of conservation of energy can be written as formula_1, where E is the total amount of energy in the universe. Similarly, the first law of thermodynamics can be written as formula_2.\n\nThe term \"scientific law\" is traditionally associated with the natural sciences, though the social sciences also contain laws. An example of a scientific law in social sciences is Zipf's law.\n\nLike theories and hypotheses, laws make predictions (specifically, they predict that new observations will conform to the law), and can be falsified if they are found in contradiction with new data.\n\nMost significant laws in science are conservation laws. These fundamental laws follow from homogeneity of space, time and phase, in other words \"symmetry\".\n\n\nConservation laws can be expressed using the general continuity equation (for a conserved quantity) can be written in differential form as:\n\nwhere ρ is some quantity per unit volume, J is the flux of that quantity (change in quantity per unit time per unit area). Intuitively, the divergence (denoted ∇•) of a vector field is a measure of flux diverging radially outwards from a point, so the negative is the amount piling up at a point, hence the rate of change of density in a region of space must be the amount of flux leaving or collecting in some region (see main article for details). In the table below, the fluxes, flows for various physical quantities in transport, and their associated continuity equations, are collected for comparison.\n\nMore general equations are the convection–diffusion equation and Boltzmann transport equation, which have their roots in the continuity equation.\n\nAll of classical mechanics, including Newton's laws, Lagrange's equations, Hamilton's equations, etc., can be derived from this very simple principle:\n\nwhere formula_5 is the action; the integral of the Lagrangian\n\nof the physical system between two times \"t\" and \"t\". The kinetic energy of the system is \"T\" (a function of the rate of change of the configuration of the system), and potential energy is \"V\" (a function of the configuration and its rate of change). The configuration of a system which has \"N\" degrees of freedom is defined by generalized coordinates q = (\"q\", \"q\", ... \"q\").\n\nThere are generalized momenta conjugate to these coordinates, p = (\"p\", \"p\", ..., \"p\"), where:\n\nThe action and Lagrangian both contain the dynamics of the system for all times. The term \"path\" simply refers to a curve traced out by the system in terms of the generalized coordinates in the configuration space, i.e. the curve q(\"t\"), parameterized by time (see also parametric equation for this concept).\n\nThe action is a \"functional\" rather than a \"function\", since it depends on the Lagrangian, and the Lagrangian depends on the path q(\"t\"), so the action depends on the \"entire\" \"shape\" of the path for all times (in the time interval from \"t\" to \"t\"). Between two instants of time, there are infinitely many paths, but one for which the action is stationary (to the first order) is the true path. The stationary value for the \"entire continuum\" of Lagrangian values corresponding to some path, \"not just one value\" of the Lagrangian, is required (in other words it is \"not\" as simple as \"differentiating a function and setting it to zero, then solving the equations to find the points of maxima and minima etc\", rather this idea is applied to the entire \"shape\" of the function, see calculus of variations for more details on this procedure).\n\nNotice \"L\" is \"not\" the total energy \"E\" of the system due to the difference, rather than the sum:\n\nThe following general approaches to classical mechanics are summarized below in the order of establishment. They are equivalent formulations, Newton's is very commonly used due to simplicity, but Hamilton's and Lagrange's equations are more general, and their range can extend into other branches of physics with suitable modifications.\n\nFrom the above, any equation of motion in classical mechanics can be derived.\n\n\n\n\nEquations describing fluid flow in various situations can be derived, using the above classical equations of motion and often conservation of mass, energy and momentum. Some elementary examples follow.\n\n\n\nPostulates of special relativity are not \"laws\" in themselves, but assumptions of their nature in terms of \"relative motion\".\n\nOften two are stated as \"the laws of physics are the same in all inertial frames\" and \"the speed of light is constant\". However the second is redundant, since the speed of light is predicted by Maxwell's equations. Essentially there is only one.\n\nThe said posulate leads to the Lorentz transformations – the transformation law between two frame of references moving relative to each other. For any 4-vector\n\nthis replaces the Galilean transformation law from classical mechanics. The Lorentz transformations reduce to the Galilean transformations for low velocities much less than the speed of light \"c\".\n\nThe magnitudes of 4-vectors are invariants - \"not\" \"conserved\", but the same for all inertial frames (i.e. every observer in an inertial frame will agree on the same value), in particular if \"A\" is the four-momentum, the magnitude can derive the famous invariant equation for mass-energy and momentum conservation (see invariant mass):\n\nin which the (more famous) mass-energy equivalence \"E\" = \"mc\" is a special case.\n\n\nGeneral relativity is governed by the Einstein field equations, which describe the curvature of space-time due to mass-energy equivalent to the gravitational field. Solving the equation for the geometry of space warped due to the mass distribution gives the metric tensor. Using the geodesic equation, the motion of masses falling along the geodesics can be calculated.\n\n\nIn a relatively flat spacetime due to weak gravitational fields, gravitational analogues of Maxwell's equations can be found; the GEM equations, to describe an analogous \"gravitomagnetic field\". They are well established by the theory, and experimental tests form ongoing research.\n\nThese equations can be modified to include magnetic monopoles, and are consistent with our observations of monopoles either existing or not existing; if they do not exist, the generalized equations reduce to the ones above, if they do, the equations become fully symmetric in electric and magnetic charges and currents. Indeed, there is a duality transformation where electric and magnetic charges can be \"rotated into one another\", and still satisfy Maxwell's equations.\n\n\nThese laws were found before the formulation of Maxwell's equations. They are not fundamental, since they can be derived from Maxwell's Equations. Coulomb's Law can be found from Gauss' Law (electrostatic form) and the Biot–Savart Law can be deduced from Ampere's Law (magnetostatic form). Lenz' Law and Faraday's Law can be incorporated into the Maxwell-Faraday equation. Nonetheless they are still very effective for simple calculations.\n\n\n\n\nClassically, optics is based on a variational principle: light travels from one point in space to another in the shortest time.\n\n\nIn geometric optics laws are based on approximations in Euclidean geometry (such as the paraxial approximation).\n\n\nIn physical optics, laws are based on physical properties of materials.\n\n\nIn actuality, optical properties of matter are significantly more complex and require quantum mechanics.\n\nQuantum mechanics has its roots in postulates. This leads to results which are not usually called \"laws\", but hold the same status, in that all of quantum mechanics follows from them.\n\nOne postulate that a particle (or a system of many particles) is described by a wavefunction, and this satisfies a quantum wave equation: namely the Schrödinger equation (which can be written as a non-relativistic wave equation, or a relativistic wave equation). Solving this wave equation predicts the time-evolution of the system's behaviour, analogous to solving Newton's laws in classical mechanics.\n\nOther postulates change the idea of physical observables; using quantum operators; some measurements can't be made at the same instant of time (Uncertainty principles), particles are fundamentally indistinguishable. Another postulate; the wavefunction collapse postulate, counters the usual idea of a measurement in science.\n\nApplying electromagnetism, thermodynamics, and quantum mechanics, to atoms and molecules, some laws of electromagnetic radiation and light are as follows.\n\n\nChemical laws are those laws of nature relevant to chemistry. Historically, observations led to many empirical laws, though now it is known that chemistry has its foundations in quantum mechanics.\n\n\nThe most fundamental concept in chemistry is the law of conservation of mass, which states that there is no detectable change in the quantity of matter during an ordinary chemical reaction. Modern physics shows that it is actually energy that is conserved, and that energy and mass are related; a concept which becomes important in nuclear chemistry. Conservation of energy leads to the important concepts of equilibrium, thermodynamics, and kinetics.\n\nAdditional laws of chemistry elaborate on the law of conservation of mass. Joseph Proust's law of definite composition says that pure chemicals are composed of elements in a definite formulation; we now know that the structural arrangement of these elements is also important.\n\nDalton's law of multiple proportions says that these chemicals will present themselves in proportions that are small whole numbers (i.e. 1:2 for Oxygen:Hydrogen ratio in water); although in many systems (notably biomacromolecules and minerals) the ratios tend to require large numbers, and are frequently represented as a fraction.\n\nMore modern laws of chemistry define the relationship between energy and its transformations.\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "28057675", "url": "https://en.wikipedia.org/wiki?curid=28057675", "title": "Skolkovo Innovation Center", "text": "Skolkovo Innovation Center\n\nThe Skolkovo Innovation Center is a high technology business area that is being built at Mozhaysky District in Moscow, Russia. Although historically Russia has been successful with development of science and technology, its lack of entrepreneur spirit led to government intervention of patents and nonproliferation of Russian tech companies beyond the scope of regional service. As corporations and individuals become \"residents\" of the city, with proposed projects and ideas receiving financial assistance. \nSkolkovo was first announced on 12 November 2009 by then Russian President Dmitry Medvedev. The complex is headed by Viktor Vekselberg and co-chaired by former Intel CEO Craig Barrett.\n\nIn March 2010, Vekselberg announced the necessity of developing a special legal order in Skolkovo and emphasized the need to offer a tax holiday lasting 5–7 years.\n\nIn April 2010, Russian Prime Minister Dmitry Medvedev charged the government with working out specific legal, administrative, tax and customs regulations on Skolkovo.\n\nIn May 2010, Dmitry Medvedev introduced two bills regulating working conditions in Skolkovo. The bills were adopted by the State Duma in September of that year and, on 28 September 2010, the President of the Russian Federation signed the bills into federal law.\n\nIn August 2010, Dmitry Medvedev introduced a bill easing migratory policies in regards to Skolkovo.\n\nOn 20 August 2010, a new government decree regulating visas for participants of the Skolkovo project was published. According to this decree, specialized and highly skilled foreign nationals who arrive in Russia with the purpose of securing employment at Skolkovo will be granted a visa for a term of up to 30 days. In the event of successful job placement they can then obtain a work visa for a term of 3 years.\n\nA new highway was opened connecting Skolkovo to the MKAD in June 2010. Railway transport will be available via Belorussky Rail Terminal and Kiyevsky Rail Terminal. A link to Vnukovo International Airport is also planned.\n\nThe innovation center will be financed primarily from the Russian federal budget. The center's 2010 budget was 3.9 Billion RUB. An additional 22 Billion RUB is planned for 2012 and 17.3 Billion RUB in 2013.\n\nSkolkovo includes five \"clusters\" specializing in different areas. These include IT, Energy, Nuclear Technologies, Biomedicine and Space Technologies.\n\nThe IT cluster is tasked with creating an effective model for successful commercialization of IT technologies in Russia. Over 450 companies have signed up for the IT cluster.IT ecosystem includes over 50 fast growing cyber-security startup companies with more than 700 employees in total\n\nThe Energy Efficient Technologies cluster aims to introduce breakthrough technologies focused on the reduction of energy consumption by industrial, housing and municipal infrastructure facilities. Today over 80 companies are on board for the energy efficiency cluster.\n\nThe Nuclear Technologies cluster aims to encourage the competitiveness of nuclear power markets and develop breakthrough technologies and products.\n\nThe strategic goal of this cluster is to create an ecosystem for biomedical innovation. In order to achieve this goal, the best practices of leading biotechnology and biomedical research centers were studied. More than 215 companies have signed on for the Biomedical Technologies cluster.\n\nThe Space Technology and Telecommunications cluster is intended to strengthen Russia's position in the respective industries. The scope of activity is wide: from space tourism to satellite navigation systems. Russian companies aim to increase their market share in this global market, the total volume of which is estimated at $300 billion.\n\nThere are examples of cooperation between the clusters. For example, in 2012 clusters of Information Technologies and Biomedical Technologies organized joint contest on Mobile Diagnostic Device \"Skolkovo M.D.\" and FRUCT was named the contest winner.\n\nThe main elements of The City are the University and a Technopark. The City will also feature a Congress Center, office buildings, laboratories, fitness centers and stores. The City will measure roughly 400 hectares and have a permanent population of 21,000. Employees, including commuters from Moscow and surrounding regions, will comprise about 31,000 people.\n\nAt least 50% of the energy consumed by the city will come from renewable sources. The well-developed water system uses significantly less water by Russian standards without compromising comfort or hygiene. The transport system prioritizes walking and cycling. The use of vehicles with internal combustion engines is prohibited in the city. Energy passive and active buildings that do not require energy from the outside and even produce more energy than they consume will be built at Skolkovo. Household and municipal waste will be disposed of in the most environmentally friendly way possible – leveraging the use of plasma incinerator technology.\n\nIn July 2012, IBM and five leading Russian innovation companies: the Skolkovo Foundation, Rusnano, Rostelecom, Russian Venture Company and ITFY, all signed a collaboration agreement to foster a culture of applied research and commercialization and attract key talent and investment from around the world in the area of microelectronics.\n\nThe agreement will give the Electronics Technology Center access to IBM’s intellectual property for chip design. IBM will also provide cloud computing technologies to form the basis of a new virtual design environment to be used to develop new microelectronic devices such as sensors to be used in smarter infrastructure projects, industry and consumer electronics.\n\nThe cloud will help unite Russia’s dispersed microelectronics development teams and provide access to advanced technologies and best practice and foster global collaboration. Russian chip designers and fabless design houses will be able to access new semiconductor technologies, including automation tools, design kits, libraries and intellectual property. The center will also provide access to a wide variety of semiconductor production processes offered by many different foundries.\n\nThe agreement was signed by Victor Vekselberg, President of the Skolkovo Foundation; Anatoly Chubais, CEO and Chairman of the Executive Board of Rosnano; Alexander Provotorov, President and Chairman of the Management Board of Rostelecom; Igor Agamirzian, CEO of Russian Venture Company; Evgeny Babayan, Chairman of the Board, ITFY; Leonid Svatkov, CEO ITFY; Bruno Di Leo, Senior Vice President IBM; and Kirill Korniliev, Country General Manager, IBM Russia & CIS.\n\nThe ETC will initially focus on microelectronics design; however in the future it may be extended to other fields where cloud computing can support collaborative development projects.\n\nSkolkovo's Open University (OpUS) isn't an educational institution in the typical sense of the word, because graduating students don't receive a diploma. Instead, OpUS is a source of prospective Masters and PhD candidates, for the Skolkovo University of Science and Technology (SkTech), and interns for Skolkovo partner companies. The educational plan of OpUS includes lecture series, master classes and courses by leading scientists, thinkers and practitioners. Students acquire knowledge in the priority research and development areas of Skolkovo (information technology, biomedicine, energetics, space and nuclear technology). In addition, they have an opportunity to gain knowledge in academic and innovative competencies (foresight, forecasting, thinking, projecting), entrepreneur competence, experience in teamwork on projecting and solving inter-disciplinary problems.\n\nOpUS was opened on 21 April 2011 in Moscow. Selection for Winter 2011-2012 students was carried out in Saint Petersburg and Tomsk. There are currently more than 250 students enrolled in OpUS.\n\nInternational partners include:\n\n\n\n"}
{"id": "3406733", "url": "https://en.wikipedia.org/wiki?curid=3406733", "title": "Social shaping of technology", "text": "Social shaping of technology\n\nAccording to Robin A. Williams and David Edge (1996), \"Central to social shaping of technology (SST) is the concept that there are choices (though not necessarily conscious choices) inherent in both the design of individual artifacts and systems, and in the direction or trajectory of innovation programs.\"\n\nIf technology does not emerge from the unfolding of a predetermined logic or a single determinant, then innovation is a 'garden of forking paths'. Different routes are available, potentially leading to different technological outcomes. Significantly, these choices could have differing implications for society and for particular social groups.\n\nSST is one of the models of the technology: society relationship which emerged in the 1980s with MacKenzie and Wajcman's influential 1985 collection, alongside Pinch and Bijker's social construction of technology framework and Callon and Latour's actor-network theory. These have a common feature of criticism of the linear model of innovation and technological determinism. It differs from these notably in the attention it pays to the influence of the social and technological context of development which shapes innovation choices. SST is concerned to explore the material consequences of different technical choices, but criticizes technological determinism, which argues that technology follows its own developmental path, outside of human influences, and in turn, influences society. In this way, social shaping theorists conceive the relationship between technology and society as one of 'mutual shaping'.\n\nSome versions of this theory state that technology affects society by affordances, constraints, preconditions, and unintended consequences (Baym, 2015). Affordance is the idea that technology makes specific tasks easier in our lives, while constraints make tasks harder to complete. The preconditions of technology are the skills and resources that are vital to using the technology to its fullest potential. Finally, the unintended consequences of technology are unanticipated effects and impact of technology. The cell phone is an example of social shaping of technology (Zulto 2009). The cell phone has evolved over the years to make our lives easier by providing people with handheld computers that can answer calls, answer emails, search for information, and complete numerous other tasks (Zulto, 2009). Yet it has constraints for those that are not technologically savvy, hindering many people in society who do not understand how to utilize these devices. There are preconditions, such as monthly bills and access to electricity. There are also many unintended consequences such as the unintended distraction they cause for many people.\n\nNot only does technology affect society, but according to SST, society affects technology by way of economics, politics, and culture (Baym, 2015). For instance, cell phones have spread in poor countries due to cell phones being more affordable than a computer and internet service (economics), government regulations which have made it fairly easy for cell phone providers to build networks (politics), and the small size of cell phones which fit easily into many cultures’ need for mobile communication (culture).\n\nDonald A. MacKenzie, Judy Wajcman, Bruno Latour, Wiebe Bijker, Thomas P. Hughes, John Law, Trevor Pinch (also Trevor J. Pinch), Michel Callon, Steve Woolgar, Carl May, Thomas J. Misa, Boelie Elzen, Robin Williams (academic), Ronald Kline, Marlei Pozzebon, and Osman Sadeck\n\n\n\n"}
{"id": "7777698", "url": "https://en.wikipedia.org/wiki?curid=7777698", "title": "Synopses of the British Fauna", "text": "Synopses of the British Fauna\n\nSynopses of the British Fauna is a series of identification guides, published by The Linnean Society and The Estuarine and Coastal Sciences Association. Each volume in the series provides and in-depth analysis of a group of animals and is designed to bridge the gap between the standard field guide and more specialised monograph or treatise. The series is now published by The Field Studies Council on behalf of The Linnean Society and The Estuarine and Coastal Sciences Association. \n\nThe series is designed for use in the field and is kept as user friendly as possible with technical terminology kept to a minimum and a glossary of terms provided, although the complexity of the subject matter makes the books more suitable for the more experienced practitioner.\n\nOn 11 March 1943, at a meeting of The Linnean Society in Burlington House, TH Savoy presented his \"Synopsis of the Opiliones\" (Harvestmen). It was so well received that a decision was made there and then to publish it as the first of a series of \"ecological fauna lists\".\n\nRe-launched by Dr Doris Kermack in the mid-1960s, the New Series of \"Synopses of the British Fauna\" went from strength to strength. From number 13, the series had been jointly sponsored by The Estuarine and Coastal Sciences Association and Dr RSK Barnes became co-editor.\n\nFrom 1993, the series has been published by The Field Studies Council and benefits from association with the extensive testing undertaken as part of the AIDGAP project.\n\nThe series contains the following volumes, many of which are out of print. Many of the volumes have been updated and reprinted under slightly different names to reflect either taxonomic changes or advances in the understanding of a group.\n\n\n"}
{"id": "30758555", "url": "https://en.wikipedia.org/wiki?curid=30758555", "title": "The Sun, the Genome and the Internet", "text": "The Sun, the Genome and the Internet\n\nThe Sun, the Genome, and the Internet is a non-fiction scientific book by renowned physicist Freeman J. Dyson, Professor Emeritus of Physics at the Institute for Advanced Study, Princeton University in the U.S.A. This short book was originally published in 1999 by the Oxford University Press.\n\nProfessor Dyson suggests that three rapidly advancing technologies, Solar Energy, Genetic Engineering and World-Wide Communication together have the potential to create a more equal distribution of the world's wealth. Amongst other things he proposes that solar power in the Third World could connect even the most remote areas to all of the information on the internet, potentially ending the cultural isolation of the poorest countries. Likewise, breakthroughs in genetics could lead to more efficient crops, thereby engendering the renewed vitality of traditional village life, currently devalued by the global market.\n\n\n"}
{"id": "31066", "url": "https://en.wikipedia.org/wiki?curid=31066", "title": "The Third Culture", "text": "The Third Culture\n\nThe Third Culture: Beyond the Scientific Revolution is a 1995 book by John Brockman which discusses the work of several well-known scientists who are directly communicating their new, sometimes provocative, ideas to the general public. John Brockman has continued the themes of 'The Third Culture' in the website of the Edge Foundation, where leading scientists and thinkers contribute their thoughts in plain English.\n\nThe title of the book refers to Charles Percy Snow's 1959 work \"The Two Cultures and the Scientific Revolution\", which described the conflict between the cultures of the humanities and science.\n\n23 people were included in the 1995 book:\n\nThe book influenced the reception of popular scientific literature in parts of the world beyond the United States. In Germany, the book inspired several newspapers to integrate scientific reports into their \"Feuilleton\" or \"culture\" sections (such as the \"Frankfurter Allgemeine Zeitung\"). At the same time, the assertions of the book were discussed as a source of controversy, especially the implicit assertion that \"third culture thinking\" is mainly an American development. Critics acknowledge that, whereas in the Anglo-Saxon cultures there is a large tradition of scientists writing popular books, such tradition was absent for a long period in the German and French languages, with journalists often filling the gap. However, some decades ago there were also scientists, like the physicists Heisenberg and Schrödinger and the psychologist Piaget, who fulfill the criteria Brockman named for \"third culture.\" The German author Gabor Paal suggested that the idea of the \"third culture\" is a rather modern version of what Hegel called Realphilosophie (\"philosophy of the real\").\n\nAlso, already during the interwar period, Otto Neurath and other members of the Vienna Circle strongly propagated the need for both the unity of science and the popularization of new scientific concepts. With the rise of the Nazis in Germany and Austria, many of the Vienna Circle's members left for the United States where they taught in several universities, causing their philosophical ideas to spread in the Anglo-Saxon world throughout the 1930s-1940s.\n\n\n\n"}
{"id": "57917109", "url": "https://en.wikipedia.org/wiki?curid=57917109", "title": "Wolfstein, the Murderer", "text": "Wolfstein, the Murderer\n\nWolfstein, The Murderer; or, The Secrets of a Robber’s Cave is an 1850 chapbook based on Percy Bysshe Shelley’s 1811 Gothic horror novel \"St. Irvyne; or, The Rosicrucian\".\n\nThe 1811 novel \"St. Irvyne, or, The Rosicrucian\" was republished by John Joseph Stockdale in 1822 following Shelley’s death. Two chapbooks were also published based on the novel. No publication date appeared on the title page.\nThe first chapbook version was entitled \"Wolfstein; or, The Mysterious Bandit\" and was published and printed by John Bailey at 116, Chancery Lane in London in 1822. The chapbook was a condensed version of the novel in 20 pages. The total length was 28 pages including the second story. Chapbooks were meant for popular consumption, serving the same function as a paperback would. The chapbook sold for sixpence.\n\nAnother more condensed twelve-page chapbook was published in 1850 by Thomas Redriffe in London entitled \"Wolfstein, the Murderer; or, The Secrets of a Robber's Cave: A Terrific Romance. To which is Added, The Two Serpents, an Oriental Apologue.\" The Ossian epigraph appeared on the title page: \"A tale of horror, of murder, and of deeds done in darkness.\" Printed for Thomas Redriffe, Piccadilly. The price was \"Two Pence\". Printed by William Bethell, 10, Marshall-street, Liverpool. No date of publication appeared on the title page. The story was six pages long, pages three through eight. The second story was four pages long, pages nine through twelve.\n\nThe frontispiece consisted of a drawing of Wolfstein confronted by a skeleton struck by lightning. He stands over the corpse of Serena. The drawing is a more condensed version of the 1822 frontispiece. The caption reads: \"Deeper grew the gloom of the cavern, and darkness seemed to press around him. Suddenly a flash of lightning burst through the cavern, followed by thunder that appeared to convulse the universal fabric of nature; and borne on the sulphurous blast, the Prince of Terrors stood before him.\"\n\nThe chapbook follows the plot of the first section of the novel \"St. Irvyne\" on the bandits but omits the second part featuring Frederic Nempere in Geneva. The account is radically condensed.\nThe name of Cavigni, leader of the bandits, is changed to Stiletto. The name of Megalena is changed to Serena. The character of Ginotti, the Rosicrucian alchemist, does not appear.\n\nThe opening scene is of a raging thunderstorm. Wolfstein is a wanderer in the Swiss Alps who seeks cover from the storm. He is a distraught outcast who plans to commit suicide. A group of monks carrying a body for burial in a torch-light procession runs into him and saves his life.\n\nBandits then attack them. Wolfstein is accepted as a member of the bandits. He becomes used to a life of crime. But a rivalry develops between him and Stilleto over Serena. He decides to poison the chief by secretly placing a white powder in his wine. Stiletto drinks the poison and dies.\n\nWolfstein is subsequently elected Captain by the other bandits after the murder of Stiletto. He then is racked by dreams of the murdered Cavigni which fill him with dread and foreboding.\nDays after the murder, Wolfstein seeks to seduce Serena. He observes her in prayer. She refuses his advances and pulls out a dagger. In response, Wolfstein unsheaths his sword and stabs her to death.\n\nThunder was heard in the cell. The Prince of Terror then appeared before him. A “strange and unnatural stench” infused the room as “the yawning gulf of hell” swallowed Wolfstein as blue flames swirled around his body.\n\nThe final paragraph concludes with a moral of the story. Plunging into despair and indulging in crime are not the ways to confront misfortune. The tenets of morality and truth should be diligently followed. Only “misery, disgrace and ruin” result when these principles are ignored.\n\n \n"}
{"id": "42716127", "url": "https://en.wikipedia.org/wiki?curid=42716127", "title": "Xenology", "text": "Xenology\n\nXenology is the scientific study of extraterrestrial life. Derived from the Greek \"xenos\", which as a substantive has the meaning \"stranger, wanderer, refugee\" and as an adjective \"foreign, alien, strange, unusual.\"\n\nIt is used to denote a hypothetical science whose object of study would be extraterrestrial societies developed by alien lifeforms. In science fiction criticism and studies the term has been advocated by writers such as David Brin (\"Xenology: The New Science of Asking 'Who's Out There?'\" \"Analog\", 26 April 1983) as an analogue of (terrestrial) ethnology. By extension the term may also refer to the fictional creation of \"alternative humankinds\".\n\nInstances in which Xenology was referred to in a work of Science Fiction include the Brothers Strugatsky's 1972 novel \"Roadside Picnic\". In section three of which one of the character's, a noble laureate by the name of Valentine Pillman, explains Xenology as \"an unnatural mixture of science fiction and formal logic. At its core is a flawed assumption—that an alien race would be psychologically human.\"\n\nThe term xenology was employed by German Indologist Wilhelm Halbfass in his \"Indien und Europa, Perspektiven ihrer geistigen Begegnung\" (India and Europe: Perspectives on Their Spiritual Encounter) (1981) to denote the study of the ethnocentric views held by societies with regard to different classes of foreigner, in other words the positive or negative ways in which a given culture defines those outside or alien to it. Xenology is thus the study of the various modalities whereby self and otherness are defined \"within a historically complex collision of cultures\".\n\nRobert A. Freitas Jr. self-published a book on the subject, \"Xenology: An Introduction to the Scientific Study of Extraterrestrial Life, Intelligence, and Civilization\" (XRI, 1979). Freitas argued for the primacy of the term in the context of extraterrestrial life in a 1983 letter to the journal Nature.\n"}
