{"id": "4973071", "url": "https://en.wikipedia.org/wiki?curid=4973071", "title": "AP Psychology", "text": "AP Psychology\n\nThe Advanced Placement Psychology (AP Psychology, AP Psych, or APPSY) course and corresponding exam are part of College Board's Advanced Placement Program. This course is tailored for students interested in the field of psychology and as an opportunity to earn Advanced Placement credit or exemption from a college-level psychology course. It was the shortest AP exam until the AP Physics C exam was split into two separate exams in 2006.\n\nThe College Board provides a course of study to help educators prepare their students for the AP Psychology exam. The exam covers the following 14 areas. The percentage indicates the portion of the multiple-choice section of the exam focused on each content area:\n\nThe exam includes two sections: a 70-minute multiple choice section (100 questions) and a 50-minute free response section (2 prompts). The multiple choice provides two-thirds of the grade and the free-response provides the remaining third.\n\nBeginning with the May 2011 AP Exam administration, total scores on the multiple-choice section are based only on the number of questions answered correctly. Points are no longer deducted for incorrect answers. Grading (the number of points needed to get a certain score) is slightly more strict as a result.\n\nThe exam was first held in 1992. Grade distributions for the Psychology exam scores since 2010 were:\n\n"}
{"id": "48898077", "url": "https://en.wikipedia.org/wiki?curid=48898077", "title": "Adhesome", "text": "Adhesome\n\nThe term Adhesome was first used by Richard Hynes to describe the complement of cell-cell and cell-matrix adhesion receptors in an organism and later expanded by Benny Geiger and co-workers to include the entire network of structural and signaling proteins involved in regulating cell-matrix adhesion. The major cell-matrix adhesion receptors are integrins and therefore the adhesome of cell-matrix adhesion is referred to as the integrin adhesome. Cell-cell adhesion is primarily mediated by cadherin receptors and therefore the adhesome of cell-cell adhesion is referred to as the cadherin adhesome or cadhesome. The first attempts to establish the set of proteins that participate directly ('bona fide' adhesome components) or affect indirectly ('associated' adhesome components) cell adhesion were based on mining of the primary research literature, and resulted in approximately 200 protein in either integrin or cadherin adhesomes. Later, unbiased proteomic approaches utilizing mass spectrometry have detected hundreds more proteins associated with integrin adhesions. However, a comparison of multiple proteomic studies of the integrin adhesome of fibroblasts attached to fibronectin found only 60 proteins common to all studies. Humphries and co-workers named these 60 proteins the 'consensus integrin adhesome'. Cell-matrix adhesions have been more extensively investigated by proteomics compared with cell-cell adhesions because they are more readily isolated from cells attached to glass. The advent of proximity biotinylation by birA* has facilitated the first proteomics-based studies of the cadherin adhesome.\n\nWhile proteomic methods identified many novel proteins that potentially might be adhesome components they cannot be regarded as adhesome components until they are validated to fulfill the following criteria: 1. they localize to a cell adhesion structure such as focal adhesion or adherens junction. 2. they directly interact with one of the core adhesome components, such as integrin, cadherin or catenins AND/OR their knockdown has a clear effect on cell adhesion.\n\nMass spectrometry has been successfully used to identify changes in the composition of the adhesome upon perturbation. Schiller et al. as well as Kuo et al. examined the effect of inhibition of myosin contractility on the integrin adhesome composition and found LIM domain proteins and beta-PIX to be tension sensitive. Gou et al. found little change in the cadherin adhesome after calcium depletion from the media, which essentially abrogates cell-cell adhesion. Reinhard Fassler and co-workers used proteomics on specifically engineered cell lines to distinguish between the adhesome of β1- and αv-class integrins.\n\nThe adhesome contains multi domain proteins with various functions, some of which are specifically enriched in the adhesome compared to the cell proteome. Protein domains enriched in the adhesome include: Pleckstrin homology (PH) and FERM domains, which target proteins to the plasma membrane; Calponin homology (CH) domain, which is an F-actin binding motif; Src homology 2 (SH2) domain, which mediate interaction with phosphorylated tyrosine residues; armadillo (ARM) GUK and LIM domains, which mediate specific protein-protein binding. The literature-based adhesome contains enzymes, such as protein tyrosine and serine/threonine kinases and phosphatases, guanine nucleotide exchange factors and GTPase activating proteins, E3-ligases and proteases, that regulate adhesion through post translational modification of the many structural and scaffolding proteins found in the adhesome. The proteomic-based studies have identified many proteins from functional groups that haven't previously been associated with cell adhesion sites, such as proteins involved in RNA splicing, translation, trafficking, golgi, endoplasmic reticulum, and metabolic enzymes. Whether these proteins are indeed an integral part of the adhesome or an artifact of the proteomic methods remains to be seen.\n\nThe availability of genomes of many organisms on the tree of life has opened up the possibility to study how the adhesome evolved from the unicellular relatives of animals through simple animals (e.g. sponges) to mammals. Surprisingly, the majority of cadherin adhesome proteins existed long before multicellularity and had other functions in cells. Later, with the emergence of the cadherin-catenin-actin structure they were co-opted into the cadhesome.\n\n"}
{"id": "10181793", "url": "https://en.wikipedia.org/wiki?curid=10181793", "title": "Azeotrope tables", "text": "Azeotrope tables\n\nThis page contains tables of azeotrope data for various binary and ternary mixtures of solvents. The data include the composition of a mixture by weight (in binary azeotropes, when only one fraction is given, it is the fraction of the second component), the boiling point (b.p.) of a component, the boiling point of a mixture, and the specific gravity of the mixture. Boiling points are reported at a pressure of 760 mm Hg unless otherwise stated. Where the mixture separates into layers, values are shown for upper (U) and lower (L) layers.\n\nThe data were obtained from Lange's 10th edition and CRC 44th edition unless otherwise noted (see color code table).\n\nA list of 15825 binary and ternary mixtures was collated and published by the ACS. An azeotrope databank is also available online through Edinburgh University\n\nTables of various ternary azeotropes (that is azeotropes consisting of three components). Fraction percentages are given by weight.\n"}
{"id": "13074924", "url": "https://en.wikipedia.org/wiki?curid=13074924", "title": "Bioindustry Park Silvano Fumero", "text": "Bioindustry Park Silvano Fumero\n\nThe Bioindustry Park Silvano Fumero (BiPCa) is a Science and Technology Park located in Canavese near Turin in the north-west of Italy.\nThe park was founded to promote and develop biotechnological research, to host companies operating research and development, and pilot production in the life science sector. Companies such as Merck Serono and Bracco chose the Park as the location for their research and development activities.\n\nThe Park offers a complete set of support services to R&D activities, not only offering research facilities and scientific services, but also flexible and attractive financing solutions, sound project management and results evaluation. \n\nThe park is the owner of the Laboratory for Advanced Methodologies (LIMA) which performs exploitation of scientific results in the fields of Chemistry, Molecular Biology, Proteomics and Bioinformatics. LIMA, thanks to its solid co-operation with the University of Turin and CNR ISPA (Consiglio Nazionale delle Ricerche), is also a permanent training site for graduates and researchers. \n\nA University Center in Imaging Technologies (CEIP) managed by University of Torino is also active and is focused on the use of different technologies (MRI, Ultrasound, X-ray, etc.) in order to evaluate new solutions in the diagnosis and assessment of the efficacy and efficiency of drugs.\n\nIn 2005 BiPCa has launched, with the support of Regione Piemonte, the Discovery initiative to identify and select innovative entrepreneurial ideas in the Biotech field. Eight start-ups have already been created from the beginning of the project in partnership with Eporgen venture, a Seed capital company focused on investment in new innovative entrepreneurial projects in the biotech field.\n\nBioindustry Park is also acting as system integrator for the development of a local cluster, bioPmed (http://www.biopmed.eu).\nSince 2009 Bioindustry Park acts as cluster management company for the development of the regional cluster bioPmed (www.biopmed.eu) on biotech and medtech. Created thanks to Regione Piemonte, it gathers more than 60 companies, research centres and three academic institutions (University of Turin, Università del Piemonte Orientale and Turin Polythecnic), who signed an agreement to create, build, support and animate the local cluster. \n\nAll the activities are carried out in cooperation with strong local actors, such as Turin Province and Turin Chamber of Commerce, CEIPiemonte, ALPS EEN and Patlib network and synergies with other local R&D initiatives are in place. The cluster is also founding member of the AlpsBioCluster (www.alpsbiocluster.eu) together with Rhône-Alpes Region (France), Lombardy, South Tirol, Bavaria and Western Switzerland that intends to promote and foster transalpine competitiveness in the biotech and medical sciences sectors. \n\n"}
{"id": "16316127", "url": "https://en.wikipedia.org/wiki?curid=16316127", "title": "Biology by Team", "text": "Biology by Team\n\nBiology by Team in German \"Biologie im Team\" - is the first Austrian biology contest for Upper Secondary Schools.\n\nStudents at upper secondary schools, who are especially interested in biology, can deepen their knowledge and\nbroaden their competence in experimental biology within the frame work of this contest.\nEach year, a team of teachers choose modules of key themes on which students work in the form of a voluntary exercise.\nThe evaluation focuses in particular on the practical work, and, since the school year 2004/05, also on teamwork. In April, a two-day closing competition takes place, in which six groups of students from participating schools are given various\nproblems to solve. A jury (persons from the science and corporate communities) evaluate the results and how they are presented.\n\nThe concept was developed by a team of teachers in co-operation with the AHS (Academic Secondary Schools) - Department of the Pedagogical Institute in Carinthia. \nSince 2008 it is situated at the Science departement of the University College of Teacher Training Carinthia.\nThe first contest in the school year 2002/03 took place under the motto: \"Hell is loose in the ground under us\".\nOther themes included \"Beautiful but dangerous\", \"www-worldwide water 1 and 2\", \"Expedition forest\", \"Relationship boxes\", \"Mole's view\", \"Biological timetravel\", \"Biology at the University\", \"Ecce Homo\", Biodiversity, \"Death in tin cans\", \"Sex sells\", \"Without a trace\", \"Biologists see more\" and \"Quo vadis biology?\" \nThe theme for the year 2018/19 is \"Biology without limits\"?\nTill now the following schools were participating:\n• BG/BRG Mössingerstraße Klagenfurt\n• BG/BRG St. Martinerstraße Villach\n• BG/BRG Peraustraße Villach\n• Österreichisches Gymnasium Prag\n• Europagymnasium Klagenfurt\n• BRG Viktring Klagenfurt\n• BORG Wolfsberg Wolfsberg\n• Stiftsgymnasium St. Paul im Lavanttal St. Paul im Lavanttal\n\nBIT was submitted for the German Innovations-prize for Sustainable Education and placed among the 13 „best of\"all nominated projects.\nWith these prerequisites the base concept of „Biology By Team\" can be replicated for other science and instructional fields and could provide an important contribution for the improvement of the subject and also team competence of our youth.\n\nBIT - the only Austrian biology competition, enabled 2008 for the first time the Austrian participation at the EUSO - European Union Science Olympiad.\n\n\n"}
{"id": "27138453", "url": "https://en.wikipedia.org/wiki?curid=27138453", "title": "Biopôle", "text": "Biopôle\n\nBiopôle is a science park of 80,000 square metres destined to host companies whose main activity is in the life sciences field. While it is open to all therapeutic areas, the main focus is on developing innovative solutions in the fields of oncology, immunology and personalised medicine. Biopôle offers space for industrial research and development or administrative use.\n\nThe Biopôle is owned at 98% by the Canton of Vaud. It is located in Épalinges, to the north of Lausanne and is the first of its kind in French-speaking part of Switzerland. The Biopôle site is located at the intersection of the large Lausanne campus, complementing the University Hospital of Lausanne (CHUV), the Swiss Federal Institute of Technology in Lausanne (EPFL), the University of Lausanne and the major industrial concerns.\n\n"}
{"id": "64221", "url": "https://en.wikipedia.org/wiki?curid=64221", "title": "Biorhythm", "text": "Biorhythm\n\nA biorhythm (from Greek βίος - \"bios\", \"life\" and ῥυθμός - \"rhuthmos\", \"any regular recurring motion, rhythm\") is an attempt to predict various aspects of a person's life through simple mathematical cycles. The theory was developed by Wilhelm Fliess in the late 19th century, and was popularized in the United States in late 1970s. Most scientists believe that the idea has no more predictive power than chance. \"The theory of biorhythms is a theory that claims our daily lives are significantly affected by rhythmic cycles.\"\n\nAccording to the theory of biorhythms, a person's life is influenced by rhythmic biological cycles that affect his or her ability in various domains, such as mental, physical and emotional activity. These cycles begin at birth and oscillate in a steady (sine wave) fashion throughout life, and by modeling them mathematically, it is suggested that a person's level of ability in each of these domains can be predicted from day to day. The theory is built on the idea that the biofeedback chemical and hormonal secretion functions within the body could show a sinusoidal behavior over time.\n\nMost biorhythm models use three cycles: a 23-day physical cycle, a 28-day emotional cycle, and a 33-day intellectual cycle. Although the 28-day cycle is the same length as the average woman's menstrual cycle and was originally described as a \"female\" cycle (see below), the two are not necessarily in synchronization. Each of these cycles varies between high and low extremes sinusoidally, with days where the cycle crosses the zero line described as \"critical days\" of greater risk or uncertainty.\n\nThe numbers from +100% (maximum) to -100% (minimum) indicate where on each cycle the rhythms are on a particular day. In general, a rhythm at 0% is crossing the midpoint and is thought to have no real impact on your life, whereas a rhythm at +100% (at the peak of that cycle) would give you an edge in that area, and a rhythm at -100% (at the bottom of that cycle) would make life more difficult in that area. There is no particular meaning to a day on which your rhythms are all high or all low, except the obvious benefits or hindrances that these rare extremes are thought to have on your life.\n\nIn addition to the three popular cycles, various other cycles have been proposed, based on linear combination of the three, or on longer or shorter rhythms.\n\nTheories published state the equations for the cycles as:\nwhere formula_4 indicates the number of days since birth. Basic arithmetic shows that the combination of the simpler 23- and 28-day cycles repeats every 644 days (or 1-3/4 years), while the triple combination of 23-, 28-, and 33-day cycles repeats every 21,252 days (or 58.18+ years).\n\nThe notion of periodic cycles in human fortunes is ancient; for instance, it is found in natal astrology and in folk beliefs about \"lucky days\". The 23- and 28-day rhythms used by biorhythmists, however, were first devised in the late 19th century by Wilhelm Fliess, a Berlin physician and patient of Sigmund Freud. Fliess believed that he observed regularities at 23- and 28-day intervals in a number of phenomena, including births and deaths. He labeled the 23-day rhythm \"male\" and the 28-day rhythm \"female\", matching the menstrual cycle.\n\nIn 1904, Viennese psychology professor Hermann Swoboda came to similar conclusions. Alfred Teltscher, professor of engineering at the University of Innsbruck, developed Swoboda's work and suggested that his students' good and bad days followed a rhythmic pattern; he believed that the brain's ability to absorb, mental ability, and alertness ran in 33-day cycles. One of the first academic researchers of biorhythms was Estonian-born Nikolai Pärna, who published a book in German called \"Rhythm, Life and Creation\" in 1923.\n\nThe practice of consulting biorhythms was popularized in the 1970s by a series of books by Bernard Gittelson, including \"Biorhythm — A Personal Science\", \"Biorhythm Charts of the Famous and Infamous\", and \"Biorhythm Sports Forecasting\". Gittelson's company, Biorhythm Computers, Inc., made a business selling personal biorhythm charts and calculators, but his ability to predict sporting events was not substantiated.\n\nCharting biorhythms for personal use was popular in the United States during the 1970s; many places (especially video arcades and amusement areas) had a biorhythm machine that provided charts upon entry of date of birth. Biorhythm programs were a common application on personal computers; and in the late 1970s, there were also handheld biorhythm calculators on the market, the \"Kosmos 1\" and the Casio \"Biolator\". Biorhythm charts appeared in the \"Chicago Tribune\" from 1977 to 1979, and Gittelson wrote daily biorhythm charts for the \"Toronto Star\" from 1981 to 1985.\n\nAlthough biorhythms have declined in popularity (pop culture magazine \"Vice\" considered them \"dead\" by the mid 2010s), there are free and proprietary apps and computer programs which have charting and analysis capabilities, as well as numerous websites that offer free biorhythm readings.\n\nThere have been some three dozen studies supporting biorhythm theory, but according to a study by Terence Hines, all of those had methodological and statistical errors. Hines rejected 134 biorhythm studies and concluded that the theory is not valid.\n\nSupporters continued to defend the theory after Hines' review, causing other scientists to consider the field as pseudoscience:\nThe physiologist Gordon Stein in the book \"Encyclopedia of Hoaxes\" (1993) has written: \"Both the theoretical underpinning and the practical scientific verification of biorhythm theory are lacking. Without those, biorhythms became just another pseudoscientific claim that people are willing to accept without required evidence. Those pushing biorhythm calculators and books on a gullible public are guilty of making fraudulent claims. They are hoaxers of the public if they know what they are saying has no factual justification.\"\n\nA 1978 study of the incidence of industrial accidents found neither empirical nor theoretical support for the biorhythm model.\n\n\n\n"}
{"id": "49269673", "url": "https://en.wikipedia.org/wiki?curid=49269673", "title": "CONICYT", "text": "CONICYT\n\nCONICYT is a Chilean government agency responsible for coordinating, promoting and aiding scientific research in the country. The name is an acronym of \"Comisión Nacional de Investigación Científica y Tecnológica\" meaning \"National Commission for Scientific and Technological Research\". CONICYT is part of the Ministry of Education.\n\nCONICYT provides grants through several programs:\n"}
{"id": "40703649", "url": "https://en.wikipedia.org/wiki?curid=40703649", "title": "Carctol", "text": "Carctol\n\nCarctol is an ineffective cancer treatment made by mixing eight Indian herbs.\n\nCarctol has been aggressively marketed as being able to treat cancer and reduce the side-effects of chemotherapy. However, there is no medical evidence that it has any benefits whatsoever for people with cancer.\n\nCarctol is a herbal dietary supplement marketed with claims it is based on traditional ayurvedic medicine. Its ingredients include \"Hemidesmus indicus\", \"Tribulus terrestris\", \"Piper cubeba\", \"Ammani vesicatoria\", \"Lepidium sativum\", \"Blepharis edulis\", \"Smilax china\", and \"Rheum emodi\".\n\nIt was first promoted in 1968 by Nandlal Tiwari. In 2009, Edzard Ernst wrote that it was still promoted in the United Kingdom; public relations companies hired by its sellers had garnered it wide coverage on the web and, according to the British Medical Journal, in the media generally.\n\nCancer Research UK say of Carctol, \"available scientific evidence does not support its use for the treatment of cancer in humans\". Edzard Ernst has written \"the claim that Carctol is of any benefit to cancer patients is not supported by scientific evidence\".\n\nHarriet A. Hall includes Carctol among the biologically-based remedies promoted by naturopaths. Hall laments that frauds and quacks persistently try to take advantage of the vulnerability of cancer patients.\n\n"}
{"id": "6313", "url": "https://en.wikipedia.org/wiki?curid=6313", "title": "Classical element", "text": "Classical element\n\nClassical elements typically refer to the concepts in ancient Greece of earth, water, air, fire, and aether, which were proposed to explain the nature and complexity of all matter in terms of simpler substances. Ancient cultures in Babylonia, Japan, Tibet, and India had similar lists, sometimes referring in local languages to \"air\" as \"wind\" and the fifth element as \"void\". The Chinese Wu Xing system lists Wood (木 \"mù\"), Fire (火 \"huǒ\"), Earth (土 \"tǔ\"), Metal (金 \"jīn\"), and Water (水 \"shuǐ\"), though these are described more as energies or transitions rather than as types of material.\n\nThese different cultures and even individual philosophers had widely varying explanations concerning their attributes and how they related to observable phenomena as well as cosmology. Sometimes these theories overlapped with mythology and were personified in deities. Some of these interpretations included atomism (the idea of very small, indivisible portions of matter) but other interpretations considered the elements to be divisible into infinitely small pieces without changing their nature.\n\nWhile the classification of the material world in ancient Indian, Hellenistic Egypt, and ancient Greece into Air, Earth, Fire and Water was more philosophical, during the Islamic Golden Age medieval middle eastern scientists used practical, experimental observation to classify materials. In Europe, the Ancient Greek system of Aristotle evolved slightly into the medieval system, which for the first time in Europe became subject to experimental verification in the 1600s, during the Scientific Revolution.\n\nModern science does not support the classical elements as the material basis of the physical world. Atomic theory classifies atoms into more than a hundred chemical elements such as oxygen, iron, and mercury. These elements form chemical compounds and mixtures, and under different temperatures and pressures, these substances can adopt different states of matter. The most commonly observed states of solid, liquid, gas, and plasma share many attributes with the classical elements of earth, water, air, and fire, respectively, but these states are due to similar behavior of different types of atoms at similar energy levels, and not due to containing a certain type of atom or a certain type of substance.\n\nIn classical thought, the four elements earth, water, air, and fire as proposed by Empedocles frequently occur; Aristotle added a fifth element, aether; it has been called akasha in India and quintessence in Europe.\n\nThe concept of the five elements formed a basis of analysis in both Hinduism and Buddhism. In Hinduism, particularly in an esoteric context, the four states-of-matter describe matter, and a fifth element describes that which was beyond the material world. Similar lists existed in ancient China, Korea and Japan. In Buddhism the four great elements, to which two others are sometimes added, are not viewed as substances, but as categories of sensory experience.\n\nIn Babylonian mythology, the cosmogony called \"Enûma Eliš\", a text written between the 18th and 16th centuries BC, involves four gods that we might see as personified cosmic elements: sea, earth, sky, wind. In other Babylonian texts these phenomena are considered independent of their association with deities, though they are not treated as the component elements of the universe, as later in Empedocles.\n\nThe system of five elements are found in Vedas, especially Ayurveda, the \"pancha mahabhuta\", or \"five great elements\", of Hinduism are \"bhūmi\" (earth), \"ap\" or \"jala\" (water), \"tejas\" or \"agni\" (fire), \"marut\", \"vayu\" or \"pavan\" (air or wind) and \"vyom\" or \"shunya\" (space or zero) or \"akash\" (aether or void). They further suggest that all of creation, including the human body, is made up of these five essential elements and that upon death, the human body dissolves into these five elements of nature, thereby balancing the cycle of nature.\n\nThe five elements are associated with the five senses, and act as the gross medium for the experience of sensations. The basest element, earth, created using all the other elements, can be perceived by all five senses – (i) hearing, (ii) touch, (iii) sight, (iv) taste, and (v) smell. The next higher element, water, has no odor but can be heard, felt, seen and tasted. Next comes fire, which can be heard, felt and seen. Air can be heard and felt. \"Akasha\" (aether) is beyond the senses of smell, taste, sight, and touch; it being accessible to the sense of hearing alone.\n\nIn the Pali literature, the \"mahabhuta\" (\"great elements\") or \"catudhatu\" (\"four elements\") are earth, water, fire and air. In early Buddhism, the four elements are a basis for understanding suffering and for liberating oneself from suffering. The earliest Buddhist texts explain that the four primary material elements are the sensory qualities solidity, fluidity, temperature, and mobility; their characterization as earth, water, fire, and air, respectively, is declared an abstraction – instead of concentrating on the fact of material existence, one observes how a physical thing is sensed, felt, perceived.\n\nThe Buddha's teaching regarding the four elements is to be understood as the base of all observation of real sensations rather than as a philosophy. The four properties are cohesion (water), solidity or inertia (earth), expansion or vibration (air) and heat or energy content (fire). He promulgated a categorization of mind and matter as composed of eight types of \"kalapas\" of which the four elements are primary and a secondary group of four are color, smell, taste, and nutriment which are derivative from the four primaries.\n\nThanissaro Bhikkhu (1997) renders an extract of Shakyamuni Buddha's from Pali into English thus:\nTibetan Buddhist medical literature speaks of the Panch Mahābhūta (five elements).\n\nThe Chinese had a somewhat different series of elements, namely Fire, Earth, Metal (literally gold), Water and Wood, which were understood as different types of energy in a state of constant interaction and flux with one another, rather than the Western notion of different kinds of material.\n\nAlthough it is usually translated as \"element\", the Chinese word \"xing\" literally means something like \"changing states of being\", \"permutations\" or \"metamorphoses of being\". In fact Sinologists cannot agree on any single translation. The Chinese elements were seen as ever changing and movingone translation of \"wu xing\" is simply \"the five changes\".\n\nThe Wu Xing are chiefly an ancient mnemonic device for systems with five stages; hence the preferred translation of \"movements\", \"phases\" or \"steps\" over \"elements.\"\n\nIn the bagua, metal is associated with the divination figure 兌 \"Duì\" (☱, the lake or marsh: 澤/泽 \"zé\") and with 乾 \"Qián\" (☰, the sky or heavens: 天 \"tiān\"). Wood is associated with 巽 \"Xùn\" (☴, the wind: 風/风 \"fēng\") and with 震 \"Zhèn\" (☳, the arousing/thunder: 雷 \"léi\"). In view of the durability of meteoric iron, metal came to be associated with the aether, which is sometimes conflated with Stoic pneuma, as both terms originally referred to air (the former being higher, brighter, more fiery or celestial and the latter being merely warmer, and thus vital or biogenetic). In Taoism, \"qi\" functions similarly to pneuma in a prime matter (a basic principle of energetic transformation) that accounts for both biological and inanimate phenomena.\n\nIn Chinese philosophy the universe consists of heaven and earth. The five major planets are associated with and even named after the elements: Jupiter 木星 is Wood (木), Mars 火星 is Fire (火), Saturn 土星 is Earth (土), Venus 金星 is Metal (金), and Mercury 水星 is Water (水). Also, the Moon represents Yin (陰), and the Sun 太陽 represents Yang (陽). Yin, Yang, and the five elements are associated with themes in the I Ching, the oldest of Chinese classical texts which describes an ancient system of cosmology and philosophy. The five elements also play an important part in Chinese astrology and the Chinese form of geomancy known as Feng shui.\n\nThe doctrine of five phases describes two cycles of balance, a generating or creation (生, shēng) cycle and an overcoming or destruction (克/剋, kè) cycle of interactions between the phases.\n\n\"Generating\"\n\n\"Overcoming\"\n\nThere are also two cycles of imbalance, an overacting cycle (cheng) and an insulting cycle (wu).\n\nThe ancient Greek belief in five basic elements, these being earth (γῆ \"ge\"), water (ὕδωρ \"hudor\"), air (ἀήρ \"aer\"), fire (πῦρ \"pur\") and aether (αἰθήρ \"aither\"), dates from pre-Socratic times and persisted throughout the Middle Ages and into the Renaissance, deeply influencing European thought and culture. These five elements are sometimes associated with the five platonic solids.\n\nSicilian philosopher Empedocles (ca. 450 BC) proved (at least to his satisfaction) that air was a separate substance by observing that a bucket inverted in water did not become filled with water, a pocket of air remaining trapped inside. Prior to Empedocles, Greek philosophers had debated which substance was the primordial element from which everything else was made; Heraclitus championed fire, Thales supported water, and Anaximenes plumped for air. Anaximander argued that the primordial substance was not any of the known substances, but could be transformed into them, and they into each other. Empedocles was the first to propose four elements, fire, earth, air, and water. He called them the four \"roots\" (ῥιζώματα, rhizōmata).\n\nPlato seems to have been the first to use the term \"element (στοιχεῖον, \"stoicheion\")\" in reference to air, fire, earth, and water. The ancient Greek word for element, \"stoicheion\" (from \"stoicheo\", \"to line up\") meant \"smallest division (of a sun-dial), a syllable\", as the composing unit of an alphabet it could denote a letter and the smallest unit from which a word is formed. A similar alphabetic metaphor may be the origin of the equivalent Latin word \"elementum\" (from which the English word comes), possibly based on the names of the letters 'l', 'm', and 'n', though the validity of this idea is debated.\n\nIn his \"On Generation and Corruption\", Aristotle related each of the four elements to two of the four sensible qualities:\n\nA classic diagram has one square inscribed in the other, with the corners of one being the classical elements, and the corners of the other being the properties. The opposite corner is the opposite of these properties, \"hot – cold\" and \"dry – wet\".\n\nAristotle added a fifth element, aether, as the quintessence, reasoning that whereas fire, earth, air, and water were earthly and corruptible, since no changes had been perceived in the heavenly regions, the stars cannot be made out of any of the four elements but must be made of a different, unchangeable, heavenly substance.\nA text written in Egypt in Hellenistic or Roman times called the \"Kore Kosmou\" (\"Virgin of the World\") ascribed to Hermes Trismegistus (associated with the Egyptian god Thoth), names the four elements fire, water, air, and earth. As described in this book:\n\nAnd Isis answer made: Of living things, my son, some are made friends with \"fire\", and some with \"water\", some with \"air\", and some with \"earth\", and some with two or three of these, and some with all. And, on the contrary, again some are made enemies of fire, and some of water, some of earth, and some of air, and some of two of them, and some of three, and some of all. For instance, son, the locust and all flies flee fire; the eagle and the hawk and all high-flying birds flee water; fish, air and earth; the snake avoids the open air. Whereas snakes and all creeping things love earth; all swimming things love water; winged things, air, of which they are the citizens; while those that fly still higher love the fire and have the habitat near it. Not that some of the animals as well do not love fire; for instance salamanders, for they even have their homes in it. It is because one or another of the elements doth form their bodies' outer envelope. Each soul, accordingly, while it is in its body is weighted and constricted by these four.\n\nAccording to Galen, these elements were used by Hippocrates in describing the human body with an association with the four humours: yellow bile (fire), black bile (earth), blood (air), and phlegm (water). Medical care was primarily about helping the patient stay in or return to his/her own personal natural balanced state.\n\nThe Neoplatonic philosopher Proclus rejected Aristotle's theory relating the elements to the sensible qualities hot, cold, wet, and dry. He maintained that each of the elements has three properties. Fire is sharp, subtle, and mobile while its opposite, earth, is blunt, dense, and immobile; they are joined by the intermediate elements, air and water, in the following fashion:\n\nIn Bön or ancient Tibetan philosophy, the five elemental processes of earth, water, fire, air and space are the essential materials of all existent phenomena or aggregates. The elemental processes form the basis of the calendar, astrology, medicine, psychology and are the foundation of the spiritual traditions of shamanism, tantra and Dzogchen.\n\nTenzin Wangyal Rinpoche states that\nThe names of the elements are analogous to categorised experiential sensations of the natural world. The names are symbolic and key to their inherent qualities and/or modes of action by analogy. In Bön the elemental processes are fundamental metaphors for working with external, internal and secret energetic forces. All five elemental processes in their essential purity are inherent in the mindstream and link the trikaya and are aspects of primordial energy. As Herbert V. Günther states:\nIn the above block quote the trikaya is encoded as: dharmakaya \"god\"; sambhogakaya \"temple\" and nirmanakaya \"house\".\n\nThe elemental system used in Medieval alchemy was developed primarily by the Arab alchemist Jābir ibn Hayyān (Geber). His system consisted of the four classical elements of air, earth, fire, and water, in addition to two philosophical elements: sulphur, characterizing the principle of combustibility, \"the stone which burns\"; and mercury, characterizing the principle of metallic properties. They were seen by early alchemists as idealized expressions of irreducibile components of the universe and are of larger consideration within philosophical alchemy.\n\nThe three metallic principles—sulphur to flammability or combustion, mercury to volatility and stability, and salt to solidity—became the \"tria prima\" of the Swiss alchemist Paracelsus. He reasoned that Aristotle’s four element theory appeared in bodies as three principles. Paracelsus saw these principles as fundamental and justified them by recourse to the description of how wood burns in fire. Mercury included the cohesive principle, so that when it left in smoke the wood fell apart. Smoke described the volatility (the mercurial principle), the heat-giving flames described flammability (sulphur), and the remnant ash described solidity (salt).\n\nThe Islamic philosophers al-Kindi, Avicenna and Fakhr al-Din al-Razi connected the four elements with the four natures heat and cold (the active force), and dryness and moisture (the recipients).\n\nJapanese traditions use a set of elements called the (\"godai\", literally \"five great\"). These five are earth, water, fire, wind/air, and void. These came from Indian Vastu shastra philosophy and Buddhist beliefs; in addition, the classical Chinese elements (, \"wu xing\") are also prominent in Japanese culture, especially to the influential Neo-Confucianists during the medieval Edo period.\n\n\nWestern astrology uses the four classical elements in connection with astrological charts and horoscopes. The twelve signs of the zodiac are divided into the four elements: Fire signs are Aries, Leo and Sagittarius, Earth signs are Taurus, Virgo and Capricorn, Air signs are Gemini, Libra and Aquarius, and Water signs are Cancer, Scorpio, and Pisces.\n\nThe Aristotelian tradition and medieval alchemy eventually gave rise to modern scientific theories and new taxonomies. By the time of Antoine Lavoisier, for example, a list of elements would no longer refer to classical elements. Some modern scientists see a parallel between the classical elements and the four states of matter: solid, liquid, gas and weakly ionized plasma.\n\nModern science recognizes classes of elementary particles which have no substructure (or rather, particles that are not made of other particles) and composite particles having substructure (particles made of other particles).\n\n\n\n"}
{"id": "8989793", "url": "https://en.wikipedia.org/wiki?curid=8989793", "title": "Coding (social sciences)", "text": "Coding (social sciences)\n\nIn the social sciences, coding is an analytical process in which data, in both quantitative form (such as questionnaires results) or qualitative form (such as interview transcripts) are categorized to facilitate analysis.\n\nOne purpose of coding is to transform the data into a form suitable for computer-aided analysis. This categorization of information is an important step, for example, in preparing data for computer processing with statistical software.\n\nSome studies will employ multiple coders working independently on the same data. This minimizes the chance of errors from coding and is believed to increase the reliability of data.\n\nOne code should apply to only one category and categories should be comprehensive. There should be clear guidelines for \"coders\" (individual who do the coding) so that code is consistent.\n\nFor quantitative analysis, data is coded usually into measured and recorded as nominal or ordinal variables.\n\nQuestionnaire data can be \"pre-coded\" (process of assigning codes to expected answers on designed questionnaire), \"field-coded\" (process of assigning codes as soon as data is available, usually during fieldwork), \"post-coded\" (coding of open questions on completed questionnaires) or \"office-coded\" (done after fieldwork). Note that some of the above are not mutually exclusive.\n\nIn social sciences, spreadsheets such as Excel and more advanced software packages such as R, Matlab, PSPP/SPSS, DAP/SAS, MiniTab and Stata are often used.\n\nFor disciplines in which a qualitative format is preferential, including ethnography, humanistic geography or phenomenological psychology a varied approach to coding can be applied. Iain Hay (2005) outlines a two-step process beginning with basic coding in order to distinguish overall themes, followed by a more in depth, interpretive code in which more specific trends and patterns can be interpreted.\n\nMuch of qualitative coding can be attributed to either grounded or \"a priori\" coding. Grounded coding refers to allowing notable themes and patterns emerge from the document themselves, where as \"a priori\" coding requires the researcher to apply pre-existing theoretical frameworks to analyze the documents. As coding methods are applied across various texts, the researcher is able to apply axial coding, which is the process of selecting core thematic categories present in several documents to discover common patterns and relations.\n\nPrior to constructing categories, a researcher must apply a first cycle coding method. There are a multitude of methods available, and a researcher will want to pick one that is suited for the format and nature of their documents. Not all methods can be applied to every type of document. Some examples of first cycle coding methods include:\n\n\nThe process can be done manually, which can be as simple as highlighting different concepts with different colours, or fed into a software package. Some examples of qualitative software packages include Atlas.ti, MAXQDA, NVivo, and QDA Miner.\n\nAfter assembling codes it is time to organize them into broader themes and categories. The process generally involves identifying themes from the existing codes, reducing the themes to a manageable number, creating hierarchies within the themes and then linking themes together through theoretical modeling.\n\nCreating memos during the coding process is integral to both grounded and a priori coding approaches. Qualitative research is inherently reflexive; as the researcher delves deeper into their subject, it is important to chronicle their own thought processes through reflective or methodological memos, as doing so may highlight their own subjective interpretations of data It is crucial to begin memoing at the onset of research. Regardless of the type of memo produced, what is important is that the process initiates critical thinking and productivity in the research. Doing so will facilitate easier and more coherent analyses as the project draws on \nMemos can be used to map research activities, uncover meaning from data, maintaining research momentum and engagement and opening communication.\n\n\nHay, I. (2005). \"Qualitative research methods in human geography\" (2nd ed.). Oxford: Oxford University Press.\n\nGrbich, Carol. (2013). \"Qualitative Data Analysis\" (2nd ed.). The Flinders University of South Australia: SAGE Publications Ltd.\n\nSaldaña, Johnny. (2015). \"The Coding Manual for Qualitative Researchers\" (3rd ed.). SAGE Publications Ltd.\n"}
{"id": "1628088", "url": "https://en.wikipedia.org/wiki?curid=1628088", "title": "Crackpot index", "text": "Crackpot index\n\nThe crackpot index is a number that rates scientific claims or the individuals that make them, in conjunction with a method for computing that number. While the indices have been created for their humorous value, their general concepts can be applied in other fields like risk management.\n\nThe method, proposed semi-seriously by mathematical physicist John C. Baez in 1992, computes an index by responses to a list of 36 questions, each positive response contributing a point value ranging from 1 to 50. The computation is initialized with a value of −5. An earlier version only had 17 questions with point values for each ranging from 1 to 40.\n\nPresumably any positive value of the index indicates crankiness.\n\nThough the index was not proposed as a serious method, it nevertheless has become popular in Internet discussions of whether a claim or an individual is cranky, particularly in physics (e.g., at the Usenet newsgroup sci.physics), or in mathematics.\n\nChris Caldwell's Prime Pages has a version adapted to prime number research which is a field with many famous unsolved problems that are easy to understand for amateur mathematicians.\n\nAn earlier crackpot index is Fred J. Gruenberger's \"A Measure for Crackpots\" published in December 1962 by the RAND Corporation.\n\n\n"}
{"id": "61889", "url": "https://en.wikipedia.org/wiki?curid=61889", "title": "Division (biology)", "text": "Division (biology)\n\nDivision is a taxonomic rank in biological classification that is used differently in zoology and in botany.\n\nIn botany and mycology, \"division\" refers to a rank equivalent to phylum. The use of either term is allowed under the International Code of Botanical Nomenclature, and both are commonly used in scientific literature.\n\nThe main Divisions of land plants, in the order in which they probably evolved, are the Marchantiophyta (liverworts), Anthocerotophyta (hornworts), Bryophyta (mosses), Filicophyta (ferns), Sphenophyta (horsetails), Cycadophyta (cycads), Ginkgophyta (ginkgo)s, Pinophyta (conifers), Gnetophyta (gnetophytes), and the Magnoliophyta (Angiosperms, flowering plants). The flowering plants now dominate terrestrial ecosystems, comprising 80% of vascular plant species.\n\nIn zoology, the term \"division\" is applied to an optional rank subordinate to the infraclass and superordinate to the cohort. A widely used classification (e.g. Carroll 1988) recognises teleost fishes as a Division Teleostei within Class Actinopterygii (the ray-finned fishes). Less commonly (as in Milner 1988), living tetrapods are ranked as Divisions Amphibia and Amniota within the clade of vertebrates with fleshy limbs (Sarcopterygii).\n\n"}
{"id": "12279653", "url": "https://en.wikipedia.org/wiki?curid=12279653", "title": "Educator Astronaut Project", "text": "Educator Astronaut Project\n\nThe Educator Astronaut Project is a NASA program designed to educate students and spur excitement in math, science, and space exploration. It is a successor to the Teacher in Space Project of the 1980s that was cancelled after the death of Christa McAuliffe in the Space Shuttle \"Challenger\" disaster (STS-51-L). NASA halted the teachers project amid concerns surrounding the risk of sending civilians to space.\n\nIn the 1990s, NASA created the Educator Astronaut Project, which carries on the objectives of the Teacher in Space Program -- seeking to elevate teaching as a profession and inspire students. Unlike the Teacher in Space Program, educator astronauts are fully trained astronauts who do the same jobs and duties that any other astronaut does. They fly as crew members with critical mission responsibilities, as well as education-related goals. In addition to their technical assignments, they assist other astronauts in connecting to students and teachers through space exploration.\n\nJoseph M. Acaba, Richard R. Arnold and Dorothy Metcalf-Lindenburger were selected as the first Educator Mission Specialists in the 2004 class. Both Acaba and Arnold were part of the crew of STS-119, a Space Shuttle mission to the International Space Station (ISS) which was flown by Space Shuttle \"Discovery\" in March 2009. Metcalf-Lindenburger flew on STS-131 in April 2010, also visiting the ISS aboard Space Shuttle \"Discovery\".\n\nBarbara Morgan, the backup to Christa McAuliffe in the Teacher in Space Project, remained involved with NASA after the \"Challenger\" disaster and continued to work with NASA’s Education Division until her selection as a Mission Specialist in 1998. Morgan completed two years of astronaut training and evaluation, and began official duties in 2000. Morgan became the first former teacher to travel to space on STS-118. While NASA press releases and media briefings often referred to her as a \"Mission Specialist Educator\" or \"Educator Astronaut\", Morgan did not train in the Educator Astronaut Project. NASA Administrator Michael D. Griffin clarified at a press conference after STS-118 that Morgan was not considered a Mission Specialist Educator, but rather was a standard Mission Specialist, who had once been a teacher. Morgan's duties as a Mission Specialist were no different from other Shuttle Mission Specialists.\n"}
{"id": "4245410", "url": "https://en.wikipedia.org/wiki?curid=4245410", "title": "Exformation", "text": "Exformation\n\nExformation (originally spelled \"eksformation\" in Danish) is a term coined by Danish science writer Tor Nørretranders in his book \"The User Illusion\" published in English 1998. It is meant to mean \"explicitly discarded information\". However, the term also has other meanings related to information, for instance \"useful and relevant information\" or a specific kind of information explosion.\n\nConsider the following phrase: \"the best horse at the race is number 7\". The information carried is very small, if considered from the point of view of information theory: just a few words. However let's assume that this phrase was spoken by a knowledgeable person, after a complex study of all the horses in the race, to someone interested in betting. The details are discarded, but the receiver of the information might get the same practical value of a complete analysis.\n\nEffective communication depends on a shared body of knowledge between the persons communicating. In using words, sounds, and gestures, the speaker has deliberately thrown away a huge body of information, though it remains implied. This shared context is called exformation.\n\nExformation is everything we do not actually say but have in our heads when, or before, we say anything at all - whereas information is the measurable, demonstrable utterance we actually come out with.\n\nIf someone is talking about computers, what is said will have more meaning if the person listening has some prior idea what a computer is, what it is good for, and in what contexts one might encounter one. From the information content of a message alone, there is no way of measuring how much exformation it contains.\n\nIn 1862 the author Victor Hugo wrote to his publisher asking how his most recent book, \"Les Misérables\", was getting on. Hugo just wrote \"?\" in his message, to which his publisher replied \"!\", to indicate it was selling well. This exchange of messages would have no meaning to a third party because the shared context is unique to those taking part in it. The amount of information (a single character) was extremely small, and yet because of exformation a meaning is clearly conveyed.\n\n\n"}
{"id": "59861", "url": "https://en.wikipedia.org/wiki?curid=59861", "title": "Experiment", "text": "Experiment\n\nAn experiment is a procedure carried out to support, refute, or validate a hypothesis. Experiments provide insight into cause-and-effect by demonstrating what outcome occurs when a particular factor is manipulated. Experiments vary greatly in goal and scale, but always rely on repeatable procedure and logical analysis of the results. There also exists natural experimental studies.\n\nA child may carry out basic experiments to understand gravity, while teams of scientists may take years of systematic investigation to advance their understanding of a phenomenon. Experiments and other types of hands-on activities are very important to student learning in the science classroom. Experiments can raise test scores and help a student become more engaged and interested in the material they are learning, especially when used over time. Experiments can vary from personal and informal natural comparisons (e.g. tasting a range of chocolates to find a favorite), to highly controlled (e.g. tests requiring complex apparatus overseen by many scientists that hope to discover information about subatomic particles). Uses of experiments vary considerably between the natural and human sciences.\n\nExperiments typically include controls, which are designed to minimize the effects of variables other than the single independent variable. This increases the reliability of the results, often through a comparison between control measurements and the other measurements. Scientific controls are a part of the scientific method. Ideally, all variables in an experiment are controlled (accounted for by the control measurements) and none are uncontrolled. In such an experiment, if all controls work as expected, it is possible to conclude that the experiment works as intended, and that results are due to the effect of the tested variable.\n\nIn the scientific method, an experiment is an empirical procedure that arbitrates competing models or hypotheses. Researchers also use experimentation to test existing theories or new hypotheses to support or disprove them.\n\nAn experiment usually tests a hypothesis, which is an expectation about how a particular process or phenomenon works. However, an experiment may also aim to answer a \"what-if\" question, without a specific expectation about what the experiment reveals, or to confirm prior results. If an experiment is carefully conducted, the results usually either support or disprove the hypothesis. According to some philosophies of science, an experiment can never \"prove\" a hypothesis, it can only add support. On the other hand, an experiment that provides a counterexample can disprove a theory or hypothesis, but a theory can always be salvaged by appropriate ad hoc modifications at the expense of simplicity. An experiment must also control the possible confounding factors—any factors that would mar the accuracy or repeatability of the experiment or the ability to interpret the results. Confounding is commonly eliminated through scientific controls and/or, in randomized experiments, through random assignment.\n\nIn engineering and the physical sciences, experiments are a primary component of the scientific method. They are used to test theories and hypotheses about how physical processes work under particular conditions (e.g., whether a particular engineering process can produce a desired chemical compound). Typically, experiments in these fields focus on replication of identical procedures in hopes of producing identical results in each replication. Random assignment is uncommon.\n\nIn medicine and the social sciences, the prevalence of experimental research varies widely across disciplines. When used, however, experiments typically follow the form of the clinical trial, where experimental units (usually individual human beings) are randomly assigned to a treatment or control condition where one or more outcomes are assessed. In contrast to norms in the physical sciences, the focus is typically on the average treatment effect (the difference in outcomes between the treatment and control groups) or another test statistic produced by the experiment. A single study typically does not involve replications of the experiment, but separate studies may be aggregated through systematic review and meta-analysis.\n\nThere are various differences in experimental practice in each of the branches of science. For example, agricultural research frequently uses randomized experiments (e.g., to test the comparative effectiveness of different fertilizers), while experimental economics often involves experimental tests of theorized human behaviors without relying on random assignment of individuals to treatment and control conditions.\n\nOne of the first methodical approaches to experiments in the modern sense is visible in the works of the Arab mathematician and scholar Ibn al-Haytham. He conducted his experiments in the field of optics - going back to optical and mathematical problems in the works of Ptolemy - by controlling his experiments due to factors such as self-criticality, reliance on visible results of the experiments as well as a criticality in terms of earlier results. He counts as one of the first scientists/ philosophers using an inductive-experimental method for achieving results. In his book \"Optics\" he describes the fundamentally new approach to knowledge and research in an experimental sense:\n\n\"“We should, that is, recommence the inquiry into its principles and premisses, beginning our investigation with an inspection of the things that exist and a survey of the conditions of visible objects. We should distinguish the properties of particulars, and gather by induction what pertains to the eye when vision takes place and what is found in the manner of sensation to be uniform, unchanging, manifest and not subject to doubt. After which we should ascend in our inquiry and reasonings, gradually and orderly, criticizing premisses and exercising caution in regard to conclusions – our aim in all that we make subject to inspection and review being to employ justice, not to follow prejudice, and to take care in all that we judge and criticize that we seek the truth and not to be swayed by opinion. We may in this way eventually come to the truth that gratifies the heart and gradually and carefully reach the end at which certainty appears; while through criticism and caution we may seize the truth that dispels disagreement and resolves doubtful matters. For all that, we are not free from that human turbidity which is in the nature of man; but we must do our best with what we possess of human power. From God we derive support in all things.“\"\n\nAccording to his explanation, a strictly controlled test execution with a sensibility for the subjectivity and susceptibility of outcomes due to the nature of man is necessary. Furthermore, a critical view on the results and outcomes of earlier scholars is necessary:\n\n\"“It is thus the duty of the man who studies the writings of scientists, if learning the truth is his goal, to make himself an enemy of all that he reads, and, applying his mind to the core and margins of its content, attack it from every side. He should also suspect himself as he performs his critical examination of it, so that he may avoid falling into either prejudice or leniency.”\"\n\nThus, a comparison of earlier results with the experimental results is necessary for an objective experiment - the visible results being more important. In the end, this may mean that an experimental researcher must find enough courage to discard traditional opinions or results, especially if these results are not experimental but results from a logical/ mental derivation. In this process of critical consideration, the man himself should not forget that he tends to subjective opinions - through \"prejudices\" and \"leniency\" - and thus has to be critical about his own way of building hypotheses.\n\nFrancis Bacon (1561–1626), an English philosopher and scientist active in the 17th century, became an influential supporter of experimental science in the English renaissance. He disagreed with the method of answering scientific questions by deduction - similar to Ibn al-Haytham - and described it as follows: \"Having first determined the question according to his will, man then resorts to experience, and bending her to conformity with his placets, leads her about like a captive in a procession.\" Bacon wanted a method that relied on repeatable observations, or experiments. Notably, he first ordered the scientific method as we understand it today. \n\nIn the centuries that followed, people who applied the scientific method in different areas made important advances and discoveries. For example, Galileo Galilei (1564-1642) accurately measured time and experimented to make accurate measurements and conclusions about the speed of a falling body. Antoine Lavoisier (1743-1794), a French chemist, used experiment to describe new areas, such as combustion and biochemistry and to develop the theory of conservation of mass (matter). Louis Pasteur (1822-1895) used the scientific method to disprove the prevailing theory of spontaneous generation and to develop the germ theory of disease. Because of the importance of controlling potentially confounding variables, the use of well-designed laboratory experiments is preferred when possible.\n\nA considerable amount of progress on the design and analysis of experiments occurred in the early 20th century, with contributions from statisticians such as Ronald Fisher (1890-1962), Jerzy Neyman (1894-1981), Oscar Kempthorne (1919-2000), Gertrude Mary Cox (1900-1978), and William Gemmell Cochran (1909-1980), among others.\n\nExperiments might be categorized according to a number of dimensions, depending upon professional norms and standards in different fields of study. In some disciplines (e.g., psychology or political science), a 'true experiment' is a method of social research in which there are two kinds of variables. The independent variable is manipulated by the experimenter, and the dependent variable is measured. The signifying characteristic of a true experiment is that it randomly allocates the subjects to neutralize experimenter bias, and ensures, over a large number of iterations of the experiment, that it controls for all confounding factors.\n\nA controlled experiment often compares the results obtained from experimental samples against \"control\" samples, which are practically identical to the experimental sample except for the one aspect whose effect is being tested (the independent variable). A good example would be a drug trial. The sample or group receiving the drug would be the experimental group (treatment group); and the one receiving the placebo or regular treatment would be the control one. In many laboratory experiments it is good practice to have several replicate samples for the test being performed and have both a positive control and a negative control. The results from replicate samples can often be averaged, or if one of the replicates is obviously inconsistent with the results from the other samples, it can be discarded as being the result of an experimental error (some step of the test procedure may have been mistakenly omitted for that sample). Most often, tests are done in duplicate or triplicate. A positive control is a procedure similar to the actual experimental test but is known from previous experience to give a positive result. A negative control is known to give a negative result. The positive control confirms that the basic conditions of the experiment were able to produce a positive result, even if none of the actual experimental samples produce a positive result. The negative control demonstrates the base-line result obtained when a test does not produce a measurable positive result. Most often the value of the negative control is treated as a \"background\" value to subtract from the test sample results. Sometimes the positive control takes the quadrant of a standard curve.\n\nAn example that is often used in teaching laboratories is a controlled protein assay. Students might be given a fluid sample containing an unknown (to the student) amount of protein. It is their job to correctly perform a controlled experiment in which they determine the concentration of protein in the fluid sample (usually called the \"unknown sample\"). The teaching lab would be equipped with a protein standard solution with a known protein concentration. Students could make several positive control samples containing various dilutions of the protein standard. Negative control samples would contain all of the reagents for the protein assay but no protein. In this example, all samples are performed in duplicate. The assay is a colorimetric assay in which a spectrophotometer can measure the amount of protein in samples by detecting a colored complex formed by the interaction of protein molecules and molecules of an added dye. In the illustration, the results for the diluted test samples can be compared to the results of the standard curve (the blue line in the illustration) to estimate the amount of protein in the unknown sample.\n\nControlled experiments can be performed when it is difficult to exactly control all the conditions in an experiment. In this case, the experiment begins by creating two or more sample groups that are \"probabilistically equivalent,\" which means that measurements of traits should be similar among the groups and that the groups should respond in the same manner if given the same treatment. This equivalency is determined by statistical methods that take into account the amount of variation between individuals and the number of individuals in each group. In fields such as microbiology and chemistry, where there is very little variation between individuals and the group size is easily in the millions, these statistical methods are often bypassed and simply splitting a solution into equal parts is assumed to produce identical sample groups.\n\nOnce equivalent groups have been formed, the experimenter tries to treat them identically except for the one \"variable\" that he or she wishes to isolate. Human experimentation requires special safeguards against outside variables such as the \"placebo effect\". Such experiments are generally \"double blind\", meaning that neither the volunteer nor the researcher knows which individuals are in the control group or the experimental group until after all of the data have been collected. This ensures that any effects on the volunteer are due to the treatment itself and are not a response to the knowledge that he is being treated.\n\nIn human experiments, researchers may give a subject (person) a stimulus that the subject responds to. The goal of the experiment is to measure the response to the stimulus by a test method.\n\nIn the design of experiments, two or more \"treatments\" are applied to estimate the difference between the mean responses for the treatments. For example, an experiment on baking bread could estimate the difference in the responses associated with quantitative variables, such as the ratio of water to flour, and with qualitative variables, such as strains of yeast. Experimentation is the step in the scientific method that helps people decide between two or more competing explanations – or hypotheses. These hypotheses suggest reasons to explain a phenomenon, or predict the results of an action. An example might be the hypothesis that \"if I release this ball, it will fall to the floor\": this suggestion can then be tested by carrying out the experiment of letting go of the ball, and observing the results. Formally, a hypothesis is compared against its opposite or null hypothesis (\"if I release this ball, it will not fall to the floor\"). The null hypothesis is that there is no explanation or predictive power of the phenomenon through the reasoning that is being investigated. Once hypotheses are defined, an experiment can be carried out and the results analysed to confirm, refute, or define the accuracy of the hypotheses.\n\nThe term \"experiment\" usually implies a controlled experiment, but sometimes controlled experiments are prohibitively difficult or impossible. In this case researchers resort to \"natural experiments\" or \"quasi-experiments.\" Natural experiments rely solely on observations of the variables of the system under study, rather than manipulation of just one or a few variables as occurs in controlled experiments. To the degree possible, they attempt to collect data for the system in such a way that contribution from all variables can be determined, and where the effects of variation in certain variables remain approximately constant so that the effects of other variables can be discerned. The degree to which this is possible depends on the observed correlation between explanatory variables in the observed data. When these variables are \"not\" well correlated, natural experiments can approach the power of controlled experiments. Usually, however, there is some correlation between these variables, which reduces the reliability of natural experiments relative to what could be concluded if a controlled experiment were performed. Also, because natural experiments usually take place in uncontrolled environments, variables from undetected sources are neither measured nor held constant, and these may produce illusory correlations in variables under study.\n\nMuch research in several science disciplines, including economics, political science, geology, paleontology, ecology, meteorology, and astronomy, relies on quasi-experiments. For example, in astronomy it is clearly impossible, when testing the hypothesis \"Stars are collapsed clouds of hydrogen\", to start out with a giant cloud of hydrogen, and then perform the experiment of waiting a few billion years for it to form a star. However, by observing various clouds of hydrogen in various states of collapse, and other implications of the hypothesis (for example, the presence of various spectral emissions from the light of stars), we can collect data we require to support the hypothesis. An early example of this type of experiment was the first verification in the 17th century that light does not travel from place to place instantaneously, but instead has a measurable speed. Observation of the appearance of the moons of Jupiter were slightly delayed when Jupiter was farther from Earth, as opposed to when Jupiter was closer to Earth; and this phenomenon was used to demonstrate that the difference in the time of appearance of the moons was consistent with a measurable speed.\n\nField experiments are so named to distinguish them from laboratory experiments, which enforce scientific control by testing a hypothesis in the artificial and highly controlled setting of a laboratory. Often used in the social sciences, and especially in economic analyses of education and health interventions, field experiments have the advantage that outcomes are observed in a natural setting rather than in a contrived laboratory environment. For this reason, field experiments are sometimes seen as having higher external validity than laboratory experiments. However, like natural experiments, field experiments suffer from the possibility of contamination: experimental conditions can be controlled with more precision and certainty in the lab. Yet some phenomena (e.g., voter turnout in an election) cannot be easily studied in a laboratory.\n\nAn observational study is used when it is impractical, unethical, cost-prohibitive (or otherwise inefficient) to fit a physical or social system into a laboratory setting, to completely control confounding factors, or to apply random assignment. It can also be used when confounding factors are either limited or known well enough to analyze the data in light of them (though this may be rare when social phenomena are under examination). For an observational science to be valid, the experimenter must know and account for confounding factors. In these situations, observational studies have value because they often suggest hypotheses that can be tested with randomized experiments or by collecting fresh data.\n\nFundamentally, however, observational studies are not experiments. By definition, observational studies lack the manipulation required for Baconian experiments. In addition, observational studies (e.g., in biological or social systems) often involve variables that are difficult to quantify or control. Observational studies are limited because they lack the statistical properties of randomized experiments. In a randomized experiment, the method of randomization specified in the experimental protocol guides the statistical analysis, which is usually specified also by the experimental protocol. Without a statistical model that reflects an objective randomization, the statistical analysis relies on a subjective model. Inferences from subjective models are unreliable in theory and practice. In fact, there are several cases where carefully conducted observational studies consistently give wrong results, that is, where the results of the observational studies are inconsistent and also differ from the results of experiments. For example, epidemiological studies of colon cancer consistently show beneficial correlations with broccoli consumption, while experiments find no benefit.\n\nA particular problem with observational studies involving human subjects is the great difficulty attaining fair comparisons between treatments (or exposures), because such studies are prone to selection bias, and groups receiving different treatments (exposures) may differ greatly according to their covariates (age, height, weight, medications, exercise, nutritional status, ethnicity, family medical history, etc.). In contrast, randomization implies that for each covariate, the mean for each group is expected to be the same. For any randomized trial, some variation from the mean is expected, of course, but the randomization ensures that the experimental groups have mean values that are close, due to the central limit theorem and Markov's inequality. With inadequate randomization or low sample size, the systematic variation in covariates between the treatment groups (or exposure groups) makes it difficult to separate the effect of the treatment (exposure) from the effects of the other covariates, most of which have not been measured. The mathematical models used to analyze such data must consider each differing covariate (if measured), and results are not meaningful if a covariate is neither randomized nor included in the model.\n\nTo avoid conditions that render an experiment far less useful, physicians conducting medical trials – say for U.S. Food and Drug Administration approval – quantify and randomize the covariates that can be identified. Researchers attempt to reduce the biases of observational studies with complicated statistical methods such as propensity score matching methods, which require large populations of subjects and extensive information on covariates. Outcomes are also quantified when possible (bone density, the amount of some cell or substance in the blood, physical strength or endurance, etc.) and not based on a subject's or a professional observer's opinion. In this way, the design of an observational study can render the results more objective and therefore, more convincing.\n\nBy placing the distribution of the independent variable(s) under the control of the researcher, an experiment – particularly when it involves human subjects – introduces potential ethical considerations, such as balancing benefit and harm, fairly distributing interventions (e.g., treatments for a disease), and informed consent. For example, in psychology or health care, it is unethical to provide a substandard treatment to patients. Therefore, ethical review boards are supposed to stop clinical trials and other experiments unless a new treatment is believed to offer benefits as good as current best practice. It is also generally unethical (and often illegal) to conduct randomized experiments on the effects of substandard or harmful treatments, such as the effects of ingesting arsenic on human health. To understand the effects of such exposures, scientists sometimes use observational studies to understand the effects of those factors.\n\nEven when experimental research does not directly involve human subjects, it may still present ethical concerns. For example, the nuclear bomb experiments conducted by the Manhattan Project implied the use of nuclear reactions to harm human beings even though the experiments did not directly involve any human subjects.\n\nThe experimental method can be useful in solving juridical problems.\n\n\n\n"}
{"id": "2312692", "url": "https://en.wikipedia.org/wiki?curid=2312692", "title": "Experimental data", "text": "Experimental data\n\nExperimental data in science and engineering is data produced by a measurement, test method, experimental design or quasi-experimental design. In clinical research any data produced are the result of a clinical trial. Experimental data may be qualitative or quantitative, each being appropriate for different investigations.\n\nGenerally speaking, qualitative data are considered more descriptive and can be subjective in comparison to having a continuous measurement scale that produces numbers. Whereas quantitative data are gathered in a manner that is normally experimentally repeatable, qualitative information is usually more closely related to phenomenal meaning and is, therefore, subject to interpretation by individual observers.\n\nExperimental data can be reproduced by a variety of different investigators and mathematical analysis may be performed on these data.\n\n\n"}
{"id": "37936179", "url": "https://en.wikipedia.org/wiki?curid=37936179", "title": "FRUCT", "text": "FRUCT\n\nThe Finnish-Russian University Cooperation in Telecommunication (FRUCT) is an independent Open Innovation Association developing ICT R&D ecosystem of Russia and Finland. In the beginning association was oriented on technologies of Nokia corporation. Association is known for its educational and R&D activities focused on improvement of the innovation ecosystem of Russia and Finland, competitiveness of the graduate students and development of cooperation between universities and industrial research groups.\n\nHistory of FRUCT Association started in 2007 when Sergey Balandin (Principal Scientist of Nokia Research Center), Eugeny Krouk (Professor of Saint Petersburg State University of Aerospace Instrumentation) and Jarkko Paavola (Professor of University of Turku) agreed to create joint work group to increase relevance and quality of students' research by involving IT and ICT experts from academia and industry to supervision of the projects. From the beginning FRUCT attracted attention of the regional media. The first seminar of the work group was held in St. Petersburg in May 2007, where the group was reorganized into the independent informal open innovations community. FRUCT Association is voluntary community that combines R&D forces of its members using Open Innovations principles to create critical mass of competences and world-class competitive results.\nNowadays more than twenty teams from universities of Russia, Finland, Ukraine and Denmark, , Nokia, Nokia Siemens Networks have joined FRUCT Association. The Association is represented and managed by companies FRUCT LLC (Russia) and FRUCT Oy (Finland). FRUCT Association is an official partner of IEEE Communications Society and a member of European Connected Health Alliance.\n\nFRUCT Association is an incubator of competences and new businesses developed in research cooperation of universities and industry. The main aim of FRUCT projects is gradual establishment of cooperation and long-term partnership between the member teams. FRUCT Association actively involves in research process undergraduates and PhD students, helps in formation of research communities in IT and ICT areas and promotes prestige of research career among young people. In most of FRUCT projects undergraduates and PhD students are the main developers, who solve problems with help and supervision of academia and industrial experts.\nThe set of activities is targeted to create the complete innovation cycle for competence and business development. The cycle starts by identification of the most promising research topics based on public information about long-term research vision and priorities of industrial leaders. Depending on the problem scale and long-team importance of the topic, FRUCT creates working group or project team to address it. Each group and project team is formed as a distributed R&D consortium with clear definition of member roles. The teams solve scientific and technological tasks based on own vision and industrial requests. Administratively project team members belong to regional FRUCT laboratories. FRUCT laboratories are organized at regional FRUCT universities. Most of the laboratory members are official employees of universities, which most of time working for FRUCT projects. After the required level of competences is gained the team either organizes innovation startup or start large research project by using regional and cross-border grants.\n\nIdeas of FRUCT projects can be originated by university professors and industrial experts. In this case the project initiator takes obligation to be the project supervisor. Also undergraduate and PhD students can propose topics of new projects. Student proposals are reviewed and commented by FRUCT advisory board. The preference is given to proposals that have large R&D potential and project targeted in development of practical applications and innovative software-hardware solutions. For approved proposals FRUCT appoints supervisors and scientific advisors.\nAccepted FRUCT projects can apply for support to cover part of direct implementation expenses, e.g., traveling to conferences, books, purchases of hardware, etc. The best students are recommended to take part in various contests, grant and scholarship programs, etc.\nThe project progress should be presented twice a year at FRUCT conferences. Project progress update consists of a report in form of conference paper, presentation and training or demo when applicable. As a result of each project at least one paper should be published in the proceedings of an international conference or journal. FRUCT conference has its own proceedings and the best papers from them are recommended for IEEE and journals publication. In addition, for papers accepted to publication in the recommended conferences, the main student author can apply for a grant to cover related travel expenses. Such travel grants are provided by FRUCT industrial members, e.g., Nokia Corporation.\n\nActualization and renewal of education is the key priority of FRUCT association. Association regularly organizes open free of charge conferences, trainings and special courses. The main FRUCT conferences are organized twice a year - in the last week of April (in Russia) and second week of November (in Finland). The conference program consists of technological trainings, lectures of world-class IT and ICT experts, reports of FRUCT teams and demos of developed solutions. The conference length is 5 days, including days for trainings and developer contests. Accepted conference papers are published in proceedings (), which are also available for free download at FRUCT site.\nFRUCT organizes free-of-charge winter and summer schools (duration 1–3 weeks). Main topics of the schools are chosen accordingly with the industrial trends and regional FRUCT teams priorities. FRUCT cooperates and is a partner in many international educational programs such as PERCCOM.\nActive academic exchange between association members is used for popularizations of new disciplines and courses, plus organization of open lectures. When it is appropriate the academic course is transformed to intensive 3–7 days course. With the help of the industrial partners best students are getting material support to participate in such programs.\nFRUCT supporting development of the exchange programs between FRUCT member universities. For example, international master programs in IT field of Tampere University of Technology and Erasmus Mundus NordSecMob of .\n\nFRUCT supports development and coordinates work of 4 professional communities: Russian Mobile Linux Community, Russian Qt community, Regional mobile Healthcare (m-Health) community and regional IoT and Smart Spaces community «Are You Smart».\nThe network of professional communities is implemented by FRUCT subsidiary E-WeREST (acronym of East-West Research and Education Society on Telecommunications). Regional activities of E-WeREST communities are coordinated by FRUCT labs at , LETI, YarSU, NNGU, SUAI and other universities.\n\n\n"}
{"id": "5337329", "url": "https://en.wikipedia.org/wiki?curid=5337329", "title": "FSU Young Scholars Program", "text": "FSU Young Scholars Program\n\nFSU Young Scholars Program (YSP) is a six-week residential science and mathematics summer program for 40 Florida high school students with significant potential for careers in the fields of science, technology, engineering, and mathematics. The program was developed in 1983 and is currently administered by the Office of Science Teaching Activities in the College of Arts and Sciences at Florida State University.\n\nEach young scholar attends a total of three courses in the fields of mathematics, science, and computer programming. The courses are designed specifically for this program—they are neither high school nor college courses.\n\nEach student who attends YSP is assigned an Independent Research Project (IRP) based on his or her interests. Students join the research teams of FSU professors, participating in scientific research for two days each week. The fields of study available range from robotics, molecular biology, chemistry, geology, physics, to zoology. At the conclusion of the program, students present their projects in an academic conference, documenting their findings and explaining their projects to both students and faculty.\n\nThe program is co-directed by Dr. Erica Staehling, Barbara Shoplock, and Teresa Callahan, members of the Office of Science Teaching Activities, Florida State University. Instructional faculty includes Dr. Steve Blumsack, Emeritus Professor of Mathematics; Dr. Dan Oberlin, Professor of Mathematics; Drs. Harrison Prosper and Horst Wahl, Professors of Physics; and Dr. Lloyd Epstein, Professor of Biological Science. Twenty additional Florida State science and engineering faculty members mentor the students in their independent research projects.\n\nYSP admits students who have completed the eleventh grade in a Florida public or private high school. A few exceptionally qualified and mature tenth graders have been selected in past years, though this is quite rare.\n\nAll applicants must have completed Pre-Calculus and maintain at least a 3.0 unweighted GPA to be considered for acceptance. Additionally, students must have scored at the 90th percentile or better in science or mathematics on a nationally standardized exam, such as the SAT, PSAT, ACT, or PLAN. Students are required to submit an application package, including high school transcripts and a letter of recommendation.\n\nSelection is extremely competitive, as there are typically over 200 highly qualified applicants competing for only 40 positions. The majority of past participants graduated in the top ten of their respective high school classes, with over 25% of students entering their senior year ranked first in their class. The average PSAT score of past young scholars was in the 97th percentile in math and 94th percentile in critical reading nationally.\n\n"}
{"id": "12684302", "url": "https://en.wikipedia.org/wiki?curid=12684302", "title": "Fauna Europaea", "text": "Fauna Europaea\n\nFauna Europaea is a database of the scientific names and distribution of all living multicellular European land and fresh-water animals. It serves as a standard taxonomic source for animal taxonomy within the Pan-European Species directories Infrastructure (PESI).\n\nIts construction was initially funded by the European Council (2000–2004). The project was co-ordinated by the University of Amsterdam which launched the first version in 2004, after which the database was transferred to the Natural History Museum Berlin in 2015.\n\n"}
{"id": "39043368", "url": "https://en.wikipedia.org/wiki?curid=39043368", "title": "Germ theory denialism", "text": "Germ theory denialism\n\nGerm theory denialism is the belief that germs do not cause infectious disease, and that the germ theory of disease is wrong. It usually involves arguing that Louis Pasteur's model of infectious disease was wrong, and that Antoine Béchamp's was right. In fact, its origins are rooted in Béchamp's empirically disproved (in the context of disease) theory of pleomorphism. Another obsolete variation is known as terrain theory and postulates that diseased tissue attracts germs rather than being caused by it.\n\nGerm theory denialism (GTD) is as old as germ theory itself beginning with the rivalry of Pasteur and Béchamp. Pasteur's work in preventing beverage contamination led him to discover that it was due to microorganisms and led him down the path to be the first scientist to show the theory was valid and popularize it in Europe. Although he was not the first to have the idea and scientists such as Girolamo Fracastoro (had the idea that fomites could harbor the seeds of contagion), Agostino Bassi (discovered the muscardine disease of silkworms was caused by a fungus that was named Beauveria bassiana), Friedrich Henle (developed the concepts of contagium vivum and contagium animatum), and others had earlier proposed ideas similar to germ theory.\n\nBéchamp strongly contested this view offering up a competing idea known as the pleomorphic theory of disease. This theory says that all life is based on forms that a certain class of organisms take during stages of their life-cycles and that germs are attracted to the environment of diseased tissue rather than being the cause of it. Proponents of this idea insist microbes that live in an organism go through the same stages of their development. According to Günther Enderlein they are as follows: Colloid — microbe (primitive phase), bacteria (middle phase), and fungus (end phase).\n\nEarlier non-germ theories, in addition to the earlier idea of miasma, focused on spontaneous generation - the idea that living matter could arise from non-living - and the terrain theory variation of Béchamp's ideas. Pasteur disproved spontaneous generation with a series of experiments in the 1870s. However, understanding the cause of a sickness does not always immediately lead to effective treatment of sickness and the great decline in mortality during the 19th century is mostly associated with improvement in hygiene and cleanliness. In fact, one of the first movements to deny the germ theory was the Sanitary Movement, and was nevertheless central in developing America's public health infrastructure. By providing clean water and sanitation there was less of an environment for pathogens to develop and mortality rates fell dramatically.\n\nGTD has significant overlap with chiropractic practice. Many chiropractors believe immunity to be a function of spine alignment and the brain's ability to communicate efficiently with the body and that it has little to nothing to do with external pathogens.\n\nA common thread among many alternative medicine proponents is opposition to vaccines and many use GTD to justify their claims. Germ theory deniers make many claims about the biological underpinnings of the theory and the historical record that are at odds with what is accepted by most modern scientists and historians. Another popular claim from the anti-vaccine community is that all diseases are caused by toxemia due to inadequate diet and health practices.\n\nHarriet Hall published an article in \"Skeptic\" where she describes her experience arguing with germ theory deniers.\n\nMembers of the medical community that are also skeptics, such as David Gorski and Steven Novella, point out that denying germ theory is counter to years of experiments and the prevailing opinion of most doctors and scientists.\n\n"}
{"id": "177793", "url": "https://en.wikipedia.org/wiki?curid=177793", "title": "Great chain of being", "text": "Great chain of being\n\nThe Great Chain of Being is a strict hierarchical structure of all matter and life, thought in medieval Christianity to have been decreed by God. The chain starts with God and progresses downward to angels, demons (fallen/renegade angels), stars, moon, kings, princes, nobles, commoners, wild animals, domesticated animals, trees, other plants, precious stones, precious metals and other minerals.\n\nThe Great Chain of Being (, \"Ladder of Being\") is a concept derived from Plato, Aristotle (in his \"Historia Animalium\"), Plotinus and Proclus. Further developed during the Middle Ages, it reached full expression in early modern Neoplatonism.\n\nThe Chain of Being is composed of a great number of hierarchical links, from the most basic and foundational elements up through the very highest perfection: God.\n\nGod sits at the top of the chain, and beneath him sit the angels, both existing wholly in \"spirit\" form. Earthly flesh is fallible and ever-changing, mutable. Spirit, however, is unchanging and permanent. This sense of permanence is crucial to understanding this conception of reality. It is generally impossible to change the position of an object in the hierarchy. (One exception might be in the realm of alchemy, where alchemists attempted to transmute base elements, such as lead, into higher elements, either silver or, more often, gold—the highest \"element\".)\n\nIn the natural order, earth (rock) is at the bottom of the chain; this element possesses only the attribute of existence. Each link succeeding upward contains the positive attributes of the previous link and adds at least one other. Rocks possess only existence; the next link up is plants which possess life \"and\" existence. Animals add motion and appetite as well.\n\nMan is both mortal flesh, as those below him, and also spirit, as those above. In this dichotomy, the struggle between flesh and spirit becomes a moral one. The way of the spirit is higher, more noble; it brings one closer to God. The desires of the flesh move one away from God. The Christian fall of Lucifer is thought of as especially terrible, as angels are wholly spirit, yet Lucifer defied God (who is the ultimate perfection).\n\nEach link in the chain might be divided further into its component parts. In medieval secular society, for example, the king is at the top, succeeded by the aristocratic lords and the clergy, and then the peasants below them. Solidifying the king's position at the top of humanity's social order is the doctrine of the Divine Right of Kings. The implied permanent state of inequality became a source of popular grievance, and led eventually to political change as in the French Revolution. In the family, the father is head of the household; below him, his wife; below her, their children.\n\nMilton's \"Paradise Lost\" ranked the angels (c.f. Pseudo-Dionysius the Areopagite's ranking of angels), and Christian culture conceives of angels in orders of archangels, seraphim, and cherubim, among others.\n\nSubdivisions are equally apparent among animals. At the top of the animals are wild beasts (such as lions), which were seen as superior as they defied training and domestication. Below them are domestic animals, further sub-divided so that useful animals (such as dogs and horses) are higher than docile creatures (such as sheep). Birds are also sub-divided, with eagles above pigeons, for example. Fish come below birds and are subdivided between actual fish and other sea creatures. Below them come insects, with useful insects such as spiders and bees and attractive creatures such as ladybirds and dragonflies at the top, and unpleasant insects such as flies and beetles at the bottom. At the very bottom of the animal sector are snakes, which are relegated to this position as punishment for the serpent's actions in the Garden of Eden.\n\nBelow animals comes the division for plants, which is further subdivided. Trees are at the top, with useful trees such as oaks at the top, and the traditionally demonic yew tree at the bottom. Food-producing plants such as cereals and vegetables are further subdivided.\n\nAt the very bottom of the chain are minerals. At the top of this section are metals (further sub-divided, with gold at the top and lead at the bottom), followed by rocks (with granite and marble at the top), soil (subdivided between nutrient-rich soil and low-quality types), sand, grit, dust, and dirt at the very bottom of the entire great chain.\n\nThe central concept of the Chain of Being is that everything imaginable fits in somewhere, giving order and meaning to the universe.\n\nGod is at the top of the chain and is also external to creation. God is believed to exist outside the physical limitations of time and space. He possessed the spiritual attributes of reason, love, and imagination, like all spiritual beings, but he alone possessed the divine attributes of omnipotence, omniscience, and omnipresence. God serves as the model of authority for the strongest, most virtuous, most excellent type of being within any category.\n\nAngels were beings of pure spirit who had no physical bodies of their own. In order to affect the physical world, angels were thought to build temporary bodies for themselves out of particles of earthly elements. Medieval and Renaissance theologians believed angels to possess reason, love, imagination, and, like God, to stand outside the physical limitations of time. They possessed sensory awareness unbound by physical organs, and they possessed language. They lacked, however, the divine attributes of omnipotence, omniscience, and omnipresence of God, and they simultaneously lacked the physical passions experienced by humans and animals. Depending upon the author, the class of angels was further subdivided into three, seven, nine, or ten ranks, variously known as triads, orders, or choirs. Each rank had greater power and responsibility than the entities below them. The most common classification is that of St. Thomas Aquinas. \n\n\nFor Medieval and Renaissance thinkers, humans occupied a unique position on the Chain of Being, straddling the world of spiritual beings and the world of physical creation. Humans were thought to possess divine powers such as reason, love, and imagination. Like angels, humans were spiritual beings, but unlike angels, human souls were \"knotted\" to a physical body. As such, they were subject to passions and physical sensations—pain, hunger, thirst, sexual desire—just like other animals lower on the Chain of Being. They also possessed the powers of reproduction unlike the minerals and rocks lowest on the Chain of Being. Humans had a particularly difficult position, balancing the divine and the animalistic parts of their nature. For instance, an angel is only capable of intellectual sin such as pride (as evidenced by Lucifer's fall from heaven in Christian belief). Humans, however, were capable of both intellectual sin and physical sins such as lust and gluttony if they let their animal appetites overrule their divine reason. Humans also possessed sensory attributes: sight, touch, taste, hearing, and smell. Unlike angels, however, their sensory attributes were limited by physical organs (they could only know things discerned through the five senses). The highest-ranking human being was the king.\n\nAnimals, like humans higher on the chain, were animated (capable of independent motion). They possessed physical appetites and sensory attributes, the number depending upon their position within the Chain of Being. They had limited intelligence and awareness of their surroundings. Unlike humans, they were thought to lack spiritual and mental attributes such as immortal souls and the ability to use logic and language. The primate of all animals (the \"king of beasts\") was variously thought to be either the lion or the elephant. However, each subgroup of animals also had its own primate, an avatar superior in qualities of its type.\n\n\nNote that avian creatures, linked to the element of air, were considered superior to aquatic creatures linked to the element of water. Air naturally tended to rise and soar above the surface of water, and analogously, aerial creatures were placed higher in the chain.\n\n\nThe chart would continue to descend through various reptiles, amphibians, and insects. The higher up the chart one went, the more noble, mobile, strong, and intelligent the creature in Renaissance belief. At the very bottom of the animal section, we find sessile creatures like the oysters, clams, and barnacles. Like the plants below them, these creatures lacked mobility, and were thought to lack various sensory organs such as sight and hearing. However, they were still considered superior to plants because they had tactile and gustatory senses (touch and taste).\n\nPlants, like other living creatures, possessed the ability to grow in size and reproduce. However, they lacked mental attributes and possessed no sensory organs. Instead, their gifts included the ability to eat soil, air, and \"heat.\" Plants did have greater tolerances for heat and cold, and immunity to the pain that afflicts most animals. At the very bottom of the botanical hierarchy, fungi and mosses, lacking leaf and blossom, were so limited in form that Renaissance thinkers thought them scarcely above the level of minerals. However, each plant was also thought to be gifted with various edible or medicinal virtues unique to its own type.\n\n\nCreations of the earth, the lowest of elements, all minerals lacked the plant's basic ability to grow and reproduce. They also lacked mental attributes and sensory organs found in beings higher on the chain. Their unique gifts, however, were typically their unusual solidity and strength. Many minerals, in fact, were thought to possess magical powers, particularly gems. The mineral primate is the diamond.\n\n\nThe basic idea of a ranking of the world's organisms goes back to Aristotle's biology. In his \"History of Animals\", where he ranked animals over plants based on their ability to move and sense, and graded the animals by their reproductive mode and possession of blood (he ranked all invertebrates as \"bloodless\").\n\nAristotle's non-religious concept of higher and lower organisms was taken up by natural philosophers during the Scholastic period to form the basis of the \"Scala Naturae\". The \"scala\" allowed for an ordering of beings, thus forming a basis for classification where each kind of mineral, plant and animal could be slotted into place. In medieval times, the great chain was seen as a God-given ordering: God at the top, dirt at the bottom, every grade of creature in its place. Just as rock never turns to flowers and worms never turn to lions, humans never turn to angels. This was not our lot in life. In the Northern Renaissance, the scientific focus shifted to biology. The threefold division of the chain below humans formed the basis for Linnaeus's \"Systema Naturæ\" from 1737, where he divided the physical components of the world into the three familiar kingdoms of minerals, plants and animals.\n\nThe set nature of species, and thus the absoluteness of creatures' places in the great chain, came into question during the 18th century. The dual nature of the chain, divided yet united, had always allowed for seeing creation as essentially one continuous whole, with the potential for overlap between the links. Radical thinkers like Jean-Baptiste Lamarck saw a progression of life forms from the simplest creatures striving towards complexity and perfection, a schema accepted by zoologists like Henri de Blainville. The very idea of an ordering of organisms, even if supposedly fixed, laid the basis for the idea of transmutation of species, for example Charles Darwin's theory of evolution.\n\nThe Chain of Being continued to be part of metaphysics in 19th century education, and the concept was well known. The geologist Charles Lyell used it as a metaphor in his 1851 \"Elements of Geology\" description of the geological column, where he used the term \"missing links\" in relation to missing parts of the continuum. The term \"missing link\" later came to signify transitional fossils, particularly those bridging the gulf between man and beasts.\n\nThe idea of the great chain as well as the derived \"missing link\" was abandoned in early 20th century science, as the notion of modern animals representing ancestors of other modern animals was abandoned in biology. The idea of a certain sequence from \"lower\" to \"higher\" however lingers on, as does the idea of progress in biology.\n\nAllenby and Garreau propose the Catholic Church's narrative of the Great Chain of Being kept the peace for centuries in Europe. The very concept of rebellion simply lay outside the reality within which most people lived for to defy the King was to defy God. King James I himself wrote, \"The state of monarchy is the most supreme thing upon earth: for kings are not only God's Lieutenants upon earth, and sit upon God's throne, but even by God himself they are called Gods.\"\n\nThe Enlightenment broke this supposed divine plan and fought the last vestiges of feudal hierarchy by creating secular governmental structures that vested power into the hands of ordinary citizens rather than divinely ordained monarchs.\n\nHowever, scholars such as Brian Tierney and Michael Novak have noted the medieval contribution to democracy and human rights.\n\nThe American spiritual writer and philosopher Ken Wilber uses a concept called the \"Great Nest of Being\" which is similar to the Great Chain of Being, and which he claims to belong to a culture-independent \"perennial philosophy\" traceable across 3000 years of mystical and esoteric writings. Wilber's system corresponds with other concepts of transpersonal psychology.\n\nIn the 1977 book \"A Guide for the Perplexed\", British philosopher and economist E. F. Schumacher wrote that fundamental gaps exist between the existence of minerals, plants, animals and humans, where each of the four classes of existence is marked by a level of existence not shared by that below. Clearly influenced by the great chain of being, but lacking the angels and God, he called his hierarchy the \"levels of being\". In the book, he claims that science has generally avoided seriously discussing these discontinuities, because they present such difficulties for strictly materialistic science, and they largely remain mysteries.\n\n"}
{"id": "48684094", "url": "https://en.wikipedia.org/wiki?curid=48684094", "title": "I Look Like an Engineer", "text": "I Look Like an Engineer\n\nThe I Look Like an Engineer movement was created in August 2015 by software developer Isis Anchalee (formerly Isis Wenger) as a response to the backlash the OneLogin recruitment ad in which she was featured received. The movement aspired to break the stereotypes and promote diversity around unrepresented groups, particularly (Women, POC, and LGBTQ+) within the engineering industry. Its primary tactic is the use of the hashtag #ILookLikeAnEngineer on social media sites such as Twitter, Facebook, and Instagram, along with pictures of engineers or engineering students.\n\nThe I Look Like an Engineer movement has sparked other similar movements that also seek to break stereotypes in their industry, such as I Look Like a Surgeon, I Look like a Professor and I Look Like a Civil Engineer.\n\nIn the summer of 2015 OneLogin, a software company in San Francisco created a recruitment campaign aimed at attracting engineers to their home office in San Francisco. Four employees were invited to participate, including Anchalee. The ads were placed in the BART public transit stations and showed several OneLogin engineers sharing their experience working for the company. The ad featuring Anchalee went viral on several social media sites. A week after the launch of the campaign went viral, OneLogin posted an article on their blog that talked about the importance of diversity, inclusion, and innovation.\n\nThe OneLogin recruitment ad featuring Isis Anchalee went viral as her particular ad received comments stating the belief that she was a model and not an actual engineer. Anchalee took to social media where she posted a picture of herself holding a piece of paper describing her job and a caption with the hashtag, #ILookLikeanEngineer. In her post she stated her belief that it is important to raise awareness in tech diversity and break the stereotypes of what an engineer should look like.\n\nHer post started the hashtag trend and the hashtag was used 86,000 times by August 7, 2015. The hashtag has been used in approximately 50 countries. The hashtag is mainly used by women and LGBTQ engineers. Subsequently, Anchalee put up a now-defunct webpage to establish a safe platform for individuals to share their experiences related to diversity issues within tech fields.\n\nIn an effort to make a lasting impact, Michelle Glauser (the spouse of Anchalee co-worker) began a fundraising campaign using Indiegogo to create billboards with pictures that people had shared on social media using the hashtag #ILookLikeanEngineer. The proceeds were used to put up more billboards to further the I Look Like an Engineer campaign and excess proceeds were used to fund organizations that teach programming to minorities. The fundraiser’s goal was to raise $3,500. The campaigned ended on September 5, 2015 with $47,285 raised.\n\nConcurrently, an #ILookLikeanEngineer community gathering organized by Michelle Glauser through Eventbrite as part of efforts to continue further the movement was hosted on August 13, 2015 in San Francisco, CA. During the gathering, photographers collected portraits of willing participants for the billboards and as an effort to document the event. The event was sponsored by Segment, Rackspace, OneLogin, and HackBright Academy. The event included networking, discussions and a Q&A panel which included Isis Wenger, Alicia Morga, Wayne Sutton, Erica Baker, Leslie Miley, and Dom DeGuzman.\n\nThe struggles that women face in the field of engineering have a long-documented history. In the postwar era, female engineering students found themselves in a mixture of conflicting and positive situations that shaped not only their professional experiences but the field itself. Women faced backlash from not just male students and professors who objectified and viewed female engineers as potential girlfriends instead of fellow students, but also from women faculty or older female students who demanded that younger women endured the same struggles they did.\n\nThe campus and corporate climate did not change until the mid-1970s when companies began to face civil rights pressure to be more inclusive of female engineers. However, even with these radical changes women continued to experience subtle and overt forms of discrimination in the workplace. During the 1970s, organizations like the Society of Women Engineers (founded in 1952) among others provided women with support and guidance both on college campuses and in the corporate world.\n\nRecognizing the challenges that women faced and continue to face in male-dominated fields raises awareness about the importance of diversity and intersectionality in the field of engineering and other tech fields. Social movements are taking place in online spaces more frequently as the community of users tends to be wider and more diverse. It is here where users can use their own personal experiences and public outcry to start global dialogues that can enact change.\n\n"}
{"id": "1629075", "url": "https://en.wikipedia.org/wiki?curid=1629075", "title": "Ignition tube", "text": "Ignition tube\n\nAn ignition tube is a piece of laboratory equipment. It is a laboratory tube used much in the same way as a boiling tube except not being as large and thick-walled. It is primarily used to hold small quantities of substances which are undergoing direct heating by a Bunsen burner or other heat source. \n\nIgnition tubes are used in the sodium fusion test.\n\nIgnition tubes are often difficult to clean due to the small bore. When used to heat substances strongly, some char may stick to the walls as well. They are usually disposable.\n\n"}
{"id": "11336178", "url": "https://en.wikipedia.org/wiki?curid=11336178", "title": "Knudsen cell", "text": "Knudsen cell\n\nIn crystal growth, a Knudsen cell is an effusion evaporator source for relatively low partial pressure elementary sources (e.g. Ga, Al, Hg, As). Because it is easy to control the temperature of the evaporating material in Knudsen cells, they are commonly used in molecular-beam epitaxy.\n\nThe Knudsen effusion cell was developed by Martin Knudsen (1871-1949). A typical Knudsen cell contains a crucible (made of pyrolytic boron nitride, quartz, tungsten or graphite), heating filaments (often made of metal tantalum), water cooling system, heat shields, and an orifice shutter.\n\nThe Knudsen cell is used to measure the vapor pressures of a solid with very low vapor pressure. Such a solid forms a vapor at low pressure by sublimation. The vapor slowly effuses through the pinhole, and the loss of mass is proportional to the vapor pressure and can be used to determine this pressure. The heat of sublimation can also be determined by measuring the vapor pressure as a function of temperature, using the Clausius–Clapeyron relation.\n"}
{"id": "70111", "url": "https://en.wikipedia.org/wiki?curid=70111", "title": "Land lab", "text": "Land lab\n\nA land lab is an area of land that has been set aside for use in biological studies. Thus, it is literally an outdoor laboratory based on an area of land.\n\nStudies may be elementary or advanced. For instance, students may simply be given the task of identifying all the tree species in a land lab, or an advanced student may be doing an intensive survey of the microbial life forms found in a soil sample.\n\nLand labs are often marked out in plots or transects for studies. A plot may be any size, usually marked out in square meters. This allows for more intensive, delimited studies of changes and inventories of biota. Transects are straight lines at which, at intervals, measurements are taken for a profile of the ecological community.\n"}
{"id": "47338914", "url": "https://en.wikipedia.org/wiki?curid=47338914", "title": "Liber de orbe", "text": "Liber de orbe\n\nLiber de orbe was a Latin translation made in 1130s CE of an Arabic work attributed to the 8th century astrologer Mashallah ibn Athari. \n\nThe work's main topic is cosmology and is considered as one of the earliest works on Aristotelian physics available in Latin.\n\n"}
{"id": "22515676", "url": "https://en.wikipedia.org/wiki?curid=22515676", "title": "List of fossil primates", "text": "List of fossil primates\n\nThis is a list of fossil primates—extinct primates for which a fossil record exists. Primates are generally thought to have evolved from a small, unspecialized mammal, which probably fed on insects and fruits. However, the precise source of the primates remains controversial and even their arboreal origin has recently been questioned. As it has been suggested, many other mammal orders are arboreal too, but they have not developed the same characteristics as primates. Nowadays, some well known genera, such as \"Purgatorius\" and \"Plesiadapis\", thought to be the most ancient primates for a long time, are not usually considered as such by recent authors, who tend to include them in the new order Plesiadapiformes, within superorder Euarchontoglires. Some, to avoid confusions, employ the unranked term Euprimates, which excludes Plesiadapiformes. That denomination is not used here.\n\nThere is an academic debate on the time the first primates appeared. One of the earliest probable primate fossils is the problematic \"Altiatlasius koulchii\", perhaps an Omomyid, but perhaps a non-Primate Plesiadapiform, which lived in Morocco, during the Paleocene, around 60 Ma. However, other studies, including molecular clock studies, have estimated the origin of the primate branch to have been in the mid-Cretaceous period, around 85 Ma, that is to say, in the time previous to the extinction of dinosaurs and the successful mammal radiation. Nevertheless, there seems to be a consensus about the monophyletic origin of the order, although the evidence is not clear. There are no fossils known that can be directly linked to the living African apes, nor any that could be considered representative of the last common ancestor between them and humans.\n\nThe order Primates, established by Linnaeus in 1758, includes humans and their immediate ancestors. However, contrarily to the common opinion, most primates do not have especially large brains. Brain size is a derived character, which only appeared with genus \"Homo\", and was lacking in the first hominid. In fact, hominid encephalization quotient is only 1.5 Ma more recent than that of some dolphin species. The encephalization quotient of some cetaceans is therefore higher than that of most primates, including the nearest relatives of humans, such as \"Australopithecus\".\n\nThis list follows partly from Walter Carl Hartwig's 2002 book \"The Fossil Primate Record\" and John G. Fleagle's 2013 book \"Primate Adaptation and Evolution\" (3rd edition). Parentheses around authors' names (and dates) indicates a change in generic name for the fossil, as stated in the International Code of Zoological Nomenclature (ICZN). Since the publication of the book as well as the creation of this article, new fossil taxon have been discovered that has helped improved the taxonomy among primates in general.\n\n\nSubfossil lemurs:\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe following is a list of books that provide useful reviews or overviews of primate fossil histories, including (e.g.) diagrams, photos and good referencing.\n\n\n"}
{"id": "42335532", "url": "https://en.wikipedia.org/wiki?curid=42335532", "title": "List of regimes", "text": "List of regimes\n\nThis list of regimes lists the results of regime-classification schemes from political science literature, including Polity data series and the Democracy-Dictatorship Index.\n\n"}
{"id": "53941711", "url": "https://en.wikipedia.org/wiki?curid=53941711", "title": "Little Science, Big Science", "text": "Little Science, Big Science\n\nLittle Science, Big Science is a book of collected lectures given by Derek J. De Solla Price, first published in 1963. The book presents the 1962 Brookhaven National Laboratory Pegram Lectures, a series of lectures dedicated to discussing science and its place in society. Price's goal in the lectures is to outline what it may look like for science to be analysed scientifically, by applying methods of measuring, hypothesizing, and deriving to science itself. With this goal in mind, he sets out to define quasi-mathematically how the shape and size of science has shifted from \"small science\" to \"big science\" in a historical and sociological way. Price presents a quantification of science as a measurable entity via an analogy to thermodynamics, conceptualizing science like a gas with individual molecules possessing individual velocities and interactions, a total volume, and general properties or laws.\n\nPrice begins the lectures by setting forth a demarcation in science centered around the modern period. He describes the phenomenon that, at the time of the lectures, 80 to 90 percent of important scientific work had occurred in one normal human life span. With this facet in mind, he sets out to describe the development of the term \"Big Science,\" as coined by Alvin M. Weinberg in 1961. As a general directive, he seeks to show that the transition from \"Little Science\" to \"Big Science,\" specifically the socio-economic and methodological changes to science in the 20th century, have been mostly gradual. To illustrate this point, he presents empirical statistical evidence from various aspects and fields of science, all of which show that the mode of growth of science is exponential, growing at compound interest. This assertion Price claims is the \"fundamental law of any analysis of science,\" stating that it even holds accurately over long time periods. With this fundamental law in mind, he states that for general measures the size of science in manpower or number of publications doubles in size every 10 to 15 years. If this rate of expansion is considered broadly, then from the 1600s until now such size measures of science have increased by a factor of 10. From this observation, Price moves to describe the \"coefficient of immediacy:\" the number of scientists alive compared to the number of scientists who have ever been, a ratio or percentage he states as 7:8 and 87.5% respectively. This measure serves to show numerically how the majority of important science has taken place within the average human life span at the time of the lecture presentation. As a result of the consistent exponential growth rate and immediacy of science, the statement that the majority of scientists throughout history are alive at any given moment must be consistent throughout history as well, meaning that in 1700 the majority of all scientists ever were alive, true also for 1800 and 1900 and so on. As a result of this facet, Price states that science has been constantly exploding into the population, increasing its size at a rate faster than the increase of total humans able to conduct it.\n\nHowever, Price asserts that this exponential growth rate cannot simply explain the transition from \"Little Science\" to \"Big Science,\" as the constant growth would not make the modern period under question any more likely to produce \"Big Science\" than any other. He conjectures that two statistical phenomena hold true for science generally, that individual metrics of science may grow at rates different from that of the exponential growth, and that the exponential growth rate may be starting to diminish. In response to his second point, he claims that the normal exponential growth may give way to a logistic growth rate, growing exponentially until it reaches a maximum size and then ceasing to grow. The possibility that science follows a rate of growth modeled by a logistic curve is suggested further by the fact that if science had continued to grow at an exponential rate in 1962, then by now there would be more scientists than people. With his claim that the growth rate actually observes a logistic curve, he provides a second basic law of the analysis of science, namely that the exponential growth rates previously mentioned must be in fact logistic. If this claim is correct, then the exponential growth rate previously observed must break down at a point in the future, and Price implies as a conclusion to this section that the onset of this breakdown may be associated with an upper bound to the size of science brought on by \"Big Science.\"\n\nIn this chapter, Price suggests various ideas and methods about conducting a science of science, or scientometrics, by first narrating some peculiar contributions to statistics made by Francis Galton. His overall goal is to further the possibility of applying scientific methods to science itself by suggesting various metrics and measures of the size, growth rate, and distribution of science. He focuses on Galton's work concerning the distribution of high achieving scientists and statesmen in the upper echelons of British society, specifically \"Hereditary Genius\" and \"English Men of Science\". These works are reviewed with the goal of understanding a basic metric for the number of people or papers in science that reach different levels of quality, an idea basic in Price's formulation of scientometrics. Further, he suggests that understanding such a metric would allow predictions to be made of science and scientists when changes associated with Big Science arrive. Galton's original approach was to estimate the distribution of high achieving practitioners of science among the eminent parts of British society, and Price takes this as a starting step in grasping a scientific metric of the productivity of science. In analyzing Galton's work and the work of another statistics researcher, Alfred J. Lotka, Price suggests that there may be a rough inverse-square law of productivity. Price moves next to define a quantity he calls someone's \"solidness\" \"s\", as the logarithm of the total papers published in one scientist's life. Keeping in mind the previous productivity law, for each unit increase in a scientist's solidness, the total number of scientists of that solidness decreases at a constant rate. With these two observations, among others, Price asserts that the foundations for an econometric-like study of science have been suggested, with the analysis of time series suggesting exponential or logistic growth and the distribution law of scientific productivity comprising them. He concludes by suggesting that these distributions and analyses contain errors relating to the non-uniform distribution of scientists across populations, noting that they tend to congregate in certain fields, institutions, countries, and journals. In keeping with his gas analogy, he maintains that just as one cannot measure the exact positions and velocities of gas molecules, one cannot pinpoint the exact productivity or contribution levels of individual scientists within science.\n\nThis chapter serves multiple purposes but overall achieves the same goal as the previous, providing a further conception of the productivity measure in science. This conclusion is reached through defining historically, sociologically, and from a communications perspective what a scientific paper is for, specifically what the purpose of this form of scientific communication is. To begin this analysis, he begins by looking at the history of the scientific paper, tracing its original purpose to discovering what was of interest within scientific practice. With the emergence of this scientific social practice, seen not as a means of publishing new knowledge but of communication between practitioners, the process of situating papers within the general body of literature came in to play. Specifically, each scientific paper is built from the foundation created by all previous papers, and with this facet exists a possibility of quantifying this foundation, the citation of references. With the idea that scientific papers were a social device of scientific communication, Price suggests that the driving force behind their emergent usage was the ability to assert and claim intellectual property within science. The possibility of communicating priority in disputes over scientific discoveries promoted the scientific paper as the best means of communication, leaving the information dissemination quality of papers as incidental in their overall purpose. With the quantification of scientific productivity by citation number and rate, there arrives a metric in science that gives the scientific importance of an individual's work or journal as its total usage within scientific practice, its total citations or references in other papers or journals. With this in mind, Price observes the fact that the total number of scientific references at a specific date across science is proportional to the total literature available within science at that date.\n\nMoving from the ability of scientific papers to facilitate communication and interactions between scientists, Price outlines an idea that allows further maximization of interactions between scientists. His term for this organizational method is the \"invisible college,\" specifically the circuit of institutions, research centers, journals, and conferences that allow intermingling and interactions within specific fields of science. Groups of scientists naturally form as a result of collaborations between individuals focusing on similar problems, but the ability for researchers to move around the globe in order to achieve interpersonal relationships with their fellow researchers is what Price suggests maximizes the group size able to keep up regular productive interactions. Thus Price defines the sociological structure of scientific practice communicating through published papers.\n\nThe final section of the lectures focuses on a larger-picture analysis of science and the monetary trends within it. As a general first statement, Price proposes that the cost of science has been increasing proportional to the square of the number of scientists. He points out that the cost of research in terms of the GDP did not increase in the years preceding World War II, yet afterward began increasing at the rate previously mentioned. As research amounts increase, the current and necessary number of researchers increases, promoting the inducement of scientists with higher salaries and better facilities in turn increasing the overall costs of science. Price suggests that it is this feedback loop that is a potential decelerator for the growth of science, and the main difference between Little Science and Big Science. What follows is his analysis of the \"explosion of science\" within non-developed countries, specifically Japan. He shows through this analysis that the United States' lack of experience of this explosion of science within the 20th century up to this point is due to the saturation of society with the activities of science, nearing costs not maintainable by the country. In countries where science has yet reached an exponential growth curve, this saturation is not present which allows the growth rate to set out at an exponential pace.\n\nThe final conceptual measure that Price offers is the idea of the \"mavericity\" of a scientist, or the likelihood that an individual will test new and unique combinations of theories and experiments unexpected in the current literature. The reactions and interactions within science to this mavericity also characterizes Big Science over Little Science, where the former serves to limit and restrain the most maverick investigators due to collaborative work and specific directed goals for scientific research. Thus the emergence of Big Science not only influences the growth rate, connectedness, and significance of science, but also the individual facets of the scientific pursuit.\n"}
{"id": "8956567", "url": "https://en.wikipedia.org/wiki?curid=8956567", "title": "Material World (radio programme)", "text": "Material World (radio programme)\n\nMaterial World was a weekly science magazine programme on BBC Radio 4 broadcast on a Thursday afternoon. The programme's regular presenter was Quentin Cooper, with contributions from scientists researching areas under discussion in each programme.\n\nThe programme began as \"The Material World\" in April 1998. It was presented by Trevor Phillips, a chemistry graduate of Imperial College. In September 2000 Phillips was told that he could no longer work at the BBC due to his close links with the Labour Party, which broke BBC rules of impartiality. He was one of the few regular black broadcasters on Radio 4. The programme was presented by Quentin Cooper from 2000 to its end in 2013.\n\nMaterial World was one of the BBC's main conduits for up-to-date scientific news, along with \"Frontiers\", \"Science in Action\", and \"Bang Goes the Theory\".\n\nFrom 5 April 2010 the programme was repeated on a Monday evening at 21.00, in the former slot of \"Costing the Earth\". For a short time, when programmes on 5 Live began webstreaming with video, \"Material World\" was also webcast.\n\nOn 14 June 2013 it was announced that the show was to be cancelled, to be replaced by a new show, \"Inside Science\". The last programme presented by Quentin Cooper was broadcast on 20 June 2013 with the final episode airing a week later on 27 June 2013, presented by Gareth Mitchell.\n\nA typical episode programme covered three or four topics, giving each 7-10 minutes. For many years the programme was divided into two sections of fifteen minutes on separate topics. It took the form of interviewing a guest scientist or engineer. Cooper often ended the programme with a terrible scientific pun.\n\nMany past programmes are available for online listening via the programme's website. Some sequential sets of programmes were made in collaboration with the Open University.\n\n\n"}
{"id": "395107", "url": "https://en.wikipedia.org/wiki?curid=395107", "title": "Mechanical philosophy", "text": "Mechanical philosophy\n\nThe mechanical philosophy is a natural philosophy describing the universe as similar to a large-scale mechanism. Mechanical philosophy is associated with the scientific revolution of Early Modern Europe. One of the first expositions of universal mechanism is found in the opening passages of \"Leviathan\" by Hobbes published in 1651.\n\nSome intellectual historians and critical theorists argue that early mechanical philosophy was tied to disenchantment and the rejection of the idea of nature as living or animated by spirits or angels. Other scholars, however, have noted that early mechanical philosophers nevertheless believed in magic, Christianity and spiritualism.\n\nSome ancient philosophies held that the universe is reducible to completely mechanical principles—that is, the motion and collision of matter. This view was closely linked with materialism and reductionism, especially that of the atomists and to a large extent, stoic physics. Later mechanists believed the achievements of the scientific revolution of the 17th century had shown that all phenomena could eventually be explained in terms of \"mechanical laws\": natural laws governing the motion and collision of matter that imply a determinism. If all phenomena can be explained entirely through the motion of matter under physical laws, as the gears of a clock determine that it must strike 2:00 an hour after striking 1:00, all phenomena must be completely determined, past, present or future.\n\nThe natural philosophers directly concerned with developing the mechanical philosophy were largely a French group, together with some of their personal connections. They included Pierre Gassendi, Marin Mersenne and René Descartes. Also involved were the English thinkers Sir Kenelm Digby, Thomas Hobbes and Walter Charleton; and the Dutch natural philosopher Isaac Beeckman.\n\nRobert Boyle used \"mechanical philosophers\" to refer both to those with a theory of \"corpuscles\" or atoms of matter, such as Gassendi and Descartes, and those who did without such a theory. One common factor was the clockwork universe view. His meaning would be problematic in the cases of Hobbes and Galileo Galilei; it would include Nicolas Lemery and Christiaan Huygens, as well as himself. Newton would be a transitional figure. Contemporary usage of \"mechanical philosophy\" dates back to 1952 and Marie Boas Hall.\n\nIn France the mechanical philosophy spread mostly through private academies and salons; in England in the Royal Society. In England it did not have a large initial impact in universities, which were somewhat more receptive in France, the Netherlands and Germany.\n\nOne of the first expositions of universal mechanism is found in the opening passages of \"Leviathan\" (1651) by Hobbes (1651); the book's second chapter invokes the principle of inertia, foundational for the mechanical philosophy. Boyle did not mention him as one of the group; but at the time they were on opposite sides of a controversy. Richard Westfall deems him a mechanical philosopher.\n\nHobbes's major statement of his natural philosophy is in \"De Corpore\" (1655). In part II and III of this work he goes a long way towards identifying fundamental physics with geometry; and he freely mixes concepts from the two areas.\n\nDescartes was also a mechanist. A substance dualist, he argued that reality was composed of two radically different types of substance: extended matter, on the one hand, and immaterial mind, on the other. Descartes argued that one cannot explain the conscious mind in terms of the spatial dynamics of mechanistic bits of matter cannoning off each other. Nevertheless, his understanding of biology was mechanistic in nature:\n\nHis scientific work was based on the traditional mechanistic understanding that animals and humans are completely mechanistic automata. Descartes' dualism was motivated by the seeming impossibility that mechanical dynamics could yield mental experiences.\n\nIsaac Beeckman's theory of mechanical philosophy described in his books \"Centuria\" and \"Journal\" is grounded in two components: matter and motion. To explain matter, Beeckman relied on atomism philosophy which explains that matter is composed of tiny inseparable particles that interact to create the objects seen in life. To explain motion, he supported the idea of inertia, a theory generated by Isaac Newton.\n\nIsaac Newton ushered in a weaker notion of mechanism that tolerated the action at a distance of gravity. Interpretations of Newton's scientific work in light of his occult research have suggested that he did not properly view the universe as mechanistic, but instead populated by mysterious forces and spirits and constantly sustained by God and angels. Later generations of philosophers who were influenced by Newton example were nonetheless often mechanists. Among them were Julien Offray de La Mettrie and Denis Diderot.\n\nThe French mechanist and determinist Pierre Simon de Laplace formulated some implications of the mechanist thesis, writing:\n\n\n"}
{"id": "43457615", "url": "https://en.wikipedia.org/wiki?curid=43457615", "title": "Membrane theory of shells", "text": "Membrane theory of shells\n\nThe membrane theory of shells, or membrane theory for short, describes the mechanical properties of shells when twisting and bending moments are small enough to be negligible.\n\nThe spectacular simplification of membrane theory makes possible the examination of a wide variety of shapes and supports, in particular, tanks and shell roofs. There are heavy penalties paid for this simplification, and such inadequacies are apparent through critical inspection, remaining within the theory, of solutions. However, this theory is more than a first approximation. If a shell is shaped and supported so as to carry the load within a membrane stress system it may be a desirable solution to the design problem, i.e., thin, light and stiff.\n\n\n"}
{"id": "30355727", "url": "https://en.wikipedia.org/wiki?curid=30355727", "title": "NASA eClips", "text": "NASA eClips\n\nNASA eClips is a web-based video and educator resource repository which focuses on grades K-5, 6-8, 9-12 and the general public.\n\nNASA eClips is public outreach effort funded by the NASA Headquarters Strategic Communication Office. The NASA eClips website offers educational video segments and instructional materials, such as lesson plans, designed for use in the classroom.\n\nNASA eClips comprises four programs, which are produced for targeted audiences.\n\nIn total, there are more than two hundred and thirty videos exploring current applications of science, technology, engineering and mathematics, or STEM, topics. The videos are searchable by title, grade level, keyword, and description. RSS feeds are available for each of the programs.\n\nThe material offered by NASA eClips is selected based on national curriculum standards identified by the National Council of Teachers of Mathematics, the National Science Teachers Association, and the International Society for Technology in Education. NASA eClips supports the 5E constructivist learning cycle.\n\nNASA eClips has won numerous awards including an iParenting Media Award, and a regional Emmy award.\n\n\n"}
{"id": "3156313", "url": "https://en.wikipedia.org/wiki?curid=3156313", "title": "Open research", "text": "Open research\n\nOpen research is research conducted in the spirit of free and open-source software. Much like open-source schemes that are built around a source code that is made public, the central theme of open research is to make clear accounts of the methodology freely available via the internet, along with any data or results extracted or derived from them. This permits a massively distributed collaboration, and one in which anyone may participate at any level of the project.\n\nEspecially if the research is scientific in nature, it is frequently referred to as open science. \"Open research\" can also include social sciences, the humanities, mathematics, engineering and medicine.\n\nImportant distinctions exist between different types of open projects.\n\nProjects that provide open data but don't offer open collaboration are referred to as \"open access\" rather than open research. Providing open data is a necessary but not sufficient condition for open research, because although the data may be used by anyone, there is no requirement for subsequent research to take place openly. For example, though there have been many calls for more open collaborative research in drug discovery and the open deposition of large amounts of data, there are very few active, openly collaborative projects in this area.\n\nCrowdsourcing projects that recruit large numbers of participants to carry out small tasks which are then assembled into a larger project outcome have delivered significant research outcomes, but these projects are distinct from those in which participants are able to influence the overall direction of the research, or in which participants are expected to have creative input into the science behind the project.\n\nMost open research is conducted within existing research groups. Primary research data are posted which can be added to, or interpreted by, anyone who has the necessary expertise and who can therefore join the collaborative effort. Thus the \"end product\" of the project (which may still be subject to future expansion or modification) arises from many contributions across multiple research groups, rather than the effort of one group or individual. Open research is therefore distinct from open access in that the output of open research is prone to change with time.\n\nUnlike open access, true open research must demonstrate live, online collaboration. Project websites that demonstrate this capability have started to become available.\n\nIssues with copyright are dealt with by using either standard copyright (where applicable), releasing the content into the Public domain or by releasing the content under licenses such as one of the Creative Commons licenses or one of the GNU General Public Licenses.\n\nIn 2005, several examples arose in the area of the search for new/improved medical treatments of Neglected Diseases.\n\nScience and engineering research to support the creation of open-source appropriate technology for sustainable development has long used open research principles. Open source research for sustainable development is now becoming formalized with open access for literature reviews, research methods, data, results and summaries for laypeople.\n\nWiki-based examples include: Appropedia, , Citizendium, Scholarpedia.\n\nWhile first attempts towards opening research were primarily aimed at opening areas such as scientific data, methodologies, software and publications, now increasingly other artifacts of the scientific workflow are also tackled, such as scientific meta-data and funding ideas.\n\nIn 2013, open research became more mainstream with web based platforms such as figshare continuing to grow in terms of users and publicly available outputs.\n\nThe Transparency and Openness Promotion (TOP) Committee met in 2014 to address one key element of the incentive systems: journals' procedures and policies for publication. The committee consisted of disciplinary leaders, journal editors, funding agency representatives, and disciplinary experts largely from the social and behavioral sciences. By developing shared standards for open practices across journals, the committee said it hopes to translate scientific norms and values into concrete actions and change the current incentive structures to drive researchers' behavior toward more openness. The committee said it sought to produce guidelines that (a) focus on the commonalities across disciplines, and that (b) define what aspects of the research process should be made available to the community to evaluate, critique, reuse, and extend. The committee added that the guidelines aim to help improve journal policies in order to help transparency, openness, and reproducibility \"become more evident in daily practice and ultimately improve the public trust in science, and science itself.\"\n\n"}
{"id": "1967161", "url": "https://en.wikipedia.org/wiki?curid=1967161", "title": "Post-normal science", "text": "Post-normal science\n\nPost-normal science (PNS) represents a novel approach for the use of science on issues where \"facts [are] uncertain, values in dispute, stakes high and decisions urgent\". PNS was developed in the 1990s by Silvio Funtowicz and Jerome R. Ravetz. It can be considered as a reaction to the styles of analysis based on risk and cost-benefit analysis prevailing at that time, and as an embodiment of concepts of a new \"critical science\" developed in previous works by the same authors. In a more recent work PNS is described as \"the stage where we are today, where all the comfortable assumptions about science, its production and its use, are in question\".\n\nIn 1962, Thomas Kuhn's \"The Structure of Scientific Revolutions\" introduced the concept of normal science as part of his theory that scientific knowledge progresses through socially constructed paradigm shifts, where normal science is what most scientists do all the time and what all scientists do most of the time. The process of a paradigm shift is essentially as follows:\n\nAn illustration of the theory in practice is the Copernican revolution, where Copernicus’ idea of a (sun-centered) solar system was largely ignored (not in the rules) when first introduced; then Galileo was deemed a heretic for supporting the idea (rules called into question); and finally, after a revolution in cosmology, the solar system became an obvious and foundational part of scientific knowledge (new rules).\n\nAnother example is the question of whether light is a particle or a wave. For a long time there was debate on this point. Advocates on both sides had many valid arguments based on scientific evidence but were lacking a theory that would resolve the conflict. After a revolution in thinking, it was realized that both perspectives could be true.\n\nPhysicist and policy adviser James J. Kay described post-normal science as a process that recognizes the potential for gaps in knowledge and understanding that cannot be resolved in ways other than revolutionary science. He argued that (between revolutions) one should not necessarily attempt to resolve or dismiss contradictory perspectives of the world, whether they are based on science or not, but instead incorporate multiple viewpoints into the same problem-solving process. From the ecological perspective post-normal science can be situated in the context of 'crisis disciplines' – a term coined by the conservation biologist Michael E. Soulé to indicate approaches addressing fears, emerging in the seventies, that the world was on the verge of ecological collapse. In this respect Michael Egan defines PNS as a 'survival science'.\n\nMoving from PNS Ziauddin Sardar developed the concept of Postnormal Times (PNT). Sardar was the editor of FUTURES when it published the article ‘Science for the post-normal age’ presently the most cited paper of the journal . A recent review of academic literature conducted on the Web of Science and encompassing the topics of Futures studies, Foresight, Forecasting and Anticipation Practice identifies the same paper as \"the all-time publication that received the highest number of citations\". \n\n\"At birth Post-normal science was conceived as an inclusive set of robust insights more than as an exclusive fully structured theory or field of practice\". Some of the ideas underpinning PNS can already be found in a work published in 1983 and entitled \"Three types of risk assessment: a methodological analysis\" This and subsequent works show that PNS concentrates on few aspects of the complex relation between science and policy: the communication of uncertainty, the assessment of quality, and the justification and practice of the extended peer communities.\n\nThe horizontal axis represents ‘Systems Uncertainties’ and the vertical one ‘Decision Stakes’. The three quadrants identify Applied Science, Professional Consultancy, and Post-Normal Science. Different standards of quality and styles of analysis are appropriate to different regions in the diagram, i.e. Post-normal science does not claim relevance and cogency on all of science's application but only on those defined by the PNS's mantram with a fourfold challenge: : ‘facts uncertain, values in dispute, stakes high and decisions urgent’. For applied research science’s own peer quality control system will suffice (or so was assumed at the moment PNS was formulated in the early nineties), while professional consultancy was considered appropriate for these settings which cannot be ‘peer-reviewed’, and where the skills and the tacit knowledge of a practitioner are needed at the forefront, e.g. in a surgery room, or in a house on fire. Here a surgeon or a fireman takes a difficult technical decision based on her or his training and appreciation of the situation (the Greek concept of ‘Metis (mythology)’).\n\nThere are important linkages between PNS and complexity science, e.g. system ecology (C. S. Holling) and hierarchy theory (Arthur Koestler). In PNS, complexity is respected through its recognition of a multiplicity of legitimate perspectives on any issue; and reflexivity is realised through the extension of accepted ‘facts’ beyond the supposedly objective productions of traditional research. Also, the new participants in the process are not treated as passive learners at the feet of the experts, being coercively convinced through scientific demonstration. Rather, they will form an ‘extended peer community’, sharing the work of quality assurance of the scientific inputs to the process, and arriving at a resolution of issues through debate and dialogue.\n\nPNS concept of extended peer community moves from and transcends the familiar concept of scientific peer community relative to a well-defined field of scientific research.\nThe peer community is extended in two respects: first, more than one discipline is assumed to have a potential bearing on the issue being debated, thereby providing different lenses to consider the problem. Second the community is extended to lay actors, taken to be all those with stakes, or an interest, in the given issue. Perhaps the best justification of the concept is offered by Paul Feyerabend in Against Method . For Feyerabend the participation of experts together with non-experts would allow the citizens to mature, inter alia by realizing that the experts are themselves lay-people outside their restricted field of competence. For Giandomenico Majone \"In any area of public policy the choice of instruments, far from being a technical exercise that can be safely delegated to the experts, reflects as in a microcosm all the political, moral, and cultural dimensions of policy-making.\" The same author notes: \"Dialectical confrontation between generalists and experts often succeeds in bringing out unstated assumptions, conflicting interpretations of the facts, and the risks posed by the projects\". These considerations justifies the need for an extended peer community, as the arena where the policy instruments and options can be discussed with - but without deference to - the experts and the authorities. \n\nThe lay members of the community thus constituted may also take upon themselves active 'research' tasks; this has happened e.g. in the so-called 'popular epidemiology' , when the official authorities have shown reticence to perform investigations deemed necessary by the communities affected - for example - by a case of air or water pollution , and more recently ‘citizen science’ . The extended community can usefully investigate the quality of the scientific assessments provided by the experts, the definition of the problem, as well as research priorities and research questions .\n\nThus, the extension of the peer community is not only ethically fair or politically correct, but also enhances the quality of the relevant science. An example is provided by Brian Wynne, who discusses the Cumbrian sheep farmers' interaction with scientist and authorities in the relation to the Chernobyl radioactive fallout .\n\nBeside its dominating influence in the literature on 'futures', PNS is considered to have influenced the ecological ‘conservation versus preservation debate’, especially via its reading by American pragmatist Bryan G. Norton. According to Jozef Keulartz the PNS concept of \"extended peer community\" influenced how Norton's developed his 'convergence hypothesis'. The hypothesis posits that ecologists of different orientation will converge once they start thinking 'as a mountain', or as a planet. For Norton this will be achieved via deliberative democracy, which will pragmatically overcome the black and white divide between conservationists and preservationists.\n\nOther authors attribute to PNS the role of having stimulated the take up of transdisciplinary methodological frameworks, reliant on the social constructivist perspective embedded in PNS.\n\nToday Post-normal science is intended as applicable to most instances where the use of evidence is contested due to different norms and values.\n\nAs summarized in a recent work \"the ideas and concepts of post normal science bring about the emergence of new problem solving strategies in which the role of science is appreciated in its full context of the complexity and the uncertainty of natural systems and the relevance of human commitments and values.\n\nFor Peter Gluckman (2014), chief science advisor to the Prime Minister of New Zealand, post normal science approaches are today appropriate for a host of problems including \"eradication of exogenous pests […], offshore oil prospecting, legalization of recreational psychotropic drugs, water quality, family violence, obesity, teenage morbidity and suicide, the ageing population, the prioritization of early-childhood education, reduction of agricultural greenhouse gases, and balancing economic growth and environmental sustainability\".\nFor Carrozza PNS can be \"framed in terms of a call for the ‘democratization of expertise’\", and as a \"reaction against long-term trends of ‘scientization’ of politics—the tendency towards assigning to experts a critical role in policymaking while marginalizing laypeople\". For Mike Hulme (2007), writing on \"The Guardian\" Climate change seems falls into the category of issues which are best dealt with in the context of PNS and notes that \"Disputes in post-normal science focus as often on the process of science - who gets funded, who evaluates quality, who has the ear of policy - as on the facts of science\". Recent reviews of the history and evolution of PNS, its definitions, conceptualizations,\nand uses can be found in Turnpenny et al., 2010, and in The Routledge Handbook of Ecological Economics (Nature and Society). There has been recently an increased reference to post-normal science, e.g. in Nature (journal).\n\nA criticism of post-normal science is offered by Weingart (1997) for whom Post-normal science does not introduce a new epistemology but retraces earlier debates linked to the so-called \"finalization thesis\".\n\nThe journal FUTURES devoted several specials issues to PNS.\n\n\n\nAnother special issue on Post Normal Science was published on the journal Science, Technology & Human Values in May 2011.\n\nMore titles and links relative to PNS special issues are available at the NUSAP net.\n\nA group of scholars of PNS orientation has published in 2016 a volume on the quality control crisis of science. The volume discusses \"inter alia\" what this community perceive as the root causes of the present crisis.\n\nAmong the quantitative styles of analysis which make reference to post-normal science one can mention NUSAP for numerical information, sensitivity auditing for indicators and mathematical modelling and MUSIASEM in the field of social metabolism.\n\nIn relation to mathematical modelling PNS suggests a participatory approach, whereby ‘models to predict and control the future’ are replaced by ‘models to map our ignorance about the future’, in the process exploring and revealing the metaphors embedded in the model. PNS is also known for the its definition of GIGO: in modelling GIGO occurs when the uncertainties in the inputs must be suppressed, lest the outputs become completely indeterminate. \n\n\n\n"}
{"id": "1855050", "url": "https://en.wikipedia.org/wiki?curid=1855050", "title": "Research associate", "text": "Research associate\n\nResearch associates are scholars and professionals that usually have an advanced degree beyond a bachelor's degree. The research associate position does not explicitly require mentoring and is a regular staff position with appointment letters processed by Human Resources. In contrast to a research assistant, a research associate often has a graduate degree, such as a master's (e.g. Master of Science) or in some cases Master of Engineering or a doctoral degree (e.g. Doctor of Philosophy, Doctor of Medicine or Doctor of Pharmacy). In some cases it can be synonymous with postdoctoral research.\n\n"}
{"id": "26798613", "url": "https://en.wikipedia.org/wiki?curid=26798613", "title": "Science Theatre", "text": "Science Theatre\n\nScience Theatre is an undergraduate student-run science outreach organization at Michigan State University's East Lansing campus. Science Theatre visits schools and events throughout Michigan performing interactive science demonstrations for K-12 students on-stage or up-close. Science Theatre performers are undergraduate and graduate student volunteers and all performances are made free of charge.\n\nThe group's performances consist of arrangements from its catalog of more than seventy demonstrations in biology, chemistry, and physics. Additionally, Science Theatre performs comprehensive shows in Astronomy, Environmental science, Pressure, the Periodic Table, Quantum Mechanics, and FRIB-related science.\n\nScience Theatre was founded in April 1991 under a grant from the National Science Foundation and received the 1993 AAAS Award for Public Understanding of Science and Technology. Science Theatre is a four-time winner of the Outreach Award from the Michigan State University Department of Physics and Astronomy.\n\n"}
{"id": "734369", "url": "https://en.wikipedia.org/wiki?curid=734369", "title": "Scientific enterprise", "text": "Scientific enterprise\n\nScientific enterprise refers to science-based projects developed by, or in cooperation with, private entrepreneurs. For example, in the Age of Exploration, leaders like Henry the Navigator founded schools of navigation, from which stemmed voyages of exploration.\n\nEach of the organizations listed below, have the ability to conduct scientific research on an extended basis, involving multiple researchers over an extended time. Generally, the research is funded not only for the science itself, but for some application which shows promise for the enterprise. But the researchers, if left to their own choices, will tend to follow their research interest, which is essential for the long-term health of their chosen field. Note that a successful scientific enterprise is not equivalent to a successful high-tech enterprise or to a successful business enterprise, but that they form an ecology, a \"food chain\".\n\n\n"}
{"id": "26615974", "url": "https://en.wikipedia.org/wiki?curid=26615974", "title": "Scientific research on the International Space Station", "text": "Scientific research on the International Space Station\n\nScientific research on the International Space Station is a collection of experiments that require one or more of the unusual conditions present in low Earth orbit. The primary fields of research include human research, space medicine, life sciences, physical sciences, astronomy and meteorology. The 2005 NASA Authorization Act designated the American segment of the International Space Station as a national laboratory with the goal of increasing the use of the ISS by other federal agencies and the private sector.\n\nResearch on the ISS improves knowledge about the effects of long-term space exposure on the human body. Subjects currently under study include muscle atrophy, bone loss, and fluid shift. The data will be used to determine whether space colonisation and lengthy human spaceflight are feasible. As of 2006, data on bone loss and muscular atrophy suggest that there would be a significant risk of fractures and movement problems if astronauts landed on a planet after a lengthy interplanetary cruise (such as the six-month journey time required to fly to Mars).\nLarge scale medical studies are conducted aboard the ISS via the National Space Biomedical Research Institute (NSBRI). Prominent among these is the Advanced Diagnostic Ultrasound in Microgravity study in which astronauts (including former ISS Commanders Leroy Chiao and Gennady Padalka) perform ultrasound scans under the guidance of remote experts. The study considers the diagnosis and treatment of medical conditions in space. Usually, there is no physician on board the ISS, and diagnosis of medical conditions is a challenge. It is anticipated that remotely guided ultrasound scans will have application on Earth in emergency and rural care situations where access to a trained physician is difficult.\n\nResearchers are investigating the effect of the station's near-weightless environment on the evolution, development, growth and internal processes of plants and animals. In response to some of this data, NASA wants to investigate microgravity's effects on the growth of three-dimensional, human-like tissues, and the unusual protein crystals that can be formed in space.\n\nThe investigation of the physics of fluids in microgravity will allow researchers to model the behaviour of fluids better. Because fluids can be almost completely combined in microgravity, physicists investigate fluids that do not mix well on Earth. In addition, an examination of reactions that are slowed by low gravity and temperatures will give scientists a deeper understanding of superconductivity.\n\nThe study of materials science is an important ISS research activity, with the objective of reaping economic benefits through the improvement of techniques used on the ground. Other areas of interest include the effect of the low gravity environment on combustion, through the study of the efficiency of burning and control of emissions and pollutants. These findings may improve our knowledge about energy production, and lead to economic and environmental benefits. Future plans are for the researchers aboard the ISS to examine aerosols, ozone, water vapour, and oxides in Earth's atmosphere, as well as cosmic rays, cosmic dust, antimatter, and dark matter in the universe.\n\n<br>\nWhen completed, The ISS will include a number of modules devoted to scientific activity as well as other hardware designed for the same purpose.\n<br>\nLaboratory modules:\nScientific hardware not attached to any laboratory module:\n\nInternal scientific hardware:\n\nExternal scientific hardware:\n\n\nPlanned for launch:\n\n<br>\nInternal scientific hardware:\nExternal scientific hardware:\n\n\n\n\n\n\n\n\n\n\n\nFee-based utilization of Kibo is available to unrestricted research groups for commercial use. Costs involved in the operation will be paid by each user. The results obtained through the utilization will belong to the user.\n\n\n\n\n\n\n\n\n\n\n\nHost Immunity in Space (FIT)\n\nAntibiotic Production in Space (CGBA-APS)\nSynaptogenesis in Microgravity (CGBA-SM)\n(MEPS)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMuch like NASA and JAXA, ESA also conducted numerous experiments on the International Space Station.\n\n\n\n\n\n\n\n\n\n\nIn May 2011, Space Shuttle Endeavour mission STS-134 carried 13 Lego kits to the ISS, where astronauts built models and saw how they reacted in microgravity, as part of the Lego Bricks in Space program. The results were shared with schools as part of an educational project.\n\n\n"}
{"id": "34909429", "url": "https://en.wikipedia.org/wiki?curid=34909429", "title": "Socio-scientific issues", "text": "Socio-scientific issues\n\nSocioscientific Issues (SSI) are controversial social issues which relate to science. They are ill-structured, open-ended problems which have multiple solutions.\n\nSSI are utilized in science education in order to promote scientific literacy, which emphasizes the ability to apply scientific and moral reasoning to real-world situations. Some examples of SSI include issues such as genetic engineering, climate change, animal testing for medical purposes, oil drilling in national parks, and \"fat taxes\" on \"unhealthy\" foods, among many others. Research studies have shown SSI to be effective at increasing students' understanding of science in various contexts, argumentation skills, empathy, and moral reasoning.\n\nSupporters of SSI argue that it can:\nevaluation, interpretation, and self-regulation Science educators often refer to all of these aspects together as,\"functional scientific literacy.\"\n\nScientific literacy has been defined by two competing visions. A Vision I approach to scientific literacy is characterized by content-driven, decontextualized science knowledge. A Vision II approach to scientific literacy is a context-driven, student-centered approach which seeks to prepare students for informed civic engagement. The SSI framework follows a Vision II approach as it is believed to provide an opportunity for contextualized learning of science content as well as an opportunity for moral development.\n\nSSI is conceptually related to Science, Technology, and Society (STS) education. However, while both approaches connect science to societal issues, SSI is distinguished from STS because of its emphasis on the development of character and virtue as well as content knowledge.\n\nResearch suggests that SSI creates cognitive dissonance by compelling students to consider claims that may be at odds with their own beliefs and values. Dissonance of this nature is believed by some to advance moral reasoning by ‘empowering students to consider how science based issues and the decisions made concerning them reflect, in part, the moral principles and qualities of virtue that encompass their own lives, as well as the physical and social world around them.'\n\nSSI education has been empirically investigated and linked to particular outcomes including:\n• Promoting developmental changes in reflective judgment;\n• Moving students to more informed views of the nature of science;\n• Increasing moral sensitivity and empathy;\n• Increasing conceptual understanding of scientific content;• Increase students’ ability to transfer concepts and scaffold ideas;\n• Revealing and reconstructing alternative perceptions of science;\n• Facilitating moral reasoning;\n• Improve argumentation skills;\n• Promote understanding of eco-justice and environmental awareness; and\n• Engage students’ interest in the inquiry of science.\n\nMore recently, SSI research has been focused on cross-cultural comparisons and research has reflected international partnerships. It has been hypothesized by some that more advanced stages of epistemological reasoning allows individuals to apply a kind of socioscientific reasoning (SSR) akin to scientific habits of mind. SSR is a theoretical construct that entails the ability to tap key traits while negotiating SSI. These include skepticism, complexity, multiple perspective and inquiry.\n\nTeachers utilize SSI to foster understanding of science content and consequences involved in everyday scientific issues. For example, in a study of ecology, an elementary class might consider whether pesticides confer more benefit or harm to our ecosystem. This type of analysis would require students to research the interractions between organisms in food webs and food chains, as well as the human impacts of pesticides. Students could make evidence-based decisions and discuss them through various means including whole-class discussions, debates, online discussion boards, etc... Similarly, older grades might consider issues such as whether genetic engineering should be used to treat genetic diseases.\n\nThis type of analysis would require extensive study of genetics and modern genetic engineering techniques, as well as the ethical issues involved in personal freedoms, religious prohibitions on intervention, and so on. Advocates suggest that, through evidence-based discourse, students learn to formulate their own informed decisions and understand those whose views differ from themselves. An essential aspect of the implementation of SSI is that the teacher is not promoting any particular belief; rather, the teacher's role is to promote evidence-based critical thinking and argumentation.\n"}
{"id": "58855040", "url": "https://en.wikipedia.org/wiki?curid=58855040", "title": "The Eppendorf &amp; Science Prize for Neurobiology", "text": "The Eppendorf &amp; Science Prize for Neurobiology\n\nThe Eppendorf & \"Science\" Prize for Neurobiology is a neurobiology prize that is awarded annually by \"Science\" magazine (published by American Association for the Advancement of Science) and underwritten by Eppendorf AG, laboratory device and supply company. Entrees are reviewed by editors from \"Science\" magazine and the top 10% are forwarded to the judging panel. The judging panel is chaired by the Neuroscience Editor of \"Science\" and the remaining judges are nominated from the Society for Neuroscience.\n\nThis Eppendorf & \"Science\" Prize for Neurobiology prize was created in 2002 to promote the work of promising new neurobiologists with cash grants to support their careers. Each applicant must submit a 1000 word essay explaining the focus and motivation for their last three years of work. The winner is awarded $25,000 and the scientist’s winning essay is then published in \"Science\". The winning essay and the essays of the other finalists are all published on \"Science Online\".\n"}
{"id": "6060316", "url": "https://en.wikipedia.org/wiki?curid=6060316", "title": "The Fringe of the Unknown", "text": "The Fringe of the Unknown\n\nThe Fringe of the Unknown is a science book by L. Sprague de Camp, first published in hardcover by Prometheus Books in 1983.\n\nThe book is a collection of articles that constitute a \"study ... of controversial and often little-known happenings in science and technology, with an emphasis on the wayward activities of those who dabble in fringe science.\" The material is organized in three sections, \"Our Ingenious Forebears,\" \"Beasts of Now and Then,\" and \"Scientists, Mad and Otherwise.\" The first debunks extravagant occult and pseudoscientific claims regarding ancient civilizations while highlighting these cultures' actual accomplishments. The second performs much the same function in regard to biology, focusing on elephants, claims regarding the survival of dinosaurs into the present day, and past extinction events. The third explores the distinction between science and pseudoscience as illustrated in the lives of a number of scientists holding extreme views.\n\n1. \"The Wisdom of the Ancients\" (from \"Science Fiction Quarterly\", Nov. 1951)<br>\n2. \"Apollonios Enlists\" (from \"Astounding Science Fiction\", Jun. 1961)<br>\n3. \"Appius Claudius Crassus\" (original title: \"Appius Claudius Crassus: Roman Builder\") (from \"Science Digest\", Jun. 1962)<br>\n4. \"The First Missile Launchers\" (from \"Science Digest\", Oct. 1960)<br>\n5. \"The Iron Pillar of Delhi\" (from \"Analog Science Fiction/Science Fact\", Sep. 1972)<br>\n6. \"The Mechanical Wizards of Alexandria\" (from \"Science Digest\", Aug. 1962)<br>\n7. \"The Landlocked Indian Ocean\" (from \"The Magazine of Fantasy & Science Fiction\", Jun. 1969)\n\n8. \"Dinosaurs Today\" (original title: \"Dinosaurs in Today's World\") (from \"The Magazine of Fantasy & Science Fiction\", Mar. 1968)<br>\n9. \"Mammoths and Mastodons\" (from \"The Magazine of Fantasy & Science Fiction\", May 1965)<br>\n10. \"Death Comes to the Megafauna\" (from \"If Worlds of Science Fiction\", Sep. 1971)<br>\n11. \"Xerxes' Okapi\" (original title: \"Xerxes' Okapi and Greek Geography\") (from \"Isis\", Mar. 1963)<br>\n12. \"The Temperamental Tank\" (original title: \"War Elephants\") (from \"Elephant\", 1964)<br>\n13. \"How to Plan a Fauna\" (from \"The Magazine of Fantasy & Science Fiction\", Oct. 1963)\n\n14. \"The Care and Feeding of Scientists\" (original title: \"The Care and Feeding of Mad Scientists\") (from \"Astounding Science Fiction\", Jul. 1951)<br>\n15. \"The Great Whale Robbery\" (from \"The Day of the Dinosaur\", 1968)<br>\n16. \"Mad Men of Science\" (originally published in two parts, as \"Mad Men of Science\" and \"More Mad Men of Science\") (from \"Future Science Fiction\", Jan. and Mar. 1957)<br>\n17. \"Orthodoxy in Science\" (from \"Astounding Science Fiction\", May 1954)<br>\n18. \"Hoaxes in Science\" (original title: \"Why Do They Do It?\") (from \"Astounding Science Fiction\", Sep. 1950)<br>\n19. \"Little Green Men from Afar\" (from \"The Humanist\", Jul./Aug. 1976)<br>\n20. \"The Need to Know\" (original title: \"Pure Science\") (from \"The Book of Knowledge Annual\", 1959)<br>\n\"Acknowledgments\"\n"}
{"id": "28513034", "url": "https://en.wikipedia.org/wiki?curid=28513034", "title": "The Varieties of Scientific Experience", "text": "The Varieties of Scientific Experience\n\nThe Varieties of Scientific Experience: A Personal View of the Search for God is a book collecting transcribed talks on the subject of natural theology that astronomer Carl Sagan delivered in 1985 at the University of Glasgow as part of the Gifford Lectures. The book was first published posthumously in 2006, 10 years after his death. The title is a reference to \"The Varieties of Religious Experience\" by William James.\n\nThe book was edited by Ann Druyan, who also provided an introduction section. The sixth chapter, \"The God Hypothesis\", was later reprinted in Christopher Hitchens' anthology \"The Portable Atheist\".\n"}
{"id": "20927937", "url": "https://en.wikipedia.org/wiki?curid=20927937", "title": "Translational research", "text": "Translational research\n\nTranslational research – often used interchangeably with translational medicine or translational science or bench to bedside – is an effort to build on basic scientific research to create new therapies, medical procedures, or diagnostics. Basic biomedical research is based on studies of disease processes using for example cell cultures or animal models. The term translational refers to the \"translation\" of basic scientific findings in a laboratory setting into potential treatments for disease.\n\nTranslational medicine is defined by the European Society for Translational Medicine (EUSTM) as \"an interdisciplinary branch of the biomedical field supported by three main pillars: benchside, bedside and community.\"\n\nIt is defined for school-based education by the Education Futures Collaboration (www.meshguides.org) as research which translates concepts to classroom practice. Examples of translational research are commonly found in education subject association journals and in the MESHGuides which have been designed for this purpose.\n\nTranslational research applies findings from basic science to enhance human health and well-being. In a medical research context, it aims to \"translate\" findings in fundamental research into medical practice and meaningful health outcomes. Translational research implements a \"bench-to-bedside\", from laboratory experiments through clinical trials to point-of-care patient applications, model, harnessing knowledge from basic sciences to produce new drugs, devices, and treatment options for patients. The end point of translational research is the production of a promising new treatment that can be used with practical applications, that can then be used clinically or are able to be commercialized.\n\nAs a relatively new research discipline, translational research incorporates aspects of both basic science and clinical research, requiring skills and resources that are not readily available in a basic laboratory or clinical setting. It is for these reasons that translational research is more effective in dedicated university science departments or isolated, dedicated research centers. Since 2009, the field has had specialized journals, the \"American Journal of Translational Research\" and \"Translational Research\" dedicated to translational research and its findings.\n\nTranslational research is broken down into different stages, including two-stage (T1 and T2), four-stage (T1, T2, T3, and T4), and five-stage (T1, T2, T3, T4, and T5) schemes. In a two-stage model, \"T1 research\", refers to the \"bench-to-bedside\" enterprise of translating knowledge from the basic sciences into the development of new treatments and \"T2 research\" refers to translating the findings from clinical trials into everyday practice. In a five-stage scheme, T1 involves basic research, T2 involves pre-clinical research, T3 involves clinical research, T4 involves clinical implementation, and T5 involves implementation in the public health sphere. Waldman et al. propose a scheme going from T0 to T5. T0 is laboratory (before human) research. In T1-translation, new laboratory discoveries are first translated to human application, which includes phase I & II clinical trials. In T2-translation, candidate health applications progress through clinical development to engender the evidence base for integration into clinical practice guidelines. This includes phase III clinical trials. In T3-translation, dissemination into community practices happens. T4-translation seeks to (1) advance scientific knowledge to paradigms of disease prevention, and (2) move health practices established in T3 into population health impact. Finally, T5-translation focuses on improving the wellness of populations by reforming suboptimal social structures. \n\nIn a two-stage scheme, translational research includes two areas of translation. One is the process of applying discoveries generated during research in the laboratory, and in preclinical studies, to the development of trials and studies in humans. The second area of translation concerns research aimed at enhancing the adoption of best practices in the community. Cost-effectiveness of prevention and treatment strategies is also an important part of translational science.\n\nBasic research is the systematic study directed toward greater knowledge or understanding of the fundamental aspects of phenomena and is performed without thought of practical ends. It results in general knowledge and understanding of nature and its laws.\n\nApplied research is a form of systematic inquiry involving the practical application of science. It accesses and uses the research communities' accumulated theories, knowledge, methods, and techniques, for a specific, often state, business, or client-driven purpose.\n\nIn medicine, translational research is increasingly a separate research field. A citation pattern between the applied and basic sides in cancer research appeared around 2000.\n\nCritics of translational research point to examples of important drugs that arose from fortuitous discoveries in the course of basic research such as penicillin and benzodiazepines, and the importance of basic research in improving our understanding of basic biological facts (e.g. the function and structure of DNA) that go on to transform applied medical research.\n\nExamples of failed translational research in the pharmaceutical industry include the failure of anti-aβ therapeutics in Alzheimer's disease. Other problems have stemmed from the widespread irreproducibility thought to exist in translational research literature.\n\nIn U.S., the National Institutes of Health has implemented a major national initiative to leverage existing academic health center infrastructure through the Clinical and Translational Science Awards. \nThe National Center for Advancing Translational Sciences (NCATS) was established on December 23, 2011.\n\nAlthough translational research is relatively new, it is being recognized and embraced globally. Some major centers for translational research include:\nAdditionally, translational research is now acknowledged by some universities as a dedicated field to study a PhD or graduate certificate in, in a medical context. These institutes currently include Monash University in Victoria, Australia, the University of Queensland, Diamantina Institute in Brisbane, Australia, at Duke University in Durham, North Carolina, America, at Creighton University in Omaha, Nebraska. and at Emory University in Atlanta, Georgia,\nThe industry and academic interactions to promote translational science initiatives has been carried out by various global centers such as European Commission, GlaxoSmithKline and Novartis Institute for Biomedical Research.\n\n\n"}
