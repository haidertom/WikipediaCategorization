{"id": "40390498", "url": "https://en.wikipedia.org/wiki?curid=40390498", "title": "Botanical expedition", "text": "Botanical expedition\n\nA botanical expedition is a scientific journey or voyage designed to explore the flora of a particular region. The expedition could be specifically designed for exploring the flora, or this could have been a part of studying the natural history of the region. A naturalist or botanist was charged with drawing and describing the flora, collecting specimens of unknown plants in a plant press, and identifying potential economically important plants. On botanical expeditions funded by governments, the plants were often collected by the person in the field, but described and named by a government sponsored scientists at botanical gardens and universities. For example, many of the species collected on the Lewis and Clark Expedition were described and named by Frederick Traugott Pursh.\n"}
{"id": "53466077", "url": "https://en.wikipedia.org/wiki?curid=53466077", "title": "Brain Electrical Oscillation Signature Profiling", "text": "Brain Electrical Oscillation Signature Profiling\n\nBrain Electrical Oscillation Signature Profiling (BEOSP or BEOS) is a technique by which a suspect's participation in a crime is detected by eliciting electrophysiological impulses.\n\nIt is a non-invasive, scientific technique with a great degree of sensitivity and a neuro-psychological method of interrogation which is sometimes also referred to as ‘brain fingerprinting’.\n\nThe methodology was developed by Champadi Raman Mukundan (C. R. Mukundan), a Neuroscientist, former Professor & Head of Clinical Psychology at the National Institute of Mental Health and Neurosciences (Bangalore, India), while he worked as a Research Consultant to TIFAC-DFS Project on ‘Normative Data for Brain Electrical Activation Profiling’.\n\nHis works are based on research that was also formerly pursued by other scientists at American universities including J. Peter Rosenfeld, Lawrence Farwell & Emanuel Donchin.\n\nThe human brain receives millions of arrays of signals in different modalities, all through the waking periods. These signals are classified and stored in terms of their relationship perceived as function of experience and available knowledge base of an individual, as well as new relationship produced through sequential processing. The process of encoding is primarily when the individual is directly participating in an activity or experiencing it.\n\nIt is considered secondary, when the information is obtained from a secondary source viz. books, conversations, hearsay etc. in which there is no primary experiential component and the brain deals mainly with conceptual aspects.\n\nPrimary encoding is deep seated and has specific source memory in terms of time and space of occurrence of experience, as individual himself/herself has shared or participated in the experience/act/event at certain time in his/her life at a certain place.\n\nIt is found that when the brain of an individual is activated by a piece of information of an event in which he/she has taken part, the brain of the individual will respond differently from that of a person who has received the same information from secondary sources (non-experiential).\n\nBEOSP is based on this principle, thereby intending to demonstrate that the suspect who have primary encoded information of those who have participated in the suspected events will show responses indicating firsthand (personally acquired) knowledge of the event.\n\n\nIdeally, no questions are to be asked while conducting the test; rather, the subject is simply provided with the probable events/scenarios in the aftermath of which, the results are analyzed to verify if the brain produces any experiential knowledge, which is essentially the recognition of events disclosed. This way, all fundamental rights are protected, as neither there are no questions that are being asked or any answers reciprocated.\n\nUniversity of Pennsylvania conducted a research along with the Brigham & Women's Hospital (Boston, Massachusetts), Children's Hospital Boston & the University Hospital of Freiburg, Germany which determined that Gamma Oscillations in the brain could help distinguish false memories from the real ones. Their analysis concluded that in the retrieval of truthful memories, as compared to false, human brain creates an extremely distinct pattern of gamma oscillations, indicating a recognition of context based information associated with a prior experience.\n\n\n"}
{"id": "4541702", "url": "https://en.wikipedia.org/wiki?curid=4541702", "title": "California State Summer School for Mathematics and Science", "text": "California State Summer School for Mathematics and Science\n\nThe California State Summer School for Mathematics and Science (COSMOS) is a summer program for high school students in California for the purpose of preparing them for careers in mathematics and sciences. It is often abbreviated COSMOS, although COSMOS does not contain the correct letters to create an accurate abbreviation. The program is hosted on four different campuses of the University of California, at Davis, Irvine, San Diego, and Santa Cruz.\n\nCOSMOS was established by the California State Legislature in the summer of 2000 to stimulate the interests of and provide opportunities for talented California high school students. The California State Summer School for Mathematics & Science is modeled after the California State Summer School for the Arts. In the first summer, 292 students enrolled in the program. Each COSMOS campus only holds 150 students, so selection is competitive. It is a great experience in exploring the sciences and a good activity for college applications, especially the University of California application. This program is designed for extremely gifted students who make amazing discoveries in STEM (Science, Technology, Engineering, Mathematics) areas.\n\n\n"}
{"id": "223607", "url": "https://en.wikipedia.org/wiki?curid=223607", "title": "Characteristics of common wasps and bees", "text": "Characteristics of common wasps and bees\n\nWhile observers can easily confuse common wasps and bees at a distance or without close observation, there are many different characteristics of large bees and wasps that can be used to identify them.\n\n\n\n"}
{"id": "50910908", "url": "https://en.wikipedia.org/wiki?curid=50910908", "title": "Clubes de Ciencia", "text": "Clubes de Ciencia\n\nClubes de Ciencia is a non-profit organization founded in 2014 that organizes hands-on week-long workshops in STEM to kids in developing countries at no cost. The instructors are PhD volunteers from top universities, such as Harvard, Princeton, MIT who organizes the workshops and mentor the students aiming at inspiring students to pursue their passions. By combining hands-on experimental learning, on-line exercises and mentorship, Clubes de Ciencia takes a unique approach to educating the millennials in developing countries. In two years, Clubes de Ciencia grew past its Mexico program, to also work in Colombia and Bolivia. Currently, it also operates in Brazil, Paraguay, Peru and Spain. \n\nThe first edition of Science Clubs was organized in Guanajuato, Mexico, in January 2014,with the support of Universidad de Guanajuato.\n\nIn 2017, Science Clubs expanded to three new countries: Paraguay, Peru and Brazil \n\nThe first edition of the Brazilian Chapter (Clubes de Ciência Brasil) was held at Universidade Federal de Minas Gerais, in Belo Horizonte, in July 2017. Four clubs in the areas of epidemiology, stem cells and gene editing, immunology and entrepreneurship were organized by researchers from top Universities from the US and Brazil, including Harvard, Northeastern and UFMG for 80 students from six different states of the country. \n\nOn April 15, 2015, the Latin American Science Education Network (now Science Clubs International) was among the winners of the MIT \"IDEAS\" Global Challenge Awards.\n\nAlso in 2015, Mohammed Mostajo-Radji, as Executive Director of Clubes de Ciencia Bolivia was awarded the \"Person of the Year\" award by the Bolivian newspaper El Deber and the Franz Tamayo Medal by the Senate of Bolivia.\n\nIn 2016, Hugo Arellano-Santoyo and Clubes de Ciencia were awarded with the Dean's Community Service Award from the Harvard Medical School. The prize is awarded \"to recognize individuals whose dedication and commitment to community service have made a positive impact on the local, national, or international community\".\n\nIn 2016, Maier Avendano, Executive Director of Clubes de Ciencia Colombia was named among the \"Latino 30 under 30\" by the El Mundo Boston. Mohammed Mostajo-Radji and the team of Clubes de Ciencia Bolivia received this award in 2017. \n\nAdditionally in 2016, Clubes de Ciencia Bolivia was awarded the Youth Peace Prize by the Government of Santa Cruz, Bolivia. \n\nIn 2018, Clubes de Ciencia Bolivia received the Melchor Pinto Parada award from the Government of Santa Cruz . This is the maximum award granted by this institution. Also in 2018, Omar Gandarilla, as Operations Director of Clubes de Ciencia Bolivia received the \"Diversity in STEM\" award from MiniPCR.\n\nThe David Rockefeller Center for Latin American Studies, the Department of Molecular and Cellular Biology at Harvard University, the Harvard Stem Cell Institute, COMEXUS and the Fundación México en Harvard University are among the key sponsors of the project.\n\nClubes de Ciencia has been endorsed by a number of academics and celebrities, including 2004 Nobel Laureate Frank Wilczek; Margot Gill, Dean of International Affairs at Harvard University and Beakman.\n"}
{"id": "56452747", "url": "https://en.wikipedia.org/wiki?curid=56452747", "title": "Cryogenic electron microscopy", "text": "Cryogenic electron microscopy\n\nCryo-Electron Microscopy (Cryo-EM) is an electron microscopy (EM) technique applied on samples cooled to cryogenic temperatures and embedded in an environment of vitreous water. An aqueous sample solution is applied to a grid-mesh and plunge-frozen in liquid ethane. While development of the technique began in the 1970s, recent advances in detector technology and software algorithms have allowed for the determination of biomolecular structures at near-atomic resolution. This has attracted wide attention to the approach as an alternative to X-ray crystallography or NMR spectroscopy for macromolecular structure determination without the need for crystallization.\n\nIn 2017, the Nobel Prize in Chemistry was awarded to Jacques Dubochet, Joachim Frank, and Richard Henderson \"for developing cryo-electron microscopy for the high-resolution structure determination of biomolecules in solution.\"\n\nTransmission electron cryomicroscopy (CryoTEM) is a transmission electron microscopy technique that is used in structural biology.\n\nIn the 1960s, scientist were faced with the issue of structure determination methods using electron microscopy damaging the specimen due to high energy electron beams, so cryogenic electron microscopy was considered to overcome this issue as it was expected that low temperatures would reduce beam damage. In 1980, Erwin Knapek and Jacques Dubochet published commenting on beam damage at cryogenic temperatures sharing observations that:Thin crystals mounted on carbon film were found to be from 30 to 300 times more beam-resistant at 4 K than at room temperature... Most of our results can be explained by assuming that cryoprotection in the region of 4 K is strongly dependent on the temperature.However, these results were not reproducible and amendments were published in the Nature international journal of science just 2 years later informing that the beam resistance was less significant than initially anticipated. The protection gained at 4 K was closer to “tenfold for standard samples of L-valine,” than what was previously stated.\n\nIn 2017, three scientists, Jacques Dubochet, Joachim Frank and Richard Henderson were awarded the Nobel Prize in Chemistry for developing a technique that would image biomolecules.\n\nIn 2018, Chemists realized that electron diffraction can be used to readily determine the structures of small molecules that form needle-like crystals, structures that would otherwise need to be determined from X-ray crystallography, by growing larger crystals of the compound.\n\nScanning electron cryomicroscopy (CryoSEM), is scanning electron microscopy technique with a scanning electron microscope's cold stage in a cryogenic chamber.\n\n"}
{"id": "5828", "url": "https://en.wikipedia.org/wiki?curid=5828", "title": "Cryptozoology", "text": "Cryptozoology\n\nCryptozoology is a pseudoscience and subculture that aims to prove the existence of entities from the folklore record, such as Bigfoot, the chupacabra, or Mokele-mbembe. Cryptozoologists refer to these entities as \"cryptids\", a term coined by the subculture. Because it does not follow the scientific method, cryptozoology is considered a pseudoscience by the academic world: it is neither a branch of zoology nor folkloristics.\n\nOriginally founded in the 1950s by zoologists Bernard Heuvelmans and Ivan T. Sanderson, scholars have noted that the pseudoscience rejected mainstream approaches from an early date, and that adherents often express hostility to mainstream science. Scholars have studied cryptozoologists and their influence (including the pseudoscience's association with young Earth creationism), and have noted parallels in cryptozoology and other pseudosciences such as ghost hunting and ufology.\n\nAs a field, cryptozoology originates from the works of colleagues Bernard Heuvelmans, a Belgian zoologist, and Ivan T. Sanderson, a Scottish zoologist. Notably, Heuvelmans published \"On the Track of Unknown Animals\" (French \"Sur la Piste des Bêtes Ignorées\") in 1955, a landmark work among cryptozoologists that was followed by numerous other like works. Similarly, Sanderson published a series of books that assisted in developing hallmarks of cryptozoology, including \"Abominable Snowmen: Legend Come to Life\" (1961).\n\nThe term \"cryptozoology\" dates from cryptozoologist circles from 1959 or before – Heuvelmans attributes the coinage of the term \"cryptozoology\" ('the study of hidden animals') to Sanderson. Patterned after \"cryptozoology\", the term \"cryptid\" was coined in 1983 by cryptozoologist J. E. Wall in the Summer issue of the International Society of Cryptozoology newsletter. According to Wall \"[It has been] suggested that new terms be coined to replace sensational and often misleading terms like 'monster'. My suggestion is 'cryptid', meaning a living thing having the quality of being hidden or unknown.\" The \"Oxford English Dictionary\" defines the noun \"cryptid\" as \"an animal whose existence or survival to the present day is disputed or unsubstantiated; any animal of interest to a cryptozoologist\". While used by most cryptozoologists, the term \"cryptid\" is not used by academic zoologists.\n\nWhile biologists regularly identify new species, cryptozoologists often focus on creatures from the folklore record. Most famously, these include the Loch Ness Monster, Bigfoot, the chupacabra, as well as other \"imposing beasts that could be labeled as monsters\". In their hunt for these entities, cryptozoologists may employ devices such as motion-sensitive cameras, night-vision equipment, and audio-recording equipment. While there have been attempts to codify cryptozoological approaches, unlike biologists, zoologists, botanists, and other academic disciplines, however, \"there are no accepted, uniform, or successful methods for pursuing cryptids\". Some scholars have identified precursors to modern cryptozoology in certain medieval approaches to the folklore record, and the psychology behind the cryptozoology approach has been the subject of academic study.\n\nFew cryptozoologists have a formal science education, and fewer still have a science background directly relevant to cryptozoology. Adherents often misrepresent the academic backgrounds of cryptozoologists. According to writer Daniel Loxton and paleontologist Donald Prothero, \"Cryptozoologists have often promoted 'Professor Roy Mackal, PhD.' as one of their leading figures and one of the few with a legitimate doctorate in biology. What is rarely mentioned, however, is that he had no training that would qualify him to undertake competent research on exotic animals. This raises the specter of 'credential mongering', by which an individual or organization feints a person's graduate degree as proof of expertise, even though his or her training is not specifically relevant to the field under consideration.\" Besides Heuvalmans, Sanderson, and Mackal, notable cryptozoologists with academic backgrounds include Grover Krantz, Karl Shuker, and Richard Greenwell.\n\nA subset of cryptozoology promotes the pseudoscience of Young Earth creationism, rejecting conventional science in favor of a Biblical interpretation and promoting concepts such as \"living dinosaurs\". Science writer Sharon A. Hill observes that the Young Earth creationist segment of cryptozoology is \"well-funded and able to conduct expeditions with a goal of finding a living dinosaur that they think would invalidate evolution.\" Anthropologist Jeb J. Card says that \"Creationists have embraced cryptozoology and some cryptozoological expeditions are funded by and conducted by creationists hoping to disprove evolution.\" In a 2013 interview, paleontologist Donald Prothero notes an uptick in creationist cryptozoologists. He observes that \"[p]eople who actively search for Loch Ness monsters or Mokele Mbembe do it entirely as creationist ministers. They think that if they found a dinosaur in the Congo it would overturn all of evolution. It wouldn't. It would just be a late-occurring dinosaur, but that's their mistaken notion of evolution.\"\n\nThe 2003 discovery of the fossil remains of \"Homo floresiensis\" was cited by paleontologist Henry Gee, a senior editor at the journal \"Nature\", as possible evidence that \"in geological terms, makes it more likely that stories of other mythical, human-like creatures such as yetis are founded on grains of truth.\" \"Cryptozoology,\" Gee says, \"can come in from the cold.\"\n\nHowever, cryptozoology is widely criticised for an array of reasons and is rejected by the academic world. There is a broad consensus from academics that cryptozoology is a pseudoscience. The field is regularly criticized for reliance on anecdotal information and because in the course of investigating animals that most scientists believe are unlikely to have existed, cryptozoologists do not follow the scientific method. Hill notes that \"there is no academic course of study in cryptozoology or no university degree program that will bestow the title 'cryptozoologist'.\"\n\nAnthropologist Jeb J. Card summarizes cryptozoology in a survey of pseudoscience and pseudoarchaeology:\n\nCard notes that \"cryptozoologists often show their disdain and even hatred for professional scientists, including those who enthusiastically participated in cryptozoology\", which he traces back to Heuvelmans's early \"rage against critics of cryptozoology\". He finds parallels with cryptozoology and other pseudosciences, such as ghost hunting and ufology, and compares the approach of cryptozoologists to colonial big-game hunters, and to aspects of European imperialism. According to Card, \"Most cryptids are framed as the subject of indigenous legends typically collected in the hayday of comparative folklore, though such legends may be heavily modified or worse. Cryptozoology's complicated mix of sympathy, interest, and appropriation of indigenous culture (or non-indigenous construction of it) is also found in New Age circles and dubious \"Indian burial grounds\" and other legends ... invoked in hauntings such as the \"Amityville\" hoax ...\".\n\nIn a 2011 foreword for \"The American Biology Teacher\", then National Association of Biology Teachers president Dan Ward uses cryptozoology as an example of \"technological pseudoscience\" that may confuse students about the scientific method. Ward says that \"Cryptozoology … is not valid science or even science at all. It is monster hunting.\" Historian of science Brian Regal includes an entry for cryptozoology in his \"Pseudoscience: A Critical Encyclopedia\" (2009). Regal says that \"as an intellectual endeavor, cryptozoology has been studied as much as cryptozoologists have sought hidden animals\".\n\nIn a 1992 issue of \"Folklore\", folklorist Véronique Campion-Vincent says:\n\nCampion-Vincent says that \"four currents can be distinguished in the study of mysterious animal appearances\": \"Forteans\" (\"compiler[s] of anomalies\" such as via publications like the \"Fortean Times\"), \"occultists\" (which she describes as related to \"Forteans\"), \"folklorists\", and \"cryptozoologists\". Regarding cryptozoologists, Campion-Vincent says that \"this movement seems to deserve the appellation of parascience, like parapsychology: the same corpus is reviewed; many scientists participate, but for those who have an official status of university professor or researcher, the participation is a private hobby\".\n\nIn her \"Encyclopedia of American Folklore\", academic Linda Watts says that \"folklore concerning unreal animals or beings, sometimes called monsters, is a popular field of inquiry\" and describes cryptozoology as an example of \"American narrative traditions\" that \"feature many monsters\".\n\nIn his analysis of cryptozoology, folklorist Peter Dendle says that \"cryptozoology devotees consciously position themselves in defiance of mainstream science\" and that:\n\nIn a paper published in 2013, Dendle refers to cryptozoologists as \"contemporary monster hunters\" that \"keep alive a sense of wonder in a world that has been very thoroughly charted, mapped, and tracked, and that is largely available for close scrutiny on Google Earth and satellite imaging\" and that \"on the whole the devotion of substantial resources for this pursuit betrays a lack of awareness of the basis for scholarly consensus (largely ignoring, for instance, evidence of evolutionary biology and the fossil record).\"\n\nAccording to historian Mike Dash, few scientists doubt there are thousands of unknown animals, particularly invertebrates, awaiting discovery; however, cryptozoologists are largely uninterested in researching and cataloging newly discovered species of ants or beetles, instead focusing their efforts towards \"more elusive\" creatures that have often defied decades of work aimed at confirming their existence.\n\nPaleontologist George Gaylord Simpson (1984) lists cryptozoology among examples of human gullibility, along with creationism:\n\nPaleontologist Donald Prothero (2007) cites cryptozoology as an example of pseudoscience, and categorizes it along with Holocaust denial and UFO abductions claims as aspects of American culture that are \"clearly baloney\".\n\nIn \"Scientifical Americans: The Culture of Amateur Paranormal Researchers\" (2017), Hill surveys the field and discusses aspects of the subculture, noting internal attempts at creating more scientific approaches and the involvement of Young Earth creationists and a prevalence of hoaxes. She concludes that many cryptozoologists are \"passionate and sincere in their belief that mystery animals exist. As such, they give deference to every report of a sighting, often without criticial questioning. As with the ghost seekers, cryptozoologists are convinced that they will be the ones to solve the mystery and make history. With the lure of mystery and money undermining diligent and ethical research, the field of cryptozoology has serious credibility problems.\"\n\nThere have been several organizations, of varying types, dedicated or related to cryptozoology. These include:\n\n"}
{"id": "22168509", "url": "https://en.wikipedia.org/wiki?curid=22168509", "title": "Does God Play Dice?", "text": "Does God Play Dice?\n\nDoes God Play Dice: The New Mathematics of Chaos is a non-fiction book about chaos theory written by British mathematician Ian Stewart. The book was initially published by Blackwell Publishing in 1989.\n\nIn this book, Stewart explains chaos theory to an audience presumably unfamiliar with it. As the book progresses the writing changes from simple explanations of chaos theory to in-depth, rigorous mathematical study. Stewart covers mathematical concepts such as differential equations, resonance, nonlinear dynamics, and probability. The book is illustrated with diagrams and graphs of mathematical concepts and equations when applicable.\n\nThe back of the book, and a summary of its content, reads, \"The science of chaos is forcing scientists to rethink Einstein's fundamental assumptions regarding the way the universe behaves. Chaos theory has already shown that simple systems, obeying precise laws, can nevertheless act in a random manner. Perhaps God plays dice within a cosmic game of complete law and order. Does God Play Dice? reveals a strange universe in which nothing may be as it seems. Familiar geometric shapes such as circles and ellipses give way to infinitely complex structures known as fractals, the fluttering of a butterfly's wings can change the weather, and the gravitational attraction of a creature in a distant galaxy can change the fate of the solar system.\"\n\nThe title of the book is a reference to a famous quote by Albert Einstein.\n\n\n"}
{"id": "57313769", "url": "https://en.wikipedia.org/wiki?curid=57313769", "title": "European Open Science Cloud", "text": "European Open Science Cloud\n\nThe European Open Science Cloud (EOSC) is a European Commission project to provide a public data repository which conforms to open science values.\n\nThe project began in 2015 with the plan that its organizers finish it by 2020. A European Union committee on research endorsed a plan for the cloud's development in May 2018.\n\nPublic meetings about the project have emphasized the ideological motivations for promoting open science.\n\n"}
{"id": "38797009", "url": "https://en.wikipedia.org/wiki?curid=38797009", "title": "Far-Fetched Facts", "text": "Far-Fetched Facts\n\nFar-fetched facts is a book by German anthropologist Richard Rottenburg published in German as \"Weit hergeholte Fakten\" in 2002; the English translation was released in 2009. The book is an ethnography-based, though fictionalized, polyphonic account of a waterworks improvement project in Tanzania (called Ruritania in the books fictionalization). The book follows the different stages of the development project by looking closely at the interactions between a Northern development bank, experts of an international consulting firm and African project managers. It focuses thereby on technologies of inscription that enable the reconnaissance and operationalization of improvement measures to be taken, but also guide the overall interaction between the different stakeholders of the project. Showing these representational and managerial practices the book lays bare the necessary day-to-day production of objectivity and legitimacy in development projects, but also the intricacies and inconsistencies of daily translation processes, that often lead to the rather disappointing results of such development enterprises. Inspired by organizational and science and technology studies “Far-fetched facts” provides an anthropological critique of the aid industry in Africa distinct from postdevelopment critique.\n\n\n\n"}
{"id": "5891930", "url": "https://en.wikipedia.org/wiki?curid=5891930", "title": "Frontiers of Science", "text": "Frontiers of Science\n\nFrontiers of Science was a popular illustrated comic strip created by Professor Stuart Butler of the School of Physics at the University of Sydney in collaboration with Robert Raymond, a documentary maker from the Australian Broadcasting Corporation (ABC) in 1961. The artist was Andrea Bresciani. After 1970 the comic was illustrated by David Emerson.\n\nIt explained scientific concepts and recent research and in a 3 or 4 panel illustrated strip in an accessible and easily comprehensible way. The strip was syndicated to hundreds of newspapers around the world for 25 years, from 1961 to 1987. It was also published as soft cover books.\n\nThe strips are archived at Rare Books and Special Collections in Fisher Library at the University of Sydney. The entire series is available for viewing online. \n\n\n"}
{"id": "44889652", "url": "https://en.wikipedia.org/wiki?curid=44889652", "title": "Future Earth", "text": "Future Earth\n\nFuture Earth is a 10-year international research program which aims to build knowledge about the environmental and human aspects of Global change, and to find solutions for sustainable development. It aims to increase the impact of scientific research on sustainable development.\n\nFuture Earth is an interdisciplinary research programme bringing together natural and social sciences, as well as the humanities, engineering and law, and focused on designing and producing research together with stakeholders from outside the scientific community.\n\nFuture Earth's mission is to \"build and connect global knowledge to intensify the impact of research and find new ways to accelerate sustainable development\". Its vision is for \"people to thrive in a sustainable and equitable world\". To do this, Future Earth aims to mobilize the international community of global environmental science researchers to: \n\nFuture Earth was launched in June 2012, at the UN Conference on Sustainable Development (Rio+20).\n\nA globally distributed consortium was appointed as the Secretariat of Future Earth in July 2014, with offices in Montreal (Canada), Stockholm (Sweden), Colorado (USA), Tokyo (Japan) and Paris (France).\n\nAmy Luers is the Executive Director.\n\nScientific research and synthesis in Future Earth is carried out by a number of international networks, known as ‘global research projects’, many of which were launched under the umbrella of the existing four global environmental change programmes, DIVERSITAS, the International Geosphere-Biosphere Programme (IGBP), the International Human Dimensions Programme (IHDP) and the World Climate Research Programme (WCRP). Some further projects arose out of the Earth System Science Partnership (ESSP). A formal process for the affiliation of these projects into Future Earth began in 2014. The projects are:\n\n\n\n"}
{"id": "30307883", "url": "https://en.wikipedia.org/wiki?curid=30307883", "title": "Guildhall Lectures", "text": "Guildhall Lectures\n\nThe Guildhall Lectures were an annual series of talks on the theme of communication, organised by the British Association.\n\nThe lectures, held in the London Guildhall, were sponsored and broadcast by Granada Television. The first set of three lectures were held in 1959, and they continued until at least 1984. Broadly on the theme of \"Communication in the Modern World\", they concerned the arts, sciences, politics and mass media.\n"}
{"id": "717634", "url": "https://en.wikipedia.org/wiki?curid=717634", "title": "Heterophenomenology", "text": "Heterophenomenology\n\nHeterophenomenology (\"phenomenology \"of another\", not oneself\") is a term coined by Daniel Dennett to describe an explicitly third-person, scientific approach to the study of consciousness and other mental phenomena. It consists of applying the scientific method with an anthropological bent, combining the subject's self-reports with all other available evidence to determine their mental state. The goal is to discover how the subject sees the world him- or herself, without taking the accuracy of the subject's view for granted.\n\nHeterophenomenology is put forth as the alternative to traditional Cartesian phenomenology, which Dennett calls \"lone-wolf autophenomenology\" to emphasize the fact that traditional phenomenology accepts the subject's self-reports as being authoritative. In contrast, heterophenomenology considers the subjects authoritative only about how things \"seem\" to them. It does not dismiss the Cartesian first-person perspective, but rather brackets it so that it can be intersubjectively verified by empirical means, allowing it to be submitted as scientific evidence. \n\nThe method requires a researcher to listen to the subjects and take what they say seriously, but to also look at everything else available to them, including the subject's bodily responses and environment, evidence provided by relevant neurological or psychological studies, the researcher's memories of their own experiences, and any other scientific data that might help to interpret what the subject has reported. \n\nDennett notes this method is actually the normal way that anyone will choose to investigate aspects of the mind. He writes: \"heterophenomenology is nothing new; it is nothing other than the method that has been used by psychophysicists, cognitive psychologists, clinical neuropsychologists, and just about everybody who has ever purported to study human consciousness in a serious, scientific way\".\n\nThe key role of heterophenomenology in Dennett's philosophy of consciousness is that it defines all that can or needs to be known about the mind. For any phenomenological question \"why do I experience X\", there is a corresponding heterophenomenological question \"why does the subject say 'I experience X'\". To quote Dennett, \"The total set of details of heterophenomenology, plus all the data we can gather about concurrent events in the brains of subjects and in the surrounding environment, comprise the total data set for a theory of human consciousness. It leaves out no objective phenomena and no subjective phenomena of consciousness.\"\n\n\n\n__notoc__\n"}
{"id": "1629075", "url": "https://en.wikipedia.org/wiki?curid=1629075", "title": "Ignition tube", "text": "Ignition tube\n\nAn ignition tube is a piece of laboratory equipment. It is a laboratory tube used much in the same way as a boiling tube except not being as large and thick-walled. It is primarily used to hold small quantities of substances which are undergoing direct heating by a Bunsen burner or other heat source. \n\nIgnition tubes are used in the sodium fusion test.\n\nIgnition tubes are often difficult to clean due to the small bore. When used to heat substances strongly, some char may stick to the walls as well. They are usually disposable.\n\n"}
{"id": "41856558", "url": "https://en.wikipedia.org/wiki?curid=41856558", "title": "Incomplete Nature", "text": "Incomplete Nature\n\nIncomplete Nature: How Mind Emerged from Matter is a 2011 book by biological anthropologist Terrence Deacon. The book covers topics in biosemiotics, philosophy of mind, and the origins of life. Broadly, the book seeks to naturalistically explain \"aboutness\", that is, concepts like intentionality, meaning, normativity, purpose, and function; which Deacon groups together and labels as ententional phenomena.\n\nDeacon's first book, \"The Symbolic Species\" focused on the evolution of human language. In that book, Deacon notes that much of the mystery surrounding language origins comes from a profound confusion on the nature of semiotic processes themselves. Accordingly, the focus of \"Incomplete Nature\" shifts from human origins to the origin of life and semiosis. \"Incomplete Nature\" can be viewed as a sizable contribution to the growing body of work positing that the problem of consciousness and the problem of the origin of life are inexorably linked. Deacon tackles these two linked problems by going back to basics. The book expands upon the classical conceptions of work and information in order to give an account of ententionality that is consistent with eliminative materialism and yet does not seek to explain away or pass off as epiphenominal the non-physical properties of life.\n\nA central thesis of the book is that absence can still be efficacious. Deacon makes the claim that just as the concept of zero revolutionized mathematics, thinking about life, mind, and other ententional phenomena in terms of constraints (i.e., what is absent) can similarly help us overcome the artificial dichotomy of the mind body problem. A good example of this concept is the hole that defines the hub of a wagon wheel. The hole itself is not a physical thing, but rather a source of constraint that helps to restrict the conformational possibilities of the wheel's components, such that, on a global scale, the property of rolling emerges. Constraints which produce emergent phenomena may not be a process which can be understood by looking at the make-up of the constituents of a pattern. Emergent phenomena are difficult to study because their complexity does not necessarily decompose into parts. When a pattern is broken down, the constraints are no longer at work; there is no hole, no absence to notice. Imagine a hub, a hole for an axle, produced only when the wheel is rolling, thus breaking the wheel may not show you how the hub emerges.\n\nDeacon notes that the apparent patterns of causality exhibited by living systems seem to be in some ways the inverse of the causal patterns of non-living systems. In an attempt to find a solution to the philosophical problems associated with teleological explanations, Deacon returns to Aristotle's four causes and attempts to modernize them with thermodynamic concepts.\n\nOrthograde changes are caused internally. They are spontaneous changes. That is, orthograde changes are generated by the spontaneous elimination of asymmetries in a thermodynamic system in disequilibrium. Because orthograde changes are driven by the internal geometry of a changing system, orthograde causes can be seen as analogous to Aristotle's formal cause. More loosely, Aristotle's final cause can also be considered orthograde, because goal oriented actions are caused from within.\n\nContragrade changes are imposed from the outside. They are non-spontaneous changes. Contragrade change is induced when one thermodynamic system interacts with the orthograde changes of another thermodynamic system. The interaction drives the first system into a higher energy, more asymmetrical state. Contragrade changes do work. Because contragrade changes are driven by external interactions with another changing system, contragrade causes can be seen as analogous to Aristotle's efficient cause.\n\nMuch of the book is devoted to expanding upon the ideas of classical thermodynamics, with an extended discussion about how consistently far from equilibrium systems can interact and combine to produce novel emergent properties. \nDeacon defines three hierarchically nested levels of thermodynamic systems: Homeodynamic systems combine to produce morphodynamic systems which combine to produce teleodynamic systems. Teleodynamic systems can be further combined to produce higher orders of self organization.\n\nHomeodynamic systems are essentially equivalent to classical thermodynamic systems like a gas under pressure or solute in solution, but the term serves to emphasize that homeodynamics is an abstract process that can be realized in forms beyond the scope of classic thermodynamics. For example, the diffuse brain activity normally associated with emotional states can be considered to be a homeodynamic system because there is a general state of equilibrium which its components (neural activity) distribute towards. In general, a homeodynamic system is any collection of components that will spontaneously eliminate constraints by rearranging the parts until a maximum entropy state (disorderliness) is achieved.\n\nA morphodynamic system consists of a coupling of two homeodynamic systems such that the constraint dissipation of each complements the other, producing macroscopic order out of microscopic interactions. Morphodynamic systems require constant perturbation to maintain their structure, so they are relatively rare in nature. The paradigm example of a morphodynamic system is a Rayleigh–Bénard cell. Other common examples are snowflake formation, whirlpools and the stimulated emission of laser light.\nMaximum entropy production: The organized structure of a morphodynamic system forms to facilitate maximal entropy production. In the case of a Rayleigh–Bénard cell, heat at the base of the liquid produces an uneven distribution of high energy molecules which will tend to diffuse towards the surface. As the temperature of the heat source increases, density effects come into play. Simple diffusion can no longer dissipate energy as fast as it is added and so the bottom of the liquid becomes hot and more buoyant than the cooler, denser liquid at the top. The bottom of the liquid begins to rise, and the top begins to sink - producing convection currents.\n\nTwo systems: The significant heat differential on the liquid produces two homeodynamic systems. The first is a diffusion system, where high energy molecules on the bottom collide with lower energy molecules on the top until the added kinetic energy from the heat source is evenly distributed. The second is a convection system, where the low density fluid on the bottom mixes with the high density fluid on the top until the density becomes evenly distributed. The second system arises when there is too much energy to be effectively dissipated by the first, and once both systems are in place, they will begin to interact.\n\nSelf organization: The convection creates currents in the fluid that disrupt the pattern of heat diffusion from bottom to top. Heat begins to diffuse into the denser areas of current, irrespective of the vertical location of these denser portions of fluid. The areas of the fluid where diffusion is occurring most rapidly will be the most viscous because molecules are rubbing against each other in opposite directions. The convection currents will shun these areas in favor of parts of the fluid where they can flow more easily. And so the fluid spontaneously segregates itself into cells where high energy, low density fluid flows up from the center of the cell and cooler, denser fluid flows down along the edges, with diffusion effects dominating in the area between the center and the edge of each cell.\n\nSynergy and constraint: What is notable about morphodynamic processes is that order spontaneously emerges explicitly because the ordered system that results is more efficient at increasing entropy than a chaotic one. In the case of the Rayleigh–Bénard cell, neither diffusion nor convection on their own will produce as much entropy as both effects coupled together. When both effects are brought into interaction, they constrain each other into a particular geometric form because that form facilitates minimal interference between the two processes. The orderly hexagonal form is stable as long as the energy differential persists, and yet the orderly form more effectively degrades the energy differential than any other form. This is why morphodynamic processes in nature are usually so short lived. They are self organizing, but also self undermining.\n\nA teleodynamic system consists of coupling two morphodynamic systems such that the self undermining quality of each is constrained by the other. Each system prevents the other from dissipating all of the energy available, and so long term organizational stability is obtained. Deacon claims that we should pinpoint the moment when two morphodynamic systems reciprocally constrain each other as the point when ententional qualities like function, purpose and normativity emerge.\n\nDeacon explores the properties of teleodynamic systems by describing a chemically plausible model system called an autogen. Deacon emphasizes that the specific autogen he describes is not a proposed description of the first life form, but rather a description of the kinds of thermodynamic synergies that the first living creature likely possessed.\n\nReciprocal catalysis: An autogen consists of two self catalyzing cyclical morphodynamic chemical reactions, similar to a chemoton. In one reaction, organic molecules react in a looped series, the products of one reaction becoming the reactants for the next. This looped reaction is self amplifying, producing more and more reactants until all the substrate is consumed. A side product of this reciprocally catalytic loop is a lipid that can be used as a reactant in a second reaction. This second reaction creates a boundary (either a microtubule or some other closed capsid like structure), that serves to contain the first reaction. The boundary limits diffusion; it keeps all of the necessary catalysts in close proximity to each other. In addition, the boundary prevents the first reaction from completely consuming all of the available substrate in the environment.\n\nThe first self: Unlike an isolated morphodynamic process whose organization rapidly eliminates the energy gradient necessary to maintain its structure, a teleodynamic process is self-limiting and self preserving. The two reactions complement each other, and ensure that neither ever runs to equilibrium - that is completion, cessation, and death. So, in a teleodynamic system there will be structures that embody a preliminary sketch of a biological function. The internal reaction network functions to create the substrates for the boundary reaction, and the boundary reaction functions to protect and constrain the internal reaction network. Either process in isolation would be abiotic but together they create a system with a normative status dependent on the functioning of its component parts.\n\nAs with other concepts in the book, in his discussion of work Deacon seeks to generalize the Newtonian conception of work such that the term can be used to describe and differentiate mental phenomena - to describe \"that which makes daydreaming effortless but metabolically equivalent problem solving difficult.\" Work is generally described as \"activity that is necessary to overcome resistance to change. Resistance can be either active or passive, and so work can be directed towards enacting change that wouldn't otherwise occur or preventing change that would happen in its absence.\" Using the terminology developed earlier in the book, work can be considered to be \"the organization of differences between orthograde processes such that a locus of contragrade process is created. Or, more simply, work is a spontaneous change inducing a non-spontaneous change to occur.\"\n\nA thermodynamic systems capacity to do work depends less upon the total energy of the system and more upon the geometric distribution of its components. A glass of water at 20 degrees Celsius will have the same amount of energy as a glass divided in half with the top fluid at 30 degrees and the bottom at 10, but only in the second glass will the top half have the capacity to do work upon the bottom. This is because work occurs at both macroscopic and microscopic levels. Microscopically, there is constant work being performed on one molecule by another when they collide. But the potential for this microscopic work to additively sum to macroscopic work depends on there being an asymmetric distribution of particle speeds, so that the average collision pushes in a focused direction. Microscopic work is necessary but not sufficient for macroscopic work. A global property of asymmetric distribution is also required.\n\nBy recognizing that asymmetry is a general property of work - that work is done as asymmetric systems spontaneously tend towards symmetry, Deacon abstracts the concept of work and applies it to systems whose symmetries are vastly more complex than those covered by classical thermodynamics. In a morphodynamic system, the tendency towards symmetry produces not global equilibrium, but a complex geometric form like a hexagonal Benard cell or the resonant frequency of a flute. This tendency towards convolutedly symmetric forms can be harnessed to do work on other morphodynamic systems, if the systems are properly coupled.\n\nResonance example: A good example of morphodynamic work is the induced resonance that can be observed by singing or playing a flute next to a string instrument like a harp or guitar. The vibrating air emitted from the flute will interact with the taut strings. If any of the strings are tuned to a resonant frequency that matches the note being played, they too will begin to vibrate and emit sound.\n\nContragrade change: When energy is added to the flute by blowing air into it, there is a spontaneous (orthograde) tendency for the system to dissipate the added energy by inducing the air within the flute to vibrate at a specific frequency. This orthograde morphodynamic form generation can be used to induce contragrade change in the system coupled to it - the taught string. Playing the flute does work on the string by causing it to enter a high energy state that could not be reached spontaneously in an uncoupled state.\n\nStructure and form: Importantly, this is not just the macro scale propagation of random micro vibrations from one system to another. The global geometric structure of the system is essential. The total energy transferred from the flute to the string matters far less than the patterns it takes in transit. That is, the amplitude of the coupled note is irrelevant, what matters is its frequency. Notes that have a higher or lower frequency than the resonant frequency of the string will not be able to do morphodynamic work.\n\nWork is generally defined to be the interaction of two orthograde changing systems such that contragrade change is produced. In teleodynamic systems, the spontaneous orthograde tendency is not to equilibriate (as in homeodynamic systems), nor to self simplify (as in morphodynamic systems) but rather to tend towards self-preservation. Living organisms spontaneously tend to heal, to reproduce and to pursue resources towards these ends. Teleodynamic work acts on these tendencies and pushes them in a contragrade, non-spontaneous direction. \nEvolution as work: Natural selection, or perhaps more accurately, adaptation, can be considered to be a ubiquitous form of teleodynamic work. The othograde self-preservation and reproduction tendencies of individual organisms tends to undermine those same tendencies in conspecifics. This competition produces a constraint that tends to mold organisms into forms that are more adapted to their environments – forms that would otherwise not spontaneously persist.\n\nFor example, in a population of New Zealand wrybill who make a living by searching for grubs under rocks, those that have a bent beak gain access to more calories. Those with bent beaks are able to better provide for their young, and at the same time they remove a disproportionate quantity of grubs from their environment, making it more difficult for those with straight beaks to provide for their own young. Throughout their lives, all the wrybills in the population do work to structure the form of the next generation. The increased efficiency of the bent beak causes that morphology to dominate the next generation. Thus an asymmetry of beak shape distribution is produced in the population - an asymmetry produced by teleodynamic work.\n\nThought as work: Mental problem solving can also be considered teleodynamic work. Thought forms are spontaneously generated, and task of problem solving is the task of molding those forms to fit the context of the problem at hand. Deacon makes the link between evolution as teleodynamic work and thought as teleodynamic work explicit. \"The experience of being sentient is what it feels like to \"be\" evolution.\"\n\nBy conceiving of work in this way, Deacon claims \"we can begin to discern \"a basis for a form of causal openness\" in the universe.\" While increases in complexity in no way alter the laws of physics, by juxtaposing systems together, pathways of spontaneous change can be made available that were inconceivably improbable prior to the systems coupling. The causal power of any complex living system lies not solely in the underlying quantum mechanics but also in the global arrangement of its components. A careful arrangement of parts can constrain possibilities such that phenomena that were formerly impossibly rare can become improbably common.\n\nOne of the central purposes of Incomplete Nature is to articulate a theory of biological information. The first formal theory of information was articulated by Claude Shannon in 1948 in his work A Mathematical Theory of Communication. Shannon's work is widely credited with ushering in the information age, but somewhat paradoxically, it was completely silent on questions of meaning and reference, i.e., what the information is \"about.\" As an engineer, Shannon was concerned with the challenge of reliably transmitting a message from one location to another. The meaning and content of the message was largely irrelevant. So, while Shannon information theory has been essential for the development of devices like computers, it has left open many philosophical questions regarding the nature of information. Incomplete Nature seeks to answer some of these questions.\n\nShannon's key insight was to recognize a link between entropy and information. Entropy is often defined as a measurement of disorder, or randomness, but this can be misleading. For Shannon's purposes, the entropy of a system is the number of possible states that the system has the capacity to be in. Any one of these potential states can constitute a message. For example, a typewritten page can bear as many different messages as there are different combinations of characters that can be arranged on the page. The information content of a message can only be understood against the background context of all of the messages that could have been sent, but weren't. Information is produced by a reduction of entropy in the message medium. \nShannon's information based conception of entropy should be distinguished from the more classic thermodynamic conception of entropy developed by Ludwig Boltzmann and others at the end of the nineteenth century. While Shannon entropy is static and has to do with the set of all possible messages/states that a signal bearing system might take, Boltzmann entropy has to do with the tendency of all dynamic systems to tend towards equilibrium. That is, there are many more ways for a collection of particles to be well mixed than to be segregated based on velocity, mass, or any other property. Boltzmann entropy is central to the theory of work developed earlier in the book because entropy dictates the direction in which a system will spontaneously tend.\n\nDeacon's addition to Shannon information theory is to propose a method for describing not just how a message is transmitted, but also how it is interpreted. Deacon weaves together Shannon entropy and Boltzmann entropy in order to develop a theory of interpretation based in teleodynamic work. Interpretation is inherently normative. Data becomes information when it has significance for its interpreter. Thus interpretive systems are teleodynamic - the interpretive process is designed to perpetuate itself. \"The interpretation of something as information indirectly reinforces the capacity to do this again.\"\n\n"}
{"id": "19887263", "url": "https://en.wikipedia.org/wiki?curid=19887263", "title": "Innovative Technology Experiences for Students", "text": "Innovative Technology Experiences for Students\n\nFunded by the National Science Foundation, the Innovative Technology Experiences for Students and Teachers (ITEST) program was established in 2003 to address the looming shortage of technology workers in the United States. ITEST engages students and teachers in authentic, hands-on learning experiences in science, technology, engineering, and mathematics (commonly referred to as 'STEM' or 'SET' [more common in the U.K.]).\n\nAs of early 2012, the program is in its ninth year. Over 195 individual projects across 43 states have been funded. It has impacted:\n\nThe ITEST Learning Resource Center (ITEST LRC) at Education Development Center assists the projects in building bridges between formal and informal learning by facilitating an inclusive community of practice. Findings and lessons learned are shared nationally to improve policy and practice. Visit the website with the link below, or view a 2-page overview (Snapshot).\n\n(1) https://www.nsf.gov/funding/pgm_summ.jsp?pims_id=5467\n(2) http://itestlrc.edc.org/\n(3) http://itestlrc.edc.org/sites/itestlrc.edc.org/files/Snapshot_2012_Final.pdf\n"}
{"id": "34188169", "url": "https://en.wikipedia.org/wiki?curid=34188169", "title": "List of philosophers of technology", "text": "List of philosophers of technology\n\nThis is a list of philosophers of technology. It includes philosophers from other disciplines who are recognised as having made an important contribution to the field, for example those commonly included in reference anthologies.\n\n\n\n\n\n"}
{"id": "267014", "url": "https://en.wikipedia.org/wiki?curid=267014", "title": "List of topics characterized as pseudoscience", "text": "List of topics characterized as pseudoscience\n\nThis is a list of topics that have, at one point or another in their history, been characterized as pseudoscience by academics or researchers. Discussion about these topics is done on their main pages. These characterizations were made in the context of educating the public about questionable or potentially fraudulent or dangerous claims and practices—efforts to define the nature of science, or humorous parodies of poor scientific reasoning.\n\nCriticism of pseudoscience, generally by the scientific community or skeptical organizations, involves critiques of the logical, methodological, or rhetorical bases of the topic in question. Though some of the listed topics continue to be investigated scientifically, others were only subject to scientific research in the past, and today are considered refuted but resurrected in a pseudoscientific fashion. Other ideas presented here are entirely non-scientific, but have in one way or another impinged on scientific domains or practices.\n\nMany adherents or practitioners of the topics listed here dispute their characterization as pseudoscience. Each section here summarizes the alleged pseudoscientific aspects of that topic.\n\n\n\n\n\n\n\n\nPseudoscientific medical practices are often known as quackery. In contrast, modern medicine is (or seeks to be) evidence-based.\n\n\n\n\n\nParanormal subjects have been subject to critiques from a wide range of sources including the following claims of paranormal significance:\n\n\n\nSpiritual and religious practices and beliefs, according to astronomer Carl Sagan, are normally not classified as pseudoscience. However, religion can sometimes nurture pseudoscience, and \"at the extremes it is difficult to distinguish pseudoscience from rigid, doctrinaire religion\", and some religions might be confused with pseudoscience, such as traditional meditation. The following religious/spiritual items have been related to or classified as pseudoscience in some way:\n\n\nCreation science or scientific creationism is a branch of creationism that claims to provide scientific support for the Genesis creation narrative in the Book of Genesis and disprove or reexplain the scientific facts, theories and scientific paradigms about geology, cosmology, biological evolution, archeology, history and linguistics.\n\n\n\n\nThe following concepts have only a very small number of proponents, yet have become notable:\n\n\n"}
{"id": "30234767", "url": "https://en.wikipedia.org/wiki?curid=30234767", "title": "Living Earth Simulator Project", "text": "Living Earth Simulator Project\n\nThe living Earth simulator is a proposed massive computer simulation system intended to simulate the interactions of all aspects of life, human economic activity, climate, and other physical processes on the planet Earth as part of the FuturICT project, in response to the European FP7 \"Future and Emerging Technologies Flagship\" initiative.\n\nThere are over 300 international teams seeking ~€1 billion for the 10-year Future and Emerging Technologies ‘flagship’ competition. The Earth Simulator was not selected since the two winners have been announced as of March 2013. The winners were Graphene and Human Brain.\n\n\n"}
{"id": "9520279", "url": "https://en.wikipedia.org/wiki?curid=9520279", "title": "Mandat International", "text": "Mandat International\n\nMandat International, also known as the International Cooperation Foundation, is an international non-governmental organization based in Geneva, Switzerland with consultative status to the United Nations Economic and Social Council, the UNDPI, and the United Nations Conference on Trade and Development.\n\nMandat Internatioal aims to promote international dialogue and cooperation, including in the research domain. It promotes international law, data protection and sustainable development. It is strictly independent from any political or religious affiliation.\n\nMandat International is a foundation with public utility status, relying on a network of members covering the various fields of activity of international cooperation. It collaborates closely with most of the international organizations and is independent of any political or religious affiliation. Its first aim is is to promote international cooperation, including in the research domain. Its activities encompass various domains, from ICT research, to international law and privacy, multistakeholder dialogue and international research projects. Over time, the foundation has developed many projects, including several international research projects, international conferences and events, support to over 10’000 delegates for attending international conferences, educational programes, creating the Eduki foundation, legal search engines, conferences, etc. The foundation usually supports projects for a limited period of time but with the objective to have a sustainable impact. When its objectives are achieved it usually pioneers and addresses new emerging challenges.\n\nThe current focus of the foundation is on promoting international cooperation with the research community, developing international research projects, supporting technology transfer from the ressearch community to standardization and international organizations, and promoting international law. The foundation is also actively engaged in the Internet of Things domain to promote international cooperation and dissemination, in line with the International Declaration on Internet of Things for Sustainable Development adopted by the IoT Forum and the ITU.\n\n"}
{"id": "11817317", "url": "https://en.wikipedia.org/wiki?curid=11817317", "title": "Mechanical explanations of gravitation", "text": "Mechanical explanations of gravitation\n\nMechanical explanations of gravitation (or kinetic theories of gravitation) are attempts to explain the action of gravity by aid of basic mechanical processes, such as pressure forces caused by pushes, without the use of any action at a distance. These theories were developed from the 16th until the 19th century in connection with the aether. However, such models are no longer regarded as viable theories within the mainstream scientific community and general relativity is now the standard model to describe gravitation without the use of actions at a distance. Modern \"quantum gravity\" hypotheses also attempt to describe gravity by more fundamental processes such as particle fields, but they are not based on classical mechanics.\n\nThis theory is probably the best-known mechanical explanation, and was developed for the first time by Nicolas Fatio de Duillier in 1690, and re-invented, among others, by Georges-Louis Le Sage (1748), Lord Kelvin (1872), and Hendrik Lorentz (1900), and criticized by James Clerk Maxwell (1875), and Henri Poincaré (1908).\n\nThe theory posits that the force of gravity is the result of tiny particles or waves moving at high speed in all directions, throughout the universe. The intensity of the flux of particles is assumed to be the same in all directions, so an isolated object A is struck equally from all sides, resulting in only an inward-directed pressure but no net directional force. With a second object B present, however, a fraction of the particles that would otherwise have struck A from the direction of B is intercepted, so B works as a shield, so-to-speak—that is, from the direction of B, A will be struck by fewer particles than from the opposite direction. Likewise, B will be struck by fewer particles from the direction of A than from the opposite direction. One can say that A and B are \"shadowing\" each other, and the two bodies are pushed toward each other by the resulting imbalance of forces.\nThis shadow obeys the inverse square law, because the imbalance of momentum flow over an entire spherical surface enclosing the object is independent of the size of the enclosing sphere, whereas the surface area of the sphere increases in proportion to the square of the radius. To satisfy the need for mass proportionality, the theory posits that a) the basic elements of matter are very small so that gross matter consists mostly of empty space, and b) that the particles are so small, that only a small fraction of them would be intercepted by gross matter. The result is, that the \"shadow\" of each body is proportional to the surface of every single element of matter.\n\n\"Criticism\": This theory was declined primarily for thermodynamic reasons because a shadow only appears in this model if the particles or waves are at least partly absorbed, which should lead to an enormous heating of the bodies. Also drag, \"i.e.\" the resistance of the particle streams in the direction of motion, is a great problem too. This problem can be solved by assuming superluminal speeds, but this solution largely increases the thermal problems and contradicts special relativity.\n\nBecause of his philosophical beliefs, René Descartes proposed in 1644 that no empty space can exist and that space must consequently be filled with matter. The parts of this matter tend to move in straight paths, but because they lie close together, they can not move freely, which according to Descartes implies that every motion is circular, so the aether is filled with vortices. Descartes also distinguishes between different forms and sizes of matter in which rough matter resists the circular movement more strongly than fine matter. Due to centrifugal force, matter tends towards the outer edges of the vortex, which causes a condensation of this matter there. The rough matter cannot follow this movement due to its greater inertia—so due to the pressure of the condensed outer matter those parts will be pushed into the center of the vortex. According to Descartes, this inward pressure is nothing else than gravity. He compared this mechanism with the fact that if a rotating, liquid filled vessel is stopped, the liquid goes on to rotate. Now, if one drops small pieces of light matter (e.g. wood) into the vessel, the pieces move to the middle of the vessel.\n\nFollowing the basic premises of Descartes, Christiaan Huygens between 1669 and 1690 designed a much more exact vortex model. This model was the first theory of gravitation which was worked out mathematically. He assumed that the aether particles are moving in every direction, but were thrown back at the outer borders of the vortex and this causes (as in the case of Descartes) a greater concentration of fine matter at the outer borders. So also in his model the fine matter presses the rough matter into the center of the vortex. Huygens also found out that the centrifugal force is equal to the force, which acts in the direction of the center of the vortex (centripetal force). He also posited that bodies must consist mostly of empty space so that the aether can penetrate the bodies easily, which is necessary for mass proportionality. He further concluded that the aether moves much faster than the falling bodies. At this time, Newton developed his theory of gravitation which is based on attraction, and although Huygens agreed with the mathematical formalism, he said the model was insufficient due to the lack of a mechanical explanation of the force law. Newton's discovery that gravity obeys the inverse square law surprised Huygens and he tried to take this into account by assuming that the speed of the aether is smaller in greater distance.\n\n\"Criticism\": Newton objected to the theory because drag must lead to noticeable deviations of the orbits which were not observed. Another problem was that moons often move in different directions, against the direction of the vortex motion. Also, Huygens' explanation of the inverse square law is circular, because this means that the aether obeys Kepler's third law. But a theory of gravitation has to explain those laws and must not presuppose them.\n\nIn a 1675 letter to Henry Oldenburg, and later to Robert Boyle, Newton wrote the following: [Gravity is the result of] “a condensation causing a flow of ether with a corresponding thinning of the ether density associated with the increased velocity of flow.” He also asserted that such a process was consistent with all his other work and Kepler's Laws of Motion. Newtons' idea of a pressure drop associated with increased velocity of flow was mathematically formalised as Bernoulli's principle published in Daniel Bernoulli's book \"Hydrodynamica\" in 1738.\n\nHowever, although he later proposed a second explanation (see section below), Newton's comments to that question remained ambiguous. In the third letter to Bentley in 1692 he wrote:\n\nIt is inconceivable that inanimate brute matter should, without the mediation of something else which is not material, operate upon and affect other matter, without mutual contact, as it must do if gravitation in the sense of Epicurus be essential and inherent in it. And this is one reason why I desired you would not ascribe 'innate gravity' to me. That gravity should be innate, inherent, and essential to matter, so that one body may act upon another at a distance, through a vacuum, without the mediation of anything else, by and through which their action and force may be conveyed from one to another, is to me so great an absurdity, that I believe no man who has in philosophical matters a competent faculty of thinking can ever fall into it. Gravity must be caused by an agent acting constantly according to certain laws; but whether this agent be material or immaterial, I have left to the consideration of my readers.\n\nOn the other hand, Newton is also well known for the phrase Hypotheses non fingo, written in 1713:\n\nI have not as yet been able to discover the reason for these properties of gravity from phenomena, and I do not feign hypotheses. For whatever is not deduced from the phenomena must be called a hypothesis; and hypotheses, whether metaphysical or physical, or based on occult qualities, or mechanical, have no place in experimental philosophy. In this philosophy particular propositions are inferred from the phenomena, and afterwards rendered general by induction.\n\nAnd according to the testimony of some of his friends, such as Nicolas Fatio de Duillier or David Gregory, Newton thought that gravitation is based directly on divine influence.\n\nSimilar to Newton, but mathematically in greater detail, Bernhard Riemann assumed in 1853 that the gravitational aether is an incompressible fluid and normal matter represents sinks in this aether. So if the aether is destroyed or absorbed proportionally to the masses within the bodies, a stream arises and carries all surrounding bodies into the direction of the central mass. Riemann speculated that the absorbed aether is transferred into another world or dimension.\n\nAnother attempt to solve the energy problem was made by Ivan Osipovich Yarkovsky in 1888. Based on his aether stream model, which was similar to that of Riemann, he argued that the absorbed aether might be converted into new matter, leading to a mass increase of the celestial bodies.\n\n\"Criticism\": As in the case of Le Sage's theory, the disappearance of energy without explanation violates the energy conservation law. Also some drag must arise, and no process which leads to a creation of matter is known.\n\nNewton updated the second edition of \"Optics\" (1717) with another mechanical-ether theory of gravity. Unlike his first explanation (1675 - see Streams), he proposed a stationary aether which gets thinner and thinner nearby the celestial bodies. On the analogy of the lift, a force arises, which pushes all bodies to the central mass. He minimized drag by stating an extremely low density of the gravitational aether.\n\nLike Newton, Leonhard Euler presupposed in 1760 that the gravitational aether loses density in accordance with the inverse square law. Similarly to others, Euler also assumed that to maintain mass proportionality, matter consists mostly of empty space.\n\n\"Criticism\": Both Newton and Euler gave no reason why the density of that static aether should change. Furthermore, James Clerk Maxwell pointed out that in this \"hydrostatic\" model \"the state of stress... which we must suppose to exist in the invisible medium, is 3000 times greater than that which the strongest steel could support\".\n\nRobert Hooke speculated in 1671 that gravitation is the result of all bodies emitting waves in all directions through the aether. Other bodies, which interact with these waves, move in the direction of the source of the waves. Hooke saw an analogy to the fact that small objects on a disturbed surface of water move to the center of the disturbance.\n\nA similar theory was worked out mathematically by James Challis from 1859 to 1876. He calculated that the case of attraction occurs if the wavelength is large in comparison with the distance between the gravitating bodies. If the wavelength is small, the bodies repel each other. By a combination of these effects, he also tried to explain all other forces.\n\n\"Criticism\": Maxwell objected that this theory requires a steady production of waves, which must be accompanied by an infinite consumption of energy.\nChallis himself admitted, that he hadn't reached a definite result due to the complexity of the processes.\n\nLord Kelvin (1871) and Carl Anton Bjerknes (1871) assumed that all bodies pulsate in the aether. This was in analogy to the fact that, if the pulsation of two spheres in a fluid is in phase, they will attract each other; and if the pulsation of two spheres is \"not\" in phase, they will repel each other. This mechanism was also used for explaining the nature of electric charges. Among others, this hypothesis has also been examined by George Gabriel Stokes and Woldemar Voigt.\n\n\"Criticism\" : To explain universal gravitation, one is forced to assume that all pulsations in the universe are in phase—which appears very implausible. In addition, the aether should be incompressible to ensure that attraction also arises at greater distances. And Maxwell argued that this process must be accompanied by a permanent new production and destruction of aether.\n\nIn 1690, Pierre Varignon assumed that all bodies are exposed to pushes by aether particles from all directions, and that there is some sort of limitation at a certain distance from the Earth's surface which cannot be passed by the particles. He assumed that if a body is closer to the Earth than to the limitation boundary, then the body would experience a greater push from above than from below, causing it to fall toward the Earth.\n\nIn 1748, Mikhail Lomonosov assumed that the effect of the aether is proportional to the complete surface of the elementary components of which matter consists (similar to Huygens and Fatio before him). He also assumed an enormous penetrability of the bodies. However, no clear description was given by him as to how exactly the aether interacts with matter so that the law of gravitation arises.\n\nIn 1821, John Herapath tried to apply his co-developed model of the kinetic theory of gases on gravitation. He assumed that the aether is heated by the bodies and loses density so that other bodies are pushed to these regions of lower density.\nHowever, it was shown by Taylor that the decreased density due to thermal expansion is compensated for by the increased speed of the heated particles; therefore, no attraction arises.\n\nThese mechanical explanations for gravity never gained widespread acceptance, although such ideas continued to be studied occasionally by physicists until the beginning of the twentieth century, by which time it was generally considered to be conclusively discredited. However, some researchers outside the scientific mainstream still try to work out some consequences of those theories.\n\nLe Sage's theory was studied by Radzievskii and Kagalnikova (1960), Shneiderov (1961), Buonomano and Engels (1976), Adamut (1982), Jaakkola (1996), Tom Van Flandern (1999), and Edwards (2007). A variety of Le Sage models and related topics are discussed in Edwards, et al.\n\nGravity due to static pressure was recently studied by Arminjon.\n\n"}
{"id": "42249546", "url": "https://en.wikipedia.org/wiki?curid=42249546", "title": "Muslim women in science and technology", "text": "Muslim women in science and technology\n\nMuslim women in sciences and technology, since the Islamic Golden Age, Muslims have been actively participating in various sciences.\n\nInternational Forum on Women in Science and Technology in Muslim Countries was held by the United Nations, regarding the changes women have brought in the sciences in Muslim countries.\n\nAnousheh Ansari is an Iranian-American engineer and co-founder and chairwoman of Prodea Systems, on September 18, 2006, a few days after her 40th birthday, she became the first Muslim woman in space.\n\n"}
{"id": "11055316", "url": "https://en.wikipedia.org/wiki?curid=11055316", "title": "Nadir line", "text": "Nadir line\n\nA nadir line is a line traced on the ground directly beneath an aircraft while taking aerial photographs of the ground from above. This line connects the image centers of the successive vertical photographs. These photographs are generally taken with some degree of \"endlap\". Can also represent a line traced by a shipping vessel.\n"}
{"id": "55212", "url": "https://en.wikipedia.org/wiki?curid=55212", "title": "Newton's laws of motion", "text": "Newton's laws of motion\n\nNewton's laws of motion are three physical laws that, together, laid the foundation for classical mechanics. They describe the relationship between a body and the forces acting upon it, and its motion in response to those forces. More precisely, the first law defines the force qualitatively, the second law offers a quantitative measure of the force, and the third asserts that a single isolated force doesn't exist. These three laws have been expressed in several ways, over nearly three centuries, and can be summarised as follows:\n\nThe three laws of motion were first compiled by Isaac Newton in his \"Philosophiæ Naturalis Principia Mathematica\" (\"Mathematical Principles of Natural Philosophy\"), first published in 1687. Newton used them to explain and investigate the motion of many physical objects and systems. For example, in the third volume of the text, Newton showed that these laws of motion, combined with his law of universal gravitation, explained Kepler's laws of planetary motion.\n\nA fourth law is often also described in the bibliography, which states that forces add up like vectors, that is, that forces obey the principle of superposition.\n\nNewton's laws are applied to objects which are idealised as single point masses, in the sense that the size and shape of the object's body are neglected to focus on its motion more easily. This can be done when the object is small compared to the distances involved in its analysis, or the deformation and rotation of the body are of no importance. In this way, even a planet can be idealised as a particle for analysis of its orbital motion around a star.\n\nIn their original form, Newton's laws of motion are not adequate to characterise the motion of rigid bodies and deformable bodies. Leonhard Euler in 1750 introduced a generalisation of Newton's laws of motion for rigid bodies called Euler's laws of motion, later applied as well for deformable bodies assumed as a continuum. If a body is represented as an assemblage of discrete particles, each governed by Newton's laws of motion, then Euler's laws can be derived from Newton's laws. Euler's laws can, however, be taken as axioms describing the laws of motion for extended bodies, independently of any particle structure.\n\nNewton's laws hold only with respect to a certain set of frames of reference called Newtonian or inertial reference frames. Some authors interpret the first law as defining what an inertial reference frame is; from this point of view, the second law holds only when the observation is made from an inertial reference frame, and therefore the first law cannot be proved as a special case of the second. Other authors do treat the first law as a corollary of the second. The explicit concept of an inertial frame of reference was not developed until long after Newton's death.\n\nIn the given interpretation mass, acceleration, momentum, and (most importantly) force are assumed to be externally defined quantities. This is the most common, but not the only interpretation of the way one can consider the laws to be a definition of these quantities.\n\nNewtonian mechanics has been superseded by special relativity, but it is still useful as an approximation when the speeds involved are much slower than the speed of light.\n\nThe first law states that if the net force (the vector sum of all forces acting on an object) is zero, then the velocity of the object is constant. Velocity is a vector quantity which expresses both the object's speed and the direction of its motion; therefore, the statement that the object's velocity is constant is a statement that both its speed and the direction of its motion are constant.\n\nThe first law can be stated mathematically when the mass is a non-zero constant, as,\nConsequently,\n\nThis is known as \"uniform motion\". An object \"continues\" to do whatever it happens to be doing unless a force is exerted upon it. If it is at rest, it continues in a state of rest (demonstrated when a tablecloth is skilfully whipped from under dishes on a tabletop and the dishes remain in their initial state of rest). If an object is moving, it continues to move without turning or changing its speed. This is evident in space probes that continuously move in outer space. Changes in motion must be imposed against the tendency of an object to retain its state of motion. In the absence of net forces, a moving object tends to move along a straight line path indefinitely.\n\nNewton placed the first law of motion to establish frames of reference for which the other laws are applicable. The first law of motion postulates the existence of at least one frame of reference called a Newtonian or inertial reference frame, relative to which the motion of a particle not subject to forces is a straight line at a constant speed. Newton's first law is often referred to as the \"law of inertia\". Thus, a condition necessary for the uniform motion of a particle relative to an inertial reference frame is that the total net force acting on it is zero. In this sense, the first law can be restated as:\n\nNewton's first and second laws are valid only in an inertial reference frame. Any reference frame that is in uniform motion with respect to an inertial frame is also an inertial frame, i.e. Galilean invariance or the principle of Newtonian relativity.\n\nThe second law states that the rate of change of momentum of a body is directly proportional to the force applied, and this change in momentum takes place in the direction of the applied force.\nThe second law can also be stated in terms of an object's acceleration. Since Newton's second law is valid only for constant-mass systems, can be taken outside the differentiation operator by the constant factor rule in differentiation. Thus,\n\nwhere F is the net force applied, \"m\" is the mass of the body, and a is the body's acceleration. Thus, the net force applied to a body produces a proportional acceleration. In other words, if a body is accelerating, then there is a force on it. An application of this notation is the derivation of G Subscript C.\n\nConsistent with the first law, the time derivative of the momentum is non-zero when the momentum changes direction, even if there is no change in its magnitude; such is the case with uniform circular motion. The relationship also implies the conservation of momentum: when the net force on the body is zero, the momentum of the body is constant. Any net force is equal to the rate of change of the momentum.\n\nAny mass that is gained or lost by the system will cause a change in momentum that is not the result of an external force. A different equation is necessary for variable-mass systems (see below).\n\nNewton's second law is an approximation that is increasingly worse at high speeds because of relativistic effects.\n\nAn impulse J occurs when a force F acts over an interval of time Δ\"t\", and it is given by\nSince force is the time derivative of momentum, it follows that\nThis relation between impulse and momentum is closer to Newton's wording of the second law.\n\nImpulse is a concept frequently used in the analysis of collisions and impacts.\n\nVariable-mass systems, like a rocket burning fuel and ejecting spent gases, are not closed and cannot be directly treated by making mass a function of time in the second law; that is, the following formula is wrong:\n\nThe falsehood of this formula can be seen by noting that it does not respect Galilean invariance: a variable-mass object with F = 0 in one frame will be seen to have F ≠ 0 in another frame.\nThe correct equation of motion for a body whose mass \"m\" varies with time by either ejecting or accreting mass is obtained by applying the second law to the entire, constant-mass system consisting of the body and its ejected/accreted mass; the result is\n\nwhere u is the velocity of the escaping or incoming mass relative to the body. From this equation one can derive the equation of motion for a varying mass system, for example, the Tsiolkovsky rocket equation.\nUnder some conventions, the quantity u d\"m\"/d\"t\" on the left-hand side, which represents the advection of momentum, is defined as a force (the force exerted on the body by the changing mass, such as rocket exhaust) and is included in the quantity F. Then, by substituting the definition of acceleration, the equation becomes F = \"m\"a.\n\nThe third law states that all forces between two objects exist in equal magnitude and opposite direction: if one object \"A\" exerts a force F on a second object \"B\", then \"B\" simultaneously exerts a force F on \"A\", and the two forces are equal in magnitude and opposite in direction: F = −F. The third law means that all forces are \"interactions\" between different bodies, or different regions within one body, and thus that there is no such thing as a force that is not accompanied by an equal and opposite force. In some situations, the magnitude and direction of the forces are determined entirely by one of the two bodies, say Body \"A\"; the force exerted by Body \"A\" on Body \"B\" is called the \"action\", and the force exerted by Body \"B\" on Body \"A\" is called the \"reaction\". This law is sometimes referred to as the \"action-reaction law\", with F called the \"action\" and F the \"reaction\". In other situations the magnitude and directions of the forces are determined jointly by both bodies and it isn't necessary to identify one force as the \"action\" and the other as the \"reaction\". The action and the reaction are simultaneous, and it does not matter which is called the \"action\" and which is called \"reaction\"; both forces are part of a single interaction, and neither force exists without the other.\n\nThe two forces in Newton's third law are of the same type (e.g., if the road exerts a forward frictional force on an accelerating car's tires, then it is also a frictional force that Newton's third law predicts for the tires pushing backward on the road).\n\nFrom a conceptual standpoint, Newton's third law is seen when a person walks: they push against the floor, and the floor pushes against the person. Similarly, the tires of a car push against the road while the road pushes back on the tires—the tires and road simultaneously push against each other. In swimming, a person interacts with the water, pushing the water backward, while the water simultaneously pushes the person forward—both the person and the water push against each other. The reaction forces account for the motion in these examples. These forces depend on friction; a person or car on ice, for example, may be unable to exert the action force to produce the needed reaction force.\n\nFrom the original Latin of Newton's \"Principia\":\nTranslated to English, this reads:\nThe ancient Greek philosopher Aristotle had the view that all objects have a natural place in the universe: that heavy objects (such as rocks) wanted to be at rest on the Earth and that light objects like smoke wanted to be at rest in the sky and the stars wanted to remain in the heavens. He thought that a body was in its natural state when it was at rest, and for the body to move in a straight line at a constant speed an external agent was needed continually to propel it, otherwise it would stop moving. Galileo Galilei, however, realised that a force is necessary to change the velocity of a body, i.e., acceleration, but no force is needed to maintain its velocity. In other words, Galileo stated that, in the \"absence\" of a force, a moving object will continue moving. (The tendency of objects to resist changes in motion was what Johannes Kepler had called \"inertia\".) This insight was refined by Newton, who made it into his first law, also known as the \"law of inertia\"—no force means no acceleration, and hence the body will maintain its velocity. As Newton's first law is a restatement of the law of inertia which Galileo had already described, Newton appropriately gave credit to Galileo.\n\nThe law of inertia apparently occurred to several different natural philosophers and scientists independently, including Thomas Hobbes in his \"Leviathan\". The 17th-century philosopher and mathematician René Descartes also formulated the law, although he did not perform any experiments to confirm it.\n\nNewton's original Latin reads:\nThis was translated quite closely in Motte's 1729 translation as:\n\nAccording to modern ideas of how Newton was using his terminology, this is understood, in modern terms, as an equivalent of:\nThis may be expressed by the formula F = p', where p' is the time derivative of the momentum p. This equation can be seen clearly in the Wren Library of Trinity College, Cambridge, in a glass case in which Newton's manuscript is open to the relevant page.\n\nMotte's 1729 translation of Newton's Latin continued with Newton's commentary on the second law of motion, reading:\n\nThe sense or senses in which Newton used his terminology, and how he understood the second law and intended it to be understood, have been extensively discussed by historians of science, along with the relations between Newton's formulation and modern formulations.\n\nTranslated to English, this reads:\nNewton's Scholium (explanatory comment) to this law:\nIn the above, as usual, \"motion\" is Newton's name for momentum, hence his careful distinction between motion and velocity.\n\nNewton used the third law to derive the law of conservation of momentum; from a deeper perspective, however, conservation of momentum is the more fundamental idea (derived via Noether's theorem from Galilean invariance), and holds in cases where Newton's third law appears to fail, for instance when force fields as well as particles carry momentum, and in quantum mechanics. \n\nNewton's laws were verified by experiment and observation for over 200 years, and they are excellent approximations at the scales and speeds of everyday life. Newton's laws of motion, together with his law of universal gravitation and the mathematical techniques of calculus, provided for the first time a unified quantitative explanation for a wide range of physical phenomena.\n\nThese three laws hold to a good approximation for macroscopic objects under everyday conditions. However, Newton's laws (combined with universal gravitation and classical electrodynamics) are inappropriate for use in certain circumstances, most notably at very small scales, very high speeds (in special relativity, the Lorentz factor must be included in the expression for momentum along with the rest mass and velocity) or very strong gravitational fields. Therefore, the laws cannot be used to explain phenomena such as conduction of electricity in a semiconductor, optical properties of substances, errors in non-relativistically corrected GPS systems and superconductivity. Explanation of these phenomena requires more sophisticated physical theories, including general relativity and quantum field theory.\n\nIn quantum mechanics, concepts such as force, momentum, and position are defined by linear operators that operate on the quantum state; at speeds that are much lower than the speed of light, Newton's laws are just as exact for these operators as they are for classical objects. At speeds comparable to the speed of light, the second law holds in the original form F = dp/d\"t\", where F and p are four-vectors.\n\nIn modern physics, the laws of conservation of momentum, energy, and angular momentum are of more general validity than Newton's laws, since they apply to both light and matter, and to both classical and non-classical physics.\n\nThis can be stated simply, \"Momentum, energy and angular momentum cannot be created or destroyed.\"\n\nBecause force is the time derivative of momentum, the concept of force is redundant and subordinate to the conservation of momentum, and is not used in fundamental theories (e.g., quantum mechanics, quantum electrodynamics, general relativity, etc.). The standard model explains in detail how the three fundamental forces known as gauge forces originate out of exchange by virtual particles. Other forces, such as gravity and fermionic degeneracy pressure, also arise from the momentum conservation. Indeed, the conservation of 4-momentum in inertial motion via curved space-time results in what we call gravitational force in general relativity theory. The application of the space derivative (which is a momentum operator in quantum mechanics) to the overlapping wave functions of a pair of fermions (particles with half-integer spin) results in shifts of maxima of compound wavefunction away from each other, which is observable as the \"repulsion\" of the fermions.\n\nNewton stated the third law within a world-view that assumed instantaneous action at a distance between material particles. However, he was prepared for philosophical criticism of this action at a distance, and it was in this context that he stated the famous phrase \"I feign no hypotheses\". In modern physics, action at a distance has been completely eliminated, except for subtle effects involving quantum entanglement. (In particular, this refers to Bell's theorem – that no local model can reproduce the predictions of quantum theory.) Despite only being an approximation, in modern engineering and all practical applications involving the motion of vehicles and satellites, the concept of action at a distance is used extensively.\n\nThe discovery of the second law of thermodynamics by Carnot in the 19th century showed that not every physical quantity is conserved over time, thus disproving the validity of inducing the opposite metaphysical view from Newton's laws. Hence, a \"steady-state\" worldview based solely on Newton's laws and the conservation laws does not take entropy into account.\n\n\n\n"}
{"id": "30858407", "url": "https://en.wikipedia.org/wiki?curid=30858407", "title": "Not even wrong", "text": "Not even wrong\n\nThe phrase \"not even wrong\" describes an argument or explanation that purports to be scientific but is based on invalid reasoning or speculative premises that can neither be proven correct nor falsified. Hence, it refers to statements that cannot be discussed in a rigorous, scientific sense. For a meaningful discussion on whether a certain statement is true or false, the statement must satisfy the criterion called \"falsifiability\"—the inherent possibility for the statement to be tested and found false. In this sense, the phrase \"not even wrong\" is synonymous to \"nonfalsifiable\".\n\nThe phrase is generally attributed to theoretical physicist Wolfgang Pauli, who was known for his colorful objections to incorrect or careless thinking. Rudolf Peierls documents an instance in which \"a friend showed Pauli the paper of a young physicist which he suspected was not of great value but on which he wanted Pauli's views. Pauli remarked sadly, 'It is not even wrong'.\" This is also often quoted as \"That is not only not right; it is not even wrong\", or in Pauli's native German, \"\". Peierls remarks that quite a few apocryphal stories of this kind have been circulated and mentions that he listed only the ones personally vouched for by him. He also quotes another example when Pauli replied to Lev Landau, \"What you said was so confused that one could not tell whether it was nonsense or not.\"\n\nThe phrase is often used to describe pseudoscience or bad science and is considered derogatory.\n\n"}
{"id": "21525175", "url": "https://en.wikipedia.org/wiki?curid=21525175", "title": "Open Notebook Science Challenge", "text": "Open Notebook Science Challenge\n\nThe Open Notebook Science Challenge is a crowdsourcing research project which collects measurements of the non-aqueous solubility of organic compounds and publishes these as open data; findings are reported in an open notebook science manner. Although anyone may contribute research data, the competition is only open to post-secondary students in the USA and UK.\n\nThe challenge in turn forms part of the UsefulChem project, an ongoing open notebook science effort to synthesize and screen potential new anti-malarial drugs. Data from the Solubility Challenge will be used to build predictive computational models of solubility for use in optimising syntheses.\n\nThe challenge began on September 28, 2008 and, as of February 2014, involves researchers and their students from at least 4 different institutions and has resulted in the acquisition of over 7672 solubility measurements.\n\nTo encourage participation, each month an award is given to the student who does, in the opinion of the judges, the best work. In order to participate, students have to be a US or UK resident. The award is a US$500 cash prize. The first three winners also received a year's subscription to the journal \"Nature\". The awards are sponsored by Submeta and \"Nature\".\n\nAs well as concentrating on compounds related to the Ugi reaction, the ONSchallenge allows anyone to request a solubility measurement experiment.\n\nSigma Aldrich is also an official sponsor of the Open Notebook Science Challenge. Sigma Aldrich is participating by donating and shipping requested chemicals to any experimenters in the US or UK for free.\n\n"}
{"id": "52660479", "url": "https://en.wikipedia.org/wiki?curid=52660479", "title": "Open energy system databases", "text": "Open energy system databases\n\nOpen energy system database projects employ open data methods to collect, clean, and republish energy-related datasets for open use. The resulting information is then available, given a suitable open license, for statistical analysis and for building numerical energy system models, including open energy system models. Permissive licenses like Creative Commons CC0 and are preferred, but some projects will house data made public under market transparency regulations and carrying unqualified copyright.\n\nThe databases themselves may furnish information on national power plant fleets, renewable generation assets, transmission networks, time series for electricity loads, dispatch, spot prices, and cross-border trades, weather information, and similar. They may also offer other energy statistics including fossil fuel imports and exports, gas, oil, and coal prices, emissions certificate prices, and information on energy efficiency costs and benefits.\n\nMuch of the data is sourced from official or semi-official agencies, including national statistics offices, transmission system operators, and electricity market operators. Data is also crowdsourced using public wikis and public upload facilities. Projects usually also maintain a strict record of the provenance and version histories of the datasets they hold. Some projects, as part of their mandate, also try to persuade primary data providers to release their data under more liberal licensing conditions.\n\nTwo drivers favor the establishment of such databases. The first is a wish to reduce the duplication of effort that accompanies each new analytical project as it assembles and processes the data that it needs from primary sources. And the second is an increasing desire to make public policy energy models more transparent to improve their acceptance by policymakers and the public. Better transparency dictates the use of open information, able to be accessed and scrutinized by third-parties, in addition to releasing the source code for the models in question.\n\nIn the mid-1990s, energy models used structured text files for data interchange but efforts were being made to migrate to relational database management systems for data processing. These early efforts however remained local to a project and did not involve online publishing or open data principles.\n\nThe first energy information portal to go live was OpenEI in late 2009, followed by reegle in 2011.\n\nA 2012 paper marks the first scientific publication to advocate the crowdsourcing of energy data. The 2012 PhD thesis by Chris Davis also discusses the crowdsourcing of energy data in some depth. A 2016 thesis surveyed the spatial (GIS) information requirements for energy planning and finds that most types of data, with the exception of energy expenditure data, are available but nonetheless remain scattered and poorly coordinated.\n\nIn terms of open data, a 2017 paper concludes that energy research has lagged behind other fields, most notably physics, biotechnology, and medicine. The paper also lists the benefits of open data and open models and discusses the reasons that many projects nonetheless remain closed. A one-page opinion piece from 2017 advances the case for using open energy data and modeling to build public trust in policy analysis. The article also argues that scientific journals have a responsibility to require that data and code be submitted alongside text for peer review.\n\nData models are central to the design and organization of databases. Open energy database projects generally try to develop and adhere to well resolved data models, using defacto and published standards where applicable. Some projects attempt to coordinate their data models in order to harmonize their data and improve its utility. Defining and maintaining suitable metadata is also a key issue. The life-cycle management of data includes, but is not limited to, the use of version control to track the provenance of incoming and cleansed data. Some sites allow users to comment on and rate individual datasets.\n\nIssues surrounding copyright remain at the forefront with regard to open energy data. As noted, most energy datasets are collated and published by official or semi-official sources. But many of the publicly available energy datasets carry no license, limiting their reuse in numerical and statistical models, open or otherwise. Copyright protected material cannot lawfully be circulated, nor can it be modified and republished.\n\nMeasures to enforce market transparency have not helped much because the associated information is again not licensed to enable modification and republication. Transparency measures include the 2013 European energy market transparency regulation 543/2013. Indeed, 543/2013 \"is only an obligation to publish, not an obligation to license\". Notwithstanding, 543/2013 does enable downloaded data to be computer processed with legal certainty.\n\nEnergy databases with hardware located with the European Union are protected under a general database law, irrespective of the legal status of the information they hold.\nDatabase rights not waived by public sector providers significantly restrict the amount of data a user can lawfully access.\n\nA December 2017 submission by energy researchers in Germany and elsewhere highlighted a number of concerns over the re-use of public sector information within the Europe Union.\nThe submission drew heavily on a recent legal opinion covering electricity data.\n\nNational and international energy statistics are published regularly by governments and international agencies, such as the IEA. In 2016 the United Nations issued guidelines for energy statistics. While the definitions and sectoral breakdowns are useful when defining models, the information provided is rarely in sufficient detail to enable its use in high-resolution energy system models.\n\nThere are few published standards covering the collection and structuring of high-resolution energy system data. The IEC Common Information Model (CIM) defines data exchange protocols for low and high voltage electricity networks.\n\nEnergy system models are data intensive and normally require detailed information from a number of sources. Dedicated projects to collect, collate, document, and republish energy system datasets have arisen to service this need. Most database projects prefer open data, issued under free licenses, but some will accept datasets with proprietary licenses in the absence of other options.\n\nThe OpenStreetMap project, which uses the Open Database License (ODbL), contains geographic information about energy system components, including transmission lines. Wikipedia itself has a growing set of information related to national energy systems, including descriptions of individual power stations.\n\nThe following table summarizes projects that specifically publish open energy system data. Some are general repositories while others (for instance, oedb) are designed to interact with open energy system models in real-time.\n\nThe Energy Research Data Portal for South Africa is being developed by the Energy Research Centre, University of Cape Town, Cape Town, South Africa. Coverage includes South Africa and certain other African countries where the Centre undertakes projects. The website uses the CKAN open source data portal software. A number of data formats are supported, including CSV and XLSX. The site also offers an API for automated downloads. , the portal contained 65datasets.\n\nThe energydata.info project from the World Bank Group, Washington, DC, USA is an energy database portal designed to support national development by improving public access to energy information. As well as sharing data, the platform also offers tools to visualize and analyze energy data. Although the World Bank Group has made available a number of dataset and apps, external users and organizations are encouraged to contribute. The concepts of open data and open source development are central to the project. energydata.info uses its own fork of the CKAN open source data portal as its web-based platform. The Creative Commons CC BY 4.0 license is preferred for data but other open licenses can be deployed. Users are also bound by the terms of use for the site.\n\n, the database held 131datasets, the great majority related to developing countries. The datasets are tagged and can be easily filtered. A number of download formats, including GIS files, are supported: CSV, XLS, XLSX, ArcGIS, Esri, GeoJSON, KML, and SHP. Some datasets are also offered as HTML. Again, , four apps are available. Some are web-based and run from a browser.\n\nThe semantic wiki-site and database Enipedia lists energy systems data worldwide. Enipedia is maintained by the Energy and Industry Group, Faculty of Technology, Policy and Management, Delft University of Technology, Delft, the Netherlands. A key tenet of Enipedia is that data displayed on the wiki is not trapped within the wiki, but can be extracted via SPARQL queries and used to populate new tools. Any programming environment that can download content from a URL can be used to obtain data. Enipedia went live in March 2011, judging by traffic figures quoted by Davis.\n\nA 2010 study describes how community driven data collection, processing, curation, and sharing is revolutionizing the data needs of industrial ecology and energy system analysis. A 2012 chapter introduces a system of systems engineering (SoSE) perspective and outlines how agent-based models and crowdsourced data can contribute to the solving of global issues.\n\nThe OpenEnergy Platform (OEP) is a collaborative versioned dataset repository for storing open energy system model datasets. A dataset is presumed to be in the form of a database table, together with metadata. Registered users can upload and download datasets manually using a web-interface or programmatically via an API using HTTP POST calls. Uploaded datasets are screened for integrity using deterministic rules and then subject to confirmation by a moderator. The use of versioning means that any prior state of the database can be accessed (as recommended in this 2012 paper). Hence, the repository is specifically designed to interoperate with energy system models. The backend is a PostgreSQL object-relational database under subversion version control. Open source licenses are specific to each dataset. Unlike other database projects, users can download the current version (the public tables) of the entire PostgreSQL database or any previous version. Initial development is being lead by the Reiner Lemoine Institute, Berlin, Germany.\n\nThe Open Data Energy Networks ( or ) portal is run by eight partners, led by the French national transmission system operator (TSO) Réseau de Transport d'Électricité (RTE). The portal was previously known as Open Data RTE. The site offers electricity system datasets under a Creative Commons compatible license, with metadata, an RSS feed for notifying updates, and an interface for submitting questions. of information obtained from the site can also register third-party URLs (be they publications or webpages) against specific datasets.\n\nThe portal uses the French Government Licence Ouverte license and this is explicitly compatible with the United Kingdom Open Government Licence (OGL), the Creative Commons license (and thereby later versions), and the Open Data Commons license.\n\nThe site hosts electricity, gas, and weather information related to France.\n\nThe Open Power System Data (OPSD) project seeks to characterize the German and western European power plant fleets, their associated transmission network, and related information and to make that data available to energy modelers and analysts. The platform was originally implemented by the University of Flensburg, DIW Berlin, the Technical University of Berlin, and the energy economics consultancy Neon Neue Energieökonomik, all from Germany. The first phase of the project, from August 2015 to July 2017, was funded by the Federal Ministry for Economic Affairs and Energy (BMWi) for . The project later received funding for a second phase, from January 2018 to December 2020, with ETH Zurich replacing Flensburg University as a partner.\n\nDevelopers collate and harmonize data from a range of government, regulatory, and industry sources throughout Europe. The website and the metadata utilize English, whereas the original material can be in any one of 24languages. Datasets follow the emerging frictionless data package standard being developed by Open Knowledge International (OKI). The website was launched on 28October 2016. , the project offers the following primary packages, for Germany and other European countries:\n\n\nIn addition, the project hosts selected contributed packages:\n\n\nTo facilitate analysis, the data is aggregated into large structured files (in CSV format) and loaded into data packages with standardized machine-readable metadata (in JSON format). The same data is usually also provided as XLSX (Excel) and SQLite files. The datasets can be accessed in real-time using stable URLs. The Python scripts deployed for data processing are available on GitHub and carry an MIT license. The licensing conditions for the data itself depends on the source and varies in terms of openness. Previous versions of the datasets and scripts can be recovered in order to track changes or replicate earlier studies. The project also engages with energy data providers, such as transmission system operators (TSO) and ENTSO-E, to encourage them to make their data available under open licenses (for instance, Creative Commons and ODbL licenses).\n\nA number of published electricity market modeling analyses are based on OPSD data.\n\nIn 2017, the Open Power System Data project won the Schleswig-Holstein Open Science Award and the Germany Land of Ideas award.\n\nOpen Energy Information (OpenEI) is a collaborative website, run by the US government, providing open energy data to software developers, analysts, users, consumers, and policymakers. The platform is sponsored by the United States Department of Energy (DOE) and is being developed by the National Renewable Energy Laboratory (NREL). OpenEI launched on 9December 2009. While much of its data is from US government sources, the platform is intended to be open and global in scope.\n\nOpenEI provides two mechanisms for contributing structured information: a semantic wiki (using MediaWiki and the Semantic MediaWiki extension) for collaboratively-managed resources and a dataset upload facility for contributor-controlled resources. US government data is distributed under a CC0 public domain dedication, whereas other contributors are free to select an open data license of their choice. Users can rate data using a five-star system, based on accessibility, adaptability, usefulness, and general quality. Individual datasets can be manually downloaded in an appropriate format, often as CSV files. Scripts for processing data can also be shared through the site. In order to build a community around the platform, a number of forums are offered covering energy system data and related topics.\n\nMost of the data on OpenEI is exposed as linked open data (LOD) (described elsewhere on this page). OpenEI also uses LOD methods to populate its definitions throughout the wiki with real-time connections to DBPedia, reegle, and Wikipedia.\n\nOpenEI has been used to classify geothermal resources in the United States. And to publicize municipal utility rates, again within the US.\n\nOpenGridMap employs crowdsourcing techniques to gather detailed data on electricity network components and then infer a realistic network structure using methods from statistics and graph theory. The scope of the project is worldwide and both distribution and transmission networks can be reverse engineered. The project is managed by the Chair of Business Information Systems, TUM Department of Informatics, Technical University of Munich, Munich, Germany. The project maintains a website and a Facebook page and provides an Android mobile app to help the public document electrical devices, such as transformers and substations. The bulk of the data is being made available under a Creative Commons license. The processing software is written primarily in Python and MATLAB and is hosted on GitHub.\n\nOpenGridMap provides a tailored GIS web application, layered on OpenStreetMap, which contributors can use to upload and edit information directly. The same database automatically stores field recordings submitted by the mobile app. Subsequent classification by experts allows normal citizens to document and photograph electrical components and have them correctly identified. The project is experimenting with the use of hobby drones to obtain better information on associated facilities, such as photovoltaic installations. Transmission line data is also sourced from and shared with OpenStreetMap. Each component record is verified by a moderator.\n\nOnce sufficient data is available, the transnet software is run to produce a likely network, using statistical correlation, Voronoi partitioning, and minimum spanning tree (MST) algorithms. The resulting network can be exported in CSV (separate files for nodes and lines), XML, and CIM formats. CIM models are well suited for translation into software-specific data formats for further analysis, including power grid simulation. Transnet also displays descriptive statistics about the resulting network for visual confirmation.\n\nThe project is motivated by the need to provide datasets for high-resolution energy system models, so that energy system transitions (like the German \"Energiewende\") can be better managed, both technically and policy-wise. The rapid expansion of renewable generation and the anticipated uptake of electric vehicles means that electricity system models must increasingly represent distribution and transmission networks in some detail.\n\n, OpenGridMap techniques have been used to estimate the low voltage network in the German city of Garching and to estimate the high voltage grids in several other countries.\n\nreegle is a clean energy information portal covering renewable energy, energy efficiency, and climate compatible development topics. reegle was launched in 2006 by REEEP and REN21 with funding from the Dutch (VROM), German (BMU), and UK (Defra) environment ministries. Originally released as a specialized internet search engine, reegle was relaunched in 2011 as an information portal.\n\nreegle offers and utilizes linked open data (LOD) (described elsewhere on this page). Sources of data include UN and World Bank databases, as well as dedicated partners around the world. reegle maintains a comprehensive structured glossary (driven by an LOD-compliant thesaurus) of energy and climate compatible development terms to assist with the tagging of datasets. The glossary also facilitates intelligent web searches.\n\nreegle offers country profiles which collate and display energy data on a per-country basis for most of the world. These profiles are kept current automatically using LOD techniques.\n\nRenewables.ninja is a website that can calculate the hourly power output from solar photovoltaic installations and wind farms located anywhere in the world. The website is a joint project between the Department of Environmental Systems Science, ETH Zurich, Zürich, Switzerland and the Centre for Environmental Policy, Imperial College London, London, United Kingdom. The website went live during September 2016. The resulting time series are provided under a Creative Commons license and the underlying power plant models are published using a BSD-new license. , only the solar model, written in Python, has been released.\n\nThe project relies on weather data derived from meteorological reanalysis models and weather satellite images. More specifically, it uses the 2016 MERRA-2 reanalysis dataset from NASA and satellite images from CM-SAF SARAH. For locations in Europe, this weather data is further \"corrected\" by country so that it better fits with the output from known PV installations and windfarms. Two 2016 papers describe the methods used in detail in relation to Europe. The first covers the calculation of PV power. And the second covers the calculation of wind power.\n\nThe website displays an interactive world map to aid the selection of a site. Users can then choose a plant type and enter some technical characteristics. , only year 2014 data can be served, due to technical restrictions. The results are automatically plotted and are available for download in hourly CSV format with or without the associated weather information. The site offers an API for programmatic dataset recovery using token-based authorization. Examples deploying cURL and Python are provided.\n\nA number of studies have been undertaking using the power production datasets underpinning the website (these studies predate the launch of the website), with the bulk focusing on energy options for Great Britain.\n\nThe SMARD site (pronounced \"smart\") serves electricity market data from Germany, Austria, and Luxembourg and also provides visual information. The electricity market plots and their underlying time series are released under a permissive CC BY 4.0 license. The site itself was launched on 3July 2017 in German and an English translation followed shortly. The data portal is mandated under the German Energy Industry Act (\"\" or \"EnWG\") section §111d, introduced as an amendment on 13October 2016. Four table formats are offered: CSV, XLS, XML, and PDF. The maximum sampling resolution is . Market data visuals or plots can be downloaded in PDF, SVG, PNG, and JPG formats. Representative output is shown in the thumbnail (on the left), in this case mid-winter dispatch over two days for the whole of Germany. The horizontal ordering by generation type is first split into renewable and conventional generation and then based on merit.\n\n\n\n"}
{"id": "54184232", "url": "https://en.wikipedia.org/wiki?curid=54184232", "title": "Out in Science, Technology, Engineering, and Mathematics", "text": "Out in Science, Technology, Engineering, and Mathematics\n\nOut in Science, Technology, Engineering, and Mathematics, Inc., abbreviated oSTEM, is a 501(c)(3) non-profit professional society dedicated to LGBTQ+ individuals within the science, technology, engineering, and mathematics (STEM) community.\n\nIn October 2005, IBM sponsored a focus group where students from across the United States convened at the Human Rights Campaign headquarters in Washington D.C. These students discussed topics relevant to LGBTQA communities at their own colleges and universities, and they debated how to structure an organization that serves students in science, technology, engineering, and mathematics.\n\nFounded in 2009 and achieving 501(c)(3) status in 2010, oSTEM, Inc. currently consists of more than 50 chapters across the United States and the United Kingdom. In 2017, oSTEM, Inc. expanded its mission to be inclusive of professionals, serving all LGBTQ people in the STEM community.\n\nAs an organization dedicated to community, oSTEM, Inc. strives to stay engaged to identify, address, and advocate for the needs of LGBTQA+ students and professionals within the STEM fields. oSTEM, Inc. fulfills these needs in many ways, including networking opportunities, mentorship connections, strategic collaborations and professional/leadership development, as well as an annual global conference.\n\noSTEM hosts annual conferences that discuss LGBT+ topics in STEM as well as intelligence fields. Topics discussed include inclusion, outreach, and diversity within the workplace. Workshops, presentations, and networking events for LGBT+ individuals aim to facilitate integration and advancement in their respective fields. The 4 annual conference was hosted jointly with NOGLSTP's Out to Innovate in Atlanta in 2014.\n\nConferences have been held in the following cities:\nOn July 5, 2018 oSTEM along with Pride in STEM, House of STEM, and InterEngineering created international awareness for LGBTQ+ people in Science, Technology, Engineering, and Math.\n\noSTEM presents a variety of awards annually to individuals and organizations that demonstrate a strong dedication to advancing and empowering LGBT+ in STEM fields.\n\nThe oSTEM Global STEM Service Award is given to present and past oSTEM members who show strong dedication to inclusion, diversity, and equality for LGBT+ and other marginalized individuals in STEM fields.\n\nAwardees are: \n\nThe oSTEM Strategic Alliance Award is presented to a current sponsoring organization, community partner, or grant provider of oSTEM who demonstrates strong dedication, engagement, and support to oSTEM and its values.\n\nAwardees are:\n\nThe oSTEM Partner Excellence Award is presented to individuals associated with oSTEM accomplished in their academic or professional lives who regularly advocate for the full inclusion of people of all marginalized identities.\n\nAwardees are: \n\nThe Overall Student Chapter of the Year is given to oSTEM chapters that educate, empower, and engage a diverse community. These chapters contribute greatly to identifying, addressing, and advocating for LGBTQ students in the STEM community\n\nAwardees are: \n\nThe Rookie Student Chapter of the Year celebrates achievements by oSTEM chapters that have been founded within two years of application submission.\n\nAwardees are:\n\noSTEM has over 90 chapters as of September, 2018. Chapters are organized into 6 geographic regions (A-F) and two types (student and professional).\nThe six regions are:\n\nThe first professional chapter is currently being tested in the Boston metropolitan area. Further cities will be announced at a later date.\n\noSTEM is sponsored by industries involved in STEM fields as well as government entities and academic STEM programs.\n\n\n\n\n\n\n"}
{"id": "44763458", "url": "https://en.wikipedia.org/wiki?curid=44763458", "title": "Particle mass analyser", "text": "Particle mass analyser\n\nParticle mass analyser is a measurement technique for classifying aerosol particles according to their mass-to-charge ratio.\n\nTechniques exist for classifying (selecting) aerosol particles in the sub 1,000 nm range according to electrical mobility using devices such as differential mobility analysers.\n\nIn electrical mobility measurement, aerosol particles are classified according to their aerodynamic drag-charge ratio. If the particle is non-spherical, the electrical-mobility diameter will not correspond to any measurable physical dimensions of the particle. (For a spherical particle, the electrical-mobility diameter will correspond to physically measurable diameter.)\n\nAn alternative technique classifies particles according to their mass-to-charge ratio, using opposing electrical and centrifugal forces. This allows the classifier to select particles of a specified mass-to-charge ratio independent of particle shape. \n\nFurther work on the technique used centrifugal and electrostatic forces to classify particles according to their mass-to-charge ratio.\n"}
{"id": "4917580", "url": "https://en.wikipedia.org/wiki?curid=4917580", "title": "Peter Murray-Rust", "text": "Peter Murray-Rust\n\nPeter Murray-Rust (born 1941) is a chemist currently working at the University of Cambridge. As well as his work in chemistry, Murray-Rust is also known for his support of open access and open data.\n\nHe was educated at Bootham School and Balliol College, Oxford. After obtaining a Doctor of Philosophy with a thesis entitled \"A structural investigation of some compounds showing charge-transfer properties,\" he became lecturer in chemistry at the (new) University of Stirling and was first warden of Andrew Stewart Hall of Residence. In 1982, he moved to Glaxo Group Research at Greenford to head Molecular Graphics, Computational Chemistry and later protein structure determination. He was Professor of Pharmacy in the University of Nottingham from 1996–2000, setting up the Virtual School of Molecular Sciences. He is now Reader Emeritus in Molecular Informatics at the University of Cambridge and Senior Research Fellow Emeritus of Churchill College, Cambridge.\n\nHis research interests have involved the automated analysis of data in scientific publications, creation of virtual communities, e.g. The Virtual School of Natural Sciences in the Globewide Network Academy, and the Semantic Web. With Henry Rzepa, he has extended this to chemistry through the development of markup languages, especially Chemical Markup Language. He campaigns for open data, particularly in science, and is on the advisory board of the Open Knowledge International and a co-author of the Panton Principles for Open scientific data. Together with a few other chemists, he was a founder member of the Blue Obelisk movement in 2005.\n\nIn 2002, Peter Murray-Rust and his colleagues proposed an electronic repository for unpublished chemical data called the World Wide Molecular Matrix (WWMM). In January 2011, a symposium around his career and visions was organized, called \"Visions of a Semantic Molecular Future\". In 2011, he and Henry Rzepa were joint recipients of the Herman Skolnik Award of the American Chemical Society. In 2014, he was awarded a Fellowship by the Shuttleworth Foundation to develop the automated mining of science from the literature.\n\nIn 2009 Murray-Rust coined the term \"\"Doctor Who\" model\" for the phenomenon exhibited by the Blue Obelisk project and other Open Science projects, where when a project leader does not have the resources to continue to lead a project (e.g. because he or she has moved to another university with other tasks), someone else will stand up to become the new leader and continue the project. This is a reference to the long-running British science fiction television series \"Doctor Who\", in which the main character periodically regenerates into a different form, which is played by a different actor.\n\nAs of 2014, Murray-Rust was granted a Fellowship by Shuttleworth Foundation in relation to the ContentMine project which uses machines to liberate 100,000,000 facts from the scientific literature.\n\nMurray-Rust is also known for his work on making scientific knowledge from literature freely available, and in such taking a stance against publishers that are not fully compliant with the Berlin Declaration on Open Access. In 2014, he actively raised awareness of glitches in the publishing system of Elsevier, where restrictions were imposed by Elsevier on the reuse of papers after the authors had paid Elsevier to make the paper freely available. He has also made statements about predatory journals; he has come under criticism by Jeffrey Beall for his involvement with publisher MDPI and for his account of this involvement.\n\n"}
{"id": "7972385", "url": "https://en.wikipedia.org/wiki?curid=7972385", "title": "Phantom of the Poles", "text": "Phantom of the Poles\n\nThe Phantom of the Poles is a book written by William Reed, and published in 1906. It attempts to explain certain mysterious phenomena reported by polar explorers by postulating that the Earth is in fact hollow, with holes at its poles.\n\nIn the General Summary chapter of \"The Phantom of the Poles\", Reed posed several questions that he claimed were explained by the Hollow Earth theory:\nAdmiral Peary claimed to have reached the North Pole on April 6, 1909. This would have invalidated Reed's premise that the poles cannot be reached. Although Peary's claim was, in its day, and continues to be controversial, on December 14, 1911, Roald Amundsen undisputedly reached the South Pole. Subsequent expeditions to and flights over the south pole have conclusively demonstrated that there are no large holes there.\n\n"}
{"id": "246066", "url": "https://en.wikipedia.org/wiki?curid=246066", "title": "Prediction", "text": "Prediction\n\nA prediction (Latin \"præ-\", \"before,\" and \"dicere\", \"to say\"), or forecast, is a statement about a future event. A prediction is often, but not always, based upon experience or knowledge. There is no universal agreement about the exact difference between the two terms; different authors and disciplines ascribe different connotations. (Contrast with estimation.)\n\nAlthough future events are necessarily uncertain, so guaranteed accurate information about the future is in many cases impossible, prediction can be useful to assist in making plans about possible developments; Howard H. Stevenson writes that prediction in business \"... is at least two things: Important and hard.\"\n\nIn a non-statistical sense, the term \"prediction\" is often used to refer to an informed guess or opinion.\n\nA prediction of this kind might be informed by a predicting person's abductive reasoning, inductive reasoning, deductive reasoning, and experience; and may be of useful — if the predicting person is a knowledgeable person in the field.\n\nThe Delphi method is a technique for eliciting such expert-judgement-based predictions in a controlled way. This type of prediction might be perceived as consistent with statistical techniques in the sense that, at minimum, the \"data\" being used is the predicting expert's cognitive experiences forming an intuitive \"probability curve.\"\n\nIn statistics, prediction is a part of statistical inference. One particular approach to such inference is known as predictive inference, but the prediction can be undertaken within any of the several approaches to statistical inference. Indeed, one possible description of statistics is that it provides a means of transferring knowledge about a sample of a population to the whole population, and to other related populations, which is not necessarily the same as prediction over time. When information is transferred across time, often to specific points in time, the process is known as forecasting. Forecasting usually requires time series methods, while prediction is often performed on cross-sectional data.\n\nStatistical techniques used for prediction include regression analysis and its various sub-categories such as linear regression, generalized linear models (logistic regression, Poisson regression, Probit regression), etc. In case of forecasting, autoregressive moving average models and vector autoregression models can be utilized. When these and/or related, generalized set of regression or machine learning methods are deployed in commercial usage, the field is known as predictive analytics.\n\nIn many applications, such as time series analysis, it is possible to estimate the models that generate the observations. If models can be expressed as transfer functions or in terms of state-space parameters then smoothed, filtered and predicted data estimates can be calculated. If the underlying generating models are linear then a minimum-variance Kalman filter and a minimum-variance smoother may be used to recover data of interest from noisy measurements. These techniques rely on one-step-ahead predictors (which minimise the variance of the prediction error). When the generating models are nonlinear then stepwise linearizations may be applied within Extended Kalman Filter and smoother recursions. However, in nonlinear cases, optimum minimum-variance performance guarantees no longer apply.\n\nTo use regression analysis for prediction, data are collected on the variable that is to be predicted, called the dependent variable or response variable, and on one or more variables whose values are hypothesized to influence it, called independent variables or explanatory variables. A functional form, often linear, is hypothesized for the postulated causal relationship, and the parameters of the function are estimated from the data—that is, are chosen so as to optimize is some way the fit of the function, thus parameterized, to the data. That is the estimation step. For the prediction step, explanatory variable values that are deemed relevant to future (or current but not yet observed) values of the dependent variable are input to the parameterized function to generate predictions for the dependent variable.\n\nIn science, a prediction is a rigorous, often quantitative, statement, forecasting what would happen under specific conditions; for example, if an apple fell from a tree it would be attracted towards the center of the earth by gravity with a specified and constant acceleration. The scientific method is built on testing statements that are logical consequences of scientific theories. This is done through repeatable experiments or observational studies.\n\nA scientific theory which is contradicted by observations and evidence will be rejected. New theories that generate many new predictions can more easily be supported or falsified (see predictive power). Notions that make no \"testable\" predictions are usually considered not to be part of science (protoscience or nescience) until testable predictions can be made.\n\nMathematical equations and models, and computer models, are frequently used to describe the past and future behaviour of a process within the boundaries of that model. In some cases the probability of an outcome, rather than a specific outcome, can be predicted, for example in much of quantum physics.\n\nIn microprocessors, branch prediction permits avoidance of pipeline emptying at branch instructions. In engineering, possible failure modes are predicted and avoided by correcting the mechanism causing the failure.\n\nAccurate prediction and forecasting are very difficult in some areas, such as natural disasters, pandemics, demography, population dynamics and meteorology. For example, it is possible to predict the occurrence of solar cycles, but their exact timing and magnitude is much more difficult (see picture to right).\n\nEstablished science makes useful predictions which are often extremely reliable and accurate; for example, eclipses are routinely predicted.\n\nNew theories make predictions which allow them to be disproved by reality. For example, predicting the structure of crystals at the atomic level is a current research challenge. In the early 20th century the scientific consensus was that there existed an absolute frame of reference, which was given the name \"luminiferous ether\". The existence of this absolute frame was deemed necessary for consistency with the established idea that the speed of light is constant. The famous Michelson-Morley experiment demonstrated that predictions deduced from this concept were not borne out in reality, thus disproving the theory of an absolute frame of reference. The special theory of relativity was proposed by Einstein as an explanation for the seeming inconsistency between the constancy of the speed of light and the non-existence of a special, preferred or absolute frame of reference.\n\nAlbert Einstein's theory of general relativity could not easily be tested as it did not produce any effects observable on a terrestrial scale. However, the theory predicted that large masses such as stars would bend light, in contradiction to accepted theory; this was observed in a 1919 eclipse.\n\nMathematical models of stock market behaviour (and economic behaviour in general) are also unreliable in predicting future behaviour. Among other reasons, this is because economic events may span several years, and the world is changing over a similar time frame, thus invalidating the relevance of past observations to the present. Thus there are an extremely small number (of the order of 1) of relevant past data points from which to project the future. In addition, it is generally believed that stock market prices already take into account all the information available to predict the future, and subsequent movements must therefore be the result of unforeseen events. Consequently, it is extremely difficult for a stock investor to anticipate or predict a stock market boom, or a stock market crash. In contrast to predicting the actual stock return, forecasting of broad economic trends tends to have better accuracy. Such analysis is provided by both non-profit groups as well as by for-profit private institutions, including brokerage housesand consulting companies.\n\nSome correlation has been seen between actual stock market movements and prediction data from large groups in surveys and prediction games.\n\nAn actuary uses actuarial science to assess and predict future business risk, such that the risk(s) can be mitigated. For example, in insurance an actuary would use a life table (which incorporates the historical experience of mortality rates and sometimes an estimate of future trends) to project life expectancy.\n\nPredicting the outcome of sporting events is a business which has grown in popularity in recent years. Handicappers predict the outcome of games using a variety of mathematical formulas, simulation models or qualitative analysis. Early, well known sports bettors, such as Jimmy the Greek, were believed to have access to information that gave them an edge. Information ranged from personal issues, such as gambling or drinking to undisclosed injuries; anything that may affect the performance of a player on the field.\n\nRecent times have changed the way sports are predicted. Predictions now typically consist of two distinct approaches: Situational plays and statistical based models. Situational plays are much more difficult to measure because they usually involve the motivation of a team. Dan Gordon, noted handicapper, wrote “Without an emotional edge in a game in addition to value in a line, I won’t put my money on it”. These types of plays consist of: Betting on the home underdog, betting against Monday Night winners if they are a favorite next week, betting the underdog in “look ahead” games etc. As situational plays become more widely known they become less useful because they will impact the way the line is set.\n\nThe widespread use of technology has brought with it more modern sports betting systems. These systems are typically algorithms and simulation models based on regression analysis. Jeff Sagarin, a sports statistician, has brought attention to sports by having the results of his models published in USA Today. He is currently paid as a consultant by the Dallas Mavericks for his advice on lineups and the use of his Winval system, which evaluates free agents. Brian Burke, a former Navy fighter pilot turned sports statistician, has published his results of using regression analysis to predict the outcome of NFL games. Ken Pomeroy is widely accepted as a leading authority on college basketball statistics. His website includes his College Basketball Ratings, a tempo based statistics system. Some statisticians have become very famous for having successful prediction systems. Dare wrote “the effective odds for sports betting and horse racing are a direct result of human decisions and can therefore potentially exhibit consistent error”. Unlike other games offered in a casino, prediction in sporting events can be both logical and consistent.\n\nIn politics it is common to attempt to predict the outcome of elections via political forecasting techniques (or assess the popularity of politicians) through the use of opinion polls. Prediction games have been used by many corporations and governments to learn about the most likely outcome of future events.\n\nPredictions have often been made, from antiquity until the present, by using paranormal or supernatural means such as prophecy or by observing omens. Methods including water divining, astrology, numerology, fortune telling, interpretation of dreams, and many other forms of divination, have been used for millennia to attempt to predict the future. These means of prediction have not been proven by scientific experiments.\n\nIn literature, vision and prophecy are literary devices used to present a possible timeline of future events. They can be distinguished by vision referring to what an individual sees happen. The New Testament book of Revelation (Bible) thus uses vision as a literary device in this regard. It is also prophecy or prophetic literature when it is related by an individual in a sermon or other public forum.\n\nDivination is the attempt to gain insight into a question or situation by way of an occultic standardized process or ritual. It is an integral part of witchcraft and has been used in various forms for thousands of years. Diviners ascertain their interpretations of how a querent should proceed by reading signs, events, or omens, or through alleged contact with a supernatural agency, most often describe as an angel or a god though viewed by Christians and Jews as a fallen angel or demon.\n\nFiction (especially fantasy, forecasting and science fiction) often features instances of prediction achieved by unconventional means.\n\n"}
{"id": "25524", "url": "https://en.wikipedia.org/wiki?curid=25524", "title": "Research", "text": "Research\n\nResearch comprises \"creative and systematic work undertaken to increase the stock of knowledge, including knowledge of humans, culture and society, and the use of this stock of knowledge to devise new applications.\" It is used to establish or confirm facts, reaffirm the results of previous work, solve new or existing problems, support theorems, or develop new theories. A research project may also be an expansion on past work in the field. Research projects can be used to develop further knowledge on a topic, or in the example of a school research project, they can be used to further a student's research prowess to prepare them for future jobs or reports. To test the validity of instruments, procedures, or experiments, research may replicate elements of prior projects or the project as a whole. The primary purposes of basic research (as opposed to applied research) are documentation, discovery, interpretation, or the research and development (R&D) of methods and systems for the advancement of human knowledge. Approaches to research depend on epistemologies, which vary considerably both within and between humanities and sciences. There are several forms of research: scientific, humanities, artistic, economic, social, business, marketing, practitioner research, life, technological, etc.\n\nThe word \"research\" is derived from the Middle French \"recherche\", which means \"to go about seeking\", the term itself being derived from the Old French term \"recerchier\" a compound word from \"re-\" + \"cerchier\", or \"sercher\", meaning 'search'. The earliest recorded use of the term was in 1577.\n\nResearch has been defined in a number of different ways, and while there are similarities, there does not appear to be a single, all-encompassing definition that is embraced by all who engage in it.\n\nOne definition of research is used by the OECD, \"Any creative systematic activity undertaken in order to increase the stock of knowledge, including knowledge of man, culture and society, and the use of this knowledge to devise new applications.\"\n\nAnother definition of research is given by John W. Creswell, who states that \"research is a process of steps used to collect and analyze information to increase our understanding of a topic or issue\". It consists of three steps: pose a question, collect data to answer the question, and present an answer to the question.\n\nThe Merriam-Webster Online Dictionary defines research in more detail as \"studious inquiry or examination; \"especially\" : investigation or experimentation aimed at the discovery and interpretation of facts, revision of accepted theories or laws in the light of new facts, or practical application of such new or revised theories or laws\"\n\nOriginal research is research that is not exclusively based on a summary, review or synthesis of earlier publications on the subject of research. This material is of a primary source character. The purpose of the original research is to produce new knowledge, rather than to present the existing knowledge in a new form (\"e.g.\", summarized or classified).\n\nOriginal research can take a number of forms, depending on the discipline it pertains to. In experimental work, it typically involves direct or indirect observation of the researched subject(s), e.g., in the laboratory or in the field, documents the methodology, results, and conclusions of an experiment or set of experiments, or offers a novel interpretation of previous results. In analytical work, there are typically some new (for example) mathematical results produced, or a new way of approaching an existing problem. In some subjects which do not typically carry out experimentation or analysis of this kind, the originality is in the particular way existing understanding is changed or re-interpreted based on the outcome of the work of the researcher.\n\nThe degree of originality of the research is among major criteria for articles to be published in academic journals and usually established by means of peer review. Graduate students are commonly required to perform original research as part of a dissertation.\n\nScientific research is a systematic way of gathering data and harnessing curiosity. This research provides scientific information and theories for the explanation of the nature and the properties of the world. It makes practical applications possible. Scientific research is funded by public authorities, by charitable organizations and by private groups, including many companies. Scientific research can be subdivided into different classifications according to their academic and application disciplines. Scientific research is a widely used criterion for judging the standing of an academic institution, but some argue that such is an inaccurate assessment of the institution, because the quality of research does not tell about the quality of teaching (these do not necessarily correlate).\n\nResearch in the humanities involves different methods such as for example hermeneutics and semiotics. Humanities scholars usually do not search for the ultimate correct answer to a question, but instead, explore the issues and details that surround it. Context is always important, and context can be social, historical, political, cultural, or ethnic. An example of research in the humanities is historical research, which is embodied in historical method. Historians use primary sources and other evidence to systematically investigate a topic, and then to write histories in the form of accounts of the past. Other studies aim to merely examine the occurrence of behaviours in societies and communities, without particularly looking for reasons or motivations to explain these. These studies may be qualitative or quantitative, and can use a variety of approaches, such as queer theory or feminist theory.\n\nArtistic research, also seen as 'practice-based research', can take form when creative works are considered both the research and the object of research itself. It is the debatable body of thought which offers an alternative to purely scientific methods in research in its search for knowledge and truth.\n\nGenerally, research is understood to follow a certain structural process. Though step order may vary depending on the subject matter and researcher, the following steps are usually part of most formal research, both basic and applied:\n\nA common misconception is that a hypothesis will be proven (see, rather, null hypothesis). Generally, a hypothesis is used to make predictions that can be tested by observing the outcome of an experiment. If the outcome is inconsistent with the hypothesis, then the hypothesis is rejected (see falsifiability). However, if the outcome is consistent with the hypothesis, the experiment is said to support the hypothesis. This careful language is used because researchers recognize that alternative hypotheses may also be consistent with the observations. In this sense, a hypothesis can never be proven, but rather only supported by surviving rounds of scientific testing and, eventually, becoming widely thought of as true.\n\nA useful hypothesis allows prediction and within the accuracy of observation of the time, the prediction will be verified. As the accuracy of observation improves with time, the hypothesis may no longer provide an accurate prediction. In this case, a new hypothesis will arise to challenge the old, and to the extent that the new hypothesis makes more accurate predictions than the old, the new will supplant it. Researchers can also use a null hypothesis, which states no relationship or difference between the independent or dependent variables.\n\nThe historical method comprises the techniques and guidelines by which historians use historical sources and other evidence to research and then to write history. There are various history guidelines that are commonly used by historians in their work, under the headings of external criticism, internal criticism, and synthesis. This includes lower criticism and sensual criticism. Though items may vary depending on the subject matter and researcher, the following concepts are part of most formal historical research:\n\nThe controversial trend of artistic teaching becoming more academics-oriented is leading to artistic research being accepted as the primary mode of enquiry in art as in the case of other disciplines. One of the characteristics of artistic research is that it must accept subjectivity as opposed to the classical scientific methods. As such, it is similar to the social sciences in using qualitative research and intersubjectivity as tools to apply measurement and critical analysis.\n\nArtistic research has been defined by the University of Dance and Circus (Dans och Cirkushögskolan, DOCH), Stockholm in the following manner - \"Artistic research is to investigate and test with the purpose of gaining knowledge within and for our artistic disciplines. It is based on artistic practices, methods, and criticality. Through presented documentation, the insights gained shall be placed in a context.\" Artistic research aims to enhance knowledge and understanding with presentation of the arts. A more simple understanding by Julian Klein defines Artistic Research as any kind of research employing the artistic mode of perception. For a survey of the central problematics of today's Artistic Research, see Giaco Schiesser.\n\nAccording to artist Hakan Topal, in artistic research, \"perhaps more so than other disciplines, intuition is utilized as a method to identify a wide range of new and unexpected productive modalities\". Most writers, whether of fiction or non-fiction books, also have to do research to support their creative work. This may be factual, historical, or background research. Background research could include, for example, geographical or procedural research.\n\nThe Society for Artistic Research (SAR) publishes the triannual \"Journal for Artistic Research\" (JAR), an international, online, open access, and peer-reviewed journal for the identification, publication, and dissemination of artistic research and its methodologies, from all arts disciplines and it runs the \"Research Catalogue\" (RC), a searchable, documentary database of artistic research, to which anyone can contribute.\n\nPatricia Leavy addresses eight arts-based research (ABR) genres: narrative inquiry, fiction-based research, poetry, music, dance, theatre, film, and visual art.\n\nIn 2016 ELIA (European League of the Institutes of the Arts) launched \"The Florence Principles' on the Doctorate in the Arts\". The Florence Principles relating to the Salzburg Principles and the Salzburg Recommendations of EUA (European University Association) name seven points of attention to specify the Doctorate / PhD in the Arts compared to a scientific doctorate / PhD The Florence Principles have been endorsed and are supported also by AEC, CILECT, CUMULUS and SAR.\n\nResearch is often conducted using the hourglass model structure of research. The hourglass model starts with a broad spectrum for research, focusing in on the required information through the method of the project (like the neck of the hourglass), then expands the research in the form of discussion and results. The major steps in conducting research are:\n\nThe steps generally represent the overall process; however, they should be viewed as an ever-changing iterative process rather than a fixed set of steps. Most research begins with a general statement of the problem, or rather, the purpose for engaging in the study. The literature review identifies flaws or holes in previous research which provides justification for the study. Often, a literature review is conducted in a given subject area before a research question is identified. A gap in the current literature, as identified by a researcher, then engenders a research question. The research question may be parallel to the hypothesis. The hypothesis is the supposition to be tested. The researcher(s) collects data to test the hypothesis. The researcher(s) then analyzes and interprets the data via a variety of statistical methods, engaging in what is known as empirical research. The results of the data analysis in rejecting or failing to reject the null hypothesis are then reported and evaluated. At the end, the researcher may discuss avenues for further research. However, some researchers advocate for the reverse approach: starting with articulating findings and discussion of them, moving \"up\" to identification of a research problem that emerges in the findings and literature review. The reverse approach is justified by the transactional nature of the research endeavor where research inquiry, research questions, research method, relevant research literature, and so on are not fully known until the findings have fully emerged and been interpreted.\n\nRudolph Rummel says, \"... no researcher should accept any one or two tests as definitive. It is only when a range of tests are consistent over many kinds of data, researchers, and methods can one have confidence in the results.\"\n\nPlato in Meno talks about an inherent difficulty, if not a paradox, of doing research that can be paraphrased in the following way, \"If you know what you're searching for, why do you search for it?! [i.e., you have already found it] If you don't know what you're searching for, what are you searching for?!\"\n\nThe goal of the research process is to produce new knowledge or deepen understanding of a topic or issue. This process takes three main forms (although, as previously discussed, the boundaries between them may be obscure):\nThere are two major types of empirical research design: qualitative research and quantitative research. Researchers choose qualitative or quantitative methods according to the nature of the research topic they want to investigate and the research questions they aim to answer:\n\n\nSocial media posts are used for qualitative research.\n\n\nThe quantitative data collection methods rely on random sampling and structured data collection instruments that fit diverse experiences into predetermined response categories. These methods produce results that are easy to summarize, compare, and generalize. Quantitative research is concerned with testing hypotheses derived from theory or being able to estimate the size of a phenomenon of interest.\n\nIf the research question is about people, participants may be randomly assigned to different treatments (this is the only way that a quantitative study can be considered a true experiment). If this is not feasible, the researcher may collect data on participant and situational characteristics to statistically control for their influence on the dependent, or outcome, variable. If the intent is to generalize from the research participants to a larger population, the researcher will employ probability sampling to select participants.\n\nIn either qualitative or quantitative research, the researcher(s) may collect primary or secondary data. Primary data is data collected specifically for the research, such as through interviews or questionnaires. Secondary data is data that already exists, such as census data, which can be re-used for the research. It is good ethical research practice to use secondary data wherever possible.\n\nMixed-method research, i.e. research that includes qualitative and quantitative elements, using both primary and secondary data, is becoming more common. This method has benefits that using one method alone cannot offer. For example, a researcher may choose to conduct a qualitative study and follow it up with a quantitative study to gain additional insights.\n\nBig data has brought big impacts on research methods so that now many researchers do not put much effort into data collection; furthermore, methods to analyze easily available huge amounts of data have also been developed.\n\nNon-empirical (theoretical) research is an approach that involves the development of theory as opposed to using observation and experimentation. As such, non-empirical research seeks solutions to problems using existing knowledge as its source. This, however, does not mean that new ideas and innovations cannot be found within the pool of existing and established knowledge. Non-empirical research is not an absolute alternative to empirical research because they may be used together to strengthen a research approach. Neither one is less effective than the other since they have their particular purpose in science. Typically empirical research produces observations that need to be explained; then theoretical research tries to explain them, and in so doing generates empirically testable hypotheses; these hypotheses are then tested empirically, giving more observations that may need further explanation; and so on. See Scientific method.\n\nA simple example of a non-empirical task is the prototyping of a new drug using a differentiated application of existing knowledge; another is the development of a business process in the form of a flow chart and texts where all the ingredients are from established knowledge. Much of cosmological research is theoretical in nature. Mathematics research does not rely on externally available data; rather, it seeks to prove theorems about mathematical objects.\n\nResearch ethics involves the application of fundamental ethical principles to a variety of topics involving research, including scientific research. These principles include deontology, consequentialism, virtue ethics and value (ethics). Ethical issues may arise in the design and implementation of research involving human experimentation or animal experimentation, such as: various aspects of academic scandal, including scientific misconduct (such as fraud, fabrication of data and plagiarism), whistleblowing; regulation of research, etc. Research ethics is most developed as a concept in medical research. The key agreement here is the 1964 Declaration of Helsinki. The Nuremberg Code is a former agreement, but with many still important notes. Research in the social sciences presents a different set of issues than those in medical research and can involve issues of researcher and participant safety, empowerment and access to justice.\n\nWhen research involves human subjects, obtaining informed consent from them is essential.\n\nIn many disciplines, Western methods of conducting research are predominant. Researchers are overwhelmingly taught Western methods of data collection and study. The increasing participation of indigenous peoples as researchers has brought increased attention to the lacuna in culturally-sensitive methods of data collection. Non-Western methods of data collection may not be the most accurate or relevant for research on non-Western societies. For example, \"Hua Oranga\" was created as a criterion for psychological evaluation in Māori populations, and is based on dimensions of mental health important to the Māori people – \"taha wairua (the spiritual dimension), taha hinengaro (the mental dimension), taha tinana (the physical dimension), and taha whanau (the family dimension)\".\n\nPeriphery scholars face the challenges of exclusion and linguicism in research and academic publication. As the great majority of mainstream academic journals are written in English, multilingual periphery scholars often must translate their work to be accepted to elite Western-dominated journals. Multilingual scholars' influences from their native communicative styles can be assumed to be incompetence instead of difference.\n\nPeer review is a form of self-regulation by qualified members of a profession within the relevant field. Peer review methods are employed to maintain standards of quality, improve performance, and provide credibility. In academia, scholarly peer review is often used to determine an academic paper's suitability for publication. Usually, the peer review process involves experts in the same field who are consulted by editors to give a review of the scholarly works produced by a colleague of theirs from an unbiased and impartial point of view, and this is usually done free of charge. The tradition of peer reviews being done for free has however brought many pitfalls which are also indicative of why most peer reviewers decline many invitations to review. It was observed that publications from periphery countries rarely rise to the same elite status as those of North America and Europe, because limitations on the availability of resources including high-quality paper and sophisticated image-rendering software and printing tools render these publications less able to satisfy standards currently carrying formal or informal authority in the publishing industry. These limitations in turn result in the under-representation of scholars from periphery nations among the set of publications holding prestige status relative to the quantity and quality of those scholars' research efforts, and this under-representation in turn results in disproportionately reduced acceptance of the results of their efforts as contributions to the body of knowledge available worldwide.\n\nThe open access movement assumes that all information generally deemed useful should be free and belongs to a \"public domain\", that of \"humanity\". This idea gained prevalence as a result of Western colonial history and ignores alternative conceptions of knowledge circulation. For instance, most indigenous communities consider that access to certain information proper to the group should be determined by relationships.\n\nThere is alleged to be a double standard in the Western knowledge system. On the one hand, \"digital right management\" used to restrict access to personal information on social networking platforms is celebrated as a protection of privacy, while simultaneously when similar functions are used by cultural groups (i.e. indigenous communities) this is denounced as \"access control\" and reprehended as censorship.\n\nEven though Western dominance seems to be prominent in research, some scholars, such as Simon Marginson, argue for \"the need [for] a plural university world\". Marginson argues that the East Asian Confucian model could take over the Western model.\n\nThis could be due to changes in funding for research both in the East and the West. Focussed on emphasizing educational achievement, East Asian cultures, mainly in China and South Korea, have encouraged the increase of funding for research expansion. In contrast, in the Western academic world, notably in the United Kingdom as well as in some state governments in the United States, funding cuts for university research have occurred, which some say may lead to the future decline of Western dominance in research.\n\nIn several national and private academic systems, the professionalisation of research has resulted in formal job titles.\n\nIn present-day Russia, the former Soviet Union and in some post-Soviet states the term \"researcher\" (, \"nauchny sotrudnik\") is both a generic term for a person who carried out scientific research, as well as a job position within the frameworks of the USSR Academy of Sciences, Soviet universities, and in other research-oriented establishments. The term is also sometimes translated as \"research fellow\", \"research associate\", etc.\n\nThe following ranks are known:\n\n\nAcademic publishing is a system that is necessary for academic scholars to peer review the work and make it available for a wider audience. The system varies widely by field and is also always changing, if often slowly. Most academic work is published in journal article or book form. There is also a large body of research that exists in either a thesis or dissertation form. These forms of research can be found in databases explicitly for theses and dissertations. In publishing, STM publishing is an abbreviation for academic publications in science, technology, and medicine.\nresearch paper guides\nMost established academic fields have their own scientific journals and other outlets for publication, though many academic journals are somewhat interdisciplinary, and publish work from several distinct fields or subfields. The kinds of publications that are accepted as contributions of knowledge or research vary greatly between fields, from the print to the electronic format. A study suggests that researchers should not give great consideration to findings that are not replicated frequently. It has also been suggested that all published studies should be subjected to some measure for assessing the validity or reliability of its procedures to prevent the publication of unproven findings. Business models are different in the electronic environment. Since about the early 1990s, licensing of electronic resources, particularly journals, has been very common. Presently, a major trend, particularly with respect to scholarly journals, is open access. There are two main forms of open access: open access publishing, in which the articles or the whole journal is freely available from the time of publication, and self-archiving, where the author makes a copy of their own work freely available on the web.\n\nMost funding for scientific research comes from three major sources: corporate research and development departments; private foundations, for example, the Bill and Melinda Gates Foundation; and government research councils such as the National Institutes of Health in the USA and the Medical Research Council in the UK. These are managed primarily through universities and in some cases through military contractors. Many senior researchers (such as group leaders) spend a significant amount of their time applying for grants for research funds. These grants are necessary not only for researchers to carry out their research but also as a source of merit.\n\nThe Social Psychology Network provides a comprehensive list of U.S. Government and private foundation funding sources.\n\n\n\n"}
{"id": "25202899", "url": "https://en.wikipedia.org/wiki?curid=25202899", "title": "Research spin-off", "text": "Research spin-off\n\nA research spin-off is a company that falls into at least one of the four following categories:\n\n\nThe two main research spin-off models in Russia are those developed from the Institutes of the Academy of Science and Svetlana. QinetiQ is an example of a research spin-off in the United Kingdom.\n\n"}
{"id": "19206529", "url": "https://en.wikipedia.org/wiki?curid=19206529", "title": "SCALE-UP", "text": "SCALE-UP\n\nSCALE-UP is a learning environment specifically created to facilitate active, collaborative learning in a studio-like setting. Some people think the rooms look more like restaurants than classrooms. The spaces are carefully designed to facilitate interactions between teams of students who work on short, interesting tasks. A decade of research indicates significant improvements in learning. The approach taken during the development and testing of the learning environment is an application of scientific teaching and has been discussed in several books. Although originated at North Carolina State University, more than five hundred colleges across the US and around the world are known to have directly adopted the SCALE-UP model and adapted it to their particular needs. Information about more than 400 of these implementation sites is available on the SCALE-UP website.\n\nThe SCALE-UP name originally stood for “Student-Centered Activities for Large Enrollment Undergraduate Physics” but since its conception many different institutions have begun teaching a variety of courses of various sizes. The acronym was changed to “Student-Centered Active Learning Environment for Undergraduate Programs.” Now, because of the increasing number of pre-college installations, plus to draw attention to the instruction as well as the space, the name has become \"Student-Centered Active Learning Environment with Upside-down Pedagogies.\" The basic idea is that students are given something interesting to investigate. While they work in teams on these \"tangibles\" (hands-on measurements or observations) and \"ponderables\" (interesting, complex problems), the instructor is free to roam around the classroom–--asking questions, sending one team to help another, or asking why someone else got a different answer. There is no separate lab class and most of the \"lectures\" are actually class-wide discussions. The groups are carefully structured and give students many opportunities to interact. Three teams (labelled a, b, and c) sit at each round table and have white boards nearby. Each team has a laptop in case they need web access. The original design called for 11 round tables of nine students, but many schools have smaller classes while a few have even larger ones. Smaller classes, particularly those in high schools, have also been using D-shaped tables that seat six students.\n"}
{"id": "42139922", "url": "https://en.wikipedia.org/wiki?curid=42139922", "title": "SchedMD", "text": "SchedMD\n\nSchedMD LLC is an American software company that is the main developer of the Slurm Workload Manager (or Slurm), an open-source workload management system. SchedMD also provides support, training and consulting services around Slurm.\n\nSchedMD was founded in 2010, specifically to develop and provide services around Slurm. Its corporate headquarters are in Lehi, Utah.\n\nSlurm began development as a collaborative effort primarily by Lawrence Livermore National Laboratory, Linux NetworX, Hewlett-Packard and Groupe Bull as a Free Software resource manager in 2001. In 2010 Morris Jette and Danny Auble incorporated SchedMD LLC, to develop and market Slurm.\n\nSchedMD provides services to many national labs, universities, corporations and government agencies, including:\n\nSchedMD partly operates on a professional open-source business model based on open source code, development within a community, professional quality assurance, and subscription-based customer support.\n\nSchedMD sells subscriptions for support, training and integration services. Customers pay a fixed price for unlimited access to services.\n\n"}
{"id": "27154352", "url": "https://en.wikipedia.org/wiki?curid=27154352", "title": "Science Buddies", "text": "Science Buddies\n\nScience Buddies, formerly the Kenneth Lafferty Hess Family Charitable Foundation, is a non-profit organization that provides a website of free science project productivity tools and mentoring to support K-12 students, especially for science fairs. Founded in 2001 by engineer and high-tech businessman, Kenneth Hess, Science Buddies features STEM content and services to assist students and educators. Since its founding, it has expanded its original mission to provide teacher resources targeted for classroom and science fair use.\nScience Buddies mission is to help students to build their literacy in science and technology so they can become productive and engaged citizens in the 21st century.\n\nThe site has personalized learning tools, over 15,000 pages of scientist-developed subject matter (including experiments based on the latest academic research), and an online community of science professionals who volunteer to advise students. \n\nScience Buddies also provides resources to support parents and teachers as they guide students seeking out and performing science projects. They attempt to provide a bridge between scientists, engineers, educators, and students, giving students access to current scientific research and simultaneously giving scientists a way to reach out to young people interested in their fields.\n\nNoticing how much fun his teenage daughter had participating in science fairs but dismayed to discover a shortage of quality science fair help online, Ken Hess thought science fair \"productivity tools\" and mentoring would allow many more students to participate in science fairs and develop inspirational relationships with science role models. Over time, such a program would help students improve their science skills and literacy while inspiring them to consider careers in science and engineering. So, in early 2001, Ken Hess started a charity with a mission of developing online tools and support for students doing science fair projects.\n\nIn collaboration with high tech companies, government labs and agencies (like NOAA and NASA), universities, and other science education resources, Science Buddies offers scientist-authored tools, tips, and techniques. Dr. Doug Osheroff, (Nobel Prize winning physicist), and Dr. Bernard Harris (retired NASA astronaut) both serve on the Science Buddies scientific advisory board.\n\nScience Buddies is an award-winning website, recommended by educational organizations such as the ALA and the SciLinks program of the National Science Teachers Association (NSTA). All resources and tools on the Science Buddies website are available free to students and teachers. Science Buddies uses an underwriting model of sponsorship (similar to PBS television) by displaying sponsor information. Current corporate sponsors include several major companies, such as Best Buy, Cisco, Genentech, and Toyota USA Foundation.\n\n\n\n"}
{"id": "53617664", "url": "https://en.wikipedia.org/wiki?curid=53617664", "title": "Science and Engineering Challenge", "text": "Science and Engineering Challenge\n\nThe Science and Engineering Challenge (SEC) is a non-profit, STEM outreach program run throughout the schools year in Australia. The goal of the program is to challenge student’s perception of science and engineering and experience aspects of those fields that they normally would not encounter in a school environment.\n\nThe SEC focuses on inspiring students in year 10 to consider a future career in science and engineering by choosing to study science and mathematics in years 11 and 12. The SEC also includes other events such as Discovery Days and the S.M.A.R.T outreach program.\n\nThe SEC began at the University of Newcastle as an initiative of the Faculties of Engineering and Built Environment, and Science and Information Technology. Initially, information nights were conducted aimed at giving students and parents from rural areas the opportunity to find out about careers in science and engineering. Based on the success of these information nights, the first SEC event was held on the Central Coast in the year 2000, as an activity for National Science Week. Throughout 2001, events were held around the Newcastle area and other parts of NSW.\n\n2002 saw the first Challenge Days held outside of NSW, this took place in Canberra. Over the next three years, Challenge Days were conducted in Queensland, Tasmania, South Australia and Victoria. In 2005 the winners from each state competed at the first National Final.\n\nIn 2017 there were 110 SMART events, 49 Discovery Days for primary-aged students, and 99 Challenge Days for year 9-10 students. Overall more than 50,000 people were involved in one or more SEC programs. Almost 2,100 teachers and 3,600 other volunteers were also involved.\nSince 1998 over half a million people have taken part in a SEC event.\n\nIn 2005 the Challenge expanded yet again to include events for primary school students, called Discovery Days. These are largely the same as regular challenge events; however, the actives are simplified and typical run for a shorter period of time.\n\n"}
{"id": "411590", "url": "https://en.wikipedia.org/wiki?curid=411590", "title": "Science and technology studies", "text": "Science and technology studies\n\nScience and technology studies, or science, technology and society studies (both abbreviated STS) is the study of how society, politics, and culture affect scientific research and technological innovation, and how these, in turn, affect society, politics and culture.\n\nLike most interdisciplinary programs, STS emerged from the confluence of a variety of disciplines and disciplinary subfields, all of which had developed an interest—typically, during the 1960s or 1970s—in viewing science and technology as socially embedded enterprises. The key disciplinary components of STS took shape independently, beginning in the 1960s, and developed in isolation from each other well into the 1980s, although Ludwik Fleck's (1935) monograph \"Genesis and Development of a Scientific Fact\" anticipated many of STS's key themes. In the 1970s Elting E. Morison founded the STS program at Massachusetts Institute of Technology (MIT), which served as a model. By 2011, 111 STS research centres and academic programs were counted worldwide.\n\n\nDuring the 1970s and 1980s, leading universities in the US, UK, and Europe began drawing these various components together in new, interdisciplinary programs. For example, in the 1970s, Cornell University developed a new program that united science studies and policy-oriented scholars with historians and philosophers of science and technology. Each of these programs developed unique identities due to variation in the components that were drawn together, as well as their location within the various universities. For example, the University of Virginia's STS program united scholars drawn from a variety of fields (with particular strength in the history of technology); however, the program's teaching responsibilities—it is located within an engineering school and teaches ethics to undergraduate engineering students—means that all of its faculty share a strong interest in engineering ethics.\n\nA decisive moment in the development of STS was the mid-1980s addition of technology studies to the range of interests reflected in science. During that decade, two works appeared \"en seriatim\" that signaled what Steve Woolgar was to call the \"turn to technology\": \"Social Shaping of Technology\" (MacKenzie and Wajcman, 1985) and \"The Social Construction of Technological Systems\" (Bijker, Hughes and Pinch, 1987). MacKenzie and Wajcman primed the pump by publishing a collection of articles attesting to the influence of society on technological design. In a seminal article, Trevor Pinch and Wiebe Bijker attached all the legitimacy of the Sociology of Scientific Knowledge to this development by showing how the sociology of technology could proceed along precisely the theoretical and methodological lines established by the sociology of scientific knowledge. This was the intellectual foundation of the field they called the social construction of technology.\n\nThe \"turn to technology\" helped to cement an already growing awareness of underlying unity among the various emerging STS programs. More recently, there has been an associated turn to ecology, nature, and materiality in general, whereby the socio-technical and natural/material co-produce each other. This is especially evident in work in STS analyses of biomedicine (such as Carl May, Annemarie Mol, Nelly Oudshoorn, and Andrew Webster) and ecological interventions (such as Bruno Latour, Sheila Jasanoff, Matthias Gross, S. Lochlann Jain, and Jens Lachmund).\n\nThe subject has several professional associations.\n\nFounded in 1975, the Society for Social Studies of Science, initially provided scholarly communication facilities, including a journal (\"Science, Technology, and Human Values\") and annual meetings that were mainly attended by science studies scholars. The society has since grown into the most important professional association of science and technology studies scholars worldwide. The Society for Social Studies of Science members also include government and industry officials concerned with research and development as well as science and technology policy; scientists and engineers who wish to better understand the social embeddedness of their professional practice; and citizens concerned about the impact of science and technology in their lives. Proposals have been made to add the word \"technology\" to the association's name, thereby reflecting its stature as the leading STS professional society, that the name is long enough as it is.\n\nIn Europe, the European Association for the Study of Science and Technology (EASST) was founded in 1981 to \"stimulate communication, exchange and collaboration in the field of studies of science and technology\". Similarly, the European Inter-University Association on Society, Science and Technology (ESST) researches and studies science and technology in society, in both historical and contemporary perspectives.\n\nIn Asia several STS associations exist.\nIn Japan, the Japanese Society for Science and Technology Studies (JSSTS) was founded in 2001. The Asia Pacific Science Technology & Society Network (APSTSN) primarily has members from Australasia, Southeast and East Asia and Oceania.\n\nIn Latin America ESOCITE (Estudios Sociales de la Ciencia y la Tecnología) is the biggest association of Science and Technology studies. The study of STS (CyT in Spanish, CTS in Portuguese) here was shaped by authors like Amílcar Herrera and Jorge Sabato y Oscar Varsavsky in Argentina, José Leite Lopes in Brazil, Miguel Wionczek in Mexico, Francisco Sagasti in Peru, Máximo Halty Carrere in Uruguay and Marcel Roche in Venezuela.\n\nFounded in 1958, the Society for the History of Technology initially attracted members from the history profession who had interests in the contextual history of technology. After the \"turn to technology\" in the mid-1980s, the society's well-regarded journal (\"Technology and Culture\") and its annual meetings began to attract considerable interest from non-historians with technology studies interests.\n\nLess identified with STS, but also of importance to many STS scholars, are the History of Science Society, the Philosophy of Science Association, and the American Association for the History of Medicine.\n\nAdditionally, within the US there are significant STS-oriented special interest groups within major disciplinary associations, including the American Anthropological Association, the American Political Science Association, the National Women's Studies Association, and the American Sociological Association.\n\nNotable peer-reviewed journals in STS include: \nStudent journals in STS include: \nSocial constructions are human created ideas, objects, or events created by a series of choices and interactions. These interactions have consequences that change the perception that different groups of people have on these constructs. Some examples of social construction include class, race, money, and citizenship.\n\nThe following also alludes to the notion that not everything is set, a circumstance or result could potentially be one way or the other. According to the article \"What is Social Construction?\" by Laura Flores, \"Social construction work is critical of the status quo. Social constructionists about X tend to hold that:\nVery often they go further, and urge that:\nIn the past, there have been viewpoints that were widely regarded as fact until being called to question due to the introduction of new knowledge. Such viewpoints include the past concept of a correlation between intelligence and the nature of a human's ethnicity or race (X may not be at all as it is).\n\nAn example of the evolution and interaction of various social constructions within science and technology can be found in the development of both the high-wheel bicycle, or velocipede, and then of the bicycle. The velocipede was widely used in the latter half of the 19th century. In the latter half of the 19th century, a social need was first recognized for a more efficient and rapid means of transportation. Consequently, the velocipede was first developed, which was able to reach higher translational velocities than the smaller non-geared bicycles of the day, by replacing the front wheel with a larger radius wheel. One notable trade-off was a certain decreased stability leading to a greater risk of falling. This trade-off resulted in many riders getting into accidents by losing balance while riding the bicycle or being thrown over the handle bars.\n\nThe first \"social construction\" or progress of the velocipede caused the need for a newer \"social construction\" to be recognized and developed into a safer bicycle design. Consequently, the velocipede was then developed into what is now commonly known as the \"bicycle\" to fit within society's newer \"social construction,\" the newer standards of higher vehicle safety. Thus the popularity of the modern geared bicycle design came as a response to the first social construction, the original need for greater speed, which had caused the high-wheel bicycle to be designed in the first place. The popularity of the modern geared bicycle design ultimately ended the widespread use of the velocipede itself, as eventually it was found to best accomplish the social-needs/ social-constructions of both greater speed and of greater safety.\n\nTechnoscience is a subset of Science, Technology, and Society studies that focuses on the inseparable connection between science and technology. It states that fields are linked and grow together, and scientific knowledge requires an infrastructure of technology in order to remain stationary or move forward. Both technological development and scientific discovery drive one another towards more advancement. Technoscience excels at shaping human thought and behavior by opening up new possibilities that gradually or quickly come to be perceived as necessities.\n\n\"Technological action is a social process.\" Social factors and technology are intertwined so that they are dependent upon each other. This includes the aspect that social, political, and economic factors are inherent in technology and that social structure influences what technologies are pursued. In other words, \"technoscientific phenomena combined inextricably with social/political/ economic/psychological phenomena, so 'technology' includes a spectrum of artifacts, techniques, organizations, and systems.\" Winner expands on this idea by saying \"in the late twentieth century technology and society, technology and culture, technology and politics are by no means separate.\"\n\n\nDeliberative democracy is a reform of representative or direct democracies which mandates discussion and debate of popular topics which affect society. Deliberative Democracy is a tool for making decisions. Deliberative democracy can be traced back all the way to Aristotle’s writings. More recently, the term was coined by Joseph Bessette in his 1980 work \"Deliberative Democracy: The Majority Principle in Republican Government\", where he uses the idea in opposition to the elitist interpretations of the United States Constitution with emphasis on public discussion.\n\nDeliberative Democracy can lead to more legitimate, credible, and trustworthy outcomes. Deliberative Democracy allows for \"a wider range of public knowledge,\" and it has been argued that this can lead to \"more socially intelligent and robust\" science. One major shortcoming of deliberative democracy is that many models insufficiently ensure critical interaction.\n\nAccording to Ryfe, there are five mechanisms that stand out as critical to the successful design of deliberative democracy:\n\nRecently, there has been a movement towards greater transparency in the fields of policy and technology. Jasanoff comes to the conclusion that there is no longer a question of if there needs to be increased public participation in making decisions about science and technology, but now there needs to be ways to make a more meaningful conversation between the public and those developing the technology.\n\nAckerman and Fishkin offer an example of a reform in their paper \"Deliberation Day.\" The deliberation is to enhance public understanding of popular, complex, and controversial issues, through devices such as Fishkin’s Deliberative Polling. Although implementation of these reforms is unlikely in a large government situation such as the United States Federal Government. However, things similar to this have been implemented in small, local, governments like New England towns and villages. New England town hall meetings are a good example of deliberative democracy in a realistic setting.\n\nAn ideal Deliberative Democracy balances the voice and influence of all participants. While the main aim is to reach consensus, a deliberative democracy should encourage the voices of those with opposing viewpoints, concerns due to uncertainties, and questions about assumptions made by other participants. It should take its time and ensure that those participating understand the topics on which they debate. Independent managers of debates should also have substantial grasp of the concepts discussed, but must \"[remain] independent and impartial as to the outcomes of the process.\"\n\nIn 1968, Garrett Hardin popularised the phrase \"tragedy of the commons.\" It is an economic theory where rational people act against the best interest of the group by consuming a common resource. Since then, the tragedy of the commons has been used to symbolize the degradation of the environment whenever many individuals use a common resource. Although Garrett Hardin was not an STS scholar, the concept of tragedy of the commons still applies to science, technology and society.\n\nIn a contemporary setting, the Internet acts as an example of the tragedy of the commons through the exploitation of digital resources and private information. Data and internet passwords can be stolen much more easily than physical documents. Virtual spying is almost free compared to the costs of physical spying. Additionally, net neutrality can be seen as an example of tragedy of the commons in an STS context. The movement for net neutrality argues that the Internet should not be a resource that is dominated by one particular group, specifically those with more money to spend on Internet access.\n\nA counterexample to the tragedy of the commons is offered by Andrew Kahrl. Privatization can be a way to deal with the tragedy of the commons. However, Kahrl suggests that the privatization of beaches on Long Island, in an attempt to combat overuse of Long Island beaches, made the residents of Long Island more susceptible to flood damage from Hurricane Sandy. The privatization of these beaches took away from the protection offered by the natural landscape. Tidal lands that offer natural protection were drained and developed. This attempt to combat the tragedy of the commons by privatization was counter-productive. Privatization actually destroyed the public good of natural protection from the landscape.\n\nAlternative modernity is a conceptual tool conventionally used to represent the state of present western society. Modernity represents the political and social structures of the society, the sum of interpersonal discourse, and ultimately a snapshot of society's direction at a point in time. Unfortunately conventional modernity is incapable of modeling alternative directions for further growth within our society. Also, this concept is ineffective at analyzing similar but unique modern societies such as those found in the diverse cultures of the developing world. Problems can be summarized into two elements: inward failure to analyze growth potentials of a given society, and outward failure to model different cultures and social structures and predict their growth potentials.\n\nPreviously, modernity carried a connotation of the current state of being modern, and its evolution through European colonialism. The process of becoming \"modern\" is believed to occur in a linear, pre-determined way, and is seen by Philip Brey as a way of to interpret and evaluate social and cultural formations. This thought ties in with modernization theory, the thought that societies progress from \"pre-modern\" to \"modern\" societies.\n\nWithin the field of science and technology, there are two main lenses with which to view modernity. The first is as a way for society to quantify what it wants to move towards. In effect, we can discuss the notion of \"alternative modernity\" (as described by Andrew Feenberg) and which of these we would like to move towards. Alternatively, modernity can be used to analyze the differences in interactions between cultures and individuals. From this perspective, alternative modernities exist simultaneously, based on differing cultural and societal expectations of how a society (or an individual within society) should function. Because of different types of interactions across different cultures, each culture will have a different modernity.\n\nPace of Innovation is the speed at which technological innovation or advancement is occurring, with the most apparent instances being too slow or too rapid. Both these rates of innovation are extreme and therefore have effects on the people that get to use this technology.\n\n\"No innovation without representation\" is a democratic ideal of ensuring that everyone involved gets a chance to be represented fairly in technological developments.\n\n\nThe privileged positions of business and science refer to the unique authority that persons in these areas hold in economic, political, and technosocial affairs. Businesses have strong decision-making abilities in the function of society, essentially choosing what technological innovations to develop. Scientists and technologists have valuable knowledge, ability to pursue the technological innovations they want. They proceed largely without public scrutiny and as if they had the consent of those potentially affected by their discoveries and creations.\n\nLegacy thinking is defined as an inherited method of thinking imposed from an external source without objection by the individual, because it is already widely accepted by society.\n\nLegacy thinking can impair the ability to drive technology for the betterment of society by blinding people to innovations that do not fit into their accepted model of how society works. By accepting ideas without questioning them, people often see all solutions that contradict these accepted ideas as impossible or impractical. Legacy thinking tends to advantage the wealthy, who have the means to project their ideas on the public. It may be used by the wealthy as a vehicle to drive technology in their favor rather than for the greater good.\nExamining the role of citizen participation and representation in politics provides an excellent example of legacy thinking in society. The belief that one can spend money freely to gain influence has been popularized, leading to public acceptance of corporate lobbying. As a result, a self-established role in politics has been cemented where the public does not exercise the power ensured to them by the Constitution to the fullest extent. This can become a barrier to political progress as corporations who have the capital to spend have the potential to wield great influence over policy. Legacy thinking however keeps the population from acting to change this, despite polls from Harris Interactive that report over 80% of Americans feel that big business holds too much power in government. Therefore, Americans are beginning to try to steer away this line of thought, rejecting legacy thinking, and demanding less corporate, and more public, participation in political decision making.\n\nAdditionally, an examination of net neutrality functions as a separate example of legacy thinking. Starting with dial-up, the internet has always been viewed as a private luxury good. Internet today is a vital part of modern-day society members. They use it in and out of life every day. Corporations are able to mislabel and greatly overcharge for their internet resources. Since the American public is so dependent upon internet there is little for them to do. Legacy thinking has kept this pattern on track despite growing movements arguing that the internet should be considered a utility. Legacy thinking prevents progress because it was widely accepted by others before us through advertising that the internet is a luxury and not a utility. Due to pressure from grassroots movements the Federal Communications Commission (FCC) has redefined the requirements for broadband and internet in general as a utility. Now AT&T and other major internet providers are lobbying against this action and are in-large able to delay the onset of this movement due to legacy thinking’s grip on American culture and politics.\n\nFor example, those who cannot overcome the barrier of legacy thinking may not consider the privatization of clean drinking water as an issue. This is partially because access to water has become such a given fact of the matter to them. For a person living in such circumstances, it may be widely accepted to not concern themselves with drinking water because they have not needed to be concerned with it in the past. Additionally, a person living within an area that does not need to worry about their water supply or the sanitation of their water supply is less likely to be concerned with the privatization of water.\n\nThis notion can be examined through the thought experiment of \"veil of ignorance\". Legacy thinking causes people to be particularly ignorant about the implications behind the \"you get what you pay for\" mentality applied to a life necessity. By utilizing the \"veil of ignorance\", one can overcome the barrier of legacy thinking as it requires a person to imagine that they are unaware of their own circumstances, allowing them to free themselves from externally imposed thoughts or widely accepted ideas.\n\n\n\nSTS is taught in several countries. According to the STS wiki, STS programs can be found in twenty countries, including 45 programs in the United States, three programs in India, and eleven programs in the UK. STS programs can be found in Israel, Malaysia, and Taiwan. Some examples of institutions offering STS programs are Stanford University, Harvard University, the University of Oxford, Mines ParisTech, and Bar-Ilan University.\n\n\n"}
{"id": "13432082", "url": "https://en.wikipedia.org/wiki?curid=13432082", "title": "Science communication", "text": "Science communication\n\nScience communication is the public communication of science-related topics to non-experts. This often involves professional scientists (called \"outreach\" or \"popularization\"), but has also evolved into a professional field in its own right. It includes science exhibitions, journalism, policy or media production. Science communication also includes communication between scientists (for instance through scientific journals), as well as between scientists and non-scientists (especially during public controversies over science and in citizen science initiatives).\nScience communication may generate support for scientific research or study, or to inform decision making, including political and ethical thinking. There is increasing emphasis on explaining methods rather than simply findings of science. This may be especially critical in addressing scientific misinformation, which spreads easily because it is not subject to the constraints of scientific method.\nScience communicators can use entertainment and persuasion including humour, storytelling and metaphors. Scientists can be trained in some of the techniques used by actors to improve their communication.\n\nPartly due to a market for professional training, science communication is also an academic discipline. Journals include \"Public Understanding of Science\" and \"Science Communication\". Researchers in this field are often linked to Science and Technology Studies, but may also come from history of science, mainstream media studies, psychology or sociology. As a reflection of growth in this field, academic departments, such as the \"Department of Life Sciences Communication\" at the University of Wisconsin-Madison, have been established to focus on applied and theoretical communication issues. Agricultural communication is considered a subset of science communication from an academic and professional standpoint relating to agriculture-related information among agricultural and non-agricultural stakeholders. Health communication is a related discipline.\nWriting in 1987, Geoffery Thomas and John Durant advocated various reasons to increase public understanding of science, or scientific literacy. If the public enjoyed science more, they suggested there would presumably be more funding, progressive regulation, and trained scientists. More trained engineers and scientists could allow a nation to be more competitive economically.\nScience can also benefit individuals. Science can simply have aesthetic appeal (e.g. popular science or science fiction). Living in an increasingly technological society, background scientific knowledge can help to negotiate it. The science of happiness is an example of a field whose research can have direct and obvious implications for individuals.\nGovernments and societies might also benefit from more scientific literacy, since an informed electorate promotes a more democratic society. Moreover, science can inform moral decision making (e.g. answering questions about whether animals can feel pain, how human activity influences climate, or even a science of morality).\nBernard Cohen points out potential pitfalls in improving scientific literacy. He explains first that we must avoid 'scientific idolatry'. In other words, science education must allow the public to respect science without worshiping it, or expecting infallibility. Ultimately scientists are humans, and neither perfectly altruistic, nor perfectly competent. Science communicators must also appreciate the distinction between understanding science and possessing a transferable skill of scientific thinking. Indeed, even trained scientists do not always manage to transfer the skill to other areas of their life.\n\nCommunicating science to the public is increasingly important in today's society. However according to some research, some scientists do not have the skills necessary to do so effectively. There has been some research done over why this is, and it has been found that the stereotype of scientists is the main reason they will not communicate to the public often. The \"Draw a Scientist\" experiment proves that from a young age, most people assume that scientists are unsocial, so scientists use that as a reason to not communicate.\n\nCohen is critical of what has been called \"Scientism\" – the claim that science is the best or only way to solve all problems. He also criticizes the teaching of 'miscellaneous information' and doubts that much of it will ever be of any use, (e.g. the distance in light years from the Earth to various stars, or the names of minerals). Much of scientific knowledge, particularly if it is not the subject of public debate and policy revision, may never really translate to practical changes for the lives of the learners.\n\nMany criticisms of academic research in public understanding of science come from scholars in Science and Technology Studies. For example, Steven Hilgartner (1990) argues that what he calls 'the dominant view' of science popularization tends to imply a tight boundary around those who can articulate true, reliable knowledge. By defining a deficient public as recipients of knowledge, the scientists get to contrast their own identity as experts. The process of popularization is a form of boundary work. Understood in this way, science communication may explicitly exist to connect scientists with the rest of society, but its very existence only acts to emphasise it: as if the scientific community only invited the public to play in order to reinforce its most powerful boundary (according to work by Massimiano Bucchi or Brian Wynne).\n\nBiologist Randy Olson adds that anti-science groups can often be so motivated, and so well funded, that the impartiality of science organizations in politics can lead to crises of public understanding of science. He cites examples of denialism (for instance of global warming) to support this worry. Journalist Robert Krulwich likewise argues that the stories scientists tell are invariably competing with the efforts of people like Adnan Oktar. Krulwich explains that attractive, easy to read, and cheap creationist textbooks were sold by the thousands to schools in Turkey (despite their strong secular tradition) due to the efforts of Oktar. Astrobiologist David Morrison has spoken of repeated disruption of his work by popular anti-scientific phenomena, having been called upon to assuage public fears of an impending cataclysm involving an unseen planetary object—first in 2008, and again in 2012 and 2017.\n\nMarine biologist and film-maker Randy Olson published \"Don't Be Such a Scientist: Talking Substance in an Age of Style\". In the book he describes how there has been this unproductive negligence when it comes to teaching scientists to communicate. \"Don't be Such a Scientist\" is written to his fellow scientists, and he says they need to \"lighten up\". He adds that scientists are ultimately the most responsible for promoting and explaining science to the public and media. This, Olson says, should be done according to a good grasp of social science; scientists must use persuasive and effective means like story telling. Olson acknowledges that the stories told by scientists need not only be compelling but also accurate to modern science - and says this added challenge must simply be confronted. He points to figures like Carl Sagan as effective popularizers, partly because such figures actively cultivate a likeable image.\n\nScience popularization figures such as Carl Sagan and Neil Degrasse Tyson are partly responsible for the view of science or a specific science discipline within the general public. However, the degree of knowledge and experience a science popularizer has can vary greatly. Because of this, some can depend on sensationalism. As a Forbes contributor put it, \"The main job of physics popularizers is the same as it is for any celebrity: get more famous.\" Because of this variation in experience, research scientists can sometimes question the credibility of science popularizers. Another point in the controversy of popular science is the idea of how public debate can affect public opinion. A relevant and highly public example of this is climate change. A science communication study appearing in the New York Times proves that \"even a fractious minority wields enough power to skew a reader’s perception of a [science news] story” and that even “firmly worded (but not uncivil) disagreements between commenters affected readers’ perception of science.” This causes some to worry that the popularizing of science in the public, questioning whether the further popularization of science will cause pressure towards generalization or sensationalism. Unfortunately, this question will fall to time for an answer.\n\nAs his commencement address to Caltech students, journalist Robert Krulwich delivered a speech entitled \"Tell me a story\". Krulwich says that scientists are actually given many opportunities to explain something interesting about science or their work, and that they must seize such opportunities. He says scientists must resist shunning the public, as Sir Isaac Newton did in his writing, and instead embrace metaphors the way Galileo did; Krulwich suggests that metaphors only become more important as the science gets more difficult to understand. He adds that telling stories of science in practice, of scientists' success stories and struggles, helps convey that scientists are real people. Finally, Krulwich advocates for the importance of scientific values in general, and helping the public to understand that scientific views are not mere opinions, but hard-won knowledge.\n\nActor Alan Alda helps scientists and PhD students get more comfortable with communication with the help of drama coaches (they use the acting techniques of Viola Spolin).\n\nMatthew Nisbet describes the use of opinion leaders as intermediaries between scientists and the public as a way to reach the public via trained individuals who are more closely engaged with their communities, such as \"teachers, business leaders, attorneys, policymakers, neighborhood leaders, students, and media professionals.\" Examples of initiatives that take this approach include Science & Engineering Ambassadors, sponsored by the National Academy of Sciences, and Science Booster Clubs, coordinated by the National Center for Science Education.\n\nIn the preface of \"The Selfish Gene\", Richard Dawkins wrote: \"Three imaginary readers looked over my shoulder while I was writing, and I now dedicate the book to them. [...] First the general reader, the layman [...] second the expert [and] third the student\".\n\nMany criticisms of the public understanding of science movement have emphasized that this thing they were calling the public was somewhat of an (unhelpful) black box. Approaches to the public changed with the move away from the public understanding of science. Science communication researchers and practitioners now often showcase their desire to listen to non-scientists as well as acknowledging an awareness of the fluid and complex nature of (post/late) modern social identities. At the very least, people will use plurals: publics or audiences. As the editor of Public Understanding of Science put it in a special issue on publics:\nWe have clearly moved from the old days of the deficit frame and thinking of publics as monolithic to viewing publics as active, knowledgeable, playing multiple roles, receiving as well as shaping science. (Einsiedel, 2007: 5)\nHowever, Einsiedel goes on to suggest both views of the public are \"monolithic\" in their own way; they both choose to declare what something called the public is. Public understanding of science might have ridiculed publics for their ignorance, but an alternative \"public engagement with science and technology\" romanticizes its publics for their participatory instincts, intrinsic morality or simple collective wisdom. As Susanna Hornig Priest (2009) concludes in her recent introduction essay on science’s contemporary audiences, the job of science communication might be to help non-scientists feel they are not excluded as opposed to always included; that they can join in if they want, rather than that there is a necessity to spend their lives engaging.\n\nThe process of quantifiably surveying public opinion of science is now largely associated with the public understanding of science movement (some would say unfairly). In the US, Jon Miller is the name most associated with such work and well known for differentiating between identifiable ‘attentive’ or ‘interested’ publics (that is to say science fans) and those who do not care much about science and technology. Miller’s work questioned whether the American public had the following four attributes of scientific literacy:\n\nIn some respects, John Durant’s work surveying British public applied similar ideas to Miller. However, they were slightly more concerned with attitudes to science and technology, rather than just how much knowledge people had. They also looked at public confidence in their knowledge, considering issues such as the gender of those ticking \"don’t know\" boxes. We can see aspects of this approach, as well as a more \"public engagement with science and technology\" influenced one, reflected within the Eurobarometer studies of public opinion. These have been running since 1973 to monitor public opinion in the member states, with the aim of helping the preparation of policy (and evaluation of policy). They look at a host of topics, not just science and technology but also defence, the euro, enlargement of the European Union, and culture. Eurobarometer’s recent study of Europeans’ Attitudes to Climate Change is a good example. It focuses on respondents’ \"subjective level of information\"; asking \"personally, do you think that you are well informed or not about…?\" rather than checking what people knew.\n\nScience communication can be analysed through frame analysis, a research method used to analyse how people understand situations and activities.\n\nSome features of this analysis are listed below.\n\nPeople make an enormous number of decisions every day, and to approach all of them in a careful, methodical manner is impractical. They therefore often use mental shortcuts known as \"heuristics\" to quickly arrive at acceptable inferences. Tversky and Kahneman originally proposed three heuristics, listed below, although there are many others that have been discussed in later research. \nThe most effective science communication efforts take into account the role that heuristics play in everyday decision-making. Many outreach initiatives focus solely on increasing the public's knowledge, but studies (e.g. Brossard et al. 2012) have found that there is little – if any – correlation between knowledge levels and attitudes towards scientific issues.\n\nThe simulation heuristic is used to judge how likely certain outcomes are based on the ease with which one can imagine a particular ending. This heuristic can be used for many tasks, including prediction (Will the Jets win this football game?) and causality (Did Jim eat the last slice of pizza?). An application of this heuristic is to the case of near misses. Consider the following example from Kahneman & Tversky:Mr. Crane and Mr. Tees were scheduled to leave the airport on different flights, at the same time. They traveled from town in the same limousine, were caught in a traffic jam, and arrived at the airport thirty minutes after the scheduled departure time of their flights.Mr. Crane is told his flight left on time.Mr. Tees is told that his flight was delayed, and just left five minutes ago.Who is more upset?Mr. Crane or Mr. Tees?Almost everyone says, \"Mr. Tees\", because they cannot imagine how Mr. Crane could have caught his flight, while Mr. Tees might have made it if not for that slow pedestrian, or the exceptionally long security line. The simulation heuristic has this ability to generate \"if only\" conditions, which can be used to understand the negative feelings of frustration, indignation, etc. that arise from near misses such as that of Mr. Tees.\n\nThis simulation of how events \"might\" have occurred is referred to as counterfactual thinking, and can be used to try to identify a unique or unusual circumstance that lead to a dramatic outcome. For example, consider a man who is shot during a robbery while shopping at a convenience store. Subjects will award more damages to a man who was shopping at a store far from his house than they will to a man who was shopping at a store near his home that he commonly visits.\n\nRegarding simulations of future events, simply imagining hypothetical events makes them seem more likely to occur. This phenomenon can be extended to a person's own behavior, as imagining oneself performing or refusing to perform an action causes changes in expectations about one's future behavior. Simulation is \"more likely to increase the perceived likelihood of a potential outcome...than to reduce perceived likelihood of a potential consequence\". Thus, the implications of research on the simulation heuristic are particularly intriguing when designing outreach efforts intended to change behaviors, such as increasing recycling or decreasing fast food consumption.\n\nWhile scientific study began to emerge as a popular discourse following the Renaissance and the Enlightenment, science was not widely funded or exposed to the public until the nineteenth century. Most science prior to this was funded by individuals under private patronage and was studied in exclusive groups, like the Royal Society. Public science emerged due to a gradual social change, resulting from the rise of the middle class in the nineteenth century. As scientific inventions, like the conveyor belt and the steam locomotive entered and enhanced the lifestyle of people in the nineteenth century, scientific inventions began to be widely funded by universities and other public institutions in an effort to increase scientific research. Since scientific achievements were beneficial to society, the pursuit of scientific knowledge resulted in science as a profession. Scientific institutions, like the National Academy of Sciences or the British Association for the Advancement of Science are examples of leading platforms for the public discussion of science. David Brewster, founder of the British Association for the Advancement of Science, believed in regulated publications in order to effectively communicate their discoveries, \"so that scientific students may know where to begin their labours.\" As the communication of science reached a wider audience, due to the professionalization of science and its introduction to the public sphere, the interest in the subject increased.\n\nThere was a change in media production in the nineteenth century. The invention of the steam-powered printing press enabled more pages to be printed per hour, which resulted in cheaper texts. Book prices gradually dropped, which gave the working classes the ability to purchase them. No longer reserved for the elite, affordable and informative texts were made available to a mass audience. Historian Aileen Fyfe noted that, as the nineteenth century experienced a set of that sought to improve the lives of those in the working classes, the availability of public knowledge was valuable for intellectual growth. As a result, there were reform efforts to further the knowledge of the less educated. The Society for the Diffusion of Useful Knowledge, led by Henry Brougham, attempted to organize a system for widespread literacy for all classes. Additionally, weekly periodicals, like the \"Penny Magazine\", were aimed to educate the general public on scientific achievements in a comprehensive manner.\n\nAs the audience for scientific texts expanded, the interest in public science did as well. 'Extension lectures' were installed in some universities, like Oxford and Cambridge, which encouraged members of the public to attend lectures. In America, travelling lectures were a common occurrence in the nineteenth century and attracted hundreds of viewers. These public lectures were a part of the lyceum movement and demonstrated basic scientific experiments, which advanced scientific knowledge for both the educated and uneducated viewers.\n\nNot only did the popularization of public science enlighten the general public through mass media, but it also enhanced communication within the scientific community. Although scientists had been communicating their discoveries and achievements through print for centuries, publications with a variety of subjects decreased in popularity. Alternatively, publications in discipline-specific journals were crucial for a successful career in the sciences in the nineteenth century. As a result, scientific journals such as \"Nature\" or \"National Geographic\" possessed a large readership and received substantial funding by the end of the nineteenth century as the popularization of science continued.\n\nScience can be communicated to the public in many different ways. According to Karen Bultitude, a science communication lecturer at University College London, these can be broadly categorised into three groups: traditional journalism, live or face-to-face events, and online interaction. Traditional journalism (for example, newspapers, magazines, television and radio) has the advantage of reaching large audiences; in the past, this is way most people regularly accessed information about science. Traditional media is also more likely to produce information that is high quality (well written or presented), as it will have been produced by professional journalists. Traditional journalism is often also responsible for setting agendas and having an impact on government policy. The traditional journalistic method of communication is one-way, so there can be no dialogue with the public, and science stories can often be reduced in scope so that there is a limited focus for a mainstream audience, who may not be able to comprehend the bigger picture from a scientific perspective. However, there is new research now available on the role of newspapers and television channels in constituting 'scientific public spheres' which enable participation of a wide range of actors in public deliberations.\n\nAnother disadvantage of traditional journalism is that, once a science story is taken up by mainstream media, the scientist(s) involved no longer has any direct control over how his or her work is communicated, which may lead to misunderstanding or misinformation. Research in this area demonstrates how the relationship between journalists and scientists has been strained in some instances. On one hand scientists have reported being frustrated with things like journalists oversimplifying or dramatizing of their work, while on the other hand journalists find scientists difficult to work with and ill-equipped to communicate their work to a general audience. Despite this potential tension, a comparison of scientists from several countries has shown that many scientists are pleased with their media interactions and engage often.\n\nHowever, it is important to note the use of traditional media sources, like newspapers and television, has steadily declined as primary sources for science information, while the internet has rapidly increased in prominence. In 2016, over half of Americans (55 percent) reported using the internet as their primary source to learn about science and technology, compared to 24 percent reporting TV and 4 percent reporting newspapers were their primary sources. Additionally, traditional media outlets have dramatically decreased the number of, or in some cases eliminated, science journalists and the amount of science-related content they publish.\n\nThe second category is live or face-to-face events, such as public lectures (for example, UCL's public lunch hour lectures – museums, debates, science busking, sci-art, science cafes and science festivals. Citizen Science or crowd-sourced science (scientific research conducted, in whole or in part, by amateur or nonprofessional scientists), which can be done with a face-to-face approach, online, or as a combination of the two to engage in science communication. Research has shown that members of the public seek out science information that is entertaining, but also helping citizens to critically participate in risk regulation and S&T governance. Therefore it is important to bear this aspect in mind when communicating scientific information to the public (for example, through events combining science communication and comedy, such as Festival of the Spoken Nerd or during scientific controversies). The advantages of this approach are that it is more personal and allows scientists to interact with the public, allowing for two-way dialogue. Scientists are also better able to control content using this method. Disadvantages of this method include the limited reach, it can also be resource-intensive and costly and also, it may be that only audiences with an existing interest in science will be attracted.\n\nThe third category is online interaction, for example, websites, blogs, wikis and podcasts can also be used for science communication, as can other social media. Online methods of communicating science have the potential to reach huge audiences, can allow direct interaction between scientists and the public, and the content is always accessible and can be somewhat controlled by the scientist. Additionally, online communication of science can help boost scientists' reputation through increased citations, better circulation of articles, and establishing new collaborations. Online communication also allows for both one-way and two-way communication, depending on the audience’s and the author's preferences. However, there are disadvantages in that it is difficult to control how content is picked up by others, and regular attention and updating is needed.\n\nWhen considering whether or not to engage in science communication online, scientists should review what science communication research has shown to be the potential positive and negative outcomes. Online communication has given rise to movements like open science, which advocates for making science more accessible. However, when engaging in communication about science online, scientists should consider not publicizing or reporting findings from their research until it has been peer-reviewed and published, as journals may not accept the work after it has been circulated under the \"Ingelfinger rule\".\n\nOther considerations revolve around how scientsts will be perceived by other scientists for engaging in communication. For example, some scholars have criticized engaged, popular scholars using concepts like the Sagan effect or Kardashian Index. Despite these criticisms, many scientists are taking to communicating their work on online platforms, a sign of potentially changing norms in the field.\n\nBy using Twitter, researchers and academics can discuss and communicate scientific topics with many types of audiences based on various points of view. Some studies indicate that the use of Twitter can positively impact the number of times a scientific article is cited. These studies show that articles that are highly tweeted about are eleven times more likely to be highly cited than those that who few people tweeted.\n\nAs noted in studies by Gunther Eysenbach, research has shed light on how Twitter has a direct link to the advances in the science community. Alison Burt, Editor in chief of Elsevier Connect and author of the article \"How to use social media for science,\" states the potential drawbacks to sharing their research on Twitter.\n\nKimberly Collins of PLOS explains reasons how some scientists are hesitant to join Twitter. Some scientists are hesitant to use social media outlets such as Twitter due to lack of knowledge of the platform, and inexperience with how to make meaningful posts. Some scientists do not see the meaning in using Twitter as a platform to share their research or have the time to add the information into the accounts themselves.\n\nScientists also believe that Twitter is not professional enough for them to put out information as well as receive relevant suggestions and comments back. Scientists did give a positive to using Twitter by (28%) of the scientists who participated in the study said communicating science on Twitter can benefit because of the size and diverse audience it reaches. BoingBoing science editor and New York Times columnist Maggie Koerth-Baker commented on the importance of keeping public and private personas on social media separate in order to maintain professionalism online. According to these findings, posting academic research on a personal social media accounts could potentially send mixed messages to Twitter users.\n\nThere have been occasions where scientific outreach on Twitter has been met with positive results. In September 2017, an 8 year old bug lover was teased at her school for her passion for bugs. This led to the Entomological society of Canada posting a tweet defending her love for bugs called #BugsR4Girls. The ESC’s use of twitter was able to make a statement saying, \"A young girl who loves insects is being bullied & needs our support. DM your email & we'll connect you! #BugsR4Girls\".\n\nIn 2017, a study done by the Pew Research Center of Journalism and Media found that \"About a quarter of social media users follow science related pages and accounts. This group places both more importance and comparatively more trust on science news that comes to them through social media\".\n\nKaren Peterson, director of Scientific Career Development at Fred Hutchinson Cancer research Center stresses the \"importance of using social networks such as Facebook and Twitter to engage in intercommunication\" for establishing an online presence as well. Online presence is necessary for career development. No matter your personality type, career advisors recommend that postdocs use online networking tools to make connections, exchange scientific ideas, and advance a career.\n\nAccording to Nature, \"more than 3,000 scientists and engineers told Nature about their awareness of various giant social media networks and research-profiling sites\". Elena Milani created the SciHashtag project which is a condensed collection of Twitter hashtags regarding science communication and science. Twitter has become a part of researchers’ lives.\n\nThe public understanding of science, public awareness of science and public engagement with science and technology are all terms coined with a movement involving governments and societies in the late 20th century. During the late 19th century, science became a professional subject and influenced by governmental suggestions. Prior to this, public understanding of science was very low on the agenda. However, some well-known figures such as Michael Faraday ran lectures aimed at the non-expert public, his being the famous Christmas Lectures which began in 1825. The 20th century saw groups founded on the basis they could position science in a broader cultural context and allow scientists to communicate their knowledge in a way that could reach and be understood by the general public. In the UK, \"The Bodmer Report\" (or \"The Public Understanding of Science\" as it is more formally known) published in 1985 by The Royal Society changed the way scientists communicated their work to the public. The report was designed to \"review the nature and extent of the public understanding of science in the United Kingdom and its adequacy for an advanced democracy.\". Chaired by the geneticist Sir Walter Bodmer alongside famous scientists such as broadcaster Sir David Attenborough, the report was evidenced by all of the major sectors concerned; scientists, politicians, journalists and industrialists but not the general public. One of the main assumptions drawn from the report was everybody should have some grasp of science and this should be introduced from a young age by teachers who are suitably qualified in the subject area. The report also asked for further media coverage of science including via newspapers and television which has ultimately led to the establishment of platforms such as the Vega Science Trust.\n\nIn both the UK and the United States following the second world war, public views of scientists swayed from great praise to resentment. Therefore, the Bodmer Report highlighted concerns from the scientific community that their withdrawal from society was causing scientific research funding to be weak. Bodmer promoted the communication of science to a wider more general public by expressing to British scientists that it was their responsibility to publicise their research. An upshot of the publication of the report was the creation of the Committee on the Public Understanding of Science (COPUS), a collaboration between the British Association for the Advancement of Science, the Royal Society and the Royal Institution. The engagement between these individual societies caused the necessity for a public understanding of science movement to be taken seriously. COPUS also awarded grants for specific outreach activities allowing the public understanding to come to the fore. Ultimately leading to a cultural shift in the way scientists publicised their work to the wider non-expert community. Although COPUS no longer exists within the UK the name has been adopted in the US by the Coalition for the Public Understanding of Science. An organisation which is funded by the US National Academy of Sciences and the National Science Foundation and focuses on popular science projects such as science cafes, festivals, magazines and citizen science schemes.\n\n\n\n"}
{"id": "12082329", "url": "https://en.wikipedia.org/wiki?curid=12082329", "title": "Science in newly industrialized countries", "text": "Science in newly industrialized countries\n\nScientific research is concentrated in the developed world, with only a marginal contribution from the rest of the world. Most Nobel Laureates are either from United States, Europe, or Japan. Many newly industrialized countries have been trying to establish scientific institutions, but with limited success. There is an insufficient dedicated, inspired and motivated labor pool for science and insufficient investment in science education.\n\nThe reason that there have been so few scientists, who have made their mark globally, from most NIC's (Newly Industrialized Countries) is partly historical and partly social A true scientist is nurtured from the school up wards to scientific establishments. Only, if there are inspired and dedicated school science teachers in abundance, there will be sufficient number of inspired students who would like to take science as a career option and who may one day become a successful scientist.\n\nA common thread can indeed be discerned in the state of science in many NICs. Thus although, most of the science establishments in the major NICs can be said to be doing fairly well, none of them have been as successful as the developed countries.\n\nAfter the Second World War, a small technical elite arose in developing countries such as India, Pakistan, Brazil, and Iraq who had been educated as scientists in the industrialized world. They spearheaded the development of science in these countries, presuming that by pushing for Manhattan project-type enterprises in nuclear power, electronics, pharmaceuticals, or space exploration they could leapfrog the dismally low level of development of science establishments in their countries. India, for example, started a nuclear energy program that mobilized thousands of technicians and cost hundreds of millions of dollars but had limited success. Though China, North Korea, India and Pakistan have been successful in deploying nuclear weapons and some of them e.g. China and India have launched fairly successful space programs, (for example, Chandrayaan I (\"Sanskrit\" चंद्रयान-1), which literally means \"Moon Craft,\" is an unmanned lunar mission by the Indian Space Research Organisation and it hopes to land a motorised rover on the moon in 2010 or 2011 as a part of its second Chandrayaan mission; Chang'e I, China's moon probing project is proceeding in full swing in a well-organized way), the fact remains that most of the scientists responsible for these deeds had received their terminal education from some institution or university in US or Europe. In addition there have been hardly any Nobel laureates in science who have conducted the path-breaking research in a native science establishment.\n\nBrazilian science effectively began in the 19th century, until then, Brazil was a poor colony, without universities, printing presses, libraries, museums, etc. This was perhaps a deliberate policy of the Portuguese colonial power, because they feared that the appearance of educated Brazilian classes would boost nationalism and aspirations toward political independence.\n\nThe first attempts of having a Brazilian science establishment were made around 1783, with the expedition of Portuguese naturalist Alexandre Rodrigues, who was sent by Portugal's prime minister, the Marquis of Pombal, to explore and identify Brazilian fauna, flora and geology. His collections, however, were lost to the French, when Napoleon invaded, and were transported to Paris by Étienne Geoffroy Saint-Hilaire. In 1772, the first learned society, the Sociedade Scientifica, was founded in Rio de Janeiro, but lasted only until 1794. Also, in 1797, the first botanic institute was founded in Salvador, Bahia. In the second and third decades of the twentieth century, the main universities in Brazil were organised from a set of existing medical, engineering and law schools. The University of Brazil dates from 1927, the University of São Paulo - today the largest in the Country - dates from 1934.\n\nToday, Brazil has a well-developed organization of science and technology. Basic research in science is largely carried out in public universities and research centers and institutes, and some in private institutions, particularly in non-profit non-governmental organizations. More than 90% of funding for basic research comes from governmental sources.\n\nApplied research, technology and engineering is also largely carried out in the university and research centers system, contrary-wise to more developed countries such as the United States, South Korea, Germany, Japan, etc. A significant trend is emerging lately. Companies such as Motorola, Samsung, Nokia and IBM have established large R&D&I centers in Brazil. One of the incentive factors for this, besides the relatively lower cost and high sophistication and skills of Brazilian technical manpower, has been the so-called Informatics Law, which exempts from certain taxes up to 5% of the gross revenue of high technology manufacturing companies in the fields of telecommunications, computers, digital electronics, etc. The Law has attracted annually more than 1,5 billion dollars of investment in Brazilian R&D&I. Multinational companies have also discovered that some products and technologies designed and developed by Brazilians are significantly competitive and are appreciated by other countries, such as automobiles, aircraft, software, fiber optics, electric appliances, and so on.\n\nThe challenges Brazilian science faces today are: to expand the system with quality, supporting the installed competence; transfer knowledge from the research sector to industry; embark on government action in strategic areas; enhance the assessment of existing programmes and commence innovative projects in areas of relevance for the Country. Furthermore, scientific dissemination plays a fundamental role in transforming the perception of the public at large of the importance of science in modern life. The government has undertaken to meet these challenges using institutional base and the operation of existing qualified scientists.\n\nA question that has been intriguing many historians studying China is the fact that China did not develop a scientific revolution and Chinese technology fell behind that of Europe. Many hypotheses have been proposed ranging from the cultural to the political and economic. has argued that China indeed had a scientific revolution in the 17th century and that we are still far from understanding the scientific revolutions of the West and China in all their political, economic and social ramifications. Some like John K. Fairbank are of the opinion that the Chinese political system was hostile to scientific progress.\n\nNeedham argued, and most scholars agreed, that cultural factors prevented these Chinese achievements from developing into what could be called \"science\". It was the religious and philosophical framework of the Chinese intellectuals which made them unable to believe in the ideas of laws of nature. More recent historians have questioned political and cultural explanations and have focused more on economic causes. Mark Elvin's high level equilibrium trap is one well-known example of this line of thought, as well as Kenneth Pomeranz' argument that resources from the New World made the crucial difference between European and Chinese development.\n\nThus, it was not that there was no order in nature for the Chinese, but rather that it was not an order ordained by a rational personal being, and hence there was no conviction that rational personal beings would be able to spell out in their lesser earthly languages the divine code of laws which he had decreed aforetime. The Taoists, indeed, would have scorned such an idea as being too naive for the subtlety and complexity of the universe as they intuited it. Similar grounds have been found for questioning much of the philosophy behind traditional Chinese medicine, which, derived mainly from Taoist philosophy, reflects the classical Chinese belief that individual human experiences express causative principles effective in the environment at all scales. Because its theory predates use of the scientific method, it has received various criticisms based on scientific thinking. Even though there are physically verifiable anatomical or histological bases for the existence of acupuncture points or meridians, for instance skin conductance measurements show increases at the predicted points.\n\nToday, science and technology establishment in the People's Republic of China is growing rapidly. Even as many Chinese scientists debate what institutional arrangements will be best for Chinese science, reforms of the Chinese Academy of Sciences continue. The average age of researchers at the Chinese Academy of Sciences has dropped by nearly ten years between 1991 and 2003. However, many of them are educated in the United States and other foreign countries.\n\nChinese university undergraduate and graduate enrollments more than doubled from 1995 to 2005. The universities now have more cited PRC papers than CAS in the Science Citation Index. Some Chinese scientists say CAS is still ahead on overall quality of scientific work but that lead will only last five to ten years.\n\nSeveral Chinese immigrants to the United States have also been awarded the Nobel Prize, including:, Samuel C. C. Ting, Chen Ning Yang, Tsung-Dao Lee, Yuan T. Lee, Daniel C. Tsui, and Gao Xingjian. Other overseas ethnic Chinese that have achieved success in sciences include Fields Medal recipient Shing-Tung Yau and Terence Tao, and Turing Award recipient Andrew Yao. Tsien Hsue-shen was a prominent scientist at NASA's Jet Propulsion Laboratory, while Chien-Shiung Wu contributed to the Manhattan Project (some argue she never received the Nobel Prize unlike her colleagues Tsung-Dao Lee and Chen Ning Yang due to sexism by the selection committee). Others include Charles K. Kao, a pioneer in fiber optics technology, and Dr. David Ho, one of the first scientists to propose that AIDS was caused by a virus, thus subsequently developing combination antiretroviral therapy to combat it. Dr. Ho was named TIME magazine's 1996 Man of the Year. In 2015, Tu Youyou, a pharmaceutical chemist, became the first native Chinese scientist, born and educated and carried out research exclusively in the People's Republic of China, to receive the Nobel Prize in natural sciences.\n\nThe earliest applications of science in India took place in the context of medicine, metallurgy, construction technology (such as ship building, manufacture of cement and paints) and in textile production and dyeing. But in the process of understanding chemical processes, led to some theories about physical processes and the forces of nature that are today studied as specific topics within the fields of chemistry and physics.\n\nMany mathematical concepts today were contributed by Indian mathematicians like Aryabhata.\n\nThere was really no place for scientists in the Indian caste system. Thus while there were/are castes for the learned brahmins, the warriors kshatriyas, the traders vaishyas and the menial workers shudras, maybe even the bureaucrats (the kayasths) there was/is hardly any formal place in the social hierarchy for a people who discover new knowledge or invent new devices based on the recently discovered knowledge, even though scientific temper has always been in India, in the form of logic, reasoning and method of acquiring knowledge. Its therefore no wonder that some Indians quickly learned to value science, especially those belonging to the privileged Brahmin caste during the British colonial rule that lasted over two centuries. Some Indians did succeed to achieve notable success and fame, examples include Satyendra Nath Bose, Meghnad Saha, Jagdish Chandra Bose and C. V. Raman even though they belonged to different castes. The science communication had begun with publication of a scientific journal, Asiatick Researches in 1788. Thereafter, the science communication in India has evolved in many facets. Following this, there has been a continuing development in the formation of scientific institutions and publication of scientific literature. Subsequently, scientific publications also started appearing in Indian languages by the end of eighteenth century. The publication of ancient scientific literature and textbooks at mass scale started in the beginning of nineteenth century. The scientific and technical terms, however, had been a great difficulty for a long time for popular science writing.\n\n\n"}
{"id": "244629", "url": "https://en.wikipedia.org/wiki?curid=244629", "title": "Scientific law", "text": "Scientific law\n\nThe laws of science, also called scientific laws or scientific principles, are statements that describe or predict a range of natural phenomena. Each scientific law is a statement based on repeated experimental observations that describes some aspect of the Universe. The term \"law\" has diverse usage in many cases (approximate, accurate, broad, or narrow theories) across all fields of natural science (physics, chemistry, biology, geology, astronomy, etc.). Scientific laws summarize and explain a large collection of facts determined by experiment, and are tested based on their ability to predict the results of future experiments. They are developed either from facts or through mathematics, and are strongly supported by empirical evidence. It is generally understood that they reflect causal relationships fundamental to reality, and are discovered rather than invented.\n\nLaws reflect scientific knowledge that experiments have repeatedly verified (and never falsified). Their accuracy does not change when new theories are worked out, but rather the scope of application, since the equation (if any) representing the law does not change. As with other scientific knowledge, they do not have absolute certainty (as mathematical theorems or identities do), and it is always possible for a law to be overturned by future observations. A law can usually be formulated as one or several statements or equations, so that it can be used to predict the outcome of an experiment, given the circumstances of the processes taking place.\n\nLaws differ from hypotheses and postulates, which are proposed during the scientific process before and during validation by experiment and observation. Hypotheses and postulates are not laws since they have not been verified to the same degree and may not be sufficiently general, although they may lead to the formulation of laws. A law is a more solidified and formal statement, distilled from repeated experiment. Laws are narrower in scope than scientific theories, which may contain one or several laws. Science distinguishes a law or theory from facts. Calling a law a fact is ambiguous, an overstatement, or an equivocation. Although the nature of a scientific law is a question in philosophy and although scientific laws describe nature mathematically, scientific laws are practical conclusions reached by the scientific method; they are intended to be neither laden with ontological commitments nor statements of logical absolutes.\n\nAccording to the unity of science thesis, \"all\" scientific laws follow fundamentally from physics. Laws which occur in other sciences ultimately follow from physical laws. Often, from mathematically fundamental viewpoints, universal constants emerge from a scientific law.\n\nA scientific law always applies under the same conditions, and implies that there is a causal relationship involving its elements. Factual and well-confirmed statements like \"Mercury is liquid at standard temperature and pressure\" are considered too specific to qualify as scientific laws. A central problem in the philosophy of science, going back to David Hume, is that of distinguishing causal relationships (such as those implied by laws) from principles that arise due to constant conjunction.\n\nLaws differ from scientific theories in that they do not posit a mechanism or explanation of phenomena: they are merely distillations of the results of repeated observation. As such, a law is limited in applicability to circumstances resembling those already observed, and may be found false when extrapolated. Ohm's law only applies to linear networks, Newton's law of universal gravitation only applies in weak gravitational fields, the early laws of aerodynamics such as Bernoulli's principle do not apply in case of compressible flow such as occurs in transonic and supersonic flight, Hooke's law only applies to strain below the elastic limit, etc. These laws remain useful, but only under the conditions where they apply.\n\nMany laws take mathematical forms, and thus can be stated as an equation; for example, the law of conservation of energy can be written as formula_1, where E is the total amount of energy in the universe. Similarly, the first law of thermodynamics can be written as formula_2.\n\nThe term \"scientific law\" is traditionally associated with the natural sciences, though the social sciences also contain laws. An example of a scientific law in social sciences is Zipf's law.\n\nLike theories and hypotheses, laws make predictions (specifically, they predict that new observations will conform to the law), and can be falsified if they are found in contradiction with new data.\n\nMost significant laws in science are conservation laws. These fundamental laws follow from homogeneity of space, time and phase, in other words \"symmetry\".\n\n\nConservation laws can be expressed using the general continuity equation (for a conserved quantity) can be written in differential form as:\n\nwhere ρ is some quantity per unit volume, J is the flux of that quantity (change in quantity per unit time per unit area). Intuitively, the divergence (denoted ∇•) of a vector field is a measure of flux diverging radially outwards from a point, so the negative is the amount piling up at a point, hence the rate of change of density in a region of space must be the amount of flux leaving or collecting in some region (see main article for details). In the table below, the fluxes, flows for various physical quantities in transport, and their associated continuity equations, are collected for comparison.\n\nMore general equations are the convection–diffusion equation and Boltzmann transport equation, which have their roots in the continuity equation.\n\nAll of classical mechanics, including Newton's laws, Lagrange's equations, Hamilton's equations, etc., can be derived from this very simple principle:\n\nwhere formula_5 is the action; the integral of the Lagrangian\n\nof the physical system between two times \"t\" and \"t\". The kinetic energy of the system is \"T\" (a function of the rate of change of the configuration of the system), and potential energy is \"V\" (a function of the configuration and its rate of change). The configuration of a system which has \"N\" degrees of freedom is defined by generalized coordinates q = (\"q\", \"q\", ... \"q\").\n\nThere are generalized momenta conjugate to these coordinates, p = (\"p\", \"p\", ..., \"p\"), where:\n\nThe action and Lagrangian both contain the dynamics of the system for all times. The term \"path\" simply refers to a curve traced out by the system in terms of the generalized coordinates in the configuration space, i.e. the curve q(\"t\"), parameterized by time (see also parametric equation for this concept).\n\nThe action is a \"functional\" rather than a \"function\", since it depends on the Lagrangian, and the Lagrangian depends on the path q(\"t\"), so the action depends on the \"entire\" \"shape\" of the path for all times (in the time interval from \"t\" to \"t\"). Between two instants of time, there are infinitely many paths, but one for which the action is stationary (to the first order) is the true path. The stationary value for the \"entire continuum\" of Lagrangian values corresponding to some path, \"not just one value\" of the Lagrangian, is required (in other words it is \"not\" as simple as \"differentiating a function and setting it to zero, then solving the equations to find the points of maxima and minima etc\", rather this idea is applied to the entire \"shape\" of the function, see calculus of variations for more details on this procedure).\n\nNotice \"L\" is \"not\" the total energy \"E\" of the system due to the difference, rather than the sum:\n\nThe following general approaches to classical mechanics are summarized below in the order of establishment. They are equivalent formulations, Newton's is very commonly used due to simplicity, but Hamilton's and Lagrange's equations are more general, and their range can extend into other branches of physics with suitable modifications.\n\nFrom the above, any equation of motion in classical mechanics can be derived.\n\n\n\n\nEquations describing fluid flow in various situations can be derived, using the above classical equations of motion and often conservation of mass, energy and momentum. Some elementary examples follow.\n\n\n\nPostulates of special relativity are not \"laws\" in themselves, but assumptions of their nature in terms of \"relative motion\".\n\nOften two are stated as \"the laws of physics are the same in all inertial frames\" and \"the speed of light is constant\". However the second is redundant, since the speed of light is predicted by Maxwell's equations. Essentially there is only one.\n\nThe said posulate leads to the Lorentz transformations – the transformation law between two frame of references moving relative to each other. For any 4-vector\n\nthis replaces the Galilean transformation law from classical mechanics. The Lorentz transformations reduce to the Galilean transformations for low velocities much less than the speed of light \"c\".\n\nThe magnitudes of 4-vectors are invariants - \"not\" \"conserved\", but the same for all inertial frames (i.e. every observer in an inertial frame will agree on the same value), in particular if \"A\" is the four-momentum, the magnitude can derive the famous invariant equation for mass-energy and momentum conservation (see invariant mass):\n\nin which the (more famous) mass-energy equivalence \"E\" = \"mc\" is a special case.\n\n\nGeneral relativity is governed by the Einstein field equations, which describe the curvature of space-time due to mass-energy equivalent to the gravitational field. Solving the equation for the geometry of space warped due to the mass distribution gives the metric tensor. Using the geodesic equation, the motion of masses falling along the geodesics can be calculated.\n\n\nIn a relatively flat spacetime due to weak gravitational fields, gravitational analogues of Maxwell's equations can be found; the GEM equations, to describe an analogous \"gravitomagnetic field\". They are well established by the theory, and experimental tests form ongoing research.\n\nThese equations can be modified to include magnetic monopoles, and are consistent with our observations of monopoles either existing or not existing; if they do not exist, the generalized equations reduce to the ones above, if they do, the equations become fully symmetric in electric and magnetic charges and currents. Indeed, there is a duality transformation where electric and magnetic charges can be \"rotated into one another\", and still satisfy Maxwell's equations.\n\n\nThese laws were found before the formulation of Maxwell's equations. They are not fundamental, since they can be derived from Maxwell's Equations. Coulomb's Law can be found from Gauss' Law (electrostatic form) and the Biot–Savart Law can be deduced from Ampere's Law (magnetostatic form). Lenz' Law and Faraday's Law can be incorporated into the Maxwell-Faraday equation. Nonetheless they are still very effective for simple calculations.\n\n\n\n\nClassically, optics is based on a variational principle: light travels from one point in space to another in the shortest time.\n\n\nIn geometric optics laws are based on approximations in Euclidean geometry (such as the paraxial approximation).\n\n\nIn physical optics, laws are based on physical properties of materials.\n\n\nIn actuality, optical properties of matter are significantly more complex and require quantum mechanics.\n\nQuantum mechanics has its roots in postulates. This leads to results which are not usually called \"laws\", but hold the same status, in that all of quantum mechanics follows from them.\n\nOne postulate that a particle (or a system of many particles) is described by a wavefunction, and this satisfies a quantum wave equation: namely the Schrödinger equation (which can be written as a non-relativistic wave equation, or a relativistic wave equation). Solving this wave equation predicts the time-evolution of the system's behaviour, analogous to solving Newton's laws in classical mechanics.\n\nOther postulates change the idea of physical observables; using quantum operators; some measurements can't be made at the same instant of time (Uncertainty principles), particles are fundamentally indistinguishable. Another postulate; the wavefunction collapse postulate, counters the usual idea of a measurement in science.\n\nApplying electromagnetism, thermodynamics, and quantum mechanics, to atoms and molecules, some laws of electromagnetic radiation and light are as follows.\n\n\nChemical laws are those laws of nature relevant to chemistry. Historically, observations led to many empirical laws, though now it is known that chemistry has its foundations in quantum mechanics.\n\n\nThe most fundamental concept in chemistry is the law of conservation of mass, which states that there is no detectable change in the quantity of matter during an ordinary chemical reaction. Modern physics shows that it is actually energy that is conserved, and that energy and mass are related; a concept which becomes important in nuclear chemistry. Conservation of energy leads to the important concepts of equilibrium, thermodynamics, and kinetics.\n\nAdditional laws of chemistry elaborate on the law of conservation of mass. Joseph Proust's law of definite composition says that pure chemicals are composed of elements in a definite formulation; we now know that the structural arrangement of these elements is also important.\n\nDalton's law of multiple proportions says that these chemicals will present themselves in proportions that are small whole numbers (i.e. 1:2 for Oxygen:Hydrogen ratio in water); although in many systems (notably biomacromolecules and minerals) the ratios tend to require large numbers, and are frequently represented as a fraction.\n\nMore modern laws of chemistry define the relationship between energy and its transformations.\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "13433031", "url": "https://en.wikipedia.org/wiki?curid=13433031", "title": "Scientific temper", "text": "Scientific temper\n\nThe Scientific temper is a way of life (defined in this context as an individual and social process of thinking and acting) which uses the scientific method and which may, consequently, include questioning, observing physical reality, testing, hypothesizing, analysing, and communicating (not necessarily in that order). \"Scientific temper\" describes an attitude which involves the application of logic. Discussion, argument and analysis are vital parts of scientific temper. Elements of fairness, equality and democracy are built into it. Jawaharlal Nehru was the first to use the phrase in 1946. He later gave a descriptive explanation:\nNehru wrote that the scientific temper goes beyond the domains to which science is conventionally understood to be limited to, and deals also with the consideration of ultimate purposes, beauty, goodness and truth. Nehru also contended that the scientific temper is the opposite of the method of religion, which relies on emotion and intuition and is (mis)applied \"to everything in life, even to those things which are capable of intellectual inquiry and observation.\" While religion tends to close the mind and produce \"intolerance, credulity and superstition, emotionalism and irrationalism\", and \"a temper of a dependent, unfree person\", a scientific temper \"is the temper of a free man\". He also indicated that the scientific temper goes beyond objectivity and fosters creativity and progress. He envisioned that the spread of scientific temper would be accompanied by a shrinking of the domain of religion, and \"the exciting adventure of fresh and never ceasing discoveries, of new panoramas opening out and new ways of living, adding to [life's] fullness and ever making it richer and more complete.\" Nehru states (that) \"It is science alone that can solve the problems of hunger and poverty, of insanitation and illiteracy, of superstition and deadening custom and tradition, of vast resources running to waste, of a rich country inhabited by starving people.\"\n\nThe genesis and development of the idea of the scientific temper is connected to ideas expressed earlier by Charles Darwin when he said, \"[F]reedom of thought is best promoted by the gradual illumination of men's minds, which follows from the advance of science,\" and by Karl Marx when he said, \"Religion is the sigh of the oppressed creature, the heart of a heartless world, and the soul of soulless conditions. It is the opium of the people. The abolition of religion as the illusory happiness of the people is the demand for their real happiness. To call on them to give up their illusions about their condition is to call on them to give up a condition that requires illusions.\"\n\n\"To develop scientific temper, humanism and the spirit of inquiry and reform\" is one of the fundamental duties of the people of the Republic of India, according to the Constitution of India.\n\nThe Government of India, through the National Council for Science and Technology Communication, dedicated the 28 February National Science Day of 2014 to the theme \"Fostering Scientific Temper\" to spread Nehru's vision.\n\nThe National Institute of Science Communication and Information Resources launched the scholarly serial \"Journal of Scientific Temper\" in 2013.\n"}
{"id": "22085930", "url": "https://en.wikipedia.org/wiki?curid=22085930", "title": "Szczeciński Park Naukowo-Technologiczny", "text": "Szczeciński Park Naukowo-Technologiczny\n\nSzczeciński Park Naukowo-Technologiczny - science park in the centre of Szczecin, in north-west Poland West Pomeranian Voivodeship. In the area: University of Szczecin, West Pomeranian University of Technology, Pomeranian Medical University, Maritime University of Szczecin, West Pomeranian Business School, Szczecin Shipyard, Zakłady Chemiczne Police SA (Police), ship (Szczecin-Świnoujście Harbour and Police Harbour), road and rail transport and Szczecin-Goleniów \"Solidarność\" Airport (Goleniów).\n\n"}
{"id": "34066587", "url": "https://en.wikipedia.org/wiki?curid=34066587", "title": "Technological transitions", "text": "Technological transitions\n\nTechnological innovations have occurred throughout history and rapidly increased over the modern age. New technologies are developed and co-exist with the old before supplanting them. Transport offers several examples; from sailing to steam ships to automobiles replacing horse-based transportation. Technological transitions (TT) describe how these technological innovations occur and are incorporated into society. Alongside the technological developments TT considers wider societal changes such as “user practices, regulation, industrial networks (supply, production, distribution), infrastructure, and symbolic meaning or culture”. For a technology to have use, it must be linked to social structures human agency and organisations to fulfil a specific need. Hughes refers to the ‘seamless web’ where physical artefacts, organisations, scientific communities, and social practices combine. A technological system includes technical and non-technical aspects, and it a major shift in the socio-technical configurations (involving at least one new technology) is when a technological transition occurs.\n\nWork on technological transitions draws on a number of fields including history of science, technology studies, and evolutionary economics. The focus of evolutionary economics is on economic change, but as a driver of this technological change has been considered in the literature. Joseph Schumpeter, in his classic \"Theory of Economic Development\" placed the emphasis on non-economic forces as the driver for growth. The human actor, the entrepreneur is seen as the cause of economic development which occurs as a cyclical process. Schumpeter proposed that radical innovations were the catalyst for Kondratiev cycles.\n\nThe Russian economist Kondratiev proposed that economic growth operated in boom and bust cycles of approximately 50 year periods. These cycles were characterised by periods of expansion, stagnation and recession. The period of expansion is associated with the introduction of a new technology, e.g. steam power or the microprocessor. At the time of publication, Kondratiev had considered that two cycles had occurred in the nineteenth century and third was beginning at the turn of the twentieth. Modern writers, such as Freeman and Perez outlined five cycles in the modern age:\n\n\nFreeman and Perez proposed that each cycle consists of pervasive technologies, their production and economic structures that support them. Termed ‘techno-economic paradigms’, they suggest that the shift from one paradigm to another is the result of emergent new technologies. \n\nFollowing the recent economic crisis, authors such as Moody and Nogrady have suggested that a new cycle is emerging from the old, centred on the use of sustainable technologies in a resource depleted world.\n\nThomas Kuhn described how a paradigm shift is a wholesale shift in the basic understanding of a scientific theory. Examples in science include the change of thought from miasma to germ theory as a cause of disease. Building on this work, Giovanni Dosi developed the concept of ’technical paradigms’ and ‘technological trajectories’. In considering how engineers work, the technical paradigm is an outlook on the technological problem, a definition of what the problems and solutions are. It charts the idea of specific progress. By identifying the problems to be solved the paradigm exerts an influence on technological change. The pattern of problem solving activity and the direction of progress is the technological trajectory. In similar fashion, Nelson and Winter (,)defined the concept of the ‘technological regime’ which directs technological change through the beliefs of engineers of what problems to solve. The work of the actors and organisations is the result of organisational and cognitive routines which determines search behaviour. This places boundaries and also trajectories (direction) to those boundaries.\n\nIn analysing (historic) cases of technological transitions researchers from the systems in transition branch of transitions research have used a multi-level perspective (MLP) as a heuristic model to understand changes in socio-technical systems. () Innovation system approaches traditionally focus on the production side. A socio-technical approach combines the science and technology in devising a production, with the application of the technology in fulfilling a societal function. Linking the two domains are the distribution, infrastructure and markets of the product. This approach considers a transition to be multi-dimensional as technology is only one aspect. \n\nThe MLP proposes three analytical levels: the niche, regime and landscape. \n\nNiche (Micro-level)\nRadical innovations occur at the niche level. These act as ‘safe havens’ for fledgling technologies to develop, largely free from market pressures which occur at the regime level. The US Military has acted as niche for major twentieth century technologies such as the aircraft, radio and the internet. More recently, California’s Silicon Valley has provided an arena for ICT focused technologies to emerge. Some innovations will challenge the existing regime while others fail. \n\nRegime (Meso-level)\nThe socio-technical regime, as defined by Geels, includes a web of inter-linking actors across different social groups and communities following a set of rules. In effect, the established practices of a given system. Seven dimensions have been identified in the socio-technical regime: technology, user practices and application, the symbolic meaning of technology, infrastructure, policy and techno-scientific knowledge. Change does occur at the regime level but it is normally slow and incremental unlike the radical change at the niche level. The actors who constitute the existing regime are set to gain from perpetuating the incumbent technology at the expense of the new. This is known as ‘lock-in’.\n\nLandscape (Macro-level)\nExogenous to the previous levels is the socio-technical landscape. A broad range of factors are contained here, such as economic pressures, cultural values, social trends, wars and environmental issues. Change occurs at an even slower rate than at the regime level. \n\nA transition is said to happen when a regime shift has occurred. This is the result of the interplay between the three levels. Regimes are relatively inert and resistant to change being structured to incremental innovation following established trajectories. As such, transitions are difficult to achieve. The current regime is typically suffering internal issues. Pressure from the landscape level may cause ‘cracks’ or ‘windows of opportunity’ through which innovations at the niche level may initially co-exist with the established technology before achieving ascendency. Once the technology has fully embedded into society the transition is said to be completed.\n\nThe MLP has been used in describing a range of historic transitions in socio-technical regimes for mobility, sanitation, food, lighting and so on. While early research focused on historical transitions, a second strand of research was more focused on transitions to sustainable technologies in key sectors such as transport, energy and housing.\n\nGeels presented three historical transitions on system innovation relating to modes of transportation. The technological transition from sailing ships to steamships in the UK will be summarised and shown in the context of a wider system innovation. \n\nGreat Britain was the world’s leading naval power in the nineteenth century, and led the way in the transition from sail to steam. At first, the introduction of steam technology co-existed with the current regime. Steam tugs assisted sail ships into port and hybrid steam / sail ships appeared. Landscape developments create the necessity for improvements in the technology. A demand for trans-Atlantic emigration was prompted by the Irish potato famine, European political instability and the lure of gold in California. The requirement for such arduous journeys had prompted a wealth of innovations at the niche level in steamship-development. From the late 1880s, as steamship technology improved and costs dropped, the new technology was widely diffused and a new regime established. The changes go beyond a technological transition as it involved new ship management and fleet management practices, new supporting infrastructures and new functionalities.\n\nThe nature of transitions varies and the differing qualities result in multiple pathways occurring. Geels and Schot defined five transition paths:\n\n\nSix characteristics of technological transitions have been identified.,\n\n\"Transitions are co-evolutionary and multi-dimensional\"\nTechnological developments occur intertwined with societal needs, wants and uses. A technology is adopted and diffused based on this interplay between innovation and societal requirements. Co-evolution has different aspects. As well as the co-evolution of technology and society, aspects between science, technology, users and culture have been considered.\n\"Multi-actors are involved\"\nScientific and engineering communities are central to the development of a technology, but a wide range of actors are involved in a transition. This can include organisations, policy-makers, government, NGOs, special interest groups and others.\n\n\"Transitions occur at multiple levels\"\nAs shown in the MLP transitions occur through the interplay of processes at different levels. \n\n\"Transitions are a long-term process\"\nComplete system-change takes time and can be decades in the making. Case studies show them to be between 40 and 90 years.\n\n\"Transitions are radical\"\nFor a true transition to occur the technology has to be a radical innovation. \n\n\"Change is Non-linear\"\nThe rate of change will vary over time. For example, the pace of change may be slow at the gestation period (at the niche level) but much more rapid when a breakthrough is occurring.\n\nDiffusion of an innovation is the concept of how it is picked up by society, at what rate and why. Everett (1962).The diffusion of a technological innovation into society can be considered in distinct phases. Pre-development is the gestation period where the new technology has yet to make an impact. Take-off is when the process of a system shift is beginning. A breakthrough is occurring when fundamental changes are occurring in existing structures through the interplay of economic, social and cultural forces. Once the rate of change has decreased and a new balance is achieved, stabilization is said to have occurred. A full transition involves an overhaul of existing rules and change of beliefs which takes time, typically spanning at least a generation. This process can be speeded-up through seismic, unforeseen events such as war or economic strife. \n\nGeels proposed a similar four phased approach which draws on the multi-level perspective (MLP) developed by Dutch scholars. Phases one sees the emergence of a novelty, born from the existing regime. Development then occurs in the niche level at phase two. As before, breakthrough then occurs at phase three. In the parlance of the MLP the new technology, having been developed at the niche level, is in competition with the established regime. To breakthrough and achieve wide diffusion, external factors – ‘windows of opportunity’ are required.\n\nA number of possible circumstances can act as windows of opportunity for the diffusion of new technologies: \n\n\nAlongside external influences, internal drivers catalyse diffusion. These include economic factors such as the price performance ration. Socio-technical perspectives focus on the links between disparate social and technological elements. Following the breakthrough, the final phases see the new technology supersede the old.\n\nThe study of technological transitions has an impact beyond academic interest. The transitions referred to in the literature may relate to historic processes, such as the transportation transitions studied by Geels, but system changes are required to achieve a safe transition to a low carbon-economy. (). Current structural problems are apparent in a range of sectors. Dependency on oil is problematic in the energy sector due to availability, access and contribution to greenhouse gas (GHG) emissions. Transportation is a major user of energy causing significant emission of GHGs. Food production will need to keep pace with an ever-growing world population while overcoming challenges presented by global warming and transportation issues. Incremental change has provided some improvements but a more radical transition is required to achieve a more sustainable future. \n\nDeveloped from the work on technological transitions is the field of transition management. Within this is an attempt to shape the direction of change complex socio-technical systems to more sustainable patterns. Whereas work on technological transitions is largely based on historic processes, proponents of transition management seek to actively steer transitions in progress.\n\nGenus and Coles outlined a number of criticisms against the analysis of technological transitions, in particular when using the MLP. Empirical research on technological transitions occurring now has been limited, with the focus on historic transitions. Depending on the perspective on transition case studies they could be presented as having occurred on a different transition path to what was shown. For example, the bicycle could be considered an intermediate transport technology between the horse and the car. Judged from shorter different time-frame this could appear a transition in its own right. Determining the nature of a transition is problematic; when it started and ended, or whether one occurred in the sense of a radical innovation displacing an existing socio-technical regime. The perception of time casts doubt on whether a transition has occurred. If viewed over a long enough period even inert regimes may demonstrate radical change in the end. The MLP has also been criticised by scholars studying sustainability transitions using Social Practice Theories.\n\n"}
{"id": "5996838", "url": "https://en.wikipedia.org/wiki?curid=5996838", "title": "The Story of Science in America", "text": "The Story of Science in America\n\nThe Story of Science in America is a 1967 science book by L. Sprague de Camp and Catherine Crook de Camp, illustrated by Leonard Everett Fisher, published by Charles Scribner's Sons. It has been translated into Spanish, Portuguese, Burmese and French.\n\nThe book traces the work of inventors and naturalists in the United States from the Colonial era through the mid-19th century, and relates scientific developments in the century following.\n\n\nCritical response to the book was positive. Jane E. Brody, writing for \"The New York Times\", called it \"a fast-moving, informative and thoroughly enjoyable chronicle, with amusing anecdotes, legends and interesting sidelights that reflect the personalities, lives and times of the men who shaped our nation scientifically.\" She noted that \"the authors have kept their writing free of chauvinism,\" and that \"[m]ost of the scientific concepts are well enough explained so that even the newcomer to science should be able to grasp at least the essence of them.\" In the same issue the book was included among seventy-five recommended titles selected by the Children's Editor of the newspaper's Book Review, described as an \"[i]nformative, thoroughly enjoyable chronicle of the development of science in our country.\"\n\n\"Publishers' Weekly\" stated that \"[t]o read the index ... is to read the names of the men and of their discoveries in science in America, from the earliest days ... to the space age. To read the book is to become familiar with the men and their contributions to science.\"\n\nGeorge Basalia, writing for \"Library Journal\", called the book \"a first-rate history of American science and technology for high-school students ... cover[ing] major American technical discoveries as well as our contributions to the purely theoretical aspects of science.\" He found \"much to be praised ... the book is intelligently conceived, carefully organized, clearly written, and handsomely designed. Unfortunately, the illustrations do not do justice to [the] excellent text.\"\n\nH. D. Allen in the \"Montreal Gazette\" wrote that the book's story \"makes fascinating reading,\" and that \"[w]hile the treatment of any one discipline may at first seem superficial and chatty, the total impact is most impressive, for the reader is left with an acquaintance with the leading figures of the age of science and some appreciation of how the contribution of each influenced a way of life.\" He concluded \"The breadth of scientific knowledge which this book represents is remarkable, as is the skill with which it has been set down and the effortlessness with which it reads.\"\n\n\"The Booklist\" called it \"[a] wide-ranging survey [that] reflects the authors' humanistic interests as well as their familiarity with several branches of science and their extensive background reading.\"\n\nHarry C. Stubbs in \"The Horn Book Magazine\" included it among \"half a dozen books dealing ... with the history of science [that] I can recommend [both] to nonscientists as guides toward the Light [and] to scientists and science teachers as reminders that what we know was long, slow, and hard in coming.\" He noted that it \"give[s] us a series of fascinating biographical and anecdotal items strung loosely on the thread of developing scientific knowledge.\"\n\nPhilip and Phylis Morrison in \"Scientific American\" felt it \"manages to convey a sense of coherence, even though it deals at staccato length with so many men, trends and ideas ... The reason is partly in the expert writing--smooth, unusually candid, cheerful and sometimes a bit condescending (as in the two or three pages about Veblen).\" They add that \"[n]ot all the dicta of the authors seem reasonable, but to find any personal judgment at work is so rare in this kind of pedagogy that one is pleased by the De Camps even when one disagrees with them.\"\n"}
{"id": "46846841", "url": "https://en.wikipedia.org/wiki?curid=46846841", "title": "What If?: Serious Scientific Answers to Absurd Hypothetical Questions", "text": "What If?: Serious Scientific Answers to Absurd Hypothetical Questions\n\nWhat If?: Serious Scientific Answers to Absurd Hypothetical Questions is a non-fiction book by Randall Munroe in which the author answers hypothetical science questions sent to him by readers of his webcomic, \"xkcd\". The book contains a selection of questions and answers originally published on his blog \"What If?\", along with several new ones. The book is divided into several dozen chapters, most of which are devoted to answering a unique question. \"What If?\" was released in September 2, 2014 and was received positively by critics.\n\nIn the introduction section of the book, Randall Munroe recounts wondering as a child whether \"there were more hard things or soft things in the world\", concluding that \"the world contained about three billion soft things and five billion hard things\". The conversation that was produced by this question impressed Munroe's mother to such a degree that she wrote it down. Though Munroe later stated that his question was rather meaningless, he used it as an example of how \"thoroughly answer[ing] a stupid question can take you to some pretty interesting places\".\n\nSince 2012, Munroe has been answering unusual questions sent in by readers of \"xkcd\" on his blog \"What If?\". The concept was inspired by a weekend program organized by the Massachusetts Institute of Technology in which volunteers can teach classes to groups of high school students on any chosen subject. Munroe signed up after hearing about it from a friend and decided to teach a class on energy. Though the lecture felt \"dry\" at first, once Munroe started bringing up examples from \"Star Wars\" and \"The Lord of the Rings\", the students became more excited. The entire second half of the class was eventually spent solving mathematical and physics problems. Munroe wrote the first entries a few years before the start of the blog, based on questions he was asked that day.\n\nBecause he was delayed in getting the website online, Munroe had a lot of time thinking about the design of the blog. He eventually chose to display his entries as individual pages rather than using an infinite scrolling page, as he considers the latter more difficult to digest. Munroe usually chooses questions he already knows something interesting about, or after reading a scientific paper, he keeps an eye out for questions in which he can bring it up. Munroe has said that the volume of questions has been high enough that it is impossible to read all of them. Answering a question and writing a post takes him about a day of solid work.\n\nMunroe announced in March, 2014 that he had signed a deal with publisher Houghton Mifflin Harcourt to compile a large number of his \"What If?\" entries into a book. \"What If?: Serious Scientific Answers to Absurd Hypothetical Questions\" would eventually be released in September that year.\n\nThe \"What If?\" book contains a selection of questions and answers from the original blog, as well as nineteen new ones. Furthermore, Munroe selected a few unanswered questions from his inbox and collected those in separate sections in the book. Alt text, which was commonly used for illustrations on the original blog, was omitted from the book in many cases, though is sometimes included as small captions underneath the images. Instead, Munroe has added footnotes to the essays in the book to inform or entertain the reader. The cover of the book depicts a Tyrannosaurus rex being lowered into a Sarlacc from \"Star Wars\", a topic not covered in the book.\n\n\"What If?\" is Munroe's second published book, his first being \"XKCD: Volume 0\", a curated collection of \"xkcd\" comics released in 2009. Munroe released a third book, titled \"Thing Explainer\", in 2015.\n\n\"What If?\" is mainly composed of answers Munroe gives to readers' hypothetical questions on various scientific topics. The questions tend to be rather unusual, assuming an improbable scenario and inquiring a logical conclusion to the situation. The first question Munroe answered for the blog was the following:\n\nUsing mathematics and physics, Munroe concluded such a situation would result in a large explosion. \"What If?\" approaches its subject matter with a sense of wit and sometimes makes use of approximations to answer questions that seem impossible to solve. Most questions demand assumptions and cross-disciplinary science skills to answer, resulting in \"back-of-the-envelope\" calculations. \"What If?\" is interspersed with \"charmingly-amateur\" stick figure illustrations.\n\nThe book also features periodic sections titled \"Weird (and Worrying) Questions from the What If? Inbox\", which are short collections of questions Munroe had not answered because he did not \"want to think about that\". In an interview, Munroe stated that he \"never got past the initial mental image\" of the question \"How cold would your teeth have to get in order for a cup of hot coffee to make them shatter on contact?\"\n\nThe book was received positively by critics. Ethan Gilsdorf of the \"Boston Globe\" stated that \"it’s fun to watch as Munroe tackles each question and examines every possible complication.\" According to Gilsdorf, \"What If?\" gives a view into \"Munroe’s playful yet existentially-tinged worldview\" by contrasting cataclysmic scenarios with more heady ideas, such as examining the effects of a magnitude minus-7 Richter scale earthquake. The \"Huffington Post\" remarked that \"What makes Munroe's work so fantastic is a combination of two elements: his commitment to trying to answer even the weirdest question with solid science, and his undeniable sense of humor.\" Rhett Allain of \"Wired\" praised \"What If?\" because even his 12-year-old son was able to enjoy it, though he found a minor error in one of the sections.\n\nSam Hewitt of \"Varsity\" and Marla Desat of \"The Escapist\" noted that the first print run had some issues processing mathematical symbols, as a square box was displayed where a delta is supposed to be printed.\n\n\"What If?\" was released on September 2, 2014 and reached the top of the \"New York Times\" bestsellers list on September 21, while getting featured as the \"Amazon Best Book of the Month\". The book was a Goodreads Choice Awards \"Best Nonfiction\" nominee in 2014 as well.\n"}
{"id": "5911169", "url": "https://en.wikipedia.org/wiki?curid=5911169", "title": "You Can with Beakman and Jax", "text": "You Can with Beakman and Jax\n\nYou Can with Beakman and Jax, also known in its Spanish-language version as \"El Mundo de Beakman\" (\"The World of Beakman\"), is an American science and education syndicated comic strip by Jok Church, which began on July 14, 1991. The educational comicstrip, and planned television series, were originally intended to be a part of the \"Star Wars\" franchise, with real world facts about science, and languages.\n\nThe comic strip is a text-based comic, that answers readers' questions, with illustrations of the main characters, various objects, and, or the experiments being discussed. It is run as a single panel comic that appears in newspapers as a color, or black and white Sunday feature, in either a 1/4 page strip, or 1/2 tab format. The comic has reached a readership of 52 million readers, in 13 countries. About 80 percent of the letters it receives are from females. From its comic origins, its lead character Beakman would later star in his own live action television series, \"Beakman's World\". The comic also branched out into other media, gaining numerous awards along the way. The comic continued for several months after its author died of a heart attack on April 29, 2016. Jok's final remaining comic was published on July 17, 2016, just three days after its 25th anniversary of publication.\n\nThe comic strip was originally named \"You Can with Beakman\" (also called \"U Can with Beakman\". Its only main character at the time was Beakman Place, a male figure with spiky blue hair, glasses, a neck tie, and a breast pocket full of instruments. Beakman is a non-scientist that learns about the world through books, and then finds ways to prove what he's read about. He was named after Beekman Place, a small street on the east side of Manhattan, New York City. The comic is in a question-and-answer format, in which a reader asks a question, addressed to either Beakman, or also later, his sister Jax Place, a red head, with her hair curled up behind her head in blue circular bands, she wears glasses, and jacks in her hair, and as earrings. Church provides the answer, usually by means of a simple experiment the children reading can do (often with parental assistance or supervision). A paragraph after the results of the experiment, in inverted text at the bottom of the comic, would explain the answer.\n\nThe idea for doing a comic strip came to Jok while he was working for Lucasfilm, and answering questions from George Lucas' fan mail, stating that he was “overcome by the bravery children showed by asking Mr. Lucas anything at all\" and he \"decided to write about real questions from real kids\". While working at Lucasfilm, Jok began working on a project called \"Here's How\" a comicstrip and educational television series featuring C-3PO teaching foreign language and R2-D2 explaining the more physical world\", the idea was eventually shelved, but the concept later evolved into Jok's Comicstrip featuring a character named Beakman. He would receive these science questions from children, and he would choose to answer them based on subjects that he didn't know about, and wanted to learn. The comic was written for an audience that includes children, but not exclusively children. He felt his purpose in making the comics was \"to make sure my readers are not intimidated by the world through which they walk\". He would then research the subject, write, draw, and color the comics by using a Macintosh computer. This process gained him criticism in May 1994 when he explained how to do an experiment separating hydrogen and oxygen from water, through electrolysis using a single jar, and a nine-volt battery, for which he defended the comic strip, by explaining the small amount of gas that would be produced in this way would not be overly dangerous. This was reported to be the first time an experiment's safety was questioned.\n\nBesides answering questions from children, Jok also took questions from adults. One such question came from the Canadian Prime Minister, Jean Chrétien, who asked about why golf balls had little dents. Jok later explained that he \"has world leaders periodically contribute to his ... feature.\" Within the comic strip Jok also introduced an annual \"Beakman and Jax Make Up Your Own Rules Contest\", in which the reader could report on an experiment or research they did. There were up to 100 winners from around the world, and the prizes were such things as a free telescopes and copies of the Beakman & Jax books.\n\nThe comic first appeared in the Marin Independent Journal, and was offered to them for free. The earlier comic strips were then reprinted in three \"Science Stuff You Can Do\" books, a \"Best of\", and was the bases for two specialty books, \"Beakman & Jax's Bubble Book\" and \"Beakman & Jax's Microscope Book\". Shortly after the release of the first book June 1, 1992, on September 18, 1992 an Emmy Award winning television series named Beakman's World began, starring Paul Zaloom as the show's main character, along with three female lab assistants over the years, Lester the Rat, and two puppet penguins. According to Jok the television series was \"written to build a bridge between children and the adult members of their family,\" and \"we created the show to be like a live action cartoon.\" Beakman's sister Jax however, was not included in the television series, which Jok referred to as \"my one disappointment with the show.\" Although his sister wasn't present in the television show, two of Beakman's other relatives did appear on the television show, his mother Beakmom, and his brother Meekman.\n\nIn 1995 an official website opened for the strip published by the \"North Bay Network\", it won many awards. It later moved to its current location in 1996, published by Network Solutions. Where it has received several positive reviews from such internet guides as \"The parents' pocket guide to kids & computers\" by Family Computer Workshop, which gave the site 5 out of 5 stars and recommended it for readers 7–13. At the time the site contained questions and answers, as well as hands-on activities, some of which required Netscape and Shockwave.\n\nThe final strip was published on July 17, 2016, three days after its 25th anniversary and three months after Jok Church died.\n"}
