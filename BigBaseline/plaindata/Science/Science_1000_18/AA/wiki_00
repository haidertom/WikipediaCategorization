{"id": "51238945", "url": "https://en.wikipedia.org/wiki?curid=51238945", "title": "Advancing Secondary Science Education thru Tetrahymena", "text": "Advancing Secondary Science Education thru Tetrahymena\n\nAdvancing Secondary Science Education thru Tetrahymena (ASSET) is an organization at Cornell University that is dedicated to expanding the use of the protist \"Tetrahymena\" in K-12 classrooms. They are funded by the National Institutes of Health through the SEPA (Science Education Partnership Award) Program. Although their name includes the word \"secondary,\" they have worked in recent years to develop materials for students in elementary, middle and high schools. The group develops modules, which are stand-alone labs or lessons that can be inserted into the curriculum of a class at the discretion of the teacher.\n\nModules are designed to be stand-alone lessons that fit into and compliment a life science curriculum. The ASSET program ships all the equipment that is needed to complete the modules to the teacher in a reusable plastic container, at ASSET's expense. The teacher who requested the materials can use them for up to two weeks. At the end of the two weeks, the teacher uses a pre-paid return label to send the materials back in the same container. Some materials, such as live cells, may be sent separately to provide for a chance for the culture to be established in the teacher's classroom.\n\nASSET has created fifteen science modules, each of which addresses a particular topic relevant to life science education in United States school curricula.\n\nThis module utilizes two species of \"Tetrahymena\": \"Tetrahymena thermophilia\" and \"Tetrahymena vorax\". In the lab, an extract, called stomatin, is made from the thermophilia, then placed into the vorax culture. There, it induces a transformation from the microstome form to the macrostome form in T. vorax. This transformation is most notable by a marked increase in the size of the cell (doubling or sometimes more), the resorption of the microstomal oral apparatus and the construction of a much larger macrostomal oral apparatus. This transformation allows the macrostomal T. vorax cells to prey on T. thermophilia, but also to cannibalize the microstomes of their own species.\n\nASSET has also created five science and society modules, which are designed to integrate social studies and science education into one unit.\n\n\n"}
{"id": "34663672", "url": "https://en.wikipedia.org/wiki?curid=34663672", "title": "Annals of Philosophy", "text": "Annals of Philosophy\n\nAnnals of Philosophy was a learned journal founded in 1813 by the Scottish chemist Thomas Thomson. It shortly became a leader in its field of commercial scientific periodicals. Contributors included John George Children, Edward Daniel Clarke, Philip Crampton, Alexander Crichton, James Cumming, John Herapath, William George Horner, Thomas Dick Lauder, John Miers, Matthew Paul Moyle, Robert Porrett, James Thomson, and Charles Wheatstone.\n\nThomson edited it until 1821, when he was succeeded in 1821 by Richard Phillips. The journal was bought by Richard Taylor in 1827, and closed down for the benefit of the \"Philosophical Magazine\".\n\nThe \"Annals of Philosophy\" were issued monthly following a standard pattern. Often the first article was a biographical article (10 pages) on a living or recently deceased scientist. This was then followed by a series of extended pieces (5-10 pages) on particular topics, sometimes by eminent authors. Then there were shorter news items and correspondence. Summaries followed: first of the proceedings of learned bodies (Royal Society, Linnean, French Institute -if available: the Napoleonic Wars made communications with the continent difficult at first, etc.), then of patents, and finally of new books. The last section was a meteorological journal. Every six months a title page, index, and preface were issued which could be bound before the six monthly issues to make a half-yearly volume. Including front matter, volumes were just under 500 pages each.\n\n\n"}
{"id": "419386", "url": "https://en.wikipedia.org/wiki?curid=419386", "title": "Applied science", "text": "Applied science\n\nApplied science is the application of existing scientific knowledge to practical applications, like technology or inventions.\n\nWithin natural science, disciplines that are basic science, also called pure science, develop basic \"information\" to \npredict and perhaps explain and understand phenomena in the natural world. Applied science is the use of scientific processes and knowledge as the means to achieve a particular practical or useful result. This includes a broad range of applied science related fields from engineering, business, medicine to early childhood education.\n\nApplied science can also apply formal science, such as statistics and probability theory, as in epidemiology. Genetic epidemiology is an applied science applying both biological and statistical methods.\n\nApplied research is the practical application of science. It accesses and uses accumulated theories, knowledge, methods, and techniques, for a specific, state-, business-, or client-driven purpose. Applied research is contrasted with pure research (basic research) in discussion about research ideals, methodologies, programs, and projects.\nApplied research deals with solving practical problems and generally employs empirical methodologies. Because applied research resides in the messy real world, strict research protocols may need to be relaxed. For example, it may be impossible to use a random sample. Thus, transparency in the methodology is crucial. Implications for interpretation of results brought about by relaxing an otherwise strict canon of methodology should also be considered. Since applied research has a provisional close-to-the-problem and close-to-the-data orientation, it may also use a more provisional conceptual framework such as working hypotheses or pillar questions. \nThe OECD's Frascati Manual describes applied research as one of the three forms of research, along with basic research & experimental development.\n\nDue to its practical focus, applied research information will be found in the literature associated with individual disciplines.\n\nEngineering fields include thermodynamics, heat transfer, fluid mechanics, statics, dynamics, mechanics of materials, kinematics, electromagnetism, materials science, earth sciences, engineering physics.\n\nMedical sciences, for instance medical microbiology and clinical virology, are applied sciences that apply biology toward medical knowledge and inventions, but not necessarily medical technology, whose development is more specifically biomedicine or biomedical engineering.\n\nIn Canada, the Netherlands and other places the Bachelor of Applied Science (BASc) is equivalent to the Bachelor of Engineering, and is classified as a professional degree. The BASc tends to focus more on the application of the engineering sciences. In Australia and New Zealand this degree is awarded in various fields of study and is considered a highly specialized professional degree.\n\nIn the United Kingdom's educational system, Applied Science refers to a suite of \"vocational\" science qualifications that run alongside \"traditional\" General Certificate of Secondary Education or A-Level Sciences. Applied Science courses generally contain more coursework (also known as portfolio or internally assessed work) compared to their traditional counterparts. These are an evolution of the GNVQ qualifications that were offered up to 2005.\nThese courses regularly come under scrutiny and are due for review following the Wolf Report 2011;\nhowever, their merits are argued elsewhere.\n\nIn the United States, The College of William & Mary offers an undergraduate minor as well as Master of Science and Doctor of Philosophy degrees in \"applied science.\" Courses and research cover varied fields including neuroscience, optics, materials science and engineering, nondestructive testing, and nuclear magnetic resonance. In New York City, the Bloomberg administration awarded the consortium of Cornell-Technion $100 million in City capital to construct the universities' proposed Applied Sciences campus on Roosevelt Island.\n"}
{"id": "445231", "url": "https://en.wikipedia.org/wiki?curid=445231", "title": "Biodynamic agriculture", "text": "Biodynamic agriculture\n\nBiodynamic agriculture is a form of alternative agriculture very similar to organic farming, but it includes various esoteric concepts drawn from the ideas of Rudolf Steiner (1861–1925). Initially developed in 1924, it was the first of the organic agriculture movements. It treats soil fertility, plant growth, and livestock care as ecologically interrelated tasks, emphasizing spiritual and mystical perspectives.\n\nBiodynamics has much in common with other organic approaches – it emphasizes the use of manures and composts and excludes the use of artificial chemicals on soil and plants. Methods unique to the biodynamic approach include its treatment of animals, crops, and soil as a single system, an emphasis from its beginnings on local production and distribution systems, its use of traditional and development of new local breeds and varieties. Some methods use an astrological sowing and planting calendar. Biodynamic agriculture uses various herbal and mineral additives for compost additives and field sprays; these are sometimes prepared by controversial methods, such as burying ground quartz stuffed into the horn of a cow, which are said to harvest \"cosmic forces in the soil\", that are more akin to sympathetic magic than agronomy.\n\nAs of 2016 biodynamic techniques were used on 161,074 hectares in 60 countries. Germany accounts for 45% of the global total; the remainder average 1750 ha per country. Biodynamic methods of cultivating grapevines have been taken up by several notable vineyards. There are certification agencies for biodynamic products, most of which are members of the international biodynamics standards group Demeter International.\n\nNo difference in beneficial outcomes has been scientifically established between certified biodynamic agricultural techniques and similar organic and integrated farming practices. Biodynamic agriculture lacks strong scientific evidence for its efficacy and has been labeled a pseudoscience because of its overreliance upon esoteric knowledge and mystical beliefs.\n\nBiodynamics was the first modern organic agriculture. Its development began in 1924 with a series of eight lectures on agriculture given by philosopher Rudolf Steiner at Schloss Koberwitz in Silesia, Germany (now Kobierzyce in Poland). These lectures, the first known presentation of organic agriculture, were held in response to a request by farmers who noticed degraded soil conditions and a deterioration in the health and quality of crops and livestock resulting from the use of chemical fertilizers. The 111 attendees, less than half of whom were farmers, came from six countries, primarily Germany and Poland. The lectures were published in November 1924; the first English translation appeared in 1928 as \"The Agriculture Course\".\n\nSteiner emphasized that the methods he proposed should be tested experimentally. For this purpose, Steiner established a research group, the \"Agricultural Experimental Circle of Anthroposophical Farmers and Gardeners of the General Anthroposophical Society\". Between 1924 and 1939, this research group attracted about 800 members from around the world, including Europe, the Americas and Australasia. Another group, the \"Association for Research in Anthroposophical Agriculture\" (\"Versuchsring anthroposophischer Landwirte\"), directed by the German agronomist Erhard Bartsch, was formed to test the effects of biodynamic methods on the life and health of soil, plants and animals; the group published a monthly journal, \"Demeter\". Bartsch was also instrumental in developing a sales organisation for biodynamic products, Demeter, which still exists today. The Research Association was renamed the Imperial Association for Biodynamic Agriculture (\"Reichsverband für biologisch-dynamische Wirtschaftsweise\") in 1933. It was dissolved by the National Socialist regime in 1941. In 1931 the association had 250 members in Germany, 109 in Switzerland, 104 in other European countries and 24 outside Europe. The oldest biodynamic farms are the Wurzerhof in Austria and Marienhöhe in Germany.\n\nIn 1938, Ehrenfried Pfeiffer's text, \"Bio-Dynamic Farming and Gardening\", was published in five languages – English, Dutch, Italian, French, and German; this became the standard work in the field for several decades. In July 1939, at the invitation of Walter James, 4th Baron Northbourne, Pfeiffer travelled to the UK and presented the Betteshanger Summer School and Conference on Biodynamic Farming at Northbourne's farm in Kent. The conference has been described as the 'missing link' between biodynamic agriculture and organic farming because, in the year after Betteshanger, Northbourne published his manifesto of organic farming, \"Look to the Land\", in which he coined the term 'organic farming' and praised the methods of Rudolf Steiner. In the 1950s, Hans Mueller was encouraged by Steiner's work to create the organic-biological farming method in Switzerland; this later developed to become the largest certifier of organic products in Europe, \"Bioland\".\n\nToday biodynamics is practiced in more than 50 countries worldwide and in a variety of circumstances, ranging from temperate arable farming, viticulture in France, cotton production in Egypt, to silkworm breeding in China. Demeter International is the primary certification agency for farms and gardens using the methods.\n\n\nIn common with other forms of organic agriculture, biodynamic agriculture uses management practices that are intended to \"restore, maintain and enhance ecological harmony.\" Central features include crop diversification, the avoidance of chemical soil treatments and off-farm inputs generally, decentralized production and distribution, and the consideration of celestial and terrestrial influences on biological organisms. The Demeter Association recommends that \"(a) minimum of ten percent of the total farm acreage be set aside as a biodiversity preserve. That may include but is not limited to forests, wetlands, riparian corridors, and intentionally planted insectaries. Diversity in crop rotation and perennial planting is required: no annual crop can be planted in the same field for more than two years in succession. Bare tillage year round is prohibited so land needs to maintain adequate green cover.\"\n\nThe Demeter Association also recommends that the individual design of the land \"by the farmer, as determined by site conditions, is one of the basic tenets of biodynamic agriculture. This principle emphasizes that humans have a responsibility for the development of their ecological and social environment which goes beyond economic aims and the principles of descriptive ecology.\" Crops, livestock, and farmer, and \"the entire socioeconomic environment\" form a unique interaction, which biodynamic farming tries to \"actively shape ...through a variety of management practices. The prime objective is always to encourage healthy conditions for life\": soil fertility, plant and animal health, and product quality. \"The farmer seeks to enhance and support the forces of nature that lead to healthy crops, and rejects farm management practices that damage the environment, soil plant, animal or human health...the farm is conceived of as an organism, a self-contained entity with its own individuality,\" holistically conceived and self-sustaining. \"Disease and insect control are addressed through botanical species diversity, predator habitat, balanced crop nutrition, and attention to light penetration and airflow. Weed control emphasizes prevention, including timing of planting, mulching, and identifying and avoiding the spread of invasive weed species.\"\n\nBiodynamic agriculture differs from many forms of organic agriculture in its spiritual, mystical, and astrological orientation. It shares a spiritual focus, as well as its view toward improving humanity, with the \"nature farming\" movement in Japan. Important features include the use of livestock manures to sustain plant growth (recycling of nutrients), maintenance and improvement of soil quality, and the health and well being of crops and animals. Cover crops, green manures and crop rotations are used extensively and the farms to foster the diversity of plant and animal life, and to enhance the biological cycles and the biological activity of the soil.\n\nBiodynamic farms often have a cultural component and encourage local community, both through developing local sales and through on-farm community building activities. Some biodynamic farms use the Community Supported Agriculture model, which has connections with social threefolding.\n\nCompared to non-organic agriculture, BD farming practices have been found to be more resilient to environmental challenges, to foster a diverse biosphere, and to be more energy efficient, factors Eric Lichtfouse describes being of increasing importance in the face of climate change, energy scarcity and population growth.\n\nIn his \"agricultural course\" Steiner prescribed nine different preparations to aid fertilization, and described how these were to be prepared. Steiner believed that these preparations mediated terrestrial and cosmic forces into the soil. The prepared substances are numbered 500 through 508, where the first two are used for preparing fields, and seven are used for making compost. A long term trial (DOK experiment) evaluating the biodynamic farming system in comparison with organic and conventional farming systems, found that both organic farming and biodynamic farming resulted in enhanced soil properties, but had lower yields than conventional farming. Regarding compost development beyond accelerating the initial phase of composting, some positive effects have been noted:\n\nAlthough the preparations have direct nutrient values, their purpose in biodynamics is to support the self-regulating capacities of the soil biota in the case of 500 and 501 and the biological life resident in the composting organics, as well as the mature compost itself, in the others.\n\nField preparations, for stimulating humus formation:\n\nThe application rate of the biodynamic field spray preparations (i.e., 500 and 501) are 300 grams per hectare of horn manure and 5 grams per hectare of horn silica. These are made by stirring the ingredients into 20-50 liters of water per hectare for an hour, using a prescribed method.\n\nCompost preparations, used for preparing compost, employ herbs which are frequently used in medicinal remedies. Many of the same herbs are used in organic practices to make foliar fertilizers, turned into the soil as green manure, or in composting. The preparations include:\n\nThe approach considers that there are lunar and astrological influences on soil and plant development—for example, choosing to plant, cultivate or harvest various crops based on both the phase of the moon and the zodiacal constellation the moon is passing through, and also depending on whether the crop is the root, leaf, flower, or fruit of the plant. This aspect of biodynamics has been termed \"astrological\" in nature.\n\nBiodynamic agriculture has focused on the open pollination of seeds (with farmers thereby generally growing their own seed) and the development of locally adapted varieties.\n\nThe Demeter biodynamic certification system established in 1924 was the first certification and labelling system for organic production. As of 2018, to receive certification as biodynamic, the farm must meet the following standards: agronomic guidelines, greenhouse management, structural components, livestock guidelines, and post-harvest handling and processing procedures.\n\nThe term \"Biodynamic\" is a trademark held by the Demeter association of biodynamic farmers for the purpose of maintaining production standards used both in farming and processing foodstuffs. The trademark is intended to protect both the consumer and the producers of biodynamic produce. Demeter International an organization of member countries; each country has its own Demeter organization which is required to meet international production standards (but can also exceed them). The original Demeter organization was founded in 1928; the U.S. Demeter Association was formed in the 1980s and certified its first farm in 1982. In France, Biodivin certifies biodynamic wine. In Egypt, SEKEM has created the Egyptian Biodynamic Association (EBDA), an association that provides training for farmers to become certified. As of 2006, more than 200 wineries worldwide were certified as biodynamic; numerous other wineries employ biodynamic methods to a greater or lesser extent.\n\nResearch into biodynamic farming has been complicated by the difficulty of isolating the distinctively biodynamic aspects when conducting comparative trials. Consequently, there is no strong body of material that provides evidence of any specific effect.\n\nSince biodynamic farming is a form of organic farming, it can be generally assumed to share its characteristics, including \"less stressed soils and thus diverse and highly interrelated soil communities\".\n\nA 2009/2011 review found that biodynamically cultivated fields:\n\nBoth factors were similar to the result in organically cultivated fields.\n\nIn a 2002 newspaper editorial, Peter Treue, agricultural researcher at the University of Kiel, characterized biodynamics as pseudoscience and argued that similar or equal results can be obtained using standard organic farming principles. He wrote that some biodynamic preparations more resemble alchemy or magic akin to geomancy.\n\nIn a 1994 analysis, Holger Kirchmann, a soil researcher with the Swedish University of Agricultural Sciences, concluded that Steiner's instructions were occult and dogmatic, and cannot contribute to the development of alternative or sustainable agriculture. According to Kirchmann, many of Steiner's statements are not provable because scientifically clear hypotheses cannot be made from his descriptions. Kirchmann asserted that when methods of biodynamic agriculture were tested scientifically, the results were unconvincing. Further, in a 2004 overview of biodynamic agriculture, Linda Chalker-Scott, a researcher at Washington State University, characterized biodynamics as pseudoscience, writing that Steiner did not use scientific methods to formulate his theory of biodynamics, and that the later addition of valid organic farming techniques has \"muddled the discussion\" of Steiner's original idea. Based on the scant scientific testing of biodynamics, Chalker-Scott concluded \"no evidence exists\" that homeopathic preparations improve the soil.\n\nIn Michael Shermer's \"The Skeptic Encyclopedia of Pseudoscience\", Dan Dugan says that the way biodynamic preparations are supposed to be implemented are formulated solely on the basis of Steiner's \"own insight\". Skeptic Brian Dunning writes \"the best way to think of 'biodynamic agriculture' would be as a magic spell cast over an entire farm. Biodynamics sees an entire farm as a single organism, with something that they call a life force.\"\n\nFlorian Leiber, Nikolai Fuchs and Hartmut Spieß, researchers at the Goetheanum, have defended the principles of biodynamics and suggested that critiques of biodynamic agriculture which deny it scientific credibility are \"not in keeping with the facts...as they take no notice of large areas of biodynamic management and research.\" Biodynamic farmers are \"charged with developing a continuous dialogue between biodynamic science and the natural sciences \"sensu stricto\",\" despite important differences in paradigms, world views, and value systems.\n\nPhilosopher of science Michael Ruse has written that followers of biodynamic agriculture rather enjoy the scientific marginalisation that comes from its pseudoscientific basis, revelling both in its esoteric aspects and the impression that they were in the vanguard of the wider anti-science sentiment that has grown in opposition to modern methods such as genetic modification.\n\nSteiners theory was similar to those of the agricultural scientist Richard Krzymowski, who was teaching in Breslau since 1922. The environmental scientist Frank M. Rauch mentioned in 1995, concerning the reprint of a book from Raoul Heinrich Francé, another source probably used by Steiner.\n\n\n"}
{"id": "17970466", "url": "https://en.wikipedia.org/wiki?curid=17970466", "title": "Biofact (philosophy)", "text": "Biofact (philosophy)\n\nIn philosophy, sociology, and the arts, the word \"biofact\" is a neologism coined from the combination of the words \"bios\" and artifact and denotes a being that is both an artifact and living being or both natural and artificial. This being has been created by purposive human action but exists by processes of growth. There are sources who cite some creations of genetic engineering as examples of biofacts.\n\n\"Biofact\" was introduced as early as 2001 by the German philosopher Nicole C. Karafyllis although her book \"Biofakte\" published in 2003 is commonly used as reference for the introduction of the term. According to Karafyllis, the word biofact first appeared in a German article (entitled 'Biofakt und Artefakt') in 1943, written by the Austrian protozoologist Bruno M. Klein. Addressing both microscopy and philosophy, Klein named a biofact something that is a visible dead product emerging from a living being while this being is still alive (e.g. a shell). However, Klein's distinction operated with the difference biotic/abiotic and dead/alive, not with nature/technology and growth/man-made. For her part, Karafyllis described biofact as a hermeneutic concept that allows the comparison between nature and technology in the domain of the living. Particularly \n\nWith the term \"biofact\", Karafyllis wants to emphasize that living entities can be highly artificial due to methods deriving from agriculture, gardening (e.g. breeding) or biotechnology (e.g. genetic engineering, cloning). Biofacts show signatures of culture and technique.\nPrimarily, the concept aims to argue against the common philosophical tradition to summarize all kinds of living beings under the category nature. The concept \"biofact\" questions if the phenomenon of growth is and was a secure candidate for differentiating between nature and technology.\n\nFor the philosophy of technology the questions arise if a) biotechnology and agriculture should not be an integral part of \"reflexion\", thereby adding new insights to the common focus on the machine and the artifact, and if b) established concepts of technique and technology which stress artificiality should not be modified. Karafyllis regards the inclusion of biofacts into a theory of techniques as a chance, to reformulate classic concepts of design and construction for defining the making of artifacts. In her view, biofacts depend on the method of provocation.\n\nFor the philosophy of nature, biofacts highlight a need to clarify if nature is self-explanatory in every case. Biophilosophy is challenged to newly reflect upon the categories organism and living being.\nIn the philosophy of science, approaches are challenged which only focus on the category thing (or epistemic thing) without historizing the technicality, mediality and materiality of its emerging as a living object. For the sociology of science the biofact concept is fruitful to discuss the exclusiveness of scientific knowledge (the role of the expert) while making scientific objects which are released into the lifeworld or public sphere. Particularly because the biofact concept deals with the phenomenon of growth and the establishing of a self, it is also influential in the philosophical disciplines phenomenology, anthropology and ontology. It was Jürgen Habermas who recently stressed the anthropological consequences if mankind gives up the differentiation of \"coming into being\" and \"making\".\n\nArtifacts are artificial, i.e. man-made objects. Contrary to biofacts, they cannot be found in nature. Therefore, biofacts demarcate an ontological intersection. They are partially man-made, but growing. Like artifacts, they have been made for a certain utility. Biofacts can be seen as biotic artifacts which show their character as hybrids in multifold perspectives.\n\nThe term is also enabling philosophers to criticize some concepts in technoscience, where the union of scientific knowledge about nature and the technical creation of technonature is seen as progress in the political sense. The term has also been adopted in the new BioArt, not rarely without using its critical impacts.\n\nAs Karafyllis complemented the growth and reproduction of organisms with technical entities, she established a typology of different kinds of organisms according to their uses and these are:\n\n\n\n\n→ See the German Wikipedia entry for further literature in German.\n\n"}
{"id": "12180808", "url": "https://en.wikipedia.org/wiki?curid=12180808", "title": "British Society for Social Responsibility in Science", "text": "British Society for Social Responsibility in Science\n\nThe British Society for Social Responsibility in Science (BSSRS) was a radical science movement most active in the 1970s. It was formed in 1968 in opposition to university research on chemical and biological weapons, and supported by 83 distinguished scientists, including William Bragg, Francis Crick, Julian Huxley and Bertrand Russell. Nobel laureate Maurice Wilkins was the founding President.\nThe main aims of the BSSRS was to raise awareness of the social responsibilities of scientists, the political aspect of science and technology, and to create an informed public.\nAmong groups that were particularly active in BSSRS were members of\n\nBSSRS's inaugural meeting, sponsored by 64 Fellows, was held at the Royal Society, and attended by more\nthan 300, mostly UK, scientists and engineers. Professor Maurice Wilkins\nwas the founding President (1969–91).\n\nOne of the groups first targets was the British Science Association. At a meeting of the BSA in Durham in 1970, they raised political issues under a banner of \"Science is not neutral\". They continued their stance against the BSA, claiming it served a \"propagandist function\".\n\nBSSRS published a newssheet (1969–72), continued by Science for People, (1972/3), and also had local societies and organized public meetings, as well as publishing longer research monographs. \n\nBurhop (1971); Dickson (1971). See also Hilary Rose and Steven Rose (1976); Pirani (1970); Werskey (1971); Fuller (ed.) (1971) and Rose (2003).\n\nThere is an archive group of BSSRS collecting materials, with a website www.bssrs.org. \nBSSRS (archive)\n\n"}
{"id": "39355158", "url": "https://en.wikipedia.org/wiki?curid=39355158", "title": "Cambridge Science Festival", "text": "Cambridge Science Festival\n\nThe Cambridge Science Festival is a series of events typically held annually in March in Cambridge, England and is the United Kingdom's largest free science festival. The festival attracts more than 30,000 visitors to over 250 events. University researches and students open their lecture halls and laboratories to the general public, and hold Talks, Exhibitions and Demonstrations, mostly free of charge.\n\nStarted in 1994 by scientists of the University of Cambridge and backed by local businesses, the festival was inspired by \"National Science Week\" and is aimed making science and engineering more accessible, showcasing research performed at Cambridge University, and encouraging young people to pursue careers in engineering and science.\n\nThe 21st festival ran from 9 to 22 March 2015 and featured themes based on light science and technology to mark UNESCO's \"International Year of Light\".\n\n"}
{"id": "57830594", "url": "https://en.wikipedia.org/wiki?curid=57830594", "title": "Candida hypersensitivity", "text": "Candida hypersensitivity\n\nCandida hypersensitivity is a pseudoscientific disease promoted by William G. Crook, M.D. It is spuriously claimed that chronic yeast infections are responsible for many common disorders and non-specific symptoms including fatigue, weight gain, constipation, dizziness, muscle and joint pain, asthma, and others.\n\n\"Candida albicans\" is a fungus that colonizes a large majority of the population (meaning it is present in the body but not causing an infection or any problems). Under certain conditions, however, it can cause an infection. The most common manifestations are thrush (a superficial \"Candida\" infection in the mouth) and vaginitis, also commonly referred to as a yeast infection. \"Candida\" can also cause serious systemic infection, but this is almost always restricted to those with compromised immune systems, such as patients undergoing chemotherapy or with advanced AIDS.\n\nAfter reading publications by C. Orian Truss, M.D., Crook proposed the idea that a condition he termed systemic candidiasis, or Candida hypersensitivity, was responsible for a long list of common conditions and non-specific symptoms including fatigue, asthma, psoriasis, sexual dysfunction, and many others. The list of symptoms is similar to that of multiple chemical sensitivity. Many patients presenting with symptoms of environmental sensitivity claim to suffer from multiple \"fashionable\" syndromes.\n\nThe American Academy of Allergy, Asthma, and Immunology strongly criticized the concept of \"candidiasis hypersensitivity syndrome\" and the diagnostic and treatment approaches its proponents use. AAAAI's position statement concludes: (1) the concept of candidiasis hypersensitivity is speculative and unproven; (2) its basic elements would apply to almost all sick patients at some time because its supposed symptoms are essentially universal; (3) overuse of oral antifungal agents could lead to the development of resistant germs that could menace others; (4) adverse effects of oral antifungal agents are rare, but some inevitably will occur; and (5) neither patients nor doctors can determine effectiveness (as opposed to coincidence) without controlled trials. Because allergic symptoms can be influenced by many factors, including emotions, experiments must be designed to separate the effects of the procedure being tested from the effects of other factors.\n\nBy 2005, scientists were taking note of \"a large pseudoscientific cult\" that had developed around the topic of yeast infections, with claims that up to one in three people were affected by yeast-related illnesses including Candida hypersensitivity. \n\nSome practitioners of alternative medicine have promoted dietary supplements as supposed cures for this non-existent illness, rendering themselves liable to prosecution. In 1990, alternative health vendor Nature's Way signed a FTC consent agreement not to misrepresent in advertising any self-diagnostic test concerning yeast conditions or to make any unsubstantiated representation concerning any food or supplement's ability to control yeast conditions, with a fine of US$30,000 payable to the National Institutes of Health for research in genuine candidiasis.\n\n"}
{"id": "40703649", "url": "https://en.wikipedia.org/wiki?curid=40703649", "title": "Carctol", "text": "Carctol\n\nCarctol is an ineffective cancer treatment made by mixing eight Indian herbs.\n\nCarctol has been aggressively marketed as being able to treat cancer and reduce the side-effects of chemotherapy. However, there is no medical evidence that it has any benefits whatsoever for people with cancer.\n\nCarctol is a herbal dietary supplement marketed with claims it is based on traditional ayurvedic medicine. Its ingredients include \"Hemidesmus indicus\", \"Tribulus terrestris\", \"Piper cubeba\", \"Ammani vesicatoria\", \"Lepidium sativum\", \"Blepharis edulis\", \"Smilax china\", and \"Rheum emodi\".\n\nIt was first promoted in 1968 by Nandlal Tiwari. In 2009, Edzard Ernst wrote that it was still promoted in the United Kingdom; public relations companies hired by its sellers had garnered it wide coverage on the web and, according to the British Medical Journal, in the media generally.\n\nCancer Research UK say of Carctol, \"available scientific evidence does not support its use for the treatment of cancer in humans\". Edzard Ernst has written \"the claim that Carctol is of any benefit to cancer patients is not supported by scientific evidence\".\n\nHarriet A. Hall includes Carctol among the biologically-based remedies promoted by naturopaths. Hall laments that frauds and quacks persistently try to take advantage of the vulnerability of cancer patients.\n\n"}
{"id": "223607", "url": "https://en.wikipedia.org/wiki?curid=223607", "title": "Characteristics of common wasps and bees", "text": "Characteristics of common wasps and bees\n\nWhile observers can easily confuse common wasps and bees at a distance or without close observation, there are many different characteristics of large bees and wasps that can be used to identify them.\n\n\n\n"}
{"id": "10806718", "url": "https://en.wikipedia.org/wiki?curid=10806718", "title": "Data sharing", "text": "Data sharing\n\nData sharing is the practice of making data used for scholarly research available to other investigators. Replication has a long history in science. The motto of The Royal Society is 'Nullius in verba', translated \"Take no man's word for it.\" Many funding agencies, institutions, and publication venues have policies regarding data sharing because transparency and openness are considered by many to be part of the scientific method.\n\nA number of funding agencies and science journals require authors of peer-reviewed papers to share any supplemental information (raw data, statistical methods or source code) necessary to understand, develop or reproduce published research. A great deal of scientific research is not subject to data sharing requirements, and many of these policies have liberal exceptions. In the absence of any binding requirement, data sharing is at the discretion of the scientists themselves. In addition, in certain situations governments and institutions prohibit or severely limit data sharing to protect proprietary interests, national security, and subject/patient/victim confidentiality. Data sharing may also be restricted to protect institutions and scientists from use of data for political purposes.\n\nData and methods may be requested from an author years after publication. In order to encourage data sharing and prevent the loss or corruption of data, a number of funding agencies and journals established policies on data archiving. Access to publicly archived data is a recent development in the history of science made possible by technological advances in communications and information technology. To take full advantage of modern rapid communication may require consensual agreement on the criteria underlying mutual recognition of respective contributions. Models recognized for improving the timely sharing of data for more effective response to emergent infectious disease threats include the data sharing mechanism introduced by the GISAID Initiative.\n\nDespite policies on data sharing and archiving, data withholding still happens. Authors may fail to archive data or they only archive a portion of the data. Failure to archive data alone is not data withholding. When a researcher requests additional information, an author sometimes refuses to provide it. When authors withhold data like this, they run the risk of losing the trust of the science community.\n\nData sharing may also indicate the sharing of personal information on a social media platform.\n\nOn August 9, 2007, President Bush signed the America COMPETES Act (or the \"America Creating Opportunities to Meaningfully Promote Excellence in Technology, Education, and Science Act\") requiring civilian federal agencies to provide guidelines, policy and procedures, to facilitate and optimize the open exchange of data and research between agencies, the public and policymakers. See Section 1009.\n\nThe NIH Final Statement of Sharing of Research Data says:\nAllegations of misconduct in medical research carry severe consequences. The United States Department of Health and Human Services established an office to oversee investigations of allegations of misconduct, including data withholding. The website defines the mission:\n\nSome research organizations feel particularly strongly about data sharing. Stanford University's WaveLab has a philosophy about reproducible research and disclosing all algorithms and source code necessary to reproduce the research. In a paper titled \"WaveLab and Reproducible Research,\" the authors describe some of the problems they encountered in trying to reproduce their own research after a period of time. In many cases, it was so difficult they gave up the effort. These experiences are what convinced them of the importance of disclosing source code. The philosophy is described:\n\nThe Data Observation Network for Earth (DataONE) and Data Conservancy are projects supported by the National Science Foundation to encourage and facilitate data sharing among research scientists and better support meta-analysis. In environmental sciences, the research community is recognizing that major scientific advances involving integration of knowledge in and across fields will require that researchers overcome not only the technological barriers to data sharing but also the historically entrenched institutional and sociological barriers. Dr. Richard J. Hodes, director of the National Institute on Aging has stated, \"the old model in which researchers jealously guarded their data is no longer applicable\".\n\nThe Alliance for Taxpayer Access is a group of organizations that support open access to government sponsored research. The group has expressed a \"Statement of Principles\" explaining why they believe open access is important. They also list a number of international public access policies. This is no more so than in timely communication of essential information to effectively respond to health emergencies. While public domain archives have been embraced for depositing data, mainly post formal publication, they have failed to encourage rapid data sharing during health emergencies, among them the Ebola and Zika, outbreaks. More clearly defined principles are required to recognize the interests of those generating the data while permitting free, unencumbered access to and use of the data (pre-publication) for research and practical application, such as those adopted by the GISAID Initiative to counter emergent threats from influenza.\n\n\nWithholding of data has become so commonplace in academic genetics that researchers at Massachusetts General Hospital published a journal article on the subject. The study found that \"Because they were denied access to data, 28% of geneticists reported that they had been unable to confirm published research.\"\n\nIn a 2006 study, it was observed that, of 141 authors of a publication from the American Psychology Association (APA) empirical articles, 103 (73%) did not respond with their data over a 6-month period. In a follow up study published in 2015, it was found that 246 out of 394 contacted authors of papers in APA journals did not share their data upon request (62%).\n\nA 2018 study reported on study of a random sample of 48 articles published during February–May 2017 in the \"Journal of Archaeological Science\" which found openly available raw data for 18 papers (53%), with compositional and dating data being the most frequently shared types. The same study also emailed authors of articles on experiments with stone artifacts that were published during 2009 and 2015 to request data relating to the publications. They contacted the authors of 23 articles and received 15 replies, resulting in a 70% response rate. They received five responses that included data files, giving an overall sharing rate of 20%.\n\nA study of scientists in training indicated many had already experienced data withholding. This study has given rise to the fear the future generation of scientists will not abide by the established practices.\n\nRequirements for data sharing are more commonly imposed by institutions, funding agencies, and publication venues in the medical and biological sciences than in the physical sciences. Requirements vary widely regarding whether data must be shared at all, with whom the data must be shared, and who must bear the expense of data sharing.\n\nFunding agencies such as the NIH and NSF tend to require greater sharing of data, but even these requirements tend to acknowledge the concerns of patient confidentiality, costs incurred in sharing data, and the legitimacy of the request. Private interests and public agencies with national security interests (defense and law enforcement) often discourage sharing of data and methods through non-disclosure agreements.\n\nData sharing poses specific challenges in participatory monitoring initiatives, for example where forest communities collect data on local social and environmental conditions. In this case, a rights-based approach to the development of data-sharing protocols can be based on principles of free, prior and informed consent, and prioritise the protection of the rights of those who generated the data, and/or those potentially affected by data-sharing.\n\n\n — discusses the international exchange of data in the natural sciences.\n\n"}
{"id": "7321888", "url": "https://en.wikipedia.org/wiki?curid=7321888", "title": "Dielectric thermal analysis", "text": "Dielectric thermal analysis\n\nDielectric thermal analysis (DETA), or dielectric analysis (DEA), is a materials science technique similar to dynamic mechanical analysis except that an oscillating electrical field is used instead of a mechanical force. For investigation of the curing behavior of thermosetting resin systems, composite materials, adhesives and paints, Dielectric Analysis (DEA) can be used in accordance with ASTM E 2038 or E 2039. The great advantage of DEA is that it can be employed not only on a laboratory scale, but also in process.\n\nIn a typical test, the sample is placed in contact with two electrodes (the dielectric sensor) and a sinusoidal voltage (the excitation) is applied to one electrode. The resulting sinusoidal current (the response) is measured at the second electrode. The response signal is attenuated in amplitude and shifted in phase in relation to the mobility of the ions and alignment of the dipoles. Dipoles in the material will attempt to align with the electric field and ions (present as impurities) will move toward the electrode of opposite polarity. The dielectric properties of permittivity ε' and loss factor ε\" are then calculated from this measured amplitude and phase change.\n"}
{"id": "5618154", "url": "https://en.wikipedia.org/wiki?curid=5618154", "title": "Earthquake light", "text": "Earthquake light\n\nAn earthquake light (EQL) is a luminous aerial phenomenon that reportedly appears in the sky at or near areas of tectonic stress, seismic activity, or volcanic eruptions. Skeptics point out that the phenomenon is poorly understood and many of the reported sightings can be accounted for by mundane explanations.\n\nThe lights are reported to appear while an earthquake is occurring, although there are reports of lights before or after earthquakes, such as reports concerning the 1975 Kalapana earthquake. They are reported to have shapes similar to those of the auroras, with a white to bluish hue, but occasionally they have been reported having a wider color spectrum. The luminosity is reported to be visible for several seconds, but has also been reported to last for tens of minutes. Accounts of viewable distance from the epicenter varies: in the 1930 Idu earthquake, lights were reported up to from the epicenter. Earthquake lights were reportedly spotted in Tianshui, Gansu, approximately north-northeast of the 2008 Sichuan earthquake's epicenter.\n\nDuring the 2003 Colima earthquake in Mexico, colorful lights were seen in the skies for the duration of the earthquake. During the 2007 Peru earthquake lights were seen in the skies above the sea and filmed by many people. The phenomenon was also observed and caught on film during the 2009 L'Aquila and the 2010 Chile earthquakes. Video footage has also recorded this happening during the 9 April 2011 eruption of Sakurajima Volcano, Japan . The phenomenon was also reported around the Amuri Earthquake in New Zealand, that occurred 1 September 1888. The lights were visible in the morning of 1 September in Reefton, and again on 8 September.\n\nMore recent appearances of the phenomenon, along with video footage of the incidents, happened in Sonoma County of California on August 24, 2014, and in Wellington New Zealand on November 14, 2016 where blue flashes like lightning were seen in the night sky, and recorded on several videos. In September 8, 2017, many people reported such sightings in Mexico City after a 8.2 magnitude earthquake with epicenter 460 miles (740 km) away, near Pijijiapan in the state of Chiapas.\n\nAppearances of the earthquake light seem to occur when the quakes have a high magnitude, generally 5 or higher on the Richter scale.\nThere have also been incidents of yellow, ball-shaped lights appearing before earthquakes.\n\nEarthquake lights may be classified into two different groups based on their time of appearance: (1) preseismic EQL, which generally occur a few seconds to up to a few weeks prior to an earthquake, and are generally observed closer to the epicenter and (2) coseismic EQL, which can occur either near the epicenter (“earthquake‐induced stress”), or at significant distances away from the epicenter during the passage of the seismic wavetrain, in particular during the passage of \"S\" waves (“wave‐induced stress”).\n\nEQL during the lower magnitude aftershock series seem to be rare. \nResearch into earthquake lights is ongoing; as such, several mechanisms have been proposed. Positive Holes is one such model.\n\nSome models suggest the generation of EQLs involve the ionization of oxygen to oxygen anions by breaking of peroxy bonds in some types of rocks (dolomite, rhyolite, etc.) by the high stress before and during an earthquake. After the ionisation, the ions travel up through the cracks in the rocks. Once they reach the atmosphere these ions can ionise pockets of air, forming plasma that emits light. Lab experiments have validated that some rocks do ionise the oxygen in them when subjected to high stress levels.\nResearch suggests that the angle of the fault is related to the likelihood of earthquake light generation, with subvertical (nearly vertical) faults in rifting environments having the most incidences of earthquake lights.\n\nOne hypothesis involves intense electric fields created piezoelectrically by tectonic movements of rocks containing quartz.\n\nAnother possible explanation is local disruption of the Earth's magnetic field and/or ionosphere in the region of tectonic stress, resulting in the observed glow effects either from ionospheric radiative recombination at lower altitudes and greater atmospheric pressure or as aurora. However, the effect is clearly not pronounced or notably observed at all earthquake events and is yet to be directly experimentally verified.\n\nDuring the American Physical Society's 2014 March meeting, research was provided that gave a possible explanation for the reason why bright lights sometimes appear during an earthquake. The research stated that when two layers of the same material rub against each other, voltage is generated. The researcher, Professor Troy Shinbrot of Rutgers University, conducted lab experiments with different types of grains to mimic the crust of the earth and emulated the occurrence of earthquakes. \"When the grains split open, they measured a positive voltage spike, and when the split closed, a negative spike.\" The crack allows the voltage to discharge into the air which then electrifies the air and creates a bright electrical light when it does so. According to the research provided, they have produced these voltage spikes every single time with every material tested. While the reason for such an occurrence was not provided, Professor Troy Shinbrot referenced the light to a phenomenon called triboluminescence. Researchers hope that by getting to the bottom of this phenomenon, it will provide more information that will allow seismologists to better predict earthquakes.\n\nAccording to Brian Dunning researchers should be concerned about the fact that there are no documented \"confirmed observations\" of earthquake lights. It is a red flag that there is no consistency of what they are, when they happen and where they happen. It is likely that there \"is not one, known, proven phenomenon\". However, a significant amount of video footage has surfaced since the advent of sites like YouTube that claim to represent this —one example being the 2017 Mexico earthquake— although no consistent explanation has been agreed upon. There is also a \"staggering volume of literature... hardly any of these papers agree on anything... I'm forced to wonder how many of these eager researchers are familiar with Hyman's Categorical Imperative: 'Do not try to explain something until you are sure there is something to be explained'\". Dunning's final conclusion is that until there is \"pending decent evidence\" be skeptical of claims of earthquake lights.\n\nRobert Sheaffer writes that he is surprised how many skeptics and science bloggers have accepted earthquake lights as a real phenomenon without checking the source of the claim, or doing basic research on what the lights might be. Sheaffer on his \"Bad UFO\" blog shows examples of what people claim are earthquake lights, then he shows photos of iridescent clouds which appear to be the same. He states that \"It's truly remarkable how mutable \"earthquake lights\" are. Sometimes they look like small globes, climbing up a mountain. Sometimes they look like flashes of lightning. Other times they look exactly like iridescent clouds. Earthquake lights can look like anything at all, when you are avidly seeking evidence for them.\"\n\nSharon Hill writes that the science isn't in on earthquake lights, not enough research has been done. She states that not all earthquakes are the same and it may be possible that \"extension\" and \"compression\" faults produce \"different behaviors on the surface as well as subsurface\". She understands why skeptics are reluctant to confirm that this might be happening \"because of the unreliability, irreproducibility and inadequate explanation for it\". Also a possibility is that \"stronger quakes break electrical wiring and cause transformers to explode, causing the flashes\". As more research is done on electrical signals concerning earthquakes we will have a better understanding of this phenomenon.\n\n\n"}
{"id": "24095765", "url": "https://en.wikipedia.org/wiki?curid=24095765", "title": "Eco-costs", "text": "Eco-costs\n\nEco-costs are the costs of the environmental burden of a product on the basis of prevention of that burden. They are the costs which should be made to reduce the environmental pollution and materials depletion in our world to a level which is in line with the carrying capacity of our earth.\n\nFor example: for each 1000 kg CO emission, one should invest €116,- in offshore windmill parks (plus in the other CO reduction systems at that price or less). When this is done consequently, the total CO emissions in the world will be reduced by 65% compared to the emissions in 2008. As a result, global warming will stabilise. In short: \"the eco-costs of 1000kg CO are € 116,-\".\n\nSimilar calculations can be made on the environmental burden of acidification, eutrophication, summer smog, fine dust, eco-toxicity, and the use of metals, rare earths, fossil fuels, water and land (nature).\nAs such, the eco-costs are 'external costs', since they are not yet integrated in the real life costs of current production chains (Life Cycle Costs). The eco-costs should be regarded as hidden obligations.\n\nThe eco-costs of a product are the sum of all eco-costs of emissions and use of resources during the life cycle \"from cradle to cradle\". The widely accepted method to make such a calculation is called life cycle assessment (LCA), which is basically a mass and energy balance, defined in the ISO 14040, and the ISO 14044 (for the building industry the EN 15804).\n\nThe practical use of eco-costs is to compare the sustainability of several product types with the same functionality. \nThe advantage of eco-costs is that they are expressed in a standardized monetary value (€) which appears to be easily understood 'by instinct'. Also the calculation is transparent and relatively easy, compared to damage based models which have the disadvantage of extremely complex calculations with subjective weighting of the various aspects contributing to the overall environmental burden.\n\nThe system of eco-costs is part of the bigger model of the ecocosts/value ratio, EVR.\n\nThe eco-costs system has been introduced in 1999 on conferences, and published in 2000-2004 in the International Journal of LCA, \nand in the \"Journal of Cleaner Production\". \nIn 2007 the system has been updated, and published in 2010. The next updates were in 2012 and 2017. It is planned to update the system every 5 years to incorporate the latest developments in science. \nThe concept of eco-costs has been made operational with general databases of the Delft University of Technology, and is described at www.ecocostsvalue.com. The method of the eco-costs is based on the sum of the marginal prevention costs (end of pipe as well as system integrated) for toxic emissions related to human health as well as ecosystems, emissions that cause global warming, and resource depletion (metals, rare earths, fossil fuels, water, and land-use). For a visual display of the system see Figure 1.\n\nMarginal prevention costs of toxic emissions are derived from the so-called prevention curve as depicted in Figure 2. The basic idea behind such a curve is that a country (or a group of countries, such as the European Union), must take prevention measures to reduce toxic emissions (more than one measure is required to reach the target). From the point of view of the economy, the cheapest measures (in terms of euro/kg) are taken first. At a certain point at the curve, the reduction of the emissions is sufficient to bring the concentration of the pollution below the so-called no-effect-level. The no-effect-level of emissions is the level that the emissions and the natural absorption of the earth are in equilibrium again at a maximum temperature rise of 2 degrees C. The no-effect-level of a toxic emission is the level where the concentration in nature is well below the toxicity threshold (most natural toxic substances have a toxicity threshold, below which they might even have a beneficial effect), or below the natural background level. For human toxicity the 'no-observed-adverse-effect level' is used.\nThe eco-costs are the marginal prevention costs of the last measure of the prevention curve to reach the no-effect-level. See the abovementioned references 4 and 8 for a full description of the calculation method (note that in the calculation 'classes' of emissions with the same 'midpoint' are combined, as explained below).\n\nThe classical way to calculate a 'single indicator' in LCA is based on the damage of the emissions. Pollutants are grouped in 'classes', multiplied by a 'characterisation' factor to account for their relative importance within a class, and totalised to the level of their 'midpoint' effect (global warming, acidification, nutrification, etc.). The classical problem is then to determine the relative importance of each midpoint effect. In damage based systems this is done by 'normalisation' (= comparison with the pollution in a country or a region) and 'weighting' (= giving each midpoint a weight, to take the relative importance into account) by an expert panel.\n\nThe calculation of the eco-costs is based on classification and characterisation tables as well (combining tables from IPCC (), the USEtox model (usetox.org), tables of the ILCD (), however has a different approach to the normalisation and weighting steps. Normalisation is done by calculating the marginal prevention costs for a region (i.e. the European Union), as described above. The weighting step is not required in the eco-costs system, since the total result is the sum of the eco-costs of all midpoints.\nThe advantage of such a calculation is that the marginal prevention costs are related to the cost of the most expensive Best Available Technology which is needed to meet the target, and the corresponding level of Tradable Emission Rights which is required in future. From a business point of view, the eco-costs are the costs of non-compliance with future governmental regulations. Example from the past: NOx emissions of Volkswagen diesel.\n\nThe eco-costs have been calculated for the situation in the European Union. It is expected that the situation in some states in the USA, like California and Pennsylvania, give similar results. It might be argued that the eco-costs are also an indication of the marginal prevention costs for other parts of the globe, under the condition of a level playing field for production companies.\n\nThe method of the eco-costs 2017 (version 1.3) comprises tables of over 36.000 emissions, and has been made operational by special database for SimaPro: Idematapp 2018 and Idemat2018 (based on LCIs from Ecoinvent V3.4), Agri Footprint, and a database for CES (Cambridge Engineering Selector). Over 10.000 materials and processes are covered in total. Excel look-up tables are provided at www.ecocostsvalue.com.\n\nFor emissions of toxic substances, the following set of multipliers (marginal prevention costs) is used in the eco-costs 2017 system:\nThe characterisation ('midpoint') tables which are applied in the eco-costs 2017 system, are recommended by the ILCD:\n\nIn addition to abovementioned eco-costs for emissions, there is a set of eco-costs to characterize the 'midpoints' of resource depletion:\n\nThe abovementioned marginal prevention costs at midpoint level can be combined to 'endpoints' in three groups, plus global warming as a separate group:\nSince the endpoints have the same monetary unit (e.g. euro, dollar), they are added up to the total eco-costs without applying a 'subjective' weighting system. This is an advantage of the eco-costs system (see also ISO 14044 section 4.4.3.4 and 4.4.5). \nSo called 'double counting' (ISO 14044 section 4.4.2.2.3) is avoided in the eco-costs system.\n\nThe eco-costs of global warming (also called eco-costs of carbon footprint) can be used as an indicator for the carbon footprint. The eco-costs of resource depletion can be regarded as an indicator for 'circularity' in the theory of the circular economy. However, it is advised to include human toxicity and eco-toxicity, and include the eco-costs of global warming in the calculations on the circular economy as well. The eco-costs of global warming are required to reveal the difference between fossil-based products and bio-based products, since biogenic CO is not counted in LCA (biogenic CO is part of the natural recycle loop in the biosphere).\nTherefore, total eco-costs can be regarded as a robust indicator for cradle-to-cradle calculations in LCA for products and services in the theory of the circular economy.\nSince the economic viability of a business model is also an important aspect of the circular economy, the added value of a product-service system should be part of the analysis. This requires the two dimensional approach of Eco-efficient Value Creation as described at the Wikipedia page on the model of the ecocosts/value ratio, EVR.\n\nThe Delft University of Technology has developed a single indicator for S-LCA as well, the so-called s-eco-costs, to incorporate the sometimes appalling working conditions in production chains (e.g. production of garments, mining of metals). Aspects are the low minimum wages in developing countries (the \"fair wage deficit\"), the aspects of \"child labour\" and extreme poverty\", the aspect of \"excessive working hours\", and the aspect of \"OSH (Occupational Safety and Health)\". The s-eco-costs system has been published in the Journal of Cleaner Production.\n\nPrevention measures will decrease the costs of the damage, related to environmental pollution. The damage costs are in most cases the same (or a bit higher) compared to the prevention costs. So the total effect of prevention measures on our society is that it results in a better environment at no extra costs.\n\nThere are many 'single indicators' for LCA. Basically, they fall into three categories:\n\nThe best known 'single issue' indicator is the carbon footprint: the total emissions of kg CO, or kg CO \"equivalent\" (taking methane and some other greenhouse gasses into account as well). The advantage of a single issue indicator is, that its calculation is simple and transparent, without any complex assumptions. It is easy as well to communicate to the public. The disadvantage is that is ignores the problems caused by other pollutants and it is not suitable for cradle-to-cradle calculations (because materials depletion is not taken into account).\nThe most common single indicators are damage based. This stems from the period of the 1990s, when LCA was developed to make people aware of the damage of production and consumption. The advantage of damage based single indicators is, that they make people aware of the fact that they should consume less, and make companies aware that they should produce cleaner. The disadvantage is that these damage based systems are very complex, not transparent for others than who make the computer calculations, need many assumptions, and suffer from the subjective weighting procedure as last step at the end. Communication of the result is not easy, since the result is expressed in 'points' (attempts to express the results in money were never very successful, because of methodological flaws and uncertainties). The most recent endpoint indicators avoid the last step to a single indicator: UseTOX 2 and ReCiPe 2016, resulting in 2 respectively 3 endpoints (human health, ecosystems and resources separately).\nPrevention based indicators, like the system of the eco-costs, are relatively new. The advantage, in comparison to the damage based systems, is that the calculations are relatively easy and transparent, and that the results can be explained in terms of money and in measures to be taken. The system is focused on the decision taking processes of architects, business people, designers and engineers. The advantage is that it provides 1 single endpoint in euro's. The disadvantage is that the system is not focused on the fact that people should consume less.\n\nThe eco-costs are calculated for the situation of the European Union, but are applicable worldwide under the assumption of a level playing field for business, and under the precautionary principle. There are two other prevention based systems, developed after the introduction of the eco-costs, which are based on the local circumstances of a specific country:\n\nIn line with the policy of the Delft University of Technology to bring LCA calculations within reach of everybody, open access excel databases (tables) are made available on the internet, free of charge. \nExperts on LCA who want to use the eco-costs as a single indicator, can download the full database for Simapro (the Eco-costs Method as well as the Idematapp LCIs), when they have a Simapro licence. \nEngineers, designers and architects can have databases, free of charge, for CES and ArchiCAD software, provided that they have a licence for the software.\n\nThe following databases are available:\n\n"}
{"id": "744997", "url": "https://en.wikipedia.org/wiki?curid=744997", "title": "Electronic voice phenomenon", "text": "Electronic voice phenomenon\n\nWithin ghost hunting and parapsychology, electronic voice phenomena (EVP) are sounds found on electronic recordings that are interpreted as spirit voices that have been either unintentionally recorded or intentionally requested and recorded. Parapsychologist Konstantīns Raudive, who popularized the idea in the 1970s, described EVP as typically brief, usually the length of a word or short phrase.\n\nEnthusiasts consider EVP to be a form of paranormal phenomenon often found in recordings with static or other background noise. However, scientists regard EVP as a form of auditory pareidolia (interpreting random sounds as voices in one's own language) and a pseudoscience promulgated by popular culture. Prosaic explanations for EVP include apophenia (perceiving patterns in random information), equipment artifacts, and hoaxes.\n\nAs the Spiritualist religious movement became prominent in the 1840s–1920s with a distinguishing belief that the spirits of the dead can be contacted by mediums, new technologies of the era including photography were employed by spiritualists in an effort to demonstrate contact with a spirit world. So popular were such ideas that Thomas Edison was asked in an interview with \"Scientific American\" to comment on the possibility of using his inventions to communicate with spirits. He replied that if the spirits were only capable of subtle influences, a sensitive recording device would provide a better chance of spirit communication than the table tipping and ouija boards mediums employed at the time. However, there is no indication that Edison ever designed or constructed a device for such a purpose. As sound recording became widespread, mediums explored using this technology to demonstrate communication with the dead as well. Spiritualism declined in the latter part of the 20th century, but attempts to use portable recording devices and modern digital technologies to communicate with spirits continued.\n\nAmerican photographer Attila von Szalay was among the first to try recording what he believed to be voices of the dead as a way to augment his investigations in photographing ghosts. He began his attempts in 1941 using a 78 rpm record, but it wasn't until 1956 — after switching to a reel-to-reel tape recorder — that he believed he was successful. Working with Raymond Bayless, von Szalay conducted a number of recording sessions with a custom-made apparatus, consisting of a microphone in an insulated cabinet connected to an external recording device and speaker. Szalay reported finding many sounds on the tape that could not be heard on the speaker at the time of recording, some of which were recorded when there was no one in the cabinet. He believed these sounds to be the voices of discarnate spirits. Among the first recordings believed to be spirit voices were such messages as \"This is G!\", \"Hot dog, Art!\", and \"Merry Christmas and Happy New Year to you all\". Von Szalay and Raymond Bayless' work was published by the Journal of the American Society for Psychical Research in 1959. Bayless later went on to co-author the 1979 book, \"Phone Calls From the Dead\".\n\nIn 1959, Swedish painter and film producer Friedrich Jürgenson was recording bird songs. Upon playing the tape later, he heard what he interpreted to be his dead father's voice and then the spirit of his deceased wife calling his name. He went on to make several more recordings, including one that he said contained a message from his late mother.\n\nKonstantin Raudive, a Latvian psychologist who had taught at the Uppsala University, Sweden and who had worked in conjunction with Jürgenson, made over 100,000 recordings which he described as being communications with people. Some of these recordings were conducted in an RF-screened laboratory and contained words Raudive said were identifiable. In an attempt to confirm the content of his collection of recordings, Raudive invited listeners to hear and interpret them. He believed that the clarity of the voices heard in his recordings implied that they could not be readily explained by normal means. Raudive published his first book, \"Breakthrough: An Amazing Experiment in Electronic Communication with the Dead\" in 1968 and it was translated into English in 1971.\n\nIn 1980, William O'Neil constructed an electronic audio device called \"The Spiricom\". O'Neil claimed the device was built to specifications which he received psychically from George Mueller, a scientist who had died six years previously. At a Washington, DC press conference on April 6, 1982, O'Neil stated that he was able to hold two-way conversations with spirits through the Spiricom device, and provided the design specifications to researchers for free. However, nobody is known to have replicated the results O'Neil claimed using his own Spiricom devices. O'Neil's partner, retired industrialist George Meek, attributed O'Neil's success, and the inability of others to replicate it, to O'Neil's mediumistic abilities forming part of the loop that made the system work. \n\nAnother electronic device specifically constructed in an attempt to capture EVP is \"Frank's Box\" or the \"Ghost Box\", created in 2002 by EVP enthusiast Frank Sumption for supposed real-time communication with the dead. Sumption claims he received his design instructions from the spirit world. The device is described as a combination white noise generator and AM radio receiver modified to sweep back and forth through the AM band selecting split-second snippets of sound. Critics of the device say its effect is subjective and incapable of being replicated, and since it relies on radio noise, any meaningful response a user gets is purely coincidental, or simply the result of pareidolia. Paranormal researcher Ben Radford writes that Frank's Box is a \"modern version of the Ouija board... also known as the 'broken radio'\".\n\nIn 1982, Sarah Estep founded the American Association of Electronic Voice Phenomena (AA-EVP) in Severna Park, Maryland, a nonprofit organization with the purpose of increasing awareness of EVP, and of teaching standardized methods for capturing it. Estep began her exploration of EVP in 1976, and says she has made hundreds of recordings of messages from deceased friends, relatives, and extraterrestrials whom she speculated originated from other planets or dimensions.\n\nThe term Instrumental Trans-Communication (ITC) was coined by Ernst Senkowski in the 1970s to refer more generally to communication through any sort of electronic device such as tape recorders, fax machines, television sets or computers between spirits or other discarnate entities and the living. One particularly famous claimed incidence of ITC occurred when the image of EVP enthusiast Friedrich Jürgenson (whose funeral was held that day) was said to have appeared on a television in the home of a colleague, which had been purposefully tuned to a vacant channel. ITC enthusiasts also look at the TV and video camera feedback loop of the Droste effect.\n\nIn 1979, parapsychologist D. Scott Rogo described an alleged paranormal phenomenon in which people report that they receive simple, brief, and usually single-occurrence telephone calls from spirits of deceased relatives, friends, or strangers. Rosemary Guiley has written \"within the parapsychology establishment, Rogo was often faulted for poor scholarship, which, critics said, led to erroneous conclusions.\"\n\nIn 1995, the parapsychologist David Fontana proposed in an article that poltergeists could haunt tape recorders. He speculated that this may have happened to the parapsychologist Maurice Grosse who investigated the Enfield Poltergeist case. However, Tom Flynn a media expert for the Committee for Skeptical Inquiry examined Fontana's article and suggested an entirely naturalistic explanation for the phenomena. According to the skeptical investigator Joe Nickell \"Occasionally, especially with older tape and under humid conditions, as the tape travels it can adhere to one of the guide posts. When this happens on a deck where both supply and take-up spindles are powered, the tape continues to feed, creating a fold. It was such a loop of tape, Flynn theorizes, that threaded its way amid the works of Grosse’s recorder.\"\n\nIn 1997, Imants Barušs, of the Department of Psychology at the University of Western Ontario, conducted a series of experiments using the methods of EVP investigator Konstantin Raudive, and the work of \"instrumental transcommunication researcher\" Mark Macy, as a guide. A radio was tuned to an empty frequency, and over 81 sessions a total of 60 hours and 11 minutes of recordings were collected. During recordings, a person either sat in silence or attempted to make verbal contact with potential sources of EVP. Barušs stated that he did record several events that sounded like voices, but they were too few and too random to represent viable data and too open to interpretation to be described definitively as EVP. He concluded: \"While we did replicate EVP in the weak sense of finding voices on audio tapes, none of the phenomena found in our study was clearly anomalous, let alone attributable to discarnate beings. Hence we have failed to replicate EVP in the strong sense.\" The findings were published in the \"Journal of Scientific Exploration\" in 2001, and include a literature review.\n\nIn 2005, the \"Journal of the Society for Psychical Research\" published a report by paranormal investigator Alexander MacRae. MacRae conducted recording sessions using a device of his own design that generated EVP.\nIn an attempt to demonstrate that different individuals would interpret EVP in the recordings the same way, MacRae asked seven people to compare some selections to a list of five phrases he provided, and to choose the best match. MacRae said the results of the listening panels indicated that the selections were of paranormal origin.\n\nPortable digital voice recorders are currently the technology of choice for some EVP investigators. Since some of these devices are very susceptible to Radio Frequency (RF) contamination, EVP enthusiasts sometimes try to record EVP in RF- and sound-screened rooms.\n\nSome EVP enthusiasts describe hearing the words in EVP as an ability, much like learning a new language. Skeptics suggest that the claimed instances may be misinterpretations of natural phenomena, inadvertent influence of the electronic equipment by researchers, or deliberate influencing of the researchers and the equipment by third parties. EVP and ITC are seldom researched within the scientific community, so most research in the field is carried out by amateur researchers who lack training and resources to conduct scientific research, and who are motivated by subjective notions.\n\nParanormal claims for the origin of EVP include living humans imprinting thoughts directly on an electronic medium through psychokinesis and communication by discarnate entities such as spirits, nature energies, beings from other dimensions, or extraterrestrials. Paranormal explanations for EVP generally assume production of EVP by a communicating intelligence through means other than the typical functioning of communication technologies. Natural explanations for reported instances of EVP tend to dispute this assumption explicitly and provide explanations which do not require novel mechanisms that are not based on recognized scientific phenomena.\n\nOne study, by psychologist Imants Barušs, was unable to replicate suggested paranormal origins for EVP recorded under controlled conditions. Brian Regal in \"Pseudoscience: A Critical Encyclopedia\" (2009) has written \"A case can be made for the idea that many EVPs are artifacts of the recording process itself with which the operators are unfamiliar. The majority of EVPS have alternative, nonspiritual sources; anomalous ones have no clear proof they are of spiritual origin.\"\n\nThere are a number of simple scientific explanations that can account for why some listeners to the static on audio devices may believe they hear voices, including radio interference and the tendency of the human brain to recognize patterns in random stimuli. Some recordings may be hoaxes created by frauds or pranksters.\n\n\"Auditory pareidolia\" is a situation created when the brain incorrectly interprets random patterns as being familiar patterns. In the case of EVP it could result in an observer interpreting random noise on an audio recording as being the familiar sound of a human voice. The propensity for an apparent voice heard in white noise recordings to be in a language understood well by those researching it, rather than in an unfamiliar language, has been cited as evidence of this, and a broad class of phenomena referred to by author Joe Banks as \"Rorschach Audio\" has been described as a global explanation for all manifestations of EVP.\n\nSkeptics such as David Federlein, Chris French, Terence Hines and Michael Shermer say that EVP are usually recorded by raising the \"noise floor\" – the electrical noise created by all electrical devices – in order to create white noise. When this noise is filtered, it can be made to produce noises which sound like speech. Federlein says that this is no different from using a wah pedal on a guitar, which is a focused sweep filter which moves around the spectrum and creates open vowel sounds. This, according to Federlein, sounds exactly like some EVP. This, in combination with such things as cross modulation of radio stations or faulty ground loops can cause the impression of paranormal voices. The human brain evolved to recognize patterns, and if a person listens to enough noise the brain will detect words, even when there is no intelligent source for them. Expectation also plays an important part in making people believe they are hearing voices in random noise.\n\n\"Apophenia\" is related to, but distinct from pareidolia. Apophenia is defined as \"the spontaneous finding of connections or meaning in things which are random, unconnected or meaningless\", and has been put forward as a possible explanation. According to the psychologist James Alcock what people hear in EVP recordings can best be explained by apophenia, cross-modulation or expectation and wishful thinking. Alcock concluded \"Electronic Voice Phenomena are the products of hope and expectation; the claims wither away under the light of scientific scrutiny.\"\n\nInterference, for example, is seen in certain EVP recordings, especially those recorded on devices which contain RLC circuitry. These cases represent radio signals of voices or other sounds from broadcast sources. Interference from CB Radio transmissions and wireless baby monitors, or anomalies generated through cross modulation from other electronic devices, are all documented phenomena. It is even possible for circuits to resonate without any internal power source by means of radio reception.\n\n\"Capture errors\" are anomalies created by the method used to capture audio signals, such as noise generated through the over-amplification of a signal at the point of recording.\n\nArtifacts created during attempts to boost the clarity of an existing recording might explain some EVP. Methods include re-sampling, frequency isolation, and noise reduction or enhancement, which can cause recordings to take on qualities significantly different from those that were present in the original recording.\n\nThe very first EVP recordings may have originated from the use of tape recording equipment with poorly aligned erasure and recording heads, resulting in the incomplete erasure of previous audio recordings on the tape. This could allow a small percentage of previous content to be superimposed or mixed into a new 'silent' recording.\n\nFor all radio transmissions above 30 MHz (which are not reflected by the ionosphere) there is a possibility of meteor reflection of the radio signal. Meteors leave a trail of ionised particles and electrons as they pass through the upper atmosphere (a process called ablation) which reflect transmission radio waves which would usually flow into space. These reflected waves are from transmitters which are below the horizon of the received meteor reflection. In Europe this means the brief scattered wave may carry a foreign voice which can interfere with radio receivers. Meteor reflected radio waves last between 0.05 seconds and 1 second, depending on the size of the meteor.\n\nThere are a number of organizations dedicated to studying EVP and instrumental transcommunication, or which otherwise express interest in the subject. Individuals within these organizations may participate in investigations, author books or journal articles, deliver presentations, and hold conferences where they share experiences. In addition organizations exist which dispute the validity of the phenomena on scientific grounds.\n\nThe Association TransCommunication (ATransC), formerly the American Association of Electronic Voice Phenomena (AA-EVP), and the International Ghost Hunters Society conduct ongoing investigations of EVP and ITC including collecting examples of purported EVP available over the internet. The Rorschach Audio Project, initiated by sound artist Joe Banks, which presents EVP as a product of radio interference combined with auditory pareidolia and the Interdisciplinary Laboratory for Biopsychocybernetics Research, a non-profit organization dedicated to studying anomalous phenomena related to neurophysiological conditions. According to the AA-EVP it is \"the only organized group of researchers we know of specializing in the study of ITC\".\n\nParapsychologists and Spiritualists have an ongoing interest in EVP. Many Spiritualists experiment with a variety of techniques for spirit communication which they believe provide evidence of the continuation of life. According to the National Spiritualist Association of Churches, \"An important modern day development in mediumship is spirit communications via an electronic device. This is most commonly known as Electronic Voice Phenomena (EVP)\". An informal survey by the organization's Department Of Phenomenal Evidence cites that 1/3 of churches conduct sessions in which participants seek to communicate with spirit entities using EVP.\n\nThe James Randi Educational Foundation offers a million dollars for proof that any phenomena, including EVP, are caused paranormally.\n\nThe concept of EVP has influenced popular culture. It is popular as an entertaining pursuit, as in ghost hunting, and as a means of dealing with grief. It has influenced literature, radio, film, television, and music.\n\nInvestigation of EVP is the subject of hundreds of regional and national groups and Internet message boards. Paranormal investigator John Zaffis claims, \"There's been a boom in ghost hunting ever since the Internet took off.\" Investigators, equipped with electronic gear—like EMF meters, video cameras, and audio recorders—scour reportedly haunted venues, trying to uncover visual and audio evidence of ghosts. Many use portable recording devices in an attempt to capture EVP.\n\nFilms involving EVP include \"Poltergeist\", \"The Sixth Sense\", \"White Noise\", and \"The Changeling\"and \"Trace\"(2015)https://www.imdb.com/title/tt3733618/?ref_=nv_sr_8. It has also been featured on television series like \"Ghost Whisperer\", \"The Omega Factor\", \"A Haunting\", \"Ghost Hunters\", \"MonsterQuest\", \"Ghost Adventures\", \"The Secret Saturdays\", \"\", \"Supernatural\", \"Derren Brown Investigates\", and \"Ghost Lab\".\n\n\"Coast To Coast AM\" hosts George Noory and Art Bell have explored the topic of EVP with featured guests such as Brendan Cook and Barbara McBeath of the Ghost Investigators Society, and paranormal investigator and 'demonologist' Lou Gentile. \"The Spirit of John Lennon\", a pay-per-view seance broadcast in 2006, in which TV crew members, a psychic, and an \"expert in paranormal activity\" claim the spirit of former Beatle John Lennon made contact with them through what was described as \"an Electronic Voice Phenomenon (EVP).\"\n\n\"Legion\", a 1983 novel by William Peter Blatty, contains a subplot where Dr. Vincent Amfortas, a terminally ill neurologist, leaves a \"to-be-opened-upon-my-death\" letter for Lt. Kinderman detailing his accounts of contact with the dead, including the doctor's recently deceased wife, Ann, through EVP recordings. Amfortas' character and the EVP subplot do not appear in the film version of the novel, \"The Exorcist III\", although in Kinderman's dream dead people are seen trying to communicate with the living by radio.\n\nIn \"Pattern Recognition\", a 2003 novel by William Gibson, the main character's mother tries to convince her that her father is communicating with her from recordings after his death/disappearance in the September 11, 2001 attacks.'\n\nIn \"Nyctivoe\" a 2001 vampire-inspired play by Dimitris Lyacos the male character as well as his deceased companion are speaking from a recording device amidst a static/white noise background.\n\nIn \"With the people from the bridge\", a 2014 play by Dimitris Lyacos based on the idea of the return of the dead, the voice of the female character NCTV is transmitted from a television monitor amidst a static/white noise background.\n\nEVP is the subject of Vyktoria Pratt Keating's song \"Disembodied Voices on Tape\" from her 2003 album \"Things that Fall from the Sky\", produced by Andrew Giddings of Jethro Tull.\n\nLaurie Anderson's \"Example #22\", from her 1981 album \"Big Science,\" interposes spoken sentences and phrases in German with sung passages in English representing EVP.\n\nDuring the outro to \"Rubber Ring\" by The Smiths, a sample from an EVP recording is repeated. The phrase \"You are sleeping, you do not want to believe,\" is a 'translation' of the 'spirit voices' from a 1970s flexitape. The original recording is from the 1971 record which accompanied Raudive's book 'Breakthrough', and which was re-issued as a flexi-disc in the 1980s free with The Unexplained magazine.\n\nBass Communion's 2004 album Ghosts on Magnetic Tape was inspired by EVP.\n\nThe band Giles Corey, founded by Dan Barrett composed a song called 'Empty Churches' which features track 2 called 'Raymond Cass', track 36 called 'Justified Theft' and track 38 called 'Tramping' from the album An Introduction to EVP by The Ghost Orchid which features excerpts from different EVP experiments produced by many researchers, although most are unknown, some have been pointed out to be more known researchers who studied EVP recordings including Friedrich Jurgenson, Raymond Cass and Konstantin Raudive.\n\nIn 2017 in Poland was published music cd ′′′Katharsis (A Small Victory)′′′ of Teatr Tworzenia by Jarosław Pijarowski with background recorded used EVP recordings (second track - „Katharsis – Pandemonium”).\n\n"}
{"id": "1059396", "url": "https://en.wikipedia.org/wiki?curid=1059396", "title": "Emotional Freedom Techniques", "text": "Emotional Freedom Techniques\n\nEmotional Freedom Techniques (EFT) is a form of counseling intervention that draws on various theories of alternative medicine including acupuncture, neuro-linguistic programming, energy medicine, and Thought Field Therapy (TFT). It is best known through Gary Craig's \"EFT Handbook\", published in the late 1990s, and related books and workshops by a variety of teachers. EFT and similar techniques are often discussed under the umbrella term \"energy psychology\".\n\nAdvocates claim that the technique may be used to treat a wide variety of physical and psychological disorders, and as a simple form of self-administered therapy. The \"Skeptical Inquirer\" describes the foundations of EFT as \"a hodgepodge of concepts derived from a variety of sources, [primarily] the ancient Chinese philosophy of chi, which is thought to be the 'life force' that flows throughout the body.\" The existence of this life force is \"not empirically supported\".\n\nEFT has no benefit as a therapy beyond the placebo effect or any known-effective psychological techniques that may be provided in addition to the purported \"energy\" technique. It is generally characterized as pseudoscience and it has not garnered significant support in clinical psychology.\n\nDuring a typical EFT session, the person will focus on a specific issue while tapping on \"end points of the body's energy meridians\".\n\nAccording to the EFT manual, the procedure consists of the participant rating the emotional intensity of their reaction on a Subjective Units of Distress Scale (SUDS) (a Likert scale for subjective measures of distress, calibrated 0-10) then repeating an orienting affirmation while rubbing or tapping specific points on the body. Some practitioners incorporate eye movements or other tasks. The emotional intensity is then rescored and repeated until no changes are noted in the emotional intensity.\n\nProponents of EFT and other similar treatments believe that tapping/stimulating acupuncture points provide the basis for significant improvement in psychological problems. However, the theory and mechanisms underlying the supposed effectiveness of EFT have \"no evidentiary support\" \"in the entire history of the sciences of biology, anatomy, physiology, neurology, physics, or psychology.\" Researchers have described the theoretical model for EFT as \"frankly bizarre\" and \"pseudoscientific.\" One review noted that one of the highest quality studies found no evidence that the location of tapping points made any difference, and attributed effects to well-known psychological mechanisms, including distraction and breathing therapy.\n\nAn article in the \"Skeptical Inquirer\" argued that there is no plausible mechanism to explain how the specifics of EFT could add to its effectiveness, and they have been described as unfalsifiable and therefore pseudoscientific. Evidence has not been found for the existence of meridians.\n\nEFT has no useful effect as a therapy beyond the placebo effect or any known-effective psychological techniques that may be used with the purported \"energy\" technique, but proponents of EFT have published material claiming otherwise. Their work, however, is flawed and hence unreliable: high-quality research has never confirmed that EFT is effective.\n\nA 2009 review found \"methodological flaws\" in research studies that had reported \"small successes\" for EFT and the related Tapas Acupressure Technique. The review concluded that positive results may be \"attributable to well-known cognitive and behavioral techniques that are included with the energy manipulation. Psychologists and researchers should be wary of using such techniques, and make efforts to inform the public about the ill effects of therapies that advertise miraculous claims.\"\n\nA 2016 systematic review found that EFT was effective in reducing anxiety compared to controls, but also called for more research comparing its effectiveness to that of established treatments.\n\nA Delphi poll of an expert panel of psychologists rated EFT on a scale describing how discredited EFT has been in the field of psychology. On average, this panel found EFT had a score of 3.8 on a scale from 1.0 to 5.0, with 3.0 meaning \"possibly discredited\" and a 4.0 meaning \"probably discredited.\" A book examining pseudoscientific practices in psychology characterized EFT as one of a number of \"fringe psychotherapeutic practices\", and a psychiatry handbook states EFT has \"all the hallmarks of pseudoscience\".\n\nEFT, along with its predecessor, Thought Field Therapy, has been dismissed with warnings to avoid their use by publications such as \"The Skeptic's Dictionary\" and Quackwatch.\n\nProponents of EFT and other energy psychology therapies have been \"particularly interested\" in seeking \"scientific credibility\" despite the implausible proposed mechanisms for EFT. A 2008 review by energy psychology proponent David Feinstein concluded that energy psychology was a potential \"rapid and potent treatment for a range of psychological conditions.\" However, this work by Feinstein has been widely criticized. One review criticized Feinstein's methodology, noting he ignored several research papers that did not show positive effects of EFT, and that Feinstein did not disclose his conflict of interest as an owner of a website that sells energy psychology products such as books and seminars, contrary to the best practices of research publication. Another review criticized Feinstein's conclusion, which was based on research of weak quality and instead concluded that any positive effects of EFT are due to the more traditional psychological techniques rather than any putative \"energy\" manipulation. A book published on the subject of evidence-based treatment of substance abuse called Feinstein's review \"incomplete and misleading\" and an example of a poorly performed evidence-based review of research.\n\nFeinstein published another review in 2012, concluding that energy psychology techniques \"consistently demonstrated strong effect sizes and other positive statistical results that far exceed chance after relatively few treatment sessions\". This review was also criticized, where again it was noted that Feinstein dismissed higher quality studies which showed no effects of EFT, in favor of methodologically weaker studies which did show a positive effect.\n"}
{"id": "8157994", "url": "https://en.wikipedia.org/wiki?curid=8157994", "title": "Energy field disturbance", "text": "Energy field disturbance\n\nEnergy field disturbance is a pseudoscientific concept rooted in alternative medicine. Supporters of this concept believe it concerns the disruptance of a metaphysical biofield that permeates the body, resulting in poor emotional or physiological health. This concept is often related to therapeutic touch.\n\nThe North American Nursing Diagnosis Association (NANDA) previously recognized the diagnosis \"Disturbed Energy Field\" in 1994, prior to implementation of a rule requiring a minimum requirement by evidence in the literature prior to accepting a new diagnosis. Later, NANDA has reported it received feedback questioning the validity of the diagnosis, including criticism from skeptic James Randi. Based on this feedback, NANDA said it would reevaluate this diagnosis based on current scientific evidence.\n\n\"Disturbed Energy Field\" was removed from the NANDA taxonomy in the 10th edition of \"Nursing Diagnoses: Definitions & Classification 2015-2017\", with the explanation that \"...all literature support currently provided for this diagnosis is regarding intervention rather than for the nursing diagnosis itself\" (p. 13).\n\nThe alleged benefits of therapeutic touch are not supported by any scientific evidence.\n\n"}
{"id": "11527579", "url": "https://en.wikipedia.org/wiki?curid=11527579", "title": "Expert elicitation", "text": "Expert elicitation\n\nIn science, engineering, and research, expert elicitation is the synthesis of opinions of authorities of a subject where there is uncertainty due to insufficient data or when such data is unattainable because of physical constraints or lack of resources. Expert elicitation is essentially a scientific consensus methodology. It is often used in the study of rare events. Expert elicitation allows for parametrization, an \"educated guess\", for the respective topic under study. Expert elicitation generally quantifies uncertainty.\n\nExpert elicitation tends to be multidisciplinary as well as interdisciplinary, with practically universal applicability, and is used in a broad range of fields. Prominent recent expert elicitation applications include climate change, modeling seismic hazard and damage, association of tornado damage to wind speed in developing the Enhanced Fujita scale, risk analysis for nuclear waste storage.\n\nIn performing expert elicitation certain factors need to be taken into consideration. The topic must be one for which there are people who have predictive expertise. Furthermore, the objective should be to obtain an experts' carefully considered judgment based on a systematic consideration of all relevant evidence. For this reason one should take care to adopt strategies designed to help the expert being interviewed to avoid overlooking relevant evidence. Additionally, vocabulary used should face intense scrutiny; qualitative uncertainty words such as \"likely\" and \"unlikely\" are not sufficient and can lead to confusion. Such words can mean very different things to different people, or to the same people in different situations.\n\n\n"}
{"id": "45254767", "url": "https://en.wikipedia.org/wiki?curid=45254767", "title": "Expression Atlas", "text": "Expression Atlas\n\nThe Expression Atlas is a database that provides information on gene expression patterns. The Expression Atlas allows searches by gene, splice variant and protein attribute. Individual genes or gene sets can be searched for. There are two components to the Expression Atlas, the Baseline Atlas and the Differential Atlas:\n\nThe Baseline Atlas provides information about which gene products are present (and at what abundance) under \"normal\" conditions. This component of the Expression Atlas consists of highly curated and quality-checked RNA-seq experiments from ArrayExpress. It aims to answer questions such as:\n\n\nThe Differential Atlas allows users to identify genes that are up- or down-regulated in different experimental conditions.\n\n\n\n"}
{"id": "53613989", "url": "https://en.wikipedia.org/wiki?curid=53613989", "title": "Homoeomeria (philosophy)", "text": "Homoeomeria (philosophy)\n\nHomoeomeria was a doctrine in the philosophy of the ancient Greek Anaxagoras, as claimed by the Roman atomist Lucretius. It was assumed that the atoms constituting a substance must themselves have the salient observed properties of that substance: so atoms of water would be wet, atoms of iron would be hard, atoms of wool would be soft, etc. This doctrine depends on the fallacy of division.\n\nProfessor Fleeming Jenkin wrote that: \"we may with the exercise of a good deal of fancy see in the doctrine of homoeomeria, which taught that all things contained the materials of everything else in a latent state, a foreshadowing of the chemical theory which proves that our bodies are made of the same chemical materials as peas, cabbages, &c., but it requires an elastic imagination to link the old and new creed together.\"\n"}
{"id": "39952681", "url": "https://en.wikipedia.org/wiki?curid=39952681", "title": "John Desmond Bernal Prize", "text": "John Desmond Bernal Prize\n\nThe John Desmond Bernal Prize is an award given annually by the Society for Social Studies of Science (4S) to scholars judged to have made a distinguished contribution to the field of Science and Technology Studies (STS). The award was launched in 1981, with the support of Eugene Garfield.\n\nThe award is named after the scientist John Desmond Bernal.\n\nSource: Society for Social Studies of Science\n\n"}
{"id": "18010608", "url": "https://en.wikipedia.org/wiki?curid=18010608", "title": "John Wilbanks", "text": "John Wilbanks\n\nJohn Wilbanks is the chief commons officer at Sage Bionetworks and a senior fellow at the Ewing Marion Kauffman Foundation and at FasterCures. He runs the Consent to Research Project.\n\nWilbanks grew up in Knoxville, Tennessee, US. He attended Tulane University and received a Bachelor of Arts in philosophy in 1994. He also studied modern letters at the Sorbonne in Paris.\n\nFrom 1994 to 1997, he worked in Washington, DC as a legislative aide to Congressman Fortney \"Pete\" Stark. During this time Wilbanks was also a grassroots coordinator and fundraiser for the American Physical Therapy Association. Wilbanks was the Berkman Center for Internet & Society's first assistant director from the fall of 1998 to the summer of 2000. There he led efforts in software development and Internet-mediated learning, and was involved in the Berkman Center's work on ICANN.\n\nWhile at the Berkman Center, Wilbanks founded Incellico, Inc., a bioinformatics company that built semantic graph networks for use in pharmaceutical research and development. He served as President and CEO, and led to the company's acquisition in the summer of 2003. He has also served as a Fellow at the World Wide Web Consortium on Semantic Web for Life Sciences, was a Visiting Scientist in the Project on Mathematics and Computation at MIT, and was a member of the National Advisory Committee for PubMed Central. He is a member of the Board of Directors for Sage Bionetworks and on the advisory boards of Genomera, Genomic Arts, and Boundless Learning. He is an original author of the Panton Principles for sharing data. \n\nWilbanks led a We the People petition supporting the free access of taxpayer-funded research data, which gained over 65,000 signatures. In February 2013, the White House responded, detailing a plan to freely publicize taxpayer-funded research data.\n\nConsent to Research (CtR) is a project that provides a platform for people to donate their health data for the purposes of scientific research and the advancement of medicine. Since health data is restricted and expensive, this project provides people the opportunity to freely donate information that can only positively benefit medicine and patients at large. Consent to Research is connected to the Access2Research project, which aims to free access over the Internet to scientific journal articles that are already taxpayer-funded. Wilbanks founded the project in 2011 and gave a TED Global talk about the project in 2012.\n\nWilbanks worked at Science Commons and Creative Commons from October 2004 to September 2011. As vice president of science he ran the Science Commons project for its five-year lifetime and continued to work on science after he joined the core Creative Commons organization. He has been interviewed by Popular Science magazine, KRUU Radio, and BioMed Central to discuss Science Commons.\n\n\"Scientific American\" featured Wilbanks in \"The Machine That Would Predict The Future\" in 2011. Seed magazine named Wilbanks among their Revolutionary Minds of 2008, as a \"Game Changer\" and the Utne Reader named him in 2009 as one of \"50 visionaries who are changing your world\". He frequently campaigns for wider adoption of open access publishing in science and the increased sharing of data by scientists.\n\n"}
{"id": "6956679", "url": "https://en.wikipedia.org/wiki?curid=6956679", "title": "Kvant (magazine)", "text": "Kvant (magazine)\n\nKvant ( for \"quantum\") is a popular science magazine in physics and mathematics for school students and teachers, issued since 1970 in Soviet Union and continued in Russia. Translation of selected articles from \"Kvant\" had been published in \"Quantum Magazine\" in 1990–2001, which in turn had been translated and published in Greece in 1994–2001. \n\nKvant was started as a joint project of the USSR Academy of Sciences and USSR Academy of Pedagogical Sciences. In Soviet time, it was published by Nauka publisher with circulation about 200,000.\n\nThe idea of the magazine was introduced by Pyotr Kapitsa. Its first chief editors were physicist Isaak Kikoin and mathematician Andrei Kolmogorov. In 1985, its editorial board had 18 Academicians and Corresponding Members of the USSR Academy of Sciences and USSR Academy of Pedagogical Sciences, 14 Doctors of Sciences and 20 Candidates of Science.\n\nAll published issues of \"Kvant\" are freely available online.\n\n\"Quantum Magazine\" was a US-based bimonthly magazine published by the National Science Teachers Association (NSTA) from 1990 to 2001. Some of its articles were translations from \"Kvant\".\n\nIn 1999, American Mathematical Society published translation of selected articles from Kvant on algebra and mathematical analysis as two volumes in the \"Mathematical World\" series. Yet another volume, published in 2002, included translation of selected articles on combinatorics. \n\nThere were two books with selected articles from Kvant published in France.\n\n"}
{"id": "27774511", "url": "https://en.wikipedia.org/wiki?curid=27774511", "title": "Lakes District Technocity", "text": "Lakes District Technocity\n\nThe Lakes District Technocity(established in 2004) is a science park located on the campus of Süleyman Demirel University. The technocity is a full member of International Association of Science Parks. The science park occupies 112000 squaremeters area.\n\nThe activities of the technocity include Energy and Renewable Energy, Internet Technology and Services / E-Business, Plasma Technology, and Environment Technologies. \n\nAs of 2010, 57% of firms on the tecnocity involved in the area of computer software industry.\n\nThe technocity made its first export(plasma sensors to Saudi Arabia) in 2008.\n\nThe following entities are shareholders of the park:\n\n"}
{"id": "18576196", "url": "https://en.wikipedia.org/wiki?curid=18576196", "title": "Lariat chain", "text": "Lariat chain\n\nA Lariat chain is a loop of chain that hangs off, and is spun by a wheel. It is often used as a science exhibit or a toy. \n\nThe original Lariat Chain was created in 1986 by Norman Tuck, as an Artist-in-Residence project at the Exploratorium in San Francisco. \n\nLariat Chain was developed from an earlier Tuck piece entitled Chain Reaction (1984). Chain Reaction was hand cranked, and utilized a heavy chain attached by magnets onto an iron flywheel. As in Lariat Chain, Chain Reaction used a brush to disrupt the motion of the traveling chain. \n\nThe speed of the chain is arranged to equal the wave speed of transverse waves, so that waves moving against the motion of the chain appear to be standing still.\n\n\n"}
{"id": "54251016", "url": "https://en.wikipedia.org/wiki?curid=54251016", "title": "List of academic fields", "text": "List of academic fields\n\nThe following outline is provided as an overview of an topical guide to academic disciplines:\n\nAn academic discipline or field of study is known as a branch of knowledge. It is taught as an accredited part of higher education. A scholar's discipline is commonly defined and recognized by a university faculties. That person will be accredited by learned societies to which he or she belongs along with the academic journals in which he or she publishes. However, no formal criteria exist for defining an academic discipline.\n\nDisciplines varies between universities and programs. These discipline will have well-defined rosters of journals and conferences supported by a few universities and publications. A discipline may have branches, that are called sub-disciplines.\n\nThere is no consensus on how some academic disciplines should be classified (e.g., whether anthropology and linguistics are disciplines of social sciences or fields within the humanities). More generally, the proper criteria for organizing knowledge into disciplines are also open to debate.\n\nHistory of academic disciplines – academic disciplines arose from learning institutions as those grew to include specialized faculties or departments \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\"Also a branch of electrical engineering\"\n\n\n\n\n\n\n\n\n\n\n\nChemical Engineering\n\nCivil Engineering\n\nElectrical Engineering\nMaterials Science and Engineering\n\nMechanical Engineering\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "8425007", "url": "https://en.wikipedia.org/wiki?curid=8425007", "title": "List of organizations for women in science", "text": "List of organizations for women in science\n\nThis is a list of organizations for women in science and, more generally; science, technology, engineering, and math.\n\n\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "49623108", "url": "https://en.wikipedia.org/wiki?curid=49623108", "title": "List of rectores magnifici of Delft University of Technology", "text": "List of rectores magnifici of Delft University of Technology\n\nA rector of a Dutch university is called a \"rector magnificus\". The following people have been rector magnificus of Delft University of Technology or its predecessor, Technische Hogeschool Delft:\n"}
{"id": "49681713", "url": "https://en.wikipedia.org/wiki?curid=49681713", "title": "List of rectores magnifici of Leiden University", "text": "List of rectores magnifici of Leiden University\n\nThis is a list of chancellors (\"rectores magnifici\") of Leiden University, as from 1575. Three Nobel laureates are among these chancellors: Hendrik Lorentz, Heike Kamerlingh Onnes and Willem Einthoven.\n\n\n\n"}
{"id": "53941711", "url": "https://en.wikipedia.org/wiki?curid=53941711", "title": "Little Science, Big Science", "text": "Little Science, Big Science\n\nLittle Science, Big Science is a book of collected lectures given by Derek J. De Solla Price, first published in 1963. The book presents the 1962 Brookhaven National Laboratory Pegram Lectures, a series of lectures dedicated to discussing science and its place in society. Price's goal in the lectures is to outline what it may look like for science to be analysed scientifically, by applying methods of measuring, hypothesizing, and deriving to science itself. With this goal in mind, he sets out to define quasi-mathematically how the shape and size of science has shifted from \"small science\" to \"big science\" in a historical and sociological way. Price presents a quantification of science as a measurable entity via an analogy to thermodynamics, conceptualizing science like a gas with individual molecules possessing individual velocities and interactions, a total volume, and general properties or laws.\n\nPrice begins the lectures by setting forth a demarcation in science centered around the modern period. He describes the phenomenon that, at the time of the lectures, 80 to 90 percent of important scientific work had occurred in one normal human life span. With this facet in mind, he sets out to describe the development of the term \"Big Science,\" as coined by Alvin M. Weinberg in 1961. As a general directive, he seeks to show that the transition from \"Little Science\" to \"Big Science,\" specifically the socio-economic and methodological changes to science in the 20th century, have been mostly gradual. To illustrate this point, he presents empirical statistical evidence from various aspects and fields of science, all of which show that the mode of growth of science is exponential, growing at compound interest. This assertion Price claims is the \"fundamental law of any analysis of science,\" stating that it even holds accurately over long time periods. With this fundamental law in mind, he states that for general measures the size of science in manpower or number of publications doubles in size every 10 to 15 years. If this rate of expansion is considered broadly, then from the 1600s until now such size measures of science have increased by a factor of 10. From this observation, Price moves to describe the \"coefficient of immediacy:\" the number of scientists alive compared to the number of scientists who have ever been, a ratio or percentage he states as 7:8 and 87.5% respectively. This measure serves to show numerically how the majority of important science has taken place within the average human life span at the time of the lecture presentation. As a result of the consistent exponential growth rate and immediacy of science, the statement that the majority of scientists throughout history are alive at any given moment must be consistent throughout history as well, meaning that in 1700 the majority of all scientists ever were alive, true also for 1800 and 1900 and so on. As a result of this facet, Price states that science has been constantly exploding into the population, increasing its size at a rate faster than the increase of total humans able to conduct it.\n\nHowever, Price asserts that this exponential growth rate cannot simply explain the transition from \"Little Science\" to \"Big Science,\" as the constant growth would not make the modern period under question any more likely to produce \"Big Science\" than any other. He conjectures that two statistical phenomena hold true for science generally, that individual metrics of science may grow at rates different from that of the exponential growth, and that the exponential growth rate may be starting to diminish. In response to his second point, he claims that the normal exponential growth may give way to a logistic growth rate, growing exponentially until it reaches a maximum size and then ceasing to grow. The possibility that science follows a rate of growth modeled by a logistic curve is suggested further by the fact that if science had continued to grow at an exponential rate in 1962, then by now there would be more scientists than people. With his claim that the growth rate actually observes a logistic curve, he provides a second basic law of the analysis of science, namely that the exponential growth rates previously mentioned must be in fact logistic. If this claim is correct, then the exponential growth rate previously observed must break down at a point in the future, and Price implies as a conclusion to this section that the onset of this breakdown may be associated with an upper bound to the size of science brought on by \"Big Science.\"\n\nIn this chapter, Price suggests various ideas and methods about conducting a science of science, or scientometrics, by first narrating some peculiar contributions to statistics made by Francis Galton. His overall goal is to further the possibility of applying scientific methods to science itself by suggesting various metrics and measures of the size, growth rate, and distribution of science. He focuses on Galton's work concerning the distribution of high achieving scientists and statesmen in the upper echelons of British society, specifically \"Hereditary Genius\" and \"English Men of Science\". These works are reviewed with the goal of understanding a basic metric for the number of people or papers in science that reach different levels of quality, an idea basic in Price's formulation of scientometrics. Further, he suggests that understanding such a metric would allow predictions to be made of science and scientists when changes associated with Big Science arrive. Galton's original approach was to estimate the distribution of high achieving practitioners of science among the eminent parts of British society, and Price takes this as a starting step in grasping a scientific metric of the productivity of science. In analyzing Galton's work and the work of another statistics researcher, Alfred J. Lotka, Price suggests that there may be a rough inverse-square law of productivity. Price moves next to define a quantity he calls someone's \"solidness\" \"s\", as the logarithm of the total papers published in one scientist's life. Keeping in mind the previous productivity law, for each unit increase in a scientist's solidness, the total number of scientists of that solidness decreases at a constant rate. With these two observations, among others, Price asserts that the foundations for an econometric-like study of science have been suggested, with the analysis of time series suggesting exponential or logistic growth and the distribution law of scientific productivity comprising them. He concludes by suggesting that these distributions and analyses contain errors relating to the non-uniform distribution of scientists across populations, noting that they tend to congregate in certain fields, institutions, countries, and journals. In keeping with his gas analogy, he maintains that just as one cannot measure the exact positions and velocities of gas molecules, one cannot pinpoint the exact productivity or contribution levels of individual scientists within science.\n\nThis chapter serves multiple purposes but overall achieves the same goal as the previous, providing a further conception of the productivity measure in science. This conclusion is reached through defining historically, sociologically, and from a communications perspective what a scientific paper is for, specifically what the purpose of this form of scientific communication is. To begin this analysis, he begins by looking at the history of the scientific paper, tracing its original purpose to discovering what was of interest within scientific practice. With the emergence of this scientific social practice, seen not as a means of publishing new knowledge but of communication between practitioners, the process of situating papers within the general body of literature came in to play. Specifically, each scientific paper is built from the foundation created by all previous papers, and with this facet exists a possibility of quantifying this foundation, the citation of references. With the idea that scientific papers were a social device of scientific communication, Price suggests that the driving force behind their emergent usage was the ability to assert and claim intellectual property within science. The possibility of communicating priority in disputes over scientific discoveries promoted the scientific paper as the best means of communication, leaving the information dissemination quality of papers as incidental in their overall purpose. With the quantification of scientific productivity by citation number and rate, there arrives a metric in science that gives the scientific importance of an individual's work or journal as its total usage within scientific practice, its total citations or references in other papers or journals. With this in mind, Price observes the fact that the total number of scientific references at a specific date across science is proportional to the total literature available within science at that date.\n\nMoving from the ability of scientific papers to facilitate communication and interactions between scientists, Price outlines an idea that allows further maximization of interactions between scientists. His term for this organizational method is the \"invisible college,\" specifically the circuit of institutions, research centers, journals, and conferences that allow intermingling and interactions within specific fields of science. Groups of scientists naturally form as a result of collaborations between individuals focusing on similar problems, but the ability for researchers to move around the globe in order to achieve interpersonal relationships with their fellow researchers is what Price suggests maximizes the group size able to keep up regular productive interactions. Thus Price defines the sociological structure of scientific practice communicating through published papers.\n\nThe final section of the lectures focuses on a larger-picture analysis of science and the monetary trends within it. As a general first statement, Price proposes that the cost of science has been increasing proportional to the square of the number of scientists. He points out that the cost of research in terms of the GDP did not increase in the years preceding World War II, yet afterward began increasing at the rate previously mentioned. As research amounts increase, the current and necessary number of researchers increases, promoting the inducement of scientists with higher salaries and better facilities in turn increasing the overall costs of science. Price suggests that it is this feedback loop that is a potential decelerator for the growth of science, and the main difference between Little Science and Big Science. What follows is his analysis of the \"explosion of science\" within non-developed countries, specifically Japan. He shows through this analysis that the United States' lack of experience of this explosion of science within the 20th century up to this point is due to the saturation of society with the activities of science, nearing costs not maintainable by the country. In countries where science has yet reached an exponential growth curve, this saturation is not present which allows the growth rate to set out at an exponential pace.\n\nThe final conceptual measure that Price offers is the idea of the \"mavericity\" of a scientist, or the likelihood that an individual will test new and unique combinations of theories and experiments unexpected in the current literature. The reactions and interactions within science to this mavericity also characterizes Big Science over Little Science, where the former serves to limit and restrain the most maverick investigators due to collaborative work and specific directed goals for scientific research. Thus the emergence of Big Science not only influences the growth rate, connectedness, and significance of science, but also the individual facets of the scientific pursuit.\n"}
{"id": "3406801", "url": "https://en.wikipedia.org/wiki?curid=3406801", "title": "Mermaid Theatre", "text": "Mermaid Theatre\n\nThe Mermaid Theatre was a theatre encompassing the site of Puddle Dock and Curriers' Alley at Blackfriars in the City of London, and the first built in the City since the time of Shakespeare. It was, importantly, also one of the first new theatres to abandon the traditional stage layout; instead a single tier of seats surrounded the stage on three sides.\n\nThe 20th-century theatre was the life's work of actor Bernard Miles with his wife, Josephine Wilson. His original Mermaid Theatre was a large barn at his house in the St. John's Wood area of London. This seated 200 people, and during 1951 and 1952 was used for concerts, plays and a celebrated opera production of \"Dido and Aeneas\" with Kirsten Flagstad, Maggie Teyte and Thomas Hemsley, conducted by Geraint Jones, which was recorded by HMV. For the third season in 1953 the Mermaid Theatre was moved to the Royal Exchange.\n\nMiles was encouraged to build a permanent theatre and, raising money from public subscriptions, and his revenues from publicity spots for the Egg Marketing Board; he oversaw the creation of the new building on land formerly occupied by a warehouse. This site was close to the location of an abortive attempt, in the Jacobean era, to build a theatre (named Porter's Hall) for the amalgamation of the Children of the Queen's Revels and Lady Elizabeth's Men. This project, undertaken by Philip Rosseter with distant backing from Henslowe and Alleyn, was ended because of complaints from the neighbourhood's residents.\n\nThe new Mermaid Theatre opened in 1959 with a successful production of \"Lock Up Your Daughters\" and it was the venue for many other very successful productions, such as \"Cowardy Custard\" (often cited as responsible for the revival of interest in Noël Coward's works) and including an annual staging of \"Treasure Island\", with Miles reprising his role of Long John Silver, which he also played in a television version. The Mermaid Theatre also ran the Molecule Club, educating children about science.\n\nIn July 1961 the poet and author Sylvia Plath read her poem \"Tulips\" at the Poetry at the Mermaid Festival, sponsored by the Arts Council of Great Britain. The programme notes that there were twelve commissioned poets at the festival, one of whom was Plath's husband, Ted Hughes.\n\nOther notable productions include the 1978 première of \"Whose Life Is It Anyway?\", with Tom Conti and Rona Anderson.. The Royal Shakespeare Company sometimes transferred Stratford productions to the Mermaid, including a residency during 1987 which saw the staging of seven plays.\n\nGomba Holdings, a property company owned by Ugandan Asian businessman Abdul Shamji and his family, which claimed to have interests in the Garrick and Duchess theatres as well as Wembley Stadium, bought the theatre in the mid-1980s in the hope of redeveloping the Puddle Dock site. Bernard Miles' tenure as honorary artistic advisor was abruptly terminated and the theatre's importance declined. In 1989 Abdul Shamji was sentenced to 15-months in prison over his involvement in the Johnson Matthey bank collapse. Josephine Wilson died in 1990 and Bernard (by then Lord) Miles died in 1991, financially destitute.\n\nMarc Sinden was appointed artistic director in 1993, opening the Bernard Miles Studio as a second performance area, but left the next year. Actor Roy Marsden and Vanessa Ford took over the running of the theatre for a few months prior to its eventual closure and the termination of the Shamji family's ownership.\n\nAfter a further change of ownership the theatre was slated for demolition in 2002 as part of redevelopment plans. Already it had fallen into disuse, the buildings being used more often as a conference centre than a theatre. A preservation campaign by actors and other supporters attempted to reverse the decision. In April 2003 Ken Livingstone, the Mayor of London, ordered the council to block the demolition. As of March 2005 new plans had been submitted for the redevelopment of the site. Nothing materialised and the building continued to operate primarily as a conference centre. The BBC Concert Orchestra used it for occasional concerts, and the BBC recorded a popular weekly radio show, \"Friday Night is Music Night\" that showcased musicians such as the violinist Nigel Kennedy and singer Josh Groban. In 2006, music duo Pet Shop Boys played a mid-length set accompanied by the BBC Concert Orchestra and special guests including Robbie Williams, Francis Barber and Rufus Wainwright which was musically directed by Trevor Horn. The show was documented on the audio release entitled Concrete.\n\nIn September 2008 the Corporation of London City Planning Committee, against the advice of the Theatres Trust and noted actors, producers and artistic directors, granted a certificate that stripped the former playhouse of its theatre status. The move may save the developer £6 million worth of Section 106 funding which it had previously agreed to pay in lieu if it closed the 600-seat Mermaid; the company could be released from the obligation because no theatrical productions have taken place for more than ten years. The existing plans would see the Puddle Dock building converted into a conference centre and fitness suite, plus offices, a nightclub and retail and restaurant space. Campaigners were concerned that the entire building might be demolished. The former chairman of the Save London's Theatres Campaign, John Levitt, called the decision “a tragedy” and “sheer meanness”.\n\nIn 2018 the Mermaid Theatre still exists as a conference centre called The Mermaid London.\n\n"}
{"id": "382251", "url": "https://en.wikipedia.org/wiki?curid=382251", "title": "Natural philosophy", "text": "Natural philosophy\n\nNatural philosophy or philosophy of nature (from Latin \"philosophia naturalis\") was the philosophical study of nature and the physical universe that was dominant before the development of modern science. It is considered to be the precursor of natural science.\n\nFrom the ancient world, starting with Aristotle, to the 19th century, the term \"natural philosophy\" was the common term used to describe the practice of studying nature. It was in the 19th century that the concept of \"science\" received its modern shape with new titles emerging such as \"biology\" and \"biologist\", \"physics\" and \"physicist\" among other technical fields and titles; institutions and communities were founded, and unprecedented applications to and interactions with other aspects of society and culture occurred. Isaac Newton's book \"Philosophiae Naturalis Principia Mathematica\" (1687), whose title translates to \"Mathematical Principles of Natural Philosophy\", reflects the then-current use of the words \"natural philosophy\", akin to \"systematic study of nature\". Even in the 19th century, a treatise by Lord Kelvin and Peter Guthrie Tait, which helped define much of modern physics, was titled \"Treatise on Natural Philosophy\" (1867).\n\nIn the German tradition, \"Naturphilosophie\" (philosophy of nature) persisted into the 18th and 19th century as an attempt to achieve a speculative unity of nature and spirit. Some of the greatest names in German philosophy are associated with this movement, including Goethe, Hegel and Schelling. \"Naturphilosophie\" was associated with Romanticism and a view that regarded the natural world as a kind of giant organism, as opposed to the philosophical approach of figures such as John Locke and Isaac Newton who espoused a more mechanical view of the world, regarding it as being like a machine.\n\nThe term \"natural philosophy\" preceded our current \"natural science\" (i.e. empirical science). Empirical science historically developed out of philosophy or, more specifically, natural philosophy. Natural philosophy was distinguished from the other precursor of modern science, natural history, in that natural philosophy involved reasoning and explanations about nature (and after Galileo, quantitative reasoning), whereas natural history was essentially qualitative and descriptive.\n\nIn the 14th and 15th centuries, natural philosophy was one of many branches of philosophy, but was not a specialized field of study. The first person appointed as a specialist in Natural Philosophy \"per se\" was Jacopo Zabarella, at the University of Padua in 1577.\n\nModern meanings of the terms \"science\" and \"scientists\" date only to the 19th century. Before that, \"science\" was a synonym for \"knowledge\" or \"study\", in keeping with its Latin origin. The term gained its modern meaning when experimental science and the scientific method became a specialized branch of study apart from natural philosophy.\n\nFrom the mid-19th century, when it became increasingly unusual for scientists to contribute to both physics and chemistry, \"natural philosophy\" came to mean just \"physics\", and the word is still used in that sense in degree titles at the University of Oxford. In general, chairs of Natural Philosophy established long ago at the oldest universities are nowadays occupied mainly by physics professors. Isaac Newton's book \"Philosophiae Naturalis Principia Mathematica\" (1687), whose title translates to \"Mathematical Principles of Natural Philosophy\", reflects the then-current use of the words \"natural philosophy\", akin to \"systematic study of nature\". Even in the 19th century, a treatise by Lord Kelvin and Peter Guthrie Tait, which helped define much of modern physics, was titled \"Treatise on Natural Philosophy\" (1867).\n\nGreek philosophers defined it as the combination of beings living in the universe, ignoring things made by humans. The other definition refers to human nature.\n\nIn Plato's earliest known dialogue, \"Charmides\" distinguishes between \"science\" or bodies of knowledge that produce a physical result, and those that do not. Natural philosophy has been categorized as a theoretical rather than a practical branch of philosophy (like ethics). Sciences that guide arts and draw on the philosophical knowledge of nature may produce practical results, but these subsidiary sciences (e.g., architecture or medicine) go beyond natural philosophy.\n\nThe study of natural philosophy seeks to explore the cosmos by any means necessary to understand the universe. Some ideas presuppose that change is a reality. Although this may seem obvious, there have been some philosophers who have denied the concept of metamorphosis, such as Plato's predecessor Parmenides and later Greek philosopher Sextus Empiricus, and perhaps some Eastern philosophers. George Santayana, in his \"Scepticism and Animal Faith,\" attempted to show that the reality of change cannot be proven. If his reasoning is sound, it follows that to be a physicist, one must restrain one's skepticism enough to trust one's senses, or else rely on anti-realism.\n\nRené Descartes' metaphysical system of mind–body dualism describes two kinds of substance: matter and mind. According to this system, everything that is \"matter\" is deterministic and natural—and so belongs to natural philosophy—and everything that is \"mind\" is volitional and non-natural, and falls outside the domain of philosophy of nature.\n\nMajor branches of natural philosophy include astronomy and cosmology, the study of nature on the grand scale; etiology, the study of (intrinsic and sometimes extrinsic) causes; the study of chance, probability and randomness; the study of elements; the study of the infinite and the unlimited (virtual or actual); the study of matter; mechanics, the study of translation of motion and change; the study of nature or the various sources of actions; the study of natural qualities; the study of physical quantities; the study of relations between physical entities; and the philosophy of space and time. (Adler, 1993)\n\nHumankind's mental engagement with nature certainly predates civilization and the record of history. Philosophical, and specifically non-religious thought about the natural world, goes back to ancient Greece. These lines of thought began before Socrates, who turned from his philosophical studies from speculations about nature to a consideration of man, viz., political philosophy. The thought of early philosophers such Parmenides, Heraclitus, and Democritus centered on the natural world. In addition, three presocratic philosophers who lived in the Ionian town of Miletus (hence the Milesian School of philosophy,) Thales, Anaximander, and Anaximenes, attempted to explain natural phenomena without recourse to creation myths involving the Greek gods. They were called the \"physikoi\" (natural philosophers,) or, as Aristotle referred to them, the \"physiologoi\". Plato followed Socrates in concentrating on man. It was Plato's student, Aristotle, who, in basing his thought on the natural world, returned empiricism to its primary place, while leaving room in the world for man. Martin Heidegger observes that Aristotle was the originator of conception of nature that prevailed in the Middle Ages into the modern era:\n\nAristotle surveyed the thought of his predecessors and conceived of nature in a way that charted a middle course between their excesses.\n\n\"The world we inhabit is an orderly one, in which things generally behave in predictable ways, Aristotle argued, because every natural object has a \"nature\"—an attribute (associated primarily with form) that makes the object behave in its customary fashion...\" Aristotle recommended four causes as appropriate for the business of the natural philosopher, or physicist, “and if he refers his problems back to all of them, he will assign the ‘why’ in the way proper to his science—the matter, the form, the mover, [and] ‘that for the sake of which’”. While the vagaries of the material cause are subject to circumstance, the formal, efficient and final cause often coincide because in natural kinds, the mature form and final cause are one and the same. The capacity to mature into a specimen of one's kind is directly acquired from “the primary source of motion”, i.e., from one's father, whose seed (\"sperma\") conveys the essential nature (common to the species), as a hypothetical \"ratio\".\n\n\nFrom the late Middle Ages into the modern era, the tendency has been to narrow \"science\" to the consideration of efficient or agency-based causes of a particular kind:\n\nMedieval thoughts on motion involved much of Aristotle's works \"Physics\" and \"Metaphysics\". The issue that medieval philosophers had with motion was the inconsistency found between book 3 of \"Physics\" and book 5 of \"Metaphysics\". Aristotle claimed in book 3 of \"Physics\" that motion can be categorized by substance, quantity, quality, and place. where in book 5 of \"Metaphysics\" he stated that motion is a magnitude of quantity. This disputation led to some important questions to natural philosophers: Which category/categories does motion fit into? Is motion the same thing as a terminus? Is motion separate from real things? These questions asked by medieval philosophers tried to classify motion.\n\nWilliam Ockham gives a good concept of motion for many people in the Middle Ages. There is an issue with the vocabulary behind motion which makes people think that there is a correlation between nouns and the qualities that make nouns. Ockham states that this distinction is what will allow people to understand motion, that motion is a property of mobiles, locations, and forms and that is all that is required to define what motion is. A famous example of this is Occam's razor which simplifies vague statements by cutting them into more descriptive examples. \"Every motion derives from an agent.\" becomes \"each thing that is moved, is moved by an agent\" this makes motion a more personal quality referring to individual objects that are moved.\n\nAristotle held many important beliefs that started a convergence of thought for natural philosophy. Aristotle believed that attributes of objects belong to the objects themselves, and share traits with other objects that fit them into a category. He uses the example of dogs to press this point. An individual dog may have very specific attributes (ex. one dog can be black and another brown) but also very general ones that classify it as a dog (ex. four-legged). This philosophy can be applied to many other objects as well. This idea is different than that of Plato, with whom Aristotle had a direct association. Aristotle argued that objects have properties \"form\" and something that is not part of its properties \"matter\" that defines the object. The form cannot be separated from the matter. Given the example that you can not separate properties and matter since this is impossible, you cannot collect properties in a pile and matter in another.\n\nAristotle believed that change was a natural occurrence. He used his philosophy of form and matter to argue that when something changes you change its properties without changing its matter. This change occurs by replacing certain properties with other properties. Since this change is always an intentional alteration whether by forced means or by natural ones, change is a controllable order of qualities. He argues that this happens through three categories of being: non-being, potential being, and actual being. Through these three states the process of changing an object never truly destroys an objects forms during this transition state but rather just blurs the reality between the two states. An example of this could be changing an object from red to blue with a transitional purple phase.\n\nEarly Greek philosophers studied motion and the cosmos. Figures like Hesiod regarded the Natural world as offspring of the gods, whereas others like Leucippus and Democritus regarded the world as lifeless atoms in a vortex. Anaximander deduced that eclipses happen because of apertures in rings of celestial fire. Heraclitus believed that the heavenly bodies were made of fire that were contained within bowls. He thought that eclipses happen when the bowl turned away from the earth. Anaximenes is believed to have stated that an underlying element was air, and by manipulating air someone could change its thickness to create fire, water, dirt, and stones. Empedocles identified the elements that make up the world which he termed the roots of all things as Fire, Air. Earth, and Water. Parmenides argued that all change is a logical impossibility. He gives the example that nothing can go from nonexistence to existence. Plato argues that the world is an imperfect replica of an idea that a divine craftsman once held. He also believed that the only way to truly know something was through reason and logic not the study of the object itself, but that changeable matter is a viable course of study.\n\nThe scientific method has ancient precedents and Galileo exemplifies a mathematical understanding of nature which is the hallmark of modern natural scientists. Galileo proposed that objects falling regardless of their mass would fall at the same rate, as long as the medium they fall in is identical. The 19th-century distinction of a scientific enterprise apart from traditional natural philosophy has its roots in prior centuries. Proposals for a more \"inquisitive\" and practical approach to the study of nature are notable in Francis Bacon, whose ardent convictions did much to popularize his insightful Baconian method. The late 17th-century natural philosopher Robert Boyle wrote a seminal work on the distinction between physics and metaphysics called, \"A Free Enquiry into the Vulgarly Received Notion of Nature\", as well as \"The Skeptical Chymist\", after which the modern science of chemistry is named, (as distinct from proto-scientific studies of alchemy). These works of natural philosophy are representative of a departure from the medieval scholasticism taught in European universities, and anticipate in many ways, the developments which would lead to science as practiced in the modern sense. As Bacon would say, \"vexing nature\" to reveal \"her\" secrets, (scientific experimentation), rather than a mere reliance on largely historical, even anecdotal, observations of empirical phenomena, would come to be regarded as a defining characteristic of modern science, if not the very key to its success. Boyle's biographers, in their emphasis that he laid the foundations of modern chemistry, neglect how steadily he clung to the scholastic sciences in theory, practice and doctrine. However, he meticulously recorded observational detail on practical research, and subsequently advocated not only this practice, but its publication, both for successful and unsuccessful experiments, so as to validate individual claims by replication.\n\nNatural philosophers of the late 17th or early 18th century were sometimes insultingly described as 'projectors'. A projector was an entrepreneur who invited people to invest in his invention but - as the caricature went - could not be trusted, usually because his device was impractical. Jonathan Swift satirized natural philosophers of the Royal Society as 'the academy of projectors' in his novel \"Gulliver's Travels.\" Historians of science have argued that natural philosophers and the so-called projectors sometimes overlapped in their methods and aims.\n\nThe modern emphasis is less on a broad empiricism (one that includes passive observation of nature's activity), but on a narrow conception of the empirical concentrating on the \"control\" exercised through experimental (active) observation for the sake of \"control\" of nature. Nature is reduced to a passive recipient of human activity.\n\nIn the middle of the 20th century, Ernst Mayr's discussions on the teleology of nature brought up issues that were dealt with previously by Aristotle (regarding final cause) and Kant (regarding reflective judgment).\n\nEspecially since the mid-20th-century European crisis, some thinkers argued the importance of looking at nature from a broad philosophical perspective, rather than what they considered a narrowly positivist approach relying implicitly on a hidden, unexamined philosophy. One line of thought grows from the Aristotelian tradition, especially as developed by Thomas Aquinas. Another line springs from Edmund Husserl, especially as expressed in \"The Crisis of European Sciences\". Students of his such as Jacob Klein and Hans Jonas more fully developed his themes. Last, but not least, there is the process philosophy inspired by Alfred North Whitehead's works.\n\nAmong living scholars, Brian David Ellis, Nancy Cartwright, David Oderberg, and John Dupré are some of the more prominent thinkers who can arguably be classed as generally adopting a more open approach to the natural world. Ellis (2002) observes the rise of a \"New Essentialism.\" David Oderberg (2007) takes issue with other philosophers, including Ellis to a degree, who claim to be essentialists. He revives and defends the Thomistic-Aristotelian tradition from modern attempts to flatten nature to the limp subject of the experimental method. In his In Praise of Natural Philosophy: A Revolution for Thought and Life (2017), Nicholas Maxwell argues that we need to reform philosophy and put science and philosophy back together again to create a modern version of natural philosophy.\n\n\n\n"}
{"id": "25695833", "url": "https://en.wikipedia.org/wiki?curid=25695833", "title": "Nature Exchange", "text": "Nature Exchange\n\nThe Nature Exchange is a specialized learning environment that encourages people to explore nature and actively observe, collect, study and share the world around them. It is a turn-key exhibit, now used in nature-based institutions around North America. Developed by Science North and AldrichPears Associates, the Nature Exchange is an interactive forum where visitors trade ethically collected natural objects and information about them to learn and engage with the natural world. Visitors earn points for each trade, based on criteria such as quality, rarity and their knowledge of the item. Science centers, nature centers and zoos use Nature Exchanges to raise awareness of key issues in the natural world, and, through personal interaction, changes attitudes and behavior.\n\n\n"}
{"id": "31915311", "url": "https://en.wikipedia.org/wiki?curid=31915311", "title": "Open science data", "text": "Open science data\n\nOpen science data is a type of open data focused on publishing observations and results of scientific activities available for anyone to analyze and reuse. A major purpose of the drive for open data is to allow the verification of scientific claims, by allowing others to look at the reprodubility of results, and to allow data from many sources to be integrated to give new knowledge. While the \"idea\" of open science data has been actively promoted since the 1950s, the rise of the Internet has significantly lowered the cost and time required to publish or obtain data.\n\nThe concept of open access to scientific data was institutionally established with the formation of the World Data Center system (now the World Data System), in preparation for the International Geophysical Year of 1957–1958. The International Council of Scientific Unions (now the International Council for Science) established several World Data Centers to minimize the risk of data loss and to maximize data accessibility, further recommending in 1955 that data be made available in machine-readable form.\n\nThe first initiative to create a database of electronic bibliography of open access data was the Educational Resources Information Center (ERIC) in 1966. In the same year, MEDLINE was created – a free access online database managed by the National Library of Medicine and the National Institute of Health (USA) with bibliographical citations from journals in the biomedical area, which later would be called PubMed, currently with over 14 million complete articles.\n\nIn 1995 GCDIS (US) put its position clearly in \n\"On the Full and Open Exchange of Scientific Data\" (A publication of the Committee on Geophysical and Environmental Data - National Research Council):\n\nThe last phrase highlights the traditional cost of disseminating information by print and post. It is the removal of this cost through the Internet which has made data vastly easier to disseminate technically. It is correspondingly cheaper to create, sell and control many data resources and this has led to the current concerns over non-open data.\n\nMore recent uses of the term include:\n\n\nIn 2004, the Science Ministers of all nations of the OECD (Organisation for Economic Co-operation and Development), which includes most developed countries of the world, signed a declaration which essentially states that all publicly funded archive data should be made publicly available. Following a request and an intense discussion with data-producing institutions in member states, the OECD published in 2007 the \"OECD Principles and Guidelines for Access to Research Data from Public Funding\" as a \"soft-law\" recommendation.\n\nIn 2005 Edd Dumbill introduced an “Open Data” theme in XTech, including:\n\nIn 2006 Science Commons ran a 2-day conference in Washington where the primary topic could be described as Open Data. It was reported that the amount of micro-protection of data (e.g. by license) in areas such as biotechnology was creating a Tragedy of the anticommons. In this the costs of obtaining licenses from a large number of owners made it uneconomic to do research in the area.\n\nIn 2007 SPARC and Science Commons announced a consolidation and enhancement of their author addenda.\n\nIn 2007 the OECD (Organisation for Economic Co-operation and Development) published the Principles and Guidelines for Access to Research Data from Public Funding. The Principles state that:\nAccess to research data increases the returns from public investment in this area; reinforces open scientific inquiry; encourages diversity of studies and opinion; promotes new areas of work and enables the exploration of topics not envisioned by the initial investigators.\n\nIn 2010 the Panton Principles launched, advocating Open Data in science and setting out for principles to which providers must comply to have their data Open.\n\nIn 2011 LinkedScience.org was launched to realize the approach of the Linked Open Science to openly share and interconnect scientific assets like datasets, methods, tools and vocabularies.\n\nIn 2012, the Royal Society published a major report, \"Science as an Open Enterprise\", advocating open scientific data and considering its benefits and requirements.\n\nIn 2013 the G8 Science Ministers released a Statement supporting a set of principles for open scientific research data\n\nIn 2015 the World Data System of the International Council for Science adopted a new set of Data Sharing Principles to embody the spirit of 'open science'. These Principles are in line with data policies of national and international initiatives and they express core ethical commitments operationalized in the WDS Certification of trusted data repositories and service.\n\nMuch data is made available through scholarly publication, which now attracts intense debate under \"Open Access\" and semantically open formats – like to offer the scientific articles in JATS format. The Budapest Open Access Initiative (2001) coined this term:\n\nBy \"open access\" to this literature, we mean its free availability on the public internet, permitting any users to read, download, copy, distribute, print, search, or link to the full texts of these articles, crawl them for indexing, pass them as data to software, or use them for any other lawful purpose, without financial, legal, or technical barriers other than those inseparable from gaining access to the internet itself. The only constraint on reproduction and distribution, and the only role for copyright in this domain, should be to give authors control over the integrity of their work and the right to be properly acknowledged and cited.\nThe logic of the declaration permits re-use of the data although the term \"literature\" has connotations of human-readable text and can imply a scholarly publication process. In Open Access discourse the term \"full-text\" is often used which does not emphasize the data contained within or accompanying the publication.\n\nSome Open Access publishers do not require the authors to assign copyright and the data associated with these publications can normally be regarded as Open Data. Some publishers have Open Access strategies where the publisher requires assignment of the copyright and where it is unclear that the data in publications can be truly regarded as Open Data.\n\nThe ALPSP and STM publishers have issued a statement about the desirability of making data freely available:\n\nPublishers recognise that in many disciplines data itself, in various forms, is now a key output of research. Data searching and mining tools permit increasingly sophisticated use of raw data. Of course, journal articles provide one ‘view’ of the significance and interpretation of that data – and conference presentations and informal exchanges may provide other ‘views’ – but data itself is an increasingly important community resource. Science is best advanced by allowing as many scientists as possible to have access to as much prior data as possible; this avoids costly repetition of work, and allows creative new integration and reworking of existing data.\n\nand\n\nWe believe that, as a general principle, data sets, the raw data outputs of research, and sets or sub-sets of that data which are submitted with a paper to a journal, should wherever possible be made freely accessible to other scholars. We believe that the best practice for scholarly journal publishers is to separate supporting data from the article itself, and not to require any transfer of or ownership in such data or data sets as a condition of publication of the article in question.\n\nEven though this statement was without any effect on the open availability of primary data related to publications in journals of the ALPSP and STM members. Data tables provided by the authors as supplement with a paper are still available to subscribers only.\n\nIn an effort to address issues with the reproducibility of research results, some scholars are asking that authors agree to share their raw data as part of the scholarly peer review process. As far back as 1962, for example, a number of psychologists have attempted to obtain raw data sets from other researchers, with mixed results, in order to reanalyze them. A recent attempt resulted in only seven data sets out of fifty requests. The notion of obtaining, let alone requiring, open data as a condition of peer review remains controversial.\n\nTo make sense of scientific data they must be analysed. In all but the simplest cases, this is done by software. The extensive use of software poses problems for the reproducibility of research. To keep research reproducible, it is necessary to publish not only all data, but also the source code of all software used, and all the parametrization used in running this software. Presently, these requests are rarely ever met. Ways to come closer to reproducible scientific computation are discussed under the catchword \"open research computation\".\n\n\n"}
{"id": "14002", "url": "https://en.wikipedia.org/wiki?curid=14002", "title": "Outline of health sciences", "text": "Outline of health sciences\n\nThe following outline is provided as an overview of and topical guide to health sciences:\n\nHealth sciences – are those sciences which focus on health, or health care, as core parts of their subject matter. Because these two subject matter relate to multiple academic disciplines, both STEM disciplines as well as emerging patient safety disciplines (such as social care research) are relevant to current health scientific knowledge.\n\nHealth sciences knowledge bases are currently diverse, with intellectual foundations which are sometimes mutually-inconsistent. There is currently an existing bias in the field, towards high valuation of knowledge deriving from controlling views on human agency (as epitomized by the epistemological basis of Randomized Control Trial designs); compare this against the more naturalistic views on human agency taken by research based on Ethnography for example).\n\nMental health\n\nSocial health\n\nPhysical health\n\nMedicine – applied science or practice of the diagnosis, treatment, and prevention of disease. It encompasses a variety of health care practices evolved to maintain and restore health by the prevention and treatment of illness. Some of its branches are:\n\n\n\n\n\n"}
{"id": "386128", "url": "https://en.wikipedia.org/wiki?curid=386128", "title": "Outline of scientific method", "text": "Outline of scientific method\n\nThe following outline is provided as an overview of and topical guide to scientific method:\n\nScientific method – body of techniques for investigating phenomena and acquiring new knowledge, as well as for correcting and integrating previous knowledge. It is based on observable, empirical, reproducible, measurable evidence, and subject to the laws of reasoning.\n\nScientific method\n\nResearch\n\nObservation\n\nHypothesis\n\nExperiment\n\n\n\n\nEmpirical methods\n\n\n\nThe problem of induction questions the logical basis of scientific statements.\n\n\n\n\n\n\n\n\n\n"}
{"id": "43529327", "url": "https://en.wikipedia.org/wiki?curid=43529327", "title": "Postnormal times", "text": "Postnormal times\n\nPostnormal times (PNT) is a concept developed by Ziauddin Sardar as a development of Post-normal Science. Sardar describes the present as \"postnormal times\", \"in an in-between period where old orthodoxies are dying, new ones have yet to be born, and very few things seem to make sense.\"\n\nIn support of engaging communities of various scope and scale on how to best navigate PNT and imagine preferred pathways toward the future(s), Sardar and Sweeney published an article in the journal \"Futures\" outlining The Three Tomorrows method, which fills a gap in the field as \"many methods of futures and foresight seldom incorporate pluralism and diversity intrinsically in their frameworks, and few, if any, emphasize the dynamic and merging nature of futures possibilities, or highlight the ignorance and uncertainties we constantly confront.\"\n\nRakesh Kapoor criticized PNT in 2011 as a Western concept, that does not apply to India and other emerging markets. Sam Cole criticised the three C’s of PNT (chaos, complexity and contradictions) as \"Alliterative Logic, theorizing through alliterative word-triads that is not based on empirical evidence\". Jay Gray has suggested that PNT is embryonic, needs a more robust framework, and should be extended to include C S Holling's adaptive cycle. Scientists working on complex evolving systems have pointed out that PNT recalls the ‘Long Waves’ of Kondratiev and Joseph Schumpeter’s view of waves of \"creative destruction.\"\n\nPNT is one of the core areas of research for the Center for Postnormal Policy and Futures Studies at East-West University in Chicago, Illinois, US. A number of articles and editorials on PNT have been published in the journal \"East-West Affairs\".\n\n"}
{"id": "55779967", "url": "https://en.wikipedia.org/wiki?curid=55779967", "title": "Priority certificate", "text": "Priority certificate\n\nA priority certificate is a document attesting that the entities named in such a certificate are the first to discover a phenomenon from nature, the first proponent of a theory, abstract idea, solution to a problem, proof of a theorem etc. A person who makes a new and useful discovery is entitled to receive such a priority certificate for that specific discovery. \n\nPrivate or public companies and organizations, such as universities, R&D institutions, trade shows and exhibitions are known to grant priority certificates to confer formal recognition upon the claimant(s).\n\n"}
{"id": "24241", "url": "https://en.wikipedia.org/wiki?curid=24241", "title": "Protoscience", "text": "Protoscience\n\nIn the philosophy of science, there are several definitions of protoscience.\n\nIts simplest meaning (most closely reflecting its roots of \"proto-\" + \"science\") involves the earliest eras of the history of science, when the scientific method was still nascent. Thus, in the late 17th century and early 18th century, Isaac Newton contributed to the dawning sciences of chemistry and physics, even though he was also an alchemist who sought chrysopoeia in various ways including some that were unscientific. \n\nAnother meaning extends this idea into the present, with protoscience being an emerging field of study which is still not completely scientific, but later becomes a proper science. An example of it would the general theory of relativity, which started being a protoscience (a theoretical work which had not been tested), but later was experimentally verified and became fully scientitific. Protoscience in this sense is distinguished from pseudoscience by a genuine willingness to be changed through new evidence, as opposed to having a theory that can always find a way to rationalize a predetermined belief.\n\nPhilosopher of chemistry Jaap Brakel defines protoscience as \"the study of \"normative\" criteria for the use of experimental technology in science.\"\nThomas Kuhn said that protosciences \"generate testable conclusions but ... nevertheless resemble philosophy and the arts rather than the established sciences in their developmental patterns. I think, for example, of fields like chemistry and electricity before the mid-18th century, of the study of heredity and phylogeny before the mid-nineteenth, or of many of the social sciences today.\" While noting that they meet the demarcation criteria of falsifiability from Popper, he questions whether the discussion in protoscience fields \"result[s] in clear-cut progress\". Kuhn concluded that protoscience, \"like the arts and philosophy, lack some element which, in the mature sciences, permits the more obvious forms of progress. It is not, however, anything that a methodological prescription can provide. ... I claim no therapy to assist the transformation of a proto-science to a science, nor do I suppose anything of this sort is to be had\".\n\nThe term \"prescientific\" means at root \"relating to an era before science existed\". For example, traditional medicine existed for thousands of years before medical science did, and thus many aspects of it can be described as prescientific. In a related but somewhat different sense, protoscientific topics (such as the alchemy of Newton's day) can be called prescientific, in which case the \"proto-\" and \"pre-\" labels can function more or less synonymously (the latter focusing more sharply on the idea that nothing but science is science).\n\nCompare fringe science, which is considered highly speculative or even strongly refuted. Some protosciences go on to become an accepted part of mainstream science.\n\n\n\n"}
{"id": "19206529", "url": "https://en.wikipedia.org/wiki?curid=19206529", "title": "SCALE-UP", "text": "SCALE-UP\n\nSCALE-UP is a learning environment specifically created to facilitate active, collaborative learning in a studio-like setting. Some people think the rooms look more like restaurants than classrooms. The spaces are carefully designed to facilitate interactions between teams of students who work on short, interesting tasks. A decade of research indicates significant improvements in learning. The approach taken during the development and testing of the learning environment is an application of scientific teaching and has been discussed in several books. Although originated at North Carolina State University, more than five hundred colleges across the US and around the world are known to have directly adopted the SCALE-UP model and adapted it to their particular needs. Information about more than 400 of these implementation sites is available on the SCALE-UP website.\n\nThe SCALE-UP name originally stood for “Student-Centered Activities for Large Enrollment Undergraduate Physics” but since its conception many different institutions have begun teaching a variety of courses of various sizes. The acronym was changed to “Student-Centered Active Learning Environment for Undergraduate Programs.” Now, because of the increasing number of pre-college installations, plus to draw attention to the instruction as well as the space, the name has become \"Student-Centered Active Learning Environment with Upside-down Pedagogies.\" The basic idea is that students are given something interesting to investigate. While they work in teams on these \"tangibles\" (hands-on measurements or observations) and \"ponderables\" (interesting, complex problems), the instructor is free to roam around the classroom–--asking questions, sending one team to help another, or asking why someone else got a different answer. There is no separate lab class and most of the \"lectures\" are actually class-wide discussions. The groups are carefully structured and give students many opportunities to interact. Three teams (labelled a, b, and c) sit at each round table and have white boards nearby. Each team has a laptop in case they need web access. The original design called for 11 round tables of nine students, but many schools have smaller classes while a few have even larger ones. Smaller classes, particularly those in high schools, have also been using D-shaped tables that seat six students.\n"}
{"id": "3549197", "url": "https://en.wikipedia.org/wiki?curid=3549197", "title": "Science Media Centre", "text": "Science Media Centre\n\nThe Science Media Centre is an organisation which formed in 2002,\ntwo years after the United Kingdom House of Lords Select Committee on Science and Technology's third report on \"Science and Society\" in 2000.\n\nThis report stated that while science was generally reported accurately in the mass media, there was a need for the promotion of more expert information at times when science is under attack in the headlines, mentioning the public reaction to GM crops, in particular.\n\nIn order to promote more informed science in the media, the Centre's main function is as a service to journalists, providing background briefings on current scientific issues and facilitating interviews with scientists. Its director is Fiona Fox who is a former member of the Revolutionary Communist Party and a former contributor to its magazine \"Living Marxism. \n\nThe SMC's stated aim is to \"facilitate more scientists to engage with the media, in the hope that the public will have improved access to accurate, evidence-based scientific information about the stories of the day\".\n\nThe setting up of the Science Media Centre was assisted by Susan Greenfield, the director of the Royal Institution of Great Britain. While the Centre is still based in a specially refurbished wing of the Royal Institution, full independence is claimed from all funders and supporters.\n\nThe Science Media Centre is funded by over 60 organisations, with individual donations capped at £12,500 per annum. The SMC receives sponsorship from a range of funders including media organisations, universities, scientific and learned societies, the UK Research Councils, government bodies, Quangos, charities, private donors and corporate bodies. For an up-to-date list of funders, see .\n\nA 2013 article in \"Nature\" stated about the SMC, \"Perhaps the biggest criticism of Fox and the SMC is that they push science too aggressively — acting more as a PR agency than as a source of accurate science information.\"\n\nIn 2002, Ronan Bennett and Alan Rusbridger described the SMC as a lobby group.\n\nDuring Professor Greenfield's term as Thinker in Residence in South Australia, a new Australian Science Media Centre was set up in Adelaide, Australia in August 2005. Science Media Centres now exist in other countries; Canada, Australia, New Zealand, Japan, and Germany. Except for the relation between the Science Media Centre in UK and the Australian Science Media Centre, these centres are independent of each other.\n\n\n"}
{"id": "55894093", "url": "https://en.wikipedia.org/wiki?curid=55894093", "title": "Science education in England", "text": "Science education in England\n\nScience education in England is generally regulated at all levels for assessments that are England's, from 'primary' to 'tertiary' (university). Below university-level, science education is the responsibility of three bodies: the Department for Education, Ofqual and the QAA, but at university-level, science education is regulated by various \"professional bodies\", and the Bologna Process via the QAA. The QAA also regulates science education for some qualifications that are not university degrees via various \"qualification boards\", but not content for GCSEs, and GCE AS and A levels. Ofqual on the other hand regulates science education for GCSEs and AS/A levels, as well as all other qualifications, except those covered by the QAA, also via qualification boards. The Department for Education prescribes the content for science education for GCSEs and AS/A levels, which is implemented by the qualification boards, who are then regulated by Ofqual. The Department for Education also regulates science education for students aged 16 years and under. The department's policies on science education (and indeed all subjects) are implemented by local government authorities on all state schools (also called \"publicly funded\" schools) in England. The content of the nationally organised science curriculum (along with other subjects) for England is published in the National Curriculum, which covers key stage 1 (KS1), key stage 2 (KS2), key stage 3 (KS3) and key stage 4 (KS4). The four key stages can be grouped a number of ways; how they are grouped significantly affects the way the science curriculum is delivered. In state schools, the four key stages are grouped into KS1–2 and KS3–4; KS1–2 covers primary education while KS3–4 covers secondary education. But in independent or public (which in the United Kingdom are historic independent) schools (not to be confused with 'publicly funded' schools), the key stage grouping is more variable, and rather than using the terms ‘primary’ and 'secondary’, the terms ‘prep’ and ‘senior’ are used instead. Science is a compulsory subject in the National Curriculum of England, Wales and Northern Ireland; state schools have to follow the National Curriculum while independent schools need not follow it. That said, science is compulsory in the Common Entrance Examination for entry into senior schools, so it does feature prominently in the curricula of independent schools. Beyond the National Curriculum and Common Entrance Examination, science is voluntary, but the government of the United Kingdom (comprising England, Wales, Scotland and Northern Ireland) provides incentives for students to continue studying science subjects. Science is regarded as vital to the economic growth of the United Kingdom (UK). For students aged 16 years (the upper limit of compulsory \"school age\" in England, but not compulsory education as a whole) and over, there is no \"compulsory\" nationally organised science curriculum for all state/publicly funded education providers in England to follow, and individual providers can set their own content, although they often (and in the case of England's state/publicly funded post-16 schools and colleges have to) get their science (and indeed all) courses \"accredited\" or made \"satisfactory\" (ultimately by either Ofqual or the QAA via the qualification boards). Universities do not need such approval, but there is a reason for them to seek accreditation regardless. Moreover, UK universities have obligations to the Bologna Process to ensure high standards. Science education in England has undergone significant changes over the centuries; facing challenges over that period, and still facing challenges to this day.\n\nGillard (2011) gives a documented account of science curriculum and education during this period. According to his work, the teaching of science in England dates back to at least Anglo-Saxon times. Gillard explains that the first schools in England (that are known of) were created by St Augustine when he brought Christianity to England around the end of the sixth century—there were almost certainly schools in Roman Britain before St Augustine, but they did not survive after the Romans left. It is thought the first grammar school was established at Canterbury in 598 during the reign of King Ethelbert. Gillard also mentions Bede's \"Ecclesiastical History\", here science (in the form of astronomy) was already part of the curriculum in the early schools of the 600s. As the founding of grammar schools spread from south to north of England, science education spread with it. Science as it is known today developed from two spheres of knowledge: natural philosophy and natural history. The former was associated with the reasoning and explanation of nature while the latter focused more on living things. Both strands of knowledge can be identified in a curriculum provided by a school in York run by Alcuin in the 770s and 780s. Subsequent Viking invasions of England interrupted the development of schools, but despite this, through the ages, education in England was provided by the church and grammar schools (which were linked to the church). The link between church and school started to change in the 1300s when schools independent of the church began to emerge. University education in England started in Oxford in the 1100s (although there is evidence that teaching began there in the 1000s). Like pre-university education, science at Oxford University was initially taught in the form of astronomy (as part of the quadrivium). The Renaissance spurred physical inquiry into nature which led to natural philosophy developing into physics and chemistry, and natural history developing into biology; these three disciplines form natural science, from which interdisciplinary fields (or at least their modern versions) that overlap two or all three branches of natural science develop. This emerging trend in physical inquiry do not appear to have been reflected in the science curriculum in schools at the time. Even in universities, the changes to science education that were necessary as a result of the Renaissance occurred very slowly. It was not till the 1800s that the science curriculum and education recognised in England today at all levels truly began to emerge.\n\nUp until the 1800s there were only two stages of education: elementary and university. However, in the nineteenth century, elementary education began to divide into primary (still called elementary) and secondary education. Elementary schools were defined in law in England through a series of acts of parliament which made education compulsory and free for children up to the age of 11 (later increased to 12). There were six (and later seven) standards for children to pass; science education did not feature in any of these standards, but for some schools it was an \"add-on\" especially at the higher standards (such as sixth and seventh—science subjects included physics, chemistry, mechanics). Promotion from one standard to the next was on merit and not age. Not all children completed all standards, which meant that by the age of 12, there were children that had not ‘completed’ their elementary education. Of course families that could afford (and wanted) to keep their children in school post-compulsory age to pass all standards did so. In fact some children stayed in school beyond the seventh standard. Schools that offered post-seventh standard education became known as \"higher grade schools\", of which science education was a recognised feature of their curricula.\n\nThis was by far the single most important development for science education in schools in England in the nineteenth century from a British parliament point of view. Ironically the original purpose of the committee that authored the 'Taunton' Report of 1868, or more formally, \"Volume II Miscellaneous Papers of the Schools Inquiry Commission\" (1868), was to examine how best endowed schools should be managed; something Parliament at the time thought was of utmost importance. The committee for the report was chaired by Lord Taunton (born Henry Labouchere). In heading the preparation for the report, Lord Taunton sent a circular letter listing four questions to a number of prominent people in different parts of England on 28 May 1866; the first three were endowment-related issues, but the fourth question was on how to encourage a due supply of qualified teachers. Apart from the contents page, the word \"science\" first appears on page 45 of the report in a reply by one of the recipients of the circular letter; that recipient was Reverend W C Lake. The reverend comments:\nOn page 77 of the report, Edward Twisleton, a member of the Schools Inquiry Commission, comments on the answers provided to the four questions set by the committee's chairman, Lord Taunton, based on feedback from the circular letter sent. To the first question, Twisleton writes:\nThere were noticeable opinions on the issue of science education from contributors that wrote to the committee to express their views. One by Robert Mosley of Holgate Seminary, York (pages 104 to 105 of the report), suggested the inclusion of physical sciences in a 'National education'; this national education being the best way to utilise educational endowment. Based on feedback from contributors, the Taunton Committee gave several arguments in favour of science education; two of them are:\nand\nThe committee subsequently made several recommendations; the first three on promoting scientific education in schools are listed below:\nThe issue of increased cost for fee payers played heavily on the minds of the committee, and although the committee felt that for \"a wealthy country like England\" (page 219 of the report), a slight increase in cost should not be a barrier to science education, it was left to individual schools to decide how to incorporate science into their curricula.\n\nBy the time of the Taunton Report there were four universities in England (Oxford, Cambridge, Durham and London), but from the 1880s, a new wave of universities / university colleges completely separate from the original four began to emerge; these universities were called red brick universities. The first of these universities was established in Manchester in 1880 and was called Victoria University. Over the subsequent 80 years, a further 11 universities outside London, Cambridge, Durham and Oxford were founded, significantly expanding the availability of university (science) education throughout England. All through the 1800s, science was becoming increasingly specialised into the different areas we know today.\n\nThe Education Act 1902 led to the higher grade schools (alluded to earlier) and fee-paying schools being absorbed into the legally defined “higher education” (meaning any education that was not elementary (as primary education was known at the time)). Despite science education in higher grade schools and the recommendations of the Taunton Report, as well as the British Association for the Advancement of Science’s campaign for a science curriculum, science was still seen as a minor subject by the most prestigious public schools. The problem was that most of these public schools had close relationships with Oxford and Cambridge universities which offered the majority of their scholarships in classics, and so science was regarded in low importance by the prestigious schools. Consequently, science education varied significantly across English schools. Numerous education-related acts were passed throughout the twentieth century, but the most important in the history of science education in England was the Education Reform Act 1988 (see next subsection). Another act of importance to the development of science education below university-level in England was the Education Act 1944. The 1944 act's contribution was indirect though—it raised the compulsory school age to 15, but made provisions for it to be raised to 16 at a future date—which happened in 1972 (which is still the case today). By raising the school leaving age to 16, this formed the basis for creating a nationally organised science curriculum and education in England. However, the 1944 Education Act did not stipulate that science be taught. For university-level science education, two significant developments were the expansion of distance learning science courses and the introduction of the World Wide Web (via the Internet) into the delivery of science teaching, although this has also been adopted below university-level.\n\nThis was the most important development in the history of science education in England. It was this act that established the National Curriculum and made science compulsory across both secondary and primary schools (alongside maths and English). The 1988 act in effect implemented the recommendation of the Taunton Committee made more than a century earlier. The act also established the now familiar “key stages”.\n\nThe most significant developments to the science curriculum and education in this period to date have been the expansion of the compulsory science content in the National Curriculum and the associated changes to its assessment. Another significant event was the passing of the Education and Skills Act 2008, which raised the education leaving age in England to 18. It is unclear whether this extension of compulsory education will result in more science learners as science is not compulsory after the age of 16—the school leaving age, which the 2008 act did not alter.\n\nCompulsory science content is provided by the National Curriculum and generally applies to children between the ages of 5 and 16. These eleven years of compulsory education are divided by the state into four key stages: KS1, KS2, KS3 and KS4. Regardless of key stage, the National Curriculum states two overarching aims of science education:\nA third aim is common to KS1–3:\nBut for KS4, the third aim is far more detailed, and there is also a fourth aim:\nThe need for mathematical skills is stressed by the National Curriculum across all key stages, but more so at KS3 and KS4.\n\nThe National Curriculum for science is a spiral curriculum; it is also prescriptive. Because of its spiral nature, this makes its learning essentially constructivist. These points are illustrated in the subsections that follow. In addition, the Science National Curriculum emphasises the need for active learning right from the child’s earliest exposure to the curriculum. Research on the value of active learning has been demonstrated and published. Experimentation by the child is underscored in the curriculum accompanied by careful discussion of what was observed. Despite these positive features, it has been argued that evaluating the effectiveness of the National Curriculum on learning is difficult to answer.\n\nKey stage 1 (KS1) covers the first two years of compulsory school education in the National Curriculum. As such, the years are referred to as years 1 and 2. Children are typically in the age range 5–7. The emphasis of science at this stage is observation and describing or drawing things that the child can see, either around them or from a book or photograph or video; the feel of materials is also an important feature of KS1 science. Abstract concepts in science are not introduced at this stage (at least not on the basis of the National Curriculum). As a result, the science curriculum at KS1 is more or less plants and animals, and materials, with the emphasis on what can easily be seen or described by feeling things.\n\nKey stage 2 (KS2) covers years 3, 4, 5 and 6 of compulsory school education in the National Curriculum. It is the longest stage of compulsory school education in England. Children are typically in the age range 7–11. The National Curriculum divides KS2 into lower KS2 (years 3 and 4) and upper KS2 (years 5 and 6). Year 3 continues from KS1, but more complex observations for the child to do on plants and animals, and materials—rocks, fossils and soils, are brought in. Setting up simple experiments and recording data become increasingly important at this stage. Hazards and dangers of certain scientific experiments (such as feeling things after they have been heated) are drilled into pupils; necessary precautions against such dangers/hazards are taught. New areas are introduced: light (and the dangers of looking directly at sunlight with necessary precautions), forces and magnets. In year 4, classification of living and non-living things come to the fore; additional areas introduced include:\nIn years 5 and 6 (upper KS2), the National Curriculum states that the emphasis should be on enabling pupils develop a deeper understanding of scientific ideas. The need to read, spell and pronounce scientific vocabulary correctly is emphasised by the National Curriculum. This emphasis probably reflects the fact that by the age of 9, 10 or 11, a child in England should be able to read and write properly. Year 5 continues on from year 4; studying increasingly more complex aspects of what was introduced in year 4. Also the pupil starts to learn about accepting or \"refuting\" ideas based on scientific evidence. Additional areas include:\nYear 6 not only continues on from year 5, adding more complex aspects of what was learnt in year 5, but also prepares the pupil for KS3 science; additional areas include:\n\nBetween the early 1990s and early 2010s, state school pupils had to take statutory SAT exams at the end of KS2 science although teacher assessments were also allowed. The KS2 SAT science exam consisted of two papers (forty-five minutes each). The scores from both papers were combined to give a final score. This score would then be converted into a \"numerical\" level, which would in turn be converted into an \"expectation\" level. The conversion scale for the levels at KS2 SAT science is shown in the table below.\n\n\"Science KS2 SATs\"\nLevel 6 (exceptional) was also available, but only in mathematics and English (reading); a separate test for level 6 assessment had to be taken, which had to be marked externally. Science KS2 SATs were discontinued in 2013 and replaced by teacher assessments (which were already allowed during the time of SATs). In addition to teacher assessments, a SAT replacement assessment called \"key stage 2 science sampling test\" is now offered to five randomly selected pupils in a school every two years. The test comprises three papers: ‘b’ for biology, ‘c’ for chemistry, and ‘p’ for physics (each twenty-five minutes). The aim of the tests is to assess how well children are getting on with the curriculum. The first test of this kind was in the summer of 2016.\n\nThis exam is taken by KS2 pupils wishing to be admitted into independent senior schools for KS3 study and higher (although not all senior schools admit 11-year-olds). The exam is typically taken while the pupil is in prep school, although some state school pupils use the exam to make the transition to an independent school. The syllabus for the 11+ CE science exam is based on the National Curriculum for KS2 science; one paper for science (one hour) is taken. In addition to the examinable syllabus for the 11+ CE, there is also prep-KS3 science material for the pupil to cover; this prep-KS3 science material is not examinable, but is required as preparation for KS3 science study in senior school if admitted.\n\nThe National Curriculum for KS3–4 science differs from KS1–2 not just in its complexity, but unlike the latter, the science curriculum is divided into three explicit parts: biology, chemistry and physics. Typically in a state secondary school there can be anything from one to up to three (or even more) teachers delivering science to a single class (depending on the breadth of knowledge of the teacher and staff resources of the school). Broadly speaking, similar areas are covered at both stages (that is KS3 and KS4), but at a more advanced level in KS4. Below is a broad (and simplified) summary of the curriculum of each part at KS3/4 level.\n\nDefined in the National Curriculum as:\nThe content for KS3/4 biology in the National Curriculum is broadly:\n\nDefined in the National Curriculum as:\nThe content for KS3/4 chemistry in the National Curriculum is broadly:\n\nDefined in the National Curriculum as:\nThe content for KS3/4 physics in the National Curriculum is broadly:\n\nKey stage 3 (KS3) covers years 7, 8 and 9 of compulsory school education in the National Curriculum. Pupils are typically in the age range 11–14.\n\nBetween the early 1990s and late 2000s (‘late noughties’) state school pupils had to take statutory SAT exams at the end of KS3 science (just like KS2), although teacher assessments were also allowed. The KS3 SAT science exam consisted of two papers (one hour each). The scores from both papers were combined to give a final score. This score would then be converted into a numerical level, which would in turn be converted into an expectation level. The conversion scale for the levels at KS3 SAT is shown below.\nThe conversion of the raw score from the two papers to a numerical level depended on the ‘tier’ taken by the student. For science KS3 SATs, two tiers were available: lower tier and higher tier. Levels 3–6 were available at the lower tier while levels 5–7 were available at the higher tier. The conversion scale for each tier’s scores are shown below.\n\n\"Science KS3 SATs: lower tier\"\n\"Science KS3 SATs: higher tier\"\nLevel 8 (exceptional) was not available to science KS3 SATs (not even at the higher tier); it was available to mathematics, but only at the highest tier (levels 6–8) out of four tiers that were available to mathematics KS3 SATs. Science KS3 SATs were discontinued in 2010 and replaced by teacher assessments (just like science KS2 SATs). Despite the discontinuation of statutory science KS3 SATs, the past papers are still used by schools today.\n\nLike the 11+ CEs, the 13+ CEs are taken by pupils wishing to be admitted to independent senior schools; in this case for KS4 study and higher. The exam is typically taken while the pupil is in prep school (some senior schools only admit from the age of 13). Some state school pupils use the exam to make the transition to an independent school. The syllabus for the 13+ CE science exam(s) is based on the National Curriculum for KS3 science, although not all of the KS3 science content is examinable in the CE, but the parts left out are recommended for teaching in year 9. For the exam the candidate can take either the simpler one paper in science (one hour) comprising biology, chemistry and physics parts, or three higher (and harder) papers (forty minutes each)—one in biology, one in chemistry, and one in physics. In addiion, individual senior schools may have exams for entry into other years; for example, 14+, 16+ (for post-16 or ‘KS5’ study); details of which they give on their websites.\n\nKey stage 4 (KS4) covers years 10 and 11 of compulsory school education, but it may start earlier for science (and mathematics) in some schools. Pupils are typically in the age range 14–16. At the end of KS4, students have to take statutory GCSE exams, which can be taken at either foundation tier or higher tier. Science GCSEs can be complicated in that they offer a vast array of ‘routes’ although this has simplified somewhat following recent changes to GCSEs. Today science GCSE can be taken either as a combined single subject (which is worth two GCSEs) or as the three separate subjects of physics, chemistry and biology, (each worth a single GCSE in its own right). When biology, chemistry and physics are taken as separate GCSE subjects the tiers can be mixed. So for instance, a student could take say, biology at higher tier, but chemistry at foundation tier. By contrast, tiers cannot be mixed in combined science (that is, all constituent parts must be taken at the same tier). Experiments (also called \"practicals\") are compulsory in the GCSE science course, but in different ways across the boards offering GCSE science to English schools. For most boards the results of the practicals do not count towards the final grade in the reformed GCSE (as this is determined entirely by the results of the written examination), but the school/college must submit a signed \"practical science statement\" to the board under which the science is being studied BEFORE the students can take the examination. The statement must declare that all students have completed all the required practicals. The skills and knowledge that should have been acquired from the practicals are subsequently assessed in the GCSE exams, which for most boards are entirely written (as alluded to earlier). For one board (CCEA) however, in addition to the examination of practical skills in the written papers, the results of some of the actual practicals do count towards the final grade in the reformed GCSE. Currently, GCSE sciences in England are available from five boards: AQA, OCR, Edexcel. WJEC-Eduqas and CCEA. Although all five boards provide GCSE science to English schools, not all of these boards are based in England: AQA, OCR and Edexcel are based in England, but WJEC-Eduqas is based in Wales, while CCEA is based in Northern Ireland. Schools are free to choose any board for their science, and where the three sciences of chemistry, physics and biology are being taken independently at GCSE level, all three sciences need not be taken from the same board. Some boards offer multiple routes for their combined science courses in the reformed GCSE in England.\n\nFollowing recent changes, a student can go for one of two routes if taking AQA combined science: \"trilogy\" or \"synergy\". In trilogy, science is delivered in the three traditional parts of biology, chemistry and physics. The trilogy specification document outlines topics for each science part and practicals are specified. The trilogy GCSE exam itself is made up of six papers (each one hour and fifteen minutes): two for biology, two for chemistry, and two for physics. In synergy, science is delivered in two parts: \"life and environmental sciences\" AND \"physical sciences\". Unlike trilogy, each of the two parts in the synergy specification document is broken down into ‘areas’ that enable biology, chemistry and physics to sit together. The synergy GCSE exam itself is made up of four papers (each one hour and forty-five minutes): two for life and environmental sciences and two for physical sciences.\n\nLike AQA combined science, following recent changes, a student can go for one of two routes if taking OCR combined science; in this case either \"combined science A\" or \"combined science B\". In combined science A, science is delivered in the three traditional parts of biology, chemistry and physics. Like AQA's trilogy, each science part is broken into topics in combined science A's specification document , but unlike AQA combined science, practicals are suggested rather than specified, although practicals are still compulsory (the same goes for combined science B). The GCSE combined science A exam is made up of six papers (each one hour and ten minutes): two each for biology, chemistry and physics respectively. In combined science B, the science curriculum is delivered in four parts: biology, chemistry, physics and combined science. Each part is broken into topics in the combined science B specification document . The exam itself is made up of four papers (each one hour and forty-five minutes): one each for biology, chemistry, physics and combined science respectively.\n\nFollowing the changes to GCSEs, only one route is available to the student that takes Edexcel or Eduqas combined science. In Edexcel's combined science specification document the curriculum is delivered in the three traditional disciplines of biology, chemistry and physics, but in Eduqas's , the science curriculum is divided into four parts: \"Concepts in Biology\", \" Concepts in Chemistry\", \"Concepts in Physics\" and \"Applications in Science\". The Eduqas combined science exam is made up of four papers (one hour and forty-five minutes each): one for each of the three 'Concepts in ...' and one for 'Applications in Science'. The Edexcel exam is made up of six papers (each one hour and ten minutes): two each for biology, chemistry and physics respectively.\n\nThe new combined science from CCEA since the GCSE reforms retains the same name as its predecessor. The specification document presents the science curriculum in the traditional disciplines of biology, chemistry and physics. The exam is the most extensive of the GCSE science boards; made up of nine papers and three practical exams. For each of biology, chemistry and physics there are three papers and one practical exam: Paper 1 is one hour long, Paper 2 is one hour and fifteen minutes, Paper 3 is a practical skills paper and is thirty minutes long, and the practical exam is one hour long.\n\nAs alluded to earlier, in the mid-2010s, the GCSE science courses of the GCSE exam boards underwent significant changes. This was in part due to changes in the National Curriculum, of which one of the areas affected the most was key stage 4 (KS4). The revised version of the National Curriculum covered more content; the one for KS4 science was published in December 2014 and a version specifically for GCSE combined science was published in June 2015, and implemented in September 2016. The increased content triggered a change in the GCSE grading system from A*–G to 9–1. Much more detail on the new grading system and how it differs from the previous can be read here. One consequence of the increased science content in the National Curriculum was that it helped simplify a bewildering array of GCSE science courses particularly from AQA, which are/were designed to accommodate students from the least able to the most able. AQA science courses such as core science, additional science, further additional science, science A, science B, additional applied science illustrate the variety. The new trilogy and synergy courses (which were developed from the recently expanded National Curriculum for science) have removed the need for the most able students to take multiple science courses unless the student decides to take chemistry, biology and physics individually. The content for GCSE physics as a stand-alone subject is more than the content for physics in GCSE combined science. For instance, in the National Curriculum for KS4 science, space physics is included, but not in the GCSE combined science version. AQA includes space physics in its GCSE specification, but only when GCSE physics is taken as an independent subject in its own right, and not when physics is taken as part of GCSE combined science.\n\nFor the ages of 16, 17 and 18 (and older for those that remain in education below university-level), students in England do what is sometimes loosely called ‘key stage 5’ or KS5; it has no legal meaning (unlike the other key stages). And unlike KS1–4 in which the levels of complexity of topics learnt at each stage are prescribed within relatively narrow limits, at KS5, the levels of complexity of topics cover a wide range, although the highest level of complexity at KS5 is RQF level 3. Whether or not a student actually studies at this level of complexity in KS5 depends on his/her GCSE results—crucially on what subjects the student obtained passes at RQF level 2 standard (including mathematics and English) as well as the actual grades themselves. In other words, unlike KS1–4, where a specific student studies at one RQF level, at KS5, a specific student may be studying at several RQF levels depending on what s/he obtained at GCSEs. Regardless of the RQF-level mix, a KS5 student can do his/her post-16 study in one of the following:\n\nThis can be done either full-time or part-time. If done part-time, the student also has to be working or volunteering for at least 20 hours a week. As already hinted, the science curriculum and education at KS5 is highly varied, often disparate and tends to be specialised as students in their late teens interested in science begin to study subjects that will prepare them for science careers. In KS5 study at RQF level 3, students are introduced to concepts they would never have heard of during their time from KS1 to KS4, which they will either study in much greater depth at university-level (if s/he continues to study the science in question) or apply at vocational placements or apprenticeships. Practical science at KS5–RQF level 3 can be more extensive. Individual A levels in chemistry, biology and physics are perhaps the best known KS5–RQF level 3 science subjects (and they take two years to complete when done full-time), but A level students may well choose only one or two of these subjects, and mix with mathematics or non-science A level subjects depending on what university degree the student wishes to study post-KS5 (typically A level students go straight to university on successful completion of A levels). Although A levels are probably the highest profile KS5 studies, there are other qualifications students can take as alternatives. KS5 science subjects (including laboratory science) can also be taken in BTECs, Cambridge Pre-Us, IBs, AQAs (non-A levels), OCRs (non-A levels). NVQs, university specific foundation year programmes (generally offered to students that have taken A levels, but not the correct ones; can also be offered to those that have failed their A levels), access to HEs (generally not available to students under 21). Although all these alternative non-A level qualifications (which are all available at RQF level 3) can offer content similar in complexity to their A/AS level counterparts (which are also RQF level 3), the make-up of their content can vary significantly depending on the subject, and the board offering it. A comprehensive list of most subjects at most levels and the boards offering them is kept by the National Careers Service and individual subjects and their boards can be searched for on their website . A search tool for only Ofqual approved list of subjects and their boards can be found at \"Ofqual: The Register\"; the list can also be downloaded from the site, while a search tool for only QAA approved access to HE subjects can be found at \"Access to Higher Education\". Both the National Careers Service and Ofqual lists include all A/AS levels, GCSEs (RQF levels 1–2) and most of the rest (RQF levels 1–8, and the RQF entry level (which is below RQF level 1)). With regards to universities in England accepting RQF level 3 science subjects for their science degrees, students with only non-A level science subjects may be accepted, or the student may require a mixture of some of these non-A level science subjects with one or two A/AS level science subjects. This all depends on the level 3 qualification in question, the university, and science degree the student wishes to study. Individual universities give details of their entry requirements for their various science (and obviously all) degrees on their websites. Some RQF level 3 students may use the KS5 science subjects they study for entry into higher/degree apprenticeships or university-level vocational training.\n\nBeyond 18 years of age, students that have already either left or finished their formal education, but return at later times in their lives to study science (having decided they do not have the appropriate level of knowledge), can do so on their return at RQF level 3 or lower. The level the student returns at will depend on his/her pre-enrollment level of knowledge of science, although science is generally not available below RQF level 1 (that is, the RQF entry (sub-1) level) to adult returners to education (but maths and English are). Typically, further education colleges admit adult returners, although some universities may offer distance learning courses. Further education and distance learning courses are often the ways these mature students can access science courses long after they have left education. Just like students that have neither left nor previously finished their education, satisfactorily passing the summative assessment at RQF level 3 is the crucial gateway into university-level education (that is RQF level 4 and higher) in England. In addition to satisfactory passes in science subjects at RQF level 3, the learner also has to have passed mathematics and English at RQF level 2 standard (typically GCSEs or equivalent with minimum (or equivalent minimum) grades of 'C' or '4'); providers of university-level education give details on their websites.\n\nLike post-16 or KS5, this is also highly varied, disparate and specialised, but more so, as a student may chose to study 'one' science, which s/he will subsequently study in depth for three or more years; the summative assessment leads to a degree (of which for science in England today is typically one of two levels—RQF level 6 or 7). Such education will enable students market themselves as (specialist) scientists to employers or postgraduate science degree programmes (although the choices available to the graduate are affected by the class of degree the graduate achieves—recruiters give details on their websites). Many concepts the student first encountered in A levels / RQF level 3 are dealt with in much greater detail. The biggest difference between A level / RQF level 3 science and university-level science occurs in physics, which at university-level becomes highly mathematical (and at times difficult to distinguish from mathematics). Practical science at university-level can be quite extensive and by the time of the dissertation project, the student may well be doing complex experiments lasting weeks or months unsupervised (although s/he will still have a supervisor on hand). Science degrees in England are offered by both universities and some further education colleges. University-level teachers (also referred to in England as \"lecturers\") will teach one area of the science the student is studying, but two notable differences between university-level science education in further education colleges and universities are that in universities, there is a close connection between teaching and research. In other words, it is common for a university teacher to be a researcher in the area s/he teaches—this applies not just to science, but to all areas; such connection between teaching and research does not occur in further education colleges in England. And the other difference is that further education colleges must have their degrees approved by universities. Although universities do not need approval for their science degrees and are free to set their own content, they generally get many of their science courses accredited by professional bodies. So for example, universities offering biology degrees commonly get these programmes accredited by the Royal Society of Biology; for chemistry degrees, it is the Royal Society of Chemistry; for physics degrees, it is the Institute of Physics; for geology degrees, it is the Geological Society, and so on. Accreditation of a science degree by a professional body is a precondition if the student studying the degree is to become a member of the body following graduation, and subsequently acquire chartered status. In addition, UK universities are obliged to ensure that their degrees meet the standards agreed to in the Bologna Process to which the UK is a co-signatory. The QAA certifies those British degrees that meet those standards. Not all university-level students studying science study for science degrees; many will study science as part of a vocational degree such as pharmacy, medicine, dentistry, nursing, veterinary medicine, allied health professions, and so on. And some will study science as part of a higher/degree apprenticeship.\n\nThe challenges of establishing a national curriculum for science below university-level in England over the last two centuries have been explored by Smith (2010) and others. In Smith's paper, she highlighted two potentially conflicting roles for science education below university-level: educating a public to be scientifically literate, and providing scientific training for aspiring science professionals. Smith further pointed out in her paper that even among the training of aspiring science professionals, three groups could be identified: those that sought science in pursuance of the truth and an abstract understanding of science; those that sought science for actual benefit to society—the applied scientists, and then the failures. The dilemma did not escape the committee led by J J Thomson (discoverer of the electron) in 1918, which is quite telling of the tension in trying to accommodate several very different groups of science learners:\nSuch tension has never really dissipated. In a report by the Royal Society from 2008, they state several challenges facing science education; the first two are reproduced here:\n\nThe first:\nand the second: \nA lack of good quality teachers has also been cited as a challenge. Difficulty recruiting science teachers, which is a current problem in England (and the UK as a whole) is certainly not new as the following extract from the report by the Thomson Committee in 1918 shows:\nSome interesting figures were quoted in the 1918 report; for instance on page 31 of the report: out of 72 schools that had 200–400 girls of all ages, only 39 had the services of two science teachers (mistresses). The report went on state that these figures had contributed to long hours and inadequate salaries. This sounds strikingly similar to the situation facing science (and indeed all) school teachers in England today; a hundred years later. Another challenge was that there was not an appreciation by the political elite on the value of a science education to the wider public; despite the fact that England was producing some of the greatest scientists in the world. Yet another challenge was that public schools were slow to respond to the needs of developing a science curriculum. For example, William Sharp was the first science teacher for Rugby School, a prestigious public school in England, which only happened for the first time in 1847; nearly 300 years after the college was established and more than 100 years after England had lost one of the world’s greatest scientists—Isaac Newton. Despite these challenges, a science curriculum and education developed through the twentieth century and eventually became a compulsory part of the new National Curriculum in 1988 (phased in from 1989 to 1992). Even at the time of the deliberations in the mid-1980s prior to the creation of the National Curriculum, there was disagreement over how much time science should occupy in the curriculum. There was pressure for science to be made to occupy 20% of curriculum time for 14–16-year-olds, but not everyone agreed with this; certainly not the then Secretary of State for Education and Science Kenneth Baker. The then Department for Education and Science settled for 12.5% of curriculum time, but schools were free to increase this. The result was the emergence of single science (which occupied 10% of curriculum time and was the minimum requirement—also called \"core\" science), double science (which occupied 20% of curriculum time, and was so called because it involved studying core science and \"additional\" science), and there was the option of doing the sciences of physics, chemistry and biology separately (also known as 'triple' science). Following the changes to the National Curriculum in the 2010s, single science has effectively been removed, and the two components of double science have been combined to form 'combined science', which is now the minimum requirement. One challenge that ties in with England's shortage of science teachers is the number of science undergraduates in higher education, which provides the pool for future trainee science teachers, but undergraduate numbers affect the three sciences differently: the number of students that study physical sciences in higher education (93050 in the year 2012/13) are less than half the students that study biological sciences (201520 in the year 2012/13). This has had a direct impact on government policy in England; for example, the UK government offers bursaries of £30000 to graduates with first class honours degrees wishing to train as physics teachers in secondary schools in England; for chemistry, the top bursary is £25000, and for biology it is £15000. For students with lower honours degrees in these subjects, correspondingly lower bursaries are offered, but they are still considerable for physics graduates (compared to bursaries offered to trainee teachers of other subjects). For instance, a physics graduate with a lower second class honours degree can still attract a bursary of £25000. But the government has also implemented a policy to increase the number of science graduates from UK universities: normally a student in England wishing to study for a first degree including an honours degree can get a UK-government-backed student loan as long as s/he does not already possess an honours degree. Exceptions are permitted, but prior to September 2017 (and in the case of postgraduate master's degrees, September 2016), these UK-government-backed loans for those in England that already had honours degrees were only available for them if the courses they were going to study led to professional qualifications such as medicine, dentistry, social care, architecture or teaching. However the range of subjects for which a student in England already in possession of an honours degree could get a second UK-government-backed student loan to study a second honours degree was expanded to include science subjects (as well as technology, engineering and mathematics), which took effect from 1 September 2017. Like before, the student has to meet both England and UK residency requirements . The inclusion of science, technology, engineering and mathematics (collectively called \"STEM\" subjects) to the list appears to have been triggered not just by teacher shortages in those subjects, but also by a general skills shortage (in those subjects) UK-wide. It remains to be seen whether the direct interventions by the UK government help alleviate the general skills shortages in STEM subjects, as well as the challenges of delivering a science curriculum and education in the long-term. As for universities, several challenges have been identified by Grove (2015); the summaries of those challenges have been reproduced below:\n\nThese challenges apply not just to the university provision of science education, but to all areas of university education.\n\n"}
{"id": "41531599", "url": "https://en.wikipedia.org/wiki?curid=41531599", "title": "Scientific collection", "text": "Scientific collection\n\nAs a scientific collection is referred to\n\nImportant objects of research collections \n\nImportant goals of these collections are the items collected for research to make tangible and accessible, hold reference objects for comparison purposes as well as the systematization and naming with scientific names of the collected objects (taxonomy) .\n\nThe indexing of the collections was historically made by directories, catalogs, index cards, today supplemented by or replaced by databases with information such as e.g. scientific description, including picture, name, location, find circumstances, fund age, scientific analysis, phylogenetic relationships, DNA and isotope analysis results, analysis of pollutants, references, condition of the property, owner changes and name changes.\n\nMany organisations support the indexing and handling of their collections by specialist libraries.\n\nResearch collections hold especially museums, notably natural history museums, botanical gardens, universities and other research institutions. There are also independent research collections, such as the Zoological State Collection Munich with over 20 million stuffed animals for research purposes. Public authorities such as national geological agencies or police units hold partly research collections too.\n\nThe Natural History Museum in London - with one of the biggest collections worldwide - is home to life and earth science specimens comprising some 70 million items within five main collections: botany, entomology, mineralogy, palaeontology and zoology.\n\nLargest German Natural History Museum is the Museum für Naturkunde, Berlin, with over 30 million objects, including 9 million beetles and 275,000 jars with preserved in alcohol animals.\n\nRemerkable Earth Sciences collections:\n\nTypical collection objects biology are fossils of organisms, in particular plants and animals, plants, and animals killed, and be protected from decay, for example, by drying or preparation, but also live plants, animals, bacteria and viruses.\n\nPlant collections are referred to as herbaria. Live plants are collected in the Botanical gardens, (trees ) in arboretums, aquariums, and partly in seedbanks, as well as e.g. algae from the Culture Collection of Algae Göttingen. Live animals are collected in zoos and aquariums.\nThe great Old Botanical Garden of the University of Göttingen e.g. represents about a collection of 17,000 species.\n\nParticularly well known in Germany are the major research collections of the Naturmuseum Senckenberg of Senckenberg Society for Nature Research in Frankfurt am Main with over 22 million natural objects (Herbaria 1 Million). Senckenberg offers to open up his collection to the SESAM database.\n\nThe Macaulay Library is the world's largest archive of animal sounds. It includes more than 175,000 audio recordings covering 75 percent of the world's bird species. There are an ever increasing numbers of insect, fish, frog, and mammal recordings. The video archive includes over 50,000 clips, representing over 3,500 species.\n\nAn example for a special collection are the objects of the Deutsche Sammlung von Mikroorganismen und Zellkulturen (German Collection of Microorganisms and Cell Cultures).\n\nRemarkable and big Biological collections (more than 1,000,000 specimens) in Europe are\nSee more: List of herbaria in Europe\n\nRemarkable and big Biological collections (more than 1,000,000 specimens) in the Americas are:\nSee more: List of herbaria in North America\n\nRemarkable and big Biological collections worldwide see: List of herbaria\n\nDendrochronology is located on the border between biology and history. An annual ring table or tree-ring calendar is a time series of tree ring s of dendrochronological art tree. Because of the specific growth of each tree species and regional differences of climate, such a table must always refer to a single species from the same region. Important tree chronologies are:\n\nRemerkable History collections:\n\n\n\n"}
{"id": "3447151", "url": "https://en.wikipedia.org/wiki?curid=3447151", "title": "Scientific demonstration", "text": "Scientific demonstration\n\nA scientific demonstration is a scientific experiment carried out for the purposes of demonstrating scientific principles, rather than for hypothesis testing or knowledge gathering (although they may originally have been carried out for these purposes).\n\nMost scientific demonstrations are simple laboratory demonstrations intended to demonstrate physical principles, often in a surprising or entertaining way. They are carried out in schools and universities, and sometimes in public demonstrations in popular science lectures and TV programs aimed at the general public. Many scientific demonstrations are chosen for their combination of educational merit and entertainment value, which is often provided by dramatic phenomena such as explosions. \n\nPublic scientific demonstrations were a common occurrence in the Age of Enlightenment, and have long been a feature of the British Royal Institution Christmas Lectures, which date back to 1825. In the television era, scientific demonstrations have featured in science-related entertainment shows such as \"MythBusters\" and \"\".\n\nSome famous scientific demonstrations include:\n\n\nNote: many scientific demonstrations are potentially dangerous, and should not be attempted without considerable laboratory experience and appropriate safety precautions. Many older well-known scientific demonstrations, once mainstays of science education, are now effectively impossible to demonstrate to an audience without breaking health and safety laws. Some older demonstrations, such as allowing the audience to play with liquid mercury, are sufficiently dangerous that they should not be attempted by anyone under any circumstances.\n\n"}
{"id": "9881376", "url": "https://en.wikipedia.org/wiki?curid=9881376", "title": "Sleepers, Wake!", "text": "Sleepers, Wake!\n\nSleepers, Wake! Technology and the Future of Work is a book written by Barry Jones, originally published in 1982 and reprinted many times. A revised and updated edition was published in 1995.\n\nBased on the premise that technologically advanced nations are currently passing through a post-industrial or information revolution, Jones analyzes the unique threats and opportunities of the sudden rise in information to the field such as manufacturing, service employment, and basic income.\n\nJones argues that science and technology have changed the quality, length, and direction of life in the past century far more than politics, education, ideology, or religion. Therefore, inventors such as Thomas Edison and Henry Ford have shaped human experience more broadly and enduringly than Lenin and Hitler.\n\nSome of the book's key points, such as the claim that technological innovation is a major component of economic growth, are more widely accepted now than in 1982. But, to quote Barry Jones himself, \"The central thesis was that people were going to be living longer, far longer, \"but it was possible that they would be working a good deal less\".\" \n\nDue to the rising issues for the labour force, Jones proposed the need to assist workers in income support and choosing to stay or leave the workforce. However, Jones noted in the 1990 edition that the Labor government did not pursue the idea of basic income when it won office in 1983.\n\n\"Sleepers, Wake!\" analyzes the major changes in the workforce and presents the possible political programs to assist the society in profiting from the technological advancements.\n\nThe fourth edition uses 1991 Commonwealth census data as confirmation of his thesis about changes in the labour force.\n\nBarry Jones was Australia's Minister for Science in the Hawke government from 1983 to 1990.\n"}
{"id": "10518546", "url": "https://en.wikipedia.org/wiki?curid=10518546", "title": "Technology life cycle", "text": "Technology life cycle\n\nThe technology life-cycle (TLC) describes the commercial gain of a product through the expense of research and development phase, and the financial return during its \"vital life\". Some technologies, such as steel, paper or cement manufacturing, have a long lifespan (with minor variations in technology incorporated with time) while in other cases, such as electronic or pharmaceutical products, the lifespan may be quite short.\n\nThe TLC associated with a product or technological service is different from product life-cycle (PLC) dealt with in product life-cycle management. The latter is concerned with the life of a product in the marketplace with respect to timing of introduction, marketing measures, and business costs. The \"technology\" underlying the product (for example, that of a uniquely flavoured tea) may be quite marginal but the process of creating and managing its life as a branded product will be very different. \n\nThe technology life cycle is concerned with the time and cost of developing the technology, the timeline of recovering cost, and modes of making the technology yield a profit proportionate to the costs and risks involved. The TLC may, further, be protected during its cycle with patents and trademarks seeking to lengthen the cycle and to maximize the profit from it.\n\nThe \"product\" of the technology may be a commodity such as polyethylene plastic or a sophisticated product like the integrated circuits used in a smartphone.\n\nThe development of a \"competitive product\" or process can have a major effect on the lifespan of the technology, making it shorter. Equally, the loss of intellectual property rights through litigation or loss of its secret elements (if any) through leakages also work to reduce a technology's lifespan. Thus, it is apparent that the \"management\" of the TLC is an important aspect of technology development.\n\nMost new technologies follow a similar technology maturity lifecycle describing the technological maturity of a product. This is not similar to a product life cycle, but applies to an entire technology, or a generation of a technology.\n\nTechnology adoption is the most common phenomenon driving the evolution of industries along the industry lifecycle. After expanding new uses of resources they end with exhausting the efficiency of those processes, producing gains that are first easier and larger over time then exhaustingly more difficult, as the technology matures.\n\nThe TLC may be seen as composed of four phases:\n\nThe shape of the technology lifecycle is often referred to as S-curve.\n\nThere is usually technology hype at the introduction of any new technology, but only after some time has passed can it be judged as mere hype or justified true acclaim.\nBecause of the logistic curve nature of technology adoption, it is difficult to see in the early stages whether the hype is excessive.\n\nThe two errors commonly committed in the early stages of a technology's development are:\nSimilarly, in the later stages, the opposite mistakes can be made relating to the possibilities of technology maturity and market saturation.\n\nThe technology adoption life cycle typically occurs in an S curve, as modelled in diffusion of innovations theory. This is because customers respond to new products in different ways. Diffusion of innovations theory, pioneered by Everett Rogers, posits that people have different levels of readiness for adopting new innovations and that the characteristics of a product affect overall adoption. Rogers classified individuals into five groups: innovators, early adopters, early majority, late majority, and laggards. In terms of the S curve, innovators occupy 2.5%, early adopters 13.5%, early majority 34%, late majority 34%, and laggards 16%.\n\nThe four stages of technology life cycle are as follows:\n\nLarge corporations develop technology for their own benefit and not with the objective of licensing. The tendency to license out technology only appears when there is a threat to the life of the TLC (business gain) as discussed later.\n\nThere are always smaller firms (SMEs) who are inadequately situated to finance the development of innovative R&D in the post-research and early technology phases. By sharing incipient technology under certain conditions, substantial risk financing can come from third parties. This is a form of quasi-licensing which takes different formats. Even large corporates may not wish to bear all costs of development in areas of significant and high risk (e.g. aircraft development) and may seek means of spreading it to the stage that proof-of-concept is obtained.\n\nIn the case of small and medium firms, entities such as venture capitalists or business angels, can enter the scene and help to materialize technologies. Venture capitalists accept both the costs and uncertainties of R&D, and that of market acceptance, in reward for high returns when the technology proves itself. Apart from finance, they may provide networking, management and marketing support. Venture capital connotes financial as well as human capital.\n\nLarger firms may opt for Joint R&D or work in a consortium for the early phase of development. Such vehicles are called strategic alliances – strategic partnerships.\nWith both venture capital funding and strategic (research) alliances, when business gains begin to neutralize development costs (the TLC crosses the X-axis), the ownership of the technology starts to undergo change.\n\nIn the case of smaller firms, venture capitalists help clients enter the stock market for obtaining substantially larger funds for development, maturation of technology, product promotion and to meet marketing costs. A major route is through initial public offering (IPO) which invites risk funding by the public for potential high gain. At the same time, the IPOs enable venture capitalists to attempt to recover expenditures already incurred by them through part sale of the stock pre-allotted to them (subsequent to the listing of the stock on the stock exchange). When the IPO is fully subscribed, the assisted enterprise becomes a corporation and can more easily obtain bank loans, etc. if needed.\n\nStrategic alliance partners, allied on research, pursue separate paths of development with the incipient technology of common origin but pool their accomplishments through instruments such as 'cross-licensing'. Generally, contractual provisions among the members of the consortium allow a member to exercise the option of independent pursuit after joint consultation; in which case the optee owns all subsequent development.\n\nThe ascent stage of the technology usually refers to some point above Point A in the TLC diagram but actually it commences when the R&D portion of the TLC curve inflects (only that the cashflow is negative and unremunerative to Point A). The ascent is the strongest phase of the TLC because it is here that the technology is superior to alternatives and can command premium profit or gain. The slope and duration of the ascent depends on competing technologies entering the domain, although they may not be \"as successful\" in that period. Strongly patented technology extends the duration period.\n\nThe TLC begins to flatten out (the region shown as M) when equivalent or challenging technologies come into the competitive space and begin to eat away marketshare.\n\nTill this stage is reached, the technology-owning firm would tend to exclusively enjoy its profitability, preferring \"not\" to license it. If an overseas opportunity does present itself, the firm would prefer to set up a controlled subsidiary rather than license a third party.\n\nThe maturity phase of the technology is a period of stable and remunerative income but its competitive viability can persist over the larger timeframe marked by its 'vital life'. However, there may be a tendency to license out the technology to third parties during this stage to lower risk of decline in profitability (or competitivity) and to expand financial opportunity.\n\nThe exercise of this option is, generally, inferior to seeking participatory exploitation; in other words, engagement in joint venture, typically in regions where the technology would be in the \"ascent phase\",as say, a developing country. In addition to providing financial opportunity it allows the technology-owner a degree of control over its use. Gain flows from the two streams of investment-based and royalty incomes. Further, the vital life of the technology is enhanced in such strategy.\n\nAfter reaching a point such as D in the above diagram, the earnings from the technology begin to decline rather rapidly. To prolong the life cycle, owners of technology might try to license it out at some point L when it can still be attractive to firms in other markets. This, then, traces the lengthening path, LL'. Further, since the decline is the result of competing rising technologies in this space, licenses may be attracted to the general lower cost of the older technology (than what prevailed during its vital life).\n\nLicenses obtained in this phase are 'straight licenses'. They are free of direct control from the owner of the technology (as would otherwise apply, say, in the case of a joint-venture). Further, there may be fewer restrictions placed on the licensee in the employment of the technology.\n\nThe utility, viability, and thus the cost of straight-licenses depends on the estimated 'balance life' of the technology. For instance, should the key patent on the technology have expired, or would expire in a short while, the residual viability of the technology may be limited, although balance life may be governed by other criteria such as knowhow which could have a longer life if properly protected.\n\n\"It is important to note that the license has no way of knowing the stage at which the prime, and competing technologies, are on their TLCs\". It would, of course, be evident to competing licensor firms, and to the originator, from the growth, saturation or decline of the profitability of their operations.\n\nThe license may, however, be able to approximate the stage by vigorously negotiating with the licensor and competitors to determine costs and licensing terms. A lower cost, or easier terms, \"may\" imply a declining technology.\n\nIn any case, access to technology in the decline phase is a large risk that the licensee accepts. (In a joint-venture this risk is substantially reduced by licensor sharing it). Sometimes, financial guarantees from the licensor may work to reduce such risk and can be negotiated.\n\nThere are instances when, even though the technology declines to becoming a technique, it may still contain important knowledge or experience which the licensee firm cannot learn of without help from the originator. This is often the form that \"technical service\" and \"technical assistance\" contracts take (encountered often in developing country contracts). Alternatively, consulting agencies may fill this role.\n\nAccording to the Encyclopedia of Earth, \"In the simplest formulation, innovation can be thought of as being composed of research, development, demonstration, and deployment.\"\n\n\"Technology development cycle\" describes the process of a new technology through the stages of technological maturity:\n\n"}
{"id": "16225496", "url": "https://en.wikipedia.org/wiki?curid=16225496", "title": "The Science Network", "text": "The Science Network\n\nThe Science Network (TSN) is a non-profit virtual forum dedicated to science and its impact on society. Initially conceived in 2003 by Roger Bingham and Terry Sejnowski as a cable science network, TSN would soon become a global digital platform. TSN currently offers free access to over 1100 videos of lectures from scientific meetings and long form one-on-one conversations with prominent scientists and communicators of science, including Neil deGrasse Tyson, V.S. Ramachandran, Helen S. Mayberg, and Barbara Landau, on topics including education, aging, neuroscience, and stem cells. As part of its mission, TSN has also sponsored and co-sponsored scientific forums, such as the landmark Symposium and Town Hall meeting, \"Stem cells: science, ethics and politics at the crossroads\", held at the Salk Institute in 2004 and the \"Beyond Belief\" conference series.\n\nTSN's signature series \"Beyond Belief\" was conceived to bring together a community of scientists, philosophers, scholars from the humanities, and social commentators. Speakers at these meetings have included Steven Weinberg, Richard Dawkins, Sam Harris, Harry Kroto, Neil deGrasse Tyson, and Stuart Kauffman. So far, the following three \"Beyond Belief\" conferences were organized:\n\n\"\", the first of The Science Network's annual Beyond Belief symposia, held from November 5 to November 7, 2006, was described by \"The New York Times\", as \"a free-for-all on science and religion,\" which seemed at times like \"the founding convention for a political party built on a single plank: in a world dangerously charged with ideology, science needs to take on an evangelical role, vying with religion as teller of the greatest story ever told.\" According to participant Melvin Konner, however, the event came to resemble a \"den of vipers” debating the issue, \"Should we bash religion with a crowbar or only with a baseball bat?”\n\nNew Scientist summed up the topics to be discussed as a list of three questions:\n\n\nSpeakers included physicists Steven Weinberg, Lawrence Krauss, author Sam Harris, biologist Joan Roughgarden, and astrophysicist Neil deGrasse Tyson.\n\n\"Beyond Belief: Enlightenment 2.0\" was the second annual symposium and was held from 31 October to 2 November 2007 at the Frederic de Hoffmann Auditorium of the Salk Institute for Biological Studies.\n\n\"Beyond Belief: Candles in the Dark\" was the third annual Beyond Belief symposium. This event was organized by The Science Network and held from 3 October to 6 October 2008 in La Jolla, CA.\n"}
{"id": "31883", "url": "https://en.wikipedia.org/wiki?curid=31883", "title": "Uncertainty principle", "text": "Uncertainty principle\n\nIn quantum mechanics, the uncertainty principle (also known as Heisenberg's uncertainty principle) is any of a variety of mathematical inequalities asserting a fundamental limit to the precision with which certain pairs of physical properties of a particle, known as complementary variables, such as position \"x\" and momentum \"p\", can be known.\n\nIntroduced first in 1927, by the German physicist Werner Heisenberg, it states that the more precisely the position of some particle is determined, the less precisely its momentum can be known, and vice versa. The formal inequality relating the standard deviation of position \"σ\" and the standard deviation of momentum \"σ\" was derived by Earle Hesse Kennard later that year and by Hermann Weyl in 1928:\n\nwhere is the reduced Planck constant, ).\n\nHistorically, the uncertainty principle has been confused with a somewhat similar effect in physics, called the observer effect, which notes that measurements of certain systems cannot be made without affecting the systems, that is, without changing something in a system. Heisenberg utilized such an observer effect at the quantum level (see below) as a physical \"explanation\" of quantum uncertainty. It has since become clearer, however, that the uncertainty principle is inherent in the properties of all wave-like systems, and that it arises in quantum mechanics simply due to the matter wave nature of all quantum objects. Thus, \"the uncertainty principle actually states a fundamental property of quantum systems and is not a statement about the observational success of current technology\". It must be emphasized that \"measurement\" does not mean only a process in which a physicist-observer takes part, but rather any interaction between classical and quantum objects regardless of any observer.\n\nSince the uncertainty principle is such a basic result in quantum mechanics, typical experiments in quantum mechanics routinely observe aspects of it. Certain experiments, however, may deliberately test a particular form of the uncertainty principle as part of their main research program. These include, for example, tests of number–phase uncertainty relations in superconducting or quantum optics systems. Applications dependent on the uncertainty principle for their operation include extremely low-noise technology such as that required in gravitational wave interferometers.\n\nThe uncertainty principle is not readily apparent on the macroscopic scales of everyday experience. So it is helpful to demonstrate how it applies to more easily understood physical situations. Two alternative frameworks for quantum physics offer different explanations for the uncertainty principle. The wave mechanics picture of the uncertainty principle is more visually intuitive, but the more abstract matrix mechanics picture formulates it in a way that generalizes more easily.\n\nMathematically, in wave mechanics, the uncertainty relation between position and momentum arises because the expressions of the wavefunction in the two corresponding orthonormal bases in Hilbert space are Fourier transforms of one another (i.e., position and momentum are conjugate variables). A nonzero function and its Fourier transform cannot both be sharply localized. A similar tradeoff between the variances of Fourier conjugates arises in all systems underlain by Fourier analysis, for example in sound waves: A pure tone is a sharp spike at a single frequency, while its Fourier transform gives the shape of the sound wave in the time domain, which is a completely delocalized sine wave. In quantum mechanics, the two key points are that the position of the particle takes the form of a matter wave, and momentum is its Fourier conjugate, assured by the de Broglie relation , where is the wavenumber.\n\nIn matrix mechanics, the mathematical formulation of quantum mechanics, any pair of non-commuting self-adjoint operators representing observables are subject to similar uncertainty limits. An eigenstate of an observable represents the state of the wavefunction for a certain measurement value (the eigenvalue). For example, if a measurement of an observable is performed, then the system is in a particular eigenstate of that observable. However, the particular eigenstate of the observable need not be an eigenstate of another observable : If so, then it does not have a unique associated measurement for it, as the system is not in an eigenstate of that observable.\n\nAccording to the de Broglie hypothesis, every object in the universe is a wave, i.e., a situation which gives rise to this phenomenon. The position of the particle is described by a wave function formula_1. The time-independent wave function of a single-moded plane wave of wavenumber \"k\" or momentum \"p\" is\n\nThe Born rule states that this should be interpreted as a probability density amplitude function in the sense that the probability of finding the particle between \"a\" and \"b\" is\n\nIn the case of the single-moded plane wave, formula_4 is a uniform distribution. In other words, the particle position is extremely uncertain in the sense that it could be essentially anywhere along the wave packet. \n\nOn the other hand, consider a wave function that is a sum of many waves, which we may write this as\n\nwhere \"A\" represents the relative contribution of the mode \"p\" to the overall total. The figures to the right show how with the addition of many plane waves, the wave packet can become more localized. We may take this a step further to the continuum limit, where the wave function is an integral over all possible modes\n\nwith formula_7 representing the amplitude of these modes and is called the wave function in momentum space. In mathematical terms, we say that formula_7 is the \"Fourier transform\" of formula_9 and that \"x\" and \"p\" are conjugate variables. Adding together all of these plane waves comes at a cost, namely the momentum has become less precise, having become a mixture of waves of many different momenta.\n\nOne way to quantify the precision of the position and momentum is the standard deviation \"σ\". Since formula_4 is a probability density function for position, we calculate its standard deviation.\n\nThe precision of the position is improved, i.e. reduced σ, by using many plane waves, thereby weakening the precision of the momentum, i.e. increased σ. Another way of stating this is that σ and σ have an inverse relationship or are at least bounded from below. This is the uncertainty principle, the exact limit of which is the Kennard bound. Click the \"show\" button below to see a semi-formal derivation of the Kennard inequality using wave mechanics.\n(Ref ) \n\nIn matrix mechanics, observables such as position and momentum are represented by self-adjoint operators. When considering pairs of observables, an important quantity is the \"commutator\". For a pair of operators and , one defines their commutator as\nIn the case of position and momentum, the commutator is the canonical commutation relation\n\nThe physical meaning of the non-commutativity can be understood by considering the effect of the commutator on position and momentum eigenstates. Let formula_13 be a right eigenstate of position with a constant eigenvalue . By definition, this means that formula_14 Applying the commutator to formula_13 yields\nwhere is the identity operator.\n\nSuppose, for the sake of proof by contradiction, that formula_13 is also a right eigenstate of momentum, with constant eigenvalue . If this were true, then one could write\nOn the other hand, the above canonical commutation relation requires that\nThis implies that no quantum state can simultaneously be both a position and a momentum eigenstate.\n\nWhen a state is measured, it is projected onto an eigenstate in the basis of the relevant observable. For example, if a particle's position is measured, then the state amounts to a position eigenstate. This means that the state is \"not\" a momentum eigenstate, however, but rather it can be represented as a sum of multiple momentum basis eigenstates. In other words, the momentum must be less precise. This precision may be quantified by the standard deviations, \n\nAs in the wave mechanics interpretation above, one sees a tradeoff between the respective precisions of the two, quantified by the uncertainty principle.\n\nThe most common general form of the uncertainty principle is the \"Robertson uncertainty relation\".\n\nFor an arbitrary Hermitian operator formula_22 we can associate a standard deviation\n\nwhere the brackets formula_24 indicate an expectation value. For a pair of operators formula_25 and formula_26, we may define their \"commutator\" as\n\nIn this notation, the Robertson uncertainty relation is given by\n\nThe Robertson uncertainty relation immediately follows from a slightly stronger inequality, the \"Schrödinger uncertainty relation\",\n\nwhere we have introduced the \"anticommutator\",\n\nSince the Robertson and Schrödinger relations are for general operators, the relations can be applied to any two observables to obtain specific uncertainty relations. A few of the most common relations found in the literature are given below.\nSuppose we consider a quantum particle on a ring, where the wave function depends on an angular variable formula_41, which we may take to lie in the interval formula_42. Define \"position\" and \"momentum\" operators formula_25 and formula_26 by\n\nand\n\nwhere we impose periodic boundary conditions on formula_26. Note that the definition of formula_25 depends on our choice to have formula_41 range from 0 to formula_50. These operators satisfy the usual commutation relations for position and momentum operators, formula_51.\n\nNow let formula_52 be any of the eigenstates of formula_26, which are given by formula_54. Note that these states are normalizable, unlike the eigenstates of the momentum operator on the line. Note also that the operator formula_25 is bounded, since formula_41 ranges over a bounded interval. Thus, in the state formula_52, the uncertainty of formula_58 is zero and the uncertainty of formula_59 is finite, so that \nAlthough this result appears to violate the Robertson uncertainty principle, the paradox is resolved when we note that formula_52 is not in the domain of the operator formula_62, since multiplication by formula_41 disrupts the periodic boundary conditions imposed on formula_26. Thus, the derivation of the Robertson relation, which requires formula_65 and formula_66 to be defined, does not apply. (These also furnish an example of operators satisfying the canonical commutation relations but not the Weyl relations.)\n\nFor the usual position and momentum operators formula_67 and formula_68 on the real line, no such counterexamples can occur. As long as formula_69 and formula_70 are defined in the state formula_52, the Heisenberg uncertainty principle holds, even if formula_52 fails to be in the domain of formula_73 or of formula_74.\n\nConsider a one-dimensional quantum harmonic oscillator (QHO). It is possible to express the position and momentum operators in terms of the creation and annihilation operators:\n\nUsing the standard rules for creation and annihilation operators on the eigenstates of the QHO,\nthe variances may be computed directly,\nThe product of these standard deviations is then\n\nIn particular, the above Kennard bound is saturated for the ground state , for which the probability density is just the normal distribution.\n\nIn a quantum harmonic oscillator of characteristic angular frequency ω, place a state that is offset from the bottom of the potential by some displacement \"x\" as\nwhere Ω describes the width of the initial state but need not be the same as ω. Through integration over the , we can solve for the -dependent solution. After many cancelations, the probability densities reduce to\nwhere we have used the notation formula_85 to denote a normal distribution of mean μ and variance σ. Copying the variances above and applying trigonometric identities, we can write the product of the standard deviations as\n\nFrom the relations\n\nwe can conclude the following: (the right most equality holds only when Ω = \"ω\") .\n\nA coherent state is a right eigenstate of the annihilation operator,\nwhich may be represented in terms of Fock states as\n\nOne expects that the factor may be replaced by , \nwhich is only known if either or is convex.\n\nThe mathematician G. H. Hardy formulated the following uncertainty principle: it is not possible for and to both be \"very rapidly decreasing\". Specifically, if in formula_91 is such that\nand\n\nthen, if , while if , then there is a polynomial of degree such that\n\nThis was later improved as follows: if formula_96 is such that\n\nthen\nwhere is a polynomial of degree and is a real positive definite matrix.\n\nThis result was stated in Beurling's complete works without proof and proved in Hörmander (the case formula_99) and Bonami, Demange, and Jaming for the general case. Note that Hörmander–Beurling's version implies the case in Hardy's Theorem while the version by Bonami–Demange–Jaming covers the full strength of Hardy's Theorem. A different proof of Beurling's theorem based on Liouville's theorem appeared in\nref.\n\nA full description of the case as well as the following extension to Schwartz class distributions appears in ref.\n\nTheorem. If a tempered distribution formula_100 is such that\n\nand\nthen\nfor some convenient polynomial and real positive definite matrix of type .\n\nWerner Heisenberg formulated the uncertainty principle at Niels Bohr's institute in Copenhagen, while working on the mathematical foundations of quantum mechanics.\n\nIn 1925, following pioneering work with Hendrik Kramers, Heisenberg developed matrix mechanics, which replaced the ad hoc old quantum theory with modern quantum mechanics. The central premise was that the classical concept of motion does not fit at the quantum level, as electrons in an atom do not travel on sharply defined orbits. Rather, their motion is smeared out in a strange way: the Fourier transform of its time dependence only involves those frequencies that could be observed in the quantum jumps of their radiation.\n\nHeisenberg's paper did not admit any unobservable quantities like the exact position of the electron in an orbit at any time; he only allowed the theorist to talk about the Fourier components of the motion. Since the Fourier components were not defined at the classical frequencies, they could not be used to construct an exact trajectory, so that the formalism could not answer certain overly precise questions about where the electron was or how fast it was going.\n\nIn March 1926, working in Bohr's institute, Heisenberg realized that the non-commutativity implies the uncertainty principle. This implication provided a clear physical interpretation for the non-commutativity, and it laid the foundation for what became known as the Copenhagen interpretation of quantum mechanics. Heisenberg showed that the commutation relation implies an uncertainty, or in Bohr's language a complementarity. Any two variables that do not commute cannot be measured simultaneously—the more precisely one is known, the less precisely the other can be known. Heisenberg wrote:It can be expressed in its simplest form as follows: One can never know with perfect accuracy both of those two important factors which determine the movement of one of the smallest particles—its position and its velocity. It is impossible to determine accurately \"both\" the position and the direction and speed of a particle \"at the same instant\".\n\nIn his celebrated 1927 paper, \"Über den anschaulichen Inhalt der quantentheoretischen Kinematik und Mechanik\" (\"On the Perceptual Content of Quantum Theoretical Kinematics and Mechanics\"), Heisenberg established this expression as the minimum amount of unavoidable momentum disturbance caused by any position measurement, but he did not give a precise definition for the uncertainties Δx and Δp. Instead, he gave some plausible estimates in each case separately. In his Chicago lecture he refined his principle:\n\nKennard in 1927 first proved the modern inequality:\n\nwhere , and , are the standard deviations of position and momentum. Heisenberg only proved relation () for the special case of Gaussian states.\n\nThroughout the main body of his original 1927 paper, written in German, Heisenberg used the word, \"Ungenauigkeit\" (\"indeterminacy\"),\nto describe the basic theoretical principle. Only in the endnote did he switch to the word, \"Unsicherheit\" (\"uncertainty\"). When the English-language version of Heisenberg's textbook, \"The Physical Principles of the Quantum Theory\", was published in 1930, however, the translation \"uncertainty\" was used, and it became the more commonly used term in the English language thereafter.\n\nThe principle is quite counter-intuitive, so the early students of quantum theory had to be reassured that naive measurements to violate it were bound always to be unworkable. One way in which Heisenberg originally illustrated the intrinsic impossibility of violating the uncertainty principle is by utilizing the observer effect of an imaginary microscope as a measuring device.\n\nHe imagines an experimenter trying to measure the position and momentum of an electron by shooting a photon at it.\n\nThe combination of these trade-offs implies that no matter what photon wavelength and aperture size are used, the product of the uncertainty in measured position and measured momentum is greater than or equal to a lower limit, which is (up to a small numerical factor) equal to Planck's constant. Heisenberg did not care to formulate the uncertainty principle as an exact limit (which is elaborated below), and preferred to use it instead, as a heuristic quantitative statement, correct up to small numerical factors, which makes the radically new noncommutativity of quantum mechanics inevitable.\n\nThe Copenhagen interpretation of quantum mechanics and Heisenberg's Uncertainty Principle were, in fact, seen as twin targets by detractors who believed in an underlying determinism and realism. According to the Copenhagen interpretation of quantum mechanics, there is no fundamental reality that the quantum state describes, just a prescription for calculating experimental results. There is no way to say what the state of a system fundamentally is, only what the result of observations might be.\n\nAlbert Einstein believed that randomness is a reflection of our ignorance of some fundamental property of reality, while Niels Bohr believed that the probability distributions are fundamental and irreducible, and depend on which measurements we choose to perform. Einstein and Bohr debated the uncertainty principle for many years.\n\nWolfgang Pauli called Einstein's fundamental objection to the uncertainty principle \"the ideal of the detached observer\" (phrase translated from the German):\n\nThe first of Einstein's thought experiments challenging the uncertainty principle went as follows:\n\nBohr's response was that the wall is quantum mechanical as well, and that to measure the recoil to accuracy , the momentum of the wall must be known to this accuracy before the particle passes through. This introduces an uncertainty in the position of the wall and therefore the position of the slit equal to , and if the wall's momentum is known precisely enough to measure the recoil, the slit's position is uncertain enough to disallow a position measurement.\n\nA similar analysis with particles diffracting through multiple slits is given by Richard Feynman.\n\nBohr was present when Einstein proposed the thought experiment which has become known as Einstein's box. Einstein argued that \"Heisenberg's uncertainty equation implied that the uncertainty in time was related to the uncertainty in energy, the product of the two being related to Planck's constant.\" Consider, he said, an ideal box, lined with mirrors so that it can contain light indefinitely. The box could be weighed before a clockwork mechanism opened an ideal shutter at a chosen instant to allow one single photon to escape. \"We now know, explained Einstein, precisely the time at which the photon left the box.\" \"Now, weigh the box again. The change of mass tells the energy of the emitted light. In this manner, said Einstein, one could measure the energy emitted and the time it was released with any desired precision, in contradiction to the uncertainty principle.\"\n\nBohr spent a sleepless night considering this argument, and eventually realized that it was flawed. He pointed out that if the box were to be weighed, say by a spring and a pointer on a scale, \"since the box must move vertically with a change in its weight, there will be uncertainty in its vertical velocity and therefore an uncertainty in its height above the table. ... Furthermore, the uncertainty about the elevation above the earth's surface will result in an uncertainty in the rate of the clock,\" because of Einstein's own theory of gravity's effect on time.\n\"Through this chain of uncertainties, Bohr showed that Einstein's light box experiment could not simultaneously measure exactly both the energy of the photon and the time of its escape.\"\n\nBohr was compelled to modify his understanding of the uncertainty principle after another thought experiment by Einstein. In 1935, Einstein, Podolsky and Rosen (see EPR paradox) published an analysis of widely separated entangled particles. Measuring one particle, Einstein realized, would alter the probability distribution of the other, yet here the other particle could not possibly be disturbed. This example led Bohr to revise his understanding of the principle, concluding that the uncertainty was not caused by a direct interaction.\n\nBut Einstein came to much more far-reaching conclusions from the same thought experiment. He believed the \"natural basic assumption\" that a complete description of reality would have to predict the results of experiments from \"locally changing deterministic quantities\" and therefore would have to include more information than the maximum possible allowed by the uncertainty principle.\n\nIn 1964, John Bell showed that this assumption can be falsified, since it would imply a certain inequality between the probabilities of different experiments. Experimental results confirm the predictions of quantum mechanics, ruling out Einstein's basic assumption that led him to the suggestion of his \"hidden variables\". These hidden variables may be \"hidden\" because of an illusion that occurs during observations of objects that are too large or too small. This illusion can be likened to rotating fan blades that seem to pop in and out of existence at different locations and sometimes seem to be in the same place at the same time when observed. This same illusion manifests itself in the observation of subatomic particles. Both the fan blades and the subatomic particles are moving so fast that the illusion is seen by the observer. Therefore, it is possible that there would be predictability of the subatomic particles behavior and characteristics to a recording device capable of very high speed tracking...Ironically this fact is one of the best pieces of evidence supporting Karl Popper's philosophy of invalidation of a theory by falsification-experiments. That is to say, here Einstein's \"basic assumption\" became falsified by experiments based on Bell's inequalities. For the objections of Karl Popper to the Heisenberg inequality itself, see below.\n\nWhile it is possible to assume that quantum mechanical predictions are due to nonlocal, hidden variables, and in fact David Bohm invented such a formulation, this resolution is not satisfactory to the vast majority of physicists. The question of whether a random outcome is predetermined by a nonlocal theory can be philosophical, and it can be potentially intractable. If the hidden variables are not constrained, they could just be a list of random digits that are used to produce the measurement outcomes. To make it sensible, the assumption of nonlocal hidden variables is sometimes augmented by a second assumption—that the size of the observable universe puts a limit on the computations that these variables can do. A nonlocal theory of this sort predicts that a quantum computer would encounter fundamental obstacles when attempting to factor numbers of approximately 10,000 digits or more; a potentially achievable task in quantum mechanics.\n\nKarl Popper approached the problem of indeterminacy as a logician and metaphysical realist. He disagreed with the application of the uncertainty relations to individual particles rather than to ensembles of identically prepared particles, referring to them as \"statistical scatter relations\". In this statistical interpretation, a \"particular\" measurement may be made to arbitrary precision without invalidating the quantum theory. This directly contrasts with the Copenhagen interpretation of quantum mechanics, which is non-deterministic but lacks local hidden variables.\n\nIn 1934, Popper published \"Zur Kritik der Ungenauigkeitsrelationen\" (\"Critique of the Uncertainty Relations\") in \"Naturwissenschaften\", and in the same year \"Logik der Forschung\" (translated and updated by the author as \"The Logic of Scientific Discovery\" in 1959), outlining his arguments for the statistical interpretation. In 1982, he further developed his theory in \"Quantum theory and the schism in Physics\", writing:\n[Heisenberg's] formulae are, beyond all doubt, derivable \"statistical formulae\" of the quantum theory. But they have been \"habitually misinterpreted\" by those quantum theorists who said that these formulae can be interpreted as determining some upper limit to the \"precision of our measurements\". [original emphasis]\n\nPopper proposed an experiment to falsify the uncertainty relations, although he later withdrew his initial version after discussions with Weizsäcker, Heisenberg, and Einstein; this experiment may have influenced the formulation of the EPR experiment.\n\nThe many-worlds interpretation originally outlined by Hugh Everett III in 1957 is partly meant to reconcile the differences between Einstein's and Bohr's views by replacing Bohr's wave function collapse with an ensemble of deterministic and independent universes whose \"distribution\" is governed by wave functions and the Schrödinger equation. Thus, uncertainty in the many-worlds interpretation follows from each observer within any universe having no knowledge of what goes on in the other universes.\n\nSome scientists including Arthur Compton and Martin Heisenberg have suggested that the uncertainty principle, or at least the general probabilistic nature of quantum mechanics, could be evidence for the two-stage model of free will. One critique, however, is that apart from the basic role of quantum mechanics as a foundation for chemistry, nontrivial biological mechanisms requiring quantum mechanics are unlikely, due to the rapid decoherence time of quantum systems at room temperature. The standard view, however, is that this decoherence is overcome by both screening and decoherence-free subspaces found in biological cells.\n\nThere is reason to believe that violating the uncertainty principle also strongly implies the violation of the second law of thermodynamics.\n\n"}
{"id": "7217838", "url": "https://en.wikipedia.org/wiki?curid=7217838", "title": "Vancouver system", "text": "Vancouver system\n\nThe Vancouver system, also known as Vancouver reference style or the author–number system, is a citation style that uses numbers within the text that refer to numbered entries in the reference list. It is popular in the physical sciences and is one of two referencing systems normally used in medicine, the other being the author–date, or \"Harvard\", system. Vancouver style is used by MEDLINE and PubMed.\n\nHundreds of scientific journals use author-number systems. They all follow the same essential logic (that is, numbered citations pointing to numbered list entries), although the trivial details of the output mask, such as punctuation, casing of titles, and italic, vary widely among them. They have existed for over a century; the names \"Vancouver system\" or \"Vancouver style\" have existed since 1978. The latest version of the latter is \"Citing Medicine\", per the \"References > Style and Format\" section of the ICMJE Recommendations.\n\nIn the broad sense, the Vancouver system refers to any author-number system regardless of the formatting details. A narrower definition of the Vancouver system refers to a specific author-number format specified by the ICMJE Recommendations (Uniform Requirements for Manuscripts, URM). For example, the AMA reference style is Vancouver style in the broad sense because it is an author-number system that conforms to the URM, but not in the narrow sense because its formatting differs in some minor details from the NLM/PubMed style (such as what is italicized and whether the citation numbers are bracketed).\n\nAuthor–number systems have existed for over a century and throughout that time have been one of the main types of citation style in scientific journals (the other being author–date). In 1978, a committee of editors from various medical journals, the International Committee of Medical Journal Editors (ICMJE), met in Vancouver, BC, Canada to agree to a unified set of requirements for the articles of such journals. This meeting led to the establishment of the Uniform Requirements for Manuscripts Submitted to Biomedical Journals (URMs). Part of the URMs is the reference style, for which the ICMJE selected the long-established author–number principle.\n\nThe URMs were developed 15 years before the World Wide Web debuted. During those years, they were published as articles or supplements in various ICMJE member journals. These included the 1991 BMJ publication, the 1995 \"CMAJ\" publication and the 1997 \"Annals of Internal Medicine\" publication. In the late 1990s and early 2000s, journals were asked to cite the 1997 \"JAMA\" version when reprinting the \"Uniform requirements\".\n\nIn the early 2000s, with the Web having become a major force in academic life, the idea gradually took hold that the logical home for the latest edition of the URMs would be the ICMJE website itself (as opposed to whichever journal article or supplement had most recently published an update). For example, as of 2004, the editors of \"Haematologica\" decided simply to invite their authors to visit www.icmje.org for the 2003 revision of the \"Uniform requirements\".\n\nSince the early to mid-2000s, the United States National Library of Medicine (which runs MEDLINE and PubMed) has hosted the ICMJE's \"Sample References\" pages. Around 2007, the NLM created \"Citing Medicine\", its style guide for citation style, as a new home for the style's details. The ICMJE Recommendations now point to \"Citing Medicine\" as the home for the formatting details of Vancouver style. For example, in the December 2013 edition of the ICMJE Recommendations, the relevant paragraph is IV.A.3.g.ii. (\"References > Style and Format\").\n\nReferences are numbered consecutively in order of appearance in the text – they are identified by Arabic numerals in parentheses (1), square brackets [1], superscript, or a combination. The number usually appears at the end of the material it supports, and an entry in the reference list would give full bibliographical information for the source:\n\nAnd the entry in the reference list would be: 1. \n\nSeveral descriptions of the Vancouver system say that the number can be placed \"outside\" the text punctuation to avoid disruption to the flow of the text, \"or\" be placed \"inside\" the text punctuation, and that there are different cultures in different traditions. The first method is recommended by some universities and colleges, while the latter method is required by scientific publications such as the MLA and IEEE except for in the end of a block quotation. (IEEE are using Vancouver style labels within brackets, for example [1] to cite the first reference in the list, but otherwise refer to Chicago Style Manual.) The original Vancouver system documents (the ICMJE recommendations and Uniform Requirements for Manuscripts Submitted to Biomedical Journals) do not discuss placement of the citation mark.\n\nDifferent formats exist for different types of sources, e.g. books, journal articles etc. Author names are abbreviated to at most two initials. Although \"Citing Medicine\" does not explicitly mandate merging initials (e.g. \"R. K.\" would be merged into \"RK\"), the examples used throughout the book do.\n\n\nAs an option, if a journal carries continuous pagination throughout a volume (as many medical journals do), the month and issue number may be omitted.\n\n\nThe NLM lists all authors for all articles, because it is appropriate for capturing all authors and all of their publications in the MEDLINE database to be found by searches. However, in the reference lists of articles, most journals truncate the list after 3 or 6 names, followed by \"et al.\" (which most medical journals do not italicize):\n\n\nOptionally, a unique identifier (such as the article's DOI or PMID) may be added to the citation:\n\n\nNLM elides ending page numbers and uses a hyphen as the range indicating character (184-5). Some journals do likewise, whereas others expand the ending page numbers in full (184-185), use an en dash instead of a hyphen (184–5), or both (184–185).\n\nVirtually all medical journal articles are published online. Many are published online only, and many others are published online ahead of print. For the date of online publication, at the end of the citation NLM puts \"[Epub Year Mon Day]\" (for online-only publication) or \"[Epub ahead of print]\" for online ahead of print (with the month and day following the year in its normal position). In contrast, AMA style puts \"[published online Month Day, Year]\" at the end of the article title. It no longer uses the term \"Epub\" and no longer includes the words \"ahead of print\". It omits the year from its normal location after the journal title abbreviation if there is no print data to give (online-only publication).\n\nThe titles of journals are abbreviated. There are no periods in the abbreviation. A period comes after the abbreviation, delimiting it from the next field. The abbreviations are standardized. The standardization was formerly incomplete and internal to organizations such as NLM. It is now formalized at the supraorganizational level by documents including \"Citing Medicine\" at Appendix A: Abbreviations for Commonly Used English Words in Journal Titles, ANSI Z39.5, ISO 4: Information and documentation -- Rules for the abbreviation of title words and titles of publications, and the ISSN.org List of Title Word Abbreviations (LTWA).\n\nAs per journal articles in English:\n\n\nThe NLM adds an English translation of the title enclosed in square brackets right after the title. The language is specified in full after the location (pagination), followed by a period.\n\n\n\n\n\nMany medical institutions maintain their own style guides, with information on how to cite sources:\n"}
{"id": "42716127", "url": "https://en.wikipedia.org/wiki?curid=42716127", "title": "Xenology", "text": "Xenology\n\nXenology is the scientific study of extraterrestrial life. Derived from the Greek \"xenos\", which as a substantive has the meaning \"stranger, wanderer, refugee\" and as an adjective \"foreign, alien, strange, unusual.\"\n\nIt is used to denote a hypothetical science whose object of study would be extraterrestrial societies developed by alien lifeforms. In science fiction criticism and studies the term has been advocated by writers such as David Brin (\"Xenology: The New Science of Asking 'Who's Out There?'\" \"Analog\", 26 April 1983) as an analogue of (terrestrial) ethnology. By extension the term may also refer to the fictional creation of \"alternative humankinds\".\n\nInstances in which Xenology was referred to in a work of Science Fiction include the Brothers Strugatsky's 1972 novel \"Roadside Picnic\". In section three of which one of the character's, a noble laureate by the name of Valentine Pillman, explains Xenology as \"an unnatural mixture of science fiction and formal logic. At its core is a flawed assumption—that an alien race would be psychologically human.\"\n\nThe term xenology was employed by German Indologist Wilhelm Halbfass in his \"Indien und Europa, Perspektiven ihrer geistigen Begegnung\" (India and Europe: Perspectives on Their Spiritual Encounter) (1981) to denote the study of the ethnocentric views held by societies with regard to different classes of foreigner, in other words the positive or negative ways in which a given culture defines those outside or alien to it. Xenology is thus the study of the various modalities whereby self and otherness are defined \"within a historically complex collision of cultures\".\n\nRobert A. Freitas Jr. self-published a book on the subject, \"Xenology: An Introduction to the Scientific Study of Extraterrestrial Life, Intelligence, and Civilization\" (XRI, 1979). Freitas argued for the primacy of the term in the context of extraterrestrial life in a 1983 letter to the journal Nature.\n"}
