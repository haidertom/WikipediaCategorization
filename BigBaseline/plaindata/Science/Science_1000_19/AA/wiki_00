{"id": "16067696", "url": "https://en.wikipedia.org/wiki?curid=16067696", "title": "Amen Clinics", "text": "Amen Clinics\n\nAmen Clinics is a group of mental and physical health clinics that work on the treatment of mood and behavior disorders. It was founded in 1989 by Daniel G. Amen a self-help guru and psychiatrist. The clinics perform clinical evaluations and brain SPECT (single photon emission computed tomography) imaging to diagnose and treat their patients. Amen Clinics uses SPECT scans, a type of brain-imaging technology, to measure neural activity through blood flow. It has a database of more than 100,000 functional brain scans from patients in 111 countries.\n\nAmen Clinics has locations in Newport Beach, California; San Francisco, California; Atlanta, Georgia; Reston, Virginia; Bellevue, Washington; and New York City.\n\nThe American Psychiatric Association have criticized the clinical appropriateness of Amen's use of brain scans, stating, \"the clinical utility of neuroimaging techniques for planning of individualized treatment has not yet been shown.\"\n\nAmen Clinics was founded in 1989. It has been using brain SPECT in an attempt to diagnose and treat psychiatric illness since 1991. Amen Clinics incorporates questionnaires, clinical histories, and clinical interviews in its practice. Some Amen Clinics locations also use quantitative electroencephalography as a diagnostic tool. Amen Clinics has scanned 50,000 people at an estimated cost of $170 million according to Daniel Amen.\n\nAs of 2014, Amen Clinics had a database of more than 100,000 functional brain scans. The subjects are from 111 countries with ages from 9 months to 101 years old. The database was funded in part by Seeds Foundation in Hong Kong, and developed by Daniel Amen with a team of researchers including Kristen Willeumier.\n\nAmen Clinics has worked to treat athletics-related brain damage for professional athletes, including current and 117 former National Football League players.\n\nAmen Clinics uses SPECT scans to measure blood flow and activity patterns in the brain. The company also uses diagnostics such as questionnaires, clinical histories, and clinical interviews. Amen Clinics claims that SPECT scans enable doctors to tailor treatment to individual patients' brains. A retrospective study released by Amen in 2010 showed that \"regional cerebral blood flow, as measured by SPECT, predicted stimulant response in 29 of 157 patients.\"\n\nHarriet Hall has written critically about SPECT scans in articles for Quackwatch and for the \"Science-Based Medicine\" website. Hall accuses the clinics of misrepresenting an unproven treatment as effective, of concealing important warning information, and of creating false hopes by promising things that can't be done. She dismisses the scans as \"pretty pictures\" and says that although Amen himself seems to believe in his approach, \"humans are very good at fooling themselves\".\n\nA 2011 paper co-authored by the neuroscientist Anjan Chatterjee discussed example cases that were found on the Amen Clinic's website. The paper noted that the example cases \"violate the standard of care\" because a normal clinical diagnosis would have been sufficient and functional neuroimaging was unnecessary. According to the American Psychiatric Association, \"the clinical utility of neuroimaging techniques for planning of individualized treatment has not yet been shown.\"\n"}
{"id": "20948", "url": "https://en.wikipedia.org/wiki?curid=20948", "title": "Brainwashing", "text": "Brainwashing\n\nBrainwashing (also known as mind control, menticide, coercive persuasion, thought control, thought reform, and re-education) is the concept that the human mind can be altered or controlled by certain psychological techniques. Brainwashing is said to reduce its subject’s ability to think critically or independently, to allow the introduction of new, unwanted thoughts and ideas into the subject’s mind, as well as to change his or her attitudes, values, and beliefs. \n\nThe concept of brainwashing was originally developed in the 1950s to explain how the Chinese government appeared to make people cooperate with them. Advocates of the concept also looked at Nazi Germany, at some criminal cases in the United States, and at the actions of human traffickers. It was later applied by Margaret Singer, Philip Zimbardo and some others in the anti-cult movement to explain conversions to some new religious movements and other groups. This resulted in scientific and legal debate with Eileen Barker, James Richardson, and other scholars, as well as legal experts, rejecting at least the popular understanding of brainwashing.\n\nOther views have been expressed by scholars including: Dick Anthony, Robert Cialdini, Stanley A. Deetz, Michael J. Freeman, Robert Jay Lifton, Joost Meerloo, Daniel Romanovsky, Kathleen Taylor, Louis Jolyon West, and Benjamin Zablocki. The concept of brainwashing is sometimes involved in legal cases, especially regarding child custody; and is also a major theme in science fiction and in criticism of modern political and corporate culture. Although the term appears in the fifth edition of the Diagnostic and Statistical Manual of Mental Disorders (DSM-5) of the American Psychiatric Association it is not accepted as scientific fact.\n\nThe Chinese term \"xǐnăo\" (洗脑，literally \"wash brain\") was originally used to describe the coercive persuasion used under the Maoist government in China, which aimed to transform \"reactionary\" people into \"right-thinking\" members of the new Chinese social system. The term punned on the Taoist custom of \"cleansing/washing the heart/mind\" (\"xǐxīn\"，洗心) before conducting ceremonies or entering holy places.\n\nThe \"Oxford English Dictionary\" records the earliest known English-language usage of the word \"brainwashing\" in an article by newspaperman Edward Hunter, in \"Miami News\", published on 24 September 1950. Hunter was an outspoken anticommunist and was said to be a CIA agent working undercover as a journalist. Hunter and others used the Chinese term to explain why, during the Korean War (1950-1953), some American prisoners of war (POWs) cooperated with their Chinese captors, even in a few cases defected to their side. British radio operator Robert W. Ford and British army Colonel James Carne also claimed that the Chinese subjected them to brainwashing techniques during their war-era imprisonment.\n\nThe U.S. military and government laid charges of brainwashing in an effort to undermine confessions made by POWs to war crimes, including biological warfare. After Chinese radio broadcasts claimed to quote Frank Schwable, Chief of Staff of the First Marine Air Wing admitting to participating in germ warfare, United Nations commander Gen. Mark W. Clark asserted:\n\nBeginning in 1953, Robert Jay Lifton interviewed American servicemen who had been POWs during the Korean War as well as priests, students, and teachers who had been held in prison in China after 1951. In addition to interviews with 25 Americans and Europeans, Lifton interviewed 15 Chinese citizens who had fled after having been subjected to indoctrination in Chinese universities. (Lifton's 1961 book \"\", was based on this research.) Lifton found that when the POWs returned to the United States their thinking soon returned to normal, contrary to the popular image of \"brainwashing.\"\n\nIn 1956, after reexamining the concept of brainwashing following the Korean War, the U.S. Army published a report entitled \"Communist Interrogation, Indoctrination, and Exploitation of Prisoners of War\", which called brainwashing a \"popular misconception\". The report states \"exhaustive research of several government agencies failed to reveal even one conclusively documented case of 'brainwashing' of an American prisoner of war in Korea.\"\n\nIn George Orwell's 1949 dystopian novel \"Nineteen Eighty-Four\" the main character is subjected to imprisonment, isolation, and torture in order to conform his thoughts and emotions to the wishes of the rulers of Orwell's fictional future totalitarian society. Orwell's vision influenced Hunter and is still reflected in the popular understanding of the concept of brainwashing. Written about the same time, J.R.R. Tolkien’s \"The Lord of the Rings\" also addressed brainwashing, although in a fantasy setting. The science fiction stories of Cordwainer Smith (written from the 1940s until his death in 1966) depict brainwashing to remove memories of traumatic events as a normal and benign part of future medical practice.\n\nIn the 1950s many American films were filmed that featured brainwashing of POWs, including \"The Rack\", \"The Bamboo Prison\", \"Toward the Unknown\", and \"The Fearmakers\". \"Forbidden Area\" told the story of Soviet secret agents who had been brainwashed through classical conditioning by their own government so they wouldn't reveal their identities. In 1962 \"The Manchurian Candidate\" (based on the 1959 novel by Richard Condon) \"put brainwashing front and center\" by featuring a plot by the Soviet government to take over the United States by use of a brainwashed presidential candidate. The concept of brainwashing became popularly associated with the research of Russian psychologist Ivan Pavlov, which mostly involved dogs, not humans, as subjects. In \"The Manchurian Candidate\" the head brainwasher is Dr. Yen Lo, of the Pavlov Institute.\n\nMind control remains an important theme in science fiction. Terry O'Brien comments: \"Mind control is such a powerful image that if hypnotism did not exist, then something similar would have to have been invented: the plot device is too useful for any writer to ignore. The fear of mind control is equally as powerful an image.\" A subgenre is \"corporate mind control\", in which a future society is run by one or more business corporations that dominate society using advertising and mass media to control the population's thoughts and feelings.\n\nFor twenty years starting in the early 1950s, the United States Central Intelligence Agency (CIA) and the United States Department of Defense conducted secret research, including Project MKUltra, in an attempt to develop practical brainwashing techniques; the results are unknown. (See also Sidney Gottlieb.) CIA experiments using various psychedelic drugs such as LSD and Mescaline drew from Nazi human experimentation.\n\nIn 1974, Patty Hearst, a member of the wealthy Hearst family, was kidnapped by a left-wing group calling itself the Symbionese Liberation Army. After several weeks of captivity she agreed to join the group and took part in their activities. In 1975, she was arrested and charged with bank robbery and use of a gun in committing a felony. Her attorney, F. Lee Bailey argued in her trial that she should not be held responsible for her actions since her treatment by her captors was the equivalent of the brainwashing of Korean War POWs. (See: diminished responsibility.) Hearst was found guilty, but her “brainwashing defense” brought the topic to renewed public attention in the United States, as did the 1969 to 1971 case of Charles Manson, who was said to have brainwashed his followers to commit murder and other crimes.\n\nBailey developed his case in conjunction with psychiatrist Louis Jolyon West and psychologist Margaret Singer. They had both studied the experiences of Korean War POWs. In 1996 Singer published her theories in her best-selling book \"Cults in Our Midst\". In 2003, the brainwashing defense was used unsuccessfully in the defense of Lee Boyd Malvo, who was charged with murder for his part in the D.C. sniper attacks. Some legal scholars have argued that the brainwashing defense undermines the law’s fundamental premise of free will.\n\nItaly has had controversy over the concept of \"plagio\", a crime consisting in an absolute psychological—and eventually physical—domination of a person. The effect is said to be the annihilation of the subject's freedom and self-determination and the consequent negation of his or her personality. The crime of plagio has rarely been prosecuted in Italy, and only one person was ever convicted. In 1981, an Italian court found that the concept is imprecise, lacks coherence, and is liable to arbitrary application. By the twenty-first century, the concept of brainwashing was being applied \"with some success\" in child custody and child sexual abuse cases. In some cases \"one parent is accused of brainwashing the child to reject the other parent, and in child sex abuse cases where one parent is accused of brainwashing the child to make sex abuse accusations against the other parent\" (possibly resulting in or causing parental alienation).\n\nIn 2003, forensic psychologist Dick Anthony said that \"no reasonable person would question that there are situations where people can be influenced against their best interests, but those arguments are evaluated on the basis of fact, not bogus expert testimony.\" In 2016, Israeli anthropologist of religion and fellow at the Van Leer Jerusalem Institute Adam Klin-Oron said about then-proposed \"anti-cult\" legislation: \n\nIn the 1970s, the anti-cult movement applied the concept of brainwashing to explain seemingly sudden and dramatic religious conversions to various new religious movements (NRMs) and other groups they considered cults. News media reports tended to support the brainwashing view and social scientists sympathetic to the anti-cult movement, who were usually psychologists, developed revised models of mind control. While some psychologists were receptive to the concept, sociologists were for the most part skeptical of its ability to explain conversion to NRMs.\n\nPhilip Zimbardo defined mind control as, \"the process by which individual or collective freedom of choice and action is compromised by agents or agencies that modify or distort perception, motivation, affect, cognition or behavioral outcomes,\" and he suggested that any human being is susceptible to such manipulation. Another adherent to this view, Jean-Marie Abgrall was heavily criticized by forensic psychologist Dick Anthony for employing a pseudo-scientific approach and lacking any evidence that anyone's worldview was substantially changed by these coercive methods. On the contrary, the concept and the fear surrounding it was used as a tool for the anti-cult movement to rationalize the persecution of minority religious groups.\n\nEileen Barker criticized the concept of mind control because it functioned to justify costly interventions such as deprogramming or exit counseling. She has also criticized some mental health professionals, including Singer, for accepting expert witness jobs in court cases involving NRMs. Her 1984 book, \"\" describes the religious conversion process to the Unification Church (whose members are sometimes informally referred to as \"Moonies\"), which had been one of the best known groups said to practice brainwashing. Barker spent close to seven years studying Unification Church members. She interviewed in depth or gave probing questionnaires to church members, ex-members, \"non-joiners,\" and control groups of uninvolved people from similar backgrounds, as well as parents, spouses, and friends of members. She also attended numerous church workshops and communal facilities. Barker writes that she rejects the \"brainwashing\" theory, because it explains neither the many people who attended a recruitment meeting and did not become members, nor the voluntary disaffiliation of members.\n\nJames Richardson observed that if the new religious movements had access to powerful brainwashing techniques, one would expect that they would have high growth rates, yet in fact most have not had notable success in recruitment. Most adherents participate for only a short time, and the success in retaining members is limited. For this and other reasons, sociologists of religion including David Bromley and Anson Shupe consider the idea that \"cults\" are brainwashing American youth to be \"implausible.\" In addition, Thomas Robbins, Massimo Introvigne, Lorne Dawson, Gordon Melton, Marc Galanter, and Saul Levine, amongst other scholars researching NRMs, have argued and established to the satisfaction of courts, relevant professional associations and scientific communities that there exists no generally accepted scientific theory, based upon methodologically sound research, that supports the concept of brainwashing as advanced by the anti-cult movement.\n\nBenjamin Zablocki responded that brainwashing is not \"a process that is directly observable,\" and that the \"real sociological issue\" is whether \"brainwashing occurs frequently enough to be considered an important social problem\", and that Richardson misunderstands brainwashing, conceiving of it as a recruiting process, instead of a retaining process, and that the number of people who attest to brainwashing in interviews (performed in accordance with guidelines of the National Institute of Mental Health and National Science Foundation) is too large result from anything other than a genuine phenomenon. Zablocki also pointed out that in the two most prestigious journals dedicated to the sociology of religion there have been no articles \"supporting the brainwashing perspective,\" while over one hundred such articles have been published in other journals \"marginal to the field.\" He concludes that the concept of brainwashing has been unfairly blacklisted.\n\nIn 1983, the American Psychological Association (APA) asked Singer to chair a taskforce called the APA Task Force on Deceptive and Indirect Techniques of Persuasion and Control (DIMPAC) to investigate whether brainwashing or coercive persuasion did indeed play a role in recruitment by NRMs.\n\"Cults and large group awareness trainings have generated considerable controversy because of their widespread use of deceptive and indirect techniques of persuasion and control. These techniques can compromise individual freedom, and their use has resulted in serious harm to thousands of individuals and families. This report reviews the literature on this subject, proposes a new way of conceptualizing influence techniques, explores the ethical ramifications of deceptive and indirect techniques of persuasion and control, and makes recommendations addressing the problems described in the report.\"\nOn 11 May 1987, the APA's Board of Social and Ethical Responsibility for Psychology (BSERP) rejected the DIMPAC report because the report \"lacks the scientific rigor and evenhanded critical approach necessary for APA imprimatur\", and concluded that \"after much consideration, BSERP does not believe that we have sufficient information available to guide us in taking a position on this issue.\"\n\nKathleen Barry, co-founder of the United Nations NGO, the Coalition Against Trafficking in Women (CATW), in her 1979 book \"Female Sexual Slavery\" prompted international awareness of human sex trafficking. In his 1986 book \"Woman Abuse: Facts Replacing Myths\" Lewis Okun reported that: “Kathleen Barry shows in Female Sexual Slavery that forced female prostitution involves coercive control practices very similar to thought reform.” In their 1996 book, \"Casting Stones: Prostitution and Liberation in Asia and the United States\", Rita Nakashima Brock and Susan Brooks Thistlethwaite report that the methods commonly used by pimps to control their victims \"closely resemble the brainwashing techniques of terrorists and paranoid cults.\"\n\nSome of the techniques used by traffickers include feigning love and concern for the victims' well-being to gain trust before beginning to track, manipulate and control the entire life of the victim, including environment, relationships, access to information and daily activities, promises of lucrative employment or corrupt marriage proposals, debt bondage, kidnapping, induced drug dependency and fear tactics such as threats about law enforcement, deportation, and harm to friends or family members. Physical captivity, shame, Stockholm Syndrome, traumatic bonding and fear of arrest can contribute to victims’ inability to seek assistance.\n\nRussian historian Daniel Romanovsky, who interviewed survivors and eyewitnesses in the 1970s, reported on what he called \"Nazi brainwashing\" of the people of Belarus by the occupying Germans during the Second World War, which took place through both mass propaganda and intense re-education, especially in schools. Romanovsky noted that very soon most people had adopted the Nazi view that the Jews were an inferior race and were closely tied to the Soviet government, views that had not been at all common before the German occupation.\n\nJoost Meerloo, a Dutch psychiatrist, was an early proponent of the concept of brainwashing. (\"Menticide\" is a neologism coined by him meaning: \"killing of the mind.\") Meerloo's view was influenced by his experiences during the German occupation of his country and his work with the Dutch government and the American military in the interrogation of accused Nazi war criminals. He later emigrated to the United States and taught at Columbia University. His best-selling 1956 book, \"The Rape of the Mind\", concludes by saying: \n\nScholars have said that modern business corporations practice mind control to create a work force that shares common values and culture. Critics have linked \"corporate brainwashing\" with globalization, saying that corporations are attempting to create a worldwide monocultural network of producers, consumers, and managers. Modern educational systems have also been criticized, by both the left and the right, for contributing to corporate brainwashing. In his 1992 book, \"Democracy in an Age of Corporate Colonization\", Stanley A. Deetz says that modern \"self awareness\" and \"self improvement\" programs provide corporations with even more effective tools to control the minds of employees than traditional brainwashing.\n\nIn his 2000 book, \"Destroying the World to Save It: Aum Shinrikyo, Apocalyptic Violence, and the New Global Terrorism\", Robert Lifton applied his original ideas about thought reform to Aum Shinrikyo and the War on Terrorism, concluding that in this context thought reform was possible without violence or physical coercion. He also pointed out that in their efforts against terrorism Western governments were also using some mind control techniques, including thought-terminating clichés.\n\nIn her 2004 popular science book, \"\", neuroscientist and physiologist Kathleen Taylor reviewed the history of mind control theories, as well as notable incidents. She suggests that persons under its influence have more rigid neurological pathways, and that can make it more difficult to rethink situations or be able to later reorganize these pathways. Reviewers praised her book for its clear presentation, while some criticized it for oversimplification.\n\n"}
{"id": "429171", "url": "https://en.wikipedia.org/wiki?curid=429171", "title": "Characterology", "text": "Characterology\n\nCharacterology (from Greek \"character\" and , \"-logia\") is a method of character reading that attempted to combine revised physiognomy, reconstructed phrenology and amplified pathognomy, with ethnology, sociology and anthropology. Developed by L. Hamilton McCormick in the 1920s, characterology was an attempt to produce a scientific, objective system to assess an individual's character.\n\nCharacterology attempted to resolve flaws in the phrenological systems of Franz Joseph Gall \nand Johann Spurzheim. McCormick tried to distance himself from those earlier systems, and wrote extensively about how his ideas improved upon them.\n\nMcCormick suggested possible applications for characterology, e.g., advice for parents and educators, guidance in military officer promotions, evaluating thinking patterns (i.e., reason-oriented or memory-oriented ), assessing business associates and competitors, career counseling, and selecting marital partners.\n\n"}
{"id": "8989793", "url": "https://en.wikipedia.org/wiki?curid=8989793", "title": "Coding (social sciences)", "text": "Coding (social sciences)\n\nIn the social sciences, coding is an analytical process in which data, in both quantitative form (such as questionnaires results) or qualitative form (such as interview transcripts) are categorized to facilitate analysis.\n\nOne purpose of coding is to transform the data into a form suitable for computer-aided analysis. This categorization of information is an important step, for example, in preparing data for computer processing with statistical software.\n\nSome studies will employ multiple coders working independently on the same data. This minimizes the chance of errors from coding and is believed to increase the reliability of data.\n\nOne code should apply to only one category and categories should be comprehensive. There should be clear guidelines for \"coders\" (individual who do the coding) so that code is consistent.\n\nFor quantitative analysis, data is coded usually into measured and recorded as nominal or ordinal variables.\n\nQuestionnaire data can be \"pre-coded\" (process of assigning codes to expected answers on designed questionnaire), \"field-coded\" (process of assigning codes as soon as data is available, usually during fieldwork), \"post-coded\" (coding of open questions on completed questionnaires) or \"office-coded\" (done after fieldwork). Note that some of the above are not mutually exclusive.\n\nIn social sciences, spreadsheets such as Excel and more advanced software packages such as R, Matlab, PSPP/SPSS, DAP/SAS, MiniTab and Stata are often used.\n\nFor disciplines in which a qualitative format is preferential, including ethnography, humanistic geography or phenomenological psychology a varied approach to coding can be applied. Iain Hay (2005) outlines a two-step process beginning with basic coding in order to distinguish overall themes, followed by a more in depth, interpretive code in which more specific trends and patterns can be interpreted.\n\nMuch of qualitative coding can be attributed to either grounded or \"a priori\" coding. Grounded coding refers to allowing notable themes and patterns emerge from the document themselves, where as \"a priori\" coding requires the researcher to apply pre-existing theoretical frameworks to analyze the documents. As coding methods are applied across various texts, the researcher is able to apply axial coding, which is the process of selecting core thematic categories present in several documents to discover common patterns and relations.\n\nPrior to constructing categories, a researcher must apply a first cycle coding method. There are a multitude of methods available, and a researcher will want to pick one that is suited for the format and nature of their documents. Not all methods can be applied to every type of document. Some examples of first cycle coding methods include:\n\n\nThe process can be done manually, which can be as simple as highlighting different concepts with different colours, or fed into a software package. Some examples of qualitative software packages include Atlas.ti, MAXQDA, NVivo, and QDA Miner.\n\nAfter assembling codes it is time to organize them into broader themes and categories. The process generally involves identifying themes from the existing codes, reducing the themes to a manageable number, creating hierarchies within the themes and then linking themes together through theoretical modeling.\n\nCreating memos during the coding process is integral to both grounded and a priori coding approaches. Qualitative research is inherently reflexive; as the researcher delves deeper into their subject, it is important to chronicle their own thought processes through reflective or methodological memos, as doing so may highlight their own subjective interpretations of data It is crucial to begin memoing at the onset of research. Regardless of the type of memo produced, what is important is that the process initiates critical thinking and productivity in the research. Doing so will facilitate easier and more coherent analyses as the project draws on \nMemos can be used to map research activities, uncover meaning from data, maintaining research momentum and engagement and opening communication.\n\n\nHay, I. (2005). \"Qualitative research methods in human geography\" (2nd ed.). Oxford: Oxford University Press.\n\nGrbich, Carol. (2013). \"Qualitative Data Analysis\" (2nd ed.). The Flinders University of South Australia: SAGE Publications Ltd.\n\nSaldaña, Johnny. (2015). \"The Coding Manual for Qualitative Researchers\" (3rd ed.). SAGE Publications Ltd.\n"}
{"id": "56206", "url": "https://en.wikipedia.org/wiki?curid=56206", "title": "Crop circle", "text": "Crop circle\n\nA crop circle or crop formation is a pattern created by flattening a crop, usually a cereal. The term was first coined in the early 1980s by Colin Andrews. Crop circles have been described as all falling \"within the range of the sort of thing done in hoaxes\" by Taner Edis, professor of physics at Truman State University. Although obscure natural causes or alien origins of crop circles are suggested by fringe theorists, there is no scientific evidence for such explanations, and all crop circles are consistent with human causation. A person who studies crop circles is known as a cereologist.\n\nThe number of crop circles has substantially increased from the 1970s to current times. There has been little scientific study of them. Circles in the United Kingdom are not distributed randomly across the landscape but appear near roads, areas of medium to dense population and cultural heritage monuments, such as Stonehenge or Avebury. In 1991, two hoaxers, Bower and Chorley, took credit for having created many circles throughout England after one of their circles was described by a circle investigator as impossible to be made by human hand.\n\nFormations are usually created overnight, although some are reported to have appeared during the day. In contrast to crop circles or crop formations, archaeological remains can cause cropmarks in the fields in the shapes of circles and squares, but they do not appear overnight, and they are always in the same places every year.\n\nThe concept of \"crop circles\" began with the original late-1970s hoaxes by Doug Bower and Dave Chorley (see Bower and Chorley, below). They said that they were inspired by the Tully \"saucer nest\" case in Australia, where a farmer claimed to first have seen a UFO, then found a flattened circle of swamp reeds.\n\nA 1678 news pamphlet \"The Mowing-Devil: or, Strange News Out of Hartfordshire\" is claimed by some cereologists to be the first depiction of a crop circle. Crop circle researcher Jim Schnabel does not consider it to be a historical precedent because it describes the stalks as being cut rather than bent (see folklore section).\n\nIn 1686, British naturalist Robert Plot reported on rings or arcs of mushrooms (see fairy rings) in \"The Natural History of Stafford-Shire\" and proposed air flows from the sky as a cause. In 1991 meteorologist Terence Meaden linked this report with modern crop circles, a claim that has been compared with those made by Erich von Däniken.\n\nAn 1880 letter to the editor of \"Nature\" by amateur scientist John Rand Capron describes how a recent storm had created several circles of flattened crops in a field.\n\nIn 1932, archaeologist E C Curwen observed four dark rings in a field at Stoughton Down near Chichester, but could examine only one: \"a circle in which the barley was 'lodged' or beaten down, while the interior area was very slightly mounded up.\"\n\nIn 1963, amateur astronomer Patrick Moore described a crater in a potato field in Wiltshire, which he considered was probably caused by an unknown meteoric body. In nearby wheat fields, there were several circular and elliptical areas where the wheat had been flattened. There was evidence of \"spiral flattening\". He thought they could be caused by air currents from the impact, since they led towards the crater. Astronomer Hugh Ernest Butler observed similar craters and said they were likely caused by lightning strikes.\n\nIn the 1960s, in Tully, Queensland, Australia, and in Canada, there were many reports of UFO sightings and circular formations in swamp reeds and sugar cane fields. For example, on 8 August 1967, three circles were found in a field in Duhamel, Alberta, Canada; Department of National Defence investigators concluded that it was artificial but couldn't say who made them or how. The most famous case is the 1966 Tully \"saucer nest\", when a farmer said he witnessed a saucer-shaped craft rise 30 or from a swamp and then fly away. On investigating he found a nearly circular area 32 feet long by 25 feet wide where the grass was flattened in clockwise curves to water level within the circle, and the reeds had been uprooted from the mud. The local police officer, the Royal Australian Air Force, and the University of Queensland concluded that it was most probably caused by natural causes, like a down draught, a willy-willy (dust devil), or a waterspout. In 1973, G.J. Odgers, Director of Public Relations, Department of Defence (Air Office), wrote to a journalist that the \"saucer\" was probably debris lifted by the causing willy-willy. Hoaxers Bower and Chorley said they were inspired by this case to start making the modern crop circles that appear today.\n\nSince the 1960s, there had been a surge of UFOlogists in Wiltshire, and there were rumours of \"saucer nests\" appearing in the area, but they were never photographed. There are other pre-1970s reports of circular formations, especially in Australia and Canada, but they were always simple circles, which could have been caused by whirlwinds. In \"Fortean Times\" David Wood reported that in 1940 he had already made crop circles near Gloucestershire using ropes. In 1997, the \"Oxford English Dictionary\" recorded the earliest usage of the term \"crop circles\" in a 1988 issue of \"Journal of Meteorology\", referring to a BBC film. The coining of the term \"crop circle\" is attributed to Colin Andrews in the late 1970s or early 1980s.\n\nThe majority of reports of crop circles have appeared in and spread since the late 1970s as many circles began appearing throughout the English countryside. This phenomenon became widely known in the late 1980s, after the media started to report crop circles in Hampshire and Wiltshire. After Bower's and Chorley's 1991 statement that they were responsible for many of them, circles started appearing all over the world. To date, approximately 10,000 crop circles have been reported internationally, from locations such as the former Soviet Union, the United Kingdom, Japan, the U.S., and Canada. Sceptics note a correlation between crop circles, recent media coverage, and the absence of fencing and/or anti-trespassing legislation.\n\nAlthough farmers expressed concern at the damage caused to their crops, local response to the appearance of crop circles was often enthusiastic, with locals taking advantage of the increase of tourism and visits from scientists, crop circle researchers, and individuals seeking spiritual experiences. The market for crop-circle interest consequently generated bus or helicopter tours of circle sites, walking tours, T-shirts, and book sales.\n\nSince the start of the 21st century, crop formations have increased in size and complexity, with some featuring as many as 2,000 different shapes and some incorporating complex mathematical and scientific characteristics.\n\nThe researcher Jeremy Northcote found that crop circles in the UK in 2002 were not spread randomly across the landscape. They tended to appear near roads, areas of medium-to-dense population, and cultural heritage monuments such as Stonehenge or Avebury. He found that they always appeared in areas that were easy to access. This suggests strongly that these crop circles were more likely to be caused by intentional human action than by paranormal activity. Another strong indication of that theory was that inhabitants of the zone with the most circles had a historical tendency for making large-scale formations, including stone circles such as Stonehenge, burial mounds such as Silbury Hill, long barrows such as West Kennet Long Barrow, and .\n\nIn 1991, self-professed pranksters Doug Bower and Dave Chorley made headlines claiming it was they who started the phenomenon in 1978 with the use of simple tools consisting of a plank of wood, rope, and a baseball cap fitted with a loop of wire to help them walk in a straight line. To prove their case they made a circle in front of journalists; a \"cereologist\" (advocate of paranormal explanations of crop circles), Pat Delgado, examined the circle and declared it authentic before it was revealed that it was a hoax. Inspired by Australian crop circle accounts from 1966, Bower and Chorley claimed to be responsible for all circles made prior to 1987, and for more than 200 crop circles in 1978–1991 (with 1000 other circles not being made by them). After their announcement, the two men demonstrated making a crop circle. According to Professor Richard Taylor, \"the pictographs they created inspired a second wave of crop artists. Far from fizzling out, crop circles have evolved into an international phenomenon, with hundreds of sophisticated pictographs now appearing annually around the globe.\"\n\n\"Smithsonian\" magazine wrote:\n\nSince the early 1990s, the UK arts collective \"Circlemakers,\" founded by artists Rod Dickinson and John Lundberg (and subsequently including artists Wil Russell and Rob Irving), have been creating crop circles in the UK and around the world as part of their art practice and for commercial clients.\n\nThe Led Zeppelin boxed set that was released on 7 September 1990, along with the remasters of the first boxed set, as well as the second boxed set, all feature an image of a crop circle that appeared in East Field in Alton Barnes, Wiltshire.\n\nOn the night of 11–12 July 1992, a crop-circle making competition with a prize of £3,000 (funded in part by the Arthur Koestler Foundation) was held in Berkshire. The winning entry was produced by three Westland Helicopters engineers, using rope, PVC pipe, a plank, string, a telescopic device and two stepladders. According to Rupert Sheldrake, the competition was organised by him and John Michell and \"co-sponsored by The Guardian and The Cerealogist\". The prize money came from \"PM\", a German magazine. Sheldrake wrote that \"The experiment was conclusive. Humans could indeed make all the features of state-of-the-art crop formations at that time. Eleven of the twelve teams made more or less impressive formations that followed the set design.\"\n\nIn 2002, Discovery Channel commissioned five aeronautics and astronautics graduate students from MIT to create crop circles of their own, aiming to duplicate some of the features claimed to distinguish \"real\" crop circles from the known fakes such as those created by Bower and Chorley. The creation of the circle was recorded and used in the Discovery Channel documentary \"Crop Circles: Mysteries in the Fields\".\n\nIn 2009, \"The Guardian\" reported that crop circle activity had been waning around Wiltshire, in part because makers preferred creating promotional crop circles for companies that paid well for their efforts.\n\nA video sequence used in connection with the opening of the 2012 Summer Olympics in London showed two crop circles in the shape of the Olympic rings. Another Olympic crop circle was visible to passengers landing at nearby Heathrow Airport before and during the Games.\n\nA crop circle depicting the emblem of the \"Star Wars\" Rebel Alliance was created in California in December 2017 by an 11-year-old boy as a spaceport for X-wing fighters.\n\nIn 1992, Hungarian youths Gábor Takács and Róbert Dallos, both then 17, were the first people to face legal action after creating a crop circle. Takács and Dallos, of the St. Stephen Agricultural Technicum, a high school in Hungary specializing in agriculture, created a diameter crop circle in a wheat field near Székesfehérvár, southwest of Budapest, on June 8, 1992. On September 3, the pair appeared on Hungarian TV and exposed the circle as a hoax, showing photos of the field before and after the circle was made. As a result, Aranykalász Co., the owners of the land, sued the teens for 630,000 Ft (~$3,000 USD) in damages. The presiding judge ruled that the students were only responsible for the damage caused in the circle itself, amounting to about 6,000 Ft (~$30 USD), and that 99% of the damage to the crops was caused by the thousands of visitors who flocked to Székesfehérvár following the media's promotion of the circle. The fine was eventually paid by the TV show, as were the students' legal fees.\n\nIn 2000, Matthew Williams became the first man in the UK to be arrested for causing criminal damage after making a crop circle near Devizes. In November 2000, he was fined £100 and £40 in costs. , no one else has been successfully prosecuted in the UK for criminal damage caused by creating crop circles.\n\nThe scientific consensus on crop circles is that they are constructed by human beings as hoaxes, advertising, or art. The most widely known method for a person or group to construct a crop formation is to tie one end of a rope to an anchor point and the other end to a board which is used to crush the plants. Sceptics of the paranormal point out that all characteristics of crop circles are fully compatible with their being made by hoaxers.\n\nBower and Chorley confessed in 1991 to making the first crop circles in southern England. When some people refused to believe them, they deliberately added straight lines and squares to show that they could not have natural causes. In a copycat effect, increasingly complex circles started appearing in many countries around the world, including fractal figures. Physicists have suggested that the most complex formations might be made with the help of GPS and lasers. In 2009, a circle formation was made over the course of three consecutive nights and was apparently left unfinished, with some half-made circles.\n\nThe main criticism of alleged non-human creation of crop circles is that while evidence of these origins, besides eyewitness testimonies, is essentially absent, some are definitely known to be the work of human pranksters, and others can be adequately explained as such. There have been cases in which researchers declared crop circles to be \"the real thing\", only to be confronted with the people who created the circle and documented the fraud, like Bower and Chorley and tabloid \"Today\" hoaxing Pat Delgado, the Wessex Sceptics and Channel 4's \"Equinox\" hoaxing Terence Meaden, or a friend of a Canadian farmer hoaxing a field researcher of the Canadian Crop Circle Research Network. In his 1997 book \"The Demon-Haunted World: Science as a Candle in the Dark\", Carl Sagan concludes that crop circles were created by Bower and Chorley and their copycats, and speculates that UFOlogists willingly ignore the evidence for hoaxing so they can keep believing in an extraterrestrial origin of the circles. Many others have demonstrated how complex crop circles can be created. \"Scientific American\" published an article by Matt Ridley, who started making crop circles in northern England in 1991. He wrote about how easy it is to develop techniques using simple tools that can easily fool later observers. He reported on \"expert\" sources such as \"The Wall Street Journal\", who had been easily fooled and mused about why people want to believe supernatural explanations for phenomena that are not yet explained. Methods of creating a crop circle are now well documented on the Internet.\n\nSome crop formations are paid for by companies who use them as advertising. Many crop circles show human symbols, like the heart and arrow symbol of love, stereotyped alien faces,\n\nHoaxers have been caught in the process of making new circles, such as in 2004 in the Netherlands for example (see more cases in \"legal implications\" section above).\n\nAdvocates of non-human causes discount on-site evidence of human involvement as attempts to discredit the phenomena. Some even argue a conspiracy theory, with governments planting evidence of hoaxing to muddle the origins of the circles. When Ridley wrote negative articles in newspapers, he was accused of spreading \"government disinformation\" and of working for the UK military intelligence service MI5. Ridley responded by noting that many cereologists make good livings from selling books and providing high-priced personal tours through crop fields, and he claimed that they have vested interests in rejecting what is by far the most likely explanation for the circles.\n\nIt has been suggested that crop circles may be the result of extraordinary meteorological phenomena ranging from freak tornadoes to ball lightning, but there is no evidence of any crop circle being created by any of these causes.\n\nIn 1880, an amateur scientist, John Rand Capron, wrote a letter to the editor of journal \"Nature\" about some circles in crops and blamed them on a recent storm, saying their shape was \"suggestive of some cyclonic wind action\".\n\nIn 1980, Terence Meaden, a meteorologist and physicist, proposed that the circles were caused by whirlwinds whose course was affected by southern England hills. As circles became more complex, Terence had to create increasingly complex theories, blaming an electromagneto-hydrodynamic \"plasma vortex\". The meteorological theory became popular, and it was even referenced in 1991 by physicist Stephen Hawking who said that, \"Corn circles are either hoaxes or formed by vortex movement of air\". The weather theory suffered a serious blow in 1991, but Hawking's point about hoaxes was supported when Bower and Chorley stated that they had been responsible for making all those circles. By the end of 1991 Meaden conceded that those circles that had complex designs were made by hoaxers.\n\nSince becoming the focus of widespread media attention in the 1980s, crop circles have become the subject of speculation by various paranormal, ufological, and anomalistic investigators ranging from proposals that they were created by bizarre meteorological phenomena to messages from extraterrestrial beings. There has also been speculation that crop circles have a relation to ley lines. Many New Age groups incorporate crop circles into their belief systems.\n\nSome paranormal advocates think that crop circles are caused by ball lighting and that the patterns are so complex that they have to be controlled by some entity. Some proposed entities are: Gaia asking to stop global warming and human pollution, God, supernatural beings (for example Indian devas), the collective minds of humanity through a proposed \"quantum field\", or extraterrestrial beings.\n\nResponding to local beliefs that \"extraterrestrial beings\" in UFOs were responsible for crop circles appearing, the Indonesian National Institute of Aeronautics and Space (LAPAN) described crop circles as \"man-made\". Thomas Djamaluddin, research professor of astronomy and astrophysics at LAPAN stated, \"We have come to agree that this 'thing' cannot be scientifically proven.\" Among others, paranormal enthusiasts, ufologists, and anomalistic investigators have offered hypothetical explanations that have been criticized as pseudoscientific by sceptical groups and scientists, including the Committee for Skeptical Inquiry. No credible evidence of extraterrestrial origin has been presented.\n\nIn 2009, the attorney general for the island state of Tasmania stated that Australian wallabies had been found creating crop circles in fields of opium poppies, which are grown legally for medicinal use, after consuming some of the opiate-laden poppies and running in circles.\n\nA small number of scientists (physicist Eltjo Haselhoff, the late biophysicist William Levengood) have found differences between the crops inside the circles and outside them, citing this as evidence they were not man-made.\n\nLevengood published papers in journal \"Physiologia Plantarum\" in 1994 and 1999. In his 1994 paper he found that certain deformities in the grain inside the circles were correlated to the position of the grain inside the circle. In 1996 sceptic Joe Nickell objected that correlation is not causation, raised several objections to the Levengood's methods and assumptions, and said \"Until his work is independently replicated by qualified scientists doing 'double-blind' studies and otherwise following stringent scientific protocols, there seems no need to take seriously the many dubious claims that Levengood makes, including his similar ones involving plants at alleged 'cattle mutilation' sites.\" (in reference to cattle mutilation).\n\nA study by Eltjo Haselhoff reported that the pulvini of wheat in 95% of the crop circles investigated were elongated in a pattern falling off with distance from the centre and that seeds from the bent-over plants grew much more slowly under controlled conditions. Furthermore, traces of crop circle patterns are sometimes found in the crop the following year, \"suggesting long-term damage to the crop field consistent with Levengood's observations of stunted seed growth.\"\n\nIn 2000, Colin Andrews, who had researched crop circles for 17 years, stated that while he believed 80% were man-made, he thought the remaining circles, with less elaborate designs, could be explained by a three-degree shift in the Earth's magnetic field, that creates a current that \"electrocutes\" the crops, causing them to flatten and form the circle.\n\nResearchers of crop circles have linked modern crop circles to old folkloric tales to support the claim that they are not artificially produced. Circle crops are culture-dependent: they appear mostly in developed and secularized Western countries where people are receptive to New Age beliefs, including Japan, but they don't appear at all in other zones, such as Muslim countries.\n\nFungi can cause circular areas of crop to die, probably the origin of tales of \"fairie rings\". Tales also mention balls of light many times but never in relation to crop circles.\n\nA 17th-century English woodcut called the \"Mowing-Devil\" depicts the devil with a scythe mowing (cutting) a circular design in a field of oats. The pamphlet containing the image states that the farmer, disgusted at the wage his mower was demanding for his work, insisted that he would rather have \"the devil himself\" perform the task. Crop circle researcher Jim Schnabel does not consider this to be a historical precedent for crop circles because the stalks were cut down, not bent. The circular form indicated to the farmer that it had been caused by the devil.\n\nIn the 1948 German story \"Die zwölf Schwäne\" (\"The Twelve Swans\"), a farmer every morning found a circular ring of flattened grain on his field. After several attempts, his son saw twelve princesses disguised as swans, who took off their disguises and danced in the field. Crop rings produced by fungi may have inspired such tales since folklore holds these rings are created by dancing wolves or fairies.\n\n\n\n"}
{"id": "31889797", "url": "https://en.wikipedia.org/wiki?curid=31889797", "title": "Cycles of Time", "text": "Cycles of Time\n\nCycles of Time: An Extraordinary New View of the Universe is a science book by mathematical physicist Roger Penrose published by The Bodley Head in 2010. The book outlines Penrose's Conformal Cyclic Cosmology (CCC) model, which is an extension of general relativity but opposed to the widely supported multidimensional string theories and cosmological inflation following the Big Bang.\n\nPenrose examines implications of the Second Law of Thermodynamics and its inevitable march toward a maximum entropy state of the universe. Penrose illustrates entropy in terms of information state phase space (with 1 dimension for every degree of freedom) where particles end up moving through ever larger grains of this phase space from smaller grains over time due to random motion. He disagrees with Stephen Hawking's back-track over whether information is destroyed when matter enters black holes. Such information loss would non-trivially lower total entropy in the universe as the black holes wither away due to Hawking radiation, resulting in a loss in phase space degrees of freedom.\n\nPenrose goes on further to state that over enormous scales of time (beyond 10 years), distance ceases to be meaningful as all mass breaks down into extremely red-shifted photon energy, whereupon time has no influence, and the universe continues to expand without event . This period from Big Bang to infinite expansion Penrose defines as an aeon. The smooth “hairless” infinite oblivion of the previous aeon becomes the low-entropy Big Bang state of the next aeon cycle. Conformal geometry preserves the angles but not the distances of the previous aeon, allowing the new aeon universe to appear quite small at its inception as its phase space starts anew.\n\nPenrose cites concentric rings found in the WMAP cosmic microwave background survey as preliminary evidence for his model, as he predicted black hole collisions from the previous aeon would leave such structures due to ripples of gravitational waves.\n\nMost nonexpert critics (nonscientists) have found the book a challenge to fully comprehend; a few such as \"Kirkus Reviews\" and Doug Johnstone for \"The Scotsman\" appreciate the against the grain innovative ideas Penrose puts forth. Manjit Kumar reviewing for \"The Guardian\" admires the Russian doll geometry play of the CCC concept, framing it as an idea of which M. C. Escher \"would have approved\". Graham Storrs for the \"New York Journal of Books\" concedes that this is not the book that an unambitious lay person should plunge into. The American fiction writer Anthony Doerr in \"The Boston Globe\" writes \"Penrose has never shied away from including mathematics in his texts, and kudos to his publisher for honoring that wish. That said, the second half of \"Cycles of Time\" offers some seriously hard sledding\"; \"If you'll forgive a skiing metaphor, \"Cycles of Time\" is a black diamond of a book.\"\n"}
{"id": "1845675", "url": "https://en.wikipedia.org/wiki?curid=1845675", "title": "Deductive-nomological model", "text": "Deductive-nomological model\n\nThe deductive-nomological model (DN model), also known as Hempel's model, the Hempel–Oppenheim model, the Popper–Hempel model, or the covering law model, is a formal view of scientifically answering questions asking, \"Why...?\". The DN model poses scientific explanation as a deductive structure—that is, one where truth of its premises entails truth of its conclusion—hinged on accurate prediction or postdiction of the phenomenon to be explained.\n\nBecause of problems concerning humans' ability to define, discover, and know causality, it was omitted in initial formulations of the DN model. Causality was thought to be incidentally approximated by realistic selection of premises that \"derive\" the phenomenon of interest from observed starting conditions plus general laws. Still, DN model formally permitted causally irrelevant factors. Also, derivability from observations and laws sometimes yielded absurd answers.\n\nWhen logical empiricism fell out of favor in the 1960s, the DN model was widely seen as a flawed or greatly incomplete model of scientific explanation. Nonetheless, it remained an idealized version of scientific explanation, and one that was rather accurate when applied to modern physics. In the early 1980s, revision to DN model emphasized \"maximal specificity\" for relevance of the conditions and axioms stated. Together with Hempel's inductive-statistical model, the DN model forms scientific explanation's covering law model, which is also termed, from critical angle, subsumption theory.\n\nThe term \"deductive\" distinguishes the DN model's intended determinism from the probabilism of inductive inferences. The term \"nomological\" is derived from the Greek word \"νόμος\" or \"nomos\", meaning \"law\". The DN model holds to a view of scientific explanation whose \"conditions of adequacy\" (CA)—semiformal but stated classically—are \"derivability\" (CA1), \"lawlikeness\" (CA2), \"empirical content\" (CA3), and \"truth\" (CA4).\n\nIn the DN model, a law axiomatizes an unrestricted generalization from antecedent \"A\" to consequent \"B\" by conditional proposition—\"If A, then B\"—and has empirical content testable. A law differs from mere true regularity—for instance, \"George always carries only $1 bills in his wallet\"—by supporting counterfactual claims and thus suggesting what \"must\" be true, while following from a scientific theory's axiomatic structure.\n\nThe phenomenon to be explained is the explanandum—an event, law, or theory—whereas the premises to explain it are explanans, true or highly confirmed, containing at least one universal law, and entailing the explanandum. Thus, given the explanans as initial, specific conditions \"C, C . . . C\" plus general laws \"L, L . . . L\", the phenomenon \"E\" as explanandum is a deductive consequence, thereby scientifically explained.\n\nAristotle's scientific explanation in \"Physics\" resembles the DN model, an idealized form of scientific explanation. The framework of Aristotelian physics—Aristotelian metaphysics—reflected the perspective of this principally biologist, who, amid living entities' undeniable purposiveness, formalized vitalism and teleology, an intrinsic morality in nature. With emergence of Copernicanism, however, Descartes introduced mechanical philosophy, then Newton rigorously posed lawlike explanation, both Descartes and especially Newton shunning teleology within natural philosophy. At 1740, David Hume staked Hume's fork, highlighted the problem of induction, and found humans ignorant of either necessary or sufficient causality. Hume also highlighted the fact/value gap, as what \"is\" does not itself reveal what \"ought\".\n\nNear 1780, countering Hume's ostensibly radical empiricism, Immanuel Kant highlighted extreme rationalism—as by Descartes or Spinoza—and sought middle ground. Inferring the mind to arrange experience of the world into \"substance\", \"space\", and \"time\", Kant placed the mind as part of the causal constellation of experience and thereby found Newton's theory of motion universally true, yet knowledge of things in themselves impossible. Safeguarding science, then, Kant paradoxically stripped it of scientific realism. Aborting Francis Bacon's inductivist mission to dissolve the veil of appearance to uncover the \"noumena\"—metaphysical view of nature's ultimate truths—Kant's transcendental idealism tasked science with simply modeling patterns of \"phenomena\". Safeguarding metaphysics, too, it found the mind's constants holding also universal moral truths, and launched German idealism, increasingly speculative.\n\nAuguste Comte found the problem of induction rather irrelevant since enumerative induction is grounded on the empiricism available, while science's point is not metaphysical truth. Comte found human knowledge had evolved from theological to metaphysical to scientific—the ultimate stage—rejecting both theology and metaphysics as asking questions unanswerable and posing answers unverifiable. Comte in the 1830s expounded positivism—the first modern philosophy of science and simultaneously a political philosophy—rejecting conjectures about unobservables, thus rejecting search for \"causes\". Positivism predicts observations, confirms the predictions, and states a \"law\", thereupon applied to benefit human society. From late 19th century into the early 20th century, the influence of positivism spanned the globe. Meanwhile, evolutionary theory's natural selection brought the Copernican Revolution into biology and eventuated in the first conceptual alternative to vitalism and teleology.\n\nWhereas Comtean positivism posed science as \"description\", logical positivism emerged in the late 1920s and posed science as \"explanation\", perhaps to better unify empirical sciences by covering not only fundamental science—that is, fundamental physics—but special sciences, too, such as biology, psychology, economics, and anthropology. After defeat of National Socialism with World War II's close in 1945, logical positivism shifted to a milder variant, \"logical empiricism\". All variants of the movement, which lasted until 1965, are neopositivism, sharing the quest of verificationism.\n\nNeopositivists led emergence of the philosophy subdiscipline philosophy of science, researching such questions and aspects of scientific theory and knowledge. Scientific realism takes scientific theory's statements at face value, thus accorded either falsity or truth—probable or approximate or actual. Neopositivists held scientific antirealism as instrumentalism, holding scientific theory as simply a device to predict observations and their course, while statements on nature's unobservable aspects are elliptical at or metaphorical of its observable aspects, rather.\n\nDN model received its most detailed, influential statement by Carl G Hempel, first in his 1942 article \"The function of general laws in history\", and more explicitly with Paul Oppenheim in their 1948 article \"Studies in the logic of explanation\". Leading logical empiricist, Hempel embraced the Humean empiricist view that humans observe sequence of sensory events, not cause and effect, as causal relations and casual mechanisms are unobservables. DN model bypasses causality beyond mere constant conjunction: first an event like \"A\", then always an event like \"B\".\n\nHempel held natural laws—empirically confirmed regularities—as satisfactory, and if included realistically to approximate causality. In later articles, Hempel defended DN model and proposed probabilistic explanation by \"inductive-statistical model\" (IS model). DN model and IS model—whereby the probability must be high, such as at least 50%—together form \"covering law model\", as named by a critic, William Dray. Derivation of statistical laws from other statistical laws goes to the \"deductive-statistical model\" (DS model). Georg Henrik von Wright, another critic, named the totality \"subsumption theory\".\n\nAmid failure of neopositivism's fundamental tenets, Hempel in 1965 abandoned verificationism, signaling neopositivism's demise. From 1930 onward, Karl Popper had refuted any positivism by asserting falsificationism, which Popper claimed had killed positivism, although, paradoxically, Popper was commonly mistaken for a positivist. Even Popper's 1934 book embraces DN model, widely accepted as the model of scientific explanation for as long as physics remained the model of science examined by philosophers of science.\n\nIn the 1940s, filling the vast observational gap between cytology and biochemistry, cell biology arose and established existence of cell organelles besides the nucleus. Launched in the late 1930s, the molecular biology research program cracked a genetic code in the early 1960s and then converged with cell biology as \"cell and molecular biology\", its breakthroughs and discoveries defying DN model by arriving in quest not of lawlike explanation but of causal mechanisms. Biology became a new model of science, while special sciences were no longer thought defective by lacking universal laws, as borne by physics.\n\nIn 1948, when explicating DN model and stating scientific explanation's semiformal \"conditions of adequacy\", Hempel and Oppenheim acknowledged redundancy of the third, \"empirical content\", implied by the other three—\"derivability\", \"lawlikeness\", and \"truth\". In the early 1980s, upon widespread view that causality ensures the explanans' relevance, Wesley Salmon called for returning \"cause\" to \"because\", and along with James Fetzer helped replace CA3 \"empirical content\" with CA3' \"strict maximal specificity\".\n\nSalmon introduced \"causal mechanical\" explanation, never clarifying how it proceeds, yet reviving philosophers' interest in such. Via shortcomings of Hempel's inductive-statistical model (IS model), Salmon introduced \"statistical-relevance model\" (SR model). Although DN model remained an idealized form of scientific explanation, especially in applied sciences, most philosophers of science consider DN model flawed by excluding many types of explanations generally accepted as scientific.\n\nAs theory of knowledge, epistemology differs from ontology, which is a subbranch of metaphysics, theory of reality. Ontology poses which categories of being—what sorts of things exist—and so, although a scientific theory's ontological commitment can be modified in light of experience, an ontological commitment inevitably precedes empirical inquiry.\n\nNatural laws, so called, are statements of humans' observations, thus are epistemological—concerning human knowledge—the \"epistemic\". Causal mechanisms and structures existing putatively independently of minds exist, or would exist, in the natural world's structure itself, and thus are ontological, the \"ontic\". Blurring epistemic with ontic—as by incautiously presuming a natural law to refer to a causal mechanism, or to trace structures realistically during unobserved transitions, or to be true regularities always unvarying—tends to generate a \"category mistake\".\n\nDiscarding ontic commitments, including causality \"per se\", DN model permits a theory's laws to be reduced to—that is, subsumed by—a more fundamental theory's laws. The higher theory's laws are explained in DN model by the lower theory's laws. Thus, the epistemic success of Newtonian theory's law of universal gravitation is reduced to—thus explained by—Einstein's general theory of relativity, although Einstein's discards Newton's ontic claim that universal gravitation's epistemic success predicting Kepler's laws of planetary motion is through a causal mechanism of a straightly attractive force instantly traversing absolute space despite absolute time.\n\nCovering law model reflects neopositivism's vision of empirical science, a vision interpreting or presuming unity of science, whereby all empirical sciences are either fundamental science—that is, fundamental physics—or are special sciences, whether astrophysics, chemistry, biology, geology, psychology, economics, and so on. All special sciences would network via covering law model. And by stating \"boundary conditions\" while supplying \"bridge laws\", any special law would reduce to a lower special law, ultimately reducing—theoretically although generally not practically—to fundamental science. (\"Boundary conditions\" are specified conditions whereby the phenomena of interest occur. \"Bridge laws\" translate terms in one science to terms in another science.)\n\nBy DN model, if one asks, \"Why is that shadow 20 feet long?\", another can answer, \"Because that flagpole is 15 feet tall, the Sun is at \"x\" angle, and laws of electromagnetism\". Yet by problem of symmetry, if one instead asked, \"Why is that flagpole 15 feet tall?\", another could answer, \"Because that shadow is 20 feet long, the Sun is at \"x\" angle, and laws of electromagnetism\", likewise a deduction from observed conditions and scientific laws, but an answer clearly incorrect. By the problem of irrelevance, if one asks, \"Why did that man not get pregnant?\", one could in part answer, among the explanans, \"Because he took birth control pills\"—if he factually took them, and the law of their preventing pregnancy—as covering law model poses no restriction to bar that observation from the explanans.\n\nMany philosophers have concluded that causality is integral to scientific explanation. DN model offers a necessary condition of a causal explanation—successful prediction—but not sufficient conditions of causal explanation, as a universal regularity can include spurious relations or simple correlations, for instance \"Z\" always following \"Y\", but not \"Z\" because of \"Y\", instead \"Y\" and then \"Z\" as an effect of \"X\". By relating temperature, pressure, and volume of gas within a container, Boyle's law permits prediction of an unknown variable—volume, pressure, or temperature—but does not explain \"why\" to expect that unless one adds, perhaps, the kinetic theory of gases.\n\nScientific explanations increasingly pose not determinism's universal laws, but probabilism's chance, \"ceteris paribus\" laws. Smoking's contribution to lung cancer fails even the inductive-statistical model (IS model), requiring probability over 0.5 (50%). (Probability standardly ranges from 0 (0%) to 1 (100%).) An applied science that applies statistics seeking associations between events, epidemiology cannot show causality, but consistently found higher incidence of lung cancer in smokers versus otherwise similar nonsmokers, although the proportion of smokers who develop lung cancer is modest. Versus nonsmokers, however, smokers as a group showed over 20 times the risk of lung cancer, and in conjunction with basic research, consensus followed that smoking had been scientifically explained as \"a\" cause of lung cancer, responsible for some cases that without smoking would not have occurred, a probabilistic counterfactual causality.\n\nThrough lawlike explanation, fundamental physics—often perceived as fundamental science—has proceeded through intertheory relation and theory reduction, thereby resolving experimental paradoxes to great historical success, resembling covering law model. In early 20th century, Ernst Mach as well as Wilhelm Ostwald had resisted Ludwig Boltzmann's reduction of thermodynamics—and thereby Boyle's law—to statistical mechanics partly \"because\" it rested on kinetic theory of gas, hinging on atomic/molecular theory of matter. Mach as well as Ostwald viewed matter as a variant of energy, and molecules as mathematical illusions, as even Boltzmann thought possible.\n\nIn 1905, via statistical mechanics, Albert Einstein predicted the phenomenon Brownian motion—unexplained since reported in 1827 by botanist Robert Brown. Soon, most physicists accepted that atoms and molecules were unobservable yet real. Also in 1905, Einstein explained the electromagnetic field's energy as distributed in \"particles\", doubted until this helped resolve atomic theory in the 1910s and 1920s. Meanwhile, all known physical phenomena were gravitational or electromagnetic, whose two theories misaligned. Yet belief in aether as the source of all physical phenomena was virtually unanimous. At experimental paradoxes, physicists modified the aether's hypothetical properties.\n\nFinding the luminiferous aether a useless hypothesis, Einstein in 1905 \"a priori\" unified all inertial reference frames to state special \"principle\" of relativity, which, by omitting aether, converted space and time into \"relative\" phenomena whose relativity aligned electrodynamics with the Newtonian principle Galilean relativity or invariance. Originally epistemic or instrumental, this was interpreted as ontic or realist—that is, a causal mechanical explanation—and the \"principle\" became a \"theory\", refuting Newtonian gravitation. By predictive success in 1919, general relativity apparently overthrew Newton's theory, a revolution in science resisted by many yet fulfilled around 1930.\n\nIn 1925, Werner Heisenberg as well as Erwin Schrödinger independently formalized quantum mechanics (QM). Despite clashing explanations, the two theories made identical predictions. Paul Dirac's 1928 model of the electron was set to special relativity, launching QM into the first quantum field theory (QFT), quantum electrodynamics (QED). From it, Dirac interpreted and predicted the electron's antiparticle, soon discovered and termed \"positron\", but the QED failed electrodynamics at high energies. Elsewhere and otherwise, strong nuclear force and weak nuclear force were discovered.\n\nIn 1941, Richard Feynman introduced QM's path integral formalism, which if taken toward \"interpretation\" as a causal mechanical model clashes with Heisenberg's matrix formalism and with Schrödinger's wave formalism, although all three are empirically identical, sharing predictions. Next, working on QED, Feynman sought to model particles without fields and find the vacuum truly empty. As each known fundamental force is apparently an effect of a field, Feynman failed. Louis de Broglie's waveparticle duality had rendered atomism—indivisible particles in a void—untenable, and highlighted the very notion of discontinuous particles as selfcontradictory.\n\nMeeting in 1947, Freeman Dyson, Richard Feynman, Julian Schwinger, and Sin-Itiro Tomonaga soon introduced \"renormalization\", a procedure converting QED to physics' most predictively precise theory, subsuming chemistry, optics, and statistical mechanics. QED thus won physicists' general acceptance. Paul Dirac criticized its need for renormalization as showing its unnaturalness, and called for an aether. In 1947, Willis Lamb had found unexpected motion of electron orbitals, shifted since the vacuum is not truly empty. Yet \"emptiness\" was catchy, abolishing aether conceptually, and physics proceeded ostensibly without it, even suppressing it. Meanwhile, \"sickened by untidy math, most philosophers of physics tend to neglect QED\".\n\nPhysicists have feared even mentioning \"aether\", renamed \"vacuum\", which—as such—is nonexistent. General philosophers of science commonly believe that aether, rather, is fictitious, \"relegated to the dustbin of scientific history ever since\" 1905 brought special relativity. Einstein was noncommittal to aether's nonexistence, simply said it superfluous. Abolishing Newtonian motion for electrodynamic primacy, however, Einstein inadvertently reinforced aether, and to explain motion was led back to aether in general relativity. Yet resistance to relativity theory became associated with earlier theories of aether, whose word and concept became taboo. Einstein explained special relativity's compatibility with an aether, but Einstein aether, too, was opposed. Objects became conceived as pinned directly on space and time by abstract geometric relations lacking ghostly or fluid medium.\n\nBy 1970, QED along with weak nuclear field was reduced to electroweak theory (EWT), and the strong nuclear field was modeled as quantum chromodynamics (QCD). Comprised by EWT, QCD, and Higgs field, this Standard Model of particle physics is an \"effective theory\", not truly fundamental. As QCD's particles are considered nonexistent in the everyday world, QCD especially suggests an aether, routinely found by physics experiments to exist and to exhibit relativistic symmetry. Confirmation of the Higgs particle, modeled as a condensation within the Higgs field, corroborates aether, although physics need not state or even include aether. Organizing regularities of \"observations\"—as in the covering law model—physicists find superfluous the quest to discover \"aether\".\n\nIn 1905, from special relativity, Einstein deduced mass–energy equivalence, particles being variant forms of distributed energy, how particles colliding at vast speed experience that energy's transformation into mass, producing heavier particles, although physicists' talk promotes confusion. As \"the contemporary locus of metaphysical research\", QFTs pose particles not as existing individually, yet as \"excitation modes\" of fields, the particles and their masses being states of aether, apparently unifying all physical phenomena as the more fundamental causal reality, as long ago foreseen. Yet a \"quantum\" field is an intricate abstraction—a \"mathematical\" field—virtually inconceivable as a \"classical\" field's physical properties. Nature's deeper aspects, still unknown, might elude any possible field theory.\n\nThough discovery of causality is popularly thought science's aim, search for it was shunned by the Newtonian research program, even more Newtonian than was Isaac Newton. By now, most theoretical physicists infer that the four, known fundamental interactions would reduce to superstring theory, whereby atoms and molecules, after all, are energy vibrations holding mathematical, geometric forms. Given uncertainties of scientific realism, some conclude that the concept \"causality\" raises comprehensibility of scientific explanation and thus is key folk science, but compromises precision of scientific explanation and is dropped as a science matures. Even epidemiology is maturing to heed the severe difficulties with presumptions about causality. Covering law model is among Carl G Hempel's admired contributions to philosophy of science.\n\nTypes of inference\n\nRelated subjects\n\n\n"}
{"id": "54194309", "url": "https://en.wikipedia.org/wiki?curid=54194309", "title": "Density ratio", "text": "Density ratio\n\nThe density ratio of a column of seawater is a measure of the relative contributions of temperature and salinity in determining the density gradient. At a density ratio of 1, temperature and salinity are said to be \"compensated\": their density signatures cancel, leaving a density gradient of zero. The formula for the density ratio, R, is:\n\nR = αθ/βS, where\n\n\nWhen a water column is \"doubly stable\"--both temperature and salinity contribute to the stable density gradient--the density ratio is negative (a doubly unstable water column would also have a negative density ratio, but does not commonly occur). A statically stable water column with a density ratio between 0 and 1 (cool fresh overlying warm salty) can support diffusive convection, and a statically stable water column with a density ratio larger than 1 can support salt fingering.\n\nDensity ratio may also be used to describe thermohaline variability over a non-vertical spatial interval, such as across a front in the mixed layer.\n\nIf the signs of both the numerator and denominator are reversed, the density ratio remains unchanged. A related quantity which avoids this ambiguity as well as the infinite values possible when the denominator vanishes is the Turner angle, Tu.\n\n"}
{"id": "18771723", "url": "https://en.wikipedia.org/wiki?curid=18771723", "title": "Discover Science &amp; Engineering", "text": "Discover Science &amp; Engineering\n\nDiscover Science & Engineering (DSE) is an Irish Government initiative that aims to increase interest in science, technology, engineering and mathematics (STEM) among students, teachers and members of the public in Ireland.\n\nDSE’s mission is to contribute to Ireland's continued growth and development as a society that has an active and informed interest and involvement in science, engineering and technology.\n\nOverall DSE objectives are to increase the numbers of students studying the physical sciences, promote a positive attitude to careers in science, technology, engineering and mathematics and to foster a greater understanding of science and its value to Irish society.\n\nIn September 2009, Discover Science & Engineering launched a redeveloped corporate website built on the open source CMS, WordPress.\n\nDSE runs numerous initiatives, including:\n\n\n"}
{"id": "54509", "url": "https://en.wikipedia.org/wiki?curid=54509", "title": "Dowsing", "text": "Dowsing\n\nDowsing is a type of divination employed in attempts to locate ground water, buried metals or ores, gemstones, oil, gravesites, and many other objects and materials without the use of scientific apparatus. Dowsing is considered a pseudoscience, and there is no scientific evidence that it is any more effective than random chance.\n\nDowsing is also known as divining (especially in reference to interpretation of results), doodlebugging (particularly in the United States, in searching for petroleum) or (when searching specifically for water) water finding, water witching (in the United States) or water dowsing.\n\nA Y-shaped twig or rod, or two L-shaped ones — individually called a dowsing rod, divining rod (Latin: virgula divina or baculus divinatorius), \"vining rod\", or witching rod — are sometimes used during dowsing, although some dowsers use other equipment or no equipment at all.\nDowsing appears to have arisen in the context of Renaissance magic in Germany, and it remains popular among believers in Forteana or radiesthesia.\n\nThe motion of dowsing rods is now generally attributed to the ideomotor response.\n\nDowsing as practiced today may have originated in Germany during the 16th century, when it was used in attempts to find metals.\n\nAs early as 1518 Martin Luther listed dowsing for metals as an act that broke the first commandment (\"i.e.\", as occultism). The 1550 edition of Sebastian Münster's \"Cosmographia\" contains a woodcut of a dowser with forked rod in hand walking over a cutaway image of a mining operation. The rod is labelled \"Virgula Divina – Glück rüt\" (Latin: divine rod; German \"Wünschelrute\": fortune rod or stick), but there is no text accompanying the woodcut. By 1556 Georgius Agricola's treatment of mining and smelting of ore, \"De Re Metallica\", included a detailed description of dowsing for metal ore.\n\nIn the sixteenth century, German deep mining technology was in enormous demand all over Europe. German miners were licensed to live and work in Elizabethan England; particularly in the Stannaries of Devon & Cornwall and in Cumbria. In other parts of England, the technique was used in Elizabeth's royal mines for calamine. By 1638 German miners were recorded using the technique in silver mines in Wales.\n\nThe Middle Low German name for a forked stick (Y-rod) was schlag-ruthe\n(striking rod). This was translated in the 16th century Cornish dialect to duschen\n\n(duschan according to ) (Middle English, to strike or fall ).\n\nIn 1691 the philosopher John Locke, who was born in the West Country, used the term deusing-rod for the old Latin name virgula divina\n. So, dowse is synonymous with strike, hence the phrases: to dowse/strike a light, to dowse/strike a sail.\n\nIn the lead-mining area of the Mendip Hills in Somerset in the 17th century the natural philosopher Robert Boyle, inspired by the writings of Agricola, watched a practitioner try to find \"latent veins of metals\". Boyle saw the hazel divining rod (\"virgula divinatoria\") stoop in the hands of the diviner, who protested that he was not applying any force to the twig; Boyle accepted the man's genuine belief but himself remained unconvinced.\n\nAlthough dowsing in search of water is considered an ancient practice by some, old texts about searching for water do not mention using the divining rod, and the first account of this practice was in 1568.\n\nSir William F. Barrett wrote in his 1911 book \"Psychical Research\" that: \nIn 1662, dowsing was declared to be \"superstitious, or rather satanic\" by a Jesuit, Gaspar Schott, though he later noted that he wasn't sure that the devil was always responsible for the movement of the rod. In the South of France in the 17th century it was used in tracking criminals and heretics. Its abuse led to a decree of the inquisition in 1701, forbidding its employment for purposes of justice.\n\nAn epigram by Samuel Sheppard, from \"Epigrams theological, philosophical, and romantick\" (1651) runs thus:\n\nEarly attempts at an explanation of dowsing were based on the notion that the divining rod was physically affected by emanations from substances of interest. The following explanation is from William Pryce's 1778 \"Mineralogia Cornubiensis\":\n\nA study towards the end of the nineteenth century concluded that the phenomenon was attributed to cryptesthesia, whereby the practitioner made unconscious observations of the terrain and involuntarily influenced the movement of the rod.\n\nDowsing was conducted in South Dakota in the late 19th and early 20th centuries to help homesteaders, farmers, and ranchers locate water wells on their property.\n\nIn the late 1960s during the Vietnam War, some United States Marines used dowsing to attempt to locate weapons and tunnels. As late as in 1986, when 31 soldiers were taken by an avalanche during an operation in the NATO drill Anchor Express in Vassdalen, Norway, the Norwegian army attempted to locate soldiers buried in the avalanche using dowsing as a search method.\n\nDespite the scientific evidence, dowsing is still used by some farmers and by water engineers in the UK.\n\nTraditionally, the most common dowsing rod is a forked (Y-shaped) branch from a tree or bush. Some dowsers prefer branches from particular trees, and some prefer the branches to be freshly cut. Hazel twigs in Europe and witch-hazel in the United States are traditionally commonly chosen, as are branches from willow or peach trees. The two ends on the forked side are held one in each hand with the third (the stem of the Y) pointing straight ahead. Often the branches are grasped palms down. The dowser then walks slowly over the places where he suspects the target (for example, minerals or water) may be, and the dowsing rod is expected to dip, incline or twitch when a discovery is made. This method is sometimes known as \"willow witching\".\n\nMany dowsers today use a pair of simple L-shaped metal rods. One rod is held in each hand, with the short arm of the L held upright, and the long arm pointing forward. When something is \"found\", the rods cross over one another. If the object is long and straight, such as a water pipe, the rods may point in opposite directions, showing its orientation. The rods may be fashioned from wire coat hangers or wire flags used for locating utilities. Glass or plastic rods have also been accepted. Straight rods are also sometimes used for the same purposes, and were not uncommon in early 19th-century New England.\n\nA number of devices have been marketed for modern police and military use, for example ADE 651, Sniffex, and the GT200. A US government study advised against buying \"bogus explosive detection equipment\" and noted that all testing has shown the devices to perform no better than random chance.\n\nDevices:\n\n\nA 1990 double-blind study was undertaken in Kassel, Germany, under the direction of the \"Gesellschaft zur Wissenschaftlichen Untersuchung von Parawissenschaften\" (Society for the Scientific Investigation of the Parasciences). James Randi offered a US$10,000 prize to any successful dowser. The three-day test of some 30 dowsers involved plastic pipes through which water flow could be controlled and directed. The pipes were buried under a level field, the position of each marked on the surface with a colored strip. The dowsers had to tell whether water was running through each pipe. All the dowsers signed a statement agreeing this was a fair test of their abilities and that they expected a 100 percent success rate. However, the results were no better than chance, thus no one was awarded the prize.\n\nIn a 1987–88 study in Munich by Hans-Dieter Betz and other scientists, 500 dowsers were initially tested for their skill, and the experimenters selected the best 43 among them for further tests. Water was pumped through a pipe on the ground floor of a two-storey barn. Before each test, the pipe was moved in a direction perpendicular to the water flow. On the upper floor, each dowser was asked to determine the position of the pipe. Over two years, the dowsers performed 843 such tests and, of the 43 pre-selected and extensively tested candidates, at least 37 showed no dowsing ability. The results from the remaining 6 were said to be better than chance, resulting in the experimenters' conclusion that some dowsers \"in particular tasks, showed an extraordinarily high rate of success, which can scarcely if at all be explained as due to chance ... a real core of dowser-phenomena can be regarded as empirically proven.\"\n\nFive years after the Munich study was published, Jim T. Enright, a professor of physiology who emphasised correct data analysis procedure, contended that the study's results are merely consistent with statistical fluctuations and not significant. He believed the experiments provided \"the most convincing disproof imaginable that dowsers can do what they claim\", stating that the data analysis was \"special, unconventional and customized\". Replacing it with \"more ordinary analyses\", he noted that the \"best\" dowser was on average out of closer to a mid-line guess, an advantage of 0.04%, and that the five other \"good\" dowsers were on average farther than a mid-line guess. Enright emphasized that the experimenters should have decided beforehand how to statistically analyze the results; if they only afterward chose the statistical analysis that showed the greatest success, then their conclusions would not be valid until replicated by another test analyzed by the same method. He further pointed out that the six \"good\" dowsers did not perform any better than chance in separate tests. Another study published in \"Pathophysiology\" hypothesized that such experiments as this one that were carried out in the 20th century could have been interfered with by man-made radio frequency radiation, as test subjects' bodies absorbed the radio waves and unconscious hand movement reactions took place following the standing waves or intensity variations.\n\nDowsing is considered to be a pseudoscience.\n\nScience writers such as William Benjamin Carpenter (1877), Millais Culpin (1920) and Martin Gardner (1957) considered the movement of dowsing rods to be the result of unconscious muscular action This view is widely accepted amongst the scientific community., and also by some in the dowsing community. The dowsing apparatus is known to amplify slight movements of the hands caused by a phenomenon known as the ideomotor response: people's subconscious minds may influence their bodies without consciously deciding to take action. This would make the dowsing rod susceptible to the dowsers's subconscious knowledge or perception; but also to confirmation bias.\n\nPsychologist David Marks in a 1986 article in \"Nature\" included dowsing in a list of \"effects which until recently were claimed to be paranormal but which can now be explained from within orthodox science.\" Specifically, dowsing could be explained in terms of sensory cues, expectancy effects and probability.\n\nScience writer Peter Daempfle has noted that when dowsing is subjected to scientific testing, it fails. Daempfle has written that although some dowsers claim success, this can be attributed to the underground water table being distributed relatively uniformly in certain areas.\n\nIn regard to dowsing and its use in archaeology, Kenneth Feder has written that \"the vast majority of archaeologists don't use dowsing, because they don't believe it works.\"\n\nPsychologist Chris French has noted that \"dowsing does not work when it is tested under properly controlled conditions that rule out the use of other cues to indicate target location.\"\n\nNotable dowsers include:\n\nA report in The Guardian on the subject of British water company employees using dowsing, including from Anglian and Severn Trent, noted that \"The disclosure has prompted calls for the regulator to stop companies passing the cost of a discredited medieval practice on to their customers. Ofwat said any firm failing to meet its commitments to customers faced a financial penalty.\" Anglian Water also attested to \"the effectiveness of dowsing rods\".\nA similar report from the BBC noted \"All the companies emphasised they do not encourage the use of divining rods nor issue them to engineers, and said modern methods such as drones and listening devices were preferred.\"\n\nFictional dowsers include:\n\n\n\n\n"}
{"id": "21155519", "url": "https://en.wikipedia.org/wiki?curid=21155519", "title": "Einstein–Cartan–Evans theory", "text": "Einstein–Cartan–Evans theory\n\nEinstein–Cartan–Evans theory or ECE theory was an attempted unified theory of physics proposed by the Welsh chemist and physicist Myron Wyn Evans (born May 26, 1950), which claimed to unify general relativity, quantum mechanics and electromagnetism. The hypothesis was largely published in the journal \"Foundations of Physics Letters\" between 2003 and 2005. Several of Evans' central claims were later shown to be mathematically incorrect and, in 2008, the new editor of \"Foundations of Physics\", Nobel laureate Gerard 't Hooft, published an editorial note effectively retracting the journal's support for the hypothesis.\n\nEarlier versions of the theory were called \"O(3) electrodynamics\". Evans claims that he is able to derive a generally covariant field equation for electromagnetism and gravity, similar to that derived by Mendel Sachs.\n\nEvans argues that Einstein's theory of general relativity does not take into account torsion, which is included in the Einstein–Cartan theory.\n\nIn 1998 Evans founded the Alpha Institute for Advanced Studies (AIAS) to keep developing his theory. Its website collects papers on the theory and recent developments.\n\nThe theory has been used to justify the motionless electromagnetic generator, a perpetual motion machine. In July 2017, Evans claimed (on his blog): \"There is immediate international interest in [papers] UFT382 and UFT383, describing the new energy from spacetime (ES) circuits. There is also great interest in UFT364, the paper that describes the circuit [...] These circuits should be [...] developed into power stations.\" In November 2017, Evans expanded on this point, as follows (again on his blog): \"There is no reasonable doubt that the vacuum (or aether or spacetime) contains a source of inexhaustible, safe and clean energy. This source can be used in patented and replicated circuits such as those of [Evans' self-published papers] UFT311, UFT364, UFT382, and UFT383.\"\n\nEvans' claims are not accepted by the mainstream physics community. In an editorial note in \"Foundations of Physics\" the Nobel laureate Gerard 't Hooft discussed the \"revolutionary paradigm switch in theoretical physics\" promised by ECE theory. He concluded that activities in the subject \"have remained limited to personal web pages and are absent from the standard electronic archives, while no reference to ECE theory can be spotted in any of the peer reviewed scientific journals\".\n\nSeveral of the published contributions in this theory have been shown to be mathematically incorrect. In response to these demonstrations, 't Hooft's editorial note concludes, \"Taking into account the findings of Bruhn, Hehl and Obukhhov, the discussion of ECE theory in the journal \"Foundations of Physics\" will be concluded herewith unless very good arguments are presented to resume the matter.\"\n\n\n\n\n"}
{"id": "19068598", "url": "https://en.wikipedia.org/wiki?curid=19068598", "title": "Electrohomeopathy", "text": "Electrohomeopathy\n\nElectrohomoeopathy (or Mattei cancer cure) is a derivative of homeopathy invented in the 19th century by Count Cesare Mattei. The name is derived from a combination of \"electro\" (referring to an electric bio-energy content supposedly extracted from plants and of therapeutic value, rather than electricity in its conventional sense) and \"homeopathy\" (referring to an alternative medicinal philosophy developed by Samuel Hahnemann in the 18th century). Electrohomeopathy has been defined as the combination of electrical devices and homeopathy.\n\nLucrative for its inventor and popular in the late nineteenth century, electrohomoeopathy has been described as \"utter idiocy\". Like all homeopathy, it is regarded by the medical and scientific communities as pseudoscience and its practice as quackery.\n\nElectrohomeopathy was devised by Cesare Mattei (1809–1896) in the latter part of the 19th century. Mattei, a nobleman living in a castle in the vicinity of Bologna, studied natural science, anatomy, physiology, pathology, chemistry and botany. He ultimately focused on the supposed therapeutic power of \"electricity\" in botanical extracts. Mattei made bold, unsupported claims for the efficacy of his treatments, including the claim that his treatments offered a nonsurgical alternative to cancer. His treatment regimens were met with scepticism by mainstream medicine:The electrohomeopathic system is an invention of Count Mattei who prates of \"red\", \"yellow\" and \"blue\", \"green\" and \"white\" electricity, a theory that, in spite of its utter idiocy, has attracted a considerable following and earned a large fortune for its chief promoter.\n\nNotwithstanding criticisms, including a challenge by the British medical establishment to the claimed success of his cancer treatments, electrohomeopathy (or Matteism, as it was sometimes known at the time) had adherents in Germany, France, the USA and the UK by the beginning of the 20th century; electrohomeopathy had been the subject of approximately 100 publications and there were three journals dedicated to it.\n\nRemedies are derived from what are said to be the active micro nutrients or mineral salts of certain plants. One contemporary account of the process of producing electrohomeopathic remedies was as follows:As to the nature of his remedies we learn ... that ... they are manufactured from certain herbs, and that the directions for the preparation of the necessary dilutions are given in the ordinary jargon of homeopathy. The globules and liquids, however, are \"instinct with a potent, vital, electrical force, which enables them to work wonders\". This process of \"fixing the electrical principle\" is carried on in the secret central chamber of a Neo-Moorish castle which Count Mattei has built for himself in the Bolognese Apennines ... The \"red electricity\" and \"white electricity\" supposed to be \"fixed\" in these \"vegetable compounds\" are in their very nomenclature and suggestion poor and miserable fictions.\n\nAccording to Mattei's own ideas however, every disease originates in the change of blood or of the lymphatic system or both, and remedies can therefore be mainly divided into two broad categories to be used in response to the dominant affected system. Mattei wrote that having obtained plant extracts, he was \"able to determine in the liquid vegetable electricity\". Allied to his theories and therapies were elements of Chinese medicine, of medical humours, of apparent Brownianism, as well as modified versions of Samuel Hahnemann's homeopathic principles. Electrohomeopathy has some associations with Spagyric medicine, a holistic medical philosophy claimed to be the practical application of alchemy in medical treatment, so that the principle of modern electrohomeopathy is that disease is typically multi-organic in cause or effect and therefore requires holistic treatment that is at once both complex and natural.\n\nA symposium took place in Bologna in 2008 to mark the 200th anniversary of the birth of Cesare Mattei, with attendees from India, Pakistan, Germany, UK, and the USA. Electrohomeopathy is practiced predominantly in India and Pakistan (RAJYA SABHA Parliamentary Bulletin- The Recognition of Electro Homeopathy System of Medicine Bill,\n2015 by E. M. Sudarsana Natchiappan, M. P), but there are also a number of electrohomeopathy organizations and institutions worldwide.\n\n\n"}
{"id": "1616185", "url": "https://en.wikipedia.org/wiki?curid=1616185", "title": "Electronic lab notebook", "text": "Electronic lab notebook\n\nAn electronic lab notebook (also known as electronic laboratory notebook, or ELN) is a computer program designed to replace paper laboratory notebooks. Lab notebooks in general are used by scientists, engineers, and technicians to document research, experiments, and procedures performed in a laboratory. A lab notebook is often maintained to be a legal document and may be used in a court of law as evidence. Similar to an inventor's notebook, the lab notebook is also often referred to in patent prosecution and intellectual property litigation.\n\nElectronic lab notebooks are a fairly new technology and offer many benefits to the user as well as organizations. For example: electronic lab notebooks are easier to search upon, simplify data copying and backups, and support collaboration amongst many users. \nELNs can have fine-grained access controls, and can be more secure than their paper counterparts. They also allow the direct incorporation of data from instruments, replacing the practice of printing out data to be stapled into a paper notebook.\n\nELNs can be divided into two categories:\n\n\nSolutions range from specialized programs designed from the ground up for use as an ELN, to modifications or direct use of more general programs. Examples of using more general software include using OpenWetWare, a MediaWiki install (running the same software that Wikipedia uses), as an ELN, or the use of general note taking software such as OneNote as an ELN.\n\nELN's come in many different forms. They can be standalone programs, use a client-server model, or be entirely web-based. Some use a lab-notebook approach, others resemble a blog.\n\nA good many variations on the \"ELN\" acronym have appeared. Differences between systems with different names are often subtle, with considerable functional overlap between them. Examples include \"ERN\" (Electronic Research Notebook), \"ERMS\" (Electronic Resource (or Research or Records) Management System (or Software) and SDMS (Scientific Data (or Document) Management System (or Software). Ultimately, these types of systems all strive to do the same thing: Capture, record, centralize and protect scientific data in a way that is highly searchable, historically accurate, and legally stringent, and which also promotes secure collaboration, greater efficiency, reduced mistakes and lowered total research costs.\n\nA good electronic laboratory notebook should offer a secure environment to protect the integrity of both data and process, whilst also affording the flexibility to adopt new processes or changes to existing processes without recourse to further software development. The package architecture should be a modular design, so as to offer the benefit of minimizing validation costs of any subsequent changes that you may wish to make in the future as your needs change.\n\nA good electronic laboratory notebook should be an \"out of the box\" solution that, as standard, has fully configurable forms to comply with the requirements of regulated analytical groups through to a sophisticated ELN for inclusion of structures, spectra, chromatograms, pictures, text, etc. where a preconfigured form is less appropriate. All data within the system may be stored in a database (e.g. MySQL, MS-SQL, Oracle) and be fully searchable. The system should enable data to be collected, stored and retrieved through any combination of forms or ELN that best meets the requirements of the user.\n\nThe application should enable secure forms to be generated that accept laboratory data input via PCs and/or laptops / palmtops, and should be directly linked to electronic devices such as laboratory balances, pH meters, etc. Networked or wireless communications should be accommodated for by the package which will allow data to be interrogated, tabulated, checked, approved, stored and archived to comply with the latest regulatory guidance and legislation. A system should also include a scheduling option for routine procedures such as equipment qualification and study related timelines. It should include configurable qualification requirements to automatically verify that instruments have been cleaned and calibrated within a specified time period, that reagents have been quality-checked and have not expired, and that workers are trained and authorized to use the equipment and perform the procedures.\n\nThe laboratory accreditation criteria found in the ISO 17025 standard needs to be considered for the protection and computer backup of electronic records. These criteria can be found specifically in clause 4.13.1.4 of the standard.\n\nElectronic lab notebooks used for development or research in regulated industries, such as medical devices or pharmaceuticals, are expected to comply with FDA regulations related to software validation. The purpose of the regulations is to ensure the integrity of the entries in terms of time, authorship, and content. Unlike ELNs for patent protection, FDA is not concerned with patent interference proceedings, but is concerned with avoidance of falsification. Typical provisions related to software validation are included in the medical device regulations at 21 CFR 820 (et seq.) and Title 21 CFR Part 11. Essentially, the requirements are that the software has been designed and implemented to be suitable for its intended purposes. Evidence to show that this is the case is often provided by a Software Requirements Specification (SRS) setting forth the intended uses and the needs that the ELN will meet; one or more testing protocols that, when followed, demonstrate that the ELN meets the requirements of the specification and that the requirements are satisfied under worst-case conditions. Security, audit trails, prevention of unauthorized changes without substantial collusion of otherwise independent personnel (i.e., those having no interest in the content of the ELN such as independent quality unit personnel) and similar tests are fundamental. Finally, one or more reports demonstrating the results of the testing in accordance with the predefined protocols are required prior to release of the ELN software for use. If the reports show that the software failed to satisfy any of the SRS requirements, then corrective and preventive action (\"CAPA\") must be undertaken and documented. Such CAPA may extend to minor software revisions, or changes in architecture or major revisions. CAPA activities need to be documented as well.\n\nAside from the requirements to follow such steps for regulated industry, such an approach is generally a good practice in terms of development and release of any software to assure its quality and fitness for use. There are standards related to software development and testing that can be applied (see ref.).\n\n\n"}
{"id": "11627", "url": "https://en.wikipedia.org/wiki?curid=11627", "title": "Faith healing", "text": "Faith healing\n\nFaith healing is the practice of prayer and gestures (such as laying on of hands) that are believed by some to elicit divine intervention in spiritual and physical healing, especially the Christian practice. Believers assert that the healing of disease and disability can be brought about by religious faith through prayer and/or other rituals that, according to adherents, can stimulate a divine presence and power. Religious belief in divine intervention does not depend on empirical evidence that faith healing achieves an evidence-based outcome.\n\nClaims \"attributed to a myriad of techniques\" such as prayer, divine intervention, or the ministrations of an individual healer can cure illness have been popular throughout history. There have been claims that faith can cure blindness, deafness, cancer, AIDS, developmental disorders, anemia, arthritis, corns, defective speech, multiple sclerosis, skin rashes, total body paralysis, and various injuries. Recoveries have been attributed to many techniques commonly classified as faith healing. It can involve prayer, a visit to a religious shrine, or simply a strong belief in a supreme being.\n\nMany people interpret the Bible, especially the New Testament, as teaching belief in, and the practice of, faith healing. According to a 2004 \"Newsweek\" poll, 72 percent of Americans said they believe that praying to God can cure someone, even if science says the person has an incurable disease. Unlike faith healing, advocates of spiritual healing make no attempt to seek divine intervention, instead believing in divine energy. The increased interest in alternative medicine at the end of the 20th century has given rise to a parallel interest among sociologists in the relationship of religion to health.\n\nVirtually all scientists and philosophers dismiss faith healing as pseudoscience. Faith healing can be classified as a spiritual, supernatural, or paranormal topic, and, in some cases, belief in faith healing can be classified as magical thinking. The American Cancer Society states \"available scientific evidence does not support claims that faith healing can actually cure physical ailments\". \"Death, disability, and other unwanted outcomes have occurred when faith healing was elected instead of medical care for serious injuries or illnesses.\" When parents have practiced faith healing rather than medical care, many children have died that otherwise would have been expected to live. Similar results are found in adults.\n\nRegarded as a Christian belief that God heals people through the power of the Holy Spirit, faith healing often involves the laying on of hands. It is also called supernatural healing, divine healing, and miracle healing, among other things. Healing in the Bible is often associated with the ministry of specific individuals including Elijah, Jesus and Paul.\n\nChristian physician Reginald B. Cherry views faith healing as a pathway of healing in which God uses both the natural and the supernatural to heal. Being healed has been described as a privilege of accepting Christ's redemption on the cross. Pentecostal writer Wilfred Graves, Jr. views the healing of the body as a physical expression of salvation. , after describing Jesus exorcising at sunset and healing all of the sick who were brought to him, quotes these miracles as a fulfillment of the prophecy in : \"He took up our infirmities and carried our diseases.\"\n\nEven those Christian writers who believe in faith healing do not all believe that one's faith presently brings about the desired healing. \"[Y]our faith does not effect your healing now. When you are healed rests entirely on what the sovereign purposes of the Healer are.\" Larry Keefauver cautions against allowing enthusiasm for faith healing to stir up false hopes. \"Just believing hard enough, long enough or strong enough will not strengthen you or prompt your healing. Doing mental gymnastics to 'hold on to your miracle' will not cause your healing to manifest now.\" Those who actively lay hands on others and pray with them to be healed are usually aware that healing may not always follow immediately. Proponents of faith healing say it may come later, and it may not come in this life. \"The truth is that your healing may manifest in eternity, not in time.\"\n\nParts of the four gospels in the New Testament say that Jesus cured physical ailments well outside the capacity of first-century medicine. One example is the case of \"a woman who had had a discharge of blood for twelve years, and who had suffered much under many physicians, and had spent all that she had, and was not better but rather grew worse.\" After healing her, Jesus tells her, \"Daughter, your faith has made you well. Go in peace! Be cured from your illness.\" At least two other times Jesus credited the sufferer's faith as the means of being healed: and .\n\nJesus endorsed the use of the medical assistance of the time (medicines of oil and wine) when he told the parable of the Good Samaritan (Luke 10:25-37), who \"bound up [an injured man's] wounds, pouring on oil and wine\" (verse 34) as a physician would. Jesus then told the doubting teacher of the law (who had elicited this parable by his self-justifying question, \"And who is my neighbor?\" in verse 29) to \"go, and do likewise\" in loving others with whom he would never ordinarily associate (verse 37).\n\nThe healing in the gospels is referred to as a \"sign\" to prove Jesus' divinity and to foster belief in him as the Christ. However, when asked for other types of miracles, Jesus refused some but granted others in consideration of the motive of the request. Some theologians' understanding is that Jesus healed \"all\" who were present every single time. Sometimes he determines whether they had faith that he would heal them.\n\nJesus told his followers to heal the sick and stated that signs such as healing are evidence of faith. Jesus also told his followers to \"cure sick people, raise up dead persons, make lepers clean, expel demons. You received free, give free\".\n\nJesus sternly ordered many who received healing from him: \"Do not tell anyone!\" Jesus did not approve of anyone asking for a sign just for the spectacle of it, describing such as coming from a \"wicked and adulterous generation.\"\n\nThe apostle Paul believed healing is one of the special gifts of the Holy Spirit, and that the possibility exists that certain persons may possess this gift to an extraordinarily high degree.\n\nIn the New Testament Epistle of James, the faithful are told that to be healed, those who are sick should call upon the elders of the church to pray over [them] and anoint [them] with oil in the name of the Lord.\n\nThe New Testament says that during Jesus' ministry and after his Resurrection, the apostles healed the sick and cast out demons, made lame men walk, raised the dead and performed other miracles.\n\nJesus used miracles to convince people that he was inaugurating the Messianic Age. as in Mt 12.28. Scholars have described Jesus' miracles as establishing the kingdom during his lifetime.\n\nAt the beginning of the 20th century, the new Pentecostal movement drew participants from the Holiness movement and other movements in America that already believed in divine healing. By the 1930s, several faith healers drew large crowds and established worldwide followings.\n\nThe first Pentecostals in the modern sense appeared in Topeka, Kansas, in a Bible school conducted by Charles Fox Parham, a holiness teacher and former Methodist pastor. Pentecostalism achieved worldwide attention in 1906 through the Azusa Street Revival in Los Angeles led by William Joseph Seymour.\n\nSmith Wigglesworth was also a well-known figure in the early part of the 20th century. A former English plumber turned evangelist who lived simply and read nothing but the Bible from the time his wife taught him to read, Wigglesworth traveled around the world preaching about Jesus and performing faith healings. Wigglesworth claimed to raise several people from the dead in Jesus' name in his meetings.\n\nDuring the 1920s and 1930s, Aimee Semple McPherson was a controversial faith healer of growing popularity during the Great Depression. Subsequently, William M. Branham has been credited as the initiater of the post-World War II healing revivals. The healing revival he began led many to emulate his style and spawned a generation of faith healers. Because of this, Branham has been recognized as the \"father of modern faith healers.\" According to writer and researcher Patsy Sims, \"the power of a Branham service and his stage presence remains a legend unparalleled in the history of the Charismatic movement.\" By the late 1940s, Oral Roberts, who was associated with and promoted by Branham's Voice of Healing magazine also became well known, and he continued with faith healing until the 1980s. Roberts discounted faith healing in the late 1950s, stating, \"I never was a faith healer and I was never raised that way. My parents believed very strongly in medical science and we have a doctor who takes care of our children when they get sick. I cannot heal anyone – God does that.\" A friend of Roberts was Kathryn Kuhlman, another popular faith healer, who gained fame in the 1950s and had a television program on CBS. Also in this era, Jack Coe and A. A. Allen were faith healers who traveled with large tents for large open-air crusades.\n\nOral Roberts's successful use of television as a medium to gain a wider audience led others to follow suit. His former pilot, Kenneth Copeland, started a healing ministry. Pat Robertson, Benny Hinn, and Peter Popoff became well-known televangelists who claimed to heal the sick. Richard Rossi is known for advertising his healing clinics through secular television and radio. Kuhlman influenced Benny Hinn, who adopted some of her techniques and wrote a book about her.\n\nThe Roman Catholic Church recognizes two \"not mutually exclusive\" kinds of healing, one justified by science and one justified by faith:\n\nIn 2000, the Congregation for the Doctrine of the Faith issued \"Instruction on prayers for healing\" with specific norms about prayer meetings for obtaining healing, which presents the Catholic Church's doctrines of sickness and healing.\n\nIt accepts \"that there may be means of natural healing that have not yet been understood or recognized by science,\" but it rejects superstitious practices which are neither compatible with Christian teaching nor compatible with scientific evidence.\n\nFaith healing is reported by Catholics as the result of intercessory prayer to a saint or to a person with the gift of healing. According to \"U.S. Catholic\" magazine, \"Even in this skeptical, postmodern, scientific age—miracles really are possible.\" Three-fourths of American Catholics say they pray for miracles.\n\nAccording to John Cavadini, when healing is granted, \"The miracle is not primarily for the person healed, but for all people, as a sign of God's work in the ultimate healing called 'salvation,' or a sign of the kingdom that is coming.\" Some might view their own healing as a sign they are particularly worthy or holy, while others do not deserve it.\n\nThe Catholic Church has a special Congregation dedicated to the careful investigation of the validity of alleged miracles attributed to prospective saints. Pope Francis tightened the rules on money and miracles in the canonization process. Since Catholic Christians believe the lives of canonized saints in the Church will reflect Christ's, many have come to expect healing miracles. While the popular conception of a miracle can be wide-ranging, the Catholic Church has a specific definition for the kind of miracle formally recognized in a canonization process.\n\nAccording to \"Catholic Encyclopedia\", it is often said that cures at shrines and during Christian pilgrimages are mainly due to psychotherapy — partly to confident trust in Divine providence, and partly to the strong expectancy of cure that comes over suggestible persons at these times and places.\n\nAmong the best-known accounts by Catholics of faith healings are those attributed to the miraculous intercession of the apparition of the Blessed Virgin Mary known as Our Lady of Lourdes at the Sanctuary of Our Lady of Lourdes in France and the remissions of life-threatening disease claimed by those who have applied for aid to Saint Jude, who is known as the \"patron saint of lost causes\".\n, Catholic medics have asserted that there have been 67 miracles and 7,000 unexplainable medical cures at Lourdes since 1858. In a 1908 book, it says these cures were subjected to intense medical scrutiny and were only recognized as authentic spiritual cures after a commission of doctors and scientists, called the Lourdes Medical Bureau, had ruled out any physical mechanism for the patient's recovery. Belgian philosopher and skeptic Etienne Vermeersch coined the term Lourdes effect as a criticism of the magical thinking and placebo effect possibilities for the claimed miraculous cures as there are no documented events where a severed arm has been reattached through faith healing at Lourdes. Vermeersch identifies ambiguity and equivocal nature of the miraculous cures as a key feature of miraculous events.\n\nChristian Science claims that healing is possible through an understanding of the underlying spiritual perfection of God's creation. The world as humanly perceived is believed to be a distortion of spiritual reality. Christian Scientists believe that healing through prayer is possible insofar as it succeeds in correcting the distortion. Christian Scientists believe that prayer does not change the spiritual creation but gives a clearer view of it, and the result appears in the human scene as healing: the human picture adjusts to coincide more nearly with the divine reality. Prayer works through love: the recognition of God's creation as spiritual, intact, and inherently lovable.\n\nAn important point in Christian Science is that effectual prayer and the moral regeneration of one's life go hand-in-hand: that \"signs and wonders are wrought in the metaphysical healing of physical disease; but these signs are only to demonstrate its divine origin, to attest the reality of the higher mission of the Christ-power to take away the sins of the world.\" Christian Science teaches that disease is mental, a mortal fear, a mistaken belief or conviction of the necessity and power of ill-health – an ignorance of God's power and goodness. The chapter \"Prayer\" in \"Science and Health with Key to the Scriptures\" gives a full account of healing through prayer, while the testimonies at the end of the book are written by people who believe they have been healed through spiritual understanding gained from reading the book.\n\nThe Church of Jesus Christ of Latter-day Saints (LDS) has had a long history of faith healings. Many members of the LDS Church have told their stories of healing within the LDS publication, the \"Ensign\". The church believes healings come most often as a result of priesthood blessings given by the laying on of hands; however, prayer often accompanied with fasting is also thought to cause healings. Healing is always attributed to be God's power. Latter-day Saints believe that the Priesthood of God, held by prophets (such as Moses) and worthy disciples of the Savior, was restored via heavenly messengers to the first prophet of this dispensation, Joseph Smith.\n\nAccording to LDS doctrine, even though members may have the restored priesthood authority to heal in the name of Jesus Christ, all efforts should be made to seek the appropriate medical help. Brigham Young stated this effectively, while also noting that the ultimate outcome is still dependent on the will of God.\n\nKonkhogin Haokip has claimed some Muslims believe that the Quran was sent not only as a revelation, but as a medicine, and that they believe the Quran heals any physical and spiritual ailments through such practices as\n\nSome critics of Scientology have referred to some of its practices as being similar to faith healing, based on claims made by L. Ron Hubbard in \"\" and other writings.\n\nNearly all scientists dismiss faith healing as pseudoscience. Some opponents of the pseudoscience label assert that faith healing makes no scientific claims and thus should be treated as a matter of faith that is not testable by science. Critics reply that claims of medical cures should be tested scientifically because, although faith in the supernatural is not in itself usually considered to be the purview of science, claims of reproducible effects are nevertheless subject to scientific investigation.\n\nScientists and doctors generally find that faith healing lacks biological plausibility or epistemic warrant, which is one of the criteria to used to judge whether clinical research is ethical and financially justified. A Cochrane review of intercessory prayer found \"although some of the results of individual studies suggest a positive effect of intercessory prayer, the majority do not\". The authors concluded: \"We are not convinced that further trials of this intervention should be undertaken and would prefer to see any resources available for such a trial used to investigate other questions in health care.\" \n\nA review in 1954 investigated spiritual healing, therapeutic touch and faith healing. Of the hundred cases reviewed, none revealed that the healer's intervention alone resulted in any improvement or cure of a measurable organic disability.\n\nIn addition, at least one study has suggested that adult Christian Scientists, who generally use prayer rather than medical care, have a higher death rate than other people of the same age.\n\nThe Global Medical Research Institute (GMRI) was created in 2012 to start collecting medical records of patients who claim to have received a supernatural healing miracle as a result of Christian Spiritual Healing practices. The organization has a panel of medical doctors who review the patient’s records looking at entries prior to the claimed miracles and entries after the miracle was claimed to have taken place. “The overall goal of GMRI is to promote an empirically grounded understanding of the physiological, emotional, and sociological effects of Christian Spiritual Healing practices.” This is accomplished by applying the same rigorous standards used in other forms of medical and scientific research.\n\nSkeptics of faith healing offer primarily two explanations for anecdotes of cures or improvements, relieving any need to appeal to the supernatural. The first is \"post hoc ergo propter hoc\", meaning that a genuine improvement or spontaneous remission may have been experienced coincidental with but independent from anything the faith healer or patient did or said. These patients would have improved just as well even had they done nothing. The second is the placebo effect, through which a person may experience genuine pain relief and other symptomatic alleviation. In this case, the patient genuinely has been helped by the faith healer or faith-based remedy, not through any mysterious or numinous function, but by the power of their own belief that they would be healed. In both cases the patient may experience a real reduction in symptoms, though in neither case has anything miraculous or inexplicable occurred. Both cases, however, are strictly limited to the body's natural abilities.\n\nAccording to the American Cancer Society:\nThe American Medical Association considers that prayer as therapy should not be a medically reimbursable or deductible expense.\n\nReliance on faith healing to the exclusion of other forms of treatment can have a public health impact when it reduces or eliminates access to modern medical techniques. This is evident in both higher mortality rates for children and in reduced life expectancy for adults. Critics have also made note of serious injury that has resulted from falsely labelled \"healings\", where patients erroneously consider themselves cured and cease or withdraw from treatment. For example, at least six people have died after faith healing by their church and being told they had been healed of HIV and could stop taking their medications. It is the stated position of the AMA that \"prayer as therapy should not delay access to traditional medical care\". Choosing faith healing while rejecting modern medicine can and does cause people to die needlessly.\n\nChristian theological criticism of faith healing broadly falls into two distinct levels of disagreement.\n\nThe first is widely termed the \"open-but-cautious\" view of the miraculous in the church today. This term is deliberately used by Robert L. Saucy in the book \"Are Miraculous Gifts for Today?\". Don Carson is another example of a Christian teacher who has put forward what has been described as an \"open-but-cautious\" view. In dealing with the claims of Warfield, particularly \"Warfield's insistence that miracles ceased,\" Carson asserts, \"But this argument stands up only if such miraculous gifts are theologically tied exclusively to a role of attestation; and that is demonstrably not so.\" However, while affirming that he does not expect healing to happen today, Carson is critical of aspects of the faith healing movement, \"Another issue is that of immense abuses in healing practises... The most common form of abuse is the view that since all illness is directly or indirectly attributable to the devil and his works, and since Christ by his cross has defeated the devil, and by his Spirit has given us the power to overcome him, healing is the inheritance right of all true Christians who call upon the Lord with genuine faith.\"\n\nThe second level of theological disagreement with Christian faith healing goes further. Commonly referred to as cessationism, its adherents either claim that faith healing will not happen today at all, or may happen today, but it would be unusual. Richard Gaffin argues for a form of cessationism in an essay alongside Saucy's in the book \"Are Miraculous Gifts for Today\"? In his book \"Perspectives on Pentecost\" Gaffin states of healing and related gifts that \"the conclusion to be drawn is that as listed in 1 Corinthians 12(vv. 9f., 29f.) and encountered throughout the narrative in Acts, these gifts, particularly when exercised regularly by a given individual, are part of the foundational structure of the church... and so have passed out of the life of the church.\" Gaffin qualifies this, however, by saying \"At the same time, however, the sovereign will and power of God today to heal the sick, particularly in response to prayer (see e.g. James 5:14,15), ought to be acknowledged and insisted on.\"\n\nSkeptics of faith healers point to fraudulent practices either in the healings themselves (such as plants in the audience with fake illnesses), or concurrent with the healing work supposedly taking place and claim that faith healing is a quack practice in which the \"healers\" use well known non-supernatural illusions to exploit credulous people in order to obtain their gratitude, confidence and money. James Randi's \"The Faith Healers\" investigates Christian evangelists such as Peter Popoff, who claimed to heal sick people on stage in front of an audience. Popoff pretended to know private details about participants' lives by receiving radio transmissions from his wife who was off-stage and had gathered information from audience members prior to the show. According to this book, many of the leading modern evangelistic healers have engaged in deception and fraud. The book also questioned how faith healers use funds that were sent to them for specific purposes. Physicist Robert L. Park and doctor and consumer advocate Stephen Barrett have called into question the ethics of some exorbitant fees.\n\nThere have also been legal controversies. For example, in 1955 at a Jack Coe revival service in Miami, Florida, Coe told the parents of a three-year-old boy that he healed their son who had polio. Coe then told the parents to remove the boy's leg braces. However, their son was not cured of polio and removing the braces left the boy in constant pain. As a result, through the efforts of Joseph L. Lewis, Coe was arrested and charged on February 6, 1956 with practicing medicine without a license, a felony in the state of Florida. A Florida Justice of the Peace dismissed the case on grounds that Florida exempts divine healing from the law. Later that year Coe was diagnosed with bulbar polio, and died a few weeks later at Dallas' Parkland Hospital on December 17, 1956.\n\nTV personality Derren Brown produced a show on faith healing entitled \"Miracles for sale\" which arguably exposed the art of faith healing as a scam. In this show, Derren trained a scuba diver trainer picked from the general public to be a faith healer and took him to Texas to successfully deliver a faith healing session to a congregation.\n\nThe 1974 Child Abuse Prevention and Treatment Act (CAPTA) required states to grant religious exemptions to child neglect and child abuse laws in order to receive federal money. The CAPTA amendments of 1996 state:\n\nThirty-one states have child-abuse religious exemptions. These are Alabama, Alaska, California, Colorado, Delaware, Florida, Georgia, Idaho, Illinois, Indiana, Iowa, Kansas, Kentucky, Louisiana, Maine, Michigan, Minnesota, Mississippi, Missouri, Montana, Nevada, New Hampshire, New Jersey, New Mexico, Ohio, Oklahoma, Oregon, Pennsylvania, Vermont, Virginia, and Wyoming. In six of these states, Arkansas, Idaho, Iowa, Louisiana, Ohio and West Virginia, the exemptions extend to murder and manslaughter. Of these, Idaho is the only state accused of having a large number of deaths due to the legislation in recent times. In February 2015, controversy was sparked in Idaho over a bill believed to further reinforce parental rights to deny their children medical care.\n\nParents have been convicted of child abuse and felony reckless negligent homicide and found responsible for killing their children when they withheld lifesaving medical care and chose only prayers.\n\n"}
{"id": "2018618", "url": "https://en.wikipedia.org/wiki?curid=2018618", "title": "First Earth Battalion", "text": "First Earth Battalion\n\nThe First Earth Battalion was the name proposed by Lieutenant Colonel Jim Channon, a U.S. soldier who had served in Vietnam, for his idea of a new military of supersoldiers to be organized along New Age lines. A book of the same name was published in 1982.\n\nAccording to the book \"The Men Who Stare at Goats\" by journalist Jon Ronson, Channon spent time in the 1970s with many of the people in California credited with starting the Human Potential Movement, and subsequently wrote an operations manual for a First Earth Battalion. The manual was a 125-page mixture of drawings, graphs, maps, polemical essays, and point-by-point redesigns of every aspect of military life. Channon imagined a new battlefield uniform that would include pouches for ginseng regulators, divining tools, food stuffs to enhance night vision, and a loudspeaker that would automatically emit \"indigenous music and words of peace.\" A movie based on the book—released in Autumn 2009—starring George Clooney, Ewan McGregor, Jeff Bridges and Kevin Spacey, fictionalized the First Earth Battalion as the \"New Earth Army\".\n\nChannon believed the Army could be the principal moral and ethical basis on which politics could harmonize in the name of the Earth. He declared that the First Earth Battalion's primary allegiance was to the planet earth. Channon envisioned that the First Earth Battalion would organize itself informally: uniforms without uniformity, structure without status, and unity powered by diversity, and members would be multicultural, with each race contributing to \"rainbow power\". He also proposed as a guiding principle that members of the First Earth Battalion seek nondestructive methods of conflict resolution because their first loyalty is to the planet.\n\nChannon adopted the term \"warrior monk\" for potential members of the First Earth Battalion.\n\nAccording to the book \"Mind Wars\" by Ronald McRae, each member of the First Earth Battalion would be sworn to uphold a credo of \"high commandos and guerrilla gurus\":\n\n\n"}
{"id": "57003665", "url": "https://en.wikipedia.org/wiki?curid=57003665", "title": "GPIb-IX-V", "text": "GPIb-IX-V\n\nThis transmembrane glycoprotein complex is composed of four subunits: GPIbα, GPIbβ, GPV and GPIX. Each of them has a variable number of leucine-rich repeats. GPIbα and GPIbβ are linked by disulfide bridges, while the GPV and GPIX associate non-covalently with the complex. GPIbα subunit bears the binding site for von Willebrand factor (vWF), α-thrombin, leukocyte integrin αMβ2 and P-selectin. The binding between GPIbα and vWF mediates the capture of platelets to the injured vascular wall. The deficiency in glycoprotein Ib-IX-V complex synthesis leads to Bernard-Soulier syndrome.\n"}
{"id": "9307303", "url": "https://en.wikipedia.org/wiki?curid=9307303", "title": "Generalised likelihood uncertainty estimation", "text": "Generalised likelihood uncertainty estimation\n\nGeneralized likelihood uncertainty estimation (GLUE) is a statistical method used in hydrology for quantifying the uncertainty of model predictions. The method was introduced by Keith Beven and Andrew Binley in 1992. The basic idea of GLUE is that given our inability to represent exactly in a mathematical model how nature works, there will always be several different models that mimic equally well an observed natural process (such as river discharge). Such equally acceptable or behavioral models are therefore called equifinal.\n\nThe methodology deals with models whose results are expressed as probability distributions of possible outcomes, often in the form of Monte Carlo simulations, and the problem can be viewed as assessing, and comparing between models, how good these representations of uncertainty are. There is an implicit understanding that the models being used are approximations to what might be obtained from a thorough Bayesian analysis of the problem if a fully adequate model of real-world hydrological processes were available.\n"}
{"id": "11415165", "url": "https://en.wikipedia.org/wiki?curid=11415165", "title": "Goodman fatigue equation", "text": "Goodman fatigue equation\n\nThe Goodman fatigue equation is used by engineers for fatigue analysis. The equation is used to make correlations of experimental fatigue data of metals and other materials.\n\nThe equation is also used to determine the failure mechanisms of sucker rods in pumping oil wells worldwide, and to help design new sucker rod strings for rod-pumped oil wells.\n\nFrom similar triangles the Goodman fatigue equation is :\n\n( 4a) τm /Sus + τa /Ses = 1/n in which the stress components are given by \n\n"}
{"id": "25214567", "url": "https://en.wikipedia.org/wiki?curid=25214567", "title": "Historia Naturalis Brasiliae", "text": "Historia Naturalis Brasiliae\n\nHistoria Naturalis Brasiliae (), originally written in Latin, is the first scientific work on the natural history of Brazil, written by Dutch naturalist Willem Piso and research by German scientist Georg Marcgraf, published in 1648. The work includes observations made by the German naturalist H. Gralitzio, in addition to humanist Johannes de Laet. It was dedicated to Johan Maurits, Count of Nassau, who was the patron of the project during the period of Dutch rule in Brazil.\n\nThough referring to Brazil generally throughout the text, the authors' research was of the coastal strip of the Northeast, occupied by the Dutch West India Company. It is based on Marcgraf and Piso's time in Brazil, starting in 1637. It offers an important early European insight into Brazilian flora and fauna by analyzing plants and animals and studying tropical diseases and indigenous therapies. Also included is William Piso's interpretation and first opinions of the indigenous people who he would go on to describe as barbarous and lacking in science. This would lead to concern amongst Piso and his contemporaries that these people might not be able to contribute to studying medicine and botany.\n\nIt was edited, as stated on its title page, in: \"Lugdun. Batavorum: Apud Franciscum Hackium ; et Amstelodami: Apud Lud. Elzevirium\". \"Elzevirium\" is the Latin name of the prestigious Elsevier publisher, which still exists.\n\nThe work consists of a single volume, originally measuring 40 centimeters (height) and its full title, with subtitle, is: \"\" Historia naturalis Brasiliae ...: in qua non tantum plantae et animalia, sed et indigenarum morbi, ingenia et mores describuntur et iconibus supra quingentas illustrantur \" \".\n\nThe Brazilian physician and researcher Juliano Moreira said of the work: This clearly masterful work, when carefully reexamined, shows, at each perquisition, new excellences, and thus it is still one of the most authentic glories of Dutch medical literature. We owe to Pies a description, so accurate and meticulous, of the then reigning endemics in Brazil and the means of treating them. He observed the yaws, tetanus, various types of paralysis, dysentery, hemeralopia, maculopapular. He described Ipecac and emeto - cathartic qualities, which aboriginals used long before the famous doctor Adrian Helvetius, grandfather of the notable French philosopher Claudio Adriano Helvetius received from Louis XIV a thousand louis gold, titles and honors for having discovered exactly those same therapeutic virtues. The Treaty of Helvetius titled \"\"Remède contre le cours du ventre\".\n\nThe work circulated widely in northern Europe and beyond, so that while it detailed the flora and fauna of coastal South America, it was an important publication for those working elsewhere. Richly illustrated scientific texts allowed knowledge to be disseminated even when scholars themselves could not travel to the research site. The work remained unsurpassed until the nineteenth century, and between its initial publication and subsequent research it remained highly influential. Diverse writers referred to the text, including Miguel Venegas, author of \"Noticia de la California\" (1757), Anglo-American Protestant theologian Cotton Mather, who saw in the text evidence of divine planning; and amateur American naturalist Thomas Jefferson, who mentioned Marcgraf in his \"Notes on the State of Virginia\".\n\nThis work would prove to be incredibly influential especially in the field of ecology, being used by a variety of different ecologists in different locations and time. The long lasting influence could be seen outside of just the field of ecology, as well, with various other forms of science utilizing the findings in various ways. In particular, Ole Worm utilized a similar organizational structure when documenting natural history of Denmark while even using some of the images in his work, Museum Wormianum. Carl Linnaeus and Albert Aublet would also use the work of Macgrave in several of their texts and images.\n\nRelevancy was still found in the 20th century with a herbarium being discovered that would contain a hefty amount of items that were used in the Netherlands during the 17th century. The utility of these documents from John Maurice, Prince of Nassau-Siegen were able to assist other researchers and academics even in a more modern context. This discovery would also cause people to seek out a variety of these books detailing herbariums in order to achieves further information that Maurice may have.\n\n"}
{"id": "14229", "url": "https://en.wikipedia.org/wiki?curid=14229", "title": "Homeopathy", "text": "Homeopathy\n\nHomeopathy or homœopathy is a system of alternative medicine created in 1796 by Samuel Hahnemann, based on his doctrine of \"like cures like\" (\"similia similibus curentur\"), a claim that a substance that causes the symptoms of a disease in healthy people would cure similar symptoms in sick people. Homeopathy is a pseudoscience – a belief that is incorrectly presented as scientific. Homeopathic preparations are not effective for treating any condition; large-scale studies have found homeopathy to be no more effective than a placebo, indicating that any positive effects that follow treatment are due to factors such as normal recovery from illness, or regression toward the mean.\n\nHahnemann believed the underlying causes of disease were phenomena that he termed \"miasms\", and that homeopathic preparations addressed these. The preparations are manufactured using a process of homeopathic dilution, in which a chosen substance is repeatedly diluted in alcohol or distilled water, each time with the containing vessel being struck against an elastic material, commonly a leather-bound book. Dilution typically continues well past the point where no molecules of the original substance remain. Homeopaths select homeopathics by consulting reference books known as \"repertories\", and by considering the totality of the patient's symptoms, personal traits, physical and psychological state, and life history.\n\nHomeopathy is not a plausible system of treatment, as its dogmas about how drugs, illness, the human body, liquids and solutions operate are contradicted by a wide range of discoveries across biology, psychology, physics and chemistry made in the two centuries since its invention. Although some clinical trials produce positive results, multiple systematic reviews have shown that this is because of chance, flawed research methods, and reporting bias. Homeopathic practice has been criticized as unethical because it discourages the use of effective treatments, with the World Health Organization warning against using homeopathy to try to treat severe diseases such as HIV and malaria. The continued practice of homeopathy, despite a lack of evidence of efficacy, has led to it being characterized within the scientific and medical communities as nonsense, quackery, and a sham.\n\nThere have been four large scale assessments of homeopathy by national or international bodies: the Australian National Health and Medical Research Council; the United Kingdom's House of Commons Science and Technology Committee; the European Academies' Science Advisory Council; and the Swiss Federal Health Office. Each concluded that homeopathy is ineffective, and recommended against the practice receiving any further funding. The National Health Service in England has announced a policy of not funding homeopathic medicine because it is \"a misuse of resources\". They called on the UK Department of Health to add homeopathic remedies to the blacklist of forbidden prescription items, and the NHS ceased funding homeopathic remedies in November 2017.\n\nThe concept of \"like cures like\" may have been suggested by Hippocrates around 400 BC, when he prescribed a small dose of mandrake root to treat mania, knowing it produces mania in much larger doses. Similarly, in the 16th century, Paracelsus wrote \"similia similibus curantur\" (similar to the subjunctive form later used by Hahnemann), often translated as \"what makes a man ill also cures him\".\n\nIn the late 18th and 19th centuries, mainstream medicine used methods like bloodletting and purging, and administered complex mixtures, such as Venice treacle, which was made from 64 substances including opium, myrrh, and viper's flesh. These treatments often worsened symptoms and sometimes proved fatal. Hahnemann rejected these practices – which had been extolled for centuries – as irrational and inadvisable;\ninstead, he advocated the use of single drugs at lower doses and promoted an immaterial, vitalistic view of how living organisms function, believing that diseases have spiritual, as well as physical causes.\n\nThe term \"homeopathy\" was coined by Hahnemann and first appeared in print in 1807.\n\nHahnemann conceived of homeopathy while translating a medical treatise by the Scottish physician and chemist William Cullen into German. Being sceptical of Cullen's theory concerning cinchona's use for curing malaria, Hahnemann ingested some bark specifically to investigate what would happen. He experienced fever, shivering and joint pain: symptoms similar to those of malaria itself. From this, Hahnemann came to believe that all effective drugs produce symptoms in healthy individuals similar to those of the diseases that they treat, in accord with the \"law of similars\" that had been proposed by ancient physicians. An account of the effects of eating cinchona bark noted by Oliver Wendell Holmes, and published in 1861, failed to reproduce the symptoms Hahnemann reported. Hahnemann's law of similars is a postulate rather than a scientific law. This led to the name \"homeopathy\", which comes from the \"hómoios\", \"-like\" and \"páthos\", \"suffering\".\n\nSubsequent scientific work showed that cinchona cures malaria because it contains quinine, which kills the \"Plasmodium falciparum\" parasite that causes the disease; the mechanism of action is unrelated to Hahnemann's ideas.\n\nHahnemann began to test what effects substances produced in humans, a procedure that would later become known as \"homeopathic proving\". These tests required subjects to test the effects of ingesting substances by clearly recording all of their symptoms as well as the ancillary conditions under which they appeared. He published a collection of provings in 1805, and a second collection of 65 preparations appeared in his book, \"Materia Medica Pura\", in 1810.\n\nBecause Hahnemann believed that large doses of drugs that caused similar symptoms would only aggravate illness, he advocated extreme dilutions of the substances; he devised a technique for making dilutions that he believed would preserve a substance's therapeutic properties while removing its harmful effects. Hahnemann believed that this process aroused and enhanced \"the spirit-like medicinal powers of the crude substances\".\nHe gathered and published a complete overview of his new medical system in his 1810 book, \"The Organon of the Healing Art\", whose 6th edition, published in 1921, is still used by homeopaths today.\n\nIn the \"Organon\", Hahnemann introduced the concept of \"miasms\" as \"infectious principles\" underlying chronic disease. Hahnemann associated each miasm with specific diseases, and thought that initial exposure to miasms causes local symptoms, such as skin or venereal diseases. If, however, these symptoms were suppressed by medication, the cause went deeper and began to manifest itself as diseases of the internal organs. Homeopathy maintains that treating diseases by directly alleviating their symptoms, as is sometimes done in conventional medicine, is ineffective because all \"disease can generally be traced to some latent, deep-seated, underlying chronic, or inherited tendency\". The underlying imputed miasm still remains, and deep-seated ailments can be corrected only by removing the deeper disturbance of the vital force.\n\nHahnemann's hypotheses for the direct or remote cause of all chronic diseases (miasms) originally presented only three, psora (the itch), syphilis (venereal disease) or sycosis (fig-wart disease). Of these three the most important was \"psora\" (Greek for \"itch\"), described as being related to any itching diseases of the skin, supposed to be derived from suppressed scabies, and claimed to be the foundation of many further disease conditions. Hahnemann believed psora to be the cause of such diseases as epilepsy, cancer, jaundice, deafness, and cataracts.\nSince Hahnemann's time, other miasms have been proposed, some replacing one or more of psora's proposed functions, including tuberculosis and cancer miasms.\n\nThe law of susceptibility implies that a negative state of mind can attract hypothetical disease entities called \"miasms\" to invade the body and produce symptoms of diseases. Hahnemann rejected the notion of a disease as a separate thing or invading entity, and insisted it was always part of the \"living whole\". Hahnemann coined the expression \"allopathic medicine\", which was used to pejoratively refer to traditional Western medicine.\n\nHahnemann's miasm theory remains disputed and controversial within homeopathy even in modern times. The theory of miasms has been criticized as an explanation developed by Hahnemann to preserve the system of homeopathy in the face of treatment failures, and for being inadequate to cover the many hundreds of sorts of diseases, as well as for failing to explain disease predispositions, as well as genetics, environmental factors, and the unique disease history of each patient.\n\nHomeopathy achieved its greatest popularity in the 19th century. It was introduced to the United States in 1825 by Hans Birch Gram, a student of Hahnemann. The first homeopathic school in the US opened in 1835, and in 1844, the first US national medical association, the American Institute of Homeopathy, was established. Throughout the 19th century, dozens of homeopathic institutions appeared in Europe and the United States, and by 1900, there were 22 homeopathic colleges and 15,000 practitioners in the United States. Because medical practice of the time relied on ineffective and often dangerous treatments, patients of homeopaths often had better outcomes than those of the doctors of the time. Homeopathic preparations, even if ineffective, would almost surely cause no harm, making the users of homeopathic preparations less likely to be killed by the treatment that was supposed to be helping them. The relative success of homeopathy in the 19th century may have led to the abandonment of the ineffective and harmful treatments of bloodletting and purging and to have begun the move towards more effective, science-based medicine.\nOne reason for the growing popularity of homeopathy was its apparent success in treating people suffering from infectious disease epidemics.\nDuring 19th-century epidemics of diseases such as cholera, death rates in homeopathic hospitals were often lower than in conventional hospitals, where the treatments used at the time were often harmful and did little or nothing to combat the diseases.\n\nFrom its inception, however, homeopathy was criticized by mainstream science. Sir John Forbes, physician to Queen Victoria, said in 1843 that the extremely small doses of homeopathy were regularly derided as useless, \"an outrage to human reason\". James Young Simpson said in 1853 of the highly diluted drugs: \"No poison, however strong or powerful, the billionth or decillionth of which would in the least degree affect a man or harm a fly.\"\n19th-century American physician and author Oliver Wendell Holmes, Sr. was also a vocal critic of homeopathy and published an essay in 1842 entitled \"Homœopathy and Its Kindred Delusions\". The members of the French Homeopathic Society observed in 1867 that some leading homeopathists of Europe not only were abandoning the practice of administering infinitesimal doses but were also no longer defending it. The last school in the US exclusively teaching homeopathy closed in 1920.\n\nAccording to , the Nazi regime in Germany was fascinated by homeopathy, and spent large sums of money on researching its mechanisms, but without gaining a positive result. Unschuld further argues that homeopathy never subsequently took root in the United States, but remained more deeply established in European thinking.\nIn the United States, the \"Food, Drug, and Cosmetic Act\" of 1938 (sponsored by Royal Copeland, a Senator from New York and homeopathic physician) recognized homeopathic preparations as drugs. In the 1950s, there were only 75 pure homeopaths practising in the U.S. However, by the mid to late 1970s, homeopathy made a significant comeback and sales of some homeopathic companies increased tenfold. Some homeopaths give credit for the revival to Greek homeopath George Vithoulkas, who performed a \"great deal of research to update the scenarios and refine the theories and practice of homeopathy\", beginning in the 1970s, but Ernst and Singh consider it to be linked to the rise of the New Age movement. Whichever is correct, mainstream pharmacy chains recognized the business potential of selling homeopathic preparations. The Food and Drug Administration held a hearing April 20 and 21, 2015, requesting public comment on regulation of homeopathic drugs. The FDA cited the growth of sales of over-the-counter homeopathic medicines, which was $2.7 billion for 2007.\n\nBruce Hood has argued that the increased popularity of homeopathy in recent times may be due to the comparatively long consultations practitioners are willing to give their patients, and to an irrational preference for \"natural\" products, which people think are the basis of homeopathic preparations.\n\nSince the beginning of the 21st century a series of meta analysis has further shown that therapeutic claims of Homeopathy lack scientific justification. In a 2010 report, the Science and Technology Committee of the United Kingdom House of Commons recommended that homeopathy should no longer be a beneficiary of NHS funding due its lack of scientific credibility; funding ceased in 2017. In March 2015, the National Health and Medical Research Council of Australia published an information paper on Homeopathy. The main findings of the report were 'there are no health conditions for which there is reliable evidence that homeopathy is effective\". Reactions to the report sparked world headlines which suggested that the NHMRC had found that homeopathy is not effective for all conditions.\n\nIn 2018, Australian pharmacies ignored recommendations for a homeopathic ban in the broader scope of the federal government accepting only three of the 45 recommendations made by the review of Pharmacy Remuneration and Regulation (which were delivered in September 2017 to Health Minister Greg Hunt).\n\nHomeopathic preparations are referred to as \"homeopathics\" or \"remedies\". Practitioners rely on two types of reference when prescribing: \"Materia Medica\" and repertories. A homeopathic \"materia medica\" is a collection of \"drug pictures\", organized alphabetically. These entries describe the symptom patterns associated with individual preparations. A homeopathic repertory is an index of disease symptoms that lists preparations associated with specific symptoms. In both cases different compilers may dispute particular inclusions. The first symptomatic homeopathic \"materia medica\" was arranged by Hahnemann. The first homeopathic repertory was Georg Jahr's \"Symptomenkodex\", published in German in 1835, and translated into English as the \"Repertory to the more Characteristic Symptoms of Materia Medica\" by Constantine Hering in 1838.\nThis version was less focused on disease categories and was the forerunner to later works by James Tyler Kent. Repertories, in particular, may be very large.\n\nHomeopathy uses animal, plant, mineral, and synthetic substances in its preparations, generally referring to them using Latin or faux-Latin names. Examples include \"arsenicum album\" (arsenic oxide), \"natrum muriaticum\" (sodium chloride or table salt), \"Lachesis muta\" (the venom of the bushmaster snake), \"opium\", and \"thyroidinum\" (thyroid hormone).\n\nSome homeopaths use so-called \"nosodes\" (from the Greek \"nosos\", disease) made from diseased or pathological products such as fecal, urinary, and respiratory discharges, blood, and tissue. Conversely, preparations made from \"healthy\" specimens are called \"sarcodes\".\n\nSome modern homeopaths use preparations they call \"imponderables\" because they do not originate from a substance but some other phenomenon presumed to have been \"captured\" by alcohol or lactose. Examples include X-rays\nand sunlight.\n\nOther minority practices include paper preparations, where the substance and dilution are written on pieces of paper and either pinned to the patients' clothing, put in their pockets, or placed under glasses of water that are then given to the patients, and the use of radionics to manufacture preparations. Such practices have been strongly criticized by classical homeopaths as unfounded, speculative, and verging upon magic and superstition.\n\nHahnemann found that undiluted doses caused reactions, sometimes dangerous ones, so specified that preparations be given at the lowest possible dose. He found that this reduced potency as well as side-effects, but formed the view that vigorous shaking and striking on an elastic surface – a process he termed \"Schütteln\", translated as \"succussion\" – nullified this. A common explanation for his settling on this process is said to be that he found preparations subjected to agitation in transit, such as in saddle bags or in a carriage, were more \"potent\". Hahnemann had a saddle-maker construct a special wooden striking board covered in leather on one side and stuffed with horsehair. Insoluble solids, such as granite, diamond, and platinum, are diluted by grinding them with lactose (\"trituration\").\n\nThe process of dilution and succussion is termed \"dynamization\" or \"potentization\" by homeopaths. In industrial manufacture this may be done by machine.\n\nSerial dilution is achieved by taking an amount of the mixture and adding solvent, but the \"Korsakovian\" method may also be used, whereby the vessel in which the preparations are manufactured is emptied, refilled with solvent, and the volume of fluid adhering to the walls of the vessel is deemed sufficient for the new batch. The Korsakovian method is sometimes referred to as K on the label of a homeopathic preparation, e.g. 200CK is a 200C preparation made using the Korsakovian method.\n\nFluxion and radionics methods of preparation do not require succussion. There are differences of opinion on the number and force of strikes, and some practitioners dispute the need for succussion at all while others reject the Korsakovian and other non-classical preparations. There are no laboratory assays and the importance and techniques for succussion cannot be determined with any certainty from the literature.\n\nThree main logarithmic potency scales are in regular use in homeopathy. Hahnemann created the \"centesimal\" or \"C scale\", diluting a substance by a factor of 100 at each stage. The centesimal scale was favoured by Hahnemann for most of his life.\n\nA 2C dilution requires a substance to be diluted to one part in 100, and then some of that diluted solution diluted by a further factor of 100.\n\nThis works out to one part of the original substance in 10,000 parts of the solution. A 6C dilution repeats this process six times, ending up with the original substance diluted by a factor of 100=10 (one part in one trillion or 1/1,000,000,000,000). Higher dilutions follow the same pattern.\n\nIn homeopathy, a solution that is more dilute is described as having a higher \"potency\", and more dilute substances are considered by homeopaths to be stronger and deeper-acting. The end product is often so diluted as to be indistinguishable from the diluent (pure water, sugar or alcohol). There is also a decimal potency scale (notated as \"X\" or \"D\") in which the preparation is diluted by a factor of 10 at each stage.\n\nHahnemann advocated 30C dilutions for most purposes (that is, dilution by a factor of 10). Hahnemann regularly used potencies up to 300C but opined that \"there must be a limit to the matter, it cannot go on indefinitely\".\n\nIn Hahnemann's time, it was reasonable to assume the preparations could be diluted indefinitely, as the concept of the atom or molecule as the smallest possible unit of a chemical substance was just beginning to be recognized.\n\nThe greatest dilution reasonably likely to contain even one molecule of the original substance is 12C.\nCritics and advocates of homeopathy alike commonly attempt to illustrate the dilutions involved in homeopathy with analogies.\nHahnemann is reported to have joked that a suitable procedure to deal with an epidemic would be to empty a bottle of poison into Lake Geneva, if it could be succussed 60 times.\nAnother example given by a critic of homeopathy states that a 12C solution is equivalent to a \"pinch of salt in both the North and South Atlantic Oceans\", which is approximately correct.\nOne-third of a drop of some original substance diluted into all the water on earth would produce a preparation with a concentration of about 13C. A popular homeopathic treatment for the flu is a 200C dilution of duck liver, marketed under the name Oscillococcinum. As there are only about 10 atoms in the entire observable universe, a dilution of one molecule in the observable universe would be about 40C. Oscillococcinum would thus require 10 more universes to simply have one molecule in the final substance.\nThe high dilutions characteristically used are often considered to be the most controversial and implausible aspect of homeopathy.\n\nNot all homeopaths advocate high dilutions. Preparations at concentrations below 4X are considered an important part of homeopathic heritage. Many of the early homeopaths were originally doctors and generally used lower dilutions such as \"3X\" or \"6X\", rarely going beyond \"12X\".\nThe split between lower and higher dilutions followed ideological lines.\nThose favouring low dilutions stressed pathology and a stronger link to conventional medicine, while those favouring high dilutions emphasized vital force, miasms and a spiritual interpretation of disease.\nSome products with such relatively lower dilutions continue to be sold, but like their counterparts, they have not been conclusively demonstrated to have any effect beyond that of a placebo.\n\nA homeopathic \"proving\" is the method by which the profile of a homeopathic preparation is determined.\n\nAt first Hahnemann used undiluted doses for provings, but he later advocated provings with preparations at a 30C dilution, and most modern provings are carried out using ultra-dilute preparations in which it is highly unlikely that any of the original molecules remain. During the proving process, Hahnemann administered preparations to healthy volunteers, and the resulting symptoms were compiled by observers into a \"drug picture\".\n\nThe volunteers were observed for months at a time and made to keep extensive journals detailing all of their symptoms at specific times throughout the day. They were forbidden from consuming coffee, tea, spices, or wine for the duration of the experiment; playing chess was also prohibited because Hahnemann considered it to be \"too exciting\", though they were allowed to drink beer and encouraged to exercise in moderation.\n\nAfter the experiments were over, Hahnemann made the volunteers take an oath swearing that what they reported in their journals was the truth, at which time he would interrogate them extensively concerning their symptoms.\n\nProvings are claimed to have been important in the development of the clinical trial, due to their early use of simple control groups, systematic and quantitative procedures, and some of the first application of statistics in medicine. The lengthy records of self-experimentation by homeopaths have occasionally proven useful in the development of modern drugs: For example, evidence that nitroglycerin might be useful as a treatment for angina was discovered by looking through homeopathic provings, though homeopaths themselves never used it for that purpose at that time.\nThe first recorded provings were published by Hahnemann in his 1796 \"Essay on a New Principle\".\nHis \"Fragmenta de Viribus\" (1805) contained the results of 27 provings, and his 1810 \"Materia Medica Pura\" contained 65.\nFor James Tyler Kent's 1905 \"Lectures on Homoeopathic Materia Medica\", 217 preparations underwent provings and newer substances are continually added to contemporary versions.\n\nThough the proving process has superficial similarities with clinical trials, it is fundamentally different in that the process is subjective, not blinded, and modern provings are unlikely to use pharmacologically active levels of the substance under proving. As early as 1842, Holmes noted the provings were impossibly vague, and the purported effect was not repeatable among different subjects.\n\nHomeopaths generally begin with detailed examinations of their patients' histories, including questions regarding their physical, mental and emotional states, their life circumstances and any physical or emotional illnesses. The homeopath then attempts to translate this information into a complex formula of mental and physical symptoms, including likes, dislikes, innate predispositions and even body type.\n\nFrom these symptoms, the homeopath chooses how to treat the patient using \"materia medica\" and repertories. In classical homeopathy, the practitioner attempts to match a single preparation to the totality of symptoms (the \"simlilum\"), while \"clinical homeopathy\" involves combinations of preparations based on the various symptoms of an illness.\n\nHomeopathic pills are made from an inert substance (often sugars, typically lactose), upon which a drop of liquid homeopathic preparation is placed and allowed to evaporate.\n\nThe process of homeopathic dilution results in no objectively detectable active ingredient in most cases, but some preparations (e.g. calendula and arnica creams) do contain pharmacologically active doses. One product, Zicam Cold Remedy, which was marketed as an \"unapproved homeopathic\" product, contains two ingredients that are only \"slightly\" diluted: zinc acetate (2X = 1/100 dilution) and zinc gluconate (1X = 1/10 dilution), which means both are present in a biologically active concentration strong enough to have caused some people to lose their sense of smell, a condition termed anosmia. Zicam also listed several normal homeopathic potencies as \"inactive ingredients\", including \"galphimia glauca\", histamine dihydrochloride (homeopathic name, \"histaminum hydrochloricum\"), \"luffa operculata\", and sulfur.\n\nIsopathy is a therapy derived from homeopathy, invented by Johann Joseph Wilhelm Lux in the 1830s. Isopathy differs from homeopathy in general in that the preparations, known as \"nosodes\", are made up either from things that cause the disease or from products of the disease, such as pus. Many so-called \"homeopathic vaccines\" are a form of isopathy. Tautopathy is a form of isopathy where the preparations are composed of drugs or vaccines that a person has consumed in the past, in the belief that this can reverse lingering damage caused by the initial use. There is no convincing scientific evidence for isopathy as an effective method of treatment.\n\nFlower preparations can be produced by placing flowers in water and exposing them to sunlight. The most famous of these are the Bach flower remedies, which were developed by the physician and homeopath Edward Bach. Although the proponents of these preparations share homeopathy's vitalist world-view and the preparations are claimed to act through the same hypothetical \"vital force\" as homeopathy, the method of preparation is different. Bach flower preparations are manufactured in allegedly \"gentler\" ways such as placing flowers in bowls of sunlit water, and the preparations are not succussed. There is no convincing scientific or clinical evidence for flower preparations being effective.\n\nThe idea of using homeopathy as a treatment for other animals termed \"veterinary homeopathy\", dates back to the inception of homeopathy; Hahnemann himself wrote and spoke of the use of homeopathy in animals other than humans. The FDA has not approved homeopathic products as veterinary medicine in the U.S. In the UK, veterinary surgeons who use homeopathy may belong to the Faculty of Homeopathy and/or to the British Association of Homeopathic Veterinary Surgeons. Animals may be treated only by qualified veterinary surgeons in the UK and some other countries. Internationally, the body that supports and represents homeopathic veterinarians is the International Association for Veterinary Homeopathy.\n\nThe use of homeopathy in veterinary medicine is controversial; the little existing research on the subject is not of a high enough scientific standard to provide reliable data on efficacy. Given that homeopathy's effects in humans appear to be mainly due to the placebo effect and the counseling aspects of the consultation, it is unlikely that homeopathic treatments would be effective in animals. Other studies have also found that giving animals placebos can play active roles in influencing pet owners to believe in the effectiveness of the treatment when none exists. The British Veterinary Association's position statement on alternative medicines says that it \"cannot endorse\" homeopathy, and the Australian Veterinary Association includes it on its list of \"ineffective therapies\". A 2016 review of peer-reviewed articles from 1981 to 2014 by scientists from the University of Kassel, Germany, concluded that there was insufficient evidence to support the use of homeopathy in livestock as a way to prevent or treat infectious diseases.\n\nThe UK's Department for Environment, Food and Rural Affairs (Defra) has adopted a robust position against use of \"alternative\" pet preparations including homeopathy.\n\nPopular in the late nineteenth century, electrohomeopathy has been described as \"utter idiocy\".\n\nElectrohomeopathy is somewhat associated with an Spagyric medicine in that the disease is usually multi-organic in cause or effect and calls for holistic treatment that is both complex and natural.\n\nThe Allahabad High Court in Kanpur handed down a decree in 2012 which stated that electrohomeopathy was an unrecognized system of medicine which was quackery.\n\nThe use of homeopathy as a preventive for serious infectious diseases is especially controversial, in the context of ill-founded public alarm over the safety of vaccines stoked by the anti-vaccination movement. Promotion of homeopathic alternatives to vaccines has been characterized as dangerous, inappropriate and irresponsible. In December 2014, Australian homeopathy supplier Homeopathy Plus! were found to have acted deceptively in promoting homeopathic alternatives to vaccines.\n\nThe low concentration of homeopathic preparations, which often lack even a single molecule of the diluted substance, has been the basis of questions about the effects of the preparations since the 19th century. Modern advocates of homeopathy have proposed a concept of \"water memory\", according to which water \"remembers\" the substances mixed in it, and transmits the effect of those substances when consumed. This concept is inconsistent with the current understanding of matter, and water memory has never been demonstrated to have any detectable effect, biological or otherwise. Pharmacological research has found instead that stronger effects of an active ingredient come from higher, not lower doses.\n\nJames Randi and the groups have highlighted the lack of active ingredients in most homeopathic products by taking large 'overdoses'. None of the hundreds of demonstrators in the UK, Australia, New Zealand, Canada and the US were injured and \"no one was cured of anything, either\".\n\nOutside of the alternative medicine community, scientists have long considered homeopathy a sham or a pseudoscience, and the mainstream medical community regards it as quackery. There is an overall absence of sound statistical evidence of therapeutic efficacy, which is consistent with the lack of any biologically plausible pharmacological agent or mechanism.\n\nAbstract concepts within theoretical physics have been invoked to suggest explanations of how or why preparations might work, including quantum entanglement, quantum nonlocality, the theory of relativity and chaos theory. Contrariwise, quantum superposition has been invoked to explain why homeopathy does \"not\" work in double-blind trials. However, the explanations are offered by nonspecialists within the field, and often include speculations that are incorrect in their application of the concepts and not supported by actual experiments. Several of the key concepts of homeopathy conflict with fundamental concepts of physics and chemistry. The use of quantum entanglement to explain homeopathy's purported effects is \"patent nonsense\", as entanglement is a delicate state that rarely lasts longer than a fraction of a second. While entanglement may result in certain aspects of individual subatomic particles acquiring linked quantum states, this does not mean the particles will mirror or duplicate each other, nor cause health-improving transformations.\n\nThe proposed mechanisms for homeopathy are precluded from having any effect by the laws of physics and physical chemistry. The extreme dilutions used in homeopathic preparations usually leave not one molecule of the original substance in the final product.\n\nA number of speculative mechanisms have been advanced to counter this, the most widely discussed being water memory, though this is now considered erroneous since short-range order in water only persists for about 1 picosecond. No evidence of stable clusters of water molecules was found when homeopathic preparations were studied using nuclear magnetic resonance, and many other physical experiments in homeopathy have been found to be of low methodological quality, which precludes any meaningful conclusion. Existence of a pharmacological effect in the absence of any true active ingredient is inconsistent with the law of mass action and the observed dose-response relationships characteristic of therapeutic drugs (whereas placebo effects are non-specific and unrelated to pharmacological activity).\n\nHomeopaths contend that their methods produce a therapeutically active preparation, selectively including only the intended substance, though critics note that any water will have been in contact with millions of different substances throughout its history, and homeopaths have not been able to account for a reason why only the selected homeopathic substance would be a special case in their process. For comparison, ISO 3696:1987 defines a standard for water used in laboratory analysis; this allows for a contaminant level of ten parts per billion, 4C in homeopathic notation. This water may not be kept in glass as contaminants will leach out into the water.\n\nPractitioners of homeopathy hold that higher dilutions―described as being of higher \"potency\"―produce stronger medicinal effects. This idea is also inconsistent with observed dose-response relationships, where effects are dependent on the concentration of the active ingredient in the body. This dose-response relationship has been confirmed in myriad experiments on organisms as diverse as nematodes, rats, and humans. Some homeopaths contend that the phenomenon of hormesis may support the idea of dilution increasing potency, but the dose-response relationship outside the zone of hormesis declines with dilution as normal, and nonlinear pharmacological effects do not provide any credible support for homeopathy.\n\nPhysicist Robert L. Park, former executive director of the American Physical Society, is quoted as saying:\n\n\"since the least amount of a substance in a solution is one molecule, a 30C solution would have to have at least one molecule of the original substance dissolved in a minimum of 1,000,000,000,000,000,000,000,000,000,000,<wbr>000,000,000,000,000,000,000,000,000,000 [or 10] molecules of water. This would require a container more than 30,000,000,000 times the size of the Earth.\"\nPark is also quoted as saying that, \"to expect to get even one molecule of the 'medicinal' substance allegedly present in 30X pills, it would be necessary to take some two billion of them, which would total about a thousand tons of lactose plus whatever impurities the lactose contained\".\n\nThe laws of chemistry state that there is a limit to the dilution that can be made without losing the original substance altogether. This limit, which is related to Avogadro's number, is roughly equal to homeopathic dilutions of 12C or 24X (1 part in 10).\n\nScientific tests run by both the BBC's \"Horizon\" and ABC's \"20/20\" programmes were unable to differentiate homeopathic dilutions from water, even when using tests suggested by homeopaths themselves.\n\nIn May 2018, the German skeptical organization GWUP issued an invitation to individuals and groups to respond to its challenge \"to identify homeopathic preparations in high potency and to give a detailed description on how this can be achieved reproducibly.\" The first participant to correctly identify selected homeopathic preparations under an agreed-upon protocol will receive €50,000.\n\nNo individual homeopathic preparation has been unambiguously shown by research to be different from placebo. The methodological quality of the primary research was generally low, with such problems as weaknesses in study design and reporting, small sample size, and selection bias. Since better quality trials have become available, the evidence for efficacy of homeopathy preparations has diminished; the highest-quality trials indicate that the preparations themselves exert no intrinsic effect. A review conducted in 2010 of all the pertinent studies of \"best evidence\" produced by the Cochrane Collaboration concluded that \"the most reliable evidence – that produced by Cochrane reviews – fails to demonstrate that homeopathic medicines have effects beyond placebo.\"\n\nGovernment-level reviews have been conducted in recent years by Switzerland (2005), the United Kingdom (2009), Australia (2015) and the European Academies' Science Advisory Council (2017).\n\nThe Swiss \"programme for the evaluation of complementary medicine\" (PEK) resulted in the peer-reviewed Shang publication (see \"Systematic reviews and meta-analyses of efficacy\") and a controversial competing analysis by homeopaths and advocates led by Gudrun Bornhöft and Peter Matthiessen, which has misleadingly been presented as a Swiss government report by homeopathy proponents, a claim that has been repudiated by the Swiss Federal Office of Public Health. The Swiss Government terminated reimbursement, though it was subsequently reinstated after a political campaign and referendum for a further six-year trial period.\n\nThe United Kingdom's House of Commons Science and Technology Committee sought written evidence and submissions from concerned parties and, following a review of all submissions, concluded that there was no compelling evidence of effect other than placebo and recommended that the Medicines and Healthcare products Regulatory Agency (MHRA) should not allow homeopathic product labels to make medical claims, that homeopathic products should no longer be licensed by the MHRA, as they are not medicines, and that further clinical trials of homeopathy could not be justified. They recommended that funding of homeopathic hospitals should not continue, and NHS doctors should not refer patients to homeopaths. The Secretary of State for Health deferred to local NHS on funding homeopathy, in the name of patient choice. By February 2011 only one-third of primary care trusts still funded homeopathy. By 2012, no British universities offered homeopathy courses. In July 2017, as part of a plan to save £200m a year by preventing the \"misuse of scarce\" funding, the NHS announced that it would no longer provide homeopathic medicines. A legal appeal by the British Homeopathic Association against the decision was rejected in June 2018.\n\nThe Australian National Health and Medical Research Council completed a comprehensive review of the effectiveness of homeopathic preparations in 2015, in which it concluded that \"there were no health conditions for which there was reliable evidence that homeopathy was effective. No good-quality, well-designed studies with enough participants for a meaningful result reported either that homeopathy caused greater health improvements than placebo, or caused health improvements equal to those of another treatment.\"\n\nOn September 20, 2017, the European Academies' Science Advisory Council (EASAC) published its official analysis and conclusion on the use of homeopathic products, finding a lack of evidence that homeopathic products are effective, and raising concerns about quality control.\n\nThe fact that individual randomized controlled trials have given positive results is not in contradiction with an overall lack of statistical evidence of efficacy. A small proportion of randomized controlled trials inevitably provide false-positive outcomes due to the play of chance: a \"statistically significant\" positive outcome is commonly adjudicated when the probability of it being due to chance rather than a real effect is no more than 5%―a level at which about 1 in 20 tests can be expected to show a positive result in the absence of any therapeutic effect. Furthermore, trials of low methodological quality (i.e. ones that have been inappropriately designed, conducted or reported) are prone to give misleading results. In a systematic review of the methodological quality of randomized trials in three branches of alternative medicine, Linde \"et al.\" highlighted major weaknesses in the homeopathy sector, including poor randomization. A separate 2001 systematic review that assessed the quality of clinical trials of homeopathy found that such trials were generally of lower quality than trials of conventional medicine.\n\nA related issue is publication bias: researchers are more likely to submit trials that report a positive finding for publication, and journals prefer to publish positive results. Publication bias has been particularly marked in alternative medicine journals, where few of the published articles (just 5% during the year 2000) tend to report null results. Regarding the way in which homeopathy is represented in the medical literature, a systematic review found signs of bias in the publications of clinical trials (towards negative representation in mainstream medical journals, and \"vice versa\" in alternative medicine journals), but not in reviews.\n\nPositive results are much more likely to be false if the prior probability of the claim under test is low.\n\nBoth meta-analyses, which statistically combine the results of several randomized controlled trials, and other systematic reviews of the literature are essential tools to summarize evidence of therapeutic efficacy. Early systematic reviews and meta-analyses of trials evaluating the efficacy of homeopathic preparations in comparison with placebo more often tended to generate positive results, but appeared unconvincing overall. In particular, reports of three large meta-analyses warned readers that firm conclusions could not be reached, largely due to methodological flaws in the primary studies and the difficulty in controlling for publication bias. The positive finding of one of the most prominent of the early meta-analyses, published in \"The Lancet\" in 1997 by Linde et al., was later reframed by the same research team, who wrote:\n\nThe evidence of bias [in the primary studies] weakens the findings of our original meta-analysis. Since we completed our literature search in 1995, a considerable number of new homeopathy trials have been published. The fact that a number of the new high-quality trials ... have negative results, and a recent update of our review for the most \"original\" subtype of homeopathy (classical or individualized homeopathy), seem to confirm the finding that more rigorous trials have less-promising results. It seems, therefore, likely that our meta-analysis at least overestimated the effects of homeopathic treatments.\n\nSubsequent work by John Ioannidis and others has shown that for treatments with no prior plausibility, the chances of a positive result being a false positive are much higher, and that any result not consistent with the null hypothesis should be assumed to be a false positive.\n\nA systematic review of the available systematic reviews confirmed in 2002 that higher-quality trials tended to have less positive results, and found no convincing evidence that any homeopathic preparation exerts clinical effects different from placebo.\n\nIn 2005, \"The Lancet\" medical journal published a meta-analysis of 110 placebo-controlled homeopathy trials and 110 matched medical trials based upon the Swiss government's Programme for Evaluating Complementary Medicine, or PEK. The study concluded that its findings were \"compatible with the notion that the clinical effects of homeopathy are placebo effects\". This was accompanied by an editorial pronouncing \"The end of homoeopathy\", which was denounced by the homeopath Peter Fisher.\n\nOther meta-analyses include homeopathic treatments to reduce cancer therapy side-effects following radiotherapy and chemotherapy, allergic rhinitis, attention-deficit hyperactivity disorder and childhood diarrhoea, adenoid vegetation, asthma, upper respiratory tract infection in children, insomnia, fibromyalgia, psychiatric conditions and Cochrane Library systematic reviews of homeopathic treatments for asthma, dementia, attention-deficit hyperactivity disorder, induction of labour, upper respiratory tract infections in children, and irritable bowel syndrome. Other reviews covered osteoarthritis, migraines, postoperative ecchymosis and edema, delayed-onset muscle soreness, preventing postpartum haemorrhage, or eczema and other dermatological conditions.\n\nA 2017 systematic review and meta-analysis found that the most reliable evidence did not support the effectiveness of non-individualized homeopathy. The authors noted that \"the quality of the body of evidence is low.\"\n\nThe results of these reviews are generally negative or only weakly positive, and reviewers consistently report the poor quality of trials. The finding of Linde \"et. al.\" that more rigorous studies produce less positive results is supported in several and contradicted by none.\n\nSome clinical trials have tested individualized homeopathy, and there have been reviews of this, specifically. A 1998 review found 32 trials that met their inclusion criteria, 19 of which were placebo-controlled and provided enough data for meta-analysis. These 19 studies showed a pooled odds ratio of 1.17 to 2.23 in favour of individualized homeopathy over the placebo, but no difference was seen when the analysis was restricted to the methodologically best trials. The authors concluded that \"the results of the available randomized trials suggest that individualized homeopathy has an effect over placebo. The evidence, however, is not convincing because of methodological shortcomings and inconsistencies.\" Jay Shelton, author of a book on homeopathy, has stated that the claim assumes without evidence that classical, individualized homeopathy works better than nonclassical variations. A 2014 systematic review and meta-analysis found that individualized homeopathic remedies may be slightly more effective than placebos, though the authors noted that their findings were based on low- or unclear-quality evidence. The same research team later reported that taking into account model validity did not significantly affect this conclusion.\n\nHealth organizations such as the UK's National Health Service, the American Medical Association, the FASEB, and the National Health and Medical Research Council of Australia, have issued statements of their conclusion that there is \"no good-quality evidence that homeopathy is effective as a treatment for any health condition\". In 2009, World Health Organization official Mario Raviglione criticized the use of homeopathy to treat tuberculosis; similarly, another WHO spokesperson argued there was no evidence homeopathy would be an effective treatment for diarrhoea.\n\nThe American College of Medical Toxicology and the American Academy of Clinical Toxicology recommend that no one use homeopathic treatment for disease or as a preventive health measure. These organizations report that no evidence exists that homeopathic treatment is effective, but that there is evidence that using these treatments produces harm and can bring indirect health risks by delaying conventional treatment.\n\nScience offers a variety of explanations for how homeopathy may appear to cure diseases or alleviate symptoms even though the preparations themselves are inert:\n\nWhile some articles have suggested that homeopathic solutions of high dilution can have statistically significant effects on organic processes including the growth of grain, histamine release by leukocytes, and enzyme reactions, such evidence is disputed since attempts to replicate them have failed. A 2007 systematic review of high-dilution experiments found that none of the experiments with positive results could be reproduced by all investigators.\n\nIn 1987, French immunologist Jacques Benveniste submitted a paper to the journal \"Nature\" while working at INSERM. The paper purported to have discovered that basophils, a type of white blood cell, released histamine when exposed to a homeopathic dilution of anti-immunoglobulin E antibody. The journal editors, sceptical of the results, requested that the study be replicated in a separate laboratory. Upon replication in four separate laboratories the study was published. Still sceptical of the findings, \"Nature\" assembled an independent investigative team to determine the accuracy of the research, consisting of \"Nature\" editor and physicist Sir John Maddox, American scientific fraud investigator and chemist Walter Stewart, and sceptic James Randi. After investigating the findings and methodology of the experiment, the team found that the experiments were \"statistically ill-controlled\", \"interpretation has been clouded by the exclusion of measurements in conflict with the claim\", and concluded, \"We believe that experimental data have been uncritically assessed and their imperfections inadequately reported.\" James Randi stated that he doubted that there had been any conscious fraud, but that the researchers had allowed \"wishful thinking\" to influence their interpretation of the data.\n\nIn 2001 and 2004, Madeleine Ennis published a number of studies that reported that homeopathic dilutions of histamine exerted an effect on the activity of basophils. In response to the first of these studies, \"Horizon\" aired a programme in which British scientists attempted to replicate Ennis' results; they were unable to do so.\n\nThe provision of homeopathic preparations has been described as unethical. Michael Baum, Professor Emeritus of Surgery and visiting Professor of Medical Humanities at University College London (UCL), has described homoeopathy as a \"cruel deception\".\n\nEdzard Ernst, the first \"Professor of Complementary Medicine\" in the United Kingdom and a former homeopathic practitioner, has expressed his concerns about pharmacists who violate their ethical code by failing to provide customers with \"necessary and relevant information\" about the true nature of the homeopathic products they advertise and sell:\n\nPatients who choose to use homeopathy rather than evidence-based medicine risk missing timely diagnosis and effective treatment of serious conditions such as cancer.\n\nIn 2013 the UK Advertising Standards Authority concluded that the Society of Homeopaths were targeting vulnerable ill people and discouraging the use of essential medical treatment while making misleading claims of efficacy for homeopathic products.\n\nIn 2015 the Federal Court of Australia imposed penalties on a homeopathic company, Homeopathy Plus! Pty Ltd and its director, for making false or misleading statements about the efficacy of the whooping cough vaccine and homeopathic remedies as an alternative to the whooping cough vaccine, in breach of the Australian Consumer Law.\n\nSome homeopathic preparations involve poisons such as Belladonna, arsenic, and poison ivy, which are highly diluted in the homeopathic preparation. In rare cases, the original ingredients are present at detectable levels. This may be due to improper preparation or intentional low dilution. Serious adverse effects such as seizures and death have been reported or associated with some homeopathic preparations.\n\nOn September 30, 2016 the FDA issued a safety alert to consumers warning against the use of homeopathic teething gels and tablets following reports of adverse events after their use. The agency recommended that parents discard these products and \"seek advice from their health care professional for safe alternatives\" to homeopathy for teething. The pharmacy CVS announced, also on September 30, that it was voluntarily withdrawing the products from sale and on October 11 Hyland's (the manufacturer) announced that it was discontinuing their teething medicine in the United States though the products remain on sale in Canada. On October 12, Buzzfeed reported that the regulator had \"examined more than 400 reports of seizures, fever and vomiting, as well as 10 deaths\" over a six-year period. The investigation (including analyses of the products) is still ongoing and the FDA does not know yet if the deaths and illnesses were caused by the products. However a previous FDA investigation in 2010, following adverse effects reported then, found that these same products were improperly diluted and contained \"unsafe levels of belladonna, also known as deadly nightshade\" and that the reports of serious adverse events in children using this product were \"consistent with belladonna toxicity\".\n\nInstances of arsenic poisoning have occurred after use of arsenic-containing homeopathic preparations. Zicam Cold remedy Nasal Gel, which contains 2X (1:100) zinc gluconate, reportedly caused a small percentage of users to lose their sense of smell; 340 cases were settled out of court in 2006 for . In 2009, the FDA advised consumers to stop using three discontinued cold remedy Zicam products because it could cause permanent damage to users' sense of smell. Zicam was launched without a New Drug Application (NDA) under a provision in the FDA's Compliance Policy Guide called \"Conditions under which homeopathic drugs may be marketed\" (CPG 7132.15), but the FDA warned Matrixx Initiatives, its manufacturer, via a Warning Letter that this policy does not apply when there is a health risk to consumers.\n\nA 2000 review by homeopaths reported that homeopathic preparations are \"unlikely to provoke severe adverse reactions\". In 2012, a systematic review evaluating evidence of homeopathy's possible adverse effects concluded that \"homeopathy has the potential to harm patients and consumers in both direct and indirect ways\". One of the reviewers, Edzard Ernst, supplemented the article on his blog, writing: \"I have said it often and I say it again: if used as an alternative to an effective cure, even the most 'harmless' treatment can become life-threatening.\" A 2016 systematic review and meta-analysis found that, in homeopathic clinical trials, adverse effects were reported among the patients who received homeopathy about as often as they were reported among patients who received placebo or conventional medicine.\n\nThe lack of convincing scientific evidence supporting its efficacy and its use of preparations without active ingredients have led to characterizations as pseudoscience and quackery, or, in the words of a 1998 medical review, \"placebo therapy at best and quackery at worst\". The Russian Academy of Sciences considers homeopathy a \"dangerous 'pseudoscience' that does not work\", and \"urges people to treat homeopathy 'on a par with magic. The Chief Medical Officer for England, Dame Sally Davies, has stated that homeopathic preparations are \"rubbish\" and do not serve as anything more than placebos. Jack Killen, acting deputy director of the National Center for Complementary and Alternative Medicine, says homeopathy \"goes beyond current understanding of chemistry and physics\". He adds: \"There is, to my knowledge, no condition for which homeopathy has been proven to be an effective treatment.\" Ben Goldacre says that homeopaths who misrepresent scientific evidence to a scientifically illiterate public, have \"... walled themselves off from academic medicine, and critique has been all too often met with avoidance rather than argument\". Homeopaths often prefer to ignore meta-analyses in favour of cherry picked positive results, such as by promoting a particular observational study (one which Goldacre describes as \"little more than a customer-satisfaction survey\") as if it were more informative than a series of randomized controlled trials.\n\nReferring specifically to homeopathy, the British House of Commons Science and Technology Committee has stated:\n\nThe National Center for Complementary and Alternative Medicine of the United States' National Institutes of Health states:\n\nBen Goldacre noted that in the early days of homeopathy, when medicine was dogmatic and frequently worse than doing nothing, homeopathy at least failed to make matters worse:\n\nOn clinical grounds, patients who choose to use homeopathy in preference to normal medicine risk missing timely diagnosis and effective treatment, thereby worsening the outcomes of serious conditions. Critics of homeopathy have cited individual cases of patients of homeopathy failing to receive proper treatment for diseases that could have been easily diagnosed and managed with conventional medicine and who have died as a result, and the \"marketing practice\" of criticizing and downplaying the effectiveness of mainstream medicine. Homeopaths claim that use of conventional medicines will \"push the disease deeper\" and cause more serious conditions, a process referred to as \"suppression\". Some homeopaths (particularly those who are non-physicians) advise their patients against immunization. Some homeopaths suggest that vaccines be replaced with homeopathic \"nosodes\", created from biological materials such as pus, diseased tissue, bacilli from sputum or (in the case of \"bowel nosodes\") faeces. While Hahnemann was opposed to such preparations, modern homeopaths often use them although there is no evidence to indicate they have any beneficial effects. Cases of homeopaths advising against the use of anti-malarial drugs have been identified. This puts visitors to the tropics who take this advice in severe danger, since homeopathic preparations are completely ineffective against the malaria parasite. Also, in one case in 2004, a homeopath instructed one of her patients to stop taking conventional medication for a heart condition, advising her on June 22, 2004 to \"Stop ALL medications including homeopathic\", advising her on or around August 20 that she no longer needed to take her heart medication, and adding on August 23, \"She just cannot take ANY drugs – I have suggested some homeopathic remedies ... I feel confident that if she follows the advice she will regain her health.\" The patient was admitted to hospital the next day, and died eight days later, the final diagnosis being \"acute heart failure due to treatment discontinuation\".\n\nIn 1978, Anthony Campbell, then a consultant physician at the Royal London Homeopathic Hospital, criticized statements by George Vithoulkas claiming that syphilis, when treated with antibiotics, would develop into secondary and tertiary syphilis with involvement of the central nervous system, saying that \"The unfortunate layman might well be misled by Vithoulkas' rhetoric into refusing orthodox treatment\".\nVithoulkas' claims echo the idea that treating a disease with external medication used to treat the symptoms would only drive it deeper into the body and conflict with scientific studies, which indicate that penicillin treatment produces a complete cure of syphilis in more than 90% of cases.\n\nA 2006 review by W. Steven Pray of the College of Pharmacy at Southwestern Oklahoma State University recommends that pharmacy colleges include a required course in unproven medications and therapies, that ethical dilemmas inherent in recommending products lacking proven safety and efficacy data be discussed, and that students should be taught where unproven systems such as homeopathy depart from evidence-based medicine.\n\nIn an article entitled \"Should We Maintain an Open Mind about Homeopathy?\" published in the \"American Journal of Medicine\", Michael Baum and Edzard Ernstwriting to other physicianswrote that \"Homeopathy is among the worst examples of faith-based medicine... These axioms [of homeopathy] are not only out of line with scientific facts but also directly opposed to them. If homeopathy is correct, much of physics, chemistry, and pharmacology must be incorrect...\".\n\nIn 2013, Mark Walport, the UK Government Chief Scientific Adviser and head of the Government Office for Science, had this to say: \"My view scientifically is absolutely clear: homoeopathy is nonsense, it is non-science. My advice to ministers is clear: that there is no science in homoeopathy. The most it can have is a placebo effect – it is then a political decision whether they spend money on it or not.\" His predecessor, John Beddington, referring to his views on homeopathy being \"fundamentally ignored\" by the Government, said: \"The only one [view being ignored] I could think of was homoeopathy, which is mad. It has no underpinning of scientific basis. In fact, all the science points to the fact that it is not at all sensible. The clear evidence is saying this is wrong, but homoeopathy is still used on the NHS.\"\n\nHomeopathy is fairly common in some countries while being uncommon in others; is highly regulated in some countries and mostly unregulated in others. It is practised worldwide and professional qualifications and licences are needed in most countries. In some countries, there are no specific legal regulations concerning the use of homeopathy, while in others, licences or degrees in conventional medicine from accredited universities are required. In Germany, to become a homeopathic physician, one must attend a three-year training programme, while France, Austria and Denmark mandate licences to diagnose any illness or dispense of any product whose purpose is to treat any illness.\n\nSome homeopathic treatment is covered by the public health service of several European countries, including France, Scotland, Luxembourg and England (though the latter will cease in February 2019). In other countries, such as Belgium, homeopathy is not covered. In Austria, the public health service requires scientific proof of effectiveness in order to reimburse medical treatments and homeopathy is listed as not reimbursable, but exceptions can be made; private health insurance policies sometimes include homeopathic treatment. The Swiss government, after a 5-year trial, withdrew coverage of homeopathy and four other complementary treatments in 2005, stating that they did not meet efficacy and cost-effectiveness criteria, but following a referendum in 2009 the five therapies have been reinstated for a further 6-year trial period from 2012.\nThe Indian government recognizes homeopathy as one of its national systems of medicine; it has established AYUSH or the Department of Ayurveda, Yoga and Naturopathy, Unani, Siddha and Homoeopathy under the Ministry of Health & Family Welfare. The south Indian state of Kerala also has a cabinet-level AYUSH department. The Central Council of Homoeopathy was established in 1973 to monitor higher education in homeopathy, and National Institute of Homoeopathy in 1975. A minimum of a recognized diploma in homeopathy and registration on a state register or the Central Register of Homoeopathy is required to practise homeopathy in India.\n\nOn September 28, 2016 the UK's Committee of Advertising Practice (CAP) Compliance team wrote to homeopaths in the UK to \"remind them of the rules that govern what they can and can't say in their marketing materials\". The letter highlights that \"homeopaths may not currently make either direct or implied claims to treat medical conditions\" and asks them to review their marketing communications \"including websites and social media pages\" to ensure compliance by November 3, 2016. The letter also includes information on sanctions in the event of non-compliance including, ultimately, \"referral by the ASA to Trading Standards under the Consumer Protection from Unfair Trading Regulations 2008\".\n\nIn February 2017, Russian Academy of Sciences declared homeopathy to be \"dangerous pseudoscience\" and \"on a par with magic\".\n\nIn the April 1997 edition of FDA Consumer, William T. Jarvis, the President of the National Council Against Health Fraud, said \"Homeopathy is a fraud perpetrated on the public with the government's blessing, thanks to the abuse of political power of Sen. Royal S. Copeland [chief sponsor of the 1938 Food, Drug, and Cosmetic Act].\"\n\nMock \"overdosing\" on homeopathic preparations by individuals or groups in \"mass suicides\" have become more popular since James Randi began taking entire bottles of homeopathic sleeping pills before giving lectures. In 2010 The Merseyside Skeptics Society from the United Kingdom launched the , encouraging groups to publicly overdose as groups. In 2011 the 10:23 campaign expanded and saw sixty-nine groups participate; fifty-four submitted videos. In April 2012, at the Berkeley SkeptiCal conference, over 100 people participated in a mass overdose, taking \"coffea cruda\", which is supposed to treat sleeplessness.\n\nIn 2011, the non-profit, educational organizations Center for Inquiry (CFI) and the associated Committee for Skeptical Inquiry (CSI) have petitioned the U.S. Food and Drug Administration (FDA) to initiate \"rulemaking that would require all over-the-counter homeopathic drugs to meet the same standards of effectiveness as non-homeopathic drugs\" and \"to place warning labels on homeopathic drugs until such time as they are shown to be effective\". In a separate petition, CFI and CSI request FDA to issue warning letters to Boiron, maker of Oscillococcinum, regarding their marketing tactic and criticize Boiron for misleading labelling and advertising of Oscillococcinum. In 2015, CFI filed comments urging the Federal Trade Commission to end the false advertising practice of homeopathy. On November 15, 2016, FTC declared that homeopathic products cannot include claims of effectiveness without \"competent and reliable scientific evidence\". If no such evidence exists, they must state this fact clearly on their labeling, and state that the product's claims are based only on 18th-century theories that have been discarded by modern science. Failure to do so will be considered a violation of the FTC Act.\nCFI in Canada is calling for persons that feel they were harmed by homeopathic products to contact them.\n\nIn August 2011, a class action lawsuit was filed against Boiron on behalf of \"all California residents who purchased Oscillo at any time within the past four years\". The lawsuit charged that it \"is nothing more than a sugar pill\", \"despite falsely advertising that it contains an active ingredient known to treat flu symptoms\". In March 2012, Boiron agreed to spend up to $12 million to settle the claims of falsely advertising the benefits of its homeopathic preparations.\n\nIn July 2012, CBC News reporter Erica Johnson for \"Marketplace\" conducted an investigation on the homeopathy industry in Canada; her findings were that it is \"based on flawed science and some loopy thinking\". Center for Inquiry (CFI) Vancouver skeptics participated in a mass overdose outside an emergency room in Vancouver, B.C., taking entire bottles of \"medications\" that should have made them sleepy, nauseous or dead; after 45 minutes of observation no ill effects were felt. Johnson asked homeopaths and company representatives about cures for cancer and vaccine claims. All reported positive results but none could offer any science backing up their statements, only that \"it works\". Johnson was unable to find any evidence that homeopathic preparations contain any active ingredient. Analysis performed at the University of Toronto's chemistry department found that the active ingredient is so small \"it is equivalent to 5 billion times less than the amount of aspirin ... in a single pellet\". Belladonna and ipecac \"would be indistinguishable from each other in a blind test\".\n\nHomeopathic services offered at Bristol Homeopathic Hospital in the UK ceased in October 2015, partly in response to increased public awareness as a result of the and a campaign led by the Good Thinking Society, University Hospitals Bristol confirmed that it would cease to offer homeopathic therapies from October 2015, at which point homeopathic therapies would no longer be included in the contract. Homeopathic services in the Bristol area were relocated to \"a new independent social enterprise\" at which Bristol Clinical Commissioning Group revealed \"there are currently no (NHS) contracts for homeopathy in place.\" Following a threat of legal action by the Good Thinking Society campaign group, the British government has stated that the Department of Health will hold a consultation in 2016 regarding whether homeopathic treatments should be added to the NHS treatments blacklist (officially, Schedule 1 of the National Health Service (General Medical Services Contracts) (Prescription of Drugs etc.) Regulations 2004), that specifies a blacklist of medicines not to be prescribed under the NHS.\n\nIn March 2016, the University of Barcelona cancelled its master's degree in Homeopathy citing \"lack of scientific basis\", after advice from the Spanish Ministry of Health stated that \"Homeopathy has not definitely proved its efficacy under any indication or concrete clinical situation\". Shortly afterwards, in April 2016, the University of Valencia announced the elimination of its Masters in Homeopathy for 2017.\n\nIn June 2016, blogger and sceptic Jithin Mohandas launched a petition through Change.org asking the government of Kerala, India, to stop admitting students to homeopathy medical colleges. Mohandas said that government approval of these colleges makes them appear legitimate, leading thousands of talented students to join them and end up with invalid degrees. The petition asks that homeopathy colleges be converted to regular medical colleges and that people with homeopathy degrees be provided with training in scientific medicine.\n\nIn Germany, physician and critic of alternative medicine Irmgard Oepen was a relentless critic of homeopathy.\n\nOn April 20–21, 2015, the FDA held a hearing on homeopathic product regulation. Invitees representing the scientific and medical community, and various pro-homeopathy stakeholders, gave testimonials on homeopathic products and the regulatory role played by the FDA.\nMichael de Dora, a representative from the Center for Inquiry (CFI), on behalf of the organization and dozens of doctors and scientists associated with CFI and the Committee for Skeptical Inquiry (CSI) gave a testimonial which summarized the basis of the organization's objection to homeopathic products, the harm that is done to the general public and proposed regulatory actions:\n\nThe CFI testimonial stated that the principle of homeopathy is at complete odds with the basic principles of modern biology, chemistry and physics and that decades of scientific examination of homeopathic products shows that there is no evidence that it is effective in treating illnesses other than acting as a placebo. Further, it noted a 2012 report by the American Association of Poison Control Centers which listed 10,311 reported cases of poison exposure related to homeopathic agents, among which 8,788 cases were attributed to young children five years of age or younger, as well as examples of harm – including deaths – caused to patients who relied on homeopathics instead of proven medical treatment.\n\nThe CFI urged the FDA to announce and implement strict guidelines that \"require all homeopathic products meet the same standards as non-homeopathic drugs\", arguing that the consumers can only have true freedom of choice (an often used argument from the homeopathy proponents) if they are fully informed of the choices. CFI proposed that the FDA take these three steps:\n\nIn December 2017, the FDA announced it would strengthen regulation of homeopathic products focusing on \"situations where homeopathic treatments are being marketed for serious diseases or conditions but have not been shown to offer clinical benefits\" and where \"products labeled as homeopathic contain potentially harmful ingredients or do not meet current good manufacturing practices.\"\n\nIn March 2015, the National Health and Medical Research Council of Australia issued the following conclusions and recommendations:\n\nIn November 2016, The United States FTC issued an \"Enforcement Policy Statement Regarding Marketing Claims for Over-the-Counter Homeopathic Drugs\" which specified that the FTC will hold efficacy and safety claims for OTC homeopathic drugs to the same standard as other products making similar claims. A November 15, 2016, FTC press release summarized the policy as follows:\n\nIn conjunction with the 2016 FTC Enforcement Policy Statement, the FTC also released its \"Homeopathic Medicine & Advertising Workshop Report\", which summarizes the panel presentations and related public comments in addition to describing consumer research commissioned by the FTC. The report concluded:\n\n\n"}
{"id": "51754640", "url": "https://en.wikipedia.org/wiki?curid=51754640", "title": "Hybrid Shipping Container", "text": "Hybrid Shipping Container\n\nA hybrid shipping container is a shipping system that uses the energy of phase-change material (PCM) in combination with the ability to recharge without removing the media. This ability is known as \"cold-energy battery\".\n\nCurrently, this technology is only being used in a limited number of shipping containers.\n\n1. SkyCell 770C\n2. SkyCell 1500C\n3. SkyCell 770CRT\n4. SkyCell 1500CRT\n\nA Cold-energy battery works by storing energy to a given temperature and using its thermal mass to maintain this temperature. It can be recharged by being placed in a temperature range applicable to its phase change window.\n"}
{"id": "636129", "url": "https://en.wikipedia.org/wiki?curid=636129", "title": "Ideation (creative process)", "text": "Ideation (creative process)\n\nIdeation is the creative process of generating, developing, and communicating new ideas, where an idea is understood as a basic element of thought that can be either visual, concrete, or abstract. Ideation comprises all stages of a thought cycle, from innovation, to development, to actualization. Ideation can be conducted by individuals, organizations, or crowds. As such, it is an essential part of the design process, both in education and practice. \n\nThe book \"Ideation: The Birth and Death of Ideas\" (Graham and Bachmann, 2004) proposes the following methods of innovation:\n\n\nThis list of methods is by no means comprehensive or necessarily accurate. Graham and Bachmann's examples of revolutionary ideas might better be described as evolutionary; both Marx and Copernicus having built upon pre-existing concepts within new or different contexts. Similarly, the description provided for artistic innovation represents one perspective.\n\nMore-nuanced understandings, such as that expressed by Stephen Nachmanovitch in \"Free Play: Improvisation in Life and Art\", recognize the generative force technical and perceptual limitations provide within specific arts practices. In painting, for example, technical limitations such as the frame, the surface and the palette, along with perceptual constraints like figure/ground relationships and perspective, provide creative frameworks for the painter. Similarly in music, harmonic scales, meter and time signatures work in tandem with choices of instrumentation and expression to both produce specific results and improvise novel outcomes.\n\nThe T.O.T.E. model, an iterative problem solving strategy based on feedback loops, provides an alternative approach to considering the process of ideation. Ideation may also be considered as a facet of other generative systems, such as Emergence.\n\nThe word \"ideation\" has come under informal criticism as being a term of meaningless jargon, as well as being inappropriately similar to the psychiatric term for suicidal ideation.\n\n\n"}
{"id": "13467271", "url": "https://en.wikipedia.org/wiki?curid=13467271", "title": "Inherence", "text": "Inherence\n\nInherence refers to Empedocles' idea that the qualities of matter come from the relative proportions of each of the four elements entering into a thing. The idea was further developed by Plato and Aristotle.\n\nThat Plato accepted (or at least did not reject) Empedocles' claim can be seen in the \"Timaeus\". However, Plato also applied it to cover the presence of form in matter. The form is an active principle. Matter, on the other hand is passive, being a mere possibility that the forms bring to life.\n\nAristotle clearly accepted Empedocles' claim, but he rejected Plato's idea of the forms. According to Aristotle, the accidents of a substance are incorporeal beings which are present in it.\n\nA closely related term is participation. If an attribute \"inheres\" in a subject, then the subject is said to \"participate\" in the attribute. For example, if the attribute \"in Athens\" inheres in Socrates, then Socrates is said to participate in the attribute, \"in Athens.\"\n\n"}
{"id": "67752", "url": "https://en.wikipedia.org/wiki?curid=67752", "title": "Insulin potentiation therapy", "text": "Insulin potentiation therapy\n\nInsulin potentiation therapy (IPT) is an unproven alternative cancer treatment using insulin as an adjunct to low-dose chemotherapy.\n\nAccording to Quackwatch, \"Insulin Potentiation Therapy (IPT) is one of several unproven, dangerous treatments that is promoted by a small group of practitioners without trustworthy evidence that it works.\"\n\nIt was developed by Donato Perez Garcia, MD in 1930. Originally, Garcia targeted syphilis, and later tried the treatment for chronic degenerative diseases and some types of cancer.\n\nGenerally, a dose of insulin is injected into a vein, followed by a low dose of chemotherapy drugs when the insulin has been absorbed. The chemotherapy dose is usually 10% to 25% of the proven dose. Then sugar water is injected to stop the hypoglycemia (low blood sugar) caused by the insulin injection.\n\nIPT has not been proven to work. Long-term outcomes, such as survival, have never been published. Four individual case studies, one small, uncontrolled clinical trial and one small prospective, randomized controlled trial have shown temporary reductions in the size of tumors for some patients.\n\nThe immediate risk is hypoglycemia.\n\nThe use of lower than normal doses of chemotherapy can cause drug resistance, which could make future treatment at standard, proven doses ineffective. For some cancers, especially breast and colon cancers, insulin may promote tumor growth.\n\nTwo main ideas about how it might work have been proposed. The first idea is that insulin makes cells more permeable, so that the chemotherapy drugs are absorbed faster into cells. The other idea is that insulin might cause the cells to start dividing, which makes them more susceptible to destruction of many cytotoxic chemotherapy drugs.\n\nCosts run up to US $2,000 per treatment session. Multiple sessions are normal. Patients often pay the full cost out of pocket, because it is an unproven therapy that is not covered by health insurance.\n\n"}
{"id": "27773710", "url": "https://en.wikipedia.org/wiki?curid=27773710", "title": "Kozyrev mirror", "text": "Kozyrev mirror\n\nA Kozyrev mirror, in Russian esoteric literature from 1990s, is a device made from aluminum (sometimes from glass, or reflecting mirror-like material) spiral shape surfaces, which, according to a non-proved hypothesis, are able to focus different types of radiation including that coming from biological objects. They are named after the famous astronomer Nikolai Aleksandrovich Kozyrev, though they were neither invented nor described by him.\n\nKozyrev mirrors were used in experiments related to extrasensory perception (ESP), conducted in the Institute of Experimental Medicine of Siberia, division of the Russian Academy of Sciences. Humans, allocated into the cylindrical spirals (usually 1.5 rotations clockwise, made of polished aluminum) allegedly experienced anomalous psycho-physical sensations, which had been recorded in the minutes of the research experiments.\n\nKozyrev mirrors were shown in a documentary on the Russian state TV channel and articles about them were published in tabloid newspapers in Russia and Ukraine but not in scientific journals.\n\nThere is a claim that during one of early experiments in the arctic village of Dixon, scientists placed an ancient symbol of Trinity into a mirror installation. Almost immediately there formed a field of force around the setup. The experiment was led by Vlail Kaznacheev MD, PhD, Academic Member of the Russian Academy of Medical Science .\n\nA 1998 Russian patent, RU2122446, \"Device for the correction of man's psychosomatic diseases\", relates to Kozyrev mirrors.\n\nIn 2014 a Czech Republic group attempted, but failed, to crowdfund 17,000-20,000 Euros for development of Kozyrev mirrors.\n"}
{"id": "52141066", "url": "https://en.wikipedia.org/wiki?curid=52141066", "title": "Leiden Bio Science Park", "text": "Leiden Bio Science Park\n\nThe Leiden Bio Science park (LBSP) ranks in the top five of the most successful science parks in Europe. It is part of Leiden and Oegstgeest and focuses on companies and Universities in the Biotechnology sector.\n\nThe park comprises 110 hectares (272 acres) with over 60 companies and knowledge-based institutions. The park is located mostly in Leiden and lies between Wassenaarseweg on the north and the Plesmanlaan on the south.\n\nThe park focuses mostly on the use of biotechnology for medical and biopharmaceutical applications.\n\nThe LBSP was founded in 1984 in the Leeuwenhoek area west of Leiden Central Station, between the Faculty of Science of the Leiden University and the former Academic Medical Hospital, known as the LUMC. The municipality decided that this area should primarily be focused on biotechnology.\n\nIn 2005 the foundation\" Leiden Life meets Science\" was founded by the Leiden University, the municipality, the LUMC, TNO, Naturalis, Chamber of Commerce, the province South Holland, University of Applied Sciences, and the ROC Leiden, with the purpose of growth the park in size and quality.\n\n\n\n"}
{"id": "634124", "url": "https://en.wikipedia.org/wiki?curid=634124", "title": "Leuchter report", "text": "Leuchter report\n\nThe Leuchter report is a pseudoscientific document authored by American execution technician Fred A. Leuchter, who was commissioned by Ernst Zündel to defend him at his trial in Canada for distributing Holocaust denial material. Leuchter compiled the report in 1988 with the intention of investigating the feasibility of mass homicidal gassings at Nazi extermination camps, specifically at Auschwitz. He travelled to the camp, collected multiple pieces of brick from the remains of the crematoria and gas chambers (without the camp's permission), brought them back to the United States, and submitted them for chemical analysis. At the trial, Leuchter was called upon to defend the report in the capacity of an expert witness; however during the trial, the court ruled that he had neither the qualifications nor experience to act as such.\n\nLeuchter cited the absence of Prussian blue in the homicidal gas chambers to support his view that they could not have been used to gas people. However, residual iron-based cyanide compounds are not a categorical consequence of cyanide exposure. By not discriminating against that, Leuchter introduced an unreliable factor into his experiment, and his findings were seriously flawed as a result. In contrast, tests conducted by Polish forensic scientists (who discriminated against iron-based compounds) confirmed the presence of cyanide in the locations, in accordance with where and how it was used in the Holocaust. In addition, the report was criticized as Leuchter had overlooked critical evidence, such as documents in the SS architectural office which recorded the mechanical operation of the gas chambers and others which verified the rate at which the Nazis could burn the bodies of those gassed.\n\nIn 1985, Ernst Zündel, a German pamphleteer and publisher living in Canada, was put on trial for publishing Richard Verrall's Holocaust denial pamphlet \"Did Six Million Really Die?\", which was deemed to violate Canadian laws against distributing false news. Zündel was found guilty, but the conviction was overturned on appeal. This led to a second prosecution.\n\nZündel and his lawyers were joined by Robert Faurisson, a French academic of literature and Holocaust denier, who came to Toronto to advise the defence, having previously testified as expert witness at the first trial. He was joined by David Irving, an English writer and Holocaust denier, who was to assist the defence and testify on Zündel's behalf. Faurisson claimed that it was technically and physically impossible for the gas chambers at Auschwitz to have functioned as extermination facilities, based on comparisons with American execution gas chambers; he therefore suggested getting an American prison warden who had participated in executions by gas to testify. Irving and Faurisson therefore invited Bill Armontrout, warden of the Missouri State Penitentiary, who agreed to testify and suggested they contact Fred A. Leuchter, a Bostonian execution equipment designer. Faurisson reported that Leuchter initially accepted the mainstream account of the Holocaust, but after two days of discussion with him, he stated that Leuchter was convinced that homicidal gassings never occurred. After having met Zündel in Toronto and agreeing to serve as an expert witness for his defence, Leuchter travelled with them to spend a week in Poland. He was accompanied by his draftsman, a cinematographer supplied by Zündel, a translator fluent in German and Polish, and his wife. Although Zündel and Faurisson did not accompany them, Leuchter said that they were with them \"every step of the way\" in spirit.\n\nAfter arriving in Poland the group spent three days at the former Auschwitz concentration camp site, and another at the former Majdanek concentration camp. At these, they filmed Leuchter illicitly collecting what he regarded to be forensic quality samples of materialsfrom the wreckage of the former gas extermination facilities, while his wife and the translator acted as lookouts. Drawings of where the samples were taken from, the film footage of their physical collection and Leuchter's notebook detailing the work were surrendered to the trial court as evidence. Leuchter claimed that his conclusions were based on his expert knowledge of gas chamber operation, his visual inspection of what remained of the structures at Auschwitz, and original drawings and blueprints of some of the facilities. He said that the blueprints had been given to him by Auschwitz Museum officials.\n\nThe compiled report was published in Canada as \"The Leuchter Report: An Engineering Report on the Alleged Execution Gas Chambers at Auschwitz, Birkenau, and Majdanek, Poland\", by Zündel's Samisdat Publications, and in England as \"Auschwitz: The End of the Line. The Leuchter Report: The First Forensic Examination of Auschwitz\" by Focal Point Publications, David Irving's publishing house. However, the court accepted the report only as evidentiary display and not as direct evidence; Leuchter was therefore required to explain it and testify to its veracity in the trial.\n\nBefore Leuchter could do this, he was examined by the court. He admitted that he was not a toxicologist and dismissed the need for having a degree in engineering:\n\nLeuchter admitted under oath that he only had a bachelor of arts degree and implicitly suggested that an engineering degree was unavailable to him by saying that his college did not offer an engineering degree during his studies. Boston University actually offered three different kinds of such qualification when he was a student there. When asked by the court if the B.A. he obtained was in a field that entitled him to operate as an engineer, he confirmed that this was so, even though his degree was in history. Similarly, Leuchter claimed that he obtained most of his research material on the camps (including original crematoria blueprints) from the Auschwitz and Majdanek camps' archives, and testified that these documents had a far more important role in shaping his conclusions than the physical samples he collected, yet after the trial the director of the Auschwitz museum denied that Leuchter had received any plans or blueprints from them.\n\nJudge Ronald Thomas began to label Leuchter's methodology as \"ridiculous\" and \"preposterous\", dismissing many of the report's conclusions on the basis that they were based on \"second-hand information\", and refused to allow him to testify on the effect of Zyklon B on humans because he had never worked with the substance, and was neither a toxicologist nor a chemist. Judge Thomas dismissed Leuchter's opinion because it was of \"no greater value than that of an ordinary tourist\", and in regards to Leuchter's opinion said:\n\nWhen questioned on the functioning of the crematoria, the judge also prevented Leuchter from testifying because \"he hasn't any expertise\". Leuchter also claimed that consultation relating to sodium cyanide and hydrogen cyanide with DuPont was \"an on-going thing\". DuPont, the largest American manufacturer of hydrogen cyanide, stated that it had \"never provided any information on cyanides to persons representing themselves as Holocaust deniers, including Fred Leuchter\", and had \"never provided any information regarding the use of cyanide at Auschwitz, Birkenau or Majdanek.\"\n\nThe contents of the report, in particular Leuchter's methodology, are heavily criticised. James Roth, the manager of the lab that carried out the analysis on the samples Leuchter collected, swore under oath to the results at the trial. Roth did not learn what the trial was about until he got off the stand. He later stated that cyanide would have only penetrated to a depth of around 10 micrometres, a tenth of the thickness of a human hair. The samples of brick, mortar and concrete that Leuchter took were of indeterminate thickness: not being aware of this, the lab ground the samples to a fine powder which thus severely diluted the cyanide-containing layer of each sample with an indeterminate amount of brick, varying for each sample. A more accurate analysis would have been obtained by analysing the surface of the samples Leuchter collected. Roth offered the analogy that the investigation was like analyzing paint on a wall by analyzing the timber behind it.\n\nLeuchter's opposition to the possibility of homicidal gassings at Auschwitz relies on residual cyanide remains found in the homicidal gas chambers and delousing chambers at Auschwitz. While both facilities were exposed to the same substance (Zyklon B), many of the delousing chambers are stained with an iron based compound known as Prussian blue, which is not apparent in the homicidal gas chambers. It is not only this disparity that Leuchter cites, but accordingly from his samples (which included measurements of it) that he claims he measured much more cyanide in the delousing chambers than in the gas chambers, which he argues is inconsistent between the amounts necessary to kill human beings and lice. This argument is often cited by Holocaust deniers, and similar claims are also made by Germar Rudolf.\n\nAccording to Richard J. Green:\n\nIn other words, Green states that Leuchter failed to show that Prussian Blue would have been produced in the homicidal gas chambers in the first place—meaning its absence is not in itself proof that no homicidal gassings took place.\nThe problem with Prussian blue is that it is by no means a categorical sign of cyanide exposure. One factor necessary in its formation is a very high concentration of cyanide. In terms of the difference between amounts measured in the delousing chambers and homicidal gas chambers, critics explain that the exact opposite of what deniers claim is true. Insects have a far higher resistance to cyanide than humans, with concentration levels up to 16,000ppm (parts per million) and an exposure time of more than 20 hours (sometimes as long as 72 hours) being necessary for them to succumb. In contrast, a cyanide concentration of only 300ppm is fatal to humans in a matter of minutes. This difference is one of the reasons behind the concentration disparity. Another exceedingly sensitive factor by which very small deviances could determine whether Prussian blue may form is pH. pH could be affected by the presence of human beings. Also, while the delousing chambers were left intact, the ruins of the crematoria at Birkenau had been exposed to the elements for over forty years by the time Leuchter collected his samples. This would have severely affected his results, because unlike Prussian blue and other iron based cyanides, cyanide salts are highly soluble in water.\n\nSince the formation of Prussian blue is not an unconditional outcome of exposure to cyanide, it is not a reliable indicator. Leuchter and Rudolf claim to have measured much more cyanide in the delousing chambers than in the homicidal gas chambers, but since they did not discriminate against an unreliable factor, Green maintains that instant bias is introduced into their experiments. Similarly, Rudolf acknowledges that Prussian blue does not always form upon exposure to cyanide and is thus not a reliable marker, yet continues to include the iron compounds in his analysis. Green describe this as \"disingenuous\". Since a building that contains Prussian blue staining would exhibit much higher levels of detectable cyanides than one without any, Green writes that Leuchter's and Rudolf's measurements reveal nothing more than what is already visible to the naked eye.\n\nIn February 1990, Professor Jan Markiewicz, director of The Institute for Forensic Research (IFRC) in Kraków conducted a fair experiment where iron compounds were excluded. Given that the ruins of the gas chambers at Birkenau have been washed by a column of water at least 35m in height based on climatological records since 1945, Markiewicz and his team were not optimistic at being able to detect cyanides so many years later; nevertheless, having the legal permission to obtain samples, they collected some from areas as sheltered from the elements as possible.\n\nLeuchter's report stated that the small amounts of cyanide he detected in the ruins of the crematoria are merely the result of fumigation. However the IFRC points out that the control samples they took from living areas which may have been fumigated only once as part of the 1942 typhus epidemic tested negative for cyanide, and that the typhus epidemic occurred before the crematoria at Birkenau even existed.\nAccordingly, the IFRC demonstrated that cyanides were present in all of the facilities where it is claimed that they were exposed, i.e. all five crematoria, the cellars of Block 11 and the delousing facilities. Critics state that any attempt to demonstrate that the crematoria could not have functioned as homicidal gas chambers on the basis that they were not exposed to cyanide is unsuccessful, given that its presence in what remains of these facilities is incontrovertible, and write that all of the gas chambers were exposed to cyanide at levels higher than background levels elsewhere in the camp, such as living areas, where no cyanides at all were detected. In addition, tests conducted at Auschwitz in 1945 revealed the presence of cyanides on ventilation grilles found in the ruins of Crematorium II (thus also demonstrating that the Leuchter report was not the first forensic examination of the camp as purported in the title of the London edition). The historian Richard J. Evans argued that due to Leuchter's ignorance of the large disparity between the amounts of cyanide necessary to kill humans and lice, instead of disproving the homicidal use of gas chambers, the small amounts of cyanide which Leuchter detected actually tended to confirm it.\n\nBy order of Heinrich Himmler, the crematoria and gas chambers at Birkenau were destroyed by the SS in order to hide evidence of genocide. Nothing more than the bases of Crematoria IV and V can be seen: the floor plans of both facilities are indicated by bricks laid out across the concrete foundations, and Crematoria II and III are in ruins. Professor Robert Jan van Pelt labels Leuchter's comment that the facilities have not changed at all since 1942 or 1941 as \"nonsense\".\n\nBecause hydrogen cyanide is explosive, Leuchter maintained that the gas chambers could never have been operated due to their proximity to the ovens of the crematoria. It is correct that hydrogen cyanide is explosive, but only at concentrations of 56,000 ppm and above – over 186 times more than the lethal dose of 300 ppm. Critics estimate conservatively that within 5 to 15 minutes, gas chamber victims were exposed to 450 – 1810 ppmv – again considerably lower than the lower explosion limit.\n\nLeuchter incorrectly assumed that the gas chambers were not ventilated. The basement gas chambers of Crematoria II and III were mechanically ventilated via motors in the roof space of the main crematorium structure capable of extracting the remaining gas and renewing the air every three to four minutes.\n\nWhen ventilation was not used such as in Crematoria IV and V (although a ventilation system was later installed in Crematorium V in May 1944), Sonderkommando prisoners wore gas masks when removing the bodies. When presented in court with a document by the chief Auschwitz architect SS-\"Sturmbannführer\" Karl Bischoff, Leuchter misconstrued aeration (\"Belüftung\") and ventilation (\"Entlüftung\") as part of the furnace blower systems, when they were actually in reference to the ventilation channels in the walls that straddle the gas chambers. These are visible on blueprints, and can still partly be seen in the ruined east wall of the Crematorium III gas chamber.\n\nLeuchter was also prepared to act as expert witness regarding crematoria ovens despite admitting during cross examination that he had no expert knowledge. Leuchter presented his own estimate of 156 corpses as the total daily incineration capacity of the installations at Auschwitz. During cross-examination, he was presented with a letter written by the Auschwitz Central Construction Office (\"Auschwitz Zentralbauleitung\") of June 28, 1943, from SS-\"Sturmbannführer\" Jahrling to SS-\"Brigadeführer\" Hans Kammler stating that the five crematoria installations had a collective daily capacity of 4,756 corpses. Leuchter conceded that this was quite different from his own figure, and that he had never seen the document in question before.\n\nA patent application by the makers of the ovens, (both of which were made during the war) and two independent testimonies confirmed the capacity of the crematoria. Because the 4,756 figure is evidence of the Nazis equipping a camp of a maximum of 125,000 prisoners with the facility to cremate 140,000 of them per month, critics of Leuchter explain that this reveals the true exterminationist purpose of Auschwitz: a camp with the capacity to reduce its entire population to ash on a monthly basis was not merely a benign internment camp.\n\nAt various times (such as in the summer of 1944 when the crematoria couldn't keep up with the extermination rate), bodies were burnt in open-air pits. Accordingly, the capacity of the crematoria was never a limiting factor, and the pits yielded practically no limit to the number of corpses that could be burnt.\n\n\n\n"}
{"id": "4340403", "url": "https://en.wikipedia.org/wiki?curid=4340403", "title": "List of adiabatic concepts", "text": "List of adiabatic concepts\n\nAdiabatic (from \"Gr.\" ἀ \"negative\" + διάβασις \"passage; transference\") refers to any process that occurs without heat transfer. This concept is used in many areas of physics and engineering. Notable examples are listed below.\n\n\n\n\n\n\n\n"}
{"id": "319094", "url": "https://en.wikipedia.org/wiki?curid=319094", "title": "List of discoveries", "text": "List of discoveries\n\nThis article presents a list of discoveries and includes famous observations. \"Discovery\" observations form acts of detecting and learning something. Discovery observations are acts in which something is found and given a productive insight. The observation assimilates the knowledge of a phenomenon or the recording of data using instruments.\n\n\"Century of discovery and item\"\n\n\"Century of discovery, item and discoverer\"\n\n\n\n\n\n"}
{"id": "24619829", "url": "https://en.wikipedia.org/wiki?curid=24619829", "title": "List of prolific inventors", "text": "List of prolific inventors\n\nThomas Alva Edison was widely known as the America's most prolific inventor, even after his death in 1931. He held a total of 1,093 U.S. patents (1,084 utility patents and 9 design patents). In 2003, he was passed by Japanese inventor Shunpei Yamazaki. On February 26, 2008, Yamazaki was passed by Australian inventor Kia Silverbrook. Yamazaki passed Silverbrook in 2017.\n\nInventors with 200 or more worldwide utility patents are shown in the following table. While in many cases this is the number of utility patents granted by the United States Patent and Trademark Office, it may include utility patents granted by other countries, as noted by the source references for an inventor.\n\nThis table was current .\nThe columns are defined as follows:\n\nAs the average number of patents per inventor is around 3, some sources define prolific inventors as five times above the average (in terms of patents), leading to a threshold of 15 patents. However, this table currently has an arbitrary cut-off limit for inclusion of 200 patents. This is purely for practical reasons – there are tens of thousands of inventors with more than 15 patents. The threshold of 200 patents means that some famous prolific inventors such as Nikola Tesla are not included in this list, as Tesla had 111 patents.\n\nThis table is a sortable list of the most prolific inventors as measured by utility patents granted. It does not include other types of invention, such as inventions that were never applied for nor granted, for which there is no known source. Nor does the table attempt to measure the significance of an inventor and their inventions. The significance of inventions is often not apparent until many decades after the invention has been made. For recent inventors, it is not yet possible to determine their place in history.\n\nThe common symbol for inventiveness, the light bulb, is an example. The first incandescent light bulb was invented by British chemist Sir Humphry Davy in 1802. Many subsequent inventors improved Davy's invention prior to the successful commercialization of electric lighting by Thomas Edison in 1880, 78 years later. Electric lighting continued to be developed. Edison's carbon filament light bulb was made obsolete by the tungsten filament light bulb, invented in 1904 by Sándor Just and Franjo Hanaman. It is this that forms the popular conception of a light bulb, though there are other major forms of lighting. The principle of fluorescent lights was known since 1845, and various inventors, including Edison and Nikola Tesla worked on them without commercial success. Various improvements were made by many other inventors, until General Electric introduced \"fluorescent lumiline lamps\" commercially in 1938, first available to the public at the 1939 World's Fair. LED lamps also have a long history, with the first light-emitting diode (LED) invented in 1927 by Oleg Losev. LEDs were initially of low brightness, and have been used as indicator lamps and seven-segment displays since 1968. It wasn't until the development of high efficiency blue LEDs by Shuji Nakamura in the 1980s that white LEDs for lighting applications became practical. Although higher cost than incandescent light bulbs, LEDs have higher efficiency and longer life and may finally displace light bulbs in general lighting applications. In each case, more than 50 years passed between the initial invention and commercial success in general lighting applications.\n\nRankings of prolific inventors have been published at various times. However, until the patent records were digitized, these lists were very tedious to prepare, as many thousands of patent records had to be checked manually. Even after digitization, it is still not a simple process. While the USPTO keeps statistics for annual rankings of inventions assigned to companies, it no longer publishes rankings of individual inventors. The last such list was published by the USPTO in 1998. Also, patents predating 1976 have not yet been digitized in the USPTO records. This means that patents before 1976 will not be included in a USPTO search by inventor name, and the number of patents granted before 1976 must be added to current searches.\n\nIn January 1936, \"Popular Science\" published a list of the \"most prolific living inventors to be found in America today\".\n\nThomas Edison was not included in the list, as he died in 1931, five years earlier.\n\nOn December 4, 2000, Time Magazine published a list of the \"top five inventors\".\n\nThis list only included U.S. inventors, so omitted Canadian inventor George Albert Lyon, with 993 U.S. patents at the time of publication, Japanese inventor Shunpei Yamazaki, with 745 U.S. patents, and Béla Barényi, with 595 German patents. Also omitted were John F. O'Connor with 949 U.S. patents, and Carleton Ellis, with 753 U.S. patents at the time of publication.\n\nOn December 13, 2005 USA Today published a list of \"the top 10 living U.S. patent holders\":\nThis research was performed by ipIQ of Chicago (now \"The Patent Board\") and 1790 Analytics of New Jersey.\nThis list only considered living inventors, and thus did not include such prolific inventors as Thomas Edison, Melvin De Groote, and Elihu Thomson. This list included design patents, which are not patents for inventions.\n\nOn October 15, 2007 Condé Nast Portfolio Magazine published a list of \"the world's most prolific inventors alive\":\nThis research was performed by The Patent Board, a Chicago patent research and advisory firm.\nAs with the USA Today list, the Portfolio list only considered living inventors, and thus did not include such prolific inventors as Thomas Edison. This list also included design patents, which are not patents for inventions.\n\nOn 6 May 2011 Business Insider published an article titled: \"The Ten Greatest Inventors In The Modern Era\" containing the following list:\n\nThis list included living and dead inventors, and only included granted utility patents (patents for inventions).\n\nStrutpatent.com publishes a list of the \"Top 10 Inventors\" listing inventors ranked by US patents (of all types) issued since 1990:\n\nThis list included only patents granted since 1990, and includes design patents as well as utility patents.\n\nStrutpatent.com publishes weekly, monthly, and annual lists of the top ten categories, inventors and assignees of US patents since 2007. These lists include all patent types, not just patents for inventions (utility patents).\n\nThe top ten inventors of US patents for 2007:\n\nThe top ten inventors of US patents for 2008:\n\nThe top ten inventors of US patents for 2009:\n\nThe top ten inventors of US patents for 2010:\n\nThe top ten inventors of US patents for 2011:\n\nThe top ten inventors of US patents for 2012:\n\nThis table omitted Rick Allen Hamilton II. The USPTO database shows Hamilton was an inventor or co-inventor of 128 US patents granted in 2012, which would place Hamilton at 6th rank for 2012.\n\nDifferences in patent numbers between the various lists are due to several reasons:\n\n"}
{"id": "49158311", "url": "https://en.wikipedia.org/wiki?curid=49158311", "title": "Liulin type instruments", "text": "Liulin type instruments\n\nLiulin-type is a class of spectrometry-dosimetry instruments.\n\nThe first Liulin device was developed in 1986-1988 time period for the scientific program of the second Bulgarian cosmonaut for the flight on “MIR” space station.\n\nAll Liulin type dosimetric instruments use one or more silicon detectors and measure the deposited energy and number of particles into the detector(s) when charged particles hit the device, that allowing it to calculate the dose rate and particle flux.\n\nThe measurements in the LIULIN instrument were based on a single silicon detector followed by a charge-sensitive shaping amplifier (CSA). The number of the pulses at the output of CSA above a given threshold was proportional to the particle flux hitting the detector; the amplitude of the pulses at the output of CSA was proportional to the particles deposited energy. Further the integral of the energy depositions of the particles accumulated in the detector during the measurement interval allowed calculation of the dose rate.\n"}
{"id": "31034199", "url": "https://en.wikipedia.org/wiki?curid=31034199", "title": "Mjärdevi Science Park", "text": "Mjärdevi Science Park\n\nScience Park Mjärdevi (formerly Mjärdevi Science Park) is a technology area in Östergötland in the city of Linköping.\n\nThe area is located just outside Linköping University Campus Valla. The area includes about 350 companies with a total of 6,500 employees (2017). Many of these companies have been started because of the innovations at Linköping University. The companies operate primarily in technology sectors such as telecommunications, business systems, software and systems engineering, electronics, home communications and vehicle safety. The largest residents of Mjärdevi are Ericsson, Releasy, IFS, Sectra, Infor, Combitech and CGI Group.\n\nA computer history museum, IT-ceum, was built up in Mjärdevi in 2005. In 2009, it was relocated to the county museum, Östergötlands Museum.\n\n"}
{"id": "15450044", "url": "https://en.wikipedia.org/wiki?curid=15450044", "title": "Normalization process theory", "text": "Normalization process theory\n\nNormalization process theory (NPT) is a derivative sociological theory of the implementation, embedding, and integration of new technologies and organizational innovations developed originally from a collective set of learning workshops and included a large number of people including Carl R. May, Tracy Finch, Elizabeth Murray, Anne Rogers, Catherine Pope, Anne Kennedy, Pauline Ong and . The theory is a contribution to the field of science and technology studies (STS), and is the result of a programme of theory building by May and a range of academics from applied social science to medicine. Through three iterations, the theory has built upon the normalization process model previously developed by May et al. to explain the social processes that lead to the routine embedding of innovative health technologies.\n\nNormalization process theory focuses attention on agentic contributions – the things that individuals and groups do to operationalize new or modified modes of practice as they interact with dynamic elements of their environments. It defines the implementation, embedding, and integration as a process that occurs when participants deliberately initiate and seek to sustain a sequence of events that bring it into operation. The dynamics of implementation processes are complex, but normalization process theory facilitates understanding by focusing attention on the mechanisms through which participants invest and contribute to them. It reveals \"the work that actors do as they engage with some ensemble of activities (that may include new or changed ways of thinking, acting, and organizing) and by which means it becomes routinely embedded in the matrices of already existing, socially patterned, knowledge and practices\". These have explored objects, agents, and contexts. In a paper published under a creative commons license, May and colleagues describe how, since 2006, NPT has undergone three iterations.\n\n\nNormalization process theory is regarded as a middle range theory that is located within the 'turn to materiality' in STS. It therefore fits well with the case-study oriented approach to empirical investigation used in STS. It also appears to be a straightforward alternative to actor–network theory in that it does not insist on the agency of non-human actors, and seeks to be explanatory rather than descriptive. However, because normalization process theory specifies a set of generative mechanisms that empirical investigation has shown to be relevant to implementation and integration of new technologies, it can also be used in larger scale structured and comparative studies. Although it fits well with the interpretive approach of ethnography and other qualitative research methods, it also lends itself to systematic review and survey research methods. As a middle range theory, it can be federated with other theories to explain empirical phenomena. It is compatible with theories of the transmission and organization of innovations, especially diffusion of innovations theory, labor process theory, and psychological theories including the theory of planned behavior and social learning theory.\n"}
{"id": "19607864", "url": "https://en.wikipedia.org/wiki?curid=19607864", "title": "Open-notebook science", "text": "Open-notebook science\n\nOpen-notebook science is the practice of making the entire primary record of a research project publicly available online as it is recorded. This involves placing the personal, or laboratory, notebook of the researcher online along with all raw and processed data, and any associated material, as this material is generated. The approach may be summed up by the slogan 'no insider information'. It is the logical extreme of transparent approaches to research and explicitly includes the making available of failed, less significant, and otherwise unpublished experiments; so called 'dark data'. The practice of open notebook science, although not the norm in the academic community, has gained significant recent attention in the research and general media as part of a general trend towards more open approaches in research practice and publishing. Open notebook science can therefore be described as part of a wider open science movement that includes the advocacy and adoption of open access publication, open data, crowdsourcing data, and citizen science. It is inspired in part by the success of open-source software and draws on many of its ideas.\n\nThe term \"open-notebook science\" was first used in 2006 in a blog post by Jean-Claude Bradley, an Associate Professor of Chemistry at Drexel University at the time. Bradley described open-notebook science as follows:\n\n\"A team of groundbreaking scientists at SGC are now sharing their lab notebooks online\".\n\n\n\n\n\nThese are initiatives more open than traditional laboratory notebooks but lacking a key component for full Open Notebook Science. Usually either the notebook is only partially shared or shared with significant delay.\n\nA public laboratory notebook makes it convenient to cite the exact instances of experiments used to support arguments in articles. For example, in a paper on the optimization of a Ugi reaction, three different batches of product are used in the characterization and each spectrum references the specific experiment where each batch was used: EXP099, EXP203 and EXP206. This work was subsequently published in the Journal of Visualized Experiments, demonstrating that the integrity data provenance can be maintained from lab notebook to final publication in a peer-reviewed journal.\n\nWithout further qualifications, Open Notebook Science implies that the research is being reported on an ongoing basis without unreasonable delay or filter. This enables others to understand exactly how research actually happens within a field or a specific research group. Such information could be of value to collaborators, prospective students or future employers. Providing access to selective notebook pages or inserting an embargo period would be inconsistent with the meaning of the term \"Open\" in this context. Unless error corrections, failed experiments and ambiguous results are reported, it will not be possible for an outside observer to understand exactly how science is being done. Terms such as Pseudo or Partial have been used as qualifiers for the sharing of laboratory notebook information in a selective way or with a significant delay.\n\nThe arguments against adopting Open Notebook Science fall mainly into three categories which have differing importance in different fields of science. The primary concern, expressed particularly by biological and medical scientists is that of 'data theft' or 'being scooped'. While the degree to which research groups steal or adapt the results of others remains a subject of debate it is certainly the case that the fear of not being first to publish drives much behaviour, particularly in some fields. This is related to the focus in these fields on the published peer reviewed paper as being the main metric of career success.\n\nThe second argument advanced against Open Notebook Science is that it constitutes prior publication, thus making it impossible to patent and difficult to publish the results in the traditional peer reviewed literature. With respect to patents, publication on the web is clearly classified as disclosure. Therefore, while there may be arguments over the value of patents, and approaches that get around this problem, it is clear that Open Notebook Science is not appropriate for research for which patent protection is an expected and desired outcome. With respect to publication in the peer reviewed literature the case is less clear cut. Most publishers of scientific journals accept material that has previously been presented at a conference or in the form of a preprint. Those publishers that accept material that has been previously published in these forms have generally indicated informally that web publication of data, including Open Notebook Science, falls into this category. Open notebook projects have been successfully published in high impact factor peer reviewed journals but this has not been tested with a wide range of publishers. It is to be expected that those publishers that explicitly exclude these forms of pre-publication will not accept material previously disclosed in an open notebook.\n\nThe final argument relates to the problem of the 'data deluge'. If the current volume of the peer reviewed literature is too large for any one person to manage, then how can anyone be expected to cope with the huge quantity of non peer reviewed material that could potentially be available, especially when some, perhaps most, would be of poor quality? A related argument is that 'my notebook is too specific' for it to be of interest to anyone else. The question of how to discover high quality and relevant material is a related issue. The issue of curation and validating data and methodological quality is a serious issue and one that arguably has relevance beyond Open Notebook Science but is a particular challenge here.\n\nThe Open Notebook Science Challenge, now directed towards reporting solubility measurements in non-aqueous solvent, has received sponsorship from Submeta, Nature and Sigma-Aldrich. The first of ten winners of the contest for December 2008 was Jenny Hale.\n\nLogos can be used on notebooks to indicate the conditions of sharing. Fully open notebooks are marked as \"All Content\" and \"Immediate\" access. Partially open notebooks can be marked as either \"Selected Content\" and/or \"Delayed\".\n\n"}
{"id": "18700697", "url": "https://en.wikipedia.org/wiki?curid=18700697", "title": "Open peer review", "text": "Open peer review\n\nOpen peer review is a process in which names of peer reviewers of papers submitted to academic journals are disclosed to the authors of the papers in question. In some cases, as with the \"BMJ\" and BioMed Central, the process also involves posting the entire pre-publication history of the article online, including not only signed reviews of the article, but also its previous versions and author responses to the reviewers.\n\nThere is no single definition of open peer review, as it is implemented differently by different academic journals, but it has been broadly defined as \"any scholarly review mechanism providing disclosure of author and referee identities to one another at any point during the peer review or publication process\".\n\nPossible advantages to an open peer-review system include reviewers being \"more tactful and constructive\" than they would be if they could remain anonymous. It has also been argued that open review leads to more honest reviewing and prevents reviewers from following their individual agendas, as well as leading to the detection of reviewers' conflicts of interests. Some studies have also found that open peer review is associated with an increase in quality of reviews, although other studies have not found such an association. A study of BioMed Central medical journals, all of which use open peer review, found that reviewers usually did not notice problems or request changes in reporting of the results of randomized trials. The same study found most, but not all, of the requested changes had a positive effect on reporting.\n\nA 1999 study found that open peer review did not affect the quality of reviews or the recommendation regarding whether the paper being reviewed should be published, but that it \"significantly increased the likelihood of reviewers declining to review\". Open review of abstracts tended to lead to bias favoring authors from English-speaking countries and prestigious academic institutions. It has also been argued that open peer review could lead to authors accumulating enemies who try to keep their papers from being published or their grant applications from being successful.\n"}
{"id": "4624242", "url": "https://en.wikipedia.org/wiki?curid=4624242", "title": "Parascience", "text": "Parascience\n\nParascience is the study of subjects that are outside the scope of the natural and social sciences because they cannot be explained by accepted scientific theory or tested by conventional scientific methods. This study may be concerned with phenomena assumed to be beyond the scope of scientific inquiry or for which no scientific explanation exists. The parasciences include history, philosophy, art, and religion.\n\nParascience can also be defined as a subject, method, etc., purporting to be scientific but regarded as unorthodox or unacceptable by the scientific community; an \"alternative\" science.\n"}
{"id": "55807611", "url": "https://en.wikipedia.org/wiki?curid=55807611", "title": "Political Economy of Research and Innovation", "text": "Political Economy of Research and Innovation\n\nThe Political Economy of Research and Innovation (PERI) (or sometimes political economy of technoscience) is an emerging academic field at the interface of science and technology studies and political economy. It focuses on the production, distribution, and consumption of knowledge, and how these shape and are shaped by different political economies. Most scholars in this field have so-far focused on the two-way relationship between science, technology, and innovation and political economic processes, practices, and logics. \n\nIt has its origins in the critique of neoclassical or orthodox economics of science by scholars like Philip Mirowski, the 'economic turn' in science and technology studies (see social studies of finance and valuation studies), and innovation studies or science policy.\n\nExamples of the field include:\n"}
{"id": "43529327", "url": "https://en.wikipedia.org/wiki?curid=43529327", "title": "Postnormal times", "text": "Postnormal times\n\nPostnormal times (PNT) is a concept developed by Ziauddin Sardar as a development of Post-normal Science. Sardar describes the present as \"postnormal times\", \"in an in-between period where old orthodoxies are dying, new ones have yet to be born, and very few things seem to make sense.\"\n\nIn support of engaging communities of various scope and scale on how to best navigate PNT and imagine preferred pathways toward the future(s), Sardar and Sweeney published an article in the journal \"Futures\" outlining The Three Tomorrows method, which fills a gap in the field as \"many methods of futures and foresight seldom incorporate pluralism and diversity intrinsically in their frameworks, and few, if any, emphasize the dynamic and merging nature of futures possibilities, or highlight the ignorance and uncertainties we constantly confront.\"\n\nRakesh Kapoor criticized PNT in 2011 as a Western concept, that does not apply to India and other emerging markets. Sam Cole criticised the three C’s of PNT (chaos, complexity and contradictions) as \"Alliterative Logic, theorizing through alliterative word-triads that is not based on empirical evidence\". Jay Gray has suggested that PNT is embryonic, needs a more robust framework, and should be extended to include C S Holling's adaptive cycle. Scientists working on complex evolving systems have pointed out that PNT recalls the ‘Long Waves’ of Kondratiev and Joseph Schumpeter’s view of waves of \"creative destruction.\"\n\nPNT is one of the core areas of research for the Center for Postnormal Policy and Futures Studies at East-West University in Chicago, Illinois, US. A number of articles and editorials on PNT have been published in the journal \"East-West Affairs\".\n\n"}
{"id": "5679464", "url": "https://en.wikipedia.org/wiki?curid=5679464", "title": "Practice research", "text": "Practice research\n\nPractice research is a form of academic research which incorporates an element of practice in the methodology or research output.\n\nRather than seeing the relationship between practice and theory as a dichotomy, as has sometimes traditionally been the case (see academia: theory and practice heading), there is a growing body of practice research academics across a number of disciplines who use practice as part of their research. For example, the practice-based research network (PBRN) within clinical medical research.\n\nWithin arts and humanities departments there are ongoing debates about how to define this emerging research phenomenon, and there are a variety of models of practice research (practice-as-research, practice-based, practice-led, mixed-mode research practice and practice through research), see for example screen media practice research. The potential, nature and scope for this research has been debated from the 1990s. Sir Christopher Frayling in 1993 adapted Herbert Read's model of education through art to describe different ways of thinking about research, noting that research could be FOR practice, where research aims are subservient to practice aims, THROUGH practice, where the practice serves a research purpose, or INTO practice, such as observing the working processes of others. Bruce Archer's statement in 1995 shows the growing recognition of arts practice as research at this time, \"There are circumstances where the best or only way to shed light on a proposition, a principle, a material, a process or a function is to attempt to construct something, or to enact something, calculated to explore, embody or test it.\". This led to the acceptance of practice research in these disciplines to be reviewed alongside traditional research disciplines in the sphere of Higher Education a debate supported by the work of Michael Biggs, John Freeman, Kristina Niedderer, Katy Macleod, Darren Newbury and others.\n\nThe UK's Arts and Humanities Research Council had a steering committee devoted to practice-led research and its report was completed in September 2007., titled AHRC Research Review in Practice-Led Research in Art, Design and Architecture (Rust, Mottram, Till). This informed continuing discussions by the Council for Higher Education in Art & Design (CHEAD) and the AHRC resulting in an evolved notion of practice as research in art, design and architecture, media, and creative writing. This in turn brought an increasing recognition in the UK of the ways in which creative departments contribute to research culture, a potential which informs elements of the Research Excellence Framework 2014.\n\n"}
{"id": "36843243", "url": "https://en.wikipedia.org/wiki?curid=36843243", "title": "Preference test", "text": "Preference test\n\nA preference test is an experiment in which animals are allowed free access to multiple environments which differ in one or more ways. Various aspects of the animal's behaviour can be measured with respect to the alternative environments, such as latency and frequency of entry, duration of time spent, range of activities observed, or relative consumption of a goal object in the environment. These measures can be recorded either by the experimenter or by motion detecting software. Strength of preference can be inferred by the magnitude of the difference in the response, but see \"Advantages and disadvantages\" below. Statistical testing is used to determine whether observed differences in such measures support the conclusion that preference or aversion has occurred. Prior to testing, the animals are usually given the opportunity to explore the environments to habituate and reduce the effects of novelty.\n\nPreference tests can be used to test for preferences of only one characteristic of an environment, e.g. cage colour, or multiple characteristics e.g. a choice between hamster wheel, Habitrail tunnels or additional empty space for extended locomotion.\n\nThe simplest of preference tests offers a choice between two alternatives. This can be done by putting different goal boxes at the ends of the arms of a 'T' shaped maze, or having a chamber divided in into differing halves. A famous example of this simple method is an investigation of the preferences of chickens for different types of wire floor in battery cages. Two types of metal mesh flooring were being used in the 1950s; one type was a large, open mesh using thick wire, the other was a smaller mesh size but the wire was considerably thinner. A prestigious committee, the Brambell Committee, conducting an investigation into farm animal welfare concluded the thicker mesh should be used as this was likely to be more comfortable for the chickens. However, preference tests showed that chickens preferred the thinner wire. Photographs taken from under the cages showed that the thinner mesh offered more points of contact for the feet than the thick mesh, thereby spreading the load on the hens' feet and presumably feeling more comfortable to the birds.\n\nThe number of choices that can be offered is theoretically limitless for some preference tests, e.g., light intensity, cage size, food types; however, the number is often limited by experimental practicalities, current practice (e.g., animal caging systems) or costs. Furthermore, animals usually investigate all areas of the apparatus in a behaviour called \"information gathering\", even those with minor preference, so the more choices that are available may dilute the data on the dominant preference(s).\n\nMost preference tests involve no 'cost' for making a choice, so they do not indicate the strength of an animals motivation or need to obtain the outcome of the choice. For example, if a laboratory mouse is offered three sizes of cage space it may prefer one of them, but this choice does not indicate whether the mouse 'needs' that particular space, or whether it has a relatively slight preference for it. To measure an animals motivation toward a choice one may perform a \"consumer demand test.\" In this sort of test, the choice involves some \"cost\" to the animal, such as physical effort (e.g., lever pressing, weighted door).\n\nPreference tests have been used widely in the study of animal behaviour and motivation, e.g.:\n\n\n\n\n\n\n\n\n\n"}
{"id": "44407149", "url": "https://en.wikipedia.org/wiki?curid=44407149", "title": "PubPeer", "text": "PubPeer\n\nPubPeer is a website that allows users to discuss and review scientific research.\n\nThe site is one of many allowing academics to engage in post-publication peer review, and has highlighted shortcomings in several high-profile papers, in some cases leading to retractions and to accusations of scientific fraud,\nas noted by Retraction Watch. Contrary to most platforms, it allows anonymous post-publication commenting, a controversial feature which is the main factor for its success. Consequently, accusations of libel have been levelled at some of PubPeer's users; correspondingly PubPeer comments are required to use only facts that can be publicly verified.\n\n"}
{"id": "1893089", "url": "https://en.wikipedia.org/wiki?curid=1893089", "title": "Public awareness of science", "text": "Public awareness of science\n\nPublic awareness of science (PAwS), public understanding of science (PUS), or more recently, Public Engagement with Science and Technology (PEST) are terms relating to the attitudes, behaviours, opinions, and activities that comprise the relations between the general public or lay society as a whole to scientific knowledge and organisation. It is a comparatively new approach to the task of exploring the multitude of relations and linkages science, technology, and innovation have among the general public. While earlier work in the discipline had focused on augmenting public knowledge of scientific topics, in line with the information deficit model of science communication, the discrediting of the model has led to an increased emphasis on how the public chooses to use scientific knowledge and on the development of interfaces to mediate between expert and lay understandings of an issue.\n\nThe area integrates a series of fields and themes such as:\n\nHow to raise public awareness and public understanding of science and technology, and how the public feels and knows about science in general, and specific subjects, such as genetic engineering, bioethics, etc., are important lines of research in this area. Professor of communication, Matthew Nisbet, points up the challenge, for example, in terms of the paradox of the success of science and engineering creating the conditions that have led to the trust or distrust of experts among certain populations and that the correlation appears to be more socioeconomic than religious or ideological.\n\nThe publication of the Royal Society's' report \"The Public Understanding of Science\" (or \"Bodmer Report\") in 1985 is widely held to be the birth of the Public Understanding of Science movement in Britain. The report led to the foundation of the Committee on the Public Understanding of Science and a cultural change in the attitude of scientists to outreach activities.\n\nIn the 1990s, a new perspective emerged in the field with the classic study of Cumbrian Sheep Farmers' interaction with the Nuclear scientists in England, where Brian Wynne demonstrated how the experts were ignorant or disinterested in taking into account the lay knowledge of the sheep farmers while conducting field experiments on the impact of the Chernobyl Nuclear fall out on the sheep in the region. Because of this shortcoming from the side of the scientists, local farmers lost their trust in them. The experts were unaware of the local environmental conditions and the behaviour of sheep and this has eventually led to the failure of their experimental models. Following this study, scholars have studies similar micro-sociological contexts of expert-lay interaction and proposed that the context of knowledge communication is important to understand public engagement with science. Instead of large scale public opinion surveys, researchers proposed studies informed by Sociology of Scientific Knowledge (SSK). The contextualist model focuses on the social impediments in the bidirectional flow of scientific knowledge between experts and laypersons/communities.\n\nThe scholarly debate on public engagement with science developed further into analyzing the deliberations on science through various institutional forms, with the help of the theory of deliberative democracy. Public deliberation of and participation in science practiced through public spheres became a major emphasis. Scholars like Sheila Jasanoff argues for wider public deliberation on science in democratic societies which is a basic condition for decision making regarding science and technology. There are also attempts to develop more inclusive participatory models of technological governance in the form of consensus conferences, citizen juries, extended peer reviews, and deliberative mapping.\n\nSocial scientists use various metrics to measure public understanding of science, including:\n\n\n\nThe nature of connections among different pieces of information in memory \n\n\nGovernment and private-led campaigns and events, such as Dana Foundation's \"Brain Awareness Week,\" are becoming a strong focus of programmes which try to promote public awareness of science.\n\nThe UK PAWS Foundation dramatically went as far as establishing a Drama Fund with the BBC in 1994. The purpose was to encourage and support the creation of new drama for television, drawing on the world of science and technology.\n\nThe Vega Science Trust was set up in 1994 to promote science through the media of television and the internet with the aim of giving scientists a platform from which to communicate to the general public.\n\nThe Simonyi Professorship for the Public Understanding of Science chair at The University of Oxford was established in 1995 for the ethologist Richard Dawkins by an endowment from Charles Simonyi. Mathematician Marcus du Sautoy has held the chair since Dawkins' retirement in 2008. Similar professorships have since been created at other British universities. Professorships in the field have been held by well-known academics including Richard Fortey and Kathy Sykes at the University of Bristol, Brian Cox at Manchester University, Tanya Byron at Edge Hill University, Jim Al-Khalili at the University of Surrey and Alice Roberts at the University of Birmingham.\n\n\n\n"}
{"id": "17887712", "url": "https://en.wikipedia.org/wiki?curid=17887712", "title": "Research program", "text": "Research program\n\nA research program (UK: research programme) is a professional network of scientists conducting basic research. The term was used by philosopher of science Imre Lakatos to blend and revise the normative model of science offered by Karl Popper's \"falsificationism\" and the descriptive model of science offered by Thomas Kuhn's \"normal science\". Lakatos found falsificationism impractical and often not practiced, and found normal science—where a \"paradigm of science\", mimicking an \"exemplar\", extinguishes differing perspectives—more monopolistic than actual.\n\nLakatos found that many research programmes coexisted. Each had a \"hard core\" of theories immune to revision, surrounded by a \"protective belt\" of malleable theories. A research programme vies against others to be most \"progressive\". Extending the research programme's theories into new domains is \"theoretical progress\", and experimentally corroborating such is \"empirical progress\", always refusing falsification of the research programme's hard core. A research programme might \"degenerate\"—lose progressiveness—but later return to progressiveness.\n\n"}
{"id": "50891749", "url": "https://en.wikipedia.org/wiki?curid=50891749", "title": "Research proposal", "text": "Research proposal\n\nA research proposal is a document proposing a research project, generally in the sciences or academia, and generally constitutes a request for sponsorship of that research. Proposals are evaluated on the cost and potential impact of the proposed research, and on the soundness of the proposed plan for carrying it out. Research proposals generally address several key points:\n\nResearch proposals may be \"solicited\", meaning that they are submitted in response to a request with specified requirements, such as a request for proposal, or they may be \"unsolicited\", meaning they are submitted without prior request. Other types of proposals include \"preproposals\", where a letter of intent or brief abstract is submitted for review prior to submission of a full proposal; continuation proposals, which re-iterate an original proposal and its funding requirements in order to ensure continued funding; and renewal proposals, which seek continued sponsorship of a project which would otherwise be terminated.\n\nAcademic research proposals are generally written as part of the initial requirements of writing a thesis, research paper, or dissertation. They generally follow the same format as a research paper, with an introduction, a literature review, a discussion of research methodology and goals, and a conclusion. This basic structure may vary between projects and between fields, each of which may have its own requirements.\n"}
{"id": "2253593", "url": "https://en.wikipedia.org/wiki?curid=2253593", "title": "Royal College of Science for Ireland", "text": "Royal College of Science for Ireland\n\nThe Royal College of Science for Ireland (RCScI) was an institute for higher education in Dublin which existed from 1867 to 1926, specialising in physical sciences and applied science. It was originally based on St. Stephen's Green, moving in 1911 to a purpose-built \"Royal College of Science\" building on Merrion Street. In 1926 it was absorbed into University College Dublin (UCD) as the faculty of Science and Engineering.\n\nThe Museum of Economic Geology was founded by the Dublin Castle administration in 1845, with chemist Robert Kane as curator, and a focus on mining in Ireland similar to the Museum of Practical Geology in London. In 1847, Kane was promoted to director, expanding its remit, and renaming it the Museum of Irish Industry (MII). In 1853 a new Science and Art Department was created within the Whitehall administration, and in 1854 the MII placed under its remit. A School of Science applied to Mining and the Arts was created, modelled on the Royal School of Mines in London, with four professorships shared jointly by the MII school and the Royal Dublin Society (RDS). The MII and School shared premises at 51, St. Stephen's Green, acquired in 1846 and fitted out by 1852. In 1864, a select committee of the UK parliament recommended that the MII and School be entirely separated from the RDS and broadened into a government-supported College of Science for Ireland. In 1865, HM Treasury agreed and in 1867 a commission was appointed by the education committee of the Privy Council. The commission, headed by William Parsons, 3rd Earl of Rosse, outlined the scope and functions of the proposed college, and the RCScI mission statement on 11 September 1867 was: \n\nGeorge Sigerson complained in 1868 that the RCScI was less open to Catholics than the MII had been.\n\nThe RCScI's remit was later changed to exclude include agriculture and include \"Physics and Natural Science\". The number of students enrolled increased gradually.\n\nBy 1898 the RCScI had chairs of Mining and Mineralogy, Physics, Chemistry, Zoology, Botany, Geology, Applied Mathematics and Mechanism, Descriptive Geometry and Engineering. In 1900, control was transferred from the Science and Art Department in London to the Department of Agriculture and Technical Instruction in Dublin, headed by Horace Plunkett. A new building was proposed in 1897 and a site from Merrion Street to Kildare Street was chosen in 1898; in 1899 the plan was extended to include government administrative offices as well as the new college. The new building was designed by Sir Aston Webb; the foundation stone was laid in 1904 by Edward VII and it was opened in 1911 by George V.\n\nIn 1926 the RCScI was absorbed into UCD where it became the UCD Science and Engineering faculties. Science moved to UCD's Earlsfort Terrace building, and on to Belfield in the 1960s; Engineering moved from Merrion Street to Belfield in 1989. The RCScI building was then absorbed into Government Buildings with the rest of Aston Webb's complex, lavishly renovated by Charles Haughey and used to house the Department of the Taoiseach.\n\n\n\n"}
{"id": "27514641", "url": "https://en.wikipedia.org/wiki?curid=27514641", "title": "Science of team science", "text": "Science of team science\n\nThe science of team science (SciTS) field encompasses both conceptual and methodological strategies aimed at understanding and enhancing the processes and outcomes of collaborative, team-based research. It is useful to distinguish between team science (TS) initiatives and the science of team science (SciTS) field. Team science initiatives are designed to promote collaborative, and often cross-disciplinary (which includes multi-, inter-, and transdisciplinary) approaches to answering research questions about particular phenomena. The SciTS field, on the other hand, is concerned with understanding and managing circumstances that facilitate or hinder the effectiveness of collaborative science, and evaluating the outcomes of collaborative science. Its principal units of analysis are the research, training, and community-based translational initiatives implemented by both public and private sector organizations.\n\nThe SciTS field focuses on understanding and enhancing the antecedent conditions, collaborative processes, and outcomes associated with team science initiatives, including their scientific discoveries, educational outcomes, and translations of research findings into new practices, patents, products, technical advances, and policies.\n\nSince the 1990s, there has been a growing interest and investment in large-scale, team-based research initiatives to address complex and multifaceted problems that require cross-disciplinary collaboration. The rise in team science parallels the increase in specialization among scientists. The rapid growth and accumulation of specialized knowledge in multiple fields has created a substantial need to establish partnerships among scientists and practitioners drawn from several different fields in order to address complex environmental, social, and public health problems.\n\nThe interdiscipline of SciTS initially emerged from practical concerns on the part of funding agencies, which needed to gauge the performance of team science, understand its added value, determine the return on investment of large research initiatives, and inform science policy The term \"science of team science\" was first introduced in October, 2006, at a conference called The Science of Team Science: Assessing the Value of Transdisciplinary Research, hosted by the National Cancer Institute, in Bethesda, Maryland. The emerging SciTS field was further developed in a supplement to the \"American Journal of Preventive Medicine\", published in July 2008. Two years later, the First Annual International Science of Team Science (SciTS) Conference was held on April 22–24, 2010 in Chicago, Illinois, organized by the Northwestern University Clinical and Translational Sciences (NUCATS) Institute. The Chicago conference brought together team science investigators and practitioners from a broad range of disciplines, including translational research; organizational behavior; social, cognitive, and health psychology; communications; complex systems; evaluation science; technology; and management.\n\nAs a nascent field of inquiry, the terminology, methodologies, and outcomes of SciTS are still being debated and defined, and new hypotheses regarding the most effective strategies for implementing and sustaining team science are beginning to be tested and the results published. While the SciTS literature base is currently limited, the field is founded on a substantial body of knowledge of team research conducted by scholars in diverse fields such as organizational science, community health promotion, and social psychology, as well as from groups outside academia, including business and the military. SciTS research findings are starting to be translated into evidence-based tools and support structures that aim to improve the efficiency and success of team science initiatives.\n\nIn 2013, the National Academy of Sciences established a National Research Council Committee on the Science of Team Science to evaluate the current state of knowledge and practice in the SciTS field. A Committee report was published in 2015.\n\nThe methods and measures used to evaluate team science are complex and evolving with the interdiscipline of SciTS. The definition of a successful team may be different depending on the stakeholder. For example, funding agencies may be more interested in performance measures related to the translation of team research findings to practical applications, whereas team researchers may use the number of publications produced and amount grant funding obtained to gauge the success of a team science endeavor. In addition, the method of evaluation and metrics of success may vary at different points during the team research project. Short-term measures may include indicators of synergistic output, whereas long-term measures may be related to the impact of the research on the evolution of a discipline or the development of public policy.\n\nSciTS uses both qualitative and quantitative methods to evaluate the antecedent conditions, collaborative processes, and outcomes associated with team science, as well as the organizational, social, and political context that influences team science. These methods include approaches such as surveys, ethnographic observation, case studies, and interviews of members of science teams; social network, mapping and visualization techniques (e.g., graphical representations of collaboration formation and dissolution, geographic distribution of collaborators, funding patterns, patent awards, time to production and commercialization, etc.); and bibliometric analyses (e.g., assessment of co-authored papers and grants).\n\nThe field would benefit from science agencies and associated policies to put into place advanced computational infrastructures, which will enable the analysis of terabytes of data to identify the factors that contribute to or hinder the success of science teams. Ideally, evaluation of team science initiatives is performed in real time, during active collaborations, so that the information results can be fed back to the scientific team to enhance its efficiency and effectiveness.\n\nScientists engaged in team science collaborations have traditionally relied on heuristics to make team science work. Through only their own experiences—what might be called the art of team science—they have tried to discern what facilitates team science and what obstacles stand in the way of a collaboration’s success. By studying the practice of team science, SciTS researchers aim to develop a set of evidence-based tools and recommendations that can then be used to improve the efficiency and effectiveness of team science initiatives. For example, SciTS researchers may identify approaches to facilitate the formation and function of successful collaborative science teams, remove inter-institutional barriers to team science, support effective collaboration among researchers who work together within a team, and develop team science training programs.\n\nProgress is being made on a few fronts toward the development of practical tools and recommendations to support the team science process. The SciTS research community is working toward the creation of an evidence base for the development of a set of \"effective practices\", which can be incorporated into team science training to ensure that the next generation of scientists develops the skills necessary to engage in effective team science. Several groups are developing team science \"toolboxes\" or \"toolkits\" that provide resources that can assist researchers with the collaborative science process, including guiding questions that support discussion on collaborative goals and common metrics of success and assigning individual tasks and responsibilities; guidelines for developing a shared \"language\" that can be used among researchers from different disciplinary backgrounds; models for communications infrastructure that can support geographically dispersed collaborations; tools for assessing team members' readiness to collaborate; curricula for training team members in skills for team science; and model \"prenuptial agreements\" for collaborators that can help to establish agreement on rights to authorship and patents that result from scientific collaborations. Other groups are developing online social networking tools to help scientists identify potential collaborators, and creating centralized databases of measures and instruments that can be used by SciTS researchers, program evaluators, or those who are engaged in team science to assess the processes and outcomes of team science initiatives.\n\n\n\n"}
{"id": "4580669", "url": "https://en.wikipedia.org/wiki?curid=4580669", "title": "Science shop", "text": "Science shop\n\nA Science Shop is a facility, often attached to a specific department of a university or an NGO, that provides independent participatory research support in response to concerns experienced by civil society. It's a demand-driven and bottom-up approach to research. Their work can be described as community-based research (CBR). Science Shops were first established in the Netherlands in the 1970s and their main function is to increase both public awareness and to provide access to science and technology to laymen or non-profit organizations.\n\nIn practice, this means civil society organizations will have access to scientific research at low or no cost. Science Shops that are based at universities give students opportunities to do community-based research as part of their curriculum. Science Shops are not restricted to the natural sciences. They can cover topics in all scientific disciplines, ranging from natural sciences to social sciences and humanities.\n\nScience Shops are managed and operated by both permanent staff members and students who screen questions provided by members of civil society. Science Shop staff use these questions to provide challenging problems to both research students and university faculty members in hope of finding solutions to the question. Students who participate in Science Shop projects often can acquire credits toward their degree. Also, many students do their postgraduate work on problems referred to by Science Shops.\n\nMyriad Science Shops have developed expertise in specific areas. For example, the first Science Shop attached to the chemistry department at Utrecht University was particularly skilled in evaluation reports on soil analysis.\n\nClients are often directed to the Science Shop that is best suited to address their particular concerns. The Dutch system has provided many benefits e.g. to environmentalists, workers, and social workers. Science Shops, in general, have aided environmentalists in better analyzing industrial pollutants, and helped workers to better evaluate the safety and employment consequences of new production processes. Moreover, they have enhanced the understanding of social workers in how to deal with disaffected teenagers.\n\nThe Dutch system has inspired Science Shops in nations across Europe such as Denmark, Austria, Germany, Norway, the UK, Belgium, Romania and Portugal . Moreover, there are currently Science Shops in countries outside of Europe such as Canada. The University of Waterloo Science Shop in Canada is a community service centre for knowledge transfer. Science Shops around the world are linked through the International Science Shop network Living Knowledge. The network’s aim is to foster public engagement with, and participation in, all levels of the research and innovation process. The Living Knowledge network is open for all organisations that are interested in community based research and the concept of Science Shops.\n\nThe European Commission (EC), which initiates and implements EU policies and spends EU funds, has been an important factor behind the international interest and progress of the Science Shop movement. The EC has financed studies and projects, such as SCIPAS, InterActs, ISSNET and TRAMS which contributed to the development of new Science Shops.\n\nThe most recent projects, namely PERARES (Public Engagement in Research and Researchers Engaging with Society), EnRRICH (Enhancing Responsible Research and Innovation through Curricula in Higher Education) and SciShops received funding from the European Union’s 7th Framework Programme and the Horizon 2020 research and innovation programme.\n\nThe Public Engagement with Research And Research Engagement with Society (PERARES) project was a four years project funded by the European Community’s 7th Framework Programme. The project started in 2010 and aimed to strengthen public engagement in research (PER) by involving researchers and Civil Society Organisations (CSOs) in the formulation of research agendas and the research process. It used various debates (or dialogues) on Science to actively articulate research request of civil society. These are forwarded to research institutes and results are used in the next phase of the debate. Thus, these debates move ‘upstream’ into agenda settings. For this, partners linked existing debate formats – such as science café’s, science festivals, online-forums – with the Science Shop network - already linking civil society and research institutes. To be able to answer to research requests, it was necessary to enlarge and strengthen the network of research bodies doing research for/with CSOs. Thus, ten new Science Shop like facilities throughout Europe are started, mentored by experienced partners. Guidelines to evaluate the impact of engagement activities are developed and tested.\n\nThe Enhancing Responsible Research and Innovation through Curricula in Higher Education (EnRRICH) project will run from July 2015 – December 2017. It will improve the capacity of students and staff in higher education to develop knowledge, skills and attitudes to support the embedding of Responsible Research and Innovation (RRI) in curricula by responding to the research needs of society as expressed by civil society organisations (CSOs). To reach this aim, the project will identify, develop, pilot and disseminate good practice and relevant resources to embed the 5 RRI policy agendas ’Public Engagement’, ‘Science Education’, ‘Open Access’, ‘Ethics’ and ‘Gender’ in academic curricula across Europe. Through sharing learning and initiating discussion and debates at institutional, national and international levels both within the project consortium and beyond it, the EnRRICH project will create a better awareness of, and enhance the policy context for, RRI in curricula and thereby produce more responsible and responsive graduates and researchers.\n\nThe SciShops project will run from September 2017 to February 2020. Its aim is to build on and expand the capacity of the Science Shops ecosystem in Europe and beyond. As part of the SciShops project, at least ten new university- and non-university-based Science Shops will be established in Europe by project partners. The SciShops team aims to demonstrate the benefits of starting a Science Shop for different kinds of organisation, as well as to show how civil society gains from collaborating with Science Shops in community-based participatory research.\n\n\n"}
{"id": "17017917", "url": "https://en.wikipedia.org/wiki?curid=17017917", "title": "Software studies", "text": "Software studies\n\nSoftware studies is an emerging interdisciplinary research field, which studies software systems and their social and cultural effects.\n\nThe implementation and use of software has been studied in recent fields such as cyberculture, Internet studies, new media studies, and digital culture, yet prior to software studies, software was rarely ever addressed as a distinct object of study.\n\nSoftware studies is an interdisciplinary field. To study software as an artifact, it draws upon methods and theory from the digital humanities and from computational perspectives on software. Methodologically, software studies usually differs from the approaches of computer science and software engineering, which concern themselves primarily with software in information theory and in practical application; however, these fields all share an emphasis on computer literacy, particularly in the areas of programming and source code. This emphasis on analyzing software sources and processes (rather than interfaces) often distinguishes software studies from new media studies, which is usually restricted to discussions of interfaces and observable effects.\n\nThe conceptual origins of software studies include Marshall McLuhan's focus on the role of media in themselves, rather than the content of media platforms, in shaping culture. Early references to the study of software as a cultural practice appear in Friedrich Kittler's essay, \"Es gibt keine Software,\" Lev Manovich's \"Language of New Media\", and Matthew Fuller's \"Behind the Blip: Essays on the culture of software\". Much of the impetus for the development of software studies has come from videogame studies, particularly platform studies, the study of videogames and other software artifacts in their hardware and software contexts. New media art, software art, motion graphics, and computer-aided design are also significant software-based cultural practices, as is the creation of new protocols and platforms.\n\nThe first conference events in the emerging field were Software Studies Workshop 2006 and SoftWhere 2008.\n\nIn 2008, MIT Press launched a \"Software Studies\" book series with an edited volume of essays (Matthew Fuller's \"Software Studies: a Lexicon\"), and the first academic program was launched, (Lev Manovich, Benjamin H. Bratton and Noah Wardrip-Fruin's \"Software Studies Initiative\" at U. California San Diego). \nIn 2011, a number of mainly British researchers established \"Computational Culture\", an open-access peer-reviewed journal. The journal provides a platform for \"inter-disciplinary enquiry into the nature of the culture of computational objects, practices, processes and structures.\"\n\nSoftware studies is closely related to a number of other emerging fields in the digital humanities that explore functional components of technology from a social and cultural perspective. Software studies' focus is at the level of the entire program, specifically the relationship between interface and code. Notably related are critical code studies, which is more closely attuned to the code rather than the program, and platform studies, which investigates the relationships between hardware and software.\n\n\n\n"}
{"id": "325542", "url": "https://en.wikipedia.org/wiki?curid=325542", "title": "Technology acceptance model", "text": "Technology acceptance model\n\nThe technology acceptance model (TAM) is an information systems theory that models how users come to accept and use a technology. The model suggests that when users are presented with a new technology, a number of factors influence their decision about how and when they will use it, notably:\nThe TAM has been continuously studied and expanded—the two major upgrades being the TAM 2 ( & ) and the Unified Theory of Acceptance and Use of Technology (or UTAUT, ). A TAM 3 has also been proposed in the context of e-commerce with an inclusion of the effects of trust and perceived risk on system use ().\n\nTAM is one of the most influential extensions of Ajzen and Fishbein's theory of reasoned action (TRA) in the literature. Davis's technology acceptance model (Davis, 1989; Davis, Bagozzi, & Warshaw, 1989)\nis the most widely applied model of users' acceptance and usage of technology\n(Venkatesh, 2000). It was developed by Fred Davis and Richard Bagozzi (, ). TAM replaces many of TRA's attitude measures with the two technology acceptance measures—\"ease of use\", and \"usefulness\". TRA and TAM, both of which have strong behavioural elements, assume that when someone forms an intention to act, that they will be free to act without limitation. In the real world there will be many constraints, such as limited freedom to act ().\n\nBagozzi, Davis and Warshaw say:\n\nEarlier research on the diffusion of innovations also suggested a prominent role for perceived ease of use. Tornatzky and Klein () analysed the adoption, finding that compatibility, relative advantage, and complexity had the most significant relationships with adoption across a broad range of innovation types. Eason studied perceived usefulness in terms of a fit between systems, tasks and job profiles, using the terms \"task fit\" to describe the metric (quoted in ) suggest that TAM must be extended to include variables that account for change processes and that this could be achieved through adoption of the innovation model into TAM.\n\nSeveral researchers have replicated Davis's original study () to provide empirical evidence on the relationships that exist between usefulness, ease of use and system use (; ; ; ; ; ). Much attention has focused on testing the robustness and validity of the questionnaire instrument used by Davis. Adams et al. () replicated the work of Davis () to demonstrate the validity and reliability of his instrument and his measurement scales. They also extended it to different settings and, using two different samples, they demonstrated the internal consistency and replication reliability of the two scales. Hendrickson et al. () found high reliability and good test-retest reliability. Szajna () found that the instrument had predictive validity for intent to use, self-reported usage and attitude toward use. The sum of this research has confirmed the validity of the Davis instrument, and to support its use with different populations of users and different software choices.\n\nSegars and Grover () re-examined Adams et al.'s () replication of the Davis work. They were critical of the measurement model used, and postulated a different model based on three constructs: usefulness, effectiveness, and ease-of-use. These findings do not yet seem to have been replicated. However, some aspects of these findings were tested and supported by Workman () by separating the dependent variable into information use versus technology use.\n\nMark Keil and his colleagues have developed (or, perhaps rendered more popularisable) Davis's model into what they call the Usefulness/EOU Grid, which is a 2×2 grid where each quadrant represents a different combination of the two attributes. In the context of software use, this provides a mechanism for discussing the current mix of usefulness and EOU for particular software packages, and for plotting a different course if a different mix is desired, such as the introduction of even more powerful software ().\nThe TAM model has been used in most technological and geographic contexts. One of these contexts is health care, which is growing rapidly \nVenkatesh and Davis extended the original TAM model to explain perceived usefulness and usage intentions in terms of social influence (subjective norms, voluntariness, image) and cognitive instrumental processes (job relevance, output quality, result demonstrability, perceived ease of use). The extended model, referred to as TAM2, was tested in both voluntary and mandatory settings. The results strongly supported TAM2 ().\n\nIn an attempt to integrate the main competing user acceptance models, Venkatesh et al. formulated the unified theory of acceptance and use of technology (UTAUT). This model was found to outperform each of the individual models (Adjusted R square of 69 percent) (). UTAUT has been adopted by some recent studies in healthcare.\n\n\nTAM has been widely criticised, despite its frequent use, leading the original proposers to attempt to redefine it several times. Criticisms of TAM as a \"theory\" include its questionable heuristic value, limited explanatory and predictive power, triviality, and lack of any practical value (). Benbasat and Barki suggest that TAM \"has diverted researchers' attention away from other \nimportant research issues and has created an illusion of progress in knowledge accumulation. Furthermore, the \nindependent attempts by several researchers to expand TAM in order to adapt it to the constantly changing IT environments has lead to a state of theoretical chaos and confusion\" (). In general, TAM focuses on the individual 'user' of a computer, with the concept of 'perceived usefulness', with extension to bring in more and more factors to explain how a user 'perceives' 'usefulness', and ignores the essentially social processes of IS development and implementation, without question where more technology is actually better, and the social consequences of IS use. Lunceford argues that the framework of perceived usefulness and ease of use overlooks other issues, such as cost and structural imperatives that force users into adopting the technology. For a recent analysis and critique of TAM, see Bagozzi ().\n\nLegris et al. claim that, together, TAM and TAM2 account for only 40% of a technological system's use.\n\nPerceived ease of use is less likely to be a determinant of attitude and usage intention according to studies of telemedicine (), mobile commerce (, and online banking ().\n\nA study conducted by Okafor, D. J., Nico, M. & Azman, B. B. (2016) discovered that perceived ease of use doesn't have any influence on the adoption of multimedia online technologies for Malaysian SMEs. The answers from the participants in this study suggest that, for them, perceived ease of use was not indicative of their behavioural intention to adopt multimedia online technologies (MOT) in the future. Instead of not adopting MOT, if they are complicated some participants said they are willing to learn it or practice more.\n\n\n"}
{"id": "1424913", "url": "https://en.wikipedia.org/wiki?curid=1424913", "title": "Vis viva", "text": "Vis viva\n\n1\nVis viva (from the Latin for \"living force\") is a historical term used for the first (known) description of what we now call kinetic energy in an early formulation of the principle of conservation of energy.\n\nProposed by Gottfried Leibniz over the period 1676–1689, the theory was controversial as it seemed to oppose the theory of conservation of momentum advocated by Sir Isaac Newton and René Descartes. The two theories are now understood to be complementary.\n\nThe theory was eventually absorbed into the modern theory of energy though the term still survives in the context of celestial mechanics through the \"vis viva\" equation.\n\nThe term is due to German Gottfried Wilhelm Leibniz, who during 1676–1689 first attempted a mathematical formulation. Leibniz noticed that in many mechanical systems (of several masses, \"m\" each with velocity \"v\") the quantity:\n\nwas conserved. He called this quantity the \"vis viva\" or \"living force\" of the system. The principle, it is now realised, represents an accurate statement of the conservation of kinetic energy in elastic collisions, and is independent of the conservation of momentum. However, many physicists at the time were unaware of this fact and, instead, were influenced by the prestige of Sir Isaac Newton in England and of René Descartes in France, both of whom advanced the conservation of momentum as a guiding principle. Thus the momentum:\n\nwas held by the rival camp to be the conserved \"vis viva\". It was largely engineers such as John Smeaton, Peter Ewart, Karl Holtzmann, Gustave-Adolphe Hirn and Marc Seguin who objected that conservation of momentum alone was not adequate for practical calculation and who made use of Leibniz's principle. The principle was also championed by some chemists such as William Hyde Wollaston.\n\nThe French mathematician Émilie du Châtelet, who had a sound grasp of Newtonian mechanics, developed Leibniz's concept and, combining it with the observations of Willem 's Gravesande, showed that \"vis viva\" was dependent on the square of the velocities.\n\nMembers of the academic establishment such as John Playfair were quick to point out that kinetic energy is clearly not conserved. This is obvious to a modern analysis based on the second law of thermodynamics but in the 18th and 19th centuries, the fate of the lost energy was still unknown. Gradually it came to be suspected that the heat inevitably generated by motion was another form of \"vis viva\". In 1783, Antoine Lavoisier and Pierre-Simon Laplace reviewed the two competing theories of \"vis viva\" and caloric theory. Count Rumford's 1798 observations of heat generation during the boring of cannons added more weight to the view that mechanical motion could be converted into heat. \"Vis viva\" now started to be known as \"energy\", after the term was first used in that sense by Thomas Young in 1807.\nThe recalibration of \"vis viva\" to include the coefficient of a half, namely:\n\nwas largely the result of the work of Gaspard-Gustave Coriolis and Jean-Victor Poncelet over the period 1819–1839, although the present-day definition can occasionally be found earlier (e.g., in Daniel Bernoulli texts).\n\nThe former called it the \"quantité de travail\" (quantity of work) and the latter, \"travail mécanique\" (mechanical work) and both championed its use in engineering calculation.\n\n\n"}
