{"id": "58560809", "url": "https://en.wikipedia.org/wiki?curid=58560809", "title": "AgBiotechNet", "text": "AgBiotechNet\n\nAgBiotechNet is a searchable online database of scientific literature on topics related to agricultural biotechnology. Its target audience consists of biotechnology researchers and policy makers. Though some features on the site are available for free, others can only be accessed by paid subscribers. First launched in January 1999, AgBiotechNet is run by the Centre for Agriculture and Bioscience International (also known as CABI), which founded it along with Michigan State University's Agricultural Biotechnology Support Project.\n"}
{"id": "40705930", "url": "https://en.wikipedia.org/wiki?curid=40705930", "title": "Aid on the Edge of Chaos", "text": "Aid on the Edge of Chaos\n\nAid on the Edge of Chaos is a 2013 book on applying cutting-edge science and innovation to international development, published by Oxford University Press. Written by global development and humanitarian expert Ben Ramalingam, it focuses on the need to improve foreign aid and the value of complex systems science and research for how global international aid efforts should be designed, implemented and evaluated.\n\nDescribed in a leading development journal as 'one of the most lauded contributions to recent mainstream development thinking', \"Aid on the Edge of Chaos\" has been endorsed by many top scientists and international leaders, including four Nobel Laureates in Medicine, Economics and Chemistry and the heads of Red Cross and United Nations as well as many NGO leaders. It has been positively reviewed by various press outlets, including \"The Economist\", the \"Financial Times\", \"The Guardian\", \"New Scientist\", \"Nature\", Lancet, Harvard Business Review and the British Medical Journal.\n\nIt was also the focus of an interview feature with the author in \"Huffington Post\".\n\"Aid on the Edge of Chaos\" was the subject of a public lecture by Ben Ramalingam at the Royal Society of Arts, London, in December 2013, an event chaired by Geoff Mulgan, CEO of NESTA. The book was discussed by Ramalingam and Sir John Holmes at the Oxford Literary Festival in March 2014, an event chaired by leading British filmmaker and author Bidisha.\n\nMany international aid agencies are applying ideas from the book in their work, including the UK Department for International Development, United States Agency for International Development, the International Rescue Committee, Mercy Corps, UNICEF, World Food Programme, World Vision, the World Bank, the United Nations and Oxfam.\n"}
{"id": "1845", "url": "https://en.wikipedia.org/wiki?curid=1845", "title": "Alternative medicine", "text": "Alternative medicine\n\nAlternative medicine, fringe medicine, pseudomedicine or simply questionable medicine is the use and promotion of practices which are unproven, disproven, impossible to prove, or excessively harmful in relation to their effect — in the attempt to achieve the healing effects of medicine. They differ from experimental medicine in that the latter employs responsible investigation, and accepts results that show it to be ineffective. The scientific consensus is that alternative therapies either do not, or cannot, work. In some cases laws of nature are violated by their basic claims; in some the treatment is so much worse that its use is unethical. Alternative practices, products, and therapies range from only ineffective to having known harmful and toxic effects. \n\nAlternative therapies may be credited for perceived improvement through placebo effects, decreased use or effect of medical treatment (and therefore either decreased side effects; or nocebo effects towards standard treatment), or the natural course of the condition or disease. Alternative treatment is not the same as experimental treatment or traditional medicine, although both can be misused in ways that are alternative. Alternative or complementary medicine is dangerous because it may discourage people from getting the best possible treatment, and may lead to a false understanding of the body and of science.\n\nAlternative medicine is used by a significant number of people, though its popularity is often overstated. Large amounts of funding go to testing alternative medicine, with more than US$2.5 billion spent by the United States government alone. Almost none show any effect beyond that of false treatment, and most studies showing any effect have been statistical flukes. Alternative medicine is a highly profitable industry, with a strong lobby. This fact is often overlooked by media or intentionally kept hidden, with alternative practice being portrayed positively when compared to \"big pharma\". The lobby has successfully pushed for alternative therapies to be subject to far less regulation than conventional medicine. Alternative therapies may even be allowed to promote use when there is demonstrably no effect, only a tradition of use. Regulation and licensing of alternative medicine and health care providers varies between and within countries. Despite laws making it illegal to market or promote alternative therapies for use in cancer treatment, many practitioners promote them. Alternative medicine is criticized for taking advantage of the weakest members of society.\n\nTerminology has shifted over time, reflecting the preferred branding of practitioners. For example, the United States National Institutes of Health department studying alternative medicine, currently named National Center for Complementary and Integrative Health, was established as the \"Office of Alternative Medicine\" and was renamed the \"National Center for Complementary and Alternative Medicine\" before obtaining its current name. Therapies are often framed as \"natural\" or \"holistic\", in apparent opposition to conventional medicine which is \"artificial\" and \"narrow in scope\", statements which are intentionally misleading. When used together with functional medical treatment, alternative therapies do not \"complement\" (improve the effect of, or mitigate the side effects of) treatment. Significant drug interactions caused by alternative therapies may instead negatively impact functional treatment by making prescription drugs less effective, such as interference by herbal preparations with warfarin.\n\nAlternative medicine is defined loosely as a set of products, practices, and theories that are believed or perceived by their users to have the healing effects of medicine, but whose effectiveness has not been clearly established using scientific methods, or whose theory and practice is not part of biomedicine, or whose theories or practices are directly contradicted by scientific evidence or scientific principles used in biomedicine. \"Biomedicine\" or \"medicine\" is that part of medical science that applies principles of biology, physiology, molecular biology, biophysics, and other natural sciences to clinical practice, using scientific methods to establish the effectiveness of that practice. Unlike medicine, an alternative product or practice does not originate from using scientific methods, but may instead be based on hearsay, religion, tradition, superstition, belief in supernatural energies, pseudoscience, errors in reasoning, propaganda, fraud, or other unscientific sources.\n\nIn \"General Guidelines for Methodologies on Research and Evaluation of Traditional Medicine\", published in 2000 by the World Health Organization (WHO), complementary and alternative medicine were defined as a broad set of health care practices that are not part of that country's own tradition and are not integrated into the dominant health care system.\n\nThe expression also refers to a diverse range of related and unrelated products, practices, and theories ranging from biologically plausible practices and products and practices with some evidence, to practices and theories that are directly contradicted by basic science or clear evidence, and products that have been conclusively proven to be ineffective or even toxic and harmful.\n\nThe terms \"alternative medicine\", \"complementary medicine\", \"integrative medicine,\" \"holistic medicine\", \"natural medicine\", \"unorthodox medicine\", \"fringe medicine\", \"unconventional medicine\", and \"new age medicine\" are used interchangeably as having the same meaning and are almost synonymous in most contexts.\n\nThe meaning of the term \"alternative\" in the expression \"alternative medicine\", is not that it is an effective alternative to medical science, although some alternative medicine promoters may use the loose terminology to give the appearance of effectiveness. Loose terminology may also be used to suggest meaning that a dichotomy exists when it does not, e.g., the use of the expressions \"western medicine\" and \"eastern medicine\" to suggest that the difference is a cultural difference between the Asiatic east and the European west, rather than that the difference is between evidence-based medicine and treatments that do not work.\n\nComplementary medicine (CM) or integrative medicine (IM) is when alternative medicine is used together with functional medical treatment, in a belief that it improves the effect of treatments. However, significant drug interactions caused by alternative therapies may instead negatively influence treatment, making treatments less effective, notably cancer therapy. Both terms refer to use of alternative medical treatments alongside conventional medicine, an example of which is use of acupuncture (sticking needles in the body to influence the flow of a supernatural energy), along with using science-based medicine, in the belief that the acupuncture increases the effectiveness or \"complements\" the science-based medicine.\n\nCAM is an abbreviation of the phrase \"complementary and alternative medicine\". It has also been called sCAM or SCAM with the addition of \"so-called\" or \"supplements\".\n\n\"Allopathic medicine\" or \"allopathy\" is an expression commonly used by homeopaths and proponents of other forms of alternative medicine to refer to mainstream medicine. It was used to describe the traditional European practice of heroic medicine, but later continued to be used to describe anything that was not homeopathy.\n\nAllopathy refers to the use of pharmacologically active agents or physical interventions to treat or suppress symptoms or pathophysiologic processes of diseases or conditions. The German version of the word, , was coined in 1810 by the creator of homeopathy, Samuel Hahnemann (1755–1843). The word was coined from (different) and (relating to a disease or to a method of treatment). In alternative medicine circles the expression \"allopathic medicine\" is still used to refer to \"the broad category of medical practice that is sometimes called Western medicine, biomedicine, evidence-based medicine, or modern medicine\" (see the article on scientific medicine).\n\nUse of the term remains common among homeopaths and has spread to other alternative medicine practices. The meaning implied by the label has never been accepted by conventional medicine and is considered pejorative. More recently, some sources have used the term \"allopathic\", particularly American sources wishing to distinguish between Doctors of Medicine (MD) and Doctors of Osteopathic Medicine (DO) in the United States. William Jarvis, an expert on alternative medicine and public health, states that \"although many modern therapies can be construed to conform to an allopathic rationale (e.g., using a laxative to relieve constipation), standard medicine has never paid allegiance to an allopathic principle\" and that the label \"allopath\" was from the start \"considered highly derisive by regular medicine\".\n\nMany conventional medical treatments do not fit the nominal definition of \"allopathy\", as they seek to prevent illness, or remove its cause.\n\nCAM is an abbreviation of complementary and alternative medicine. It has also been called sCAM or SCAM with the addition of \"so-called\" or \"supplements\". The words balance and holism are often used, claiming to take into account a \"whole\" person, in contrast to the supposed reductionism of medicine. Due to its many names the field has been criticized for intense rebranding of what are essentially the same practices: as soon as one name is declared synonymous with quackery, a new name is chosen.\n\nTraditional medicine refers to the pre-scientific practices of a certain culture, contrary to what is typically practiced in other cultures where medical science dominates.\n\n\"Eastern medicine\" typically refers to the traditional medicines of Asia where conventional bio-medicine penetrated much later.\n\nThe words \"balance\" and \"holism\" are often used alongside \"complementary\" or \"integrative\" medicine, claiming to take into account a \"whole\" person, in contrast to the supposed reductionism of medicine. Due to its many names the field has been criticized for intense rebranding of what are essentially the same practices.\n\nProminent members of the science and biomedical science community say that it is not meaningful to define an alternative medicine that is separate from a conventional medicine, that the expressions \"conventional medicine\", \"alternative medicine\", \"complementary medicine\", \"integrative medicine\", and \"holistic medicine\" do not refer to any medicine at all.\n\nOthers in both the biomedical and CAM communities say that CAM \"cannot\" be precisely defined because of the diversity of theories and practices it includes, and because the boundaries between CAM and biomedicine overlap, are porous, and change. The expression \"complementary and alternative medicine\" (CAM) resists easy definition because the health systems and practices it refers to are diffuse, and its boundaries poorly defined. Healthcare practices categorized as alternative may differ in their historical origin, theoretical basis, diagnostic technique, therapeutic practice and in their relationship to the medical mainstream. Some alternative therapies, including traditional Chinese medicine (TCM) and Ayurveda, have antique origins in East or South Asia and are entirely alternative medical systems; others, such as homeopathy and chiropractic, have origins in Europe or the United States and emerged in the eighteenth and nineteenth centuries. Some, such as osteopathy and chiropractic, employ manipulative physical methods of treatment; others, such as meditation and prayer, are based on mind-body interventions. Treatments considered alternative in one location may be considered conventional in another. Thus, chiropractic is not considered alternative in Denmark and likewise osteopathic medicine is no longer thought of as an alternative therapy in the United States.\n\nCritics say the expression is deceptive because it implies there is an effective alternative to science-based medicine, and that \"complementary\" is deceptive because it implies that the treatment increases the effectiveness of (complements) science-based medicine, while alternative medicines that have been tested nearly always have no measurable positive effect compared to a placebo.\n\nOne common feature of all definitions of alternative medicine is its designation as \"other than\" conventional medicine. For example, the widely referenced descriptive definition of complementary and alternative medicine devised by the US National Center for Complementary and Integrative Health (NCCIH) of the National Institutes of Health (NIH), states that it is \"a group of diverse medical and health care systems, practices, and products that are not generally considered part of conventional medicine\". For conventional medical practitioners, it does not necessarily follow that either it or its practitioners would no longer be considered alternative.\n\nSome definitions seek to specify alternative medicine in terms of its social and political marginality to mainstream healthcare. This can refer to the lack of support that alternative therapies receive from the medical establishment and related bodies regarding access to research funding, sympathetic coverage in the medical press, or inclusion in the standard medical curriculum. In 1993, the British Medical Association (BMA), one among many professional organizations who have attempted to define alternative medicine, stated that it referred to \"...those forms of treatment which are not widely used by the conventional healthcare professions, and the skills of which are not taught as part of the undergraduate curriculum of conventional medical and paramedical healthcare courses\". In a US context, an influential definition coined in 1993 by the Harvard-based physician, David M. Eisenberg, characterized alternative medicine \"as interventions neither taught widely in medical schools nor generally available in US hospitals\". These descriptive definitions are inadequate in the present-day when some conventional doctors offer alternative medical treatments and CAM introductory courses or modules can be offered as part of standard undergraduate medical training; alternative medicine is taught in more than 50 per cent of US medical schools and increasingly US health insurers are willing to provide reimbursement for CAM therapies. In 1999, 7.7% of US hospitals reported using some form of CAM therapy; this proportion had risen to 37.7% by 2008.\n\nAn expert panel at a conference hosted in 1995 by the US Office for Alternative Medicine (OAM), devised a theoretical definition of alternative medicine as \"a broad domain of healing resources ... other than those intrinsic to the politically dominant health system of a particular society or culture in a given historical period\". This definition has been widely adopted by CAM researchers, cited by official government bodies such as the UK Department of Health, attributed as the definition used by the Cochrane Collaboration, and, with some modification, was preferred in the 2005 consensus report of the US Institute of Medicine, \"Complementary and Alternative Medicine in the United States\".\n\nThe 1995 OAM conference definition, an expansion of Eisenberg's 1993 formulation, is silent regarding questions of the medical effectiveness of alternative therapies. Its proponents hold that it thus avoids relativism about differing forms of medical knowledge and, while it is an essentially political definition, this should not imply that the dominance of mainstream biomedicine is solely due to political forces. According to this definition, alternative and mainstream medicine can only be differentiated with reference to what is \"intrinsic to the politically dominant health system of a particular society of culture\". However, there is neither a reliable method to distinguish between cultures and subcultures, nor to attribute them as dominant or subordinate, nor any accepted criteria to determine the dominance of a cultural entity. If the culture of a politically dominant healthcare system is held to be equivalent to the perspectives of those charged with the medical management of leading healthcare institutions and programs, the definition fails to recognize the potential for division either within such an elite or between a healthcare elite and the wider population.\n\nNormative definitions distinguish alternative medicine from the biomedical mainstream in its provision of therapies that are unproven, unvalidated, or ineffective and support of theories with no recognized scientific basis. These definitions characterize practices as constituting alternative medicine when, used independently or in place of evidence-based medicine, they are put forward as having the healing effects of medicine, but are not based on evidence gathered with the scientific method. Exemplifying this perspective, a 1998 editorial co-authored by Marcia Angell, a former editor of \"The New England Journal of Medicine\", argued that:\n\nIt is time for the scientific community to stop giving alternative medicine a free ride. There cannot be two kinds of medicine – conventional and alternative. There is only medicine that has been adequately tested and medicine that has not, medicine that works and medicine that may or may not work. Once a treatment has been tested rigorously, it no longer matters whether it was considered alternative at the outset. If it is found to be reasonably safe and effective, it will be accepted. But assertions, speculation, and testimonials do not substitute for evidence. Alternative treatments should be subjected to scientific testing no less rigorous than that required for conventional treatments.\n\nThis line of division has been subject to criticism, however, as not all forms of standard medical practice have adequately demonstrated evidence of benefit, and it is also unlikely in most instances that conventional therapies, if proven to be ineffective, would ever be classified as CAM.\n\nSimilarly, the public information website maintained by the National Health and Medical Research Council (NHMRC) of the Commonwealth of Australia uses the acronym \"CAM\" for a wide range of health care practices, therapies, procedures and devices not within the domain of conventional medicine. In the Australian context this is stated to include acupuncture; aromatherapy; chiropractic; homeopathy; massage; meditation and relaxation therapies; naturopathy; osteopathy; reflexology, traditional Chinese medicine; and the use of vitamin supplements.\n\nThe Danish National Board of Health's \"Council for Alternative Medicine\" (Sundhedsstyrelsens Råd for Alternativ Behandling (SRAB)), an independent institution under the National Board of Health (Danish: \"Sundhedsstyrelsen\"), uses the term \"alternative medicine\" for:\n\nProponents of an evidence-base for medicine such as the Cochrane Collaboration (founded in 1993 and from 2011 providing input for WHO resolutions) take a position that \"all\" systematic reviews of treatments, whether \"mainstream\" or \"alternative\", ought to be held to the current standards of scientific method. In a study titled \"Development and classification of an operational definition of complementary and alternative medicine for the Cochrane Collaboration\" (2011) it was proposed that indicators that a therapy is accepted include government licensing of practitioners, coverage by health insurance, statements of approval by government agencies, and recommendation as part of a practice guideline; and that if something is currently a standard, accepted therapy, then it is not likely to be widely considered as CAM.\n\nAlternative medicine consists of a wide range of health care practices, products, and therapies. The shared feature is a claim to heal that is not based on the scientific method. Alternative medicine practices are diverse in their foundations and methodologies. Alternative medicine practices may be classified by their cultural origins or by the types of beliefs upon which they are based. Methods may incorporate or be based on traditional medicinal practices of a particular culture, folk knowledge, superstition, spiritual beliefs, belief in supernatural energies (antiscience), pseudoscience, errors in reasoning, propaganda, fraud, new or different concepts of health and disease, and any bases other than being proven by scientific methods. Different cultures may have their own unique traditional or belief based practices developed recently or over thousands of years, and specific practices or entire systems of practices.\nAlternative medicine, such as using naturopathy or homeopathy in place of conventional medicine, is based on belief systems not grounded in science.\n\nAlternative medical systems may be based on traditional medicine practices, such as traditional Chinese medicine (TCM), Ayurveda in India, or practices of other cultures around the world. Some useful applications of traditional medicines have been researched and accepted within ordinary medicine, however the underlying belief systems are seldom scientific and are not accepted.\n\nTraditional medicine is considered alternative when it is used outside its home region; or when it is used together with or instead of known functional treatment; or when it can be reasonably expected that the patient or practitioner knows or should know that it will not work – such as knowing that the practice is based on superstition.\n\nSince ancient times, in many parts of the world a number of herbs reputed to possess abortifacient properties have been used in folk medicine. Among these are: tansy, pennyroyal, black cohosh, and the now-extinct silphium. Historian of science Ann Hibner Koblitz has written of the probable protoscientific origins of this folk knowledge in observation of farm animals. Women who knew that grazing on certain plants would cause an animal to abort (with negative economic consequences for the farm) would be likely to try out those plants on themselves in order to avoid an unwanted pregnancy.\n\nHowever, modern users of these plants often lack knowledge of the proper preparation and dosage. The historian of medicine John Riddle has spoken of the \"broken chain of knowledge\" caused by urbanization and modernization, and Koblitz has written that \"folk knowledge about effective contraception techniques often disappears over time or becomes inextricably mixed with useless or harmful practices.\" The ill-informed or indiscriminant use of herbs as abortifacients can cause serious and even lethal side-effects.\n\nBases of belief may include belief in existence of supernatural energies undetected by the science of physics, as in biofields, or in belief in properties of the energies of physics that are inconsistent with the laws of physics, as in energy medicine.\n\nSubstance based practices use substances found in nature such as herbs, foods, non-vitamin supplements and megavitamins, animal and fungal products, and minerals, including use of these products in traditional medical practices that may also incorporate other methods. Examples include healing claims for nonvitamin supplements, fish oil, Omega-3 fatty acid, glucosamine, echinacea, flaxseed oil, and ginseng. Herbal medicine, or phytotherapy, includes not just the use of plant products, but may also include the use of animal and mineral products. It is among the most commercially successful branches of alternative medicine, and includes the tablets, powders and elixirs that are sold as \"nutritional supplements\". Only a very small percentage of these have been shown to have any efficacy, and there is little regulation as to standards and safety of their contents. This may include use of known toxic substances, such as use of the poison lead in traditional Chinese medicine.\n\nA US agency, National Center on Complementary and Integrative Health (NCCIH), has created a classification system for branches of complementary and alternative medicine that divides them into five major groups. These groups have some overlap, and distinguish two types of energy medicine: \"veritable\" which involves scientifically observable energy (including magnet therapy, colorpuncture and light therapy) and \"putative\", which invokes physically undetectable or unverifiable energy. None of these energies have any evidence to support that they effect the body in any positive or health promoting way.\n\n\nThe history of alternative medicine may refer to the history of a group of diverse medical practices that were collectively promoted as \"alternative medicine\" beginning in the 1970s, to the collection of individual histories of members of that group, or to the history of western medical practices that were labeled \"irregular practices\" by the western medical establishment. It includes the histories of complementary medicine and of integrative medicine. Before the 1970s, western practitioners that were not part of the increasingly science-based medical establishment were referred to \"irregular practitioners\", and were dismissed by the medical establishment as unscientific and as practicing quackery. Until the 1970s, irregular practice became increasingly marginalized as quackery and fraud, as western medicine increasingly incorporated scientific methods and discoveries, and had a corresponding increase in success of its treatments. In the 1970s, irregular practices were grouped with traditional practices of nonwestern cultures and with other unproven or disproven practices that were not part of biomedicine, with the entire group collectively marketed and promoted under the single expression \"alternative medicine\".\n\nUse of alternative medicine in the west began to rise following the counterculture movement of the 1960s, as part of the rising new age movement of the 1970s. This was due to misleading mass marketing of \"alternative medicine\" being an effective \"alternative\" to biomedicine, changing social attitudes about not using chemicals and challenging the establishment and authority of any kind, sensitivity to giving equal measure to beliefs and practices of other cultures (cultural relativism), and growing frustration and desperation by patients about limitations and side effects of science-based medicine. At the same time, in 1975, the American Medical Association, which played the central role in fighting quackery in the United States, abolished its quackery committee and closed down its Department of Investigation. By the early to mid 1970s the expression \"alternative medicine\" came into widespread use, and the expression became mass marketed as a collection of \"natural\" and effective treatment \"alternatives\" to science-based biomedicine. By 1983, mass marketing of \"alternative medicine\" was so pervasive that the British Medical Journal (BMJ) pointed to \"an apparently endless stream of books, articles, and radio and television programmes urge on the public the virtues of (alternative medicine) treatments ranging from meditation to drilling a hole in the skull to let in more oxygen\".\n\nMainly as a result of reforms following the Flexner Report of 1910 medical education in established medical schools in the US has generally not included alternative medicine as a teaching topic. Typically, their teaching is based on current practice and scientific knowledge about: anatomy, physiology, histology, embryology, neuroanatomy, pathology, pharmacology, microbiology and immunology. Medical schools' teaching includes such topics as doctor-patient communication, ethics, the art of medicine, and engaging in complex clinical reasoning (medical decision-making). Writing in 2002, Snyderman and Weil remarked that by the early twentieth century the Flexner model had helped to create the 20th-century academic health center, in which education, research, and practice were inseparable. While this had much improved medical practice by defining with increasing certainty the pathophysiological basis of disease, a single-minded focus on the pathophysiological had diverted much of mainstream American medicine from clinical conditions that were not well understood in mechanistic terms, and were not effectively treated by conventional therapies.\n\nBy 2001 some form of CAM training was being offered by at least 75 out of 125 medical schools in the US. Exceptionally, the School of Medicine of the University of Maryland, Baltimore includes a research institute for integrative medicine (a member entity of the Cochrane Collaboration). Medical schools are responsible for conferring medical degrees, but a physician typically may not legally practice medicine until licensed by the local government authority. Licensed physicians in the US who have attended one of the established medical schools there have usually graduated Doctor of Medicine (MD). All states require that applicants for MD licensure be graduates of an approved medical school and complete the United States Medical Licensing Exam (USMLE).\n\nThere is a general scientific consensus that alternative therapies lack the requisite scientific validation, and their effectiveness is either unproved or disproved. Many of the claims regarding the efficacy of alternative medicines are controversial, since research on them is frequently of low quality and methodologically flawed. Selective publication bias, marked differences in product quality and standardisation, and some companies making unsubstantiated claims call into question the claims of efficacy of isolated examples where there is evidence for alternative therapies.\n\n\"The Scientific Review of Alternative Medicine\" points to confusions in the general population – a person may attribute symptomatic relief to an otherwise-ineffective therapy just because they are taking something (the placebo effect); the natural recovery from or the cyclical nature of an illness (the regression fallacy) gets misattributed to an alternative medicine being taken; a person not diagnosed with science-based medicine may never originally have had a true illness diagnosed as an alternative disease category.\n\nEdzard Ernst characterized the evidence for many alternative techniques as weak, nonexistent, or negative and in 2011 published his estimate that about 7.4% were based on \"sound evidence\", although he believes that may be an overestimate. Ernst has concluded that 95% of the alternative treatments he and his team studied, including acupuncture, herbal medicine, homeopathy, and reflexology, are \"statistically indistinguishable from placebo treatments\", but he also believes there is something that conventional doctors can usefully learn from the chiropractors and homeopath: this is the therapeutic value of the placebo effect, one of the strangest phenomena in medicine.\n\nIn 2003, a project funded by the CDC identified 208 condition-treatment pairs, of which 58% had been studied by at least one randomized controlled trial (RCT), and 23% had been assessed with a meta-analysis. According to a 2005 book by a US Institute of Medicine panel, the number of RCTs focused on CAM has risen dramatically.\n\n, the Cochrane Library had 145 CAM-related Cochrane systematic reviews and 340 non-Cochrane systematic reviews. An analysis of the conclusions of only the 145 Cochrane reviews was done by two readers. In 83% of the cases, the readers agreed. In the 17% in which they disagreed, a third reader agreed with one of the initial readers to set a rating. These studies found that, for CAM, 38.4% concluded positive effect or possibly positive (12.4%), 4.8% concluded no effect, 0.7% concluded harmful effect, and 56.6% concluded insufficient evidence. An assessment of conventional treatments found that 41.3% concluded positive or possibly positive effect, 20% concluded no effect, 8.1% concluded net harmful effects, and 21.3% concluded insufficient evidence. However, the CAM review used the more developed 2004 Cochrane database, while the conventional review used the initial 1998 Cochrane database.\n\nIn the same way as for conventional therapies, drugs, and interventions, it can be difficult to test the efficacy of alternative medicine in clinical trials. In instances where an established, effective, treatment for a condition is already available, the Helsinki Declaration states that withholding such treatment is unethical in most circumstances. Use of standard-of-care treatment in addition to an alternative technique being tested may produce confounded or difficult-to-interpret results.\n\nCancer researcher Andrew J. Vickers has stated:\n\nContrary to much popular and scientific writing, many alternative cancer treatments have been investigated in good-quality clinical trials, and they have been shown to be ineffective. The label \"unproven\" is inappropriate for such therapies; it is time to assert that many alternative cancer therapies have been \"disproven\".\n\nA research methods expert and author of \"Snake Oil Science\", R. Barker Bausell, has stated that \"it's become politically correct to investigate nonsense.\" There are concerns that just having NIH support is being used to give unfounded \"legitimacy to treatments that are not legitimate.\"\n\nUse of placebos to achieve a placebo effect in integrative medicine has been criticized as, \"...diverting research time, money, and other resources from more fruitful lines of investigation in order to pursue a theory that has no basis in biology.\"\n\nAnother critic has argued that academic proponents of integrative medicine sometimes recommend misleading patients by using known placebo treatments to achieve a placebo effect. However, a 2010 survey of family physicians found that 56% of respondents said they had used a placebo in clinical practice as well. Eighty-five percent of respondents believed placebos can have both psychological and physical benefits.\n\nIntegrative medicine has been criticized in that its practitioners, trained in science-based medicine, deliberately mislead patients by pretending placebos are not. \"quackademic medicine\" is a pejorative term used for \"integrative medicine\", which medical professionals consider an infiltration of quackery into academic science-based medicine.\n\nAn analysis of trends in the criticism of complementary and alternative medicine (CAM) in five prestigious American medical journals during the period of reorganization within medicine (1965–1999) was reported as showing that the medical profession had responded to the growth of CAM in three phases, and that in each phase, changes in the medical marketplace had influenced the type of response in the journals. Changes included relaxed medical licensing, the development of managed care, rising consumerism, and the establishment of the USA Office of Alternative Medicine (later National Center for Complementary and Alternative Medicine, currently National Center for Complementary and Integrative Health). In the \"condemnation\" phase, from the late 1960s to the early 1970s, authors had ridiculed, exaggerated the risks, and petitioned the state to contain CAM; in the \"reassessment\" phase (mid-1970s through early 1990s), when increased consumer utilization of CAM was prompting concern, authors had pondered whether patient dissatisfaction and shortcomings in conventional care contributed to the trend; in the \"integration\" phase of the 1990s physicians began learning to work around or administer CAM, and the subjugation of CAM to scientific scrutiny had become the primary means of control.\n\nPractitioners of complementary medicine usually discuss and advise patients as to available alternative therapies. Patients often express interest in mind-body complementary therapies because they offer a non-drug approach to treating some health conditions.\n\nIn addition to the social-cultural underpinnings of the popularity of alternative medicine, there are several psychological issues that are critical to its growth. One of the most critical is the placebo effect – a well-established observation in medicine. Related to it are similar psychological effects, such as the will to believe, cognitive biases that help maintain self-esteem and promote harmonious social functioning, and the \"post hoc, ergo propter hoc\" fallacy.\n\nThe popularity of complementary & alternative medicine (CAM) may be related to other factors that Edzard Ernst mentioned in an interview in \"The Independent\":\n\nWhy is it so popular, then? Ernst blames the providers, customers and the doctors whose neglect, he says, has created the opening into which alternative therapists have stepped. \"People are told lies. There are 40 million websites and 39.9 million tell lies, sometimes outrageous lies. They mislead cancer patients, who are encouraged not only to pay their last penny but to be treated with something that shortens their lives. \"At the same time, people are gullible. It needs gullibility for the industry to succeed. It doesn't make me popular with the public, but it's the truth.\n\nPaul Offit proposed that \"alternative medicine becomes quackery\" in four ways: by recommending against conventional therapies that are helpful, promoting potentially harmful therapies without adequate warning, draining patients' bank accounts, or by promoting \"magical thinking.\"\n\nAuthors have speculated on the socio-cultural and psychological reasons for the appeal of alternative medicines among the minority using them \"in lieu\" of conventional medicine. There are several socio-cultural reasons for the interest in these treatments centered on the low level of scientific literacy among the public at large and a concomitant increase in antiscientific attitudes and new age mysticism. Related to this are vigorous marketing of extravagant claims by the alternative medical community combined with inadequate media scrutiny and attacks on critics.\n\nThere is also an increase in conspiracy theories toward conventional medicine and pharmaceutical companies, mistrust of traditional authority figures, such as the physician, and a dislike of the current delivery methods of scientific biomedicine, all of which have led patients to seek out alternative medicine to treat a variety of ailments. Many patients lack access to contemporary medicine, due to a lack of private or public health insurance, which leads them to seek out lower-cost alternative medicine. Medical doctors are also aggressively marketing alternative medicine to profit from this market.\n\nPatients can be averse to the painful, unpleasant, and sometimes-dangerous side effects of biomedical treatments. Treatments for severe diseases such as cancer and HIV infection have well-known, significant side-effects. Even low-risk medications such as antibiotics can have potential to cause life-threatening anaphylactic reactions in a very few individuals. Many medications may cause minor but bothersome symptoms such as cough or upset stomach. In all of these cases, patients may be seeking out alternative treatments to avoid the adverse effects of conventional treatments.\n\nComplementary and alternative medicine (CAM) has been described as a broad domain of healing resources that encompasses all health systems, modalities, and practices and their accompanying theories and beliefs, other than those intrinsic to the politically dominant health system of a particular society or culture in a given historical period. CAM includes all such practices and ideas self-defined by their users as preventing or treating illness or promoting health and well-being. Boundaries within CAM and between the CAM domain and that of the dominant system are not always sharp or fixed.\n\nAccording to recent research, the increasing popularity of the CAM needs to be explained by moral convictions or lifestyle choices rather than by economic reasoning.\n\nIn developing nations, access to essential medicines is severely restricted by lack of resources and poverty. Traditional remedies, often closely resembling or forming the basis for alternative remedies, may comprise primary healthcare or be integrated into the healthcare system. In Africa, traditional medicine is used for 80% of primary healthcare, and in developing nations as a whole over one-third of the population lack access to essential medicines.\n\nSome have proposed adopting a prize system to reward medical research. However, public funding for research exists. Increasing the funding for research on alternative medicine techniques is the purpose of the US National Center for Complementary and Alternative Medicine. NCCIH and its predecessor, the Office of Alternative Medicine, have spent more than US$2.5 billion on such research since 1992; this research has largely not demonstrated the efficacy of alternative treatments.\n\nThat alternative medicine has been on the rise \"in countries where Western science and scientific method generally are accepted as the major foundations for healthcare, and 'evidence-based' practice is the dominant paradigm\" was described as an \"enigma\" in the Medical Journal of Australia.\n\nIn the United States, the 1974 Child Abuse Prevention and Treatment Act (CAPTA) required that for states to receive federal money, they had to grant religious exemptions to child neglect and abuse laws regarding religion-based healing practices. Thirty-one states have child-abuse religious exemptions.\n\nThe use of alternative medicine in the US has increased, with a 50 percent increase in expenditures and a 25 percent increase in the use of alternative therapies between 1990 and 1997 in America. Americans spend many billions on the therapies annually. Most Americans used CAM to treat and/or prevent musculoskeletal conditions or other conditions associated with chronic or recurring pain. In America, women were more likely than men to use CAM, with the biggest difference in use of mind-body therapies including prayer specifically for health reasons\". In 2008, more than 37% of American hospitals offered alternative therapies, up from 27 percent in 2005, and 25% in 2004. More than 70% of the hospitals offering CAM were in urban areas.\n\nA survey of Americans found that 88 percent thought that \"there are some good ways of treating sickness that medical science does not recognize\". Use of magnets was the most common tool in energy medicine in America, and among users of it, 58 percent described it as at least \"sort of scientific\", when it is not at all scientific. In 2002, at least 60 percent of US medical schools have at least some class time spent teaching alternative therapies. \"Therapeutic touch\" was taught at more than 100 colleges and universities in 75 countries before the practice was debunked by a nine-year-old child for a school science project.\n\nThe most common CAM therapies used in the US in 2002 were prayer (45%), herbalism (19%), breathing meditation (12%), meditation (8%), chiropractic medicine (8%), yoga (5–6%), body work (5%), diet-based therapy (4%), progressive relaxation (3%), mega-vitamin therapy (3%) and Visualization (2%)\n\nIn Britain, the most often used alternative therapies were Alexander technique, Aromatherapy, Bach and other flower remedies, Body work therapies including massage, Counseling stress therapies, hypnotherapy, Meditation, Reflexology, Shiatsu, Ayurvedic medicine, Nutritional medicine, and Yoga. Ayurvedic medicine remedies are mainly plant based with some use of animal materials. Safety concerns include the use of herbs containing toxic compounds and the lack of quality control in Ayurvedic facilities.\n\nAccording to the National Health Service (England), the most commonly used complementary and alternative medicines (CAM) supported by the NHS in the UK are: acupuncture, aromatherapy, chiropractic, homeopathy, massage, osteopathy and clinical hypnotherapy.\n\nComplementary therapies are often used in palliative care or by practitioners attempting to manage chronic pain in patients. Integrative medicine is considered more acceptable in the interdisciplinary approach used in palliative care than in other areas of medicine. \"From its early experiences of care for the dying, palliative care took for granted the necessity of placing patient values and lifestyle habits at the core of any design and delivery of quality care at the end of life. If the patient desired complementary therapies, and as long as such treatments provided additional support and did not endanger the patient, they were considered acceptable.\" The non-pharmacologic interventions of complementary medicine can employ mind-body interventions designed to \"reduce pain and concomitant mood disturbance and increase quality of life.\"\n\nIn Austria and Germany complementary and alternative medicine is mainly in the hands of doctors with MDs, and half or more of the American alternative practitioners are licensed MDs. In Germany herbs are tightly regulated: half are prescribed by doctors and covered by health insurance.\n\nSome professions of complementary/traditional/alternative medicine, such as chiropractic, have achieved full regulation in North America and other parts of the world and are regulated in a manner similar to that governing science-based medicine. In contrast, other approaches may be partially recognized and others have no regulation at all. Regulation and licensing of alternative medicine ranges widely from country to country, and state to state.\n\nGovernment bodies in the US and elsewhere have published information or guidance about alternative medicine. The U.S. Food and Drug Administration (FDA), has issued online warnings for consumers about medication health fraud. This includes a section on Alternative Medicine Fraud, such as a warning that Ayurvedic products generally have not been approved by the FDA before marketing.\n\nMany of the claims regarding the safety and efficacy of alternative medicine are controversial. Some alternative treatments have been associated with unexpected side effects, which can be fatal.\n\nA commonly voiced concerns about complementary alternative medicine (CAM) is the way it's regulated. There have been significant developments in how CAMs should be assessed prior to re-sale in the United Kingdom and the European Union (EU) in the last 2 years. Despite this, it has been suggested that current regulatory bodies have been ineffective in preventing deception of patients as many companies have re-labelled their drugs to avoid the new laws. There is no general consensus about how to balance consumer protection (from false claims, toxicity, and advertising) with freedom to choose remedies.\n\nAdvocates of CAM suggest that regulation of the industry will adversely affect patients looking for alternative ways to manage their symptoms, even if many of the benefits may represent the placebo affect. Some contend that alternative medicines should not require any more regulation than over-the-counter medicines that can also be toxic in overdose (such as paracetamol).\n\nForms of alternative medicine that are biologically active can be dangerous even when used in conjunction with conventional medicine. Examples include immuno-augmentation therapy, shark cartilage, bioresonance therapy, oxygen and ozone therapies, and insulin potentiation therapy. Some herbal remedies can cause dangerous interactions with chemotherapy drugs, radiation therapy, or anesthetics during surgery, among other problems. An example of these dangers was reported by Associate Professor Alastair MacLennan of Adelaide University, Australia regarding a patient who almost bled to death on the operating table after neglecting to mention that she had been taking \"natural\" potions to \"build up her strength\" before the operation, including a powerful anticoagulant that nearly caused her death.\n\nTo \"ABC Online\", MacLennan also gives another possible mechanism:\n\nAnd lastly there's the cynicism and disappointment and depression that some patients get from going on from one alternative medicine to the next, and they find after three months the placebo effect wears off, and they're disappointed and they move on to the next one, and they're disappointed and disillusioned, and that can create depression and make the eventual treatment of the patient with anything effective difficult, because you may not get compliance, because they've seen the failure so often in the past.\n\nConventional treatments are subjected to testing for undesired side-effects, whereas alternative treatments, in general, are not subjected to such testing at all. Any treatment – whether conventional or alternative – that has a biological or psychological effect on a patient may also have potential to possess dangerous biological or psychological side-effects. Attempts to refute this fact with regard to alternative treatments sometimes use the \"appeal to nature\" fallacy, i.e., \"That which is natural cannot be harmful.\" Specific groups of patients such as patients with impaired hepatic or renal function are more susceptible to side effects of alternative remedies.\n\nAn exception to the normal thinking regarding side-effects is Homeopathy. Since 1938, the U.S. Food and Drug Administration (FDA) has regulated homeopathic products in \"several significantly different ways from other drugs.\" Homeopathic preparations, termed \"remedies\", are extremely dilute, often far beyond the point where a single molecule of the original active (and possibly toxic) ingredient is likely to remain. They are, thus, considered safe on that count, but \"their products are exempt from good manufacturing practice requirements related to expiration dating and from finished product testing for identity and strength\", and their alcohol concentration may be much higher than allowed in conventional drugs.\n\nThose having experienced or perceived success with one alternative therapy for a minor ailment may be convinced of its efficacy and persuaded to extrapolate that success to some other alternative therapy for a more serious, possibly life-threatening illness. For this reason, critics argue that therapies that rely on the placebo effect to define success are very dangerous. According to mental health journalist Scott Lilienfeld in 2002, \"unvalidated or scientifically unsupported mental health practices can lead individuals to forgo effective treatments\" and refers to this as \"opportunity cost\". Individuals who spend large amounts of time and money on ineffective treatments may be left with precious little of either, and may forfeit the opportunity to obtain treatments that could be more helpful. In short, even innocuous treatments can indirectly produce negative outcomes. Between 2001 and 2003, four children died in Australia because their parents chose ineffective naturopathic, homeopathic, or other alternative medicines and diets rather than conventional therapies.\n\nThere have always been \"many therapies offered outside of conventional cancer treatment centers and based on theories not found in biomedicine. These alternative cancer cures have often been described as 'unproven,' suggesting that appropriate clinical trials have not been conducted and that the therapeutic value of the treatment is unknown.\" However, \"many alternative cancer treatments have been investigated in good-quality clinical trials, and they have been shown to be ineffective...The label 'unproven' is inappropriate for such therapies; it is time to assert that many alternative cancer therapies have been 'disproven'.\"\n\nEdzard Ernst has stated:\n\n...any alternative cancer cure is bogus by definition. There will never be an alternative cancer cure. Why? Because if something looked halfway promising, then mainstream oncology would scrutinize it, and if there is anything to it, it would become mainstream almost automatically and very quickly. All curative \"alternative cancer cures\" are based on false claims, are bogus, and, I would say, even criminal.\n\n\"CAM\", meaning \"complementary and alternative medicine\", is not as well researched as conventional medicine, which undergoes intense research before release to the public. Funding for research is also sparse making it difficult to do further research for effectiveness of CAM. Most funding for CAM is funded by government agencies. Proposed research for CAM are rejected by most private funding agencies because the results of research are not reliable. The research for CAM has to meet certain standards from research ethics committees, which most CAM researchers find almost impossible to meet. Even with the little research done on it, CAM has not been proven to be effective.\n\nSteven Novella, a neurologist at Yale School of Medicine, wrote that government funded studies of integrating alternative medicine techniques into the mainstream are \"used to lend an appearance of legitimacy to treatments that are not legitimate.\" Marcia Angell considered that critics felt that healthcare practices should be classified based solely on scientific evidence, and if a treatment had been rigorously tested and found safe and effective, science-based medicine will adopt it regardless of whether it was considered \"alternative\" to begin with. It is possible for a method to change categories (proven vs. unproven), based on increased knowledge of its effectiveness or lack thereof. A prominent supporter of this position is George D. Lundberg, former editor of the Journal of the American Medical Association (JAMA).\n\nWriting in 1999 in \"CA: A Cancer Journal for Clinicians\" Barrie R. Cassileth mentioned a 1997 letter to the US Senate Subcommittee on Public Health and Safety, which had deplored the lack of critical thinking and scientific rigor in OAM-supported research, had been signed by four Nobel Laureates and other prominent scientists. (This was supported by the National Institutes of Health (NIH).)\n\nIn March 2009 a staff writer for \"the Washington Post\" reported that the impending national discussion about broadening access to health care, improving medical practice and saving money was giving a group of scientists an opening to propose shutting down the National Center for Complementary and Alternative Medicine. They quoted one of these scientists, Steven Salzberg, a genome researcher and computational biologist at the University of Maryland, as saying \"One of our concerns is that NIH is funding pseudoscience.\" They noted that the vast majority of studies were based on fundamental misunderstandings of physiology and disease, and had shown little or no effect.\n\nWriters such as Carl Sagan, a noted astrophysicist, advocate of scientific skepticism and the author of \"The Demon-Haunted World: Science as a Candle in the Dark\" (1996), have lambasted the lack of empirical evidence to support the existence of the putative energy fields on which these therapies are predicated.\n\nSampson has also pointed out that CAM tolerated contradiction without thorough reason and experiment. Barrett has pointed out that there is a policy at the NIH of never saying something doesn't work only that a different version or dose might give different results. Barrett also expressed concern that, just because some \"alternatives\" have merit, there is the impression that the rest deserve equal consideration and respect even though most are worthless, since they are all classified under the one heading of alternative medicine.\n\nSome critics of alternative medicine are focused upon health fraud, misinformation, and quackery as public health problems, notably Wallace Sampson and Paul Kurtz founders of Scientific Review of Alternative Medicine and Stephen Barrett, co-founder of The National Council Against Health Fraud and webmaster of Quackwatch. Grounds for opposing alternative medicine include that:\n\nMany alternative medical treatments are not patentable, which may lead to less research funding from the private sector. In addition, in most countries, alternative treatments (in contrast to pharmaceuticals) can be marketed without any proof of efficacy – also a disincentive for manufacturers to fund scientific research.\n\nEnglish evolutionary biologist Richard Dawkins, in his 2003 book \"A Devil's Chaplain\", defined alternative medicine as a \"set of practices that cannot be tested, refuse to be tested, or consistently fail tests.\" Dawkins argued that if a technique is demonstrated effective in properly performed trials then it ceases to be alternative and simply becomes medicine.\n\nCAM is also often less regulated than conventional medicine. There are ethical concerns about whether people who perform CAM have the proper knowledge to treat patients. CAM is often done by non-physicians who do not operate with the same medical licensing laws which govern conventional medicine, and it is often described as an issue of non-maleficence.\n\nAccording to two writers, Wallace Sampson and K. Butler, marketing is part of the training required in alternative medicine, and propaganda methods in alternative medicine have been traced back to those used by Hitler and Goebels in their promotion of pseudoscience in medicine.\n\nIn November 2011 Edzard Ernst stated that the \"level of misinformation about alternative medicine has now reached the point where it has become dangerous and unethical. So far, alternative medicine has remained an ethics-free zone. It is time to change this.\"\n\nSome commentators have said that special consideration must be given to the issue of conflicts of interest in alternative medicine. Edzard Ernst has said that most researchers into alternative medicine are at risk of \"unidirectional bias\" because of a generally uncritical belief in their chosen subject. Ernst cites as evidence the phenomenon whereby 100% of a sample of acupuncture trials originating in China had positive conclusions. David Gorski contrasts evidence-based medicine, in which researchers try to disprove hyphotheses, with what he says is the frequent practice in pseudoscience-based research, of striving to confirm pre-existing notions. Harriet Hall writes that there is a contrast between the circumstances of alternative medicine practitioners and disinterested scientists: in the case of acupuncture, for example, an acupuncturist would have \"a great deal to lose\" if acupuncture were rejected by research; but the disinterested skeptic would not lose anything if its effects were confirmed; rather their change of mind would enhance their skeptical credentials.\n\n\n\n\n\n\n\n"}
{"id": "6793192", "url": "https://en.wikipedia.org/wiki?curid=6793192", "title": "Anecdotal cognitivism", "text": "Anecdotal cognitivism\n\nAnecdotal cognitivism is a psychological methodology that attributes mental states to animals on the basis of anecdotes and on the observation of particular cases, other than those observations made during controlled experiments. It is opposed to behaviorism; behaviorists are critical of anecdotal cognitivism, suggesting that controlled experiments are necessary to measure stimuli and record observable behavior. \n\nAnecdotal cognitivism is often criticized by behaviorists using specific cases, such as that of Clever Hans, to discredit using anecdotal evidence in assessing animal cognition. In the case of Clever Hans, a horse was purported to be able to add and subtract using its hooves, and even answer questions surrounding European politics, but it was determined by later research that the horse's owner was, in fact, unknowingly cueing the horse, and that when he was removed from the room, the horse would not respond.\n\nAnecdotal cognitivists respond to behaviorists by saying that behaviorism would have the animals 'lose their minds' and that it is clear that, by observation, they can know a great deal about the cognitive processes of animals. They add that the debate can start with simple observation, rather than in a controlled setting or in a lab.\n\n\n"}
{"id": "25611728", "url": "https://en.wikipedia.org/wiki?curid=25611728", "title": "Aromatherapy", "text": "Aromatherapy\n\nAromatherapy uses plant materials and aromatic plant oils, including essential oils, and other aroma compounds for improving psychological or physical well-being.\n\nIt can be offered as a complementary therapy or as a form of alternative medicine. Complementary therapy can be offered alongside standard treatment, with alternative medicine offered instead of conventional, evidence-based treatments.\n\nAromatherapists, who specialize in the practice of aromatherapy, utilize blends of therapeutic essential oils that can be issued through topical application, massage, inhalation or water immersion to stimulate a desired response.\n\nThere is no good medical evidence that aromatherapy can either prevent or cure any disease. There is some evidence that it is more effective than placebo in combating postoperative nausea and vomiting, but also that it is less effective than standard anti-emetic drugs. \n\nThe use of essential oils for therapeutic, spiritual, hygienic and ritualistic purposes goes back to a number of ancient civilizations including the Chinese, Indians, Egyptians, Greeks, and Romans who used them in cosmetics, perfumes and drugs.\nOils were used for aesthetic pleasure and in the beauty industry. It was a luxury item and a means of payment. It was believed the essential oils increased the shelf life of wine and improved the taste of food.\n\nOils are described by Dioscorides, along with beliefs of the time regarding their healing properties, in his \"De Materia Medica\", written in the first century. Distilled essential oils have been employed as medicines since the eleventh century, when Avicenna isolated essential oils using steam distillation.\n\nThe concept of aromatherapy was first mooted by a small number of European scientists and doctors, in about 1907. In 1937, the word first appeared in print in a French book on the subject: \"Aromathérapie: Les Huiles Essentielles, Hormones Végétales\" by , a chemist. An English version was published in 1993. In 1910, Gattefossé burned a hand very badly and later claimed he treated it effectively with lavender oil.\n\nA French surgeon, , pioneered the medicinal uses of essential oils, which he used as antiseptics in the treatment of wounded soldiers during World War II.\n\nThe modes of application of aromatherapy include:\n\nSome of the materials employed include:\n\nAromatherapy is the treatment or prevention of disease by use of essential oils. Other stated uses include pain and anxiety reduction, enhancement of energy and short-term memory, relaxation, hair loss prevention, and reduction of eczema-induced itching.\n\nTwo basic mechanisms are offered to explain the purported effects. One is the influence of aroma on the brain, especially the limbic system through the olfactory system. The other is the direct pharmacological effects of the essential oils.\n\nIn the English-speaking world, practitioners tend to emphasize the use of oils in massage. Aromatherapy tends to be regarded as a pseudoscientific fraud at worst.\n\nOils with standardized content of components (marked FCC, for Food Chemicals Codex) are required to contain a specified amount of certain aroma chemicals that normally occur in the oil. There is no law that the chemicals cannot be added in synthetic form to meet the criteria established by the FCC for that oil. For instance, lemongrass essential oil must contain 75% aldehyde to meet the FCC profile for that oil, but that aldehyde can come from a chemical refinery instead of from lemongrass. To say that FCC oils are \"food grade\" makes them seem natural when they are not necessarily so.\n\nUndiluted essential oils suitable for aromatherapy are termed 'therapeutic grade', but there are no established and agreed standards for this category.\n\nAnalysis using gas liquid chromatography (GLC) and mass spectrometry (MS) establishes the quality of essential oils. These techniques are able to measure the levels of components to a few parts per billion. This does not make it possible to determine whether each component is natural or whether a poor oil has been 'improved' by the addition of synthetic aromachemicals, but the latter is often signaled by the minor impurities present. For example, linalool made in plants will be accompanied by a small amount of hydro-linalool, whilst synthetic linalool has traces of dihydro-linalool.\n\nThere is no good medical evidence that aromatherapy can prevent or cure any disease. In 2015, the Australian Government's Department of Health published the results of a review of alternative therapies that sought to determine if any were suitable for being covered by health insurance; aromatherapy was one of 17 therapies evaluated for which no clear evidence of effectiveness was found. Evidence for the efficacy of aromatherapy in treating medical conditions is poor, with a particular lack of studies employing rigorous methodology. A number of systematic reviews have studied the clinical effectiveness of aromatherapy in respect to pain management in labor, the treatment of post-operative nausea and vomiting, managing behaviors that challenge in dementia, and symptom relief in cancer. All of these reviews report a lack of evidence on the effectiveness of aromatherapy. Studies were found to be of low quality, meaning more well-designed, large scale randomized controlled trials are needed before clear conclusions can be drawn as to the effectiveness of aromatherapy.\n\nAromatherapy carries a risk of a number of adverse effects and this consideration, combined with the lack of evidence of its therapeutic benefit, makes the practice of questionable worth.\n\nBecause essential oils are highly concentrated they can irritate the skin when used in undiluted form. Therefore, they are normally diluted with a carrier oil for topical application, such as jojoba oil, olive oil, or coconut oil. Phototoxic reactions may occur with citrus peel oils such as lemon or lime. Also, many essential oils have chemical components that are sensitisers (meaning that they will, after a number of uses, cause reactions on the skin, and more so in the rest of the body). Some of the chemical allergies could even be caused by pesticides, if the original plants are cultivated. Some oils can be toxic to some domestic animals, with cats being particularly prone.\n\nA child hormone specialist at the University of Cambridge claimed \"... these oils can mimic estrogens\" and \"people should be a little bit careful about using these products.\" The Aromatherapy Trade Council of the UK has issued a rebuttal.\nThe Australian Tea Tree Association, a group that promotes the interests of Australian tea tree oil producers, exporters and manufacturers issued a letter that questioned the study and called on the \"New England Journal of Medicine\" for a retraction.\nThe \"New England Journal of Medicine\" has so far not replied and has not retracted the study.\n\nAs with any bioactive substance, an essential oil that may be safe for the general public could still pose hazards for pregnant and lactating women.\n\nWhile some advocate the ingestion of essential oils for therapeutic purposes, licensed aromatherapy professionals do not recommend self-prescription due to the highly toxic nature of some essential oils. Some very common oils like eucalyptus are extremely toxic when taken internally. Doses as low as one teaspoon have been reported to cause clinically significant symptoms and severe poisoning can occur after ingestion of 4 to 5 ml.\nA few reported cases of toxic reactions like liver damage and seizures have occurred after ingestion of sage, hyssop, thuja, and cedar. Accidental ingestion may happen when oils are not kept out of reach of children.\n\nOils both ingested and applied to the skin can potentially have negative interactions with conventional medicine. For example, the topical use of methyl salicylate-heavy oils like sweet birch and wintergreen may cause bleeding in users taking the anticoagulant warfarin.\n\nAdulterated oils may also pose problems depending on the type of substance used.\n\n\n"}
{"id": "18674371", "url": "https://en.wikipedia.org/wiki?curid=18674371", "title": "Atmospheric correction", "text": "Atmospheric correction\n\nAtmospheric correction is the process of removing the effects of the atmosphere on the reflectance values of images taken by satellite or airborne sensors.\n\n\n"}
{"id": "32524419", "url": "https://en.wikipedia.org/wiki?curid=32524419", "title": "Baunscheidtism", "text": "Baunscheidtism\n\nBaunscheidtism is a form of alternative medicine created in the 19th century. The practice, a form of homeopathy, is named for its founder Carl Baunscheidt (1809–1873), a German mechanic and inventor.\nThe legitimacy of baunscheidtism as an effective medical practice was questioned by at least 1880, when a Melbourne practitioner named Samuel Fischer lost a lawsuit he brought against a patient who failed to pay him, based on the objection that Fischer (a bootmaker) was not a qualified medical practitioner.\n\nThe lebenswecker (\"life awakener\") or \"artificial leech\" was a medical device invented by Baunscheidt to pierce the skin with many fine needles. Billed as being able to cure myriad illnesses, the lebenswecker was used on skin treated with toxic oil. The resulting inflammation was alleged to draw the body's attention away from the patient's illness, thus effecting a cure. The diseases that could allegedly be cured with the lebenswecker included whooping cough, baldness, toothaches, and mental disorders. The device's popularity was great enough to support a market for \"counterfeit\" versions of the lebenswecker that were produced by Baunscheidt.\n"}
{"id": "43605222", "url": "https://en.wikipedia.org/wiki?curid=43605222", "title": "COART", "text": "COART\n\nCOART (Coupled Ocean-Atmospheric Radiative Transfer code) - COART is established on the Coupled DIScrete Ordinate Radiative Transfer (Coupled DISORT or CDISORT) code, developed from DISORT. It is designed to simulate radiance (including water-leaving radiance) and irradiance (flux) at any levels in the atmosphere and ocean consistently.\n\n\nJin, Z., T.P. Charlock, K. Rutledge, K. Stamnes, and Y. Wang, An analytical solution of radiative transfer in the coupled atmosphere-ocean system with rough surface. Appl. Opt., 45, 7443-7455, 2006.\n\nJin, Z., and K. Stamnes, Radiative transfer in nonuniformly refracting layered media: atmosphere-ocean system, Appl. Opt., 33, 431-442, 1994.\n\n"}
{"id": "2259059", "url": "https://en.wikipedia.org/wiki?curid=2259059", "title": "Chronometry", "text": "Chronometry\n\nChronometry (from Greek χρόνος \"chronos\", \"time\" and μέτρον \"metron\", \"measure\") is the science of the measurement of time, or timekeeping. Chronometry applies to electronic devices, while horology refers to mechanical devices.\n\nIt should not to be confused with chronology, the science of locating events in time, which often relies upon it.\n\n"}
{"id": "49602444", "url": "https://en.wikipedia.org/wiki?curid=49602444", "title": "DU spectrophotometer", "text": "DU spectrophotometer\n\nThe DU spectrophotometer or Beckman DU, introduced in 1941, was the first commercially viable scientific instrument for measuring the amount of ultraviolet light absorbed by a substance. This model of spectrophotometer enabled scientists to easily examine and identify a given substance based on its absorption spectrum, the pattern of light absorbed at different wavelengths. Arnold O. Beckman's National Technical Laboratories (later Beckman Instruments) developed three in-house prototype models (A, B, C) and one limited distribution model (D) before moving to full commercial production with the DU. Approximately 30,000 DU spectrophotometers were manufactured and sold between 1941 and 1976.\n\nSometimes referred to as a UV–Vis spectrophotometer because it measured both the ultraviolet (UV) and visible spectra, the DU spectrophotometer is credited as being a truly revolutionary technology. It yielded more accurate results than previous methods for determining the chemical composition of a complex substance, and substantially reduced the time needed for an accurate analysis from weeks or hours to minutes. The Beckman DU was essential to several critical secret research projects during World War II, including the development of penicillin and synthetic rubber.\n\nBefore the development of the DU spectrophotometer, analysis of a test sample to determine its components was a long, costly, and often inaccurate process. A classical wet laboratory contained a wide variety of complicated apparatus. Test samples were run through a series of awkward and time-consuming qualitative processes to separate out and identify their components. Determining quantitative concentrations of those components in the sample involved further steps. Processes could involve techniques for chemical reactions, precipitations, filtrations and dissolutions. Determination of the concentrations of known impurities in a known inorganic substance such as molten iron could be done in under thirty minutes. The determination of complex organic structures such as chlorophyll using wet and dry methods could take decades. \n\nSpectroscopic methods for observing the absorption of electromagnetic radiation in the visible spectrum were known as early as the 1860s.\nScientists had observed that light traveling through a medium would be absorbed at different wavelengths, depending on the matter-composition of the medium involved. A white light source would emit light at multiple wavelengths over a range of frequencies. A prism could be used to separate a light source into specific wavelengths. Shining the light through a sample of a material would cause some wavelengths of light to be absorbed, while others would be unaffected and continue to be transmitted. Wavelengths in the resulting absorption spectrum would differ depending upon the atomic and molecular composition if the material involved.\n\nSpectroscopic methods were predominantly used by physicists and astrophysicists. Spectroscopic techniques were rarely taught in chemistry classes and were unfamiliar to most practicing chemists. Beginning around 1904, Frank Twyman of the London instrument making firm Adam Hilger, Ltd. tried to develop spectroscopic instruments for chemists, but his customer base was consistently made up of physicists rather than chemists.\n\nBy the 1940s, both academic and industrial chemists were becoming increasingly interested in problems involving the composition and detection of biological molecules. Biological molecules, including proteins and nucleic acids, absorb light energy in both the ultraviolet and visible range. The spectrum of visible light was not broad enough to enable scientists to examine substances such as vitamin A. Accurate characterization of complex samples, particularly of biological materials, would require the accurate reading of absorption frequencies in the ultraviolet and infrared (IR) sections of the spectrum in addition to visible light. Existing instruments such as the Cenco \"Spectrophotelometer\" and the Coleman Model DM Spectrophotometer could not be effectively used to examine wavelengths in the ultraviolet range.\n\nThe array of equipment needed to measure light energy reaching beyond the visible spectrum towards the ultraviolet could cost a laboratory as much as $3,000, a huge amount in 1940. Repeated readings of a sample were taken to produce photographic plates showing the absorption spectrum of a material at different wavelengths. An experienced human could compare these to the known images to identify a match. Then information from the plates had to be combined to create a graph showing the spectrum as a whole. Ultimately, the accuracy of such approaches was dependent on accurate, consistent development of the photographic plates, and on human visual acuity and practice in reading the wavelengths.\n\nThe DU was developed at National Technical Laboratories (later Beckman Instruments) under the direction of Arnold Orville Beckman, an American chemist and inventor. Beginning in 1940, National Technical Laboratories developed three in-house prototype models (A, B, C) and one limited distribution model (D) before moving to full commercial production with the DU in 1941. Beckman's research team was led by Howard Cary, who went on to co-found Applied Physics Corporation (later Cary Instruments) which became one of Beckman Instruments' strongest competitors. Other scientists included Roland Hawes and Kenyon George.\n\nColeman Instruments had recently coupled a pH meter with an optical phototube unit to examine the visual spectrum (the Coleman Model DM). Beckman had already developed a successful pH meter for measuring acidity of solutions, his company's breakthrough product. Seeing the potential to build upon their existing expertise, Beckman made it a goal to create an easy-to-use integrated instrument which would both register and report specific wavelengths extending into the ultraviolet range. Rather than depending on development of photographic plates, or a human observer's visual ability to detect wavelengths in the absorption spectrum, phototubes would be used to register and report the specific wavelengths that were detected. This had the potential to increase the instrument's accuracy and reliability as well as its speed and ease of use.\n\nThe first prototype Beckman spectrophotometer, the Model A, was created at National Technologies Laboratories in 1940. It used a tungsten light source with a glass Fery prism as a monochromator. Tungsten was used for incandescent light filaments because it was strong, withstood heat, and emitted a steady light. Types of light sources differed in the range of wavelengths of light that they emitted. Tungsten lamps were useful in the visible light range but gave poor coverage in the ultraviolet range. However, they had the advantage of being readily available because they were used as automobile headlamps. An external amplifier from the Beckman pH meter and a vacuum tube photocell were used to detect wavelengths.\n\nIt was quickly realized that a glass dispersive prism was not suitable for use in the ultraviolet spectrum. Glass absorbed electromagnetic radiation below 400 millimicrons rather than dispersing it. In the Model B, a quartz prism was substituted for the earlier glass.\n\nA tangent bar mechanism was used to adjust the monochromator. The mechanism was highly sensitive and required a skilled operator. Only two Model B prototypes were made. One was sold: in February 1941, to the University of California Chemistry department in Los Angeles.\n\nThe Model B prototype should be distinguished from a later production model of spectrophotometer that was also referred to as the Model \"B\". The production Model \"B\" was introduced in 1949 as a less-expensive, simple-to-use alternative to the Beckman DU. It used a glass Fery prism as a chromator and operated in a narrower range, roughly from 320 millimicrons to 950 millimicrons, and 5 to 20 Å.\n\nThree Model C instruments were then built, improving the instrument's wavelength resolution. The Model B's rotary cell compartment was replaced with a linear sample chamber. The tangent bar mechanism was replaced by a scroll drive mechanism, which could be more precisely controlled to reset the quartz prism and select the desired wavelength. With this new mechanism, results could be more easily and reliably obtained, without requiring a highly skilled operator. This set the pattern for all of Beckman's later quartz prism instruments. Although only three Model B prototypes were built, all were sold, one to Caltech and the other two to companies in the food industry.\n\nThe A, B, and C prototype models all coupled an external Beckman pH meter to the optical component to obtain readouts. In developing the Model D, Beckman took the direct-coupled amplifier circuit from the pH meter and combined the optical and electronic components in a single housing, making it more economical.\n\nMoving from a prototype to production of the Model D involved challenges. \nBeckman originally approached Bausch and Lomb about making quartz prisms for the spectrophotometer. When they turned down the opportunity, National Technical Laboratories designed its own optical system, including both a control mechanism and a quartz prism. Large, high optical quality quartz suitable for creating prisms was difficult to obtain. It came from Brazil, and was in demand for wartime radio oscillators. Beckman had to obtain a wartime priority listing for the spectrophotometer to get access to suitable quartz supplies.\n\nBeckman had previously attempted to find a source of reliable hydrogen lamps, seeking better sensitivity to wavelengths in the ultraviolet range than was possible with tungsten. As described in July 1941, the Beckman spectrophotomter could use a \"hot cathode hydrogen discharge tube\" or a tungsten light source interchangeably. However, Beckman was still unsatisfied with the available hydrogen lamps. National Technical Laboratories designed its own hydrogen lamp, an anode enclosed in a thin blown-glass window. By December 1941, the in-house design was being used in production of the Model D.\n\nThe instrument's design also required a more sensitive phototube than was commercially available at that time. Beckman was able to obtain small batches of an experimental phototube from RCA for the first Model D instruments.\n\nThe Model D spectrophotometer, using the experimental RCA phototube, was shown at MIT's Summer Conference on Spectroscopy in July 1941. The paper that Cary and Beckman presented there was published in the \"Journal of the Optical Society of America\". In it, Cary and Beckman compared designs for a modified self-collimating quartz Fery prism, a mirror-collimated quartz Littrow prism, and various gratings. The Littrow prism was a half-prism, which had a mirrored face. Use of a tungsten light source with the quartz Littrow prism as a monochromator was reported to minimize light scattering within the instrument.\n\nThe Model D was the first model to enter actual production. A small number of Model D instruments were sold, beginning in July 1941, before it was superseded by the DU.\n\nWhen RCA could not meet Beckman's demand for experimental phototubes, National Technical Laboratories again had to design its own components in-house. They developed a pair of phototubes, sensitive to the red and blue areas of the spectrum, capable of amplifying the signals they received. With the incorporation of Beckman's UV-sensitive phototubes, the Model D became the Model DU UV–Vis spectrophotometer. Its designation as a \"UV–Vis\" spectrophotometer indicates its ability to measure light in both the visible and ultraviolet spectra.\n\nThe DU was the first commercially viable scientific instrument for measuring the amount of ultraviolet light absorbed by a substance. As he had done with the pH meter, Beckman had replaced an array of complicated equipment with a single, easy-to-use instrument. One of the first fully integrated instruments or \"black boxes\" used in modern chemical laboratories, it sold for $723 in 1941.\n\nIt is generally assumed that the \"DU\" in the name was a combination of \"D\" for the Model D on which it was based, and \"U\" for the ultraviolet spectrum. However, it has been suggested that \"DU\" may also reference Beckman's fraternity at the University of Illinois, Delta Upsilon, whose members were called \"DU\"s.\n\nA publication in the scholarly literature compared the optical quality of the DU to the Cary 14 Spectrophotometer, another leading UV–Vis spectrophotometer of the time.\n\n<BR>\nFrom 1941 until 1976, when it was discontinued, the Model DU spectrophotometer was built upon what was essentially the same design. It was a single beam instrument.\nThe DU spectrophotometers used a quartz prism to separate light from a lamp into its absorption spectrum and a phototube to electrically measure the light energy across the spectrum. This allowed the user to plot the light absorption spectrum of a substance to obtain a standardized \"fingerprint\" characteristic of a compound. All modern UV–Vis spectrophotometer are built on the same basic principles as the DU spectrophotometer.\n\n<BR>\n\nAlthough the default light source for the instrument was tungsten, a hydrogen or mercury lamp could be substituted depending on the optimal range of measurement for which the instrument was to be used. The tungsten lamp was suitable for transmittance of wavelengths between 320 and 1000 millimicrons; the hydrogen lamp for 220 to 320 millimicrons, and the mercury lamp for checking the calibration of the spectrophotometer.\n\n<BR>\nAs advertised in the 1941 News Edition of the American Chemical Society, the Beckman Spectrophotometer used an autocollimating quartz crystal prism for a monochromator, capable of covering a range from the ultraviolet (200 millimicrons) to the infrared (2000 millimicrons), with a nominal bandwidth of 2 millimicrons or less for most of its spectral range. The slit mechanism was continuously adjustable from .01 to 2.0 mm and claimed to have less than 1/10% of stray light over most of the spectral range. It featured an easy-to-read wavelength scale, simultaneously reporting % Transmission and Density information.\n\nThe sample holder held up to 4 cells. Cells could be moved into the light path via an external control, allowing the user to take multiple readings without opening the cell compartment. As described in the DU's manual, absorbance measurements of a sample were made in comparison to a blank, or standard, \"a solution identical in composition with the sample except that the absorbing material being measured is absent.\" The standard could be a cell filled with a solvent such as distilled water or a prepared solvent of a known concentration. At each wavelength two measurements are made: with the sample and with the standard in the light beam. This enables the ratio, transmittance, to be obtained. For quantitative measurements transmittance is converted to absorbance which is proportional to the solute concentration according to Beer's law. This makes possible the quantitative determination of the amount of a substance in solution.\n\nThe user could also switch between phototubes without removing the sample holder. A 1941 advertisement indicates that three types of phototubes were available, with maximum sensitivity to red, blue and ultraviolet light ranges.\n\nThe 1954 DU spectrophotometer differs in that it claims to be useful from 200 to 1000 millimicrons, and does not mention the ultraviolet phototube. The wavelength selector, however, still ranged from 200 to 2000 millimicrons. and an \"Ultraviolet accessory set\" was available. This shift away from using the DU for infrared measurement is understandable, since by 1954 Beckman Instruments was marketing a separate infrared spectrophotometer. Beckman developed the IR-1 infrared spectrophotometer during World War II, and redesigned it as the IR-4 between 1953 and 1956.\n\nThe Beckman spectrophotometer was the first easy-to-use single instrument containing both the optical and electronic components needed for ultraviolet-absorption spectrophotometry within a single housing. The user could insert a cell tray with standard and sample cells, dial up the desired wavelength of light, confirm that the instrument was properly set by measuring the standard, and then measure the amount of absorption of the sample, reading the frequency from a simple meter. A series of readings at different wavelengths could be taken without disturbing the sample. The DU spectrophotometer's manual scanning method was extremely fast, reducing analysis times from weeks or hours to minutes.\n\nIt was accurate in both the visible and ultraviolet ranges. \nWorking in both the ultraviolet and the visible regions of the spectrum, the model DU produced accurate absorption spectra which could be obtained with relative ease and accurately replicated. The National Bureau of Standards ran tests to certify that the DU's results were accurate and repeatable and recommended its use.\n\nOther advantages included its high resolution and the minimization of stray light in the ultraviolet region. Although it was not cheap, its initial price of $723 made it available to the average laboratory. In comparison, in 1943, the GE Hardy Spectrophotometer cost $6,400. Practical and reliable, the DU rapidly established itself as a standard for laboratory equipment.\n\nCredited with having \"brought about a breakthrough in optical spectroscopy\", the Beckman DU has been identified as \"an indispensable tool for chemistry\" and \"the Model T of laboratory instruments\". Approximately 30,000 DU spectrophotometers were manufactured and sold between 1941 and 1976.\n\nThe DU enabled researchers to perform easier analysis of substances by quickly taking measurements at more than one wavelength to produce an absorption spectrum describing the complete substance. For example, the standard method of analysis of the vitamin A content of shark liver oil, before the introduction of the DU spectrophotometer, involved feeding the oil to rats for 21 days, then cutting off the rats' tails and examining their bone structure. With the DU's UV technology, vitamin A content of shark liver oil could be determined directly in a matter of minutes.\n\nThe Scripps Research Institute and the Massachusetts Institute of Technology credit the DU with improving both accuracy and speed of chemical analysis. MIT states: \"This device forever simplified and streamlined chemical analysis, by allowing researchers to perform a 99.9% accurate quantitative measurement of a substance within minutes, as opposed to the weeks required previously for results of only 25% accuracy.\"\n\nOrganic chemist and philosopher of science Theodore L. Brown states that it \"revolutionized the measurement of light signals from samples\". Nobel laureate Bruce Merrifield is quoted as calling the DU spectrophotometer \"probably the most important instrument ever developed towards the advancement of bioscience.\" Historian of science Peter J. T. Morris identifies the introduction of the DU and other scientific instruments in the 1940s as the beginning of a Kuhnian revolution. \n\nFor the Beckman company, the DU was one of three foundational inventions – the pH meter, the DU spectrophotometer, and the helipot potentiometer – that established the company on a secure financial basis and enabled it to expand.\n\nDevelopment of the spectrophotometer had direct relevance to World War II and the American war effort. The role of vitamins in health was of significant concern, as scientists wanted to identify Vitamin A-rich foods to keep soldiers healthy. Previous methods of assessing Vitamin A levels involved feeding rats a food for several weeks and then performing a biopsy to estimate ingested Vitamin A levels. In contrast, examining a food sample with a DU spectrophotometer yielded better results in a matter of minutes. The DU spectrophotometer could be used to study both vitamin A and its precursor carotenoids, and rapidly became the preferred method of spectrophotometric analysis.\n\nThe DU spectrophotometer was also an important tool for scientists studying and producing the new wonder drug penicillin.\nThe development of penicillin was a secret national mission, involving 17 drug companies, with the goal of providing penicillin to all U.S. Forces engaged in World War II. It was known that penicillin was more effective than sulfa drugs, and that its use reduced mortality, severity of long-term wound trauma, and recovery time. However, its structure was not understood, isolation procedures used to create pure cultures were primitive, and production using known surface culture techniques was slow.\n\nAt Northern Regional Research Laboratory in Peoria, Illinois, researchers collected and examined more than 2,000 specimens of molds (as well as other microorganisms). An extensive research team included Robert Coghill, Norman Heatley, Andrew Moyer, Mary Hunt, Frank H. Stodola and Morris E. Friedkin. Friedkin recalls that an early model of the Beckman DU spectrophotometer was used by the penicillin researchers in Peoria. The Peoria lab was successful in isolating and commercially producing superior strains of the mold, which were 200 times more effective than the original forms discovered by Alexander Fleming. By the end of the war, American pharmaceutical companies were producing 650 billion units of penicillin each month. Much of the work done in this area during World War II was kept secret until after the war.\n\nThe DU spectrophotometer was also used for critical analysis of hydrocarbons. A number of hydrocarbons were of interest to the war effort. Toluene, a hydrocarbon in crude oil, was used in production of TNT for military use. Benzene and butadienes were used in the production of synthetic rubber. Rubber, used in tires for jeeps, airplanes and tanks, was in critically short supply because the United States was cut off from foreign supplies of natural rubber. The Office of Rubber Reserve organized researchers at universities and in industry to secretly work on the problem. The demand for synthetic rubber caused Beckman Instruments to develop infrared spectrophotometers. Infrared spectrophotometers were better suited than UV–Vis spectrophotometers to the analysis of C hydrocarbons, particularly for applications in petroleum refining and gasoline production.\n\nGerty Cori and her husband Carl Ferdinand Cori won the Nobel Prize in Physiology or Medicine in 1947 in recognition of their work on enzymes. They made several discoveries critical to understanding carbohydrate metabolism, including the isolation and discovery of the Cori ester, glucose 1-phosphate, and the understanding of the Cori cycle. They determined that the enzyme phosphorylase catalyzes formation of glucose 1-phosphate, which is the beginning and ending step in the conversions of glycogen into glucose and blood glucose to glycogen. Gerty Cori was also the first to show that a defect in an enzyme can be the cause of a human genetic disease. The Beckman DU spectrophotometer was used in the Cori laboratory to calculate enzyme concentrations, including phosphorylase.\n\nAnother researcher who spent six months in 1947 at the Cori laboratory, \"the most vibrant place in biochemistry\" at that time, was Arthur Kornberg. Kornberg was already familiar with the DU spectrophotometer, which he had used at Severo Ochoa's laboratory at New York University. The \"new and scarce\" Beckman DU, loaned to Ochoa by the American Philosophical Society, was highly prized and in constant use. Kornberg used it to purify aconitase, an enzyme in the citric acid cycle.\n\nKornberg and Bernard L. Horecker used the Beckman DU spectrophotometer for enzyme assays measuring NADH and NADPH. They determined their extinction coefficients, establishing a basis for quantitative measurements in reactions involving nucleotides. This work became one of the most cited papers in biochemistry. Kornberg went on to study nucleotides in DNA synthesis, isolating the first DNA polymerizing enzyme (DNA polymerase I) in 1956 and receiving the Nobel Prize in Physiology or Medicine with Severo Ochoa in 1959.\n\nThe bases of DNA absorbed ultraviolet light near 260 nm. Inspired by the work of Oswald Avery on DNA, Erwin Chargaff used a DU spectrophotometer in the 1940s in measuring the relative concentrations of bases in DNA. Based on this research, he formulated Chargaff's rules. In the first complete quantitative analysis of DNA, he reported the near-equal correspondence of pairs of bases in DNA, with the number of guanine units equaling the number of cytosine units, and the number of adenine units equaling the number of thymine units. He further demonstrated that the relative amounts of guanine, cytosine, adenine and thymine varied between species. In 1952, Chargaff met Francis Crick and James D. Watson, discussing his findings with them. Watson and Crick built upon his ideas in their determination of the structure of DNA.\n\nUltraviolet spectroscopy has wide applicability in molecular biology, particularly the study of photosynthesis. It has been used to study a wide variety of flowering plants and ferns by researchers in departments of biology, plant physiology and agriculture science as well as molecular genetics.\n\nParticularly useful in detecting conjugated double bonds, the new technology made it possible for researchers like Ralph Holman and George O. Burr to study dietary fats, work that had significant implications for human diet. The DU spectrophotometer was also used in the study of steroids by researchers like Alejandro Zaffaroni, who helped to develop the birth control pill, the nicotine patch, and corticosteroids.\n\nThe Beckman team eventually developed additional models, as well as a number of accessories or attachments which could be used to modify the DU for different types of work. One of the first accessories was a flame attachment with a more powerful photo multiplier to enable the user to examine flames such as potassium, sodium and cesium (1947).\n\nIn the 1950s, Beckman Instruments developed the DR and the DK, both of which were double-beam ultraviolet spectrophotometers. The DK was named for Wilbur I. Kaye, who developed it by modifying the DU to expand its range into the near-infrared. He did the initial work while at Tennessee Eastman Kodak, and later was hired by Beckman Instruments. The DKs introduced an automatic recording feature. The DK-1 used a non-linear scroll, and the DK-2 used a linear scroll to automatically record the spectra.\n\nThe DR incorporated a \"robot operator\" which would reset the knobs on the DU to complete a sequence of measurements at different wavelengths, just like a human operator would to generate results for a full spectrum. It used a linear shuttle with four positions, and a superstructure to change the knobs. It had a moving chart recorder to plot results, with red, green and black dots. The price of recording spectrophotometers was substantially higher than non-recording machines.\n\nThe DK was ten times faster than the DR, but not quite as accurate. It used a photomultiplier, which had introduced a source of error. The DK's speed made it preferred to the DR. Kaye eventually developed the DKU, combining infrared and ultraviolet features in one instrument, but it was more expensive than other models.\n\nThe last DU spectrophotometer was produced on July 6, 1976. By the 1980s, computers were being incorporated into scientific instruments such as Bausch & Lomb's Spectronic 2000 UV–Vis spectrophotometer, to improve data acquisition and provide instrument control. Specialized spectrophotometers designed for specific tasks now tend to be used rather than general \"all-purpose machines\" like the DU.\n"}
{"id": "3961885", "url": "https://en.wikipedia.org/wiki?curid=3961885", "title": "Daffy's Elixir", "text": "Daffy's Elixir\n\nDaffy's Elixir (also sometimes known as Daffey's Elixir or Daffye's Elixir) is a name that has been used by several patent medicines over the years. It was originally designed for diseases of the stomach, but was later marketed as a universal cure. It remained a popular remedy in Britain and later the United States of America throughout the eighteenth and nineteenth centuries.\n\nDaffy's Elixir was one of the most popular and frequently advertised patent medicines in Britain during the 18th century. It is reputed to have been invented by clergyman Thomas Daffy rector of Redmile, Leicestershire, in 1647. He named it elixir salutis (lit. \"elixir of health\") and promoted as a generic cure-all.\n\nAn early recipe for \"True Daffy\" from 1700 lists the following ingredients: aniseed, brandy, cochineal, elecampane, fennel seed, jalap, manna, parsley seed, raisin, rhubarb, saffron, senna and spanish liquorice. Chemical analysis has shown this to be a laxative made mostly from alcohol. Other recipes include Guiuacum wood chips, caraway, Salt of Tartar, and scammony.\n\nAccording to an early nineteenth century advertisement it was used for the following ailments: The Stone in Babies and Children; Convulsion fits; Consumption and Bad Digestives; Agues; Piles; Surfeits; Fits of the Mother and Vapours from the Spleen; Green Sickness; Children's Distempers, whether the Worms, Rickets, Stones, Convulsions, Gripes, King's Evil, Joint Evil or any other disorder proceeding from Wind or Crudities; Gout and Rheumatism; Stone or Gravel in the Kidnies; Cholic and Griping of the Bowels; the Phthisic (both as cure and preventative provided always that the patient be moderate in drinking, have a care to prevent taking cold and keep a good diet; Dropsy and Scurvy. The frequent use of the medicine to treat Colic, gripes or fret in horses was deplored in early veterinary manuals.\n\nAfter Daffy's death in 1680 the recipe was left to his daughter Catherine, and his kinsmen Anthony and Daniel who were apothecaries in Nottingham. Anthony Daffy moved to London in the 1690s and began to exploit the product issuing pamphlets such as \"Directions for taking elixir salutis or, the famous purging cordial, known by the name of Daffy's elixir salutis\" [London], [1690?]. His widow Elleanor Daffy continued from about 1693 and (their daughter?) Katharine from about 1707. During the early 18th century the product was advertised widely in the emerging national and local newspapers. The success attracted several counterfeit copies, using inferior alcohol rather than brandy.\n\nThe medicine was later produced by William and Cluer Dicey & Co. of Bow Church yard c.1775 who claimed the sole rights of manufacture of the True Daffy's Elixir, although the recipe was not subject to any patent. Proprietorship was also then claimed by Peter Swinton of Salisbury Court and his son Anthony Daffy Swinton who may have been descended from the inventor. Dicey and Co. and their successors marketed it in the United States of America.\n\nIt then passed to Dicey and Sutton, and later to Messrs W. Sutton & Co. of Enfield Middlesex who continuing to market it throughout the nineteenth century. The use of Daffy's elixir is referred to in Anthony Trollope's novel Barchester Towers, 1857.\n\nDaffy's elixir is also mentioned on several occasions in Thomas Pynchon's novel Mason & Dixon, particularly by Jeremiah Dixon, who attempts to procure large quantities before beginning his surveying trip with Charles Mason. Dixon is warned by Benjamin Franklin, however, that imported Daffy's Elixir is extremely expensive, and he would be better off ordering a customized version from the apothecary. During the same visit, Dixon also orders laudanum, a well-known constipating agent.\n\nDaffy's elixir is also mentioned in the Charles Dickens book, Oliver Twist, Ch. II, where it is referred to as Daffy, in the sentence: 'Why, it's what I'm obliged to keep a little of in the house, to put into the blessed infants' Daffy, when they ain't well, Mr. Bumble,(the Parish Beadle)' replied Mrs. Mann as she opened a corner cupboard, and took down a bottle and glass. 'It's gin. I'll not deceive you, Mr. B. It's gin.'\n\nDaffy’s Elixir is also mentioned in the Works of William Makepeace Thackeray book, Vanity Fair, Chapter XXXVIII A Family In a Small Way, where it is referenced in the sentence ‘..and there found Mrs. Sedley in the act of surreptitiously administering Daffy’s Elixir to the infant.’\n\n\"Daffy’s original elixir salutis, vindicated against all counterfeits, &c. or, An advertisement by mee, Anthony Daffy, of London, citizen and student in physick, By way of vindication of my famous and generally approved cordial drink, (called elixir salutis) from the notoriously false suggestions of one Tho. Witherden of Bear-steed in the county of Kent, Gent. (as pretended;) Jane White, Robert Brooke, apothecary, and Edward Willet; all new upstatrt counterfitors of my elixir, and Ape-like imitators of my long since printed Books and Directions, (some of them, nigh verbatim, or word for word) and that to the jeopardy of many good, (but mis-in-formed) Peoples Healths, and Lives too; as also, from the false pretentions of other more sneaking Cub-Quacks, not yet lickt into form, but remaining Moon-blind brats, (still in swadling-clouts) I mean the numerous crew of libellous pamphleteeirs, which are (if possible) more dangerous counterfeiters of my Elixer\" . . . Advertisement by mee, Anthony Daffy s.n., 1690?].\n\n\"Daffy’s original and famous elixir salutis: the choice drink of health: or, health-bringing drink. Being a famous cordial drink, found out by the providence of the Almighty, and (for above twenty years) experienced by himself, and divers persons (whose names are at most of their desires here inserted) a most excellent preservative of man-kind. A secret far beyond any medicament yet known, and is found so agreeable to nature, that it effects all its operations, as nature would have it, and as a virtual expedient proposed by her, for reducing all her extreams unto an equal temper; the same being fitted unto all ages, sexes, complexions, and constitutions, and highly fortifying nature against any noxious humour, invading or offending the noble parts. Never published by any but by Anthony Daffy, student in physick, and since continued by his widow Elleanor Daffy\", London : printed with allowance, for the author, by Tho. Milbourn dwelling in Jewen-Street, 1693.\n\n\n"}
{"id": "15110709", "url": "https://en.wikipedia.org/wiki?curid=15110709", "title": "Detoxification foot pads", "text": "Detoxification foot pads\n\nDetoxification foot pads are adhesive foot pads or patches that manufacturers claim can dramatically improve health when placed under the feet during sleep. Some of these pads may contain ingredients such as \"distilled bamboo vinegar\" that allegedly pull toxins from the body, but critics have shown that the process is not scientifically viable.\n\nOn January 3, 2008, the United States Food and Drug Administration (FDA) released an urgent warning regarding the potential dangers of many imported pharmaceutical substances including several brands of detox foot patches. In April 2008, in response to questions from the Associated Press, an FDA spokeswoman said regarding the agency's investigation of the claims made for Kinoki foot pads that \"basically, when we open up a case it means that the violation might be in terms of the Food, Drug and Cosmetics Act, such as when (product makers) make false, misleading claims.\"\n\nIn August 2008, National Public Radio commissioned a laboratory test to look for heavy metals in used pads, which Kinoki claims are extracted from the body. The test found none. NPR also discovered that the pads change from white to grey when they are exposed to moisture, including sweat, and not necessarily because they are absorbing other substances.\n\nThe Japanese company Kenrico claim that their pads have a positive effect on the health of the users, and that they remove heavy metals from the body. There is no evidence that these products work and although the skin is one of the bodies largest organs of detoxification, there is no proposed mechanism as to why these patches would increase the detoxification rate above baseline.\n"}
{"id": "51452660", "url": "https://en.wikipedia.org/wiki?curid=51452660", "title": "Doktor Koster's Antigaspills", "text": "Doktor Koster's Antigaspills\n\nDoktor Koster's Antigaspills were an early 20th century alternative medication intended to treat stomach upset and excessive flatulence. They are best known for being administered to Adolf Hitler by his physician, Theodor Morell, to treat Hitler's stomach ailments. Morrell, regarded as a quack by Hitler's associates, administered a wide variety of unorthodox concoctions and medications to Hitler beginning in 1936.\n\nThe pills active ingredients consisted primarily of atropine (an extract of \"Atropa belladonna\") and strychnine.\n"}
{"id": "49626134", "url": "https://en.wikipedia.org/wiki?curid=49626134", "title": "Earthquake sensitivity", "text": "Earthquake sensitivity\n\nEarthquake sensitivity and earthquake sensitive are pseudoscientific terms defined by Jim Berkland to refer to certain people who claim sensitivity to the precursors of impending earthquakes, manifested in \"dreams or visions, psychic impressions, or physiological symptoms\", the latter including \"ear tones\" (ringing in the ears), headaches, and agitation. It is claimed that \"[a] person with a very sensitive body may also have some subtle reaction to whatever animals react to\". Proponents have speculated that these may result from: 1) piezoelectric effects due to changes in the stress of the earth's crust, 2) low-frequency electromagnetic signals, or 3) from the emission of radon gas.\n\nAlthough proponents suggest the \"possibility\" that the claimed effects might work through known physical phenomena, and thus be amenable to scientific study, these claims are pseudoscientific in that no evidence of such effects, nor any theory of how such effects might be perceived, has been presented in the scientific literature. What the scientific literature does have is various reports showing that animals do \"not\" show disturbed or altered behavior attributable to earthquake precursors (other than foreshocks ). Aside from whether such phenomena can be \"detected\" (by any means), the \"consistent failure to find reliable earthquake precursors\" has led many scientists to question whether such precursor phenomena even exist.\n\nCould \"earthquake sensitives\" be responding to some kind of \"psychic impressions\" or other paranormal phenomena as yet unknown to science? After reviewing the scientific literature the \"International Commission on Earthquake Forecasting for Civil Protection\" (ICEF) concluded that \n\nOn their side, the proponents claim that there have been \"many scientific papers\" supporting their views, but \"most have been totally rejected by the keepers of high wisdom.\" While scientists are quick to dismiss theories they \"know, or have good reason to believe, are not credible\", and especially predictions by amateurs on account of their lack of scientific rigor,\nproponents claim that successful predictions can indicate a significant breakthrough, even if the details are not understood. In this regard Berkland claims \"a 75 percent accuracy rate of forecasting quakes.\" However, these results (besides being disputed) are irrelevant in demonstrating any kind of \"earthquake sensitive\" effect as Berkland's predictions appear to not involve such effects.\n\nBerkland ceased posting his predictions after June 2010. Though a few others have continued to post their predictions on Berkland's website, there appears to be no effort to correlate \"ear tones\" or any other physiological effect with subsequent earthquakes.\n\n\n"}
{"id": "4217297", "url": "https://en.wikipedia.org/wiki?curid=4217297", "title": "Electromagnetic hypersensitivity", "text": "Electromagnetic hypersensitivity\n\nElectromagnetic hypersensitivity (EHS) is a claimed sensitivity to electromagnetic fields, to which negative symptoms are attributed. EHS has no scientific basis and is not a recognised medical diagnosis. Claims are characterized by a \"variety of non-specific symptoms, which afflicted individuals attribute to exposure to electromagnetic fields\".\nThose who are self-described with EHS report adverse reactions to electromagnetic fields at intensities well below the maximum levels permitted by international radiation safety standards. The majority of provocation trials to date have found that such claimants are unable to distinguish between exposure and non-exposure to electromagnetic fields. A systematic review in 2005 showed no convincing scientific evidence for symptoms being caused by electromagnetic fields. Since then, several double-blind experiments have shown that people who report electromagnetic hypersensitivity are unable to detect the presence of electromagnetic fields and are as likely to report ill health following a sham exposure as they are following exposure to genuine electromagnetic fields, suggesting the cause in these cases to be the nocebo effect.\nA 2005 review by the UK Health Protection Agency and a 2006 systematic review each evaluated the evidence for various medical, psychological, behavioral, and alternative treatments for EHS and each found that the evidence-base was limited and not generalizable, but that the best evidence favored cognitive behavioural therapy. As of 2005, WHO recommended that people presenting with claims of EHS be evaluated to determine if they have a medical condition that may be causing the symptoms the person is attributing to EHS, that they have a psychological evaluation, and that the person's environment be evaluated for issues like air or noise pollution that may be causing problems.\nSome people who feel they are sensitive to electromagnetic fields may seek to reduce their exposure or use alternative medicine. Government agencies have enforced false advertising claims against companies selling devices to shield against EM radiation.\n\nThere are no specific symptoms associated with claims of EHS and reported symptoms range widely between individuals. They include headache, fatigue, stress, sleep disturbances, skin prickling, burning sensations and rashes, pain and ache in muscles and many other health problems. In severe cases such symptoms can be a real and sometimes disabling problem for the affected person, causing psychological distress. There is no scientific basis to link such symptoms to electromagnetic field exposure.\n\nThe prevalence of some reported symptoms is geographically or culturally dependent and does not imply \"a causal relationship between symptoms and attributed exposure\". Many such reported symptoms overlap with other syndromes known as symptom-based conditions, functional somatic syndromes, and IEI (idiopathic environmental intolerance).\n\nThose reporting electromagnetic hypersensitivity will usually describe different levels of susceptibility to electric fields, magnetic fields, and various frequencies of electromagnetic waves. Devices implicated include fluorescent and low-energy lights, mobile, cordless/portable phones, and Wi-Fi. A 2001 survey found that people self-diagnosing as EHS related their symptoms most frequently to mobile phone base stations (74%), followed by mobile phones (36%), cordless phones (29%), and power lines (27%). Surveys of electromagnetic hypersensitivity sufferers have not been able to find any consistent pattern to these symptoms.\n\nMost blinded conscious provocation studies have failed to show a correlation between exposure and symptoms, leading to the suggestion that psychological mechanisms play a role in causing or exacerbating EHS symptoms. In 2010, Rubin et al. published a follow-up to their 2005 review, bringing the totals to 46 double-blind experiments and 1175 individuals with self-diagnosed hypersensitivity. Both reviews found no robust evidence to support the hypothesis that electromagnetic exposure causes EHS, as have other studies. They also concluded that the studies supported the role of the nocebo effect in triggering acute symptoms in those with EHS.\n\nSome other types of studies suggest evidence for symptoms at non-thermal levels of electromagnetic exposure. A review in 2010 of ten studies on neurobehavioral and cancer outcomes near cell phone base stations found eight with increased prevalence, including sleep disturbance and headaches. Since 1962, the microwave auditory effect or tinnitus has been shown from radio frequency exposure at levels below significant heating. Studies during the 1960s in Europe and Russia claimed to show effects on humans, especially the nervous system, from low energy RF radiation; the studies were disputed at the time.\n\nOther studies on sensitivity have looked at therapeutic procedures using non-thermal electromagnetic exposure, genetic factors, an alteration in mast cells, oxidative stress, protein expression and voltage-gated calcium channels. Mercury release from dental amalgam and heavy metal toxicity have also been implicated in exposure effects and symptoms. Another line of study has been the nature of hyper-sensitivity or intolerance and the range of environmental exposures which may be related to it. Some 80% of people with self-diagnosed electromagnetic intolerance also claim intolerance to low levels of chemical exposure.\n\nElectromagnetic hypersensitivity is not an accepted diagnosis; medically there is no case definition or clinical practice guideline and there is no specific test to identify it, nor is there an agreed-upon definition with which to conduct clinical research.\n\nComplaints of electromagnetic hypersensitivity may mask organic or psychiatric illness. Diagnosis of those underlying conditions involves investigating and identifying possible known medical causes of any symptoms observed. It may require both a thorough medical evaluation to identify and treat any specific conditions that may be responsible for the symptoms, and a psychological evaluation to identify alternative psychiatric/psychological conditions that may be responsible or contribute to the symptoms.\n\nSymptoms may also be brought on by imagining that exposure is causing harm, an example of the nocebo effect. Studies have shown that reports of symptoms are more closely associated with belief that one is being exposed than with any actual exposure.\n\nA 2006 systematic review and a 2005 review by the UK Health Protection Agency each evaluated the evidence for various medical, psychological, behavioral, and alternative treatments for EHS and each found that the evidence-base was limited and not generalizable. The conclusion of the 2006 review stated: \"The evidence base concerning treatment options for electromagnetic hypersensitivity is limited and more research is needed before any definitive clinical recommendations can be made. However, the best evidence currently available suggests that cognitive behavioural therapy is effective for patients who report being hypersensitive to weak electromagnetic fields.\"\n\nAs of 2005, WHO recommended that people presenting with claims of EHS be evaluated to determine if they have a medical condition that may be causing the symptoms the person is attributing to EHS, that they have a psychological evaluation, and that the person's environment be evaluated for issues like air or noise pollution that may be causing problems.\n\nThe prevalence of claimed electromagnetic hypersensitivity has been estimated as being between a few cases per million to 5% of the population depending on the location and definition of the condition.\n\nIn 2002, a questionnaire survey of 2,072 people in California found that the prevalence of self-reported electromagnetic hypersensitivity within the sample group was 3% (95% CI 2.8–3.68%), with electromagnetic hypersensitivity being defined as \"being allergic or very sensitive to getting near electrical appliances, computers, or power lines\" (response rate 58.3%).\n\nA similar questionnaire survey from the same year in Stockholm County (Sweden), found a 1.5% prevalence of self-reported electromagnetic hypersensitivity within the sample group, with electromagnetic hypersensitivity being defined as \"hypersensitivity or allergy to electric or magnetic fields\" (response rate 73%).\n\nA 2004 survey in Switzerland found a 5% prevalence of claimed electromagnetic hypersensitivity in the sample group of 2,048.\n\nIn 2007, a UK survey aimed at a randomly selected group of 20,000 people found a prevalence of 4% for symptoms self-attributed to electromagnetic exposure.\n\nA group of scientists also attempted to estimate the number of people reporting \"subjective symptoms\" from electromagnetic fields for the European Commission. In the words of a HPA review, they concluded that \"the differences in prevalence were at least partly due to the differences in available information and media attention around electromagnetic hypersensitivity that exist in different countries. Similar views have been expressed by other commentators.\"\n\nIn 2010, a cell tower operator in South Africa revealed at a public meeting that the tower that nearby residents were blaming for their current EHS symptoms had been turned off over six weeks prior to the meeting, thus making it a highly unlikely cause of EHS symptoms.\n\nIn February 2014, the UK Advertising Standards Authority found that claims of harm from electromagnetic radiation, made in a product advertisement, were unsubstantiated and misleading.\n\nPeople have filed lawsuits to try to win damages due to harm claimed from electromagnetic radiation. In 2012, a New Mexico judge dismissed a lawsuit in which one person sued his neighbor, claiming to have been harmed by EM radiation from his neighbor's cordless telephones, dimmer switches, chargers, Wi-Fi and other devices. The plaintiff brought the testimony of his doctor, who also believed she had EHS, and a person who represented himself as a neurotoxicologist; the judge found none of their testimony credible. In 2015, parents of a boy at a school in Southborough, Massachusetts alleged that the school's Wi-Fi was making the boy sick.\n\nIn November 2015, a depressed teenage girl in England committed suicide. Her suicide was attributed to EHS by her parents and taken up by tabloids and EHS advocates.\n\nSome people who feel they are sensitive to electromagnetic fields self-treat by trying to reduce their exposure to electromagnetic sources by avoiding sources of exposure, disconnecting or removing electrical devices, shielding or screening of self or residence, and alternative medicine. In Sweden, some municipalities provide disability grants to people who claim to have EHS in order to have abatement work done in their homes even though the public health authority does not recognize EHS as an actual medical condition; towns in Halland do not provide such funds and this decision was challenged and upheld in court.\n\nThe United States National Radio Quiet Zone is an area where wireless signals are restricted for scientific research purposes, and some people who believe they have EHS have relocated there seeking relief.\n\nGro Harlem Brundtland, former prime minister of Norway and Director general of the World Health Organization, claims to suffer from EHS. In 2015 she said that she had been sensitive for 25 years.\n\nIn the fictional television crime drama \"Better Call Saul\", the character Charles \"Chuck\" McGill is depicted as experiencing the symptoms of EHS. In the episode \"Alpine Shepherd Boy\", a skeptical doctor surreptitiously operates a switch controlling the electronics in Chuck's hospital bed. This does not affect his symptoms, suggesting that his electromagnetic hypersensitivity is not genuine. A similar instance of Chuck's symptoms being objectively psychosomatic is seen on the episode \"Chicanery\". Although a fully charged cellphone battery is planted on his person without his knowledge, Chuck experiences no adverse effects by having an electronic device on his body for close to two hours. When this fact is revealed to him, he is profoundly shaken, and comes to see \"beyond a shadow of a doubt\" that his symptoms are an indication of mental disease spurred on by past emotional trauma, rather than EHS.\n\n\n"}
{"id": "5976300", "url": "https://en.wikipedia.org/wiki?curid=5976300", "title": "Engines (children's book)", "text": "Engines (children's book)\n\nEngines: Man's Use of Power, from the Water Wheel to the Atomic Pile is a science book for children by L. Sprague de Camp, illustrated by Jack Coggins, published by Golden Press in 1959. A revised edition issued as part of the publisher's Golden Library of Knowledge Series was published in 1961, and a paperback edition in 1969.\n\nAs stated on the cover, the work is a survey of \"Man's use of power, from the water wheel to the atomic pile.\"\n\n\"The Science News-Letter\", in its July 18, 1959 issue, listed the book among its \"Books of the Week,\" describing the work as a \"[f]actual book for young readers.\"\n"}
{"id": "12891588", "url": "https://en.wikipedia.org/wiki?curid=12891588", "title": "Experimental design diagram", "text": "Experimental design diagram\n\nExperimental Design Diagram (EDD) is a diagram used in science classrooms to design an experiment. This diagram helps to identify the essential components of an experiment. It includes a title, the research hypothesis and null hypothesis, the independent variable, the levels of the independent variable, the number of trials, the dependent variable, the operational definition of the dependent variable and the constants.\n\n[science way of controlling the data]\n"}
{"id": "27163443", "url": "https://en.wikipedia.org/wiki?curid=27163443", "title": "Fauna Entomologica Scandinavica", "text": "Fauna Entomologica Scandinavica\n\nFauna Entomologica Scandinavica is a scientific book series of entomological identification manuals for insects (and other terrestrial arthropods) in North-West Europe, mainly Fennoscandia and Denmark. The series is used by a number of groups, such as ecologists, biologists, and insect collectors. The books are in English, and published by the Dutch academic publishing house Brill.\n"}
{"id": "46815172", "url": "https://en.wikipedia.org/wiki?curid=46815172", "title": "Food and Nutrition Bulletin", "text": "Food and Nutrition Bulletin\n\nThe Food and Nutrition Bulletin is a quarterly peer-reviewed scientific journal that was founded by Dr. Nevin S. Scrimshaw in 1978 and is published by Sage Publications. The journal publishes articles that cover policy analysis, original scientific and social research, and academic reviews related to human nutrition and malnutrition in developing countries. It is an official publication of the Nevin Scrimshaw International Nutrition Foundation and is housed at the Friedman School of Nutrition Science and Policy at Tufts University.\n\nThe journal is abstracted and indexed by MEDLINE and Scopus. According to the \"Journal Citation Reports\", the journal's 2017 impact factor is 1.881, ranking it 63rd out of 133 journals in the category \"Food Science and Technology\", and 62nd out of 81 in the category \"Nutrition and Dietetics.\"\n"}
{"id": "46729393", "url": "https://en.wikipedia.org/wiki?curid=46729393", "title": "Gibilmanna Observatory", "text": "Gibilmanna Observatory\n\nThe Gibilmanna Observatory is a research station used for a diverse range of studies set up and run by the Istituto Nazionale di Geofisica e Vulcanologia (INGV) and it is located on Cozzo Timpa Rossa at 1005 m.a.s.l. near Cefalù, a town in the district of Palermo, Italy.\n\nIt is one of the 120 stations of the Italian Magnetic Network measuring Earth magnetism field in Italy, and in 2005 INGV's Centro Nazionale Terremoti (CNT) selected it to set up the OBS Lab aimed at designing, manufacturing and managing the Ocean-Bottom Seismometer with Hydrophone (OBS/H), later deployed for monitoring the Marsili submarine volcano in the Tyrrhenian sea.\n\nIn addition, the observatory is one of the 110 Data Collection Platform (D.C.P.) stations of the Aeronautica Militare Italiana for automatic weather data gathering and transmission to the METEOSAT satellite.\nSince 1976 this station has also been equipped for Ionospheric research and it is the most southern station of its kind in Europe. Real time ionograms are recorded by the INGV own developed AIS-INGV ionosonde installed at the station and reported on the INGV ionospheric website.\n\n"}
{"id": "40542997", "url": "https://en.wikipedia.org/wiki?curid=40542997", "title": "Gold effect", "text": "Gold effect\n\nThe Gold effect is the phenomenon in which a scientific idea, particularly in medicine, is developed to the status of an accepted position within a professional body or association by the social process itself of scientific conferences, committees, and consensus building, despite not being supported by conclusive evidence. The effect was described by Professor T. Gold in 1979. The effect was reviewed by Drs. Petr Skrabanek and James McCormick in their book \"Follies and Fallacies in Medicine\". The Gold effect is used to analyze errors in public health policy and practice, such as the widespread use of cholesterol screening in the prevention of cardiovascular disease.\n\nIn their book, Skrabanek and McCormick describe the Gold effect as: \"At the beginning a few people arrive at a state of near belief in some idea. A meeting is held to discuss the pros and cons of the idea. More people favouring the idea than those disinterested will be present. A representative committee will be nominated to prepare a collective volume to propagate and foster interest in the idea. The totality of resulting articles based on the idea will appear to show an increasing consensus. A specialised journal will be launched. Only orthodox or near orthodox articles will pass the referees and the editor.\"\n"}
{"id": "7066527", "url": "https://en.wikipedia.org/wiki?curid=7066527", "title": "Height Modernization", "text": "Height Modernization\n\nHeight Modernization is the name of a series of state-by-state programs recently begun by the United States' National Geodetic Survey, a division of the National Oceanic and Atmospheric Administration. The goal of each state program is to place GPS base stations at various locations within each participating state to measure topographic changes in the directions of latitude and longitude caused by subsidence or earthquakes, as well as to measure changes in height (elevation).\n\n"}
{"id": "27508128", "url": "https://en.wikipedia.org/wiki?curid=27508128", "title": "Hologram bracelet", "text": "Hologram bracelet\n\nA hologram bracelet or energy bracelet is a small rubber wristband supposedly fitted with a hologram. Manufacturers have said supposedly that the holograms \"optimise the natural flow of energy around the body, and supposedly improve an athlete's strength, balance and flexibility\". Only anecdotal evidence supports these claims and tests performed by the Australian Skeptics, the University of Wales Institute, Cardiff, and the RMIT's School of Health Sciences have been unable to identify any effect on performance.\n\nHologram bracelets include a small hologram which manufacturers say is \"programmed\" through an undisclosed process. Power Balance, who have manufactured the bracelets since 2007, say that the programming \"mimics Eastern philosophies\". The holograms are most usually installed in bracelets and wristbands but are also sold as pendants or necklaces, anklets, shoe inserts, pet tags, or separately for users to apply to the back of a watch, for example.\n\nManufacturers including Power Balance and EFX Performance make no claims on their websites for their products, but carry testimonials from users who say that they improve athletic performance. Until 2010, Power Balance said that their bracelets helped improve an athlete's strength, balance and flexibility because the holograms are embedded with an \"electrical frequency\" that restores the body's \"electrical balance\" on contact with its natural energy field. In December 2010, following a successful legal action by the Australian Competition and Consumer Commission, Power Balance admitted that there was no credible scientific evidence for these claims.\n\nMark Hodgkinson, writing in the Daily Telegraph in 2010, called hologram bracelets a fad with many professional athletes seen wearing them and several actively endorsing them. Footballers David Beckham and Cristiano Ronaldo have worn them, and tennis players Sam Querrey and Mardy Fish both wore them during the final of the 2010 Queen's Club Championships. Endorsements for the Power Balance bracelet have come from Shaquille O'Neal, Rubens Barrichello, and the London Wasps rugby team, while ice hockey team the Cardiff Devils announced a partnership with Power Balance in early 2010. NASCAR reported in 2011 that many drivers wore EFX Performance bracelets with the Hendrick Motorsports and Stewart-Haas Racing teams entering into licensing deals.\n\nSeveral groups have investigated the effects of hologram bracelets on athletic performance. A 2011 study by RMIT University's School of Health Sciences found that there was an overall decrease in the balance and stability of wearers, although it was not statistically significant and the overall conclusion was that the bracelets had no effect on performance. The Australian Skeptics group found that the bracelets has no more than a placebo effect.\n\nResearch by the University of Wales Institute, Cardiff, commissioned by the BBC, also found that wearing the bracelets had no effect on performance in standard sports industry tests, adding that neither the physiology or the biology of wearers was changed. However, Dr Gareth Irwin who carried out the tests said that there may be changes in performance because of the placebo effect, a view which has been echoed by sports psychologists. Sports psychologist Victor Thompson says the bracelets play on superstition, simply giving people the expectation that they can improve their sporting performance. Cricket coach Jeremy Snape said he prefers that athletes have belief in themselves rather than in an external product, while Roberto Forzoni described the bracelets as \"gimmicks\" which allow athletes to avoid addressing real issues in their performance, with the high-profile endorsements giving the sense of belonging to an elite group of athletes.\n\n"}
{"id": "52003847", "url": "https://en.wikipedia.org/wiki?curid=52003847", "title": "Hyland's", "text": "Hyland's\n\nHyland's is a brand of homeopathic products sold in the United States and Canada. Hyland's operates in the United States as Hyland's Inc and in Canada as Hyland’s Homeopathic Canada Inc and is a division of Standard Homeopathic Co..\n\nThe company was founded in 1903 in Los Angeles as Standard Homeopathic Pharmacy. It was purchased by George H. Hyland in 1910 and the name was changed to Standard Homeopathic Company. Hyland's began selling products in Canada in 1990.\n\nIn 2013, the FDA conducted a review of Hyland's labeling and marketing information which revealed numerous products misbranded in violation of sections 503 and 301 of the Federal Food, Drug, and Cosmetic Act (FD&C Act) [21 U.S.C. §§ 353 and 331]. This review included the products for \"Infant Earache Drops,\" \"Restful Legs,\" \"Teething Tablets,\" and other products. The products are marketed as over-the-counter for diagnoses and treatments that require a physician's consultation.\n\nIn 2004, Hyland's Teething Tablets were the second most popular teething product.\n\nThe FDA warned consumers about Hyland's teething products in 2010, citing concern over the toxicity of its belladonna ingredient and lack of child proof caps. Hyland's voluntarily recalled its Hyland's Teething Tablets product after the 2010 warning in both the US and Canada. Hyland's began selling a reformulated version of the product with a child proof cap in 2011.\n\nIn 2016, Hyland's indicated it would stop selling its Hyland's Teething Tablets product in the U.S. after the FDA claimed it received reports of 10 child deaths and 400 adverse effects associated with the product. Hyland's calls the FDA's claims unsubstantiated. Hyland's suggests other possible causes, noting possible allergies and citing articles listing possible causes of seizures in children unrelated to the formulation of its product. The FDA confirmed in 2017 that inconsistent and sometimes excessive levels of belladonna had been found in the product.\n"}
{"id": "450976", "url": "https://en.wikipedia.org/wiki?curid=450976", "title": "Ian Stevenson", "text": "Ian Stevenson\n\nIan Pretyman Stevenson (October 31, 1918 – February 8, 2007) was a Canadian-born U.S. psychiatrist. He worked for the University of Virginia School of Medicine for fifty years, as chair of the department of psychiatry from 1957 to 1967, Carlson Professor of Psychiatry from 1967 to 2001, and Research Professor of Psychiatry from 2002 until his death.\n\nAs founder and director of the university's Division of Perceptual Studies, which investigates the paranormal, Stevenson became known internationally for his research into reincarnation, the idea that emotions, memories, and even physical bodily features can be transferred from one life to another. He traveled extensively over a period of forty years, investigating three thousand cases of children around the world who claimed to remember past lives. His position was that certain phobias, philias, unusual abilities and illnesses could not be fully explained by heredity or the environment. He believed that reincarnation provided a third type of explanation.\n\nStevenson helped to found the Society for Scientific Exploration in 1982 and was the author of around three hundred papers and fourteen books on reincarnation, including \"Twenty Cases Suggestive of Reincarnation\" (1966) and \"European Cases of the Reincarnation Type\" (2003). His major work was the 2,268-page, two-volume \"Reincarnation and Biology: A Contribution to the Etiology of Birthmarks and Birth Defects\" (1997). This reported two hundred cases of birthmarks and birth defects that seemed to correspond in some way to a wound on the deceased person whose life the child recalled. He wrote a shorter version of the same research for the general reader, \"Where Reincarnation and Biology Intersect\" (1997).\n\nReaction to his work was mixed. In his \"New York Times\" obituary, Margalit Fox wrote that Stevenson's supporters saw him as a misunderstood genius, but that most scientists had simply ignored his research and that his detractors regarded him as earnest but gullible. His life and work became the subject of three supportive books, \"\" (1999) by Tom Shroder, a \"Washington Post\" journalist, \"Life Before Life\" (2005) by Jim B. Tucker, a psychiatrist and colleague at the University of Virginia, and \"Science, the Self, and Survival after Death\" (2012), by Emily Williams Kelly. Critics, particularly the philosophers C.T.K. Chari (1909–1993) and Paul Edwards (1923–2004), raised a number of issues, including claims that the children or parents interviewed by Stevenson had deceived him, that he had asked them leading questions, that he had often worked through translators who believed what the interviewees were saying, and that his conclusions were undermined by confirmation bias, where cases not supportive of his hypothesis were not presented as counting against it.\n\nStevenson was born in Montreal and raised in Ottawa, one of three children. His father, John Stevenson, was a Scottish lawyer who was working in Ottawa as the Canadian correspondent for \"The Times\" of London or \"The New York Times\". His mother, Ruth, had an interest in theosophy and an extensive library on the subject, to which Stevenson attributed his own early interest in the paranormal. As a child he was often bedridden with bronchitis, a condition that continued into adulthood and engendered in him a lifelong love of books. According to Emily Williams Kelly, a colleague of his at the University of Virginia, he maintained a list of the books he had read, which numbered 3,535 between 1935 and 2003.\n\nHe studied medicine at St. Andrews University in Scotland from 1937 to 1939, but had to complete his studies in Canada because of the outbreak of the Second World War. He graduated from McGill University with a B.S.c. in 1942 and an M.D. in 1943. He was married to Octavia Reynolds from 1947 until her death in 1983. In 1985, he married Dr. Margaret Pertzoff (1926–2009), professor of history at Randolph-Macon Woman's College. She did not share his views on the paranormal, but tolerated them with what Stevenson called \"benevolent silences.\"\n\nAfter graduating, Stevenson conducted research in biochemistry. His first residency was at the Royal Victoria Hospital in Montreal (1944–1945), but his lung condition continued to bother him, and one of his professors at McGill advised him to move to Arizona for his health. He took up a residency at St. Joseph's Hospital in Phoenix, Arizona (1945–1946). After that, he held a fellowship in internal medicine at the Alton Ochsner Medical Foundation in New Orleans, became a Denis Fellow in Biochemistry at Tulane University School of Medicine (1946–1947), and a Commonwealth Fund Fellow in Medicine at Cornell University Medical College and New York Hospital (1947–1949). He became a U.S. citizen in 1949.\n\nKelly writes that Stevenson became dissatisfied with the reductionism he encountered in biochemistry, and wanted to study the whole person. He became interested in psychosomatic medicine, psychiatry and psychoanalysis, and in the late 1940s, worked at New York Hospital exploring psychosomatic illness and the effects of stress, and in particular why one person's response to stress might be asthma and another's high blood pressure.\n\nHe taught at Louisiana State University School of Medicine from 1949 to 1957 as assistant, then associate, professor of psychiatry. In the 1950s, he met the English writer Aldous Huxley (1894–1963), known for his advocacy of psychedelic drugs, and studied the effects of L.S.D. and mescaline, one of the first academics to do so. Kelly writes that he tried L.S.D. himself, describing three days of \"perfect serenity.\" He wrote that at the time he felt he could \"never be angry again,\" but added, \"As it happens that didn't work out, but the memory of it persisted as something to hope for.\"\n\nFrom 1951, he studied psychoanalysis at the New Orleans Psychoanalytic Institute and the Washington Psychoanalytic Institute, graduating from the latter in 1958, a year after being appointed head of the department of psychiatry at the University of Virginia. He argued against the orthodoxy within psychiatry and psychoanalysis at the time that the personality is more plastic in the early years; his paper on the subject, \"Is the human personality more plastic in infancy and childhood?\" (\"American Journal of Psychiatry\", 1957), was not received well by his colleagues. He wrote that their response prepared him for the rejection he experienced over his work on the paranormal.\n\nStevenson described as the leitmotif of his career his interest in why one person would develop one disease, and another something different. He came to believe that neither environment nor heredity could account for certain fears, illnesses and special abilities, and that some form of personality or memory transfer might provide a third type of explanation. He acknowledged, however, the absence of evidence of a physical process by which a personality could survive death and transfer to another body, and he was careful not to commit himself fully to the position that reincarnation occurs.\n\nThere is something essential to some human personalities ... which we cannot plausibly construe solely in terms of either brain states, or properties of brain states ... and, further, after biological death this non-reducible essential trait sometimes persists for some time, in some way, in some place, and for some reason or other, existing independently of the person's former brain and body. Moreover, after some time, some of these irreducible essential traits of human personality, for some reason or other, and by some mechanism or other, come to reside in other human bodies either some time during the gestation period, at birth, or shortly after birth.\n\nIn 1958 and 1959, Stevenson contributed several articles and book reviews to \"Harper's\" about parapsychology, including psychosomatic illness and extrasensory perception, and in 1958, he submitted the winning entry to a competition organized by the American Society for Psychical Research, in honor of the philosopher William James (1842–1910). The prize was for the best essay on \"paranormal mental phenomena and their relationship to the problem of survival of the human personality after bodily death.\" Stevenson's essay, \"The Evidence for Survival from Claimed Memories of Former Incarnations\" (1960), reviewed forty-four published cases of people, mostly children, who claimed to remember past lives. It caught the attention of Eileen J. Garrett (1893–1970), the founder of the Parapsychology Foundation, who gave Stevenson a grant to travel to India to interview a child who was claiming to have past-life memories. According to Jim Tucker, Stevenson found twenty-five other cases in just four weeks in India and was able to publish his first book on the subject in 1966, \"Twenty Cases Suggestive of Reincarnation\".\n\nChester Carlson (1906–1968), the inventor of xerography, offered further financial help. Tucker writes that this allowed Stevenson to step down as chair of the psychiatry department and set up a separate division within the department, which he called the Division of Personality Studies, later renamed the Division of Perceptual Studies. When Carlson died in 1968, he left $1,000,000 to the University of Virginia to continue Stevenson's work. The bequest caused controversy within the university because of the nature of the research, but the donation was accepted, and Stevenson became the first Carlson Professor of Psychiatry.\n\nThe bequest allowed Stevenson to travel extensively, sometimes as much as 55,000 miles a year, collecting around three thousand case studies based on interviews with children from Africa to Alaska.\n\nAccording to journalist Tom Shroder, \"In interviewing witnesses and reviewing documents, Dr. Stevenson searched for alternate ways to account for the testimony: that the child came upon the information in some normal way, that the witnesses were engaged in fraud or self-delusion, that the correlations were the result of coincidence or misunderstanding. But in scores of cases, Dr. Stevenson concluded that no normal explanation sufficed.\"\n\nIn some cases, a child in a \"past life\" case may have birthmarks or birth defects that in some way correspond to physical features of the \"previous person\" whose life the child seems to remember. Stevenson's \"Reincarnation and Biology: A Contribution to the Etiology of Birthmarks and Birth Defects\" (1997) examined two hundred cases of birth defects or birthmarks on children claiming past-life memories. These included children with malformed or missing fingers who said they recalled the lives of people who had lost fingers; a boy with birthmarks resembling entrance and exit wounds who said he recalled the life of someone who had been shot; and a child with a scar around her skull three centimetres wide who said she recalled the life of a man who had had skull surgery. In many of the cases, in Stevenson's view, the witness testimony or autopsy reports appeared to support the existence of the injuries on the deceased's body.\n\nIn the case of the boy who said he recalled the life of someone who had been shot, the sister of the deceased told Stevenson that her brother had shot himself in the throat. The boy had shown Stevenson a birthmark on his throat. Stevenson suggested that he might also have a birthmark on the top of his head, representing the exit wound, and found one there underneath the boy's hair.\n\nThe \"Journal of the American Medical Association\" referred to Stevenson’s \"Cases of the Reincarnation Type\" (1975) as a \"painstaking and unemotional\" collection of cases that were \"difficult to explain on any assumption other than reincarnation.\" In September 1977, the \"Journal of Nervous and Mental Disease\" devoted most of one issue to Stevenson's research. Writing in the journal, the psychiatrist Harold Lief described Stevenson as a methodical investigator and added, \"Either he is making a colossal mistake, or he will be known (I have said as much to him) as 'the Galileo of the 20th century'.\" The issue proved popular: the journal's editor, the psychiatrist Eugene Brody, said he had received 300–400 requests for reprints.\n\nDespite this early interest, most scientists ignored Stevenson's work. According to his \"New York Times\" obituary, his detractors saw him as \"earnest, dogged but ultimately misguided, led astray by gullibility, wishful thinking and a tendency to see science where others saw superstition.\" Critics suggested that the children or their parents had deceived him, that he was too willing to believe them, and that he had asked them leading questions. In addition, critics said, the results were subject to confirmation bias, in that cases not supportive of the hypothesis were not presented as counting against it. Leonard Angel, a philosopher of religion, told \"The New York Times\" that Stevenson did not follow proper standards. \"[B]ut you do have to look carefully to see it; that's why he's been very persuasive to many people.\"\n\nSkeptics have written that Stevenson's evidence was anecdotal and by applying Occam's razor there are prosaic explanations for the cases without invoking the paranormal. Science writer Terence Hines has written:\n\nRobert Baker wrote that many of the alleged past-life experiences investigated by Stevenson and other parapsychologists can be explained in terms of known psychological factors. Baker attributed the recalling of past lives to a mixture of cryptomnesia and confabulation.\n\nIan Wilson argued that a large number of Stevenson’s cases consisted of poor children remembering wealthy lives or belonging to a higher caste. He speculated that such cases may represent a scheme to obtain money from the family of the alleged former incarnation.\n\nThe philosopher C. T. K. Chari of Madras Christian College in Chennai, a specialist in parapsychology, argued that Stevenson was naive and that the case studies were undermined by his lack of local knowledge. Chari wrote that many of the cases had come from societies, such as that of India, where people believed in reincarnation, and that the stories were simply cultural artifacts; he argued that, for children in many Asian countries, the recall of a past life is the equivalent of an imaginary playmate. The philosopher Keith Augustine made a similar argument. Stevenson responded that it was precisely those societies that listened to children's claims about past lives, which in Europe or North America would normally be dismissed without investigation. To address the cultural concern, he wrote \"European Cases of the Reincarnation Type\" (2003), which presented forty cases he had examined in Europe.\n\nThe philosopher Paul Edwards, editor-in-chief of Macmillan's \"Encyclopedia of Philosophy\", became Stevenson's chief critic. From 1986 onwards, he devoted several articles to Stevenson's work, and discussed Stevenson in his \"Reincarnation: A Critical Examination\" (1996). He argued that Stevenson's views were \"absurd nonsense\" and that when examined in detail his case studies had \"big holes\" and \"do not even begin to add up to a significant counterweight to the initial presumption against reincarnation.\" Stevenson, Edwards wrote, \"evidently lives in a cloud-cuckoo-land.\" \n\nChampe Ransom, whom Stevenson hired as an assistant in the 1970s, wrote an unpublished report about Stevenson's work, which Edwards cites in his \"Immortality\" (1992) and \"Reincarnation\" (1996). According to Ransom, Edwards wrote, Stevenson asked the children leading questions, filled in gaps in the narrative, did not spend enough time interviewing them, and left too long a period between the claimed recall and the interview; it was often years after the first mention of a recall that Stevenson learned about it. In only eleven of the 1,111 cases Ransom looked at had there been no contact between the families of the deceased and of the child before the interview; in addition, according to Ransom, seven of those eleven cases were seriously flawed. He also wrote that there were problems with the way Stevenson presented the cases, in that he would report his witnesses' conclusions, rather than the data upon which the conclusions rested. Weaknesses in cases would be reported in a separate part of his books, rather than during the discussion of the cases themselves. Ransom concluded that it all amounted to anecdotal evidence of the weakest kind.\n\nIn \"Death and Personal Survival\" (1992), Almeder holds that Ransom was false in stating that there were only 11 cases with no prior contact between the two families concerned.. According to Almeder there were 23 such cases.\n\nEdwards cited the case of Corliss Chotkin, Jr., in Angoon, Alaska, described in Stevenson’s \"Twenty Cases Suggestive of Reincarnation\" (1966), as an example that relied entirely on the word of one woman, the niece of Victor Vincent, a fisherman. (Victor Vincent was the person whose life Corliss Chotkin, Jr., seemed to remember.) Edwards wrote that, among the many weaknesses in the case, the family were religious believers in reincarnation, Chotkin had birthmarks that were said to have resembled scars that Vincent had but Stevenson had not seen Vincent's scars, and all the significant details relied on the niece. Edwards said that Stevenson offered no information about her, except that several people told him she had a tendency, as Stevenson put it, to embellish or invent stories. Edwards wrote that similar weaknesses could be found in all Stevenson's case studies. In Stevenson’s defense, Robert Almeder wrote in 1997 that the Chotkin case was one of Stevenson's weaker ones.\n\nEdwards charged that Stevenson referred to himself as a scientist but did not act like one. According to Edwards, he failed to respond to, or even mention, significant objections; the large bibliography in Stevenson's \"Children Who Remember Previous Lives\" (1987) does not include one paper or book from his opponents.\n\nIn support of Stevenson, Almeder argued in \"Death and Personal Survival\" that Edwards had begged the question by stating in advance that the idea of consciousness existing without the brain in the interval between lives was incredible, and that Edwards's \"dogmatic materialism\" had forced him to the view that Stevenson's case studies must be examples of fraud or delusional thinking. According to Almeder, the possibility of fraud was indeed investigated in the cases Edwards mentioned.\n\nStevenson wrote an introduction to a book, \"Second Time Round\" (1975), in which Edward Ryall, an Englishman, told of what he believed to be his memories of a past life as John Fletcher, a man who was born in 1645 in Taunton, England, and died forty years later near his home in Westonzoyland, Somerset. Stevenson investigated the case and discovered that some of the historical features from Ryall's book were accurate. Stevenson wrote, \"I think it most probable that he has memories of a real previous life and that he is indeed John Fletcher reborn, as he believes himself to be\". In 1976, however, John Taylor discovered that none of the available church records at the Westonzoyland church from 1645 to 1685 had entries for births, marriages, or deaths for the name Fletcher. Since no trace of the name could be found, he concluded that no man called John Fletcher actually existed and that the supposed memories were a fantasy Ryall had developed over the years. Stevenson later altered his opinion about the case. In his book \"European Cases of the Reincarnation Type,\" he wrote, \"I can no longer believe that \"all\" of Edward Ryall's apparent memories derive from a previous life, because some of his details are clearly wrong,\" but he still suggested that Ryall acquired some information about 17th-century Somerset by paranormal means.\n\nIn an article published on \"Scientific American's\" website in 2013, favorably reviewing Stevenson's work, Jesse Bering, a professor of science communication, wrote, \"Towards the end of her own storied life, the physicist Doris Kuhlmann-Wilsdorf—whose groundbreaking theories on surface physics earned her the prestigious Heyn Medal from the German Society for Material Sciences, surmised that Stevenson’s work had established that 'the statistical probability that reincarnation does in fact occur is so overwhelming … that cumulatively the evidence is not inferior to that for most if not all branches of science.' \"\n\nAlthough Stevenson mainly focused on cases of children who seemed to remember past lives, he also studied two cases in which adults under hypnosis seemed to remember a past life and show rudimentary use of a language they had not learned in the present life. Stevenson called this phenomenon \"xenoglossy.\" The linguist Sarah Thomason, critiquing these cases, wrote that Stevenson is \"unsophisticated about language\" and that the cases are unconvincing. Thomason concluded, \"the linguistic evidence is too weak to provide support for the claims of xenoglossy.\" William J. Samarin, a linguist from the University of Toronto, wrote that Stevenson corresponded with linguists in a selective and unprofessional manner. He said that Stevenson corresponded with one linguist in a period of six years \"without raising any discussion about the kinds of thing that linguists would need to know.\" Another linguist, William Frawley, wrote, \"Stevenson does not consider enough linguistic evidence in these cases to warrant his metaphysics.\"\n\nStevenson stepped down as director of the Division of Perceptual Studies in 2002, although he continued to work as Research Professor of Psychiatry. Bruce Greyson, editor of the \"Journal of Near-Death Studies\", became director of the division. Jim Tucker, the department's associate professor of psychiatry and neurobehavioral sciences, continued Stevenson's research with children, examined in Tucker's book, \"Life Before Life: A Scientific Investigation of Children's Memories of Previous Lives\" (2005). Stevenson died of pneumonia in February 2007 at his retirement home in Charlottesville, Virginia. In his will he endowed the Stevenson Chair in the Philosophy and History of Science including Medicine, at McGill University Department of Social Studies of Medicine. The inaugural chairholder is Professor Annmarie Adams.\n\nAs one experiment to test for personal survival of bodily death, in the 1960s Stevenson set a combination lock using a secret word or phrase and placed it in a filing cabinet in the department, telling his colleagues he would try to pass the code to them after his death. Emily Williams Kelly told \"The New York Times\": \"Presumably, if someone had a vivid dream about him, in which there seemed to be a word or a phrase that kept being repeated—I don't quite know how it would work—if it seemed promising enough, we would try to open it using the combination suggested.\" The Morning News reported in October 2014 that the lock was still unopened.\n\n\n\n\n\n\n"}
{"id": "20375066", "url": "https://en.wikipedia.org/wiki?curid=20375066", "title": "Iatromathematicians", "text": "Iatromathematicians\n\nIatromathematicians (from Greek ἰατρική \"medicine\" and μαθηματικά \"mathematics\") were a school of physicians in 17th-century Italy who tried to apply the laws of mathematics and mechanics in order to understand the functioning of the human body. They were also keen students of anatomy. These iantromathematicians made an effort to prove that applying a purely mechanical conception to the study of the human body is futile. The mechanical conceptions that they had referred to was Leonardo da Vinci’s studies of the human body, and the writings of Aristotle about the motion of animals related to geometric analysis. Iantromathematicians considered the bodies functioning to be measured by quantifiable numbers, weights, and measures.\n\nThe field of iatromathematics is allied to science; however, it lacks the applicability of the proper scientific method and is therefore considered a form of pseudoscience. It applies the study of astrology to medicine.\n\nIatromathematicians viewed the human body through astrological reasoning as well as mechanics. They associate various stars, or zodiac signs with the functioning of the human body. The twelve astrological signs contribute to each part of the body from head to toe. Moreover, planets and existing cosmos in space are correlated with certain parts of the body. Through examining a natal chart, iatromathematicians attempt to predict biological setbacks in an individual.\n\nIatromathematicians examine the active and energetic temperament of the human body. Moreover, they explore the causes of various health problems and attempt to find ways to treat certain detrimental diseases. In iatromathematics, there is a particular assumption that there is an impact of various energetic fields caused on the star bodies. The star body of an individual is often referred to by astrologists as an energetic matrix and is believed to be spawned by heavenly bodies such as the sun, moon, planets, and several other astrological signs.\n\nIatromathematicians study these conceptions and try to regulate the path of the star body of individuals so that it will give a positive, rather than a negative result. By doing so, they believe that it will contribute to a healthier lifestyle. Its doctrine is based on cosmobiology in which several emotional and physiological dilemmas in the body are associated with the positioning of celestial bodies in outer space.\n\nIatromathematics is closely correlated with biomechanics because the field of biomechanics investigates macrobiotic bodies to a macroscopic degree through the appliance of several engineering principles. The perspective of iatromathematicians differed from that of iatrophysicists and iatrochemists in terms of the way human bodies function. Iatrophysicists predicted the deviations from the biological norm of the body through the appliance of physics, while iatrochemists measured the detrimental problems of the body by chemical means.\n\nSeveral individuals contributed to this field study of iatromathematics. For example, Ibn Ezra (Rabbi Avraham Ben Meir Ibn Ezra) wrote nine different astrological treatises. He covered all the subsections of astrology which include the branches of natal, medicinal, horary, electional, and mundane. Ibn Ezra’s best known work was known as \"The Beginning of Wisdom\". Over time, various individuals studied his works comprehensively. One such person was George Sarton, who is the founder of the History of Science Society.\n\nRecently, Archibald Pitcairne was mentioned as the \"forgotten father of mathematical medicine\" and his contribution praised to creating the bases of iatromathematics.\n\n\n"}
{"id": "49227605", "url": "https://en.wikipedia.org/wiki?curid=49227605", "title": "Laboratory for the Conservation of Endangered Species", "text": "Laboratory for the Conservation of Endangered Species\n\nLaCONES or Laboratory for the Conservation of Endangered Species, is a Council of Scientific and Industrial Research lab located in Hyderabad. It was conceptualised by Lalji Singh. It is India's only research facility engaged in conservation and preservation of wildlife and its resources. It was established in 1998 with the help of Central Zoo Authority of India, CSIR and the government of Andhra Pradesh. It was dedicated to the nation in 2007 by then President of India APJ Abdul Kalam.It is a part of ccmb (centre for cellular and molecular biology).India's 1st genetic bank for wilflife conservation nwgrb (national wildlife genetic resource bank) stablished by government in lacones\n"}
{"id": "821148", "url": "https://en.wikipedia.org/wiki?curid=821148", "title": "Level of measurement", "text": "Level of measurement\n\nLevel of measurement or scale of measure is a classification that describes the nature of information within the values assigned to variables. Psychologist Stanley Smith Stevens developed the best-known classification with four levels, or scales, of measurement: nominal, ordinal, interval, and ratio. This framework of distinguishing levels of measurement originated in psychology and is widely criticized by scholars in other disciplines. Other classifications include those by Mosteller and Tukey, and by Chrisman.\n\nStevens proposed his typology in a 1946 \"Science\" article titled \"On the theory of scales of measurement\". In that article, Stevens claimed that all measurement in science was conducted using four different types of scales that he called \"nominal,\" \"ordinal,\" \"interval,\" and \"ratio,\" unifying both \"qualitative\" (which are described by his \"nominal\" type) and \"quantitative\" (to a different degree, all the rest of his scales). The concept of scale types later received the mathematical rigour that it lacked at its inception with the work of mathematical psychologists Theodore Alper (1985, 1987), Louis Narens (1981a, b), and R. Duncan Luce (1986, 1987, 2001). As Luce (1997, p. 395) wrote:\nTo give a better overview the values in 'Mathematical Operators', 'Advanced operations' and 'Central tendency' are only the ones this level of measurement introduces. The complete list includes the values of previous levels. This is inverted for the 'Measure property'. \nThe nominal type differentiates between items or subjects based only on their names or (meta-)categories and other qualitative classifications they belong to; thus dichotomous data involves the construction of classifications as well as the classification of items. Discovery of an exception to a classification can be viewed as progress. Numbers may be used to represent the variables but the numbers do not have numerical value or relationship: for example, a globally unique identifier.\n\nExamples of these classifications include gender, nationality, ethnicity, language, genre, style, biological species, and form. In a university one could also use hall of affiliation as an example. Other concrete examples are \n\nNominal scales were often called qualitative scales, and measurements made on qualitative scales were called qualitative data. However, the rise of qualitative research has made this usage confusing. The numbers in nominal measurement are assigned as labels and have no specific numerical value or meaning. No form of arithmetic computation (+, −, ×, etc.) may be performed on nominal measures. The nominal level is the lowest measurement level used from a statistical point of view.\n\nEquality and other operations that can be defined in terms of equality, such as inequality and set membership, are the only non-trivial operations that generically apply to objects of the nominal type.\n\nThe mode, i.e. the \"most common\" item, is allowed as the measure of central tendency for the nominal type. On the other hand, the median, i.e. the \"middle-ranked\" item, makes no sense for the nominal type of data since ranking is meaningless for the nominal type.\n\nThe ordinal type allows for rank order (1st, 2nd, 3rd, etc.) by which data can be sorted, but still does not allow for relative \"degree of difference\" between them. Examples include, on one hand, dichotomous data with dichotomous (or dichotomized) values such as 'sick' vs. 'healthy' when measuring health, 'guilty' vs. 'not-guilty' when making judgments in courts, 'wrong/false' vs. 'right/true' when measuring truth value, and, on the other hand, non-dichotomous data consisting of a spectrum of values, such as 'completely agree', 'mostly agree', 'mostly disagree', 'completely disagree' when measuring opinion.\n\nThe median, i.e. \"middle-ranked\", item is allowed as the measure of central tendency; however, the mean (or average) as the measure of central tendency is not allowed. The mode is allowed.\n\nIn 1946, Stevens observed that psychological measurement, such as measurement of opinions, usually operates on ordinal scales; thus means and standard deviations have no validity, but they can be used to get ideas for how to improve operationalization of variables used in questionnaires. Most psychological data collected by psychometric instruments and tests, measuring cognitive and other abilities, are ordinal, although some theoreticians have argued they can be treated as interval or ratio scales. However, there is little prima facie evidence to suggest that such attributes are anything more than ordinal (Cliff, 1996; Cliff & Keats, 2003; Michell, 2008). In particular, IQ scores reflect an ordinal scale, in which all scores are meaningful for comparison only. There is no absolute zero, and a 10-point difference may carry different meanings at different points of the scale.\n\nThe interval type allows for the \"degree of difference\" between items, but not the ratio between them. Examples include \"temperature\" with the Celsius scale, which has two defined points (the freezing and boiling point of water at specific conditions) and then separated into 100 intervals, \"date\" when measured from an arbitrary epoch (such as AD), \"percentage\" such as a percentage return on a stock, \"location\" in Cartesian coordinates, and \"direction\" measured in degrees from true or magnetic north. Ratios are not meaningful since 20 °C cannot be said to be \"twice as hot\" as 10 °C, nor can multiplication/division be carried out between any two dates directly. However, \"ratios of differences\" can be expressed; for example, one difference can be twice another. Interval type variables are sometimes also called \"scaled variables\", but the formal mathematical term is an affine space (in this case an affine line).\n\nThe mode, median, and arithmetic mean are allowed to measure central tendency of interval variables, while measures of statistical dispersion include range and standard deviation. Since one can only divide by \"differences\", one cannot define measures that require some ratios, such as the coefficient of variation. More subtly, while one can define moments about the origin, only central moments are meaningful, since the choice of origin is arbitrary. One can define standardized moments, since ratios of differences are meaningful, but one cannot define the coefficient of variation, since the mean is a moment about the origin, unlike the standard deviation, which is (the square root of) a central moment.\n\nThe ratio type takes its name from the fact that measurement is the estimation of the ratio between a magnitude of a continuous quantity and a unit magnitude of the same kind (Michell, 1997, 1999). A ratio scale possesses a meaningful (unique and non-arbitrary) zero value. Most measurement in the physical sciences and engineering is done on ratio scales. Examples include mass, length, duration, plane angle, energy and electric charge. In contrast to interval scales, ratios are now meaningful because having a non-arbitrary zero point makes it meaningful to say, for example, that one object has \"twice the length.\" Very informally, many ratio scales can be described as specifying \"how much\" of something (i.e. an amount or magnitude) or \"how many\" (a count). The Kelvin temperature scale is a ratio scale because it has a unique, non-arbitrary zero point called absolute zero. \n\nThe geometric mean and the harmonic mean are allowed to measure the central tendency, in addition to the mode, median, and arithmetic mean. The studentized range and the coefficient of variation are allowed to measure statistical dispersion. All statistical measures are allowed because all necessary mathematical operations are defined for the ratio scale.\n\nWhile Stevens's typology is widely adopted, it is still being challenged by other theoreticians, particularly in the cases of the nominal and ordinal types (Michell, 1986).\n\nDuncan (1986) objected to the use of the word \"measurement\" in relation to the nominal type, but Stevens (1975) said of his own definition of measurement that \"the assignment can be any consistent rule. The only rule not allowed would be random assignment, for randomness amounts in effect to a nonrule\". However, so-called nominal measurement involves arbitrary assignment, and the \"permissible transformation\" is any number for any other. This is one of the points made in Lord's (1953) satirical paper \"On the Statistical Treatment of Football Numbers\".\n\nThe use of the mean as a measure of the central tendency for the ordinal type is still debatable among those who accept Stevens's typology. Many behavioural scientists use the mean for ordinal data, anyway. This is often justified on the basis that the ordinal type in behavioural science is in fact somewhere between the true ordinal and interval types; although the interval difference between two ordinal ranks is not constant, it is often of the same order of magnitude.\n\nFor example, applications of measurement models in educational contexts often indicate that total scores have a fairly linear relationship with measurements across the range of an assessment. Thus, some argue that so long as the unknown interval difference between ordinal scale ranks is not too variable, interval scale statistics such as means can meaningfully be used on ordinal scale variables. Statistical analysis software such as SPSS requires the user to select the appropriate measurement class for each variable. This ensures that subsequent user errors cannot inadvertently perform meaningless analyses (for example correlation analysis with a variable on a nominal level).\n\nL. L. Thurstone made progress toward developing a justification for obtaining the interval type, based on the law of comparative judgment. A common application of the law is the analytic hierarchy process. Further progress was made by Georg Rasch (1960), who developed the probabilistic Rasch model that provides a theoretical basis and justification for obtaining interval-level measurements from counts of observations such as total scores on assessments.\n\nTypologies aside from Stevens's typology has been proposed. For instance, Mosteller and Tukey (1977), Nelder (1990) described continuous counts, continuous ratios, count ratios, and categorical modes of data. See also Chrisman (1998), van den Berg (1991).\n\nMosteller and Tukey noted that the four levels are not exhaustive and proposed:\n\nFor example, percentages (a variation on fractions in the Mosteller-Tukey framework) do not fit well into Stevens’s framework: No transformation is fully admissible.\n\nNicholas R. Chrisman introduced an expanded list of levels of measurement to account for various measurements that do not necessarily fit with the traditional notions of levels of measurement. Measurements bound to a range and repeating (like degrees in a circle, clock time, etc.), graded membership categories, and other types of measurement do not fit to Stevens's original work, leading to the introduction of six new levels of measurement, for a total of ten:\n\nWhile some claim that the extended levels of measurement are rarely used outside of academic geography, graded membership is central to fuzzy set theory, while absolute measurements include probabilities and the plausibility and ignorance in Dempster-Shafer theory. Cyclical ratio measurements include angles and times. Counts appear to be ratio measurements, but the scale is not arbitrary and fractional counts are commonly meaningless. Log-interval measurements are commonly displayed in stock market graphics. All these types of measurements are commonly used outside academic geography, and do not fit well to Stevens' original work.\n\nThe theory of scale types is the intellectual handmaiden to Stevens's \"operational theory of measurement\", which was to become definitive within psychology and the behavioral sciences, despite Michell's characterization as its being quite at odds with measurement in the natural sciences (Michell, 1999). Essentially, the operational theory of measurement was a reaction to the conclusions of a committee established in 1932 by the British Association for the Advancement of Science to investigate the possibility of genuine scientific measurement in the psychological and behavioral sciences. This committee, which became known as the \"Ferguson committee\", published a Final Report (Ferguson, et al., 1940, p. 245) in which Stevens's sone scale (Stevens & Davis, 1938) was an object of criticism:\n\nThat is, if Stevens's \"sone\" scale genuinely measured the intensity of auditory sensations, then evidence for such sensations as being quantitative attributes needed to be produced. The evidence needed was the presence of \"additive structure\" – a concept comprehensively treated by the German mathematician Otto Hölder (Hölder, 1901). Given that the physicist and measurement theorist Norman Robert Campbell dominated the Ferguson committee's deliberations, the committee concluded that measurement in the social sciences was impossible due to the lack of concatenation operations. This conclusion was later rendered false by the discovery of the theory of conjoint measurement by Debreu (1960) and independently by Luce & Tukey (1964). However, Stevens's reaction was not to conduct experiments to test for the presence of additive structure in sensations, but instead to render the conclusions of the Ferguson committee null and void by proposing a new theory of measurement:\n\nStevens was greatly influenced by the ideas of another Harvard academic, the Nobel laureate physicist Percy Bridgman (1927), whose doctrine of \"operationism\" Stevens used to define measurement. In Stevens's definition, for example, it is the use of a tape measure that defines length (the object of measurement) as being measurable (and so by implication quantitative). Critics of operationism object that it confuses the relations between two objects or events for properties of one of those of objects or events (Hardcastle, 1995; Michell, 1999; Moyer, 1981a,b; Rogers, 1989).\n\nThe Canadian measurement theorist William Rozeboom (1966) was an early and trenchant critic of Stevens's theory of scale types.\n\nAnother issue is that the same variable may be a different scale type depending on how it is measured and on the goals of the analysis. For example, hair color is usually thought of as a nominal variable, since it has no apparent ordering. However, it is possible to order colors (including hair colors) in various ways, including by hue; this is known as colorimetry. Hue is an interval level variable.\n\n\n"}
{"id": "640228", "url": "https://en.wikipedia.org/wiki?curid=640228", "title": "List of mnemonics", "text": "List of mnemonics\n\nThis article contains lists of mnemonics used to remember various objects, lists etc.\n\n\n\n\n\n\n\n\n\nThe articulation of the quadratic equation can be sung to the tune of various songs as a mnemonic device.\n\nFor helping students in remembering the rules in adding and multiplying two signed numbers, Balbuena and Buayan (2015) made the letter strategies LAUS (like signs, add; unlike signs, subtract) and LPUN (like signs, positive; unlike signs, negative), respectively.\n<br>\nOrder of Operations\n<br>\nPEMDAS\n<br>\nPlease - Parenthesis\n<br>\nExcuse - Exponents\n<br>\nMy - Multiplication\n<br>\nDear - Division\n<br>\nAunt - Addition\n<br>\nSally - Subtraction\n<br>\n\nPEMDAS\n<br>\nPineapples - Parenthesis\n<br>\nEat - Exponents\n<br>\nMangoes - Multiplication\n<br>\nDuring - Division\n<br>\nAutumn - Addition\n<br>\nSeason - Subtraction\n\n'A OF A OF A'\n\nThenar (lateral to medial-palmar surface):\nHypothenar (lateral to medial-palmar surface):\n\n\n\n\nThus we get the names of the strings from 6th string to the 1st string in that order.\n\nConversely, a mnemonic listing the strings in the reverse order is:\n\nAs for guitar tuning, there is also a mnemonic for ukuleles. \n\n\n\n\n\n\n\n\n\n\n\nIn most words like friend, field, piece, pierce, mischief, thief, tier, it is \"\"i\" which comes before \"e\". But on some words with c just before the pair of e and i, like receive, perceive, \"e\" comes before \"i\"\". This can be remembered by the following mnemonic,\n\nBut this is not always obeyed as in case of weird and weigh, weight, height, neighbor etc. and can be remembered by extending that mnemonic as given below\n\nAnother variant, which avoids confusion when the two letters represent different sounds instead of a single sound, as in atheist or being, runs\n\nMost frequently \"u\" follows \"q\". e.g.: Que, queen, question, quack, quark, quartz, quarry, quit, Pique, torque, macaque, exchequer. Hence the mnemonic:\n\n\nAdvice, Practice, Licence etc. (those with c) are nouns and Advise, Practise, License etc. are verbs.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "4647220", "url": "https://en.wikipedia.org/wiki?curid=4647220", "title": "List of multiple discoveries", "text": "List of multiple discoveries\n\nHistorians and sociologists have remarked the occurrence, in science, of \"multiple independent discovery\". Robert K. Merton defined such \"multiples\" as instances in which similar discoveries are made by scientists working independently of each other. \"Sometimes,\" writes Merton, \"the discoveries are simultaneous or almost so; sometimes a scientist will make a new discovery which, unknown to him, somebody else has made years before.\"\n\nCommonly cited examples of multiple independent discovery are the 17th-century independent formulation of calculus by Isaac Newton, Gottfried Wilhelm Leibniz and others, described by A. Rupert Hall; the 18th-century discovery of oxygen by Carl Wilhelm Scheele, Joseph Priestley, Antoine Lavoisier and others; and the theory of the evolution of species, independently advanced in the 19th century by Charles Darwin and Alfred Russel Wallace.\n\nMultiple independent discovery, however, is not limited to only a few historic instances involving giants of scientific research. Merton believed that it is multiple discoveries, rather than unique ones, that represent the \"common\" pattern in science.\n\nMerton contrasted a \"multiple\" with a \"singleton\"—a discovery that has been made uniquely by a single scientist or group of scientists working together.\n\nA distinction is drawn between a discovery and an invention, as discussed for example by Bolesław Prus. However, since the same phenomenon of multiplicity occurs in relation to both discoveries and inventions, this article lists both multiple discoveries and multiple \"inventions\".\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "256043", "url": "https://en.wikipedia.org/wiki?curid=256043", "title": "List of science magazines", "text": "List of science magazines\n\nA science magazine is a periodical publication with news, opinions and reports about science, generally written for a non-expert audience. In contrast, a periodical publication, usually including primary research and/or reviews, that is written by scientific experts is called a \"scientific journal\". Science magazines are read by non-scientists and scientists who want accessible information on fields outside their specialization.\n\nArticles in science magazines are sometimes republished or summarized by the general press.\n\n\n\n\n"}
{"id": "41885308", "url": "https://en.wikipedia.org/wiki?curid=41885308", "title": "Living educational theory", "text": "Living educational theory\n\nLiving educational theory (LET) is a research method in educational research.\n\nThe idea of action research as a living practice entered the mainstream of action research from the book, \"Action Research as a Living Practice\" by Terrance Carson and Dennis Sumara in 1997. Carson and Sumara transformed the concept of traditional action research with the idea that, ...\" participation in action research practices are particular ways of living and understanding that require more of the researcher than the \"application\" of research methods. Rather, action research is a lived practice that requires that the researcher not only investigate the subject at hand but, as well, provide some account of the way in which the investigation both shapes and is shaped by the investigator . This requires what Martin Buber called an \"I-Thou\" approach toward other and this approach applied to action research as well. To make Buber's language more modern and accessible, LET translated Buber's \"I-Thou\" approach toward another human being to an \"I/you/we\" approach to action research. This differs greatly from an approach to living theory action research imagined by Jack Whitehead (2002) where he imagines living theory action research as forming an \"I-theory\" of knowledge. Director of the Philosophy for Children Project at Notre Dame de Namur University William Barry proposes LET focuses on the connections between the researcher and the other person or subject where the lives of action researchers are inextricable linked in a profound manner with the individuals and communities involved in the subject of study. LET from a Barryian perspective is a critical theory and emancipatory action research approach which seeks the dialectic, not debate and battles of [discourse].\n\nA major difference of William Barry's version of living educational theory, which was the focus of his successful completion of a Ph.D. thesis at Nottingham Trent University, UK, is the essential question behind the living educational theory approach to action research (2012b). The question is not \"How can I generate a living legacy for myself through an I-It theory approach toward knowledge and other forms of life?\" Rather the essential question is, \"How does one conduct a life that includes the practice of educational action research?\" The theory/practice problem disappears when honesty about one's biases regarding spiritual, existential, and emotional intelligence are made clear in the action research process.\n\nThe phraseology \"educational theory\" originated with the work of Jack Whitehead, a former lecturer at the University of Bath, and it was further developed and greatly improved methodologically by Jean McNiff in 2009 because of her willingness to be transparent about her values and intentions. Whitehead's main emphasis for conducting research is to promote the individual under the guise of collaboration and research outcomes must be captured on video for authentic validation. Whitehead's view of action research promotes that living educational theory (he uses living theory and living educational theory interchangeably so it is difficult for a reader to know what he is writing about) should be aimed at the bringing of energy-flowing values as explanatory principles and standards of judgments into the Academy for the legitimation of living educational theories (Whitehead 2008). In the eyes of Whitehead radical constructivism is at the core of living educational theory research. In 2013, Whitehead and McNiff separated as collaborators as McNiff saw spiritual, emotional intelligence as key to action research while Whitehead disagreed and believed that media accounts (primarily video tapping people) of action research could provide clues to virtues which held the future of humanity though energy flowing examples of collaboration. McNiff stated in a May 2013 conference she San Francisco, California (ARNA Conference) that she would never appear or work with Whitehead again. She repeated this message again months later at a UK conference in York. American William Barry believed the concept of LET was too important and found a dialectic between McNiff and Whitehead and he created a new understanding of LET which was presented at a three-day international conference in 2013 at Liverpool Hope University titled, \"Researching Our Own Practice\" .\n\nLiving educational theory was first clearly defined and developed by California Professor of Philosophy William Barry (2012b) in Liz Atkins and Susan Wallace's book , \"Qualitative Research in Education\", co-published by Sage and the British Educational Research Association (BERA). This book was one of four in a series sponsored by the BERA regarding best practice progressive research methods in educational research. The originality and uniqueness of Barry's development of living educational theory (LET) action research is the importance of gaining \"ontological weight\" through the action research process. Ontological weight empowers the researcher's ability, and the ability of other people involved in the action research project, to have the research experience and focus of the research be transformational and add, or at least reinforce, a sense of meaning in learning and life. Barry was influenced to use the concept \"ontological weight\" by the existentialist Catholic philosopher Gabriel Marcel (1963).\n\nThe idea of action research as a living practice entered the mainstream of action research from the book, \"Action Research as a Living Practice\" by Terrance Carson and Dennis Sumara in 1997. The term \"educational theory\" originated with the work of Jack Whitehead, a former lecturer at the University of Bath, and it was further developed and greatly improved methodologically by Jean McNiff in 2009. Whitehead's main emphasis for conducting research is to promote the individual under the guise of collaboration and research outcomes must be captured on video for authentic validation. Whitehead's view of action research promotes that living educational theory (he uses living theory and living educational theory interchangeably so it is difficult for a reader to know what he is writing about) should be aimed at the bringing of energy-flowing values as explanatory principles and standards of judgments into the Academy for the legitimation of living educational theories (Whitehead 2008). In the eyes of Whitehead, radical constructivism is at the core of living educational theory research.\n\nBarry was asked by the BERA sponsored authors to reflect on the nature of living educational theory (LET) because there existed no clear definition of LET in the literature. Barry was asked because he had successfully used LET in an innovative fashion, and was the first to clearly define LET as based in critical theory which embraced transpersonal psychology through his earned 2012 PhD thesis at Nottingham Trent University Nottingham, UK. He proposed LET as a way of challenging the oppressive use of power using critical theory in a need fulfilling way (Glasser 1998). Barry proposed the following definition and approach to action research he calls living educational theory and his approach has been used as an action research method in undergraduate and graduate courses and research at Notre Dame de Namur University in Silicon Valley, California as well as by other researchers around the world.\n\nBarry explained that living educational theory \"[is] a critical and transformational approach to action research. It confronts the researcher to challenge the status quo of their educational practice and to answer the question, 'How can I improve that I'm doing?' Researchers who use this approach must be willing to recognize and assume responsibility for being a 'living contradiction' in their professional practice – thinking one way and acting in another. The mission of the LET action researcher is to overcome workplace norms and self – behavior which contradict the researcher's values and beliefs. The vision of the LET researcher is to make an original contribution to knowledge through generating an educational theory proven to improve the learning of people within a social learning space. The standard of judgment for theory validity is evidence of workplace reform, transformational growth of the researcher, and improved learning by the people researcher claimed to have influenced...\" .\n\nBarry's LET approach to action research was heavily influenced by action researchers focused on emancipatory social change, collaboration, and liberation theology (2012a). Prominent developers of LET, without whose work LET would most likely never had been developed by Barry, are notable action researchers, educators, and philosophers such as Martin Buber's (1970) conception of 'I and Thou' and Krishnamurti's (1953) liberation pedagogy emphasizing education as significant to leading a quality filled life; Paulo Freire (1998 and 1970) and his concept of participatory action research and the need to be politically aware; the work of Carr and Kremmis (1986) and Habermas (1992) and their concept and building critical educational knowledge; Professor Manheimer of UNC (1999) and his challenge to his readers to enfold the past into the living present in order to become historical to oneself and then strive to linking life times with each other; Apple (1982) and Michel Foucault (1990) and the role of power and politics in education and Joe Kincheloe (2008), Henry Giroux (1997), and Peter McLaren (1989) and their promotion of critical pedagogy.\n\nProf. Barry was the first Ph.D. researcher to successfully use living educational theory in conjunction with neuro-linguistic programming (NLP), spiral dynamics and autoethnography (based on a multiple intelligences model which includes spiritual and emotional intelligence and embraces transpersonal knowing) as valid methods of research working under the methodological umbrella of phenomenology and hermeneutics. His LET approach to Ph.D.level action research led to the unique use of fictional storytelling as a vehicle by which to replace the traditional literature review chapter in Ph.D. research but in a more rigorous and creative fashion. The process of storytelling allows the researcher to exercise their emotional intelligence in a superior manner than the traditional research literature review allows.\n\nLiving educational theory as defined and created by Barry is part of the curriculum of multiple courses at Notre Dame de Namur University located in Silicon Valley, California in their credentialing program for teacher education. Barry's Living Educational Theory Action Research Method is based on a six step process based on research questions that normally start from the format, \" How can I influence the transformation of...?\" or \"How can I contribute to the improvement of...?\" The research is dialectical in nature:\n\n\n\n\n"}
{"id": "21511851", "url": "https://en.wikipedia.org/wiki?curid=21511851", "title": "March of Progress", "text": "March of Progress\n\nThe March of Progress, properly called The Road to Homo Sapiens, is an illustration that presents 25 million years of human evolution. It was created for the \"Early Man\" volume of the \"Life Nature Library\", published in 1965.\n\nIt has been viewed as a picture of the discredited view (orthogenesis) that evolution is progressive. As such, it has been widely parodied and imitated to create images of progress of other kinds.\n\nThe picture of progress in evolution was anticipated by Thomas Henry Huxley's 1863 \"Evidence as to Man's Place in Nature\".\n\nThe illustration is part of a section of text and images commissioned by Time-Life Books for the \"Early Man\" volume (1965) of the \"Life Nature Library\", by F. Clark Howell.\n\nThe illustration is a foldout entitled \"The Road to Homo Sapiens\". It shows a sequence of figures, drawn by natural history painter and muralist Rudolph Zallinger (1919–1995). The 15 human evolutionary forebears are lined up as if they were marching in a parade from left to right. The first two sentences of the caption read \"What were the stages of man's long march from apelike ancestors to \"sapiens\"? Beginning at right and progressing across four more pages are milestones of primate and human evolution as scientists know them today, pieced together from the fragmentary fossil evidence.\"\n\nThe 15 primate figures in Zallinger's image, from left to right, are listed below. The datings follow the original graphic and may no longer reflect current scientific opinion.\n\n\nContrary to appearances and some complaints, the original 1965 text of \"The Road to Homo Sapiens\" reveals an understanding of the fact that a linear presentation of a sequence of primate species, all in the direct line of human ancestors, would not be a correct interpretation. For example, the fourth of Zallinger's figures (\"Oreopithecus\") is said to be \"a likely side branch on man's family tree\". Only the next figure (\"Ramapithecus\") is described as \"now thought by some experts to be the oldest of man's ancestors in a direct line\" (something no longer considered likely). That implies that the first four primates are not to be considered actual human ancestors. Likewise, the seventh figure (\"Paranthropus\") is said to be \"an evolutionary dead end\". In addition, the colored stripes, across the top of the figure, which indicate the age and duration of the various lineages clearly imply that there is no evidence of direct continuity between extinct and extant lineages and also, multiple lineages of the figured hominids occurred contemporaneously at several points in the history of the group.\n\nThe image has frequently been copied, modified, and parodied. It has been criticized as \"unintentionally and wrongly\" implying that \"evolution is progressive\".\n\nWith regard to the way the illustration has been interpreted, the anthropologist and author of the section, F. Clark Howell, remarked:\nStephen Jay Gould (1941–2002) condemned the iconology of the image in several pages of his 1989 book, \"Wonderful Life\", reproducing several advertisements and political cartoons that make use of the illustration to make their various points. In a chapter, \"The Iconography of an Expectation\", he asserted that\n\nThe intelligent design advocate Jonathan Wells wrote in \"\" (2002), \"Although it is widely used to show that we are just animals, and that our very existence is a mere accident, the ultimate icon goes far beyond the evidence.\" The book likens a selection of evolution theory textbook topics to the cover illustration thus qualified.\n\nBrian Switek, writing for \"Scientific American\", argues that the idea of a \"march of progress\", as depicted in the 1965 Time-Life illustration, dates back to the medieval great chain of being and the 19th century idea of the \"missing link\" in the fossil record. In his view, to understand life and evolution, \"step one involves casting out types of imagery which constrain rather than enlighten.\" Writing in \"Wired\", Switek added that \"There is perhaps no other illustration that is as immediately recognizable as representing evolution, but the tragedy of this is that it conveys a view of life that does not resemble our present understanding of life's history.\"\n\n\nThomas Henry Huxley's frontispiece to his 1863 book \"Evidence as to Man's Place in Nature\" was intended simply to compare the skeletons of apes and humans, but its unintentional left-to-right progressionist sequence has according to the historian Jennifer Tucker \"become an iconic and instantly recognizable visual shorthand for evolution\".\n\nAn illustration, with the caption \"Evolution\", showing two sequences of four images, each illustrating a gradual transformation of an animal into a human, appeared in the 1889 edition of Mark Twain's \"A Connecticut Yankee In King Arthur's Court\".\n\n"}
{"id": "55071594", "url": "https://en.wikipedia.org/wiki?curid=55071594", "title": "Neutron embrittlement", "text": "Neutron embrittlement\n\nNeutron embrittlement, sometimes more broadly radiation embrittlement, is the embrittlement of various materials due to the action of neutrons. This is primarily seen in nuclear reactors, where the release of high-energy neutrons causes the long-term degradation of the reactor materials. The embrittlement is caused by the microscopic movement of atoms that are hit by the neutrons, this same action also gives rise to neutron-induced swelling causes materials to grow in size, and the Wigner effect causes energy buildup in certain materials that can lead to sudden releases of energy.\n\nNeutron embrittlement mechanisms include:\n\nNeutron irradiation embrittlement limits the service life of reactor-pressure vessels (RPV) in nuclear power plants due to the degradation of reactor materials. In order to perform at high efficiency and safely contain coolant water at temperatures around 290ºC and pressures of ~7 MPa (for boiling water reactors) to 14 MPa (for pressurized water reactors), the RPV must be heavy-section steel. Due to regulations, RPV failure probabilities must be very low. To achieve sufficient safety, the design of the reactor assumes large cracks and extreme loading conditions. Under such conditions, a probable failure mode is rapid, catastrophic fracture if the vessel steel is brittle. Tough RPV base metals that are typically used are A302B, A533B plates, or A508 forgings; these are quenched and tempered, low-alloy steels with primarily tempered bainitic microstructures. Over the past few decades, RPV embrittlement has been addressed by the use of tougher steels with lower trace impurity contents, the decrease of neutron flux that the vessel is subject to, and the elimination of beltline welds. However, embrittlement remains an issue for older reactors.\n\nPressurized water reactors are more susceptible to embrittlement than boiling water reactors. This is due to PWRs sustaining more neutron impacts. To counteract this, many PWRs have a specific core design that reduces the number of neutrons hitting the vessel wall. Moreover, PWR designs must be especially mindful of embrittlement because of pressurized thermal shock, an accident scenario that occurs when cold water enters a pressurized reactor vessel, introducing large thermal stress. This thermal stress may cause fracture if the reactor vessel is sufficiently brittle.\n\n\n"}
{"id": "34139758", "url": "https://en.wikipedia.org/wiki?curid=34139758", "title": "Orr's Circle of the Sciences", "text": "Orr's Circle of the Sciences\n\nOrr's Circle of the Sciences was a scientific encyclopedia of the 1850s, published in London by William Somerville Orr. \n\nWilliam S. Orr & Co. was a publisher in Paternoster Row, London. It put out the \"British Cyclopædia\" in ten volumes of the 1830s. It also was in business selling engravings (for example the Kenny Meadows illustrations to Shakespeare), and maps, such as a mid-century \"Cab Fare and Guide Map of London\" (c. 1853).\n\nThe firm was a general commercial publisher, with a specialist area of natural history, and also published periodicals. It was innovative in its use of wood engraving, in its 1838 edition of \"Paul et Virginie\". In children's literature, it published Christoph von Schmid's \"Basket of Flowers\" in an English translation of 1848, in partnership with J. B. Müller of Stuttgart.\n\nOrr himself was a publishers' agent from the 1830s, and was a close associate of Robert and William Chambers. He printed a London edition of \"Chambers's Edinburgh Journal\" by mid-1832. The arrangement used stereotype plates, and brought the circulation up to 50,000. By 1845 the circulation was declining from its peak, and Orr wrote to Chambers explaining that the market was changing. In 1846 Chambers terminated the arrangement with Orr.\n\n\"Punch\" magazine, set up in 1841, brought in Orr to help with distribution to booksellers and news agents. Orr died in 1873.\n\n\"Orr's Circle of the Sciences\" was announced first as a part publication, a series in weekly parts, price 2d. beginning 5 January 1854. The series editor was John Stevenson Bushnan, who also wrote the introductory section of the first volume.\n"}
{"id": "21324929", "url": "https://en.wikipedia.org/wiki?curid=21324929", "title": "Post office box (electricity)", "text": "Post office box (electricity)\n\nThe post office box was a Wheatstone bridge-style testing device with pegs and spring arms to close electrical circuits and measure properties of the circuit under test.\n\nThe boxes were used in the United Kingdom by engineers from the then General Post Office, who were responsible for UK telecommunications to trace electrical faults, i.e. to determine where a break occurred in a cable which could be several miles in length. It works on the principle of Wheatstone bridge to identify the resistance of wire connected and then by using wire resistivity and cross section calculating length of wire and thus determining where the cable had broken.\nPost office boxes were common pieces of scientific apparatus in the UK O-Level and A-Level schools public examination physics syllabus in the 1960s.\n\nA typical post office box is in a wooden box with a hinged lid and a metal or bakelite panel showing circuit connections. Coils of wire are wound non-inductively, mounted in the body of the box, and have a negligible temperature co-efficient.\n\nPairs of ratio arms are each 10, 100, 1000 ohms. Resistance arms contains a number of coils from 1 to 5000 ohms with a plug for infinite resistance.\n"}
{"id": "16624451", "url": "https://en.wikipedia.org/wiki?curid=16624451", "title": "Research fellow", "text": "Research fellow\n\nA research fellow is an academic research position at a university or a similar research institution, usually for academic staff or faculty members. A research fellow may act either as an independent investigator or under the supervision of a principal investigator.\n\nIn contrast to a research assistant, the position of research fellow normally requires a doctoral degree, or equivalent work experience for instance in industry and research centers. Some research fellows undertake postdoctoral research or have some moderate teaching responsibilities. Research fellow positions vary in different countries and academic institutions. In some cases, they are permanent with the possibility of promotion, while in other instances they are temporary.\n\nIn many universities this position is a career grade of a \"Research Career Pathway\", following on from a postdoctoral position such as Research Associate, and may be open-ended, subject to normal probation regulations. Within such a path, the next two higher career grades are usually senior research fellow and professorial fellow. Although similar to the position of a research fellow, these two positions are research only posts, with the rise of the career grade there will normally be a formal requirement of a moderate amount of teaching and/or supervision (often at postgraduate level). These positions are for researchers with a proven track record of generating research income to fund themselves and producing high-quality research output that is internationally recognised.\n\nIn some universities, research career grades roughly correspond to the grades of the \"Teaching and Scholarship Career Pathways\" in the following way: research fellow—lecturer, professorial fellow—professor, whereas senior research fellow is somewhere between a reader and a senior lecturer. However, at some top universities, a senior research fellowship may be a position of comparable academic standing to a full professorship at these universities, without any teaching requirements.\n\nIn the past, the term research fellow often referred to a junior researcher, who worked on a specific project on a temporary basis. Research fellows tended to be paid either from central university funds or by an outside organisation such as a charity or company, or through an external grant-awarding body such as a research council or a royal society, for example in the Royal Society University Research Fellowship. Particularly in Oxbridge style colleges, research fellows appointed as fellows of a college tended to, or still do, partially receive remuneration in form of college housing and subsistence. Colleges may award junior research fellowships as the equivalent of post-doctoral research posts, lasting for three or four years. In contrast, senior research fellows tended to be established academics, often a professor on sabbatical from another institution, conducting temporally research elsewhere.\n\nIn India, the position of research fellowship is provided to scholars from various streams like science, arts, literature, agriculture to reward their excellence . Research fellowship is funded by government academic and research institutes, and private companies as well. Research fellows research under the supervision of experienced faculty, professor, head of department, Dean on two different posts known as junior research fellow(JRF) and senior research fellow(SRF). Research organisations like ICAR, CSIR, UGC, ICMR, SERB recruit research fellows through National Eligibility Test. After the completion of pre-defined tenure, JRF can be considered for senior research fellowship based on research fellow's performance & interview conducted by committee by research institute, research fellow is working with.\n\nIn the Russian Federation, the position and title research fellow is unknown; however, there is a broadly similar position of (, literally \"scientific worker\"). This position normally requires a degree of Candidate of Sciences approximately corresponding to the PhD. More senior positions normally require, in addition to the aforementioned degree, a track record of publications or certified inventions, as well as practical contributions to major research and development projects.\nResearch fellows in South Africa are considered as the best asset to research organisations and universities. There are highly ranked universities like University of the Witwatersrand, University of Stellenbosch Business School, rhode university which offers fellowship to South Africa's nationals in a certain field of research.\n\nIn some countries, the English term research fellow is sometimes used to refer to the \"holder\" of a \"research fellowship\" that funds research.\n\nIn Germany, institutions such as the Alexander von Humboldt Foundation offer research fellowship for postdoctoral research and refer to the holder as research fellows, while the award holder may formally hold a specific title at his or her home institution (e.g., \"Privatdozent\").\n\n"}
{"id": "1593617", "url": "https://en.wikipedia.org/wiki?curid=1593617", "title": "Retrodiction", "text": "Retrodiction\n\nRetrodiction (also known as postdiction—although this should not be confused with the use of the term in criticisms of parapsychological research) is the act of making a \"prediction\" about the past.\n\nThe activity of retrodiction (or postdiction) involves moving backwards in time, step-by-step, in as many stages as are considered necessary, from the present into the speculated past to establish the ultimate cause of a specific event (for instance, in the case of reverse engineering, forensics, etc.).\n\nGiven that retrodiction is a process in which \"past observations, events and data are used as evidence to infer the process(es) that produced them\" and that diagnosis \"involve[s] going from visible effects such as symptoms, signs and the like to their prior causes\" the essential balance between prediction and retrodiction could be characterized as:\nregardless of whether the prognosis is of the course of the disease in the absence of treatment, or of the application of a specific treatment regimen to a specific disorder in a particular patient:\n\nIn scientific method, the terms \"retrodiction\" or \"postdiction\" are used in several senses.\n\nOne use refers to the act of evaluating a scientific theory by predicting known rather than new events. For example, a theory in physics that claims to extend or replace the standard model but that fails to predict the existence of known particles has not met the test of \"postdiction\".\n\nMichael Clive Price has written: \n\nA retrodiction occurs when already gathered data is accounted for by a later theoretical advance in a more convincing fashion. The advantage of a retrodiction over a prediction is that the already gathered data is more likely to be free of experimenter bias. An example of a retrodiction is the perihelion shift of Mercury which Newtonian mechanics plus gravity was unable, totally, to account for whilst Einstein's general relativity made short work of it.\n\nAnother use refers to a process by which one attempts to test a theory whose predictions are too long-term to be tested by waiting for a future event to occur. Instead, one speculates about uncertain events in the more distant past, and applies the theory to consider how it would have predicted a known event in the less distant past. This is useful in, for example, the fields of archaeology, climatology, evolutionary biology, financial analysis, forensic science, and cosmology.\n\nIn the field of neuroscience, the term \"postdiction\" was introduced by David Eagleman to describe a perceptual process in which the brain collects information after an event before it retrospectively decides what happened at the time of the event (Eagleman and Sejnowski, 2000). Some perceptual illusions in which the brain mistakenly perceives the location of moving stimuli may involve postdiction. Such illusions include the flash lag illusion and the cutaneous rabbit illusion.\n\n\n"}
{"id": "35744977", "url": "https://en.wikipedia.org/wiki?curid=35744977", "title": "STEAM fields", "text": "STEAM fields\n\nSTEAM fields are science, technology, engineering, art, and mathematics, or applied mathematics. STEAM is designed to integrate STEM subjects into various relevant education disciplines. These programs aim to teach students innovation, to think critically and use engineering or technology in imaginative designs or creative approaches to real-world problems while building on students' mathematics and science base. STEAM programs add art to STEM curriculum by drawing on design principles and encouraging creative solutions.\n\nOne early founder of the STEAM initiative is Georgette Yakman, who in addition to raising the idea of adding the arts to the STEM acronym, claims to have found a formal way to link the subjects together and correspond them to the global socioeconomic world: \"Science and Technology, interpreted through Engineering and the Arts, all based in elements of Mathematics.\" She provides professional development training to individual educators and programs on how to use the STEAM framework. In 2009, Senator Mark Warner announced Yakman's nomination as NCTC’s STEAM Teacher of the Year 2009.\n\n\n\nAmerican Lisa La Bonte, CEO of the Arab Youth Venture Foundation based in the United Arab Emirates, uses the STEAM acronym however her work does not include arts integration. In 2007, she created high profile public STEAM programs in 2007 having added an A for 'inspired STEM', with the A standing for Aeronautics, Aviation, Astronomy, Aerospace, Ad Astra! and using all things \"air and space\" as a hook for youth to embark on greater experimentation, studies, and careers in the region's burgeoning space related industries. One of AYVF's best-known programs, \"STEAM@TheMall\" served over 200,000 its first two years at the most popular shopping malls, provided free weekend activity stations such as Mars robotics, science experiments, SkyLab portable planetarium, art/design, and creative writing. In 2008, Sharjah Sheikha Maisa kicked off the \"Design booth for youth for Al Ain Summer S.T.E.A.M. funded by the Foundation created by the Crown Prince of Abu Dhabi\". In 2010, the American Association of Arts & Sciences (AAAS) included a chapter on AYVF's most popular STEAM program in its book, \"Building Mathematical and Scientific Talent in the Broader Middle East and North Africa (BMENA) Region\".\n\n\n"}
{"id": "34403339", "url": "https://en.wikipedia.org/wiki?curid=34403339", "title": "Sage Bionetworks", "text": "Sage Bionetworks\n\nSage Bionetworks is a nonprofit organization in Seattle that promotes open science and patient engagement in the research process. It is led by Lara Mangravite. It was co-founded by Stephen Friend and Eric Schadt.\n\nSage Bionetworks is notable for being an early advocate of open science. The company operates a software platform for collaborative data analysis called Synapse that allows researchers to work together on data curation and computational modeling asynchronously in a manner inspired by GitHub. Synapse also serves as the software infrastructure for running computational challenges. Sage is also developing a citizen-science platform called Bridge.\n\nThe bulk of Sage's scientific results emerge from cancer and neurosciences, with notable contributions to the Cancer Genome Atlas Pan-Cancer project. Another Sage initiative, \"The Resilience Project\" describes itself as a search for individuals who have genetic changes expected to cause severe illness but who remain perfectly healthy. The hope is to yield insight into factors that protect these individuals from disease.\n\nSage Bionetworks was founded in 2009 as a spinout of Merck & Co., who released software, hardware, intellectual property, and staff connected to its Rosetta Inpharmatics unit. A donation from Quintiles provided early funding.\n\nIn March 2011 Sage partnered with CHDI Foundation to develop computer simulations for studying Huntington's disease. At the same time Sage also announced a partnership with Takeda Pharmaceutical Company wherein Sage would do research to identify biological targets for central nervous system diseases.\n\nIn February 2013, Sage Bionetworks partnered with the Dialogue on Reverse Engineering Assessment and Methods (DREAM) project to provide expertise and infrastructure for DREAM Challenges on the Synapse.org platform.\n\n\n"}
{"id": "3549197", "url": "https://en.wikipedia.org/wiki?curid=3549197", "title": "Science Media Centre", "text": "Science Media Centre\n\nThe Science Media Centre is an organisation which formed in 2002,\ntwo years after the United Kingdom House of Lords Select Committee on Science and Technology's third report on \"Science and Society\" in 2000.\n\nThis report stated that while science was generally reported accurately in the mass media, there was a need for the promotion of more expert information at times when science is under attack in the headlines, mentioning the public reaction to GM crops, in particular.\n\nIn order to promote more informed science in the media, the Centre's main function is as a service to journalists, providing background briefings on current scientific issues and facilitating interviews with scientists. Its director is Fiona Fox who is a former member of the Revolutionary Communist Party and a former contributor to its magazine \"Living Marxism. \n\nThe SMC's stated aim is to \"facilitate more scientists to engage with the media, in the hope that the public will have improved access to accurate, evidence-based scientific information about the stories of the day\".\n\nThe setting up of the Science Media Centre was assisted by Susan Greenfield, the director of the Royal Institution of Great Britain. While the Centre is still based in a specially refurbished wing of the Royal Institution, full independence is claimed from all funders and supporters.\n\nThe Science Media Centre is funded by over 60 organisations, with individual donations capped at £12,500 per annum. The SMC receives sponsorship from a range of funders including media organisations, universities, scientific and learned societies, the UK Research Councils, government bodies, Quangos, charities, private donors and corporate bodies. For an up-to-date list of funders, see .\n\nA 2013 article in \"Nature\" stated about the SMC, \"Perhaps the biggest criticism of Fox and the SMC is that they push science too aggressively — acting more as a PR agency than as a source of accurate science information.\"\n\nIn 2002, Ronan Bennett and Alan Rusbridger described the SMC as a lobby group.\n\nDuring Professor Greenfield's term as Thinker in Residence in South Australia, a new Australian Science Media Centre was set up in Adelaide, Australia in August 2005. Science Media Centres now exist in other countries; Canada, Australia, New Zealand, Japan, and Germany. Except for the relation between the Science Media Centre in UK and the Australian Science Media Centre, these centres are independent of each other.\n\n\n"}
{"id": "411590", "url": "https://en.wikipedia.org/wiki?curid=411590", "title": "Science and technology studies", "text": "Science and technology studies\n\nScience and technology studies, or science, technology and society studies (both abbreviated STS) is the study of how society, politics, and culture affect scientific research and technological innovation, and how these, in turn, affect society, politics and culture.\n\nLike most interdisciplinary programs, STS emerged from the confluence of a variety of disciplines and disciplinary subfields, all of which had developed an interest—typically, during the 1960s or 1970s—in viewing science and technology as socially embedded enterprises. The key disciplinary components of STS took shape independently, beginning in the 1960s, and developed in isolation from each other well into the 1980s, although Ludwik Fleck's (1935) monograph \"Genesis and Development of a Scientific Fact\" anticipated many of STS's key themes. In the 1970s Elting E. Morison founded the STS program at Massachusetts Institute of Technology (MIT), which served as a model. By 2011, 111 STS research centres and academic programs were counted worldwide.\n\n\nDuring the 1970s and 1980s, leading universities in the US, UK, and Europe began drawing these various components together in new, interdisciplinary programs. For example, in the 1970s, Cornell University developed a new program that united science studies and policy-oriented scholars with historians and philosophers of science and technology. Each of these programs developed unique identities due to variation in the components that were drawn together, as well as their location within the various universities. For example, the University of Virginia's STS program united scholars drawn from a variety of fields (with particular strength in the history of technology); however, the program's teaching responsibilities—it is located within an engineering school and teaches ethics to undergraduate engineering students—means that all of its faculty share a strong interest in engineering ethics.\n\nA decisive moment in the development of STS was the mid-1980s addition of technology studies to the range of interests reflected in science. During that decade, two works appeared \"en seriatim\" that signaled what Steve Woolgar was to call the \"turn to technology\": \"Social Shaping of Technology\" (MacKenzie and Wajcman, 1985) and \"The Social Construction of Technological Systems\" (Bijker, Hughes and Pinch, 1987). MacKenzie and Wajcman primed the pump by publishing a collection of articles attesting to the influence of society on technological design. In a seminal article, Trevor Pinch and Wiebe Bijker attached all the legitimacy of the Sociology of Scientific Knowledge to this development by showing how the sociology of technology could proceed along precisely the theoretical and methodological lines established by the sociology of scientific knowledge. This was the intellectual foundation of the field they called the social construction of technology.\n\nThe \"turn to technology\" helped to cement an already growing awareness of underlying unity among the various emerging STS programs. More recently, there has been an associated turn to ecology, nature, and materiality in general, whereby the socio-technical and natural/material co-produce each other. This is especially evident in work in STS analyses of biomedicine (such as Carl May, Annemarie Mol, Nelly Oudshoorn, and Andrew Webster) and ecological interventions (such as Bruno Latour, Sheila Jasanoff, Matthias Gross, S. Lochlann Jain, and Jens Lachmund).\n\nThe subject has several professional associations.\n\nFounded in 1975, the Society for Social Studies of Science, initially provided scholarly communication facilities, including a journal (\"Science, Technology, and Human Values\") and annual meetings that were mainly attended by science studies scholars. The society has since grown into the most important professional association of science and technology studies scholars worldwide. The Society for Social Studies of Science members also include government and industry officials concerned with research and development as well as science and technology policy; scientists and engineers who wish to better understand the social embeddedness of their professional practice; and citizens concerned about the impact of science and technology in their lives. Proposals have been made to add the word \"technology\" to the association's name, thereby reflecting its stature as the leading STS professional society, that the name is long enough as it is.\n\nIn Europe, the European Association for the Study of Science and Technology (EASST) was founded in 1981 to \"stimulate communication, exchange and collaboration in the field of studies of science and technology\". Similarly, the European Inter-University Association on Society, Science and Technology (ESST) researches and studies science and technology in society, in both historical and contemporary perspectives.\n\nIn Asia several STS associations exist.\nIn Japan, the Japanese Society for Science and Technology Studies (JSSTS) was founded in 2001. The Asia Pacific Science Technology & Society Network (APSTSN) primarily has members from Australasia, Southeast and East Asia and Oceania.\n\nIn Latin America ESOCITE (Estudios Sociales de la Ciencia y la Tecnología) is the biggest association of Science and Technology studies. The study of STS (CyT in Spanish, CTS in Portuguese) here was shaped by authors like Amílcar Herrera and Jorge Sabato y Oscar Varsavsky in Argentina, José Leite Lopes in Brazil, Miguel Wionczek in Mexico, Francisco Sagasti in Peru, Máximo Halty Carrere in Uruguay and Marcel Roche in Venezuela.\n\nFounded in 1958, the Society for the History of Technology initially attracted members from the history profession who had interests in the contextual history of technology. After the \"turn to technology\" in the mid-1980s, the society's well-regarded journal (\"Technology and Culture\") and its annual meetings began to attract considerable interest from non-historians with technology studies interests.\n\nLess identified with STS, but also of importance to many STS scholars, are the History of Science Society, the Philosophy of Science Association, and the American Association for the History of Medicine.\n\nAdditionally, within the US there are significant STS-oriented special interest groups within major disciplinary associations, including the American Anthropological Association, the American Political Science Association, the National Women's Studies Association, and the American Sociological Association.\n\nNotable peer-reviewed journals in STS include: \nStudent journals in STS include: \nSocial constructions are human created ideas, objects, or events created by a series of choices and interactions. These interactions have consequences that change the perception that different groups of people have on these constructs. Some examples of social construction include class, race, money, and citizenship.\n\nThe following also alludes to the notion that not everything is set, a circumstance or result could potentially be one way or the other. According to the article \"What is Social Construction?\" by Laura Flores, \"Social construction work is critical of the status quo. Social constructionists about X tend to hold that:\nVery often they go further, and urge that:\nIn the past, there have been viewpoints that were widely regarded as fact until being called to question due to the introduction of new knowledge. Such viewpoints include the past concept of a correlation between intelligence and the nature of a human's ethnicity or race (X may not be at all as it is).\n\nAn example of the evolution and interaction of various social constructions within science and technology can be found in the development of both the high-wheel bicycle, or velocipede, and then of the bicycle. The velocipede was widely used in the latter half of the 19th century. In the latter half of the 19th century, a social need was first recognized for a more efficient and rapid means of transportation. Consequently, the velocipede was first developed, which was able to reach higher translational velocities than the smaller non-geared bicycles of the day, by replacing the front wheel with a larger radius wheel. One notable trade-off was a certain decreased stability leading to a greater risk of falling. This trade-off resulted in many riders getting into accidents by losing balance while riding the bicycle or being thrown over the handle bars.\n\nThe first \"social construction\" or progress of the velocipede caused the need for a newer \"social construction\" to be recognized and developed into a safer bicycle design. Consequently, the velocipede was then developed into what is now commonly known as the \"bicycle\" to fit within society's newer \"social construction,\" the newer standards of higher vehicle safety. Thus the popularity of the modern geared bicycle design came as a response to the first social construction, the original need for greater speed, which had caused the high-wheel bicycle to be designed in the first place. The popularity of the modern geared bicycle design ultimately ended the widespread use of the velocipede itself, as eventually it was found to best accomplish the social-needs/ social-constructions of both greater speed and of greater safety.\n\nTechnoscience is a subset of Science, Technology, and Society studies that focuses on the inseparable connection between science and technology. It states that fields are linked and grow together, and scientific knowledge requires an infrastructure of technology in order to remain stationary or move forward. Both technological development and scientific discovery drive one another towards more advancement. Technoscience excels at shaping human thought and behavior by opening up new possibilities that gradually or quickly come to be perceived as necessities.\n\n\"Technological action is a social process.\" Social factors and technology are intertwined so that they are dependent upon each other. This includes the aspect that social, political, and economic factors are inherent in technology and that social structure influences what technologies are pursued. In other words, \"technoscientific phenomena combined inextricably with social/political/ economic/psychological phenomena, so 'technology' includes a spectrum of artifacts, techniques, organizations, and systems.\" Winner expands on this idea by saying \"in the late twentieth century technology and society, technology and culture, technology and politics are by no means separate.\"\n\n\nDeliberative democracy is a reform of representative or direct democracies which mandates discussion and debate of popular topics which affect society. Deliberative Democracy is a tool for making decisions. Deliberative democracy can be traced back all the way to Aristotle’s writings. More recently, the term was coined by Joseph Bessette in his 1980 work \"Deliberative Democracy: The Majority Principle in Republican Government\", where he uses the idea in opposition to the elitist interpretations of the United States Constitution with emphasis on public discussion.\n\nDeliberative Democracy can lead to more legitimate, credible, and trustworthy outcomes. Deliberative Democracy allows for \"a wider range of public knowledge,\" and it has been argued that this can lead to \"more socially intelligent and robust\" science. One major shortcoming of deliberative democracy is that many models insufficiently ensure critical interaction.\n\nAccording to Ryfe, there are five mechanisms that stand out as critical to the successful design of deliberative democracy:\n\nRecently, there has been a movement towards greater transparency in the fields of policy and technology. Jasanoff comes to the conclusion that there is no longer a question of if there needs to be increased public participation in making decisions about science and technology, but now there needs to be ways to make a more meaningful conversation between the public and those developing the technology.\n\nAckerman and Fishkin offer an example of a reform in their paper \"Deliberation Day.\" The deliberation is to enhance public understanding of popular, complex, and controversial issues, through devices such as Fishkin’s Deliberative Polling. Although implementation of these reforms is unlikely in a large government situation such as the United States Federal Government. However, things similar to this have been implemented in small, local, governments like New England towns and villages. New England town hall meetings are a good example of deliberative democracy in a realistic setting.\n\nAn ideal Deliberative Democracy balances the voice and influence of all participants. While the main aim is to reach consensus, a deliberative democracy should encourage the voices of those with opposing viewpoints, concerns due to uncertainties, and questions about assumptions made by other participants. It should take its time and ensure that those participating understand the topics on which they debate. Independent managers of debates should also have substantial grasp of the concepts discussed, but must \"[remain] independent and impartial as to the outcomes of the process.\"\n\nIn 1968, Garrett Hardin popularised the phrase \"tragedy of the commons.\" It is an economic theory where rational people act against the best interest of the group by consuming a common resource. Since then, the tragedy of the commons has been used to symbolize the degradation of the environment whenever many individuals use a common resource. Although Garrett Hardin was not an STS scholar, the concept of tragedy of the commons still applies to science, technology and society.\n\nIn a contemporary setting, the Internet acts as an example of the tragedy of the commons through the exploitation of digital resources and private information. Data and internet passwords can be stolen much more easily than physical documents. Virtual spying is almost free compared to the costs of physical spying. Additionally, net neutrality can be seen as an example of tragedy of the commons in an STS context. The movement for net neutrality argues that the Internet should not be a resource that is dominated by one particular group, specifically those with more money to spend on Internet access.\n\nA counterexample to the tragedy of the commons is offered by Andrew Kahrl. Privatization can be a way to deal with the tragedy of the commons. However, Kahrl suggests that the privatization of beaches on Long Island, in an attempt to combat overuse of Long Island beaches, made the residents of Long Island more susceptible to flood damage from Hurricane Sandy. The privatization of these beaches took away from the protection offered by the natural landscape. Tidal lands that offer natural protection were drained and developed. This attempt to combat the tragedy of the commons by privatization was counter-productive. Privatization actually destroyed the public good of natural protection from the landscape.\n\nAlternative modernity is a conceptual tool conventionally used to represent the state of present western society. Modernity represents the political and social structures of the society, the sum of interpersonal discourse, and ultimately a snapshot of society's direction at a point in time. Unfortunately conventional modernity is incapable of modeling alternative directions for further growth within our society. Also, this concept is ineffective at analyzing similar but unique modern societies such as those found in the diverse cultures of the developing world. Problems can be summarized into two elements: inward failure to analyze growth potentials of a given society, and outward failure to model different cultures and social structures and predict their growth potentials.\n\nPreviously, modernity carried a connotation of the current state of being modern, and its evolution through European colonialism. The process of becoming \"modern\" is believed to occur in a linear, pre-determined way, and is seen by Philip Brey as a way of to interpret and evaluate social and cultural formations. This thought ties in with modernization theory, the thought that societies progress from \"pre-modern\" to \"modern\" societies.\n\nWithin the field of science and technology, there are two main lenses with which to view modernity. The first is as a way for society to quantify what it wants to move towards. In effect, we can discuss the notion of \"alternative modernity\" (as described by Andrew Feenberg) and which of these we would like to move towards. Alternatively, modernity can be used to analyze the differences in interactions between cultures and individuals. From this perspective, alternative modernities exist simultaneously, based on differing cultural and societal expectations of how a society (or an individual within society) should function. Because of different types of interactions across different cultures, each culture will have a different modernity.\n\nPace of Innovation is the speed at which technological innovation or advancement is occurring, with the most apparent instances being too slow or too rapid. Both these rates of innovation are extreme and therefore have effects on the people that get to use this technology.\n\n\"No innovation without representation\" is a democratic ideal of ensuring that everyone involved gets a chance to be represented fairly in technological developments.\n\n\nThe privileged positions of business and science refer to the unique authority that persons in these areas hold in economic, political, and technosocial affairs. Businesses have strong decision-making abilities in the function of society, essentially choosing what technological innovations to develop. Scientists and technologists have valuable knowledge, ability to pursue the technological innovations they want. They proceed largely without public scrutiny and as if they had the consent of those potentially affected by their discoveries and creations.\n\nLegacy thinking is defined as an inherited method of thinking imposed from an external source without objection by the individual, because it is already widely accepted by society.\n\nLegacy thinking can impair the ability to drive technology for the betterment of society by blinding people to innovations that do not fit into their accepted model of how society works. By accepting ideas without questioning them, people often see all solutions that contradict these accepted ideas as impossible or impractical. Legacy thinking tends to advantage the wealthy, who have the means to project their ideas on the public. It may be used by the wealthy as a vehicle to drive technology in their favor rather than for the greater good.\nExamining the role of citizen participation and representation in politics provides an excellent example of legacy thinking in society. The belief that one can spend money freely to gain influence has been popularized, leading to public acceptance of corporate lobbying. As a result, a self-established role in politics has been cemented where the public does not exercise the power ensured to them by the Constitution to the fullest extent. This can become a barrier to political progress as corporations who have the capital to spend have the potential to wield great influence over policy. Legacy thinking however keeps the population from acting to change this, despite polls from Harris Interactive that report over 80% of Americans feel that big business holds too much power in government. Therefore, Americans are beginning to try to steer away this line of thought, rejecting legacy thinking, and demanding less corporate, and more public, participation in political decision making.\n\nAdditionally, an examination of net neutrality functions as a separate example of legacy thinking. Starting with dial-up, the internet has always been viewed as a private luxury good. Internet today is a vital part of modern-day society members. They use it in and out of life every day. Corporations are able to mislabel and greatly overcharge for their internet resources. Since the American public is so dependent upon internet there is little for them to do. Legacy thinking has kept this pattern on track despite growing movements arguing that the internet should be considered a utility. Legacy thinking prevents progress because it was widely accepted by others before us through advertising that the internet is a luxury and not a utility. Due to pressure from grassroots movements the Federal Communications Commission (FCC) has redefined the requirements for broadband and internet in general as a utility. Now AT&T and other major internet providers are lobbying against this action and are in-large able to delay the onset of this movement due to legacy thinking’s grip on American culture and politics.\n\nFor example, those who cannot overcome the barrier of legacy thinking may not consider the privatization of clean drinking water as an issue. This is partially because access to water has become such a given fact of the matter to them. For a person living in such circumstances, it may be widely accepted to not concern themselves with drinking water because they have not needed to be concerned with it in the past. Additionally, a person living within an area that does not need to worry about their water supply or the sanitation of their water supply is less likely to be concerned with the privatization of water.\n\nThis notion can be examined through the thought experiment of \"veil of ignorance\". Legacy thinking causes people to be particularly ignorant about the implications behind the \"you get what you pay for\" mentality applied to a life necessity. By utilizing the \"veil of ignorance\", one can overcome the barrier of legacy thinking as it requires a person to imagine that they are unaware of their own circumstances, allowing them to free themselves from externally imposed thoughts or widely accepted ideas.\n\n\n\nSTS is taught in several countries. According to the STS wiki, STS programs can be found in twenty countries, including 45 programs in the United States, three programs in India, and eleven programs in the UK. STS programs can be found in Israel, Malaysia, and Taiwan. Some examples of institutions offering STS programs are Stanford University, Harvard University, the University of Oxford, Mines ParisTech, and Bar-Ilan University.\n\n\n"}
{"id": "2636884", "url": "https://en.wikipedia.org/wiki?curid=2636884", "title": "Source criticism", "text": "Source criticism\n\nSource criticism (or information evaluation) is the process of evaluating an information source, i.e. a document, a person, a speech, a fingerprint, a photo, an observation, or anything used in order to obtain knowledge. In relation to a given purpose, a given information source may be more or less valid, reliable or relevant. Broadly, \"source criticism\" is the interdisciplinary study of how information sources are evaluated for given tasks.\n\nProblems in translation: The Danish word \"kildekritik\" like the Norwegian word \"kildekritikk\" and the Swedish word \"källkritik\" derived from the German \"Quellenkritik\" and is closely associated with the German historian Leopold von Ranke (1795–1886). Hardtwig writes: \"His [Ranke's] first work \"Geschichte der romanischen und germanischen Völker\" von 1494–1514 (History of the Latin and Teutonic Nations from 1494 to 1514) (1824) was a great success. It already showed some of the basic characteristics of his conception of Europe, and was of historiographical importance particularly because Ranke made an exemplary critical analysis of his sources in a separate volume, \"Zur Kritik neuerer Geschichtsschreiber\" (On the Critical Methods of Recent Historians). In this work he raised the method of textual criticism used in the late eighteenth century, particularly in classical philology to the standard method of scientific historical writing\" (Hardtwig, 2001, p. 12739).\nThe larger part of the nineteenth and twentieth\ncenturies would be dominated by the research-oriented\nconception of historical method of the so-called\nHistorical School in Germany, led by historians as\nLeopold Ranke and Berthold Niebuhr. Their conception\nof history, long been regarded as the beginning\nof modern, 'scientific' history, harked back to the\n'narrow' conception of historical method, limiting the\nmethodical character of history to source criticism\" (Lorenz, 2001).\nBible studies dominate the use of \"source criticism\" in America (cf. Hjørland, 2008). The term is thus relatively seldom used in English about historical methods and historiography (cf. Hjørland, 2008). This difference between European and American use of \"source criticism\" is somewhat strange considering the influence of Ranke on both sides of the Atlantic Ocean. It has been suggested that differences in the use of the term are not accidental but due to different views of the historical method. In the German/Scandinavian tradition this subject is seen as important, whereas in the Anglo-American tradition it is believed that historical methods must be specific and associated with the subject studied, for which reason there is no general field of \"source criticism\".\n\nIn the Scandinavian countries and elsewhere source evaluation (or information evaluation) is also studied interdisciplinarily from many different points of view, partly caused by the influence of the Internet. It is a growing field in, among other fields, library and information science. In this context source criticism is studied from a broader perspective than just, for example, history or biblical studies.\n\nThe following principles are cited from two Scandinavian textbooks on source criticism, Olden-Jørgensen (1998) and Thurén (1997) written by historians:\n\nWe may add the following principles:\n\n\"Because each source teaches you more and more about your subject, you will be able to judge with ever-increasing precision the usefulness and value of any prospective source. In other words, the more you know about the subject, the more precisely you can identify what you must still find out\". (Bazerman, 1995, p. 304).\n\"The empirical case study showed that most people find it difficult to assess questions of cognitive authority and media credibility in a general sense, for example, by comparing the overall credibility of newspapers and the Internet. Thus these assessments tend to be situationally sensitive. Newspapers, television and the Internet were frequently used as sources of orienting information, but their credibility varied depending on the actual topic at hand\" (Savolainen, 2007).\nThe following questions are often good ones to ask about any source according to the American Library Association (1994) and Engeldinger (1988):\n\n\nFor literary sources we might add complementing criteria:\n\n\nHow general are principles of source criticism?\nSome principles are universal, other principles are specific for certain kinds of information sources. One may ask whether principles of source criticism are unique to the humanities?\n\nThere is today no consensus about the similarities and differences between natural science and humanities. Logical positivism claimed that all fields of knowledge were based on the same principles. Much of the criticism of logical positivism claimed that positivism is the basis of the sciences, whereas hermeneutics is the basis of the humanities. This was, for example, the position of Jürgen Habermas. A newer position, in accordance with, among others, Hans-Georg Gadamer and Thomas Kuhn understands both science and humanities as determined by researchers' preunderstanding and paradigms. Hermeneutics is thus a universal theory. The difference is, however, that the sources of the humanities are themselves products of human interests and preunderstanding, whereas the sources of the natural sciences are not. Humanities are thus \"doubly hermeneutic\".\n\nNatural scientists, however, are also using human products (such as scientific papers) which are products of preunderstanding (and, for example, academic fraud).\n\nEpistemological theories are the basic theories about how knowledge is obtained and thus the most general theories about how to evaluate information sources. Empiricism evaluates sources by considering the observations (or sensations) on which they are based. Sources without basis in experience are not seen as valid. Rationalism provides low priority to sources based on observations. In order to be meaningful observations must be grasped by clear ideas or concepts. It is the logical structure and the well definedness that is in focus in evaluating information sources from the rationalist point of view. Historicism evaluates information sources on the basis of their reflection of their sociocultural context and their theoretical development. Pragmatism evaluate sources on the basis of how their values and usefulness to accomplish certain outcomes. Pragmatism is skeptical about claimed neutral information sources.\n\nThe evaluation of knowledge or information sources cannot be more certain than is the construction of knowledge. If we accept the principle of fallibilism we also have to accept that source criticism can never 100% verify knowledge claims. As discussed in the next section is source criticism intimately linked to scientific methods.\n\nThe presence of fallacies of argument in sources is another kind of philosophical criteria for evaluating sources. Fallacies are presented by Walton (1998). Among the fallacies are the 'ad hominem fallacy' (the use of personal attack to try to undermine or refute a person's argument) and the 'straw man fallacy' (when one arguer misrepresents another's position to make it appear less plausible than it really is, in order more easily to criticize or refute it.) See also fallacy.\n\nResearch methods are methods used to produce scholarly knowledge. The methods that are relevant for producing knowledge are also relevant for evaluating knowledge. An example of a book that turns methodology upside-down and uses it to evaluate produced knowledge is Katzer; Cook & Crouch (1998). See also Unobtrusive measures, Triangulation (social science).\n\nStudies of quality evaluation processes such as peer review, book reviews and of the normative criteria used in evaluation of scientific and scholarly research. Another field is the study of scientific misconduct.\n\nHarris (1979) provides a case study of how a famous experiment in psychology, Little Albert, has been distorted throughout the history of psychology, starting with the author (Watson) himself, general textbook authors, behavior therapists, and a prominent learning theorist. Harris proposes possible causes for these distortions and analyzes the Albert study as an example of myth making in the history of psychology. Studies of this kind may be regarded a special kind of reception history (how Watson's paper was received). It may also be regarded as a kind of critical history (opposed to ceremonial history of psychology, cf. Harris, 1980). Such studies are important for source criticism in revealing the bias introduced by referring to classical studies.\n\nSee also Hjørland (2008): Empirical studies of the quality of science.\n\nTextual criticism (or broader: text philology) is a part of philology, which is not just devoted to the study of texts, but also to edit and produce \"scientific editions\", \"scholarly editions\", \"standard editions\", \"historical editions\", \"reliable editions\", \"reliable texts\", \"text editions\" or \"critical editions\", which are editions in which careful scholarship has been employed to ensure that the information contained within is as close to the author's/composer's original intentions as possible (and which allows the user to compare and judge changes in editions published under influence by the author/composer). The relation between these kinds of works and the concept \"source criticism\" is evident in Danish, where they may be termed \"kildekritisk udgave\" (directly translated \"source critical edition\").\n\nIn other words, it is assumed that most editions of a given works is filled with noise and errors provided by publishers, why it is important to produce \"scholarly editions\". The work provided by text philology is an important part of source criticism in the humanities.\n\n\ncomplete works and monumental editions\n\nThe study of eyewitness testimony is an important field of study used, among other purposes, to evaluate testimony in courts. The basics of eyewitness fallibility includes factors such as poor viewing conditions, brief exposure, and stress. More subtle factors, such as expectations, biases, and personal stereotypes can intervene to create erroneous reports. Loftus (1996) discuss all such factors and also shows that eyewitness memory is chronically inaccurate in surprising ways. An ingenious series of experiments reveals that memory can be radically altered by the way an eyewitness is questioned after the fact. New memories can be implanted and old ones unconsciously altered under interrogation.\n\nAnderson (1978) and Anderson & Pichert (1977) reported an elegant experiment demonstrating how change in perspective affected people's ability to recall information that was unrecallable from another perspective.\n\nIn psychoanalysis the concept of defence mechanism is important and may be considered a contribution to the theory of source criticism because it explains psychological mechanisms, which distort the reliability of human information sources.\n\nIn schools of library and information science (LIS) is source criticism of taught as part of the growing field Information literacy.\nStudy issues like relevance, quality indicators for documents, kinds of documents and their qualities (e.g. scholarly editions) and related issues are studied in LIS and are relevant for source criticism. Bibliometrics is often used to find the most influential journal, authors, countries and institutions. The study of book reviews and their function in evaluating books should also be mentioned. The well-known comparison of Wikipedia and Encyclopædia Britannica (Giles, 2005) - although not done by information scientists - contained an interview with an information scientist (Michael Twidale) and should be obvious to include in LIS.\n\nIt could be argued that library and information education should provide teaching in source criticism at least at the same level as is taught in Upper Secondary School (see Gudmundsson, 2007).\n\nIn library and information science the checklist approach has often been used. A criticism of this approach is given by Meola (2004): \"Chucking the checklist\".\n\nLibraries sometimes provide advice on how their users may evaluate sources.\n\nThe Library of Congress has a \"Teaching with Primary Sources\" (TPS) program.\n\nSource criticism is also about ethical behavior and culture. It is about a free press and an open society, including the protecting information sources from being persecuted (cf., Whistleblower).\n\nPhotos are often manipulated during wars and for political purposes. One well known example is Joseph Stalin's manipulation of a photograph from May 5, 1920 on which Stalin's predecessor Lenin held a speech for Soviet troops that Leon Trotsky attended. Stalin had later Trotsky retouched out of this photograph. (cf. King, 1997). A recent example is reported by Healy (2008) about North Korean leader Kim Jong Il.\n\nMuch interest in evaluating Internet sources (such as Wikipedia) is reflected in the scholarly literature of Library and information science and in other fields. Mintz (2002) is an edited volume about this issue. Examples of literature examining Internet sources include Chesney (2006), Fritch & Cromwell (2001), Leth & Thurén (2000) and Wilkinson, Bennett, & Oliver (1997).\n\n\"In history, the term historical method was first introduced in a systematic way in the sixteenth century by Jean Bodin in his treatise of source criticism, \"Methodus ad facilem historiarium cognitionem\" (1566). Characteristically, Bodin's treatise intended to establish the ways by which reliable knowledge of the past could be established by checking sources against one another and by so assessing the reliability of the information conveyed by them, relating them to the interests involved.\" (Lorenz, 2001, p. 6870).\n\nAs written above, modern source criticism in history is closely associated with the German historian Leopold von Ranke (1795–1886), who influenced historical methods on both sides of the Atlantic Ocean, although in rather different ways. American history developed in a more empirist and antiphilosophical way (cf., Novick, 1988).\n\nTwo of the best-known rule books from History's childhood are Bernheim (1889) and Langlois & Seignobos (1898). These books provided a seven-step procedure (here quoted from Howell & Prevenier, 2001, p. 70-71):\n\nGudmundsson (2007, p. 38) writes: \"Source criticism should not totally dominate later courses. Other important perspectives, for example, philosophy of history/view of history, should not suffer by being neglected\" (Translated by BH). This quote makes a distinction between source criticism on the one hand and historical philosophy on the other hand. However, different views of history and different specific theories about the field being studied may have important consequences for how sources are selected, interpreted and used. Feminist scholars may, for example, select sources made by women and may interpret sources from a feminist perspective. Epistemology should thus be considered a part of source criticism. It is in particular related to \"tendency analysis\".\n\nIn archaeology, radiocarbon dating is an important technique to establish the age of information sources. Methods of this kind were the ideal when history established itself as both a scientific discipline and as a profession based on \"scientific\" principles in the last part of the 1880s (although radiocarbon dating is a more recent example of such methods). The empiricist movement in history brought along both \"source criticism\" as a research method and also in many countries large scale publishing efforts to make valid editions of \"source materials\" such as important letters and official documents (e.g. as facsimiles or transcriptions).\n\nHistoriography and Historical method include the study of the reliability of the sources used, in terms of, for example, authorship, credibility of the author, and the authenticity or corruption of the text.\n\nSource criticism, as the term is used in biblical criticism, refers to the attempt to establish the sources used by the author and/or redactor of the final text. The term \"literary criticism\" is occasionally used as a synonym.\n\nBiblical source criticism originated in the 18th century with the work of Jean Astruc, who adapted the methods already developed for investigating the texts of Classical antiquity (Homer's Iliad in particular) to his own investigation into the sources of the Book of Genesis. It was subsequently considerably developed by German scholars in what was known as \"the Higher Criticism\", a term no longer in widespread use. The ultimate aim of these scholars was to reconstruct the history of the biblical text, as well as the religious history of ancient Israel.\n\nRelated to Source Criticism is Redaction Criticism which seeks to determine how and why the redactor (editor) put the sources together the way he did. Also related is form criticism and tradition history which try to reconstruct the oral prehistory behind the identified written sources.\n\nJournalists often work with strong time pressure and have access to only a limited number of information sources such as news bureaus, persons which may be interviewed, newspapers, journals and so on (see journalism sourcing). Journalists' possibility for conducting serious source criticism is thus limited compared to, for example, historians' possibilities.\n\nThe most important legal sources are created by parliaments, governments, courts, and legal researchers. They may be written or informal and based on established practices. Views concerning the quality of sources differ among legal philosophies: Legal positivism is the view that the text of the law should be considered in isolation, while legal realism, interpretivism (legal), critical legal studies and feminist legal criticism interprets the law on a broader cultural basis.\n\n\n"}
{"id": "6752869", "url": "https://en.wikipedia.org/wiki?curid=6752869", "title": "Strong inference", "text": "Strong inference\n\nIn philosophy of science, strong inference is a model of scientific inquiry that emphasizes the need for alternative hypotheses, rather than a single hypothesis to avoid confirmation bias.\n\nThe term \"strong inference\" was coined by John R. Platt, a biophysicist at the University of Chicago. Platt notes that some fields, such as molecular biology and high-energy physics, seem to adhere strongly to strong inference, with very beneficial results for the rate of progress in those fields.\n\nThe problem with single hypotheses, confirmation bias, was aptly described by Thomas Chrowder Chamberlin in 1897:\nDespite the admonitions of Platt, reviewers of grant-applications often require \"A Hypothesis\" as part of the proposal (note the singular). Peer-review of research can help avoid the mistakes of single-hypotheses, but only so long as the reviewers are not in the thrall of the same hypothesis. If there is a shared enthrallment among the reviewers in a commonly believed hypothesis, then innovation becomes difficult because alternative hypotheses are not seriously considered, and sometimes not even permitted.\n\nThe method, very similar to the scientific method, is described as:\n\nThe original paper outlining strong inference has been criticized, particularly for overstating the degree that certain fields used this method.\n\nThe limitations of Strong-Inference can be corrected by having two preceding phases:\nThese phases create the critical seed observation(s) upon which one can base alternative hypotheses.\n"}
{"id": "44104302", "url": "https://en.wikipedia.org/wiki?curid=44104302", "title": "Student Spaceflight Experiments Program", "text": "Student Spaceflight Experiments Program\n\nThe Student Spaceflight Experiments Program (SSEP) provides an opportunity for student groups from upper elementary school through university to design and fly microgravity experiments in low Earth orbit (LEO). SSEP is a program of the National Center for Earth and Space Science Education (NCESSE, a project of the Tides Center), the Arthur C. Clarke Institute for Space Education, and the private space hardware company NanoRacks. SSEP operates under a Space Act Agreement between the sponsoring organizations and NASA, allowing the International Space Station (ISS) to be utilized as a national laboratory.\n\nThe program was launched in June 2010, by NCESSE in the U.S. and by the Clarke Institute internationally. , SSEP has sponsored fourteen missions to LEO – two on board the Space Shuttle, and twelve to the ISS – with a thirteenth mission to the ISS announced in March 2018, and expected to fly in the spring/summer of 2019.\n\nIn the first fourteen SSEP flight opportunities, 86,800 students in grades 5 through 16 (senior undergraduate in the U.S. higher education system) participated in experiment design and proposal writing. Of 18,759 proposals received, a total of 240 experiments were selected for flight, with one from each community participating in each flight opportunity. , 206 of these experiments have been successfully launched. The 18 experiments comprising Mission 6 to the ISS were lost when the Cygnus CRS Orb-3 vehicle exploded shortly after launch on 28 October 2014.\n\nThe competition to select student projects for flight is designed to resemble a standard research proposal process. Interested groups must submit proposals in response to announced criteria; these proposals are then peer-reviewed against the criteria in a two-stage selection process, with the vast majority of proposals rejected.\n\nEach selected experiment is provided with one mini-laboratory, which is flown on the ISS and then returned to Earth for analysis. Experiments selected for flight have included research into crystal growth, composting, cell division, seed germination, and calcium metabolism. The cost of each experiment is on the order of US$24,500, which must be raised by the community developing the experiment.\n\nStudents have an opportunity to share their research at a national conference sponsored by the Smithsonian National Air and Space Museum, NCESSE, and the Clarke Institute. Students participating in the program have also been given the chance to participate in a videoconference with space station astronauts.\n\n"}
{"id": "10469862", "url": "https://en.wikipedia.org/wiki?curid=10469862", "title": "Technological somnambulism", "text": "Technological somnambulism\n\nTechnological somnambulism is a concept used when talking about the philosophy of technology. The term was used by Langdon Winner in his essay \"Technology as forms of life\". Winner puts forth the idea that we are simply in a state of \"sleepwalking\" in our mediations with technology. This sleepwalking is caused by a number of factors. One of the primary causes is the way we view technology as tools, something that can be put down and picked up again. Because of this view of objects as something we can easily separate ourselves from technology, and so we fail to look at the long term implications of using that object. A second factor is the separation of those who make the technology and those who use the technology. This division causes there to be little thought and research going into the effects of using/developing that technology. The third and most important idea is the way in which technology seems to create new \"worlds\" in which we live. These worlds are created by the restructuring of the common and seemingly everyday things around us. In most situations the changes take place with little attention or care from us because we are more focused on the menial aspects of the technology (Winner 105-107).\n\nThe concept can be found in the earlier work of Marshall McLuhan, cf. \"Understanding Media\", where he refers to a comment made by David Sarnoff expressing a socially deterministic view of \"value free\" technology whose value is solely defined by its usage as representing, \"...the voice of the current somnambulism\". Given that this piece by McLuhan has become standard reading in Media Theory it is reasonable to suspect that Winner encountered the concept there or elsewhere and then went on to develop it further.\n"}
{"id": "31066", "url": "https://en.wikipedia.org/wiki?curid=31066", "title": "The Third Culture", "text": "The Third Culture\n\nThe Third Culture: Beyond the Scientific Revolution is a 1995 book by John Brockman which discusses the work of several well-known scientists who are directly communicating their new, sometimes provocative, ideas to the general public. John Brockman has continued the themes of 'The Third Culture' in the website of the Edge Foundation, where leading scientists and thinkers contribute their thoughts in plain English.\n\nThe title of the book refers to Charles Percy Snow's 1959 work \"The Two Cultures and the Scientific Revolution\", which described the conflict between the cultures of the humanities and science.\n\n23 people were included in the 1995 book:\n\nThe book influenced the reception of popular scientific literature in parts of the world beyond the United States. In Germany, the book inspired several newspapers to integrate scientific reports into their \"Feuilleton\" or \"culture\" sections (such as the \"Frankfurter Allgemeine Zeitung\"). At the same time, the assertions of the book were discussed as a source of controversy, especially the implicit assertion that \"third culture thinking\" is mainly an American development. Critics acknowledge that, whereas in the Anglo-Saxon cultures there is a large tradition of scientists writing popular books, such tradition was absent for a long period in the German and French languages, with journalists often filling the gap. However, some decades ago there were also scientists, like the physicists Heisenberg and Schrödinger and the psychologist Piaget, who fulfill the criteria Brockman named for \"third culture.\" The German author Gabor Paal suggested that the idea of the \"third culture\" is a rather modern version of what Hegel called Realphilosophie (\"philosophy of the real\").\n\nAlso, already during the interwar period, Otto Neurath and other members of the Vienna Circle strongly propagated the need for both the unity of science and the popularization of new scientific concepts. With the rise of the Nazis in Germany and Austria, many of the Vienna Circle's members left for the United States where they taught in several universities, causing their philosophical ideas to spread in the Anglo-Saxon world throughout the 1930s-1940s.\n\n\n\n"}
{"id": "5934705", "url": "https://en.wikipedia.org/wiki?curid=5934705", "title": "Wholeness and the Implicate Order", "text": "Wholeness and the Implicate Order\n\nWholeness and the Implicate Order is a book by theoretical physicist David Bohm. It was originally published 1980 by Routledge, Great Britain.\n\nThe book is considered a basic reference for Bohm's concepts of undivided wholeness and of implicate and explicate orders, as well as of Bohm's rheomode - an experimental language based on verbs. The book is cited, for example, by philosopher Steven M. Rosen in his book \"The Self-evolving Cosmos\", by mathematician and theologian Kevin J. Sharpe in his book \"David Bohm's World\", by theologian Joseph P. Farrell in \"Babylon's Banksters\", and by theologian John C. Polkinghorne in his book \"One World\".\n\n\n"}
