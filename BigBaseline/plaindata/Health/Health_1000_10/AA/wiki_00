{"id": "38509307", "url": "https://en.wikipedia.org/wiki?curid=38509307", "title": "10 Essential Public Health Services", "text": "10 Essential Public Health Services\n\nThe 10 Essential Public Health Services is a US government document which codifies the responsibilities of public health agencies and institutions in the United States.\n\nOrganized according to the \"three fundamental purposes of public health\" — assessment, policy development, and assurance — the essential services include the following:\n\n\n\n\nIn 1988, the Institute of Medicine (IOM) released an assessment of the U.S. public health system titled, \"The Future of Public Health\". The report described the network of county, state, and national public health agencies as being in \"disarray\" and prompted a national discussion about the state of public health in the country. Questioning the ability of existing public health systems to provide essential services, the report sought to establish a comprehensive framework delineating the \"three fundamental purposes of public health.\" These purposes included \"assessment, policy development, and assurance.\"\n\nThough health policy academicians identified with and understood the IOM framework, policy makers found its highly conceptual language difficult to apply. Therefore, as part of President Clinton’s 1994 healthcare reform efforts, a federal working-group was tasked with reviewing and supplementing the framework. Under the auspices of both the Center for Disease Control and Prevention’s (CDC) Public Health Practice Program Office and the Office of Disease Prevention and Health Promotion, the group sought to\n\nThese efforts culminated in the publication of \"The Essential Services of Public Health\" in late 1994.\nPublic Health Units\n\nThe report was well received. Public health agencies and professional organizations began to align guidelines and employ self-assessment tools in support of the ten Essential Services. The CDC launched a number of research projects aimed at developing strategies for measuring how well public health agencies provided the services. Many of these studies were the first of their kind and ushered in an era of health agency monitoring and assessment.\n\nIn 2002, the CDC and several national public health associations launched the National Public Health Performance Standards Program (NPHPSP). With the goal of developing a consensus-based set of performance standards for state and local public health delivery systems, the NPHPSP adopted the Essential Public Health Services as \"the fundamental framework\" underpinning its assessment strategy. The NPHPSP consists of three instruments — The State Public Health System Assessment, The Local Public Health System Assessment, and The Local Public Health Governance Assessment — and covers the gamut of public health action as described by the Essential Public Health Services.\n\nIt is essential for public health organizations to monitor and evaluate the health status of populations in order to identify trends and to target health resources. Components of this service include: utilization of appropriate tools to interpret and disseminate data to audiences of interest; collaboration in integrating and managing public health; and accurate and periodic assessment of the community’s health status. Specifically, public health organizations can monitor and evaluate the health status of their populations by creating a disease reporting system, community health profiles, and health surveys. For example, the Los Angeles Department of Public Health completes a Los Angeles County Health Survey (LACHS) every two to four years to obtain population-based data. This information is used for public health assessment and planning and for evaluating changes in health over time in Los Angeles County.\n\nIn order to appropriately allocate public health resources, it is essential to diagnose and investigate health problems and hazards in the community. Components in this service include: population-based screening of diseases; access to public health laboratories capable of completing rapid screening and high volume testing; and epidemiologic investigations of disease outbreaks and patterns of disease. The Los Angeles County Health and Examination Survey (LA HANES) was piloted in 2011 and aims to establish a profile description of health outcomes of residents of Los Angeles County (e.g. adult obesity and cardiovascular disease is monitored via collection of physical measurements such as blood pressure and body mass index). These survey results will inform public education and chronic disease prevention efforts. Emergency preparedness is also an essential component of public health organizations. Teams must be available and prepared to combat natural disasters, severe weather, outbreaks, bioterrorism, mass casualties and chemical emergencies.\n\nOnce public health priorities have been established through monitoring and investigation of health problems in the community, educational activities that promote improved health should be disseminated. Components in this service include: both the availability of health information and educational resources and the presence of health education and health promotion programs. This can be achieved through media advocacy and social marketing. An example of this is the Eat Less, Weigh Less campaign executed by the Los Angeles County Department of Public Health that aims to increase awareness about appropriate portion sizes using advertisements on buses, billboards and social media. It is also essential to establish health education and health promotion program partnerships with organizations in the community, such as schools, churches and employment facilities.\n\nPublic health organizations on the local, state and national level can mobilize community partnerships to identify and solve health problems. Components of this service include: building coalitions to utilize the full range of available resources; convening and facilitating partnerships that will undertake defined health improvement projects; and provide assistance to partners and communities to solve health problems. Of particular importance is identification of potential stakeholders who will contribute to or benefit from public health activities. First 5 LA is a community partner who supports targeted programs that address the needs of Los Angeles County children ages zero to five and their families. They work closely with the Los Angeles County Department of Public Health to improve the physical and emotional health of this population. It is important to note that many of these stakeholders may not be considered to be health-related at first glance. For example, organizations involved in urban planning may be influential in improving the health of its residents. This could include increasing the walkability of a community or the number of parks or bike trails in a neighborhood.\n\nPolicies can be effective in modifying human behavior and reducing negative health outcomes. Components in this service include: development of policy to guide the practice of public health; alignment of resources and strategies for community health efforts; and systematic health planning strategies to guide community health improvement. In response to increasing rates of obesity and cardiovascular disease, the New York City Board of Health passed a ban on the sale of sodas and other sugary drinks larger than 16 ounces at restaurants, street carts and movie theaters. In addition to policies that can support health efforts, laws can reduce negative health outcomes. For example, dram shop liability is a law that holds the owner or server at a bar or restaurant where a patron consumed their last alcoholic beverage responsible for injuries or deaths caused as a result of alcohol-related incidents. A systematic review completed by the Task Force on Community Preventive Services, found strong evidence of the effectiveness of this law in reducing alcohol-related harms.\n\nIt is important that individuals and organizations comply with existing laws and regulations in order to ensure the overall health and safety of the general public. Components of this service include: reviewing, evaluating, and revising laws and regulations put in place to protect the health and safety of the public; educating persons and organizations about these laws and regulations to improve compliance and encourage enforcement of them; and enforcing actions that protect the health of the public (e.g., protection of drinking water; enforcement of clean air standards; enforcement of laws prohibiting the sale of alcoholic and tobacco products to minors, of laws concerning the use of seat belts and child safety seats; mandating childhood immunizations; facilitating timely follow-ups in the event of hazards and outbreaks of exposure-related diseases; monitoring quality of health services; conducting the timely review of new drugs, biologics, and medical devices; ensuring food safety; and enforcing housing and sanitation codes).\n\nHaving access to care when it is needed is important in helping individuals prevent and avoid unfavorable health outcomes and medical costs. At the local level, components of this service include: identifying populations that face barriers to accessing health services and addressing their personal health needs, assuring the linkage of these populations to appropriate health services by coordinating provider services, and developing and implementing interventions that address the barriers they face in attempting to access care. At the state and governance levels, components of this service include: assessing access to and availability of state health services; partnering with public, private, and non-profit sectors to provide a coordinated system of health care; assuring access to this coordinated health care system by using outreach efforts that link individuals to the health services they need; developing and implementing a continuous improvement process to assure the equitable distribution of resources for those in greatest need of these services. The National HIV/AIDS Strategy (NHAS) employs this service idea as one of the action steps for achieving increased access to care and improved health outcomes for people living with HIV.\n\nHealth care workers and staff who are competent (i.e., skilled in the core principles of public health practice) are more likely to provide care and other services more effectively and efficiently compared to those who are not. Components of this service include: making sure that the workforce meets the health needs of the population, maintaining public health workforce standards by developing and implementing efficient licensure and credentialing processes and incorporating core public health competencies into personnel systems, and adopting continuous quality improvement methods and long-term learning opportunities for public health workforce members. In two 2002 reports, the Institute of Medicine (IOM) recommended instituting a certification examination as a way of ensuring minimum competence in public health. Web-based training strategies may be useful in providing the long-term learning opportunities that many current and upcoming public health workers need in order to serve as well-informed advocates of public health and safety.\n\nGiven scarce resources, it is important to keep track of whether or not programs and/or policies end up producing intended outcomes. Components of this service include: assessing the accessibility, quality and effectiveness of services and programs delivered; providing policymakers with the information they need in order to make well-informed decisions regarding the allocation of scarce resources; tracking efficiency, effectiveness, and quality of services analyzing data on health status and service utilization; and striving to improve the public health system’s capacity to well serve the population. Cost-effectiveness analysis has been proposed as one possible strategy for informing policymakers on how best to allocate health care resources.\n\nThrough research, the health and health care problems that individuals face can be better understood, and therefore be better and more appropriately addressed given the evidence provided by such research efforts. Components of this service include: fostering the development of a continuum of innovative solutions for health programming in terms of both practical field-based efforts as well as academic efforts, establishing a consortium of research institutions and other institutions of higher learning to encourage more collaborative and cross-cutting efforts, and ensuring the public health system’s capacity to perform timely epidemiological and health policy analyses.\n\n\n"}
{"id": "936557", "url": "https://en.wikipedia.org/wiki?curid=936557", "title": "Acceptable daily intake", "text": "Acceptable daily intake\n\nAcceptable daily intake or ADI is a measure of the amount of a specific substance (originally applied for a food additive, later also for a residue of a veterinary drug or pesticide) in food or drinking water that can be ingested (orally) on a daily basis over a lifetime without an appreciable health risk. ADIs are expressed usually in milligrams (of the substance) per kilograms of body weight per day.\n\nThis concept was first introduced in 1961 by the Council of Europe and later, the Joint FAO/WHO Expert Committee on Food Additives (JECFA), a committee maintained by two United Nations bodies: the Food and Agriculture Organization (FAO) and the World Health Organization (WHO).\n\nAn ADI value is based on current research, with long-term studies on animals and observations of humans. First, a no-observed-adverse-effect level (NOAEL), the amount of a substance that shows no toxic effects, is determined. Usually the studies are performed with several doses including high doses. In the case of several studies on different effects, the lowest NOAEL is usually taken. Then, the NOAEL (or another point of departure such as a benchmark dose level (BMDL)) is divided by a safety factor, conventionally 100, to account for the differences between test animals and humans (factor of 10) and possible differences in sensitivity between humans (another factor of 10). safety factors with values other than 100 may be used if information on uncertainty about the value of the point of departure (NOAEL or BMDL) justify it. For instance, if the ADI is based on data from humans the safety factor is usually 10 instead of 100. The ADI is usually given in mg per kg body weight.\n\nThe ADI is considered a safe intake level for a healthy adult of normal weight who consumes an average daily amount of the substance in question. Increased safety factors for infants have been discussed, but are not needed, because elimination of chemicals is in fact often more rapid in children and as children generally suffer higher illness rates than adults, adverse effects caused by food additives can easily be disguised as any number of things children usually suffer with. It would be far more difficult to argue the case with a healthy adult. The ADI does not take into account allergic reactions that are individual responses rather than dose-dependent phenomena.\n\nThe higher the ADI, the larger amounts of a compound are safe for regular ingestion. The concept of tolerable daily intake is often used for unwanted contaminants or other chemicals.\n\nThe ADI concept can be understood as a measure to indicate the toxicity from long-term exposure to repeated ingestion of chemical compounds in foods (present and/or added), as opposed to acute toxicity.\n\nThe threshold limit value (TLV) of a chemical substance is a level to which it is believed a worker can be exposed day after day for a working lifetime without adverse effects.\n\n\n"}
{"id": "53513858", "url": "https://en.wikipedia.org/wiki?curid=53513858", "title": "Administrative controls", "text": "Administrative controls\n\nAdministrative controls are training, procedure, policy, or shift designs that lessen the threat of a hazard to an individual. Administrative controls typically change the behavior of people (e.g., factory workers) rather than removing the actual hazard or providing personal protective equipment (PPE).\n\nAdministrative controls are fourth in larger hierarchy of hazard controls, which ranks the effectiveness and efficiency of hazard controls. Administrative controls are more effective than PPE because they involve some manner of prior planning and avoidance, whereas PPE only serves only as a final barrier between the hazard and worker. Administrative controls are second lowest because they require workers or employers to actively think or comply with regulations and do not offer permanent solutions to problems. Generally, administrative controls are cheaper to begin, but they may become more expensive over time as higher failure rates and the need for constant training or re-certification eclipse the initial investments of the three more desirable hazard controls in the hierarchy. The U.S. National Institute for Occupational Safety and Health recommends administrative controls when hazards cannot be removed or changed, and engineering controls are not practical.\n\nSome common examples of administrative controls include work practice controls such as prohibiting mouth pipetting and recapping of needles, as well as rotating worker shifts in coal mines to prevent hearing loss. Other examples include hours of service regulations for commercial vehicle operators, warning signage for hazards, and regular maintenance of equipment. \n"}
{"id": "19880333", "url": "https://en.wikipedia.org/wiki?curid=19880333", "title": "American Society for Nutrition", "text": "American Society for Nutrition\n\nThe American Society for Nutrition (ASN) is an American society for professional researchers and practitioners in the field of nutrition. ASN publishes four journals in the field of nutrition. It has been criticized for its financial ties to the food and beverage industry.\n\nIn 1928 a group of United States biochemists and physiologists grouped together to form the first scientific society focused on nutrition, the American Institute for Nutrition. The Society held its first meeting at the Cornell Medical School in 1934. The society was renamed the American Society for Nutritional Sciences in 1996.\n\nIn 2005, the American Society for Nutritional Sciences, the American Society for Clinical Nutrition (established 1961); and the Society for International Nutrition (established 1996) merged to form The American Society for Nutrition (ASN).\n\nASN currently (2015) has a membership of about 5,000. It is one of the constituent societies comprising the Federation of American Societies for Experimental Biology, a non-profit organization that is the principal umbrella organization of U.S. societies in the field of biological and medical research.\n\nIn October 2010, the American College of Nutrition and American Society for Nutrition proposed to merge.\n\nThe ASN administered the \"Smart Choices\" food labelling program, which was suspended in 2009 after criticism.\n\nASN publishes four academic journals. Both publisher and some editorial staff accept funding from food industry organizations.\n\nThe 2017 \"Current Developments in Nutrition\" is an open-access journal aiming for rapid publication and a broader range of topics than the ASN's other journals.\n\nEligibility for membership:\n\nASN uses the term \"Sustaining Partners\" for corporate sponsors donating over $10,000 per year. According to their website:\n\nIndustry companies with the highest level of commitment to the nutrition profession are recognized as Sustaining Partners of the American Society for Nutrition. Engage with ASN as a Sustaining Partner today, and benefit from a number of advantages! Recognition includes print and online exposure, annual meeting benefits, and the ability to sponsor educational opportunities, grants and other items. However, you will derive the greatest benefit by aligning your company with ASN's superlative scientific reputation.\n\nThe American Society for Nutrition's sustaining partners, as listed on its website as of March 2018, are:\nAbbott Nutrition,\nAlmond Board of California,\nBayer HealthCare,\nBiofortis Clinical Research,\nCalifornia Walnut Commission,\nCargill, Inc.,\nCorn Refiners Association,\nCouncil for Responsible Nutrition,\nDairy Research Institute,\nDSM Nutritional Products (LLC),\nDuPont Nutrition & Health,\nthe \"Egg Nutrition Center\" of the American Egg Board,\nGeneral Mills Bell Institute of Health and Nutrition,\nHerbalife/Herbalife Nutrition Institute,\nInternational Bottled Water Foundation,\nKellogg Company,\nKyowa Hakko USA Inc.,\nMars Inc.,\nMcCormick Science Institute,\nMondelez International Technical Center,\nMonsanto Company,\nNational Cattlemen's Beef Association (a contractor to \"The Beef Checkoff\"),\nNestlé Nutrition, Medical Affairs,\nPepsiCo,\nPfizer, Inc.,\nPharmavite (LLC),\nTate & Lyle,\nThe a2 Milk Company,\nThe Coca Cola Company,\nThe Dannon Company Inc.,\nThe Sugar Association, and \nUnilever.\n\nThe ASN has conflicting interests in taking funding from food industry marketing groups while providing unbiassed information on nutrition; these conflicting interests have caused criticism and concerns of bias. ASN actions have also been criticized for being better-aligned with the nutritional advice of sponsors than the advice of the World Health Organization and other public health, public interest, and government organizations.\n\nLong-time member Marion Nestle has voiced concerns about what she sees as a \"too-cozy relationship with food company sponsors\" within the organization. In a 2015 report, Michele Simon also voiced concerns regarding corporate involvement with the society.\n\n"}
{"id": "42898", "url": "https://en.wikipedia.org/wiki?curid=42898", "title": "Anthrax", "text": "Anthrax\n\nAnthrax is an infection caused by the bacterium \"Bacillus anthracis\". It can occur in four forms: skin, lungs, intestinal, and injection. Symptoms begin between one day and two months after the infection is contracted. The skin form presents with a small blister with surrounding swelling that often turns into a painless ulcer with a black center. The inhalation form presents with fever, chest pain, and shortness of breath. The intestinal form presents with diarrhea which may contain blood, abdominal pains, and nausea and vomiting. The injection form presents with fever and an abscess at the site of drug injection.\nAnthrax is spread by contact with the bacterium's spores, which often appear in infectious animal products. Contact is by breathing, eating, or through an area of broken skin. It does not typically spread directly between people. Risk factors include people who work with animals or animal products, travelers, postal workers, and military personnel. Diagnosis can be confirmed based on finding antibodies or the toxin in the blood or by culture of a sample from the infected site.\nAnthrax vaccination is recommended for people who are at high risk of infection. Immunizing animals against anthrax is recommended in areas where previous infections have occurred. Two months of antibiotics such as ciprofloxacin, levofloxacin, and doxycycline after exposure can also prevent infection. If infection occurs treatment is with antibiotics and possibly antitoxin. The type and number of antibiotics used depends on the type of infection. Antitoxin is recommended for those with widespread infection.\nAlthough a rare disease, human anthrax, when it does occur, is most common in Africa and central and southern Asia. It also occurs more regularly in Southern Europe than elsewhere on the continent, and is uncommon in Northern Europe and North America. Globally, at least 2,000 cases occur a year with about two cases a year in the United States. Skin infections represent more than 95% of cases. Without treatment, the risk of death from skin anthrax is 24%. For intestinal infection, the risk of death is 25 to 75%, while respiratory anthrax has a mortality of 50 to 80%, even with treatment. Until the 20th century, anthrax infections killed hundreds of thousands of people and animals each year. Anthrax has been developed as a weapon by a number of countries. In plant-eating animals, infection occurs when they eat or breathe in the spores while grazing. Carnivores may become infected by eating infected animals.\n\nCutaneous anthrax, also known as Hide porter's disease, is when anthrax occurs on the skin. It is the most common form (>90% of anthrax cases). It is also the least dangerous form of anthrax (low mortality with treatment, 20% mortality without). Cutaneous anthrax presents as a boil-like skin lesion that eventually forms an ulcer with a black center (eschar). The black eschar often shows up as a large, painless, necrotic ulcer (beginning as an irritating and itchy skin lesion or blister that is dark and usually concentrated as a black dot, somewhat resembling bread mold) at the site of infection. In general, cutaneous infections form within the site of spore penetration between two and five days after exposure. Unlike bruises or most other lesions, cutaneous anthrax infections normally do not cause pain. Nearby lymph nodes may become infected, reddened, swollen, and painful. A scab forms over the lesion soon, and falls off in a few weeks. Complete recovery may take longer.\nCutaneous anthrax is typically caused when \"B. anthracis\" spores enter through cuts on the skin. This form is found most commonly when humans handle infected animals and/or animal products.\n\nCutaneous anthrax is rarely fatal if treated, because the infection area is limited to the skin, preventing the lethal factor, edema factor, and protective antigen from entering and destroying a vital organ. Without treatment, about 20% of cutaneous skin infection cases progress to toxemia and death.\n\nRespiratory infection in humans is relatively rare and presents as two stages. It infects the lymph nodes in the chest first, rather than the lungs themselves, a condition called hemorrhagic mediastinitis, causing bloody fluid to accumulate in the chest cavity, therefore causing shortness of breath. The first stage causes cold and flu-like symptoms. Symptoms include fever, shortness of breath, cough, fatigue, and chills. This can last hours to days. Often, many fatalities from inhalational anthrax are when the first stage is mistaken for the cold or flu and the victim does not seek treatment until the second stage, which is 90% fatal. The second (pneumonia) stage occurs when the infection spreads from the lymph nodes to the lungs. Symptoms of the second stage develop suddenly after hours or days of the first stage. Symptoms include high fever, extreme shortness of breath, shock, and rapid death within 48 hours in fatal cases. Historical mortality rates were over 85%, but when treated early (seen in the 2001 anthrax attacks), observed case fatality rate dropped to 45%. Distinguishing pulmonary anthrax from more common causes of respiratory illness is essential to avoiding delays in diagnosis and thereby improving outcomes. An algorithm for this purpose has been developed.\n\nGastrointestinal (GI) infection is most often caused by consuming anthrax-infected meat and is characterized by diarrhea, potentially with blood, abdominal pains, acute inflammation of the intestinal tract, and loss of appetite. Occasional vomiting of blood can occur. Lesions have been found in the intestines and in the mouth and throat. After the bacterium invades the gastrointestinal system, it spreads to the bloodstream and throughout the body, while continuing to make toxins. GI infections can be treated, but usually result in fatality rates of 25% to 60%, depending upon how soon treatment commences. This form of anthrax is the rarest form.\n\n\"Bacillus anthracis\" is a rod-shaped, Gram-positive, aerobic bacterium about 1 by 9 μm in size. It was shown to cause disease by Robert Koch in 1876 when he took a blood sample from an infected cow, isolated the bacteria, and put them into a mouse. The bacterium normally rests in spore form in the soil, and can survive for decades in this state. Herbivores are often infected whilst grazing, especially when eating rough, irritant, or spiky vegetation; the vegetation has been hypothesized to cause wounds within the gastrointestinal tract permitting entry of the bacterial spores into the tissues, though this has not been proven. Once ingested or placed in an open wound, the bacteria begin multiplying inside the animal or human and typically kill the host within a few days or weeks. The spores germinate at the site of entry into the tissues and then spread by the circulation to the lymphatics, where the bacteria multiply.\n\nThe production of two powerful exotoxins and lethal toxin by the bacteria causes death. Veterinarians can often tell a possible anthrax-induced death by its sudden occurrence, and by the dark, nonclotting blood that oozes from the body orifices. Most anthrax bacteria inside the body after death are outcompeted and destroyed by anaerobic bacteria within minutes to hours \"post mortem\". However, anthrax vegetative bacteria that escape the body via oozing blood or through the opening of the carcass may form hardy spores. These vegetative bacteria are not contagious. One spore forms per one vegetative bacterium. The triggers for spore formation are not yet known, though oxygen tension and lack of nutrients may play roles. Once formed, these spores are very hard to eradicate.\n\nThe infection of herbivores (and occasionally humans) by the inhalational route normally proceeds as follows: Once the spores are inhaled, they are transported through the air passages into the tiny air sacs (alveoli) in the lungs. The spores are then picked up by scavenger cells (macrophages) in the lungs and are transported through small vessels (lymphatics) to the lymph nodes in the central chest cavity (mediastinum). Damage caused by the anthrax spores and bacilli to the central chest cavity can cause chest pain and difficulty in breathing. Once in the lymph nodes, the spores germinate into active bacilli that multiply and eventually burst the macrophages, releasing many more bacilli into the bloodstream to be transferred to the entire body. Once in the blood stream, these bacilli release three proteins named lethal factor, edema factor, and protective antigen. The three are not toxic by themselves, but their combination is incredibly lethal to humans. Protective antigen combines with these other two factors to form lethal toxin and edema toxin, respectively. These toxins are the primary agents of tissue destruction, bleeding, and death of the host. If antibiotics are administered too late, even if the antibiotics eradicate the bacteria, some hosts still die of toxemia because the toxins produced by the bacilli remain in their system at lethal dose levels.\n\nThe spores of anthrax are able to survive in harsh conditions for decades or even centuries. Such spores can be found on all continents, including Antarctica. Disturbed grave sites of infected animals have been known to cause infection after 70 years.\n\nOccupational exposure to infected animals or their products (such as skin, wool, and meat) is the usual pathway of exposure for humans. Workers who are exposed to dead animals and animal products are at the highest risk, especially in countries where anthrax is more common. Anthrax in livestock grazing on open range where they mix with wild animals still occasionally occurs in the United States and elsewhere. Many workers who deal with wool and animal hides are routinely exposed to low levels of anthrax spores, but most exposure levels are not sufficient to develop anthrax infections. A lethal infection is reported to result from inhalation of about 10,000–20,000 spores, though this dose varies among host species. Little documented evidence is available to verify the exact or average number of spores needed for infection.\n\nHistorically, inhalational anthrax was called woolsorters' disease because it was an occupational hazard for people who sorted wool. Today, this form of infection is extremely rare in advanced nations, as almost no infected animals remain.\n\nAnthrax can enter the human body through the intestines (ingestion), lungs (inhalation), or skin (cutaneous) and causes distinct clinical symptoms based on its site of entry. In general, an infected human will be quarantined. However, anthrax does not usually spread from an infected human to a noninfected human. But, if the disease is fatal to the person's body, its mass of anthrax bacilli becomes a potential source of infection to others and special precautions should be used to prevent further contamination. Inhalational anthrax, if left untreated until obvious symptoms occur, is usually fatal.\n\nAnthrax can be contracted in laboratory accidents or by handling infected animals, their wool or their hides. It has also been used in biological warfare agents and by terrorists to intentionally infect as exemplified by the 2001 anthrax attacks.\n\nThe lethality of the anthrax disease is due to the bacterium's two principal virulence factors: the poly-D-glutamic acid capsule, which protects the bacterium from phagocytosis by host neutrophils, and the tripartite protein toxin, called anthrax toxin. Anthrax toxin is a mixture of three protein components: protective antigen (PA), edema factor (EF), and lethal factor (LF). PA plus LF produces lethal toxin, and PA plus EF produces edema toxin. These toxins cause death and tissue swelling (edema), respectively.\n\nTo enter the cells, the edema and lethal factors use another protein produced by \"B. anthracis\" called protective antigen, which binds to two surface receptors on the host cell. A cell protease then cleaves PA into two fragments: PA and PA. PA dissociates into the extracellular medium, playing no further role in the toxic cycle. PA then oligomerizes with six other PA fragments forming a heptameric ring-shaped structure named a prepore. Once in this shape, the complex can competitively bind up to three EFs or LFs, forming a resistant complex. Receptor-mediated endocytosis occurs next, providing the newly formed toxic complex access to the interior of the host cell. The acidified environment within the endosome triggers the heptamer to release the LF and/or EF into the cytosol. It is unknown how exactly the complex results in the death of the cell.\n\nEdema factor is a calmodulin-dependent adenylate cyclase. Adenylate cyclase catalyzes the conversion of ATP into cyclic AMP (cAMP) and pyrophosphate. The complexation of adenylate cyclase with calmodulin removes calmodulin from stimulating calcium-triggered signaling, thus inhibiting the immune response. To be specific, LF inactivates neutrophils (a type of phagocytic cell) by the process just described so they cannot phagocytose bacteria. Throughout history, lethal factor was presumed to cause macrophages to make TNF-alpha and interleukin 1, beta (IL1B). TNF-alpha is a cytokine whose primary role is to regulate immune cells, as well as to induce inflammation and apoptosis or programmed cell death. Interleukin 1, beta is another cytokine that also regulates inflammation and apoptosis. The overproduction of TNF-alpha and IL1B ultimately leads to septic shock and death. However, recent evidence indicates anthrax also targets endothelial cells that line serous cavities such as the pericardial cavity, pleural cavity, and peritoneal cavity, lymph vessels, and blood vessels, causing vascular leakage of fluid and cells, and ultimately hypovolemic shock and septic shock.\n\nVarious techniques may be used for the direct identification of \"B. anthracis\" in clinical material. Firstly, specimens may be Gram stained. \"Bacillus\" spp. are quite large in size (3 to 4 μm long), they may grow in long chains, and they stain Gram-positive. To confirm the organism is \"B. anthracis\", rapid diagnostic techniques such as polymerase chain reaction-based assays and immunofluorescence microscopy may be used.\n\nAll \"Bacillus\" species grow well on 5% sheep blood agar and other routine culture media. Polymyxin-lysozyme-EDTA-thallous acetate can be used to isolate \"B. anthracis\" from contaminated specimens, and bicarbonate agar is used as an identification method to induce capsule formation. \"Bacillus\" spp. usually grow within 24 hours of incubation at 35 °C, in ambient air (room temperature) or in 5% CO. If bicarbonate agar is used for identification, then the medium must be incubated in 5% CO. \"B. anthracis\" colonies are medium-large, gray, flat, and irregular with swirling projections, often referred to as having a \"medusa head\" appearance, and are not hemolytic on 5% sheep blood agar. The bacteria are not motile, susceptible to penicillin, and produce a wide zone of lecithinase on egg yolk agar. Confirmatory testing to identify \"B. anthracis\" includes gamma bacteriophage testing, indirect hemagglutination, and enzyme-linked immunosorbent assay to detect antibodies. The best confirmatory precipitation test for anthrax is the Ascoli test.\n\nIf a person is suspected as having died from anthrax, precautions should be taken to avoid skin contact with the potentially contaminated body and fluids exuded through natural body openings. The body should be put in strict quarantine. A blood sample should then be collected and sealed in a container and analyzed in an approved laboratory to ascertain if anthrax is the cause of death. Then, the body should be incinerated. Microscopic visualization of the encapsulated bacilli, usually in very large numbers, in a blood smear stained with polychrome methylene blue (McFadyean stain) is fully diagnostic, though culture of the organism is still the gold standard for diagnosis. Full isolation of the body is important to prevent possible contamination of others. Protective, impermeable clothing and equipment such as rubber gloves, rubber apron, and rubber boots with no perforations should be used when handling the body. No skin, especially if it has any wounds or scratches, should be exposed. Disposable personal protective equipment is preferable, but if not available, decontamination can be achieved by autoclaving. Disposable personal protective equipment and filters should be autoclaved, and/or burned and buried. Anyone working with anthrax in a suspected or confirmed person should wear respiratory equipment capable of filtering particles of their size or smaller. The US National Institute for Occupational Safety and Health – and Mine Safety and Health Administration-approved high-efficiency respirator, such as a half-face disposable respirator with a high-efficiency particulate air filter, is recommended. All possibly contaminated bedding or clothing should be isolated in double plastic bags and treated as possible biohazard waste. The body of an infected person should be sealed in an airtight body bag. Dead people who are opened and not burned provide an ideal source of anthrax spores. Cremating people is the preferred way of handling body disposal. No embalming or autopsy should be attempted without a fully equipped biohazard laboratory and trained, knowledgeable personnel.\n\nVaccines against anthrax for use in livestock and humans have had a prominent place in the history of medicine. The French scientist Louis Pasteur developed the first effective vaccine in 1881. Human anthrax vaccines were developed by the Soviet Union in the late 1930s and in the US and UK in the 1950s. The current FDA-approved US vaccine was formulated in the 1960s.\n\nCurrently administered human anthrax vaccines include acellular (United States) and live vaccine (Russia) varieties. All currently used anthrax vaccines show considerable local and general reactogenicity (erythema, induration, soreness, fever) and serious adverse reactions occur in about 1% of recipients. The American product, BioThrax, is licensed by the FDA and was formerly administered in a six-dose primary series at 0, 2, 4 weeks and 6, 12, 18 months, with annual boosters to maintain immunity. In 2008, the FDA approved omitting the week-2 dose, resulting in the currently recommended five-dose series. New second-generation vaccines currently being researched include recombinant live vaccines and recombinant subunit vaccines. In the 20th century the use of a modern product (BioThrax) to protect American troops against the use of anthrax in biological warfare was controversial.\n\nPreventative antibiotics are recommended in those who have been exposed. Early detection of sources of anthrax infection can allow preventive measures to be taken. In response to the anthrax attacks of October 2001, the United States Postal Service (USPS) installed biodetection systems (BDSs) in their large-scale mail processing facilities. BDS response plans were formulated by the USPS in conjunction with local responders including fire, police, hospitals and public health. Employees of these facilities have been educated about anthrax, response actions, and prophylactic medication. Because of the time delay inherent in getting final verification that anthrax has been used, prophylactic antibiotic treatment of possibly exposed personnel must be started as soon as possible.\n\nAnthrax cannot be spread directly from person to person, but a person's clothing and body may be contaminated with anthrax spores. Effective decontamination of people can be accomplished by a thorough wash-down with antimicrobial soap and water. Waste water should be treated with bleach or another antimicrobial agent. Effective decontamination of articles can be accomplished by boiling them in water for 30 minutes or longer. Chlorine bleach is ineffective in destroying spores and vegetative cells on surfaces, though formaldehyde is effective. Burning clothing is very effective in destroying spores. After decontamination, there is no need to immunize, treat, or isolate contacts of persons ill with anthrax unless they were also exposed to the same source of infection.\n\nEarly antibiotic treatment of anthrax is essential; delay significantly lessens chances for survival.\n\nTreatment for anthrax infection and other bacterial infections includes large doses of intravenous and oral antibiotics, such as fluoroquinolones (ciprofloxacin), doxycycline, erythromycin, vancomycin, or penicillin. FDA-approved agents include ciprofloxacin, doxycycline, and penicillin.\n\nIn possible cases of pulmonary anthrax, early antibiotic prophylaxis treatment is crucial to prevent possible death.\n\nIn recent years, many attempts have been made to develop new drugs against anthrax, but existing drugs are effective if treatment is started soon enough.\n\nIn May 2009, Human Genome Sciences submitted a biologic license application (BLA, permission to market) for its new drug, raxibacumab (brand name ABthrax) intended for emergency treatment of inhaled anthrax. On 14 December 2012, the US Food and Drug Administration approved raxibacumab injection to treat inhalational anthrax. Raxibacumab is a monoclonal antibody that neutralizes toxins produced by \"B. anthracis\". On March, 2016, FDA approved a second anthrax treatment using a monoclonal antibody which neutralizes the toxins produced by \"B. anthracis\". Obiltoxaximab is approved to treat inhalational anthrax in conjunction with appropriate antibacterial drugs, and for prevention when alternative therapies are not available or appropriate.\n\nGlobally, at least 2,000 cases occur a year.\n\nThe last fatal case of natural inhalational anthrax in the United States occurred in California in 1976, when a home weaver died after working with infected wool imported from Pakistan. To minimize the chance of spreading the disease, the deceased was transported to UCLA in a sealed plastic body bag within a sealed metal container for autopsy.\n\nGastrointestinal anthrax is exceedingly rare in the United States, with two cases on record, the first was reported in 1942, according to the Centers for Disease Control and Prevention.\n\nDuring December 2009, the New Hampshire Department of Health and Human Services confirmed a case of gastrointestinal anthrax in an adult female. The CDC investigated the source and the possibility that it was contracted from an African drum recently used by the woman taking part in a drum circle. The woman apparently inhaled anthrax [in spore form] from the hide of the drum. She became critically ill, but with gastrointestinal anthrax rather than inhaled anthrax, which made her unique in American medical history. The building where the infection took place was cleaned and reopened to the public and the woman recovered. Jodie Dionne-Odom, New Hampshire state epidemiologist, stated, \"It is a mystery. We really don't know why it happened.\"\n\nIn November 2008, a drum maker in the United Kingdom who worked with untreated animal skins died from anthrax. In December 2009, an outbreak of anthrax occurred amongst heroin addicts in the Glasgow and Stirling areas of Scotland, resulting in 14 deaths. The source of the anthrax is believed to be dilution of the heroin with bone meal in Afghanistan.\n\nThe English name comes from \"anthrax\" (), the Greek word for coal, possibly having Egyptian etymology, because of the characteristic black skin lesions developed by victims with a cutaneous anthrax infection. The central, black eschar, surrounded by vivid red skin has long been recognised as typical of the disease. The first recorded use of the word \"anthrax\" in English is in a 1398 translation of Bartholomaeus Anglicus' work \"\" (\"On the Properties of Things\", 1240).\n\nAnthrax has been known by a wide variety of names, indicating its symptoms, location and groups considered most vulnerable to infection. These include Siberian plague, Cumberland disease, charbon, splenic fever, malignant edema, woolsorter's disease, and even \"\".\n\nRobert Koch, a German physician and scientist, first identified the bacterium that caused the anthrax disease in 1875 in Wolsztyn (now part of Poland). His pioneering work in the late 19th century was one of the first demonstrations that diseases could be caused by microbes. In a groundbreaking series of experiments, he uncovered the lifecycle and means of transmission of anthrax. His experiments not only helped create an understanding of anthrax, but also helped elucidate the role of microbes in causing illness at a time when debates still took place over spontaneous generation versus cell theory. Koch went on to study the mechanisms of other diseases and won the 1905 Nobel Prize in Physiology or Medicine for his discovery of the bacterium causing tuberculosis.\n\nAlthough Koch arguably made the greatest theoretical contribution to our understanding of anthrax, other researchers were more concerned with the practical questions of how to prevent the disease. In Britain, where anthrax affected workers in the wool, worsted, hides and tanning industries, it was viewed with fear. John Henry Bell, a doctor based in Bradford, first made the link between the mysterious and deadly \"woolsorter's disease\" and anthrax, showing in 1878 that they were one and the same. In the early twentieth century, Friederich Wilhelm Eurich, the German bacteriologist who settled in Bradford with his family as a child, carried out important research for the local Anthrax Investigation Board. Eurich also made valuable contributions to a Home Office Departmental Committee of Inquiry, established in 1913 to address the continuing problem of industrial anthrax. His work in this capacity, much of it collaboration with the factory inspector G. Elmhirst Duckering, led directly to the Anthrax Prevention Act (1919).\n\nAnthrax posed a major economic challenge in France and elsewhere during the nineteenth century. Horses, cattle and sheep were particularly vulnerable, and national funds were set aside to investigate the production of a vaccine. The noted French scientist Louis Pasteur was charged with the production of a vaccine, following his successful work in developing methods which helped to protect the important wine and silk industries.\n\nIn May 1881, Pasteur – in collaboration with his assistants Jean-Joseph Henri Toussaint, Émile Roux and others – performed a public experiment at Pouilly-le-Fort to demonstrate his concept of vaccination. He prepared two groups of 25 sheep, one goat, and several cows. The animals of one group were injected with an anthrax vaccine prepared by Pasteur twice, at an interval of 15 days; the control group was left unvaccinated. Thirty days after the first injection, both groups were injected with a culture of live anthrax bacteria. All the animals in the unvaccinated group died, while all of the animals in the vaccinated group survived.\n\nAfter this apparent triumph, which was widely reported in the local, national and international press, Pasteur made strenuous efforts to export the vaccine beyond France. He used his celebrity status to establish Pasteur Institutes across Europe and Asia, and his nephew, Adrien Loir, travelled to Australia in 1888 to try and introduce the vaccine to combat anthrax in New South Wales. Ultimately the vaccine was unsuccessful in the challenging climate of rural Australia, and it was soon superseded by a more robust version developed by local researchers John Gunn and John McGarvie Smith.\n\nThe human vaccine for anthrax became available in 1954. This was a cell-free vaccine instead of the live-cell Pasteur-style vaccine used for veterinary purposes. An improved cell-free vaccine became available in 1970.\n\n\nAnthrax spores can survive for very long periods of time in the environment after release. Chemical methods for cleaning anthrax-contaminated sites or materials may use oxidizing agents such as peroxides, ethylene oxide, Sandia Foam, chlorine dioxide (used in the Hart Senate Office Building), peracetic acid, ozone gas, hypochlorous acid, sodium persulfate, and liquid bleach products containing sodium hypochlorite. Nonoxidizing agents shown to be effective for anthrax decontamination include methyl bromide, formaldehyde, and metam sodium. These agents destroy bacterial spores. All of the aforementioned anthrax decontamination technologies have been demonstrated to be effective in laboratory tests conducted by the US EPA or others.\nA bleach solution for treating hard surfaces has been approved by the EPA.\n\nChlorine dioxide has emerged as the preferred biocide against anthrax-contaminated sites, having been employed in the treatment of numerous government buildings over the past decade. Its chief drawback is the need for \"in situ\" processes to have the reactant on demand.\n\nTo speed the process, trace amounts of a nontoxic catalyst composed of iron and tetroamido macrocyclic ligands are combined with sodium carbonate and bicarbonate and converted into a spray. The spray formula is applied to an infested area and is followed by another spray containing tert-butyl hydroperoxide.\n\nUsing the catalyst method, a complete destruction of all anthrax spores can be achieved in under 30 minutes. A standard catalyst-free spray destroys fewer than half the spores in the same amount of time.\n\nCleanups at a Senate office building, several contaminated postal facilities, and other US government and private office buildings, a collaborative effort headed by the Environmental Protection Agency, showed decontamination to be possible but time-consuming and costly. Clearing the Senate office building of anthrax spores cost $27 million, according to the Government Accountability Office. Cleaning the Brentwood postal facility in Washington cost $130 million and took 26 months. Since then, newer and less costly methods have been developed.\n\nCleanup of anthrax-contaminated areas on ranches and in the wild is much more problematic. Carcasses may be burned, though it often takes up to three days to burn a large carcass and this is not feasible in areas with little wood. Carcasses may also be buried, though the burying of large animals deeply enough to prevent resurfacing of spores requires much manpower and expensive tools. Carcasses have been soaked in formaldehyde to kill spores, though this has environmental contamination issues. Block burning of vegetation in large areas enclosing an anthrax outbreak has been tried; this, while environmentally destructive, causes healthy animals to move away from an area with carcasses in search of fresh grass. Some wildlife workers have experimented with covering fresh anthrax carcasses with shadecloth and heavy objects. This prevents some scavengers from opening the carcasses, thus allowing the putrefactive bacteria within the carcass to kill the vegetative \"B. anthracis\" cells and preventing sporulation. This method also has drawbacks, as scavengers such as hyenas are capable of infiltrating almost any exclosure.\n\nThe experimental site at Gruinard Island is said to have been decontaminated with a mixture of formaldehyde and seawater by the Ministry of Defence. It is not clear whether similar treatments had been applied to US test sites.\n\nAnthrax spores have been used as a biological warfare weapon. Its first modern incidence occurred when Nordic rebels, supplied by the German General Staff, used anthrax with unknown results against the Imperial Russian Army in Finland in 1916. Anthrax was first tested as a biological warfare agent by Unit 731 of the Japanese Kwantung Army in Manchuria during the 1930s; some of this testing involved intentional infection of prisoners of war, thousands of whom died. Anthrax, designated at the time as Agent N, was also investigated by the Allies in the 1940s.\n\nA long history of practical bioweapons research exists in this area. For example, in 1942, British bioweapons trials severely contaminated Gruinard Island in Scotland with anthrax spores of the Vollum-14578 strain, making it a no-go area until it was decontaminated in 1990. The Gruinard trials involved testing the effectiveness of a submunition of an \"N-bomb\" a biological weapon containing dried anthrax spores. Additionally, five million \"cattle cakes\" (animal feed pellets impregnated with anthrax spores) were prepared and stored at Porton Down for \"Operation Vegetarian\" antilivestock attacks against Germany to be made by the Royal Air Force. The plan was for anthrax-based biological weapons to be dropped on Germany in 1944. However, the edible cattle cakes and the bomb were not used; the cattle cakes were incinerated in late 1945.\n\nWeaponized anthrax was part of the US stockpile prior to 1972, when the United States signed the Biological Weapons Convention. President Nixon ordered the dismantling of US biowarfare programs in 1969 and the destruction of all existing stockpiles of bioweapons. In 1978–79, the Rhodesian government used anthrax against cattle and humans during its campaign against rebels. The Soviet Union created and stored 100 to 200 tons of anthrax spores at Kantubek on Vozrozhdeniya Island. They were abandoned in 1992 and destroyed in 2002.\n\nAmerican military and British Army personnel are routinely vaccinated against anthrax prior to active service in places where biological attacks are considered a threat.\n\nDespite signing the 1972 agreement to end bioweapon production, the government of the Soviet Union had an active bioweapons program that included the production of hundreds of tons of weapons-grade anthrax after this period. On 2 April 1979, some of the over one million people living in Sverdlovsk (now called Ekaterinburg, Russia), about east of Moscow, were exposed to an accidental release of anthrax from a biological weapons complex located near there. At least 94 people were infected, of whom at least 68 died. One victim died four days after the release, 10 over an eight-day period at the peak of the deaths, and the last six weeks later. Extensive cleanup, vaccinations, and medical interventions managed to save about 30 of the victims. Extensive cover-ups and destruction of records by the KGB continued from 1979 until Russian President Boris Yeltsin admitted this anthrax accident in 1992. Jeanne Guillemin reported in 1999 that a combined Russian and United States team investigated the accident in 1992.\n\nNearly all of the night-shift workers of a ceramics plant directly across the street from the biological facility (compound 19) became infected, and most died. Since most were men, some NATO governments suspected the Soviet Union had developed a sex-specific weapon. The government blamed the outbreak on the consumption of anthrax-tainted meat, and ordered the confiscation of all uninspected meat that entered the city. They also ordered all stray dogs to be shot and people not have contact with sick animals. Also, a voluntary evacuation and anthrax vaccination program was established for people from 18–55.\n\nTo support the cover-up story, Soviet medical and legal journals published articles about an outbreak in livestock that caused GI anthrax in people having consumed infected meat, and cutaneous anthrax in people having come into contact with the animals. All medical and public health records were confiscated by the KGB. In addition to the medical problems the outbreak caused, it also prompted Western countries to be more suspicious of a covert Soviet bioweapons program and to increase their surveillance of suspected sites. In 1986, the US government was allowed to investigate the incident, and concluded the exposure was from aerosol anthrax from a military weapons facility. In 1992, President Yeltsin admitted he was \"absolutely certain\" that \"rumors\" about the Soviet Union violating the 1972 Bioweapons Treaty were true. The Soviet Union, like the US and UK, had agreed to submit information to the UN about their bioweapons programs, but omitted known facilities and never acknowledged their weapons program.\n\nIn theory, anthrax spores can be cultivated with minimal special equipment and a first-year collegiate microbiological education.\nTo make large amounts of an aerosol form of anthrax suitable for biological warfare requires extensive practical knowledge, training, and highly advanced equipment.\n\nConcentrated anthrax spores were used for bioterrorism in the 2001 anthrax attacks in the United States, delivered by mailing postal letters containing the spores. The letters were sent to several news media offices and two Democratic senators: Tom Daschle of South Dakota and Patrick Leahy of Vermont. As a result, 22 were infected and five died. Only a few grams of material were used in these attacks and in August 2008, the US Department of Justice announced they believed that Dr. Bruce Ivins, a senior biodefense researcher employed by the United States government, was responsible. These events also spawned many anthrax hoaxes.\n\nDue to these events, the US Postal Service installed biohazard detection systems at its major distribution centers to actively scan for anthrax being transported through the mail.\n\nIn response to the postal anthrax attacks and hoaxes, the United States Postal Service sterilized some mail using gamma irradiation and treatment with a proprietary enzyme formula supplied by Sipco Industries.\n\nA scientific experiment performed by a high school student, later published in \"The Journal of Medical Toxicology\", suggested a domestic electric iron at its hottest setting (at least ) used for at least 5 minutes should destroy all anthrax spores in a common postal envelope.\n\nAnthrax is especially rare in dogs and cats, as is evidenced by a single reported case in the United States in 2001. Anthrax outbreaks occur in some wild animal populations with some regularity.\n\nRussian researchers estimate arctic permafrost contains around 1.5 million anthrax-infected reindeer carcasses, and the spores may survive in the permafrost for 105 years. There is a risk that global warming in the Arctic can thaw the permafrost, releasing anthrax spores in the carcasses. In 2016, an anthrax outbreak in reindeer was linked to a 75-year-old carcass that defrosted during a heat wave.\n\n"}
{"id": "55504612", "url": "https://en.wikipedia.org/wiki?curid=55504612", "title": "Body image law", "text": "Body image law\n\nBody image law is the developing area of law that, according to Dr Marilyn Bromberg of the University of Western Australia Law School and Cindy Halliwell, a law student at Deakin University, \"encompasses the bills, laws and government actions (such as establishing parliamentary inquiries and creating policies) that may help to improve the body image of the general public, and particularly of young people\".<ref name=\"mb/ch\"></ref> Among the reasons for implementing law in this area is to prevent the images of unhealthily thin women causing poor body image which can, along with other factors, lead to an eating disorder.\n\nThe Israeli government passed a body image law in 2012 which became operational the following year. The law requires models to have a minimum body mass index to work and if an image was photoshopped to make the model appear thinner, it must have a warning. The warning must state that the image was modified and it must take up at least seven percent of the image. Breaches can result in a civil lawsuit. \n\nThe French Government passed a similar law in 2015 which came into effect in 2017. This law requires that models provide their employers with a \"medical certificate, valid for up to two years, confirming their general physical well-being and the fact that they are not excessively underweight.\" The BMI of models older than 16 will also be taken into consideration, when determining their overall health. \n\nIn contrast to the Israeli law, breaching it attracts criminal sanctions. Additionally, any photo that has been digitally altered must be labeled as such; failure to label these photos will result in a \"fine of 37,500 euros, or more than $41,000,\" and hiring a model without the verified medical certificate and requirements \"carries a fine of €75,000 and six months in jail.\" The law dictates that digitally altered images must be labeled \"applies only to advertising, not to editorial images in magazines or newspapers.\" \n\nThe Greater London Authority banned advertisements that promote unhealthy body image on Transport for London public transport in 2016.<ref name=\"ap/mo\"></ref> Similarly, Trondheim in Norway banned advertisements that promote unhealthy body image in public places in 2017. \n\nThe Australian Government's position in this area is that it is up to industry to solve the problem of poor body image. Likewise, the previous Labor Government created a non-binding Voluntary Industry Code of Conduct on Body Image.\n"}
{"id": "811714", "url": "https://en.wikipedia.org/wiki?curid=811714", "title": "Comparison of the healthcare systems in Canada and the United States", "text": "Comparison of the healthcare systems in Canada and the United States\n\nComparison of the healthcare systems in Canada and the United States is often made by government, public health and public policy analysts. The two countries had similar healthcare systems before Canada changed its system in the 1960s and 1970s. The United States spends much more money on healthcare than Canada, on both a per-capita basis and as a percentage of GDP. In 2006, per-capita spending for health care in Canada was US$3,678; in the U.S., US$6,714. The U.S. spent 15.3% of GDP on healthcare in that year; Canada spent 10.0%. In 2006, 70% of healthcare spending in Canada was financed by government, versus 46% in the United States. Total government spending per capita in the U.S. on healthcare was 23% higher than Canadian government spending, and U.S. government expenditure on healthcare was just under 83% of total Canadian spending (public and private) though these statistics don't take into account population differences.\n\nStudies have come to different conclusions about the result of this disparity in spending. A 2007 review of all studies comparing health outcomes in Canada and the US in a Canadian peer-reviewed medical journal found that \"health outcomes may be superior in patients cared for in Canada versus the United States, but differences are not consistent.\" Some of the noted differences were a higher life expectancy in Canada, as well as a lower infant mortality rate than the United States.\n\nOne commonly cited comparison, the 2000 World Health Organization's ratings of \"overall health service performance\", which used a \"composite measure of achievement in the level of health, the distribution of health, the level of responsiveness and fairness of financial contribution\", ranked Canada 30th and the US 37th among 191 member nations. This study rated the US \"responsiveness\", or quality of service for individuals receiving treatment, as 1st, compared with 7th for Canada. However, the average life expectancy for Canadians was 80.34 years compared with 78.6 years for residents of the US.\n\nThe WHO's study methods were criticized by some analyses.\nWhile life-expectancy and infant mortality are commonly used in comparing nationwide health care, they are in fact affected by many factors other than the quality of a nation's health care system, including individual behavior and population makeup. A 2007 report by the Congressional Research Service carefully summarizes some recent data and noted the \"difficult research issues\" facing international comparisons.\n\nIn 2004, government funding of healthcare in Canada was equivalent to $1,893 per person. In the US, government spending per person was $2,728.\n\nThe Canadian healthcare system is composed of at least 10 mostly autonomous provincial healthcare systems that report to their provincial governments, and a federal system which covers the military and First Nations. This causes a significant degree of variation in funding and coverage within the country.\n\nCanada and the US had similar healthcare systems in the early 1960s, but now have a different mix of funding mechanisms. Canada's universal single-payer healthcare system covers about 70% of expenditures, and the Canada Health Act requires that all insured persons be fully insured, without co-payments or user fees, for all medically necessary hospital and physician care. About 91% of hospital expenditures and 99% of total physician services are financed by the public sector. In the United States, with its mixed public-private system, 16% or 45 million American residents are uninsured at any one time. The U.S. is one of two OECD countries not to have some form of universal health coverage, the other being Turkey. Mexico established a universal healthcare program by November 2008.\n\nThe governments of both nations are closely involved in healthcare. The central structural difference between the two is in health insurance. In Canada, the federal government is committed to providing funding support to its provincial governments for healthcare expenditures as long as the province in question abides by accessibility guarantees as set out in the Canada Health Act, which explicitly prohibits billing end users for procedures that are covered by Medicare. While some label Canada's system as \"socialized medicine\", health economists do not use that term. Unlike systems with public delivery, such as the UK, the Canadian system provides public coverage for a combination of public and private delivery. Princeton University health economist Uwe E. Reinhardt says that single-payer systems are not \"socialized medicine\" but \"social insurance\" systems, since providers (such as doctors) are largely in the private sector. Similarly, Canadian hospitals are controlled by private boards or regional health authorities, rather than being part of government.\n\nIn the US, direct government funding of health care is limited to Medicare, Medicaid, and the State Children's Health Insurance Program (SCHIP), which cover eligible senior citizens, the very poor, disabled persons, and children. The federal government also runs the Veterans Administration, which provides care directly to retired or disabled veterans, their families, and survivors through medical centers and clinics.\n\nThe U.S. government also runs the Military Health System. In fiscal year 2007, the MHS had a total budget of $39.4 billion and served approximately 9.1 million beneficiaries, including active-duty personnel and their families, and retirees and their families. The MHS includes 133,000 personnel, 86,000 military and 47,000 civilian, working at more than 1,000 locations worldwide, including 70 inpatient facilities and 1,085 medical, dental, and veterans' clinics.\n\nOne study estimates that about 25 percent of the uninsured in the U.S. are eligible for these programs but remain unenrolled; however, extending coverage to all who are eligible remains a fiscal and political challenge.\n\nFor everyone else, health insurance must be paid for privately. Some 59% of U.S. residents have access to health care insurance through employers, although this figure is decreasing, and coverages as well as workers' expected contributions vary widely. Those whose employers do not offer health insurance, as well as those who are self-employed or unemployed, must purchase it on their own. Nearly 27 million of the 45 million uninsured U.S. residents worked at least part-time in 2007, and more than a third were in households that earned $50,000 or more per year.\n\nDespite the greater role of private business in the US, federal and state agencies are increasingly involved, paying about 45% of the $2.2 trillion the nation spent on medical care in 2004. The U.S. government spends more on healthcare than on Social Security and national defense combined, according to the Brookings Institution.\n\nBeyond its direct spending, the US government is also highly involved in healthcare through regulation and legislation. For example, the Health Maintenance Organization Act of 1973 provided grants and loans to subsidize Health Maintenance Organizations and contained provisions to stimulate their popularity. HMOs had been declining before the law; by 2002 there were 500 such plans enrolling 76 million people.\n\nThe Canadian system has been 69–75% publicly funded, though most services are delivered by private providers, including physicians (although they may derive their revenue primarily from government billings). Although some doctors work on a purely fee-for-service basis (usually family physicians), some family physicians and most specialists are paid through a combination of fee-for-service and fixed contracts with hospitals or health service management organizations.\n\nCanada's universal health plans do not cover certain services. Non-cosmetic dental care is covered for children up to age 14 in some provinces. Outpatient prescription drugs are not required to be covered, but some provinces have drug cost programs that cover most drug costs for certain populations. In every province, seniors receiving the Guaranteed Income Supplement have significant additional coverage; some provinces expand forms of drug coverage to all seniors, low-income families, those on social assistance, or those with certain medical conditions. Some provinces cover all drug prescriptions over a certain portion of a family's income. Drug prices are also regulated, so brand-name prescription drugs are often significantly cheaper than in the U.S. Optometry is only covered in some provinces and is sometimes only covered for children under a certain age. Visits to non-physician specialists may require an additional fee. Also, some procedures are only covered under certain circumstances. For example, circumcision is not covered, and a fee is usually charged when a parent requests the procedure; however, if an infection or medical necessity arises, the procedure would be covered.\n\nAccording to Dr. Albert Schumacher, former president of the Canadian Medical Association, an estimated 75 percent of Canadian healthcare services are delivered privately, but funded publicly.\n\nFrontline practitioners whether they're GPs or specialists by and large are not salaried. They're small hardware stores. Same thing with labs and radiology clinics ... The situation we are seeing now are more services around not being funded publicly but people having to pay for them, or their insurance companies. We have sort of a passive privatization.\n\nIn both Canada and the United States, access can be a problem. Studies suggest that 40% of U.S. citizens do not have adequate health insurance, if any at all. In Canada, 5% of Canadian citizens have not been able to find a regular doctor, with a further 9% having never looked for one. Yet, even if some cannot find a family doctor, every Canadian citizen is covered by the national health care system. The U.S. data is evidenced in a 2007 Consumer Reports study on the U.S. health care system which showed that the underinsured account for 24% of the U.S. population and live with skeletal health insurance that barely covers their medical needs and leaves them unprepared to pay for major medical expenses. When added to the population of uninsured (approximately 16% of the U.S. population), a total of 40% of Americans ages 18–64 have inadequate access to healthcare, according to the Consumer Reports study. The Canadian data comes from the 2003 Canadian Community Health Survey,\n\nIn the U.S., the federal government does not guarantee universal healthcare to all its citizens, but publicly funded healthcare programs help to provide for the elderly, disabled, the poor, and children. The Emergency Medical Treatment and Active Labor Act or EMTALA also ensures public access to emergency services. The EMTALA law forces emergency healthcare providers to stabilize an emergency health crisis and cannot withhold treatment for lack of evidence of insurance coverage or other evidence of the ability to pay. EMTALA does not absolve the person receiving emergency care of the obligation to meet the cost of emergency healthcare not paid for at the time and it is still within the right of the hospital to pursue any debtor for the cost of emergency care provided. In Canada, emergency room treatment for legal Canadian residents is not charged to the patient at time of service but is met by the government.\n\nAccording to the United States Census Bureau, 59.3% of U.S. citizens have health insurance related to employment, 27.8% have government-provided health-insurance; nearly 9% purchase health insurance directly (there is some overlap in these figures), and 15.3% (45.7 million) were uninsured in 2007. An estimated 25 percent of the uninsured are eligible for government programs but unenrolled. About a third of the uninsured are in households earning more than $50,000 annually. A 2003 report by the Congressional Budget Office found that many people lack health insurance only temporarily, such as after leaving one employer and before a new job. The number of chronically uninsured (uninsured all year) was estimated at between 21 and 31 million in 1998. Another study, by the Kaiser Commission on Medicaid and the Uninsured, estimated that 59 percent of uninsured adults have been uninsured for at least two years. One indicator of the consequences of Americans' inconsistent health care coverage is a study in \"Health Affairs\" that concluded that half of personal bankruptcies involved medical bills. Although other sources dispute this, it is possible that medical debt is the principal cause of bankruptcy in the United States.\n\nA number of clinics provide free or low-cost non-emergency care to poor, uninsured patients. The National Association of Free Clinics claims that its member clinics provide $3 billion in services to some 3.5 million patients annually.\n\nA peer-reviewed comparison study of healthcare access in the two countries published in 2006 concluded that U.S. residents are one third less likely to have a regular medical doctor, one fourth more likely to have unmet healthcare needs, and are more than twice as likely to forgo needed medicines. The study noted that access problems \"were particularly dire for the US uninsured.\" Those who lack insurance in the U.S. were much less satisfied, less likely to have seen a doctor, and more likely to have been unable to receive desired care than both Canadians and insured Americans.\n\nAnother cross-country study compared access to care based on immigrant status in Canada and the U.S. Findings showed that in both countries, immigrants had worse access to care than non-immigrants. Specifically, immigrants living in Canada were less likely to have timely Pap tests compared with native-born Canadians; in addition, immigrants in the U.S. were less likely to have a regular medical doctor and an annual consultation with a health care provider compared with native-born Americans. In general, immigrants in Canada had better access to care than those in the U.S., but most of the differences were explained by differences in socioeconomic status (income, education) and insurance coverage across the two countries. However, immigrants in the U.S. were more likely to have timely Pap tests than immigrants in Canada.\n\nCato Institute has expressed concerns that the U.S. government has restricted the freedom of Medicare patients to spend their own money on healthcare, and has contrasted these developments with the situation in Canada, where in 2005 the Supreme Court of Canada ruled that the province of Quebec could not prohibit its citizens from purchasing covered services through private health insurance. The institute has urged the Congress to restore the right of American seniors to spend their own money on medical care.\n\nThe Canada Health Act covers the services of psychiatrists, who are medical doctors with additional training in psychiatry but does not cover treatment by a psychologist or psychotherapist unless the practitioner is also a medical doctor. Goods and Services Tax or Harmonized Sales Tax (depending on the province) applies to the services of psychotherapists. Some provincial or territorial programs and some private insurance plans may cover the services of psychologists and psychotherapists, but there is no federal mandate for such services in Canada. In the U.S., the Affordable Care Act includes prevention, early intervention, and treatment of mental and/or substance use disorders as an \"essential health benefit\" (EHB) that must be covered by health plans that are offered through the Health Insurance Marketplace. Under the Affordable Care Act, most health plans must also cover certain preventive services without a copayment, co-insurance, or deductible. In addition, the U.S. Mental Health Parity and Addiction Equity Act (MHPAEA) of 2008 mandates \"parity\" between mental health and/or substance use disorder (MH/SUD) benefits and medical/surgical benefits covered by a health plan. Under that law, if a health care plan offers mental health and/or substance use disorder benefits, it must offer the benefits on par with the other medical/surgical benefits it covers.\n\nOne complaint about both the U.S. and Canadian systems is waiting times, whether for a specialist, major elective surgery, such as hip replacement, or specialized treatments, such as radiation for breast cancer; wait times in each country are affected by various factors. In the United States, access is primarily determined by whether a person has access to funding to pay for treatment and by the availability of services in the area and by the willingness of the provider to deliver service at the price set by the insurer. In Canada, the wait time is set according to the availability of services in the area and by the relative need of the person needing treatment.\n\nAs reported by the Health Council of Canada, a 2010 Commonwealth survey found that 39% of Canadians waited 2 hours or more in the emergency room, versus 31% in the U.S.; 43% waited 4 weeks or more to see a specialist, versus 10% in the U.S. The same survey states that 37% of Canadians say it is difficult to access care after hours (evenings, weekends or holidays) without going to the emergency department over 34% of Americans. Furthermore, 47% of Canadians and 50% of Americans who visited emergency departments over the past two years feel that they could have been treated at their normal place of care if they were able to get an appointment.\n\nA report published by Health Canada in 2008 included statistics on self-reported wait times for diagnostic services. The median wait time for diagnostic services such as MRI and CAT scans is two weeks with 89.5% waiting less than 3 months. The median wait time to see a special physician is a little over four weeks with 86.4% waiting less than 3 months. The median wait time for surgery is a little over four weeks with 82.2% waiting less than 3 months. In the U.S., patients on Medicaid, the low-income government programs, can wait three months or more to see specialists. Because Medicaid payments are low, some have claimed that some doctors do not want to see Medicaid patients. For example, in Benton Harbor, Michigan, specialists agreed to spend one afternoon every week or two at a Medicaid clinic, which meant that Medicaid patients had to make appointments not at the doctor's office, but at the clinic, where appointments had to be booked months in advance. A 2009 study found that on average the wait in the United States to see a medical specialist is 20.5 days.\n\nIn a 2009 survey of physician appointment wait times in the United States, the average wait time for an appointment with an orthopedic surgeon in the country as a whole was 17 days. In Dallas, Texas the wait was 45 days (the longest wait being 365 days). Nationwide across the U.S. the average wait time to see a family doctor was 20 days. The average wait time to see a family practitioner in Los Angeles, California was 59 days and in Boston, Massachusetts it was 63 days.\n\nStudies by the Commonwealth Fund found that 42% of Canadians waited 2 hours or more in the emergency room, vs. 29% in the U.S.; 57% waited 4 weeks or more to see a specialist, vs. 23% in the U.S., but Canadians had more chances of getting medical attention at nights, or on weekends and holidays than their American neighbors without the need to visit an ER (54% compared to 61%). Statistics from the Canadian free market think tank Fraser Institute in 2008 indicate that the average wait time between the time when a general practitioner refers a patient for care and the receipt of treatment was almost four and a half months in 2008, roughly double what it had been 15 years before.\n\nA 2003 survey of hospital administrators conducted in Canada, the U.S., and three other countries found dissatisfaction with both the U.S. and Canadian systems. For example, 21% of Canadian hospital administrators, but less than 1% of American administrators, said that it would take over three weeks to do a biopsy for possible breast cancer on a 50-year-old woman; 50% of Canadian administrators versus none of their American counterparts said that it would take over six months for a 65-year-old to undergo a routine hip replacement surgery. However, U.S. administrators were the most negative about their country's system. Hospital executives in all five countries expressed concerns about staffing shortages and emergency department waiting times and quality.\n\nIn a letter to the \"Wall Street Journal\", Robert Bell, the President and CEO of University Health Network, Toronto, said that Michael Moore's film \"Sicko\" \"exaggerated the performance of the Canadian health system — there is no doubt that too many patients still stay in our emergency departments waiting for admission to scarce hospital beds.\" However, \"Canadians spend about 55% of what Americans spend on health care and have longer life expectancy and lower infant mortality rates. Many Americans have access to quality healthcare. All Canadians have access to similar care at a considerably lower cost.\" There is \"no question\" that the lower cost has come at the cost of \"restriction of supply with sub-optimal access to services,\" said Bell. A new approach is targeting waiting times, which are reported on public websites.\n\nIn 2007 Shona Holmes, a Waterdown, Ontario woman who had a Rathke's cleft cyst removed at the Mayo Clinic in Arizona, sued the Ontario government for failing to reimburse her $95,000 in medical expenses.\nHolmes had characterized her condition as an emergency, said she was losing her sight and portrayed her condition as a life-threatening brain cancer.\nIn July 2009 Holmes agreed to appear in television ads broadcast in the United States warning Americans of the dangers of adopting a Canadian-style health care system.\nThe ads she appeared in triggered debates on both sides of the border.\nAfter her ad appeared critics pointed out discrepancies in her story, including that Rathke's cleft cyst, the condition she was treated for, was not a form of cancer, and was not life-threatening.\n\nHealthcare is one of the most expensive items of both nations' budgets. In the United States, the various levels of government spend more per capita than levels of government do in Canada. In 2004, Canada government-spending was $2,120 (in US dollars) per person, while the United States government-spending $2,724.\n\nA 1999 report found that after exclusions, administration accounted for 31.0% of healthcare expenditures in the United States, as compared with 16.7% in Canada. In looking at the insurance element, in Canada, the provincial single-payer insurance system operated with overheads of 1.3%, comparing favourably with private insurance overheads (13.2%), U.S. private insurance overheads (11.7%) and U.S. Medicare and Medicaid program overheads (3.6% and 6.8% respectively). The report concluded by observing that gap between U.S. and Canadian spending on administration had grown to $752 per capita and that a large sum might be saved in the United States if the U.S. implemented a Canadian-style system.\n\nHowever, U.S. government spending covers less than half of all healthcare costs. Private spending is also far greater in the U.S. than in Canada. In Canada, an average of $917 was spent annually by individuals or private insurance companies for health care, including dental, eye care, and drugs. In the U.S., this sum is $3,372. In 2006, healthcare consumed 15.3% of U.S. annual GDP. In Canada, only 10% of GDP was spent on healthcare. This difference is a relatively recent development. In 1971 the nations were much closer, with Canada spending 7.1% of GDP while the U.S. spent 7.6%.\n\nSome who advocate against greater government involvement in healthcare have asserted that the difference in costs between the two nations is partially explained by the differences in their demographics. Illegal immigrants, more prevalent in the U.S. than in Canada, also add a burden to the system, as many of them do not carry health insurance and rely on emergency rooms — which are legally required to treat them under EMTALA — as a principal source of care. In Colorado, for example, an estimated 80% of undocumented immigrants do not have health insurance.\n\nThe mixed system in the United States has become more similar to the Canadian system. In recent decades, managed care has become prevalent in the United States, with some 90% of privately insured Americans belonging to plans with some form of managed care. In \"managed care\", insurance companies control patients' health care to reduce costs, for instance by demanding a second opinion prior to some expensive treatments or by denying coverage for treatments not considered worth their cost.\n\nAdministrative costs are also higher in the United States than in Canada.\n\nThrough all entities in its public–private system, the US spends more per capita than any other nation in the world, but is the only wealthy industrialized country in the world that lacks some form of universal healthcare. In March 2010, the US Congress passed regulatory reform of the American \"health insurance\" system. However, since this legislation is not fundamental \"healthcare\" reform, it is unclear what its effect will be and as the new legislation is implemented in stages, with the last provision in effect in 2018, it will be some years before any empirical evaluation of the full effects on the comparison could be determined.\n\nHealthcare costs in both countries are rising faster than inflation. As both countries consider changes to their systems, there is debate over whether resources should be added to the public or private sector. Although Canadians and Americans have each looked to the other for ways to improve their respective health care systems, there exists a substantial amount of conflicting information regarding the relative merits of the two systems. In the U.S., Canada's mostly monopsonistic health system is seen by different sides of the ideological spectrum as either a model to be followed or avoided.\n\nSome of the extra money spent in the United States goes to physicians, nurses, and other medical professionals. According to health data collected by the OECD, average income for physicians in the United States in 1996 was nearly twice that for physicians in Canada. In 2012, the gross average salary for doctors in Canada was CDN$328,000. Out of the gross amount, doctors pay for taxes, rent, staff salaries and equipment. When comparing average incomes of doctors in Canada and U.S., it should be kept in mind that malpractice insurance premiums may differ significantly between Canada and the U.S., and the proportion of doctors who are specialists differs. In Canada, less than half of doctors are specialists whereas more than 70% of doctors are specialists in the U.S.\n\nCanada has fewer doctors per capita than the United States. In the U.S, there were 2.4 doctors per 1,000 people in 2005; in Canada, there were 2.2. Some doctors leave Canada to pursue career goals or higher pay in the U.S., though significant numbers of physicians from countries such as China, India, Pakistan and South Africa immigrate to practice in Canada. Many Canadian physicians and new medical graduates also go to the U.S. for post-graduate training in medical residencies. As it is a much larger market, new and cutting-edge sub-specialties are more widely available in the U.S. as opposed to Canada. However, statistics published in 2005 by the Canadian Institute for Health Information (CIHI), show that, for the first time since 1969 (the period for which data are available), more physicians returned to Canada than moved abroad.\n\nBoth Canada and the United States have limited programs to provide prescription drugs to the needy. In the U.S., the introduction of Medicare Part D has extended partial coverage for pharmaceuticals to Medicare beneficiaries. In Canada all drugs given in hospitals fall under Medicare, but other prescriptions do not. The provinces all have some programs to help the poor and seniors have access to drugs, but while there have been calls to create one, no national program exists. About two thirds of Canadians have private prescription drug coverage, mostly through their employers. In both countries, there is a significant population not fully covered by these programs. A 2005 study found that 20% of Canada's and 40% of America's sicker adults did not fill a prescription because of cost.\n\nFurthermore, the 2010 Commonwealth Fund International Health Policy Survey indicates that 4% of Canadians indicated that they did not visit a doctor because of cost compared with 22% of Americans. Additionally, 21% of Americans have said that they did not fill a prescription for medicine or have skipped doses due to cost. That is compared with 10% of Canadians.\n\nOne of the most important differences between the two countries is the much higher cost of drugs in the United States. In the U.S., $728 per capita is spent each year on drugs, while in Canada it is $509. At the same time, consumption is higher in Canada, with about 12 prescriptions being filled per person each year in Canada and 10.6 in the United States. The main difference is that patented drug prices in Canada average between 35% and 45% lower than in the United States, though generic prices are higher. The price differential for brand-name drugs between the two countries has led Americans to purchase upward of $1 billion US in drugs per year from Canadian pharmacies.\n\nThere are several reasons for the disparity. The Canadian system takes advantage of centralized buying by the provincial governments that have more market heft and buy in bulk, lowering prices. By contrast, the U.S. has explicit laws that prohibit Medicare or Medicaid from negotiating drug prices. In addition, price negotiations by Canadian health insurers are based on evaluations of the clinical effectiveness of prescription drugs, allowing the relative prices of therapeutically similar drugs to be considered in context. The Canadian Patented Medicine Prices Review Board also has the authority to set a fair and reasonable price on patented products, either comparing it to similar drugs already on the market, or by taking the average price in seven developed nations. Prices are also lowered through more limited patent protection in Canada. In the U.S., a drug patent may be extended five years to make up for time lost in development. Some generic drugs are thus available on Canadian shelves sooner.\n\nThe pharmaceutical industry is important in both countries, though both are net importers of drugs. Both countries spend about the same amount of their GDP on pharmaceutical research, about 0.1% annually\n\nThe United States spends more on technology than Canada. In a 2004 study on medical imaging in Canada, it was found that Canada had 4.6 MRI scanners per million population while the U.S. had 19.5 per million. Canada's 10.3 CT scanners per million also ranked behind the U.S., which had 29.5 per million. The study did not attempt to assess whether the difference in the number of MRI and CT scanners had any effect on the medical outcomes or were a result of overcapacity but did observe that MRI scanners are used more intensively in Canada than either the U.S. or Great Britain. This disparity in the availability of technology, some believe, results in longer wait times. In 1984 wait times of up to 22 months for an MRI were alleged in Saskatchewan. However, according to more recent official statistics (2007), all emergency patients receive MRIs within 24 hours, those classified as urgent receive them in under 3 weeks and the maximum elective wait time is 19 weeks in Regina and 26 weeks in Saskatoon, the province's two largest metropolitan areas.\n\nAccording to the Health Council of Canada's 2010 report \"Decisions, Decisions: Family doctors as gatekeepers to prescription drugs and diagnostic imaging in Canada\", the Canadian federal government invested $3 billion over 5 years (2000–2005) in relation to diagnostic imaging and agreed to invest a further $2 billion to reduce wait times. These investments led to an increase in the number of scanners across Canada as well as the number of exams being performed. The number of CT scanners increased from 198 to 465 and MRI scanners increased from 19 to 266 (more than tenfold) between 1990 and 2009. Similarly, the number of CT exams increased by 58% and MRI exams increased by 100% between 2003 and 2009. In comparison to other OECD countries, including the US, Canada's rates of MRI and CT exams falls somewhere in the middle. Nevertheless, the Canadian Association of Radiologists claims that as many as 30% of diagnostic imaging scans are inappropriate and contribute no useful information.\n\nThe extra cost of malpractice lawsuits is a proportion of health spending in both the U.S. (1.7% in 2002) and Canada (0.27% in 2001 or $237 million). In Canada the total cost of settlements, legal fees, and insurance comes to $4 per person each year, but in the United States it is over $16. Average payouts to American plaintiffs were $265,103, while payouts to Canadian plaintiffs were somewhat higher, averaging $309,417. However, malpractice suits are far more common in the U.S., with 350% more suits filed each year per person. While malpractice costs are significantly higher in the U.S., they make up only a small proportion of total medical spending. The total cost of defending and settling malpractice lawsuits in the U.S. in 2004 was over $28 billion. Critics say that defensive medicine consumes up to 9% of American healthcare expenses., but CBO studies suggest that it is much smaller.\n\nThere are a number of ancillary costs that are higher in the U.S. Administrative costs are significantly higher in the U.S.; government mandates on record keeping and the diversity of insurers, plans and administrative layers involved in every transaction result in greater administrative effort. One recent study comparing administrative costs in the two countries found that these costs in the U.S. are roughly double what they are in Canada. Another ancillary cost is marketing, both by insurance companies and health care providers. These costs are higher in the U.S., contributing to higher overall costs in that nation.\n\nIn the World Health Organization's rankings of healthcare system performance among 191 member nations published in 2000, Canada ranked 30th and the U.S. 37th, while the overall health of Canadians was ranked 35th and Americans 72nd. However, the WHO's methodologies, which attempted to measure how efficiently health systems translate expenditure into health, generated broad debate and criticism.\n\nResearchers caution against inferring healthcare quality from some health statistics. June O'Neill and Dave O'Neill point out that \"... life expectancy and infant mortality are both poor measures of the efficacy of a health care system because they are influenced by many factors that are unrelated to the quality and accessibility of medical care\".\n\nIn 2007, Gordon H. Guyatt et al. conducted a meta-analysis, or systematic review, of all studies that compared health outcomes for similar conditions in Canada and the U.S., in \"Open Medicine\", an open-access peer-reviewed Canadian medical journal. They concluded, \"Available studies suggest that health outcomes may be superior in patients cared for in Canada versus the United States, but differences are not consistent.\" Guyatt identified 38 studies addressing conditions including cancer, coronary artery disease, chronic medical illnesses and surgical procedures. Of 10 studies with the strongest statistical validity, 5 favoured Canada, 2 favoured the United States, and 3 were equivalent or mixed. Of 28 weaker studies, 9 favoured Canada, 3 favoured the United States, and 16 were equivalent or mixed. Overall, results for mortality favoured Canada with a 5% advantage, but the results were weak and varied. The only consistent pattern was that Canadian patients fared better in kidney failure.\n\nIn terms of population health, life expectancy in 2006 was about two and a half years longer in Canada, with Canadians living to an average of 79.9 years and Americans 77.5 years. Infant and child mortality rates are also higher in the U.S. Some comparisons suggest that the American system underperforms Canada's system as well as those of other industrialized nations with universal coverage. For example, a ranking by the World Health Organization of health care system performance among 191 member nations, published in 2000, ranked Canada 30th and the U.S. 37th, and the overall health of Canada 35th to the American 72nd. The WHO did not merely consider health care outcomes, but also placed heavy emphasis on the health disparities between rich and poor, funding for the health care needs of the poor, and the extent to which a country was reaching the potential health care outcomes they believed were possible for that nation. In an international comparison of 21 more specific quality indicators conducted by the Commonwealth Fund International Working Group on Quality Indicators, the results were more divided. One of the indicators was a tie, and in 3 others, data was unavailable from one country or the other. Canada performed better on 11 indicators; such as survival rates for colorectal cancer, childhood leukemia, and kidney and liver transplants. The U.S. performed better on 6 indicators, including survival rates for breast and cervical cancer, and avoidance of childhood diseases such as pertussis and measles. It should be noted that the 21 indicators were distilled from a starting list of 1000. The authors state that, \"It is an opportunistic list, rather than a comprehensive list.\"\n\nSome of the difference in outcomes may also be related to lifestyle choices. The OECD found that Americans have slightly higher rates of smoking and alcohol consumption than do Canadians as well as significantly higher rates of obesity. A joint US-Canadian study found slightly higher smoking rates among Canadians. Another study found that Americans have higher rates not only of obesity, but also of other health risk factors and chronic conditions, including physical inactivity, diabetes, hypertension, arthritis, and chronic obstructive pulmonary disease.\n\nWhile a Canadian systematic review stated that the differences in the systems of Canada and the United States could not alone explain differences in healthcare outcomes, the study didn't consider that over 44,000 Americans die every year due to not having a single payer system for healthcare in the United States and it didn't consider the millions more that live without proper medical care due to a lack of insurance.\n\nThe United States and Canada have different racial makeups, different obesity rates and different alcoholism rates, which would likely cause the US to have a shorter average life expectancy and higher infant mortality even with equal healthcare provided. The US population is 12.2% African Americans and 16.3% Hispanic Americans (2010 Census), whereas Canada has only 2.5% African Canadians and 0.97% Hispanic Canadians (2006 Census). African Americans have higher mortality rates than any other racial or ethnic group for eight of the top ten causes of death. The cancer incidence rate among African Americans is 10% higher than among European Americans. U.S. Latinos have higher rates of death from diabetes, liver disease, and infectious diseases than do non-Latinos. Adult African Americans and Latinos have approximately twice the risk as European Americans of developing diabetes. The infant mortality rates for African Americans is twice that of whites. Unfortunately, directly comparing infant mortality rates between countries is difficult, as countries have different definitions of what qualifies as an infant death.\n\nAnother issue with comparing the two systems is the baseline health of the patient's for which the systems must treat. Canada has only half the obesity rate that the US system must deal with (14.3% vs 30.6%). On average, obesity reduces life expectancy by 6–7 years.\n\nA 2004 study found that Canada had a slightly higher mortality rate for acute myocardial infarction (heart attack) because of the more conservative Canadian approach to revascularizing (opening) coronary arteries.\n\nNumerous studies have attempted to compare the rates of cancer incidence and mortality in Canada and the U.S., with varying results. Doctors who study cancer epidemiology warn that the diagnosis of cancer is subjective, and the \"reported\" incidence of a cancer will rise if screening is more aggressive, even if the \"real\" cancer incidence is the same. Statistics from different sources may not be compatible if they were collected in different ways. The proper interpretation of cancer statistics has been an important issue for many years. Dr. Barry Kramer of the National Institutes of Health points to the fact that cancer incidence rose sharply over the past few decades as screening became more common. He attributes the rise to increased detection of benign early stage cancers that pose little risk of metastasizing. Furthermore, though patients who were treated for these benign cancers were at little risk, they often have trouble finding health insurance after the fact.\n\nCancer survival time increases with later years of diagnosis, because cancer treatment improves, so cancer survival statistics can only be compared for cohorts in the same diagnosis year. For example, as doctors in British Columbia adopted new treatments, survival time for patients with metastatic breast cancer increased from 438 days for those diagnosed in 1991–1992, to 667 days for those diagnosed in 1999–2001.\n\nAn assessment by Health Canada found that cancer mortality rates are almost identical in the two countries. Another international comparison by the National Cancer Institute of Canada indicated that incidence rates for most, but not all, cancers were higher in the U.S. than in Canada during the period studied (1993–1997). Incidence rates for certain types, such as colorectal and stomach cancer, were actually higher in Canada than in the U.S. In 2004, researchers published a study comparing health outcomes in the Anglo countries. Their analysis indicates that Canada has greater survival rates for both colorectal cancer and childhood leukemia, while the United States has greater survival rates for Non-Hodgkin's lymphoma as well as breast and cervical cancer.\n\nA study based on data from 1978 through 1986 found very similar survival rates in both the United States and in Canada. However, a study based on data from 1993 through 1997 found lower cancer survival rates among Canadians than among Americans.\n\nA few comparative studies have found that cancer survival rates vary more widely among different populations in the U.S. than they do in Canada. Mackillop and colleagues compared cancer survival rates in Ontario and the U.S. They found that cancer survival was more strongly correlated with socio-economic class in the U.S. than in Ontario. Furthermore, they found that the American survival advantage in the four highest quintiles was statistically significant. They strongly suspected that the difference due to prostate cancer was a result of greater detection of asymptomatic cases in the U.S. Their data indicates that neglecting the prostate cancer data reduces the American advantage in the four highest quintiles and gives Canada a statistically significant advantage in the lowest quintile. Similarly, they believe differences in screening mammography may explain part of the American advantage in breast cancer. Exclusion of breast and prostate cancer data results in very similar survival rates for both countries.\n\nHsing et al. found that prostate cancer mortality incidence rate ratios were lower among U.S. whites than among any of the nationalities included in their study, including Canadians. U.S. African Americans in the study had lower rates than any group except for Canadians and U.S. whites. Echoing the concerns of Dr. Kramer and Professor Mackillop, Hsing later wrote that reported prostate cancer incidence depends on screening. Among whites in the U.S., the death rate for prostate cancer remained constant, even though the incidence increased, so the additional reported prostate cancers did not represent an increase in real prostate cancers, said Hsing. Similarly, the death rates from prostate cancer in the U.S. increased during the 1980s and peaked in early 1990. This is at least partially due to \"attribution bias\" on death certificates, where doctors are more likely to ascribe a death to prostate cancer than to other diseases that affected the patient, because of greater awareness of prostate cancer or other reasons.\n\nBecause health status is \"considerably affected\" by socioeconomic and demographic characteristics, such as level of education and income, \"the value of comparisons in isolating the impact of the healthcare system on outcomes is limited,\" according to health care analysts. Experts say that the incidence and mortality rates of cancer cannot be combined to calculate survival from cancer. Nevertheless, researchers have used the ratio of mortality to incidence rates as one measure of the effectiveness of healthcare. Data for both studies was collected from registries that are members of the North American Association of Central Cancer Registries, an organization dedicated to developing and promoting uniform data standards for cancer registration in North America.\n\nThe U.S. and Canada differ substantially in their demographics, and these differences may contribute to differences in health outcomes between the two nations. Although both countries have white majorities, Canada has a proportionately larger immigrant minority population. Furthermore, the relative size of different ethnic and racial groups vary widely in each country. Hispanics and peoples of African descent constitute a much larger proportion of the U.S. population. Non-Hispanic North American aboriginal peoples constitute a much larger proportion of the Canadian population. Canada also has a proportionally larger South Asian and East Asian population. Also, the proportion of each population that is immigrant is higher in Canada.\n\nA study comparing aboriginal mortality rates in Canada, the U.S. and New Zealand found that aboriginals in all three countries had greater mortality rates and shorter life expectancies than the white majorities. That study also found that aboriginals in Canada had both shorter life expectancies and greater infant mortality rates than aboriginals in the United States and New Zealand. The health outcome differences between aboriginals and whites in Canada was also larger than in the United States.\n\nThough few studies have been published concerning the health of Black Canadians, health disparities between whites and African Americans in the U.S. have received intense scrutiny. African Americans in the U.S. have significantly greater rates of cancer incidence and mortality. Drs. Singh and Yu found that neonatal and postnatal mortality rates for American African Americans are more than double the non-Hispanic white rate. This difference persisted even after controlling for household income and was greatest in the highest income quintile. A Canadian study also found differences in neonatal mortality between different racial and ethnic groups. Although Canadians of African descent had a greater mortality rate than whites in that study, the rate was somewhat less than double the white rate.\n\nThe racially heterogeneous Hispanic population in the U.S. has also been the subject of several studies. Although members of this group are significantly more likely to live in poverty than are non-Hispanic whites, they often have disease rates that are comparable to or better than the non-Hispanic white majority. Hispanics have lower cancer incidence and mortality, lower infant mortality, and lower rates of neural tube defects. Singh and Yu found that infant mortality among Hispanic sub-groups varied with the racial composition of that group. The mostly white Cuban population had a neonatal mortality rate (NMR) nearly identical to that found in non-Hispanic whites and a postnatal mortality rate (PMR) that was somewhat lower. The largely Mestizo, Mexican, Central, and South American Hispanic populations had somewhat lower NMR and PMR. The Puerto Ricans who have a mix of white and African ancestry had higher NMR and PMR rates.\n\nIn 2002, automotive companies claimed that the universal system in Canada saved labour costs. In 2004, healthcare cost General Motors $5.8 billion, and increased to $7 billion. The UAW also claimed that the resulting escalating healthcare premiums reduced workers' bargaining powers.\n\nIn Canada, increasing demands for healthcare, due to the aging population, must be met by either increasing taxes or reducing other government programs. In the United States, under the current system, more of the burden will be taken up by the private sector and individuals.\n\nSince 1998, Canada's successive multibillion-dollar budget surpluses have allowed a significant injection of new funding to the healthcare system, with the stated goal of reducing waiting times for treatment. However, this may be hampered by the return to deficit spending as of the 2009 Canadian federal budget.\n\nOne historical problem with the U.S. system was known as job lock, in which people become tied to their jobs for fear of losing their health insurance. This reduces the flexibility of the labor market. Federal legislation passed since the mid-1980s, particularly COBRA and HIPAA, has been aimed at reducing job lock. However, providers of group health insurance in many states are permitted to use experience rating and it remains legal in the United States for prospective employers to investigate a job candidate's health and past health claims as part of a hiring decision. Someone who has recently been diagnosed with cancer, for example, may face job lock not out of fear of losing their health insurance, but based on prospective employers not wanting to add the cost of treating that illness to their own health insurance pool, for fear of future insurance rate increases. Thus, being diagnosed with an illness can cause someone to be forced to stay in their current job.\n\nMore imaginative solutions in both countries have come from the sub-national level.\n\nIn Canada, the right-wing and now defunct Reform Party and its successor, the Conservative Party of Canada considered increasing the role of the private sector in the Canadian system. Public backlash caused these plans to be abandoned, and the Conservative government that followed re-affirmed its commitment to universal public medicine.\n\nIn Canada, it was Alberta under the Conservative government that had experimented most with increasing the role of the private sector in healthcare. Measures included the introduction of private clinics allowed to bill patients for some of the cost of a procedure, as well as 'boutique' clinics offering tailored personal care for a fixed preliminary annual fee.\n\nIn the U.S., President Bill Clinton attempted a significant restructuring of health care, but the effort collapsed under political pressure against it despite tremendous public support. The 2000 U.S. election saw prescription drugs become a central issue, although the system did not fundamentally change. In the 2004 U.S. election healthcare proved to be an important issue to some voters, though not a primary one.\n\nIn 2006, Massachusetts adopted a plan that vastly reduced the number of uninsured making it the state with the lowest percentage of non-insured residents in the union. It requires everyone to buy insurance and subsidizes insurance costs for lower income people on a sliding scale. Some have claimed that the state's program is unaffordable, which the state itself says is \"a commonly repeated myth\". In 2009, in a minor amendment, the plan did eliminate dental, hospice and skilled nursing care for certain categories of noncitizens covering 30,000 people (victims of human trafficking and domestic violence, applicants for asylum and refugees) who do pay taxes.\n\nIn July 2009, Connecticut passed into law a plan called SustiNet, with the goal of achieving health care coverage of 98% of its residents by 2014.\n\nUS President Donald Trump has declared his intent to repeal the Affordable Care Act, but has failed to do so, thus far.\n\nThe Canada Health Act of 1984 \"does not directly bar private delivery or private insurance for publicly insured services,\" but provides financial disincentives for doing so. \"Although there are laws prohibiting or curtailing private health care in some provinces, they can be changed,\" according to a report in the New England Journal of Medicine. Governments attempt to control health care costs by being the sole purchasers and thus they do not allow private patients to bid up prices. Those with non-emergency illnesses such as cancer cannot pay out of pocket for time-sensitive surgeries and must wait their turn on waiting lists. According to the Canadian Supreme Court in its 2005 ruling in \"Chaoulli v. Quebec\", waiting list delays \"increase the patient's risk of mortality or the risk that his or her injuries will become irreparable.\" The ruling found that a Quebec provincial ban on private health insurance was unlawful, because it was contrary to Quebec's own legislative act, the 1975 Charter of Human Rights and Freedoms.\n\nIn the United States, Congress has enacted laws to promote consumer-driven healthcare with health savings accounts (HSAs), which were created by the Medicare bill signed by President George W. Bush on December 8, 2003. HSAs are designed to provide tax incentives for individuals to save for future qualified medical and retiree health expenses. Money placed in such accounts is tax-free. To qualify for HSAs, individuals must carry a high-deductible health plan (HDHP). The higher deductible shifts some of the financial responsibility for health care from insurance providers to the consumer. This shift towards a market-based system with greater individual responsibility increased the differences between the US and Canadian systems.\n\nSome economists who have studied proposals for universal healthcare worry that the consumer driven healthcare movement will reduce the social redistributive effects of insurance that pools high-risk and low-risk people together. This concern was one of the driving factors behind a provision of the Patient Protection and Affordable Care Act, informally known as \"Obamacare\", which limited the types of purchases which could be made with HSA funds. For example, as of January 1, 2011, these funds can no longer be used to buy over-the-counter drugs without a medical prescription.\n\n\n"}
{"id": "1864519", "url": "https://en.wikipedia.org/wiki?curid=1864519", "title": "Condom fatigue", "text": "Condom fatigue\n\nCondom fatigue is a term used by medical professionals and safer sex educators to refer to the phenomenon of decreased condom use. It can also be used to describe a general weariness of and decreased effectiveness of safer sex messages. This is sometimes called \"prevention fatigue\".\n\nThe term has particularly been used to describe men who have sex with men, though the term applies to people of all genders and sexual orientations. Condom fatigue has been partially blamed for an increase in HIV infection rates, though this has not been substantiated in any study.\n\nCondom fatigue is not a universal phenomenon. In Germany, condom use between new sexual partners has increased between 1994 and 2010 from 65% to 87%.\n\nHIV infection is increasing at a rate of 12% annually among 13–24-year-old American men who have sex with men. Experts attribute this to \"AIDS fatigue\" among younger people who have no memory of the worst phase of the epidemic in the 1980s and early 1990s, as well as \"condom fatigue\" among those who have grown tired of and disillusioned with the unrelenting safer sex message. The increase may also be because of new treatments.\n\n"}
{"id": "8313008", "url": "https://en.wikipedia.org/wiki?curid=8313008", "title": "Condoms, needles, and negotiation", "text": "Condoms, needles, and negotiation\n\nCondoms, needles, and negotiation, also known as the CNN approach, is a harm reduction approach to reducing the rate of transmission of sexually transmitted infections such as HIV/AIDS by:\n\nIn contrast with the abstinence, be faithful, use a condom, or \"ABC\" approach to this problem, the \"CNN\" approach aims primarily at reducing the rate of transmission among high-risk groups such as women in areas where women have low levels of social power, prostitutes and their clients, and intravenous drug users.\n\nPope Benedict XVI has strongly criticized reduction policies with regards to HIV/AIDS, saying that \"it is a tragedy that cannot be overcome by money alone, that cannot be overcome through the distribution of condoms, which even aggravates the problems\". This position has been widely criticised for misrepresenting and oversimplifying the role of condoms in preventing infections. Other experts, including the Director of Harvard University's AIDS Prevention Research Project, have supported the Pope's position.\n\n\n"}
{"id": "54569736", "url": "https://en.wikipedia.org/wiki?curid=54569736", "title": "Debug Project", "text": "Debug Project\n\nDebug Project is an ongoing project by Alphabet Inc. subsidiary Verily in California to reduce the numbers of mosquitos in a given area through interruption of the reproductive cycle. Through laboratory methods naturally-occurring bacteria Wolbachia infect healthy male mosquitos of the species Aedes aegypti. Subsequently these mosquitos are released into the wild with the intent to mate with female mosquitos and by virtue of the Wolbachia infection lay non-producing eggs. It is believed that through this process the overall population of mosquitos will be reduced by interrupting the reproductive cycle. The hope is that if successful, these might be released in more endemic areas of the world where mosquitos pose a health risk through the diseases they carry.\n"}
{"id": "15558462", "url": "https://en.wikipedia.org/wiki?curid=15558462", "title": "Dietary Guidelines for Americans", "text": "Dietary Guidelines for Americans\n\nThe Dietary Guidelines for Americans (DGA) provide nutritional advice for Americans who are more than 2 years old. The Guidelines are published every 5 years by the US Department of Agriculture, together with the US Department of Health and Human Services. The most recent edition is the 2015-2020 Dietary Guidelines for Americans. The nominal purpose of the Dietary Guidelines for Americans is to help health professionals and policymakers to advise Americans about healthy choices for their diet. Although the Dietary Guidelines for Americans are purported to be based on a systematic review of the current body of nutrition science, the Advisory Committee tasked with formulating the plan for retrieval and analysis of the scientific evidence for the current edition of the DGA used a less than rigorous process for assessing the health effects of consumption of saturated fat and salt and for assessing the health effects of a low-fat, high-carbohydrate diet. This less than rigorous review of the nutrition science literature resulted in omission of multiple large, high-quality, clinical trials and also omission of some high-quality prospective observational studies. Some Advisory Committee members also had conflicts-of-interest that in some cases were not fully disclosed. For these reasons, the quality of the Advisory Committee's Scientific Report and the validity of the 2015 - 2020 DGA itself has been challenged by critics as being unduly influenced by commercial interests and as being flawed due to confirmation bias of some members of the Advisory Committee.\n\nThe efforts of the US Federal Government to establish a scientific basis for human nutrition began with Wilbur Olin Atwater, who published the first dietary recommendations for Americans in 1894, notably stating that, \"We live not upon what we eat, but upon what we digest.\"\n\nThe Dietary Guidelines for Americans has been published every 5 years beginning in 1980, producing eight guidelines to date. One consistent recommendation of these eight guidelines has been that Americans reduce their dietary consumption of fat and animal products, including meat, dairy, and eggs and to increase their dietary consumption of carbohydrates and plant foods, including fruits, vegetables, and grains.\n\nThe Guidelines were established so as to provide dietary advice that would improve the health of Americans and reduce their risk for chronic conditions, such as cancer, atherosclerosis, hypertension, heart disease, stroke, and renal disease. The Dietary Guidelines have the purpose of guiding the development of Federal policies and programs related to food, nutrition, and health. The guidelines influence and guide policymakers for Federally-financed food and dietary education programs. They also influence clinicians in the United States and in other countries.\n\nThe intended audience for the Dietary Guidelines for Americans are policymakers, nutrition scientists, and dieticians and other health professionals. The Guidelines themselves are not intended to directly inform the general public, but instead to serve as an authoritative, evidence-based information source that policymakers and health professionals can use to advise Americans about making healthy choices in their daily lives so as to enjoy a healthy diet that also prevents chronic disease. The Dietary Guidelines for Americans provide an evidence-base that is used by the Federal government to develop nutrition education materials for Americans.\n\nFederal law and regulation require that Federal government publications provide dietary guidance consistent with the Dietary Guidelines for Americans. For the United States Department of Agriculture (USDA) the guidelines provide the scientific rationale for the National School Lunch Program and School Breakfast Program, feeding 30 million children every school day, and the Special Supplemental Nutrition Program for Women, Infants and Children, which has 8 million beneficiaries. For the United States Department of Health and Human Services Administration on Aging, the guidelines provide the rationale for the Older Americans Act Nutrition Services programs which include more than 5,000 community-based nutrition service providers (e.g., Meals on Wheels), serving more than 900,000 meals a day across the United States. The Department of Defense uses the guidelines as the rationale for meal rations for military personnel and the Department of Veterans Affairs uses the guidelines to inform nutrition education for veterans who are patients of the VA Hospital System. In addition to these governmental audiences, the Dietary Guidelines for Americans are widely used by state and local governments, schools, commercial enterprises, community groups, the media, and the food industry to inform policy and program development intended to serve the general public.\n\nThe current Dietary Guidelines for Americans (2015-2020) were developed in three stages, beginning with a review of scientific evidence, followed by development of the guidelines, and finally with implementation of the guidelines. Compared to previous guidelines, the 2015–2020 guidelines put emphasize replacing saturated fats with unsaturated fats, particularly polyunsaturated fats, with the goal of preventing heart attack and stroke.\n\nThe guidelines provide a general recommendation that people follow a healthy eating pattern with appropriate calories, and that the evaluation of ones eating pattern accounts for all foods and beverages, including snacks. The recommended healthy eating pattern includes:\n\nInclude these in diet:\n\nLimit these in diet:\n\nThe Dietary Guidelines also include a key recommendation to meet the Physical Activity Guidelines for Americans.\n\nThe MyPlate initiative, based on the recommendations of the 2015 -2020 Dietary Guidelines for Americans and produced by the USDA Center for Nutrition Policy and Promotion, is a nutrition education program directed at the general public, providing a guide to \"finding healthy eating solutions to fit your lifestyle.\"\n\nThe USDA has invited interested parties, including members of the general public, to participate and follow the development of the 2020 - 2025 edition of the Dietary Guidelines for Americans. As mandated by the Agriculture Act of 2014, this next edition of the guidelines will cover the full life-span of Americans, as expansion of the guidelines are planned to include recommendations for pregnant women, infants, and children younger than 2 years old. The Trump Administration has proposed a budget of more than $12 million for the evaluation of scientific evidence, development of the 2020-2025 Dietary Guidelines for Americans, and dissemination of the new edition to its target audience of policymakers, nutrition experts, and clinicians; this budget request has been supported by multiple organizations.\n\nEach edition of the Dietary Guidelines for Americans has had attendant controversy, with objections particularly from scientists whose point-of-view was not reflected in the guidelines and from commercial interests negatively affected by the recommendations therein. The response to the 2015-2020 guidelines was particularly contentious, resulting in action by Congress mandating the National Academies of Sciences, Engineering, and Medicine to evaluate the process used to update the DGA. This review by the National Academies resulted in two reports. The first report, entitled \"Optimizing the Process for Establishing the Dietary Guidelines for Americans: The Selection Process\", identified opportunities for improving the process for selecting members of the Dietary Guidelines Advisory Committee. In September, 2018 the USDA issued an official response to the first report of the National Academies committee. The second report from National Academies of Sciences, entitled \"Redesigning the Process for Establishing the Dietary Guidelines for Americans\", offers an exhaustive review and provides recommendations for improving the process of revising the Dietary Guidelines so as to best identify, analyze, and present the scientific evidence.\n\nOne critic contends that the current guidelines do not promote the public interest and have been corrupted by regulatory capture, advancing the commercial concerns of agribusiness and large food processors, and the political concerns of scientists intent on preserving their point-of-view. Another critic notes that the DGAs make recommendations that overvalue the findings of observational studies and surrogate measures of outcomes and that undervalue the findings of high-quality randomized controlled trials.\n\nThere is compelling evidence that food frequency questionnaires and other methods that rely on human memory do not accurately measure dietary intake. An analysis of the validity of the methods used by the USDA to estimate per capita calorie consumption found that these methods lack validity and the authors of this study recommend that these methods not be used to inform public policy. A systematic review found that only a few studies have measured the accuracy or reliability of dietary assessment methods in schoolchildren. The few studies that have been done found that schoolchildren did not accurately report foods consumed but that they did accurately report total calories consumed. The 2015 Guidelines were based on the Scientific Report of the 2015 Dietary Guidelines Advisory Committee, which did not rely on actual measurements of dietary intake but instead relied on memory-based dietary assessments, including interviews and surveys despite clear evidence that such methods markedly underestimate actual calorie consumption and nutrient intake. Thus, the conclusions expressed in the Scientific Report have been criticized, and the Dietary Guidelines for Americans are considered invalid by some experts, as the DGAs rely on invalid methods and draw conclusions that do not agree with the available scientific literature.\n\nThe Dietary Guidelines for Americans have been criticized for recommending a diet that is low in total fat, and for not adequately emphasizing the harmful effect of industrially-produced trans fats. A systematic review of 62,421 participants in 10 dietary trials found that reducing dietary fat intake had no effect on coronary heart disease and had no effect on overall mortality. The authors of this meta-analysis conclude that the available evidence from randomized controlled trials does not support the recommendation of the 2015 - 2020 Dietary Guidelines for Americans that people reduce their fat intake.\n\nThe Dietary Guidelines for Americans have been criticized for recommending a diet that contains less than 2.3  grams of sodium (5.8 grams of salt/day). Notably, 95% of the world's populations have a mean intake of salt that is between 6 g and 12 g daily and evidence on the health effects of salt does not support such a severe restriction on salt intake. An analysis of dietary guidelines found that this recommendation for restriction of salt intake is not supported by evidence from randomized controlled trials nor is it supported by evidence from prospective observational studies. In fact, intake of less than 5.8 g of salt per day typically results in activation of the renin-angiotensin-aldosterone system, which leads to an increase in plasma lipids and increased mortality. The authors of this analysis suggest a redesign of the dietary guidelines for salt intake is needed.\n\nA Committee of the National Academies Institute of Medicine evaluated the evidence about dietary salt intake and health. Overall, the committee found evidence that higher salt intake was associate with increased risk of cardiovascular disease. However, the Committee also found that the evidence did not support the claim that lowering sodium intake in the general population to less than 2,300 mg/day was associated with a lower risk of death nor with a higher risk of death.\n\nThe Dietary Guidelines for Americans recommend limiting alcoholic beverages consumption to no more than 1 drink daily for women and no more than 2 drinks daily for men. The 2015 - 2020 Scientific Report of the Dietary Guidelines Advisory Committee asserts that most studies show that moderate consumption of alcohol has been shown to be part of a beneficial dietary pattern. However, a systematic review and meta-analysis of scientific studies of alcohol consumption and all-cause mortality found that consumption of up to 2 alcoholic beverages per day had no net mortality benefit compared with lifetime abstention from alcohol. A systematic analysis of data from the Global Burden of Disease study found that consumption of ethanol increases the risk of cancer and increases the risk of all-cause mortality, and that the level of ethanol consumption that minimizes disease is zero consumption.\nThe Guidelines recommend that people not mix alcohol and beverages containing caffeine, as this combined intake may result in greater alcohol consumption, with a greater risk of alcohol-related injury.\n\nProducers of honey and maple syrup have objected to the proposed Federal regulatory requirement that honey and maple syrup include the term \"added sugar\" on product labeling, despite the fact that no additional sugar is added to these products. This regulatory requirement follows from the recommendation in the 2015 - 2020 Guidelines that added sugars be limited to less than 10% of calories and that honey and maple syrup are themselves considered by federal regulators to be added sugars.\n"}
{"id": "900035", "url": "https://en.wikipedia.org/wiki?curid=900035", "title": "Dipsomania", "text": "Dipsomania\n\nDipsomania is a historical term describing a medical condition involving an uncontrollable craving for alcohol. In the 19th century, the term dipsomania was used to refer to a variety of alcohol-related problems, most of which are known today as alcoholism. Dipsomania is occasionally still used to describe a particular condition of periodic, compulsive bouts of alcohol intake. The idea of dipsomania is important for its historical role in promoting a disease theory of chronic drunkenness. The word comes from Greek \"dipso\" (= thirst) and \"mania\".\n\nIt is still mentioned in the WHO ICD-10 classification as an alternative description for Alcohol Dependence Syndrome, episodic use F10.26\n\nThe term was coined by the German physician Christoph Wilhelm Hufeland in 1819, when, in a preface to an influential book by German-Russian doctor C. von Brühl-Cramer, he translated Brühl-Cramer's term \"trunksucht\" as \"dipsomania\".\n\nDue to the influence of Brühl-Cramer's pioneering work, dipsomania became popular in medical circles throughout the 19th century. Political scientist Mariana Valverde describes dipsomania as \"the most medical\" of the many terms used to describe habitual drunkenness in the 19th century. Along with terms such as \"inebriety\", the idea of dipsomania was used as part of an effort of medical professionals and reformers to change attitudes about habitual drunkenness from being a criminally punishable vice to being a medically treatable disease. As historian Roy MacLeod wrote about this dipsomania reform movement, it \"illuminates certain features of the gradual transformation taking place in national attitudes towards the prevention and cure of social illnesses during the last quarter of the 19th century.\"\n\nAlthough \"dipsomania\" was used in a variety of somewhat contradictory ways by different individuals, by the late 19th century the term was usually used to describe a periodic or acute condition, in contrast to chronic drunkenness. In his 1893 book \"Clinical Lessons on Mental Diseases: The Mental State of Dipsomania\", Magnan characterized dipsomania as a crisis lasting from one day to two weeks, and consisting of a rapid and huge ingestion of alcohol or whatever other strong, excitatory liquid was available. Magnan further described dipsomania as solitary alcohol abuse, with loss of all other interests, and these crises recurred at indeterminate intervals, separated by periods when the subject was generally sober.\n\nOver time, the term dipsomania became less common, replaced by newer ideas and terms concerning chronic and acute drunkenness and alcoholism.\n\n"}
{"id": "5273119", "url": "https://en.wikipedia.org/wiki?curid=5273119", "title": "Disability management program", "text": "Disability management program\n\nA disability management program, or DMP, is used by employers to assist employees who are unable to work due to injury or illness. The DMP consists of several components, however not all DMPs have all possible components. Smaller programs may only include the basic components while larger programs generally have more components. The purpose of the DMP is to benefit the employer by returning experienced, trained employees to work quickly. The central distinction required to plan and operated a DMP is between the terms \"impairment\" and \"disability\". Although physicians diagnose and treat impairments, employers determine disability.\n\nDisability management programs are applied in different ways. Their implementation depends on a nation´s social security system and disability policies.\n\nIn Germany, rehabilitation and social participation for people with disabilities, or people at risk of developing a disability, are covered by Book IX of the Social Code since 2001. \nThe focus of the legislation is on social and occupational participation in society and elimination of barriers in order to achieve equal opportunities. \nBook IX of the German Social Code aims at ensuring the (re)integration of people with disabilities into society and the labor market. \nThe German disability management program, called “corporate integration management” (Betriebliches Eingliederungsmanagement), is covered by § 84 (2), Book IX, Social Code since 2004. \nThis paragraph stresses the need for preventive measures in the workplace, as well as occupational rehabilitation, and addresses employers` responsibility in this area:\n\n\n"}
{"id": "8810434", "url": "https://en.wikipedia.org/wiki?curid=8810434", "title": "Drug Abuse Warning Network", "text": "Drug Abuse Warning Network\n\nThe Drug Abuse Warning Network (DAWN) was a public health surveillance system in the United States that monitored drug-related visits to hospital emergency departments and drug-related deaths. DAWN was discontinued in 2011, but its creator, the Substance Abuse and Mental Health Services Administration (SAMHSA), continues to develop other sources of data on drug-related emergency visits.\n\nHospitals participating in DAWN are non-federal, short-stay general hospitals that feature a 24-hour emergency department. Patients are never interviewed. All data are collected through a retrospective review of patient medical records and decedent case files. DAWN collects detailed drug data, including illegal drugs of abuse, prescription and over-the-counter medications, dietary supplements, and non-pharmaceutical inhalants. Because the DAWN cases are defined broadly, DAWN captures many different types of drug-related cases. The whole point of this organization is to find out how many people abuse most drugs. They also seek short-stay hospitals, when a case is drug-related.\n\nIn 1974, DAWN was designed and developed by the scientific staff of the DEA's Office of Science and Technology. It was jointly funded with the National Institute of Drug Abuse (NIDA). DAWN then became a division of the United States Department of Justice before becoming part of NIDA in 1980.\nOn October 1, 1992, DAWN became part the Substance Abuse and Mental Health Services Administration (SAMHSA), an agency of the United States Department of Health and Human Services. SAMHSA has contracted with Westat, a private research corporation, to manage the New DAWN on the agency’s behalf.\n\nInformation collected by DAWN is widely cited by drug policy officials, who have sometimes confused drug-related episodes – emergency department visits induced by drugs – with drug mentions. The Wisconsin Department of Justice claimed, \"In Wisconsin, marijuana overdose visits in emergency rooms equal to heroin or morphine, twice as common as Valium.\" Common Sense for Drug Policy called this as a distortion, noting, \"The federal DAWN report itself notes that reports of marijuana do not mean people are going to the hospital for a marijuana overdose, it only means that people going to the hospital mention marijuana as a drug they use\". This criticism is also not correct. DAWN has recently clarified their use of the term \"drug mention\" in methodology because of this erroneous claim. The data is collected by a systematic and confidential review of patients' medical records. Thus, for example, a patient who broke an arm while high on marijuana would not be included in the data. A report released by DAWN in 2002 claims that marijuana overdose alone resulted in documented deaths in Atlanta and Boston, respectively. However, there is no known record or evidence to support the existence of a case of human fatality by result of marijuana overdose.\n\n"}
{"id": "44013878", "url": "https://en.wikipedia.org/wiki?curid=44013878", "title": "Economic evaluation", "text": "Economic evaluation\n\nEconomic evaluation is the process of systematic identification, measurement and valuation of the inputs and outcomes of two alternative activities, and the subsequent comparative analysis of these. The purpose of economic evaluation is to identify the best course of action, based on the evidence available. It is most commonly employed in the context of health economics and health technology assessment; in the UK, the National Institute for Health and Care Excellence publishes guidelines for the conduct of economic evaluations.\n\nEconomic evaluations can take a number of forms, namely:\n"}
{"id": "25096340", "url": "https://en.wikipedia.org/wiki?curid=25096340", "title": "Effects of sleep deprivation on cognitive performance", "text": "Effects of sleep deprivation on cognitive performance\n\nIt has been estimated that over 20% of adults suffer from some form of sleep deprivation. Insomnia and sleep deprivation are common symptoms of depression and can be an indication of other mental disorders. The consequences of not getting enough sleep could have dire results; not only to the health of the individual, but those around them as sleep deprivation increases the risk of human-error related accidents, especially with vigilance-based tasks involving technology. \n\nThe parietal lobes of the brain are largely involved in attention. Lesions to this region of the brain in humans result in difficulty or inability to attend to events that are contralateral to the lesioned hemisphere. Those with lesions to the posterior parietal lobe have little to no difficulty shifting attention to and from stimuli appearing in the space ipsilateral to the lesioned hemisphere. However, they do display a slowed response in shifting their focus of current attention to events and stimuli appearing contralateral to the lesioned hemisphere.\n\nStudies involving single-unit recordings from the parietal lobes of monkeys have indicated that there are neurons solely involved in integrating visual spatial information with postural information. Without this apparent combining of spatial information, it would be difficult or impossible to locate objects in external space, as information provided solely by the retina is insufficient. The position of the eyes, head and body must also be taken into consideration.\n\nIn addition, studies involving transcranial magnetic stimulation application over the parietal lobes as well as positron emission tomography (PET) analysis of the parietal lobes suggest that this region is involved in conjunction searches, but not in single-feature searches. (See Visual search for supplementary information.)\n\nAuditory attention has been examined following sleep deprivation. Researchers examined the auditory attention of twelve non-sleep-deprived subjects and twelve sleep-deprived subjects at various time intervals. Subjects were involved in an auditory attention task, which required the reproduction of the spatial relationships between four letters, using a graph composed of six squares, immediately following the presentation of an item from a tape recorder. It was found that auditory attention of sleep-deprived individuals is affected as the total amount of sleep-deprivation increases, possibly due to lowered perceptual vigilance.\n\nFunctional magnetic resonance imaging (fMRI) scans of the brains of subjects exposed to thirty-five hours of sleep deprivation indicate that sleep deprivation is related to increases in prefrontal cortex and parietal lobe activation during tasks that combine verbal learning and arithmetic. This is particularly apparent in the right hemisphere. In non sleep-deprived individuals involved in verbal learning and arithmetic tasks the anterior cingulate cortex and the right prefrontal cortex are active. Following sleep deprivation there is increased activation of the left inferior frontal gyrus and the bilateral parietal lobes. This information suggests that divided attention tasks require more attentional resources than normally required by a non sleep-deprived individual.\n\nStudies utilizing event-related potential (ERP) recordings have found that twenty-four hours of sleep deprivation decreases ERP response to signal inputs from endogenous, but not exogenous, sources. Therefore, it is suggested that sleep deprivation affects endogenously driven selective attention to a greater extent than exogenously driven selected attention.\n\nTwenty-four hours of sleep deprivation has been found to affect the functional connectivity between the inferior frontal parietal region (IPS) and the parahippocampal place area (PPA). However, sleep deprivation does not affect the attention modulation index of the PPA. With this information, researchers have concluded that the psychophysiological interaction (PPI) is more involved in selective attention than the IPS and PPA.\n\nResearch has found that together, attention and sleep deprivation modulate the parahippocampal place area (PPA) activation and scene processing. Specifically, sleep deprivation was related to significant decreases in PPA activation while attending to scenes and when ignoring scenes. This is explained by the absence of change in the Attention Modulation Index (AMI). Face recognition is not affected by sleep deprivation.\n\nSleep deprivation has been shown to negatively affect picture classification speed and accuracy, as well as recognition memory. It results in an inability to avoid attending to irrelevant information displayed during attention-related tasks. (Norton) It also decreases activation in the ventral visual area and the frontal parietal control regions.\n\nStudies involving sleep deprived subjects’ performance on choice reaction time tests, in which response inhibition, task shifting skill and task strategy were involved, have been conducted and analyzed. These three cognitive processes are involved and critical in tasks involving supervisory attention, which is defined as behaviour that arises through the selection and implementation of schemas. Following one night of total sleep deprivation, subjects showed no decline in task shifting or response inhibition performance. However, sleep deprivation does affect the ability to use preparatory bias to increase performance speed. It is suggested that the brain’s cognitive resources make an active effort to succeed in a challenging task when subjected to sleep deprivation, and that this deficit becomes apparent in tasks involving preparatory bias.\n\nDeficits in cognitive performance due to continuous sleep restriction are not well understood. However, there have been studies looking into physiological arousal of the sleep-deprived brain. Participants, whose total amount of sleep had been restricted by 33% throughout one week, were subjected to reaction time tests. The results of these tests were analyzed using quantitative EEG analysis. The results indicate that the frontal regions of the brain are first to be affected, whereas the parietal regions remain active until the effects of sleep deprivation become more severe, which occurred towards the end of the week. In addition, EEG and ERP analysis reveals that activation deficits are more apparent in the non-dominant hemisphere than in the dominant hemisphere.\n\nThe effects of sleep deprivation on cognitive performance have been studied through the use of parametric visual attention tasks. Functional magnetic resonance imaging of participants' brains who were involved in ball-tracking tasks of various difficulty levels were obtained. These images were taken during rested wakefulness and again after one night of sleep deprivation. The thalamus is more highly activated when accompanied by sleep deprivation than when the subject is in a state of rested wakefulness. Contrarily, the thalamus is more highly activated during difficult tasks accompanied by rested wakefulness, but not during a state of sleep deprivation. Researchers propose that the thalamic resources, which are normally activated during difficult tasks, are being activated in an attempt to maintain alertness during states of sleep deprivation. In addition, an increase in thalamic activation is related to a decrease in the parietal, prefrontal and cingulate cortex activation, resulting in the overall impairment of attentional networks, which are necessary for visuospatial attention performance.\n\nFunctional Magnetic Resonance Imaging (fMRI) studies indicate that the posterior cingulate (PCC) and medial prefrontal cortex are involved in the anticipatory allocation of spatial attention. When sleep-deprived, PCC activity decreases, impairing selective attention. Subjects were exposed to an attention-shifting task involving spatially informative, misleading and uninformative cues preceding the stimuli. When sleep-deprived, subjects showed increased activation in the left intraparietal sulcus. This region is activated when exposed to stimuli in unexpected locations. These findings suggest that sleep-deprived individuals may be impaired in their ability to anticipate the locations of upcoming events. In addition, inability to avoid attending to irrelevant events may also be induced by sleep-deprivation.\n\nBy contrast, other studies have indicated that the effects of sleep deprivation on cognitive performance, specifically, sustained visual attention, are more global and bilateral in nature, as opposed to more lateralized deficit explanations. In a study utilizing the Choice Visual Perception Task, subjects were exposed to stimuli appearing in various locations in visual space. Results indicate that sleep deprivation results in a general decline in visual attention. It is also suggested that the sleep-deprived brain is able to maintain a certain level of cognitive performance during tasks requiring divided attention by recruiting additional cortical regions that are not normally used for such tasks.\n\nExecutive functioning is \"the ability to plan and coordinate a willful action in the face of alternatives, to monitor and update action as necessary and suppress distracting material by focusing attention on the task at hand\". The prefrontal cortex has been identified as the most important region involved in executive functioning.\n\nResearchers believe that three of the most 'basic' executive functions are: shifting, updating, and inhibition. Shifting back and forth between different tasks is considered a very important mental behavior involved in executive functioning as it involves active disengagement from the present task and engaging in a new task. Updating is believed to be involved in working memory (closely associated with the function of the prefrontal cortex), where the information that is active needs to be updated by replacing old, now irrelevant information with new, relevant information based on the objective. Inhibition involves controlled and deliberate impedance of automatic, predominant responses.\n\nThe anterior cingulate cortex has been implemented in the process of inhibiting a habitual response or detecting possible conflicts in responses; this is shown by the Stroop test. Studies have found that as little as 36 hours of sleep deprivation cause a performance reduction in tasks requiring these executive functions.\nThe processes above illustrate a model of controlled versus automatic behavior that was hypothesized by Shallice et al. (1989), called the supervisory attentional system. This system is believed to come into play when intervention of habitual response is necessary. Damage to the prefrontal cortex will cause a breakdown in this system, resulting in utilization behaviors. These behaviors would include spontaneous sequences of action on irrelevant objects in the surroundings with no clear goal in mind. This theory has helped to extend the knowledge we now have on executive functions.\n\nDecision making involves a range of executive functions that need to be combined and organized in order to respond in the most appropriate manner, i.e. respond with the most advantageous decision. There are many aspects to the process of decision making, including those discussed above. Other processes involved that correlate to executive function will be discussed below.\n\nWhile most important decisions are made over a longer period of time involving more in-depth cognitive analysis, usually we have limited time in which to assimilate a large amount of information into an informed decision. Lack of sleep appears to negatively affect our ability to appreciate and respond to increasing complexity, as was found in performance deficits after 1 night of sleep deprivation on a simulated marketing game.\n\nThe game involved subjects promoting a fictional product while getting feedback on the financial effects of their decisions. They would continuously have to take into account new variables to succeed which would increase the game's complexity.\n\nOther examples of inability to process complex information includes a decrease in ability to assess facial expressions, an increase in resolving to the use of stereotypes and racial biases in evaluations, and an increase in taking the easier solution to solving interpersonal problems.\n\nIntuitively, because sleep deprivation had a negative effect on handling the complexity of the simulated marketing game, it also affected innovative processes as subjects failed to adopt a more innovative (and rewarding) style of play. Innovative thinking involves the construction of new behaviors based on the assimilation of continuously changing or novel information. In a study of military personnel who had undergone two nights of sleep deprivation, results showed marked reductions in the ability to generate ideas about a given topic (categories test); this is known as ideational fluency.\n\nRisk versus reward analysis is an important part of decision making. Attempting to create a representation and response to a risky situation highly involves the orbitofrontal cortex. In a study that involved risk taking analysis of drivers who had been driving for 12 hours straight, it was found that they were more willing to make hazardous maneuvers and were reluctant to adopt any form of a cautious driving style.\n\nSome studies shed further light on this phenomenon. One study used real life decision making scenarios involving choosing cards from 1 of 4 decks of cards. Different cards were considered as a reward while the others were a penalty. The sleep deprived subjects failed to alter their selection methods, continuing to choose cards from decks that were producing a high amount of penalty cards, whereas the control subjects were able to change their choosing strategy by a cost-benefit analysis based on monitoring the outcomes they were getting as they went along.\n\nThe process of planning would be done congruently with decision making in determining the outcome behavior. As has been shown so far, sleep deprivation has many detrimental effects on executive functions and planning is not spared. One study involved cadets who were required to complete simulated military operations under sleep deprived conditions. Results showed a decrease in the subjects ability to 'plan on the fly' and overall outcomes were less than those for well rested cadets.\n\nAnother psychological test used to assess planning and decision making is the Tower of London test. This test has been widely used in the testing of executive functions as well as studies of sleep deprived subjects. In a study examining performance on this test after 45–50 hours of sleep deprivation, it was found that the sleep deprived subjects not only took longer, but required more moves to complete the task than did the controls.\n\nBeing able to show insight into one's performance on different tasks and correct or modify those behaviors if they are incorrect is an important part of executive functioning. The problems that could be associated with being unable to learn from a mistake or adapt to a mistake could impair many behaviors.\n\nA common test used to assess error correction and trouble shooting with regards to the frontal lobe is the Wisconsin Card Sorting Test. This test involves a change in the rules which requires a shift in strategy. In the same study discussed above, detriments were also found on this task in the sleep deprived individuals.\n\nResearch evidence suggests that sleep is involved in the acquisition, maintenance and retrieval of memories as well as memory consolidation. Subsequently, sleep deprivation has been shown to affect both working memory and long-term memory processes.\n\nSleep deprivation increases the number of errors made on working memory tasks. In one study, the working memory task involved illuminating a sequence of 3 or 4 coloured lights, then asking both sleep deprived and non-sleep deprived individuals to memorize and repeat back the sequence. The sleep deprived performed the task much faster than those in the control condition (i.e., not sleep deprived), which initially appeared to be a positive effect. However, there was a significant difference in the number of errors made, with the fatigued group performing much worse.\n\nEvidence from imaging studies also demonstrates the influence of sleep deprivation on working memory. EEG studies have documented lower accuracy and slower reaction times among sleep deprived participants performing working memory tasks. Decreasing alertness and lack of focus triggered deficits in working memory that are accompanied by significant degradation of event-related potentials.\n\nPET scans shows global decrease in glucose metabolism in response to sleep deprivation. As subjects become increasingly impaired on working memory tasks, a more specific decrease of glucose occurs in the thalamus, prefrontal cortex and posterior parietal cortex.\n\nfMRI scans following brief sleep deprivation (24 hours or less) show increases in thalamic activation. Verbal working memory tasks normally cause increases in left temporal lobe activity. However, after 35 hours of deprivation, there are noted decreases in temporal lobe activation and increases in parietal lobe activation.\n\nThe working memory span is also affected by sleep deprivation. When sleep deprived participants in a study were asked to remember a nonsense word and locate it among a number of similar words, the length of time they could hold it in their working memory decreased by 38% compared to rested individuals.\n\nOne way sleep is involved in the creation of long-term memories is through memory consolidation, which is the process by which a new memory is changed into a more permanent form. This is believed to be accomplished by creating connections between the medial temporal lobes and neocortical areas. NREM (non-REM) and REM sleep are both believed to be implicated, with current theories suggesting NREM is most particularly involved in procedural memory and REM with declarative memory.\n\nAnimal studies have partly validated these claims. For instance, one study conducted with rats showed that REM sleep deprivation after learning a new task disrupted their ability to perform the task again later. This was especially true if the task was complex (i.e., involved using unusual information or developing novel adaptive behaviours).\n\nThere is similar evidence for the role of sleep in procedural memory in humans. Participants in one study were trained on a procedural memory skill involving perceptual-motor skills. Those who were NREM sleep deprived performed significantly worse on subsequent trials compared to those who were fully rested. Another study using a visuo-motor procedural memory task documented similar results. Participants who were sleep deprived following the initial training showed no improvement on trials the next day, while those who received sleep showed significant positive changes.\nStudies such as these clearly demonstrate the disruptive influence sleep deprivation has on memory consolidation of procedural and declarative memories.\n\nSleep deprivation also has a documented effect on the ability to acquire new memories for subsequent consolidation. A study done on mice that were sleep deprived before learning a new skill but allowed to rest afterward displayed a similar number of errors on later trials as the mice that were sleep deprived only after the initial learning. In this case, it is hypothesized that rather than preventing the memory from being consolidated, sleep deprivation interfered with the initial acquisition of the memory. Mice with pre-trial sleep deprivation also took significantly longer to learn a task than well-rested mice.\n\nSleep deprivation is also implicated in impaired ability to retrieve stored long-term memories. When an aversive stimulus was included in a trial (i.e., a blowdryer would blast hot air and noise at a mouse), mice that were sleep deprived were less anxious on subsequent trials. This suggests they had not retrieved all of the memory related to the unpleasant experience.\n\nSeveral theories have been put forth to explain the effect sleep deprivation has on memory.\n\nOne early study into neurochemical influences on sleep and memory was conducted with cats and demonstrated that sleep deprivation increased brain protein synthesis. There is evidence that these altered levels of proteins could increase the excitability of the central nervous system, thus increasing the susceptibility of the brain to other neurochemical agents that can cause amnesia. Further research has revealed that the protein kinase A (PKA) signalling pathway is crucial to long-term memory. If PKA or protein synthesis inhibition occurs at certain moments during sleep, memory consolidation can be disrupted. In addition, mice with genetic inhibition of PKA have been shown to have long-term memory deficits. Thus, sleep deprivation may act through the inhibition of these protein synthesis pathways.\n\nAcetylcholine (ACh) may also be involved in the effects of sleep deprivation, particularly with regards to spatial memory. Muscarinic antagonists, or chemicals that block ACh, impair spatial learning when administered prior to a training task among rats. ACh levels are also found to be lower when measured following a period of sleep deprivation. ACh has also been shown to increase the activity of the PKA pathway, which is needed for memory consolidation.\n\nSerotonin levels (in the form of 5-HT) have been shown to decrease during REM and NREM sleep, leading some researchers to believe that it is also involved in memory consolidation during sleep. Mice lacking the receptor gene for 5-HT engage in more REM sleep and perform better on spatial memory tasks. Researchers have hypothesized that sleep deprivation interferes with the normal reduction in levels of 5-HT, impairing the process of memory consolidation.\n\nAnother theory suggests that the stress brought on by sleep deprivation affects memory consolidation by changing the concentration of corticosteroids in the body. This was simulated in one study by elevating the concentration of glucocorticoids during early sleep stages. The observed effects on memory retention the next day were similar to those obtained from individuals who had received no sleep.\n\nSleep deprivation may affect memory by interfering with neuroplasticity as measured by long-term potentiation in the hippocampus. This reduced plasticity may be the root cause of impairments in both working memory among humans and spatial memory among rats. Sleep deprivation may additionally affect memory by reducing the proliferation of cells in the hippocampus.\n\nSleep deprivation has also been associated with decreased overall membrane excitability of neurons in the brain. Activation of these membranes is critical for the formation of memories. Mitochondria play an essential role in modulating neuron excitability, and research has shown that sleep deprivation is involved in inhibiting mitochondrial metabolism.\n\nReduced duration of sleep, as well as an increase in time spent awake, are factors that highly contribute to the risk of traffic collisions, the severity and fatality rates of which are on the same level as driving under the influence of alcohol, with 19 hours of wakefulness corresponding to a BAC of 0.05%, and 24 hours of wakefulness corresponding to a BAC of 0.10%. Compounding this issue is the proven dissociation between objective performance and subjective alertness; individuals vastly underestimate the effect that sleep deprivation has on their cognitive performance, particularly during the circadian night. Much of the effect of acute sleep deprivation can be countered by napping, with longer naps giving more benefit than shorter naps. Some industries, particularly the Fire Service, have traditionally allowed workers to sleep while on duty, between calls for service. In one study of EMS providers, 24-hour shifts were not associated with a higher frequency of negative safety outcomes when compared to shorter shifts.\n\nThis is especially relevant for young adults as they require 8–9 hours of sleep at night to overcome excessive daytime sleepiness and are among the highest risk group for driving while feeling tired and sleep deprivation related crashes.\n\n"}
{"id": "39419396", "url": "https://en.wikipedia.org/wiki?curid=39419396", "title": "Evans County Heart Study", "text": "Evans County Heart Study\n\nThe Evans County Heart Study was a long-term cardiovascular study on residents of Evans County, Georgia. The study, which was funded by the National Institute of Health, began in July, 1958, and was last updated as recent as May 2016. It resulted in more than 560 published papers that ultimately showed the importance of HDL cholesterol. The study was conducted by Dr. Curtis Gordon Hames, a family doctor from Claxton, Georgia. The study took place in Evans Country because the area had a high death count due to heart complications, such as heart disease and other cardiovascular diseases. The studies showed that there was a significant abundance of heart conditions within a variety of ethnicities. People as young as fourteen and as old as seventy-four participated in the tests, and were divided into different age groups; however only males were eligible for the study. Anyone as old as one-hundred years old could be eligible, and any younger.\n\nThe study involved 6,596 people from Evans County and 3,921 people from Bulloch County. The study population from Evans county consisted of African-American and Caucasian males, ages 40–74, along with a 15-39 year old group that were involved in other studies. Then, the people were divided up into 10 random, equal in size, random samples. Also, a 50% random sample of the 15-39 year old group was taken by including people from the first five samples solely. By comprising this type of sampling, secular trends in disease patterns, along with possible aging of laboratory reagents were randomized.\n\nThe study was done in Evans County, Georgia. Evans County is located in the southeastern portion of the United States. It is situated on the coastal plain estimating around 60 miles inland from the seaport of Savannah, GA. Around half of the population lives on farms while the other half lives in meager little villages. An example is Claxton. Claxton is a town that has a population of 2,000, about 40 percent of which is black. The white population of Evans County was large enough to provide an adequete number for the study, but the black population was considered too small and some blacks from the Bulloch county were incorporated into this study.\n\nThe study can be categorized as a cohort study due to the longevity of the research and its focus on the residents of Evans County, Georgia. It was the leader in the field of epidemiology to analyze effects and differences of coronary heart disease on race. The study aimed to determine how cardiovascular disease affected different races among a specific population. To determine the population size, a specific census was conducted in the county for the study during the first two months of 1960, recording 6,596 persons. From there, the study population included a total of 3,377 residents within Evans County. These residents included a group of randomly selected blacks and whites age 40-74 and an additional group of blacks were selected from Bulloch sub-County. A group of 15-39 year old residents were also selected into this group mainly to participate in other experiments as well. The study population was subsequently randomly divided into 10 groups.\n\nThe examination process consisted of two cardiovascular tests including \"history and physical, blood pressure determination, standard electrocardiogram and chest X-ray, and cholesterol determination.\" Ninety-two percent of the study population responded with an examination process. Therefore, data from 3,102 residents was obtained through the process.\n\nUsing a prospective cohort model, the investigators sought to examine the entire population of the county aged over 40 years and half the population aged between 15 and 39 years. They ultimately achieved a 92% response rate for a total study population of 3,102 residents. From 1960-1962, each participant was given an initial examination in which serum cholesterol, blood pressure, weight, height and the participant’s responses to a number of health behavior and medical history questions. A follow-up consisting of a similar battery of tests was completed from 1967-1969. For 90.9% of the study population, a physical similar to the preliminary one was completed. For another 7.9%, health status was ascertained by a phone survey of either subject or a family member. The total rate of follow-up was 98.8%. Factors assessed at the time of follow-up were systolic and diastolic blood pressure, serum cholesterol level, cigarette smoking, body weight, hematocrit value, ECG abnormalities and diet. Socioeconomic status was also assessed for whites, but not for blacks, as little variability was found in this factor among black subjects.\n\nA central finding of the study was that, across race and social class differences, factors associated with increased risk of coronary heart disease (CHD) did so equally for blacks and whites whether the risk factors were considered singly or in combination. CHD incidence remained markedly higher for white Evans County residents. The only social grouping of white residents that had equally low incidence of CHD was white sharecroppers, which was attributed to higher rates of physical activity among sharecroppers when compared to whites of higher socioeconomic status.\n\nA strength of the study was the high response rate of the population in its entirety, one of only two of its kind to successfully examine an entire community. The publication of the initial study results did not include statistical analysis. Hames attributes his decision to avoid statistical analysis to the unique design of the study. He wrote, “…it certainly is not clear to us [the investigators] what differences such tests could make in our interpretations in the context of this study… the population over 40 does not constitute a sample of some universe. They are a universe.”\n"}
{"id": "696369", "url": "https://en.wikipedia.org/wiki?curid=696369", "title": "Food First Information and Action Network", "text": "Food First Information and Action Network\n\nIntroduction\n\nFIAN International - formerly, FoodFirst Information and Action Network - was founded in 1986 as the first international human rights organization to advocate for the realization of the right to food and nutrition.\nHolding a consultative status with the United Nations Economic and Social Council, FIAN is active in more than 50 countries, through national sections and seeds groups, which account for 25 of these, as well as individual members and international networks. With no religious or political affiliation dictating its work, FIAN International exposes violations of people's right to food and related rights wherever they may occur and stands up against undue and oppressive practices that prevent people from feeding themselves. \n\nBy holding governments accountable, FIAN International strives to secure people's access to and control over natural resources and means of subsistence, crucial to ensure a life of dignity, now and for future generations. Nutrition, as an inherent component of the right to food, and a fundamental act of food sovereignty, remains a core objective in FIAN's work. As the struggle against gender discrimination and other forms of exclusion is an integral part of its mission, FIAN works with and in favor of the most marginalized and affected groups: its vision is of a world free from hunger, in which every woman, man, boy and girl can fully enjoy their right to food, as laid down in the Universal Declaration of Human Rights and other international human rights instruments. \n\nFIAN International's Secretariat is based in Heidelberg, Germany, and Geneva, Switzerland.\n\nOrganizational Structure\n\nFIAN is an international membership based organisation. FIAN's institutional members are its national sections which are legal entities in their own right and have their own membership and elected decision-making bodies.\n\nCurrent sections of FIAN can be found in some 25 countries, for updates look at Worldwide. \n\nThe overall mission, vision and the strategy of FIAN is defined and revised by the International Council (IC), and represented by delegates of the sections who meet once every three years. The IC elects the International Executive Committee (IEC) -headed by the president, which supervises the implementation of FIAN’s strategic plans. The IEC meets bi-annually to review and strategize FIAN programs, methods and budgets. \n\nThe operative working unit being in charge of the execution of FIAN’s programs is its International Secretariat (IS) located in Heidelberg (Germany) and Geneva (Switzerland), and coordinated by the Secretary General.\n\nFIAN International's work\n\nFIAN uses various working tools to achieve the realization of the right to adequate food.\n\nCase-work and Interventions\n\nAt international Fact Finding Missions, FIAN identifies and addresses human rights violations. FIAN interviews people threatened or affected by violations of their right to food and verifies the facts of a situation. Face-to-face contacts to local counterparts are established and serve as a basis for trustful co-operation. On request of those affected, FIAN reacts quickly, analyses the case and mobilises members and supporters worldwide to send out Urgent Action protest letters. Violations are also followed-up in long-term case-work by local FIAN action groups. In close co-operation with the affected communities, FIAN persistently approaches the responsible authorities and identifies breaches of right to food obligations. FIAN’s analysis is based on the International Covenant on Economic, Social and Cultural Rights as interpreted in the UN General Comments, in particular General Comment 12 on the right to adequate food. Existing recourse mechanisms and legal remedies under national and international human rights law are applied to provide redress to the victims.\n\nLobby and Advocacy\n\nReliable contacts and networks, a sound documentation of cases and two decades of experience provide a solid basis for effective lobbying and advocating the right to food. FIAN holds states, international institutions and private actors accountable at the national and international levels. The Right to Food Guidelines adopted by the FAO in 2004 is one of the tools FIAN uses to monitor states’ right to food policies. FIAN tries to improve the existing right to food protection system and to establish new instruments. Intense follow-up work strives to secure the effective implementation of existing instruments, making the right to food politically and judicially enforceable everywhere for everyone.\n\nInformation and Education\n\nTargeted information campaigns and awareness-raising on the right to food are at the core of FIAN’s work - to empower social movements and non-governmental organizations to hold states accountable for violations of the right to food, to clarify for governments and other duty-bearers the content and implementation needs of their obligations, and to motivate supporters from civil society to join action against human rights violations. The systematic information gathered from more than 400 individual cases over the past two decades is analyzed and fed into various professional publications.\n\nImpact\n\nFIAN was involved prominently in the development of the human rights protection system, for instance in the strengthened procedure of State’s and parallel reporting on economic, social, and cultural human rights in the UN human rights system; the elaboration of the UN General Comment No. 12 on the Right to Food in 1999 as the most authoritative legal interpretation of the right to food in international law; and the adoption of the “Voluntary Guidelines on the Right to Food” by the FAO member states in 2004.\n\nFIAN’s commitment was awarded internationally:\n\nPublic Eye on Davos - Positive Award: In 2006, FIAN International together with two partners won the Public Eye on Davos - Positive Award, which is given every year on the occasion of the World Economic Forum in Davos, for their commitment in favour of the rights of workers of the Mexican tyre factory Euzkadi.\n\nSilver Rose Award: In 2005, two staff members of the FIAN Section Germany received the Silver Rose Award for their commitment for the rights of workers in the flower industry, the participation in the development of the International Code of Conduct for the Production of Cut flowers (ICC) and its successful implementation in the framework of the Flower Label Programm (FLP).\n\nFace-It-Act-Now Campaign: http://www.face-it-act-now.org/\nUrgent Actions: http://fian.org/cases/letter-campaigns\nPublications: http://fian.org/resources/documents/others\n"}
{"id": "54372951", "url": "https://en.wikipedia.org/wiki?curid=54372951", "title": "GESTIS Substance Database", "text": "GESTIS Substance Database\n\nGESTIS Substance Database is a freely accessible online information system on chemical compounds. It is maintained by the Institut für Arbeitsschutz der Deutschen Gesetzlichen Unfallversicherung (IFA, Institute for Occupational Safety and Health of the German Social Accident Insurance). Information on occupational medicine and first aid is compiled by Henning Heberer and his team (TOXICHEM, Leuna).\n\nThe database contains information for the safe handling of hazardous substances and other chemical substances at work:\n\nThe available information relates to about 9,400 substances. Data are updated immediately after publication of new official regulations or after the issue of new scientific results.\n\nA mobile version of the GESTIS Substance Database, suitable for smartphones and tablets, is also available.\n\n"}
{"id": "26082350", "url": "https://en.wikipedia.org/wiki?curid=26082350", "title": "Global Health Initiatives", "text": "Global Health Initiatives\n\nGlobal Health Initiatives (GHIs) are humanitarian initiatives that raise and disburse additional funds for infectious diseases– such as AIDS, tuberculosis, and malaria– for immunization and for strengthening health systems in developing countries. GHIs classify a type of global initiative, which is defined as an organized effort integrating the involvement of organizations, individuals, and stakeholders around the world to address a global issue (i.e.: climate change, human rights, etc.).\n\nExamples of GHIs are the President’s Emergency Plan for AIDS Relief (PEPFAR), the Global Fund to Fight AIDS, Tuberculosis and Malaria (Global Fund), and the World Bank's Multi-country AIDS Programme (MAP), all of which focus on HIV/AIDS. The Gavi (formerly the GAVI Alliance) focuses on immunization, particularly with respect to child survival.\n\nIn terms of their institutional structure, GHIs have little in common with each other. In terms of their function – specifically their ability to raise and disburse funds, provide resources and coordinate and/or implement disease control in multiple countries – GHIs share some common ground, even if the mechanisms through which each of these functions is performed are different.\n\nPEPFAR - an initiative established in 2003 by the Bush Administration - and PEPFAR II (PEPFAR’s successor in 2009 under the Obama Administration) are bilateral agreements between the United States and a recipient of its development aid for HIV/AIDS – typically an international non-government organisation INGO or a recipient country’s government. The Global Fund, established in 2002, and the GAVI Alliance, launched in 2000, are public-private partnerships that raise and disburse funds to treat AIDS, Tuberculosis and Malaria, and for immunization and vaccines. The World Bank is an International financial institution. It is the largest funder of HIV/AIDS within the United Nations system and has a portfolio of HIV/AIDS programmes dating back to 1989. In 2000, the Bank launched the first phase of its response to HIV/AIDS in Sub-Saharan Africa – the Multi-Country AIDS Program (MAP). This came to an end in 2006 when a second phase – Agenda for Action 2007-11 – came into effect.\n\nTracking funding from GHIs poses challenges. However, it is possible to determine the amounts of funding GHIs commit and disburse from sources such as the OECD CRS online database, as well as data provided by individual GHIs (Figure 1).\n\nSince 1989, the World Bank has committed approximately US$4.2bn in loans and credits for programs, and has disbursed US$3.1bn. Of this total, the Bank's MAP has committed US$1.9bn since 2000. Through bilateral contributions to HIV/AIDS and Tuberculosis programmes and donations to the Global Fund, PEPFAR has donated approximately US$25.6bn since 2003. In July 2008, the U.S Senate re-authorised a further US$48 bn over five years for PEPFAR II, of which US$6.7bn has been requested for FY 2010. During the period 2001-2010, donors have pledged US$21.1bn to the Global Fund, of which US$15.8bn has been paid by donors to the Fund. Gavi has approved US$3.7bn for the period 2000-2015\n\nThe amount of political priority given to Global Health Initiatives varies between national and international governing powers. Though evidence shows that there exists an inequity between resource allocation for initiatives concerning issues such as child immunization, HIV/AIDS, and family planning in comparison to initiatives for high-burden disorders such as malnutrition and pneumonia, the source of this variance is unknown due to lack of systematic research pertaining to this subject. Global political priority is defined as the extent to which national and international political leaders address an issue of international concern through support in the forms of human capital, technology, and/or finances in order to aid efforts to resolve the problem. Global political priority is demonstrated through national and international leaders expressing sustained concern both privately and publicly, political systems and organizations enacting policies to help alleviate the issue, and national and international agencies providing resource levels that reflect the severity of the given crisis.\n\nThe amount of attention a given global initiative receives is considerably dependent on the power and authority of actors connected to the issue, the power and impact of ideas defining and describing the issue, the power of political contexts framing the environments in which the actors operate to address the issue, as well as the weight and power of issue characteristics indicating the severity of the issue (i.e.: statistical indicators, severity metrics, efficacy of proposed interventions, etc.). Factors including objective measurability, scalability of the issue and proposed interventions, ability to track and monitor progress, risk of perceived harm, as well as simplicity and affordability of proposed solutions all contribute to the degree to which a given global initiative is likely to receive political attention.\n\nHowever, case studies have shown that the likelihood of global initiatives garnering public and political attention is not limited to the aforementioned factors. For example, initiatives concerning polio eradication continue to receive substantial resources in spite of the relatively small global burden of disease as compared to chronic diseases such as cancer, cardiovascular disorders, diabetes, and some communicable diseases such as pneumonia which comparatively attract fewer worldwide resources irrespective of the high morbidity and mortality rates associated with such diseases. These cases highlight the need for extensive research methods and evaluative measures to assess the relative causal weights of factors used to determine how global political priority is attributed to global health initiatives. Existing debates also attribute factors such as the increasing influences of economic globalization, international organizations, and economic actors with little to no previous health remit as each contributing to the evolution of global health governance.\n\nThere is much discussion about the extent to which the volume of these additional funds creates multiple effects that positively and/or negatively impact both health systems and health outcomes for specific diseases. Assessing the direct impact of GHIs on specific diseases and health systems poses challenges pertaining to the issue of attributing particular effects to individual GHIs. As such, a common response in evaluations of GHIs is to acknowledge the inherent limitations of establishing causal chains in what is a highly complex public health environment, and to base conclusions on adequacy statements resulting from trends that demonstrate substantial growth in process and impact indicators.\n\nHowever, existing literature argues that this approach towards evaluating GHIs can inadvertently result in overlooking the impact of social determinants on a disease, as implementers and evaluators are less likely to tackle the complexity of a disease within the larger social, political, cultural, and environmental system. Even if a GHI is effectively evaluated– perhaps showing a decrease in disease prevalence– the challenge remains of comprehensively analyzing the long-term impacts of the GHI by addressing the root social, political, or environmental causes of the disease. Accordingly, existing debates suggest that GHIs should be less concerned with the eradication of specific diseases, and should instead focus primarily on factors– such as basic living conditions, sanitation, and access to nutritious food– that are essential to delivering a sustainable heath program.\n\nA small number of institutions have shaped, and continue to shape, research on GHIs. In 2003, researchers at Abt Associates devised an influential framework for understanding the system-wide effects of the Global fund which has informed much subsequent research, including their own studies of system-wide effects of the Global Fund in Benin, Ethiopia, Georgia and Malawi - often referred to as the 'SWEF' studies.\n\nAbt continues to support ongoing research on the effects of GHIs in multiple countries. The Washington-based Center for Global Development has also been very active in its analysis of GHIs, particularly PEPFAR financing. The Center's HIV/AIDS Monitor is essential reading for researchers of GHIs. With hubs in London and Dublin, the Global Health Initiatives Network (GHIN) has been coordinating and supporting research in 22 countries on the effects of GHIs on existing health systems.\n\nKnowledge of the effects of GHIs on specific diseases and on health systems comes from multiple sources.Longitudinal studies enable researchers to establish baseline data and then track and compare GHI effects on disease control or country health systems over time. In addition to Abt Associates' SWEF studies, additional early examples of this type of analysis were three-year, multi-country studies of the Global Fund in Mozambique, Tanzania, Uganda and Zambia. In 2009, research findings were published from tracking studies in Kyrgyzstan, Peru and Ukraine that sought to identify the health effects of the Global Fund at national and sub-national levels.\n\nIn contrast to longitudinal studies, multi-country analyses of GHIs can provide a ‘snapshot’ of GHI effects but are often constrained by “aggressive timelines”. The Maximising Positive Synergies Academic Consortium, for example, reported in 2009 on the effects of the Global Fund and PEPFAR on disease control and health systems, drawing on data from 20 countries. Most GHI evaluations – both internally and externally commissioned – rely on this type of short-term analysis and, inevitably, there is often a trade-off between depth and breadth of reporting.\n\nSynthesis of data from multiple sources is an invaluable resource for making sense of the effects of GHIs. Early synthesis studies include a 2004 synthesis of findings on the effects of the Global Fund in four countries by researchers at the London School of Hygiene and Tropical Medicine (LSHTM), a 2005 study by McKinsey & Company and an assessment of the comparative advantages of the Global Fund and World Bank AIDS programs.\n\nTwo wide-ranging studies were published in 2009: a study of interactions between GHIs and country health systems commissioned by the World Health Organisation and a study by researchers from LSHTM and the Royal College of Surgeons in Ireland. The latter study - The effects of global health initiatives on country health systems: a review of the evidence from HIV/AIDS control – reviewed the literature on the effects of the Global fund, the World Bank MAP and PEPFAR on country health systems with respect to: 1) national policy; 2) coordination and planning; 3) stakeholder involvement; 4) disbursement, absorptive capacity and management; 5) monitoring & evaluation; and 6) human resources (Table 2).\n\nIn a comparison between the three largest donors in sponsoring efforts to win the fight against AIDS in Africa, a research study found that PEPFAR performs best in money transfer and data collection; the Global Fund outperforms in tailoring programmatic initiatives and sharing data; and MAP performs highest in collaborating with government systems, strengthening health systems, and helping to build the capacity of local recipients. Each of the four GHIs summarized has been evaluated at least once since 2005 and all four produce their own annual reports.\n\nThe primary purpose of the MAP initiative was to introduce a major upscaling of multi-sectoral approach to responding to the HIV/AIDS crisis in Sub-Saharan Africa by involving a multitude of stakeholders including community-based organizations (CBOs), non-governmental organizations (NGOs), line ministries, and state governments at the highest levels.\n\nA comprehensive study of MAP programs published in 2007 reviewed whether MAP was implemented as designed, but did not evaluate MAP or assess its impact. In addition, there have been two evaluations that provide important additional insight into the effectiveness of the Bank's HIV/AIDS programmes (though not specifically MAP focused). In 2005, the Bank conducted an internal evaluation - Committing to Results: Improving the Effectiveness of HIV/AIDS Assistance - which found that National AIDS strategies were not always prioritised or costed.\n\nSupervision, and monitoring and evaluation (M&E), were weak; civil society had not been engaged; political commitment and capacity had been overestimated, and mechanisms for political mobilisation were weak; and bank research and analysis, whilst perceived to be useful, was not reaching policy makers in Africa. In 2009, a hard-hitting evaluation of the Bank’s Health, Nutrition and Population support – Improving Effectiveness of Outcomes for the Poor in Health, Nutrition and Population – found that a third of the Bank’s HNP lending had not performed well, and that while the performance of the Bank’s International Finance Corporation investments had improved, accountability was weak.\n\nUnlike many implementing agencies, the Global Fund has no presence in the countries it supports; rather it is a financial mechanism which provides funding to countries in the form of grants through a Secretariat in Geneva on the competitive basis of country proposals. Special emphasis is placed on proposals demonstrating country ownership as well as those that meet other evidence-based, performance-based, and inclusivity-based criteria.\n\nA five-year, comprehensive evaluation of the Global Fund published a synthesis report in 2009 of findings from three Study areas. The Fund’s Technical Evaluation Research Group (TERG) Five Year Evaluation (5YE) of the Global Fund drew on data from 24 countries to evaluate the Fund’s organisational effectiveness and efficiency, partnership environment and impact on AIDS, TB and Malaria. The Evaluation highlighted the possible decline in HIV incidence rate in some countries, and rapid scale up of funding for HIV/AIDS, access and coverage, but also identified major gaps in support for national health information systems, and poor drug availability.\n\nThough GHIs have been instrumental in bringing national and international attention to crucial global health issues, existing debates suggest that they can also negatively impact country health systems. As such, disease-specific GHIs such as GAVI have worked to integrate health system strengthening (HSS) measures into programmatic implementation. However, the existing global debate questions the efficacy of HSS programs aimed at targeting technical solutions with clear measurable outcomes versus those more broadly focused on supporting holistic health systems.\n\nIn 2008, an evaluation of GAVI’s vaccine and immunization support - Evaluation of the GAVI Phase 1 performance - reported increased coverage of HepB3, Hib3 and DTP3 and increased coverage in rural areas but also a lack of cost data disaggregated by vaccine that prevented GAVI from accurately evaluating the cost effectiveness of its programs and vaccines, and an “unrealistic” reliance by GAVI on the market to reduce the cost of vaccines. The same year, a study of the financial sustainability of GAVI vaccine support - Introducing New Vaccines in the Poorest Countries: What did we learn from the GAVI Experience with - found that although GAVI funding equated to $5 per infant in developing countries per year for the period 2005-10, resource need was accelerating faster than growth in financing.\n\nFindings from two evaluations of GAVI’s support for health systems strengthening (HSS) were published in 2009. An external evaluation by HLSP found insufficient technical support provided to countries applying for GAVI grants, an under-performing Independent Review Committee (IRC), and weaknesses in GAVI’s monitoring of grant activities. The study also found that countries were using GAVI grants for ‘downstream’ short-term HSS fixes rather than ‘upstream’ and long-term structural reform.\nA study by John Snow, Inc praised the multi-year, flexible and country-driven characteristics of GAVI HSS grant funding and encouraged GAVI to continue this support. But also found weak M&E of grant activity, low Civil Society involvement in the HSS proposal development process, unclear proposal writing guidelines, and over-reliance by countries on established development partners for assistance in implementing health system reform.\n\nA quantitative study by Stanford University in 2009 – The President's Emergency Plan for AIDS Relief in Africa: An Evaluation of Outcomes – calculated a 10.5% reduction in the death rate in PEPFAR’s 12 focus countries, equating to 1.2 million lives saved at a cost of $2450 per death averted. In 2007, an evaluation of PEPFAR by the Institute of Medicine found that PEPFAR had made significant progress in reaching its targets for prevention, treatment and care but also reported that budget allocations \"limit the Country Teams ability to harmonize PEPFARs activities with those of the partner government and other donors\", and PEPFARs ABC (Abstinence, Be faithful, and correct and consistent Condom use) priorities \"fragment the natural continuum of needs and services, often in ways that do not correspond with global standards\".\n\nThe PEPFAR program has brought about substantial impact in its recipient countries. The level of urgency and scale of initiatives led through the PEPFAR program were commensurate with that of the HIV/AIDS epidemic at the time of implementation. Existing debates suggest that the next phase of the program consider placing emphasis on the development of knowledge surrounding HIV/AIDS programming.\n\n"}
{"id": "13312969", "url": "https://en.wikipedia.org/wiki?curid=13312969", "title": "Global mental health", "text": "Global mental health\n\nGlobal mental health is the international perspective on different aspects of mental health. It is 'the area of study, research and practice that places a priority on improving mental health and achieving equity in mental health for all people worldwide'. There is a growing body of criticism of the global mental health movement, and has been widely criticised as a neo-colonial or \"missionary\" project and as primarily a front for pharmaceutical companies seeking new clients for psychiatric drugs.\n\nIn theory, taking into account cultural differences and country-specific conditions, it deals with the epidemiology of mental disorders in different countries, their treatment options, mental health education, political and financial aspects, the structure of mental health care systems, human resources in mental health, and human rights issues among others.\n\nThe overall aim of the field of global mental health is to strengthen mental health all over the world by providing information about the mental health situation in all countries, and identifying mental health care needs in order to develop cost-effective interventions to meet those specific needs.\n\nMental, neurological, and substance use disorders make a substantial contribution to the global burden of disease (GBD). This is a global measure of so-called disability-adjusted life years (DALY's) assigned to a certain disease/disorder, which is a sum of the years lived with disability and years of life lost due to this disease within the total population. Neuropsychiatric conditions account for 14% of the global burden of disease. Among non-communicable diseases, they account for 28% of the DALY's – more than cardiovascular disease or cancer. However, it is estimated that the real contribution of mental disorders to the global burden of disease is even higher, due to the complex interactions and co-morbidity of physical and mental illness.\n\nAround the world, almost one million people die due to suicide every year, and it is the third leading cause of death among young people. The most important causes of disability due to health-related conditions worldwide include unipolar depression, alcoholism, schizophrenia, bipolar depression and dementia. In low- and middle-income countries, these conditions represent a total of 19.1% of all disability related to health conditions.\n\nIt is estimated that one in four people in the world will be affected by mental or neurological disorders at some point in their lives. Although many effective interventions for the treatment of mental disorders are known, and awareness of the need for treatment of people with mental disorders has risen, the proportion of those who need mental health care but who do not receive it remains very high. This so-called \"treatment gap\" is estimated to reach between 76–85% for low- and middle-income countries, and 35–50% for high-income countries.\n\nDespite the acknowledged need, for the most part there have not been substantial changes in mental health care delivery during the past years. Main reasons for this problem are public health priorities, lack of a mental health policy and legislation in many countries, a lack of resources – financial and human resources – as well as inefficient resource allocation.\n\nIn 2011, the World Health Organization estimated a shortage of 1.18 million mental health professionals, including 55,000 psychiatrists, 628,000\nnurses in mental health settings, and 493,000 psychosocial care providers needed to treat mental disorders in 144 low- and middle-income countries. The annual wage bill to remove this health workforce shortage was estimated at about US$4.4 billion.\n\nInformation and evidence about cost-effective interventions to provide better mental health care are available. Although most of the research (80%) has been carried out in high-income countries, there is also strong evidence from low- and middle-income countries that pharmacological and psychosocial interventions are effective ways to treat mental disorders, with the strongest evidence for depression, schizophrenia, bipolar disorder and hazardous alcohol use.\n\nRecommendations to strengthen mental health systems around the world have been first mentioned in the WHO's \"World Health Report 2001\", which focused on mental health:\n\n\nBased on the data of 12 countries, assessed by the \"WHO Assessment Instrument for Mental Health Systems\" (WHO-AIMS), the costs of scaling up mental health services by providing a core treatment package for schizophrenia, bipolar affective disorder, depressive episodes and hazardous alcohol use have been estimated. Structural changes in mental health systems according to the WHO recommendations have been taken into account.\n\nFor most countries, this model suggests an initial period of investment of US$0.30 – 0.50 per person per year. The total expenditure on mental health would have to rise at least ten-fold in low-income countries. In those countries, additional financial resources will be needed, while in middle- and high-income countries the main challenge will be the reallocation of resources within the health system to provide better mental health service.\n\nPrevention is beginning to appear in mental health strategies, including the 2004 WHO report \"Prevention of Mental Disorders\", the 2008 EU \"Pact for Mental Health\" and the 2011 US National Prevention Strategy. NIMH or the National Institute of Mental Health has over 400 grants.\n\nTwo of WHO's core programmes for mental health are WHO MIND (Mental health improvements for Nations Development) and Mental Health Gap Action Programme (mhGAP).\n\nWHO MIND focuses on 5 areas of action to ensure concrete changes in people's daily lives. These are:\n\nMental Health Gap Action Programme (mhGAP) is WHO’s action plan to scale up services for mental, neurological and substance use disorders for countries especially with low and lower middle incomes. The aim of mhGAP is to build partnerships for collective action and to reinforce the commitment of governments, international organizations and other stakeholders.\n\nThe mhGAP Intervention Guide (mhGAP-IG) was launched in October 2010. It is a technical tool for the management of mental, neurological and substance use disorders in non-specialist health settings. The priority conditions included are: depression, psychosis, bipolar disorders, epilepsy, developmental and behavioural disorders in children and adolescents, dementia, alcohol use disorders, drug use disorders, self-harm/suicide and other significant emotional or medically unexplained complaints.\n\nOne of the most prominent critics of the Movement for Global Mental Health has been China Mills, author of the book \"Decolonizing Global Mental Health: The Psychiatrization of the Majority World\".\n\nMills writes that:\n\nAnother prominent critic is Ethan Watters, author of \"Crazy Like Us: The Globalization of the American Psyche\".\n\n\n"}
{"id": "33292706", "url": "https://en.wikipedia.org/wiki?curid=33292706", "title": "Health 3.0", "text": "Health 3.0\n\nHealth 3.0 is a health-related extension of the concept of Web 3.0 whereby the users' interface with the data and information available on the web is personalized to optimize their experience. This is based on the concept of the Semantic Web, wherein websites' data is accessible for sorting in order to tailor the presentation of information based on user preferences. Health 3.0 will use such data access to enable individuals to better retrieve and contribute to personalized health-related information within networked electronic health records, and social networking resources.\n\nHealth 3.0 has also been described as the idea of semantically organizing electronic health records to create an Open Healthcare Information Architecture. Health care could also make use of social media, and incorporate virtual tools for enhanced interactions between health care providers and consumers/patients.\n\n\nSocial networking is a popular and powerful tool for engaging patients in their health care. These virtual communities provide a real-time resource for obtaining health-related knowledge and counselling. Pew Internet and American Life Project report that greater than 90% of young adults and nearly three quarters of all Americans access the internet on a regular basis. Greater than 60% of online adults regularly access social networking resources. In addition, 80% of internet users search for health-related information. Definitive evidence of health benefit from interaction with health-related virtual communities is currently lacking as further research needs to be performed.\n\n"}
{"id": "1810614", "url": "https://en.wikipedia.org/wiki?curid=1810614", "title": "Health equity", "text": "Health equity\n\nHealth equity refers to the study and causes of differences in the quality of health and healthcare across different populations. Health equity is different from health equality, as it refers only to the absence of disparities in controllable or remediable aspects of health. It is not possible to work towards complete equality in health, as there are some factors of health that are beyond human influence. Inequity implies some kind of social injustice. Thus, if one population dies younger than another because of genetic differences, a non-remediable/controllable factor, we tend to say that there is a health inequality. On the other hand, if a population has a lower life expectancy due to lack of access to medications, the situation would be classified as a health inequity. These inequities may include differences in the \"presence of disease, health outcomes, or access to health care\" between populations with a different race, ethnicity, sexual orientation or socioeconomic status.\n\nHealth equity falls into two major categories: horizontal equity, the equal treatment of individuals or groups in the same circumstances; and vertical equity, the principle that individuals who are unequal should be treated differently according to their level of need. Disparities in the quality of health across populations are well-documented globally in both developed and developing nations. The importance of equitable access to healthcare has been cited as crucial to achieving many of the Millennium Development Goals.\n\nSocioeconomic status is both a strong predictor of health, and a key factor underlying health inequities across populations. Poor socioeconomic status has the capacity to profoundly limit the capabilities of an individual or population, manifesting itself through deficiencies in both financial and social capital. It is clear how a lack of financial capital can compromise the capacity to maintain good health. In the UK, prior to the institution of the NHS reforms in the early 2000s, it was shown that income was an important determinant of access to healthcare resources. Because one's job or career is a primary conduit for both financial and social capital, work is an important, yet under represented, factor in health inequities research and prevention efforts. Maintenance of good health through the utilization of proper healthcare resources can be quite costly and therefore unaffordable to certain populations.\n\nIn China, for instance, the collapse of the Cooperative Medical System left many of the rural poor uninsured and unable to access the resources necessary to maintain good health. Increases in the cost of medical treatment made healthcare increasingly unaffordable for these populations. This issue was further perpetuated by the rising income inequality in the Chinese population. Poor Chinese were often unable to undergo necessary hospitalization and failed to complete treatment regimens, resulting in poorer health outcomes.\n\nSimilarly, in Tanzania, it was demonstrated that wealthier families were far more likely to bring their children to a healthcare provider: a significant step towards stronger healthcare. Some scholars have noted that unequal income distribution itself can be a cause of poorer health for a society as a result of \"underinvestment in social goods, such as public education and health care; disruption of social cohesion and the erosion of social capital\".\n\nThe role of socioeconomic status in health equity extends beyond simple monetary restrictions on an individual's purchasing power. In fact, social capital plays a significant role in the health of individuals and their communities. It has been shown that those who are better connected to the resources provided by the individuals and communities around them (those with more social capital) live longer lives. The segregation of communities on the basis of income occurs in nations worldwide and has a significant impact on quality of health as a result of a decrease in social capital for those trapped in poor neighborhoods. Social interventions, which seek to improve healthcare by enhancing the social resources of a community, are therefore an effective component of campaigns to improve a community's health. A 1998 epidemiological study\" \"showed that community healthcare approaches fared far better than individual approaches in the prevention of heart disease mortality.\n\nUnconditional cash transfers for reducing poverty used by some programs in the developing world appear to lead to a reduction in the likelihood of being sick.<ref name=\"doi10.1002/14651858.CD011135.pub2\"></ref> Such evidence can guide resource allocations to effective interventions.\n\nEducation is an important factor in healthcare utilization, though it is closely intertwined with economic status. An individual may not go to a medical professional or seek care if they don’t know the ills of their failure to do so, or the value of proper treatment. In Tajikistan, since the nation gained its independence, the likelihood of giving birth at home has increased rapidly among women with lower educational status. Education also has a significant impact on the quality of prenatal and maternal healthcare. Mothers with primary education consulted a doctor during pregnancy at significantly lower rates (72%) when compared to those with a secondary education (77%), technical training (88%) or a higher education (100%). There is also evidence for a correlation between socioeconomic status and health literacy; one study showed that wealthier Tanzanian families were more likely to recognize disease in their children than those that were coming from lower income backgrounds.\n\nFor some populations, access to healthcare and health resources is physically limited, resulting in health inequities. For instance, an individual might be physically incapable of traveling the distances required to reach healthcare services, or long distances can make seeking regular care unappealing despite the potential benefits. Costa Rica, for example, has demonstrable health spatial inequities with 12–14% of the population living in areas where healthcare is inaccessible. Inequity has decreased in some areas of the nation as a result of the work of healthcare reform programs, however those regions not served by the programs have experienced a slight increase in inequity.\n\nChina experienced a serious decrease in spatial health equity following the Chinese economic revolution in the 1980s as a result of the degradation of the Cooperative Medical System (CMS). The CMS provided an infrastructure for the delivery of healthcare to rural locations, as well as a framework to provide funding based upon communal contributions and government subsidies. In its absence, there was a significant decrease in the quantity of healthcare professionals (35.9%), as well as functioning clinics (from 71% to 55% of villages over 14 years) in rural areas, resulting in inequitable healthcare for rural populations. The significant poverty experienced by rural workers (some earning less than 1 USD per day) further limits access to healthcare, and results in malnutrition and poor general hygiene, compounding the loss of healthcare resources. The loss of the CMS has had noticeable impacts on life expectancy, with rural regions such as areas of Western China experiencing significantly lower life expectancies.\n\nSimilarly, populations in rural Tajikistan experience spatial health inequities. A study by Jane Falkingham noted that physical access to healthcare was one of the primary factors influencing quality of maternal healthcare. Further, many women in rural areas of the country did not have adequate access to healthcare resources, resulting in poor maternal and neonatal care. These rural women were, for instance, far more likely to give birth in their homes without medical oversight.\n\nAlong with the socioeconomic factor of health disparities, race is another key factor. The United States historically had large disparities in health and access to adequate healthcare between races, and current evidence supports the notion that these racially centered disparities continue to exist and are a significant social health issue. The disparities in access to adequate healthcare include differences in the quality of care based on race and overall insurance coverage based on race. A 2002 study in the \"Journal of the American Medical Association\" identifies race as a significant determinant in the level of quality of care, with blacks receiving lower quality care than their white counterparts. This is in part because members of ethnic minorities such as African Americans are either earning low incomes, or living below the poverty line. In a 2007 Census Bureau, African American families made an average of $33,916, while their white counterparts made an average of $54,920. Due to a lack of affordable health care, the African American death rate reveals that African Americans have a higher rate of dying from treatable or preventable causes. According to a study conducted in 2005 by the Office of Minority Health—a U.S. Department of Health—African American men were 30% more likely than white men to die from heart disease. Also African American women were 34% more likely to die from breast cancer than their white counterparts.\n\nThere are also considerable racial disparities in access to insurance coverage, with ethnic minorities generally having less insurance coverage than non-ethnic minorities. For example, Hispanic Americans tend to have less insurance coverage than white Americans and as a result receive less regular medical care. The level of insurance coverage is directly correlated with access to healthcare including preventative and ambulatory care. A 2010 study on racial and ethnic disparities in health done by the Institute of Medicine has shown that the aforementioned disparities cannot solely be accounted for in terms of certain demographic characteristics like: insurance status, household income, education, age, geographic location and quality of living conditions. Even when the researchers corrected for these factors, the disparities persist. Slavery has contributed to disparate health outcomes for generations of African Americans in the United States.\n\nEthnic health inequities also appear in nations across the African continent. A survey of the child mortality of major ethnic groups across 11 African nations (Central African Republic, Côte d'Ivoire, Ghana, Kenya, Mali, Namibia, Niger, Rwanda, Senegal, Uganda, and Zambia) was published in 2000 by the WHO. The study described the presence of significant ethnic parities in the child mortality rates among children younger than 5 years old, as well as in education and vaccine use. In South Africa, the legacy of apartheid still manifests itself as a differential access to social services, including healthcare based upon race and social class, and the resultant health inequities. Further, evidence suggests systematic disregard of indigenous populations in a number of countries. The Pygmys of Congo, for instance, are excluded from government health programs, discriminated against during public health campaigns, and receive poorer overall healthcare.\n\nIn a survey of five European countries (Sweden, Switzerland, the UK, Italy, and France), a 1995 survey noted that only Sweden provided access to translators for 100% of those who needed it, while the other countries lacked this service potentially compromising healthcare to non-native populations. Given that non-natives composed a considerable section of these nations (6%, 17%, 3%, 1%, and 6% respectively), this could have significant detrimental effects on the health equity of the nation. In France, an older study noted significant differences in access to healthcare between native French populations, and non-French/migrant populations based upon health expenditure; however this was not fully independent of poorer economic and working conditions experienced by these populations.\n\nA 1996 study of race-based health inequity in Australia revealed that Aborigines experienced higher rates of mortality than non-Aborigine populations. Aborigine populations experienced 10 times greater mortality in the 30–40 age range; 2.5 times greater infant mortality rate, and 3 times greater age standardized mortality rate. Rates of diarrheal diseases and tuberculosis are also significantly greater in this population (16 and 15 times greater respectively), which is indicative of the poor healthcare of this ethnic group. At this point in time, the parities in life expectancy at birth between indigenous and non-indigenous peoples were highest in Australia, when compared to the US, Canada and New Zealand. In South America, indigenous populations faced similarly poor health outcomes with maternal and infant mortality rates that were significantly higher (up to 3 to 4 times greater) than the national average. The same pattern of poor indigenous healthcare continues in India, where indigenous groups were shown to experience greater mortality at most stages of life, even when corrected for environmental effects.\n\nSexuality is a basis of health discrimination and inequity throughout the world. Homosexual, bisexual, transgender, and gender-variant populations around the world experience a range of health problems related to their sexuality and gender identity, some of which are complicated further by limited research.\n\nIn spite of recent advances, LGBT populations in China, India, and Chile continue to face significant discrimination and barriers to care. The World Health Organization (WHO) recognizes that there is inadequate research data about the effects of LGBT discrimination on morbidity and mortality rates in the patient population. In addition, retrospective epidemiological studies on LGBT populations are difficult to conduct as a result of the practice that sexual orientation is not noted on death certificates. WHO has proposed that more research about the LGBT patient population is needed  for improved understanding of its  unique health needs and barriers to accessing care.\n\nRecognizing the need for LGBT healthcare research, the Director of the National Institute on Minority Health and Health Disparities (NIMHD) at the U.S. Department of Health and Human Services designated sexual and gender minorities (SGMs) as a health disparity population for NIH research in October 2016. For the purposes of this designation, the Director defines SGM as \"encompass[ing] lesbian, gay, bisexual, and transgender populations, as well as those whose sexual orientation, gender identity and expressions, or reproductive development varies from traditional, societal, cultural, or physiological norms\". This designation has prioritized research into the extent, cause, and potential mitigation of health disparities among SGM populations within the larger LGBT community.\n\nWhile many aspects of LGBT health disparities are heretofore uninvestigated, at this stage, it is known that one of the main forms of healthcare discrimination  LGBT individuals face is discrimination from healthcare workers or institutions themselves. A systematic literature review of publications in English and Portuguese from 2004–2014 demonstrate significant difficulties in accessing care secondary to discrimination and homophobia from healthcare professionals. This discrimination can take the form of verbal abuse, disrespectful conduct, refusal of care, the withholding of health information,  inadequate treatment, and outright violence. In a study analyzing the quality of healthcare for South African men who have sex with men (MSM), researchers interviewed a cohort of individuals about their health experiences, finding that MSM who identified as homosexual felt their access to healthcare was limited due to an inability to find clinics employing healthcare workers who did not discriminate against their sexuality. They also reportedly faced \"homophobic verbal harassment from healthcare workers when presenting for STI treatment\". Further, MSM who did not feel comfortable disclosing their sexual activity to healthcare workers failed to identify as homosexuals, which limited the quality of the treatment they received.\n\nAdditionally, members of the LGBT community contend with health care disparities due, in part, to lack of provider training and awareness of the population’s healthcare needs. Studies regarding patient-provider communication in the LGBT patient community show that providers themselves report a significant lack of awareness regarding the health issues LGBT-identifying patients face. As a component of this fact, medical schools do not focus much attention on LGBT health issues in their curriculum; the LGBT-related topics that are discussed tend to be limited to HIV/AIDS, sexual orientation, and gender identity.\n\nAmong LGBT-identifying individuals, transgender individuals face especially significant barriers to treatment. Many countries still do not have legal recognition of transgender or non-binary gender individuals leading to placement in mis-gendered hospital wards and medical discrimination. Seventeen European states mandate sterilization of individuals who seek recognition of a gender identity that diverges from their birth gender. In addition to many of the same barriers as the rest of the LGBT community, a WHO bulletin points out that globally, transgender individuals often also face a higher disease burden. A 2010 survey of transgender and gender-variant people in the United States revealed that transgender individuals faced a significant level of discrimination. The survey indicated that 19% of individuals experienced a healthcare worker refusing care because of their gender, 28% faced harassment from a healthcare worker, 2% encountered violence, and 50% saw a doctor who was not able or qualified to provide transgender-sensitive care. In Kuwait, there have been reports of transgender individuals being reported to legal authorities by medical professionals, preventing safe access to care. An updated version of the U.S. survey from 2015 showed little change in terms of healthcare experiences for transgender and gender variant individuals. The updated survey revealed that 23% of individuals reported not seeking necessary medical care out of fear of discrimination, and 33% of individuals who had been to a doctor within a year of taking the survey reported negative encounters with medical professionals related to their transgender status.\n\nThe stigmatization represented particularly in the transgender population  creates a health disparity for LGBT individuals with regard to mental health. The LGBT community is at increased risk for psychosocial distress, mental health complications, suicidality, homelessness, and substance abuse, often complicated by access-based under-utilization or fear of health services. Transgender and gender-variant individuals have been found to experience higher rates of mental health disparity than LGB individuals. According to the 2015 U.S. Transgender Survey, for example, 39% of respondents reported serious psychological distress, compared to 5% of the general population.\n\nThese mental health facts are informed by a history of anti-LGBT bias in health care. The Diagnostic and Statistical Manual of Mental Disorders (DSM) listed homosexuality as a disorder until 1973; transgender status was listed as a disorder until 2012. This was amended in 2013 with the DSM-5 when \"gender identity disorder\" was replaced with \"gender dysphoria\", reflecting that simply identifying as transgender is not itself pathological and that the diagnosis is instead for the distress a transgender person may experience as a result of the discordance between assigned gender and gender identity.\n\nLGBT health issues have received disproportionately low levels of medical research, leading to difficulties in assessing appropriate strategies for LGBT treatment. For instance, a review of medical literature regarding LGBT patients revealed that there are significant gaps in the medical understanding of cervical cancer in lesbian and bisexual individuals it is unclear whether its prevalence in this community is a result of probability or some other preventable cause. For example, LGBT people report poorer cancer care experiences. It is incorrectly assumed that LGBT women have a lower incidence of cervical cancer than their heterosexual counterparts, resulting in lower rates of screening.  Such findings illustrate the need for continued research focused on the circumstances and needs of LGBT individuals and the inclusion in policy frameworks of sexual orientation and gender identity as social determinants of health.\n\nA June 2017 review sponsored by the European commission as part of a larger project to identify and diminish health inequities, found that LGB are at higher risk of some cancers and that LGBTI were at higher risk of mental illness, and that these risks were not adequately addressed. The causes of health inequities were, according to the review, \"i) cultural and social norms that preference and prioritise heterosexuality; ii) minority stress associated with sexual orientation, gender identity and sex characteristics; iii) victimisation; iv) discrimination (individual and institutional), and; v) stigma.\"\n\nBoth gender and sex are significant factors that influence health. Sex is characterized by female and male biological differences in regards to gene expression, hormonal concentration, and anatomical characteristics. Gender is an expression of behavior and lifestyle choices. Both sex and gender inform each other, and it is important to note that differences between the two genders influence disease manifestation and associated healthcare approaches. Understanding how the interaction of sex and gender contributes to disparity in the context of health allows providers to ensure quality outcomes for patients. This interaction is complicated by the difficulty of distinguishing between sex and gender given their intertwined nature; sex modifies gender, and gender can modify sex, thereby impacting health.  Sex and gender can both be considered sources of health disparity; both contribute to men and women’s susceptibility to various health conditions, including cardiovascular disease and autoimmune disorders.\n\nAs sex and gender are inextricably linked in day-to-day life, their union is apparent in medicine. Gender and sex are both components of health disparity in the male population. In non-Western regions, males tend to have a health advantage over women due to gender discrimination, evidenced by infanticide, early marriage, and domestic abuse for females. In most regions of the world, the mortality rate is higher for adult men than for adult women; for example, adult men suffer from fatal illnesses with more frequency than females. The leading causes of the higher male death rate are accidents, injuries, violence, and cardiovascular diseases. In a number of countries, males also face a heightened risk of mortality as a result of behavior and greater propensity for violence.\n\nPhysicians tend to offer invasive procedures to male patients more than female patients. Furthermore, men are more likely to smoke than women and experience smoking-related health complications later in life as a result; this trend is also observed in regard to other substances, such as marijuana, in Jamaica, where the rate of use is 2–3 times more for men than women. Lastly, men are more likely to have severe chronic conditions and a lower life expectancy than women in the United States.\n\nGender and sex are also components of health disparity in the female population. The 2012 World Development Report (WDR) noted that women in developing nations experience greater mortality rates than men in developing nations. Additionally, women in developing countries have a much higher risk of maternal death than those in developed countries. The highest risk of dying during childbirth is 1 in 6 in Afghanistan and Sierra Leone, compared to nearly 1 in 30,000 in Sweden—a disparity that is much greater than that for neonatal or child mortality.\n\nWhile women in the United States tend to live longer than men, they generally are of lower socioeconomic status (SES) and therefore have more barriers to accessing healthcare. Being of lower SES also tends to increase societal pressures, which can lead to higher rates of depression and chronic stress and, in turn, negatively impact health. Women are also more likely than men to suffer from sexual or intimate-partner violence both in the United States and worldwide. In Europe, women who grew up in poverty are more likely to have lower muscle strength and higher disability in old age.\n\nWomen have better access to healthcare in the United States than they do in many other places in the world. In one population study conducted in Harlem, New York, 86% of women reported having privatized or publicly assisted health insurance, while only 74% of men reported having any health insurance. This trend is representative of the general population of the United States.\n\nIn addition, women's pain tends to be treated less seriously and initially ignored by clinicians when compared to their treatment of men's pain complaints. Historically, women have not been included in the design or practice of clinical trials, which has slowed the understanding of women's reactions to medications and created a research gap. This has led to post-approval adverse events among women, resulting in several drugs being pulled from the market. However, the clinical research industry is aware of the problem, and has made progress in correcting it.\n\nHealth disparities are also due in part to cultural factors that involve practices based not only on sex, but also gender status. For example, in China, health disparities have distinguished medical treatment for men and women due to the cultural phenomenon of preference for male children. Recently, gender-based disparities have decreased as females have begun to receive higher-quality care. Additionally, a girl’s chances of survival are impacted by the presence of a male sibling; while girls do have the same chance of survival as boys if they are the oldest girl, they have a higher probability of being aborted or dying young if they have an older sister.\n\nIn India, gender-based health inequities are apparent in early childhood. Many families provide better nutrition for boys in the interest of maximizing future productivity given that boys are generally seen as breadwinners. In addition, boys receive better care than girls and are hospitalized at a greater rate. The magnitude of these disparities increases with the severity of poverty in a given population.\n\nAdditionally, the cultural practice of female genital mutilation (FGM) is known to impact women's health, though is difficult to know the worldwide extent of this practice. While generally thought of as a Sub-Saharan African practice, it may have roots in the Middle East as well. The estimated 3 million girls who are subjected to FGM each year potentially suffer both immediate and lifelong negative effects. Immediately following FGM, girls commonly experience excessive bleeding and urine retention. Long-term consequences include urinary tract infections, bacterial vaginosis, pain during intercourse, and difficulties in childbirth that include prolonged labor, vaginal tears, and excessive bleeding. Women who have undergone FGM also have higher rates of post-traumatic stress disorder (PTSD) and herpes simplex virus 2 (HSV2) than women who have not.\n\nMinority populations have increased exposure to environmental hazards that include lack of neighborhood resources, structural and community factors as well as residential segregation that result in a cycle of disease and stress. The environment that surrounds us can influence individual behaviors and lead to poor health choices and therefore outcomes. Minority neighborhoods have been continuously noted to have more fast food chains and fewer grocery stores than predominantly white neighborhoods. These food deserts affect a family’s ability to have easy access to nutritious food for their children. This lack of nutritious food extends beyond the household into the schools that have a variety of vending machines and deliver over processed foods. These environmental condition have social ramifications and in the first time in US history is it projected that the current generation will live shorter lives than their predecessors will.\n\nIn addition, minority neighborhoods have various health hazards that result from living close to highways and toxic waste factories or general dilapidated structures and streets. These environmental conditions create varying degrees of health risk from noise pollution, to carcinogenic toxic exposures from asbestos and radon that result in increase chronic disease, morbidity, and mortality. The quality of residential environment such as damaged housing has been shown to increase the risk of adverse birth outcomes, which is reflective of a communities health. Housing conditions can create varying degrees of health risk that lead to complications of birth and long-term consequences in the aging population. In addition, occupational hazards can add to the detrimental effects of poor housing conditions. It has been reported that a greater number of minorities work in jobs that have higher rates of exposure to toxic chemical, dust and fumes.\n\nRacial segregation is another environmental factor that occurs through the discriminatory action of those organizations and working individuals within the real estate industry, whether in the housing markets or rentals. Even though residential segregation is noted in all minority groups, blacks tend to be segregated regardless of income level when compared to Latinos and Asians. Thus, segregation results in minorities clustering in poor neighborhoods that have limited employment, medical care, and educational resources, which is associated with high rates of criminal behavior. In addition, segregation affects the health of individual residents because the environment is not conducive to physical exercise due to unsafe neighborhoods that lack recreational facilities and have nonexistent park space. Racial and ethnic discrimination adds an additional element to the environment that individuals have to interact with daily. Individuals that reported discrimination have been shown to have an increase risk of hypertension in addition to other physiological stress related affects. The high magnitude of environmental, structural, socioeconomic stressors leads to further compromise on the psychological and physical being, which leads to poor health and disease.\n\nIndividuals living in rural areas, especially poor rural areas, have access to fewer health care resources. Although 20 percent of the U.S. population lives in rural areas, only 9 percent of physicians practice in rural settings. Individuals in rural areas typically must travel longer distances for care, experience long waiting times at clinics, or are unable to obtain the necessary health care they need in a timely manner. Rural areas characterized by a largely Hispanic population average 5.3 physicians per 10,000 residents compared with 8.7 physicians per 10,000 residents in nonrural areas. Financial barriers to access, including lack of health insurance, are also common among the urban poor.\n\nReasons for disparities in access to health care are many, but can include the following:\n\n\nHealth disparities in the quality of care exist and are based on language and ethnicity/race which includes:\n\nCommunication is critical for the delivery of appropriate and effective treatment and care, regardless of a patient’s race, and miscommunication can lead to incorrect diagnosis, improper use of medications, and failure to receive follow-up care. The patient provider relationship is dependent on the ability of both individuals to effectively communicate. Language and culture both play a significant role in communication during a medical visit. Among the patient population, minorities face greater difficulty in communicating with their physicians. Patients when surveyed responded that 19% of the time they have problems communicating with their providers which included understanding doctor, feeling doctor listened, and had questions but did not ask. In contrast, the Hispanic population had the largest problem communicating with their provider, 33% of the time. Communication has been linked to health outcomes, as communication improves so does patient satisfaction which leads to improved compliance and then to improved health outcomes. Quality of care is impacted as a result of an inability to communicate with health care providers. Language plays a pivotal role in communication and efforts need to be taken to ensure excellent communication between patient and provider. Among limited English proficient patients in the United States, the linguistic barrier is even greater. Less than half of non-English speakers who say they need an interpreter during clinical visits report having one. The absence of interpreters during a clinical visit adds to the communication barrier. Furthermore, inability of providers to communicate with limited English proficient patients leads to more diagnostic procedures, more invasive procedures, and over prescribing of medications. Poor communication contributes to poor medical compliance and health outcomes. Many health-related settings provide interpreter services for their limited English proficient patients. This has been helpful when providers do not speak the same language as the patient. However, there is mounting evidence that patients need to communicate with a language concordant physician (not simply an interpreter) to receive the best medical care, bond with the physician, and be satisfied with the care experience. Having patient-physician language discordant pairs (i.e. Spanish-speaking patient with an English-speaking physician) may also lead to greater medical expenditures and thus higher costs to the organization. Additional communication problems result from a decrease or lack of cultural competence by providers. It is important for providers to be cognizant of patients’ health beliefs and practices without being judgmental or reacting. Understanding a patients’ view of health and disease is important for diagnosis and treatment. So providers need to assess patients’ health beliefs and practices to improve quality of care. Patient health decisions can be influenced by religious beliefs, mistrust of Western medicine, and familial and hierarchical roles, all of which a white provider may not be familiar with. Other type of communication problems are seen in LGBT health care with the spoken heterosexist (conscious or unconscious) attitude on LGBT patients, lack of understanding on issues like having no sex with men (lesbians, gynecologic examinations) and other issues.\n\nProvider discrimination occurs when health care providers either unconsciously or consciously treat certain racial and ethnic patients differently from other patients. This may be due to stereotypes that providers may have towards ethnic/racial groups. Doctors are more likely to ascribe negative racial stereotypes to their minority patients. This may occur regardless of consideration for education, income, and personality characteristics. Two types of stereotypes may be involved, automatic stereotypes or goal modified stereotypes. Automated stereotyping is when stereotypes are automatically activated and influence judgments/behaviors outside of consciousness. Goal modified stereotype is a more conscious process, done when specific needs of clinician arise (time constraints, filling in gaps in information needed) to make a complex decisions. Physicians are unaware of their implicit biases. Some research suggests that ethnic minorities are less likely than whites to receive a kidney transplant once on dialysis or to receive pain medication for bone fractures. Critics question this research and say further studies are needed to determine how doctors and patients make their treatment decisions. Others argue that certain diseases cluster by ethnicity and that clinical decision making does not always reflect these differences.\n\nAccording to the 2009 National Healthcare Disparities Report, uninsured Americans are less likely to receive preventive services in health care. For example, minorities are not regularly screened for colon cancer and the death rate for colon cancer has increased among African Americans and Hispanic populations. Furthermore, limited English proficient patients are also less likely to receive preventive health services such as mammograms. Studies have shown that use of professional interpreters have significantly reduced disparities in the rates of fecal occult testing, flu immunizations and pap smears. In the UK, Public Health England, a universal service free at the point of use, which forms part of the NHS, offers regular screening to any member of the population considered to be in an at-risk group (such as individuals over 45) for major disease (such as colon cancer, or diabetic-retinopathy).\n\nThere are a multitude of strategies for achieving health equity and reducing disparities outlined in scholarly texts, some examples include:\n\n\nHealth inequality is the term used in a number of countries to refer to those instances whereby the health of two demographic groups (not necessarily ethnic or racial groups) differs despite comparative access to health care services. Such examples include higher rates of morbidity and mortality for those in lower occupational classes than those in higher occupational classes, and the increased likelihood of those from ethnic minorities being diagnosed with a mental health disorder. In Canada, the issue was brought to public attention by the LaLonde report.\n\nIn UK, the Black Report was produced in 1980 to highlight inequalities. On 11 February 2010, Sir Michael Marmot, an epidemiologist at University College London, published the \"Fair Society, Healthy Lives\" report on the relationship between health and poverty. Marmot described his findings as illustrating a \"social gradient in health\": the life expectancy for the poorest is seven years shorter than for the most wealthy, and the poor are more likely to have a disability. In its report on this study, \"The Economist\" argued that the material causes of this contextual health inequality include unhealthful lifestyles - smoking remains more common, and obesity is increasing fastest, amongst the poor in Britain.\n\nIn June 2018, the European Commission launched the Joint Action Health Equity in Europe. Forty-nine participants from 25 European Union Member States will work together to address health inequalities and the underlying social determinants of health across Europe. Under the coordination of the Italian Institute of Public Health, the Joint Action aims to achieve greater equity in health in Europe across all social groups while reducing the inter-country heterogeneity in tackling health inequalities.\n\nPoor health outcomes appear to be an effect of economic inequality across a population. Nations and regions with greater economic inequality show poorer outcomes in life expectancy, mental health, drug abuse, obesity, educational performance, teenage birthrates, and ill health due to violence. On an international level, there is a positive correlation between developed countries with high economic equality and longevity. This is unrelated to average income per capita in wealthy nations. Economic gain only impacts life expectancy to a great degree in countries in which the mean per capita annual income is less than approximately $25,000.\nThe United States shows exceptionally low health outcomes for a developed country, despite having the highest national healthcare expenditure in the world. The US ranks 31st in life expectancy. Americans have a lower life expectancy than their European counterparts, even when factors such as race, income, diet, smoking, and education are controlled for.\n\nRelative inequality negatively affects health on an international, national, and institutional levels. The patterns seen internationally hold true between more and less economically equal states in the United States. The patterns seen internationally hold true between more and less economically equal states in the United States, that is, more equal states show more desirable health outcomes. Importantly, inequality can have a negative health impact on members of lower echelons of institutions. The Whitehall I and II studies looked at the rates of cardiovascular disease and other health risks in British civil servants and found that, even when lifestyle factors were controlled for, members of lower status in the institution showed increased mortality and morbidity on a sliding downward scale from their higher status counterparts.\nThe negative aspects of inequality are spread across the population. For example, when comparing the United States (a more unequal nation) to England (a less unequal nation), the US shows higher rates of diabetes, hypertension, cancer, lung disease, and heart disease across all income levels. This is also true of the difference between mortality across all occupational classes in highly equal Sweden as compared to less-equal England.\n\n\n\n\n[[Category:Determinants of health]]\n[[Category:Healthcare quality]]\n[[Category:Health economics]]\n[[Category:Social inequality]]\n[[Category:Social problems in medicine]]\n[[Category:Public health]]"}
{"id": "9541887", "url": "https://en.wikipedia.org/wiki?curid=9541887", "title": "Healthcare proxy", "text": "Healthcare proxy\n\nIn the field of medicine, a healthcare proxy (commonly referred to as HCP) is a document (legal instrument) with which a patient (primary individual) appoints an agent to legally make healthcare decisions on behalf of the patient, when the patient is incapable of making and executing the healthcare decisions stipulated in the proxy. Once the health care proxy is effective, the primary individual continues making healthcare decisions as long as the primary individual is legally competent to decide. Moreover, in legal-administrative functions, the healthcare proxy is a legal instrument akin to a \"springing\" health care power of attorney. The proxy must declare the healthcare agent who will gain durable power attorney. This document also notifies of the authority given from the principal to the agent and states the limitations of this authority. \n\nThose over the age of 18 are allowed to have a healthcare proxy, and these documents are useful in situations that render a person unable to communicate their wishes such as being in a persistent vegetative state, having a form of dementia of an illness that takes away one's ability to effectively communicate, or being under anesthesia when a decision needs to be made. Healthcare proxies are one of three ways that surrogate decision makers are enacted, the other two being court orders and laws for the automatic succession of decision makers. In contrast to a living will, healthcare proxies do not set out possible outcomes with predetermined reactions, rather they appoint someone to carry out the wishes of an individual. \n\nThe methods of healthcare planning and tools of advanced preparation have changed dramatically over the years. The concept of durable power of attorney arose in Virginia in 1954 for the purpose of setting property matters. This allowed for a continued existence of power of attorney following the original person losing capacity to carry out the necessary actions. This concept evolved over the years and in 1983, the President's Commission for the Study of Ethical Problems in Medicine and Biomedical and Behavioral Research addressed this idea as one of great potential in the healthcare industry. This commission also stated the possibility of abuse as a noted concern going forward. In response to this commission, there was an evolution of this concept throughout the 1980's and the 1990's that eventually led to all states in America having a healthcare power of attorney statute by 1997. \n\nSome jurisdictions place limitations on the persons who can act as agents. (Some forbid the appointment of treating physicians as the healthcare proxy.) In any event the agent should be someone close to and trusted by the primary individual. According to the state of Massachusetts, no person who is an employee or administrator of a facility can be an agent unless it is for someone who is of familial relation to them. In any event the agent is recommended to be someone close to and trusted by the primary individual. In the absence of a power of attorney, a legal guardian must be appointed.\n\nHealthcare proxies are permitted in forty-nine states as well as the District of Columbia.\nHealthcare forms may differ in structure from state to state and pre-made forms are not compulsory as long as certain guidelines are met. The common guidelines include:\n\n\nThe agent is empowered when a qualified physician determines that the primary individual is unable to make decisions regarding healthcare. The agent may be granted the power to remove or sustain feeding tubes from the primary individual if these tubes are the only things that are keeping the primary individual alive. The agent's decision should draw upon knowledge of the patient's desire in this matter. If the primary individual made his or her wishes clear on the proxy form, then they must be followed despite any possible objections from the agent.\n\nAn individual may have identified end-of-life decisions in a separate document, such as a living will or advanced health care directive, in which case it is necessary to examine all of the documents to determine if any supersede the agent's authority as granted in the healthcare proxy, based on the language of the interrelated documents and governing state law. An agent will not be legally or financially liable for decisions made on behalf of the primary individual as long as they follow the terms of the healthcare proxy.\n\nThere are limited legal foundation to determine the ability of someone to appoint a healthcare proxy. Although physicians are allowed to deliver life-saving treatment in emergent situations, in non-emergencies, the it is determined if the patient has the ability to then appoint a healthcare proxy. It is possible for a patient lacking the ability to make healthcare decisions, to still have the capacity to appoint an agent and have a proxy.\n\nIn England and Wales, an independent mental health capacity advocate may be appointed under the Mental Capacity Act 2005; the provisions made in the same Act for a lasting power of attorney may also provide a satisfactory basis for providing care via an attorney, who does not require to be professionally qualified. Different arrangements apply elsewhere in the UK.\n\n\n\n"}
{"id": "2468967", "url": "https://en.wikipedia.org/wiki?curid=2468967", "title": "Hexavalent chromium", "text": "Hexavalent chromium\n\nHexavalent chromium (chromium(VI), Cr(VI), chromium 6) is any chemical compound that contains the element chromium in the +6 oxidation state (thus hexavalent). Virtually all chromium ore is processed via hexavalent chromium, specifically the salt sodium dichromate. Approximately of hexavalent chromium were produced in 1985. Additional hexavalent chromium compounds are chromium trioxide and various salts of chromate and dichromate, among others. Hexavalent chromium is used in textile dyes, wood preservation, anti-corrosion products, chromate conversion coatings, and a variety of niche uses. Industrial uses of hexavalent chromium compounds include chromate pigments in dyes, paints, inks, and plastics; chromates added as anticorrosive agents to paints, primers, and other surface coatings; and chromic acid electroplated onto metal parts to provide a decorative or protective coating. Hexavalent chromium can be formed when performing \"hot work\" such as welding on stainless steel or melting chromium metal. In these situations the chromium is not originally hexavalent, but the high temperatures involved in the process result in oxidation that converts the chromium to a hexavalent state. Hexavalent chromium can also be found in drinking water and public water systems.\n\nInhaled hexavalent chromium is recognized as a human carcinogen. Workers in many occupations are exposed to hexavalent chromium. Problematic exposure is known to occur among workers who handle chromate-containing products and those who grind and/ or weld stainless steel. Workers who are exposed to hexavalent chromium are at increased risk of developing lung cancer, asthma, or damage to the nasal epithelia and skin. Within the European Union, the use of hexavalent chromium in electronic equipment is largely prohibited by the Restriction of Hazardous Substances Directive.\n\nHexavalent chromium compounds are genotoxic carcinogens. Due to its structural similarity to sulfate, chromate (a typical form of chromium(VI) at neutral pH) is transported into cells via sulfate channels. Inside the cell, hexavalent chromium (chromium(VI)) is reduced first to pentavalent chromium (chromium(V)) then to trivalent chromium (chromium(III)) without the aid of any enzymes. The reduction occurs via direct electron transfer primarily from ascorbate and some nonprotein thiols. Vitamin C and other reducing agents combine with chromate to give chromium(III) products inside the cell. The resultant chromium(III) forms stable complexes with nucleic acids and proteins. This causes strand breaks and Cr-DNA adducts which are responsible for mutagenic damage. According to Shi et al., the DNA can also be damaged by hydroxyl radicals produced during reoxidation of pentavalent chromium by hydrogen peroxide molecules present in the cell, which can cause double-strand breakage.\n\nBoth insoluble salts of lead and barium chromates as well as soluble chromates were negative in the implantation model of lung carcinogenesis. Yet, soluble chromates are a confirmed carcinogen so it would be prudent to consider all chromates carcinogenic.\n\nChronic inhalation from occupational exposures increases the risk of respiratory cancers. The most common form of lung malignancies in chromate workers is squamous cell carcinoma. Ingestion of chromium(VI) through drinking water has been found to cause cancer in the oral cavity and small intestine. It can also cause irritation or ulcers in the stomach and intestines, and toxicity in the liver. Liver toxicity shows the body’s apparent inability to detoxify chromium(VI) in the GI tract where it can then enter the circulatory system.\n\nOf 2,345 unsafe products in 2015 listed by the EU Commission for Justice, Consumers and Gender Equality some 64% came from China, and 23% were clothing articles, including leather goods (and shoes) contaminated with hexavalent chromium. Chromate-dyed textiles or chromate-tanned leather shoes can cause skin sensitivity.\n\nIn the U.S., the OSHA PEL for airborne exposures to hexavalent chromium is 5 µg/m (0.005 mg/m). The U.S. National Institute for Occupational Safety and Health proposed a REL of 0.2 µg/m for airborne exposures to hexavalent chromium.\n\nFor drinking water the United States Environmental Protection Agency (EPA) does not have a Maximum Contaminant Level (MCL) for hexavalent chromium. California has finalized a Public Health Goal of 0.02 parts per billion (ppb or micrograms per liter) and established a MCL of 10 ppb.\n\nThere are mainly three types of methods to remediate hexavalent chromium in ground water and drinking water: 1) reduction of toxicity, 2) removal technologies and 3) containment technologies. Reduction of toxicity of hexavalent chromium involves methods using chemicals, microbes and plants. Some removal technologies include transporting contaminated soil offsite to a landfill, using ion exchange resins to reduce chromium(VI) concentrations to less than detectable limit and granular activated carbon (GAC) filter. Containment technologies can be employed with the use of physical barriers such as grouts, slurries or sheet piling. \n\nAttempts have been made to test the removal or reduction of hexavalent chromium from aqueous solutions and environment. For example, a research study conducted by the School of Industrial Technology, University Sains Malaysia in 2010 found that chitosan coated with poly 3-methyl thiophene can be effectively employed for removal of hexavalent chromium ions from aqueous solutions. Chitosan is a very cheap, economical, and environmentally friendly substrate for coating of this polymer. Adsorption of chromium(VI) is found to be effective in the lower pH range and at higher temperatures and subsequent desorption is readily achieved upon alkaline treatment of the adsorbent. Another study done by the American Industrial Hygiene Association indicates that the airborne hexavalent chromium in acidic mists of an electroplating tank collected on PVC filters was reduced over time after mist generation. A number of other emerging technologies for removing chromium from water are also currently under research, including the use of cationic metal-organic frameworks to selectively adsorb chromium oxyanions.\n\nHexavalent chromium is a constituent of tobacco smoke.\n\nHexavalent chromium was released from the Newcastle Orica Kooragang Island ammonium nitrate plant on August 8, 2011. The incident occurred when the plant entered the ‘start up’ phase after the completion of a five-yearly maintenance overhaul. The “High Temperature Shift catalyst began the process of ‘reduction’” where steam passes through the catalyst bed and out the SP8 vent stack. At this time lower temperatures in parts of the plant caused some of the steam to condense lower which caused chromium(VI) from the catalyst bed to dissolve into the liquid present. The amount of condensate overwhelmed the drainage arrangements resulting in the emission of condensate through the SP8 vent stack. The leak went undetected for 30-minutes releasing 200 kg of chromium(VI) into the atmosphere exposing up to 20 workers at the plant and 70 nearby homes in Stockton.\n\nThe town was not notified of the exposure until three days later on Wednesday morning. This accident sparked a major public controversy, with Orica criticized for playing down the extent and possible risks of the leak.\n\nThe office of Environment and Heritage in Stockton collected 71 samples. Low levels of chromium were detected in 11 of them. These 11 samples were taken within six residential blocks close to the Orica plant, two of which were from water samples collected immediately south of the six block area.\n\nThe Select Committee on the Kooragnang Island Orica Chemical Leak released their report on the incident in February 2012. They found Orica’s approach to addressing the leak’s impact was grossly inadequate. Orica failed to realize the potential impact that prevailing winds would have on an emission 60 meters high. Orica failed to inspect the area immediately downwind and notify the Office of Environment and Heritage until August 9, 2011. In Orica’s initial report to the Office of Environment and Heritage they failed to disclose that the emissions had escaped off-site. In the initial report to WorkCover Orica did not disclose potential impacts on workers as well as that the substance emitted was chromium(VI). Orica’s Emergency Response plan was not well understood by employees particularly about notification procedures. The original notification of residents in Stockton was only to households immediately downwind of the emission which failed to realize the potential for contamination of the surrounding area as well. The information presented at the original notification downplayed potential health risks and subsequently provided incomplete information and has led to a lack of trust between Stockton residents and Orica officials.\n\nIn 2014, Orica plead guilty to nine charges before the Land and Environment court and was fined $768,000. NSW Health findings ruled that it is very unlikely that anyone in Stockton will develop cancer as a result of the incident.\n\nToxic poultry feed contaminated by chromium-based leather tanning (as opposed to traditional slower vegetable tanning) is entering the food supply in Bangladesh through chicken meat, which is the most common source of protein in the country. Tanneries in Hazaribagh Thana, an industrial neighborhood of Dhaka, emit around 21,600 cubic meters of toxic waste each day. The tanneries also generate as much as 100 tonnes per day of scraps, trimmed raw hide, flesh and fat, which are processed into feed by neighborhood recycling plants and used in chicken and fish farms across the country. Chromium levels ranging from 350 to 4,520 micrograms (0.35 to 4.52 mg) per kilogram were found in different organs of chickens which had been fed the tannery-scraps feed for two months, according to Abul Hossain, a chemistry professor at the University of Dhaka. The study estimated up to 25% of the chickens in Bangladesh contained harmful levels of chromium(VI).\n\nThe chemistry of the groundwater in eastern Central Greece (central Euboea and the Asopos valley) revealed high concentrations of hexavalent chromium in groundwater systems sometimes exceeding the Greek and the EU drinking water maximum acceptable level for total chromium. Hexavalent chromium pollution here is associated with industrial waste.\n\nBy using the GFAAS for total chromium, diphenylcarbazide-Cr(VI) complex colorimetric method for hexavalent chromium, and flame-AAS and ICP-MS for other toxic elements, their concentrations were investigated in several groundwater samples. The contamination of water by hexavalent chromium in central Euboea is mainly linked to natural processes, but there are anthropogenic cases.\n\nIn the Thebes – Tanagra – Malakasa basin, Eastern Central Greece, which supports many industrial activities, concentrations of chromium (up to 80 μg/L Cr(VI)) and Inofyta (up to 53 μg/L Cr(VI) were found in the urban water supply of Oropos). Chromium(VI) concentrations ranging from 5 to 33 μg/L Cr(VI) were found in groundwater that is used for Thiva's water supply. Arsenic concentrations up to 34 μg/L along with chromium(VI) levels up to 40 μg/L were detected in Schimatari's water supply.\nIn the Asopos River, total chromium values were up to 13 μg/L, hexavalent chromium was less than 5 μg/L and other toxic elements were relatively low.\n\nIn 2008, defense contractor KBR was alleged to have exposed 16 members of the Indiana National Guard, as well as its own workers, to hexavalent chromium at the Qarmat Ali water treatment facility in Iraq in 2003. Later, 433 members of the Oregon National Guard's 162nd Infantry Battalion were informed of possible exposure to hexavalent chromium while escorting KBR contractors.\nOne of the National Guard soldiers, David Moore, died in February 2008. The cause was lung disease at age 42. His death was ruled service-related. His brother believes it was hexavalent chromium. On November 2, 2012, a Portland, Oregon jury found KBR negligent in knowingly exposing twelve National Guard soldiers to hexavalent chromium while working at the Qarmat Ali water treatment facility and awarded damages of $85 million to the plaintiffs.\n\nPrior to 1970, the federal government had limited reach in monitoring and enforcing environmental regulations. Local governments were tasked with environmental monitoring and regulations, such as the monitoring of heavy metals in wastewater. Examples of this can be seen in larger municipalities such Chicago, Los Angeles, and New York. A specific example was in 1969, the Chicago Metropolitan Sanitary District imposed regulations on factories that were identified as having large amounts of heavy metal discharge.\n\nOn December 2, 1970 the Environmental Protection Agency (EPA) was formed. With the formation of the EPA, the federal government had the funds and the oversight to influence major environmental changes. Following the formation of the EPA, the United States saw groundbreaking legislation such as the Clean Water Act (1972) and the Safe Drinking Water Act (1974).\n\nThe Federal Water Pollution Control Act (FWPCA) of 1948 was amended in 1972 to what is more commonly known as the Clean Water Act (CWA). The subsequent amendments provided a basis for the federal government to begin regulating pollutants, implement wastewater standards, and increase funding for water treatment facilities among other things. Two years later, in 1974, the Safe Drinking Water Act (SDWA) was passed by congress. The SDWA aimed to monitor and protect the United States drinking water, and the water sources it is drawn from.\n\nAs part of the SDWA, in 1991, the EPA placed chromium under its list of maximum contaminant level goals (MCLG), to have a maximum contaminant level (MCL) of 100 ppb. In 1996, the SDWA was amended to include a provision known as the Unregulated Contaminant Monitoring Rule (UCMR). Under this rule, the EPA issues a list of 30 or less contaminants that aren’t normally regulated under the SDWA. Chromium was monitored under the third UCMR, from January 2013 through December 2015. The EPA uses data from these reports to assist in making regulatory decisions.\n\nThe current EPA standard in measuring chromium, is in reference to total chromium, both trivalent and hexavalent. However, preliminary findings from water tests conducted by the Environmental Working Group (EWG), suggest that chromium is most often found in the hexavalent form; the more toxic of the two. Often trivalent and hexavalent chromium are mentioned together, when in fact they are very different from each other. This is an issue because hexavalent chromium can cause cancer, whereas trivalent chromium does not.\n\nIn 1991, the MCL for chromium exposure was set based on potential of “adverse dermatological effects” related to long-term chromium exposure. Chromium’s MCL of 100 ppb has not changed since its 1991 recommendation. In 1998 the EPA released a toxicological review of hexavalent chromium. This report examined current literature, at the time, and came to the conclusion that chromium was associated with various health issues.\n\nBefore the EPA can adjust the policy on chromium levels in drinking water, they have to release a final human health assessment. The EPA mentions two specific documents that are currently under review to determine whether or not to adjust the current drinking water standard for chromium. The first study the EPA mentioned that is under review is a 2008 study conducted by the Department of Health and Human Services National Toxicology Program. This study looks at chronic oral exposure of hexavalent chromium in rats, and its association with cancer. The other study mentioned is a human health assessment of chromium, titled Toxicological Review of Hexavalent Chromium. The final human health assessment is currently in the stage of draft development. This stage is the first of seven. The EPA gives no forecast to when the review will be finalized and if a decision will be made.\n\nSince World War II, the U.S. Army relied on hexavalent chromium compounds, called Cr(VI), to protect its vehicles, equipment, aviation and missile systems from corrosion. The wash primer was sprayed as a pretreatment and protective layer on bare metal.\n\nFrom 2012-2015, Army Research Laboratory conducted research on a wash primer replacement, as a part of the DoD’s effort to eliminate the use of toxic wash primers in the military. Studies indicated that Cr(VI) contained hazardous air pollutants and high levels of volatile organic compounds.\n\nThe project resulted in ARL qualifying three wash primer alternatives in 2015 for use on Army depots, installations and repair facilities. The research led to the removal of chromate products from Army facilities in 2017.\n\nARL’s researchers won the fiscal 2016 Secretary of the Army Award for Environmental Excellence in Weapon System Acquisition for their efforts on the wash primer replacement.\n\nIn 2010, the Environmental Working Group studied the drinking water in 35 American cities. The study was the first nationwide analysis measuring the presence of the chemical in U.S. water systems. The study found measurable hexavalent chromium in the tap water of 31 of the cities sampled, with Norman, Oklahoma, at the top of list; 25 cities had levels that exceeded California's proposed limits. The EPA currently limits total chromium in drinking water to 100 parts per billion, but there is no established specifically for chromium(VI). In the same year, the California Environmental Protection Agency had proposed a goal of 0.2 parts per billion, despite a 2001 state law requiring a standard be set by 2005. A final Public Health Goal of 0.02 ppb was established in July 2011.\n\nAs of August 2016, the EWG alongside activist Erin Brockovich is pushing for the EPA to change the nationwide MCL to be more reflective of the California standards. This response comes as result of the resignation of a top state health official in North Carolina. Governor Pat McCrory had been attempting to remove warnings against drinking water potentially contaminated by hexavalent chromium from Duke Energy coal burning facilities. The confusion on the regulating guidelines and health safety of hexavalent chromium within drinking water has led to multiple standards. The EWG's push for stricter national guidelines is done in hopes to eliminate the confusion among citizens and state officials as well as to protect the health of the population from this carcinogen.\n\nThe EWG released an analysis in September 2016 of federal data from nationwide drinking water tests which showed that seventy-five percent of samples contained hexavalent chromium at levels higher than the California public health goal of 0.02 ppb. The EWG estimates that the US will see 12,000 excess cases in cancer by the end of the century if hexavalent chromium exposure in drinking water is not reduced.\n\nMonterey Bay Unified Air Pollution Control District monitored airborne levels of hexavalent chromium at an elementary school and fire department, as well as the point-source. They concluded that there were high levels of hexavalent chromium in the air, originating from a local cement plant, called Cemex. The levels of hexavalent chromium were 8 to 10 times higher than the air district's acceptable level at Pacific Elementary School and the Davenport Fire Department. The County of Santa Cruz sought help of the Health Services Agency (HSA) to investigate the findings of the Air District's report. Cemex voluntarily ceased operations due to the growing concern within the community, while additional air samples were analyzed. The HSA worked with Cemex to implement engineering controls, such as dust scavenging systems and other dust mitigation procedures. Cemex also made a change in the materials they used, trying to replace current materials with materials lower in chromium. The HSA also monitored the surrounding schools to determine if there were any health risks. Most schools came back with low levels, but in the case of higher levels a contractor was hired to clean up the chromium deposits. This case highlights the previously unrecognized possibility that hexavalent chromium can be released from cement-making.\n\nAir quality officials are taking enforcement action against two metal-processing plants they believe are contributing to hexavalent chromium emissions in Paramount, California.\n\nHexavalent chromium was found in drinking water in the southern California town of Hinkley and was brought to popular attention by the involvement of Erin Brockovich and Attorney Edward Masry. The source of contamination was from the evaporating ponds of a PG&E (Pacific Gas and Electric) natural gas pipeline Compressor Station located approximately 2 miles southeast of Hinkley. Between 1952 and 1966, chromium(VI) was used to prevent corrosion in the cooling stacks. The wastewater was then dumped into the unlined evaporating ponds. As a result, the chromium(VI) leaked into the groundwater source. The 580 ppb chromium(VI) in the groundwater in Hinkley exceeded the Maximum Contaminant Level (MCL) of 100 ppb for total chromium currently set by the United States Environmental Protection Agency (EPA). It also exceeded the California MCL of 50 ppb (as of November 2008) for all types of chromium. California first established an MCL specifically for hexavalent chromium in 2014, set at 10 ppb. Prior to that only total chromium standards applied.\n\nA more recent study found that from 1996 to 2008, 196 cancers were identified among residents of the census tract that included Hinkley — a slightly lower number than the 224 cancers that would have been expected given its demographic characteristics. In June 2013 \"Mother Jones\" published an article regarding work by the Center for Public Integrity that was critical of the study, and some others by the same researcher, John Morgan. This comes in contrast with the conclusions reached by the EPA and California’s Department of Public Health that chromium(VI) does in fact cause cancer.\n\nAt the time that a PG&E background study of chromium(VI) was conducted, average chromium(VI) levels in Hinkley were recorded as 1.19 ppb with a peak of 3.09 ppb. The PG&E Topock Compressor Station averaged 7.8ppb and peaked at 31.8ppb. The California MCL standard was still at 50 ppb at the completion of this background study. In comparison, the Office of Environmental Health Hazard Assessment (OEHHA) of the California EPA, proposed a health goal of 0.06 ppb of chromium(VI) in drinking water in 2009. In 2010, Erin Brockovich returned to Hinkley in the midst of claims that the plume was spreading, despite PG&E cleanup efforts. PG&E continues to provide bottled water for the residents of Hinkley as well as offer to buy their homes. All other ongoing cleanup documentation is maintained at California EPA's page.\n\nIn Chicago's first ever testing for the toxic metal contaminant, results show that the city's local drinking water contains levels of hexavalent chromium more than 11 times higher than the health standard set in California in July 2011. The results of the test showed that the water which is sent to over 7 million residents had average levels of 0.23 ppb of the toxic metal. California's Office of Environmental Health Hazard Assessment designated the nation's new \"public health goal\" limit as 0.02 ppb. Echoing their counterparts in other cities where the metal has been detected, Chicago officials stress that local tap water is safe and suggest that if a national limit is adopted, it likely would be less stringent than California's goal. The Illinois Environmental Protection Agency (Illinois EPA) has developed a chromium(VI) strategic plan which outlines tasks in order to reduce the levels of chromium(VI) in Illinois' drinking water. One of which is to work with the U.S. EPA to provide significant technical assistance to the City of Chicago to ensure they quickly develop an effective chromium(VI) specific monitoring program that makes use of the U.S. EPA-approved methods.\n\nCambridge Plating Company, now known as Purecoat North, was an electroplating business in Belmont, Massachusetts. A report was conducted by the Agency for Toxic Substances and Disease Registry (ATSDR), to evaluate the association between environmental exposures from the Cambridge Plating Company and health effects on the surrounding community. The report indicated that residents of Belmont were exposed to chromium via air emissions, as well as groundwater and soil. However, six types of cancer were evaluated, and the incidence was actually found to be average, in most cases, across all types, if not a little bit lower than average. For example, in kidney cancer the number of observed cases was 7 versus an expected 16. While that was the case for most diseases, it was not for all. The incidence of leukemia among females was elevated in Belmont, MA during 1982–1999 (32 diagnoses observed vs. 23.2 expected). Elevations in females were due to four excess cases in each time period (11 diagnoses observed vs. 6.9 expected during 1988–1993; 13 diagnoses observed vs. 8.7 expected during 1994–1999) while elevations among males were based on one to three excess cases. ATSDR deemed Cambridge Plating as an Indeterminate Public Health Hazard in the past, but No Apparent Public Health Hazard in the present or future.\n\nIn 2009, a lawsuit was filed against Prime Tanning Corporation of St. Joseph, Missouri, over alleged hexavalent chromium contamination in Cameron, Missouri. A cluster of brain tumors had developed in the town that was above average for the population size of the town. The lawsuit alleges that the tumors were caused by waste hexavalent chromium that had been distributed to local farmers as free fertilizer. In 2010 a government study found hexavalent chromium within the soil but not at levels that were hazardous to human health. In 2012, the case ruled that $10 million would be distributed to over a dozen farmers affected in the northwest Missouri area. The Tanning Corporation still denies that their fertilizer caused any harm. Some residents claim that the tumors were a direct cause from the chromium exposure, but it is difficult to determine what other future impacts might arise from exposure in the specific Missouri counties.\n\nOn April 8, 2009, the Texas Commission on Environmental Quality (TCEQ) collected ground water samples from a domestic well on West County Road 112 in Midland, Texas (U.S.), in response to a resident complaint of yellow water. The well was found to be contaminated with chromium(VI). The Midland groundwater reached higher levels of contamination than the EPA mandated maximum contaminant level (MCL) of 100 parts per billion. The current groundwater plume of chromium lies under approximately 260 acres of land at the West County Road 112 Groundwater Site. In response, the TCEQ installed filtration systems on water-well sites that showed contamination of chromium.\n\nAs of 2016, TCEQ had sampled water from 235 wells and has installed over 45 anion-exchange filtration systems from this site determined to be centered at 2601 West County Road 112, Midland, Texas. The TCEQ continues to sample wells surrounding the area to monitor the movement of the plume. In addition, they continue to monitor the effectiveness of the anion-exchange filtration systems by sampling on a year-quarterly and the filters are maintained at no cost to the residents.\n\nAs of March 2011, the West County Road 112 Ground Water site was added to the National Priorities List (NPL) also known as the Superfund List by the U.S. Environmental Protection Agency (EPA). From 2011 to 2013, TCEQ installed groundwater monitors and conducted groundwater sampling. In 2013, TCEQ began sampling residential soil and confirmed that it was contaminated from use of the contaminated groundwater for garden and lawn care.\n\nAccording to the EPA, ongoing investigations have not concluded the source of the contamination and cleanup solutions are still being developed. Until such investigations are complete and remediation established, residents will continue to be at risk for health effects from exposure to the groundwater contamination.\n\nOn January 7, 2011 it was announced that Milwaukee, Wisconsin had tested its water and hexavalent chromium was found to be present. Officials stated that it was in such small quantities that it was nothing to worry about, although this contaminant is a carcinogen. In Wisconsin, Milwaukee's average chromium(VI) level is 0.194 parts per billion (the EPA recommended maximum contaminant level (MCL) is 100 ppb). All 13 water systems tested positive for chromium(VI). Four out of seven systems detected the chemical in Waukesha County, and both Racine and Kenosha Counties had the highest levels averaging more than 0.2 parts per billion. Further testing was being conducted as of 2011. There was no further information available as of October 2016.\n\n\n"}
{"id": "147918", "url": "https://en.wikipedia.org/wiki?curid=147918", "title": "Industrial robot", "text": "Industrial robot\n\nAn industrial robot is a robot system used for manufacturing. Industrial robots are automated, programmable and capable of movement on two or more axes.\n\nTypical applications of robots include welding, painting, assembly, pick and place for printed circuit boards, packaging and labeling, palletizing, product inspection, and testing; all accomplished with high endurance, speed, and precision. They can assist in material handling.\n\nIn the year 2015, an estimated 1.64 million industrial robots were in operation worldwide according to International Federation of Robotics (IFR).\n\nThe most commonly used robot configurations are articulated robots, SCARA robots, delta robots and cartesian coordinate robots, (gantry robots or x-y-z robots). In the context of general robotics, most types of robots would fall into the category of robotic arms (inherent in the use of the word \"manipulator\" in ISO standard 1738).\nRobots exhibit varying degrees of autonomy: \n\nThe earliest known industrial robot, conforming to the ISO definition was completed by \n\"Bill\" Griffith P. Taylor in 1937 and published in Meccano Magazine, March 1938. The crane-like device was built almost entirely using Meccano parts, and powered by a single electric motor. Five axes of movement were possible, including \"grab\" and \"grab rotation\". Automation was achieved using punched paper tape to energise solenoids, which would facilitate the movement of the crane's control levers. The robot could stack wooden blocks in pre-programmed patterns. The number of motor revolutions required for each desired movement was first plotted on graph paper. This information was then transferred to the paper tape, which was also driven by the robot's single motor. Chris Shute built a complete replica of the robot in 1997.\nGeorge Devol applied for the first robotics patents in 1954 (granted in 1961). The first company to produce a robot was Unimation, founded by Devol and Joseph F. Engelberger in 1956. Unimation robots were also called \"programmable transfer machines\" since their main use at first was to transfer objects from one point to another, less than a dozen feet or so apart. They used hydraulic actuators and were programmed in \"joint coordinates\", i.e. the angles of the various joints were stored during a teaching phase and replayed in operation. They were accurate to within 1/10,000 of an inch (note: although accuracy is not an appropriate measure for robots, usually evaluated in terms of repeatability - see later). Unimation later licensed their technology to Kawasaki Heavy Industries and GKN, manufacturing Unimates in Japan and England respectively. For some time Unimation's only competitor was Cincinnati Milacron Inc. of Ohio. This changed radically in the late 1970s when several big Japanese conglomerates began producing similar industrial robots.\n\nIn 1969 Victor Scheinman at Stanford University invented the Stanford arm, an all-electric, 6-axis articulated robot designed to permit an arm solution. This allowed it accurately to follow arbitrary paths in space and widened the potential use of the robot to more sophisticated applications such as assembly and welding. Scheinman then designed a second arm for the MIT AI Lab, called the \"MIT arm.\" Scheinman, after receiving a fellowship from Unimation to develop his designs, sold those designs to Unimation who further developed them with support from General Motors and later marketed it as the Programmable Universal Machine for Assembly (PUMA).\n\nIndustrial robotics took off quite quickly in Europe, with both ABB Robotics and KUKA Robotics bringing robots to the market in 1973. ABB Robotics (formerly ASEA) introduced IRB 6, among the world's first \"commercially available\" all electric micro-processor controlled robot. The first two IRB 6 robots were sold to Magnusson in Sweden for grinding and polishing pipe bends and were installed in production in January 1974. Also in 1973 KUKA Robotics built its first robot, known as FAMULUS, also one of the first articulated robots to have six electromechanically driven axes.\n\nInterest in robotics increased in the late 1970s and many US companies entered the field, including large firms like General Electric, and General Motors (which formed joint venture FANUC Robotics with FANUC LTD of Japan). U.S. startup companies included Automatix and Adept Technology, Inc. At the height of the robot boom in 1984, Unimation was acquired by Westinghouse Electric Corporation for 107 million U.S. dollars. Westinghouse sold Unimation to Stäubli Faverges SCA of France in 1988, which is still making articulated robots for general industrial and cleanroom applications and even bought the robotic division of Bosch in late 2004.\n\nOnly a few non-Japanese companies ultimately managed to survive in this market, the major ones being: Adept Technology, Stäub, the Swedish-Swiss company ABB Asea Brown Boveri, the German company KUKA Robotics and the Italian company Comau.\n\nAccuracy and repeatability are different measures. Repeatability is usually the most important criterion for a robot and is similar to the concept of 'precision' in measurement—see accuracy and precision. ISO 9283 sets out a method whereby both accuracy and repeatability can be measured. Typically a robot is sent to a taught position a number of times and the error is measured at each return to the position after visiting 4 other positions. Repeatability is then quantified using the standard deviation of those samples in all three dimensions. A typical robot can, of course make a positional error exceeding that and that could be a problem for the process. Moreover, the repeatability is different in different parts of the working envelope and also changes with speed and payload. ISO 9283 specifies that accuracy and repeatability should be measured at maximum speed and at maximum payload. But this results in pessimistic values whereas the robot could be much more accurate and repeatable at light loads and speeds.\nRepeatability in an industrial process is also subject to the accuracy of the end effector, for example a gripper, and even to the design of the 'fingers' that match the gripper to the object being grasped. For example, if a robot picks a screw by its head, the screw could be at a random angle. A subsequent attempt to insert the screw into a hole could easily fail. These and similar scenarios can be improved with 'lead-ins' e.g. by making the entrance to the hole tapered.\n\nThe setup or programming of motions and sequences for an industrial robot is typically taught by linking the robot controller to a laptop, desktop computer or (internal or Internet) network.\n\nA robot and a collection of machines or peripherals is referred to as a workcell, or cell. A typical cell might contain a parts feeder, a molding machine and a robot. The various machines are 'integrated' and controlled by a single computer or PLC. How the robot interacts with other machines in the cell must be programmed, both with regard to their positions in the cell and synchronizing with them.\n\n\"Software:\" The computer is installed with corresponding interface software. The use of a computer greatly simplifies the programming process. Specialized robot software is run either in the robot controller or in the computer or both depending on the system design.\n\nThere are two basic entities that need to be taught (or programmed): positional data and procedure. For example, in a task to move a screw from a feeder to a hole the positions of the feeder and the hole must first be taught or programmed. Secondly the procedure to get the screw from the feeder to the hole must be programmed along with any I/O involved, for example a signal to indicate when the screw is in the feeder ready to be picked up. The purpose of the robot software is to facilitate both these programming tasks.\n\nTeaching the robot positions may be achieved a number of ways:\n\n\"Positional commands\" The robot can be directed to the required position using a GUI or text based commands in which the required X-Y-Z position may be specified and edited.\n\n\"Teach pendant:\" Robot positions can be taught via a teach pendant. This is a handheld control and programming unit. The common features of such units are the ability to manually send the robot to a desired position, or \"inch\" or \"jog\" to adjust a position. They also have a means to change the speed since a low speed is usually required for careful positioning, or while test-running through a new or modified routine. A large emergency stop button is usually included as well. Typically once the robot has been programmed there is no more use for the teach pendant.\n\n\"Lead-by-the-nose:\" this is a technique offered by many robot manufacturers. In this method, one user holds the robot's manipulator, while another person enters a command which de-energizes the robot causing it to go into limp. The user then moves the robot by hand to the required positions and/or along a required path while the software logs these positions into memory. The program can later run the robot to these positions or along the taught path. This technique is popular for tasks such as paint spraying.\n\n\"Offline programming\" is where the entire cell, the robot and all the machines or instruments in the workspace are mapped graphically. The robot can then be moved on screen and the process simulated. A robotics simulator is used to create embedded applications for a robot, without depending on the physical operation of the robot arm and end effector. The advantages of robotics simulation is that it saves time in the design of robotics applications. It can also increase the level of safety associated with robotic equipment since various \"what if\" scenarios can be tried and tested before the system is activated.[8] Robot simulation software provides a platform to teach, test, run, and debug programs that have been written in a variety of programming languages. \n\n\"Robot simulation\" tools allow for robotics programs to be conveniently written and debugged off-line with the final version of the program tested on an actual robot. The ability to preview the behavior of a robotic system in a virtual world allows for a variety of mechanisms, devices, configurations and controllers to be tried and tested before being applied to a \"real world\" system. Robotics simulators have the ability to provide real-time computing of the simulated motion of an industrial robot using both geometric modeling and kinematics modeling.\n\n\"Others\" In addition, machine operators often use user interface devices, typically touchscreen units, which serve as the operator control panel. The operator can switch from program to program, make adjustments within a program and also operate a host of peripheral devices that may be integrated within the same robotic system. These include end effectors, feeders that supply components to the robot, conveyor belts, emergency stop controls, machine vision systems, safety interlock systems, bar code printers and an almost infinite array of other industrial devices which are accessed and controlled via the operator control panel.\n\nThe teach pendant or PC is usually disconnected after programming and the robot then runs on the program that has been installed in its controller. However a computer is often used to 'supervise' the robot and any peripherals, or to provide additional storage for access to numerous complex paths and routines.\n\nThe most essential robot peripheral is the end effector, or end-of-arm-tooling (EOT). Common examples of end effectors include welding devices (such as MIG-welding guns, spot-welders, etc.), spray guns and also grinding and deburring devices (such as pneumatic disk or belt grinders, burrs, etc.), and grippers (devices that can grasp an object, usually electromechanical or pneumatic). Other common means of picking up objects is by vacuum or magnets. End effectors are frequently highly complex, made to match the handled product and often capable of picking up an array of products at one time. They may utilize various sensors to aid the robot system in locating, handling, and positioning products.\n\nFor a given robot the only parameters necessary to completely locate the end effector (gripper, welding torch, etc.) of the robot are the angles of each of the joints or displacements of the linear axes (or combinations of the two for robot formats such as SCARA). However, there are many different ways to define the points. The most common and most convenient way of defining a point is to specify a Cartesian coordinate for it, i.e. the position of the 'end effector' in mm in the X, Y and Z directions relative to the robot's origin. In addition, depending on the types of joints a particular robot may have, the orientation of the end effector in yaw, pitch, and roll and the location of the tool point relative to the robot's faceplate must also be specified. For a jointed arm these coordinates must be converted to joint angles by the robot controller and such conversions are known as Cartesian Transformations which may need to be performed iteratively or recursively for a multiple axis robot. The mathematics of the relationship between joint angles and actual spatial coordinates is called kinematics. See robot control\n\nPositioning by Cartesian coordinates may be done by entering the coordinates into the system or by using a teach pendant which moves the robot in X-Y-Z directions. It is much easier for a human operator to visualize motions up/down, left/right, etc. than to move each joint one at a time. When the desired position is reached it is then defined in some way particular to the robot software in use, e.g. P1 - P5 below.\n\nMost articulated robots perform by storing a series of positions in memory, and moving to them at various times in their programming sequence. For example, a robot which is moving items from one place to another might have a simple 'pick and place' program similar to the following:\n\n\"Define points P1–P5:\"\n\n\n\"Define program:\"\n\n\nFor examples of how this would look in popular robot languages see industrial robot programming.\n\nThe American National Standard for Industrial Robots and Robot Systems — Safety Requirements (ANSI/RIA R15.06-1999) defines a singularity as “a condition caused by the collinear alignment of two or more robot axes resulting in unpredictable robot motion and velocities.” It is most common in robot arms that utilize a “triple-roll wrist”. This is a wrist about which the three axes of the wrist, controlling yaw, pitch, and roll, all pass through a common point. An example of a wrist singularity is when the path through which the robot is traveling causes the first and third axes of the robot’s wrist (i.e. robot's axes 4 and 6) to line up. The second wrist axis then attempts to spin 180° in zero time to maintain the orientation of the end effector. Another common term for this singularity is a “wrist flip”. The result of a singularity can be quite dramatic and can have adverse effects on the robot arm, the end effector, and the process. Some industrial robot manufacturers have attempted to side-step the situation by slightly altering the robot’s path to prevent this condition. Another method is to slow the robot’s travel speed, thus reducing the speed required for the wrist to make the transition. The ANSI/RIA has mandated that robot manufacturers shall make the user aware of singularities if they occur while the system is being manually manipulated.\n\nA second type of singularity in wrist-partitioned vertically articulated six-axis robots occurs when the wrist center lies on a cylinder that is centered about axis 1 and with radius equal to the distance between axes 1 and 4. This is called a shoulder singularity. Some robot manufacturers also mention alignment singularities, where axes 1 and 6 become coincident. This is simply a sub-case of shoulder singularities. When the robot passes close to a shoulder singularity, joint 1 spins very fast.\n\nThe third and last type of singularity in wrist-partitioned vertically articulated six-axis robots occurs when the wrist's center lies in the same plane as axes 2 and 3.\n\nSingularities are closely related to the phenomena of gimbal lock, which has a similar root cause of axes becoming lined up.\n\nA video illustrating these three types of singular configurations is available here.\n\nAccording to the International Federation of Robotics (IFR) study \"World Robotics 2018\", there were about 2,097,500 operational industrial robots by the end of 2017. This number is estimated to reach 3,788,000 by the end of 2021. For the year 2017 the IFR estimates the worldwide sales of industrial robots with US$16.2 billion. Including the cost of software, peripherals and systems engineering, the annual turnover for robot systems is estimated to be US$48.0 billion in 2017.\n\nChina is the largest industrial robot market, with 137,900 units sold in 2017. Japan had the largest operational stock of industrial robots, with 286,554 at the end of 2015. The biggest customer of industrial robots is automotive industry with 33% market share, then electrical/electronics industry with 32%, metal and machinery industry with 12%, rubber and plastics industry with 5%, food industry with 3%. In textiles, apparel and leather industry, 1,580 units are operational.\n\nEstimated worldwide annual supply of industrial robots (in units):\nThe International Federation of Robotics has predicted a worldwide increase in adoption of industrial robots and they estimated 1.7 million new robot installations in factories worldwide by 2020 [IFR 2017]. Rapid advances in automation technologies (e.g. fixed robots, collaborative and mobile robots, and exoskeletons) have the potential to improve work conditions but also to introduce workplace hazards in manufacturing workplaces. Despite the lack of occupational surveillance data on injuries associated specifically with robots, researchers from the US National Institute for Occupational Safety and Health (NIOSH) identified 61 robot-related deaths between 1992 and 2015 using keyword searches of the Bureau of Labor Statistics (BLS) Census of Fatal Occupational Injuries research database (see info from Center for Occupational Robotics Research). Using data from the Bureau of Labor Statistics, NIOSH and its state partners have investigated 4 robot-related fatalities under the Fatality Assessment and Control Evaluation Program. In addition the Occupational Safety and Health Administration (OSHA) has investigated dozens of robot-related deaths and injuries, which can be reviewed at OSHA Accident Search page. Injuries and fatalities could increase over time because of the increasing number of collaborative and co-existing robots, powered exoskeletons, and autonomous vehicles into the work environment.\n\nSafety standards are being developed by the Robotic Industries Association (RIA) in conjunction with the American National Standards Institute (ANSI). On October 5, 2017, OSHA, NIOSH and RIA signed an alliance to work together to enhance technical expertise, identify and help address potential workplace hazards associated with traditional industrial robots and the emerging technology of human-robot collaboration installations and systems, and help identify needed research to reduce workplace hazards. On October 16 NIOSH launched the Center for Occupational Robotics Research to \"provide scientific leadership to guide the development and use of occupational robots that enhance worker safety, health, and wellbeing.\" So far, the research needs identified by NIOSH and its partners include: tracking and preventing injuries and fatalities, intervention and dissemination strategies to promote safe machine control and maintenance procedures, and on translating effective evidence-based interventions into workplace practice.\n\n\n\n"}
{"id": "71617", "url": "https://en.wikipedia.org/wiki?curid=71617", "title": "Infant mortality", "text": "Infant mortality\n\nInfant mortality is the death of young children under the age of 1. This death toll is measured by the infant mortality rate (IMR), which is the number of deaths of children under one year of age per 1000 live births. The under-five mortality rate is also an important statistic, considering the infant mortality rate focuses only on children under one year of age. \n\nPremature birth is the biggest contributor to the IMR. Other leading causes of infant mortality are birth asphyxia, pneumonia, congenital malformations, term birth complications such as abnormal presentation of the foetus umbilical cord prolapse, or prolonged labor, neonatal infection, diarrhea, malaria, measles and malnutrition. One of the most common preventable causes of infant mortality is smoking during pregnancy. Many factors contribute to infant mortality, such as the mother's level of education, environmental conditions, and political and medical infrastructure. Improving sanitation, access to clean drinking water, immunization against infectious diseases, and other public health measures can help reduce high rates of infant mortality.\n\nChild mortality is the death of a child before the child's fifth birthday, measured as the under-5 child mortality rate (U5MR). National statistics sometimes group these two mortality rates together. Globally, 9.2 million children die each year before their fifth birthday; more than 60% of these deaths are seen as being avoidable with low-cost measures such as continuous breast-feeding, vaccinations and improved nutrition.\n\nInfant mortality rate was an indicator used to monitor progress towards the Fourth Goal of the Millennium Development Goals of the United Nations for the year 2015. It is now a target in the Sustainable Development Goals for Goal Number 3 (\"Ensure healthy lives and promote well-being for all at all ages\").\n\nThroughout the world, infant mortality rate (IMR) fluctuates drastically, and according to Biotechnology and Health Sciences, education and life expectancy in the country is the leading indicator of IMR. This study was conducted across 135 countries over the course of 11 years, with the continent of Africa having the highest infant mortality rate of any other region studied with 68 deaths per 1,000 live births.\n\nInfant mortality rate (IMR) is the number of deaths per 1,000 live births of children under one year of age. The rate for a given region is the number of children dying under one year of age, divided by the number of live births during the year, multiplied by 1,000.\n\nForms of infant mortality:\n\nCauses of infant mortality directly lead to the death. Environmental and social barriers prevent access to basic medical resources and thus contribute to an increasing infant mortality rate; 99% of infant deaths occur in developing countries, and 86% of these deaths are due to infections, premature births, complications during delivery, and perinatal asphyxia and birth injuries. Greatest percentage reduction of infant mortality occurs in countries that already have low rates of infant mortality.\nCommon causes are preventable with low-cost measures. In the United States, a primary determinant of infant mortality risk is infant birth weight with lower birth weights increasing the risk of infant mortality. The determinants of low birth weight include socio-economic, psychological, behavioral and environmental factors.\n\nCauses of infant mortality that are related to medical conditions include: low birth weight, sudden infant death syndrome, malnutrition,congenital malformations, and infectious diseases, including neglected tropical diseases.\n\nCongenital malformations are birth defects that babies are born with, such as cleft lip and palate, Down syndrome, and heart defects. Often times, this occurs when the mother consumes alcohol, but it can also be a cause of genetics or have an unknown cause. \ncongenital malformations have had a significant impact on infant mortality. Malnutrition and infectious diseases were the main cause of death in more undeveloped countries. In the Caribbean and Latin America, congenital malformations only accounted for 5% of the infant deaths in these countries while malnutrition and infectious diseases took 7% to 27% of infants in the 1980s. In more developed countries such as the United States, there was a rise in infant deaths due to congenital malformations. These birth defects mostly had to do with heart and central nervous system. In the 19th century, there was a decrease in the number of infant deaths from heart diseases. From 1979 to 1997, there was a 39% decline in infant mortality due to heart problems.\n\nLow birth weight makes up 60–80% of the infant mortality rate in developing countries. \"The New England Journal of Medicine\" stated that \"The lowest mortality rates occur among infants weighing . For infants born weighing or less, the mortality rate rapidly increases with decreasing weight, and most of the infants weighing or less die. As compared with normal-birth-weight infants, those with low weight at birth are almost 40 times more likely to die in the neonatal period; for infants with very low weight at birth the relative risk of neonatal death is almost 200 times greater.\" Infant mortality due to low birth weight is usually a direct cause stemming from other medical complications such as preterm birth, poor maternal nutritional status, lack of prenatal care, maternal sickness during pregnancy, and an unhygienic home environments. Along with birth weight, period of gestation makes up the two most important predictors of an infant's chances of survival and their overall health.\n\nAccording to the \"New England Journal of Medicine\", \"in the past two decades, the infant mortality rate (deaths under one year of age per thousand live births) in the United States has declined sharply.\" Low birth weights from African American mothers remain twice as high as that of white women. LBW may be the leading cause of infant deaths, and it is greatly preventable. Although it is preventable, the solutions may not be the easiest but effective programs to help prevent LBW are a combination of health care, education, environment, mental modification and public policy, influencing a culture supporting lifestyle. Preterm birth is the leading cause of newborn deaths worldwide. Even though America excels past many other countries in the care and saving of premature infants, the percentage of American woman who deliver prematurely is comparable to those in developing countries. Reasons for this include teenage pregnancy, increase in pregnant mothers over the age of thirty-five, increase in the use of in-vitro fertilization which increases the risk of multiple births, obesity and diabetes. Also, women who do not have access to health care are less likely to visit a doctor, therefore increasing their risk of delivering prematurely.\n\nSudden infant death syndrome(SIDS) is a syndrome where an infant dies in their sleep with no reason behind it. Even with a complete autopsy, no one has been able to figure out what causes this disease. This disease is more common in Western countries. Even though researchers are not sure what causes this disease, they have discovered that it is healthier for babies to sleep on their backs instead of their stomachs. This discovery saved many families from the tragedy that this disease causes. Scientists have also discovered three causes within a model they created called, the contemporary triple risk model. This model states that three conditions such as the mother smoking while pregnant, the age of the infant, and stress referring to conditions such as overheating, prone sleeping, co-sleeping, and head covering.\n\nMalnutrition or undernutrition is defined as inadequate intake of nourishment, such as proteins and vitamins, which adversely affects the growth, energy and development of people all over the world. It is especially prevalent in women and infants under 5 who live in developing countries within the poorer regions of Africa, Asia, and Latin America. Children are most vulnerable as they are yet to fully develop a strong immune system, as well as being dependent upon parents to provide the necessary food and nutritional intake. It is estimated that about 3.5 million children die each year as a result of childhood or maternal malnutrition, with stunted growth, low body weight and low birth weight accounting for about 2.2 million associated deaths. Factors which contribute to malnutrition are socioeconomic, environmental, gender status, regional location, and breastfeeding cultural practices. It is difficult to assess the most pressing factor as they can intertwine and vary among regions. \n\nChildren suffering from malnutrition face adverse physical effects such as stunting, wasting, or being overweight. Such characteristics entail difference in weight-and-height ratios for age in comparison to adequate standards. In Africa the number of stunted children has risen, while Asia holds the most children under 5 suffering from wasting. The amount of overweight children has increased among all regions of the globe. Inadequate nutrients adversely effect physical and cognitive developments, increasing susceptibility to severe health problems. Micronutrient deficiency such as iron has been linked to children with anemia, fatigue, and poor brain development. Similarly, the lack of Vitamin A is the leading cause of blindness among malnourished children. The outcome of malnutrition in children results in decreased ability of the immune system to fight infections, resulting in higher rates of death from diseases such as malaria, respiratory disease and diarrhea.\n\nBabies born in low to middle income countries in sub-Saharan Africa and southern Asia are at the highest risk of neonatal death. Bacterial infections of the bloodstream, lungs, and the brain's covering (meningitis) are responsible for 25% of neonatal deaths. Newborns can acquire infections during birth from bacteria that are present in their mother's reproductive tract. The mother may not be aware of the infection, or she may have an untreated pelvic inflammatory disease or sexually transmitted disease. These bacteria can move up the vaginal canal into the amniotic sac surrounding the baby. Maternal blood-borne infection is another route of bacterial infection from mother to baby. Neonatal infection is also more likely with the premature rupture of the membranes (PROM) of the amniotic sac.\n\nSeven out of ten childhood deaths are due to infectious diseases: acute respiratory infection, diarrhea, measles, and malaria. Acute respiratory infection such as pneumonia, bronchitis, and bronchiolitis account for 30% of childhood deaths; 95% of pneumonia cases occur in the developing world. Diarrhea is the second-largest cause of childhood mortality in the world, while malaria causes 11% of childhood deaths. Measles is the fifth-largest cause of childhood mortality. Folic acid for mothers is one way to combat iron deficiency. A few public health measures used to lower levels of iron deficiency anemia include iodize salt or drinking water, and include vitamin A and multivitamin supplements into a mother's diet. A deficiency of this vitamin causes certain types of anemia (low red blood cell count).\n\nInfant mortality rate can be a measure of a nation's health and social condition. It is a composite of a number of component rates which have their separate relationship with various social factors and can often be seen as an indicator to measure the level of socioeconomic disparity within a country.\n\nOrganic water pollution is a better indicator of infant mortality than health expenditures per capita. Water contaminated with various pathogens houses a host of parasitic and microbial infections. Infectious disease and parasites are carried via water pollution from animal wastes. Areas of low socioeconomic status are more prone to inadequate plumbing infrastructure, and poorly maintained facilities. The burning of inefficient fuels doubles the rate of children under 5 years old with acute respiratory tract infections. Climate and geography often play a role in sanitation conditions. For example, the inaccessibility of clean water exacerbates poor sanitation conditions.\n\nPeople who live in areas where particulate matter (PM) air pollution is higher tend to have more health problems across the board. Short-term and long-term effects of ambient air pollution are associated with an increased mortality rate, including infant mortality. Air pollution is consistently associated with post neonatal mortality due to respiratory effects and sudden infant death syndrome. Specifically, air pollution is highly associated with SIDs in the United States during the post-neonatal stage. High infant mortality is exacerbated because newborns are a vulnerable subgroup that is affected by air pollution. Newborns who were born into these environments are no exception. Women who are exposed to greater air pollution on a daily basis who are pregnant should be closely watched by their doctors, as well as after the baby is born. Babies who live in areas with less air pollution have a greater chance of living until their first birthday. As expected, babies who live in environments with more air pollution are at greater risk for infant mortality. Areas that have higher air pollution also have a greater chance of having a higher population density, higher crime rates and lower income levels, all of which can lead to higher infant mortality rates.\n\nThe key pollutant for infant mortality rates is carbon monoxide. Carbon monoxide is a colorless, odorless gas that does great harm especially to infants because of their immature respiratory system.\nAnother major pollutant is second-hand smoke, which is a pollutant that can have detrimental effects on a fetus. According to the \"American Journal of Public Health\", \"in 2006, more than 42 000 Americans died of second hand smoke-attributable diseases, including more than 41 000 adults and nearly 900 infants ... fully 36% of the infants who died of low birth weight caused by exposure to maternal smoking in utero were Blacks, as were 28% of those dying of respiratory distress syndrome, 25% dying of other respiratory conditions, and 24% dying of sudden infant death syndrome.\" \"The American Journal of Epidemiology\" also stated that \"Compared with nonsmoking women having their first birth, women who smoked less than one pack of cigarettes per day had a 25% greater risk of mortality, and those who smoked one or more packs per day had a 56% greater risk. Among women having their second or higher birth, smokers experienced 30% greater mortality than nonsmokers.\"\n\nModern research in the United States on racial disparities in infant mortality suggests a link between the institutionalized racism that pervades the environment and high rates of African American infant mortality. In synthesis of this research, it has been observed that \"African American infant mortality remains elevated due to the social arrangements that exist between groups and the lifelong experiences responding to the resultant power dynamics of these arrangements.\"\n\nIt is important to note that infant mortality rates do not decline among African Americans even if their socio-economic status does improve. Parker Dominguez at the University of Southern California has made some headway in determining the reasoning behind this, claiming black women are more prone to psychological stress than other women of different races in the United States. Stress is a lead factor in inducing labor in pregnant women, and therefore high levels of stress during pregnancy could lead to premature births that have the potential to be fatal for the infant.\n\nEarly childhood trauma includes physical, sexual, and psychological abuse of a child ages zero to five years-old. Trauma in early development has extreme impact over the course of a lifetime and is a significant contributor to infant mortality. Developing organs are fragile. When an infant is shaken, beaten, strangled, or raped the impact is exponentially more destructive than when the same abuse occurs in a fully developed body. Studies estimate that 1–2 per 100,000 U.S. children annually are fatally injured. Unfortunately, it is reasonable to assume that these statistics under represent actual mortality. Three-quarters (74.8 percent) of child fatalities in FFY 2015 involved children younger than 3 years, and children younger than 1 year accounted for 49.4 percent of all fatalities. In particular, correctly identifying deaths due to neglect is problematic and children with sudden unexpected death or those with what appear to be unintentional causes on the surface often have preventable risk factors which are substantially similar to those in families with maltreatment. \n\nThere is a direct relationship between age of maltreatment/injury and risk for death. The younger an infant is, the more dangerous the maltreatment. \n\nFamily configuration, child gender, social isolation, lack of support, maternal youth, marital status, poverty, parental ACES, and parenting practices are thought to contribute to increased risk.\nSocial class is a major factor in infant mortality, both historically and today. Between 1912 and 1915, the Children's Bureau in the United States examined data across eight cities and nearly 23,000 live births. They discovered that lower incomes tend to correlate with higher infant mortality. In cases where the father had no income, the rate of infant mortality was 357% more than that for the highest income earners ($1,250+). Differences between races were also apparent. African-American mothers experience infant mortality at a rate 44% higher than average; however, research indicates that socio-economic factors do not totally account for the racial disparities in infant mortality.\n\nWhile infant mortality is normally negatively correlated with GDP, there may indeed be some opposing short-term effects from a recession. A recent study by \"The Economist\" showed that economic slowdowns reduce the amount of air pollution, which results in a lower infant mortality rate. In the late 1970s and early 1980s, the recession's impact on air quality is estimated to have saved around 1,300 US babies. It is only during deep recessions that infant mortality increases. According to Norbert Schady and Marc-François Smitz, recessions when GDP per capita drops by 15% or more increase infant mortality.\n\nSocial class dictates which medical services are available to an individual. Disparities due to socioeconomic factors have been exacerbated by advances in medical technology. Developed countries, most notably the United States, have seen a divergence between those living in poverty who cannot afford medical advanced resources, leading to an increased chance of infant mortality, and others.\n\nIn policy, there is a lag time between realization of a problem's possible solution and actual implementation of policy solutions. Infant mortality rates correlate with war, political unrest, and government corruption.\n\nIn most cases, war-affected areas will experience a significant increase in infant mortality rates. Having a war taking place where a woman is planning on having a baby is not only stressful on the mother and foetus, but also has several detrimental effects.\n\nHowever, many other significant factors influence infant mortality rates in war-torn areas. Health care systems in developing countries in the midst of war often collapse. Attaining basic medical supplies and care becomes increasingly difficult. During the Yugoslav Wars in the 1990s Bosnia experienced a 60% decrease in child immunizations. Preventable diseases can quickly become epidemic given the medical conditions during war.\n\nMany developing countries rely on foreign aid for basic nutrition. Transport of aid becomes significantly more difficult in times of war. In most situations the average weight of a population will drop substantially. Expecting mothers are affected even more by lack of access to food and water. During the Yugoslav Wars in Bosnia the number of premature babies born increased and the average birth weight decreased.\n\nThere have been several instances in recent years of systematic rape as a weapon of war. Women who become pregnant as a result of war rape face even more significant challenges in bearing a healthy child. Studies suggest that women who experience sexual violence before or during pregnancy are more likely to experience infant death in their children. Causes of infant mortality in abused women range from physical side effects of the initial trauma to psychological effects that lead to poor adjustment to society. Many women who became pregnant by rape in Bosnia were isolated from their hometowns making life after childbirth exponentially more difficult.\n\nDeveloping countries have a lack of access to affordable and professional health care resources, and skilled personnel during deliveries. Countries with histories of extreme poverty also have a pattern of epidemics, endemic infectious diseases, and low levels of access to maternal and child healthcare.\n\nThe American Academy of Pediatrics recommends that infants need multiple doses of vaccines such as diphtheria-tetanus-acellular pertussis vaccine, Haemophilus influenzae type b (Hib) vaccine, Hepatitis B (HepB) vaccine, inactivated polio vaccine (IPV), and pneumococcal vaccine (PCV). Research was conducted by the Institute of Medicine's Immunization Safety Review Committee concluded that there is no relationship between these vaccines and risk of SIDS in infants. This tells us that not only is it extremely necessary for every child to get these vaccines to prevent serious diseases, but there is no reason to believe that if your child does receive an immunization that it will have any effect on their risk of SIDS.\n\nPolitical modernization perspective, the neo-classical economic theory that scarce goods are most effectively distributed to the market, say that the level of political democracy influences the rate of infant mortality. Developing nations with democratic governments tend to be more responsive to public opinion, social movements, and special interest groups for issues like infant mortality. In contrast, non-democratic governments are more interested in corporate issues and less so in health issues. Democratic status effects the dependency a nation has towards its economic state via export, investments from multinational corporations and international lending institutions.\n\nLevels of socioeconomic development and global integration are inversely related to a nation's infant mortality rate. Dependency perspective occurs in a global capital system. A nation's internal impact is highly influenced by its position in the global economy and has adverse effects on the survival of children in developing countries. Countries can experience disproportionate effects from its trade and stratification within the global system. It aids in the global division of labor, distorting the domestic economy of developing nations. The dependency of developing nations can lead to a reduce rate of economic growth, increase income inequality inter- and intra-national, and adversely affects the wellbeing of a nation's population. A collective cooperation between economic countries plays a role in development policies in the poorer, peripheral, countries of the world.\n\nThese economic factors present challenges to governments' public health policies. If the nation's ability to raise its own revenues is compromised, governments will lose funding for its health service programs, including services that aim in decreasing infant mortality rates. Peripheral countries face higher levels of vulnerability to the possible negative effects of globalization and trade in relation to key countries in the global market.\n\nEven with a strong economy and economic growth (measured by a country's gross national product), the advances of medical technologies may not be felt by everyone, lending itself to increasing social disparities.\n\nHigh rates of infant mortality occur in developing countries where financial and material resources are scarce and there is a high tolerance to high number of infant deaths. There are circumstances where a number of developing countries to breed a culture where situations of infant mortality such as favoring male babies over female babies are the norm. In developing countries such as Brazil, infant mortality rates are commonly not recorded due to failure to register for death certificates. Failure to register is mainly due to the potential loss of time and money and other indirect costs to the family. Even with resource opportunities such as the 1973 Public Registry Law 6015, which allowed free registration for low-income families, the requirements to qualify hold back individuals who are not contracted workers.\n\nAnother cultural reason for infant mortality, such as what is happening in Ghana, is that \"besides the obvious, like rutted roads, there are prejudices against wives or newborns leaving the house.\" Because of this it is making it even more difficult for the women and newborns to get the treatment that is available to them and that is needed.\n\nCultural influences and lifestyle habits in the United States can account for some deaths in infants throughout the years. According to the Journal of the American Medical Association \"the post neonatal mortality risk (28 to 364 days) was highest among continental Puerto Ricans\" compared to babies of the non-Hispanic race. Examples of this include teenage pregnancy, obesity, diabetes and smoking. All are possible causes of premature births, which constitute the second highest cause of infant mortality. Ethnic differences experienced in the United States are accompanied by higher prevalence of behavioral risk factors and sociodemographic challenges that each ethnic group faces.\n\nHistorically, males have had higher infant mortality rates than females. The difference between male and female infant mortality rates have been dependent on environmental, social, and economic conditions. More specifically, males are biologically more vulnerable to infections and conditions associated with prematurity and development. Before 1970, the reasons for male infant mortality were due to infections, and chronic degenerative diseases. However, since 1970, certain cultures emphasizing males has led to a decrease in the infant mortality gap between males and females. Also, medical advances have resulted in a growing number of male infants surviving at higher rates than females due to the initial high infant mortality rate of males.\n\nGenetic components results in newborn females being biologically advantaged when it comes to surviving their first birthday. Males, biologically, have lower chances of surviving infancy in comparison to female babies. As infant mortality rates saw a decrease on a global scale, the gender most affected by infant mortality changed from males experiences a biological disadvantage, to females facing a societal disadvantage. Some developing nations have social and cultural patterns that reflects adult discrimination to favor boys over girls for their future potential to contribute to the household production level. A country's ethnic composition, homogeneous versus heterogeneous, can explain social attitudes and practices. Heterogeneous level is a strong predictor in explaining infant mortality.\n\nBirth spacing is the time between births. Births spaced at least three years apart from one another are associated with the lowest rate of mortality. The longer the interval between births, the lower the risk for having any birthing complications, and infant, childhood and maternal mortality. Higher rates of pre-term births, and low birth weight are associated with birth to conception intervals of less than six months and abortion to pregnancy interval of less than six months. Shorter intervals between births increase the chances of chronic and general under-nutrition; 57% of women in 55 developing countries reported birth spaces shorter than three years; 26% report birth spacing of less than two years. Only 20% of post-partum women report wanting another birth within two years; however, only 40% are taking necessary steps such as family planning to achieve the birth intervals they want.\n\nUnplanned pregnancies and birth intervals of less than twenty-four months are known to correlate with low birth weights and delivery complications. Also, women who are already small in stature tend to deliver smaller than average babies, perpetuating a cycle of being underweight.\n\nTo reduce infant mortality rates across the world health practitioners, governments, and non-governmental organizations have worked to create institutions, programs and policies to generate better health outcomes. Improvements such as better sanitation practices have proven to be effective in reducing public health outbreaks and rates of disease among mothers and children. Efforts to increase a households' income through direct assistance or economic opportunities decreases mortality rates, as families possess some means for more food and access to healthcare. Education campaigns, disseminating knowledge among urban and rural regions, and better access to education attainment prove to be an effective strategy to reduce infant and mother mortality rates. Current efforts from NGOs and governments are focused developing human resources, strengthening health information systems, health services delivery, etc. Improvements in such areas have increased regional health systems and aided in efforts to reduce mortality rates. \n\nReductions in infant mortality are possible in any stage of a country's development. Rate reductions are evidence that a country is advancing in human knowledge, social institutions and physical capital. Governments can reduce the mortality rates by addressing the combined need for education (such as universal primary education), nutrition, and access to basic maternal and infant health services. A policy focus has the potential to aid those most at risk for infant and childhood mortality allows rural, poor and migrant populations.\n\nReducing chances of babies being born at low birth weights and contracting pneumonia can be accomplished by improving air quality. Improving hygiene can prevent infant mortality. Home-based technology to chlorinate, filter, and solar disinfection for organic water pollution could reduce cases of diarrhea in children by up to 48%. Improvements in food supplies and sanitation has been shown to work in the United States' most vulnerable populations, one being African Americans. Overall, women's health status need to remain high.\n\nSimple behavioral changes, such as hand washing with soap, can significantly reduce the rate of infant mortality from respiratory and diarrheal diseases. According to UNICEF, hand washing with soap before eating and after using the toilet can save more lives of children than any single vaccine or medical intervention, by cutting deaths from diarrhea and acute respiratory infections.\n\nFuture problems for mothers and babies can be prevented. It is important that women of reproductive age adopt healthy behaviors in everyday life, such as taking folic acid, maintaining a healthy diet and weight, being physically active, avoiding tobacco use, and avoiding excessive alcohol and drug use. If women follow some of the above guidelines, later complications can be prevented to help decrease the infant mortality rates. Attending regular prenatal care check-ups will help improve the baby's chances of being delivered in safer conditions and surviving.\n\nFocusing on preventing preterm and low birth weight deliveries throughout all populations can help to eliminate cases of infant mortality and decrease health care disparities within communities. In the United States, these two goals have decreased infant mortality rates on a regional population, it has yet to see further progress on a national level.\n\nTechnological advances in medicine would decrease the infant mortality rate and an increased access to such technologies could decrease racial and ethnic disparities. It has been shown that technological determinants are influenced by social determinants. Those who cannot afford to utilize advances in medicine tend to show higher rates of infant mortality. Technological advances has, in a way, contributed to the social disparities observed today. Providing equal access has the potential to decrease socioeconomic disparities in infant mortality. Specifically, Cambodia is facing issues with a disease that is unfortunately killing infants. The symptoms only last 24 hours and the result is death. As stated if technological advances were increased in countries it would make it easier to find the solution to diseases such as this. Recently, there have been declines in the United States that could be attributed to advances in technology. Advancements in the Neonatal Intensive Care Unit can be related to the decline in infant mortality in addition to the advancement of surfactants. However, the importance of the advancement of technology remains unclear as the number of high-risk births increases in the United States.\n\nIt has been well documented that increased education among mothers, communities, and local health workers results in better family planning, improvement on children's health, and lower rates of children's deaths. High-risk areas, such as Sub-Saharan Africa, have demonstrated that an increase in women's education attainment leads to a reduction in infant mortality by about 35%. Similarly, coordinated efforts to train community health workers in diagnosis, treatment, malnutrition prevention, reporting and referral services has reduced infant mortality in children under 5 as much as 38%. Public health campaigns centered around the \"First 1,000 Days\" of conception have been successful in providing cost-effective supplemental nutrition programs, as well as assisting young mothers in sanitation, hygiene and breastfeeding promotion. Increased intake of nutrients and better sanitation habits have a positive impact on health, especially developing children. Educational attainment and public health campaigns provide the knowledge and means to practice better habits and leads to better outcomes against infant mortality rates. \n\nAwareness of health services, education, and economic opportunities provide means to sustain and increase chance of development and survival. A decrease on GPD, for example, results in increased rates of infant mortality. Negative effects on household income reduces amount being spent on food and healthcare, affecting the quality of life and access to medical services to ensure full development and survival. On the contrary, increased household income translates to more access to nutrients and healthcare, reducing the risks associated with malnutrition and infant mortality. Moreover, increased aggregate household incomes will produce better health facilities, water and sewer infrastructures for the entire community. \n\nGranting women employment raises their status and autonomy. Having a gainful employment can raise the perceived worth of females. This can lead to an increase in the number of women getting an education and a decrease in the number of female infanticide. In the social modernization perspective, education leads to development. Higher number of skilled workers means more earning and further economic growth. According to the economic modernization perspective, this is one type economic growth viewed as the driving force behind the increase in development and standard of living in a country. This is further explained by the modernization theory- economic development promotes physical wellbeing. As economy rises, so do technological advances and thus, medical advances in access to clean water, health care facilities, education, and diet. These changes may decrease infant mortality.\n\nEconomically, governments could reduce infant mortality by building and strengthening capacity in human resources. Increasing human resources such as physicians, nurses, and other health professionals will increase the number of skilled attendants and the number of people able to give out immunized against diseases such as measles. Increasing the number of skilled professionals is negatively correlated with maternal, infant, and childhood mortality. Between 1960 and 2000, the infant mortality rate decreased by half as the number of physicians increased by four folds. With the addition of one physician to every 1000 persons in a population, infant mortality will reduce by 30%.\n\nIn certain parts of the U.S., specific modern programs aim to reduce levels of infant mortality. An example of one such program is the 'Healthy Me, Healthy You' program based in Northeast Texas. It intends to identify factors that contribute to negative birth outcomes throughout a 37-county area. An additional program that aims to reduce infant mortality is the \"Best Babies Zone\" (BBZ) based at the University of California, Berkeley. The BBZ uses the life course approach to address the structural causes of poor birth outcomes and toxic stress in three U.S. neighborhoods. By employing community-generated solutions, the Best Babies Zone's ultimate goal is to achieve health equity in communities that are disproportionately impacted by infant death.\n\nThe infant mortality rate correlates very strongly with, and is among the best predictors of, state failure. IMR is therefore also a useful indicator of a country's level of health or development, and is a component of the physical quality of life index.\n\nHowever, the method of calculating IMR often varies widely between countries, and is based on how they define a live birth and how many premature infants are born in the country. Reporting of infant mortality rates can be inconsistent, and may be understated, depending on a nation's live birth criterion, vital registration system, and reporting practices. The reported IMR provides one statistic which reflects the standard of living in each nation. Changes in the infant mortality rate reflect social and technical capacities of a nation's population. The World Health Organization (WHO) defines a live birth as any infant born demonstrating independent signs of life, including breathing, heartbeat, umbilical cord pulsation or definite movement of voluntary muscles. This definition is used in Austria, for example. The WHO definition is also used in Germany, but with one slight modification: muscle movement is not considered to be a sign of life. Many countries, however, including certain European states (e.g. France) and Japan, only count as live births cases where an infant breathes at birth, which makes their reported IMR numbers somewhat lower and increases their rates of perinatal mortality. In the Czech Republic and Bulgaria, for instance, requirements for live birth are even higher.\n\nAlthough many countries have vital registration systems and certain reporting practices, there are many inaccuracies, particularly in undeveloped nations, in the statistics of the number of infants dying. Studies have shown that comparing three information sources (official registries, household surveys, and popular reporters) that the \"popular death reporters\" are the most accurate. Popular death reporters include midwives, gravediggers, coffin builders, priests, and others—essentially people who knew the most about the child's death. In developing nations, access to vital registries, and other government-run systems which record births and deaths, is difficult for poor families for several reasons. These struggles force stress on families, and make them take drastic measures in unofficial death ceremonies for their deceased infants. As a result, government statistics will inaccurately reflect a nation's infant mortality rate. Popular death reporters have first-hand information, and provided this information can be collected and collated, can provide reliable data which provide a nation with accurate death counts and meaningful causes of deaths that can be measured/studied.\n\nUNICEF uses a statistical methodology to account for reporting differences among countries:\nAnother challenge to comparability is the practice of counting frail or premature infants who die before the normal due date as miscarriages (spontaneous abortions) or those who die during or immediately after childbirth as stillborn. Therefore, the quality of a country's documentation of perinatal mortality can matter greatly to the accuracy of its infant mortality statistics. This point is reinforced by the demographer Ansley Coale, who finds dubiously high ratios of reported stillbirths to infant deaths in Hong Kong and Japan in the first 24 hours after birth, a pattern that is consistent with the high recorded sex ratios at birth in those countries. It suggests not only that many female infants who die in the first 24 hours are misreported as stillbirths rather than infant deaths, but also that those countries do not follow WHO recommendations for the reporting of live births and infant deaths.\n\nAnother seemingly paradoxical finding, is that when countries with poor medical services introduce new medical centers and services, instead of declining, the reported IMRs often increase for a time. This is mainly because improvement in access to medical care is often accompanied by improvement in the registration of births and deaths. Deaths that might have occurred in a remote or rural area, and not been reported to the government, might now be reported by the new medical personnel or facilities. Thus, even if the new health services reduce the actual IMR, the reported IMR may increase.\n\nCollecting the accurate statistics of infant mortality rate could be an issue in some rural communities in developing countries. In those communities, some other alternative methods for calculating infant mortality rate are emerged, for example, popular death reporting and household survey.\nThe country-to-country variation in child mortality rates is huge, and growing wider despite the progress. Among the world's roughly 200 nations, only Somalia showed no decrease in the under-5 mortality rate over the past two decades.The lowest rate in 2011 was in Singapore, which had 2.6 deaths of children under age 5 per 1,000 live births. The highest was in Sierra Leone, which had 185 child deaths per 1,000 births. The global rate is 51 deaths per 1,000 births. For the United States, the rate is eight per 1,000 births.\n\nInfant mortality rate (IMR) is not only a group of statistic but instead it is a reflection of the socioeconomic development and effectively represents the presence of medical services in the countries. IMR is an effective resource for the health department to make decision on medical resources reallocation. IMR also formulates the global health strategies and help evaluate the program success. The existence of IMR helps solve the inadequacies of the other vital statistic systems for global health as most of the vital statistic systems usually neglect the infant mortality statistic number from the poor. There are certain amounts of unrecorded infant deaths in the rural area as they do not have information about infant mortality rate statistic or do not have the concept about reporting early infant death.\n\nThe exclusion of any high-risk infants from the denominator or numerator in reported IMRs can cause problems in making comparisons. Many countries, including the United States, Sweden and Germany, count an infant exhibiting any sign of life as alive, no matter the month of gestation or the size, but according to United States some other countries differ in these practices. All of the countries named adopted the WHO definitions in the late 1980s or early 1990s, which are used throughout the European Union. However, in 2009, the US CDC issued a report that stated that the American rates of infant mortality were affected by the United States' high rates of premature babies compared to European countries. It also outlined the differences in reporting requirements between the United States and Europe, noting that France, the Czech Republic, Ireland, the Netherlands, and Poland do not report all live births of babies under 500 g and/or 22 weeks of gestation. However, the differences in reporting are unlikely to be the primary explanation for the United States' relatively low international ranking. Rather, the report concluded that primary reason for the United States’ higher infant mortality rate when compared with Europe was the United States’ much higher percentage of preterm births.\n\nThe US National Institute of Child Health and Human Development (NICHD) has made great strides in lowering US infant mortality rates. Since the institute was created the US infant mortality rate has dropped 70%, in part due to their research.\n\nUntil the 1990s, Russia and the Soviet Union did not count, as a live birth or as an infant death, extremely premature infants (less than 1,000 g, less than 28 weeks gestational age, or less than 35 cm in length) that were born alive (breathed, had a heartbeat, or exhibited voluntary muscle movement) but failed to survive for at least seven days. Although such extremely premature infants typically accounted for only about 0.5% of all live-born children, their exclusion from both the numerator and the denominator in the reported IMR led to an estimated 22%–25% lower reported IMR. In some cases, too, perhaps because hospitals or regional health departments were held accountable for lowering the IMR in their catchment area, infant deaths that occurred in the 12th month were \"transferred\" statistically to the 13th month (i.e., the second year of life), and thus no longer classified as an infant death.\n\nIn certain rural developing areas, such as northeastern Brazil, infant births are often not recorded in the first place, resulting in the discrepancies between the infant mortality rate (IMR) and the actual amount of infant deaths. Access to vital registry systems for infant births and deaths is an extremely difficult and expensive task for poor parents living in rural areas. Government and bureaucracies tend to show an insensitivity to these parents and their recent suffering from a lost child, and produce broad disclaimers in the IMR reports that the information has not been properly reported, resulting in these discrepancies. Little has been done to address the underlying structural problems of the vital registry systems in respect to the lack of reporting from parents in rural areas, and in turn has created a gap between the official and popular meanings of child death. It is also argued that the bureaucratic separation of vital death recording from cultural death rituals is to blame for the inaccuracy of the infant mortality rate (IMR). Vital death registries often fail to recognize the cultural implications and importance of infant deaths. It is not to be said that vital registry systems are not an accurate representation of a region's socio-economic situation, but this is only the case if these statistics are valid, which is unfortunately not always the circumstance. \"Popular death reporters\" is an alternative method for collecting and processing statistics on infant and child mortality. Many regions may benefit from \"popular death reporters\" who are culturally linked to infants may be able to provide more accurate statistics on the incidence of infant mortality. According to ethnographic data, \"popular death reporters\" refers to people who had inside knowledge of \"anjinhos\", including the grave-digger, gatekeeper, midwife, popular healers etc. —— all key participants in mortuary rituals. By combining the methods of household surveys, vital registries, and asking \"popular death reporters\" this can increase the validity of child mortality rates, but there are many barriers that can reflect the validity of our statistics of infant mortality. One of these barriers are political economic decisions. Numbers are exaggerated when international funds are being doled out; and underestimated during reelection.\n\nThe bureaucratic separation of vital death reporting and cultural death rituals stems in part due to structural violence. Individuals living in rural areas of Brazil need to invest large capital for lodging and travel in order to report infant birth to a Brazilian Assistance League office. The negative financial aspects deters registration, as often individuals are of lower income and cannot afford such expenses. Similar to the lack of birth reporting, families in rural Brazil face difficult choices based on already existing structural arrangements when choosing to report infant mortality. Financial constraints such as reliance on food supplementations may also lead to skewed infant mortality data.\n\nIn developing countries such as Brazil the deaths of impoverished infants are regularly unrecorded into the countries vital registration system; this causes a skew statistically. Culturally validity and contextual soundness can be used to ground the meaning of mortality from a statistical standpoint. In northeast Brazil they have accomplished this standpoint while conducting an ethnographic study combined with an alternative method to survey infant mortality. These types of techniques can develop quality ethnographic data that will ultimately lead to a better portrayal of the magnitude of infant mortality in the region. Political economic reasons have been seen to skew the infant mortality data in the past when governor Ceara devised his presidency campaign on reducing the infant mortality rate during his term in office. By using this new way of surveying, these instances can be minimized and removed, overall creating accurate and sound data.\n\nFor the world, and for both less developed countries (LDCs) and more developed countries (MDCs), IMR declined significantly between 1960 and 2001. According to the State of the World's Mothers report by Save the Children, the world IMR declined from 126 in 1960 to 57 in 2001.\n\nHowever, IMR was, and remains, higher in LDCs. In 2001, the IMR for LDCs (91) was about 10 times as large as it was for MDCs (8). On average, for LDCs, the IMR is 17 times as higher than that of MDCs. Also, while both LDCs and MDCs made significant reductions in infant mortality rates, reductions among less developed countries are, on average, much less than those among the more developed countries.\n\nA factor of about 67 separate countries with the highest and lowest reported infant mortality rates. The top and bottom five countries by this measure (taken from The World Factbook's 2012 estimates) are shown below.\n\nAccording to Guillot, Gerland, Pelletier and Saabneh \"birth histories, however, are subject to a number of errors, including omission of deaths and age misreporting errors.\"\n\nThe infant mortality rate in the US decreased by 2.3% to a historic low of 582 infant deaths per 100,000 live births in 2014.\n\nOf the 27 most developed countries, the U.S. has the highest Infant Mortality Rate, despite spending much more on health care per capita. Significant racial and socio-economic differences in the United States affect the IMR, in contrast with other developed countries, which have more homogeneous populations. In particular, IMR varies greatly by race in the US. The average IMR for the whole country is therefore not a fair representation of the wide variations that exist between segments of the population. Many theories have been explored as to why these racial differences exist with socio economic factors usually coming out as a reasonable explanation. However, more studies have been conducted around this matter, and the largest advancement is around the idea of stress and how it affects pregnancy.\n\nIn the 1850s, the infant mortality rate in the United States was estimated at 216.8 per 1,000 babies born for whites and 340.0 per 1,000 for African Americans, but rates have significantly declined in the West in modern times. This declining rate has been mainly due to modern improvements in basic health care, technology, and medical advances. In the last century, the infant mortality rate has decreased by 93%. Overall, the rates have decreased drastically from 20 deaths in 1970 to 6.9 deaths in 2003 (per every 1000 live births). In 2003, the leading causes of infant mortality in the United States were congenital anomalies, disorders related to immaturity, SIDS, and maternal complications. Babies born with low birth weight increased to 8.1% while cigarette smoking during pregnancy declined to 10.2%. This reflected the amount of low birth weights concluding that 12.4% of births from smokers were low birth weights compared with 7.7% of such births from non-smokers. According to the \"New York Times\", \"the main reason for the high rate is preterm delivery, and there was a 10% increase in such births from 2000 to 2006.\" Between 2007 and 2011, however, the preterm birth rate has decreased every year. In 2011 there was a 11.73% rate of babies born before the 37th week of gestation, down from a high of 12.80% in 2006.\n\nEconomic expenditures on labor and delivery and neonatal care are relatively high in the United States. A conventional birth averages 9,775 USD with a C-section costing 15,041 USD. Preterm births in the US have been estimated to cost $51,600 per child, with a total yearly cost of $26.2 billion. Despite this spending, several reports state that infant mortality rate in the United States is significantly higher than in other developed nations. Estimates vary; the CIA's \"World Factbook\" ranks the US 55th internationally in 2014, with a rate of 6.17, while the UN figures from 2005-2010 place the US 34th.\n\nAforementioned differences in measurement could play a substantial role in the disparity between the US and other nations. A non-viable live birth in the US could be registered as a stillbirth in similarly developed nations like Japan, Sweden, Norway, Ireland, the Netherlands, and France – thereby reducing the infant death count. Neonatal intensive care is also more likely to be applied in the US to marginally viable infants, although such interventions have been found to increase both costs and disability. A study following the implementation of the Born Alive Infant Protection Act of 2002 found universal resuscitation of infants born between 20–23 weeks increased the neonatal spending burden by $313.3 million while simultaneously decreasing quality-adjusted life years by 329.3.\nThe vast majority of research conducted in the late twentieth and early twenty-first century indicates that African-American infants are more than twice as likely to die in their first year of life than white infants. Although following a decline from 13.63 to 11.46 deaths per 1000 live births from 2005 to 2010, non-Hispanic black mothers continued to report a rate 2.2 times as high as that for non-Hispanic white mothers.\n\nContemporary research findings have demonstrated that nationwide racial disparities in infant mortality are linked to the experiential state of the mother and that these disparities cannot be totally accounted for by socio-economic, behavioral or genetic factors. The Hispanic paradox, an effect observed in other health indicators, appears in the infant mortality rate, as well. Hispanic mothers see an IMR comparable to non-Hispanic white mothers, despite lower educational attainment and economic status. A study in North Carolina, for example, concluded that \"white women who did not complete high school have a lower infant mortality rate than black college graduates.\" According to Mustillo's CARDIA (Coronary Artery Risk Development in Young Adults) study, \"self reported experiences of racial discrimination were associated with pre-term and low-birthweight deliveries, and such experiences may contribute to black-white disparities in prenatal outcomes.\" Likewise, dozens of population-based studies indicate that \"the subjective, or perceived experience of racial discrimination is strongly associated with an increased risk of infant death and with poor health prospects for future generations of African Americans.\"\n\nWhile earlier parts of this article have addressed the racial differences in infant deaths, a closer look into the effects of racial differences within the country is necessary to view discrepancies. Non-Hispanic Black women lead all other racial groups in IMR with a rate of 11.3, while the Infant Mortality Rate among white women is 5.1. Black women in the United States experience a shorter life expectancy than white women, so while a higher IMR amongst black women is not necessarily out of line, it is still rather disturbing. While the popular argument leads to the idea that due to the trend of a lower socio-economic status had by black women there is in an increased likelihood of a child suffering. While this does correlate, the theory that it is the contributing factor falls apart when we look at Latino IMR in the United States. Latino people are almost just as likely to experience poverty as blacks in the U.S., however, the Infant Mortality Rate of Latinos is much closer to white women than it is to black women. The Poverty Rates of blacks and Latinos are 24.1% and 21.4% respectively. If there is a direct correlation, then the IMR of these two groups should be rather similar, however, blacks have an IMR double that of Latinos. Also, as black women move out of poverty or never experienced it in the first place, their IMR is not much lower than their counterparts experiencing higher levels of poverty.\n\nSome believe black women are predisposed to a higher IMR, meaning ancestrally speaking, all black women from African descent should experience an elevated rate. This theory is quickly disproven by looking at women of African descent who have immigrated to the United States. These women who come from a completely different social context are not prone to the higher IMR experienced by American-born black women.\n\nTyan Parker Dominguez at the University of Southern California offers a theory to explain the disproportionally high IMR among black women in the United States. She claims African American women experience stress at much higher rates than any other group in the country. Stress produces particular hormones that induce labor and contribute to other pregnancy problems. Considering early births are one of the leading causes of death of infants under the age of one, induced labor is a very legitimate factor. The idea of stress spans socio-economic status as Parker Dominguez claims stress for lower-class women comes from unstable family life and chronic worry over poverty. For black middle-class women, battling racism, real or perceived, can be an extreme stressor. \n\nArline Geronimus, a professor at the University of Michigan School of Public Health calls the phenomenon \"weathering.\" She claims constantly dealing with disadvantages and racial prejudice causes black women's birth outcomes to deteriorate with age. Therefore, younger black women may experience stress with pregnancy due to social and economic factors, but older women experience stress at a compounding rate and therefore have pregnancy complications aside from economic factors.\n\nMary O. Hearst, a professor in the Department of Public Health at Saint Catherine University, researched the effects of segregation on the African American community to see if it contributed to the high IMR amongst black children. Hearst claims that residential segregation contributes to the high rates because of the political, economic, and negative health implications it poses on black mothers regardless of their socioeconomic status. Racism, economic disparities, and sexism in segregated communities are all examples of the daily stressors that pregnant black women face that can affect their pregnancies with conditions such as pre-eclampsia and hypertension.\n\nStudies have also shown that high IMR is due to the inadequate care that pregnant African Americans receive compared to other women in the country. This unequal treatment stems from the idea that there are racial medical differences and is also rooted in racial biases and controlled images of black women. Because of this unequal treatment, research finds that black women do not receive the same urgency in medical care and are not taken as seriously regarding pain they feel or complications they think they are having, as exemplified by the complications tennis-star Serena Williams faced during her delivery.\n\nStrides have been made, however, to combat this epidemic. In Los Angeles County, health officials have partnered with non-profits around the city to help black women after the delivery of their child. One non-profit in particular has made a large impact on many lives is Great Beginnings For Black Babies in Inglewood. The non-profit centers around helping women deal with stress by forming support networks, keeping an open dialogue around race and family life, and also finding these women a secure place in the workforce.\n\nSome research argues that to end high IMR amongst black children, the country needs to fix the social and societal issues that plague African Americans. Some scholars argue that Issues such as institutional racism, mass incarceration, poverty, and health care disparities that are present amongst the African American country need to be addressed by the United States Government in order for policy to be created to combat these issues. Following this theory, if institutional inequalities are addresses and repaired by the United States Government, daily stressors for African Americans, and African American women in particular, will be reduced, therefore lessening the risk of complications in pregnancy and infant mortality. Others argue that adding diversity in the health care industry can help reduce the high IMR because more representation can tackle deep-rooted racial biases and stereotypes that exist towards African American women. Another more recent form of action to reduce high IMR amongst black children is the use of doulas throughout pregnancy. \n\nIt was in the early 1900's that countries around the world started to notice that there was a need for better child health care services. Europe started this rally, the United States fell behind them by creating a campaign to decrease the infant mortality rate. With this program, they were able to lower the IMR to 10 deaths rather than 100 deaths per every 1000 births. Infant mortality was also seen as a social problem when it was being noticed as a national problem. American women who had middle class standing with an educational background started to create a movement that provided housing for families of a lower class. By starting this, they were able to establish public health care and government agencies that were able to make more sanitary and healthier environments for infants. Medical professionals helped further the cause for infant health by creating a pediatrics field that was experienced in medicine for children.\n\nDecreases in infant mortality in given countries across the world during the 20th century have been linked to several common trends, scientific advancements, and social programs. Some of these include the state improving sanitation, improving access to healthcare, improving education, and the development of medical advancements such as penicillin, and safer blood transfusions.\n\nIn the United States, improving infant mortality in the early half of the 20th century meant tackling environmental factors. Improving sanitation, and especially access to safe drinking water, helped the United States dramatically decrease infant mortality, a growing concern in the United States since the 1850s. On top of these environmental factors, during this time the United States endeavored to increase education and awareness regarding infant mortality. Pasteurization of milk also helped the United States combat infant mortality in the early 1900s, a practice which allowed the United States to curb disease in infants. These factors, on top of a general increase in the standard of living for those living in urban settings, helped the United States make dramatic improvements in their rates of infant mortality in the early 20th century.\n\nAlthough the overall infant mortality rate was sharply dropping during this time, within the United States infant mortality varied greatly among racial and socio-economic groups. The change in infant mortality from 1915 to 1933 was, for the white population, 98.6 in 1,000 to 52.8 in 1,000, and for the black population, 181.2 in 1,000 to 94.4 in 1,000. Studies imply that this has a direct correlation with relative economic conditions between these populations. Additionally, infant mortality in southern states was consistently 2% higher than other states in the US across a 20 year period from 1985. Southern states also tend to perform worse in predictors for higher infant mortality, such as per capita income and poverty rate.\n\nIn the latter half of the 20th century, a focus on greater access to medical care for women spurred declines in infant mortality in the United States. The implementation of Medicaid, granting wider access to healthcare, contributed to a dramatic decrease in infant mortality, in addition to access to greater access to legal abortion and family-planning care, such the IUD and the birth control pill.\n\nIn the decades following the 1970's, the United State's decreasing infant mortality rates began to slow, falling behind China's, Cuba's, and other developed countries'. Funding for the federally subsidized Medicaid and Maternal and Infant Care was sharply reduced, and availability of prenatal care greatly decreased for low-income parents.\n\nThe People's Republic of China's growth of medical resources in the latter half of the 20th century partly explains its dramatic improvement with regards to infant mortality during this time. Part of this increase included the adoption of the Rural Cooperative Medical System, which was founded in the 1950s. The Cooperative Medical System granted healthcare access to previously underserved rural populations, is estimated to have covered 90% of China's rural population throughout the 1960s. The Cooperative Medical System achieved an infant mortality rate of 25.09 per 1,000. The Cooperative Medical System was later defunded, leaving many rural populations to rely on an expensive fee-for-service system, although the rate continued to decline in general. This change in medical systems caused a socio-economic gap in accessibility to medical care in China, which fortunately was not reflected in its infant mortality rate of decline. Prenatal care was increasingly used, even as the Cooperative Medical System was replaced, and delivery assistance remained accessible.\n\nChina's one-child policy, adopted in the 1980s, negatively impacted its infant mortality. Women carrying unapproved pregnancies faced state consequences and social stigma and were thus less likely to use prenatal care. Additionally, economic realities and long-held cultural factors incentivized male offspring, leading some families who already had sons to avoid prenatal care or professional delivery services, and causing China to have unusually high female infant mortality rates during this time.\n\n\nRelated statistical categories:\n\n"}
{"id": "45213097", "url": "https://en.wikipedia.org/wiki?curid=45213097", "title": "Iron Triangle of Health Care", "text": "Iron Triangle of Health Care\n\nThe concept of the Iron Triangle of Health Care was first introduced in William Kissick’s book, \"Medicine’s Dilemmas: Infinite Needs Versus Finite Resources\" in 1994, describing three competing health care issues: access, quality, and cost containment. Each of the vertices represents identical priorities. Increasing or decreasing one results in changes to one or both of the other two. For example, a policy that increases access to health services would lower quality of health care and/or increase cost. The desired state of the triangle, high access and quality with low cost represents value in a health care system.\n\nCritics of the Iron Triangle state that the model is not actually as rigid as its name indicates, but is more dynamic because costs of care are constantly changing. Health care costs change faster than the other two dimensions of the triangle, affecting access to care, which in turn influences quality. Other skeptics argue that the Iron Triangle is not a fixed framework, but an observation and reflection of the current state of health care. In line with Clayton Christensen’s theory on disruptive innovation, critics of the Iron Triangle believe that health care, particularly in the United States, has not yet been disrupted like fields such as computer production. The belief is that with time and innovation, the current Iron Triangle will be disrupted, and just as the cost of computer production has fallen as quality and access to computers has increased, health care access and quality will rise, and cost will decrease. Critics argue that the Iron Triangle is not a one-size-fits-all model that can be applied to an entire population.\n"}
{"id": "35417634", "url": "https://en.wikipedia.org/wiki?curid=35417634", "title": "Labial thermistor clip", "text": "Labial thermistor clip\n\nThe labial thermistor clip is a device used measure the skin temperature of the labia minora and is associated blood engorgement. This device consists of a thermistor affixed to a small metal clip that can be attached to the labia minora. The labial thermistor clip is the second most commonly used physiological measure of female genital response, next to the vaginal photoplethysmograph (VPG). Both devices can be used simultaneously.\nThe labial thermistor clip has some advantages over VPG, including better test-retest reliability, greater correlation between genital and self-reported sexual arousal, and an absolute unit of change (temperature). Like VPG, the labial thermistor clip has discriminant validity; that is, it detects differences between sexual and nonsexual stimuli. It is also sensitive to different levels of sexual arousal. The labial thermistor clip has some disadvantages because participants have difficulty with placing the device correctly and some report discomfort with using the device.\n\n\n"}
{"id": "25441942", "url": "https://en.wikipedia.org/wiki?curid=25441942", "title": "List of health deities", "text": "List of health deities\n\nA health deity is a god or goddess in mythology or religion associated with health, healing and wellbeing. They may also be related to childbirth or Mother Goddesses. They are a common feature of polytheistic religions.\n\n\n\n\n\n\n\n\n\nVaidyanatha - Shiva as healer of all\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "32952670", "url": "https://en.wikipedia.org/wiki?curid=32952670", "title": "List of healthcare occupations", "text": "List of healthcare occupations\n\nA listing of healthcare professions by medical discipline. \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "56656329", "url": "https://en.wikipedia.org/wiki?curid=56656329", "title": "List of vaginal tumors", "text": "List of vaginal tumors\n\nVaginal tumors are neoplasms (tumors) found in the vagina. They can be benign or malignant. A neoplasm is an abnormal growth of tissue that usually forms a tissue mass.\nVaginal neoplasms may be solid, cystic or of mixed type.\n\nVaginal cancers arise from vaginal tissue, with vaginal sarcomas develop from bone, cartilage, fat, muscle, blood vessels or other connective or supportive tissue. Tumors in the vagina may also be metastases (malignant tissue that has spread to the vagina from other parts of the body).\nSome neoplastic growths of the vagina are sufficiently rare as to be only described in case studies.\n\nSigns and symptoms may include a feeling of pressure, painful intercourse or bleeding. Most vaginal tumors are located during a pelvic exam. Ultrasonography, CT and MRI imaging is used to establish the location and presence or absence of fluid in a tumor. Biopsy provides a more definitive diagnosis.\n\n\nVaginal tumors also can be found in domesticated animals:\n\n\n"}
{"id": "9258044", "url": "https://en.wikipedia.org/wiki?curid=9258044", "title": "MEDCIN", "text": "MEDCIN\n\nMEDCIN, a system of standardized medical terminology, is a proprietary medical vocabulary and was developed by Medicomp Systems, Inc. MEDCIN is a point-of-care terminology, intended for use in Electronic Health Record (EHR) systems, and it includes over 280,000 clinical data elements encompassing symptoms, history, physical examination, tests, diagnoses and therapy. This clinical vocabulary contains over 38 years of research and development as well as the capability to cross map to leading codification systems such as SNOMED CT, CPT, ICD-9-CM/ICD-10-CM, DSM, LOINC, CDT, CVX, and the Clinical Care Classification (CCC) System for nursing and allied health.\n\nThe MEDCIN coding system is marketed for point-of-care documentation. Several Electronic Health Record (EHR) systems embed MEDCIN, which allows them to produce structured and numerically codified patient charts. Such structuring enables the aggregation, analysis, and mining of clinical and practice management data related to a disease, a patient or a population.\n\nMEDCIN was initially developed by Peter S. Goltra, founder of Medicomp Systems “as an intelligent clinical database for documentation at the time of care.\" \nThe first few years of the development were spent in designing the structure of a knowledge engine that would enable the population of relationships between clinical events.\nSince 1978, the MEDCIN database engine has been continuously refined and expanded to include concepts from clinical histories, test, physical examination, therapies and diagnoses to enable coding of complete patient encounters with the collaboration of physicians and teaching institutions such as Cornell, Harvard, and Johns Hopkins.\n\nMEDCIN data elements are organized in multiple clinical hierarchies, where users can easily navigate to a medical term by following down the tree of clinical propositions. The clinical propositions define unique intellectual clinical content. An example of such similar propositions include \"wheezing which is worse during cold weather\" and \"wheezing which is worse with a cold\" differ in meaning significantly to clinicians and therefore it enables the software to present relevant items to clinical users.\n\nThis hierarchy provides an inheritance of clinical properties between data elements, which greatly enhances the capabilities of EHR systems and as well providing logical presentation structures for the clinical users. The linkage of MEDCIN data elements through the use of describing many diagnoses in the diagnostic index creates multiple hierarchies. The MEDCIN engine uses Intelligent Prompting and navigation tools to enable clinicians to select specific clinical terms that they need rather than having to create new terms for rapid documentation. \n\nMEDCIN has been designed to work as an interface terminology to include components to make EHRs more usable when it is used in conjunction with proprietary physician and nursing documentation tools. According to Rosenbloom et al. (2006), investigators such as Chute et al., McDonald et al., Rose et al. and Campbell et al. have defined clinical interface terminologies as “a systematic collection of health care-related phrases (term)” (p. 277) that supports the capturing of patient-related clinical information entered by clinicians into software programs such as clinical note capture and decision support tools.\n\nFor an interface terminology to be clinical usable, it has to be able to describe any clinical presentation with speed, ease of use, and accuracy for clinicians to accomplish the intended tasks (e.g. documenting patient care) when using the medical terminology. In addition, the terms in medical terminology must have medical relationships. MEDCIN’s presentation engine, accomplishes this usability criteria by using the Intelligent Prompting capabilities to present a relevant list of MEDCIN clinical terms for rapid clinical documentations. Another usability feature that the MEDCIN presentation engine provides is the medical relationships of clinical terms through multiple clinical hierarchies for each MEDCIN term.\n\nIn August 2012, Medicomp Systems released an updated version of the software embedded with ICD-10-CM (International Classification of Diseases, 10th Revision, Clinical Modification) mappings and functionality to comply with the transition from ICD-9-CM to ICD-10-CM as mandated by the US Department of Health and Human Services. This new version is specially designed to make the ICD-10 more usable in the EHR systems by providing clinicians easier access to bi-directional mappings, accurate data and codes through their EHR products. The ICD-10 is published by the World Health Organization (WHO) to enable the systematic collection of morbidity and mortality data from different countries for statistical analysis.\n\nMEDCIN terminology engine can be easily integrated into existing EHRs and legacy systems to enable mapping of existing terminologies and other coding systems such as ICD, DSM, CPT, LOINC, SNOMED CT and the Clinical Care Classification (CCC) System to generate seamless codified data at point of care. MEDCIN’s interoperability features enable easy access and sharing of patient data between health care facilities.\n\nMEDCIN has been implemented into several commercial EHR systems as an interface terminology to support integrated care, clinical documentation, health maintenance monitoring and disease management, and the care planning functions of physicians, nurses and allied health professionals.\nSuch commercial EHR systems include EHRs from EPIC, Allscripts, Pulse, Mckesson, and the United States Department of Defence’s (DoD) EHR system, the Armed Forces Health Longitudinal Technology Application (AHLTA).\nAHLTA is an EHR system developed for the US Department of Defense. This application uses the Medicomp’s MEDCIN terminology engine for clinical documentation purposes. Figure 1, shows an example of the MEDCIN terminology where the physician can search for the correct terms for input into the patient note.\n\nThe Nursing Plan of Care (POC) was developed by Medicomp Systems, for the Clinical Care Classification (CCC) System. The CCC System is a standardized, coded nursing terminology that provides a unique framework and coding structure for accessing, classifying and documenting patient care by nurses and other allied health professionals. The CCC is directly linked in the MEDCIN nursing POC to medical terminology with the purpose of creating patient plan of care by extracting a pool of documentation from the EHR history. The CCC nursing terminology is integrated into the MEDCIN clinical database through a contextual hierarchical tree, providing an array of terminology standards and concepts with Intelligent Prompting capabilities of the MEDCIN engine.\n\n\n\n"}
{"id": "4974742", "url": "https://en.wikipedia.org/wiki?curid=4974742", "title": "Mandated reporter", "text": "Mandated reporter\n\nIn many parts of the western world, mandated reporters are people who have regular contact with vulnerable people and are therefore legally required to ensure a report is made when abuse is observed or suspected. Specific details vary across jurisdictions—the abuse that must be reported may include neglect, or financial, physical, sexual, or other types of abuse. Mandated reporters may include paid or unpaid people who have assumed full or intermittent responsibility for the care of a child, dependent adult, or elder.\n\nIn 1962, United States doctors C. Henry Kempe and Brandt Steele published \"The Battered Child Syndrome\", which helped doctors identify child abuse, its effects, and the need to report serious physical abuse to legal authorities. Its publication changed the prevalent views in the United States, where child abuse was previously seen as uncommon, and not a regular issue. In 1974, the United States Congress passed the Child Abuse Prevention and Treatment Act (CAPTA), which provides funds to states for development of Child Protective Services (CPS) and hotlines to prevent serious injuries to children. These laws and the media and advocacy coverage and research brought about a gradual change in societal expectations on reporting in the United States and, at different rates, in other western nations.\n\nOriginally created to respond to physical abuse, reporting systems in various countries began to expand to address sexual and emotional abuse, neglect, and exposure to domestic abuse. This expansion was accompanied by broader requirements for reporting abuse: previously reports were only submitted when an incident caused serious physical injury, but as the definitions changed, more minor physical injuries and developmental and psychological trauma began to be included as well.\n\nThere has been a huge increase of reporting over the decades with enormous numbers of unsubstantiated cases. Referrals increase each year, but the actual substantiated cases remain low and are approximately the same or decline each year.\nMedia and commentators often take the number of referrals to be synonymous with the number of cases of actual child maltreatment, which makes the problem appear larger than it actually is.\n\nVarious jurisdictions have mandatory reporting requirements for different types of vulnerable people, such as:\n\nOverall, the total number of substantiations in Australia has nearly doubled since 2001 (1.77 times higher) but have shown a slight downward trend since 2005-06. Specific comparisons cannot be made before this time, as different jurisdictions have made substantial changes throughout this period in reporting requirements and definitions of terms. In Australia there was an increase in notifications from 2000-01 to 2008-09, followed by a decrease to 2011-12, reflecting, in part, different mandated thresholds.\n\nBrazil has a mandatory reporting system for child maltreatment that is enforced by the health and educational systems, but due to the absence of national prevalence surveys, the difference between data generated by such mandatory reports and actual incidence of abuse is not known, although it is believed that mandatory report systems may result in underreporting. While specific data on mandatory reporting is unavailable, data collected from 314 municipalities (out of 5564) across the country revealed that in the second half of 2005 alone, 27,986 children received attention from the Social Welfare Centers: either because of sexual abuse (13,240), psychological violence (4,340), neglect (4,073), physical violence (3,436) and sexual exploitation (2,887). Most victims were in the 7–14 years group (17,738). 4,936 were under 6 years old.\n\nCanada provides data on substantiations but not reports. In Canada in 2008, 36% of all investigations were substantiated, with a further 8% of investigations where maltreatment remained suspected by the worker at the conclusion of the investigation and a further 5% with a risk of future maltreatment. 30% of investigations were unfounded and 17% resulted in no risk of future maltreatment was indicated.\n\nEngland provides data on substantiations but not reports. \nIn 2012, the UK reports 50,573 children were on child protection registers or subject to a child protection plan: England (42,850), Scotland (2,706), Wales (2,890), Northern Ireland (2,127).\n\nIn the US there was a 2348% increase in hotline calls from 150,000 in 1963 to 3.3 million in 2009. In 2011, there were 3.4 million calls.\nFrom 1992 to 2009 in the US, substantiated cases of sexual abuse declined 62%, physical abuse decreased 56% and neglect 10%. Although the referrals increase each year, about 1% of the child population is affected by any form of substantiated maltreatment.\n\nIn the US, there are approximately 3.6 million calls each year: 9,000/day, 63,000/week, affecting on average 1 out of 10 U.S. families with children under the age of 18 each year (there are 32.2 million such families). From 1998 to 2011 there were a total of 43 million hotline calls. Of those substantiated, over half are minor situations and many are situations where the worker thinks something may happen in the future. The largest category was neglect,\n\nEach year in the US, approximately 85% of hotline calls either do not warrant investigation or are not substantiated. Approximately 78% of all investigations are unsubstantiated and approximately 22% are substantiated, with around 9% where \"alternative responses\" are offered in some states, which have a focus on working with the family to address issues rather than confirming maltreatment.\n\nThe criteria for reporting vary significantly based on jurisdiction. Typically, mandatory reporting applies to people who have reason to suspect the abuse or neglect of a child, but it can also apply to people who suspect abuse or neglect of a dependent adult or the elderly, or to any members of society.\nThe Council of Europe has urged all countries to have mandatory reporting of child abuse but several European countries do not. A large majority of European countries – 86 per cent – have some form of mandatory reporting; 77 per cent of African countries do; 72 per cent of Asian countries and 90 per cent of the Americas do.\n\nIn Australia, the Northern Territory requires all citizens to report suspected child abuse, and the other states and territories have mandatory reporting for designated work roles.\n\nIn Brazil, notification is mandatory in the health system, in schools and by the Child Protection Councils (CPC) network, present in many municipalities.\n\nIn Malaysia, The Child Act 2001 requires any medical officer or medical practitioner, childcare provider or member of the family to notify his/her concerns, suspicions or beliefs that a child may have been abused or neglected to the appropriate child protection authority in the country. Failure to do so can result in criminal charges.\n\nIn South Africa, Section 110 of the Children's Act, 2005 mandates 'Any correctional official, dentist, homeopath, immigration official, labour inspector, legal practitioner, medical practitioner, midwife, minister of religion, nurse, occupational therapist, physiotherapist, psychologist, religious leader, social service professional, social worker, speech therapist, teacher, traditional health practitioner, traditional leader or member of staff or volunteer worker at a partial care facility, drop-in centre or child and youth care centre' to report when they suspect that a child has been abused 'in a manner causing physical injury, sexually abused or deliberately neglected'. The Sexual Offences Act, 1957, compels all citizens who are aware of the sexual exploitation of children to report the offence to the police.\n\nIn the United States, states frequently amend their laws, but as of November 2013 all states, the District of Columbia, American Samoa, Guam, the Northern Mariana Islands, Puerto Rico, and the U.S. Virgin Islands have statutes identifying persons who are required to report suspected child maltreatment to an appropriate agency.\n\nApproximately 48 states, the District of Columbia, American Samoa, Guam, the Northern Mariana Islands, Puerto Rico, and the Virgin Islands designate professions the members of which are mandated by law to report child maltreatment.\n\nAs of November 2013, in 18 states and Puerto Rico, any person who suspects child abuse or neglect is required to report suspected abuse or neglect regardless of profession.\nIn all other States, territories, and the District of Columbia, any non-mandated person is also allowed to report.\n\nCanada imposes a mandatory requirement on all citizens, except in the Yukon Territory where it is restricted to those who come in contact with children in their professional roles.\n\nMexico also has legislative reporting duties.\n\nIn 15 member States (Bulgaria, Croatia, Denmark, Estonia, France, Hungary, Ireland, Lithuania, Luxembourg, Poland, Romania, Slovenia, Spain, Sweden and the United Kingdom) reporting obligations are in place for all professionals. In 10 member states (Austria, Belgium, Cyprus, the Czech Republic, Greece, Finland, Italy, Latvia, Portugal and Slovakia) existing obligations only address certain professional groups such as social workers or teachers.\n\nMore than half (15) of the EU member states (Bulgaria, Croatia, Cyprus, the Czech Republic, Denmark, Estonia, Finland, Ireland, Italy, Latvia, Lithuania, Portugal, Slovakia, Slovenia and Sweden) have specific reporting obligations addressing civilians, with specific obligations for civilians to report cases of child abuse, neglect and/or exploitation. In many member states without specific provisions, general provisions on the obligation for all citizens to report a criminal act under national law apply, but with no specific obligation to report a child at risk of abuse.\n\nIn Germany, Malta and the Netherlands, no reporting obligations were in place in March 2014. In Malta, however, the new draft Child Protection Act (Out of Home Care), introduces the obligation of mandatory reporting for all professionals and volunteers.\n\nUnder UK law only local authority social workers, health and social service board social workers (Northern Ireland) and police have a duty to report suspicions that a child is in need of care and protection. Local child protection guidelines and professional codes of conduct may expect other professionals, such as teachers and medical staff, to report, but they do not have to do so as a matter of law. Front line professionals are also required to report cases of female genital mutilation.\n\nThe criteria for reporting vary greatly among jurisdictions. In some, for example, only physical or sexual abuse to be reported must be reported, while elsewhere any signs of sexual abuse, physical abuse, emotional abuse or neglect are included.\n\nMandated reporters are usually required to give their name when they make a report. This allows investigators to contact them for further details if needed, and protects the mandated reporter from accusations that they did not report as required by law.\n\nTypically, reporters are encouraged to report their suspicions and not to investigate or wait for absolute proof, which can lead to further harm directed at the suspected victim, and allow for perpetrators to prepare their defence through intimidation. The investigation of the abuse is then left to professionals. Some jurisdictions allow clear protections for reports made in good faith, protecting the disclosure of the reporter's name.\n\nInnocence should be presumed unless and until evidence establishing guilt is obtained and it must be remembered that only suspicions are being reported.\n\nMandated reporting requirements generally apply to professions that have frequent contact with children, although in some jurisdictions all citizens are required to report suspicions of some forms of abuse. Other jurisdictions have mandated requirements only of doctors or medical professionals.\nMandated professions may include, but are not limited to the following:\n\nJurisdictions may note that, while these groups are \"legally required\" (mandated) to report, most jurisdictions allow for \"voluntary\" reports by any concerned people.\n\nConflicts between a mandated reporter's duties and some privileged communication statutes are common but, in general, attorney-client privileges and clergy-penitent privileges are exempt from mandatory reporting in many jurisdictions. In some states in the US, psychiatrist and psychologists are also exempt from mandatory reporting.\n\n\"Clergy-penitent privilege\" is privileged communication that protects communication between a member of the clergy and a communicant, who shares information in confidence. When applied, neither the minister nor the \"penitent\" can be forced to testify in court, by deposition, or other legal proceedings, about the contents of the communication. Most US states provide the privilege, typically in rules of evidence or civil procedure, and the confidentiality privilege has also been extended to non-catholic clergy and non-Sacramental counseling.\n\nOriginally created to respond to physical abuse, reporting systems in various countries have expanded the reportable incidents, when it was recognised that sexual and emotional abuse, neglect, and exposure to domestic abuse also have profound impacts on children's wellbeing. Critics of investigations into reports of child abuse state that\nCritics state that mandatory reporting may also\nThey also state that mandatory reporting laws have had unintended consequences for the accused. Individuals, including juveniles, who have never been convicted of anything may be placed on CPS Central Registries/databases (different from Sex Abuse Registries) for decades, limiting educational and employment opportunities due to background checks. There is a 1.2–12.3% recidivism rate (repeat substantiations within 6 months of initial substantiations).\n\nSome feel that unsubstantiated rates are enormous and go beyond anything reasonably needed, and that emotions about child abuse and sensational media coverage has led to an overreaction by some professionals and citizens, who report many cases that do not amount to child abuse. Media and commentators often take the number of referrals to be synonymous with the number of cases of actual child maltreatment, which makes the problem appear larger than it actually is. Conversely, Dr. Jill McLeigh from Colorado University has stated that, in the United States, there is \"gross underreporting\" of child abuse and neglect, but she does not believe mandatory reporting laws are the appropriate way to address it. Ben Mathews of Queensland University of Technology said he had studied 10 years of data from Australia and other countries, and concluded that there were more benefits than not where reporting was compulsory.\n\n\n"}
{"id": "12571217", "url": "https://en.wikipedia.org/wiki?curid=12571217", "title": "Medical food", "text": "Medical food\n\nMedical foods are foods that are specially formulated and intended for the dietary management of a disease that has distinctive nutritional needs that cannot be met by normal diet alone. In the United States they were defined in the Food and Drug Administration's 1988 Orphan Drug Act Amendments and are subject to the general food and safety labeling requirements of the Federal Food, Drug, and Cosmetic Act. In Europe the European Food Safety Authority established definitions for \"foods for special medical purposes\" (FSMPs) in 2015.\n\nMedical foods, called \"food for special medical purposes\" in Europe, are distinct from the broader category of foods for special dietary use, from traditional foods that bear a health claim, and from dietary supplements. In order to be considered a medical food the product must, at a minimum:\n\nMedical foods can be classified into the following categories:\n\nMedical foods are regulated by the US Food and Drug Administration under the Food Drug and Cosmetic Act regulations. 21 CFR 101.9(j) (8).\n\nThe term medical food, as defined in section 5(b) of the Orphan Drug Act (21 U.S.C. 360ee (b) (3)) is \"a food which is formulated to be consumed or administered enterally under the supervision of a physician and which is intended for the specific dietary management of a disease or condition for which distinctive nutritional requirements, based on recognized scientific principles, are established by medical evaluation.\"\n\nMedical foods are not required to undergo premarket review or approval by FDA. Additionally, they are exempted from the labeling requirements for health claims and nutrient content claims under the Nutrition Labeling and Education Act of 1990. In 2016 the FDA published an update: Guidance for Industry: Frequently Asked Questions About Medical Foods; Second Edition. Definitions and labeling requirements are included.\n\n\n"}
{"id": "5470358", "url": "https://en.wikipedia.org/wiki?curid=5470358", "title": "Microgreen", "text": "Microgreen\n\nMicrogreens are a vegetable green, harvested after sprouting as shoots, that are used both as a visual and flavor component or ingredient primarily in fine dining restaurants. Fine dining chefs use microgreens to enhance the attractiveness and taste of their dishes with their delicate textures and distinctive flavors. Smaller than “baby greens,” and harvested later than sprouts, microgreens can provide a variety of leaf flavors, such as sweet and spicy. They are also known for their various colors and textures. Among upscale markets, they are now considered a specialty genre of greens that are good for garnishing salads, soups, plates, and sandwiches.\n\nEdible young greens and grains are produced from various kinds of vegetables, herbs or other plants. They range in size from , including the stem and leaves. A microgreen has a single central stem which has been cut just above the soil line during harvesting. It has fully developed cotyledon leaves and usually has one pair of very small, partially developed true leaves. The average crop-time for most microgreens is 10–14 days from seeding to harvest.\n\nMicrogreens began showing up on chefs' menus as early as the 1980s, in San Francisco, California.\nIn Southern California, microgreens have been grown since about the mid1990s. There were initially very few varieties offered. Those available were such as arugula, basil, beets, kale, cilantro and a mixture called Rainbow Mix. Having spread eastward from California, they are now being grown in most areas of the country with an increasing number of varieties being produced. Today, the U.S. microgreens industry consists of a variety of seed companies and growers.\n\nMicrogreens have three basic parts: a central stem, cotyledon leaf or leaves, and typically the first pair of very young true leaves. They vary in size depending upon the specific variety grown, with the typical size being in total length. When the green grows beyond this size, it should no longer be considered a microgreen. Larger sizes have been called petite greens. Microgreens are typically 2–4 weeks old from germination to harvest. Both baby greens and microgreens lack any legal definition. The terms \"baby greens\" and \"microgreens\" are marketing terms used to describe their respective categories. Sprouts are germinated seeds and are typically consumed as an entire plant (root, seed, and shoot), depending on the species. For example, sprouts from almond, pumpkin, and peanut reportedly have a preferred flavor when harvested prior to root development. Sprouts are legally defined, and have additional regulations concerning their production and marketing due to their relatively high risk of microbial contamination compared to other greens. Growers interested in producing sprouts for sale need to be aware of the risks and precautions summarized in the FDA publication Guidance for Industry: Reducing Microbial Food Safety Hazards for Sprouted Seeds (FDA 1999).\n\nGrowing microgreens is relatively easy; many small \"backyard\" growers have sprung up selling their greens at farmers' markets or to restaurants. A shallow plastic container with drainage holes, such as a nursery flat or prepackaged-salad box, will facilitate sprouting and grow out on a small scale. Growing and marketing high-quality microgreens commercially is much more difficult.\n\nResearchers at the USDA Agricultural Research Service have published, as of early 2014, several studies that identify the nutritional make-up and the shelf life of microgreens. Twenty-five varieties were tested, key nutrients measured were ascorbic acid (vitamin C), tocopherols (vitamin E), phylloquinone (vitamin K), and beta-carotene (a vitamin A precursor), plus other related carotenoids in the cotyledons.\n\nAmong the 25 microgreens tested, red cabbage, cilantro, garnet amaranth, and green daikon radish had the highest concentrations of vitamin C, carotenoids, vitamin K, and vitamin E, respectively. In general, microgreens contained considerably higher levels of vitamins and carotenoids—about five times greater—than their mature plant counterparts, an indication that microgreens may be worth the trouble of delivering them fresh during their short lives.A nutritional study of microgreens was done in the summer of 2012 by the Department of Nutrition and Food Science, University of Maryland, indicating promising potential that microgreens may indeed have particularly high nutritional value compared to mature vegetables. Bhimu Patil, a professor of horticulture and director of the Vegetable and Fruit Improvement Center at Texas A&M University, agrees that microgreens may potentially have higher levels of nutrients than mature vegetables. But he says more studies are needed to compare the two side by side. \"This is a very good start, but there can be a lot of variation in nutrients depending on where you grow it, when you harvest, and the soil medium,\" Patil says. When choosing a microgreen, researchers say to look for the most intensely colored ones, which will be the most nutritious.\nResults of the microgreens research project conducted by the University of Maryland and the USDA has garnered attention from several national media outlets including National Public Radio (NPR) and The Huffington Post.\n\nSprouts are germinated or partially germinated seeds. A sprout consists of the seed, root, stem, while microgreens are harvested without the roots.\n\nMicrogreens have stronger flavors compared to sprouts, and come in a wide selection of leaf shapes, textures and colors.\n\nMicrogreens are grown in soil or soil-like materials such as peat moss. Microgreens require high light levels, preferably natural sunlight with low humidity and good air circulation. Microgreens are planted with very low seed density compared to sprout processing. Crop times are generally one to two weeks for most varieties, though some can take four to six weeks. Microgreens are ready to harvest when the leaves are fully expanded. Harvesting is usually with scissors cutting just above the soil surface, excluding any roots. Some growers sell them while still growing, rooted in the growing trays so that they can be cut later. Once removed from their growing environment, these trays of microgreens must be used quickly or they will rapidly begin to elongate and lose color and flavor.\n\nSprout seeds are soaked in water for usually eight hours and then drained. A high density of seed is placed inside of sprouting equipment or enclosed containers. The seed germinates rapidly due to the high moisture and humidity levels maintained in the enclosures. Seeds can also be sprouted in cloth bags that are repeatedly soaked in water. The sprouting process occurs in dark or very low light conditions. These dark, wet, crowded conditions are ideal for the rapid proliferation of dangerous pathogenic bacteria. After a few days of soaking and repeated rinsing in water (several times a day to minimize spoilage), the processing is complete and the sprouts are ready to consume.\n\nThe conditions that are ideal for properly grown microgreens do not encourage the growth of dangerous pathogens. These growing methods would not work for the production of sprouts.\n\nHowever, the potential for food safety issues with microgreens may be increasing due to the number of indoor microgreen growing operations in which excessive seed density, low light intensity, low air circulation or most commonly, a lack of GAP (good agricultural practices) and GMP (good manufacturing practices) based food safety procedures. Certain provisions of the Guidance for Industry: Reducing Microbial Food Safety Hazards For Sprouted Seeds may be beneficial and prudent for growers of microgreens to follow.\n\nMicrogreens have a short shelf life and better methods of storing and transporting microgreens are currently being studied, which at this time are mainly focusing on buckwheat. Commercial microgreens are most often stored in plastic clamshell containers, which do not provide the right balance of oxygen and carbon dioxide for any live greens to breathe. Among package materials called films, differences in permeability, (see Permeation), are referred to as the oxygen transmission rate.\n\nThe ARS researchers found that buckwheat microgreens packaged in films with an oxygen transmission rate of 225 cubic centimeters per square inch per day had a fresher appearance and better cell membrane integrity than those packaged in other films tested. Following these steps, the team maintained acceptable buckwheat microgreen quality for more than 14 days—a significant extension, according to authors. This study was published in LWT-Food Science and Technology in 2013.\n\nLight-emitting diodes. otherwise known as LEDs, now provide the ability to measure impacts of narrow-band wavelengths of light on seedling physiology. The carotenoid zeaxanthin has been hypothesized to be a blue light receptor in plant physiology. A study was carried out to measure the impact of short-duration blue light on phytochemical compounds, which impart the nutritional quality of sprouting broccoli microgreens. Broccoli microgreens were grown in a controlled environment under LEDs using growing pads. Short-duration blue light acted to increase important phytochemical compounds influencing the nutritional value of broccoli microgreens.\n"}
{"id": "53880998", "url": "https://en.wikipedia.org/wiki?curid=53880998", "title": "National Colon Cancer Awareness Month", "text": "National Colon Cancer Awareness Month\n\nNational Colon Cancer Awareness Month is an annual celebration observed in the United States during the month of March, to increase awareness of colorectal cancer. In the United States it is organized by the Colorectal Cancer Alliance.\"\n\nNational Colon Cancer Awareness Month (or National Colorectal Cancer Awareness Month) in the United States was first established via Presidential Proclamation, signed by William Jefferson Clinton on February 29, 2000.\n\nA National Colorectal Cancer Awareness Month proclamation was issued by President Barack Obama for three years between 2014-2016.\n\nThe manner of celebration for national colon cancer awareness month varies, but many organizations host special events to help engage their local communities in raising awareness, such as with the Dress in Blue Day promoted by the Colorectal Cancer Alliance.\n\n"}
{"id": "52559072", "url": "https://en.wikipedia.org/wiki?curid=52559072", "title": "Natural Cycles", "text": "Natural Cycles\n\nNatural Cycles is the first app to be certified for contraception in Europe. The app predicts the days on which a woman is fertile and may be used for planning pregnancy and contraception. It was developed by CERN scientist Dr. Elina Berglund, who founded the company with her husband, Dr. Raoul Scherwitzl.\n\nIn August 2018, the Food and Drug Administration approved U.S. marketing for Natural Cycles.\n\nDr. Berglund was a top physicist at CERN who was part of the Higgs boson discovery, which led to the Nobel Prize in Physics in 2013, before co-founding the company with her husband Dr. Scherwitzl. Because the couple was in search of an alternative natural contraceptive themselves, Dr. Berglund used data analysis to develop an algorithm designed to pinpoint her ovulation. \n\nThe couple then decided to create an app with the underlying algorithm, Natural Cycles. Following several medical trials, the app became the first tech-based device to be certified for use as contraception in the EU, in February 2017 by the European inspection and certification organisation TÜV SÜD. In November 2017 Natural Cycles received a $30M investment in series B round led by EQT Ventures fund, with participation from existing investors Sunstone, E-ventures and Bonnier Growth Media (the VC arm of privately held Swedish media group, the Bonnier Group).\n\nWhile the product is only currently certified in the European Union, where its users are concentrated in the United Kingdom and the Scandinavian countries, it is available worldwide. Natural Cycles offers a subscription product, which now has over 800,000 users across 160 countries (as of June 2018). 75 percent use the app as a contraceptive, and the rest use it to try and become pregnant.\n\nThe app works by taking your temperature each morning, under the tongue as soon as you wake up in the morning and logging it into the app. This is done with a basal thermometer which has a higher sensitivity than a typical fever thermometer as it displays two decimal places. The apps algorithm calculation is based on the observation that post-ovulation, progesterone warms the female body by up to 0.45C.\nNatural Cycles algorithm then determines, based on the temperature, whether the user is fertile or not. A red day means fertile (which is when you should abstain or use a condom), a green day means not fertile.\n\nThe most recent published clinical study showed that natural cycles, when used correctly, is as effective in preventing pregnancies as the contraceptive pill.\nThe study analysed 22,785 women through a total of 224,563 menstrual cycles across a year, to calculate the app’s Pearl Index – the rate used to measure a contraceptive effectiveness. It found that if used perfectly – using protection such as condoms on red days – effectiveness is 99 percent. Typical use, where people become pregnant for all possible reasons, leads to 93 percent effectiveness. This is well above other Fertility awareness based methods that rate at around 75 percent and even the contraceptive pill, which rates at 91 percent. Further studies include the “Identification and prediction of the fertile window using Natural Cycles\" and “Fertility awareness-based mobile application for contraception”. For the app to remain effective, women need to follow the app's instructions correctly, and it does not protect its users from sexually transmitted diseases.\n\nIn 2018, Södersjukhuset, a hospital in Stockholm, Sweden filed a complaint with the Medical Products Agency of Sweden after 37 women who had been using Natural Cycles as their primary method of contraception sought an abortion at the hospital after becoming pregnant unintentionally. Natural Cycles responded by saying the number of pregnancies was within the reported effectiveness rates. In the UK, the app is currently under investigation by the Advertising Standards Authority over supposedly misleading claims in its marketing over the accuracy of the app. A number of users and healthcare professionals have expressed concerns over the efficacy of the app. \n\nIn August 2018, Lauren Streicher, professor of clinical obstetrics at Northwestern University's Feinberg School of Medicine expressed concerns over the Food and Drug Administration's approval of the app. Streicher has claimed that the app is \"problematic\" as the app relies on users' self-reported temperatures which must be taken as soon as they wake up each morning in order to be accurate. In an interview with Vox, Streicher claimed \"The minute you rely on action, the efficacy goes down.” \n\nNatural Cycles has also been criticised for its marketing strategy of paying social media influencers to promote the app. In July 2018 researchers at the London School of Hygiene and Tropical Medicine published a study which claimed “Natural Cycles’ marketing materials ought to be entirely transparent, more clear than they currently are about the limitations of their app and pregnancy risks”\n"}
{"id": "53858250", "url": "https://en.wikipedia.org/wiki?curid=53858250", "title": "Non-consensual condom removal", "text": "Non-consensual condom removal\n\nNon-consensual condom removal, or \"stealthing\", is the practice of one sex partner covertly removing a condom, when consent has only been given by the other sex partner for condom-protected safer sex.\n\nNews and media outlets have reported on a research article by Alexandra Brodsky published in the \"Columbia Journal of Gender and Law\" about stealthing. In the article, Brodsky described victims' experiences, legal implications, and legal avenues to address stealthing. The term \"stealthing\" has been in use in the gay community to describe this practice \n\nBrodsky described how the practice of stealthing is discussed, described, and advocated for on various websites and forums. These forums are sometimes used to brag about committing stealthing and to share tips on how to do it. The practice has also been described as \"a threat to [a victim's] bodily agency and as a dignitary harm\", and men who do this \"justify their actions as a natural male instinct\". Columbia Law School professor Suzanne Goldberg says that the practice of stealthing is likely not new, but its promotion on the internet among men is new. One such website, \"The Experience Project\", posted how-to guides for men.\n\nCondom negotiation is often silenced by male partners in adolescent relationships, partially due to the female's fear of her partner’s response, a feeling of obligation, and a lack of knowledge or skills in negotiating condom use. To prevent this, it is important that male partners are reached with the information as to why condoms are beneficial for them as well. Forums for this outreach could include community-wide interventions fostering discussion of healthy and unhealthy relationship practices and prevention programs for HIV/AIDS and STIs. Schools can provide a safe site for prevention interventions, but high-risk adolescents who are not in school must be reached through additional means, such as in community centers or detention centers.\n\nStatistics on the prevalence of stealthing are limited. However, a 2014 study by Kelly Cue Davis and colleagues reported that 9.0% of participants in their sample of young men reported having engaged in condom sabotage, which included non-consensual condom removal. The National Sexual Assault Hotline reports receiving calls about stealthing.\n\nIn UK law, consent to a specific sex act, but not to any sex act without exceptions, is known as conditional consent. In 2017, a Swiss court convicted a French man for rape for removing a condom during sex against the expectations of the woman he was having sex with. A 2014 Supreme Court of Canada ruling upheld a sexual assault conviction of a man who poked holes in his condom.\n\nExisting laws in the United States do not specifically cover stealthing and there are no known legal cases about it. In her research on stealthing, Brodsky noted that Swiss and Canadian courts have prosecuted cases of condoms broken or removed by men unbeknownst to their partners. Brodsky describes stealthing as legally \"rape-adjacent\" and akin to rape.\n\nAn Australian court case is underway regarding stealthing. The president of the NSW law society has described stealthing as sexual assault because it changes the terms of consent.\n\nBy removing the condom during sex, stealthing increases the risks of pregnancy and the transmittal of STIs and STDs. Victims may experience emotional and psychological distress as well, especially those who have experienced sexual violence in the past.\n\n"}
{"id": "5007230", "url": "https://en.wikipedia.org/wiki?curid=5007230", "title": "Nutrient density", "text": "Nutrient density\n\nNutrient density identifies the proportion of nutrients in foods, with terms such as nutrient rich and micronutrient dense referring to similar properties. Several different national and international standards have been developed and are in use (see Nutritional rating systems).\n\nAccording to the World Health Organization, nutrient profiling classifies and/or ranks foods by their nutritional composition in order to promote human (and/or animal) health and to prevent disease. Ranking by nutrient density is one such nutrient profiling strategy. Ordering foods by nutrient density is a statistical method of comparing foods by the proportion of nutrients in foods. Some such comparisons can be the glycemic index and the Overall Nutritional Quality Index.\n\nNutrient-dense foods such as fruits and vegetables are the opposite of energy-dense food (also called \"empty calorie\" food), such as alcohol and foods high in added sugar or processed cereals.\n\nThe Academy of Nutrition and Dietetics reported in 2013 that:\n\nSeveral indicators of nutrient quality have been summarized by the Academy.\nThe Nutrient Rich Food Index has been developed by a research coalition involving food and nutrition practitioners. This index uses nutrient profiles that have been validated against accepted measures of a healthy diet, such as the Healthy Eating Index created by the USDA.\n\nThe Nutrient Profiling Scoring Calculator (NPSC) in Australia and New Zealand is a calculator for determining whether health claims can be made for a food by its reference to the Nutrient Profiling Scoring Criterion (NPSC). It is defined by the FSANZ Board, which operates under the FSANZ Act.\n\nThe United Kingdom Ofcom nutrient profiling model provides \"a single score for any given food product, based on calculating the number of points for ‘negative’ nutrients which can be offset by points for ‘positive’ nutrients.\" A 2007 UK-commissioned review of nutrient profiling models commissioned by the UK Food Standards Agency identified over 40 different schemes.\n\nThe World Health Organization reviews scientific and operational issues related to human nutrition, specifically when developing world populations are impacted.\n\n"}
{"id": "2613847", "url": "https://en.wikipedia.org/wiki?curid=2613847", "title": "Overnutrition", "text": "Overnutrition\n\nOvernutrition or hyperalimentation is a form of malnutrition in which the intake of nutrients is oversupplied. The amount of nutrients exceeds the amount required for normal growth, development, and metabolism. \n\nThe term can also refer to:\n\nFor mineral excess, see:\n\nOvernutrition may also refer to greater food consumption than appropriate, as well as other feeding procedures such as parenteral nutrition.\n\n"}
{"id": "788074", "url": "https://en.wikipedia.org/wiki?curid=788074", "title": "Physical strength", "text": "Physical strength\n\nPhysical strength is the measure of an animal's exertion of force on physical objects. Increasing physical strength is the goal of strength training.\n\nAn individual's physical strength is determined by two factors; the cross-sectional area of muscle fibers recruited to generate force and the intensity of the recruitment. Individuals with a high proportion of type I slow twitch muscle fibers will be relatively weaker than a similar individual with a high proportion of type II fast twitch fibers, but would have a greater inherent capacity for physical endurance. The genetic inheritance of muscle fiber type sets the outermost boundaries of physical strength possible (barring the use of enhancing agents such as testosterone), though the unique position within this envelope is determined by training. Individual muscle fiber ratios can be determined through a muscle biopsy. Other considerations are the ability to recruit muscle fibers for a particular activity, joint angles, and the length of each limb. For a given cross-section, shorter limbs are able to lift more weight. The ability to gain muscle also varies person to person, based mainly upon genes dictating the amounts of hormones secreted, but also on sex, age, health of the person, and adequate nutrients in the diet. A one-repetition maximum test is the most accurate way to determine maximum muscular strength.\n\nThere are various ways to measure physical strength of a person or population. Strength capability analysis is usually done in the field of ergonomics where a particular task (e.g. lifting a load, pushing a cart, etc.) and/or a posture is evaluated and compared to the capabilities of the section of the population that the task is intended towards. The external reactive moments and forces on the joints are usually used in such cases. The strength capability of the joint is denoted by the amount of moment that the muscle force can create at the joint to counter the external moment.\n\nSkeletal muscles produce reactive forces and moments at the joints. To avoid injury or fatigue, when person is performing a task, such as pushing or lifting a load, the external moments created at the joints due to the load at the hand and the weight of the body segments must be ideally less than the muscular moment strengths at the joint.\n\nOne of the first sagittal-plane models to predict strength was developed by Chaffin in 1969. Based on this model, the external moments at each joint must not exceed the muscle strength moments at that joint.\n\nM < S\n\nWhere, S is the muscle strength moment at joint, j, and M is the external moment at the joint, j, due to load, L and the body segments preceding the joint in the top-down analysis.\n\nTop-down analysis is the method of calculating the reactive moments and forces at each joint starting at the hand, all the way till the ankle and foot. In a 6-segment model, the joints considered are elbow, shoulder, L5/S1 disc of the spine, hip, knee and ankle. It is common to ignore the wrist joint in manual calculations. Software intended for such calculation use the wrist joint also, dividing the lower arm into hand and forearm segments.\n\nStatic strength prediction is the method of predicting the strength capabilities of a person or a population (based on anthropometry) for a particular task and/or posture (an isometric contraction). Manual calculations are usually performed using the top-down analysis on a six or seven-link model, based on available information about the case and then compared to standard guidelines, such as the one provided by the National Institute for Occupational Safety and Health, to predict capability.\n\n"}
{"id": "14655632", "url": "https://en.wikipedia.org/wiki?curid=14655632", "title": "Piskacek's sign", "text": "Piskacek's sign\n\nIn medicine, Piskacek's sign is an indication of pregnancy. This sign, however, may or may not be a concrete probability of pregnancy along with other signs of early pregnancy. Other signs of early pregnancy include Goodell, Hegar, von Braun Fernwald, and Chadwick. \n\nSpecifically, Piskacek's sign consists noting a palpable lateral bulge or soft prominence one of the locations where the uterine tube meets the uterus. Piskacek's sign can be noted in the seventh to eight week of gestation. Non pregnant uterus is pyriform in shape. By 12 weeks of gestation it becomes globular. In lateral implantation, there is asymmetrical enlargement of the uterus. One half of the uterus where the implantation occurred is firm while the other half is soft. This is known as Piskacek's sign.\nThe sign is named after Ludwig Piskaçek.\n"}
{"id": "54696723", "url": "https://en.wikipedia.org/wiki?curid=54696723", "title": "Postpartum chills", "text": "Postpartum chills\n\nPostpartum chills is a physiological response that occurs within two hours of childbirth. It appears as uncontrollable shivering. It is seen in many women after delivery and can be unpleasant. It lasts for a short time. It is thought to be a result of a nervous system response. It may also be related to fluid shifts and the actual strenuous work of labor. It is considered a normal response and there is no accompanying fever. A fever would indicate an infection. Reassurance is all that is needed and for the mother to be kept warm. It has been described as a fairly common and normal occurrence. It is thought to be possibly related to the environmental temperature.\n"}
{"id": "2550591", "url": "https://en.wikipedia.org/wiki?curid=2550591", "title": "Religion and drugs", "text": "Religion and drugs\n\nMany religions have expressed positions on what is acceptable to consume as a means of intoxication for spiritual, pleasure, or medicinal purposes. Psychoactive substances may also play a significant part in the development of religion and religious views as well as in rituals.\n\nIn the book \"Inside the Neolithic Mind\", the authors, archaeologists David Lewis-Williams and David Pearce argue that hallucinogenic drugs formed the basis of neolithic religion and rock art. Similar practices and images are found in some contemporary \"stone-age\" Indigenous peoples in South America using yaje.\n\nSome scholars have suggested that Ancient Greek mystery religions employed entheogen, such as the Kykeon central to the Eleusinian Mysteries, to induce a trance or dream state. Research conducted by John R. Hale, Jelle Zeilinga de Boer, Jeffrey P. Chanton and Henry A. Spiller suggests that the prophecies of the Delphic Oracle were uttered by Priestesses under the influence of gaseous vapors exuded from the ground. Their findings are published in \"Questioning the Delphic Oracle: Overview / An Intoxicating Tale\".\n\nIn Buddhism the Right View (' / ') can also be translated as \"right perspective\", \"right outlook\" or \"right understanding\", is the right way of looking at life, nature, and the world as they really are for us. It is to understand how our reality works. It acts as the reasoning with which someone starts practicing the path. It explains the reasons for our human existence, suffering, sickness, aging, death, the existence of greed, hatred, and delusion. Right view gives direction and efficacy to the other seven path factors. It begins with concepts and propositional knowledge, but through the practice of right concentration, it gradually becomes transmuted into wisdom, which can eradicate the fetters of the mind. An understanding of right view will inspire the person to lead a virtuous life in line with right view. In the Pāli and Chinese canons, it is explained thus:\n\nRight livelihood (\"samyag-ājīva\" / \"sammā-ājīva\"). This means that practitioners ought not to engage in trades or occupations which, either directly or indirectly, result in harm for other living beings. In the Chinese and Pali Canon, it is explained thus:\n\nMore concretely today interpretations include \"work and career need to be integrated into life as a Buddhist,\" it is also an ethical livelihood, \"wealth obtained through rightful means\" (Bhikku Basnagoda Rahula) – that means being honest and ethical in business dealings, not to cheat, lie or steal. As people are spending most of their time at work, it’s important to assess how our work affects our mind and heart. So important questions include \"How can work become meaningful? How can it be a support, not a hindrance, to spiritual practice — a place to deepen our awareness and kindness?\"\n\nThe five types of businesses that should not be undertaken:\n\n\nAccording to the fifth precept of the Pancasila, Buddhists are meant to refrain from any quantity of \"fermented or distilled beverages\" which would prevent mindfulness or cause heedlessness. In the Pali Tipitaka the precept is explicitly concerned with alcoholic beverages:\n\"I undertake the training rule to abstain from fermented drink that causes heedlessness.\"\n\n\"Surāmerayamajjapamādaṭṭhānā veramaṇī sikkhāpadaṃ samādiyāmi.\"\nHowever, caffeine and tea are permitted, even encouraged for monks of most traditions, as it is believed to promote wakefulness.\n\nGenerally speaking, the vast majority of Buddhists and Buddhist sects denounce and have historically frowned upon the use of any intoxicants by an individual who has taken the five precepts. Most Buddhists view the use and abuse of intoxicants to be a hindrance in the development of an enlightened mind. However, there are a few historical and doctrinal exceptions.\n\nMany modern Buddhist schools have strongly discouraged the use of psychoactive drugs of any kind; however, they may not be prohibited in all circumstances in all traditions. Some denominations of tantric or esoteric Buddhism especially exemplify the latter, often with the principle skillful means:\n\nFor example, as part of the ganachakra \"tsok\" ritual (as well as Homa, abhisheka and sometimes drubchen) some Tibetan Buddhists and Bönpos have been known to ingest small amounts of grain alcohol (called \"amrit\" or \"amrita\") as an offering. If a member is an alcoholic, or for some other reason does not wish to partake in the drinking of the alcoholic offering, then he or she may dip a finger in the alcohol and then flick it three times as part of the ceremony.\n\nAmrita is also possibly the same as, or at least in some sense a conceptual derivative of the ancient Hindu \"soma\". (The latter which historians often equate with \"Amanita muscaria\" or other \"Amanita\" psychoactive fungi.) Crowley (1996) states:\n\"Undoubtedly, the striking parallels between \"The legend about Chakdor\" and the Hindu legend of the origin of soma show that the Buddhist amrita and the Hindu soma were at one time understood to be identical. Moreover, the principal property of amrita is, to this day, perceived by Buddhists as being a species of inebriation, however symbolically this inebriation may be interpreted. Why else would beer (Tibetan \"chhang\", \"barley beer\") be used by yogins as a symbolic substitute for amrita [Ardussi]? Conversely, why else would the term bDud.rTsi be used as a poetic synonym for beer?\nConversely, in Tibetan and Sherpa lore there is a story about a monk who came across a woman who told him that he must either:\n\nThe monk thought to himself, \"well, surely if I kill the goat then I will be causing great suffering since a living being will die. If I sleep with the woman then I will have broken another great vow of a monk and will surely be lost to the ways of the world. Lastly, if I drink the beer then perhaps no great harm will come and I will only be intoxicated for a while, and most importantly I will only be hurting myself.\" (In the context of the story this instance is of particular importance to him because monks in the Mahayana and Vajrayana try to bring all sentient beings to enlightenment as part of their goal.)\n\nSo the monk drank the mug of beer and then he became very drunk. In his drunkenness he proceeded to kill the woman and sleep with the goat, breaking all three vows and, at least in his eyes, doing much harm in the world. The lesson of this story is meant to be that, at least according to the cultures from which it delineates, alcohol causes one to break all of one's vows, in a sense that one could say it is the cause of all other harmful deeds.\n\nThe Vajrayana teacher Drupon Thinley Ningpo Rinpoche has said that as part of the five precepts which a layperson takes upon taking refuge, that although they must refrain from taking intoxicants, they may drink enough so as they do not become drunk. Bhikkus and Bhikkunis (monks and nuns, respectively), on the other hand, who have taken the ten vows as part of taking refuge and becoming ordained, cannot imbibe any amount of alcohol or other drugs, other than pharmaceuticals taken as medicine.\n\nTenzin Gyatso, the 14th Dalai Lama of Tibet, is known as teetotaler and non-smoker.\n\nThere is some evidence regarding the use of deliriant \"Datura\" seeds (known as \"candabija\") in Dharmic rituals associated with many tantras – namely the \"Vajramahabhairava\", \"Samputa\", \"Mahakala\", \"Guhyasamaja\", \"Tara\" and \"Krsnayamari\" tantras – as well as cannabis and other entheogens in minority Vajrayana sanghas. Ronald M Davidson says that in Indian Vajrayana, Datura was:\n\n“employed as a narcotic paste or as wood in a fire ceremony and could be easily absorbed through the skin or the lungs. The seeds of this powerful narcotic, termed \"passion seeds\" (candabija), are the strongest elements and contain the alkaloids hyoscine, hyoscyamine, and atropine in forms that survive burning or boiling. In even moderate doses, datura can render a person virtually immobile with severe belladonna-like hallucinations.”\n\nIn the \"Profound Summarizing Notes on the Path Presented as the Three Continua\" a Sakya Lamdre text, by Jamyang Khyentse Wangchuk (1524-1568), the use of Datura in combination with other substances, is prescribed as part of a meditation practice meant to establish that \"All the phenomena included in apparent existence, samsara and nirvana, are not established outside of one's mind.\"\n\nIan Baker writes that Tibetan terma literature such as the Vima Nyingtik describes \"various concoctions of mind altering substances, including datura and oleander, which can be formed into pills or placed directly in the eyes to induce visions and illuminate hidden contents of the psyche.\"\n\nA book titled \"Zig Zag Zen: Buddhism and Psychedelics\" (2002), details the history of Buddhism and the use of psychedelic drugs, and includes essays by modern Buddhist teachers on the topic.\n\nZen Buddhism is known for stressing the precepts. In Japan, however, where Zen flourished historically, there are a number of examples of misconduct on the part of monks and laypeople alike. This often involved the use of alcohol, as sake drinking has and continues to be a well known aspect of Japanese culture.\n\nThe Japanese Zen monk and abbot, shakuhachi player and poet Ikkyu was known for his unconventional take on Zen Buddhism: His style of expressing dharma is sometimes deemed \"Red Thread Zen\" or \"Crazy Cloud Zen\" for its unorthodox characteristics. Ikkyu is considered both a heretic and saint in the Rinzai Zen tradition, and was known for his derogatory poetry, open alcoholism and for frequenting the services of prostitutes in brothels. He personally found no conflict between his lifestyle and Buddhism.\n\nThere are several koans (Zen riddles) referencing the drinking of sake (rice wine); for instance Mumonkan's tenth koan titled \"Seizei Is Utterly Destitute\":\n'Seizei said to Sozan, \"Seizei is utterly destitute. Will you give him support?\" Sozan called out: \"Seizei!\" Seizei responded, \"Yes sir?!\" Sozan said, \"You have finished three cups of the finest wine in China, and still you say you have not yet moistened your lips!\"'\nAnother monk, Gudo, is mentioned in a koan called \"Finding a Diamond on a Muddy Road\" buying a gallon of sake.\n\nJudaism maintains that people do not own their bodies – they belong to God. As a result, Jews are not permitted to harm, mutilate, destroy or take risks with their bodies, life or health with activities such as taking life-threatening drugs. However, there is no general prohibition against drugs in Judaism, as long as they don't interfere with one's ritual duties and don't cause definite harm, though most Rabbis generally prohibit drugs, in order to avoid social, legal and medical problems in their community.\n\nSpiritual use of various alcoholic beverages, sometimes in very large quantities, is common and well known. In some Jewish communities there is a tradition to get drunk on Purim until they forget the difference between the Hebrew phrases \"Cursed is Haman\" and \"Blessed is Mordechai\", which signified reaching the spiritual world Atzilut where all opposites unite. In many Jewish communities it is customary to drink on Simchat Torah as well. Drinking in small quantities as a mind-altering practice is commonly used during the Farbrengens of the Chabad Hasidim. A large body of Chabad literature refers to the spiritual dangers of drinking, but a few anecdotal references refer to the spirutal power of alcohol, when used for the sake of connecting to God and achieving brotherly love among fellows Jews. The Lubavitcher Rebbe forbade his Chassidim under the age of 40 to consume more than 4 small shots of hard liqueurs. Wine plays a prominent role in many Jewish rituals, most notably the kiddush.\nHasidic Jews often engage in a free ceremony called \"Tisch\" in which drinks such as Vodka are drunk in a group. Drinking is accompanied by singing and the study of the Torah.\n\nSome Hasidic Rabbis, e.g. the Ribnitzer Rebbe used to drink large amounts of Vodka on some special occasions, apparently as a powerful mind-altering method. The Ribnitzer Rebbe also practiced severe sleep deprivation, extremely long meditative prayers and a number of ascetic purification rituals. During his life in the USSR he used to immerse himself every day in ice water.\n\nThe spiritual use of caffeine and nicotine as stimulants is well known in the Hasidic communities. Many stories are told about miracles and spiritual journeys performed by the Baal Shem Tov and other famous Tzaddikim with the help of their smoking pipe. Some people suggest that, judging by the nature of these stories, the tobacco was sometimes mixed with strong mind-altering drugs.\n\nA popular Hasidic saying relates coffee to the Psalmic verse \"Hope in God\". The Hebrew word for \"hope\" (\"Kave\") sounds identical to the Yiddish word for coffee. Coffee is believed to have power to awaken the soul to the worship of God.\n\nSome Kabbalists, including Isaac of Acco and Abraham Abulafia, mention a method of \"philosophical meditation\", which involves drinking a cup of \"strong wine of Avicenna\", which would induce a trance and would help the adept to ponder over difficult philosophical questions. The exact recipe of this wine remains unknown; Avicenna refers in his works to the effects of opium and datura extracts.\n\nRabbi Aryeh Kaplan, a prominent researcher of Jewish meditations, suggested that some medieval Kabbalists may have used some psychedelic drugs Indeed, one can find in Kabbalistic medical manuals cryptic references to the hidden powers of mandrake, harmal and other psychoactive plants, though the exact usage of these powers is hard to decipher.\n\nAccording to Aryeh Kaplan, cannabis was an ingredient in the Holy anointing oil mentioned in various sacred Hebrew texts. The herb of interest is most commonly known as \"kaneh-bosem\" (קְנֵה-בֹשֶׂם) which is mentioned several times in the Old Testament as a bartering material, incense, and an ingredient in Holy anointing oil used by the high priest of the temple. Many Rastafarians, who use cannabis as a sacrament, identify as Jewish.\n\nAccording to Josephus, the head-dress of the Jewish High Priests' was modeled upon the capsule of the Hyoscyamus flower, which he calls \"Saccharus\". This Greek word for sugar stems from the Hebrew root that means \"intoxicating\".\n\nBenny Shanon, a psychology professor at the Hebrew University of Jerusalem, proposed that Moses may have been high on hallucinogenic mushrooms at the time he received the Ten Commandments.\n\nMany Christian denominations disapprove of the use of most illicit drugs. Many denominations permit the moderate use of socially and legally acceptable drugs like alcohol, caffeine and tobacco. Some Christian denominations permit smoking tobacco, while others disapprove of it. Many denominations do not have any official stance on drug use, some more-recent Christian denominations (e.g. Mormons, Seventh-day Adventists and Jehovah’s Witnesses) discourage or prohibit the use of any of these substances.\n\nBecause Jesus and many Biblical figures drank wine, most Christian denominations do not require teetotalism. In the Eucharist, wine represents (or among Christians who believe in some form of Real Presence, like the Catholic, Lutheran and Orthodox churches, actually \"is\") the blood of Christ. Lutherans believe in the real presence of the body and blood of Christ in the Eucharist, that the body and blood of Christ are \"truly and substantially present in, with and under the forms.\" of the consecrated bread and wine (the elements), so that communicants orally eat and drink the holy body and blood of Christ Himself as well as the bread and wine (cf. Augsburg Confession, Article 10) in this Sacrament. The Lutheran doctrine of the Real Presence is more accurately and formally known as \"the Sacramental Union.\" It has been inaccurately called \"consubstantiation\", a term which is specifically rejected by most Lutheran churches and theologians.\n\nOn the other hand, some Protestant Christian denominations, such as Baptists and Methodists associated with the temperance movement, encourage or require teetotalism. In some Protestant denomination, grape juice or non-alcoholic wine is used in place of wine to represent the blood of Christ.\nThe best known Western prohibition against alcohol happened in the United States in the 1920s, where concerned prohibitionists were worried about its dangerous side effects. However, the demand for alcohol remained and criminals stepped in and created the supply. The consequences of organized crime and the popular demand for alcohol, led to alcohol being legalized again.\n\nIslam prohibits all drugs that are not medically prescribed. Islam's prohibition of drugs stems from two concerns:\n\nThere are numerous verses in the Qur'an and hadith that ban \"khamr\" (intoxicants, including alcohol). Muhammad said:\nEvery intoxicant is like alcohol, and every (type of) alcohol is prohibited. (Muslim)\n\nThe second reason for banning drugs is that they are believed to have a harmful effect on the body. The Qur'an says,\n\n\"And make not your own hands contribute to your destruction.\" Surah, Al-Baqara, 2: 195\n\nThe Muslim nations of Turkey and Egypt were instrumental in banning opium, cocaine, and cannabis when the League of Nations committed to the 1925 International Convention relating to opium and other drugs (later the 1934 Dangerous Drugs Act). The primary goal was to ban opium and cocaine, but cannabis was added to the list, and it remained there largely unnoticed due to the much more heated debate over opium and coca. The 1925 Act has been the foundation upon which every subsequent policy in the United Nations has been founded. Cannabis use and abuse as an intoxicant was largely unknown in the West at that point, but Islamic leaders have been critical of it since the 13th century.\n\nO You who believe! Intoxicants and gambling, (dedication of) stones and (divination by) arrows are an abomination of Satan’s handiwork. Avoid (such abominations) that you may prosper. (5:90)\n\nSatan’s plan is to sow hatred and enmity amongst you with intoxicants and gambling, and to hamper you from the remembrance of Allah and from prayer. Will you not give up? (5:91)\n\nThere are no prohibitions in Islam on alcohol for scientific, industrial or automotive use.\n\nIn spite of these restrictions on substance use, tobacco and caffeine are still widely used throughout many Muslim nations.\n\nBahá'ís are forbidden to drink alcohol or to take drugs, unless prescribed by doctors. Accordingly, the sale and trafficking of such substances is also forbidden. Smoking is discouraged but not prohibited.\n\nMany Rastafari believe cannabis, which they call \"ganja,\" \"the herb,\" or \"Kaya,\" is a sacred gift of Jah. It may be used for spiritual purposes to commune with God, but should not be used profanely. The use of other drugs, however, including alcohol, is frowned upon. Many believe that the wine Jesus/Iyesus drank was not an alcoholic beverage, but simply the juice of grapes or other fruits.\n\nWhile some Rastafari suggest that the Bible may refer to marijuana, it is generally held by academics specializing in the archaeology and paleobotany of Ancient Israel, and those specializing in the lexicography of the Hebrew Bible, that cannabis is not documented or mentioned in early Judaism. Against this some popular writers have argued that there is evidence for religious use of cannabis in the Hebrew Bible, although this hypothesis and some of the specific case studies (e.g., John Allegro in relation to Qumran, 1970) have been \"widely dismissed as erroneous\" (Merlin, 2003). The primary advocate of a religious use of cannabis plant in early Judaism was Sula Benet (1967), who claimed that the plant \"kaneh bosm קְנֵה-בֹשֶׂם\" mentioned five times in the Hebrew Bible, and used in the holy anointing oil of the Book of Exodus, was in fact cannabis, although lexicons of Hebrew and dictionaries of plants of the Bible such as by Michael Zohary (1985), Hans Arne Jensen (2004) and James A. Duke (2010) and others identify the plant in question as either \"Acorus calamus\" or \"Cymbopogon citratus\".\n\nA \"reasoning\" is a simple event where the Rastas gather, smoke cannabis (\"ganja\"), and discuss. The person honored by being allowed to light the herb says a short sentence beforehand. The ganja is passed in a clockwise fashion (passing 'pon the lef' han' side) except in times of war, when it is passed counterclockwise. It is used to reason with Jah.\n\nA \"groundation\" (also spelled \"grounation\") or \"binghi\" is a holy day; the name \"binghi\" is derived from \"Nyabinghi\" (literally \"Nya\" meaning \"black\" and \"Binghi\" meaning \"victory\"). Binghis are marked by much dancing, singing, feasting, and the smoking of \"ganja\", and can last for several days.\n\n...thou shalt eat the herb of the field.\n\nGenesis 3.18\n\n...eat every herb of the land.\n\nExodus 10:12\n\nBetter is a dinner of herb where love is, than a stalled ox and hatred therewith.\n\nProverbs 15:17\n\nAccording to many Rastas, the illegality of cannabis in many nations is evidence of persecution of Rastafari. They are not surprised that it is illegal, viewing Cannabis as a powerful substance that opens people's minds to the truth – something the Babylon system, they reason, clearly does not want. Cannabis use is contrasted with the use of alcohol and other drugs, which they feel destroy the mind.\n\nAlcoholic drinks are commonly used during Asatru blots but non-alcoholic drinks can be substituted.\n\n"}
{"id": "5584134", "url": "https://en.wikipedia.org/wiki?curid=5584134", "title": "Safe bottle lamp", "text": "Safe bottle lamp\n\nThe Safe bottle lamp, called sudeepa or sudipa for \"good lamp\", is a safer kerosene lamp designed by Wijaya Godakumbura of Sri Lanka. The safety comes from heavier glass, a secure screw-on metal lid, and two flat sides which prevent it from rolling if knocked over.\n\nAs surgeon Dr. Godakumbura saw many burn cases caused by kerosene lamp fires. Over 1 million homes in Sri Lanka do not have electricity, and rely on kerosene lamps for illumination, often improvised lamps made from bottles. These tall lamps tip easily, and when they do, the wick holder often falls out and starts a sudden, intense fire. Often the fuel falls on a nearby person, setting them ablaze and resulting in severe burns, often fatal.\n\nIn 1992, Dr. Godakumbura set out to design a new lamp that was both safer, and inexpensive enough to be affordable by the impoverished Sri Lankans at risk for these fires. The resulting lamp is a small, flattened sphere, which resists tipping and rolling. It is made of thick glass to resist breaking, and has a screw-on metal cap that holds the wick in place and prevents spilling.\n\nIn 1993, with contributions from numerous sources, including Science Fiction writer and Sri Lanka resident Arthur C. Clarke, and the Canadian High Commission, the lamp was put into production.\n\nAvailable for a cost of less than US$0.25 each, over half a million of the new lamps have been sold, and Dr. Godakumbura hopes to continue producing the new lamps until use of improvised lamps drops to a small percentage of lamp use in Sri Lanka.\n\nHaving received a Rolex Award for Enterprise in 1998, Dr. Godakumbura established the Safe Bottle Lamp Foundation (SBLF), a non-profit organization. The Foundation is governed by a board of directors and employs two full-time staff .\n\nIn addition to the Rolex Award, the foundation and Dr. Godakumbura have received a range of other local and international awards and grants. Among these are a Lindbergh Foundation Grant and a BBC World Challenge Award. The project has been featured in many international publications such as TIME, Newsweek, Science and Nature, National Geographic and La Figaro.\n\nDr. Godakumbura has represented the foundation in many international conferences on burn and accident prevention as a speaker or as a participant. The foundation and the Sudeepa lamp have been promoted as a replicable solution for other developing countries where accidental burns due to unsafe lamps is prevalent.\n\n\n"}
{"id": "22287305", "url": "https://en.wikipedia.org/wiki?curid=22287305", "title": "Tannerella forsythia", "text": "Tannerella forsythia\n\nTannerella forsythia is an anaerobic, Gram-negative bacterial species of the Cytophaga-Bacteroidetes family. It has been implicated in periodontal diseases and is a member of the red complex of periodontal pathogens. \"T. forsythia\" was previously named \"Bacteroides forsythus\" and \"Tannerella forsythensis\".\n\n\"Tannerella forsythia\" was discovered by and named after Dr. Anne Tanner who works at The Forsyth Institute located in Cambridge, Massachusetts.\n\n\"T. forsythia\" has been identified in atherosclerotic lesions. Lee et al. found that infecting mice with \"T. forsythia\" induced foam cell formation and accelerated the formation of atherosclerotic lesions. It has also been isolated from women with bacterial vaginosis. The presence of oral \"T. forsythia\" has been found to be associated with an increased risk of esophageal cancer.\n\nList of bacterial vaginosis microbiota\n\n"}
{"id": "41622664", "url": "https://en.wikipedia.org/wiki?curid=41622664", "title": "Women's health nurse practitioner", "text": "Women's health nurse practitioner\n\nA women's health nurse practitioner (WHNP) is a nurse practitioner that specializes in continuing and comprehensive healthcare for women across the lifespan with emphasis on conditions unique to women from menarche through the remainder of their life cycle. \n\nFollowing educational preparation at the master's or doctoral level, WHNPs must become board certified by an approved certification body. Board certification must be maintained by obtaining continuing nursing education credits. In the US, board certification is provided through the National Certification Corporation (awards the WHNP-BC credential).\n\nWHNPs deliver a range of acute, chronic, and preventive healthcare services:\n\n"}
{"id": "54729058", "url": "https://en.wikipedia.org/wiki?curid=54729058", "title": "World Mental Health survey initiative", "text": "World Mental Health survey initiative\n\nThe World Mental Health Survey Initiative is a collaborative project by World Health Organization, Harvard University, University of Michigan, and country-based researchers worldwide to coordinate the analysis and implementation of epidemiological surveys of mental and behavioral disorders and substance abuse in all WHO Regions.\n\nIt is estimated that the burden of mental and addictive disorders are among the highest in the world with expected increase over the next decades. However, those estimations are not based on cross-sectional epidemiological surveys, rather, they are mainly based on literature reviews and isolated studies.\nThe WMH Survey Initiative aim is to accurately address the global burden of mental disorders by obtaining accurate cross-sectional information about the prevalences and correlates of mental and behavioral disorders as well as substance abuse, allowing for evaluation of risk factors and study of patterns of service use in order to target appropriate interventions.\n\nCollaborators in this survey come from all WHO regions of the world, with 27 participating countries.\n"}
