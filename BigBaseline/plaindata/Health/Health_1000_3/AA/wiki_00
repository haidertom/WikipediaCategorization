{"id": "58831897", "url": "https://en.wikipedia.org/wiki?curid=58831897", "title": "Ann MacKinnon", "text": "Ann MacKinnon\n\nAnn MacKinnon, also Annie P. MacKinnon, (1879-1953) was born in Roag, in the Isle of Skye. During the First World War she joined the French Flag Nursing Corps and served in France. For this she was awarded the French military award, the Croix de Guerre. Following the war she remained in France to work before moving to the USA and working for the Frontier Nursing Service in Kentucky.\n\nMacKinnon was born on the 4 March 1879 in Roag, Isle of Skye. Her parents were John MacKinnon and Georgina Urquhart. MacKinnon was educated at Dunvegan Primary School before moving to the central belt to train as a nurse. She spent four years at the School of Nursing in Ayr Hospital then went on to gain the Queen's District Nursing and Midwifery qualification in Edinburgh. MacKinnon worked as a probationer nurse in Braehead Cottage Hospital, Dumbarton from 1901.\n\nIn 1914, Ann MacKinnon joined the French Flag Nursing Corps and soon after travelled to France as the First World War intensified. MacKinnon and the other 'British Nurses in France' were on the Front Line. In 1918 MacKinnon was awarded the Croix de Guerre for bravery during the Third Battle of the Aisne. This was recorded by The Times newspaper and The British Journal of Nursing: \"Following is the citation of the French army conferring the Croix de Guerre on Miss Annie Mackinnon, a British nurse attached to the French Flag Nursing coprs. A nurse who under the difficult circumstances of the withdrawal of the ambulance under enemy fire continued to attend the sick and wounded up to the last minute with remarkable courage and thus demonstrating the three years that she has devoted herself to the French soldiers.\"Following the war MacKinnon remained in France working in Marseille and Paris for the Rockefeller Foundation which was part of the relief effort sending food supplies to the European continent. Her main role was to educate a group of French girls about nursing, including tuberculosis and infant welfare.\n\nMacKinnon travelled to America on the Mormacswan, arriving in New York in 1928. She worked in Beech Fork Nursing Centre and the Hayden Hospital, Kentucky. Mary Breckinridge appointed MacKinnon superintendent of Hayden Hospital in 1929. MacKinnon held this post until 1940, when she travelled to Europe during the Second World War, returning to the USA and the role in 1948. Hayden Hospital was part of the newly created the Frontier Nursing Service which included midwifes in their team of public health services and covered a population of close to ten thousand people. The Frontier Nursing Service (FNS) established six outposts in the Appalachian Mountains in eastern North America. The FNS was established by Mary Breckinridge after she travelled to the Highlands and Islands to see how healthcare is provided in rural locations. Her goal was to replicate the Highlands and Islands Medical and Nursing Service model in the Appalachian area of America. MacKinnon became known as Ann of Appalachia and rode a mule called Tenacity to work.\n\nIn 1930, along with Breckinridge, MacKinnon was a founding member of The Kentucky State Association of Midwives. This was the first association of midwives in the USA.\n\nAnn MacKinnon died of a heart attack on 6 February 1953 in Hayden, Kentucky. She is buried at Wendover, Kentucky.\n\nAnn MacKinnon was awarded the Croix de Guerre for bravery during the First World War.\n"}
{"id": "11101687", "url": "https://en.wikipedia.org/wiki?curid=11101687", "title": "Bachelor of Science in Public Health", "text": "Bachelor of Science in Public Health\n\nThe Bachelor of Science in Public Health (BSPH) (or Bachelor of Public Health) is an undergraduate degree that prepares students to pursue careers in the public, private, or non-profit sector in areas such as public health, environmental health, health administration, epidemiology, nutrition, biostatistics, or health policy and planning. Postbaccalaureate training is available in public health, health administration, public affairs, and related areas.\n\nThe University of California at Irvine, Program in Public Health, Department of Population Health and Disease Prevention, has the largest enrollment of undergraduate majors in Public Health, with about 1,500 students including ~1,000 in the Bachelor of Science in Public Health Sciences, and another ~500 students in the Bachelor of Arts in Public Health Policy (2014). UC Irvine also offers a minor in Public Health for students of other majors.\n\nThe Council on Education for Public Health includes undergraduate public health degrees in the accreditation review of public health programs and schools.\n\n\n"}
{"id": "43772633", "url": "https://en.wikipedia.org/wiki?curid=43772633", "title": "Back labor", "text": "Back labor\n\nBack labor (less commonly called posterior labor) is a term referring to sensations of pain or discomfort that occur in the lower back, just above the tailbone, to a mother during childbirth.\n\nBack labor may be noted when the baby is face up in the birth canal (occiput posterior), and not face down, so that the back of the baby's skull (occiput) is pushed against the mother's sacrum. But back labor can also occur when the baby is not in that position. The discomfort is often noted to be intensely painful, and may not completely abate between contractions. Whether back labor will occur cannot be predicted in advance. Reports of how many mothers experience back labor vary, though estimates in the range of 30% are common.\n\nActions that have been suggested to ameliorate back labor include physical activity, changing positions, back rubbing, water massage, application of hot or cold to the lower back, use of a birthing ball and medication including an epidural. Some research has suggested that injecting sterile water into the lower back may provide pain relief, but there is no consensus that it actually helps.\n\n"}
{"id": "20218615", "url": "https://en.wikipedia.org/wiki?curid=20218615", "title": "Birth spacing", "text": "Birth spacing\n\nBirth spacing, pregnancy spacing, or inter-pregnancy interval refers to how soon after a prior pregnancy a woman becomes pregnant or gives birth again. There are health risks associated both with pregnancies placed closely together and those placed far apart, but the majority of health risks are associated with births that occur too close together. The March of Dimes recommends a minimum of 18 months before becoming pregnant again following an uncomplicated vaginal delivery of a full term infant; the WHO recommends 24 months. A shorter interval may be appropriate if the pregnancy ended in abortion or miscarriage, typically 6 months. If the mother has had a prior C-section, it is advisable to wait before giving birth again due to the risk of uterine rupture in the mother during childbirth, with recommendations of a minimum inter-delivery interval ranging from a year to three years. Pregnancy intervals longer than 5 years are associated with an increased risk of pre-eclampsia. The global public heath burden of short inter-pregnancy intervals is substantial. Family planning can help increase inter-pregnancy interval.\n\nFamily planning, such as the use of contraceptives can increase inter-pregnancy interval. Breastfeeding and extended breastfeeding can also increase birth spacing due to lactational amenorrhea.\n\nCultural and religious attitudes towards both sex and the use of contraceptives, price and availability of health care, and poverty are all factors which can decrease inter-pregnancy intervals.\n\nShort inter-pregnancy interval is associated with an increase in maternal mortality, stillbirth, and child mortality. Inter-pregnancy interval of lower than 18 months is associated with an increased risk of pre-term birth. \n\nShort inter-pregnancy interval after a prior C-section can be a contraindication for having a vaginal birth after a prior C-section (VBAC). In one study inter-pregnancy intervals shorter than 6 months were associated with 2-3 times increased risk of uterine rupture, major morbidity, and blood transfusion during vaginal delivery in mothers with at least one prior C-section. \n\nComplications of a short inter-pregnancy interval are lower after an abortion or miscarriage compared to a full-term pregnancy. \n\nA inter-pregnancy interval of greater than 5 years is associated in an increased risk of pre-eclampsia.\n\nThe global public heath burden of short inter-pregnancy intervals is substantial. In developing countries, children born two years or early after an older sibling were at a 60% increased risk of death in infancy, while those born between two and three years had a 10% increase, compared with those born after intervals of four to five years. Various organisations, including the World Health Organization and the Bill and Melinda Gates Foundation have identified birth spacing as an important area of public health intervention. \n"}
{"id": "1709515", "url": "https://en.wikipedia.org/wiki?curid=1709515", "title": "Bloody show", "text": "Bloody show\n\nBloody show is the passage of a small amount of blood or blood-tinged mucus through the vagina near the end of pregnancy. It is caused by the detachment of the cervical mucus plug that seals the cervix during pregnancy, and is one of the signs that labor may be imminent. Although the bloody show is a common and harmless cause of bleeding in late pregnancy, there are many other possible causes for vaginal bleeding, some of which may indicate serious medical problems such as miscarriage or placental abruption, and vaginal bleeding should not be ignored as a possible sign of serious complications. \n\n"}
{"id": "1591856", "url": "https://en.wikipedia.org/wiki?curid=1591856", "title": "Cervical dilation", "text": "Cervical dilation\n\nCervical dilation (or cervical dilatation) is the opening of the cervix, the entrance to the uterus, during childbirth, miscarriage, induced abortion, or gynecological surgery. Cervical dilation may occur naturally, or may be induced by surgical or medical means.\n\nIn the later stages of pregnancy, the cervix may already have opened up to 1–3 cm (or more in rarer circumstances), but during labor, repeated uterine contractions lead to further widening of the cervix to about 6 centimeters. From that point, pressure from the presenting part (head in vertex births or bottom in breech births), along with uterine contractions, will dilate the cervix to 10 centimeters, which is \"complete.\" Cervical dilation is accompanied by effacement, the thinning of the cervix.\n\nGeneral guidelines for cervical dilation:\n\nDuring pregnancy, the os (opening) of the cervix is blocked by a thick plug of mucus to prevent bacteria from entering the uterus. During dilation, this plug is loosened. It may come out as one piece, or as thick mucus discharge from the vagina. When this occurs, it is an indication that the cervix is beginning to dilate, although not all women will notice this mucus plug being released.\n\nBloody show is another indication that the cervix is dilating. Bloody show usually comes along with the mucus plug, and may continue throughout labor, making the mucus tinged pink, red or brown. Fresh, red blood is usually not associated with dilation, but rather serious complications such as placental abruption, or placenta previa. Red blood in small quantities often also follows an exam.\n\nThe pain experienced during dilation is similar to that of menstruation (although markedly more intense), as period pains are thought to be due to the passing of endometrium through the cervix. Most of the pain during labor is caused by the uterus contracting to dilate the cervix.\n\nProstaglandins (P2 and PGE2) contribute to cervical ripening and dilation. The body produces these hormones naturally. Sometimes prostaglandins in synthesized forms are applied directly to the cervix to induce labor. In women who have had a previous cesarean section, the American College of Obstetricians and Gynecologists issued a bulletin that misoprostol never be used for this purpose. ACOG's findings conclude that the collagen softening properties of misoprostol could be absorbed through the cervix and vaginal vault up into the low transverse scar of a typical cesarean section, and significantly increase the risk of uterine rupture. Prostaglandins are also present in human semen, and sexual intercourse is commonly recommended for promoting the onset of labor, although the limited data available makes the effectiveness of this method uncertain.\n\nOther means of natural cervical ripening include nipple stimulation, which produces oxytocin, a hormone which is necessary for uterine contractions. Nipple stimulation can be performed manually, by use of a breast pump, or by suckling. Henci Goer, in her comprehensive book, The Thinking Woman's Guide to a Better Birth, details how this practice was researched in two separate studies of 100 and 200 women in the mid nineteen-eighties. Women were assigned randomly to two groups. In one group, nipples were stimulated for one-hour sessions, three times per day. In the other group, women were to avoid any form of nipple stimulation or sexual intercourse. The researchers concluded in both studies that nipple stimulation could indeed ripen the cervix and in some cases induce uterine contractions. Goer further notes that in the smaller study, an external fetal monitor was used, and no uterine hyperstimulation was noted.\n\nCervical dilation may be induced mechanically by placing devices inside the cervix that will expand while in place. A balloon catheter may be used. Other products include laminaria stick (made of dried seaweed) or synthetic hygroscopic materials, which expand when placed in a moist environment.\n\nIn hysteroscopy, the diameter of the hysteroscope is generally too large to conveniently pass the cervix directly, thereby necessitating cervical dilation to be performed prior to insertion. Cervical dilation can be performed by temporarily stretching the cervix with a series of \"(cervical) dilators\" of increasing diameter. Misoprostol prior to hysteroscopy for cervical dilation appears to facilitate an easier and uncomplicated procedure only in premenopausal women.\n"}
{"id": "15807148", "url": "https://en.wikipedia.org/wiki?curid=15807148", "title": "Chelates in animal nutrition", "text": "Chelates in animal nutrition\n\nChelates ( che·late ) [kee-leyt] in animal feed are organic forms of essential trace minerals such as copper, iron, manganese and zinc.\n\nAnimals absorb, digest and use mineral chelates better than inorganic minerals. This means that lower concentrations can be used in animal feeds. In addition, animals fed chelated sources of essential trace minerals excrete lower amounts in their faeces, and so there is less environmental contamination. Mineral chelates also offers health and welfare benefits in animal nutrition\n\nSince the 1950s, animal feeds have been supplemented with essential trace minerals such as copper (Cu), iron (Fe), iodine (I), manganese (Mn), molybdenum (Mo), selenium (Se) and zinc (Zn). Initially, such supplementation was by means of inorganic salts of essential trace elements. From the 1960s onwards, genetic improvement of farm livestock resulted in increased nutritional requirements for these nutrients. Chelated minerals were developed in the 1980s and 1990s. Trace mineral chelates have proven to be better than inorganic minerals in meeting the nutritional needs of modern farm animals.\n\nThe objective of supplementation with trace minerals is to avoid a variety of deficiency diseases. Trace minerals carry out key functions in relation to many metabolic processes, most notably as catalysts for enzymes and hormones, and are essential for optimum health, growth and productivity. For example, supplementary minerals help ensure good growth, bone development, feathering in birds, hoof, skin and hair quality in mammals, enzyme structure and functions, and appetite. Deficiency of trace minerals affect many metabolic processes and so may be manifested by different symptoms, such as poor growth and appetite, reproductive failures, impaired immune responses, and general ill-thrift. From the 1950s to the 1990s most trace mineral supplementation of animal diets was in the form of inorganic minerals, and these largely eradicated associated deficiency diseases in farm animals.\n\nThe role in fertility and reproductive diseases of dairy cattle highlights that organic forms of Zn are retained better than inorganic sources and so may provide greater benefit in disease prevention, notably mastitis and lameness\n\nIn recent decades, global food animal production has intensified and genetic potential for growth and yields has improved. As a result, commercial tendencies have been to increase trace mineral supplementation,in order to allow for the greater mineral requirements of superior stock reared under industrial conditions. Increasing the concentration of inorganic minerals in animal diets has led to several problems.\n\nThe use of high Cu in swine and poultry rations has caused accidental Cu poisoning in more sensitive animals, such as sheep grazing pastures fertilised with pig or poultry manure SCAN (2003a) Opinion of the Scientific Committee for Animal Nutrition on the use of copper in feedingstuffs. Secondly, inorganic minerals may form insoluble complexes with other dietary agents resulting in low absorption. In addition, it is thought that the positive charge of many inorganic minerals reduces access to the enterocytes due to repulsion by the negatively charged mucin layer and competition for binding sites.\n\nFinally, the poor retention and high excretion rates of inorganic minerals led to environmental concerns during the 1980s and 1990s, especially in Europe .Opinion of the Scientific Committee for Animal Nutrition on the use of zinc in feedingstuffs]. The European Union is concerned about possible detrimental effects of excess supplementation with trace minerals on the environment or human and animal health, and so in 2003 legislated a reduction in permitted feed concentrations of several trace metals (Co, Cu, Fe, Mn and Zn).\n\nResearch in trace element nutrition has led to the development of more bioavailable organic minerals, including trace minerals derived from chelates. Chelates allow a lower supplementation rate of trace minerals with an equivalent or improved effect on animal health, growth and productivity . Some samples of natural minerals at the Wikimedia Commons:\n\n\nSome notice concluded that the utilisation of organic Cu from a copper chelate or copper lysine were higher than that of inorganic Cu sulfate when fed to rats in the presence and absence of elemental Zn or Fe. The data suggest that, unlike inorganic Cu, organic Cu chelates exhibit absorption and excretion mechanisms that do not interfere with Fe. Copper chelate also achieved higher liver Zn, suggesting less interference at gut absorption sites in comparison with the other forms of Cu.\n\nEffect of organic zinc sources on performance, zinc status and carcass, meat and claw quality in fattening bulls. Livestock Prod. compared a Zn chelate, a Zn polysaccharide complex and ZnO (inorganic zinc oxide) in bull beef cattle, and concluded that the organic forms resulted in some improvement in hoof claw quality.\n\nCompared the bioavailability of Cu and Zn chelates in sheep with the inorganic sulfate forms, at \"low\" and \"high\" supplementation rates. Copper and Zn chelates at the lower rates caused significantly greater increases in blood plasma concentrations than the corresponding treatments with Zn sulfate (p<0.05) and Cu sulfate (p<0.01).\nIn addition, Zinc chelate supplementation resulted in significantly greater hoof horn Zn content than did Zn sulfate (p<0.05). At the \"low\" supplementation rate Zinc chelate achieved better hoof quality than Zn sulfate (p<0.05). The data suggest that Cu and Zn chelates are more readily absorbed and more easily deposited in key tissues such as hooves, in comparison with inorganic Zn forms.\n\nIn weaned piglets evaluated various supplementation rates of organic Zn in the form of a chelate or as a polysaccharide complex and compared these with ZnO, zinc oxide, at 2,000 ppm. Feeding lower concentrations of organic Zn greatly decreased the amount of Zn excreted in comparison with inorganic Zn, without loss of growth performance.\n\nStudied a Copper chelate in weaned pigs in comparison with inorganic Cu and sulfate. Piglet performance was consistently better with organic Cu at 50 to 100 ppm, in comparison with inorganic Cu at 250 ppm. In addition, organic Cu increased Cu absorption and retention, and decreased Cu excretion 77% and 61% respectively, compared with 250 ppm inorganic Cu.\n\nThe effects of an Mg chelate in broiler chickens in comparison with magnesium oxide and an unsupplemented control group. Diets for fattening chicken are not normally supplemented with Mg, but this study indicated positive effects on performance and meat quality. During the first 3 weeks of life, the Mg chelate improved feed efficiency significantly in comparison with both the inorganic MgO and the negative control group (p<0.05). Thigh meat pH and oxidative deterioration during storage were also studied. The Mg chelate increased thigh meat pH in comparison with the negative control (p<0.05). Mg supplementation significantly reduced chemical indicators (TBARS) of oxidative deterioration in liver and thigh muscle (p<0.01), with Mg chelate significantly more efficient than MgO (p<0.01). The data suggest that organic Mg in the form of a chelate is capable of reducing oxidation, and so improve chicken meat quality.\n\nA Zn chelate supplement was compared with Zn sulfate in broiler chickens.Weight gain and feed intake increased quadratically (p<0.05) with increasing Zn concentrations from the chelate and linearly with Zn sulfate. The relative bioavailability of the Zn chelate was 183% and 157% of Zn sulfate for weight gain and tibia Zn, respectively. The authors concluded that the supplemental concentration of Zn required in corn-soy diets for broilers from 1–21 days of age would be 9.8 mg/kg diet as Zn chelate and 20.1 mg/kg diet as Zn sulfate,respectively.\n\nThe effect of replacing inorganic minerals with organic minerals in broiler chickens. One group of chickens received inorganic sulfates of Cu (12 ppm), Fe (45 ppm), Mn (70 ppm) and Zn (37 ppm) and their performance was compared to a similar group supplemented with chelates of Cu (2.5 ppm), Fe, Mn, and Zn (all at 10 ppm). \nThere were no differences in performance between the birds fed the high inorganic minerals and the birds fed the low organic chelates. Faecal concentrations of Cu, Fe, Mn and Zn were 55%, 73%, 46% and 63%, respectively, of control birds fed inorganic minerals.\n\nA broiler study reported also compared inorganic and organic mineral supplementation in broiler chickens. Control birds were fed Cu, Fe, Mn Se and Zn in inorganic forms (15 ppm Cu 15 from sulfate; 60 ppm Fe from sulfate etc.),and compared with three treatment groups supplemented with organic forms. Apart from improved feathering, most likely associated with the presence of organic Se, there were no significant performance differences between birds fed inorganic and organic minerals. The authors concluded that the use of organic trace minerals permits a reduction of at least 33% in supplement rates in comparison with inorganic minerals, without compromising performance.\n\n\n"}
{"id": "7472359", "url": "https://en.wikipedia.org/wiki?curid=7472359", "title": "Chronic care", "text": "Chronic care\n\nChronic care refers to medical care which addresses pre-existing or long term illness, as opposed to acute care which is concerned with short term or severe illness of brief duration. Chronic medical conditions include asthma, diabetes, emphysema, chronic bronchitis, congestive heart disease, cirrhosis of the liver, hypertension and depression. Without effective treatment chronic conditions may lead to disability.\n\nThe incidence of chronic disease has increased as mortality rates have decreased. It is estimated that by 2030 half of the population of the USA will have one or more chronic conditions.\n\nConditions, injuries and diseases which were previously fatal can now be treated with chronic care. Chronic care aims to maintain wellness by keeping symptoms in remission while balancing treatment regimes and quality of life. Many of the core functions of primary health care are central to chronic care. Chronic care is complex in nature because it may extend over a pro-longed period of time, requires input from a diverse set of health professionals, various medications and possibly monitoring equipment.\n\nAccording to 2008 figures from the Centers for Disease Control and Prevention chronic medical care accounts for more than 75% of health care spending in the US. In response to the increased government expenditure in dealing with chronic care policy makers are searching for effective interventions and strategies. These strategies can broadly be described within four categories. These are disease prevention and early detection, new providers, settings and qualifications, disease management programs and integrated care models.\n\nOne of the major problems from a health care system which is poorly coordinated for sufferers of chronic conditions is the incidence of patients receiving conflicting advice from different providers. Patients will often be given prescriptions for medication that adversely interact with\none another. One recent study estimated that more than 20% of older patients in the USA took at least one medication which could negatively impact another condition. This is referred to as therapeutic competition.\n\nEffective chronic care requires an information platform to track patients' status and ensure appropriate treatments are given.\n\nThere is a recognised gap between treatment guidelines and current practice for chronic care. Individualised treatment plans are critical in treating chronic conditions because patients will place varying important on health outcomes. For example, some patients will fore-go complex, inconvenient medication regimes at the expense of quality of life.\n\nOne of the greatest challenges in this field of health care is dealing with the co-existence of multiple disorders, which is called multi-morbidity. There are few incentives within current health care systems to coordinate care across multiple providers and varying services. A 2001 survey by Mathematica Policy Research found that physicians feel they have inadequate training to deal with multiple chronic conditions. An increase in the number of chronic conditions correlates with an increase in the number of inappropriate hospitalizations. Self-management can be challenging because recommended activities for one condition may be made difficult because of another condition.\n\nA nurse has to be qualified to handle all the needs of a chronic client and has to be an advocate to put the case of the chronically ill across to the health administration, hospital board or their families.\n\nA variety of specialists such as surgeons, dietitians, nutritionists, and occupational therapists have to be in attendance for the maximum benefit of the client. Someone suffering from chronic pain for a long time may need the help of a psychiatrist. Everyday activities that the physically fit see as normal may be a Herculean feat for the chronically ill and they need all the support that they can get. The nurse may be privy to some of these help that the chronically ill can benefit from. They need to be proactive and put these patients in contact with these help but also sensitive enough to give their client the freedom to decline any help if they think that they do not need it.\n\nChronic pain might also get the person to start questioning their faith and/or wanting to have a deeper spiritual experience because of their pain and suffering.\n\nThe patient also needs to take time to participate in some fun activities. They may need to check out of the facility/hospital or get out of the house occasionally preventing an association of hospitals with pain. This further helps the patients keep their sanity and keeps them psychologically sound.\n\nThey may need a nurse who is qualified in palliative care. Some may be dying and they need respect and dignity as they die in pain. They also need a nurse who is non-judgmental and one who is also compassionate and caring. The family has to be involved to help the client better manage the pain. One very important quality is co-ordinating the best care for the client and some amount of diplomacy and empathy.\n\nIn some cases, such as with diabetes or sleep apnea, the treatment is long term and difficult for patients to understand and comply with. In these cases chronic care management is highly recommended to help the patient learn about the consequences of refusing treatment and how to best follow treatment.\n\n"}
{"id": "811714", "url": "https://en.wikipedia.org/wiki?curid=811714", "title": "Comparison of the healthcare systems in Canada and the United States", "text": "Comparison of the healthcare systems in Canada and the United States\n\nComparison of the healthcare systems in Canada and the United States is often made by government, public health and public policy analysts. The two countries had similar healthcare systems before Canada changed its system in the 1960s and 1970s. The United States spends much more money on healthcare than Canada, on both a per-capita basis and as a percentage of GDP. In 2006, per-capita spending for health care in Canada was US$3,678; in the U.S., US$6,714. The U.S. spent 15.3% of GDP on healthcare in that year; Canada spent 10.0%. In 2006, 70% of healthcare spending in Canada was financed by government, versus 46% in the United States. Total government spending per capita in the U.S. on healthcare was 23% higher than Canadian government spending, and U.S. government expenditure on healthcare was just under 83% of total Canadian spending (public and private) though these statistics don't take into account population differences.\n\nStudies have come to different conclusions about the result of this disparity in spending. A 2007 review of all studies comparing health outcomes in Canada and the US in a Canadian peer-reviewed medical journal found that \"health outcomes may be superior in patients cared for in Canada versus the United States, but differences are not consistent.\" Some of the noted differences were a higher life expectancy in Canada, as well as a lower infant mortality rate than the United States.\n\nOne commonly cited comparison, the 2000 World Health Organization's ratings of \"overall health service performance\", which used a \"composite measure of achievement in the level of health, the distribution of health, the level of responsiveness and fairness of financial contribution\", ranked Canada 30th and the US 37th among 191 member nations. This study rated the US \"responsiveness\", or quality of service for individuals receiving treatment, as 1st, compared with 7th for Canada. However, the average life expectancy for Canadians was 80.34 years compared with 78.6 years for residents of the US.\n\nThe WHO's study methods were criticized by some analyses.\nWhile life-expectancy and infant mortality are commonly used in comparing nationwide health care, they are in fact affected by many factors other than the quality of a nation's health care system, including individual behavior and population makeup. A 2007 report by the Congressional Research Service carefully summarizes some recent data and noted the \"difficult research issues\" facing international comparisons.\n\nIn 2004, government funding of healthcare in Canada was equivalent to $1,893 per person. In the US, government spending per person was $2,728.\n\nThe Canadian healthcare system is composed of at least 10 mostly autonomous provincial healthcare systems that report to their provincial governments, and a federal system which covers the military and First Nations. This causes a significant degree of variation in funding and coverage within the country.\n\nCanada and the US had similar healthcare systems in the early 1960s, but now have a different mix of funding mechanisms. Canada's universal single-payer healthcare system covers about 70% of expenditures, and the Canada Health Act requires that all insured persons be fully insured, without co-payments or user fees, for all medically necessary hospital and physician care. About 91% of hospital expenditures and 99% of total physician services are financed by the public sector. In the United States, with its mixed public-private system, 16% or 45 million American residents are uninsured at any one time. The U.S. is one of two OECD countries not to have some form of universal health coverage, the other being Turkey. Mexico established a universal healthcare program by November 2008.\n\nThe governments of both nations are closely involved in healthcare. The central structural difference between the two is in health insurance. In Canada, the federal government is committed to providing funding support to its provincial governments for healthcare expenditures as long as the province in question abides by accessibility guarantees as set out in the Canada Health Act, which explicitly prohibits billing end users for procedures that are covered by Medicare. While some label Canada's system as \"socialized medicine\", health economists do not use that term. Unlike systems with public delivery, such as the UK, the Canadian system provides public coverage for a combination of public and private delivery. Princeton University health economist Uwe E. Reinhardt says that single-payer systems are not \"socialized medicine\" but \"social insurance\" systems, since providers (such as doctors) are largely in the private sector. Similarly, Canadian hospitals are controlled by private boards or regional health authorities, rather than being part of government.\n\nIn the US, direct government funding of health care is limited to Medicare, Medicaid, and the State Children's Health Insurance Program (SCHIP), which cover eligible senior citizens, the very poor, disabled persons, and children. The federal government also runs the Veterans Administration, which provides care directly to retired or disabled veterans, their families, and survivors through medical centers and clinics.\n\nThe U.S. government also runs the Military Health System. In fiscal year 2007, the MHS had a total budget of $39.4 billion and served approximately 9.1 million beneficiaries, including active-duty personnel and their families, and retirees and their families. The MHS includes 133,000 personnel, 86,000 military and 47,000 civilian, working at more than 1,000 locations worldwide, including 70 inpatient facilities and 1,085 medical, dental, and veterans' clinics.\n\nOne study estimates that about 25 percent of the uninsured in the U.S. are eligible for these programs but remain unenrolled; however, extending coverage to all who are eligible remains a fiscal and political challenge.\n\nFor everyone else, health insurance must be paid for privately. Some 59% of U.S. residents have access to health care insurance through employers, although this figure is decreasing, and coverages as well as workers' expected contributions vary widely. Those whose employers do not offer health insurance, as well as those who are self-employed or unemployed, must purchase it on their own. Nearly 27 million of the 45 million uninsured U.S. residents worked at least part-time in 2007, and more than a third were in households that earned $50,000 or more per year.\n\nDespite the greater role of private business in the US, federal and state agencies are increasingly involved, paying about 45% of the $2.2 trillion the nation spent on medical care in 2004. The U.S. government spends more on healthcare than on Social Security and national defense combined, according to the Brookings Institution.\n\nBeyond its direct spending, the US government is also highly involved in healthcare through regulation and legislation. For example, the Health Maintenance Organization Act of 1973 provided grants and loans to subsidize Health Maintenance Organizations and contained provisions to stimulate their popularity. HMOs had been declining before the law; by 2002 there were 500 such plans enrolling 76 million people.\n\nThe Canadian system has been 69–75% publicly funded, though most services are delivered by private providers, including physicians (although they may derive their revenue primarily from government billings). Although some doctors work on a purely fee-for-service basis (usually family physicians), some family physicians and most specialists are paid through a combination of fee-for-service and fixed contracts with hospitals or health service management organizations.\n\nCanada's universal health plans do not cover certain services. Non-cosmetic dental care is covered for children up to age 14 in some provinces. Outpatient prescription drugs are not required to be covered, but some provinces have drug cost programs that cover most drug costs for certain populations. In every province, seniors receiving the Guaranteed Income Supplement have significant additional coverage; some provinces expand forms of drug coverage to all seniors, low-income families, those on social assistance, or those with certain medical conditions. Some provinces cover all drug prescriptions over a certain portion of a family's income. Drug prices are also regulated, so brand-name prescription drugs are often significantly cheaper than in the U.S. Optometry is only covered in some provinces and is sometimes only covered for children under a certain age. Visits to non-physician specialists may require an additional fee. Also, some procedures are only covered under certain circumstances. For example, circumcision is not covered, and a fee is usually charged when a parent requests the procedure; however, if an infection or medical necessity arises, the procedure would be covered.\n\nAccording to Dr. Albert Schumacher, former president of the Canadian Medical Association, an estimated 75 percent of Canadian healthcare services are delivered privately, but funded publicly.\n\nFrontline practitioners whether they're GPs or specialists by and large are not salaried. They're small hardware stores. Same thing with labs and radiology clinics ... The situation we are seeing now are more services around not being funded publicly but people having to pay for them, or their insurance companies. We have sort of a passive privatization.\n\nIn both Canada and the United States, access can be a problem. Studies suggest that 40% of U.S. citizens do not have adequate health insurance, if any at all. In Canada, 5% of Canadian citizens have not been able to find a regular doctor, with a further 9% having never looked for one. Yet, even if some cannot find a family doctor, every Canadian citizen is covered by the national health care system. The U.S. data is evidenced in a 2007 Consumer Reports study on the U.S. health care system which showed that the underinsured account for 24% of the U.S. population and live with skeletal health insurance that barely covers their medical needs and leaves them unprepared to pay for major medical expenses. When added to the population of uninsured (approximately 16% of the U.S. population), a total of 40% of Americans ages 18–64 have inadequate access to healthcare, according to the Consumer Reports study. The Canadian data comes from the 2003 Canadian Community Health Survey,\n\nIn the U.S., the federal government does not guarantee universal healthcare to all its citizens, but publicly funded healthcare programs help to provide for the elderly, disabled, the poor, and children. The Emergency Medical Treatment and Active Labor Act or EMTALA also ensures public access to emergency services. The EMTALA law forces emergency healthcare providers to stabilize an emergency health crisis and cannot withhold treatment for lack of evidence of insurance coverage or other evidence of the ability to pay. EMTALA does not absolve the person receiving emergency care of the obligation to meet the cost of emergency healthcare not paid for at the time and it is still within the right of the hospital to pursue any debtor for the cost of emergency care provided. In Canada, emergency room treatment for legal Canadian residents is not charged to the patient at time of service but is met by the government.\n\nAccording to the United States Census Bureau, 59.3% of U.S. citizens have health insurance related to employment, 27.8% have government-provided health-insurance; nearly 9% purchase health insurance directly (there is some overlap in these figures), and 15.3% (45.7 million) were uninsured in 2007. An estimated 25 percent of the uninsured are eligible for government programs but unenrolled. About a third of the uninsured are in households earning more than $50,000 annually. A 2003 report by the Congressional Budget Office found that many people lack health insurance only temporarily, such as after leaving one employer and before a new job. The number of chronically uninsured (uninsured all year) was estimated at between 21 and 31 million in 1998. Another study, by the Kaiser Commission on Medicaid and the Uninsured, estimated that 59 percent of uninsured adults have been uninsured for at least two years. One indicator of the consequences of Americans' inconsistent health care coverage is a study in \"Health Affairs\" that concluded that half of personal bankruptcies involved medical bills. Although other sources dispute this, it is possible that medical debt is the principal cause of bankruptcy in the United States.\n\nA number of clinics provide free or low-cost non-emergency care to poor, uninsured patients. The National Association of Free Clinics claims that its member clinics provide $3 billion in services to some 3.5 million patients annually.\n\nA peer-reviewed comparison study of healthcare access in the two countries published in 2006 concluded that U.S. residents are one third less likely to have a regular medical doctor, one fourth more likely to have unmet healthcare needs, and are more than twice as likely to forgo needed medicines. The study noted that access problems \"were particularly dire for the US uninsured.\" Those who lack insurance in the U.S. were much less satisfied, less likely to have seen a doctor, and more likely to have been unable to receive desired care than both Canadians and insured Americans.\n\nAnother cross-country study compared access to care based on immigrant status in Canada and the U.S. Findings showed that in both countries, immigrants had worse access to care than non-immigrants. Specifically, immigrants living in Canada were less likely to have timely Pap tests compared with native-born Canadians; in addition, immigrants in the U.S. were less likely to have a regular medical doctor and an annual consultation with a health care provider compared with native-born Americans. In general, immigrants in Canada had better access to care than those in the U.S., but most of the differences were explained by differences in socioeconomic status (income, education) and insurance coverage across the two countries. However, immigrants in the U.S. were more likely to have timely Pap tests than immigrants in Canada.\n\nCato Institute has expressed concerns that the U.S. government has restricted the freedom of Medicare patients to spend their own money on healthcare, and has contrasted these developments with the situation in Canada, where in 2005 the Supreme Court of Canada ruled that the province of Quebec could not prohibit its citizens from purchasing covered services through private health insurance. The institute has urged the Congress to restore the right of American seniors to spend their own money on medical care.\n\nThe Canada Health Act covers the services of psychiatrists, who are medical doctors with additional training in psychiatry but does not cover treatment by a psychologist or psychotherapist unless the practitioner is also a medical doctor. Goods and Services Tax or Harmonized Sales Tax (depending on the province) applies to the services of psychotherapists. Some provincial or territorial programs and some private insurance plans may cover the services of psychologists and psychotherapists, but there is no federal mandate for such services in Canada. In the U.S., the Affordable Care Act includes prevention, early intervention, and treatment of mental and/or substance use disorders as an \"essential health benefit\" (EHB) that must be covered by health plans that are offered through the Health Insurance Marketplace. Under the Affordable Care Act, most health plans must also cover certain preventive services without a copayment, co-insurance, or deductible. In addition, the U.S. Mental Health Parity and Addiction Equity Act (MHPAEA) of 2008 mandates \"parity\" between mental health and/or substance use disorder (MH/SUD) benefits and medical/surgical benefits covered by a health plan. Under that law, if a health care plan offers mental health and/or substance use disorder benefits, it must offer the benefits on par with the other medical/surgical benefits it covers.\n\nOne complaint about both the U.S. and Canadian systems is waiting times, whether for a specialist, major elective surgery, such as hip replacement, or specialized treatments, such as radiation for breast cancer; wait times in each country are affected by various factors. In the United States, access is primarily determined by whether a person has access to funding to pay for treatment and by the availability of services in the area and by the willingness of the provider to deliver service at the price set by the insurer. In Canada, the wait time is set according to the availability of services in the area and by the relative need of the person needing treatment.\n\nAs reported by the Health Council of Canada, a 2010 Commonwealth survey found that 39% of Canadians waited 2 hours or more in the emergency room, versus 31% in the U.S.; 43% waited 4 weeks or more to see a specialist, versus 10% in the U.S. The same survey states that 37% of Canadians say it is difficult to access care after hours (evenings, weekends or holidays) without going to the emergency department over 34% of Americans. Furthermore, 47% of Canadians and 50% of Americans who visited emergency departments over the past two years feel that they could have been treated at their normal place of care if they were able to get an appointment.\n\nA report published by Health Canada in 2008 included statistics on self-reported wait times for diagnostic services. The median wait time for diagnostic services such as MRI and CAT scans is two weeks with 89.5% waiting less than 3 months. The median wait time to see a special physician is a little over four weeks with 86.4% waiting less than 3 months. The median wait time for surgery is a little over four weeks with 82.2% waiting less than 3 months. In the U.S., patients on Medicaid, the low-income government programs, can wait three months or more to see specialists. Because Medicaid payments are low, some have claimed that some doctors do not want to see Medicaid patients. For example, in Benton Harbor, Michigan, specialists agreed to spend one afternoon every week or two at a Medicaid clinic, which meant that Medicaid patients had to make appointments not at the doctor's office, but at the clinic, where appointments had to be booked months in advance. A 2009 study found that on average the wait in the United States to see a medical specialist is 20.5 days.\n\nIn a 2009 survey of physician appointment wait times in the United States, the average wait time for an appointment with an orthopedic surgeon in the country as a whole was 17 days. In Dallas, Texas the wait was 45 days (the longest wait being 365 days). Nationwide across the U.S. the average wait time to see a family doctor was 20 days. The average wait time to see a family practitioner in Los Angeles, California was 59 days and in Boston, Massachusetts it was 63 days.\n\nStudies by the Commonwealth Fund found that 42% of Canadians waited 2 hours or more in the emergency room, vs. 29% in the U.S.; 57% waited 4 weeks or more to see a specialist, vs. 23% in the U.S., but Canadians had more chances of getting medical attention at nights, or on weekends and holidays than their American neighbors without the need to visit an ER (54% compared to 61%). Statistics from the Canadian free market think tank Fraser Institute in 2008 indicate that the average wait time between the time when a general practitioner refers a patient for care and the receipt of treatment was almost four and a half months in 2008, roughly double what it had been 15 years before.\n\nA 2003 survey of hospital administrators conducted in Canada, the U.S., and three other countries found dissatisfaction with both the U.S. and Canadian systems. For example, 21% of Canadian hospital administrators, but less than 1% of American administrators, said that it would take over three weeks to do a biopsy for possible breast cancer on a 50-year-old woman; 50% of Canadian administrators versus none of their American counterparts said that it would take over six months for a 65-year-old to undergo a routine hip replacement surgery. However, U.S. administrators were the most negative about their country's system. Hospital executives in all five countries expressed concerns about staffing shortages and emergency department waiting times and quality.\n\nIn a letter to the \"Wall Street Journal\", Robert Bell, the President and CEO of University Health Network, Toronto, said that Michael Moore's film \"Sicko\" \"exaggerated the performance of the Canadian health system — there is no doubt that too many patients still stay in our emergency departments waiting for admission to scarce hospital beds.\" However, \"Canadians spend about 55% of what Americans spend on health care and have longer life expectancy and lower infant mortality rates. Many Americans have access to quality healthcare. All Canadians have access to similar care at a considerably lower cost.\" There is \"no question\" that the lower cost has come at the cost of \"restriction of supply with sub-optimal access to services,\" said Bell. A new approach is targeting waiting times, which are reported on public websites.\n\nIn 2007 Shona Holmes, a Waterdown, Ontario woman who had a Rathke's cleft cyst removed at the Mayo Clinic in Arizona, sued the Ontario government for failing to reimburse her $95,000 in medical expenses.\nHolmes had characterized her condition as an emergency, said she was losing her sight and portrayed her condition as a life-threatening brain cancer.\nIn July 2009 Holmes agreed to appear in television ads broadcast in the United States warning Americans of the dangers of adopting a Canadian-style health care system.\nThe ads she appeared in triggered debates on both sides of the border.\nAfter her ad appeared critics pointed out discrepancies in her story, including that Rathke's cleft cyst, the condition she was treated for, was not a form of cancer, and was not life-threatening.\n\nHealthcare is one of the most expensive items of both nations' budgets. In the United States, the various levels of government spend more per capita than levels of government do in Canada. In 2004, Canada government-spending was $2,120 (in US dollars) per person, while the United States government-spending $2,724.\n\nA 1999 report found that after exclusions, administration accounted for 31.0% of healthcare expenditures in the United States, as compared with 16.7% in Canada. In looking at the insurance element, in Canada, the provincial single-payer insurance system operated with overheads of 1.3%, comparing favourably with private insurance overheads (13.2%), U.S. private insurance overheads (11.7%) and U.S. Medicare and Medicaid program overheads (3.6% and 6.8% respectively). The report concluded by observing that gap between U.S. and Canadian spending on administration had grown to $752 per capita and that a large sum might be saved in the United States if the U.S. implemented a Canadian-style system.\n\nHowever, U.S. government spending covers less than half of all healthcare costs. Private spending is also far greater in the U.S. than in Canada. In Canada, an average of $917 was spent annually by individuals or private insurance companies for health care, including dental, eye care, and drugs. In the U.S., this sum is $3,372. In 2006, healthcare consumed 15.3% of U.S. annual GDP. In Canada, only 10% of GDP was spent on healthcare. This difference is a relatively recent development. In 1971 the nations were much closer, with Canada spending 7.1% of GDP while the U.S. spent 7.6%.\n\nSome who advocate against greater government involvement in healthcare have asserted that the difference in costs between the two nations is partially explained by the differences in their demographics. Illegal immigrants, more prevalent in the U.S. than in Canada, also add a burden to the system, as many of them do not carry health insurance and rely on emergency rooms — which are legally required to treat them under EMTALA — as a principal source of care. In Colorado, for example, an estimated 80% of undocumented immigrants do not have health insurance.\n\nThe mixed system in the United States has become more similar to the Canadian system. In recent decades, managed care has become prevalent in the United States, with some 90% of privately insured Americans belonging to plans with some form of managed care. In \"managed care\", insurance companies control patients' health care to reduce costs, for instance by demanding a second opinion prior to some expensive treatments or by denying coverage for treatments not considered worth their cost.\n\nAdministrative costs are also higher in the United States than in Canada.\n\nThrough all entities in its public–private system, the US spends more per capita than any other nation in the world, but is the only wealthy industrialized country in the world that lacks some form of universal healthcare. In March 2010, the US Congress passed regulatory reform of the American \"health insurance\" system. However, since this legislation is not fundamental \"healthcare\" reform, it is unclear what its effect will be and as the new legislation is implemented in stages, with the last provision in effect in 2018, it will be some years before any empirical evaluation of the full effects on the comparison could be determined.\n\nHealthcare costs in both countries are rising faster than inflation. As both countries consider changes to their systems, there is debate over whether resources should be added to the public or private sector. Although Canadians and Americans have each looked to the other for ways to improve their respective health care systems, there exists a substantial amount of conflicting information regarding the relative merits of the two systems. In the U.S., Canada's mostly monopsonistic health system is seen by different sides of the ideological spectrum as either a model to be followed or avoided.\n\nSome of the extra money spent in the United States goes to physicians, nurses, and other medical professionals. According to health data collected by the OECD, average income for physicians in the United States in 1996 was nearly twice that for physicians in Canada. In 2012, the gross average salary for doctors in Canada was CDN$328,000. Out of the gross amount, doctors pay for taxes, rent, staff salaries and equipment. When comparing average incomes of doctors in Canada and U.S., it should be kept in mind that malpractice insurance premiums may differ significantly between Canada and the U.S., and the proportion of doctors who are specialists differs. In Canada, less than half of doctors are specialists whereas more than 70% of doctors are specialists in the U.S.\n\nCanada has fewer doctors per capita than the United States. In the U.S, there were 2.4 doctors per 1,000 people in 2005; in Canada, there were 2.2. Some doctors leave Canada to pursue career goals or higher pay in the U.S., though significant numbers of physicians from countries such as China, India, Pakistan and South Africa immigrate to practice in Canada. Many Canadian physicians and new medical graduates also go to the U.S. for post-graduate training in medical residencies. As it is a much larger market, new and cutting-edge sub-specialties are more widely available in the U.S. as opposed to Canada. However, statistics published in 2005 by the Canadian Institute for Health Information (CIHI), show that, for the first time since 1969 (the period for which data are available), more physicians returned to Canada than moved abroad.\n\nBoth Canada and the United States have limited programs to provide prescription drugs to the needy. In the U.S., the introduction of Medicare Part D has extended partial coverage for pharmaceuticals to Medicare beneficiaries. In Canada all drugs given in hospitals fall under Medicare, but other prescriptions do not. The provinces all have some programs to help the poor and seniors have access to drugs, but while there have been calls to create one, no national program exists. About two thirds of Canadians have private prescription drug coverage, mostly through their employers. In both countries, there is a significant population not fully covered by these programs. A 2005 study found that 20% of Canada's and 40% of America's sicker adults did not fill a prescription because of cost.\n\nFurthermore, the 2010 Commonwealth Fund International Health Policy Survey indicates that 4% of Canadians indicated that they did not visit a doctor because of cost compared with 22% of Americans. Additionally, 21% of Americans have said that they did not fill a prescription for medicine or have skipped doses due to cost. That is compared with 10% of Canadians.\n\nOne of the most important differences between the two countries is the much higher cost of drugs in the United States. In the U.S., $728 per capita is spent each year on drugs, while in Canada it is $509. At the same time, consumption is higher in Canada, with about 12 prescriptions being filled per person each year in Canada and 10.6 in the United States. The main difference is that patented drug prices in Canada average between 35% and 45% lower than in the United States, though generic prices are higher. The price differential for brand-name drugs between the two countries has led Americans to purchase upward of $1 billion US in drugs per year from Canadian pharmacies.\n\nThere are several reasons for the disparity. The Canadian system takes advantage of centralized buying by the provincial governments that have more market heft and buy in bulk, lowering prices. By contrast, the U.S. has explicit laws that prohibit Medicare or Medicaid from negotiating drug prices. In addition, price negotiations by Canadian health insurers are based on evaluations of the clinical effectiveness of prescription drugs, allowing the relative prices of therapeutically similar drugs to be considered in context. The Canadian Patented Medicine Prices Review Board also has the authority to set a fair and reasonable price on patented products, either comparing it to similar drugs already on the market, or by taking the average price in seven developed nations. Prices are also lowered through more limited patent protection in Canada. In the U.S., a drug patent may be extended five years to make up for time lost in development. Some generic drugs are thus available on Canadian shelves sooner.\n\nThe pharmaceutical industry is important in both countries, though both are net importers of drugs. Both countries spend about the same amount of their GDP on pharmaceutical research, about 0.1% annually\n\nThe United States spends more on technology than Canada. In a 2004 study on medical imaging in Canada, it was found that Canada had 4.6 MRI scanners per million population while the U.S. had 19.5 per million. Canada's 10.3 CT scanners per million also ranked behind the U.S., which had 29.5 per million. The study did not attempt to assess whether the difference in the number of MRI and CT scanners had any effect on the medical outcomes or were a result of overcapacity but did observe that MRI scanners are used more intensively in Canada than either the U.S. or Great Britain. This disparity in the availability of technology, some believe, results in longer wait times. In 1984 wait times of up to 22 months for an MRI were alleged in Saskatchewan. However, according to more recent official statistics (2007), all emergency patients receive MRIs within 24 hours, those classified as urgent receive them in under 3 weeks and the maximum elective wait time is 19 weeks in Regina and 26 weeks in Saskatoon, the province's two largest metropolitan areas.\n\nAccording to the Health Council of Canada's 2010 report \"Decisions, Decisions: Family doctors as gatekeepers to prescription drugs and diagnostic imaging in Canada\", the Canadian federal government invested $3 billion over 5 years (2000–2005) in relation to diagnostic imaging and agreed to invest a further $2 billion to reduce wait times. These investments led to an increase in the number of scanners across Canada as well as the number of exams being performed. The number of CT scanners increased from 198 to 465 and MRI scanners increased from 19 to 266 (more than tenfold) between 1990 and 2009. Similarly, the number of CT exams increased by 58% and MRI exams increased by 100% between 2003 and 2009. In comparison to other OECD countries, including the US, Canada's rates of MRI and CT exams falls somewhere in the middle. Nevertheless, the Canadian Association of Radiologists claims that as many as 30% of diagnostic imaging scans are inappropriate and contribute no useful information.\n\nThe extra cost of malpractice lawsuits is a proportion of health spending in both the U.S. (1.7% in 2002) and Canada (0.27% in 2001 or $237 million). In Canada the total cost of settlements, legal fees, and insurance comes to $4 per person each year, but in the United States it is over $16. Average payouts to American plaintiffs were $265,103, while payouts to Canadian plaintiffs were somewhat higher, averaging $309,417. However, malpractice suits are far more common in the U.S., with 350% more suits filed each year per person. While malpractice costs are significantly higher in the U.S., they make up only a small proportion of total medical spending. The total cost of defending and settling malpractice lawsuits in the U.S. in 2004 was over $28 billion. Critics say that defensive medicine consumes up to 9% of American healthcare expenses., but CBO studies suggest that it is much smaller.\n\nThere are a number of ancillary costs that are higher in the U.S. Administrative costs are significantly higher in the U.S.; government mandates on record keeping and the diversity of insurers, plans and administrative layers involved in every transaction result in greater administrative effort. One recent study comparing administrative costs in the two countries found that these costs in the U.S. are roughly double what they are in Canada. Another ancillary cost is marketing, both by insurance companies and health care providers. These costs are higher in the U.S., contributing to higher overall costs in that nation.\n\nIn the World Health Organization's rankings of healthcare system performance among 191 member nations published in 2000, Canada ranked 30th and the U.S. 37th, while the overall health of Canadians was ranked 35th and Americans 72nd. However, the WHO's methodologies, which attempted to measure how efficiently health systems translate expenditure into health, generated broad debate and criticism.\n\nResearchers caution against inferring healthcare quality from some health statistics. June O'Neill and Dave O'Neill point out that \"... life expectancy and infant mortality are both poor measures of the efficacy of a health care system because they are influenced by many factors that are unrelated to the quality and accessibility of medical care\".\n\nIn 2007, Gordon H. Guyatt et al. conducted a meta-analysis, or systematic review, of all studies that compared health outcomes for similar conditions in Canada and the U.S., in \"Open Medicine\", an open-access peer-reviewed Canadian medical journal. They concluded, \"Available studies suggest that health outcomes may be superior in patients cared for in Canada versus the United States, but differences are not consistent.\" Guyatt identified 38 studies addressing conditions including cancer, coronary artery disease, chronic medical illnesses and surgical procedures. Of 10 studies with the strongest statistical validity, 5 favoured Canada, 2 favoured the United States, and 3 were equivalent or mixed. Of 28 weaker studies, 9 favoured Canada, 3 favoured the United States, and 16 were equivalent or mixed. Overall, results for mortality favoured Canada with a 5% advantage, but the results were weak and varied. The only consistent pattern was that Canadian patients fared better in kidney failure.\n\nIn terms of population health, life expectancy in 2006 was about two and a half years longer in Canada, with Canadians living to an average of 79.9 years and Americans 77.5 years. Infant and child mortality rates are also higher in the U.S. Some comparisons suggest that the American system underperforms Canada's system as well as those of other industrialized nations with universal coverage. For example, a ranking by the World Health Organization of health care system performance among 191 member nations, published in 2000, ranked Canada 30th and the U.S. 37th, and the overall health of Canada 35th to the American 72nd. The WHO did not merely consider health care outcomes, but also placed heavy emphasis on the health disparities between rich and poor, funding for the health care needs of the poor, and the extent to which a country was reaching the potential health care outcomes they believed were possible for that nation. In an international comparison of 21 more specific quality indicators conducted by the Commonwealth Fund International Working Group on Quality Indicators, the results were more divided. One of the indicators was a tie, and in 3 others, data was unavailable from one country or the other. Canada performed better on 11 indicators; such as survival rates for colorectal cancer, childhood leukemia, and kidney and liver transplants. The U.S. performed better on 6 indicators, including survival rates for breast and cervical cancer, and avoidance of childhood diseases such as pertussis and measles. It should be noted that the 21 indicators were distilled from a starting list of 1000. The authors state that, \"It is an opportunistic list, rather than a comprehensive list.\"\n\nSome of the difference in outcomes may also be related to lifestyle choices. The OECD found that Americans have slightly higher rates of smoking and alcohol consumption than do Canadians as well as significantly higher rates of obesity. A joint US-Canadian study found slightly higher smoking rates among Canadians. Another study found that Americans have higher rates not only of obesity, but also of other health risk factors and chronic conditions, including physical inactivity, diabetes, hypertension, arthritis, and chronic obstructive pulmonary disease.\n\nWhile a Canadian systematic review stated that the differences in the systems of Canada and the United States could not alone explain differences in healthcare outcomes, the study didn't consider that over 44,000 Americans die every year due to not having a single payer system for healthcare in the United States and it didn't consider the millions more that live without proper medical care due to a lack of insurance.\n\nThe United States and Canada have different racial makeups, different obesity rates and different alcoholism rates, which would likely cause the US to have a shorter average life expectancy and higher infant mortality even with equal healthcare provided. The US population is 12.2% African Americans and 16.3% Hispanic Americans (2010 Census), whereas Canada has only 2.5% African Canadians and 0.97% Hispanic Canadians (2006 Census). African Americans have higher mortality rates than any other racial or ethnic group for eight of the top ten causes of death. The cancer incidence rate among African Americans is 10% higher than among European Americans. U.S. Latinos have higher rates of death from diabetes, liver disease, and infectious diseases than do non-Latinos. Adult African Americans and Latinos have approximately twice the risk as European Americans of developing diabetes. The infant mortality rates for African Americans is twice that of whites. Unfortunately, directly comparing infant mortality rates between countries is difficult, as countries have different definitions of what qualifies as an infant death.\n\nAnother issue with comparing the two systems is the baseline health of the patient's for which the systems must treat. Canada has only half the obesity rate that the US system must deal with (14.3% vs 30.6%). On average, obesity reduces life expectancy by 6–7 years.\n\nA 2004 study found that Canada had a slightly higher mortality rate for acute myocardial infarction (heart attack) because of the more conservative Canadian approach to revascularizing (opening) coronary arteries.\n\nNumerous studies have attempted to compare the rates of cancer incidence and mortality in Canada and the U.S., with varying results. Doctors who study cancer epidemiology warn that the diagnosis of cancer is subjective, and the \"reported\" incidence of a cancer will rise if screening is more aggressive, even if the \"real\" cancer incidence is the same. Statistics from different sources may not be compatible if they were collected in different ways. The proper interpretation of cancer statistics has been an important issue for many years. Dr. Barry Kramer of the National Institutes of Health points to the fact that cancer incidence rose sharply over the past few decades as screening became more common. He attributes the rise to increased detection of benign early stage cancers that pose little risk of metastasizing. Furthermore, though patients who were treated for these benign cancers were at little risk, they often have trouble finding health insurance after the fact.\n\nCancer survival time increases with later years of diagnosis, because cancer treatment improves, so cancer survival statistics can only be compared for cohorts in the same diagnosis year. For example, as doctors in British Columbia adopted new treatments, survival time for patients with metastatic breast cancer increased from 438 days for those diagnosed in 1991–1992, to 667 days for those diagnosed in 1999–2001.\n\nAn assessment by Health Canada found that cancer mortality rates are almost identical in the two countries. Another international comparison by the National Cancer Institute of Canada indicated that incidence rates for most, but not all, cancers were higher in the U.S. than in Canada during the period studied (1993–1997). Incidence rates for certain types, such as colorectal and stomach cancer, were actually higher in Canada than in the U.S. In 2004, researchers published a study comparing health outcomes in the Anglo countries. Their analysis indicates that Canada has greater survival rates for both colorectal cancer and childhood leukemia, while the United States has greater survival rates for Non-Hodgkin's lymphoma as well as breast and cervical cancer.\n\nA study based on data from 1978 through 1986 found very similar survival rates in both the United States and in Canada. However, a study based on data from 1993 through 1997 found lower cancer survival rates among Canadians than among Americans.\n\nA few comparative studies have found that cancer survival rates vary more widely among different populations in the U.S. than they do in Canada. Mackillop and colleagues compared cancer survival rates in Ontario and the U.S. They found that cancer survival was more strongly correlated with socio-economic class in the U.S. than in Ontario. Furthermore, they found that the American survival advantage in the four highest quintiles was statistically significant. They strongly suspected that the difference due to prostate cancer was a result of greater detection of asymptomatic cases in the U.S. Their data indicates that neglecting the prostate cancer data reduces the American advantage in the four highest quintiles and gives Canada a statistically significant advantage in the lowest quintile. Similarly, they believe differences in screening mammography may explain part of the American advantage in breast cancer. Exclusion of breast and prostate cancer data results in very similar survival rates for both countries.\n\nHsing et al. found that prostate cancer mortality incidence rate ratios were lower among U.S. whites than among any of the nationalities included in their study, including Canadians. U.S. African Americans in the study had lower rates than any group except for Canadians and U.S. whites. Echoing the concerns of Dr. Kramer and Professor Mackillop, Hsing later wrote that reported prostate cancer incidence depends on screening. Among whites in the U.S., the death rate for prostate cancer remained constant, even though the incidence increased, so the additional reported prostate cancers did not represent an increase in real prostate cancers, said Hsing. Similarly, the death rates from prostate cancer in the U.S. increased during the 1980s and peaked in early 1990. This is at least partially due to \"attribution bias\" on death certificates, where doctors are more likely to ascribe a death to prostate cancer than to other diseases that affected the patient, because of greater awareness of prostate cancer or other reasons.\n\nBecause health status is \"considerably affected\" by socioeconomic and demographic characteristics, such as level of education and income, \"the value of comparisons in isolating the impact of the healthcare system on outcomes is limited,\" according to health care analysts. Experts say that the incidence and mortality rates of cancer cannot be combined to calculate survival from cancer. Nevertheless, researchers have used the ratio of mortality to incidence rates as one measure of the effectiveness of healthcare. Data for both studies was collected from registries that are members of the North American Association of Central Cancer Registries, an organization dedicated to developing and promoting uniform data standards for cancer registration in North America.\n\nThe U.S. and Canada differ substantially in their demographics, and these differences may contribute to differences in health outcomes between the two nations. Although both countries have white majorities, Canada has a proportionately larger immigrant minority population. Furthermore, the relative size of different ethnic and racial groups vary widely in each country. Hispanics and peoples of African descent constitute a much larger proportion of the U.S. population. Non-Hispanic North American aboriginal peoples constitute a much larger proportion of the Canadian population. Canada also has a proportionally larger South Asian and East Asian population. Also, the proportion of each population that is immigrant is higher in Canada.\n\nA study comparing aboriginal mortality rates in Canada, the U.S. and New Zealand found that aboriginals in all three countries had greater mortality rates and shorter life expectancies than the white majorities. That study also found that aboriginals in Canada had both shorter life expectancies and greater infant mortality rates than aboriginals in the United States and New Zealand. The health outcome differences between aboriginals and whites in Canada was also larger than in the United States.\n\nThough few studies have been published concerning the health of Black Canadians, health disparities between whites and African Americans in the U.S. have received intense scrutiny. African Americans in the U.S. have significantly greater rates of cancer incidence and mortality. Drs. Singh and Yu found that neonatal and postnatal mortality rates for American African Americans are more than double the non-Hispanic white rate. This difference persisted even after controlling for household income and was greatest in the highest income quintile. A Canadian study also found differences in neonatal mortality between different racial and ethnic groups. Although Canadians of African descent had a greater mortality rate than whites in that study, the rate was somewhat less than double the white rate.\n\nThe racially heterogeneous Hispanic population in the U.S. has also been the subject of several studies. Although members of this group are significantly more likely to live in poverty than are non-Hispanic whites, they often have disease rates that are comparable to or better than the non-Hispanic white majority. Hispanics have lower cancer incidence and mortality, lower infant mortality, and lower rates of neural tube defects. Singh and Yu found that infant mortality among Hispanic sub-groups varied with the racial composition of that group. The mostly white Cuban population had a neonatal mortality rate (NMR) nearly identical to that found in non-Hispanic whites and a postnatal mortality rate (PMR) that was somewhat lower. The largely Mestizo, Mexican, Central, and South American Hispanic populations had somewhat lower NMR and PMR. The Puerto Ricans who have a mix of white and African ancestry had higher NMR and PMR rates.\n\nIn 2002, automotive companies claimed that the universal system in Canada saved labour costs. In 2004, healthcare cost General Motors $5.8 billion, and increased to $7 billion. The UAW also claimed that the resulting escalating healthcare premiums reduced workers' bargaining powers.\n\nIn Canada, increasing demands for healthcare, due to the aging population, must be met by either increasing taxes or reducing other government programs. In the United States, under the current system, more of the burden will be taken up by the private sector and individuals.\n\nSince 1998, Canada's successive multibillion-dollar budget surpluses have allowed a significant injection of new funding to the healthcare system, with the stated goal of reducing waiting times for treatment. However, this may be hampered by the return to deficit spending as of the 2009 Canadian federal budget.\n\nOne historical problem with the U.S. system was known as job lock, in which people become tied to their jobs for fear of losing their health insurance. This reduces the flexibility of the labor market. Federal legislation passed since the mid-1980s, particularly COBRA and HIPAA, has been aimed at reducing job lock. However, providers of group health insurance in many states are permitted to use experience rating and it remains legal in the United States for prospective employers to investigate a job candidate's health and past health claims as part of a hiring decision. Someone who has recently been diagnosed with cancer, for example, may face job lock not out of fear of losing their health insurance, but based on prospective employers not wanting to add the cost of treating that illness to their own health insurance pool, for fear of future insurance rate increases. Thus, being diagnosed with an illness can cause someone to be forced to stay in their current job.\n\nMore imaginative solutions in both countries have come from the sub-national level.\n\nIn Canada, the right-wing and now defunct Reform Party and its successor, the Conservative Party of Canada considered increasing the role of the private sector in the Canadian system. Public backlash caused these plans to be abandoned, and the Conservative government that followed re-affirmed its commitment to universal public medicine.\n\nIn Canada, it was Alberta under the Conservative government that had experimented most with increasing the role of the private sector in healthcare. Measures included the introduction of private clinics allowed to bill patients for some of the cost of a procedure, as well as 'boutique' clinics offering tailored personal care for a fixed preliminary annual fee.\n\nIn the U.S., President Bill Clinton attempted a significant restructuring of health care, but the effort collapsed under political pressure against it despite tremendous public support. The 2000 U.S. election saw prescription drugs become a central issue, although the system did not fundamentally change. In the 2004 U.S. election healthcare proved to be an important issue to some voters, though not a primary one.\n\nIn 2006, Massachusetts adopted a plan that vastly reduced the number of uninsured making it the state with the lowest percentage of non-insured residents in the union. It requires everyone to buy insurance and subsidizes insurance costs for lower income people on a sliding scale. Some have claimed that the state's program is unaffordable, which the state itself says is \"a commonly repeated myth\". In 2009, in a minor amendment, the plan did eliminate dental, hospice and skilled nursing care for certain categories of noncitizens covering 30,000 people (victims of human trafficking and domestic violence, applicants for asylum and refugees) who do pay taxes.\n\nIn July 2009, Connecticut passed into law a plan called SustiNet, with the goal of achieving health care coverage of 98% of its residents by 2014.\n\nUS President Donald Trump has declared his intent to repeal the Affordable Care Act, but has failed to do so, thus far.\n\nThe Canada Health Act of 1984 \"does not directly bar private delivery or private insurance for publicly insured services,\" but provides financial disincentives for doing so. \"Although there are laws prohibiting or curtailing private health care in some provinces, they can be changed,\" according to a report in the New England Journal of Medicine. Governments attempt to control health care costs by being the sole purchasers and thus they do not allow private patients to bid up prices. Those with non-emergency illnesses such as cancer cannot pay out of pocket for time-sensitive surgeries and must wait their turn on waiting lists. According to the Canadian Supreme Court in its 2005 ruling in \"Chaoulli v. Quebec\", waiting list delays \"increase the patient's risk of mortality or the risk that his or her injuries will become irreparable.\" The ruling found that a Quebec provincial ban on private health insurance was unlawful, because it was contrary to Quebec's own legislative act, the 1975 Charter of Human Rights and Freedoms.\n\nIn the United States, Congress has enacted laws to promote consumer-driven healthcare with health savings accounts (HSAs), which were created by the Medicare bill signed by President George W. Bush on December 8, 2003. HSAs are designed to provide tax incentives for individuals to save for future qualified medical and retiree health expenses. Money placed in such accounts is tax-free. To qualify for HSAs, individuals must carry a high-deductible health plan (HDHP). The higher deductible shifts some of the financial responsibility for health care from insurance providers to the consumer. This shift towards a market-based system with greater individual responsibility increased the differences between the US and Canadian systems.\n\nSome economists who have studied proposals for universal healthcare worry that the consumer driven healthcare movement will reduce the social redistributive effects of insurance that pools high-risk and low-risk people together. This concern was one of the driving factors behind a provision of the Patient Protection and Affordable Care Act, informally known as \"Obamacare\", which limited the types of purchases which could be made with HSA funds. For example, as of January 1, 2011, these funds can no longer be used to buy over-the-counter drugs without a medical prescription.\n\n\n"}
{"id": "58828704", "url": "https://en.wikipedia.org/wiki?curid=58828704", "title": "Deaf mental health care", "text": "Deaf mental health care\n\nDeaf mental health care is the providing of counseling, therapy, and other psychiatric services to people who are deaf and hard of hearing in ways that are culturally aware and linguistically accessible. It term also covers research, training, and services in ways that improve mental health for deaf people. These services consider those with a variety of hearing levels and experiences with deafness focusing on their psychological well-being. The National Association of the Deaf has identified that specialized services and knowledge of the Deaf increases successful mental health services to this population. States such as North Carolina, South Carolina, and Alabama have specialized Deaf mental health services. The Alabama Department of Mental Health has established an office of Deaf services to serve the more than 39,000 deaf and hard of hearing person who will require mental health services.\n\nThere are multiple models of deafness; Deaf mental health focuses on a cultural model in that people who are deaf view themselves as part of a socio-cultural linguistic community, rather than people with a medical deficit or disability. Accordingly, providing deaf mental heath care to people of the Deaf community requires services from clinicians, doctors, and interpreters who are trained with this perspective and the inclusion of deaf professionals in this system of health care. \n\nEarly access to language in deaf children is important for normal development of language. Deprivation of language can negatively affect mental health and in severe cases can cause mental health syndrome. Access to auditory and visual language is important, and availability differs based on each child's abilities. Approximately 40% of deaf children also have additional disabilities.\n\nMany states have deaf schools and institutions that provide appropriate language models along with mental health services for their students and those in the surrounding Deaf communities. The Lexington School for the Deaf in Queens, New York provides a variety of educational and social services for the deaf. The Texas School for the Deaf in Austin, Texas also provides a focus on mental health for students.\n\nDeaf children in mainstream schools may be more neglected in the classroom than their hearing peers. It is also more common for deaf children to have a harder time making friends. Bullying can occur frequently among children who are deaf or hard of hearing, which can lead to negative mental health outcomes.\n\nFor a deaf person, obtaining access to proper medical treatment is challenging and they face a variety of obstacles in communication and access. This can include the way in which medical professionals initiate patient's various health exams without prior modification suitable for deaf individuals. Communication challenges and lack of doctor awareness of the culture and language of the deaf can lead deaf patients to avoid making medical appointments. An increase in the amount of professionals who are trained in American Sign Language (ASL) and have experience with Deaf culture increase positive mental health outcomes for deaf people.\n\nAge-related hearing loss gradually occurs in many people as they get older, typically affecting those over the age of 65. This type of hearing loss can lead to feelings of embarrassment and isolation due to the fact that those affected may no longer be able to hear family, friends, or simple everyday sounds. Those with hearing loss are less likely to want to engage in social activities due to frustration over not being able to hear. A study conducted by the National Council of Aging (NCOA) showed that a large portion of elders with hearing loss who were studied, reported symptoms of lasting depression. Higher rates of exclusion from social and employment opportunities due to higher rates of miscommunication, making deaf adults more susceptible to mental illnesses. \n\nStudies have found that when a person becomes deaf at an older age, it has a less extreme impact on their mental health than it does when hearing loss begins at an earlier age. However, those who were either born deaf or lost their hearing at a younger age and then age as a deaf person face some particularly difficult challenges. When a non-deaf person ages, isolationist tendencies are generally increased. This increase is even more drastic for deaf people. Furthermore, many technological advancements that are heavily dependent on auditory communication also present challenges to deaf people.\n\nThe type and onset of deafness may cause different types of language disfluencies, diagnoses, and treatments of clients who are deaf. Cultural knowledge, language skills (e.g., fluency in ASL or access to trained interpreters), and other social-cultural factors are part of the deaf mental health access model. Lack of knowledge about Deaf culture and sign language among mental health professionals can make it difficult for deaf people to access appropriate services.\n\nThe National Association of the Deaf has eight recommendations for qualifications of interpreters working in mental health settings:\n\n\nSpecific knowledge and training in mental health contexts is necessary for adequate sign language interpreting for mental health clients. Accordingly, the State of Alabama requires \"Certification of mental health interpreters for persons who are deaf\" for interpreters to work in mental health contexts, and this certification must be renewed yearly by either: a) working 40 hours in clinical settings; b) attending 40 hours of training; or c) a combination of work in clinical settings and training equaling 40 hours. To provide the opportunity for education and training, the Alabama Department of Mental Health's Office of Deaf Services established the Alabama's Mental Health Interpreter Training Project. \n\n"}
{"id": "28859692", "url": "https://en.wikipedia.org/wiki?curid=28859692", "title": "Electronic patient-reported outcome", "text": "Electronic patient-reported outcome\n\nAn electronic patient-reported outcome (ePRO) is a patient-reported outcome that is collected by electronic methods. ePRO methods are most commonly used in clinical trials, but they are also used elsewhere in health care. As a function of the regulatory process, a majority of ePRO questionnaires undergo the linguistic validation process.\n\nThe two main methods currently used for ePRO are computers/Smartphones and telephone systems\n\n\"Computers\" are most often touch-screen devices, ranging from small hand-helds such as the Palm Pilot, through tablet PCs. The smaller devices are often used as electronic diaries, designed to be used for symptom reporting on a daily basis. Larger devices are generally used in a clinic setting. Computers generally run dedicated ePRO applications - the use of the web for ePRO is not yet widespread. Typically a single question at a time is presented on the screen, with a set of possible response options. The user taps on the appropriate response with finger or stylus, then moves on to the next questions.\n\n\"Telephones\" normally use an interactive voice response system (IVR). The user calls into a dedicated phone line, and hears a spoken script which details the question, and the possible responses. Each response option is given a number, and the user presses the corresponding number key on the phone keypad to record the choice. IVR systems are more often used for diaries, with the patient phoning in e.g. from home, but they can be used in a clinic setting.\n\nThere are also a number of custom devices designed specifically for use as ePRO data collection devices.\n\nDiaries are used when it is desirable to obtain frequent assessments over a period of time, for example when a condition fluctuates in severity. In such cases recall of severity over a period of time is unlikely to be accurate. Research has shown substantial bias in such summary recall, with ratings unduly influenced by how the patient is feeling at the time of making the rating, and by maximum severity rather than average severity during the assessment interval. Diaries can overcome this problem by recording severity either on a momentary basis (\"How bad is your pain right now?\") or by recall over short periods (\"How bad has your pain been today?\"). However, when diary data is collected on paper, it is not known when the ratings are actually made, and there is evidence that compliance may be quite poor. In one study, patients were given an instrumented paper diary that recorded covertly when it was opened. The study showed frequent cases of \"back-filling\", filling in a batch of entries some time, often days, after they were due, and even in some cases of \"forward filling\", completing entries before they were due.\n\nElectronic diaries automatically time-stamp all entries, and can be set up to only allow entry within specified time-windows. This improves compliance, and ensures that true compliance can be documented. Documenting compliance is important if ePRO data are to be used to support regulatory applications. ePRO applications typically achieve compliance rates of over 80%, often over 90%. Electronic diaries also have benefits in that they only allow valid, in-range entries to be made. Devices such as PDAs allow reminders to be given to patients when entries are due. Systems generally transfer data promptly to a central server, allowing tailored feedback to be given to patients. This can also improve compliance. Electronic diaries also eliminate the need for manual editing and entry of data, time-consuming and error-prone processes.\n\nThe other main setting for ePRO is the clinic, with questionnaires completed when patients come in for their scheduled visits. In this supervised situation, compliance is less of an issue. The questionnaires used in site-based ePRO are often longer and more complex than those used in diaries, assessing quality of life and activities of daily living, for example, in some detail. They more often include branching logic (\"if YES continue with the next question, if NO, goto question 34\"). Such branching logic can be handled automatically by the ePRO application, and it is often not necessary for the patient even to know that branching is taking place. This makes it easier for the patient to use.\n\nAs with electronic diaries, the prevention of out of range or inconsistent entries, and elimination of manual editing and data entry are important features of site-based ePRO. Missing data within a questionnaire can be reduced or eliminated, and this is important as missing data has been identified as a crucial quality issue in questionnaire data.\n\nFrom the early years of ePRO there has been concern about whether all patients can cope with computer technology. This is important,as if significant numbers of patients refuse to take part in clinical trials because of dislike of computers then there will be bias in the study population. One of the earliest ePRO studies used a LINC-2 minicomputer to collect patient data. The majority of patients preferred the computer to paper data collection. Similar findings have been reported from many later studies.\n\nElderly patients, and those not familiar with computers, might be expected to have more problems. But these groups also show high acceptance of ePRO, and again often prefer it to paper. Thus there seems to be no great barrier to recruiting representative patient samples in ePRO studies.\n\nEstablishing the validity of an ePRO instrument is in principle no different from that for other methods, such as paper. However most of the instruments in current use have been validated in paper form, and we need to ask (1) whether the paper data can be used to establish validity of the electronic version, and (2) whether data from paper and electronic versions can be used interchangeably.\n\nThe International Society for Pharmacoeconomomics and Outcomes Research (ISPOR) has issued guidelines on establishing equivalence between modes of administration. Their approach is hierarchical, and depends on the degree of change made during the process of migration from paper to electronic format. Three levels are suggested. At the lowest level, where least change has been made, cognitive interviewing of patients as a check that they construe ePRO and paper in the same way is sufficient. This level includes both trivial changes (touch rather than circle a response choice) as well as changes that are supported by empirical findings in the literature. At the second level, equivalence studies comparing the scores obtained from the two modes should be carried out. At the third level, where most change has occurred, the ePRO instrument must be treated as a new instrument, and complete psychometric validation carried out.\n\nThere is a great deal of evidence supporting the general equivalence of paper and ePRO methods. Gwaltney and colleagues have reported a meta-analysis in which they included 46 studies evaluating 278 scales. They concluded that there was good agreement between paper and ePRO, and no evidence of systematic bias. This general finding, of course, does not guarantee that any specific migration will lead to equivalence, and each case must be reviewed and documented.\n\nIt is not always necessary to validate an ePRO measure against a pre-existing paper version. In some cases an instrument may be developed and validated from the beginning in electronic form. More commonly, perhaps, new instruments will be developed in parallel for paper and electronic use, as is the case with the PROMIS (Patient-Reported Outcomes Measurement Information System) initiative.\n\nStandards of validity must be maintained throughout every target language population. In order to ensure that developmental standards are consistent in translated versions of an ePRO instrument, the translated instrument undergoes a process known as Linguistic validation in which the preliminary translation is adapted to reflect cultural and linguistic differences between diverse target populations. This process also accounts for any formatting errors that may occur in languages using non-Roman fonts.\n\nSeveral successful regulatory approvals have used ePRO data in recent years, including ketorolac for ocular pain, eszopiclone for insomnia, milnacipran for fibromyalgia, estradiol/levonorgestrel for post-menopausal symptoms, and ruxolitinib for myelofibrosis. In the case of estradiol/levonorgestrel, detailed ePRO data on bleeding/spotting from a one-year clinical trial are presented in the patient information leaflet.\n\nAs well as clinical trial use, ePRO methods may be used to support patients in regular care. An example of this is the collection of symptom data from patients undergoing chemotherapy, using handheld diaries. This allows clinic staff to monitor outpatients, and to identify the occurrence of adverse reactions that may require intervention.\n\n\n"}
{"id": "19686742", "url": "https://en.wikipedia.org/wiki?curid=19686742", "title": "Electronic prescribing", "text": "Electronic prescribing\n\nElectronic prescribing (e-prescribing or e-Rx) is the computer-based electronic generation, transmission, and filling of a medical prescription, taking the place of paper and faxed prescriptions. E-prescribing allows a physician, pharmacist, nurse practitioner, or physician assistant to use digital prescription software to electronically transmit a new prescription or renewal authorization to a community or mail-order pharmacy. It outlines the ability to send error-free, accurate, and understandable prescriptions electronically from the healthcare provider to the pharmacy. E-prescribing is meant to reduce the risks associated with traditional prescription script writing. It is also one of the major reasons for the push for electronic medical records. By sharing medical prescription information, e-prescribing seeks to connect the patient's team of healthcare providers to facilitate knowledgeable decision making.\n\nA \"qualified\" e-prescribing system must be capable of performing all of the following functions:\n\n\nThe basic components of an electronic prescribing system are the:\n\nThe PBM and transaction hub work closely together. The PBM works as an intermediate actor to ensure the accuracy of information, although other models may not include this to streamline the communication process.\n\nIn addition to pharmacies, medical tests can also be prescribed.\n\nThe prescriber, generally a clinician or healthcare staff, is defined as the electronic prescribing system user and sign into the system through a verification process to authenticate their identity.\n\nThe prescriber searches through the database of patient records by using patient-specific information such as first and last name, date of birth, current address etc. Once the correct patient file has been accessed, the prescriber reviews the current medical information and uploads or updates new prescription information to the medical file.\n\nThe transaction hub provides the common link between all actors (prescriber, pharmacy benefit manager, and pharmacy). It stores and maintains a master patient index for quick access to their medical information as well as a list of pharmacies.\n\nWhen the prescriber uploads new prescription information to the patient file, this is sent to the transaction hub. The transaction hub will verify against the patient index. This will automatically send information about this transaction to the PBM, who will respond to the hub with information on patient eligibility, formulary, and medication history back to the transaction hub. The transaction hub then sends this information to the prescriber to improve patient management and care by completing and authorizing the prescription. Upon which, the prescription information is sent to the pharmacy that the patient primarily goes to.\n\nWhen a pharmacy receives the prescription information from the transaction hub, it will send a confirmation message. The pharmacy also has the ability to communicate to the prescriber that the prescription order has been filled through the system. Further system development will soon allow different messages such as a patient not picking up their medication or is late to pick up medication to improve patient management.\n\nWhen an imaging center receives the prescription, the imaging center will then contact the patient and schedule the patient for his/her scan. The advantage of ePrescribing radiology is that oftentimes when a patient is handed a paper script, the patient will lose the prescription or wait to call and schedule. This can be disastrous for patients with severe underlying conditions. The imaging center will call and schedule the patient as soon as the referral arrives. There are mobile ePrescribing portals as well as web portals that handle this well, and there are advantages.\n\nCompared to paper-based prescribing, e-prescribing can improve health and reduce costs because it can:\n\nSafety improvements are highly desirable; in 2000, the Institute of Medicine identified medication errors as the most common type of medical error in health care, estimating that this leads to several thousand deaths each year.\n\nIllegibility from handwritten prescriptions is eliminated, decreasing the risk of medication errors while simultaneously decreasing risks related to liability. Oral miscommunications regarding prescriptions can be reduced, as e-prescribing should decrease the need for phone calls between prescribers and dispensers. Causes of medication errors include mistakes by the pharmacist incorrectly interpreting illegible handwriting or ambiguous nomenclature, and lapses in the prescriber's knowledge of desired dosage of a drug or undesired interactions between multiple drugs. Electronic prescribing has the potential to eliminate most of these types of errors. Warning and alert systems are provided at the point of care. E-prescribing systems can enhance an overall medication management process through clinical decision support systems that can perform checks against the patient's current medications for drug-drug interactions, drug-allergy interactions, diagnoses, body weight, age, drug appropriateness, and correct dosing. Based on these algorithms, the system can alert prescribers to contradictions, adverse reactions, and duplicate therapies. The computer can also ensure that clear and unambiguous instructions are encoded in a structured message to the pharmacist, and decision support systems can flag lethal dosages and lethal combinations of drugs. E-prescribing allows for increased access to the patient's medical records and their medication history. Having access to this information from all health care providers at the time of prescribing can support alerts related to drug inappropriateness, in combination with other medications or with specific medical issues at hand.\n\nAccording to estimates, almost 30 percent of prescriptions require pharmacy callbacks. This translates into less time available to the pharmacist for other important functions, such as educating consumers about their medications. In response, E-prescribing can significantly reduce the volume of pharmacy call-backs related to illegibility, mistaken prescription choices, formulary and pharmacy benefits, decreasing the amount of time wasted on the phone. This ultimately impacts office workflow efficiency and overall productivity in a positive manner.\n\nBoth prescribers and pharmacists can save time and resources spent on faxing prescriptions through a reduction in labor costs, handling costs, and paper expenses waste due to unreliability.\n\nWith e-prescribing, renewal authorization can be an automated process that provides efficiencies for both the prescriber and pharmacist. Pharmacy staff can generate a renewal request (authorization request) that is delivered through the electronic network to the prescriber's system. The prescriber can then review the request and act accordingly by approving or denying the request through updating the system. With limited resource utilization and just a few clicks on behalf of the prescriber, they can complete a medication renewal task while enhancing continuous patient documentation.\n\nIt is estimated that 20% of paper-based prescription orders go unfilled by the patient, partly due to the hassle of dropping off a paper prescription and waiting for it to be filled. By elimination or reducing this waiting period, e-prescribing may help reduce the number of unfilled prescriptions and hence, increasing medication adherence. Allowing the renewal of medications through this electronic system also helps improve the efficiency of this process, reducing obstacles that may result in less patient compliance. Availability of information on when patient's prescriptions are filled can also help clinicians assess patient adherence.\n\nImproved prescriber convenience can be achieved when using mobile devices, that work on a wireless network, to write and renew prescriptions. Such mobile devices may include laptops, PDAs, tablet computers, or mobile phones. This freedom of mobility allows prescribers to write/renew prescriptions anywhere, even when not in the office.\n\nE-prescribing systems enable embedded, automated analytic tools to produce queries and reports, which would be close to impossible with a paper-based system. Common examples of such reporting would be: finding all patients with a particular prescription during a drug recall, or the frequency and types of medication provided by certain health care providers.\n\nAlthough e-prescribing has the ability to streamline workflow process and increase the system's efficiency, challenges and limitations that may hinder the widespread adoption of e-prescribing practices include:\n\n\nElectronic prescription in Australia is currently provided by two service providers, MediSecure and eRx. Both services can be integrated into many of the existing clinical and pharmacy prescribing software systems. Since December 2012, they have become interoperable allowing bilateral transfer of information.\n\nPrivate companies started working with electronic prescriptions. On 2017 July easypres.com launched Bangladesh first cloud-based electronic prescription and patient management software for Doctors in Bangladesh. Within a year, more than thousand doctors registered for the software out of 83 thousand registered MBBS doctors in Bangladesh for this Digital prescription writing software. High court of Bangladesh issued a rule that doctors need to write the prescription in readable format meaning they need to use software of ALL caps later while writing prescription. This software also stores the medical history of patients and doctors can access these data easily from anywhere using the Internet.\n\nOn March 22, 2016, the Government of Canada allocated funds to Canada Health Infoway to develop an e-prescribing service. Infoway is working with Health Canada, the provinces and territories and industry stakeholders to create PrescribeIT, a multi-jurisdiction e-prescribing service. Infoway will create, operate and maintain the service, along with its partners. The service will be financially self-sustaining and is designed to be scaled across the country and will enable prescribers to electronically transmit a prescription to a patient’s pharmacy of choice. Physicians, nurse practitioners and other prescribers will be able to use the system either through their existing electronic medical record or through a standalone application. Health Canada included supporting better prescribing practices, including e-prescribing, as part of its Action on Opioid Misuse plan.\n\nUntil recently in Canada, it was the position of Health Canada that, to allow for e-prescribing, amendments to Part C of the Food and Drugs Regulations made under the Food and Drugs Act, regulations made under the Controlled Drugs and Substances Act and possibly regulations made under Personal Information Protection and Electronic Documents Act would be required. After further review, Health Canada has concluded that there are currently no regulatory impediments to moving ahead with electronically generated and transmitted prescriptions and that these are permissible to the extent that they achieve the same objectives as written prescriptions. Provinces and territories wishing to proceed with e-prescribing are obligated to ensure that electronic prescriptions meet existing regulatory requirements and achieve the same objectives as written prescriptions. For example, there must be evidence of a genuine practitioner/patient relationship, and in the case of controlled substances, pharmacists filling prescriptions must verify prescriptions are signed by the practitioner before selling or providing drugs containing controlled substances to a patient. Health Canada has collaborated with Canada Health Infoway on the development of a technical document entitled Ensuring the Authenticity of Electronic Prescriptions, in order to provide advice about how to ensure the authenticity of electronic signatures.\n\nThe use of electronic prescription has been designated as an important strategic policy to improve health care in Europe. The aim of the European Union is to have a cross-border electronic healthcare system in Europe which will enable EU citizens to obtain e-Prescriptions anywhere in Europe. The Scandinavian countries are leading Europe in deploying e-Prescription. Electronic prescriptions were introduced in Estonia in January 2010 and by mid-2013, 95% of all prescriptions in the country were being issued electronically. Other countries which use the prescription process routinely are Norway, Denmark, Finland, Sweden, Belgium, the Netherlands, Italy, Iceland, Greece, England, Scotland, Wales and Northern Ireland.\nThe European Union is pushing for more cross border health data exchange. Despite favourable attitudes towards cross border e-Prescriptions, multiple perceived barriers impede its incorporation in clinical practice. There are varying interpretations and implementations of data protection and confidentiality laws in the 27 member states. Infrastructures are not in place to support the system and stakeholders in some jurisdictions are reluctant to embrace e-health due to the high cost and the lack of security of the systems. Member states have varying degrees of health care policy, privacy enforcement and laws concerning data protection, telecommunication services and digital signature with regards to e-Prescription. Interoperability of different systems is only a partial solution. Security and enforcement of privacy must also be equally enforced.\n\nIn India some private hospitals started using electronic prescription. But a major step was taken by government of West Bengal in August 2014 when they started the process of issuing e-prescriptions instead of hand-written instructions in top government hospitals.\nThe biggest advantage of the system is that a patient has all his medical data stored in the server of state health department which can be referred to in future.\nIn the private sector number companies have initiated to build software to support Electronic Prescription in India. ERXPAD.COM is one among the pioneer player offer cloud based electronic prescription software ( erx) in India.\n\nWith the development and implementation of electronic technologies in Russian healthcare system, electronic prescription became part of the project called EMIAS. EMIAS is the digital system designed to increase the quality and access of the medical aid in the public health facility. The project was designed and being implemented as part of «Digital city» program in execution of the Moscow Government's order from April 7, 2014 (as Moscow government amended on 21.05.2013 № 22-PP). \nThe system offers special portal Emias.Info, that provides appointment service to the patients and client area with different services including e-Prescription. Government social program allows getting pharmaceutical products for free or with the discount, depending on the category of the citizen.\n\nAbout 420 million repeat prescriptions are generated in the UK each year - about 200 for each general practitioner each week. They account for about 80% of the cost of medication in primary care. Paper based Repeat Dispensing Services were introduced by the NHS in 2005, and in 2009 it became possible to use the NHS Electronic Prescription Service for this purpose. In 2017 awareness of the scheme among patients was low.\nIn October 2017 Keith McNeil, NHS England's chief clinical information officer demanded that NHS hospitals should be moved rapidly onto electronic prescribing in the light of research showing it would cut serious prescribing errors by more than half. There was no information about the extent to which it is happening in hospitals. \n\nAfter successful pilots in London and the East Midlands it was agreed in April 2018 that electronic prescribing should be introduced in all urgent care settings in England, including NHS 111 and other Out-of-hours services so that dispensed medication can be ready for collection at a pharmacy when patients arrive. \n\nIn the United States, the HITECH Act promotes adoption of this technology by defining e-prescribing as one meaningful use of an electronic medical record. Standards for transmitting, recording, and describing prescriptions have been developed by the National Council for Prescription Drug Programs, in particular the SCRIPT standard, which describes data formats. Elsewhere in the world, health care systems have been slower to adopt e-prescribing standards.\n\nAdoption of e-prescribing technology has accelerated in the United States, in large part, due to the arrival of Stage 2 of meaningful use. One of the Stage 2 core measures is: \"Generate and transmit permissible prescriptions electronically (e-Rx.)\" In order to meet this measure, practices must prescribe and transmit at least 50 percent of permissible prescriptions electronically.\n\nAccording to data released in May 2012 by Surescripts, a company which operates the nation's largest health information (e-prescribing) network, roughly 317,000 office-based physicians now e-prescribe in the United States.\nA more recent report released by the Office of the National Coordinator for Health IT in June 2012 finds that 48 percent of U.S. physicians use e-prescribing systems. National growth in e-prescribing over the period September 2008 through June 2012 increased over 40 percent, with individual states increasing adoption anywhere from 28 percent to 70 percent.\n\n\n"}
{"id": "66997", "url": "https://en.wikipedia.org/wiki?curid=66997", "title": "Epidemiology", "text": "Epidemiology\n\nEpidemiology is the study and analysis of the distribution (who, when, and where) and determinants of health and disease conditions in defined populations.\n\nIt is the cornerstone of public health, and shapes policy decisions and evidence-based practice by identifying risk factors for disease and targets for preventive healthcare. Epidemiologists help with study design, collection, and statistical analysis of data, amend interpretation and dissemination of results (including peer review and occasional systematic review). Epidemiology has helped develop methodology used in clinical research, public health studies, and, to a lesser extent, basic research in the biological sciences.\n\nMajor areas of epidemiological study include disease causation, transmission, outbreak investigation, disease surveillance, forensic epidemiology, occupational epidemiology, screening, biomonitoring, and comparisons of treatment effects such as in clinical trials. Epidemiologists rely on other scientific disciplines like biology to better understand disease processes, statistics to make efficient use of the data and draw appropriate conclusions, social sciences to better understand proximate and distal causes, and engineering for exposure assessment.\n\n\"Epidemiology\", literally meaning \"the study of what is upon the people\", is derived , suggesting that it applies only to human populations. However, the term is widely used in studies of zoological populations (veterinary epidemiology), although the term \"epizoology\" is available, and it has also been applied to studies of plant populations (botanical or plant disease epidemiology).\n\nThe distinction between \"epidemic\" and \"endemic\" was first drawn by Hippocrates, to distinguish between diseases that are \"visited upon\" a population (epidemic) from those that \"reside within\" a population (endemic). The term \"epidemiology\" appears to have first been used to describe the study of epidemics in 1802 by the Spanish physician Villalba in \"Epidemiología Española\". Epidemiologists also study the interaction of diseases in a population, a condition known as a syndemic.\n\nThe term epidemiology is now widely applied to cover the description and causation of not only epidemic disease, but of disease in general, and even many non-disease, health-related conditions, such as high blood pressure and obesity. Therefore, this epidemiology is based upon how the pattern of the disease causes change in the function of everyone.\n\nThe Greek physician Hippocrates, known as the father of medicine, sought a logic to sickness; he is the first person known to have examined the relationships between the occurrence of disease and environmental influences. Hippocrates believed sickness of the human body to be caused by an imbalance of the four humors (air, fire, water and earth “atoms”). The cure to the sickness was to remove or add the humor in question to balance the body. This belief led to the application of bloodletting and dieting in medicine. He coined the terms \"endemic\" (for diseases usually found in some places but not in others) and \"epidemic\" (for diseases that are seen at some times but not others).\n\nIn the middle of the 16th century, a doctor from Verona named Girolamo Fracastoro was the first to propose a theory that these very small, unseeable, particles that cause disease were alive. They were considered to be able to spread by air, multiply by themselves and to be destroyable by fire. In this way he refuted Galen's miasma theory (poison gas in sick people). In 1543 he wrote a book \"De contagione et contagiosis morbis\", in which he was the first to promote personal and environmental hygiene to prevent disease. The development of a sufficiently powerful microscope by Antonie van Leeuwenhoek in 1675 provided visual evidence of living particles consistent with a germ theory of disease.\n\nWu Youke (1582–1652) developed the concept that some diseases were caused by transmissible agents, which he called liqi (pestilential factors). His book Wenyi Lun (Treatise on Acute Epidemic Febrile Diseases) can be regarded as the main etiological work that brought forward the concept, ultimately attributed to Westerners, of germs as a cause of epidemic diseases (source: http://baike.baidu.com/view/143117.htm). His concepts are still considered in current scientific research in relation to Traditional Chinese Medicine studies (see: http://apps.who.int/medicinedocs/en/d/Js6170e/4.html).\n\nAnother pioneer, Thomas Sydenham (1624–1689), was the first to distinguish the fevers of Londoners in the later 1600s. His theories on cures of fevers met with much resistance from traditional physicians at the time. He was not able to find the initial cause of the smallpox fever he researched and treated.\nJohn Graunt, a haberdasher and amateur statistician, published \"Natural and Political Observations ... upon the Bills of Mortality\" in 1662. In it, he analysed the mortality rolls in London before the Great Plague, presented one of the first life tables, and reported time trends for many diseases, new and old. He provided statistical evidence for many theories on disease, and also refuted some widespread ideas on them.\n\nJohn Snow is famous for his investigations into the causes of the 19th century cholera epidemics, and is also known as the father of (modern) epidemiology. He began with noticing the significantly higher death rates in two areas supplied by Southwark Company. His identification of the Broad Street pump as the cause of the Soho epidemic is considered the classic example of epidemiology. Snow used chlorine in an attempt to clean the water and removed the handle; this ended the outbreak. This has been perceived as a major event in the history of public health and regarded as the founding event of the science of epidemiology, having helped shape public health policies around the world. However, Snow's research and preventive measures to avoid further outbreaks were not fully accepted or put into practice until after his death.\n\nOther pioneers include Danish physician Peter Anton Schleisner, who in 1849 related his work on the prevention of the epidemic of neonatal tetanus on the Vestmanna Islands in Iceland. Another important pioneer was Hungarian physician Ignaz Semmelweis, who in 1847 brought down infant mortality at a Vienna hospital by instituting a disinfection procedure. His findings were published in 1850, but his work was ill-received by his colleagues, who discontinued the procedure. Disinfection did not become widely practiced until British surgeon Joseph Lister 'discovered' antiseptics in 1865 in light of the work of Louis Pasteur.\n\nIn the early 20th century, mathematical methods were introduced into epidemiology by Ronald Ross, Janet Lane-Claypon, Anderson Gray McKendrick, and others.\n\nAnother breakthrough was the 1954 publication of the results of a British Doctors Study, led by Richard Doll and Austin Bradford Hill, which lent very strong statistical support to the link between tobacco smoking and lung cancer.\n\nIn the late 20th century, with advancement of biomedical sciences, a number of molecular markers in blood, other biospecimens and environment were identified as predictors of development or risk of a certain disease. Epidemiology research to examine the relationship between these biomarkers analyzed at the molecular level, and disease was broadly named “molecular epidemiology”. Specifically, \"genetic epidemiology\" has been used for epidemiology of germline genetic variation and disease. Genetic variation is typically determined using DNA from peripheral blood leukocytes. Since the 2000s, genome-wide association studies (GWAS) have been commonly performed to identify genetic risk factors for many diseases and health conditions.\n\nWhile most molecular epidemiology studies are still using conventional disease diagnosis and classification systems, it is increasingly recognized that disease progression represents inherently heterogeneous processes differing from person to person. Conceptually, each individual has a unique disease process different from any other individual (“the unique disease principle”), considering uniqueness of the exposome (a totality of endogenous and exogenous / environmental exposures) and its unique influence on molecular pathologic process in each individual. Studies to examine the relationship between an exposure and molecular pathologic signature of disease (particularly cancer) became increasingly common throughout the 2000s. However, the use of molecular pathology in epidemiology posed unique challenges including lack of research guidelines and standardized statistical methodologies, and paucity of interdisciplinary experts and training programs. Furthermore, the concept of disease heterogeneity appears to conflict with the long-standing premise in epidemiology that individuals with the same disease name have similar etiologies and disease processes. To resolve these issues and advance population health science in the era of molecular precision medicine, “molecular pathology” and “epidemiology” was integrated to create a new interdisciplinary field of “molecular pathological epidemiology” (MPE), defined as “epidemiology of molecular pathology and heterogeneity of disease”. In MPE, investigators analyze the relationships between; (A) environmental, dietary, lifestyle and genetic factors; (B) alterations in cellular or extracellular molecules; and (C) evolution and progression of disease. A better understanding of heterogeneity of disease pathogenesis will further contribute to elucidate etiologies of disease. The MPE approach can be applied to not only neoplastic diseases but also non-neoplastic diseases. The concept and paradigm of MPE have become widespread in the 2010s.\n\nBy 2012 it was recognized that many pathogens' evolution is rapid enough to be highly relevant to epidemiology, and that therefore much could be gained from an interdisciplinary approach to infectious disease integrating epidemiology and molecular evolution to \"inform control strategies, or even patient treatment.\"\n\nEpidemiologists employ a range of study designs from the observational to experimental and generally categorized as descriptive, analytic (aiming to further examine known associations or hypothesized relationships), and experimental (a term often equated with clinical or community trials of treatments and other interventions). In observational studies, nature is allowed to “take its course,\" as epidemiologists observe from the sidelines. Conversely, in experimental studies, the epidemiologist is the one in control of all of the factors entering a certain case study. Epidemiological studies are aimed, where possible, at revealing unbiased relationships between exposures such as alcohol or smoking, biological agents, stress, or chemicals to mortality or morbidity. The identification of causal relationships between these exposures and outcomes is an important aspect of epidemiology. Modern epidemiologists use informatics as a tool.\n\nObservational studies have two components, descriptive and analytical. Descriptive observations pertain to the “who, what, where and when of health-related state occurrence”. However, analytical observations deal more with the ‘how’ of a health-related event. Experimental epidemiology contains three case types: randomized controlled trials (often used for new medicine or drug testing), field trials (conducted on those at a high risk of contracting a disease), and community trials (research on social originating diseases).\n\nThe term 'epidemiologic triad' is used to describe the intersection of \"Host\", \"Agent\", and \"Environment\" in analyzing an outbreak.\n\nCase-series may refer to the qualitative study of the experience of a single patient, or small group of patients with a similar diagnosis, or to a statistical factor with the potential to produce illness with periods when they are unexposed.\n\nThe former type of study is purely descriptive and cannot be used to make inferences about the general population of patients with that disease. These types of studies, in which an astute clinician identifies an unusual feature of a disease or a patient's history, may lead to a formulation of a new hypothesis. Using the data from the series, analytic studies could be done to investigate possible causal factors. These can include case-control studies or prospective studies. A case-control study would involve matching comparable controls without the disease to the cases in the series. A prospective study would involve following the case series over time to evaluate the disease's natural history.\n\nThe latter type, more formally described as self-controlled case-series studies, divide individual patient follow-up time into exposed and unexposed periods and use fixed-effects Poisson regression processes to compare the incidence rate of a given outcome between exposed and unexposed periods. This technique has been extensively used in the study of adverse reactions to vaccination and has been shown in some circumstances to provide statistical power comparable to that available in cohort studies.\n\nCase-control studies select subjects based on their disease status. It is a retrospective study. A group of individuals that are disease positive (the \"case\" group) is compared with a group of disease negative individuals (the \"control\" group). The control group should ideally come from the same population that gave rise to the cases. The case-control study looks back through time at potential exposures that both groups (cases and controls) may have encountered. A 2×2 table is constructed, displaying exposed cases (A), exposed controls (B), unexposed cases (C) and unexposed controls (D). The statistic generated to measure association is the odds ratio (OR), which is the ratio of the odds of exposure in the cases (A/C) to the odds of exposure in the controls (B/D), i.e. OR = (AD/BC).\n\nIf the OR is significantly greater than 1, then the conclusion is \"those with the disease are more likely to have been exposed,\" whereas if it is close to 1 then the exposure and disease are not likely associated. If the OR is far less than one, then this suggests that the exposure is a protective factor in the causation of the disease.\nCase-control studies are usually faster and more cost effective than cohort studies, but are sensitive to bias (such as recall bias and selection bias). The main challenge is to identify the appropriate control group; the distribution of exposure among the control group should be representative of the distribution in the population that gave rise to the cases. This can be achieved by drawing a random sample from the original population at risk. This has as a consequence that the control group can contain people with the disease under study when the disease has a high attack rate in a population.\n\nA major drawback for case control studies is that, in order to be considered to be statistically significant, the minimum number of cases required at the 95% confidence interval is related to the odds ratio by the equation:\n\nwhere N is the ratio of cases to controls.\nAs the odds ratio approached 1, approaches 0; rendering case control studies all but useless for low odds ratios. For instance, for an odds ratio of 1.5 and cases = controls, the table shown above would look like this:\n\nFor an odds ratio of 1.1:\n\nCohort studies select subjects based on their exposure status. The study subjects should be at risk of the outcome under investigation at the beginning of the cohort study; this usually means that they should be disease free when the cohort study starts. The cohort is followed through time to assess their later outcome status. An example of a cohort study would be the investigation of a cohort of smokers and non-smokers over time to estimate the incidence of lung cancer. The same 2×2 table is constructed as with the case control study. However, the point estimate generated is the relative risk (RR), which is the probability of disease for a person in the exposed group, \"P\" = \"A\" / (\"A\" + \"B\") over the probability of disease for a person in the unexposed group, \"P\" = \"C\" / (\"C\" + \"D\"), i.e. \"RR\" = \"P\" / \"P\".\n\nAs with the OR, a RR greater than 1 shows association, where the conclusion can be read \"those with the exposure were more likely to develop disease.\"\n\nProspective studies have many benefits over case control studies. The RR is a more powerful effect measure than the OR, as the OR is just an estimation of the RR, since true incidence cannot be calculated in a case control study where subjects are selected based on disease status. Temporality can be established in a prospective study, and confounders are more easily controlled for. However, they are more costly, and there is a greater chance of losing subjects to follow-up based on the long time period over which the cohort is followed.\n\nCohort studies also are limited by the same equation for number of cases as for cohort studies, but, if the base incidence rate in the study population is very low, the number of cases required is reduced by ½.\n\nAlthough epidemiology is sometimes viewed as a collection of statistical tools used to elucidate the associations of exposures to health outcomes, a deeper understanding of this science is that of discovering \"causal\" relationships.\n\n\"Correlation does not imply causation\" is a common theme for much of the epidemiological literature. For epidemiologists, the key is in the term inference. Correlation, or at least association between two variables, is a necessary but not sufficient criteria for inference that one variable causes the other. Epidemiologists use gathered data and a broad range of biomedical and psychosocial theories in an iterative way to generate or expand theory, to test hypotheses, and to make educated, informed assertions about which relationships are causal, and about exactly how they are causal.\n\nEpidemiologists emphasize that the \"one cause – one effect\" understanding is a simplistic mis-belief. Most outcomes, whether disease or death, are caused by a chain or web consisting of many component causes. Causes can be distinguished as necessary, sufficient or probabilistic conditions. If a necessary condition can be identified and controlled (e.g., antibodies to a disease agent, energy in an injury), the harmful outcome can be avoided (Robertson, 2015).\n\nIn 1965, Austin Bradford Hill proposed a series of considerations to help assess evidence of causation, which have come to be commonly known as the \"Bradford Hill criteria\". In contrast to the explicit intentions of their author, Hill's considerations are now sometimes taught as a checklist to be implemented for assessing causality. Hill himself said \"None of my nine viewpoints can bring indisputable evidence for or against the cause-and-effect hypothesis and none can be required \"sine qua non\".\"\n\n\nEpidemiological studies can only go to prove that an agent could have caused, but not that it did cause, an effect in any particular case:\n\n\"Epidemiology is concerned with the incidence of disease in populations and does not address the question of the cause of an individual's disease. This question, sometimes referred to as specific causation, is beyond the domain of the science of epidemiology. Epidemiology has its limits at the point where an inference is made that the relationship between an agent and a disease is causal (general causation) and where the magnitude of excess risk attributed to the agent has been determined; that is, epidemiology addresses whether an agent can cause a disease, not whether an agent did cause a specific plaintiff's disease.\"\n\nIn United States law, epidemiology alone cannot prove that a causal association does not exist in general. Conversely, it can be (and is in some circumstances) taken by US courts, in an individual case, to justify an inference that a causal association does exist, based upon a balance of probability.\n\nThe subdiscipline of forensic epidemiology is directed at the investigation of specific causation of disease or injury in individuals or groups of individuals in instances in which causation is disputed or is unclear, for presentation in legal settings.\n\nEpidemiological practice and the results of epidemiological analysis make a significant contribution to emerging population-based health management frameworks.\n\nPopulation-based health management encompasses the ability to:\n\n\nModern population-based health management is complex, requiring a multiple set of skills (medical, political, technological, mathematical etc.) of which epidemiological practice and analysis is a core component, that is unified with management science to provide efficient and effective health care and health guidance to a population. This task requires the forward looking ability of modern risk management approaches that transform health risk factors, incidence, prevalence and mortality statistics (derived from epidemiological analysis) into management metrics that not only guide how a health system responds to current population health issues, but also how a health system can be managed to better respond to future potential population health issues.\n\nExamples of organizations that use population-based health management that leverage the work and results of epidemiological practice include Canadian Strategy for Cancer Control, Health Canada Tobacco Control Programs, Rick Hansen Foundation, Canadian Tobacco Control Research Initiative.\n\nEach of these organizations use a population-based health management framework called Life at Risk that combines epidemiological quantitative analysis with demographics, health agency operational research and economics to perform:\n\n\nApplied epidemiology is the practice of using epidemiological methods to protect or improve the health of a population. Applied field epidemiology can include investigating communicable and non-communicable disease outbreaks, mortality and morbidity rates, and nutritional status, among other indicators of health, with the purpose of communicating the results to those who can implement appropriate policies or disease control measures.\n\nAs the surveillance and reporting of diseases and other health factors becomes increasingly difficult in humanitarian crisis situations, the methodologies used to report the data are compromised. One study found that less than half (42.4%) of nutrition surveys sampled from humanitarian contexts correctly calculated the prevalence of malnutrition and only one-third (35.3%) of the surveys met the criteria for quality. Among the mortality surveys, only 3.2% met the criteria for quality. As nutritional status and mortality rates help indicate the severity of a crisis, the tracking and reporting of these health factors is crucial.\n\nVital registries are usually the most effective ways to collect data, but in humanitarian contexts these registries can be non-existent, unreliable, or inaccessible. As such, mortality is often inaccurately measured using either prospective demographic surveillance or retrospective mortality surveys. Prospective demographic surveillance requires lots of manpower and is difficult to implement in a spread-out population. Retrospective morality surveys are prone to selection and reporting biases. Other methods are being developed, but are not common practice yet.\n\nDifferent fields in epidemiology have different levels of validity. One way to assess the validity of findings is the ratio of false-positives (claimed effects that are not correct) to false-negatives (studies which fail to support a true effect). To take the field of genetic epidemiology, candidate-gene studies produced over 100 false-positive findings for each false-negative. By contrast genome-wide association appear close to the reverse, with only one false positive for every 100 or more false-negatives. This ratio has improved over time in genetic epidemiology as the field has adopted stringent criteria. By contrast other epidemiological fields have not required such rigorous reporting and are much less reliable as a result.\n\nRandom error is the result of fluctuations around a true value because of sampling variability. Random error is just that: random. It can occur during data collection, coding, transfer, or analysis. Examples of random error include: poorly worded questions, a misunderstanding in interpreting an individual answer from a particular respondent, or a typographical error during coding. Random error affects measurement in a transient, inconsistent manner and it is impossible to correct for random error.\n\nThere is random error in all sampling procedures. This is called sampling error.\n\nPrecision in epidemiological variables is a measure of random error. Precision is also inversely related to random error, so that to reduce random error is to increase precision. Confidence intervals are computed to demonstrate the precision of relative risk estimates. The narrower the confidence interval, the more precise the relative risk estimate.\n\nThere are two basic ways to reduce random error in an epidemiological study. The first is to increase the sample size of the study. In other words, add more subjects to your study. The second is to reduce the variability in measurement in the study. This might be accomplished by using a more precise measuring device or by increasing the number of measurements.\n\nNote, that if sample size or number of measurements are increased, or a more precise measuring tool is purchased, the costs of the study are usually increased. There is usually an uneasy balance between the need for adequate precision and the practical issue of study cost.\n\nA systematic error or bias occurs when there is a difference between the true value (in the population) and the observed value (in the study) from any cause other than sampling variability. An example of systematic error is if, unknown to you, the pulse oximeter you are using is set incorrectly and adds two points to the true value each time a measurement is taken. The measuring device could be precise but not accurate. Because the error happens in every instance, it is systematic. Conclusions you draw based on that data will still be incorrect. But the error can be reproduced in the future (e.g., by using the same mis-set instrument).\n\nA mistake in coding that affects \"all\" responses for that particular question is another example of a systematic error.\n\nThe validity of a study is dependent on the degree of systematic error. Validity is usually separated into two components:\n\n\nSelection bias occurs when study subjects are selected or become part of the study as a result of a third, unmeasured variable which is associated with both the exposure and outcome of interest. For instance, it has repeatedly been noted that cigarette smokers and non smokers tend to differ in their study participation rates. (Sackett D cites the example of Seltzer et al., in which 85% of non smokers and 67% of smokers returned mailed questionnaires.) It is important to note that such a difference in response will not lead to bias if it is not also associated with a systematic difference in outcome between the two response groups.\n\nInformation bias is bias arising from systematic error in the assessment of a variable. An example of this is recall bias. A typical example is again provided by Sackett in his discussion of a study examining the effect of specific exposures on fetal health: \"in questioning mothers whose recent pregnancies had ended in fetal death or malformation (cases) and a matched group of mothers whose pregnancies ended normally (controls) it was found that 28% of the former, but only 20% of the latter, reported exposure to drugs which could not be substantiated either in earlier prospective\ninterviews or in other health records\". In this example, recall bias probably occurred as a result of women who had had miscarriages having an apparent tendency to better recall and therefore report previous exposures.\n\nConfounding has traditionally been defined as bias arising from the co-occurrence or mixing of effects of extraneous factors, referred to as confounders, with the main effect(s) of interest. A more recent definition of confounding invokes the notion of \"counterfactual\" effects. According to this view, when one observes an outcome of interest, say Y=1 (as opposed to Y=0), in a given population A which is entirely exposed (i.e. exposure \"X\" = 1 for every unit of the population) the risk of this event will be \"R\". The counterfactual or unobserved risk \"R\" corresponds to the risk which would have been observed if these same individuals had been unexposed (i.e. \"X\" = 0 for every unit of the population). The true effect of exposure therefore is: \"R\" − \"R\" (if one is interested in risk differences) or \"R\"/\"R\" (if one is interested in relative risk). Since the counterfactual risk \"R\" is unobservable we approximate it using a second population B and we actually measure the following relations: \"R\" − \"R\" or \"R\"/\"R\". In this situation, confounding occurs when \"R\" ≠ \"R\". (NB: Example assumes binary outcome and exposure variables.)\n\nSome epidemiologists prefer to think of confounding separately from common categorizations of bias since, unlike selection and information bias, confounding stems from real causal effects.\n\nTo date, few universities offer epidemiology as a course of study at the undergraduate level. One notable undergraduate program exists at Johns Hopkins University, where students who major in public health can take graduate level courses, including epidemiology, their senior year at the Bloomberg School of Public Health.\n\nAlthough epidemiologic research is conducted by individuals from diverse disciplines, including clinically trained professionals such as physicians, formal training is available through Masters or Doctoral programs including Master of Public Health (MPH), Master of Science of Epidemiology (MSc.), Doctor of Public Health (DrPH), Doctor of Pharmacy (PharmD), Doctor of Philosophy (PhD), Doctor of Science (ScD). Many other graduate programs, e.g., Doctor of Social Work (DSW), Doctor of Clinical Practice (DClinP), Doctor of Podiatric Medicine (DPM), Doctor of Veterinary Medicine (DVM), Doctor of Nursing Practice (DNP), Doctor of Physical Therapy (DPT), or for clinically trained physicians, Doctor of Medicine (MD) or Bachelor of Medicine and Surgery (MBBS or MBChB) and Doctor of Osteopathic Medicine (DO), include some training in epidemiologic research or related topics, but this training is generally substantially less than offered in training programs focused on epidemiology or public health. Reflecting the strong historical tie between epidemiology and medicine, formal training programs may be set in either schools of public health and medical schools.\n\nAs public health/health protection practitioners, epidemiologists work in a number of different settings. Some epidemiologists work 'in the field'; i.e., in the community, commonly in a public health/health protection service, and are often at the forefront of investigating and combating disease outbreaks. Others work for non-profit organizations, universities, hospitals and larger government entities such as state and local health departments, various Ministries of Health, Doctors without Borders, the Centers for Disease Control and Prevention (CDC), the Health Protection Agency, the World Health Organization (WHO), or the Public Health Agency of Canada. Epidemiologists can also work in for-profit organizations such as pharmaceutical and medical device companies in groups such as market research or clinical development.\n\n"}
{"id": "36335582", "url": "https://en.wikipedia.org/wiki?curid=36335582", "title": "Epidemiology data for low-linear energy transfer radiation", "text": "Epidemiology data for low-linear energy transfer radiation\n\nEpidemiological studies of the health effects of low levels of ionizing radiation, in particular the incidence and mortality from various forms of cancer, have been carried out in different population groups exposed to such radiation. These have included survivors of the atomic bombings of Hiroshima and Nagasaki in 1945, workers at nuclear reactors, and medical patients treated with X-rays.\n\nSurvivors of the atomic bomb explosions at Hiroshima and Nagasaki, Japan have been the subjects of a Life Span Study (LSS), which has provided valuable epidemiological data.\n\nThe LSS population went through several changes:\n\nHowever, some 44,000 individuals were censured or excluded from the LSS project, so there remained about 86,000 people who were followed through the study. There is a gap in knowledge of the earliest cancer that developed in the first few years after the war, which impacts the assessment of leukemia to an important extent and for solid cancers to a minor extent. Table 1 shows summary statistics of the number of persons and deaths for different dose groups. These comparisons show that the doses that were received by the LSS population overlap strongly with the doses that are of concern to NASA Exploration mission (i.e., 50 to 2,000 milliSieverts (mSv)).\n\nFigure 1 shows the dose response for the excess relative risk (ERR) for all solid cancers from Preston et al. Tables 2 and 3 show several summary parameters for tissue-specific cancer mortality risks for females and males, respectively, including estimates of ERR, excess absolute risk (EAR), and percentage attributable risks. Cancer incidence risks from low-LET radiation are about 60% higher than cancer mortality risks.\n\nThe BEIR VII Report contains an extensive review of data sets from human populations, including nuclear reactor workers and patients who were treated with radiation. The recent report from Cardis et al. describes a meta-analysis for reactor workers from several countries. A meta-analysis at specific cancer sites, including breast, lung, and leukemia, has also been performed. These studies require adjustments for photon energy, dose-rate, and country of origin as well as adjustments made in single population studies. Table 4 shows the results that are derived from Preston et al. for a meta-analysis of breast cancer risks in eight populations, including the atomic-bomb survivors. The median ERR varies by slightly more than a factor of two, but confidence levels significantly overlap. Adjustments for photon energy or dose-rate and fractionation have not been made. These types of analysis lend confidence to risk assessments as well as showing the limitations of such data sets.\n\nOf special interest to NASA is the dependence on age at exposure of low-LET cancer risk projections. The BEIR VII report prefers models that show less than a 25% reduction in risk over the range from 35 to 55 years, while NCRP Report No. 132 shows about a two-fold reduction over this range.\n\n"}
{"id": "14020590", "url": "https://en.wikipedia.org/wiki?curid=14020590", "title": "Gluconasturtiin", "text": "Gluconasturtiin\n\nGluconasturtiin (phenethylglucosinolate) is one of the most widely distributed glucosinolates in the cruciferous vegetables, mainly in the roots, and is probably one of the plant compounds responsible for the natural pest-inhibiting properties of growing crucifers, such as cabbage, mustard or rape, in rotation with other crops. This effect of gluconasturtiin is most likely due to its degradation by the plant enzyme myrosinase into phenethyl isothiocyanate, which is toxic to many organisms.\n\nGluconasturtiin is named from its occurrence in watercress (\"Nasturtium officinale\"). Among the vegetables, it is also found in horseradish (\"Armoracia rusticana\") along with sinigrin. Both compounds elicit a pungent taste.\n\nIn one investigation of horseradish roots, sinigrin concentration represented 83% and gluconasturtiin 11% of the extracted glucosinolates.\n\n"}
{"id": "808818", "url": "https://en.wikipedia.org/wiki?curid=808818", "title": "Health effects of tea", "text": "Health effects of tea\n\nAlthough health benefits have been assumed throughout the history of using \"Camellia sinensis\" as a common beverage, there is no high-quality evidence that tea confers significant benefits. In clinical research over the early 21st century, tea has been studied extensively for its potential to lower the risk of human diseases, but none of this research is conclusive as of 2017.\n\nIn regions without access to safe drinking water, boiling water to make tea is effective for reducing waterborne diseases by destroying pathogenic microorganisms.\n\nTea drinking accounts for a high proportion of aluminum in the human diet. The levels are safe, but there has been some concern that aluminum traces may be associated with Alzheimer's disease. A 2013 study additionally indicated that some teas contained lead (mostly Chinese) and aluminum (Indian/Sri Lanka blends, China). There is still insufficient evidence to draw firm conclusions on this subject.\n\nMost studies have found no association between tea intake and iron absorption. However, drinking excessive amounts of black tea may inhibit the absorption of iron, and may harm people with anaemia.\n\nConcerns have been raised about the traditional method of over-boiling tea to produce a decoction, which may increase the amount of environmental contaminants released and consumed.\n\nAll tea leaves contain fluoride; however, mature leaves contain as much as 10 to 20 times the fluoride levels of young leaves from the same plant.\n\nThe fluoride content of a tea leaf depends on the leaf picking method used and the fluoride content of the soil from which it has been grown; tea plants absorb this element at a greater rate than other plants. Care in the choice of the location where the plant is grown may reduce the risk. It is speculated that hand-picked tea would contain less fluoride than machine-harvested tea, because there is a much lower chance of harvesting older leaves during the harvest process. A 2013 British study of 38 teas found that cheaper UK supermarket tea blends had the highest levels of fluoride with about 580 mg per kilogram, green teas averaged about 397 mg per kg and pure blends about 132 mg per kg. The researchers suggested that economy teas may use older leaves which contain more fluoride. They calculated a person drinking a litre of economy tea per day would consume about 6 mg of fluoride, above the recommended average dietary intake level of 3-4 mg of fluoride per day, but below the maximum tolerable amount of 10 mg of fluoride per day.\n\nTea contains oxalate, overconsumption of which can cause kidney stones, as well as binding with free calcium in the body. The bioavailability of oxalate from tea is low, thus a possible negative effect requires a large intake of tea. Massive black tea consumption has been linked to kidney failure due to its high oxalate content (acute oxalate nephropathy).\n\nTea also contains theanine and the stimulant caffeine at about 3% of its dry weight, translating to between 30 mg and 90 mg per 8 oz (250 ml) cup depending on type, brand and brewing method. Tea also contains small amounts of theobromine and theophylline. Dry tea has more caffeine by weight than dry coffee; nevertheless, more dry coffee than dry tea is used in typical drink preparations, which results in a cup of brewed tea containing significantly less caffeine than a cup of coffee of the same size.\n\nThe caffeine in tea is a mild diuretic. However, the British Dietetic Association has suggested that tea can be used to supplement normal water consumption, and that \"the style of tea and coffee and the amounts we drink in the UK are unlikely to have a negative effect [on hydration]\".\n\nDrinking caffeinated tea may improve mental alertness. There is preliminary evidence that the caffeine from long-term tea (or coffee) consumption provides a small amount of protection against the progression of dementia or Alzheimer's disease during aging, although the results across numerous studies were inconsistent.\n\nIn 2011, the US Food and Drug Administration (FDA) reported that there was very little evidence to support the claim that green tea consumption may reduce the risk of breast and prostate cancer.\n\nThe US National Cancer Institute reports that in epidemiological studies and the few clinical trials of tea for the prevention of cancer, the results have been inconclusive. The institute \"does not recommend for or against the use of tea to reduce the risk of any type of cancer.\" ... \"Inconsistencies in study findings regarding tea and cancer risk may be due to variability in tea preparation, tea consumption, the bioavailability of tea compounds (the amounts that can be absorbed by the body), lifestyle differences, and individual genetic differences.\" Though there is some positive evidence for risk reduction of breast, prostate, ovarian, and endometrial cancers with green tea, it is weak and inconclusive.\n\nMeta-analyses of observational studies have concluded that black tea consumption does not appear to protect against the development of oral cancers in Asian or Caucasian populations, the development of esophageal cancer or prostate cancer in Asian populations, or the development of lung cancer.\n\nIn preliminary long-term clinical studies, black tea consumption showed evidence for providing a small, reduced risk of stroke, whereas, in another review, green tea and black tea did not have significant effects on the risk of coronary heart disease. Two reviews of randomized controlled trials concluded that long-term consumption of black tea slightly lowers systolic and diastolic blood pressures (about 1–2 mmHg), a finding based on limited evidence. A 2013 Cochrane review found some evidence of benefit from tea consumption on cardiovascular disease, though more research is needed. \n\nTea consumption does not appear to affect the risk of bone fracture including hip fractures or fractures of the humerus in men or women.\n\nAlthough green tea is commonly believed to be a weight loss aid, there is no good evidence that its long-term consumption has any meaningful benefit in helping overweight or obese people to lose weight, or that it helps to maintain a healthy body weight. Use of green tea for attempted weight loss carries a small risk of adverse effects, such as nausea, constipation, and stomach discomfort.\n\n\n"}
{"id": "2176031", "url": "https://en.wikipedia.org/wiki?curid=2176031", "title": "Healthcare Effectiveness Data and Information Set", "text": "Healthcare Effectiveness Data and Information Set\n\nThe Healthcare Effectiveness Data and Information Set (HEDIS) is a widely used set of performance measures in the managed care industry, developed and maintained by the National Committee for Quality Assurance (NCQA).\n\nHEDIS was designed to allow consumers to compare health plan performance to other plans and to national or regional benchmarks. Although not originally intended for trending, HEDIS results are increasingly used to track year-to-year performance. HEDIS is one component of NCQA's accreditation process, although some plans submit HEDIS data without seeking accreditation. An incentive for many health plans to collect HEDIS data is a Centers for Medicare and Medicaid Services (CMS) requirement that health maintenance organizations (HMOs) submit Medicare HEDIS data in order to provide HMO services for Medicare enrollees under a program called .\n\nHEDIS was originally titled the \"HMO Employer Data and Information Set\" as of version 1.0 of 1991. In 1993, Version 2.0 of HEDIS was known as the \"Health Plan Employer Data and Information Set\". Version 3.0 of HEDIS was released in 1997. In July 2007, NCQA announced that the meaning of \"HEDIS\" would be changed to \"Healthcare Effectiveness Data and Information Set.\"\n\nIn current usage, the \"reporting year\" after the term \"HEDIS\" is one year following the year reflected in the data; for example, the \"HEDIS 2009\" reports, available in June 2009, contain analyses of data collected from \"measurement year\" January–December 2008.\n\nThe 83 HEDIS measures are divided into five \"domains of care\": \n\nMeasures are added, deleted, and revised annually. For example, a measure for the length of stay after giving birth was deleted after legislation mandating minimum length of stay rendered this measure nearly useless. Increased attention to medical care for seniors prompted the addition of measures related to glaucoma screening and osteoporosis treatment for older adults. Other health care concerns covered by HEDIS are immunizations, cancer screenings, treatment after heart attacks, diabetes, asthma, flu shots, access to services, dental care, alcohol and drug dependence treatment, timeliness of handling phone calls, prenatal and postpartum care, mental health care, well-care or preventive visits, inpatient utilization, drug utilization, and distribution of members by age, sex, and product lines.\n\nNew measures in HEDIS 2013 are “Asthma Medication Ratio,” “Diabetes Screening for People With Schizophrenia and Bipolar Disorder Who Are Using Antipsychotic Medications,” “Diabetes Monitoring for People With Diabetes and Schizophrenia,” “Cardiovascular Monitoring for People With Cardiovascular Disease and Schizophrenia,” and “Adherence to Antipsychotic Medications for Individuals With Schizophrenia.”\n\nHEDIS data are collected through surveys, medical charts and insurance claims for hospitalizations, medical office visits and procedures. Survey measures must be conducted by an NCQA-approved external survey organization. Clinical measures use the administrative or hybrid data collection methodology, as specified by NCQA. Administrative data are electronic records of services, including insurance claims and registration systems from hospitals, clinics, medical offices, pharmacies and labs. For example, a measure titled Childhood Immunization Status requires health plans to identify 2 year old children who have been enrolled for at least a year. The plans report the percentage of children who received specified immunizations. Plans may collect data for this measure by reviewing insurance claims or automated immunization records, but this method will not include immunizations received at community clinics that do not submit insurance claims. For this measure, plans are allowed to select a random sample of the population and supplement claims data with data from medical records. By doing so, plans may identify additional immunizations and report more favorable and accurate rates. However, the hybrid method is more costly, time-consuming and requires nurses or medical record reviewers who are authorized to review confidential medical records.\n\nHEDIS results must be audited by an NCQA-approved auditing firm for public reporting. NCQA has an on-line reporting tool called Quality Compass that is available for a fee of several thousand dollars. It provides detailed data on all measures and is intended for employers, consultants and insurance brokers who purchase health insurance for groups. NCQA's web site includes a summary of HEDIS results by health plan. NCQA also collaborates annually with U.S. News & World Report to rank HMOs using an index that combines many HEDIS measures and accreditation status. The \"Best Health Plans\" list is published in the magazine in October and is available on the magazine's web site. Other local business organizations, governmental agencies and media report HEDIS results, usually when they are released in the fall.\n\nProponents cite the following advantages of HEDIS measures:\n\n\nHEDIS was described in 1995 as \"very controversial\". Criticisms of HEDIS measures have included:\n\n\nhttp://www.ncqa.org/Portals/0/HEDISQM/HEDIS2013/HEDIS_2013_October_Update_Final_10.1.12.pdf\n\n"}
{"id": "43144300", "url": "https://en.wikipedia.org/wiki?curid=43144300", "title": "Healthcare in Egypt", "text": "Healthcare in Egypt\n\nHealthcare in Egypt consists of both a public and private sector. Public health coverage is offered through Ministry of Health, which operates a series of medical facilities providing free health services. There are two main private insurers. The Health Insurance Organization covers employed persons, students, and widows through premiums deducted from employee salaries and employer payrolls. It operates its own network of medical facilities and at times contracts with private healthcare providers. The Curative Care Organization operates in specific govern orates, and contracts with other entities for provision of care. There are also private insurance options, and a network of private healthcare providers and medical facilities. Many mosques also operate their own clinics, especially in the large cities. Many churches offer subsidized or free clinics.\n\nMedical care offered by the public health insurance system is generally of poor quality. Government hospitals are known to be rife with negligence and generally provide minimal care. Only about 6% of Egyptians covered by the Health Insurance Organization actually utilize its services due to dissatisfaction with the level of services it funds. In 2008/2009, 72% of health expenditure in Egypt was paid out of pocket by people seeking treatment.\n\nEgypt is currently working on an overhaul of its public healthcare system to improve its quality. A draft law was approved in October 2017 which would increase citizens’ contribution from the present 4% of wages to 5% , with 4% to be paid by employers and 1% by employees. The Egyptian Medical Syndicate is concerned that this may lead to privatisation of public hospitals, especially those that might not meet the quality standards set by the new law. They say only 20% of the 660 government hospitals are committed to safety and infection control standards.\n\nDoctors' pay in Egypt is low. Monthly salaries range from 1,218 Egyptian pounds ($69) to 6,365 Egyptian pounds ($361) per month. Of the Egyptian Medical Syndicate's members only about a third are working in the country. \n\n"}
{"id": "9541887", "url": "https://en.wikipedia.org/wiki?curid=9541887", "title": "Healthcare proxy", "text": "Healthcare proxy\n\nIn the field of medicine, a healthcare proxy (commonly referred to as HCP) is a document (legal instrument) with which a patient (primary individual) appoints an agent to legally make healthcare decisions on behalf of the patient, when the patient is incapable of making and executing the healthcare decisions stipulated in the proxy. Once the health care proxy is effective, the primary individual continues making healthcare decisions as long as the primary individual is legally competent to decide. Moreover, in legal-administrative functions, the healthcare proxy is a legal instrument akin to a \"springing\" health care power of attorney. The proxy must declare the healthcare agent who will gain durable power attorney. This document also notifies of the authority given from the principal to the agent and states the limitations of this authority. \n\nThose over the age of 18 are allowed to have a healthcare proxy, and these documents are useful in situations that render a person unable to communicate their wishes such as being in a persistent vegetative state, having a form of dementia of an illness that takes away one's ability to effectively communicate, or being under anesthesia when a decision needs to be made. Healthcare proxies are one of three ways that surrogate decision makers are enacted, the other two being court orders and laws for the automatic succession of decision makers. In contrast to a living will, healthcare proxies do not set out possible outcomes with predetermined reactions, rather they appoint someone to carry out the wishes of an individual. \n\nThe methods of healthcare planning and tools of advanced preparation have changed dramatically over the years. The concept of durable power of attorney arose in Virginia in 1954 for the purpose of setting property matters. This allowed for a continued existence of power of attorney following the original person losing capacity to carry out the necessary actions. This concept evolved over the years and in 1983, the President's Commission for the Study of Ethical Problems in Medicine and Biomedical and Behavioral Research addressed this idea as one of great potential in the healthcare industry. This commission also stated the possibility of abuse as a noted concern going forward. In response to this commission, there was an evolution of this concept throughout the 1980's and the 1990's that eventually led to all states in America having a healthcare power of attorney statute by 1997. \n\nSome jurisdictions place limitations on the persons who can act as agents. (Some forbid the appointment of treating physicians as the healthcare proxy.) In any event the agent should be someone close to and trusted by the primary individual. According to the state of Massachusetts, no person who is an employee or administrator of a facility can be an agent unless it is for someone who is of familial relation to them. In any event the agent is recommended to be someone close to and trusted by the primary individual. In the absence of a power of attorney, a legal guardian must be appointed.\n\nHealthcare proxies are permitted in forty-nine states as well as the District of Columbia.\nHealthcare forms may differ in structure from state to state and pre-made forms are not compulsory as long as certain guidelines are met. The common guidelines include:\n\n\nThe agent is empowered when a qualified physician determines that the primary individual is unable to make decisions regarding healthcare. The agent may be granted the power to remove or sustain feeding tubes from the primary individual if these tubes are the only things that are keeping the primary individual alive. The agent's decision should draw upon knowledge of the patient's desire in this matter. If the primary individual made his or her wishes clear on the proxy form, then they must be followed despite any possible objections from the agent.\n\nAn individual may have identified end-of-life decisions in a separate document, such as a living will or advanced health care directive, in which case it is necessary to examine all of the documents to determine if any supersede the agent's authority as granted in the healthcare proxy, based on the language of the interrelated documents and governing state law. An agent will not be legally or financially liable for decisions made on behalf of the primary individual as long as they follow the terms of the healthcare proxy.\n\nThere are limited legal foundation to determine the ability of someone to appoint a healthcare proxy. Although physicians are allowed to deliver life-saving treatment in emergent situations, in non-emergencies, the it is determined if the patient has the ability to then appoint a healthcare proxy. It is possible for a patient lacking the ability to make healthcare decisions, to still have the capacity to appoint an agent and have a proxy.\n\nIn England and Wales, an independent mental health capacity advocate may be appointed under the Mental Capacity Act 2005; the provisions made in the same Act for a lasting power of attorney may also provide a satisfactory basis for providing care via an attorney, who does not require to be professionally qualified. Different arrangements apply elsewhere in the UK.\n\n\n\n"}
{"id": "3256943", "url": "https://en.wikipedia.org/wiki?curid=3256943", "title": "High-altitude cerebral edema", "text": "High-altitude cerebral edema\n\nHigh-altitude cerebral edema (HACE) is a medical condition in which the brain swells with fluid because of the physiological effects of traveling to a high altitude. It generally appears in patients who have acute mountain sickness and involves disorientation, lethargy, and nausea among other symptoms. It occurs when the body fails to acclimatize while ascending to a high altitude.\n\nIt appears to be a vasogenic edema (fluid penetration of the blood–brain barrier), although cytotoxic edema (cellular retention of fluids) may play a role as well. Individuals with the condition must immediately descend to a lower altitude or coma and death can occur. Patients are usually given supplemental oxygen and dexamethasone as well.\n\nHACE can be prevented by ascending to heights slowly to allow the body more time to acclimatize. Acetazolamide also helps prevent the condition. Untreated patients usually die within 48 hours. Those who receive treatment may take weeks to fully recover. It is a rare condition, occurring in less than one percent of people who ascend to . First described in 1913, little was known about the cause of the condition until MRI studies were performed in the 1990s.\n\nEarly symptoms of high-altitude cerebral edema (HACE) generally correspond with those of moderate to severe acute mountain sickness (AMS). Initial symptoms of HACE commonly include confusion, loss of consciousness, fever, ataxia, photophobia, rapid heart beat, lassitude, and an altered mental state. Sufferers generally attempt to cease physical activities, regardless of their necessity for survival. Severe headaches develop and sufferers lose the ability to sit up. Retinal venous dilation occurs in 59% of people with HACE. Rarer symptoms include brisk deep tendon reflexes, retinal hemorrhages, blurred vision, extension plantar reflexes, and ocular paralysis. Cranial nerve palsies occur in some unusual cases.\n\nIn the bestselling 1996 non-fiction book \"Into Thin Air: A Personal Account of the Mt. Everest Disaster\", Jon Krakauer describes the effects of HACE upon Dale Kruse, a forty-four-year-old dentist and one of the members of Scott Fischer's team: <br>\n‘Kruse was having an incredibly difficult time simply trying to dress himself. He put his climbing harness on inside out, threaded it through the fly of his wind suit, and failed to fasten the buckle; fortunately, Fisher and Neal Beidleman noticed the screwup before Kruse started to descend. \"If he'd tried to rappel down the ropes like that,\" says Beidleman, \"he would have immediately popped out of his harness and fallen to the bottom of the Lhotse Face.\"\n‘\"It was like I was very drunk,\" Kruse recollects. \"I couldn't walk without stumbling, and completely lost the ability to think or speak. It was a really strange feeling. I'd have some word in my mind, but I couldn't figure out how to bring it to my lips. So Scott and Neal had to get me dressed and make sure my harness was on correctly, then Scott lowered me down the fixed ropes.\" By the time Kruse arrived in Base Camp, he says, \"it was still another three or four days before I could walk from my tent to the mess tent without stumbling all over the place.\"’\n\nPatients with HACE have an elevated white blood cell count, but otherwise their blood count and biochemistry are normal. If a lumbar puncture is performed, it will show normal cerebral spinal fluid and cell counts but an increase in pressure. In one study, CT scans of patients with HACE exhibited ventricle compression and low density in the cerebellum. Only a few autopsies have been performed on fatal cases of HACE; they showed swollen gyri, spongiosis of white matter, and compressed sulci. There was some variation between individuals, and the results may not be typical of HACE deaths.\n\nMost people who travel to high altitudes acclimatize. Acclimatization precludes the development of HACE by maintaining adequate levels of cerebral oxygen. The primary cause of HACE is hypoxia (oxygen deprivation). This occurs after the body is exposed to a low-oxygen environment and before it acclimatizes. The rate of change from a normal oxygen environment and how little oxygen is in the new environment can be used to predict the chance of developing HACE. Prolonged exertion in low oxygen also causes serious hypocapnia, lower carbon dioxide in the bloodstream, which may play a role in HACE. These factors cause the brain to swell with fluid, resulting in severe impairment. If the swelling is untreated, it causes death by brain herniation.\n\nThe brain swelling is likely a result of vasogenic edema, the penetration of the blood–brain barrier by fluids. This process has been observed in MRI studies. Hypoxia increases extracellular fluid, which passes through the vasogenic endothelium in the brain. The leaking may be caused by increased pressure, or it may be caused by inflammation that makes the endothelium vulnerable to leaking. An MRI study found microhemorrhages in the corpus callosum of HACE patients, and hypoxia may also cause microvascular permeability. It has been hypothesized that vascular endothelial growth factor may cause the vascular permeability at the root of HACE. MRI scans of patients with HACE showed increased T2 in the corpus callosum, although grey matter was unchanged. This demonstrated that the blood-brain barrier was broken by cerebral blood vessels, thus interfering with white matter metabolism. Another study looked at the brains of HACE sufferers several months after their recovery; it showed hemosiderin deposits in the corpus callosum, evidence of vascular permeability.\n\nWhile there is strong evidence that vasogenic edema plays a major role in HACE, cytotoxic edema, cellular retention of fluids, may contribute as well. Cytotoxic edema may be caused by the failure of cellular ion pumps, which results from hypoxia. Then intracellular sodium and osmolarity increase, and there is an influx of water that causes cellular swelling. After the failure of the ATPase pumps, free radicals form and cause damage that complicates the edema. Evidence against cytotoxic edema includes the high levels of hypoxemia (low bloodstream oxygen) needed to cause it.\n\nIt is not known why some are more vulnerable to HACE than others. One theory is that variations in brain size play a role, but the increase in brain volume from edema does not likely cause cranial vault impingement. The presence of large sulci indicate the condition may be influenced by the brain tightly fitting. Elevated intracranial pressure is generally accepted to be a late effect of HACE. High central venous pressure may also occur late in the condition's progression.\n\nOne study demonstrated that normal autorelation of cerebral blood flow does not cause HACE. What role the sympathetic nervous system plays in determining who gets HACE is unclear, but it may have an effect.\n\nAnother theory about the cause of HACE is that hypoxia may induce nitrous oxide synthase. Vasodilation is caused by the release of nitric oxide and adenosine. This in turn can increase vascular permeability and causes edema. This may combine with low levels of cytokines to cause HACE.\n\nGenerally, high-altitude pulmonary edema (HAPE) or AMS precede HACE. In patients with AMS, the onset of HACE is usually indicated by vomiting, headache that does not respond to non-steroidal anti-inflammatory drugs, hallucinations, and stupor. In some situations, however, AMS progresses to HACE without these symptoms. HACE must be distinguished from conditions with similar symptoms, including stroke, intoxication, psychosis, diabetic symptoms, meningitis, or ingestion of toxic substances. It should be the first diagnosis ruled out when sickness occurs while ascending to a high altitude.\n\nHACE is generally preventable by ascending gradually with frequent rest days while climbing or trekking. Not ascending more than daily and not sleeping at a greater height than more than the previous night is recommended. The risk of developing HACE is diminished if acetazolamide or dexamethasone are administered. Generally, the use of acetazolamide is preferred, but dexamethasone can be used for prevention if there are side effects or contraindications. Some individuals are more susceptible to HACE than others, and physical fitness is not preventive. Age and sex do not by themselves affect vulnerability to HACE.\n\nPatients with HACE should be brought to lower altitudes and provided supplemental oxygen, and rapid descent is sometimes needed to prevent mortality. Early recognition is important because as the condition progresses patients are unable to descend without assistance. Dexamethasone should also be administered, although it fails to ameliorate some symptoms that can be cured by descending to a lower altitude. It can also mask symptoms, and they sometimes resume upon discontinuation. Dexamethasone's prevention of angiogenesis may explain why it treats HACE well. Three studies that examined how mice and rat brains react to hypoxia gave some credence to this idea.\n\nIf available, supplemental oxygen can be used as an adjunctive therapy, or when descent is not possible. FiO2 should be titrated to maintain arterial oxygen saturation of greater than 90%, bearing in mind that oxygen supply is often limited in high altitude clinics/environments.\n\nIn addition to oxygen therapy, a portable hyperbaric chamber (Gamow bag) can by used as a temporary measure in the treatment of HACE. These devices simulate a decrease in altitude of up to 7000 ft, but they are resource intensive and symptoms will often return after discontinuation of the device. Portable hyperbaric chambers should not be used in place of descent or evacuation to definitive care.\n\nDiuretics may be helpful, but pose risks outside of a hospital environment. Sildenafil and tadalafil may help HACE, but there is little evidence of their efficacy. Theophylline is also theorized to help the condition.\n\nAlthough AMS is not life-threatening, HACE is usually fatal within 24 hours if untreated. Without treatment, the patient will enter a coma and then die. In some cases, patients have died within a few hours, and a few have survived for two days. Descriptions of fatal cases often involve climbers who continue ascending while suffering from the condition's symptoms.\n\nRecovery varies between days and weeks, but most recover in a few days. After the condition is successfully treated, it is possible for climbers to reascend. Dexamethesone should be discontinued, but continual acetazolamide is recommended. In one study, it took patients between one week and one month to display a normal CT scan after suffering from HACE.\n\nHACE occurs in 0.5% to 1% of people who climb or trek between and . In some unusual cases, up to 30% of members of expeditions have suffered from the condition. The condition is seldom seen below , but in some rare cases it has developed as low as . The condition generally does not occur until an individual has spent 48 hours at an altitude of .\n\nHACE was first described by a medical officer stationed in Chile in 1913, but few took note of it. Later, access to air travel made the condition more common because it allowed more people access to high mountains, such as those in the Himalayas. One early description of HACE may have been published in 1969 after a group of Indian soldiers made a rapid ascent to almost . It is not definitely established whether they had HACE or acute decompression sickness. MRI has been used to study the effects of high altitude on the brain, providing the best evidence about the condition. A 1998 MRI study of nine climbers with HACE clearly demonstrated vasogenic edema.\n\nData about HACE are lacking because it generally occurs in remote areas, far from hospitals and is generally rare. It is uncommon for doctors to be able to study victims within six days of the condition's development. Animal models of HACE have not been developed. Several genes are being examined for the role they may play in the development of the condition.\n\nIncreased education and helicopter capabilities have combined to cut the number of deaths from the condition. Symptoms of HACE have been reported in many cases of deaths while descending Mount Everest, although HACE may not be the only problem they suffered. HACE also posed a threat to workers on the Qinghai–Tibet Railway.\n\n"}
{"id": "31974016", "url": "https://en.wikipedia.org/wiki?curid=31974016", "title": "History of USDA nutrition guides", "text": "History of USDA nutrition guides\n\nThe history of USDA nutrition guides includes over 100 years of American nutrition advice. The guides have been updated over time, to adopt new scientific findings and new public health marketing techniques. The current guidelines are the Dietary Guidelines for Americans 2015 - 2020. Over time they have described from 4 to 11 food groups. Various guides have been criticized as not accurately representing scientific information about optimal nutrition, and as being overly influenced by the agricultural industries the USDA promotes.\n\nThe USDA's first nutrition guidelines were published in 1894 by Dr. Wilbur Olin Atwater as a farmers' bulletin. In Atwater's 1904 publication titled \"Principles of Nutrition and Nutritive Value of Food,\" he advocated variety, proportionality and moderation; measuring calories; and an efficient, affordable diet that focused on nutrient-rich foods and less fat, sugar and starch. This information preceded the discovery of individual vitamins beginning in 1910.\n\nA new guide in 1916, \"Food for Young Children\" by nutritionist Caroline Hunt, categorized foods into milk and meat; cereals; vegetables and fruits; fats and fatty foods; and sugars and sugary foods. \"How to Select Food\" in 1917 promoted these five food groups to adults, and the guidelines remained in place through the 1920s. In 1933, the USDA introduced food plans at four different cost levels in response to the Great Depression.\n\nIn 1941, the first Recommended Dietary Allowances were created, listing specific intakes for calories, protein, iron, calcium, and vitamins A, B, B B, C and D.\n\nIn 1943, during World War II, The USDA introduced a nutrition guide promoting the \"Basic 7\" food groups to help maintain nutritional standards under wartime food rationing. The Basic 7 food groups were:\n\n\nFrom 1956 until 1992 the United States Department of Agriculture recommended its \"Basic Four\" food groups. These food groups were:\n\n\n\"Other foods\" were said to round out meals and satisfy appetites. These included additional servings from the Basic Four, or foods such as butter, margarine, salad dressing and cooking oil, sauces, jellies and syrups.\n\nThe Basic Four guide was omnipresent in nutrition education in the United States. A notable example is the 1972 series Mulligan Stew, providing nutrition education for schoolchildren in reruns until 1981.\n\nThe introduction of the USDA's food guide pyramid in 1992 attempted to express the recommended servings of each food group, which previous guides did not do. 6 to 11 servings of bread, cereal, rice and pasta occupied the large base of the pyramid; followed by 3 to 5 servings of vegetables; then fruits (2 to 4); then milk, yogurt and cheese (2 to 3); followed by meat, poultry, fish, dry beans, eggs, and nuts (2 to 3); and finally fats, oils and sweets in the small apex (to be used sparingly). Inside each group were several images of representative foods, as well as symbols representing the fat and sugar contents of the foods.\n\nA modified food pyramid was proposed for adults aged over 70. This \"Modified Food Pyramid for 70+ Adults\" accounted for changing diets with age by emphasizing water consumption as well as nutrient-dense and high-fiber foods.\n\nThe first chart suggested to the USDA by nutritional experts in 1992 featured fruits and vegetables as the biggest group, not breads. This chart was overturned at the hand of special interests in the grain, meat, and dairy industries, all of which are heavily subsidized by the USDA.\n\n\"The 'Pyramid' emphasized eating more vegetables and fruits, less meat, salt, sugary foods, bad fat, and additive-rich factory foods. USDA censored that research-based version of the food guide and altered it to include more refined grains, meat, commercial snacks and fast foods, only releasing their revamped version 12 years after it was originally scheduled for release. \" \n\nIn 2005, the USDA updated its guide with MyPyramid, which replaced the hierarchical levels of the Food Guide Pyramid with colorful vertical wedges, often displayed without images of foods, creating a more abstract design. Stairs were added up the left side of the pyramid with an image of a climber to represent a push for exercise. The share of the pyramid allotted to grains now only narrowly edged out vegetables and milk, which were of equal proportions. Fruits were next in size, followed by a narrower wedge for protein and a small sliver for oils. An unmarked white tip represented discretionary calories for items such as candy, alcohol, or additional food from any other group.\n\nMyPlate is the current nutrition guide published by the United States Department of Agriculture, consisting of a diagram of a plate and glass divided into five food groups. It replaced the USDA's MyPyramid diagram on June 2, 2011, ending 19 years of food pyramid iconography. The guide will be displayed on food packaging and used in nutritional education in the United States.\n\nThe Center for Nutrition Policy and Promotion in the USDA and the United States Department of Health and Human Services jointly released a longer textual document called \"Dietary Guidelines for Americans 2015 - 2020\", to be updated in 2020. The first edition was published in 1980, and since 1985 has been updated every five years by the Dietary Guidelines Advisory Committee. Like the USDA Food Pyramid, these guidelines have been criticized as being overly influenced by the agriculture industry.\nThese criticisms of the Dietary Guidelines arose due to the omission of high-quality evidence that the Public Health Service decided to exclude. The phrasing of recommendations was extremely important and widely affected everyone who read it. The wording had to be changed constantly as there were protests due to comments such as “cut down on fatty meats”, which led to the U.S Department of Agriculture having to stop the publication of the USDA Food Book. Slight alterations of various dietary guidelines had to be made throughout the 1970s and 1980s in an attempt to calm down the protests emerged. As a compromise, the phrase was changed to “choose lean meat” but didn’t result in a better situation. In 2015 the committee factored in environmental sustainability for the first time in its recommendations. The committee's 2015 report found that a healthy diet should comprise higher plant based foods and lower animal based foods. It also found that a plant food based diet was better for the environment than one based on meat and dairy.\n\nIn 2013 and again in 2015, Edward Archer and colleagues published a series of research articles in PlosOne and Mayo Clinic Proceedings demonstrating that the dietary data used to develop the US Dietary Guidelines were physiologically implausible (i.e., incompatible with survival) and therefore these data were \"inadmissible\" as scientific evidence and should not be used to inform public policy.\n\nIn 2016, Nina Teicholz authored a critique of the US Dietary Guidelines in the British Medical Journal titled The scientific report guiding the US dietary guidelines: is it scientific? Teicholz suggested that \"the scientific committee advising the US government has not used standard methods for most of its analyses and instead relies heavily on systematic reviews from professional bodies such as the American Heart Association and the American College of Cardiology, which are heavily supported by food and drug companies.\"\n\n"}
{"id": "11186931", "url": "https://en.wikipedia.org/wiki?curid=11186931", "title": "Hydrocolpos", "text": "Hydrocolpos\n\nHydrocolpos is the distension of the vagina caused by accumulation of fluid due to congenital vaginal obstruction. The obstruction is often caused by an imperforate hymen or less commonly a transverse vaginal septum. The fluid consists of cervical and endometrial mucus or in rare instances urine accumulated through a vesicovaginal fistula proximal to the obstruction. In some cases, it is associated with Bardet-Biedl Syndrome. If it occurs in prepubertal girls, it may show up as abdominal swelling. It may be detected by using ultrasound. It may also present at birth as a distended lower abdomen and vagina. It also associated with vaginal atresia.\n"}
{"id": "55338077", "url": "https://en.wikipedia.org/wiki?curid=55338077", "title": "Incarceration and health", "text": "Incarceration and health\n\nThe relationship between incarceration and health, compared to research on other social effects of incarceration, has been a topic of research for a relatively short period of time. Most of the foundational research on this topic was conducted in the 25 years before 2015, and indicates that incarceration generally has negative effects on prisoners' mental health, but some positive effects on their physical health. In the United States, the negative health effects of incarceration contribute to racial disparities in health between white and black women.\n\nFormer prisoners have higher odds of hospitalization and death from cardiovascular disease, even after controlling for socioeconomic status and race.\n\nThe incarceration of juveniles often results in adverse mental health consequences, especially in adult facilities. Such incarceration is also related to worse health across the life course.\n"}
{"id": "20479854", "url": "https://en.wikipedia.org/wiki?curid=20479854", "title": "Jenny Is a Good Thing", "text": "Jenny Is a Good Thing\n\nJenny Is a Good Thing is a 1969 American short documentary film about children and poverty, directed by Joan Horvath. Produced by Project Head Start, it shows the importance of good nutrition for underprivileged nursery school children. The film was nominated for an Academy Award for Best Documentary Short.\n\n"}
{"id": "322276", "url": "https://en.wikipedia.org/wiki?curid=322276", "title": "John Boyd Orr", "text": "John Boyd Orr\n\nJohn Boyd Orr, 1st Baron Boyd Orr of Brechin Mearns, (23 September 1880 – 25 June 1971), styled Sir John Boyd Orr from 1935 to 1949, was a Scottish teacher, medical doctor, biologist and politician who was awarded the Nobel Peace Prize for his scientific research into nutrition and his work as the first Director-General of the United Nations Food and Agriculture Organization (FAO). He was the co-founder and the first President (1960–1971) of the World Academy of Art and Science (WAAS).\n\nJohn Boyd Orr was born at Kilmaurs, near Kilmarnock, East Ayrshire, Scotland, the middle child in a family of seven children. His father, Robert Clark Orr, was a quarry owner, and a man of deep religious convictions, being a member of the Free Church of Scotland. His mother, Annie Boyd, was the daughter of another quarry master, wealthier than Robert Orr, and grandmaster of a Freemason's Lodge.\n\nThe family home was well supplied with books, and his father was widely read in political, sociological and metaphysical subjects, as well as religion. As he grew older, John would regularly discuss these subjects with his father, brothers, and visiting friends. There was also family worship each evening.\n\nWhen John was five years old, the family suffered a setback when a ship owned by Robert Orr was lost at sea. They had to sell their home in Kilmaurs, and moved to West Kilbride, a village on the North Ayrshire coast. According to Kay, the new house and environment were a great improvement on Kilmaurs, despite the family's reduced means. The major part of his upbringing took place in and around West Kilbride. He attended the village school until he was thirteen. Religion was then an important part of junior education in Scotland, and the school gave him a good knowledge of the Bible, which stayed with him for the rest of his life.\n\nAt the age of thirteen, John won a bursary to Kilmarnock Academy, a significant achievement as such bursaries were then rare. The new school was some from his home in West Kilbride, but his father owned a quarry about two miles (3 km) from the Academy, and John was provided with accommodation nearby. His family cut short his education at the Academy because he was spending too much time in the company of the quarry workers (where he picked up a \"wonderful vocabulary of swear words\"), and after four months he returned to the village school in West Kilbride where he continued his education until he was seventeen under the inspirational tutelage of Headmaster John G. Lyons. There he became a pupil teacher at a salary of £10 for the first year, and £20 for the second. This was a particularly demanding time for the young Boyd Orr, as in addition to his teaching duties, and studying at home for his university and teacher-training qualifications, he also had to work every day in his father's business.\n\nAfter four years as a pupil teacher, at the age of 19 he won a Queen's Scholarship to study at a teacher training college in Glasgow, plus a bursary which paid for his lodgings there. At the same time he entered a three-year degree course in theology at the University, for which the fees were also covered.\n\nAs an undergraduate in Glasgow, he explored the interior of the city, usually at weekends. He was shocked by what he found in the poverty-stricken slums and tenements, which then made up a large part of the city. Rickets was obvious among the children, malnutrition (in some cases, associated with drunkenness) was shown by many of the adults, and many of the aged were destitute. In his first teaching job after graduating M.A. in 1902, he was posted to a school in the slums. His first class was overcrowded, and the children ill-fed or actually hungry, inadequately clothed, visibly lousy and physically wretched. He resigned after a few days, realising that he could not teach children in such a condition, and that there was nothing he could do to relieve their misery.\n\nAfter working for a few months in his father's business, he taught for three years at Kyleshill School in Saltcoats, also a poor area, but less squalid than the slums of Glasgow.\n\nBoyd Orr needed to augment his teacher's salary, and decided to do so by instructing an evening class in book-keeping and accountancy. After intensive study he passed the necessary examinations, and duly instructed his class. The knowledge and skills he learned by studying for, and teaching, this class were to prove very useful in his later career.\n\nHowever his heart was not in teaching, and after fulfilling his teaching obligations under the terms of his Queen's Scholarship, he returned to the University to study biology, a subject he had always been interested in since childhood. As a precaution, he entered simultaneously for a degree in medicine.\n\nHe found the university to be a very stimulating environment. Diarmid Noel Paton (son of the artist Joseph Noel Paton) was Regius Professor of Physiology, and Edward Provan Cathcart head of Physiological Chemistry, both men of outstanding scientific ability. He was impressed by Samuel Gemmill, Professor of Clinical Medicine, a philosopher whose deep thinking on social affairs also influenced Boyd Orr's approach to such questions.\n\nHalf-way through his medical studies, his savings ran out. Reluctant to ask his family for support, he bought a block of tenanted flats on mortgage, with the help of a bank overdraft, and used the rents to pay for the rest of his studies. On graduating, he sold the property for a small profit.\n\nHe graduated B.Sc. in 1910, and M.B. Ch.B. in 1912, at the age of 32, placing sixth in a year of 200 students. Two years later, in 1914, he graduated M.D. with honours, receiving the Bellahouston Gold Medal for the most distinguished thesis of the year.\n\nOn leaving the university, he took a position as a ship's surgeon on a ship trading between Scotland and West Africa, choosing this job because it offered the possibility of paying off his bank overdraft faster than any other. He resigned after four months, when he had repaid the debt. He then tried general practice, working as a \"locum\" in the practice of his family doctor in Saltcoats, and was offered a partnership there. Realising that a career in medicine was not for him, he instead accepted the offer of a two-year Carnegie research scholarship, to work in E.P. Cathcart's laboratory. The work he began there covered malnutrition, protein and creatine metabolism, the effect of water intake on nitrogenous metabolism in humans, and the energy expenditure of military recruits in training.\n\nOn 1 April 1914, Boyd Orr took charge of a new research institute in Aberdeen, a project of a joint committee for research into animal nutrition of the North of Scotland College of Agriculture and Aberdeen University. He had been offered the post on the recommendation of E.P. Cathcart, who had originally been offered the job, but had turned it down in favour of a chair in physiology in London.\n\nThe joint committee had allocated a budget of £5,000 for capital expenditure and £1,500 for annual running costs. Boyd Orr recognised immediately that these sums were inadequate. Using his experience in his father's business of drawing up plans and estimating costs, he submitted a budget of £50,000 for capital expenditure and £5,000 for annual running costs. Meanwhile, with the £5,000 he had already been allocated he specified a building, not of wood as had been envisaged by the committee, but of granite and designed so that it could serve as a wing of his proposed £50,000 Institute. He accepted the lowest tender of £5,030, and told the contractors to begin work immediately. The committee were not pleased, but had to accept the \"fait accompli\". When war broke out the contractors were told to finish the walls and roof, but to do no more for the time being.\n\nOn the outbreak of the First World War he was given leave to join the British Army, and asked his former colleague Cathcart to help him obtain a medical commission in an infantry unit overseas. Cathcart thought he would be more useful at home, and his first commission was in a special civilian section of the R.A.M.C. dealing with sanitation. Several divisions of non-conscripted recruits were in training in emergency camps at home, some of them in very poor sanitary conditions. Boyd Orr was able to push through schemes for improvement in hygiene, preventing much sickness.\n\nAfter 18 months he was posted as Medical Officer to an infantry unit, the 1st Sherwood Foresters. He spent much of his time in shell holes, patching up the many wounded. His courage under fire and devotion to duty were recognised by the award of a Military Cross after the Battle of the Somme, and of the Distinguished Service Order after Passchendaele. He also made arrangements for the battalion's diet to be supplemented by vegetables collected from local deserted gardens and fields. As a result, unlike other units, he did not need to send any of the men in his medical charge to hospital. He also prevented his men getting trench foot by personally ensuring they were fitted with boots a size larger than usual.\n\nWorried that he was losing touch with medical and nutritional advances, he asked to be transferred to the navy, where he thought he would have more time available for reading and research. The army was reluctant to let him go, but agreed, since he was still a civilian surgeon. He spent a busy three months in the naval hospital at Chatham, studying hard while practicing medicine in the wards, before being posted to HMS \"Furious\". On board ship his medical duties were light, enabling him to do a great deal of reading. He was later recalled to work studying food requirements of the army.\n\nWhen Boyd Orr returned to Aberdeen in early 1919, his plan for a larger Institute had still not been accepted. Indeed, even his plans for the annual maintenance grant had to be approved by the Professor of Agriculture in Cambridge, Thomas Barlow Wood. Despite gaining the latter's support, his expansion plans were at first rebuffed, although he succeeded in having the annual grant increased to £4,000. In 1920 he was introduced to John Quiller Rowett, a businessman who seemed to have qualms of conscience over the large profits he had made during the war. Shortly afterwards, the government agreed to finance half the cost of Boyd Orr's plan, provided he could raise the other half elsewhere. Rowett agreed to provide £10,000 for the first year, £10,000 for the second year, and gave an additional £2,000 for the purchase of a farm, provided that, \"if any work done at the Institute on animal nutrition was found to have a bearing on human nutrition, the Institute would be allowed to follow up this work\", a condition the Treasury was willing to accept. By September 1922 the buildings were nearly completed, and the renamed Rowett Research Institute was opened shortly thereafter by Queen Mary.\n\nBoyd Orr proved to be an effective fund-raiser from both government and private sources, expanding the experimental farm to around , building a well-endowed library, and expanding the buildings. He also built a centre for accommodating students and scientists attracted by the Institute's growing reputation, a reputation enhanced by Boyd Orr's many publications. His research output suffered from the time and energy he had to devote to fund-raising, and in later life he said, \"I still look with bitter resentment at having to spend half my time in the humiliating job of hunting for money for the Institute.\"\n\nThrough the 1920s, his own research was devoted mainly to animal nutrition, his focus changed to human nutrition both as a researcher and an active lobbyist and propagandist for improving people's diets. In 1927, he proved the value of milk being supplied to school children, which led to free school milk provision in the UK. His 1936 report \"Food, Health and Income\" showed that at least one third of the UK population were so poor that they could not afford to buy sufficient food to provide a healthy diet and revealed that there was a link between low-income, malnutrition and under-achievement in schools.\n\nFrom 1929 to 1944, Boyd Orr was Consultant Director to the Imperial Bureau of Animal Nutrition, later the Commonwealth Bureau of Nutrition (part of the Commonwealth Agricultural Bureaux), which was based at the Rowett Research Institute. During the Second World War he was a member of Churchill's Scientific Committee on Food Policy and helped to formulate food rationing\n\nIn October 1945, Orr was elected Rector of the University of Glasgow after standing as an Independent Progressive candidate. He was elected as an independent Member of Parliament (MP) for the Combined Scottish Universities in a by-election in April 1945, and kept his seat at the general election shortly after. He resigned in 1946.\n\nAfter the Second World War, Boyd Orr resigned from the Rowett Institute, and took several posts, most notably as Director-General of the United Nations' new Food and Agriculture Organization (FAO). Although his tenure in this position was short (1945–1948), he worked not only to alleviate the immediate postwar food shortage through the International Emergency Food Committee (IEFC) but also to propose comprehensive plans for improving food production and its equitable distribution—his proposal to create a World Food Board. Although the board failed to get the support of Britain and the US, Boyd Orr laid a firm foundation for the new U.N. specialized agency.\n\nHe then resigned from the FAO and became director of a number of companies and proved a canny investor in the stock market, making a considerable personal fortune. When he received the Nobel Peace Prize in 1949, he donated the entire financial award to organizations devoted to world peace and a united world government.\nHe was elevated to the peerage in 1949 as Baron Boyd Orr, of Brechin Mearns in the County of Angus.\n\nIn 1960 Boyd Orr was elected the first president of the World Academy of Art and Science, which was set up by eminent scientists of the day concerned about the potential misuse of scientific discoveries, most especially nuclear weapons.\n\nThe University of Glasgow has a Boyd Orr Building and the Boyd Orr Centre for Population and Ecosystem Health named after him, and the University's Hunterian Museum holds his Nobel medal. There is a street named after Boyd Orr in his home town of Kilmaurs in Ayrshire, as well as in Brechin, Angus, Penicuik, Midlothian, and in Saltcoats, Ayrshire.\n\n\n\n"}
{"id": "55862417", "url": "https://en.wikipedia.org/wiki?curid=55862417", "title": "Kraus–Weber test", "text": "Kraus–Weber test\n\nThe Kraus–Weber test (or K–W test) is a fitness test devised by Hans Kraus and Sonja Weber of New York Presbyterian Hospital.\n\nThe Kraus–Weber test involved six simple movements and took 90 seconds to administer. Some early studies using this test were performed by Bonnie Prudden. In 1940s and 1950s, she applied this test to students in her conditioning classes. To her surprise the new students failed the test at 58% while the students who had been in the program failed at only 8%. Over a period of seven years, Prudden and her volunteers tested 4,458 children between the ages of 6 and 16 in the United States. The failure rate was 56.6%. While climbing in Europe, Prudden and Kraus arranged to test children in Europe. In Italy, Austria and Switzerland, the children tested exhibited an eight percent failure rate.\n\nIn 1952, Prudden and Kraus began writing papers for medical and physical education journals concerning their findings on \"Hypokinetic Disease: Role of Inactivity in Production of Disease\" and various media outlets began to pick up the story.\n\nIn 1955, armed with statistics and a personal invitation to the Eisenhower White House, Bonnie Prudden presented her findings on the fitness level of American public school children compared to that of their peers in Europe. This became known as \"The Report that Shocked the President\" or the \"Shape of the Nation\".\n\nThe six exercises involved in the test are:\n"}
{"id": "17747", "url": "https://en.wikipedia.org/wiki?curid=17747", "title": "Lead", "text": "Lead\n\nLead is a chemical element with symbol Pb (from the Latin \"plumbum\") and atomic number 82. It is a heavy metal that is denser than most common materials. Lead is soft and malleable, and has a relatively low melting point. When freshly cut, lead is silvery with a hint of blue; it tarnishes to a dull gray color when exposed to air. Lead has the highest atomic number of any stable element and three of its isotopes each conclude a major decay chain of heavier elements.\n\nLead is a relatively unreactive post-transition metal. Its weak metallic character is illustrated by its amphoteric nature; lead and lead oxides react with acids and bases, and it tends to form covalent bonds. Compounds of lead are usually found in the +2 oxidation state rather than the +4 state common with lighter members of the carbon group. Exceptions are mostly limited to organolead compounds. Like the lighter members of the group, lead tends to bond with itself; it can form chains, rings and polyhedral structures.\n\nLead is easily extracted from its ores; prehistoric people in Western Asia knew of it. Galena, a principal ore of lead, often bears silver, interest in which helped initiate widespread extraction and use of lead in ancient Rome. Lead production declined after the fall of Rome and did not reach comparable levels until the Industrial Revolution. In 2014, annual global production of lead was about ten million tonnes, over half of which was from recycling. Lead's high density, low melting point, ductility and relative inertness to oxidation make it useful. These properties, combined with its relative abundance and low cost, resulted in its extensive use in construction, plumbing, batteries, bullets and shot, weights, solders, pewters, fusible alloys, white paints, leaded gasoline, and radiation shielding.\n\nIn the late 19th century, lead's toxicity was recognized, and its use has since been phased out of many applications. Lead is a toxin that accumulates in soft tissues and bones, it acts as a neurotoxin damaging the nervous system and interfering with the function of biological enzymes. It is particularly problematic in children: even if blood levels are promptly normalized with treatment, neurological disorders, such as brain damage and behavioral problems, may result.\n\nA lead atom has 82 electrons, arranged in an electron configuration of [Xe]4f5d6s6p. The sum of lead's first and second ionization energies—the total energy required to remove the two 6p electrons—is close to that of tin, lead's upper neighbor in the carbon group. This is unusual; ionization energies generally fall going down a group, as an element's outer electrons become more distant from the nucleus, and more shielded by smaller orbitals. The similarity of ionization energies is caused by the lanthanide contraction—the decrease in element radii from lanthanum (atomic number 57) to lutetium (71), and the relatively small radii of the elements from hafnium (72) onwards. This is due to poor shielding of the nucleus by the lanthanide 4f electrons. The sum of the first four ionization energies of lead exceeds that of tin, contrary to what periodic trends would predict. Relativistic effects, which become significant in heavier atoms, contribute to this behavior. One such effect is the inert pair effect: the 6s electrons of lead become reluctant to participate in bonding, making the distance between nearest atoms in crystalline lead unusually long.\n\nLead's lighter carbon group congeners form stable or metastable allotropes with the tetrahedrally coordinated and covalently bonded diamond cubic structure. The energy levels of their outer s- and p-orbitals are close enough to allow mixing into four hybrid sp orbitals. In lead, the inert pair effect increases the separation between its s- and p-orbitals, and the gap cannot be overcome by the energy that would be released by extra bonds following hybridization. Rather than having a diamond cubic structure, lead forms metallic bonds in which only the p-electrons are delocalized and shared between the Pb ions. Lead consequently has a face-centered cubic structure like the similarly sized divalent metals calcium and strontium.\n\nPure lead has a bright, silvery appearance with a hint of blue. It tarnishes on contact with moist air, and takes on a dull appearance, the hue of which depends on the prevailing conditions. Characteristic properties of lead include high density, malleability, ductility, and high resistance to corrosion due to passivation.\nLead's close-packed face-centered cubic structure and high atomic weight result in a density of 11.34 g/cm, which is greater than that of common metals such as iron (7.87 g/cm), copper (8.93 g/cm), and zinc (7.14 g/cm). This density is the origin of the idiom \"to go over like a lead balloon\". Some rarer metals are denser: tungsten and gold are both at 19.3 g/cm, and osmium—the densest metal known—has a density of 22.59 g/cm, almost twice that of lead.\n\nLead is a very soft metal with a Mohs hardness of 1.5; it can be scratched with a fingernail. It is quite malleable and somewhat ductile. The bulk modulus of lead—a measure of its ease of compressibility—is 45.8 GPa. In comparison, that of aluminium is 75.2 GPa; copper 137.8 GPa; and mild steel 160–169 GPa. Lead's tensile strength, at 12–17 MPa, is low (that of aluminium is 6 times higher, copper 10 times, and mild steel 15 times higher); it can be strengthened by adding small amounts of copper or antimony.\n\nThe melting point of lead—at 327.5 °C (621.5 °F)—is very low compared to most metals. Its boiling point of 1749 °C (3180 °F) is the lowest among the carbon group elements. The electrical resistivity of lead at 20 °C is 192 nanoohm-meters, almost an order of magnitude higher than those of other industrial metals (copper at 15.43 nΩ·m; gold 20.51 nΩ·m; and aluminium at 24.15 nΩ·m). Lead is a superconductor at temperatures lower than 7.19 K; this is the highest critical temperature of all type-I superconductors and the third highest of the elemental superconductors.\n\nNatural lead consists of four stable isotopes with mass numbers of 204, 206, 207, and 208, and traces of five short-lived radioisotopes. The high number of isotopes is consistent with lead's atomic number being even. Lead has a magic number of protons (82), for which the nuclear shell model accurately predicts an especially stable nucleus. Lead-208 has 126 neutrons, another magic number, which may explain why lead-208 is extraordinarily stable.\n\nWith its high atomic number, lead is the heaviest element whose natural isotopes are regarded as stable; lead-208 is the heaviest stable nucleus. (This distinction formerly fell to bismuth, with an atomic number of 83, until its only primordial isotope, bismuth-209, was found in 2003 to decay very slowly.) The four stable isotopes of lead could theoretically undergo alpha decay to isotopes of mercury with a release of energy, but this has not been observed for any of them; their predicted half-lives range from 10 to 10 years (at least 10 times the current age of the universe).\n\nThree of the stable isotopes are found in three of the four major decay chains: lead-206, lead-207, and lead-208 are the final decay products of uranium-238, uranium-235, and thorium-232, respectively. These decay chains are called the uranium chain, the actinium chain, and the thorium chain. Their isotopic concentrations in a natural rock sample depends greatly on the presence of these three parent uranium and thorium isotopes. For example, the relative abundance of lead-208 can range from 52% in normal samples to 90% in thorium ores; for this reason, the standard atomic weight of lead is given to only one decimal place. As time passes, the ratio of lead-206 and lead-207 to lead-204 increases, since the former two are supplemented by radioactive decay of heavier elements while the latter is not; this allows for lead–lead dating. As uranium decays into lead, their relative amounts change; this is the basis for uranium–lead dating. Lead-207 exhibits nuclear magnetic resonance, a property that has been used to study its compounds in solution and solid state, including in human body.\nApart from the stable isotopes, which make up almost all lead that exists naturally, there are trace quantities of a few radioactive isotopes. One of them is lead-210; although it has a half-life of only 22.3 years, small quantities occur in nature because lead-210 is produced by a long decay series that starts with uranium-238 (which has been present for billions of years on Earth). Lead-211, -212, and -214 are present in the decay chains of uranium-235, thorium-232, and uranium-238, respectively, so traces of all three of these lead isotopes are found naturally. Minute traces of lead-209 arise from the very rare cluster decay of radium-223, one of the daughter products of natural uranium-235, and the decay chain of neptunium-237, traces of which are produced by neutron capture in uranium ores. Lead-210 is particularly useful for helping to identify the ages of samples by measuring its ratio to lead-206 (both isotopes are present in a single decay chain).\n\nIn total, 43 lead isotopes have been synthesized, with mass numbers 178–220. Lead-205 is the most stable radioisotope, with a half-life of around 1.5 years. The second-most stable is lead-202, which has a half-life of about 53,000 years, longer than any of the natural trace radioisotopes.\n\nBulk lead exposed to moist air forms a protective layer of varying composition. Lead(II) carbonate is a common constituent; the sulfate or chloride may also be present in urban or maritime settings. This layer makes bulk lead effectively chemically inert in the air. Finely powdered lead, as with many metals, is pyrophoric, and burns with a bluish-white flame.\n\nFluorine reacts with lead at room temperature, forming lead(II) fluoride. The reaction with chlorine is similar but requires heating, as the resulting chloride layer diminishes the reactivity of the elements. Molten lead reacts with the chalcogens to give lead(II) chalcogenides.\n\nLead metal resists sulfuric and phosphoric acid but not hydrochloric or nitric acid; the outcome depends on insolubility and subsequent passivation of the product salt. Organic acids, such as acetic acid, dissolve lead in the presence of oxygen. Concentrated alkalis will dissolve lead and form plumbites.\n\nLead shows two main oxidation states: +4 and +2. The tetravalent state is common for the carbon group. The divalent state is rare for carbon and silicon, minor for germanium, important (but not prevailing) for tin, and is the more important of the two oxidation states for lead. This is attributable to relativistic effects, specifically the inert pair effect, which manifests itself when there is a large difference in electronegativity between lead and oxide, halide, or nitride anions, leading to a significant partial positive charge on lead. The result is a stronger contraction of the lead 6s orbital than is the case for the 6p orbital, making it rather inert in ionic compounds. The inert pair effect is less applicable to compounds in which lead forms covalent bonds with elements of similar electronegativity, such as carbon in organolead compounds. In these, the 6s and 6p orbitals remain similarly sized and sp hybridization is still energetically favorable. Lead, like carbon, is predominantly tetravalent in such compounds.\n\nThere is a relatively large difference in the electronegativity of lead(II) at 1.87 and lead(IV) at 2.33. This difference marks the reversal in the trend of increasing stability of the +4 oxidation state going down the carbon group; tin, by comparison, has values of 1.80 in the +2 oxidation state and 1.96 in the +4 state.\n\nLead(II) compounds are characteristic of the inorganic chemistry of lead. Even strong oxidizing agents like fluorine and chlorine react with lead to give only PbF and PbCl. Lead(II) ions are usually colorless in solution, and partially hydrolyze to form Pb(OH) and finally [Pb(OH)] (in which the hydroxyl ions act as bridging ligands), but are not reducing agents as tin(II) ions are. Techniques for identifying the presence of the Pb ion in water generally rely on the precipitation of lead(II) chloride using dilute hydrochloric acid. As the chloride salt is sparingly soluble in water, in very dilute solutions the precipitation of lead(II) sulfide is achieved by bubbling hydrogen sulfide through the solution.\n\nLead monoxide exists in two polymorphs, litharge α-PbO (red) and massicot β-PbO (yellow), the latter being stable only above around 488 °C. Litharge is the most commonly used inorganic compound of lead. There is no lead(II) hydroxide; increasing the pH of solutions of lead(II) salts leads to hydrolysis and condensation.\nLead commonly reacts with heavier chalcogens. Lead sulfide is a semiconductor, a photoconductor, and an extremely sensitive infrared radiation detector. The other two chalcogenides, lead selenide and lead telluride, are likewise photoconducting. They are unusual in that their color becomes lighter going down the group.\nLead dihalides are well-characterized; this includes the diastatide, and mixed halides, such as PbFCl. The relative insolubility of the latter forms a useful basis for the gravimetric determination of fluorine. The difluoride was the first solid ionically conducting compound to be discovered (in 1834, by Michael Faraday). The other dihalides decompose on exposure to ultraviolet or visible light, especially the diiodide. Many lead(II) pseudohalides are known, such as the cyanide, cyanate, and thiocyanate. Lead(II) forms an extensive variety of halide coordination complexes, such as [PbCl], [PbCl], and the [PbCl] chain anion.\n\nLead(II) sulfate is insoluble in water, like the sulfates of other heavy divalent cations. Lead(II) nitrate and lead(II) acetate are very soluble, and this is exploited in the synthesis of other lead compounds.\n\nFew inorganic lead(IV) compounds are known. They are only formed in highly oxidizing solutions and do not normally exist under standard conditions. Lead(II) oxide gives a mixed oxide on further oxidation, PbO. It is described as lead(II,IV) oxide, or structurally 2PbO·PbO, and is the best-known mixed valence lead compound. Lead dioxide is a strong oxidizing agent, capable of oxidizing hydrochloric acid to chlorine gas. This is because the expected PbCl that would be produced is unstable and spontaneously decomposes to PbCl and Cl. Analogously to lead monoxide, lead dioxide is capable of forming plumbate anions. Lead disulfide and lead diselenide are only stable at high pressures. Lead tetrafluoride, a yellow crystalline powder, is stable, but less so than the difluoride. Lead tetrachloride (a yellow oil) decomposes at room temperature, lead tetrabromide is less stable still, and the existence of lead tetraiodide is questionable.\n\nSome lead compounds exist in formal oxidation states other than +4 or +2. Lead(III) may be obtained, as an intermediate between lead(II) and lead(IV), in larger organolead complexes; this oxidation state is not stable, as both the lead(III) ion and the larger complexes containing it are radicals. The same applies for lead(I), which can be found in such radical species.\n\nNumerous mixed lead(II,IV) oxides are known. When PbO is heated in air, it becomes PbO at 293 °C, PbO at 351 °C, PbO at 374 °C, and finally PbO at 605 °C. A further sesquioxide, PbO, can be obtained at high pressure, along with several non-stoichiometric phases. Many of them show defective fluorite structures in which some oxygen atoms are replaced by vacancies: PbO can be considered as having such a structure, with every alternate layer of oxygen atoms absent.\n\nNegative oxidation states can occur as Zintl phases, as either free lead anions, as in BaPb, with lead formally being lead(−IV), or in oxygen-sensitive ring-shaped or polyhedral cluster ions such as the trigonal bipyramidal Pb ion, where two lead atoms are lead(−I) and three are lead(0). In such anions, each atom is at a polyhedral vertex and contributes two electrons to each covalent bond along an edge from their sp hybrid orbitals, the other two being an external lone pair. They may be made in liquid ammonia via the reduction of lead by sodium.\n\nLead can form multiply-bonded chains, a property it shares with its lighter homologs in the carbon group. Its capacity to do so is much less because the Pb–Pb bond energy is over three and a half times lower than that of the C–C bond. With itself, lead can build metal–metal bonds of an order up to three. With carbon, lead forms organolead compounds similar to, but generally less stable than, typical organic compounds (due to the Pb–C bond being rather weak). This makes the organometallic chemistry of lead far less wide-ranging than that of tin. Lead predominantly forms organolead(IV) compounds, even when starting with inorganic lead(II) reactants; very few organolead(II) compounds are known. The most well-characterized exceptions are Pb[CH(SiMe)] and Pb(\"η\"-CH).\n\nThe lead analog of the simplest organic compound, methane, is plumbane. Plumbane may be obtained in a reaction between metallic lead and atomic hydrogen. Two simple derivatives, tetramethyllead and tetraethyllead, are the best-known organolead compounds. These compounds are relatively stable: tetraethyllead only starts to decompose if heated or if exposed to sunlight or ultraviolet light. (Tetraphenyllead is even more thermally stable, decomposing at 270 °C.) With sodium metal, lead readily forms an equimolar alloy that reacts with alkyl halides to form organometallic compounds such as tetraethyllead. The oxidizing nature of many organolead compounds is usefully exploited: lead tetraacetate is an important laboratory reagent for oxidation in organic synthesis, and tetraethyllead was once produced in larger quantities than any other organometallic compound. Other organolead compounds are less chemically stable. For many organic compounds, a lead analog does not exist.\n\nLead's per-particle abundance in the Solar System is 0.121 ppb (parts per billion). This figure is two and a half times higher than that of platinum, eight times more than mercury, and seventeen times more than gold. The amount of lead in the universe is slowly increasing as most heavier atoms (all of which are unstable) gradually decay to lead. The abundance of lead in the Solar System since its formation 4.5 billion years ago has increased by about 0.75%. The solar system abundances table shows that lead, despite its relatively high atomic number, is more prevalent than most other elements with atomic numbers greater than 40.\n\nPrimordial lead—which comprises the isotopes lead-204, lead-206, lead-207, and lead-208—was mostly created as a result of repetitive neutron capture processes occurring in stars. The two main modes of capture are the s- and r-processes.\n\nIn the s-process (s is for \"slow\"), captures are separated by years or decades, allowing less stable nuclei to undergo beta decay. A stable thallium-203 nucleus can capture a neutron and become thallium-204; this undergoes beta decay to give stable lead-204; on capturing another neutron, it becomes lead-205, which has a half-life of around 15 million years. Further captures result in lead-206, lead-207, and lead-208. On capturing another neutron, lead-208 becomes lead-209, which quickly decays into bismuth-209. On capturing another neutron, bismuth-209 becomes bismuth-210, and this beta decays to polonium-210, which alpha decays to lead-206. The cycle hence ends at lead-206, lead-207, lead-208, and bismuth-209.\nIn the r-process (r is for \"rapid\"), captures happen faster than nuclei can decay. This occurs in environments with a high neutron density, such as a supernova or the merger of two neutron stars. The neutron flux involved may be on the order of 10 neutrons per square centimeter per second. The r-process does not form as much lead as the s-process. It tends to stop once neutron-rich nuclei reach 126 neutrons. At this point, the neutrons are arranged in complete shells in the atomic nucleus, and it becomes harder to energetically accommodate more of them. When the neutron flux subsides, these nuclei beta decay into stable isotopes of osmium, iridium, and platinum.\n\nLead is classified as a chalcophile under the Goldschmidt classification, meaning it is generally found combined with sulfur. It rarely occurs in its native, metallic form. Many lead minerals are relatively light and, over the course of the Earth's history, have remained in the crust instead of sinking deeper into the Earth's interior. This accounts for lead's relatively high crustal abundance of 14 ppm; it is the 38th most abundant element in the crust.\n\nThe main lead-bearing mineral is galena (PbS), which is mostly found with zinc ores. Most other lead minerals are related to galena in some way; boulangerite, PbSbS, is a mixed sulfide derived from galena; anglesite, PbSO, is a product of galena oxidation; and cerussite or white lead ore, PbCO, is a decomposition product of galena. Arsenic, tin, antimony, silver, gold, copper, and bismuth are common impurities in lead minerals.\nWorld lead resources exceed two billion tons. Significant deposits are located in Australia, China, Ireland, Mexico, Peru, Portugal, Russia, and the United States. Global reserves—resources that are economically feasible to extract—totaled 88 million tons in 2016, of which Australia had 35 million, China 17 million, and Russia 6.4 million.\n\nTypical background concentrations of lead do not exceed 0.1 μg/m in the atmosphere; 100 mg/kg in soil; and 5 μg/L in freshwater and seawater.\n\nThe modern English word \"lead\" is of Germanic origin; it comes from the Middle English \"leed\" and Old English \"lēad\" (with the macron above the \"e\" signifying that the vowel sound of that letter is long). The Old English word is derived from the hypothetical reconstructed Proto-Germanic \"*lauda-\" (\"lead\"). According to linguistic theory, this word bore descendants in multiple Germanic languages of exactly the same meaning.\n\nThe origin of the Proto-Germanic \"*lauda-\" is not agreed in the linguistic community. One hypothesis suggests it is derived from Proto-Indo-European \"*lAudh-\" (\"lead\"; capitalization of the vowel is equivalent to the macron). Another hypothesis suggests it is borrowed from Proto-Celtic \"*ɸloud-io-\" (\"lead\"). This word is related to the Latin \"plumbum\", which gave the element its chemical symbol \"Pb\". The word \"*ɸloud-io-\" is thought to be the origin of Proto-Germanic \"*bliwa-\" (which also means \"lead\"), from which stemmed the German \"Blei\".\n\nThe name of the chemical element is not related to the verb of the same spelling, which is derived from Proto-Germanic \"*laidijan-\" (\"to lead\").\n\nMetallic lead beads dating back to 7000–6500 BCE have been found in Asia Minor and may represent the first example of metal smelting. At that time lead had few (if any) applications due to its softness and dull appearance. The major reason for the spread of lead production was its association with silver, which may be obtained by burning galena (a common lead mineral). The Ancient Egyptians were the first to use lead minerals in cosmetics, an application that spread to Ancient Greece and beyond; the Egyptians may have used lead for sinkers in fishing nets, glazes, glasses, enamels, and for ornaments. Various civilizations of the Fertile Crescent used lead as a writing material, as currency, and for construction. Lead was used in the Ancient Chinese royal court as a stimulant, as currency, and as a contraceptive; the Indus Valley civilization and the Mesoamericans used it for making amulets; and the eastern and southern African peoples used lead in wire drawing.\n\nBecause silver was extensively used as a decorative material and an exchange medium, lead deposits came to be worked in Asia Minor since 3000 BCE; later, lead deposits were developed in the Aegean and Laurion. These three regions collectively dominated production of mined lead until c. 1200 BCE. Since 2000 BCE, the Phoenicians worked deposits in the Iberian peninsula; by 1600 BCE, lead mining existed in Cyprus, Greece, and Sardinia.\n\nRome's territorial expansion in Europe and across the Mediterranean, and its development of mining, led to it becoming the greatest producer of lead during the classical era, with an estimated annual output peaking at 80,000 tonnes. Like their predecessors, the Romans obtained lead mostly as a by-product of silver smelting. Lead mining occurred in Central Europe, Britain, the Balkans, Greece, Anatolia, and Hispania, the latter accounting for 40% of world production.\n\nLead tablets were commonly used as a material for letters. Lead coffins, cast in flat sand forms, with interchangeable motifs to suit the faith of the deceased were used in ancient Judea.\n\nLead was used for making water pipes in the Roman Empire; the Latin word for the metal, \"plumbum\", is the origin of the English word \"plumbing\". Its ease of working and resistance to corrosion ensured its widespread use in other applications including pharmaceuticals, roofing, currency, and warfare. Writers of the time, such as Cato the Elder, Columella, and Pliny the Elder, recommended lead (or lead-coated) vessels for the preparation of sweeteners and preservatives added to wine and food. The lead conferred an agreeable taste due to the formation of \"sugar of lead\" (lead(II) acetate), whereas copper or bronze vessels could impart a bitter flavor through verdigris formation.\n\nThe Roman author Vitruvius reported the health dangers of lead and modern writers have suggested that lead poisoning played a major role in the decline of the Roman Empire. Other researchers have criticized such claims, pointing out, for instance, that not all abdominal pain is caused by lead poisoning. According to archaeological research, Roman lead pipes increased lead levels in tap water but such an effect was \"unlikely to have been truly harmful\". When lead poisoning did occur, victims were called \"saturnine\", dark and cynical, after the ghoulish father of the gods, Saturn. By association, lead was considered the father of all metals. Its status in Roman society was low as it was readily available and cheap.\n\nDuring the classical era (and even up to the 17th century), tin was often not distinguished from lead: Romans called lead \"plumbum nigrum\" (\"black lead\"), and tin \"plumbum candidum\" (\"bright lead\"). The association of lead and tin can be seen in other languages: the word \"olovo\" in Czech translates to \"lead\", but in Russian the cognate \"олово\" (\"olovo\") means \"tin\". To add to the confusion, lead bore a close relation to antimony: both elements commonly occur as sulfides (galena and stibnite), often together. Pliny incorrectly wrote that stibnite would give lead on heating, instead of antimony. In countries such as Turkey and India, the originally Persian name \"surma\" came to refer to either antimony sulfide or lead sulfide, and in some languages, such as Russian, gave its name to antimony \"(сурьма).\"\n\nLead mining in Western Europe declined after the fall of the Western Roman Empire, with Arabian Iberia being the only region having a significant output. The largest production of lead occurred in South and East Asia, especially China and India, where lead mining grew rapidly.\n\nIn Europe, lead production began to increase in the 11th and 12th centuries, when it was again used for roofing and piping. Starting in the 13th century, lead was used to create stained glass. In the European and Arabian traditions of alchemy, lead (symbol in the European tradition) was considered an impure base metal which, by the separation, purification and balancing of its constituent essences, could be transformed to pure and incorruptible gold. During the period, lead was used increasingly for adulterating wine. The use of such wine was forbidden for use in Christian rites by a papal bull in 1498, but it continued to be imbibed and resulted in mass poisonings up to the late 18th century. Lead was a key material in parts of the printing press, which was invented around 1440; lead dust was commonly inhaled by print workers, causing lead poisoning. Firearms were invented at around the same time, and lead, despite being more expensive than iron, became the chief material for making bullets. It was less damaging to iron gun barrels, had a higher density (which allowed for better retention of velocity), and its lower melting point made the production of bullets easier as they could be made using a wood fire. Lead, in the form of Venetian ceruse, was extensively used in cosmetics by Western European aristocracy as whitened faces were regarded as a sign of modesty. This practice later expanded to white wigs and eyeliners, and only faded out with the French Revolution in the late 18th century. A similar fashion appeared in Japan in the 18th century with the emergence of the geishas, a practice that continued long into the 20th century. The white faces of women \"came to represent their feminine virtue as Japanese women\", with lead commonly used in the whitener.\n\nIn the New World, lead was produced soon after the arrival of European settlers. The earliest recorded lead production dates to 1621 in the English Colony of Virginia, fourteen years after its foundation. In Australia, the first mine opened by colonists on the continent was a lead mine, in 1841. In Africa, lead mining and smelting were known in the Benue Trough and the lower Congo Basin, where lead was used for trade with Europeans, and as a currency by the 17th century, well before the scramble for Africa.\n\nIn the second half of the 18th century, Britain, and later continental Europe and the United States, experienced the Industrial Revolution. This was the first time during which lead production rates exceeded those of Rome. Britain was the leading producer, losing this status by the mid-19th century with the depletion of its mines and the development of lead mining in Germany, Spain, and the United States. By 1900, the United States was the leader in global lead production, and other non-European nations—Canada, Mexico, and Australia—had begun significant production; production outside Europe exceeded that within. A great share of the demand for lead came from plumbing and painting—lead paints were in regular use. At this time, more (working class) people were exposed to the metal and lead poisoning cases escalated. This led to research into the effects of lead intake. Lead was proven to be more dangerous in its fume form than as a solid metal. Lead poisoning and gout were linked; British physician Alfred Baring Garrod noted a third of his gout patients were plumbers and painters. The effects of chronic ingestion of lead, including mental disorders, were also studied in the 19th century. The first laws aimed at decreasing lead poisoning in factories were enacted during the 1870s and 1880s in the United Kingdom.\n\nFurther evidence of the threat that lead posed to humans was discovered in the late 19th and early 20th centuries. Mechanisms of harm were better understood, lead blindness was documented, and the element was phased out of public use in the United States and Europe. The United Kingdom introduced mandatory factory inspections in 1878 and appointed the first Medical Inspector of Factories in 1898; as a result, a 25-fold decrease in lead poisoning incidents from 1900 to 1944 was reported. The last major human exposure to lead was the addition of tetraethyllead to gasoline as an antiknock agent, a practice that originated in the United States in 1921. It was phased out in the United States and the European Union by 2000. Most European countries banned lead paint—commonly used because of its opacity and water resistance—for interiors by 1930.\n\nIn the 1970s, the United States and Western European countries introduced legislation to reduce lead air pollution. The impact was significant: while a study conducted by the Centers for Disease Control and Prevention in the United States in 1976–1980 showed that 77.8% of the population had elevated blood lead levels, in 1991–1994, a study by the same institute showed the share of people with such high levels dropped to 2.2%. The main product made of lead by the end of the 20th century was the lead–acid battery, which posed no direct threat to humans. From 1960 to 1990, lead output in the Western Bloc grew by a third. The share of the world's lead production by the Eastern Bloc increased from 10% to 30%, from 1950 to 1990, with the Soviet Union being the world's largest producer during the mid-1970s and the 1980s, and China starting major lead production in the late 20th century. Unlike the European communist countries, China was largely unindustrialized by the mid-20th century; in 2004, China surpassed Australia as the largest producer of lead. As was the case during European industrialization, lead has had a negative effect on health in China.\n\nProduction of lead is increasing worldwide due to its use in lead–acid batteries. There are two major categories of production: primary from mined ores, and secondary from scrap. In 2014, 4.58 million metric tons came from primary production and 5.64 million from secondary production. The top three producers of mined lead concentrate in that year were China, Australia, and the United States. The top three producers of refined lead were China, the United States, and South Korea. According to the International Resource Panel's Metal Stocks in Society report of 2010, the total amount of lead in use, stockpiled, discarded, or dissipated into the environment, on a global basis, is 8 kg per capita. Much of this is in more developed countries (20–150 kg per capita) rather than less developed ones (1–4 kg per capita).\n\nThe primary and secondary lead production processes are similar. Some primary production plants now supplement their operations with scrap lead, and this trend is likely to increase in the future. Given adequate techniques, lead obtained via secondary processes is indistinguishable from lead obtained via primary processes. Scrap lead from the building trade is usually fairly clean and is re-melted without the need for smelting, though refining is sometimes needed. Secondary lead production is therefore cheaper, in terms of energy requirements, than is primary production, often by 50% or more.\n\nMost lead ores contain a low percentage of lead (rich ores have a typical content of 3–8%) which must be concentrated for extraction. During initial processing, ores typically undergo crushing, dense-medium separation, grinding, froth flotation, and drying. The resulting concentrate, which has a lead content of 30–80% by mass (regularly 50–60%), is then turned into (impure) lead metal.\n\nThere are two main ways of doing this: a two-stage process involving roasting followed by blast furnace extraction, carried out in separate vessels; or a direct process in which the extraction of the concentrate occurs in a single vessel. The latter has become the most common route, though the former is still significant.\n\nFirst, the sulfide concentrate is roasted in air to oxidize the lead sulfide:\n\nAs the original concentrate was not pure lead sulfide, roasting yields not only the desired lead(II) oxide, but a mixture of oxides, sulfates, and silicates of lead and of the other metals contained in the ore. This impure lead oxide is reduced in a coke-fired blast furnace to the (again, impure) metal:\n\nImpurities are mostly arsenic, antimony, bismuth, zinc, copper, silver, and gold. The melt is treated in a reverberatory furnace with air, steam, and sulfur, which oxidizes the impurities except for silver, gold, and bismuth. Oxidized contaminants float to the top of the melt and are skimmed off. Metallic silver and gold are removed and recovered economically by means of the Parkes process, in which zinc is added to lead. Zinc, which is immiscible in lead, dissolves the silver and gold. The zinc solution can be separated from the lead, and the silver and gold retrieved. De-silvered lead is freed of bismuth by the Betterton–Kroll process, treating it with metallic calcium and magnesium. The resulting bismuth dross can be skimmed off.\n\nVery pure lead can be obtained by processing smelted lead electrolytically using the Betts process. Anodes of impure lead and cathodes of pure lead are placed in an electrolyte of lead fluorosilicate (PbSiF). Once electrical potential is applied, impure lead at the anode dissolves and plates onto the cathode, leaving the majority of the impurities in solution. This is a high-cost process and thus mostly reserved for refining bullion containing high percentages of impurities.\n\nIn this process, lead bullion and slag is obtained directly from lead concentrates. The lead sulfide concentrate is melted in a furnace and oxidized, forming lead monoxide. Carbon (as coke or coal gas) is added to the molten charge along with fluxing agents. The lead monoxide is thereby reduced to metallic lead, in the midst of a slag rich in lead monoxide.\n\nAs much as 80% of the lead in very high-content initial concentrates can be obtained as bullion; the remaining 20% forms a slag rich in lead monoxide. For a low-grade feed, all of the lead can be oxidized to a high-lead slag. Metallic lead is further obtained from the high-lead (25–40%) slags via submerged fuel combustion or injection, reduction assisted by an electric furnace, or a combination of both.\n\nResearch on a cleaner, less energy-intensive lead extraction process continues; a major drawback is that either too much lead is lost as waste, or the alternatives result in a high sulfur content in the resulting lead metal. Hydrometallurgical extraction, in which anodes of impure lead are immersed into an electrolyte and pure lead is deposited onto a cathode, is a technique that may have potential.\n\nSmelting, which is an essential part of the primary production, is often skipped during secondary production. It is only performed when metallic lead has undergone significant oxidation. The process is similar to that of primary production in either a blast furnace or a rotary furnace, with the essential difference being the greater variability of yields: blast furnaces produce hard lead (10% antimony) while reverberatory and rotary kiln furnaces produced semisoft lead (3–4% antimony). The Isasmelt process is a more recent method that may act as an extension to primary production; battery paste from spent lead–acid batteries has sulfur removed by treating it with alkali, and is then treated in a coal-fueled furnace in the presence of oxygen, which yields impure lead, with antimony the most common impurity. Refining of secondary lead is similar to that of primary lead; some refining processes may be skipped depending on the material recycled and its potential contamination.\n\nOf the sources of lead for recycling, lead–acid batteries are the most important; lead pipe, sheet, and cable sheathing are also significant.\n\nContrary to popular belief, pencil leads in wooden pencils have never been made from lead. When the pencil originated as a wrapped graphite writing tool, the particular type of graphite used was named \"plumbago\" (literally, \"act for lead\" or \"lead mockup\").\n\nLead metal has several useful mechanical properties, including high density, low melting point, ductility, and relative inertness. Many metals are superior to lead in some of these aspects but are generally less common and more difficult to extract from parent ores. Lead's toxicity has led to its phasing out for some uses.\n\nLead has been used for bullets since their invention in the Middle Ages. It is inexpensive; its low melting point means small arms ammunition and shotgun pellets can be cast with minimal technical equipment; and it is denser than other common metals, which allows for better retention of velocity. Concerns have been raised that lead bullets used for hunting can damage the environment.\n\nLead's high density and resistance to corrosion have been exploited in a number of related applications. It is used as ballast in sailboat keels; its density allows it to take up a small volume and minimize water resistance, thus counterbalancing the heeling effect of wind on the sails. It is used in scuba diving weight belts to counteract the diver's buoyancy. In 1993, the base of the Leaning Tower of Pisa was stabilized with 600 tonnes of lead. Because of its corrosion resistance, lead is used as a protective sheath for underwater cables.\nLead has many uses in the construction industry; lead sheets are used as architectural metals in roofing material, cladding, flashing, gutters and gutter joints, and on roof parapets. Detailed lead moldings are used as decorative motifs to fix lead sheet. Lead is still used in statues and sculptures, including for armatures. In the past it was often used to balance the wheels of cars; for environmental reasons this use is being phased out in favor of other materials.\n\nLead is added to copper alloys, such as brass and bronze, to improve machinability and for its lubricating qualities. Being practically insoluble in copper the lead forms solid globules in imperfections throughout the alloy, such as grain boundaries. In low concentrations, as well as acting as a lubricant, the globules hinder the formation of swarf as the alloy is worked, thereby improving machinability. Copper alloys with larger concentrations of lead are used in bearings. The lead provides lubrication, and the copper provides the load-bearing support.\n\nLead's high density, atomic number, and formability form the basis for use of lead as a barrier that absorbs sound, vibration, and radiation. Lead has no natural resonance frequencies; as a result, sheet-lead is used as a sound deadening layer in the walls, floors, and ceilings of sound studios. Organ pipes are often made from a lead alloy, mixed with various amounts of tin to control the tone of each pipe. Lead is an established shielding material from radiation in nuclear science and in X-ray rooms due to its denseness and high attenuation coefficient. Molten lead has been used as a coolant for lead-cooled fast reactors.\n\nThe largest use of lead in the early 21st century is in lead–acid batteries. The reactions in the battery between lead, lead dioxide, and sulfuric acid provide a reliable source of voltage. The lead in batteries undergoes no direct contact with humans, so there are fewer toxicity concerns. Supercapacitors incorporating lead–acid batteries have been installed in kilowatt and megawatt scale applications in Australia, Japan, and the United States in frequency regulation, solar smoothing and shifting, wind smoothing, and other applications. These batteries have lower energy density and charge-discharge efficiency than lithium-ion batteries, but are significantly cheaper.\n\nLead is used in high voltage power cables as sheathing material to prevent water diffusion into insulation; this use is decreasing as lead is being phased out. Its use in solder for electronics is also being phased out by some countries to reduce the amount of environmentally hazardous waste. Lead is one of three metals used in the Oddy test for museum materials, helping detect organic acids, aldehydes, and acidic gases.\n\nIn addition to being the main application for lead metal, lead-acid batteries are also the main consumer of lead compounds. The energy storage/release reaction used in these devices involves lead sulfate and lead dioxide:\n\nOther applications of lead compounds are very specialized and often fading. Lead-based coloring agents are used in ceramic glazes and glass, especially for red and yellow shades. While lead paints are phased out in Europe and North America, they remain in use in less developed countries such as China or India. Lead tetraacetate and lead dioxide are used as oxidizing agents in organic chemistry. Lead is frequently used in the polyvinyl chloride coating of electrical cords. It can be used to treat candle wicks to ensure a longer, more even burn. Because of its toxicity, European and North American manufacturers use alternatives such as zinc. Lead glass is composed of 12–28% lead oxide, changing its optical characteristics and reducing the transmission of ionizing radiation. Lead-based semiconductors such as lead telluride and lead selenide are used in photovoltaic cells and infrared detectors.\n\nLead has no confirmed biological role. Its prevalence in the human body—at an adult average of 120 mg—is nevertheless exceeded only by zinc (2500 mg) and iron (4000 mg) among the heavy metals. Lead salts are very efficiently absorbed by the body. A small amount of lead (1%) is stored in bones; the rest is excreted in urine and feces within a few weeks of exposure. Only about a third of lead is excreted by a child. Continual exposure may result in the bioaccumulation of lead.\n\nLead is a highly poisonous metal (whether inhaled or swallowed), affecting almost every organ and system in the human body. At airborne levels of 100 mg/m, it is immediately dangerous to life and health. Most ingested lead is absorbed into the bloodstream. The primary cause of its toxicity is its predilection for interfering with the proper functioning of enzymes. It does so by binding to the sulfhydryl groups found on many enzymes, or mimicking and displacing other metals which act as cofactors in many enzymatic reactions. Among the essential metals that lead interacts with are calcium, iron, and zinc. High levels of calcium and iron tend to provide some protection from lead poisoning; low levels cause increased susceptibility.\n\nLead can cause severe damage to the brain and kidneys and, ultimately, death. By mimicking calcium, lead can cross the blood–brain barrier. It degrades the myelin sheaths of neurons, reduces their numbers, interferes with neurotransmission routes, and decreases neuronal growth. In the human body, lead inhibits porphobilinogen synthase and ferrochelatase, preventing both porphobilinogen formation and the incorporation of iron into protoporphyrin IX, the final step in heme synthesis. This causes ineffective heme synthesis and microcytic anemia.\n\nSymptoms of lead poisoning include nephropathy, colic-like abdominal pains, and possibly weakness in the fingers, wrists, or ankles. Small blood pressure increases, particularly in middle-aged and older people, may be apparent and can cause anemia. Several studies, mostly cross-sectional, found an association between increased lead exposure and decreased heart rate variability. In pregnant women, high levels of exposure to lead may cause miscarriage. Chronic, high-level exposure has been shown to reduce fertility in males.\n\nIn a child's developing brain, lead interferes with synapse formation in the cerebral cortex, neurochemical development (including that of neurotransmitters), and the organization of ion channels. Early childhood exposure has been linked with an increased risk of sleep disturbances and excessive daytime drowsiness in later childhood. High blood levels are associated with delayed puberty in girls. The rise and fall in exposure to airborne lead from the combustion of tetraethyl lead in gasoline during the 20th century has been linked with historical increases and decreases in crime levels, a hypothesis which is not universally accepted.\n\nLead exposure is a global issue since lead mining and smelting, and battery manufacturing/disposal/recycling, are common in many countries. Lead enters the body via inhalation, ingestion, or skin absorption. Almost all inhaled lead is absorbed into the body; for ingestion, the rate is 20–70%, with children absorbing a higher percentage than adults.\n\nPoisoning typically results from ingestion of food or water contaminated with lead, and less commonly after accidental ingestion of contaminated soil, dust, or lead-based paint. Seawater products can contain lead if affected by nearby industrial waters. Fruit and vegetables can be contaminated by high levels of lead in the soils they were grown in. Soil can be contaminated through particulate accumulation from lead in pipes, lead paint, and residual emissions from leaded gasoline.\n\nThe use of lead for water pipes is problematic in areas with soft or acidic water. Hard water forms insoluble layers in the pipes whereas soft and acidic water dissolves the lead pipes. Dissolved carbon dioxide in the carried water may result in the formation of soluble lead bicarbonate; oxygenated water may similarly dissolve lead as lead(II) hydroxide. Drinking such water, over time, can cause health problems due to the toxicity of the dissolved lead. The harder the water the more calcium bicarbonate and sulfate it will contain, and the more the inside of the pipes will be coated with a protective layer of lead carbonate or lead sulfate.\n\nIngestion of applied lead-based paint is the major source of exposure for children:\na direct source is chewing on old painted window sills. Alternatively, as the applied dry paint deteriorates, it peels, is pulverized into dust and then enters the body through hand-to-mouth contact or contaminated food, water, or alcohol. Ingesting certain home remedies may result in exposure to lead or its compounds.\n\nInhalation is the second major exposure pathway, affecting smokers and especially workers in lead-related occupations. Cigarette smoke contains, among other toxic substances, radioactive lead-210.\n\nSkin exposure may be significant for people working with organic lead compounds. The rate of skin absorption is lower for inorganic lead.\n\nTreatment for lead poisoning normally involves the administration of dimercaprol and succimer. Acute cases may require the use of disodium calcium edetate, the calcium chelate, and the disodium salt of ethylenediaminetetraacetic acid (EDTA). It has a greater affinity for lead than calcium, with the result that lead chelate is formed by exchange and excreted in the urine, leaving behind harmless calcium.\n\nThe extraction, production, use, and disposal of lead and its products have caused significant contamination of the Earth's soils and waters. Atmospheric emissions of lead were at their peak during the Industrial Revolution, and the leaded gasoline period in the second half of the twentieth century. Lead releases originate from natural sources (i.e., concentration of the naturally occurring lead), industrial production, incineration and recycling, and mobilization of previously buried lead. Elevated concentrations of lead persist in soils and sediments in post-industrial and urban areas; industrial emissions, including those arising from coal burning, continue in many parts of the world, particularly in the developing countries.\n\nLead can accumulate in soils, especially those with a high organic content, where it remains for hundreds to thousands of years. Environmental lead can compete with other metals found in and on plants surfaces potentially inhibiting photosynthesis and at high enough concentrations, negatively affecting plant growth and survival. Contamination of soils and plants can allow lead to ascend the food chain affecting microorganisms and animals. In animals, lead exhibits toxicity in many organs, damaging the nervous, renal, reproductive, hematopoietic, and cardiovascular systems after ingestion, inhalation, or skin absorption. Fish uptake lead from both water and sediment; bioaccumulation in the food chain poses a hazard to fish, birds, and sea mammals.\n\nAntropogenic lead includes lead from shot and sinkers. These are among the most potent sources of lead contamination along with lead production sites. Lead was banned for shot and sinkers in the United States in 2017, although that ban was only effective for a month, and a similar ban is being considered in the European Union.\n\nAnalytical methods for the determination of lead in the environment include spectrophotometry, X-ray fluorescence, atomic spectroscopy and electrochemical methods. A specific ion-selective electrode has been developed based on the ionophore S,S'-methylenebis(N,N-diisobutyldithiocarbamate). An important biomarker assay for lead poisoning is δ-aminolevulinic acid levels in plasma, serum, and urine.\n\nBy the mid-1980s, there was significant decline in the use of lead in industry. In the United States, environmental regulations reduced or eliminated the use of lead in non-battery products, including gasoline, paints, solders, and water systems. Particulate control devices were installed in coal-fired power plants to capture lead emissions. Lead use was further curtailed by the European Union's 2003 Restriction of Hazardous Substances Directive. A large drop in lead deposition occurred in the Netherlands after the 1993 national ban on use of lead shot for hunting and sport shooting: from 230 tonnes in 1990 to 47.5 tonnes in 1995.\n\nIn the United States, the permissible exposure limit for lead in the workplace, comprising metallic lead, inorganic lead compounds, and lead soaps, was set at 50 μg/m over an 8-hour workday, and the blood lead level limit at 5 μg per 100 g of blood in 2012. Lead may still be found in harmful quantities in stoneware, vinyl (such as that used for tubing and the insulation of electrical cords), and Chinese brass. Old houses may still contain lead paint. White lead paint has been withdrawn from sale in industrialized countries, but specialized uses of other pigments such as yellow lead chromate remain. Stripping old paint by sanding produces dust which can be inhaled. Lead abatement programs have been mandated by some authorities in properties where young children live.\n\nLead waste, depending on the jurisdiction and the nature of the waste, may be treated as household waste (in order to facilitate lead abatement activities), or potentially hazardous waste requiring specialized treatment or storage. Lead is released to the wildlife in shooting places and a number of lead management practices, such as stewardship of the environment and reduced public scrutiny, have been developed to counter the lead contamination. Lead migration can be enhanced in acidic soils; to counter that, it is advised soils be treated with lime to neutralize the soils and prevent leaching of lead.\n\nResearch has been conducted on how to remove lead from biosystems by biological means: Fish bones are being researched for their ability to bioremediate lead in contaminated soil. The fungus \"Aspergillus versicolor\" is effective at removing lead ions. Several bacteria have been researched for their ability to remove lead from the environment, including the sulfate-reducing bacteria \"Desulfovibrio\" and \"Desulfotomaculum\", both of which are highly effective in aqueous solutions.\n\n\n\n"}
{"id": "31479177", "url": "https://en.wikipedia.org/wiki?curid=31479177", "title": "List of Spanish flu cases", "text": "List of Spanish flu cases\n\nThis is a list of cases from the January 1918 – December 1920 flu pandemic, commonly referred to as the Spanish flu.\n\nTo maintain morale, wartime censors minimized early reports of illness and mortality in Germany, the United Kingdom, France, and the United States. Papers were free to report the epidemic's effects in neutral Spain (such as the grave illness of King Alfonso XIII). This created a false impression of Spain as especially hard hit, thereby giving rise to the pandemic's nickname, \"Spanish Flu\".\n\nListed alphabetically by surname\n\nChildren of women who were pregnant during the pandemic ran the risk of lifelong effects. One in three of the more than 25 million who contracted the flu in the United States was a woman of childbearing age. A study of US census data from 1960 to 1980 found that the children born to this group of women had more physical ailments and a lower lifetime income than those born a few months earlier or later. The study also found that persons born in states with more severe exposure to the pandemic experienced worse outcomes than persons born in states with less severe exposures.\n\n"}
{"id": "49541051", "url": "https://en.wikipedia.org/wiki?curid=49541051", "title": "List of countries by body mass index", "text": "List of countries by body mass index\n\nThis page serves as a partial list of countries by adult mean body weight and incidence of obese and overweight populations as calculated by body mass index (BMI). \n\nThe data for 2014 was first published by the World Health Organization in 2015.\n\nMean body mass index (BMI) provides a simplified measure of the comparative weight of populations on a country by country basis. BMI calculates a person's mass (weight) divided by the square of their height. An individual with a BMI of 25 kg/m or more is considered overweight. An individual with a BMI of 30 kg/m or more is considered obese.\n\nThe data highlighted on this page comes from World Health Organization statistics for adult (18 years old and older) populations. Mean BMI data is shown separately for males and females, as well as a combined figure. Mean data highlights the central tendency of the population data and is but one method of calculating relative body weight between populations.\n\nThere are significant limitations to the usefulness of comparative BMI data cited by both the medical community and statisticians. BMI data has significant weaknesses in terms of scalability and in accounting for variations in physical characteristics. \n\nData published in 2015.\n\nData published in 2017.\n\nData published in 2015.\n"}
{"id": "42766494", "url": "https://en.wikipedia.org/wiki?curid=42766494", "title": "List of countries by risk of death from non-communicable disease", "text": "List of countries by risk of death from non-communicable disease\n\nThis is a list of countries by risk of premature death from non-communicable disease such as cardiovascular disease, cancer, diabetes, or chronic respiratory disease between ages 30 and 70 as published by the World Health Organization in 2008. Measuring the risk of dying from target NCDs is important to assess the extent of burden from mortality due NCDs in a population.\n\nLife tables specifying all-cause mortality rates by age and sex for WHO Member States are developed from available death registration data, sample registration systems (India, China) and data on child and adult mortality from censuses and surveys.\nCause-of-death distributions are estimated from death registration data, and data from population-based epidemiological studies, disease registers and notifications systems for selected specific causes of death. Causes of death for populations without usable death-registration data are estimated using cause-of-death models together with data from population-based epidemiological studies, disease registers and notifications systems.\n\n"}
{"id": "2215129", "url": "https://en.wikipedia.org/wiki?curid=2215129", "title": "Nkosi's Haven", "text": "Nkosi's Haven\n\nNkosi's Haven is an NGO in the Johannesburg, South Africa area that offers residential, holistic care and support for mothers and their children whose lives have been impacted by HIV/AIDS. Nkosi's Haven also provides support for orphans, HIV/AIDS affected or not. It aims to improve the productivity of their residents through providing access to medical care, therapy, education and skill building workshops. The goal is to empower residents while providing a safe, dignified home in hopes that all mothers and children are able to become responsible and contributing members of society.\n\nNkosi's Haven was named after Nkosi Johnson, an AIDS activist who dedicated his life to ensuring that mothers and their children are kept together under the belief that no mother should have to leave her child due to HIV diagnosis. It is a recognized non-governmental organization that is largely funded by international donors and governmental organizations. Funding is allocated to assist accommodations, which include housing, food, water, medication, and hospice care. It also assists with education costs such as school fees, uniforms, and other expenditures. Residents share household tasks among themselves, including tasks such as cooking, cleaning, laundry, and childcare.\nAt the time of inception, it was the first and only care centre in South Africa that provided residential care for mothers living with HIV/AIDS and their children.\n\n“Through all of the work we do, we ensure that our residents learn how to live with AIDS, not die from it.” Nkosi's Haven has built a home in which residents can live free from prejudice and discrimination against HIV/AIDS and children can grow into self-sufficient and responsible members of society.\n\nNkosi's Haven was named after Nkosi Johnson, a young AIDS activist who died on International Children's Day in 2001. Born Xolani Nkosi, Nkosi was infected with HIV through mother-to-child transmission. Nkosi and his mother were admitted to an AIDS care centre in Johannesburg, where they met Gail Johnson, a volunteer worker.\n\nDue to financial restraints, the care centre that Nkosi and his mother were living in closed down. Nkosi's mother's health was deteriorating rapidly and she was unable to keep Nkosi due to her health conditions and financial dependency. She also feared that the community would find out that both herself and Nkosi were HIV positive and exile them. As a result, Gail Johnson took Nkosi into her care, changing his name to Nkosi Johnson, and became his legal foster mother.\n\nGail Johnson attempted to enroll Nkosi into school in Johannesburg and was faced with backlash as soon as Nkosi's HIV status was made public. In 1997, Johnson took the case to court, winning her case and forcing the South African educational system to revise their admittance policies discriminating against children with AIDS.\n\nAfter Nkosi's experience with discrimination due to his health status and the forced separation between him and his mother, Nkosi's dream was to create an HIV/AIDS care centre in which children and their mothers could live freely without prejudice or discrimination because of their health status. With the help of his foster mother, Gail, Nkosi opened Nkosi's Haven in 1999, providing hundreds of mothers and their children with a safe space and communal environment.\n\nNkosi's journey made him the national figure in the fight against AIDS. In July 2000, Nkosi wrote and presented a speech at the 13th International Aids Conference, held in Durban, which was televised worldwide. At 11 years old, he spoke to delegates about his experience growing up with HIV. His fight with HIV birthed his dream to raise awareness and erase the stigma of HIV/AIDS through communal, supportive environments and care centres. Nkosi is most famously known for his quote,\n“We are normal. We have hands. We have feet. We can walk, we can talk, we have needs just like everyone else- don't be afraid of us- we are all the same!\"\n\nA year before his death, Nkosi suffered through severe brain damage and viral infections due to AIDS. He was bedridden, emaciated, suffering from seizures and unable to eat solid foods. After battling the disease for 12 years, Nkosi Johnson died in his sleep on 1 June 2001.\n\nNkosi was given a hero's burial memorial service in Johannesburg with thousands of attendees paying their respects to the late activist.\n\nAt the time of his death, Nkosi Johnson was South Africa's longest surviving child born with HIV/AIDS. In 2005, he was awarded with the International Children's Peace Prize for his fight against HIV/AIDS and activist efforts. After his death, Nelson Mandela released a statement, recognizing Nkosi as an \"icon of the struggle for life\" who fought fearlessly against HIV/AIDS. He urged everyone to give their support during this time, acknowledging Nkosi as an ambassador for South Africa and its people, especially those living with HIV.\n\nNkosi Johnson's legacy lives on through the work of Nkosi's Haven.\n\nSouth Africa has had high rates of HIV/AIDS diagnoses for years, with approximately 6.19 million accounted individuals living with HIV/AIDS in 2015. Within the age group of adults aged 15–49 years old, 16.6% of that cohort is HIV positive. HIV/AIDS related deaths have been declining as of recent years, with 2005 being the year with the highest number of deaths in South Africa. Thereafter, antiretroviral treatment was administered and easily accessible, changing the patterns of HIV/AIDS and extending the lifespan of many South Africans who would have otherwise died at an earlier age.\n\nHIV/AIDS related illnesses are still high, with an increase of 4.02 million people living with HIV in South Africa from 2002 to 2015. Statistics South Africa approximates that every one out of five South African females in their reproductive ages are living with HIV.\n\nUNAIDS reports that approximately 2.1 million children in South Africa are orphans due to HIV/AIDS related illnesses, with 180,000 deaths occurring in the year of 2015. Women over the age of 15 account for 4 million people living with HIV, and approximately 240,000 children under the age of 14 have been affected by HIV.\n\nEvidence has shown that the HIV/AIDS crisis has had a significant impact on South African family life. Responsibility for children growing up in South African society has been increasingly separate from biological parenthood due to the lack of parental care available. Under many circumstances, children often care for their own families due to the circumstances created by the HIV/AIDS epidemic.\n\nIn 1999, Nkosi and Gail opened their first Nkosi's Haven location in Berea, Johannesburg. Despite being close to the city centre, selling properties became increasingly difficult in Berea, with the neighbouring area of Hillbrow beside, as it became an area that was frequented with street children and the unemployed. The 1980s was an era of high crime rates in downtown Johannesburg, which neighboured Hillbrow and Berea. After the apartheid ended in the 1990s, many gangs, along with residents from neighbouring townships, moved into the area making it unsafe. Thus, the property was offered to Gail Johnson, rent free for 5 years.\n\nThe house was located on 23 Mitchell street and consisted of four bedrooms, one bathroom upstairs and one downstairs, a lounge, a room used as a dormitory, and a courtyard. Renovations and repairs were needed, in addition to furniture and basic house necessities. To raise funds for the completion of the house, Gail, a former public relations consultant, held a “kitchen tea” in which a wish list was created and publicized in the media and to those who were potential contributors. At the tea, Gail explained the reasons behind the creation of Nkosi's Haven. She explained that Nkosi's Haven was to offer a kibbutz style of living in which residents would take turns at cooking, cleaning, caring for the children, and helping care for each other as needed. Gail also told the guests that they already reached capacity for residence at the house. At the time, the house fit nine mothers and their children with presumed costs starting at R28,000 monthly. The results of the kitchen tea were successful and Gail managed to fundraise R45,000 worth of furniture and equipment, free electrical services, along with appliances valued at R30,000 for the nursery and dormitory.\n\nAfter 3 years in operation, Nkosi's Haven saw need for a second home as the Berea residence was rapidly expanding with mothers and children living with HIV/AIDS and orphans. In 2002, a separate plot of land was purchased south of Johannesburg. Upon development, both residences catered to over 160 residents and their children providing meals, medication, education, daily necessities for residents living with or without HIV/AIDS, and care for orphans.\n\nDue to financial restraints, in 2012, the NGO sold its property in Berea and moved all occupants to Nkosi's Haven Village in Alan Manor, South of Johannesburg. Costs for operating the Berea house were as much as R150,000 to R180,000 monthly, and because all occupants could be accommodated at the village, consolidating all residents meant that no one was left without a home or unemployed. Given that Nkosi's Haven village had bigger plans in motion, the shift would focus on different things such as training opportunities for residents at the newly purchased farm and other facilities at the Nkosi's Haven Village. These opportunities were unavailable in Berea due to space and financial constraints. Gail reasoned this by stating, “We are not the Titanic. We are consolidating... it makes sense to consolidate and save money.”\n\nNkosi's Haven grew so rapidly that in 2002, a second 2.5 acre plot of land was purchased in the Alan Manor neighbourhood, south of Johannesburg to begin the development of Nkosi's Haven Village. This was made possible through a grant from the Gauteng Department of Housing, Social Housing and Special Needs. Although the project received backlash from neighbouring communities, key sponsors made the initiative possible and enabled Nkosi's Haven to host up to 180 residents at a time.\n\nNkosi's Haven Village in Alan Manor consists of 17 cottages in which residents reside in, a sickbay, a library, a baby day care, a therapy block, a kitchen, a bakery, and a leisure block. There are also workshops and classrooms such as a computer room and an arts room for additional education and skill building. Administrative offices are located near the sickbay. All residents share household tasks and duties that help with the functioning of the village, providing a sense of leadership and empowerment to residents through skill-building activities.\n\nIn 2010, Nkosi's Haven received a bakery donation from the South African Whole Grain Bread Project (SAGWBP). In addition to the onsite bakery container donation, SAGWBP donated enough bread mix for six months. White and brown loaves are still produced for resident consumption and are marketed to other organizations in the community at a competitive price. The residents of Nkosi's Haven Village handle oversight of operations in the bakery. While providing skill-building opportunities, those who assist can also earn a stipend for their labour.\n\nIn 2009, Nkosi's Haven opened their new, industrial sized kitchen through generous donations from sponsors. Employed resident mothers cook to feed the large number of residents, serving hundreds of meals daily. Three well-balanced meals are provided to every resident mother, all children and volunteers.\n\nA leisure room was built at Nkosi's Haven Village to further meet the requirements to become a Child Care Centre in South Africa. Dance lessons take place in the large space on a weekly basis. The room is painted with bright colours, equipped with colourful furniture, bean bag chairs, a television, and foosball tables.\n\nNkosi's Haven Library provides a space for resident children to do their homework, leisurely reading, art sessions and workshops. Workshops include HIV/AIDS awareness sessions and life skills for women. Through funding, the Haven continues to expand its book collection in hopes to educate and provide relevant resources for residents.\n\nA recent project development was the implementation of bibliotherapy at the library in 2010. The bibliotherapy program used books to assist the children and young teens with coping with social, emotional and physical issues. The program identified the children's needs and used a variety of programs to help eradicate the isolation of people living with HIV/AIDS through education, self-awareness, skill development, and recreation. Some sessions included storytelling, reading aloud, and \"The Wall of Happiness Sessions\" in which participants openly discussed what made them happy with the facilitator. The program was made available to all residents at Nkosi's Haven, including orphans and those who were free of HIV/AIDS in order to increase overall awareness. Results of the program showed success among residents, with 92% of young participants demonstrating increased knowledge and understanding about HIV/AIDS related issues, 78% demonstrating an increase in comfort and decrease in fear around HIV/AIDS related issues, and 91% of adults demonstrating an improvement in self-esteem and optimistic attitudes.\n\nA music and arts centre has been built onsite for resident use. This centre includes a keyboard and drums and is also a space that is used by the choir. Due to limited space and resources, Nkosi's Haven relies on the help of volunteers to teach music and arts lessons onsite. Programs are run year-round for mothers and their children.\n\nAn ongoing workshop is artsINSIDEOUT's annual two-week art camp that runs at Nkosi's Haven. With the help of ASTEP (Artists Striving to End Poverty), a unique art program featuring successful and competitive professionals from all areas of art including students and graduates from Juilliard, ASTEP volunteer artists use storytelling, singing, acting, dancing and visual arts to inspire the youth at Nkosi's Haven to build community, consciousness and self-awareness. To date, artsINSIDEOUT has organized trips to Nkosi's Haven for 6 years and has provided funding for year-round programming, including workshops and cultural programs. Local artists are invited to join the diverse artsINSIDEOUT team to enhance cultural programs available to Nkosi's Haven.\n\nIn 2006, Nkosi's Haven Village completed the Sickbay, consisting of a 9-bed unit, qualified nursing staff, and family physicians to help care for HIV positive residents. Many residents are sick due to AIDS related illnesses and are being administered quality treatment and ARV's as needed.\n\nAll children at Nkosi's Haven Village receive various forms of therapy, including remedial, play, occupational and speech. The therapy block was completed in December 2009 and employs two full-time therapists onsite to assist residents living with HIV/AIDS. Therapists also assist with helping resident mothers disclose their HIV status to their children.\n\nNkosi's Haven Village has one classroom and two workshops that act as a site for homework and afterschool programs. These areas offer skill-building workshops on a regular basis for residents to learn various skills such as knitting, pottery, resume writing, and more.\n\nWith the help of BT South Africa, an ICT infrastructure company, Nkosi's Haven opened a communications centre in 2010. The room comes equipped with computer monitors valued at R100,000 which allows the youth at Nkosi's Haven to videoconference and chat with their mentors and friends worldwide. Infinite Family, an American organization that enables children living with HIV/AIDS to connect with mentors worldwide, has provided more than 50 youth at Nkosi's Haven with mentors.\n\nThis room is also used to assist resident youth with their homework.\n\nNkosi's Haven 4Life Farm, located 50 km south of Johannesburg, has been in operation since 2008. The farm is 12 acres and currently provides crops for meals at Nkosi's Haven Village. Foundation 4Life has helped support Nkosi's Haven for years and made contributions that helped begin the production of the farm.\n\nWithin the next 5 years of full operation, Nkosi's Haven 4LIFE farm aims to implement a self-sustaining style of living in which mothers living with HIV/AIDS and their children from a neighbouring township and the surrounding area will assist in growing organic food to serve themselves, Nkosi's Haven's residents, and the local markets and supermarkets. The goal is to produce crops for sale in supermarket chains in South Africa once the farm is sustainable and crops are of good standard. A number of new cottages will be built on the farm to accommodate more mothers and their children in need of Nkosi's Haven's assistance.\n\nNkosi's Haven has begun construction on the premises to begin early interventions of child development, skill transfer and various forms of therapy. Given that children in South Africa are held back from attending school if they are undergoing forms of therapy, an onsite preschool bypasses those restrictions and enables youth to develop their skills while receiving therapy. Nkosi's Haven pays R1000 monthly for each toddler to attend preschool outside of Nkosi's Haven, thus, an onsite preschool will save financial resources and assist Nkosi's Haven in becoming self-sufficient and sustainable. Nkosi's Haven plans to hire certified teachers in addition to training resident mothers to assist at the preschool, also building their skills and capacity.\n\n"}
{"id": "49183081", "url": "https://en.wikipedia.org/wiki?curid=49183081", "title": "Occupational skin diseases", "text": "Occupational skin diseases\n\nOccupational skin diseases are ranked among the top five occupational diseases in many countries.\n\nContact Dermatitis due to irritation is inflammation of the skin which results from a contact with an irritant. It has been observed that this type of dermatitis does not require prior sensitization of the immune system. There have been studies to support that past or present atopic dermatitis is a risk factor for this type of dermatitis. Common irritants include detergents, acids, alkalies, oils, organic solvents and reducing agents.\n\nThe acute form of this dermatitis develops on exposure of the skin to a strong irritant or caustic chemical. This exposure can occur as a result of accident at a workplace . The irritant reaction starts to increase in its intensity within minutes to hours of exposure to the irritant and reaches its peak quickly. After the reaction has reached its peak level, it starts to heal. This process is known as decrescendo phenomenon. The most frequent potent irritants leading to this type of dermatitis are acids and alkaline solutions. The symptoms include redness and swelling of the skin along with the formation of blisters.\n\nThe chronic form occurs as a result of repeated exposure of the skin to weak irritants over long periods of time.\n\nClinical manifestations of the contact dermatitis are also modified by external factors such as environmental factors (mechanical pressure, temperature, and humidity) and predisposing characteristics of the individual (age, sex, ethnic origin, preexisting skin disease, atopic skin diathesis, and anatomic region exposed.\n\nAnother occupational skin disease is glove-related hand urticaria, believed to be caused by repeated wearing and removal of the gloves. It has been reported as an occupational problem among the health care workers. The reaction is caused by the latex or the nitrile present in the gloves.\n\nPrevention measures include avoidance of the irritant through its removal from the workplace or through technical shielding by the use of potent irritants in closed systems or automation, irritant replacement or removal and personal protection of the workers. Low quality evidence exists for the effectiveness of certain therapies and their ability to specifically prevent hand skin irritation in the workplace. The limited evidence that does exist suggests that moisturizers used alone or in combination with a barrier cream can result in a clinically beneficial effect in the long or short-term primary prevention of occupational irritant hand dermatitis. \n"}
{"id": "28147113", "url": "https://en.wikipedia.org/wiki?curid=28147113", "title": "Parasitic nutrition", "text": "Parasitic nutrition\n\nParasitic nutrition is a mode of heterotrophic nutrition where a parasitic organism lives on the body surface or inside the body of another type of organism (a host) and gets nutrition directly from the body of the host. Since these parasites derive nourishment from their host, this symbiotic interaction is often harmful to the host. Parasites depend on their host for survival, since the host provides nutrition and protection. As a result of this dependence, parasites have considerable modifications to optimise parasitic nutrition and therefore their survival.\n\nParasites are divided into two groups: \"endoparasites\" and \"ectoparasites\". Endoparasites are parasites that live inside the body of the host, whereas ectoparasites are parasites that live on the outer surface of the host and generally attach themselves during feeding. Due to the different strategies of endoparasites and ectoparasites, they require different adaptations to derive nutrients from their host.\n\nParasites require nutrients to carry out essential functions including reproduction and growth. Essentially, the nutrients required from the host are carbohydrates, amino acids and lipids. Carbohydrates are utilised to generate energy, whilst amino acids and fatty acids are involved in the synthesis of macromolecules and the production of eggs. Most parasites are heterotrophs, so they therefore are unable to synthesise their own 'food' i.e. organic compounds and must acquire these from their host.\n\n\"Endoparasites\" live inside the body of the host. This group includes helminths, trematodes and cestodes. Endoparasites are two groups of parasites: intercellular and intracellular parasites. Intercellular parasites live in spaces within the host e.g. the alimentary canal, whereas intracellular parasites live in cells within the host e.g. erythrocytes. Intracellular parasites typically rely on a third organism, a vector, to transmit the parasite between hosts. Rather than requiring adaptations to penetrate the host, as ectoparasites do, endoparasites are in a nutrient-rich location so they instead have adaptations to maximise nutrient absorption. Endoparasites have a readily available and renewable supply of nutrients inside the host, which in some cases is pre-digested by the host, so mechanisms of nutrient absorption across their body surface is a common feature. As part of their life cycle strategy, endoparasites must also be able to transmit from within the host body and survive the hostile environment within the host. Only by achieving this can they benefit from acquiring nutrition in this way.\n\nEndoparasites have various anatomical and biochemical adaptations, typically at the host-parasite interface, to maximise nutrient acquisition. One such adaptation is the tegument, a metabolically active external cover that plays an important role in nutrient extraction from the host. The parasite tegument is permeable to various organic solutes and has transporters for the facilitated or active uptake of nutrients. Various studies have attempted to characterise these transporters in a number of parasites e.g. the amino acid transporter molecules in protozoa. Cestodes do not have a gut so the tegument is therefore critical for nutrient uptake. In cestodes the tegument is highly efficient with spine-like microtriches, similar to microvilli, to increase the surface area available for nutrient acquisition. In many parasites the tegument structure has folds or microvilli to maximise the surface area available for diffusion and uptake of nutrients. The tegument also commonly has additional organelles and features with important functions in metabolism including the glycocalyx. The glycocalyx is a carbohydrate-rich layer that enhances nutrient absorption and secretes enzymes to aid primary digestion.\n\nAnother important adaptation of endoparasites is the gut, which digests host macromolecules into soluble utilisable products. This feature is particularly important for endoparasites that are not in the alimentary canal, because nutrients are not pre-digested by the host. The gut lining typically has a layer of endodermal cells that secrete proteolytic enzymes to aid digestion. Some endoparasites have both a gut and anus, some lack an anus and some have neither—i.e., those that live in the alimentary canal, which instead diffuse pre-digested host nutrients across their body surface. The relative importance of the tegument, gut and other adaptations involved in nutrient acquisition varies between different endoparasitic species.\n\nTapeworms are endoparasites with numerous adaptations to enhance parasitic nutrition. Tapeworms live in the small intestine of humans, providing an ideal location to access a readily available, rich source of pre-digested nutrients. Since nutrients in the small intestine are plentiful and pre-digested by the host, tapeworms do not require a gut and instead have adaptations to maximise nutrient absorption. Tapeworms have a tegument that they use to absorb nutrients directly from the host small intestine by diffusion. They also have anatomical adaptations in the form of a scolex with hookers and suckers that they use to attach to the host small intestine wall, preventing the tapeworm from being egested following peristalsis. Tapeworms have a flattened body with microtriches to maximise the surface area available for nutrient absorption and they additionally have various transporter molecules. Tapeworms have to compete with the host epithelium for nutrients, so it is essential that they compete more efficiently for nutrients. They also secrete enzymes to enhance host digestive enzymes e.g. pancreatic α-amylase.\n\nSchistosomes, another type of endoparasite, also live inside the body of the host but instead these parasites acquire their nutrients from host blood. Schistosomes are in direct contact with host blood, a rich source of amino acids, and they therefore do not require penetrative structures to reach host nutrients. Schistosomes take blood up through the negative pressure created by muscle contractions of their sucker and oesophagus. They obtain amino acids from host blood through a mechanism of haemoglobin degradation, which remains unresolved but is suggested to involve a series of proteases. Mechanisms to overcome blood clotting are also employed. Various studies have attempted to characterise the components of the schistosome tegument, including transporter molecules that may be involved in nutrient uptake. Such transporter molecules include schistosome alkaline phosphatase (SmAP) and cathepsin B, which may be important in nutrient acquisition.\n\nMalaria, caused by the apicomplexan parasite \"Plasmodium falciparum\", is an intracellular endoparasite. This parasite relies on a third organism, in the form of an \"Anopheles\" mosquito vector. The host blood provides an ideal rich source of glucose and amino acids to the parasite, particularly during blood stage infection where \"Plasmodium\" infects erythrocytes. To acquire essential nutrients \"Plasmodium\" has to compete with both the vertebrate and insect host and therefore must be highly efficient, regulating uptake according to nutrient availability. \"Plasmodium\", along with many other endoparasitic parasites, have numerous channels in their parasitophorous vacuole membrane rendering it permeable to organic solutes to allow the uptake of necessary nutrients. The \"Plasmodium falciparum\" hexose transporter (PfHT) is such a transporter, which is critical for the uptake of glucose and fructose and therefore survival of the parasite. These organic molecules have to cross three membranes altogether; the plasma membrane of the erythrocyte, the parasitophorous vacuole membrane and the \"Plasmodium\" plasma membrane, facilitated by transporters such as PfHT.\n\nEctoparasites live on the outer surface of the host. This group includes ticks, leeches, mites and the tsetse fly. Ectoparasites do not have a readily available source of nutrients available on the outer surface of the host, so they need adaptations to access host nutrients. This requires penetrating features they can insert into the host, as well as the ability to secrete digestive enzymes and the presence of a gut to digest host-derived nutrients. Ectoparasites also have a variety of parasite transporters and permeases to enable them to acquire nutrition from their host, across numerous membranes. Many ectoparasites are known pathogen vectors, so they transmit these pathogens during nutrient acquisition.\n\nTsetse flies, the insect vector of Trypanosoma brucei, the causative agent of African trypanosomiasis are an example of an ectoparasite. These insects have a specialised structure, a \"proboscis\", that they use to pierce and draw nutrients from their host. These then employ transport proteins to transport the essential nutrients across membranes, ultimately from the host to the tsetse fly gut for digestion. Various permeases have been characterised including those that import hexoses, carboxylates, and amino acids.\n\nAnother ectoparasite is scabies, caused by \"Sarcoptes scabiei\". Scabies, transmitted by female mites, depends on nutrition from its host for survival. This ectoparasite obtains nutrients by burrowing into the skin of the host. Studies have also identified the presence of scabies mite inactivated protease paralogues (SMIPPs), which are believed to compete with host proteases.\n\nParasitic nutrition is beneficial to the parasite but typically detrimental to the host since it deprives the host of nutrients. This mode of nutrition has numerous side effects on the host including weight loss, anaemia, obstruction of the intestine, damage to the host intestinal wall and in some cases transmission of serious pathogens—e.g., the ectoparasitic tsetse fly, which transmits African trypanosomiasis.\n\nNutrient acquisition is an important component of parasite pathogenesis since it is critical to parasite survival. Understanding the mechanisms of parasite nutrient acquisition therefore identifies novel targets we can exploit as a form of parasite control—e.g., through knock-out or inhibition of crucial transporters or destruction of penetrative anatomical features. Several studies have looked into nutrient acquisition as a method of parasite control, including the development of vaccines against helminth parasites by targeting digestive proteases.\n\nPlants are typically autotrophic organisms meaning that they synthesise their own 'food' from inorganic compounds by photosynthesis. Some plants however are unable to synthesise their own 'food' by photosynthesis and therefore acquire nutrients by parasitic nutrition from other living plants. Plants that acquire nutrients in this way are called \"parasitic plants\". These plants have modified root structures called \"haustorium\" that they use to penetrate the vascular bundle of host plants and, essentially, 'steal' nutrients from host plants.\n\nParasitic plants include Dodder, \"Rafflesia\" and Broomrape.\n\n"}
{"id": "38719870", "url": "https://en.wikipedia.org/wiki?curid=38719870", "title": "Patient-Reported Outcomes Measurement Information System", "text": "Patient-Reported Outcomes Measurement Information System\n\nThe Patient-Reported Outcomes Measurement Information System (PROMIS) provides clinicians and researchers access to reliable, valid, and flexible measures of health status that assess physical, mental, and social well–being from the patient perspective. PROMIS measures are standardized, allowing for assessment of many patient-reported outcome domains—including pain, fatigue, emotional distress, physical functioning and social role participation—based on common metrics that allow for comparisons across domains, across chronic diseases, and with the general population. Further, PROMIS tools allow for computer adaptive testing, efficiently achieving precise measurement of health status domains with few items. There are PROMIS measures for both adults and children. PROMIS was established in 2004 with funding from the National Institutes of Health (NIH) as one of the initiatives of the NIH Roadmap for Medical Research.\n\nThe NIH established the Roadmap for Medical Research in 2004 to identify major opportunities for medical research and the development of new scientific expertise and technology that would lead to tangible benefits for patients. One of the programs within the Roadmap, Re-engineering the Clinical Research Enterprise, called for developing rigorous and systematic infrastructure for clinical research and for translating scientific discoveries into practical applications or tools that can be used by healthcare providers. PROMIS is one initiative within this program.\nThe PROMIS initiative develops and evaluates standard measures for key patient-reported health indicators and symptoms. Patient-reported measures such as pain, fatigue, emotional distress, and physical functioning complement clinical measures (e.g., x-rays and lab tests) by providing healthcare providers with information about what patients are able to do and how they feel.\n\nPROMIS has worked to unify the field of patient-reported outcome (PRO) measurement through the promotion of a common, systematic measurement system broadly applicable across clinical research. PROMIS measures are intended to assess the most common or salient dimensions of patient–relevant outcomes for the widest possible range of chronic disorders and diseases, thus they are \"generic\" measures vs. specific to given disease or condition. Structured as a multi-institutional collaboration with NIH, PROMIS has advanced the consensus process within the field of PRO measurement through the involvement of the funded research collaborative in establishing a rigorous, systematic infrastructure for measure development and psychometric evaluation.\n\nPROMIS takes advantage of developments in technology, as well as advances in the sciences of psychometric, qualitative, cognitive, and health survey research, to create new models and methods for collecting PROs for use in clinical research and evaluation of medical care. PROMIS incorporates and translates cutting-edge science into practical, easy to use tools for clinicians: For example, PROMIS implements Computer Adaptive Test (CAT) software which tailors the PRO assessment to the individual patient by selecting the most informative set of questions based on responses to previous questions. CAT questionnaires allow an accurate measurement of health status using the fewest possible questions.\n\nIn November 2012, the PROMIS network held it first international strategy meeting with organizational partners from 8 European countries, China and Canada to develop a strategic action plan for the international spread of PROMIS.\n\nIn early 2013, PROMIS unveiled new materials to expand its outreach to researchers and clinicians: the PROMIS e-newsletter and two instructional videos series about PROMIS and Item Response Theory.\n\nIn 2016, an updated PROMIS website at www.HealthMeasures.net was created to provide more information about measure selection, data collection tools, score calculation, score interpretation, item response theory, and support an online forum for posting questions to the PROMIS user community.\n\nThe PROMIS initiative is fulfilled by a network of primary research sites and coordinating centers that collaborate to develop the items and tools to measure PROs, and to evaluate the reliability and validity of these measures.\nBetween 2004 and 2009, PROMIS consisted of a Statistical Coordinating Center, located at Evanston Northwestern Healthcare, and six research sites located at Duke University, University of North Carolina at Chapel Hill, University of Pittsburgh, Stanford University, Stony Brook University, and University of Washington. In 2010, NIH renewed funding for PROMIS and expanded the program to six additional research sites: Children's Hospital of Philadelphia; Boston University / University of Michigan, Ann Arbor; University of California, Los Angeles; Georgetown University; Children's Hospital Medical Center, Cincinnati; and University of Maryland, Baltimore. PROMIS also added a Network Center, operated by the American Institutes for Research, Washington DC as well as a Statistical Center and a Technology Center, both operated by Northwestern University. These centers provided logistical and technical support to PROMIS. \n\nIn September 2014, the NIH extended its support to PROMIS through funding the National Person Centered Assessment Resource (PCAR/HealthMeasures). Three other measurement systems, Quality of Life in Neurologic Disorders (Neuro-QoL), Adult Sickle Cell Quality of Life Measurement system (ASCQ-Me), and the NIH Toolbox for the Assessment of Neurological and Behavioral Function (NIH Toolbox) are also supported through HealthMeasures. HealthMeasures aims to facilitate the dissemination, implementation, and self-sustainability of these four measurement systems. The HealthMeasures grant was awarded to Northwestern University with additional sites at American Institutes for Research, University of California, Los Angeles, University of California, San Diego, University of North Carolina, Chapel Hill, and University of Pittsburgh.\n\nPROMIS uses measurement science to create a state-of-the-science assessment system for self–reported health.\n\nPROMIS has self-reported health measures in the domains of physical health, mental health and social health for adult self-reported and pediatric-self and proxy-reported health.\n\nUnder each main domain (physical health, mental health, social health) are sub-domains associated with symptoms, function, affect, behavior, cognition, relationships or function. The sub-domains developed as of November 2016 are listed below. Domains that are “PROMIS Profile Domains” are included in either PROMIS Adult Profile Instruments (PROMIS-29, PROMIS-43, PROMIS-57) and Pediatric or Parent Proxy Profile Instruments (PROMIS Pediatric/Parent Proxy 25, PROMIS Pediatric/Parent Proxy 37, PROMIS Pediatric/Parent Proxy 49). There are also Sexual Function and Satisfaction Profiles for adults.\n\n\n\n\n"}
{"id": "414219", "url": "https://en.wikipedia.org/wiki?curid=414219", "title": "Penile fracture", "text": "Penile fracture\n\nPenile fracture is rupture of one or both of the \"tunica albuginea\", the fibrous coverings that envelop the penis's \"corpora cavernosa\". It is caused by rapid blunt force to an erect penis, usually during vaginal intercourse, or aggressive masturbation. It sometimes also involves partial or complete rupture of the urethra or injury to the dorsal nerves, veins and arteries.\n\nA popping or cracking sound, significant pain, swelling, immediate loss of erection leading to flaccidity, and skin hematoma of various sizes are commonly associated with the sexual event.\n\nPenile fracture is a relatively uncommon clinical condition. Vaginal intercourse and aggressive masturbation are the most common causes. A 2014 study of accident and emergency records at three hospitals in Campinas, Brazil, showed that woman on top positions caused the greatest risk with the missionary position being the safest. The research conjectured that when the receptive partner is on top, they usually control the movement and are not able to interrupt movement when the penis suffers a misaligned penetration. Conversely, when the penetrative partner is controlling the movement, they have better chances of stopping in response to pain from misalignment, minimizing harm.\n\nThe practice of \"taqaandan\" (also \"taghaandan\") also puts men at risk of penile fracture. Taqaandan, which comes from a Kurdish word meaning \"to click\", involves bending the top part of the erect penis while holding the lower part of the shaft in place, until a click is heard and felt. Taqaandan is said to be painless and has been compared to cracking one's knuckles, but the practice of taqaandan has led to an increase in the prevalence of penile fractures in western Iran. Taqaandan may be performed to achieve detumescence.\n\nUltrasound examination is able to depict the tunica albuginea tear in the majority of cases (as a hypoechoic discontinuity in the normally echogenic tunica). In a study on 25 patients, Zare Mehrjardi et al. concluded that ultrasound is unable to find the tear just when it is located at the penile base. In their study magnetic resonance imaging (MRI) accurately diagnosed all of the tears (as a discontinuity in the normally low signal tunica on both T1- and T2-weighted sequences). They concluded that ultrasound should be considered as the initial imaging method, and MRI can be helpful in cases that ultrasound does not depict any tear but clinical suspicious for fracture is still high. In the same study, authors investigated accuracy of ultrasound and MRI for determining the tear location (mapping of fracture) in order to perform a tailored surgical repair. MRI was more accurate than ultrasound for this purpose, but ultrasound mapping was well correlated with surgical results in cases where the tear was clearly visualized on ultrasound exam.\n\nPenile fracture is a medical emergency, and emergency surgical repair is the usual treatment. Delay in seeking treatment increases the complication rate. Non-surgical approaches result in 10–50% complication rates including erectile dysfunction, permanent penile curvature, damage to the urethra and pain during sexual intercourse, while operatively treated patients experience an 11% complication rate.\n\nIn some cases, retrograde urethrogram may be performed to rule out concurrent urethral injury.\nIn the United States, the case of \"Doe v. Moe\", 63 Mass. App. Ct. 516, 827 N.E.2d 240 (2005), tested liability for a penile fracture injury caused during sexual intercourse. The court declined to find duty as between two consensual adults. The plaintiff in this case, a man who suffered a fractured penis, complained that the defendant, his ex-girlfriend, had caused his injury while she was on top of him during sexual intercourse. The court ruled in her favor, determining that her conduct was neither legally wanton nor reckless.\n"}
{"id": "5708805", "url": "https://en.wikipedia.org/wiki?curid=5708805", "title": "Peptide YY", "text": "Peptide YY\n\nPeptide YY (PYY) also known as peptide tyrosine tyrosine is a peptide that in humans is encoded by the PYY gene. Peptide YY is a short (36-amino acid) peptide released from cells in the ileum and colon in response to feeding. In the blood, gut, and other elements of periphery, PYY acts to reduce appetite; similarly, when injected directly into the central nervous system, PYY is also anorexigenic, i.e., it reduces appetite.\n\nDietary fibers from fruits, vegetables, and whole grains, consumed, increase the speed of transit of intestinal chyme into the ileum, to raise PYY, and induce satiety. Peptide YY can be produced as the result of enzymatic breakdown of crude fish proteins and ingested as a food product. \n\nPeptide YY is related to the pancreatic peptide family by having 18 of its 36 amino acids located in the same positions as pancreatic peptide. The two major forms of peptide YY are PYY and PYY, which have PP fold structural motifs. However, the most common form of circulating PYY immunoreactivity is PYY, which binds to the Y receptor (Y2R) of the Y family of receptors. Peptide YY (PYY) is a linear polypeptide consisting of 34 amino acids with structural homology to NPY and pancreatic polypeptide.\n\nPYY is found in L cells in the mucosa of gastrointestinal tract, especially in ileum and colon. Also, a small amount of PYY, about 1-10%, is found in the esophagus, stomach, duodenum and jejunum. PYY concentration in the circulation increases postprandially (after food ingestion) and decreases by fasting. In addition, PYY is produced by a discrete population of neurons in the brainstem, specifically localized to the gigantocellular reticular nucleus of the medulla oblongata. C. R. Gustavsen\" et al.\" had found PYY-producing cells located in the islets of Langerhans in rats. They were observed either alone or co-localized with glucagon or PP.\n\nPYY exerts its action through NPY receptors; it inhibits gastric motility and increases water and electrolyte absorption in the colon. PYY may also suppress pancreatic secretion. It is secreted by the neuroendocrine cells in the ileum and colon in response to a meal, and has been shown to reduce appetite. PYY works by slowing the gastric emptying; hence, it increases efficiency of digestion and nutrient absorption after a meal. Research has also indicated PYY may be useful in removing aluminium accumulated in the brain.\n\nSeveral studies have shown acute peripheral administration of PYY inhibits feeding of rodents and primates. Other studies on Y2R-knockout mice have shown no anorectic effect on them. These findings indicate PYY has an anorectic (losing appetite) effect, which is suggested to be mediated by Y2R. PYY-knockout female mice increase in body weight and fat mass. PYY-knockout mice, on the other hand, are resistant to obesity, but have higher fat mass and lower glucose tolerance when fed a high-fat diet, compared to control mice. Thus, PYY also plays a very important role in energy homeostasis by balancing food intake. PYY oral spray was found to promote fullness. Viral gene therapy of the salivary glands resulted in long-term intake reduction.\n\nLeptin also reduces appetite in response to feeding, but obese people develop a resistance to leptin. Obese people secrete less PYY than non-obese people, and attempts to use PYY directly as a weight-loss drug have met with some success. Researchers noted the caloric intake during a buffet lunch offered two hours after the infusion of PYY was decreased by 30% in obese subjects (P<0.001) and 31% in lean subjects (P<0.001).\n\nWhile some studies have shown obese persons have lower circulating level of PYY postprandially, other studies have reported they have normal sensitivity to the anorectic effect of PYY. Thus, reduction in PYY sensitivity may not be one of the causes of obesity, in contrast to the reduction of leptin sensitivity. The anorectic effect of PYY could possibly be a future obesity drug.\n\nThe consumption of protein boosts PYY levels, so some benefit was observed in experimental subjects in reducing hunger and promoting weight loss. This could partially explain the weight-loss experienced with high-protein diets, but the high thermic effect of protein appears to be the leading cause.\n\nObese patients undergoing gastric bypass showed marked metabolic adaptations, resulting in frequent diabetes remission 1 year later. When the confounding of calorie restriction is factored out, β-cell function improves rapidly, very possibly under the influence of enhanced GLP-1 responsiveness. Insulin sensitivity improves in proportion to weight loss, with a possible involvement of PYY.\n\n"}
{"id": "240096", "url": "https://en.wikipedia.org/wiki?curid=240096", "title": "Photokeratitis", "text": "Photokeratitis\n\nPhotokeratitis or ultraviolet keratitis is a painful eye condition caused by exposure of insufficiently protected eyes to the ultraviolet (UV) rays from either natural (e.g. intense sunlight) or artificial (e.g. the electric arc during welding) sources. Photokeratitis is akin to a sunburn of the cornea and conjunctiva, and is not usually noticed until several hours after exposure. Symptoms include increased tears and a feeling of pain, likened to having sand in the eyes.\n\nThe injury may be prevented by wearing eye protection that blocks most of the ultraviolet radiation, such as welding goggles with the proper filters, a welder's helmet, sunglasses rated for sufficient UV protection, or appropriate snow goggles. The condition is usually managed by removal from the source of ultraviolet radiation, covering the corneas, and administration of pain relief. Photokeratitis is known by a number of different terms including: snow blindness, arc eye, welder's flash, bake eyes, corneal flash burns, sand man's eye, flash burns, niphablepsia, potato eye, or keratoconjunctivitis photoelectrica.\n\nCommon symptoms include pain, intense tears, eyelid twitching, discomfort from bright light, and constricted pupils.\n\nAny intense exposure to UV light can lead to photokeratitis. Common causes include welders who have failed to use adequate eye protection such as an appropriate welding helmet or welding goggles. This is termed \"arc eye\", while photokeratitis caused by exposure to sunlight reflected from ice and snow, particularly at elevation, is commonly called \"snow blindness\". It can also occur due to using tanning beds without proper eyewear. Natural sources include bright sunlight reflected from snow or ice or, less commonly, from sea or sand. Fresh snow reflects about 80% of the UV radiation compared to a dry, sandy beach (15%) or sea foam (25%). This is especially a problem in polar regions and at high altitudes, as with every thousand feet (approximately 305 meters) of elevation (above sea level), the intensity of UV rays increases by four percent.\n\nFluorescein dye staining will reveal punctate areas of uptake under ultraviolet light.\n\nPhotokeratitis can be prevented by using sunglasses or eye protection that transmits 5–10% of visible light and absorbs almost all UV rays. Additionally, these glasses should have large lenses and side shields to avoid incidental light exposure. Sunglasses should always be worn, even when the sky is overcast, as UV rays can pass through clouds.\n\nThe Inuit, Yupik, and other Arctic peoples carved snow goggles from materials such as driftwood or caribou antlers to help prevent snow blindness. Curved to fit the user's face with a large groove cut in the back to allow for the nose, the goggles allowed in a small amount of light through a long thin slit cut along their length. The goggles were held to the head by a cord made of caribou sinew.\n\nIn the event of missing sunglass lenses, emergency lenses can be made by cutting slits in dark fabric or tape folded back onto itself. The \"SAS Survival Guide\" recommends blackening the skin underneath the eyes with charcoal (as the ancient Egyptians did) to avoid any further reflection.\n\nThe pain may be temporarily alleviated with anaesthetic eye drops for the examination; however, they are not used for continued treatment, as anaesthesia of the eye interferes with corneal healing, and may lead to corneal ulceration and even loss of the eye. Cool, wet compresses over the eyes and artificial tears may help local symptoms when the feeling returns. Nonsteroidal anti-inflammatory drug (NSAID) eyedrops are widely used to lessen inflammation and eye pain, but have not been proven in rigorous trials. Systemic (oral) pain medication is given if discomfort is severe. Healing is usually rapid (24–72 hours) if the injury source is removed. Further injury should be avoided by isolation in a dark room, removing contact lenses, not rubbing the eyes, and wearing sunglasses until the symptoms improve.\n\n"}
{"id": "472231", "url": "https://en.wikipedia.org/wiki?curid=472231", "title": "Phytochemical", "text": "Phytochemical\n\nPhytochemicals are chemical compounds produced by plants, generally to help them thrive or thwart competitors, predators, or pathogens. The name comes . Some phytochemicals have been used as poisons and others as traditional medicine.\n\nAs a term, \"phytochemicals\" is generally used to describe plant compounds that are under research with unestablished effects on health and are not scientifically defined as essential nutrients. Regulatory agencies governing food labeling in Europe and the United States have provided guidance for industry limiting or preventing health claims about phytochemicals on food product or nutrition labels.\n\nPhytochemicals are chemicals of plant origin. Phytochemicals (from Greek \"phyto\", meaning \"plant\") are chemicals produced by plants through primary or secondary metabolism. They generally have biological activity in the plant host and play a role in plant growth or defense against competitors, pathogens, or predators.\n\nPhytochemicals generally are regarded as research compounds rather than essential nutrients because proof of their possible health effects has not been established yet. Phytochemicals under research can be classified into major categories, such as carotenoids and polyphenols, which include phenolic acids, flavonoids, and stilbenes/lignans. Flavonoids can be further divided into groups based on their similar chemical structure, such as anthocyanins, flavones, flavanones, and isoflavones, and flavanols. Flavanols further are classified as catechins, epicatechins, and proanthocyanidins.\n\nPhytochemists study phytochemicals by first extracting and isolating compounds from the origin plant, followed by defining their structure or testing in laboratory model systems, such as cell cultures, in vitro experiments, or in vivo studies using laboratory animals. Challenges in that field include isolating specific compounds and determining their structures, which are often complex, and identifying what specific phytochemical is primarily responsible for any given biological activity.\n\nWithout specific knowledge of their cellular actions or mechanisms, phytochemicals have been used as poison and in traditional medicine. For example, salicin, having anti-inflammatory and pain-relieving properties, was originally extracted from the bark of the white willow tree and later synthetically produced to become the common, over-the-counter drug, aspirin. The tropane alkaloids of \"A. belladonna\" were used as poisons, and early humans made poisonous arrows from the plant. In Ancient Rome, it was used as a poison by Agrippina the Younger, wife of Emperor Claudius on advice of Locusta, a lady specialized in poisons, and Livia, who is rumored to have used it to kill her husband Emperor Augustus.\n\nThe English yew tree was long known to be extremely and immediately toxic to animals that grazed on its leaves or children who ate its berries; however, in 1971, paclitaxel was isolated from it, subsequently becoming an important cancer drug.\n\nAs of 2017, the biological activities for most phytochemicals are unknown or poorly understood, in isolation or as part of foods. Phytochemicals with established roles in the body are classified as essential nutrients.\n\nThe phytochemical category includes compounds recognized as essential nutrients, which are naturally contained in plants and are required for normal physiological functions, so must be obtained from the diet in humans.\n\nSome phytochemicals are known phytotoxins that are toxic to humans; for example aristolochic acid is carcinogenic at low doses. Some phytochemicals are antinutrients that interfere with the absorption of nutrients. Others, such as some polyphenols and flavonoids, may be pro-oxidants in high ingested amounts.\n\nNondigestible dietary fibers from plant foods, often considered as a phytochemical, are now generally regarded as a nutrient group having approved health claims for reducing the risk of some types of cancer and coronary heart disease.\n\nEating a diet high in fruits, vegetables, grains, legumes and plant-based beverages has long-term health benefits, but there is no evidence that taking dietary supplements of non-nutrient phytochemicals extracted from plants similarly benefits health. Phytochemical supplements are neither recommended by health authorities for improving health nor approved by regulatory agencies for health claims on product labels.\n\nWhile health authorities encourage consumers to eat diets rich in fruit, vegetables, whole grains, legumes, and nuts to improve and maintain health, evidence that such effects result from specific, non-nutrient phytochemicals is limited or absent. For example, systematic reviews and/or meta-analyses indicate weak or no evidence for phytochemicals from plant food consumption having an effect on breast, lung, or bladder cancers. Further, in the United States, regulations exist to limit the language on product labels for how plant food consumption may affect cancers, excluding mention of any phytochemical except for those with established health benefits against cancer, such as dietary fiber, vitamin A, and vitamin C.\n\nPhytochemicals, such as polyphenols, have been specifically discouraged from food labeling in Europe and the United States because there is no evidence for a cause-and-effect relationship between dietary polyphenols and inhibition or prevention of any disease.\n\nAmong carotenoids such as the tomato phytochemical, lycopene, the US Food and Drug Administration found insufficient evidence for its effects on any of several cancer types, resulting in limited language for how products containing lycopene can be described on labels.\n\nPhytochemicals in freshly harvested plant foods may be degraded by processing techniques, including cooking. The main cause of phytochemical loss from cooking is thermal decomposition.\n\nA converse exists in the case of carotenoids, such as lycopene present in tomatoes, which may remain stable or increase in content from cooking due to liberation from cellular membranes in the cooked food. Food processing techniques like mechanical processing can also free carotenoids and other phytochemicals from the food matrix, increasing dietary intake.\n\nIn some cases, processing of food is necessary to remove phytotoxins or antinutrients; for example societies that use cassava as a staple have traditional practices that involve some processing (soaking, cooking, fermentation, etc.), which are necessary to avoid getting sick from cyanogenic glycosides present in unprocessed cassava.\n\n\n\n"}
{"id": "29146741", "url": "https://en.wikipedia.org/wiki?curid=29146741", "title": "Population Impact Measures", "text": "Population Impact Measures\n\nPopulation Impact Measures (PIMs) are biostatistical measures of risk and benefit used in epidemiological and public health research. They are used to describe the impact of health risks and benefits in a population, to inform health policy.\n\nFrequently used measures of risk and benefit identified by Jerkel, Katz and Elmore, describe measures of risk difference (attributable risk), rate difference (often expressed as the odds ratio or relative risk), Population Attributable Risk (PAR), and the relative risk reduction, which can be recalculated into a measure of \"absolute benefit\", called the Number needed to treat. Population Impact Measures are an extension of these statistics, as they are measures of absolute risk at the population level, which are calculations of number of people in the population who are at risk to be harmed, or who will benefit from Public Health interventions. \n\nThey are measures of absolute risk and benefit, producing numbers of people who will benefit from an intervention or be at risk from a risk factor within a particular local or national population. They provide local context to previous measures, allowing policy-makers to identify and prioritise the potential benefits of interventions on their own population. They are simple to compute, and contain the elements to which policy-makers would have to pay attention in the commissioning or improvement of services. They may have special relevance for local policy-making. They depend on the ability to obtain and use local data, and by being explicit about the data required may have the added benefit of encouraging the collection of such data.\n\nTo describe the impact of preventive and treatment interventions, the Number of Events Prevented in a Population (NEPP) is defined as \"the number of events prevented by the intervention in your population over a defined time period\". NEPP extends the well-known measure Number needed to treat (NNT) beyond the individual patient to the population. To describe the impact of a risk factor on causing ill health and disease the Population Impact Number of Eliminating a Risk factor (PIN-ER-t) is defined as \"the potential number of disease events prevented in a population over the next t years by eliminating a risk factor\". The PIN-ER-t extends the well-known Population Attributable Risk (PAR) to a particular population and relates it to disease incidence, converting the PAR from a measure of relative to absolute risk.\n\nThe components for the calculations are as follows: Population denominator (size of the population); Proportion of the population with the disease; Proportion of the population exposed to the risk factor or the incremental proportion of the diseased population eligible for the proposed intervention (the latter requires the actual or estimated proportion who are currently receiving the interventions ‘subtracted’ from best practice goal from guidelines or targets, adjusted for likely compliance with the intervention); Baseline risk – the probability of the outcome of interest in this or similar populations; and Relative Risk of outcome given exposure to a risk factor or Relative Risk Reduction associated with the intervention.\n\nThe formula is: NEPP=N*Pd*Pe*ru*RRR where: N = population size, Pd = prevalence of the disease, Pe = proportion eligible for treatment, ru = risk of the event of interest in the untreated group or baseline risk over appropriate time period (can be multiplied by life expectancy to produce life-years), RRR = relative risk reduction associated with treatment.\n\nIn order to reflect the incremental effect of changing from current to ‘best’ practice, and to adjust for levels of compliance, the proportion eligible for treatment, Pe, is (Pb-Pt)*Pc, where Pt is the proportion currently treated, Pb is the proportion that would be treated if best practice was adopted, and Pc is the proportion of the population who are compliant with the intervention.\n\nYou can calculate the NEPP and its confidence intervals at this web site from the Population Health Decision Support & Simulation site.\n\n[Note: Number Needed to Treat (NNT): 1/(Baseline risk x Relative Risk Reduction)]\n\nThe formula is: PIN-ER-t = N*Ip*PAR Where: N is the number of people in the population; Ip the baseline risk of the outcome of interest in the population as a whole; t is the time period over which the outcome is measured.\n\nThe PAR/F, Population Attributable Risk (or Fraction), is calculated for two or multiple strata. The basic formula to compute the PAR for dichotomous variables is PAR = Pe*(RR-1)/1+ Pe*(RR-1). Where: Pe is the prevalence of the population within each income stratum as the exposure, and RR is the prevalence of risk factors in each stratum relative to the highest income fifth. This is modified where there are multiple strata to: PAR = [Pe1(RR1-1)+Pe2(RR2-1)+Pe3(RR3-1)…]/[1+Pe1(RR1-1)+Pe2(RR2-1)+ Pe3(RR3-1)...].\nYou can calculate the PIN-ER-t and its confidence intervals at this web site from the Population Health Decision Support & Simulation site.\n"}
{"id": "463734", "url": "https://en.wikipedia.org/wiki?curid=463734", "title": "Public health", "text": "Public health\n\nPublic health is \"the science and art of preventing disease, prolonging life and promoting human health through organized efforts and informed choices of society, organizations, public and private, communities and individuals\". Analyzing the health of a population and the threats is the basis for public health. The \"public\" in question can be as small as a handful of people, an entire village or it can be as large as several continents, in the case of a pandemic. \"Health\" takes into account physical, mental and social well-being. It is not merely the absence of disease or infirmity, according to the World Health Organization. Public health is interdisciplinary. For example, epidemiology, biostatistics and health services are all relevant. Environmental health, community health, behavioral health, health economics, public policy, mental health and occupational safety, gender issues in health, sexual and reproductive health are other important subfields.\n\nPublic health aims to improve the quality of life through prevention and treatment of disease, including mental health. This is done through the surveillance of cases and health indicators, and through the promotion of healthy behaviors. Common public health initiatives include promoting handwashing and breastfeeding, delivery of vaccinations, suicide prevention and distribution of condoms to control the spread of sexually transmitted diseases.\n\nModern public health practice requires multidisciplinary teams of public health workers and professionals. Teams might include epidemiologists, biostatisticians, medical assistants, public health nurses, midwives, medical microbiologists, economists, sociologists, geneticists and data managers. Depending on the need environmental health officers or public health inspectors, bioethicists, and even veterinarians, gender experts, sexual and reproductive health specialists might be called on.\n\nAccess to health care and public health initiatives are difficult challenges in developing countries. Public health infrastructures are still forming in those countries.\n\nThe focus of a public health intervention is to prevent and manage diseases, injuries and other health conditions through surveillance of cases and the promotion of healthy behaviors, communities and environments. Many diseases are preventable through simple, nonmedical methods. For example, research has shown that the simple act of handwashing with soap can prevent the spread of many contagious diseases. In other cases, treating a disease or controlling a pathogen can be vital to preventing its spread to others, either during an outbreak of infectious disease or through contamination of food or water supplies. Public health communications programs, vaccination programs and distribution of condoms are examples of common preventive public health measures. Measures such as these have contributed greatly to the health of populations and increases in life expectancy.\n\nPublic health plays an important role in disease prevention efforts in both the developing world and in developed countries through local health systems and non-governmental organizations. The World Health Organization (WHO) is the international agency that coordinates and acts on global public health issues. Most countries have their own government public health agencies, sometimes known as ministries of health, to respond to domestic health issues. For example, in the United States, the front line of public health initiatives are state and local health departments. The United States Public Health Service (PHS), led by the Surgeon General of the United States, and the Centers for Disease Control and Prevention, headquartered in Atlanta, are involved with several international health activities, in addition to their national duties. In Canada, the Public Health Agency of Canada is the national agency responsible for public health, emergency preparedness and response, and infectious and chronic disease control and prevention. The Public health system in India is managed by the Ministry of Health & Family Welfare of the government of India with state-owned health care facilities.\n\nMost governments recognize the importance of public health programs in reducing the incidence of disease, disability, and the effects of aging and other physical and mental health conditions. However, public health generally receives significantly less government funding compared with medicine. Public health programs providing vaccinations have made strides in promoting health, including the eradication of smallpox, a disease that plagued humanity for thousands of years.\nThe World Health Organization (WHO) identifies core functions of public health programs including:\n\nIn particular, public health surveillance programs can:\n\nPublic health surveillance has led to the identification and prioritization of many public health issues facing the world today, including HIV/AIDS, diabetes, waterborne diseases, zoonotic diseases, and antibiotic resistance leading to the reemergence of infectious diseases such as tuberculosis. Antibiotic resistance, also known as drug resistance, was the theme of World Health Day 2011. Although the prioritization of pressing public health issues is important, Laurie Garrett argues that there are following consequences. When foreign aid is funnelled into disease-specific programs, the importance of public health in general is disregarded. This public health problem of stovepiping is thought to create a lack of funds to combat other existing diseases in a given country.\n\nFor example, the WHO reports that at least 220 million people worldwide suffer from diabetes. Its incidence is increasing rapidly, and it is projected that the number of diabetes deaths will double by the year 2030. In a June 2010 editorial in the medical journal \"The Lancet\", the authors opined that \"The fact that type 2 diabetes, a largely preventable disorder, has reached epidemic proportion is a public health humiliation.\" The risk of type 2 diabetes is closely linked with the growing problem of obesity. The WHO’s latest estimates as of June 2016 highlighted that globally approximately 1.9 billion adults were overweight in 2014, and 41 million children under the age of five were overweight in 2014. The United States is the leading country with 30.6% of its population being obese. Mexico follows behind with 24.2% and the United Kingdom with 23%. Once considered a problem in high-income countries, it is now on the rise in low-income countries, especially in urban settings. Many public health programs are increasingly dedicating attention and resources to the issue of obesity, with objectives to address the underlying causes including healthy diet and physical exercise.\n\nSome programs and policies associated with public health promotion and prevention can be controversial. One such example is programs focusing on the prevention of HIV transmission through safe sex campaigns and needle-exchange programmes. Another is the control of tobacco smoking. Changing smoking behavior requires long-term strategies, unlike the fight against communicable diseases, which usually takes a shorter period for effects to be observed. Many nations have implemented major initiatives to cut smoking, such as increased taxation and bans on smoking in some or all public places. Proponents argue by presenting evidence that smoking is one of the major killers, and that therefore governments have a duty to reduce the death rate, both through limiting passive (second-hand) smoking and by providing fewer opportunities for people to smoke. Opponents say that this undermines individual freedom and personal responsibility, and worry that the state may be emboldened to remove more and more choice in the name of better population health overall.\n\nSimultaneously, while communicable diseases have historically ranged uppermost as a global health priority, non-communicable diseases and the underlying behavior-related risk factors have been at the bottom. This is changing, however, as illustrated by the United Nations hosting its first General Assembly Special Summit on the issue of non-communicable diseases in September 2011.\n\nMany health problems are due to maladaptive personal behaviors. From an evolutionary psychology perspective, over consumption of novel substances that are harmful is due to the activation of an evolved reward system for substances such as drugs, tobacco, alcohol, refined salt, fat, and carbohydrates. New technologies such as modern transportation also cause reduced physical activity. Research has found that behavior is more effectively changed by taking evolutionary motivations into consideration instead of only presenting information about health effects. The marketing industry has long known the importance of associating products with high status and attractiveness to others. Films are increasingly being recognized as a public health tool. In fact, film festivals and competitions have been established to specifically promote films about health. Conversely, it has been argued that emphasizing the harmful and undesirable effects of tobacco smoking on other persons and imposing smoking bans in public places have been particularly effective in reducing tobacco smoking.\n\nAs well as seeking to improve population health through the implementation of specific population-level interventions, public health contributes to medical care by identifying and assessing population needs for health care services, including:\n\nTo improve public health, one important strategy is to promote modern medicine and scientific neutrality to drive the public health policy and campaign, which is recommended by Armanda Solorzana, through a case study of the Rockefeller Foundation's hookworm campaign in Mexico in the 1920s. Soloranza argues that public health policy can't concern only politics or economics. Political concerns can lead government officials to hide the real numbers of people affected by disease in their regions, such as upcoming elections. Therefore, scientific neutrality in making public health policy is critical; it can ensure treatment needs are met regardless of political and economic conditions.\n\nThe history of public health care clearly shows the global effort to improve health care for all. However, in modern-day medicine, real, measurable change has not been clearly seen, and critics argue that this lack of improvement is due to ineffective methods that are being implemented. As argued by Paul E. Farmer, structural interventions could possibly have a large impact, and yet there are numerous problems as to why this strategy has yet to be incorporated into the health system. One of the main reasons that he suggests could be the fact that physicians are not properly trained to carry out structural interventions, meaning that the ground level health care professionals cannot implement these improvements. While structural interventions can not be the only area for improvement, the lack of coordination between socioeconomic factors and health care for the poor could be counterproductive, and end up causing greater inequity between the health care services received by the rich and by the poor. Unless health care is no longer treated as a commodity, global public health will ultimately not be achieved. This being the case, without changing the way in which health care is delivered to those who have less access to it, the universal goal of public health care cannot be achieved.\n\nAnother reason why measurable changes may not be noticed in public health is because agencies themselves may not be measuring their programs' efficacy. Perrault et al. analyzed over 4,000 published objectives from Community Health Improvement Plans (CHIPs) of 280 local accredited and non-accredited public health agencies in the U.S., and found that the majority of objectives - around two-thirds - were focused on achieving agency outputs (e.g., developing communication plans, installing sidewalks, disseminating data to the community). Only about one-third focused on seeking measurable changes in the populations they serve (i.e., changing people's knowledge, attitudes, behaviors). What this research showcases is that if agencies are only focused on accomplishing tasks (i.e., outputs) and do not have a focus on measuring actual changes in their populations with the activities they perform, it should not be surprising when measurable changes are not reported. Perrault et al. advocate for public health agencies to work with those in the discipline of Health Communication to craft objectives that are measurable outcomes, and to assist agencies in developing tools and methods to be able to track more proximal changes in their target populations (e.g., knowledge and attitude shifts) that may be influenced by the activities the agencies are performing.\n\n\"Public Health 2.0\" is a movement within public health that aims to make the field more accessible to the general public and more user-driven. The term is used in three senses. In the first sense, \"Public Health 2.0\" is similar to \"Health 2.0\" and describes the ways in which traditional public health practitioners and institutions are reaching out (or could reach out) to the public through social media and health blogs.\n\nIn the second sense, \"Public Health 2.0\" describes public health research that uses data gathered from social networking sites, search engine queries, cell phones, or other technologies. A recent example is the proposal of statistical framework that utilizes online user-generated content (from social media or search engine queries) to estimate the impact of an influenza vaccination campaign in the UK.\n\nIn the third sense, \"Public Health 2.0\" is used to describe public health activities that are completely user-driven. An example is the collection and sharing of information about environmental radiation levels after the March 2011 tsunami in Japan. In all cases, Public Health 2.0 draws on ideas from Web 2.0, such as crowdsourcing, information sharing, and user-centred design. While many individual healthcare providers have started making their own personal contributions to \"Public Health 2.0\" through personal blogs, social profiles, and websites, other larger organizations, such as the American Heart Association (AHA) and United Medical Education (UME), have a larger team of employees centered around online driven health education, research, and training. These private organizations recognize the need for free and easy to access health materials often building libraries of educational articles.\n\nThere is a great disparity in access to health care and public health initiatives between developed nations and developing nations. In the developing world, public health infrastructures are still forming. There may not be enough trained health workers, monetary resources or, in some cases, sufficient knowledge to provide even a basic level of medical care and disease prevention. As a result, a large majority of disease and mortality in the developing world results from and contributes to extreme poverty. For example, many African governments spend less than US$10 per person per year on health care, while, in the United States, the federal government spent approximately US$4,500 per capita in 2000. However, expenditures on health care should not be confused with spending on public health. Public health measures may not generally be considered \"health care\" in the strictest sense. For example, mandating the use of seat belts in cars can save countless lives and contribute to the health of a population, but typically money spent enforcing this rule would not count as money spent on health care.\n\nLarge parts of the developing world remained plagued by largely preventable or treatable infectious diseases. In addition to this however, many developing countries are also experiencing an epidemiological shift and polarization in which populations are now experiencing more of the effects of chronic diseases as life expectancy increases with, the poorer communities being heavily affected by both chronic and infectious diseases. Another major public health concern in the developing world is poor maternal and child health, exacerbated by malnutrition and poverty. The WHO reports that a lack of exclusive breastfeeding during the first six months of life contributes to over a million avoidable child deaths each year. Intermittent preventive therapy aimed at treating and preventing malaria episodes among pregnant women and young children is one public health measure in endemic countries.\n\nEach day brings new front-page headlines about public health: emerging infectious diseases such as SARS, rapidly making its way from China (see Public health in China) to Canada, the United States and other geographically distant countries; reducing inequities in health care access through publicly funded health insurance programs; the HIV/AIDS pandemic and its spread from certain high-risk groups to the general population in many countries, such as in South Africa; the increase of childhood obesity and the concomitant increase in type II diabetes among children; the social, economic and health effects of adolescent pregnancy; and the public health challenges related to natural disasters such as the 2004 Indian Ocean tsunami, 2005's Hurricane Katrina in the United States and the 2010 Haiti earthquake.\n\nSince the 1980s, the growing field of population health has broadened the focus of public health from individual behaviors and risk factors to population-level issues such as inequality, poverty, and education. Modern public health is often concerned with addressing determinants of health across a population. There is a recognition that our health is affected by many factors including where we live, genetics, our income, our educational status and our social relationships; these are known as \"social determinants of health\". The upstream drivers such as environment, education, employment, income, food security, housing, social inclusion and many others effect the distribution of health between and within populations and are often shaped by policy. A social gradient in health runs through society. The poorest generally suffer the worst health, but even the middle classes will generally have worse health outcomes than those of a higher social stratum. The new public health advocates for population-based policies that improve health in an equitable manner.\n\nHealth aid to developing countries is an important source of public health funding for many developing countries. Health aid to developing countries has shown a significant increase after World War II as concerns over the spread of disease as a result of globalization increased and the HIV/AIDS epidemic in sub-Saharan Africa surfaced. From 1990 to 2010, total health aid from developed countries increased from 5.5 billion to 26.87 billion with wealthy countries continuously donating billions of dollars every year with the goal of improving population health. Some efforts, however, receive a significantly larger proportion of funds such as HIV which received an increase in funds of over $6 billion dollars between 2000 and 2010 which was more than twice the increase seen in any other sector during those years. Health aid has seen an expansion through multiple channels including private philanthropy, non-governmental organizations, private foundations such as the Bill & Melinda Gates Foundation, bilateral donors, and multilateral donors such as the World Bank or UNICEF. In 2009 health aid from the OECD amounted to $12.47 billion which amounted to 11.4% of its total bilateral aid. In 2009, Multilateral donors were found to spend 15.3% of their total aid on bettering public healthcare. Recent data, however, shows that international health aid has plateaued and may begin to decrease.\n\nDebates exist questioning the efficacy of international health aid. Proponents of aid claim that health aid from wealthy countries is necessary in order for developing countries to escape the poverty trap. Opponents of health aid claim that international health aid actually disrupts developing countries' course of development, causes dependence on aid, and in many cases the aid fails to reach its recipients. For example, recently, health aid was funneled towards initiatives such as financing new technologies like antiretroviral medication, insecticide-treated mosquito nets, and new vaccines. The positive impacts of these initiatives can be seen in the eradication of smallpox and polio; however, critics claim that misuse or misplacement of funds may cause many of these efforts to never come into fruition.\n\nEconomic modeling based on the Institute for Health Metrics and Evaluation and the World Health Organization has shown a link between international health aid in developing countries and a reduction in adult mortality rates. However, a 2014-2016 study suggests that a potential confounding variable for this outcome is the possibility that aid was directed at countries once they were already on track for improvement. That same study, however, also suggests that 1 billion dollars in health aid was associated with 364,000 fewer deaths occurring between ages 0 and 5 in 2011.\n\nTo address current and future challenges in addressing health issues in the world, the United Nations have developed the Sustainable Development Goals building off of the Millennium Development Goals of 2000 to be completed by 2030. These goals in their entirety encompass the entire spectrum of development across nations, however Goals 1-6 directly address health disparities, primarily in developing countries. These six goals address key issues in global public health: Poverty, Hunger and food security, Health, Education, Gender equality and women's empowerment, and water and sanitation. Public health officials can use these goals to set their own agenda and plan for smaller scale initiatives for their organizations. These goals hope to lessen the burden of disease and inequality faced by developing countries and lead to a healthier future.\n\nThe links between the various sustainable development goals and public health are numerous and well established:\n\nThe U.S. Global Health Initiative was created in 2009 by President Obama in an attempt to have a more holistic, comprehensive approach to improving global health as opposed to previous, disease-specific interventions. The Global Health Initiative is a six-year plan, \"to develop a comprehensive U.S. government strategy for global health, building on the President's Emergency Plan for AIDS Relief (PEPFAR) to combat HIV as well as U.S. efforts to address tuberculosis (TB) and malaria, and augmenting the focus on other global health priorities, including neglected tropical diseases (NTDs), maternal, newborn and child health (MNCH), family planning and reproductive health (FP/RH), nutrition, and health systems strengthening (HSS)\". The GHI programs are being implemented in more than 80 countries around the world and works closely with the United States Agency for International Development, the Centers for Disease Control and Prevention, the United States Deputy Secretary of State.\n\nThere are seven core principles:\n\n\nThe aid effectiveness agenda is a useful tool for measuring the impact of these large scale programs such as The Global Fund to Fight AIDS, Tuberculosis and Malaria and the Global Alliance for Vaccines and Immunization (GAVI) which have been successful in achieving rapid and visible results. The Global Fund claims that its efforts have provided antiretroviral treatment for over three million people worldwide. GAVI claims that its vaccination programs have prevented over 5 million deaths since it began in 2000.\n\nEducation and training of public health professionals is available throughout the world in Schools of Public Health, Medical Schools, Veterinary Schools, Schools of Nursing, and Schools of Public Affairs. The training typically requires a university degree with a focus on core disciplines of biostatistics, epidemiology, health services administration, health policy, health education, behavioral science, gender issues, sexual and reproductive health, public health nutrition and environmental and occupational health. In the global context, the field of public health education has evolved enormously in recent decades, supported by institutions such as the World Health Organization and the World Bank, among others. Operational structures are formulated by strategic principles, with educational and career pathways guided by competency frameworks, all requiring modulation according to local, national and global realities. It is critically important for the health of populations that nations assess their public health human resource needs and develop their ability to deliver this capacity, and not depend on other countries to supply it.\n\nIn the United States, the Welch-Rose Report of 1915 has been viewed as the basis for the critical movement in the history of the institutional schism between public health and medicine because it led to the establishment of schools of public health supported by the Rockefeller Foundation. The report was authored by William Welch, founding dean of the Johns Hopkins Bloomberg School of Public Health, and Wickliffe Rose of the Rockefeller Foundation. The report focused more on research than practical education. Some have blamed the Rockefeller Foundation's 1916 decision to support the establishment of schools of public health for creating the schism between public health and medicine and legitimizing the rift between medicine's laboratory investigation of the mechanisms of disease and public health's nonclinical concern with environmental and social influences on health and wellness.\n\nEven though schools of public health had already been established in Canada, Europe and North Africa, the United States had still maintained the traditional system of housing faculties of public health within their medical institutions. A $25,000 donation from businessman Samuel Zemurray instituted the School of Public Health and Tropical Medicine at Tulane University in 1912 conferring its first doctor of public health degree in 1914. The Yale School of Public Health was founded by Charles-Edward Avory Winslow in 1915. The Johns Hopkins School of Hygiene and Public Health became an independent, degree-granting institution for research and training in public health, and the largest public health training facility in the United States, when it was founded in 1916. By 1922, schools of public health were established at Columbia and Harvard on the Hopkins model. By 1999 there were twenty nine schools of public health in the US, enrolling around fifteen thousand students.\n\nOver the years, the types of students and training provided have also changed. In the beginning, students who enrolled in public health schools typically had already obtained a medical degree; public health school training was largely a second degree for medical professionals. However, in 1978, 69% of American students enrolled in public health schools had only a bachelor's degree.\n\nSchools of public health offer a variety of degrees which generally fall into two categories: professional or academic. The two major postgraduate degrees are the Master of Public Health (MPH) or the Master of Science in Public Health (MSPH). Doctoral studies in this field include Doctor of Public Health (DrPH) and Doctor of Philosophy (PhD) in a subspeciality of greater Public Health disciplines. DrPH is regarded as a professional degree and PhD as more of an academic degree.\n\nProfessional degrees are oriented towards practice in public health settings. The Master of Public Health, Doctor of Public Health, Doctor of Health Science (DHSc) and the Master of Health Care Administration are examples of degrees which are geared towards people who want careers as practitioners of public health in health departments, managed care and community-based organizations, hospitals and consulting firms, among others. Master of Public Health degrees broadly fall into two categories, those that put more emphasis on an understanding of epidemiology and statistics as the scientific basis of public health practice and those that include a more eclectic range of methodologies. A Master of Science of Public Health is similar to an MPH but is considered an academic degree (as opposed to a professional degree) and places more emphasis on scientific methods and research. The same distinction can be made between the DrPH and the DHSc. The DrPH is considered a professional degree and the DHSc is an academic degree.\n\nAcademic degrees are more oriented towards those with interests in the scientific basis of public health and preventive medicine who wish to pursue careers in research, university teaching in graduate programs, policy analysis and development, and other high-level public health positions. Examples of academic degrees are the Master of Science, Doctor of Philosophy, Doctor of Science (ScD), and Doctor of Health Science (DHSc). The doctoral programs are distinct from the MPH and other professional programs by the addition of advanced coursework and the nature and scope of a dissertation research project.\n\nIn the United States, the Association of Schools of Public Health represents Council on Education for Public Health (CEPH) accredited schools of public health. Delta Omega is the honor society for graduate studies in public health. The society was founded in 1924 at the Johns Hopkins School of Hygiene and Public Health. Currently, there are approximately 68 chapters throughout the United States and Puerto Rico.\n\nPublic health has early roots in antiquity. From the beginnings of human civilization, it was recognized that polluted water and lack of proper waste disposal spread communicable diseases (theory of miasma). Early religions attempted to regulate behavior that specifically related to health, from types of food eaten, to regulating certain indulgent behaviors, such as drinking alcohol or sexual relations. Leaders were responsible for the health of their subjects to ensure social stability, prosperity, and maintain order.\n\nBy Roman times, it was well understood that proper diversion of human waste was a necessary tenet of public health in urban areas. The ancient Chinese medical doctors developed the practice of variolation following a smallpox epidemic around 1000 BC. An individual without the disease could gain some measure of immunity against it by inhaling the dried crusts that formed around lesions of infected individuals. Also, children were protected by inoculating a scratch on their forearms with the pus from a lesion.\n\nIn 1485 the Republic of Venice established a permanent Venetian Magistrate for Health comprising supervisors of health with special attention to the prevention of the spread of epidemics in the territory from abroad. The three supervisors were initially appointed by the Venetian Senate. In 1537 it was assumed by the Grand Council, and in 1556 added two judges, with the task of control, on behalf of the Republic, the efforts of the supervisors.\n\nHowever, according to Michel Foucault, the plague model of governmentality was later controverted by the cholera model. A Cholera pandemic devastated Europe between 1829 and 1851, and was first fought by the use of what Foucault called \"social medicine\", which focused on flux, circulation of air, location of cemeteries, etc. All those concerns, born of the miasma theory of disease, were mixed with urbanistic concerns for the management of populations, which Foucault designated as the concept of \"biopower\". The German conceptualized this in the \"Polizeiwissenschaft\" (\"Police science\").\n\nThe 18th century saw rapid growth in voluntary hospitals in England. The latter part of the century brought the establishment of the basic pattern of improvements in public health over the next two centuries: a social evil was identified, private philanthropists brought attention to it, and changing public opinion led to government action.\nThe practice of vaccination became prevalent in the 1800s, following the pioneering work of Edward Jenner in treating smallpox. James Lind's discovery of the causes of scurvy amongst sailors and its mitigation via the introduction of fruit on lengthy voyages was published in 1754 and led to the adoption of this idea by the Royal Navy. Efforts were also made to promulgate health matters to the broader public; in 1752 the British physician Sir John Pringle published \"Observations on the Diseases of the Army in Camp and Garrison\", in which he advocated for the importance of adequate ventilation in the military barracks and the provision of latrines for the soldiers.\n\nWith the onset of the Industrial Revolution, living standards amongst the working population began to worsen, with cramped and unsanitary urban conditions. In the first four decades of the 19th century alone, London's population doubled and even greater growth rates were recorded in the new industrial towns, such as Leeds and Manchester. This rapid urbanisation exacerbated the spread of disease in the large conurbations that built up around the workhouses and factories. These settlements were cramped and primitive with no organized sanitation. Disease was inevitable and its incubation in these areas was encouraged by the poor lifestyle of the inhabitants. Unavailable housing led to the rapid growth of slums and the per capita death rate began to rise alarmingly, almost doubling in Birmingham and Liverpool. Thomas Malthus warned of the dangers of overpopulation in 1798. His ideas, as well as those of Jeremy Bentham, became very influential in government circles in the early years of the 19th century.\n\nThe first attempts at sanitary reform and the establishment of public health institutions were made in the 1840s. Thomas Southwood Smith, physician at the London Fever Hospital, began to write papers on the importance of public health, and was one of the first physicians brought in to give evidence before the Poor Law Commission in the 1830s, along with Neil Arnott and James Phillips Kay. Smith advised the government on the importance of quarantine and sanitary improvement for limiting the spread of infectious diseases such as cholera and yellow fever.\n\nThe Poor Law Commission reported in 1838 that \"the expenditures necessary to the adoption and maintenance of measures of prevention would ultimately amount to less than the cost of the disease now constantly engendered\". It recommended the implementation of large scale government engineering projects to alleviate the conditions that allowed for the propagation of disease. The Health of Towns Association was formed in Exeter on 11 December 1844, and vigorously campaigned for the development of public health in the United Kingdom. Its formation followed the 1843 establishment of the Health of Towns Commission, chaired by Sir Edwin Chadwick, which produced a series of reports on poor and insanitary conditions in British cities.\n\nThese national and local movements led to the Public Health Act, finally passed in 1848. It aimed to improve the sanitary condition of towns and populous places in England and Wales by placing the supply of water, sewerage, drainage, cleansing and paving under a single local body with the General Board of Health as a central authority. The Act was passed by the Liberal government of Lord John Russell, in response to the urging of Edwin Chadwick. Chadwick's seminal report on \"The Sanitary Condition of the Labouring Population\" was published in 1842 and was followed up with a supplementary report a year later.\n\nVaccination for various diseases was made compulsory in the United Kingdom in 1851, and by 1871 legislation required a comprehensive system of registration run by appointed vaccination officers.\n\nFurther interventions were made by a series of subsequent Public Health Acts, notably the 1875 Act. Reforms included latrinization, the building of sewers, the regular collection of garbage followed by incineration or disposal in a landfill, the provision of clean water and the draining of standing water to prevent the breeding of mosquitoes.\n\nThe Infectious Disease (Notification) Act 1889 mandated the reporting of infectious diseases to the local sanitary authority, which could then pursue measures such as the removal of the patient to hospital and the disinfection of homes and properties.\n\nIn the United States, the first public health organization based on a state health department and local boards of health was founded in New York City in 1866.\n\nThe science of epidemiology was founded by John Snow's identification of a polluted public water well as the source of an 1854 cholera outbreak in London. Dr. Snow believed in the germ theory of disease as opposed to the prevailing miasma theory. He first publicized his theory in an essay, \"On the Mode of Communication of Cholera\", in 1849, followed by a more detailed treatise in 1855 incorporating the results of his investigation of the role of the water supply in the Soho epidemic of 1854.\n\nBy talking to local residents (with the help of Reverend Henry Whitehead), he identified the source of the outbreak as the public water pump on Broad Street (now Broadwick Street). Although Snow's chemical and microscope examination of a water sample from the Broad Street pump did not conclusively prove its danger, his studies of the pattern of the disease were convincing enough to persuade the local council to disable the well pump by removing its handle.\n\nSnow later used a dot map to illustrate the cluster of cholera cases around the pump. He also used statistics to illustrate the connection between the quality of the water source and cholera cases. He showed that the Southwark and Vauxhall Waterworks Company was taking water from sewage-polluted sections of the Thames and delivering the water to homes, leading to an increased incidence of cholera. Snow's study was a major event in the history of public health and geography. It is regarded as the founding event of the science of epidemiology.\n\nWith the pioneering work in bacteriology of French chemist Louis Pasteur and German scientist Robert Koch, methods for isolating the bacteria responsible for a given disease and vaccines for remedy were developed at the turn of the 20th century. British physician Ronald Ross identified the mosquito as the carrier of malaria and laid the foundations for combating the disease. Joseph Lister revolutionized surgery by the introduction of antiseptic surgery to eliminate infection. French epidemiologist Paul-Louis Simond proved that plague was carried by fleas on the back of rats, and Cuban scientist Carlos J. Finlay and U.S. Americans Walter Reed and James Carroll demonstrated that mosquitoes carry the virus responsible for yellow fever. Brazilian scientist Carlos Chagas identified a tropical disease and its vector.\n\nWith onset of the epidemiological transition and as the prevalence of infectious diseases decreased through the 20th century, public health began to put more focus on chronic diseases such as cancer and heart disease. Previous efforts in many developed countries had already led to dramatic reductions in the infant mortality rate using preventative methods. In Britain, the infant mortality rate fell from over 15% in 1870 to 7% by 1930.\n\nFrance 1871-1914 followed well behind Bismarckian Germany, as well as Great Britain, in developing the welfare state including public health. Tuberculosis was the most dreaded disease of the day, especially striking young people in their 20s. Germany set up vigorous measures of public hygiene and public sanatoria, but France let private physicians handle the problem, which left it with a much higher death rate. The French medical profession jealously guarded its prerogatives, and public health activists were not as well organized or as influential as in Germany, Britain or the United States. For example, there was a long battle over a public health law which began in the 1880s as a campaign to reorganize the nation's health services, to require the registration of infectious diseases, to mandate quarantines, and to improve the deficient health and housing legislation of 1850. However the reformers met opposition from bureaucrats, politicians, and physicians. Because it was so threatening to so many interests, the proposal was debated and postponed for 20 years before becoming law in 1902. Success finally came when the government realized that contagious diseases had a national security impact in weakening military recruits, and keeping the population growth rate well below Germany's.\n\nModern public health began developing in the 19th century, as a response to advances in science that led to the understanding of, the source and spread of disease. As the knowledge of contagious diseases increased, means to control them and prevent infection were soon developed. Once it became understood that these strategies would require community-wide participation, disease control began being viewed as a public responsibility. Various organizations and agencies were then created to implement these disease preventing strategies.\n\nMost of the Public health activity in the United States took place at the municipal level before the mid-20th century. There was some activity at the national and state level as well.\n\nIn the administration of the second president of the United States John Adams, the Congress authorized the creation of hospitals for mariners. As the U.S. expanded, the scope of the governmental health agency expanded.\n\nIn the United States, public health worker Sara Josephine Baker, M.D. established many programs to help the poor in New York City keep their infants healthy, leading teams of nurses into the crowded neighborhoods of Hell's Kitchen and teaching mothers how to dress, feed, and bathe their babies.\n\nAnother key pioneer of public health in the U.S. was Lillian Wald, who founded the Henry Street Settlement house in New York. The Visiting Nurse Service of New York was a significant organization for bringing health care to the urban poor.\n\nDramatic increases in average life span in the late 19th century and 20th century, is widely credited to public health achievements, such as vaccination programs and control of many infectious diseases including polio, diphtheria, yellow fever and smallpox; effective health and safety policies such as road traffic safety and occupational safety; improved family planning; tobacco control measures; and programs designed to decrease non-communicable diseases by acting on known risk factors such as a person's background, lifestyle and environment.\n\nAnother major public health improvement was the decline in the \"urban penalty\" brought about by improvements in sanitation. These improvements included chlorination of drinking water, filtration and sewage treatment which led to the decline in deaths caused by infectious waterborne diseases such as cholera and intestinal diseases.\nThe federal Office of Indian Affairs (OIA) operated a large-scale field nursing program. Field nurses targeted native women for health education, emphasizing personal hygiene and infant care and nutrition.\n\nPublic health issues were important for the Spanish empire during the colonial era. Epidemic disease was the main factor in the decline of indigenous populations in the era immediately following the sixteenth-century conquest era and was a problem during the colonial era. The Spanish crown took steps in eighteenth-century Mexico to bring in regulations to make populations healthier.\n\nIn the late nineteenth century, Mexico was in the process of modernization, and public health issues were again tackled from a scientific point of view. Even during the Mexican Revolution (1910–20), public health was an important concern, with a text on hygiene published in 1916. During the Mexican Revolution, feminist and trained nurse Elena Arizmendi Mejia founded the Neutral White Cross, treating wounded soldiers no matter for what faction they fought.\n\nIn the post-revolutionary period after 1920, improved public health was a revolutionary goal of the Mexican government.\nThe Mexican state promoted the health of the Mexican population, with most resources going to cities. Concern about disease conditions and social impediments to the improvement of Mexicans' health were important in the formation of the Mexican Society for Eugenics. The movement flourished from the 1920s to the 1940s. Mexico was not alone in Latin America or the world in promoting eugenics. Government campaigns against disease and alcoholism were also seen as promoting public health.\n\nThe Mexican Social Security Institute was established in 1943, during the administration of President Manuel Avila Camacho to deal with public health, pensions, and social security.\n\nSince the 1959 Cuban Revolution the Cuban government has devoted extensive resources to the improvement of health conditions for its entire population via universal access to health care. Infant mortality has plummeted. Cuban medical internationalism as a policy has seen the Cuban government sent doctors as a form of aid and export to countries in need in Latin America, especially Venezuela, as well as Oceania and Africa countries.\n\nPublic health was important elsewhere in Latin America in consolidating state power and integrating marginalized populations into the nation-state. In Colombia, public health was a means for creating and implementing ideas of citizenship. In Bolivia, a similar push came after their 1952 revolution.\n\nThough curable and preventative, malaria remains a huge public health problem and is the third leading cause of death in Ghana. In the absence of a vaccine, mosquito control, or access to anti-malaria medication, public health methods become the main strategy for reducing the prevalence and severity of malaria. These methods include reducing breeding sites, screening doors and windows, insecticide sprays, prompt treatment following infection, and usage of insecticide treated mosquito nets. Distribution and sale of insecticide-treated mosquito nets is a common, cost-effective anti-malaria public health intervention; however, barriers to use exist including cost, hosehold and family organization, access to resources, and social and behavioral determinants which have not only been shown to affect malaria prevalence rates but also mosquito net use.\n\n"}
{"id": "22140663", "url": "https://en.wikipedia.org/wiki?curid=22140663", "title": "Quality of life (healthcare)", "text": "Quality of life (healthcare)\n\nIn general, quality of life (QoL or QOL) is the perceived quality of an individual's daily life, that is, an assessment of their well-being or lack thereof. This includes all emotional, social and physical aspects of the individual's life. In health care, health-related quality of life (HRQoL) is an assessment of how the individual's well-being may be affected over time by a disease, disability or disorder.\n\nEarly versions of healthcare-related quality of life measures referred to simple assessments of physical abilities by an external rater (for example, the patient is able to get up, eat and drink, and take care of personal hygiene without any help from others) or even to a single measurement (for example, the angle to which a limb could be flexed).\n\nThe current concept of health-related quality of life acknowledges that subjects put their actual situation in relation to their personal expectation. The latter can vary over time, and react to external influences such as length and severity of illness, family support, etc. As with any situation involving multiple perspectives, patients' and physicians' rating of the same objective situation have been found to differ significantly. Consequently, health-related quality of life is now usually assessed using patient questionnaires. These are often multidimensional and cover physical, social, emotional, cognitive, work- or role-related, and possibly spiritual aspects as well as a wide variety of disease related symptoms, therapy induced side effects, and even the financial impact of medical conditions. Although often used interchangeably with the measurement of health status, both health-related quality of life and health status measure different concepts.\n\nBecause health problems can interfere with even the most basic aspects of daily living (for example, breathing comfortably, quality of sleep, eliminating wastes, feeding oneself, dressing, and others), the health care professions have codified the concepts of activities of daily living (ADLs) and instrumental activities of daily living (IADLs). Such analysis and classification helps to at least partially objectify quality of life. It cannot eliminate all subjectivity, but it can help improve measurement and communication by quantifying and by reducing ineffability.\n\nSimilar to other psychometric assessment tools, health-related quality of life questionnaires should meet certain quality criteria, most importantly with regard to their reliability and validity. Hundreds of validated health-related quality of life questionnaires have been developed, some of which are specific to various illnesses. \nThe questionnaires can be generalized into two categories:\n\n\"Generic instruments\"\n\"Disease, disorder or condition specific instruments\" \n\nA variety of validated surveys exist for healthcare providers to use for measuring a patient’s health-related quality of life. The results are then used to help determine treatment options for the patient based on past results from other patients, and to measure intra-individual improvements in QoL pre- and post-treatment.\n\nWhen it is used as a longitudinal study device that surveys patients before, during, and after treatment, it can help health care providers determine which treatment plan is the best option, thereby improving healthcare through an evolutionary process.\n\nThere is a growing field of research concerned with developing, evaluating, and applying quality of life measures within health related research (e.g. within randomized controlled studies), especially in relation to Health Services Research. Well-executed health-related quality of life research informs those tasked with health rationing or anyone involved in the decision-making process of agencies such as the Food and Drug Administration, European Medicines Agency or National Institute for Clinical Excellence. Additionally, health-related quality of life research may be used as the final step in clinical trials of experimental therapies.\n\nThe understanding of Quality of Life is recognized as an increasingly important healthcare topic because the relationship between cost and value raises complex problems, often with high emotional attachment because of the potential impact on human life. For instance, healthcare providers must refer to cost-benefit analysis to make economic decisions about access to expensive drugs that may prolong life by short amount of time and/or provide a minimal increase to quality of life. Additionally, these treatment drugs must be weighed against the cost of alternative treatments or preventative medicine. In the case of chronic and/or terminal illness where no effective cure is available, an emphasis is placed on improving health-related quality of life through interventions such as symptom management, adaptive technology, and palliative care.\n\nIn the realm of elder care, research indicates that improvements in quality of life ratings may also improve resident outcomes, which can lead to substantial cost savings over time. Research has also shown that quality of life ratings can be successfully used as a key-performance metric when designing and implementing organizational change initiatives in nursing homes.\n\nResearch revolving around Health Related Quality of Life is extremely important because of the implications that it can have on current and future treatments and health protocols. Thereby, validated health-related quality of life questionnaires can become an integral part of clinical trials in determining the trial drugs' value in a cost-benefit analysis. For example, the Center for Disease Control and Prevention (CDC) is using their health-related quality of life survey, Healthy Day Measure, as part of research to identify health disparities, track population trends, and build broad coalitions around a measure of population health. This information can then be used by multiple levels of government or other officials to \"increase quality and years of life\" and to \"eliminate health disparaties\" for equal opportunity.\nThe quality of life ethic refers to an ethical principle that uses assessments of the quality of life that a person could potentially experience as a foundation for making decisions about the continuation or termination of life. It is often used in contrast to or in opposition to the sanctity of life ethic.\n\nIt is not considered uncommon for there to be some statistical anomalies during data analysis. Some of the more frequently seen in health-related quality of life analysis are the ceiling effect, the floor effect, and response shift bias.\n\nThe ceiling effect refers to how patients who start with a higher quality of life than the average patient do not have much room for improvement when treated. The opposite of this is the floor effect, where patients with a lower quality of life average have much more room for improvement. Consequentially, if the spectrum of quality of life before treatment is too unbalanced, there is a greater potential for skewing the end results, creating the possibility for incorrectly portraying a treatment's effectiveness or lack thereof.\n\nResponse shift bias is an increasing problem within longitudinal studies that rely on patient reported outcomes. It refers to the potential of a subject’s views, values, or expectations changing over the course of a study, thereby adding another factor of change on the end results. Clinicians and healthcare providers must recalibrate surveys over the course of a study to account for Response Shift Bias. The degree of recalibration varies due to factors based on the individual area of investigation and length of study.\n\nIn a study by Norman et al. about health-related quality of life surveys, it was found that most survey results were within a half standard deviation. Norman et al. theorized that this is due to the limited human discrimination ability as identified by George A. Miller in 1956. Utilizing the Magic Number of 7 ± 2, Miller theorized that when the scale on a survey extends beyond 7 ± 2, humans fail to be consistent and lose ability to differentiate individual steps on the scale because of channel capacity.\n\nNorman et al. proposed health-related quality of life surveys use a half standard deviation as the statistically significant benefit of a treatment instead of calculating survey-specific “minimally important differences\", which are the supposed real-life improvements reported by the subjects. In other words, Norman et al. proposed all health-related quality of life survey scales be set to a half standard deviation instead of calculating a scale for each survey validation study where the steps are referred to as \"minimally important differences\".\n\n\n"}
{"id": "57335451", "url": "https://en.wikipedia.org/wiki?curid=57335451", "title": "Real world data", "text": "Real world data\n\nReal world data (RWD) in medicine is data derived from a number of sources that are associated with outcomes in a heterogeneous patient population in real-world settings, such as patient surveys, clinical trials, and observational cohort studies. Real-world data refer to observational data as opposed to data gathered in an experimental setting such as a randomized controlled trial (RCT). They are derived from Electronic health records (EHRs), claims and billing activities, product and disease registries, etc. \n\n\n\"Real World Evidence\" at FDA\n"}
{"id": "854081", "url": "https://en.wikipedia.org/wiki?curid=854081", "title": "Regeneration (biology)", "text": "Regeneration (biology)\n\nIn biology, regeneration is the process of renewal, restoration, and growth that makes genomes, cells, organisms, and ecosystems resilient to natural fluctuations or events that cause disturbance or damage. Every species is capable of regeneration, from bacteria to humans. Regeneration can either be complete where the new tissue is the same as the lost tissue, or incomplete where after the necrotic tissue comes fibrosis. At its most elementary level, regeneration is mediated by the molecular processes of gene regulation. Regeneration in biology, however, mainly refers to the morphogenic processes that characterize the phenotypic plasticity of traits allowing multi-cellular organisms to repair and maintain the integrity of their physiological and morphological states. Above the genetic level, regeneration is fundamentally regulated by asexual cellular processes. Regeneration is different from reproduction. For example, hydra perform regeneration but reproduce by the method of budding.\n\nThe hydra and the planarian flatworm have long served as model organisms for their highly adaptive regenerative capabilities. Once wounded, their cells become activated and start to remodel tissues and organs back to the pre-existing state. The Caudata (\"urodeles\"; salamanders and newts), an order of tailed amphibians, is possibly the most adept vertebrate group at regeneration, given their capability of regenerating limbs, tails, jaws, eyes and a variety of internal structures. The regeneration of organs is a common and widespread adaptive capability among metazoan creatures. In a related context, some animals are able to reproduce asexually through fragmentation, budding, or fission. A planarian parent, for example, will constrict, split in the middle, and each half generates a new end to form two clones of the original.\n\nEchinoderms (such as the sea star), crayfish, many reptiles, and amphibians exhibit remarkable examples of tissue regeneration. The case of autotomy, for example, serves as a defensive function as the animal detaches a limb or tail to avoid capture. After the limb or tail has been autotomized, cells move into action and the tissues will regenerate. Limited regeneration of limbs occurs in most fishes and salamanders, and tail regeneration takes place in larval frogs and toads (but not adults). The whole limb of a salamander or a triton will grow again and again after amputation. In reptiles, chelonians, crocodilians and snakes are unable to regenerate lost parts, but many (not all) kinds of lizards, geckos and iguanas possess regeneration capacity in a high degree. Usually, it involves dropping a section of their tail and regenerating it as part of a defense mechanism. While escaping a predator, if the predator catches the tail, it will disconnect.\n\nEcosystems can be regenerative. Following a disturbance, such as a fire or pest outbreak in a forest, pioneering species will occupy, compete for space, and establish themselves in the newly opened habitat. The new growth of seedlings and community assembly process is known as regeneration in ecology.\n\nPattern formation in the morphogenesis of an animal is regulated by genetic induction factors that put cells to work after damage has occurred. Neural cells, for example, express growth-associated proteins, such as GAP-43, tubulin, actin, an array of novel neuropeptides, and cytokines that induce a cellular physiological response to regenerate from the damage. Many of the genes that are involved in the original development of tissues are reinitialized during the regenerative process. Cells in the primordia of zebrafish fins, for example, express four genes from the homeobox \"msx\" family during development and regeneration.\n\n\"Strategies include the rearrangement of pre-existing tissue, the use of adult somatic stem cells and the dedifferentiation and/or transdifferentiation of cells, and more than one mode can operate in different tissues of the same animal. All these strategies result in the re-establishment of appropriate tissue polarity, structure and form.\" During the developmental process, genes are activated that serve to modify the properties of cell as they differentiate into different tissues. Development and regeneration involves the coordination and organization of populations cells into a blastema, which is \"a mound of stem cells from which regeneration begins\". Dedifferentiation of cells means that they lose their tissue-specific characteristics as tissues remodel during the regeneration process. This should not be confused with the transdifferentiation of cells which is when they lose their tissue-specific characteristics during the regeneration process, and then re-differentiate to a different kind of cell.\n\nArthropods are known to regenerate appendages following loss or autotomy. Regeneration among arthropods is restricted by molting such that hemimetabolous insects are capable of regeneration only until their final molt whereas most crustaceans can regenerate throughout their lifetimes. Molting cycles are hormonally regulated in arthropods, although premature molting can be induced by autotomy. Mechanisms underlying appendage regeneration in hemimetabolous insects and crustaceans is highly conserved. During limb regeneration species in both taxa form a blastema following autotomy with regeneration of the excised limb occurring during proecdysis. Arachnids, including scorpions, are known to regenerate their venom, although the content of the regenerated venom is different than the original venom during its regeneration, as the venom volume is replaced before the active proteins are all replenished.\n\nMany annelids (segmented worms) are capable of regeneration. For example, \"Chaetopterus variopedatus\" and \"Branchiomma nigromaculata\" can regenerate both anterior and posterior body parts after latitudinal bisection. The relationship between somatic and germline stem cell regeneration has been studied at the molecular level in the annelid \"Capitella teleta\". Leeches, however, appear incapable of segmental regeneration. Furthermore, their close relatives, the branchiobdellids, are also incapable of segmental regeneration. However, certain individuals, like the lumbriculids, can regenerate from only a few segments. Segmental regeneration in these animals is epimorphic and occurs through blastema formation. Segmental regeneration has been gained and lost during annelid evolution, as seen in oligochaetes, where head regeneration has been lost three separate times.\n\nAlong with epimorphosis, some polychaetes like \"Sabella pavonina\" experience morphallactic regeneration. Morphallaxis involves the de-differentiation, transformation, and re-differentation of cells to regenerate tissues. How prominent morphallactic regeneration is in oligochaetes is currently not well understood. Although relatively under-reported, it is possible that morphallaxis is a common mode of inter-segment regeneration in annelids. Following regeneration in \"L. variegatus\", past posterior segments sometimes become anterior in the new body orientation, consistent with morphallaxis.\n\nFollowing amputation, most annelids are capable of sealing their body via rapid muscular contraction. Constriction of body muscle can lead to infection prevention. In certain species, such as \"Limnodrilus\", autolysis can be seen within hours after amputation in the ectoderm and mesoderm. Amputation is also thought to cause a large migration of cells to the injury site, and these form a wound plug.\n\nTissue regeneration is widespread among echinoderms and has been well documented in starfish \"(Asteroidea)\", sea cucumbers \"(Holothuroidea)\", and sea urchins \"(Echinoidea).\" Appendage regeneration in echinoderms has been studied since at least the 19th century. In addition to appendages, some species can regenerate internal organs and parts of their central nervous system. In response to injury starfish can autotomize damaged appendages. Autotomy is the self-amputation of a body part, usually an appendage.  Depending on severity, starfish will then go through a four-week process where the appendage will be regenerated. Some species must retain mouth cells in order to regenerate an appendage, due to the need for energy. The first organs to regenerate, in all species documented to date, are associated with the digestive tract. Thus, most knowledge about visceral regeneration in holothurians concerns this system.\n\nRegeneration research using Planarians began in the late 1800s and was popularized by T.H. Morgan at the beginning of the 20th century. Alejandro Sanchez-Alvarado and Philip Newmark transformed planarians into a model genetic organism in the beginning of the 20th century to study the molecular mechanisms underlying regeneration in these animals. Planarians exhibit an extraordinary ability to regenerate lost body parts. For example, a planarian split lengthwise or crosswise will regenerate into two separate individuals. In one experiment, T.H. Morgan found that a piece corresponding to 1/279th of a planarian or a fragment with as few as 10,000 cells can successfully regenerate into a new worm within one to two weeks. After amputation, stump cells form a blastema formed from neoblasts, pluripotent cells found throughout the planarian body. New tissue grows from neoblasts with neoblasts comprising between 20 and 30% of all planarian cells. Recent work has confirmed that neoblasts are totipotent since one single neoblast can regenerate an entire irradiated animal that has been rendered incapable of regeneration. In order to prevent starvation a planarian will use their own cells for energy, this phenomenon is known as de-growth.\n\nLimb regeneration in the axolotl and newt has been extensively studied and researched. Urodele amphibians, such as salamanders and newts, display the highest regenerative ability among tetrapods. As such, they can fully regenerate their limbs, tail, jaws, and retina via epimorphic regeneration leading to functional replacement with new tissue. Salamander limb regeneration occurs in two main steps. First, the local cells dedifferentiate at the wound site into progenitor to form a blastema. Second, the blastemal cells will undergo proliferation, patterning, differentiation and growth using similar genetic mechanisms that deployed during embryonic development. Ultimately, blastemal cells will generate all the cells for the new structure.\nAfter amputation, the epidermis migrates to cover the stump in 1–2 hours, forming a structure called the wound epithelium (WE). Epidermal cells continue to migrate over the WE, resulting in a thickened, specialized signaling center called the apical epithelial cap (AEC). Over the next several days there are changes in the underlying stump tissues that result in the formation of a blastema (a mass of dedifferentiated proliferating cells). As the blastema forms, pattern formation genes – such as HoxA and HoxD – are activated as they were when the limb was formed in the embryo. The positional identity of the distal tip of the limb (i.e. the autopod, which is the hand or foot) is formed first in the blastema. Intermediate positional identities between the stump and the distal tip are then filled in through a process called intercalation. Motor neurons, muscle, and blood vessels grow with the regenerated limb, and reestablish the connections that were present prior to amputation. The time that this entire process takes varies according to the age of the animal, ranging from about a month to around three months in the adult and then the limb becomes fully functional. Researchers at Australian Regenerative Medicine Institute at Monash University, have published that when macrophages, which eat up material debris, were removed, salamanders lost their ability to regenerate and formed scarred tissue instead.\n\nIn spite of the historically few researchers studying limb regeneration, remarkable progress has been made recently in establishing the neotenous amphibian the axolotl (\"Ambystoma mexicanum\") as a model genetic organism. This progress has been facilitated by advances in genomics, bioinformatics, and somatic cell transgenesis in other fields, that have created the opportunity to investigate the mechanisms of important biological properties, such as limb regeneration, in the axolotl. The Ambystoma Genetic Stock Center (AGSC) is a self-sustaining, breeding colony of the axolotl supported by the National Science Foundation as a Living Stock Collection. Located at the University of Kentucky, the AGSC is dedicated to supplying genetically well-characterized axolotl embryos, larvae, and adults to laboratories throughout the United States and abroad. An NIH-funded NCRR grant has led to the establishment of the Ambystoma EST database, the Salamander Genome Project (SGP) that has led to the creation of the first amphibian gene map and several annotated molecular data bases, and the creation of the research community web portal.\n\nAnurans can only regenerate their limbs during embryonic development. Once the limb skeleton has developed regeneration does not occur (Xenopus can grow a cartilaginous spike after amputation). Reactive oxygen species (ROS) appear to be required for a regeneration response in the anuran larvae. ROS production is essential to activate the Wnt signaling pathway, which has been associated with regeneration in other systems. Limb regeneration in salamanders occurs in two major steps. First, adult cells de-differentiate into progenitor cells which will replace the tissues they are derived from. Second, these progenitor cells then proliferate and differentiate until they have completely replaced the missing structure.\n\n\"Hydra\" is a genus of freshwater polyp in the phylum Cnidaria with highly proliferative stem cells that gives them the ability to regenerate their entire body. Any fragment larger than a few hundred epithelial cells that is isolated from the body has the ability to regenerate into a smaller version of itself. The high proportion of stem cells in the hydra supports its efficient regenerative ability.\n\nRegeneration among hydra occurs as foot regeneration arising from the basal part of the body, and head regeneration, arising from the apical region. Regeneration tissues that are cut from the gastric region contain polarity, which allows them to distinguish between regenerating a head in the apical end and a foot in the basal end so that both regions are present in the newly regenerated organism. Head regeneration requires complex reconstruction of the area, while foot regeneration is much simpler, similar to tissue repair. In both foot and head regeneration, however, there are two distinct molecular cascades that occur once the tissue is wounded: early injury response and a subsequent, signal-driven pathway of the regenerating tissue that leads to cellular differentiation. This early-injury response includes epithelial cell stretching for wound closure, the migration of interstitial progenitors towards the wound, cell death, phagocytosis of cell debris, and reconstruction of the extracellular matrix.\n\nRegeneration in hydra has been defined as morphallaxis, the process where regeneration results from remodeling of existing material without cellular proliferation. If a hydra is cut into two pieces, the remaining severed sections form two fully functional and independent hydra, approximately the same size as the two smaller severed sections. This occurs through the exchange and rearrangement of soft tissues without the formation of new material.\n\nOwing to a limited literature on the subject, birds are believed to have very limited regenerative abilities as adults. Some studies on roosters have suggested that birds can adequately regenerate some parts of the limbs and depending on the conditions in which regeneration takes place, such as age of the animal, the inter-relationship of the injured tissue with other muscles, and the type of operation, can involve complete regeneration of some musculoskeletal structure. Werber and Goldschmidt (1909) found that the goose and duck were capable of regenerating their beaks after partial amputation and Sidorova (1962) observed liver regeneration via hypertrophy in roosters. Birds are also capable of regenerating the hair cells in their cochlea following noise damage or ototoxic drug damage. Despite this evidence, contemporary studies suggest reparative regeneration in avian species is limited to periods during embryonic development. An array of molecular biology techniques have been successful in manipulating cellular pathways known to contribute to spontaneous regeneration in chick embryos. For instance, removing a portion of the elbow joint in a chick embryo via window excision or slice excision and comparing joint tissue specific markers and cartilage markers showed that window excision allowed 10 out of 20 limbs to regenerate and expressed joint genes similarly to a developing embryo. In contrast, slice excision did not allow the joint to regenerate due to the fusion of the skeletal elements seen by an expression of cartilage markers.\n\nSimilar to the physiological regeneration of hair in mammals, birds can regenerate their feathers in order to repair damaged feathers or to attract mates with their plumage. Typically, seasonal changes that are associated with breeding seasons will prompt a hormonal signal for birds to begin regenerating feathers. This has been experimentally induced using thyroid hormones in the Rhode Island Red Fowls.\n\nMammals are capable of cellular and physiological regeneration, but have generally poor reparative regenerative ability across the group. Examples of physiological regeneration in mammals include epithelial renewal (e.g., skin and intestinal tract), red blood cell replacement, antler regeneration and hair cycling. Male deer lose their antlers annually during the months of January to April then through regeneration are able to regrow them as an example of physiological regeneration. A deer antler is the only appendage of a mammal that can be regrown every year. While reparative regeneration is a rare phenomenon in mammals, it does occur. A well-documented example is regeneration of the digit tip distal to the nail bed. Reparative regeneration has also been observed in rabbits, pikas and African spiny mice. In 2012, researchers discovered that two species of African Spiny Mice, \"Acomys kempi\" and \"Acomys percivali\", were capable of completely regenerating the autotomically released or otherwise damaged tissue. These species can regrow hair follicles, skin, sweat glands, fur and cartilage. In addition to these two species, subsequent studies demonstrated that \"Acomys cahirinus\" could regenerate skin and excised tissue in the ear pinna.\n\nDespite these examples, it is generally accepted that adult mammals have limited regenerative capacity compared to most vertebrate embryos/larvae, adult salamanders and fish. But the regeneration therapy approach of Robert O. Becker, using electrical stimulation, has shown promising results for rats and mammals in general.\n\nSome researchers have also claimed that the MRL mouse strain exhibits enhanced regenerative abilities. Work comparing the differential gene expression of scarless healing MRL mice and a poorly-healing C57BL/6 mouse strain, identified 36 genes differentiating the healing process between MRL mice and other mice. Study of the regenerative process in these animals is aimed at discovering how to duplicate them in humans, such as deactivation of the p21 gene. However, recent work has shown that MRL mice actually close small ear holes with scar tissue, rather than regeneration as originally claimed.\n\nMRL mice are not protected against myocardial infarction; heart regeneration in adult mammals (neocardiogenesis) is limited, because heart muscle cells are nearly all terminally differentiated. MRL mice show the same amount of cardiac injury and scar formation as normal mice after a heart attack. However, recent studies provide evidence that this may not always be the case, and that MRL mice can regenerate after heart damage. \n\nThe regrowth of lost tissues or organs in the human body is being researched. Some tissues such as skin regrow quite readily; others have been thought to have little or no capacity for regeneration, but ongoing research suggests that there is some hope for a variety of tissues and organs. Human organs that have been regenerated include the bladder, vagina and the penis.\n\nAs are all metazoans, humans are capable of physiological regeneration (i.e. the replacement of cells during homeostatic maintenance that does not necessitate injury). For example, the regeneration of red blood cells via erythropoiesis occurs through the maturation of erythrocytes from hematopoietic stem cells in the bone marrow, their subsequent circulation for around 90 days in the blood stream, and their eventual cell-death in the spleen. Another example of physiological regeneration is the sloughing and rebuilding of a functional endometrium during each menstrual cycle in females in response to varying levels of circulating estrogen and progesterone.\n\nHowever, humans are limited in their capacity for reparative regeneration, which occurs in response to injury. One of the most studied regenerative responses in humans is the hypertrophy of the liver following liver injury. For example, the original mass of the liver is re-established in direct proportion to the amount of liver removed following partial hepatectomy, which indicates that signals from the body regulate liver mass precisely, both positively and negatively, until the desired mass is reached. This response is considered cellular regeneration (a form of compensatory hypertrophy) where the function and mass of the liver is regenerated through the proliferation of existing mature hepatic cells (mainly hepatocytes), but the exact morphology of the liver is not regained. This process is driven by growth factor and cytokine regulated pathways. The normal sequence of inflammation and regeneration does not function accurately in cancer. Specifically, cytokine stimulation of cells leads to expression of genes that change cellular functions and suppress the immune response.\n\nAdult neurogenesis is also a form of cellular regeneration. For example, hippocampal neuron renewal occurs in normal adult humans at an annual turnover rate of 1.75% of neurons. Cardiac myocyte renewal has been found to occur in normal adult humans, and at a higher rate in adults following acute heart injury such as infarction. Even in adult myocardium following infarction, proliferation is only found in around 1% of myocytes around the area of injury, which is not enough to restore function of cardiac muscle. However, this may be an important target for regenerative medicine as it implies that regeneration of cardiomyocytes, and consequently of myocardium, can be induced.\n\nAnother example of reparative regeneration in humans is fingertip regeneration, which occurs after phalange amputation distal to the nail bed (especially in children) and rib regeneration, which occurs following osteotomy for scoliosis treatment (though usually regeneration is only partial and may take up to 1 year).\n\nThe ability and degree of regeneration in reptiles differs among the various species, but the most notable and well-studied occurrence is tail-regeneration in lizards. In addition to lizards, regeneration has been observed in the tails and maxillary bone of crocodiles and adult neurogenesis has also been noted. Tail regeneration has never been observed in snakes. Lizards possess the highest regenerative capacity as a group. Following autotomous tail loss, epimorphic regeneration of a new tail proceeds through a blastema-mediated process that results in a functionally and morphologically similar structure.\n\nStudies have shown that some chondrichthyans can regenerate rhodopsin by cellular regeneration, micro RNA organ regeneration, teeth physiological teeth regeneration, and reparative skin regeneration. Rhodopsin regeneration has been studied in skates and rays. After complete photo-bleaching, rhodopsin can completely regenerate within 2 hours in the retina. White bamboo sharks can regenerate at least two-thirds of their liver and this has been linked to three micro RNAs, xtr-miR-125b, fru-miR-204, and has-miR-142-3p_R-. In one study two thirds of the liver was removed and within 24 hours more than half of the liver had undergone hypertrophy. Leopard sharks routinely replace their teeth every 9–12 days and this is an example of physiological regeneration. This can occur because shark teeth are not attached to a bone, but instead are developed within a bony cavity. It has been estimated that the average shark loses about 30,000 to 40,000 teeth in a lifetime. Some sharks can regenerate scales and even skin following damage. Within two weeks of skin wounding the mucus is secreted into the wound and this initiates the healing process. One study showed that the majority of the wounded area was regenerated within 4 months, but the regenerated area also showed a high degree of variability.\n\n\n\n"}
{"id": "1571927", "url": "https://en.wikipedia.org/wiki?curid=1571927", "title": "Social support", "text": "Social support\n\nSocial support is the perception and actuality that one is cared for, has assistance available from other people, and most popularly, that one is part of a supportive social network. These supportive resources can be emotional (e.g., nurturance), tangible (e.g., financial assistance), informational (e.g., advice), or companionship (e.g., sense of belonging) and intangible (e.g., personal advice).\n\nSocial support can be measured as the perception that one has assistance available, the actual received assistance, or the degree to which a person is integrated in a social network. Support can come from many sources, such as family, friends, pets, neighbors, coworkers, organizations, etc. Government-provided social support is often referred to as public aid in foreign nations. \n\nSocial support is studied across a wide range of disciplines including psychology, medicine, sociology, nursing, public health, education, rehabilitation, and social work. Social support has been linked to many benefits for both physical and mental health, but \"social support\" (e.g., gossiping about friends) is not always beneficial.\n\nSocial support theories and models were prevalent as intensive academic studies in the 1980s and 1990s , and are linked to the development of caregiving and payment models, and community delivery systems in the US and around the world. \n\nTwo main models have been proposed to describe the link between social support and health: the buffering hypothesis and the direct effects hypothesis. Gender and cultural differences in social support have also been found in fields such as education \"which may not control for age, disability, income and social status, ethnic and racial, or other significant factors\".\n\nSocial support can be categorized and measured in several different ways.\n\nThere are four common functions of social support:\n\n\nResearchers also commonly make a distinction between perceived and received support. \"Perceived support\" refers to a recipient’s subjective judgment that providers will offer (or have offered) effective help during times of need. \"Received support\" (also called enacted support) refers to specific supportive actions (e.g., advice or reassurance) offered by providers during times of need.\n\nFurthermore, social support can be measured in terms of structural support or functional support. \"Structural support\" (also called \"social integration\") refers to the extent to which a recipient is connected within a social network, like the number of social ties or how integrated a person is within his or her social network. Family relationships, friends, and membership in clubs and organizations contribute to social integration. \"Functional support\" looks at the specific functions that members in this social network can provide, such as the emotional, instrumental, informational, and companionship support listed above.\nData suggests that emotional support may play a more significant role in protecting individuals from the deleterious effects of stress than structural means of support, such as social involvement or activity.\n\nThese different types of social support have different patterns of correlations with health, personality, and personal relationships. For example, perceived support is consistently linked to better mental health whereas received support and social integration are not. In fact, research indicates that perceived social support that is untapped can be more effective and beneficial than utilized social support. Some have suggested that invisible support, a form of support where the person has support without his or her awareness, may be the most beneficial.\n\nSocial support can come from a variety of sources, including (but not limited to): family, friends, romantic partners, pets, community ties, and coworkers. Sources of support can be natural (e.g., family and friends) or more formal (e.g., mental health specialists or community organizations). \nThe source of the social support is an important determinant of its effectiveness as a coping strategy. Support from a romantic partner is associated with health benefits, particularly for men. However, one study has found that although support from spouses buffered the negative effects of work stress, it did not buffer the relationship between marital and parental stresses, because the spouses were implicated in these situations.However, work-family specific support worked more to alleviate work-family stress that feeds into marital and parental stress. Employee humor is negatively associated with burnout, and positively with, stress, health and stress coping effectiveness. Additionally, social support from friends did provide a buffer in response to marital stress, because they were less implicated in the marital dynamic.\n\nEarly familial social support has been shown to be important in children’s abilities to develop social competencies, and supportive parental relationships have also had benefits for college-aged students. Teacher and school personnel support have been shown to be stronger than other relationships of support. This is hypothesized to be a result of family and friend social relationships to be subject to conflicts whereas school relationships are more stable.\n\nSocial support is also available among social media sites. As technology advances, the availability for online support increases. Social support can be offered through social media websites such as blogs, Facebook groups, health forums, and online support groups. According to Hwang, the support is similar to face-to-face social support, but also offers the unique aspects of convenience, anonymity, and non-judgmental interactions. Support sought through social media also provides users with emotional comfort that relates them to others while creating awareness about particular health issues.\n\nResearch conducted by Winzelberg et al. evaluated an online support group for women with breast cancer finding participants were able to form fulfilling supportive relationships in an asynchronous format and this form of support proved to be effective in reducing participants' scores on depression, perceived stress, and cancer-related trauma measures. This type of online communication can increase the ability to cope with stress. Social support through social media is available to everyone with internet access and allows users to create relationships and receive encouragement for whatever issue they may be facing.\n\nCoulson claims online support groups provide a unique opportunity for health professionals to learn about the experiences and views of individuals. This type of social support can also benefit users by providing them with a variety of information. Seeking informational social support allows users to access suggestions, advice, and information regarding health concerns or recovery. Many need social support, and with its emergence on social media access can be obtained from a wider range of people in need. Wong and Ma (2016) have done research that shows online social support affects users' online subjective well-being. \n\nSocial support profile is associated with increased psychological well-being in the workplace and in response to important life events.\nIn stressful times, social support helps people reduce psychological distress (e.g., anxiety or depression). Social support can simultaneously function as a problem-focused (e.g. receiving tangible information that helps resolve an issue) and emotion-focused coping strategy (e.g. used to regulate emotional responses that arise from the stressful event) Social support ≤has been found to promote psychological adjustment in conditions with chronic high stress like HIV, rheumatoid arthritis, cancer, stroke, and coronary artery disease. Additionally, social support has been associated with various acute and chronic pain variables (for more information, see Chronic pain).\n\nPeople with low social support report more sub-clinical symptoms of depression and anxiety than do people with high social support. In addition, people with low social support have higher rates of major mental disorder than those with high support. These include posttraumatic stress disorder, panic disorder, social phobia, major depressive disorder, dysthymic disorder, and eating disorders. Among people with schizophrenia, those with low social support have more symptoms of the disorder. In addition, people with low support have more suicidal ideation, and more alcohol and (illicit and prescription) drug problems. Similar results have been found among children. Religious coping has especially been shown to correlate positively with positive psychological adjustment to stressors with enhancement of faith-based social support hypothesized as the likely mechanism of effect. However, more recent research reveals the role of religiosity/spirituality in enhancing social support may be overstated and in fact disappears when the personality traits of \"agreeableness\" and \"conscientiousness\" are also included as predictors.\n\nIn a 2013 study, Akey et al. did a qualitative study of 34 men and women diagnosed with an eating disorder and used the Health Belief Model (HBM) to explain the reasons for which they forgo seeking social support. Many people with eating disorders have a low perceived susceptibility, which can be explained as a sense of denial about their illness. Their perceived severity of the illness is affected by those to whom they compare themselves to, often resulting in people believing their illness is not severe enough to seek support. Due to poor past experiences or educated speculation, the perception of benefits for seeking social support is relatively low. The number of perceived barriers towards seeking social support often prevents people with eating disorders from getting the support they need to better cope with their illness. Such barriers include fear of social stigma, financial resources, and availability and quality of support. Self-efficacy may also explain why people with eating disorders do not seek social support, because they may not know how to properly express their need for help. This research has helped to create a better understanding of why individuals with eating disorders do not seek social support, and may lead to increased efforts to make such support more available. Eating disorders are classified as mental illnesses but can also have physical health repercussions. Creating a strong social support system for those affected by eating disorders may help such individuals to have a higher quality of both mental and physical health.\n\nVarious studies have been performed examining the effects of social support on psychological distress. Interest in the implications of social support were triggered by a series of articles published in the mid-1970s, each reviewing literature examining the association between psychiatric disorders and factors such as change in marital status, geographic mobility, and social disintegration. Researchers realized that the theme present in each of these situations is the absence of adequate social support and the disruption of social networks. This observed relationship sparked numerous studies concerning the effects of social support on mental health.\n\nOne particular study documented the effects of social support as a coping strategy on psychological distress in response to stressful work and life events among police officers. Talking things over among coworkers was the most frequent form of coping utilized while on duty, whereas most police officers kept issues to themselves while off duty. The study found that the social support between co-workers significantly buffered the relationship between work-related events and distress.\n\nOther studies have examined the social support systems of single mothers. One study by D'Ercole demonstrated that the effects of social support vary in both form and function and will have drastically different effects depending upon the individual. The study found that supportive relationships with friends and co-workers, rather than task-related support from family, was positively related to the mother's psychological well-being. D'Ercole hypothesizes that friends of a single parent offer a chance to socialize, match experiences, and be part of a network of peers. These types of exchanges may be more spontaneous and less obligatory than those between relatives. Additionally, co-workers can provide a community away from domestic life, relief from family demands, a source of recognition, and feelings of competence. D'Ercole also found an interesting statistical interaction whereby social support from co-workers decreased the experience of stress only in lower income individuals. The author hypothesizes that single women who earn more money are more likely to hold more demanding jobs which require more formal and less dependent relationships. Additionally, those women who earn higher incomes are more likely to be in positions of power, where relationships are more competitive than supportive.\n\nMany studies have been dedicated specifically to understanding the effects of social support in individuals with posttraumatic stress disorder (PTSD). In a study by Haden et al., when victims of severe trauma perceived high levels of social support and engaged in interpersonal coping styles, they were less likely to develop severe PTSD when compared to those who perceived lower levels of social support. These results suggest that high levels of social support alleviate the strong positive association between level of injury and severity of PTSD, and thus serves as a powerful protective factor. In general, data shows that the support of family and friends has a positive influence on an individual's ability to cope with trauma. In fact, a meta-analysis by Brewin et al. found that social support was the strongest predictor, accounting for 40%, of variance in PTSD severity. However, perceived social support may be directly affected by the severity of the trauma. In some cases, support decreases with increases in trauma severity.\n\nCollege students have also been the target of various studies on the effects of social support on coping. Reports between 1990 and 2003 showed college stresses were increasing in severity. Studies have also shown that college students' perceptions of social support have shifted from viewing support as stable to viewing them as variable and fluctuating. In the face of such mounting stress, students naturally seek support from family and friends in order to alleviate psychological distress. A study by Chao found a significant two-way correlation between perceived stress and social support, as well as a significant three-way correlation between perceived stress, social support, and dysfunctional coping. The results indicated that high levels of dysfunctional coping deteriorated the association between stress and well-being at both high and low levels of social support, suggesting that dysfunctional coping can deteriorate the positive buffering action of social support on well-being. Students who reported social support were found more likely to engage in less healthy activities, including sedentary behavior, drug and alcohol use, and too much or too little sleep. Lack of social support in college students is also strongly related to life dissatisfaction and suicidal behavior.\n\nSocial support has a clearly demonstrated link to physical health outcomes in individuals, with numerous ties to physical health including mortality. People with low social support are at a much higher risk of death from a variety of diseases (e.g., cancer or cardiovascular disease). Numerous studies have shown that people with higher social support have an increased likelihood for survival.\n\nIndividuals with lower levels of social support have: more cardiovascular disease, more inflammation and less effective immune system functioning, more complications during pregnancy, and more functional disability and pain associated with rheumatoid arthritis, among many other findings. Conversely, higher rates of social support have been associated with numerous positive outcomes, including faster recovery from coronary artery surgery, less susceptibility to herpes attacks, a lowered likelihood to show age-related cognitive decline, and better diabetes control. People with higher social support are also less likely to develop colds and are able to recover faster if they are ill from a cold. There is sufficient evidence linking cardiovascular, neuroendocrine, and immune system function with higher levels of social support. Social support predicts less atherosclerosis and can slow the progression of an already diagnosed cardiovascular disease. There is also a clearly demonstrated link between social support and better immune function, especially in older adults. While links have been shown between neuroendocrine functionality and social support, further understanding is required before specific significant claims can be made. Social support is also hypothesized to be beneficial in the recovery from less severe cancers. Research focuses on breast cancers, but in more serious cancers factors such as severity and spread are difficult to operationalize in the context of impacts of social support. The field of physical health often struggles with the confoundment of variables by external factors that are difficult to control, such as the entangled impact of life events on social support and the buffering impact these events have. There are serious ethical concerns involved with controlling too many factors of social support in individuals, leading to an interesting crossroads in the research.\n\nSocial support is integrated into service delivery schemes and sometimes are a primary service provided by governmental contracted entities (e.g., companionship, peer services, family caregiving). Community services known by the nomenclature community support, and workers by a similar title, Direct Support Professional, have a base in social and community support \"ideology\". All supportive services from supported employment to supported housing, family support, supported education, and supportive living are based upon the relationship between \"informal and formal\" supports, and \"paid and unpaid caregiving\". Inclusion studies, based upon affiliation and friendship, or the conversely, have a similar theoretical basis as do \"person-centered support\" strategies.\n\nSocial support theories are often found in \"real life\" in cultural, music and arts communities, and as might be expected within religious communities. Social support is integral in theories of aging, and the \"social care systems\" have often been challenged (e.g., creativity throughout the lifespan, extra retirement hours). Ed Skarnulis' (state director) adage, \"Support, don't supplant the family\" applies to other forms of social support networks.\n\nAlthough there are many benefits to social support, it is not always beneficial. It has been proposed that in order for social support to be beneficial, the social support desired by the individual has to match the support given to him or her; this is known as the matching hypothesis. \nPsychological stress may increase if a different type of support is provided than what the recipient wishes to receive (e.g., informational is given when emotional support is sought). Additionally, elevated levels of perceived stress can impact the effect of social support on health-related outcomes.\n\nOther costs have been associated with social support. For example, received support has not been linked consistently to either physical or mental health; perhaps surprisingly, received support has sometimes been linked to worse mental health. Additionally, if social support is overly intrusive, it can increase stress. It is important when discussing social support to always consider the possibility that the social support system is actually an antagonistic influence on an individual.\n\nThere are two dominant hypotheses addressing the link between social support and health: the buffering hypothesis and the direct effects hypothesis. The main difference between these two hypotheses is that the direct effects hypothesis predicts that social support is beneficial all the time, while the buffering hypothesis predicts that social support is mostly beneficial during stressful times. Evidence has been found for both hypotheses.\n\nIn the buffering hypothesis, social support protects (or \"buffers\") people from the bad effects of stressful life events (e.g., death of a spouse, job loss). Evidence for stress buffering is found when the correlation between stressful events and poor health is weaker for people with high social support than for people with low social support. The weak correlation between stress and health for people with high social support is often interpreted to mean that social support has protected people from stress. Stress buffering is more likely to be observed for perceived support than for social integration or received support. The theoretical concept or construct of resiliency is associated with coping theories. \n\nIn the direct effects (also called main effects) hypothesis, people with high social support are in better health than people with low social support, regardless of stress. In addition to showing buffering effects, perceived support also shows consistent direct effects for mental health outcomes. Both perceived support and social integration show main effects for physical health outcomes. However, received (enacted) support rarely shows main effects.\n\nSeveral theories have been proposed to explain social support’s link to health. Stress and coping social support theory dominates social support research and is designed to explain the buffering hypothesis described above. According to this theory, social support protects people from the bad health effects of stressful events (i.e., stress buffering) by influencing how people think about and cope with the events. An example in 2018 are the effects of school shootings on the well being and future of children and children's health. According to stress and coping theory, events are stressful insofar as people have negative thoughts about the event (appraisal) and cope ineffectively. Coping consists of deliberate, conscious actions such as problem solving or relaxation. As applied to social support, stress and coping theory suggests that social support promotes adaptive appraisal and coping. Evidence for stress and coping social support theory is found in studies that observe stress buffering effects for perceived social support. One problem with this theory is that, as described previously, stress buffering is not seen for social integration, and that received support is typically not linked to better health outcomes.\n\nRelational regulation theory (RRT) is another theory, which is designed to explain main effects (the direct effects hypothesis) between perceived support and mental health. As mentioned previously, perceived support has been found to have both buffering and direct effects on mental health. RRT was proposed in order to explain perceived support’s main effects on mental health which cannot be explained by the stress and coping theory. RRT hypothesizes that the link between perceived support and mental health comes from people regulating their emotions through ordinary conversations and shared activities rather than through conversations on how to cope with stress. This regulation is relational in that the support providers, conversation topics and activities that help regulate emotion are primarily a matter of personal taste. This is supported by previous work showing that the largest part of perceived support is relational in nature.\n\nLife-span theory is another theory to explain the links of social support and health, which emphasizes the differences between perceived and received support. According to this theory, social support develops throughout the life span, but especially in childhood attachment with parents. Social support develops along with adaptive personality traits such as low hostility, low neuroticism, high optimism, as well as social and coping skills. Together, support and other aspects of personality (\"psychological theories\") influence health largely by promoting health practices (e.g., exercise and weight management) and by preventing health-related stressors (e.g., job loss, divorce). Evidence for life-span theory includes that a portion of perceived support is trait-like, and that perceived support is linked to adaptive personality characteristics and attachment experiences. Lifespan theories are popular from their origins in Schools of Human Ecology at the universities, aligned with family theories, and researched through federal centers over decades (e.g., University of Kansas, Beach Center for Families; Cornell University, School of Human Ecology). \n\nOf the Big Five Personality Traits, agreeableness is associated with people receiving the most social support and having the least-strained relationships at work and home. Receiving support from a supervisor in the workplace is associated with alleviating tensions both at work and at home, as are interdependency and idiocentrism of an employee.\n\nMany studies have tried to identify biopsychosocial pathways for the link between social support and health. Social support has been found to positively impact the immune, neuroendocrine, and cardiovascular systems. Although these systems are listed separately here, evidence has shown that these systems can interact and affect each other.\n\n\nThough many benefits have been found, not all research indicates positive effects of social support on these systems. For example, sometimes the presence of a support figure can lead to increased neuroendocrine and physiological activity.\n\nSocial support groups can be a source of informational support, by providing valuable educational information, and emotional support, including encouragement from people experiencing similar circumstances. Studies have generally found beneficial effects for social support group interventions for various conditions, including Internet support groups. These groups may be termed \"self help\" groups in nation-states, may be offered by non-profit organizations, and in 2018, may be paid for as part of governmental reimbursement schemes.\n\nThere are both costs and benefits to providing support to others. Providing long-term care or support for someone else is a chronic stressor that has been associated with anxiety, depression, alterations in the immune system, and increased mortality. Thus, family caregivers and \"university personnel\" alike have advocated for both respite or relief, and higher payments related to ongoing, long-term caregiving. However, providing support has also been associated with health benefits. In fact, providing instrumental support to friends, relatives, and neighbors, or emotional support to spouses has been linked to a significant decrease in the risk for mortality. Also, a recent neuroimaging study found that giving support to a significant other during a distressful experience increased activation in reward areas of the brain.\n\nIn 1959 Isabel Menzies Lyth identified that threat to a person’s identity in a group where they share similar characteristics develops a defence system inside the group which stems from emotions experienced by members of the group, which are difficult to articulate, cope with and finds solutions to. Together with an external pressure on efficiency a collusive and injunctive system develops that is resistant to change, supports their activities and prohibit others from performing their major tasks.\n\nGender differences have been found in social support research. Women provide more social support to others and are more engaged in their social networks. Evidence has also supported the notion that women may be better providers of social support. In addition to being more involved in the giving of support, women are also more likely to seek out social support to deal with stress, especially from their spouses. However, one study indicates that there are no differences in the extent to which men and women seek appraisal, informational, and instrumental types of support. Rather, the big difference lies in seeking emotional support. Additionally, social support may be more beneficial to women. Shelley Taylor and her colleagues have suggested that these gender differences in social support may stem from the biological difference between men and women in how they respond to stress (i.e., flight or fight versus tend and befriend). Married men are less likely to be depressed compared to non-married men after the presence of a particular stressor because men are able to delegate their emotional burdens to their partner, and women have been shown to be influenced and act more in reaction to social context compared to men. It has been found that men’s behaviors are overall more antisocial, with less regard to the impact their coping may have upon others, and women more prosocial-active with importance stressed on how their coping affects people around them. This may explain why women are more likely to experience negative psychological problems such as depression and anxiety based on how women receive and process stressors. In general, women are likely to find situations more stressful than males are. It is important to note that when the perceived stress level is the same, men and women have much fewer differences in how they seek and use social support.\n\nAlthough social support is thought to be a universal resource, cultural differences exist in social support. In many Asian cultures, the person is seen as more of a collective unit of society, whereas Western cultures are more individualistic and conceptualize social support as a transaction in which one person seeks help from another. In more interdependent Eastern cultures, people are less inclined to enlist the help of others. For example, European Americans have been found to call upon their social relationships for social support more often than Asian Americans or Asians during stressful occasions, and Asian Americans expect social support to be less helpful than European Americans. These differences in social support may be rooted in different cultural ideas about social groups. It is important to note that these differences are stronger in emotional support than instrumental support. Additionally, ethnic differences in social support from family and friends have been found.\n\nCultural differences in coping strategies other than social support also exist. One study shows that Koreans are more likely to report substance abuse than European Americans are. Further, European Americans are more likely to exercise in order to cope than Koreans. Some cultural explanations are that Asians are less likely to seek it from fear of disrupting the harmony of their relationships and that they are more inclined to settle their problems independently and avoid criticism. However, these differences are not found among Asian Americans relative to their Europeans American counterparts.\n\n"}
{"id": "39776872", "url": "https://en.wikipedia.org/wiki?curid=39776872", "title": "Sullivan's Index", "text": "Sullivan's Index\n\nSullivan's index also known as Disability Free Life Expectancy (DFLE) is a method to compute life expectancy free of disability. It is calculated by formula: <br>\nLife expectancy formula_1 duration of disability\n\n\n"}
{"id": "52549787", "url": "https://en.wikipedia.org/wiki?curid=52549787", "title": "Tampon tax", "text": "Tampon tax\n\nA tampon tax is a popular term used to call attention to the fact that tampons—and other feminine hygiene products—are subject to value-added tax, unlike the tax exemption status granted to other products considered basic necessities. Proponents of tax exemption argue that tampons, sanitary napkins, menstrual cups and comparable products constitute basic, unavoidable necessities for women and thus should be made tax exempt.\n\nProponents argue that feminine hygiene products serving the basic menstrual cycle should be classified alongside other unavoidable, tax exempt necessities, such as groceries and personal medical items. The BBC estimates that women—half of the global population—need to use feminine hygiene products for about a week each month for about 30 years. While sales tax policy varies across jurisdictions, these products were typically taxed at the same rate as non-essential goods, such as in the United States, while other countries, such as the United Kingdom and Ireland, reduced or eliminated their general consumption tax on sanitary products. When asked about equivalent exemptions for men, proponents argue that no male products, condoms included, are comparable to feminine hygiene products, since menstruation is biological and \"feminine hygiene is not a choice\". As the vast majority of consumers of feminine hygiene products are women, the increased cost has been criticized as being discriminatory against women. The tampon tax is not a special tax levied directly on feminine hygiene products.\n\nAfter the tax in Canada was removed mid-2015, women began protesting in other countries later that year. On July 21, 2018, India eliminated the Goods and Services Tax (GST) of 12% from sanitary napkins.\n\n\nThe United Kingdom has levied a value-added tax on sanitary products since it joined the European Economic Community in 1973. This rate was reduced to 5% specifically for sanitary products in 2000 with lobbying from Member of Parliament Dawn Primarolo saying that this reduction was \"about fairness, and doing what we can to lower the cost of a necessity.\" This is the lowest rate possible under the European Union's value added tax law, which as of 2015 does not allow zero rates. The UK Independence Party raised the issue in the 2015 general election with promises to withdraw from the European Union and allow the zero rate. Prime Minister David Cameron commented, when prompted, that the tampon tax campaign was \"long-standing\" and a complicated issue within the European Union. In England, one in ten women between 14 and 21 cannot afford menstrual management products.\n\nLaura Coryton led a \"Stop taxing periods, period\" campaign with an online petition to have the European Union remove the value-added tax for sanitary products. George Osborne mentioned the petition by name in his 2015 Autumn Statement pledge to end the tampon tax at the European Union level. The petition platform's CEO cited the campaign as an example of successful clicktivism, with over 320,000 signatures. In March 2016, Parliament created legislation to eliminate the tampon VAT. It was expected to go into effect by April 2018 but did not do so; several British women protested for it publicly while displaying blood stains from their periods. On the 3rd October 2018, new EU VAT rules that will allow the UK to stop taxing sanitary products were approved by the European Parliament.\n\nIn July 2017, a pilot program began in Scotland to have free sanitary products available at schools and food banks for women who cannot afford them. The pilot scheme was launched for six months in Aberdeen, Scotland, with £42,500 of funding from the devolved Scottish Government in order to address the growing scandal of \"period poverty\". It was believed 1,000 girls would benefit from the scheme, as there were reports of teenage girls using tissues, toilet roll, torn T-shirts and even newspaper as makeshift sanitary products, with some girls even skipping school altogether. It was decided to launch the scheme to improve attainment and school attendance, as well as improve confidence amongst teenage girls during their period; and Scotland is believed to be the first country in the world to give out free sanitary products as part of a government-sponsored initiative.\n\nFurther to this half year pilot program, Scotland's opposition Labour Party intends to introduce a bill to make this permanent. Scotland is the first country to ban period poverty.\n\nA study by the WHO and UNICEF showed that one out of five women in Scotland have been forced to improvise with items including toilet paper and old clothes due to the high cost of commercial products.\n\nIn the United States, almost all states tax \"tangible individual property\" but exempt non-luxury \"necessities\": groceries, prescriptions, prosthetics, agriculture supplies, and sometimes clothes—the exemptions vary between states. Five states do not have a state sales tax (Alaska, Delaware, Montana, New Hampshire, and Oregon), and , ten states specifically exempted essential hygiene products (Connecticut, Florida, Illinois, Maryland, Massachusetts, Minnesota, New Jersey, New York, Nevada, and Pennsylvania).\n\nIn the U.S., most states charge sales tax for women's pads and tampons. Ten states have dropped the tampon tax — Minnesota, Illinois, Nevada, Pennsylvania, New York, Massachusetts, Maryland, New Jersey, Connecticut and Florida, according to NPR and CNN. Seven other states have introduced such legislation, most recently Nebraska, Virginia and Arizona. In January 2018, California rejected a proposal to eliminate tampon tax.\n\nMany federal assistance programs such as SNAP (Supplemental Nutrition Assistance Program) and WIC (Women, Infants and Children) don’t allow the use of an EBT for products such as pads or tampons despite the products' classification as medical devices.  The IRS does not classify female products as medical devices, thus blocking women from buying them with pre-tax dollars in both flexible spending accounts and health savings accounts. \n\nThere has been some changes to the tampon taxes but most of these changes are state level or by city. On a smaller scale, individual cities have also changed their laws in favor of eliminating the tampon tax. \n\nCalifornia Assemblywoman Cristina Garcia reported that California women each pay roughly US$7 per month over 40 years, constituting US$20 million in annual taxes. Garcia and Ling Ling Chang proposed a bill to remove the tampon tax in early 2016. At this period, only a handful of the country's states exempted tampons, and several others had no state sales tax. Garcia held that women were taxed \"for being women\" and bore an economic burden for having no other choice but to buy these products. Garcia and Chang added that the tax was \"regulatory discrimination\" that disproportionately affected poor women and women of color, and that it likely persisted due to social taboos against discussing menstruation. Both houses of the California State Legislature voted to exempt tampons from taxation in June 2016, but the bill was vetoed by the state's governor, Jerry Brown, three months later.\n\nCalifornia Gov. Jerry Brown vetoed AB-1561 due to the potential loss of money in taxing feminine hygiene products. In response, Cristina Garcia co-authored AB-0479: Common Cents Tax Reform Act with Lorena Gonzalez Fletcher, which is a new measure outlining a solution to offset the feminine product and diaper tax exemption by increasing the tax on hard liquor.\n\nIn 2017, California State Legislature passed AB 10 requiring public middle schools and high schools where at least 40% of students meet the federal poverty level to stock half of the restrooms with free tampons and sanitary napkins. The law was passed in an effort to eliminate the cost burden and keep low-income students in schools during their menstrual cycle.\n\nCompanies involved in supplying the necessary feminine hygiene products (tampons and pads) for complete menstrual care in the restrooms of schools include WAXIE and Hospeco. They also supply various options for menstrual product dispensers that have a time delay mechanism to prevent products from being overused and/or abused.\n\nIn July 2016, New York State exempted feminine hygiene products from taxation, reducing the state's tax revenue by an estimated US$10 million annually. Connecticut and Illinois also removed their tax in 2016, with Florida following suit in 2017.\n\nA 2018 empirical study on New Jersey's 2005 tax break on menstrual products found that \"repealing tampon taxes removes an unequal tax burden and could make menstrual hygiene products more accessible for low-income consumers.\" The study utilized data from more than 16,000 purchases in 2004-2006 made in New Jersey, Delaware, Connecticut, Maryland, and Pennsylvania, using these latter nearby states as the control group. Through a differences-in-differences approach, they found that after the repeal, consumer prices on menstrual products decreased by 7.3%, relative to the control states. This was greater than the 6.9% sales tax, suggesting that the consumers benefitted from the tax break. Upon further analysis, the study also found that the decrease in consumer prices was greater for low-income consumers than high-income consumers (3.9% decrease versus 12.4% decrease). This suggests that low-income consumers received the most benefit from the tax break, while high-income consumers shared the benefit with producers of menstrual products.\n\nSupporters of the exemption of said taxes are calling their efforts \"menstrual equity\", explaining it as a social movement that strives for feminine products like tampons to be considered necessities. Things that are considered necessities, for example toilet paper, are not taxed. Activists are often being led by members of government. Councilwomen Julissa Ferreras-Copeland led a movement with a tampon tax pilot project ultimately providing free pads and tampons at a local high school in Queens, New York. Ferreras-Copeland's effort has now been expanded into 25 different schools around New York City. Other democrats including Ydanis Rodriguez and council speaker Melissa Mark-Viverito are advocating for state legislature to stop taxing sanitary products. \n\nFree the Tampon, an advocate for free menstrual products estimates that it would cost less than $5 a year per user to provide tampons and pads in restrooms at schools and businesses.\n\n"}
{"id": "54729058", "url": "https://en.wikipedia.org/wiki?curid=54729058", "title": "World Mental Health survey initiative", "text": "World Mental Health survey initiative\n\nThe World Mental Health Survey Initiative is a collaborative project by World Health Organization, Harvard University, University of Michigan, and country-based researchers worldwide to coordinate the analysis and implementation of epidemiological surveys of mental and behavioral disorders and substance abuse in all WHO Regions.\n\nIt is estimated that the burden of mental and addictive disorders are among the highest in the world with expected increase over the next decades. However, those estimations are not based on cross-sectional epidemiological surveys, rather, they are mainly based on literature reviews and isolated studies.\nThe WMH Survey Initiative aim is to accurately address the global burden of mental disorders by obtaining accurate cross-sectional information about the prevalences and correlates of mental and behavioral disorders as well as substance abuse, allowing for evaluation of risk factors and study of patterns of service use in order to target appropriate interventions.\n\nCollaborators in this survey come from all WHO regions of the world, with 27 participating countries.\n"}
