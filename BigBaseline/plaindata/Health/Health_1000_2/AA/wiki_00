{"id": "38509307", "url": "https://en.wikipedia.org/wiki?curid=38509307", "title": "10 Essential Public Health Services", "text": "10 Essential Public Health Services\n\nThe 10 Essential Public Health Services is a US government document which codifies the responsibilities of public health agencies and institutions in the United States.\n\nOrganized according to the \"three fundamental purposes of public health\" — assessment, policy development, and assurance — the essential services include the following:\n\n\n\n\nIn 1988, the Institute of Medicine (IOM) released an assessment of the U.S. public health system titled, \"The Future of Public Health\". The report described the network of county, state, and national public health agencies as being in \"disarray\" and prompted a national discussion about the state of public health in the country. Questioning the ability of existing public health systems to provide essential services, the report sought to establish a comprehensive framework delineating the \"three fundamental purposes of public health.\" These purposes included \"assessment, policy development, and assurance.\"\n\nThough health policy academicians identified with and understood the IOM framework, policy makers found its highly conceptual language difficult to apply. Therefore, as part of President Clinton’s 1994 healthcare reform efforts, a federal working-group was tasked with reviewing and supplementing the framework. Under the auspices of both the Center for Disease Control and Prevention’s (CDC) Public Health Practice Program Office and the Office of Disease Prevention and Health Promotion, the group sought to\n\nThese efforts culminated in the publication of \"The Essential Services of Public Health\" in late 1994.\nPublic Health Units\n\nThe report was well received. Public health agencies and professional organizations began to align guidelines and employ self-assessment tools in support of the ten Essential Services. The CDC launched a number of research projects aimed at developing strategies for measuring how well public health agencies provided the services. Many of these studies were the first of their kind and ushered in an era of health agency monitoring and assessment.\n\nIn 2002, the CDC and several national public health associations launched the National Public Health Performance Standards Program (NPHPSP). With the goal of developing a consensus-based set of performance standards for state and local public health delivery systems, the NPHPSP adopted the Essential Public Health Services as \"the fundamental framework\" underpinning its assessment strategy. The NPHPSP consists of three instruments — The State Public Health System Assessment, The Local Public Health System Assessment, and The Local Public Health Governance Assessment — and covers the gamut of public health action as described by the Essential Public Health Services.\n\nIt is essential for public health organizations to monitor and evaluate the health status of populations in order to identify trends and to target health resources. Components of this service include: utilization of appropriate tools to interpret and disseminate data to audiences of interest; collaboration in integrating and managing public health; and accurate and periodic assessment of the community’s health status. Specifically, public health organizations can monitor and evaluate the health status of their populations by creating a disease reporting system, community health profiles, and health surveys. For example, the Los Angeles Department of Public Health completes a Los Angeles County Health Survey (LACHS) every two to four years to obtain population-based data. This information is used for public health assessment and planning and for evaluating changes in health over time in Los Angeles County.\n\nIn order to appropriately allocate public health resources, it is essential to diagnose and investigate health problems and hazards in the community. Components in this service include: population-based screening of diseases; access to public health laboratories capable of completing rapid screening and high volume testing; and epidemiologic investigations of disease outbreaks and patterns of disease. The Los Angeles County Health and Examination Survey (LA HANES) was piloted in 2011 and aims to establish a profile description of health outcomes of residents of Los Angeles County (e.g. adult obesity and cardiovascular disease is monitored via collection of physical measurements such as blood pressure and body mass index). These survey results will inform public education and chronic disease prevention efforts. Emergency preparedness is also an essential component of public health organizations. Teams must be available and prepared to combat natural disasters, severe weather, outbreaks, bioterrorism, mass casualties and chemical emergencies.\n\nOnce public health priorities have been established through monitoring and investigation of health problems in the community, educational activities that promote improved health should be disseminated. Components in this service include: both the availability of health information and educational resources and the presence of health education and health promotion programs. This can be achieved through media advocacy and social marketing. An example of this is the Eat Less, Weigh Less campaign executed by the Los Angeles County Department of Public Health that aims to increase awareness about appropriate portion sizes using advertisements on buses, billboards and social media. It is also essential to establish health education and health promotion program partnerships with organizations in the community, such as schools, churches and employment facilities.\n\nPublic health organizations on the local, state and national level can mobilize community partnerships to identify and solve health problems. Components of this service include: building coalitions to utilize the full range of available resources; convening and facilitating partnerships that will undertake defined health improvement projects; and provide assistance to partners and communities to solve health problems. Of particular importance is identification of potential stakeholders who will contribute to or benefit from public health activities. First 5 LA is a community partner who supports targeted programs that address the needs of Los Angeles County children ages zero to five and their families. They work closely with the Los Angeles County Department of Public Health to improve the physical and emotional health of this population. It is important to note that many of these stakeholders may not be considered to be health-related at first glance. For example, organizations involved in urban planning may be influential in improving the health of its residents. This could include increasing the walkability of a community or the number of parks or bike trails in a neighborhood.\n\nPolicies can be effective in modifying human behavior and reducing negative health outcomes. Components in this service include: development of policy to guide the practice of public health; alignment of resources and strategies for community health efforts; and systematic health planning strategies to guide community health improvement. In response to increasing rates of obesity and cardiovascular disease, the New York City Board of Health passed a ban on the sale of sodas and other sugary drinks larger than 16 ounces at restaurants, street carts and movie theaters. In addition to policies that can support health efforts, laws can reduce negative health outcomes. For example, dram shop liability is a law that holds the owner or server at a bar or restaurant where a patron consumed their last alcoholic beverage responsible for injuries or deaths caused as a result of alcohol-related incidents. A systematic review completed by the Task Force on Community Preventive Services, found strong evidence of the effectiveness of this law in reducing alcohol-related harms.\n\nIt is important that individuals and organizations comply with existing laws and regulations in order to ensure the overall health and safety of the general public. Components of this service include: reviewing, evaluating, and revising laws and regulations put in place to protect the health and safety of the public; educating persons and organizations about these laws and regulations to improve compliance and encourage enforcement of them; and enforcing actions that protect the health of the public (e.g., protection of drinking water; enforcement of clean air standards; enforcement of laws prohibiting the sale of alcoholic and tobacco products to minors, of laws concerning the use of seat belts and child safety seats; mandating childhood immunizations; facilitating timely follow-ups in the event of hazards and outbreaks of exposure-related diseases; monitoring quality of health services; conducting the timely review of new drugs, biologics, and medical devices; ensuring food safety; and enforcing housing and sanitation codes).\n\nHaving access to care when it is needed is important in helping individuals prevent and avoid unfavorable health outcomes and medical costs. At the local level, components of this service include: identifying populations that face barriers to accessing health services and addressing their personal health needs, assuring the linkage of these populations to appropriate health services by coordinating provider services, and developing and implementing interventions that address the barriers they face in attempting to access care. At the state and governance levels, components of this service include: assessing access to and availability of state health services; partnering with public, private, and non-profit sectors to provide a coordinated system of health care; assuring access to this coordinated health care system by using outreach efforts that link individuals to the health services they need; developing and implementing a continuous improvement process to assure the equitable distribution of resources for those in greatest need of these services. The National HIV/AIDS Strategy (NHAS) employs this service idea as one of the action steps for achieving increased access to care and improved health outcomes for people living with HIV.\n\nHealth care workers and staff who are competent (i.e., skilled in the core principles of public health practice) are more likely to provide care and other services more effectively and efficiently compared to those who are not. Components of this service include: making sure that the workforce meets the health needs of the population, maintaining public health workforce standards by developing and implementing efficient licensure and credentialing processes and incorporating core public health competencies into personnel systems, and adopting continuous quality improvement methods and long-term learning opportunities for public health workforce members. In two 2002 reports, the Institute of Medicine (IOM) recommended instituting a certification examination as a way of ensuring minimum competence in public health. Web-based training strategies may be useful in providing the long-term learning opportunities that many current and upcoming public health workers need in order to serve as well-informed advocates of public health and safety.\n\nGiven scarce resources, it is important to keep track of whether or not programs and/or policies end up producing intended outcomes. Components of this service include: assessing the accessibility, quality and effectiveness of services and programs delivered; providing policymakers with the information they need in order to make well-informed decisions regarding the allocation of scarce resources; tracking efficiency, effectiveness, and quality of services analyzing data on health status and service utilization; and striving to improve the public health system’s capacity to well serve the population. Cost-effectiveness analysis has been proposed as one possible strategy for informing policymakers on how best to allocate health care resources.\n\nThrough research, the health and health care problems that individuals face can be better understood, and therefore be better and more appropriately addressed given the evidence provided by such research efforts. Components of this service include: fostering the development of a continuum of innovative solutions for health programming in terms of both practical field-based efforts as well as academic efforts, establishing a consortium of research institutions and other institutions of higher learning to encourage more collaborative and cross-cutting efforts, and ensuring the public health system’s capacity to perform timely epidemiological and health policy analyses.\n\n\n"}
{"id": "39993576", "url": "https://en.wikipedia.org/wiki?curid=39993576", "title": "5-SPICE framework", "text": "5-SPICE framework\n\nThe 5-SPICE framework is an instrument designed for global health practitioners to guide discussions about community health worker (CHW) projects.\n\nThe 5-SPICE framework was developed by clinicians and researchers from Partners In Health, Harvard Medical School, and Brigham and Women’s Hospital in Boston, MA. The framework lays out a model for integrating community health workers into public health systems, learning from the experiences Partners In Health and partner organizations at their project sites in resource-poor settings around the world. 5-SPICE draws upon experiences from Haiti, Rwanda, Lesotho, Liberia, Nepal, Mali, and elsewhere, where CHWs have been employed to improve patient outcomes and overcome personnel shortages. The framework allows for all stakeholders in a community health program to participate in discussions and analyses to strengthen the impact of CHWs.\n\n5-SPICE allows for all stakeholders in a community health program to participate in discussions and analyses to strengthen the impact of community health workers.\n\nThe name 5-SPICE is derived from Chinese cuisine emphasizing the balance between inputs and elements. The five main elements form an acronym:\n\nThese elements are not a static list, but a way to holistically analyze how core programmatic elements affect each other in the field. In the Freirean tradition of awareness, the 5-SPICE model emphasizes facilitated discussion and contemplation among stakeholders, particularly CHWs, to maximize program outputs. Ultimately, the 5-SPICE framework allows program implementers to study the relationship between the health system and the local community.\n\nOther CHW program frameworks exist, such as the CHW Assessment and Improvement Matrix (AIM) developed by the USAID-funded Health Care Improvement (HCI) project. 5-SPICE complements these other frameworks by providing an acronym that condenses the many elements discussed in other frameworks into an easy-to-remember heuristic, allowing for more effective and efficient assessments that are exploratory rather than prescriptive.\n\nThe 5-SPICE framework was first formally introduced in an April 2013 publication in Global Health Action, in an article entitled “5-SPICE: the application of an original framework for community health worker program design, quality improvement and research agenda setting.” The framework was subsequently presented at the 2013 Consortium of Universities for Global Health Conference, and the 2013 Swedish Society of Medicine’s annual conference, Global Health—Beyond 2015. The article has also been accepted for presentation at the 2013 Annual Meeting of the American Public Health Association.\n"}
{"id": "35807581", "url": "https://en.wikipedia.org/wiki?curid=35807581", "title": "African Nutrition Leadership Programme", "text": "African Nutrition Leadership Programme\n\nThe African Nutrition Leadership Programme (ANLP) is a 10 days training course that started in 2002 to assist the development of future leaders in the field of human nutrition in Africa. The emphasis of the programme is on understanding and developing the qualities and skills of leaders, team building, communication and understanding nutrition information in a broader context. The long-term aim of the ANLP is to meet the demands for leadership in Africa to solve its nutritional challenges.\n\nThe programme is designed for individuals who have experience in various fields of nutrition. Preference will be given to candidates with a postgraduate qualification, postdoctoral fellows and candidates with comparable working experience in the broader human nutrition sciences, studying or working in Africa.\n\nThe ANLP is a leadership development and networking seminar aimed at assisting the development of future leaders in the field of human nutrition in Africa. Emphasis is placed on understanding the qualities and skills of leaders, team building, communication and nutrition information in a broader context, and to understanding the role of nutrition science in the world around us.\n\n\nThe course is held each year at Elgro River Lodge, Free State, South Africa.\n\nEach year, an average of 30 candidates from all around Africa participate to the ANLP.\n\n"}
{"id": "53836551", "url": "https://en.wikipedia.org/wiki?curid=53836551", "title": "Age-related mobility disability", "text": "Age-related mobility disability\n\nAge-related mobility disability is a self-reported inability to walk due to impairments, limited mobility, dexterity or stamina. It has been found mostly in older adults with decreased strength in lower extremities.\n\nAccording to the National Research Council, the population of older adults is expected to increase in the United States by 2030 due to the aging population of the baby boomer generation; this will increase the population of mobility disabled individuals in the community. This raises the importance of being able to predict disability due to inability to walk at an early stage, which will eventually decrease health care costs. Aging cause a decrease in physical strength and in lower extremities which ultimately leads to decrease in functional mobility, in turn leading to disability which is shown to be common in women due to differences in distribution of resources and opportunities. The early detection of mobility disabilities will help clinicians and patients in determining the early management of the conditions which could be associated with the future disability. Mobility disabilites are not restricted to older and hospitalized individuals; such disabilities have been reported in young and non-hospitalized individuals as well due to decreased functional mobility. The increase in the rate of disability causes loss of functional independence and increases the risk of future chronic diseases. \n\nThere are many definitions that contribute in providing the meaning of mobility disability, it is stated as the “self-reports of a lot of difficulties or inability to walk a quarter section|quarter of a mile” without sitting and within time limit of fifteen minutes Failure to walk within this time frame results in the use of mobility aid devices such as mobility scooter, wheelchair, crutches or a walker which will eventually help in the community ambulation. Another term that is coined to define mobility disabilities based on performance is “performance based mobility disability”. It is the inability to increase your walking speed more than 0.4 m/s. If an individual is unable to walk at >0.4 m/s, he or she is considered severely disabled and would require a mobility device to walk in community.\n\nThere are number of factors that could be associated with mobility disability, but according to the Centers for Disease Control and Prevention, “stroke is found to be the leading cause of mobility disability, in turn reducing functional mobility in more than half of the stroke survivors above 65 years of age”.\n\nThere are several measurement scales designed to detect mobility disabilities. The measures that can detect mobility disabilities are classified into two categories, self-reported measures and performance measures. There is a need to differentiate between these measures based on their ability to detect mobility disabilities, such as differences in their reliability and validity. Self-reported measures are commonly used to detect mobility disabilities, but recently developed performance measures have been shown to be effective in predicting future mobility disabilities in older adults.\n\nSeveral qualitative research studies use survey (human research), questionnaires and self-reported scales to detect a decrease in functional mobility or to predict future mobility disability in older adults. The advantages of these qualitative research scales are easier data acquisition and can be performed on the larger population. Although there is difference in perception of condition between subjects (gender difference), type of chronic conditions and age-related changes such as memory and reasoning, all of which can affect the information and scores of the individual, still self-reported measures have been used extensively in behavioral and correlation studies. The commonly used self-reported measures to detect mobility disability are Stroke Impact scale, Rosow- Breslau scale, Barthel index, Tinetti Falls Efficacy Scale. \nBased on reliability and validity of these scales, Stroke Impact scale has proven to have excellent test-retest reliability and construct validity, however, if it can predict future mobility disability in older adults is yet to be found. In contrast, Rosow- Breslau scale, Barthel Index and Tinetti Falls Efficacy Scale proved important to predict future mobility disability based on the activities involved in these questionnaire scales.\n\nMobility disabilities due to age-related musculoskeletal pain or increase in chronic conditions are easier to detect by performance measures. Some commonly used performance measures to detect mobility disabilities are the 400-meter walking test, 5-minute walk test , walking speed, short physical performance battery test. \nAmong these measures, 400-meter walk test and short physical performance battery test has been proven to be strong predictors of mobility disability in older adults. In addition to prediction, there is moderate to excellent correlation between these two tests. \nBased on reliability and validity of measurement scales to predict mobility disability, self-reported measures such as Barthel index, and performance measures such as 400 m walk test and short physical performance battery test are strongly associated with prediction of mobility disability in older adults.\n"}
{"id": "2785602", "url": "https://en.wikipedia.org/wiki?curid=2785602", "title": "Alcohol and weight", "text": "Alcohol and weight\n\nThe relationship between alcohol and weight is the subject of inconclusive studies. Findings of these studies range from increase in body weight to a small decrease among women who begin consuming alcohol. Some of these studies are conducted with a large number of subjects; one involved nearly 80,000 and another 140,000 subjects.\n\nFindings are inconclusive because alcohol itself contains 7 calories per gram, but research suggests that alcohol energy is not efficiently used. Alcohol also appears to increase metabolic rate significantly, thus causing more calories to be burned rather than stored in the body as fat (Klesges \"et al.\", 1994). Other research has found consumption of sugar to decrease as consumption of alcohol increases.\n\nAccording to Dr. Kent Bunting, the research results do not necessarily mean that people who wish to lose weight should continue to consume alcohol because consumption is known to have an enhancing effect on appetite. Due to these discrepancies in findings, the relationship between alcohol and weight remains unresolved and requires further research.\n\nBiological and environmental factors are thought to contribute to alcoholism and obesity. The physiologic commonalities between excessive eating and excessive alcohol drinking shed light on intervention strategies, such as pharmaceutical compounds that may help those who suffer from both.\nSome of the brain signaling proteins that mediate excessive eating and weight gain also mediate uncontrolled alcohol consumption. Some physiological substrates that underlie food intake and alcohol intake have been identified. Melanocortins, a group of signaling proteins, are found to be involved in both excessive food intake and alcohol intake.\n\nAlcohol may contribute to obesity. A study found frequent, light drinkers (three to seven drinking days per week, one drink per drinking day) had lower BMIs than infrequent, but heavier drinkers. Although calories in liquids containing ethanol may fail to trigger the physiologic mechanism that produces the feeling of fullness in the short term, long-term, frequent drinkers may compensate for energy derived from ethanol by eating less.\n\n"}
{"id": "262772", "url": "https://en.wikipedia.org/wiki?curid=262772", "title": "Autoerotic fatality", "text": "Autoerotic fatality\n\nAutoerotic fatalities are accidental deaths that occur during sexual self-stimulation when an apparatus, device or prop that is being employed to enhance pleasure causes the death. Researchers only apply the term to unintentional deaths resulting from solitary sexual activity, not suicide or acts with a partner. The incidence of autoerotic fatalities in Western countries is around 0.5 per million inhabitants each year.\n\nAutoerotic asphyxia is the leading cause. 70 to 80% of autoerotic deaths are caused by hanging, while 10 to 30% are attributed to plastic bags or chemical use. Both of these lead to autoerotic asphyxia. 5 to 10% are related to electrocution, foreign body insertion, overdressing/body wrapping, or another atypical method. Specific causes include the use of chemicals such as amyl nitrite, GHB, or nitrous oxide, and props and tools such as knives, oversized dildos, ligatures or bags for asphyxiation, duct tape, electrical apparatus for shocks, water for self-immersion, fire-making equipment for self-immolation, or sharp, unhygienic or large fetishized objects. Male victims are much more likely to use a variety of devices during autoerotic behaviour than female victims.\n\nThe subject has been treated in two books, \"Autoerotic Fatalities\" by Hazelwood et al. (1983) and \"Autoerotic Asphyxiation: Forensic, Medical, and Social Aspects\" by Sheleg et al. (2006).\n\n\n\n\n"}
{"id": "39670586", "url": "https://en.wikipedia.org/wiki?curid=39670586", "title": "Balloon phobia", "text": "Balloon phobia\n\nBalloon phobia or globophobia is a fear of balloons. The source of fear may be the sound of balloons popping.\n\nGenerally, globophobics will refuse to touch, feel, or go near a balloon for fear it will pop.\n"}
{"id": "54779004", "url": "https://en.wikipedia.org/wiki?curid=54779004", "title": "Breastfeeding contraindications", "text": "Breastfeeding contraindications\n\nContraindications to breastfeeding are those conditions that could compromise the health of the infant if breast milk from their mother is consumed. One example of this is galactosemia. This is. If the mother has HIV or antiviral therapy, untreated active tuberculosis, Human T-lymphotropic virus 1 or II, uses illicit drugs, or mothers undergoing chemotherapy or radiation treatment.\n\nBreastfeeding contradiction is an act of breastfeeding a child while the mother has conditions such as an addiction or disease. Breast milk contains many nutrients that formulas in store shelves do not have which makes breast feeding a healthier and ideal way to feed an infant. Breastfeeding contraindications can occur if a mother does not take medication or seek medical advice prior to breast. The effects of the contradiction can do damage to both the infant and even the mother. It is important to explore the factors that can do damage that way a mother can be informed and well educated on the subject manner.\n\nBreastfeeding contradiction is an act of breastfeeding a child while the mother has conditions such as an addiction or disease. Breast milk contains many nutrients that formulas in store shelves do not have which makes breast feeding a healthier and ideal way to feed an infant. Breastfeeding contraindications can occur if a mother does not take medication or seek medical advice prior to breast. The effects of the contradiction can do damage to both the infant and even the mother. It is important to explore the factors that can do damage that way a mother can be informed and well educated on the subject manner.\n\nBreastfeeding contradiction is an act of breastfeeding a child while the mother has conditions such as an addiction or disease. Breast milk contains many nutrients that formulas in store shelves do not have which makes breast feeding a healthier and ideal way to feed an infant. Breastfeeding contraindications can occur if a mother does not take medication or seek medical advice prior to breast. The effects of the contradiction can do damage to both the infant and even the mother. It is important to explore the factors that can do damage that way a mother can be informed and well educated on the subject manner.\n\nAn individual with T cell lymphotropic virus type 1 and 2 will have excessive amounts of T-cell leukemia and HTLV-1. This often happens through the spread of needles and can affect anyone at any age. If a mother contains this virus and is not aware of it the spread to her infant can be at an all time high of 25%. There is currently no antivirals a mother can take to decrease the spread which is why breastfeeding is not recommended.\n\nAlcohol intake can also pose a threat to an infant, the fat content in your breast can cause toxins from alcohol to build up. It is advised that mothers only limit their drinking to one or two drinks a week so the spread of toxins does not reach the breast. If a mother is binge drinking while breastfeeding and the toxins spread to the infant; there will be a risk of slow weight gain for the infant.\n"}
{"id": "5672", "url": "https://en.wikipedia.org/wiki?curid=5672", "title": "Cadmium", "text": "Cadmium\n\nCadmium is a chemical element with symbol Cd and atomic number 48. This soft, bluish-white metal is chemically similar to the two other stable metals in group 12, zinc and mercury. Like zinc, it demonstrates oxidation state +2 in most of its compounds, and like mercury, it has a lower melting point than the transition metals in groups 3 through 11. Cadmium and its congeners in group 12 are often not considered transition metals, in that they do not have partly filled \"d\" or \"f\" electron shells in the elemental or common oxidation states. The average concentration of cadmium in Earth's crust is between 0.1 and 0.5 parts per million (ppm). It was discovered in 1817 simultaneously by Stromeyer and Hermann, both in Germany, as an impurity in zinc carbonate.\n\nCadmium occurs as a minor component in most zinc ores and is a byproduct of zinc production. Cadmium was used for a long time as a corrosion-resistant plating on steel, and cadmium compounds are used as red, orange and yellow pigments, to color glass, and to stabilize plastic. Cadmium use is generally decreasing because it is toxic (it is specifically listed in the European Restriction of Hazardous Substances) and nickel-cadmium batteries have been replaced with nickel-metal hydride and lithium-ion batteries. One of its few new uses is cadmium telluride solar panels.\n\nAlthough cadmium has no known biological function in higher organisms, a cadmium-dependent carbonic anhydrase has been found in marine diatoms.\n\nCadmium is a soft, malleable, ductile, bluish-white divalent metal. It is similar in many respects to zinc but forms complex compounds. Unlike most other metals, cadmium is resistant to corrosion and is used as a protective plate on other metals. As a bulk metal, cadmium is insoluble in water and is not flammable; however, in its powdered form it may burn and release toxic fumes.\n\nAlthough cadmium usually has an oxidation state of +2, it also exists in the +1 state. Cadmium and its congeners are not always considered transition metals, in that they do not have partly filled d or f electron shells in the elemental or common oxidation states. Cadmium burns in air to form brown amorphous cadmium oxide (CdO); the crystalline form of this compound is a dark red which changes color when heated, similar to zinc oxide. Hydrochloric acid, sulfuric acid, and nitric acid dissolve cadmium by forming cadmium chloride (CdCl), cadmium sulfate (CdSO), or cadmium nitrate (Cd(NO)). The oxidation state +1 can be produced by dissolving cadmium in a mixture of cadmium chloride and aluminium chloride, forming the Cd cation, which is similar to the Hg cation in mercury(I) chloride.\nThe structures of many cadmium complexes with nucleobases, amino acids, and vitamins have been determined.\n\nNaturally occurring cadmium is composed of 8 isotopes. Two of them are radioactive, and three are expected to decay but have not done so under laboratory conditions. The two natural radioactive isotopes are Cd (beta decay, half-life is 7.7 × 10 years) and Cd (two-neutrino double beta decay, half-life is 2.9 × 10 years). The other three are Cd, Cd (both double electron capture), and Cd (double beta decay); only lower limits on these half-lives have been determined. At least three isotopes – Cd, Cd, and Cd – are stable. Among the isotopes that do not occur naturally, the most long-lived are Cd with a half-life of 462.6 days, and Cd with a half-life of 53.46 hours. All of the remaining radioactive isotopes have half-lives of less than 2.5 hours, and the majority have half-lives of less than 5 minutes. Cadmium has 8 known meta states, with the most stable being Cd (\"t\" = 14.1 years), Cd (\"t\" = 44.6 days), and Cd (\"t\" = 3.36 hours).\n\nThe known isotopes of cadmium range in atomic mass from 94.950 u (Cd) to 131.946 u (Cd). For isotopes lighter than 112 u, the primary decay mode is electron capture and the dominant decay product is element 47 (silver). Heavier isotopes decay mostly through beta emission producing element 49 (indium).\n\nOne isotope of cadmium, Cd, absorbs neutrons with high selectivity: With very high probability, neutrons with energy below the \"cadmium cut-off\" will be absorbed; those higher than the \"cut-off will be transmitted\". The cadmium cut-off is about 0.5 eV, and neutrons below that level are deemed slow neutrons, distinct from intermediate and fast neutrons.\n\nCadmium is created via the s-process in low- to medium-mass stars with masses of 0.6 to 10 solar masses, over thousands of years. In that process, a silver atom captures a neutron and then undergoes beta decay.\n\nCadmium (Latin \"cadmia\", Greek \"καδμεία\" meaning \"calamine\", a cadmium-bearing mixture of minerals that was named after the Greek mythological character Κάδμος, Cadmus, the founder of Thebes) was discovered simultaneously in 1817 by Friedrich Stromeyer and Karl Samuel Leberecht Hermann, both in Germany, as an impurity in zinc carbonate. Stromeyer found the new element as an impurity in zinc carbonate (calamine), and, for 100 years, Germany remained the only important producer of the metal. The metal was named after the Latin word for calamine, because it was found in this zinc ore. Stromeyer noted that some impure samples of calamine changed color when heated but pure calamine did not. He was persistent in studying these results and eventually isolated cadmium metal by roasting and reducing the sulfide. The potential for cadmium yellow as pigment was recognized in the 1840s, but the lack of cadmium limited this application.\n\nEven though cadmium and its compounds are toxic in certain forms and concentrations, the British Pharmaceutical Codex from 1907 states that cadmium iodide was used as a medication to treat \"enlarged joints, scrofulous glands, and chilblains\".\n\nIn 1907, the International Astronomical Union defined the international ångström in terms of a red cadmium spectral line (1 wavelength = 6438.46963 Å). This was adopted by the 7th General Conference on Weights and Measures in 1927. In 1960, the definitions of both the metre and ångström were changed to use krypton.\n\nAfter the industrial scale production of cadmium started in the 1930s and 1940s, the major application of cadmium was the coating of iron and steel to prevent corrosion; in 1944, 62% and in 1956, 59% of the cadmium in the United States was used for plating. In 1956, 24% of the cadmium in the United States was used for a second application in red, orange and yellow pigments from sulfides and selenides of cadmium.\n\nThe stabilizing effect of cadmium chemicals like the carboxylates cadmium laurate and cadmium stearate on PVC led to an increased use of those compounds in the 1970s and 1980s. The demand for cadmium in pigments, coatings, stabilizers, and alloys declined as a result of environmental and health regulations in the 1980s and 1990s; in 2006, only 7% of to total cadmium consumption was used for plating, and only 10% was used for pigments.\nAt the same time, these decreases in consumption were compensated by a growing demand for cadmium for nickel-cadmium batteries, which accounted for 81% of the cadmium consumption in the United States in 2006.\n\nCadmium makes up about 0.1 ppm of Earth's crust. It is much rarer than zinc, which makes up about 65 ppm. No significant deposits of cadmium-containing ores are known. The only cadmium mineral of importance, greenockite (CdS), is nearly always associated with sphalerite (ZnS). This association is caused by geochemical similarity between zinc and cadmium, with no geological process likely to separate them. Thus, cadmium is produced mainly as a byproduct of mining, smelting, and refining sulfidic ores of zinc, and, to a lesser degree, lead and copper. Small amounts of cadmium, about 10% of consumption, are produced from secondary sources, mainly from dust generated by recycling iron and steel scrap. Production in the United States began in 1907, but wide use began after World War I.\n\nMetallic cadmium can be found in the Vilyuy River basin in Siberia.\n\nRocks mined for phosphate fertilizers contain varying amounts of cadmium, resulting in a cadmium concentration of as much as 300 mg/kg in the fertilizers and a high cadmium content in agricultural soils. Coal can contain significant amounts of cadmium, which ends up mostly in flue dust. Cadmium in soil can be absorbed by crops such as rice. Chinese ministry of agriculture measured in 2002 that 28% of rice it sampled had excess lead and 10% had excess cadmium above limits defined by law. Some plants such as willow trees and poplars have been found to clean both lead and cadmium from soil.\n\nTypical background concentrations of cadmium do not exceed 5 ng/m in the atmosphere; 2 mg/kg in soil; 1 μg/L in freshwater and 50 ng/L in seawater.\n\nThe British Geological Survey reports that in 2001, China was the top producer of cadmium with almost one-sixth of the world's production, closely followed by South Korea and Japan.\n\nCadmium is a common impurity in zinc ores, and it is most often isolated during the production of zinc. Some zinc ores concentrates from sulfidic zinc ores contain up to 1.4% of cadmium. In the 1970s, the output of cadmium was 6.5 pounds per ton of zinc. Zinc sulfide ores are roasted in the presence of oxygen, converting the zinc sulfide to the oxide. Zinc metal is produced either by smelting the oxide with carbon or by electrolysis in sulfuric acid. Cadmium is isolated from the zinc metal by vacuum distillation if the zinc is smelted, or cadmium sulfate is precipitated from the electrolysis solution.\n\nCadmium is a common component of electric batteries, pigments, coatings, and electroplating.\n\nIn 2009, 86% of cadmium was used in batteries, predominantly in rechargeable nickel-cadmium batteries. Nickel-cadmium cells have a nominal cell potential of 1.2 V. The cell consists of a positive nickel hydroxide electrode and a negative cadmium electrode plate separated by an alkaline electrolyte (potassium hydroxide). The European Union put a limit on cadmium in electronics in 2004 of 0.01%, with some exceptions, and reduced the limit on cadmium content to 0.002%. Another type of battery based on cadmium is the silver-cadmium battery.\n\nCadmium electroplating, consuming 6% of the global production, is used in the aircraft industry to reduce corrosion of steel components. This coating is passivated by chromate salts. A limitation of cadmium plating is hydrogen embrittlement of high-strength steels from the electroplating process. Therefore, steel parts heat-treated to tensile strength above 1300 MPa (200 ksi) should be coated by an alternative method (such as special low-embrittlement cadmium electroplating processes or physical vapor deposition).\n\nTitanium embrittlement from cadmium-plated tool residues resulted in banishment of those tools (and the implementation of routine tool testing to detect cadmium contamination) in the A-12/SR-71, U-2, and subsequent aircraft programs that use titanium.\n\nCadmium is used in the control rods of nuclear reactors, acting as a very effective \"neutron poison\" to control neutron flux in nuclear fission. When cadmium rods are inserted in the core of a nuclear reactor, cadmium absorbs neutrons, preventing them from creating additional fission events, thus controlling the amount of reactivity. The pressurized water reactor designed by Westinghouse Electric Company uses an alloy consisting of 80% silver, 15% indium, and 5% cadmium.\n\nQLED TVs have been starting to include cadmium in construction. Some companies have been looking to reduce the environmental impact of human exposure and pollution of the material in televisions during production.\n\nCadmium oxide was used in black and white television phosphors and in the blue and green phosphors of color television cathode ray tubes. Cadmium sulfide (CdS) is used as a photoconductive surface coating for photocopier drums.\nVarious cadmium salts are used in paint pigments, with CdS as a yellow pigment being the most common. Cadmium selenide is a red pigment, commonly called \"cadmium red\". To painters who work with the pigment, cadmium provides the most brilliant and durable yellows, oranges, and reds — so much so that during production, these colors are significantly toned down before they are ground with oils and binders or blended into watercolors, gouaches, acrylics, and other paint and pigment formulations. Because these pigments are potentially toxic, users should use a barrier cream on the hands to prevent absorption through the skin even though the amount of cadmium absorbed into the body through the skin is reported to be less than 1%.\n\nIn PVC, cadmium was used as heat, light, and weathering stabilizers. Currently, cadmium stabilizers have been completely replaced with barium-zinc, calcium-zinc and organo-tin stabilizers. Cadmium is used in many kinds of solder and bearing alloys, because it has a low coefficient of friction and fatigue resistance. It is also found in some of the lowest-melting alloys, such as Wood's metal.\n\nHelium–cadmium lasers are a common source of blue-ultraviolet laser light. They operate at either 325 or 422 nm in fluorescence microscopes and various laboratory experiments. Cadmium selenide quantum dots emit bright luminescence under UV excitation (He-Cd laser, for example). The color of this luminescence can be green, yellow or red depending on the particle size. Colloidal solutions of those particles are used for imaging of biological tissues and solutions with a fluorescence microscope.\n\nCadmium is a component of some compound semiconductors, such as cadmium sulfide, cadmium selenide, and cadmium telluride, used for light detection and solar cells. HgCdTe is sensitive to infrared light and can be used as an infrared detector, motion detector, or switch in remote control devices.\n\nIn molecular biology, cadmium is used to block voltage-dependent calcium channels from fluxing calcium ions, as well as in hypoxia research to stimulate proteasome-dependent degradation of Hif-1α.\n\nCadmium-selective sensors based on the fluorophore BODIPY have been developed for imaging and sensing of cadmium in cells. One of the most popular way to monitor cadmium in aqueous environments is the use of electrochemistry, one example is by attaching a self-assembled monolayer that can help obtain a cadmium selective electrode with a ppt-level sensitivity.\n\nCadmium has no known function in higher organisms, but a cadmium-dependent carbonic anhydrase has been found in some marine diatoms. The diatoms live in environments with very low zinc concentrations and cadmium performs the function normally carried out by zinc in other anhydrases. This was discovered with X-ray absorption fluorescence spectroscopy (XAFS).\n\nThe highest concentration of cadmium is absorbed in the kidneys of humans, and up to about 30 mg of cadmium is commonly inhaled throughout human childhood and adolescence. Cadmium is under preliminary research for its toxicity in humans, potentially affecting mechanisms and risks of cancer, cardiovascular disease, and osteoporosis.\n\nThe biogeochemistry of cadmium and its release to the environment has been the subject of review, as has the speciation of cadmium in the environment.\n\nIndividuals and organizations have been reviewing cadmium's bioinorganic aspects for its toxicity. The most dangerous form of occupational exposure to cadmium is inhalation of fine dust and fumes, or ingestion of highly soluble cadmium compounds. Inhalation of cadmium fumes can result initially in metal fume fever but may progress to chemical pneumonitis, pulmonary edema, and death.\n\nCadmium is also an environmental hazard. Human exposure is primarily from fossil fuel combustion, phosphate fertilizers, natural sources, iron and steel production, cement production and related activities, nonferrous metals production, and municipal solid waste incineration. Bread, root crops, and vegetables also contribute to the cadmium in modern populations.\nThere have been a few instances of general population poisoning as the result of long-term exposure to cadmium in contaminated food and water, and research into an estrogen mimicry that may induce breast cancer is ongoing. In the decades leading up to World War II, mining operations contaminated the Jinzū River in Japan with cadmium and traces of other toxic metals. As a consequence, cadmium accumulated in the rice crops along the riverbanks downstream of the mines. Some members of the local agricultural communities consumed the contaminated rice and developed itai-itai disease and renal abnormalities, including proteinuria and glucosuria. The victims of this poisoning were almost exclusively post-menopausal women with low iron and other mineral body stores. Similar general population cadmium exposures in other parts of the world have not resulted in the same health problems because the populations maintained sufficient iron and other mineral levels. Thus, although cadmium is a major factor in the itai-itai disease in Japan, most researchers have concluded that it was one of several factors.\n\nCadmium is one of six substances banned by the European Union's Restriction on Hazardous Substances (RoHS) directive, which regulates hazardous substances in electrical and electronic equipment but allows for certain exemptions and exclusions from the scope of the law.\nThe International Agency for Research on Cancer has classified cadmium and cadmium compounds as carcinogenic to humans. Although occupational exposure to cadmium is linked to lung and prostate cancer, there is still a substantial controversy about the carcinogenicity of cadmium in low environmental exposure. Recent data from epidemiological studies suggest that intake of cadmium through diet associates to higher risk of endometrial, breast and prostate cancer as well as to osteoporosis in humans. A recent study has demonstrated that endometrial tissue is characterized by higher levels of cadmium in current and former smoking females.\n\nCadmium exposure is a risk factor associated with a large number of illnesses including kidney disease, early atherosclerosis, hypertension, and cardiovascular diseases. Although studies show a significant correlation between cadmium exposure and occurrence of disease in human populations, a necessary molecular mechanism has not been identified. One hypothesis holds that cadmium is an endocrine disruptor and some experimental studies have shown that it can interact with different hormonal signaling pathways. For example, cadmium can bind to the estrogen receptor alpha, and affect signal transduction along the estrogen and MAPK signaling pathways at low doses.\n\nThe tobacco plant readily absorbs and accumulates heavy metals, such as cadmium from the surrounding soil into its leaves. These are readily absorbed into the user's body following smoke inhalation. Tobacco smoking is the most important single source of cadmium exposure in the general population. An estimated 10% of the cadmium content of a cigarette is inhaled through smoking. Absorption of cadmium through the lungs is more effective than through the gut, and as much as 50% of the cadmium inhaled in cigarette smoke may be absorbed.\nOn average, cadmium concentrations in the blood of smokers is 4 times 5 times greater and in the kidney, 2–3 times greater than non-smokers. Despite the high cadmium content in cigarette smoke, there seems to be little exposure to cadmium from passive smoking.\n\nIn a non-smoking population, food is the greatest source of exposure. High quantities of cadmium can be found in crustaceans, mollusks, offal, and algae products. However, grains, vegetables, and starchy roots and tubers are consumed in much greater quantity in the US, and are the source of the greatest dietary exposure. Most plants bio-accumulate metal toxins like Cd, and when composted to form organic fertilizers yield a product which can often contain high amounts (e.g., over 0.5 mg) of metal toxins for every kilo of fertilizer. Fertilizers made from animal dung (e.g., cow dung) or urban waste can contain similar amounts of Cd. The Cd added to the soil from fertilizers (rock phosphates or organic fertilizers) become bio-available and toxic only if the soil pH is low (i.e., acidic soils). Zinc is chemically similar to cadmium and some evidence indicates the presence of Zn ions reduces cadmium toxicity.\n\nZinc, Cu, Ca, and Fe ions, and selenium with vitamin C are used to treat Cd intoxication, though it is not easily reversed.\n\nBecause of the adverse effects of cadmium on the environment and human health, the supply and use of cadmium is restricted in Europe under the REACH Regulation.\n\nThe EFSA Panel on Contaminants in the Food Chain specifies that 2.5 μg/kg body weight is a tolerable weekly intake for humans. The Joint FAO/WHO Expert Committee on Food Additives has declared 7 μg/kg bw to be the provisional tolerable weekly intake level.\n\nThe US Occupational Safety and Health Administration (OSHA) has set the permissible exposure limit (PEL) for cadmium at a time-weighted average (TWA) of 0.005 ppm. The National Institute for Occupational Safety and Health (NIOSH) has not set a recommended exposure limit (REL) and has designated cadmium as a known human carcinogen. The IDLH (immediately dangerous to life and health) level for cadmium is 9 mg/m.\nIn May 2006, a sale of the seats from Arsenal F.C.'s old stadium, Highbury in London, England was cancelled when the seats were discovered to contain trace amounts of cadmium. Reports of high levels of cadmium use in children's jewelry in 2010 led to a US Consumer Product Safety Commission investigation. The U.S. CPSC issued specific recall notices for cadmium content in jewelry sold by Claire's and Wal-Mart stores.\n\nIn June 2010, McDonald's voluntarily recalled more than 12 million promotional \"Shrek Forever After 3D\" Collectable Drinking Glasses because of the cadmium levels in paint pigments on the glassware. The glasses were manufactured by Arc International, of Millville, NJ, USA.\n\n\n\n"}
{"id": "3626760", "url": "https://en.wikipedia.org/wiki?curid=3626760", "title": "Campaign for Access to Essential Medicines", "text": "Campaign for Access to Essential Medicines\n\nThe Campaign for Access to Essential Medicines is an international campaign started by Médecins Sans Frontières (MSF) to increase the availability of essential medicines in developing countries. MSF often has difficulties treating patients because the medicines required are too expensive or are no longer produced. Sometimes, the only drugs available are highly toxic or ineffective, and they often have to resort to inadequate testing methods to diagnose patients.\n\nThe lack of research into diseases that affect most of the world’s poor population is known as the 10-90 gap and it occurs because pharmaceutical companies rarely make a profit when developing drugs for these diseases. Although some countries have created legislation to encourage development of essential but commercially ignored medicines, which are called orphan drugs in the United States, MSF started this campaign in November 1999 to bring more awareness to the issue, using its prize money from its 1999 Nobel Peace Prize to fund the project. \n\nMSF’s Campaign for Access to Essential Medicines is pushing to lower the prices of existing drugs, vaccines and diagnostic tests, to stimulate research and development into new treatments for diseases that primarily affect the poor, and to overcome other barriers that prevent patients getting the treatment they need.\n\nThe Campaign is made up of a team of medical, legal, policy and communications specialists working together to tackle these various issues.\n\nThe Campaign is calling for improvements to the quality of food aid to meet growing children’s nutritional needs. They are also urging a rapid scale-up in the use and production of Ready-to-Use Foods (RUF) to reduce childhood deaths from malnutrition.\n\nIt is campaigning for new TB drugs and simple diagnostics to be developed while continuing to call to attention the serious underfunding for TB research.\n\nThe Campaign continues to fight to ensure that patients have access to better and improved HIV treatments as well as further scaling up of treatment.\n\nThe Campaign supports the use of flexibilities in world trade rules and the creation of a patent pool to ensure that patents don’t get in the way of access to the medicines patients need.\n\nThe Campaign is working to promote alternative ways of steering and funding medical research to meet the medical needs of people in developing countries, rather than market priorities.\n\nMSF has supported policies that large drugmakers have resisted, in an effort to improve access to essential medicines.\n\nThe director of policy advocacy Michelle Childs of MSF said \"An important precedent, not just for the people living with HIV within its country, who have been campaigning for this, but also for other developing countries. As medicines for HIV and hepatitis B are increasingly under patent in developing counties, Indonesia has shown that countries can and should take action to enable the production of low-cost versions of essential life-saving medicines for their citizens\"\n\n\n"}
{"id": "1077065", "url": "https://en.wikipedia.org/wiki?curid=1077065", "title": "Cancer registry", "text": "Cancer registry\n\nA cancer registry is a systematic collection of data about cancer and tumor diseases. The data are collected by Cancer Registrars. Cancer Registrars capture a complete summary of patient history, diagnosis, treatment, and status for every cancer patient in the United States, and other countries.\n\nThe Surveillance, Epidemiology and End Results (SEER) program of the National Cancer Institute (NCI) was established in 1973 as a result of the National Cancer Act of 1971. The National Program of Cancer Registries (NPCR) was established by Congress through the Cancer Registries Amendment Act in 1992, and administered by the Centers for Disease Control and Prevention (CDC). NPCR and SEER together collect cancer data for the entire U.S. population. CDC and NCI, in collaboration with the North American Association of Central Cancer Registries, have been publishing annual federal cancer statistics in the United States Cancer Statistics: Incidence and Mortality report. Information maintained in the cancer registry includes: demographic information, medical history, diagnostic findings, cancer therapy and follow up details. The data is used to evaluate patient outcome, quality of life, provide follow-up information, calculate survival rates, analyze referral pattern, allocate resources at regional or state level, report cancer incidence as required under state law, and evaluate efficacy of treatment modalities.\n\nThere exist population-based cancer registries, hospital cancer registries (also called hospital-based cancer registries), and special purpose registries.\n\nIn 1926, Yale-New Haven Hospital became the first to set up a cancer registry. In 1956, the American College of Surgeons (ACoS) formally adopted a policy to encourage, through their Approvals Program, the development of hospital-based cancer registries. In 1973, The Surveillance, Epidemiology and End Results (SEER) Program of NCI establishes the first national cancer registry program. In 1992, U.S. Public Law 102-515 establishes the National Program of Cancer Registries (NPCR) and is administered by the US Centers for Disease Control and Prevention (CDC).. By 1993, most states considered cancer a reportable disease.\n\nPopulation-based cancer registries monitor the frequency of new cancer cases (so-called incident cases) every year in well defined populations and over time by collecting case reports from different sources (treatment facilities, clinicians and pathologists, and death certificates). The frequency of these incident cases are expected per 100,000 of the mother population. If an unexpected accumulation can be observed, a hypothesis about possible causes is generated. This hypothesis is investigated in a second step by collecting more detailed data. The aim is to recognize and to reduce risks. Population-based registries can also monitor the effects of preventive measures. All population-based central registries in the United States and Canada are members of the North American Association of Central Cancer Registries. This organization acts as a voice for the registries when dealing with national standard-setting organizations, sets standards for digital cancer record transmission, and certifies the registries for the quality of their data, among other functions.\n\nHospital cancer registries aim at the improvement of cancer therapy. Therefore they have to collect detailed data about diagnosis and therapy. Improvements can be achieved by:\n\n\nSince the data needed by hospital cancer registries usually include those of population-based cancer registries and both use the same classifications, data can be sent from a hospital cancer registry to a population-based registry thus reducing documentation efforts.\n\nHospital and population-based cancer registries report their incidence data to national organizations that aggregate and publish the data. The way in which these data are formatted to be submitted to these organizations are determined by standards released by standard-setting organizations. Edits are run on the data to check for inaccuracies and duplicate cases before being submitted electronically. Different organizations have different standards for data reliability and completeness, and some award certifications based on the adherence to these standards.\n\nThe Surveillance, Epidemiology, and End Results (SEER) Program of the National Cancer Institute collects and publish data on cancer incidence and survival throughout the United States. The information from population-based cancer registries covers approximately 28 percent of the US population. This coverage includes 26 percent of African Americans, 41 percent of Hispanics, 43 percent of American Indians and Alaska Natives, 54 percent of Asians, and 71 percent of Hawaiian/Pacific Islanders. The SEER program population-based cancer registries include Arizona Indians, Cherokee Nation, Connecticut, Detroit, Georgia Center for Cancer Statistics (Atlanta, Great Georgia, and Rural Georgia), Greater Bay Area Cancer Registry (San Francisco-Oakland and San Jose-Monterey), Greater California, Hawaii, Iowa, Kentucky, Los Angeles, Louisiana, New Jersey, New Mexico, Seattle-Puget Sound, and Utah. Selection of the geographic areas is based on the ability to operate and maintain a high quality population-based cancer reporting system.\n\nThe program has state-based cancer registries that collect, analyze and report cancer cases and deaths to a central cancer registry. NPCR was established in 1992 and administered by the CDC. NPCR supports central cancer registries in 45 states, District of Columbia, Puerto Rico, and the U.S. Pacific Island Jurisdictions (covers approximately 96% of the U.S. population). State Cancer Registries monitor cancer trends, determine cancer patterns, direct planning and evaluation of cancer control programs, help set priorities for allocating health resources, promote research, and provide information on cancer incidence.\nThe data collected helps public health professionals understand and address the cancer burden. The ninth volume of Cancer Incidence in Five Continents, published by International Agency for Research on Cancer, includes cancer incidence data from 32 NPCR-funded registries. NPCR’s future direction is to expand the use of information technology designed to support, improve, and enhance the management and exchange of electronic data in cancer surveillance.\n\nThe Swedish Cancer Registry was established in 1958. The health care providers in Sweden are required to report newly detected cancer cases diagnosed at clinical, morphological, and laboratory examination (as well as those discovered during autopsy) to the registry. Every year, the regional registries send cancer data to the National Cancer Register. The information available in the registry include patient’s personal information (PIN, sex, age and place of residence), medical records (date of diagnosis, site of the tumor, method used for diagnosis, and hospital where the patient is being treated), and follow-up data (date and cause of death or date of migration).\n\n"}
{"id": "5669", "url": "https://en.wikipedia.org/wiki?curid=5669", "title": "Chromium", "text": "Chromium\n\nChromium is a chemical element with symbol Cr and atomic number 24. It is the first element in group 6. It is a steely-grey, lustrous, hard and brittle transition metal. Chromium boasts a high usage rate as a metal that is able to be highly polished while resisting tarnishing. Chromium is also the main component of stainless steel, a popular steel alloy due to its uncommonly high specular reflection. Simple polished chromium reflects almost 70% of the visible spectrum, with almost 90% of infrared light waves being reflected. The name of the element is derived from the Greek word χρῶμα, \"chrōma\", meaning color, because many chromium compounds are intensely colored.\n\nFerrochromium alloy is commercially produced from chromite by silicothermic or aluminothermic reactions and chromium metal by roasting and leaching processes followed by reduction with carbon and then aluminium. Chromium metal is of high value for its high corrosion resistance and hardness. A major development in steel production was the discovery that steel could be made highly resistant to corrosion and discoloration by adding metallic chromium to form stainless steel. Stainless steel and chrome plating (electroplating with chromium) together comprise 85% of the commercial use.\n\nIn the United States, trivalent chromium (Cr(III)) ion is considered an essential nutrient in humans for insulin, sugar and lipid metabolism. However, in 2014, the European Food Safety Authority, acting for the European Union, concluded that there was not sufficient evidence for chromium to be recognized as essential.\n\nWhile chromium metal and Cr(III) ions are not considered toxic, hexavalent chromium (Cr(VI)) is both toxic and carcinogenic. Abandoned chromium production sites often require environmental cleanup.\n\nChromium is the fourth transition metal found on the periodic table, and has an electron configuration of [Ar] 3d 4s. It is also the first element in the periodic table whose ground-state electron configuration violates the Aufbau principle. This occurs again later in the periodic table with other elements and their electron configurations, such as copper, niobium, and molybdenum. This occurs because electrons in the same orbital repel each other due to their like charges. In the previous elements, the energetic cost of promoting an electron to the next higher energy level is too great to compensate for that released by lessening inter-electronic repulsion. However, in the 3d transition metals, the energy gap between the 3d and the next-higher 4s subshell is very small, and because the 3d subshell is more compact than the 4s subshell, inter-electron repulsion is smaller between 4s electrons than between 3d electrons. This lowers the energetic cost of promotion and increases the energy released by it, so that the promotion becomes energetically feasible and one or even two electrons are always promoted to the 4s subshell. (Similar promotions happen for every transition metal atom but one, palladium.)\n\nChromium is the first element in the 3d series where the 3d electrons are starting to sink into the inert core; they thus contribute less to metallic bonding, and hence the melting and boiling points and the enthalpy of atomisation of chromium are lower than those of the preceding element vanadium. Chromium(VI) is a strong oxidising agent in contrast to the molybdenum(VI) and tungsten(VI) oxides.\n\nChromium has an unusually high specular reflection in comparison to that of other transitional metals. At 425 μm, chromium was found to have a relative maximum reflection of about 72% reflectance, before entering a depression in reflectivity, reaching a minimum of 62% reflectance at 750 μm before rising again to reflecting roughly 90% of 4000 μm of infrared waves.. When chromium is formed into a stainless steel alloy and polished, the specular reflection decreases with the inclusion of additional metals, yet is still rather high in comparison with other alloys. Between 40% and 60% of the visible spectrum is reflected off of polished stainless steel. The explanation on why chromium displays such a high turnout of reflected photon waves in general, especially the 90% of infrared waves that were reflected, can be attributed to chromium's magnetic properties. Chromium has unique magnetic properties in the sense that chromium is the only elemental solid which shows antiferromagnetic ordering at room temperature (and below). Above 38 °C, its magnetic ordering changes to paramagnetic.. The antiferromagnetic properties, which cause the chromium atoms to temporarily ionize and bond with themselves because the body-centric cubic's magnetic properties are disproportionate to the lattice periodicity. This is due to the fact that the magnetic moments at the cube's corners and the cube centers are not equal, but still antiparallel. From here, the frequency-dependent relative permittivity of chromium, deriving from Maxwell's equations in conjunction with its antiferromagnetivity, leave chromium with one of the highest infrared and visible light reflectance out of the known chemical elements.\n\nChromium metal left standing in air is passivated by oxidation, forming a thin, protective, surface layer. This layer is a spinel structure only a few molecules thick. It is very dense, and prevents the diffusion of oxygen into the underlying metal. This is different from the oxide that forms on iron and carbon steel, through which elemental oxygen continues to migrate, reaching the underlying material to cause incessant rusting. Passivation can be enhanced by short contact with oxidizing acids like nitric acid. Passivated chromium is stable against acids. Passivation can be removed with a strong reducing agent that destroys the protective oxide layer on the metal. Chromium metal treated in this way readily dissolves in weak acids.\n\nChromium, unlike such metals as iron and nickel, does not suffer from hydrogen embrittlement. However, it does suffer from nitrogen embrittlement, reacting with nitrogen from air and forming brittle nitrides at the high temperatures necessary to work the metal parts.\n\nNaturally occurring chromium is composed of three stable isotopes; Cr, Cr and Cr, with Cr being the most abundant (83.789% natural abundance). 19 radioisotopes have been characterized, with the most stable being Cr with a half-life of (more than) 1.8 years, and Cr with a half-life of 27.7 days. All of the remaining radioactive isotopes have half-lives that are less than 24 hours and the majority less than 1 minute. This element also has 2 meta states.\n\nCr is the radiogenic decay product of Mn (half-life = 3.74 million years). Chromium isotopes are typically collocated (and compounded) with manganese isotopes. This circumstance is useful in isotope geology. Manganese-chromium isotope ratios reinforce the evidence from Al and Pd concerning the early history of the solar system. Variations in Cr/Cr and Mn/Cr ratios from several meteorites indicate an initial Mn/Mn ratio that suggests Mn-Cr isotopic composition must result from in-situ decay of Mn in differentiated planetary bodies. Hence Cr provides additional evidence for nucleosynthetic processes immediately before coalescence of the solar system.\n\nThe isotopes of chromium range in atomic mass from 43 u (Cr) to 67 u (Cr). The primary decay mode before the most abundant stable isotope, Cr, is electron capture and the primary mode after is beta decay. Cr has been posited as a proxy for atmospheric oxygen concentration.\n\nChromium is a member of group 6, of the transition metals. Chromium(0) has an electron configuration of [Ar]3d4s, owing to the lower energy of the high spin configuration. Chromium exhibits a wide range of oxidation states, but chromium being ionized into a cation with a positive 3 charge serves as chromium's most stable ionic state. The +3 and +6 states occur the most commonly within chromium compounds; charges of +1, +4 and +5 for chromium are rare, but nevertheless due occasionally exist for chromium.\n\nA large number of chromium(III) compounds are known, such as chromium(III) nitrate, chromium(III) acetate, and chromium(III) oxide. Chromium(III) can be obtained by dissolving elemental chromium in acids like hydrochloric acid or sulfuric acid, but it can also be formed through the reduction of chromium(VI) by cytochrome c7. The ion has a similar radius (63 pm) to (radius 50 pm), and they can replace each other in some compounds, such as in chrome alum and alum. When a trace amount of replaces in corundum (aluminium oxide, AlO), pink sapphire or red-colored ruby is formed, depending on the amount of chromium.\n\nChromium(III) tends to form octahedral complexes. Commercially available chromium(III) chloride hydrate is the dark green complex [CrCl(HO)]Cl. Closely related compounds are the pale green [CrCl(HO)]Cl and violet [Cr(HO)]Cl. If water-free green chromium(III) chloride is dissolved in water, the green solution turns violet after some time as the chloride in the inner coordination sphere is replaced by water. This kind of reaction is also observed with solutions of chrome alum and other water-soluble chromium(III) salts.\n\nChromium(III) hydroxide (Cr(OH)) is amphoteric, dissolving in acidic solutions to form [Cr(HO)], and in basic solutions to form . It is dehydrated by heating to form the green chromium(III) oxide (CrO), a stable oxide with a crystal structure identical to that of corundum.\n\nChromium(VI) compounds are oxidants at low or neutral pH. Chromate anions () and dichromate (CrO) anions are the principal ions at this oxidation state. They exist at an equilibrium, determined by pH:\nChromium(VI) halides are known also and include the hexafluoride CrF and chromyl chloride ().\nSodium chromate is produced industrially by the oxidative roasting of chromite ore with calcium or sodium carbonate. The change in equilibrium is visible by a change from yellow (chromate) to orange (dichromate), such as when an acid is added to a neutral solution of potassium chromate. At yet lower pH values, further condensation to more complex oxyanions of chromium is possible.\n\nBoth the chromate and dichromate anions are strong oxidizing reagents at low pH:\n\nThey are, however, only moderately oxidizing at high pH:\nChromium(VI) compounds in solution can be detected by adding an acidic hydrogen peroxide solution. The unstable dark blue chromium(VI) peroxide (CrO) is formed, which can be stabilized as an ether adduct .\n\nChromic acid has the hypothetical formula . It is a vaguely described chemical, despite many well-defined chromates and dichromates being known. The dark red chromium(VI) oxide , the acid anhydride of chromic acid, is sold industrially as \"chromic acid\". It can be produced by mixing sulfuric acid with dichromate, and is a strong oxidizing agent.\n\nThe oxidation state +5 is only realized in few compounds but are intermediates in many reactions involving oxidations by chromate. The only binary compound is the volatile chromium(V) fluoride (CrF). This red solid has a melting point of 30 °C and a boiling point of 117 °C. It can be prepared by treating chromium metal with fluorine at 400 °C and 200 bar pressure. The peroxochromate(V) is another example of the +5 oxidation state. Potassium peroxochromate (K[Cr(O)]) is made by reacting potassium chromate with hydrogen peroxide at low temperatures. This red brown compound is stable at room temperature but decomposes spontaneously at 150–170 °C.\n\nCompounds of chromium(IV) (in the +4 oxidation state) are slightly more common than those of chromium(V). The tetrahalides, CrF, CrCl, and CrBr, can be produced by treating the trihalides () with the corresponding halogen at elevated temperatures. Such compounds are susceptible to disproportionation reactions and are not stable in water.\n\nMany chromium(II) compounds are known, such as the water-stable chromium(II) chloride that can be made by reducing chromium(III) chloride with zinc. The resulting bright blue solution created from dissolving chromium(II) chloride is only stable at neutral pH. Some other notable chromium(II) compounds include chromium(II) oxide , and chromium(II) sulfate . Many chromous carboxylates are known as well, the most famous of these being the red chromium(II) acetate (Cr(OCCH)) that features a quadruple bond.\n\nMost chromium(I) compounds are obtained solely by oxidation of electron-rich, octahedral chromium(0) complexes. Other chromium(I) complexes contain cyclopentadienyl ligands. As verified by X-ray diffraction, a Cr-Cr quintuple bond (length 183.51(4)  pm) has also been described. Extremely bulky monodentate ligands stabilize this compound by shielding the quintuple bond from further reactions.\n\nMany chromium(0) compounds are currently known; however, most of these compounds are derivatives of the compounds chromium hexacarbonyl or bis(benzene)chromium.\n\nChromium is the 13th most abundant element in Earth's crust with an average concentration of 100 ppm. Chromium compounds are found in the environment from the erosion of chromium-containing rocks, and can be redistributed by volcanic eruptions. Typical background concentrations of chromium in environmental media are: atmosphere <10 ng m; soil <500 mg kg; vegetation <0.5 mg kg; freshwater <10 ug L; seawater <1 ug L; sediment <80 mg kg.\n\nChromium is mined as chromite (FeCrO) ore. About two-fifths of the chromite ores and concentrates in the world are produced in South Africa, about a third in Kazakhstan, while India, Russia, and Turkey are also substantial producers. Untapped chromite deposits are plentiful, but geographically concentrated in Kazakhstan and southern Africa.\n\nAlthough rare, deposits of native chromium exist. The Udachnaya Pipe in Russia produces samples of the native metal. This mine is a kimberlite pipe, rich in diamonds, and the reducing environment helped produce both elemental chromium and diamond.\n\nThe relation between Cr(III) and Cr(VI) strongly depends on pH and oxidative properties of the location. In most cases, Cr(III) is the dominating species, but in some areas, the ground water can contain up to 39 µg/liter of total chromium of which 30 µg/liter is Cr(VI).\n\nChromium was first discovered as an element after it came to the attention of the Western world in the red crystalline mineral crocoite (which is lead(II) chromate). This mineral was discovered in 1761 and was initially used as a pigment; the distinctive color was attributed to the chromium from within the crocoite. In present day, nearly all chromium is commercially extracted from the only viable ore for extensiveness and predicted long term use, being chromite, which is iron chromium oxide (FeCrO); chromite is now the principal source of chromium for use in pigments.\n\nWeapons found in burial pits dating from the late 3rd century B.C. Qin Dynasty of the Terracotta Army near Xi'an, China, have been analyzed by archaeologists. Although these weapons were presumably buried more than two millennia ago, the ancient bronze tips of both the swords and crossbow bolts found at the site showed unexpectedly little corrosion, possibly because the bronze was deliberately coated with a thin layer of chromium oxide. Still, this oxide layer that was found on the weapons was not pure chromium metal or chrome plating as it is commonly produced today, but a mere 10-15 μm layer of chromium oxide molecules at up to 2% chromium was discovered, which turned out to be enough to protect the bronze from corroding.\n\nChromium minerals as pigments came to the attention of the west in the 18th century. On 26 July 1761, Johann Gottlob Lehmann found an orange-red mineral in the Beryozovskoye mines in the Ural Mountains which he named \"Siberian red lead\". Though misidentified as a lead compound with selenium and iron components, the mineral was in fact crocoite (or lead(II) chromate) with a formula of PbCrO. In 1770, Peter Simon Pallas visited the same site as Lehmann and found a red lead mineral that was discovered to possess useful properties as a pigment in paints. After Pallas, the use of Siberian red lead as a paint pigment began to develop rapidly throughout the region.\n\nIn 1794, Louis Nicolas Vauquelin received samples of crocoite ore. He produced chromium trioxide (CrO) by mixing crocoite with hydrochloric acid. In 1797, Vauquelin discovered that he could isolate metallic chromium by heating the oxide in a charcoal oven, for which he is credited as the one who truly discovered the element. Vauquelin was also able to detect traces of chromium in precious gemstones, such as ruby or emerald.\n\nDuring the 19th century, chromium was primarily used not only as a component of paints, but in tanning salts as well. For quite some time, the crocoite found in Russia was the main source for such tanning materials. In 1827, a larger chromite deposit was discovered near Baltimore, United States, which quickly met the demand for tanning salts much more adequately than the crocoite that had been used previously. This made the United States the largest producer of chromium products until the year 1848, when larger deposits of chromite were uncovered near the city of Bursa, Turkey.\n\nChromium is also famous for its reflective, metallic luster when polished. It is used as a protective and decorative coating on car parts, plumbing fixtures, furniture parts and many other items, usually applied by electroplating. Chromium was used for electroplating as early as 1848, but this use only became widespread with the development of an improved process in 1924.\n\nApproximately 28.8 million metric tons (Mt) of marketable chromite ore was produced in 2013, and converted into 7.5 Mt of ferrochromium. According to John F. Papp, writing for the USGS, \"Ferrochromium is the leading end use of chromite ore, [and] stainless steel is the leading end use of ferrochromium.\"\n\nThe largest producers of chromium ore in 2013 have been South Africa (48%), Kazakhstan (13%), Turkey (11%), India (10%) with several other countries producing the rest of about 18% of the world production.\n\nThe two main products of chromium ore refining are ferrochromium and metallic chromium. For those products the ore smelter process differs considerably. For the production of ferrochromium, the chromite ore (FeCrO) is reduced in large scale in electric arc furnace or in smaller smelters with either aluminium or silicon in an aluminothermic reaction.\n\nFor the production of pure chromium, the iron must be separated from the chromium in a two step roasting and leaching process. The chromite ore is heated with a mixture of calcium carbonate and sodium carbonate in the presence of air. The chromium is oxidized to the hexavalent form, while the iron forms the stable FeO. The subsequent leaching at higher elevated temperatures dissolves the chromates and leaves the insoluble iron oxide. The chromate is converted by sulfuric acid into the dichromate.\n\nThe dichromate is converted to the chromium(III) oxide by reduction with carbon and then reduced in an aluminothermic reaction to chromium.\n\nThe creation of metal alloys account for 85% of the available chromium's usage. The remainder of chromium is used in the chemical, refractory, and foundry industries.\n\nThe strengthening effect of forming stable metal carbides at the grain boundaries and the strong increase in corrosion resistance made chromium an important alloying material for steel. The high-speed tool steels contain between 3 and 5% chromium. Stainless steel, the primary corrosion-resistant metal alloy, is formed when chromium is introduced to iron in sufficient concentrations, usually where the chromium concentration is above 11%. For stainless steel's formation, ferrochromium is added to the molten iron. Also, nickel-based alloys increase in strength due to the formation of discrete, stable metal carbide particles at the grain boundaries. For example, Inconel 718 contains 18.6% chromium. Because of the excellent high-temperature properties of these nickel superalloys, they are used in jet engines and gas turbines in lieu of common structural materials.\n\nThe relative high hardness and corrosion resistance of unalloyed chromium makes chrome a reliable metal for surface coating; it is still the most popular metal concerning sheet coating with its above average durability compared to other coating metals. A layer of chromium is deposited on pretreated metallic surfaces by electroplating techniques. There are two deposition methods: thin and thick. Thin deposition involves a layer of chromium below 1 µm thickness deposited by chrome plating, and are used for decorative surfaces. Thicker chromium layers are deposited if wear-resistant surfaces are needed. Both methods use acidic chromate or dichromate solutions. To prevent the energy-consuming change in oxidation state, the use of chromium(III) sulfate is under development; for most applications of chromium, the previously established process is used.\n\nIn the chromate conversion coating process, the strong oxidative properties of chromates are used to deposit a protective oxide layer on metals like aluminium, zinc and cadmium. This passivation and the self-healing properties by the chromate stored in the chromate conversion coating, which is able to migrate to local defects, are the benefits of this coating method. Because of environmental and health regulations on chromates, alternative coating methods are under development.\n\nChromic acid anodizing (or Type I anodizing) of aluminium is another electrochemical process, which does not lead to the deposition of chromium, but uses chromic acid as electrolyte in the solution. During anodization, an oxide layer is formed on the aluminium. The use of chromic acid, instead of the normally used sulfuric acid, leads to a slight difference of these oxide layers.\nThe high toxicity of Cr(VI) compounds, used in the established chromium electroplating process, and the strengthening of safety and environmental regulations demand a search for substitutes for chromium or at least a change to less toxic chromium(III) compounds.\n\nThe mineral crocoite (which is also lead chromate PbCrO) was used as a yellow pigment shortly after its discovery. After a synthesis method became available starting from the more abundant chromite, chrome yellow was, together with cadmium yellow, one of the most used yellow pigments. The pigment does not photodegrade, but it tends to darken due to the formation of chromium(III) oxide. It has a strong color, and was used for school buses in the United States and for the Postal Service (for example, the Deutsche Post) in Europe. The use of chrome yellow has since declined due to environmental and safety concerns and was replaced by organic pigments or other alternatives that are free from lead and chromium. Other pigments that are based around chromium are, for example, the deep shade of red pigment chrome red, which is simply lead chromate with lead(II) hydroxide (PbCrO·Pb(OH)). A very important chromate pigment, which was used widely in metal primer formulations, was zinc chromate, now replaced by zinc phosphate. A wash primer was formulated to replace the dangerous practice of pre-treating aluminium aircraft bodies with a phosphoric acid solution. This used zinc tetroxychromate dispersed in a solution of polyvinyl butyral. An 8% solution of phosphoric acid in solvent was added just before application. It was found that an easily oxidized alcohol was an essential ingredient. A thin layer of about 10–15 µm was applied, which turned from yellow to dark green when it was cured. There is still a question as to the correct mechanism. Chrome green is a mixture of Prussian blue and chrome yellow, while the chrome oxide green is chromium(III) oxide.\n\nChromium oxides are also used as a green pigment in the field of glassmaking and also as a glaze for ceramics. Green chromium oxide is extremely lightfast and as such is used in cladding coatings. It is also the main ingredient in infrared reflecting paints, used by the armed forces to paint vehicles and to give them the same infrared reflectance as green leaves.\n\nNatural rubies are corundum (aluminum oxide) crystals that are colored red (the rarest type) due to chromium (III) ions (other colors of corundum gems are termed sapphires). A red-colored artificial ruby may also be achieved by doping chromium(III) into artificial corundum crystals, thus making chromium a requirement for making synthetic rubies. Such a synthetic ruby crystal was the basis for the first laser, produced in 1960, which relied on stimulated emission of light from the chromium atoms in such a crystal. A ruby laser is lasing at 694.3 nanometers, in a deep red color.\n\nBecause of their toxicity, chromium(VI) salts are used for the preservation of wood. For example, chromated copper arsenate (CCA) is used in timber treatment to protect wood from decay fungi, wood-attacking insects, including termites, and marine borers. The formulations contain chromium based on the oxide CrO between 35.3% and 65.5%. In the United States, 65,300 metric tons of CCA solution were used in 1996.\n\nChromium(III) salts, especially chrome alum and chromium(III) sulfate, are used in the tanning of leather. The chromium(III) stabilizes the leather by cross linking the collagen fibers. Chromium tanned leather can contain between 4 and 5% of chromium, which is tightly bound to the proteins. Although the form of chromium used for tanning is not the toxic hexavalent variety, there remains interest in management of chromium in the tanning industry such as recovery and reuse, direct/indirect recycling, use of less chromium or \"chrome-less\" tanning are practiced to better manage chromium in tanning.\n\nThe high heat resistivity and high melting point makes chromite and chromium(III) oxide a material for high temperature refractory applications, like blast furnaces, cement kilns, molds for the firing of bricks and as foundry sands for the casting of metals. In these applications, the refractory materials are made from mixtures of chromite and magnesite. The use is declining because of the environmental regulations due to the possibility of the formation of chromium(VI). \n\nSeveral chromium compounds are used as catalysts for processing hydrocarbons. For example, the Phillips catalyst, prepared from chromium oxides, is used for the production of about half the world's polyethylene. Fe-Cr mixed oxides are employed as high-temperature catalysts for the water gas shift reaction. Copper chromite is a useful hydrogenation catalyst.\n\n\nIn the form trivalent chromium, Cr(III), or Cr, chromium was tentatively identified as an essential nutrient in the late 1950s and later accepted as a trace element for its roles in the action of insulin, a hormone critical to the metabolism and storage of carbohydrate, fat and protein. The precise mechanism of its actions in the body, however, have not been fully defined, leaving in question whether chromium is essential for healthy people. \n\nTrivalent chromium occurs in trace amounts in foods, wine and water. In contrast, hexavalent chromium (Cr(VI) or Cr) is highly toxic and mutagenic when inhaled. Ingestion of chromium(VI) in water has been linked to stomach tumors, and it may also cause allergic contact dermatitis (ACD).\n\nChromium deficiency, involving a lack of Cr(III) in the body, or perhaps some complex of it, such as glucose tolerance factor is controversial. Some studies suggest that the biologically active form of chromium (III) is transported in the body via an oligopeptide called low-molecular-weight chromium-binding substance (LMWCr), which might play a role in the insulin signaling pathway.\n\nChromium content of common foods is generally low (1-13 micrograms per serving). Chromium content of food varies widely due to differences in soil mineral content, growing season, plant cultivar, and contamination during processing. In addition, chromium (and nickel) leach into food cooked in stainless steel, with the effect largest when the cookware is new. Acidic foods such as tomato sauce which are cooked for many hours also exacerbate this effect.\n\nThere is disagreement on chromium's status as an essential nutrient. Governmental departments from Australia, New Zealand, India, Japan and the United States consider chromium essential while the European Food Safety Authority (EFSA), representing the European Union, does not.\n\nThe National Academy of Medicine (NAM) updated the Estimated Average Requirements (EARs) and the Recommended Dietary Allowances (RDAs) for chromium in 2001. For chromium, there was not sufficient information to set EARs and RDAs, so its needs are described as estimates for Adequate Intakes (AIs). The current AIs of chromium for women ages 14 through 50 is 25 μg/day, and the AIs for women ages 50 and above is 20 μg/day. The AIs for women who are pregnant are 30 μg/day, and for women who are lactating, the set AIs are 45 μg/day. The AIs for men ages 14 through 50 are 35 μg/day, and the AIs for men ages 50 and above are 30 μg/day. For children ages 1 through 13, the AIs increase with age from 0.2 μg/day up to 25 μg/day. As for safety, the NAM sets Tolerable Upper Intake Levels (ULs) for vitamins and minerals when the evidence is sufficient. In the case of chromium, there is not yet enough information and hence no UL has been established. Collectively, the EARs, RDAs, AIs and ULs are the parameters for the nutrition recommendation system known as Dietary Reference Intake (DRI). Australia and New Zealand consider chromium to be an essential nutrient, with an AI of 35 μg/day for men, 25 μg/day for women, 30 μg/day for women who are pregnant, and 45 μg/day for women who are lactating. A UL has not been set due to the lack of sufficient data. India considers chromium to be an essential nutrient, with an adult recommended intake of 33 μg/day. Japan also considers chromium to be an essential nutrient, with an AI of 10 μg/day for adults, including women who are pregnant or lactating. A UL has not been set. The EFSA of the European Union however, does not consider chromium to be an essential nutrient; chromium is the only mineral for which the United States and the European Union disagree.\n\nFor the United States' food and dietary supplement labeling purposes, the amount of the substance in a serving is expressed as a percent of the Daily Value (%DV). For chromium labeling purposes, 100% of the Daily Value was 120 μg. As of May 27, 2016, the percentage of daily value was revised to 35 μg to bring the chromium intake into a consensus with the official Recommended Dietary Allowance. The original deadline to be in compliance was July 28, 2018, but on September 29, 2017 the Food and Drug Administration released a proposed rule that extended the deadline to January 1, 2020 for large companies and January 1, 2021 for small companies.\n\nFood composition databases such as the those maintained by the U.S. Department of Agriculture do not contain information on the chromium content of foods. A wide variety of animal-sourced and vegetable-sourced foods contain chromium. Content per serving is influenced by the chromium content of the soil in which the plants are grown and by feedstuffs fed to animals; also by processing methods, as chromium is leached into foods if processed or cooked in chromium-containing stainless steel equipment. One diet analysis study conducted in Mexico reported an average daily chromium intake of 30 micrograms. An estimated 31% of adults in the United States consume multi-vitamin/mineral dietary supplements which often contain 25 to 60 micrograms of chromium.\n\nChromium is an ingredient in total parenteral nutrition (TPN) because deficiency can occur after months of intravenous feeding with chromium-free TPN. For this reason, chromium is added to TPN solutions, along with other trace minerals. It is also in nutritional products for preterm infants. Although the mechanism in biological roles for chromium is unclear, in the United States chromium-containing products are sold as non-prescription dietary supplements in amounts ranging from 50 to 1,000 μg. Lower amounts of chromium are also often incorporated into multi-vitamin/mineral supplements consumed by an estimated 31% of adults in the United States. Chemical compounds used in dietary supplements include chromium chloride, chromium citrate, chromium(III) picolinate, chromium(III) polynicotinate, and other chemical compositions. The benefit of supplements has not been proven.\n\nIn 2005, the U.S. Food and Drug Administration had approved a Qualified Health Claim for chromium picolinate with a requirement for very specific label wording: \"One small study suggests that chromium picolinate may reduce the risk of insulin resistance, and therefore possibly may reduce the risk of type 2 diabetes. FDA concludes, however, that the existence of such a relationship between chromium picolinate and either insulin resistance or type 2 diabetes is highly uncertain.\" At the same time, in answer to other parts of the petition, the FDA rejected claims for chromium picolinate and cardiovascular disease, retinopathy or kidney disease caused be abnormally high blood sugar levels. In 2010, chromium(III) picolinate was approved by Health Canada to be used in dietary supplements. Approved labeling statements include: a factor in the maintenance of good health, provides support for healthy glucose metabolism, helps the body to metabolize carbohydrates and helps the body to metabolize fats. The European Food Safety Authority (EFSA) approved claims in 2010 that chromium contributed to normal macronutrient metabolism and maintenance of normal blood glucose concentration, but rejected claims for maintenance or achievement of a normal body weight, or reduction of tiredness or fatigue.\n\nGiven the evidence for chromium deficiency causing problems with glucose management in the context of intravenous nutrition products formulated without chromium, research interest turned to whether chromium supplementation for people who have type 2 diabetes but are not chromium deficient could benefit. Looking at the results from four meta-analyses, one reported a statistically significant decrease in fasting plasma glucose levels (FPG) and a non-significant trend in lower hemoglobin A1C. A second reported the same, a third reported significant decreases for both measures, while a fourth reported no benefit for either. A review published in 2016 listed 53 randomized clinical trials that were included in one or more of six meta-analyses. It concluded that whereas there may be modest decreases in FPG and/or HbA1C that achieve statistical significance in some of these meta-analyses, few of the trials achieved decreases large enough to be expected to be relevant to clinical outcome.\n\nTwo systematic reviews looked at chromium supplements as a mean of managing body weight in overweight and obese people. One, limited to chromium picolinate, a popular supplement ingredient, reported a statistically significant -1.1 kg (2.4 lb) weight loss in trials longer than 12 weeks. The other included all chromium compounds and reported a statistically significant -0.50 kg (1.1 lb) weight change. Change in percent body fat did not reach statistical significance. Authors of both reviews considered the clinical relevance of this modest weight loss as uncertain/unreliable. The European Food Safety Authority reviewed the literature and concluded that there was insufficient evidence to support a claim.\n\nChromium is promoted as a sports performance dietary supplement, based on the theory that it potentiated insulin activity, with anticipated results of increased muscle mass, and faster recovery of glycogen storage during post-exercise recovery. A review of clinical trials reported that chromium supplementation did not improve exercise performance or increase muscle strength. The International Olympic Committee reviewed dietary supplements for high-performance athletes in 2018 and concluded there was no need to increase chromium intake for athletes, nor support for claims of losing body fat.\n\nWater-insoluble chromium(III) compounds and chromium metal are not considered a health hazard, while the toxicity and carcinogenic properties of chromium(VI) have been known for a long time. Because of the specific transport mechanisms, only limited amounts of chromium(III) enter the cells. Several \"in vitro\" studies indicated that high concentrations of chromium(III) in the cell can lead to DNA damage. Acute oral toxicity ranges between 1.5 and 3.3 mg/kg. A 2008 review suggested that moderate uptake of chromium(III) through dietary supplements poses no genetic-toxic risk. In the US, the Occupational Safety and Health Administration (OSHA) has designated a permissible exposure limit (PEL) in the workplace as a time-weighted average (TWA) of 1 mg/m. The National Institute for Occupational Safety and Health (NIOSH) has set a recommended exposure limit (REL) of 0.5 mg/m, time-weighted average. The IDLH (immediately dangerous to life and health) value is 250 mg/m.\n\nThe acute oral toxicity for chromium(VI) ranges between 50 and 150 mg/kg. In the body, chromium(VI) is reduced by several mechanisms to chromium(III) already in the blood before it enters the cells. The chromium(III) is excreted from the body, whereas the chromate ion is transferred into the cell by a transport mechanism, by which also sulfate and phosphate ions enter the cell. The acute toxicity of chromium(VI) is due to its strong oxidational properties. After it reaches the blood stream, it damages the kidneys, the liver and blood cells through oxidation reactions. Hemolysis, renal, and liver failure result. Aggressive dialysis can be therapeutic.\n\nThe carcinogenity of chromate dust has been known for a long time, and in 1890 the first publication described the elevated cancer risk of workers in a chromate dye company. Three mechanisms have been proposed to describe the genotoxicity of chromium(VI). The first mechanism includes highly reactive hydroxyl radicals and other reactive radicals which are by products of the reduction of chromium(VI) to chromium(III). The second process includes the direct binding of chromium(V), produced by reduction in the cell, and chromium(IV) compounds to the DNA. The last mechanism attributed the genotoxicity to the binding to the DNA of the end product of the chromium(III) reduction.\n\nChromium salts (chromates) are also the cause of allergic reactions in some people. Chromates are often used to manufacture, amongst other things, leather products, paints, cement, mortar and anti-corrosives. Contact with products containing chromates can lead to allergic contact dermatitis and irritant dermatitis, resulting in ulceration of the skin, sometimes referred to as \"chrome ulcers\". This condition is often found in workers that have been exposed to strong chromate solutions in electroplating, tanning and chrome-producing manufacturers.\n\nBecause chromium compounds were used in dyes, paints, and leather tanning compounds, these compounds are often found in soil and groundwater at active and abandoned industrial sites, needing environmental cleanup and remediation. Primer paint containing hexavalent chromium is still widely used for aerospace and automobile refinishing applications.\n\nIn 2010, the Environmental Working Group studied the drinking water in 35 American cities in the first nationwide study. The study found measurable hexavalent chromium in the tap water of 31 of the cities sampled, with Norman, Oklahoma, at the top of list; 25 cities had levels that exceeded California's proposed limit.\n\n\n"}
{"id": "2300688", "url": "https://en.wikipedia.org/wiki?curid=2300688", "title": "Complications of pregnancy", "text": "Complications of pregnancy\n\nComplications of pregnancy are health problems that are caused by pregnancy. Complications that occur primarily during childbirth are termed obstetric labor complications, and problems that occur primarily after childbirth are termed puerperal disorders. Severe complications of pregnancy, childbirth, and the puerperium are present in 1.6% of mothers in the US and in 1.5% of mothers in Canada. In the immediate postpartum period (puerperium), 87% to 94% of women report at least one health problem. Long-term health problems (persisting after six months postpartum) are reported by 31% of women.\n\nIn 2016, complications of pregnancy, childbirth, and the puerperium resulted globally in 230,600 deaths, down from 377,000 deaths in 1990. The most common causes of maternal mortality are maternal bleeding, maternal sepsis and other infections, hypertensive diseases of pregnancy, obstructed labor, and , which includes miscarriage, ectopic pregnancy, and elective abortion.\n\nThere is no clear distinction between complications of pregnancy and symptoms and discomforts of pregnancy. However, the latter do not significantly interfere with activities of daily living or pose any significant threat to the health of the mother or baby. Still, in some cases the same basic feature can manifest as either a discomfort or a complication depending on the severity. For example, mild nausea may merely be a discomfort (morning sickness), but if severe and with vomiting causing water-electrolyte imbalance it can be classified as a pregnancy complication (hyperemesis gravidarum).\n\nThe following problems originate mainly in the mother.\n\nGestational diabetes is when a woman without diabetes develops high blood sugar levels during pregnancy.\n\nHyperemesis gravidarum is the presence of severe and persistent vomiting, causing dehydration and weight loss. It is more severe than the more common morning sickness and is estimated to affect 0.5–2.0% of pregnant women.\n\n\nPotential severe hypertensive states of pregnancy are mainly:\n\nDeep vein thrombosis (DVT) has an incidence of 0.5 to 7 per 1,000 pregnancies, and is the second most common cause of maternal death in developed countries after bleeding.\n\nLevels of hemoglobin are lower in the third trimesters. According to the United Nations (UN) estimates, approximately half of pregnant women suffer from anemia worldwide. Anemia prevalences during pregnancy differed from 18% in developed countries to 75% in South Asia.\n\nTreatment varies due to the severity of the anaemia, and can be used by increasing iron containing foods, oral iron tablets or by the use of parenteral iron.\n\nA pregnant woman is more susceptible to certain infections. This increased risk is caused by an increased immune tolerance in pregnancy to prevent an immune reaction against the fetus, as well as secondary to maternal physiological changes including a decrease in respiratory volumes and urinary stasis due to an enlarging uterus. Pregnant women are more severely affected by, for example, influenza, hepatitis E, herpes simplex and malaria. The evidence is more limited for coccidioidomycosis, measles, smallpox, and varicella. Mastitis, or inflammation of the breast occurs in 20% of lactating women.\n\nSome infections are vertically transmissible, meaning that they can affect the child as well.\n\nPeripartum cardiomyopathy is decrease in heart function which occurs in the last month of pregnancy, or up to six months post-pregnancy. It increases the risk of congestive heart failure, heart arrhythmias, thromboembolism, and cardiac arrest.\n\nHypothyroidism (also called Hashimoto's disease) is an autoimmune disease that affects the thyroid in women. This condition can have a profound effect during pregnancy and on the child. The infant may be seriously affected and have a variety of birth defects. Many women with Hashimoto's disease develop an underactive thyroid. The clinician will do an exam and order one or more tests.\n\nThe following problems occur in the fetus or placenta, but may have serious consequences on the mother as well.\n\nEctopic pregnancy is implantation of the embryo outside the uterus\n\nMiscarriage is the loss of a pregnancy prior to 20 weeks. In the UK, miscarriage is defined as the loss of a pregnancy during the first 23 weeks.\n\nPlacental abruption is the separation of the placenta from the uterus.\n\n\nPlacenta praevia is when the placenta fully or partially covers the cervix.\n\nPlacenta accreta is an abnormal adherence of the placenta to the uterine wall.\n\nMultiples may become monochorionic, sharing the same chorion, with resultant risk of twin-to-twin transfusion syndrome. Monochorionic multiples may even become monoamniotic, sharing the same amniotic sac, resulting in risk of umbilical cord compression and entanglement. In very rare cases, there may be conjoined twins, possibly impairing function of internal organs.\n\nThe embryo and fetus have little or no immune function. They depend on the immune function of their mother. Several pathogens can cross the placenta and cause (perinatal) infection. Often microorganisms that produce minor illness in the mother are very dangerous for the developing embryo or fetus. This can result in spontaneous abortion or major developmental disorders. For many infections, the baby is more at risk at particular stages of pregnancy. Problems related to perinatal infection are not always directly noticeable.\n\nThe term TORCH complex refers to a set of several different infections that may be caused by transplacental infection.\n\nBabies can also become infected by their mother during birth. During birth, babies are exposed to maternal blood and body fluids without the placental barrier intervening and to the maternal genital tract. Because of this, blood-borne microorganisms (hepatitis B, HIV), organisms associated with sexually transmitted disease (e.g., gonorrhoea and chlamydia), and normal fauna of the genito-urinary tract (e.g., Candida) are among those commonly seen in infection of newborns.\n\nThere have been rare but known cases of intrauterine bleeding caused by injury inflicted by the fetus with its fingernails or toenails.\n\nFactors increasing the risk (to either the woman, the fetus/es, or both) of pregnancy complications beyond the normal level of risk may be present in a woman's medical profile either before she becomes pregnant or during the pregnancy. These pre-existing factors may relate to physical or mental health, or to social issues, or a combination of those.\n\nSome common risk factors include:\n\nSome disorders and conditions can mean that pregnancy is considered high-risk (about 6-8% of pregnancies in the USA) and in extreme cases may be contraindicated. High-risk pregnancies are the main focus of doctors specialising in maternal-fetal medicine.\n\nSerious pre-existing disorders which can reduce a woman's physical ability to survive pregnancy include a range of congenital defects (that is, conditions with which the woman herself was born, for example, those of the heart or , some of which are listed above) and diseases acquired at any time during the woman's life.\n\nA Dutch 2010 research paper showed that \"low-risk\" pregnancy in the Netherlands may actually carry a higher risk of perinatal death than a \"high-risk\" pregnancy.\n\n"}
{"id": "40623623", "url": "https://en.wikipedia.org/wiki?curid=40623623", "title": "Computer-assisted interventions", "text": "Computer-assisted interventions\n\nComputer-assisted interventions (CAI) is a field of research and practice, where medical interventions are supported by computer-based tools and methodologies. Examples include:\n\nThe basic paradigm of patient-specific interventional medicine is a closed loop process, consisting of \n\nThe experience gathered over many patients may be combined to improve treatment plans and protocols for future patients. This process has existed since ancient times. Traditionally, all these steps have taken place in the physicians head. The ability of modern computer-based technology to assist humans in processing and acting on complex information will profoundly enhance this process in the 21st Century.\n\nyour text here\n\nyour text here\n\nyour text here\n\nyour text here\n\nyour text here\n\nyour text here\n\nIn order to gain an explicit and formal understanding of surgery, the field of analyses and modelling of surgical procedures has recently emerged. The challenge is to support the surgeon and the surgical procedure through the understanding of Operating Room (OR) activities, with the help of sensor- or human-based systems. Related surgical models can then be introduced into a new generation of Computer-Assisted Interventions systems to improve the management of complex multimodal information, improve surgical workflows, increase surgical efficiency and the quality of care in the OR. Models created by these different approaches may have a large impact in future surgical innovations, whether for planning, intra-operative or post-operative purposes.\n\nThis idea of describing the surgical procedure as a sequence of tasks was first introduced by MacKenzie et al. (2001). and formalised in Jannin et al., 2001. The term Surgical Process (SP) has been defined as a set of one or more linked procedures or activities that collectively realise a surgical objective within the context of an organisational structure defining functional roles and relationships. This term is generally used to describe the steps involved in a surgical procedure. A Surgical Process Model (SPM) has been defined as a simplified pattern of an SP that reflects a predefined subset of interest of the SP in a formal or semi-formal representation. It relates to the performance of an SP with support from a workflow management system.\n\nSurgical process models are described from observer based acquisition, or sensor-based acquisition (such as signals, or videos,).\n\nRelated terms: Surgical workflow analysis, …\n\nyour text here\n\nyour text here\n\nyour text here\n\nyour text here\n\nyour text here\n\nyour text here\n\nyour text here\n\nyour text here\n\nyour text here\n\nThe international society Medical Image Computing and Computer Assisted Intervention Society (the MICCAI Society) is dedicated to the promotion, preservation and facilitation of research, education and practice in the field of medical image computing and computer-assisted medical interventions including biomedical imaging and robotics, through the organization and operation of regular high quality international conferences and publications which promote and foster the exchange and dissemination of advanced knowledge, expertise and experience in the field produced by leading institutions and outstanding scientists, physicians and educators around the world.\n\nThe International Society for Computer Assisted Surgery (ISCAS) is a non-profit association. Its mission is to encourage all scientific and clinical advancement of computer-aided surgery and related medical interventions throughout the world.\nIts primary goals are:\nIts scope encompasses all fields within surgery, as well as biomedical imaging and instrumentation, and digital technology employed as an adjunct to imaging in diagnosis, therapeutics, and surgery.\n\nMICCAI organizes an annual conference and associated workshops. Proceedings for this conference are published by Springer in the Lecture Notes in Computer Science series. General topics of the conference include medical image computing, computer-assisted intervention, guidance systems and robotics, visualization and virtual reality, computer-aided diagnosis, bioscience and biology applications, specific imaging systems, and new imaging applications.\n\nInternational Conference on Information Processing in Computer-Assisted Interventions (IPCAI) is a premiere international forum for technical innovations, system development and clinical studies in computer-assisted interventions. IPCAI includes papers presenting novel technical concepts, clinical needs and applications as well as hardware, software and systems and their validation.\n\nThe Computer Assisted Radiology and Surgery (CARS) congress is the annual event for a distinguished international community of scientists, engineers and physicians to present and discuss the key innovations that shape modern medicine on a worldwide basis. Founded in 1985, CARS has played a leading role in medical and imaging informatics for more than 25 years by focusing on research and development on novel algorithms and systems and their applications in radiology and surgery. Its growth and impact is due to CARS’s close collaboration with the ISCAS and EuroPACS societies, and CAR, CAD and CMI organizations.\n\n"}
{"id": "265128", "url": "https://en.wikipedia.org/wiki?curid=265128", "title": "Cost-effectiveness analysis", "text": "Cost-effectiveness analysis\n\nCost-effectiveness analysis (CEA) is a form of economic analysis that compares the relative costs and outcomes (effects) of different courses of action. Cost-effectiveness analysis is distinct from cost–benefit analysis, which assigns a monetary value to the measure of effect. Cost-effectiveness analysis is often used in the field of health services, where it may be inappropriate to monetize health effect. Typically the CEA is expressed in terms of a ratio where the denominator is a gain in health from a measure (years of life, premature births averted, sight-years gained) and the numerator is the cost associated with the health gain. The most commonly used outcome measure is quality-adjusted life years (QALY).\n\nCost-utility analysis is similar to cost-effectiveness analysis. Cost-effectiveness analyses are often visualized on a plane consisting of four-quadrants, the cost represented on one axis and the effectiveness on the other axis. Cost-effectiveness analysis focuses on maximising the average level of an outcome, distributional cost-effectiveness analysis extends the core methods of CEA to incorporate concerns for the distribution of outcomes as well as their average level and make trade-offs between equity and efficiency, these more sophisticated methods are of particular interest when analysing interventions to tackle health inequality.\n\nThe concept of cost-effectiveness is applied to the planning and management of many types of organized activity. It is widely used in many aspects of life. In the acquisition of military tanks, for example, competing designs are compared not only for purchase price, but also for such factors as their operating radius, top speed, rate of fire, armor protection, and caliber and armor penetration of their guns. If a tank's performance in these areas is equal or even slightly inferior to its competitor, but substantially less expensive and easier to produce, military planners may select it as more cost effective than the competitor.\n\nConversely, if the difference in price is near zero, but the more costly competitor would convey an enormous battlefield advantage through special ammunition, radar fire control and laser range finding, enabling it to destroy enemy tanks accurately at extreme ranges, military planners may choose it instead—based on the same cost-effectiveness principle.\n\nIn the context of pharmacoeconomics, the cost-effectiveness of a therapeutic or preventive intervention is the ratio of the cost of the intervention to a relevant measure of its effect. Cost refers to the resource expended for the intervention, usually measured in monetary terms such as dollars or pounds. The measure of effects depends on the intervention being considered. Examples include the number of people cured of a disease, the mm Hg reduction in diastolic blood pressure and the number of symptom-free days experienced by a patient. The selection of the appropriate effect measure should be based on clinical judgment in the context of the intervention being considered.\n\nA special case of CEA is cost–utility analysis, where the effects are measured in terms of years of full health lived, using a measure such as quality-adjusted life years or disability-adjusted life years. Cost-effectiveness is typically expressed as an incremental cost-effectiveness ratio (ICER), the ratio of change in costs to the change in effects. A complete compilation of cost-utility analyses in the peer reviewed medical literature is available from the Cost-Effectiveness Analysis Registry website.\n\nA 1995 study of the cost-effectiveness of over 500 life-saving medical interventions found that the median cost per intervention was $42,000 per life-year saved. A 2006 systematic review found that industry-funded studies often concluded with cost effective ratios below $20,000 per QALY and low quality studies and those conducted outside the US and EU were less likely to be below this threshold. While the two conclusions of this article may indicate that industry-funded ICER measures are lower methodological quality than those published by non-industry sources, there is also a possibility that, due to the nature of retrospective or other non-public work, publication bias may exist rather than methodology biases. There may be incentive for an organization not to develop or publish an analysis that does not demonstrate the value of their product. Additionally, peer reviewed journal articles should have a strong and defendable methodology, as that is the expectation of the peer-review process.\n\nCEA has been applied to energy efficiency investments in buildings to calculate the value of energy saved in $/kWh. The energy in such a calculation is virtual in the sense that it was never consumed but rather saved due to some energy efficiency investment being made. Such savings are sometimes called \"negawatts\". The benefit of the CEA approach in energy systems is that it avoids the need to guess future energy prices for the purposes of the calculation, thus removing the major source of uncertainty in the appraisal of energy efficiency investments.\n\n\n"}
{"id": "1393154", "url": "https://en.wikipedia.org/wiki?curid=1393154", "title": "Cost–utility analysis", "text": "Cost–utility analysis\n\nCost–utility analysis (CUA) is a form of financial analysis used to guide procurement decisions.\n\nThe most common and well-known application of this analysis is in pharmacoeconomics, especially health technology assessment (HTA).\n\nIn health economics the purpose of CUA is to estimate the ratio between the cost of a health-related intervention and the benefit it produces in terms of the number of years lived in full health by the beneficiaries. Hence it can be considered a special case of cost-effectiveness analysis, and the two terms are often used interchangeably.\n\nCost is measured in monetary units. Benefit needs to be expressed in a way that allows health states that are considered less preferable to full health to be given quantitative values. However, unlike cost–benefit analysis, the benefits do not have to be expressed in monetary terms. In HTAs it is usually expressed in quality-adjusted life years (QALYs).\n\nIf, for example, intervention A allows a patient to live for three additional years than if no intervention had taken place, but only with a quality of life weight of 0.6, then the intervention confers 3 * 0.6 = 1.8 QALYs to the patient. If intervention B confers two extra years of life at a quality of life weight of 0.75, then it confers an additional 1.5 QALYs to the patient. The net benefit of intervention A over intervention B is therefore 1.8 – 1.5 = 0.3 QALYs.\n\nThe incremental cost-effectiveness ratio (ICER) is the ratio between the difference in costs and the difference in benefits of two interventions. The ICER may be stated as (C1 – C0)/(E1 – E0) in a simple example where C0 and E0 represent the cost and gain, respectively, from taking no health intervention action. C1 and E1 would represent the cost and gain, respectively of taking a specific action. So, an example in which the costs and gains, respectively, are $140,000 and 3.5 QALYs, would yield a value of $40,000 per QALY. These values are often used by policy makers and hospital administrators to determine relative priorities when determining treatments for disease conditions. It is important to note that CUA measures relative patient or general population utility of a treatment or pharmacoeconomic intervention. Its results give no absolute indicator of the value of a certain treatment.\n\nThe National Institute for Health and Care Excellence (NICE) in the UK has been using QALYs to measure the health benefits delivered by various treatment regimens. There is some question as to how well coordinated NICE and NHS are in making decisions about resource allocation. According to a recent study \"cost effectiveness often does not appear to be the dominant consideration in decisions about resource allocation made elsewhere in the NHS\". While QALYs are used in the United States, they are not utilized to the same degree as they are in Europe.\n\nIn the United Kingdom, in January 2005, the NICE is believed to have a threshold of about £30,000 per QALY – roughly twice the mean income after tax – although a formal figure has never been made public. Thus, any health intervention which has an incremental cost of more than £30,000 per additional QALY gained is likely to be rejected and any intervention which has an incremental cost of less than or equal to £30,000 per extra QALY gained is likely to be accepted as cost-effective. This implies a value of a full life of about £2.4 million. For end of life treatments, a higher threshold of £50,000 per additional QALY gained is used by NICE.\n\nIn North America, a similar figure of US$50000 per QALY is often suggested as a threshold ICER for a cost-effective intervention.\n\nA complete compilation of cost–utility analyses in the peer reviewed medical literature is available at the CEA Registry Website\n\nOn the plus side, CUA allows comparison across different health programs and policies by using a common unit of measure (money/QALYs gained). CUA provides a more complete analysis of total benefits than simple cost–benefit analysis does. This is because CUA takes into account the quality of life that an individual has, while CBA does not.\n\nHowever, in CUA, societal benefits and costs are often not taken into account. Furthermore, some economists believe that measuring QALYs is more difficult than measuring the monetary value of life through health improvements, as is done with cost–benefit analysis. This is because in CUA you need to measure the health improvement effects for every remaining year of life after the program is initiated. While for Cost-benefit analysis (CBA) we have an approximate value of life ($2 million is one of the estimates), we do not have a QALY estimate for nearly every medical treatment or disease.\n\nIn addition, some people believe that life is priceless and there are ethical problems with placing a value on human life.\n\nAlso, the weighting of QALYs through time-trade-off, standard gamble, or visual analogue scale is highly subjective.\n\nThere are criticisms of QALY. One involves QALY's lack of usefulness to the healthcare provider in determining the applicability of alternative treatments in the individual patient environment, and the absence of incorporating the patient's willingness to pay (i.e. behavioral economics) in decisions to finance new treatments. Another criticism involves age; elderly individuals are assumed to have lower QALYs since they do not have as many years to influence the calculation of the measurement; so comparing a health intervention's impact on a teenager's QALYs to an older individual's QALYs may not be considered \"fair\" since age is such an important factor. Specific health outcomes may also be difficult to quantify, thus making it difficult to compare all factors that may influence an individual's QALY.\nExample: Comparing an intervention's impact on the livelihood of a single woman to a mother of three; QALYs do not take into account the importance that an individual person may have for others' lives.\n\nIn the US, the health care reform law (Patient Protection and Affordable Care Act) has forbidden the use of QALYs \"as a threshold to establish what type of health care is cost effective or recommended. Also, \"The Secretary shall not utilize such an adjusted life year (or such a similar measure) as a threshold to determine coverage, reimbursement, or incentive programs under title XVIII\".\n"}
{"id": "22273190", "url": "https://en.wikipedia.org/wiki?curid=22273190", "title": "Cuban medical internationalism", "text": "Cuban medical internationalism\n\nCuban medical internationalism is the Cuban programme, since the 1959 Cuban Revolution, of sending Cuban medical personnel overseas, particularly to Latin America, Africa and, more recently, Oceania, and of bringing medical students and patients to Cuba. In 2007, \"Cuba has 42,000 workers in international collaborations in 103 different countries, of whom more than 30,000 are health personnel, including no fewer than 19,000 physicians.\" Cuba provides more medical personnel to the developing world than all the G8 countries combined, although this comparison does not take into account G8 development aid spent on developing world healthcare. The Cuban missions have had substantial positive local impact on the populations served. It is widely believed that medical workers are Cuba's most important export commodity.\n\nIn 2007, one academic study on Cuban internationalism surveyed the history of the programme, noting its broad sweep: \"Since the early 1960s, 28,422 Cuban health workers have worked in 37 Latin American countries, 31,181 in 33 African countries, and 7,986 in 24 Asian countries. Throughout a period of four decades, Cuba sent 67,000 health workers to structural cooperation programs, usually for at least two years, in 94 countries ... an average of 3,350 health workers working abroad every year between 1960 and 2000.\"\n\nThe programme was initiated in 1963 as part of Cuba's foreign policy of supporting anti-colonial struggles. It began when Cuba sent a small medical brigade to Algeria, which suffered from the mass withdrawal of French medical personnel during the independence war; some wounded soldiers and war orphans were also transported back to Cuba for treatment. Cuba did this at a time when, following the Cuban revolution, \"half of the country’s 6,000 doctors fled\". Between 1966 and 1974, Cuban doctors worked alongside Cuban artillery in Guinea-Bissau during its independence war against Portugal. Cuba's largest foreign campaign was in Angola: within two years of the campaign, by 1977, \"only one Angolan province out of sixteen was without Cuban health technicians.\" After 1979, Cuba developed a strong relationship with Nicaragua.\n\nHowever, alongside internationalism driven by foreign policy objectives, humanitarian objectives also played a role, with medical teams despatched to countries governed by ideological foes. For example, in 1960, 1972 and 1990 it dispatched emergency assistance teams to Chile and Nicaragua, and Iran following earthquakes. Similarly, Venezuela's Mission Barrio Adentro programme grew out of the emergency assistance provided by Cuban doctors in the wake of the December 1999 mudslides in Vargas state, which killed 20,000 people.\n\nCuban medical missions were sent to Honduras, Guatemala and Haiti following 1998's Hurricane Mitch and Hurricane Georges, and remained there semi-permanently. This has been part of a dramatic expansion of Cuba’s international cooperation in health since 1998. The number of Cuban doctors working abroad jumped from about 5000 in 2003 to more than 25,000 in 2005.\n\nIn Honduras the medical personnel had a substantial impact: \"In the areas they served, infant mortality rates were reduced from 30.8 to 10.1 per 1,000 live births and maternal mortality rates from 48.1 to 22.4 per 1,000 live births between 1998 and 2003.\" However, as one academic paper notes, \"The idea of a nation saving lives and improving the human condition is alien to traditional statecraft and is therefore discounted as a rationale for the Cuban approach.\" In 2004 the 1700 medical personnel in Guatemala received the Order of the Quetzal, the country's highest state honour. A 2005 attempt by Honduras to expel the Cuban mission on the basis that it was threatening Honduran jobs was successfully resisted by trade unions and community organisations.\n\nFollowing the 2004 Asian tsunami, Cuba sent medical assistance to Banda Aceh and Sri Lanka. In response to Hurricane Katrina, Cuba prepared to send 1500 doctors to the New Orleans; the offer was refused. Several months later the mission was dispatched to Pakistan following the 2005 Kashmir earthquake there. Ultimately Cuba sent \"more than 2,500 disaster response experts, surgeons, family doctors, and other health personnel\", who stayed through the winter for more than 6 months. Cuba is helping with the medical crisis in Haiti due to the 2010 Haiti earthquake. All 152 Cuban medical and educational personnel in the Haitian capital Port-au-Prince at the time of the earthquake were reported to be safe, with two suffering minor injuries. In 2014, Cuba sent 103 nurses and 62 doctors to help fight the Ebola virus epidemic in West Africa, the biggest contribution of health care staff by any single country.\n\nCuba's largest and most extensive medical aid effort is with Venezuela. The program grew out of the emergency assistance provided by Cuban doctors in the wake of the December 1999 mudslides in Vargas state, which killed 20,000 people. Under this bilateral effort, also known as the \"oil for doctors\" program, Cuba provides Venezuela with 31,000 Cuban doctors and dentists and provides training for 40,000 Venezuelan medical personnel. In exchange, Venezuela provides Cuba with 100,000 barrels of oil per day. Based on February 2010 prices, the oil is worth $7.5 million per day, or nearly $3 billion per year.\n\nCuba has also sent notable missions to Bolivia (particularly since the 2005 election of Evo Morales) and South Africa, the latter in particular after a post-apartheid brain drain of white doctors. Since 1995, a co-operation agreement with South Africa has seen hundreds of Cuban doctors practice in South Africa, while South Africa sends medical students to Cuba to study. In 2012, the two governments signed another deal, increasing numbers on both sides. South African can now send 1 000 students to Cuba for training which, South Africa believes, will help train the doctors it so desperately needs for the implementation of its National Health Insurance Scheme. After the 1999 violence in East Timor, the country of a million people was left with only 35 physicians and 75% of its population displaced. The number later increased to 79 physicians by 2004, and Cuba sent an additional 182 physicians and technicians.\n\n\"From 1963 to 2004, Cuba was involved in the creation of nine medical faculties in Yemen, Guyana, Ethiopia, Guinea-Bissau, Uganda, Ghana, Gambia, Equatorial Guinea, and Haiti.\"\n\nIn the 2000s, Cuba began establishing or strengthening relations with Pacific Island countries, and providing medical aid to those countries. Cuba's medical aid to Pacific countries has been two-pronged, consisting in sending doctors to Oceania, and in providing scholarships for Pacific students to study medicine in Cuba at Cuba's expense.\n\nThere are currently sixteen doctors providing specialised medical care in Kiribati, with sixteen more scheduled to join them. Cubans have also offered training to I-Kiribati doctors. Cuban doctors have reportedly provided a dramatic improvement to the field of medical care in Kiribati, reducing the child mortality rate in that country by 80%, and winning the proverbial hearts and minds in the Pacific. In response, the Solomon Islands began recruiting Cuban doctors in July 2007, while Papua New Guinea and Fiji considered following suit.\n\nIn 2008, Cuba was due to send doctors to the Solomon Islands, Vanuatu, Tuvalu, Nauru and Papua New Guinea, while seventeen medical students from Vanuatu would study in Cuba. It was reported that it might also provide training for Fiji doctors.\n\nAs of September 2008, fifteen Cuban doctors were serving in Kiribati, sixty-four Pacific students were studying medicine in Cuba, and Cuban authorities were offering \"up to 400 scholarships to young people of that region\". Among those sixty-four students were twenty-five Solomon Islanders, twenty I-Kiribati, two Nauruans and seventeen ni-Vanuatu. Pacific Islanders have been studying in Cuba since 2006.\n\nIn June 2009, Prensa Latina reported that Cuban doctors had \"inaugurated a series of new health services in Tuvalu\". One Cuban doctor had been serving in Tuvalu since October 2008, and two more since February 2009. They had reportedly \"attended 3,496 patients, and saved 53 lives\", having \"opened ultrasound and abortion services, as well as specialized consultations on hypertension, diabetes, and chronic diseases in children\". They had visited all the country's islands, and were training local staff in \"primary health care, and how to deal with seriously ill patients, among other subjects\".\n\nMissions abroad are intended to provide services at low cost to the host country. \"Patients are not charged for services, and the recipient countries are expected to cover only the cost of collective housing, air fare, and limited food and supplies not exceeding $200 a month. While\nCuban doctors are abroad, they continue to receive their salaries as well as a stipend in foreign currency.\" In 2008, the pay for Cuban doctors abroad was 183$ per month, whereas the pay for doctors working domestically was 23$ per month. \nA new term - \"disaster tourism\" - has arisen in response to a growing number of large-scale natural disasters. This phrase refers to individuals, governments and organizations who travel to a disaster area with the primary goal of having an \"experience\" rather than providing meaningful aid. Such aid is often short-lived, and may even get in the way of more serious rescue efforts. Cuban medical internationalism represents a polar opposite to this disaster tourism mentality, with a focus on large-scale, sustained aid targeting the most marginalized and under-served populations across the globe. \n\nSince 1990, Cuba has provided long-term care for 18,000 victims of the Chernobyl disaster, \"offering treatment for hair loss, skin disorders, cancer, leukemia, and other illnesses attributed to radioactivity.\"\n\nIn response to the 1998 Hurricane Mitch, Cuba set up the \"Escuela Latinoamericana de Medicina\" (\"Latin American School of Medicine\"; ELAM) outside Havana, converted from a former naval base. It accepts around 1500 students per year. ELAM forms part of a range of medical education and training initiatives; \"Cubans, with the help of Venezuela, are currently educating more doctors, about 70,000 in all, than all the medical schools in the United States, which typically have somewhere between 64,000 to 68,000 students enrolled in their programs.\"\n\nFollowing the development of cooperation with Venezuela through Mission Barrio Adentro, Mission Milagro / Operación Milagro was set up to provide ophthalmology services to Cuban, Venezuelan and Latin American patients, both in Cuba and in other countries. \"As of August 2007, Cuba had performed over 750,000 eye surgeries, at no cost, including 113,000 surgeries for its own citizens.\"\n\nAlthough humanitarian principles figure, ideological factors were prominent in Cuba's \"doctor diplomacy\", particularly during the Cold War. Subsequently, its continuation has been seen as a vital means to promote Cuba's image abroad and prevent international isolation. For Cuba's re-establishment of diplomatic relations with Honduras in 2002, Cuba's health missions in that country were \"undoubtedly a deciding factor\"; Guatemala reestablished diplomatic relations with Cuba in 1998.\n\nAt the 2009 5th Summit of the Americas, U.S. President Barack Obama commented that at the summit he had heard much about the impact of Cuban \"soft diplomacy\" in the form of its medical internationalism. He said this might be a reminder to the United States that limiting its interactions with Latin American countries to military and drug interdiction might be limiting its influence.\n\nIt has also been suggested that Cuban medical internationalism promotes exports of Cuban medical technology, and may be a source of hard currency (although the targeting of poor countries reduces the hard currency potential of missions abroad). In 2006 Cuba's earnings from medical services (including export of doctors) amounted to US$2,312m – 28% of total export receipts and net capital payments. This exceeded earnings from both nickel and cobalt exports and from tourism. These earnings are achieved despite the fact that a substantial part of Cuba's medical internationalism since 1998 has been organised within the framework of the \"Integrated Health Program\" (Programa Integral de Salud, PIS); this cooperation program is free for the receiving country. Cuba's co-operation with Venezuela provides Cuba with cheap oil in exchange for its medical support to Mission Barrio Adentro. Bloomberg reported in March 2014 that Cuban state-controlled media forecasted revenue of $8.2 million that year from the program.\n\nIt has also been argued that the programme has, particularly in the 1980s and 1990s, \"perform[ed] a critical function in consolidating socialist consciousness\" within Cuba.\n\nAlthough Cuba's large-scale medical training programmes and high doctor-patient ratios give it much latitude, the expansion of doctor diplomacy since 2004, particularly with the Barrio Adentro programme, has been dramatic: the number of Cuban doctors working abroad jumped from about 5000 in 2003 to more than 25,000 in 2005. This has had some impact on the domestic health system, for example with increased waiting times, particularly with regard to family doctors. The number of patients per doctor rose from 139 to 179. In March 2008 Cuba announced a reorganisation of its domestic family doctor programme for greater efficiency.\n\nIn 2000, two Cuban doctors working in Zimbabwe attempted to defect to Canada. They were prevented from doing so by two Zimbabwean soldiers, who handed them over to Cuban officials. United Nations officials said Zimbabwe appeared to have violated national and international laws.\n\nIn August 2006 the United States under George W. Bush created the Cuban Medical Professional Parole program, specifically targeting Cuban medical personnel and encouraging them to defect when they are working in a country outside of Cuba. Of an estimated 40,000 eligible medical personnel, over 1000 had entered the United States under the program by October 2007, according to the chief of staff for U.S. Rep. Lincoln Diaz-Balart. The promised fast-track visa is not always forthcoming, with at least one applicant waiting a year for his visa; although according to Dr. Julio Cesar Alfonso of the Cuban dissident organization \"Outside the Barrio,\" the U.S. government has rejected only a handful of the hundreds of applications for visas. On 12 January 2017, President Obama announced the end of the program, saying that both Cuba and the US work to \"combat diseases that endanger the health and lives of our people. By providing preferential treatment to Cuban medical personnel, the medical parole program contradicts those efforts, and risks harming the Cuban people.\"\n\nAccording to a 2007 paper published in \"The Lancet\" medical journal, \"growing numbers of Cuban doctors sent overseas to work are defecting to the USA\", some via Colombia, where they have sought temporary asylum. In February 2007, at least 38 doctors were requesting asylum in the U.S. embassy in Bogotá after asylum was denied by the Colombian government. Cuban doctors working abroad are reported to be monitored by \"minders\" and subject to curfew.\n\nAccording to Luis Zuñiga, director of human rights for the Cuban American National Foundation, Cuban doctors are \"slave workers\" who labor for meager wages while bolstering Cuba's image as a donor nation and \"the Cuban government exports these doctors as merchandise\".\n\nAn article by Laurie Garrett in \"Foreign Affairs\" warns that lifting of the United States trade and travel restrictions on Cuba could have dire consequences for Cuba's healthcare system, leading to an exodus of thousands of well-trained Cuban healthcare professionals. U.S. companies could also transform the remaining healthcare system into a destination for medical tourism. Garrett concludes that if politicians do not take great care, lifting of the restrictions would rob Cuba of its greatest triumph.\n\n\n\n"}
{"id": "41216691", "url": "https://en.wikipedia.org/wiki?curid=41216691", "title": "David Vanderpool", "text": "David Vanderpool\n\nDavid Vanderpool (born February 18, 1960) is an American medical missionary and the CEO and founder of Live Beyond, which has provided medical, spiritual and logistical support to disaster ridden countries.\n\nVanderpool's work is unusual in that he combines his medical training with an explicit effort to convert his patients to Christianity.\n\nVanderpool was born in Dallas, Texas, graduated from St. Mark's School of Texas, and received his undergraduate degree from Abilene Christian University in 1982. He then attended the School of Medicine at Texas Tech University Health Sciences Center. After medical school, Vanderpool completed two surgical residencies at Baylor University Medical Center where he trained as a vascular surgeon.\n\nVanderpool wife, Laurie, is a speaker for Women's Retreats and Bible Studies, and speaks frequently for Down Syndrome organizations.\n\nVanderpool remained in Texas after residency and practiced as a vascular surgeon before moving to Brentwood, Tennessee in 2001 and opening his private practice Lave MD in 2003.\n\nDr. Vanderpool created Lave MD to act as both a medical facility and a spa. \nAfter the establishment of his international organization in 2005, Dr. Vanderpool used much of Lave MD's proceeds to fund the organization's efforts abroad.\n\nIn 2005, after Hurricane Katrina hit the southeastern part of the United States, Dr. Vanderpool delivered healthcare across the Mississippi Coast out of a trailer. His goal was to provide as much free healthcare as possible while the medical infrastructure could recover. Months later, Dr. Vanderpool established his organization, Medical Mobile Disaster Relief, as a 501(c)(3) non-profit organization with goals to provide disaster relief through medical clinics, clean water projects, and micro-finance projects to areas hit by disasters. \nVanderpool and the Mobile Medical Disaster Relief team began working in Mozambique in 2006. His goal was to provide healthcare to the indigenous people of the country in addition to enhancing the economy by implementing micro-finance projects among widows living in the communities. By 2008, Dr. Vanderpool teamed up with the Belmont School of Nursing to construct a nursing curriculum that could teach the Mozambique women to be self-sufficient in caring for themselves and their children.\n\nVanderpool and his Mobile Medical Disaster Relief partnered with PACODEP (Partners in Community Development) in Ghana in order to provide medical care, educate the locals on water purification and distribute water purifiers. PACODEP works to free enslaved children who are trafficked through Ghana for purposes of fishing work. Dr. Vanderpool has also partnered with local hospitals in Ghana in order to provide free invasive surgeries to these rescued children.\n\nDr. Vanderpool partnered with Mission Lazarus in 2009 to build a sustainable medical clinic in Cedeño, Honduras. Given sufficient medical supplies and equipment, Vanderpool allowed Mission Lazarus to take over the clinic and provision of healthcare for the people of Cedeño.\n\nDr. Vanderpool shifted his focus in the aftermath of the earthquake that devastated the entire country of Haiti in 2010. In 2010, Vanderpool officially changed the name of his organization from Mobile Medical Disaster Relief to Live Beyond with a stated mission to be \"an organization that chooses to Live Beyond...ourselves, our culture, our borders and this life so that others can Live Beyond…disease, hunger, poverty and despair.\"\n\nThe initial aid Vanderpool and his team brought to Haiti was primarily mobile medical care to relieve the thousands devastated and injured by the earthquake. Since then however, Vanderpool has grounded his missionary work in Thomazeau, Haiti where his organization began building a base. Vanderpool continued to provide free medical care, establishing a surgical hospital and clinic. In addition, clean water projects, orphanages, and other widow and orphan advocacy projects were begun.\n\nDr. Vanderpool is a Christian, who combines religion and the spread of his faith with his medical work. In Haiti, he has made it one of his objectives to bring Haiti away from its traditional voodoo culture and provide \"spiritual guidance\" to the Haitians in the role of Christianity, with the belief that Christianity will lead to a better Haiti. Prior to moving to Haiti, each medical outreach trip made by Vanderpool and other Live Beyond participants included prayer and Christian ministry along with healthcare to the voodoo priests, island chiefs, idol worshipers and the sick and dying in Haiti. On the Live Beyond site, Vanderpool's religious impact since being in Haiti has been characterized as leading to the baptism of dozens, the saving of tribes through \"Bibles being read in their own languages\" and \"the Kingdom is being expanded.\" Dr. Vanderpool promotes religious missionary work in tandem with his medical relief and sustainable development efforts. A worship center is being built in Thomazeau, Haiti and monthly mission trips are promoted and scheduled for Americans to travel to Thomazeau and volunteer.\n\n"}
{"id": "38346167", "url": "https://en.wikipedia.org/wiki?curid=38346167", "title": "Gavi, the Vaccine Alliance", "text": "Gavi, the Vaccine Alliance\n\nGavi, the Vaccine Alliance (Gavi for short; previously the GAVI Alliance, and before that the Global Alliance for Vaccines and Immunization) is a public–private global health partnership committed to increasing access to immunisation in poor countries.\n\nGavi brings together developing country and donor governments, the World Health Organization, UNICEF, the World Bank, the vaccine industry in both industrialised and developing countries, research and technical agencies, civil society, the Bill & Melinda Gates Foundation and other private philanthropists.\n\nGavi was created in 2000 as a successor to the Children's Vaccine Initiative, which had been launched in 1990. \n\nDr. Seth Berkley has been the CEO of GAVI since 2011.\n\nThe Bill and Melinda Gates Foundation has donated $1.5 billion to the alliance as of January 2013.\n\nIn August 2014, Gavi changed its name from \"GAVI Alliance\" and introduced a new logo. Both changes were revealed in August; it had by then acquired the gavi.org domain name and changed its primary domain from gavialliance.org to gavi.org.\n\n"}
{"id": "8430768", "url": "https://en.wikipedia.org/wiki?curid=8430768", "title": "Globalization and disease", "text": "Globalization and disease\n\nGlobalization, the flow of information, goods, capital, and people across political and geographic boundaries, allows infectious diseases to rapidly spread around the world, while also allowing the alleviation of factors such as hunger and poverty, which are key determinants of global health. The spread of diseases across wide geographic scales has increased through history. Early diseases that spread from Asia to Europe were bubonic plague, influenza of various types, and similar infectious diseases.\n\nIn the current era of globalization, the world is more interdependent than at any other time. Efficient and inexpensive transportation has left few places inaccessible, and increased global trade in agricultural products has brought more and more people into contact with animal diseases that have subsequently jumped species barriers (see zoonosis).\n\nGlobalization intensified during the Age of Exploration, but trading routes had long been established between Asia and Europe, along which diseases were also transmitted. An increase in travel has helped spread diseases to natives of lands who had not previously been exposed. When a native population is infected with a new disease, where they have not developed antibodies through generations of previous exposure, the new disease tends to run rampant within the population.\n\nEtiology, the modern branch of science that deals with the causes of infectious disease, recognizes five major modes of disease transmission: airborne, waterborne, bloodborne, by direct contact, and through vector (insects or other creatures that carry germs from one species to another). As humans began traveling over seas and across lands which were previously isolated, research suggests that diseases have been spread by all five transmission modes.\n\nThe Age of Exploration generally refers to the period between the 15th and 17th centuries. During this time, technological advances in shipbuilding and navigation made it easier for nations to explore outside previous boundaries. Globalization has had many benefits, for example, new products to Europeans were discovered, such as tea, silk and sugar when Europeans developed new trade routes around Africa to India and the Spice Islands, Asia, and eventually running to the Americas.\n\nIn addition to trading in goods, many nations began to trade in slavery. Trading in slaves was another way by which diseases were carried to new locations and peoples, for instance, from sub-Saharan Africa to the Caribbean and the Americas. During this time, different societies began to integrate, increasing the concentration of humans and animals in certain places, which led to the emergence of new diseases as some jumped in mutation from animals to humans.\n\nDuring this time sorcerers' and witch doctors' treatment of disease was often focused on magic and religion, and healing the entire body and soul, rather than focusing on a few symptoms like modern medicine. Early medicine often included the use of herbs and meditation. Based on archeological evidence, some prehistoric practitioners in both Europe and South America used trephining, making a hole in the skull to release illness. Severe diseases were often thought of as supernatural or magical. The result of the introduction of Eurasian diseases to the Americas was that many more native peoples were killed by disease and germs than by the colonists' use of guns or other weapons. Scholars estimate that over a period of four centuries, epidemic diseases wiped out as much as 90 percent of the American indigenous populations.\n\nIn Europe during the age of exploration, diseases such as smallpox, measles and tuberculosis (TB) had already been introduced centuries before through trade with Asia and Africa. People had developed some antibodies to these and other diseases from the Eurasian continent. When the Europeans traveled to new lands, they carried these diseases with them. (Note: Scholars believe TB was already endemic in the Americas.) When such diseases were introduced for the first time to new populations of humans, the effects on the native populations were widespread and deadly. The Columbian Exchange, referring to Christopher Columbus's first contact with the native peoples of the Caribbean, began the trade of animals, and plants, and unwittingly began an exchange of diseases.\n\nIt was not until the 1800s that humans began to recognize the existence and role of germs and microbes in relation to disease. Although many thinkers had ideas about germs, it was not until Louis Pasteur spread his theory about germs, and the need for washing hands and maintaining sanitation (particularly in medical practice), that anyone listened. Many people were quite skeptical, but on May 22, 1881 Pasteur persuasively demonstrated the validity of his germ theory of disease with an early example of vaccination. The anthrax vaccine was administered to 25 sheep while another 25 were used as a control. On May 31, 1881 all of the sheep were exposed to anthrax. While every sheep in the control group died, each of the vaccinated sheep survived. Pasteur’s experiment would become a milestone in disease prevention. His findings, in conjunction with other vaccines that followed, changed the way globalization affected the world.\n\nModern modes of transportation allow more people and products to travel around the world at a faster pace; they also open the airways to the transcontinental movement of infectious disease vectors. One example is the West Nile Virus. It is believed that this disease reached the United States via “mosquitoes that crossed the ocean by riding in airplane wheel wells and arrived in New York City in 1999.” With the use of air travel, people are able to go to foreign lands, contract a disease and not have any symptoms of illness until after they get home, and having exposed others to the disease along the way.\n\nAs medicine has progressed, many vaccines and cures have been developed for some of the worst diseases (plague, syphilis, typhus, cholera, malaria) which people suffer. But, because the evolution of disease organisms is very rapid, even with vaccines, there is difficulty providing full immunity to many diseases. Finding vaccines at all for some diseases remains extremely difficult. Without vaccines, the global world remains vulnerable to infectious diseases.\n\nEvolution of disease presents a major threat in modern times. For example, the current \"swine flu\" or H1N1 virus is a new strain of an old form of flu, known for centuries as Asian flu based on its origin on that continent. From 1918–1920, a post-World War I global influenza epidemic killed an estimated 50–100 million people, including half a million in the United States alone. H1N1 is a virus that has evolved from and partially combined with portions of avian, swine, and human flu.\n\nGlobalization has increased the spread of infectious diseases from South to North, but also the risk of non-communicable diseases by transmission of culture and behavior from North to South. It is important to target and reduce the spread of infectious diseases in developing countries. However, addressing the risk factors of non-comunicable diseases and lifestyle risks in the South that cause disease, such as use or consumption of tobacco, alcohol, and unhealthy foods, is important as well.\n\nBubonic plague is a variant of the deadly flea-borne disease plague, which is caused by the enterobacteria \"Yersinia pestis\", that devastated human populations beginning in the 14th century. Bubonic plague is primarily spread by fleas that lived on the black rat, an animal that originated in south Asia and spread to Europe by the 6th century. It became common to cities and villages, traveling by ship with explorers. A human would become infected after being bitten by an infected flea. The first sign of an infection of bubonic plague is swelling of the lymph nodes, and the formation of buboes. These buboes would first appear in the groin or armpit area, and would often ooze pus or blood. Eventually infected individuals would become covered with dark splotches caused by bleeding under the skin. The symptoms would be accompanied by a high fever, and within four to seven days of infection, more than half the victims would die. During the 14th and 15th century, humans did not know that a bacterium was the cause of plague, and efforts to slow the spread of disease were futile.\n\nThe first recorded outbreak of plague occurred in China in the 1330s, a time when China was engaged in substantial trade with western Asia and Europe. The plague reached Europe in October 1347. It was thought to have been brought into Europe through the port of Messina, Sicily, by a fleet of Genoese trading ships from Kaffa, a seaport on the Crimean peninsula. When the ship left port in Kaffa, many of the inhabitants of the town were dying, and the crew was in a hurry to leave. By the time the fleet reached Messina, all the crew were either dead or dying; the rats that took passage with the ship slipped unnoticed to shore and carried the disease with them and their fleas.\n\nWithin Europe, the plague struck port cities first, then followed people along both sea and land trade routes. It raged through Italy into France and the British Isles. It was carried over the Alps into Switzerland, and eastward into Hungary and Russia. For a time during the 14th and 15th centuries, the plague would recede. Every ten to twenty years, it would return. Later epidemics, however, were never as widespread as the earlier outbreaks, when 60% of the population died.\n\nThe plague has never died out. From 1896–1918 the plague swept through India, taking the lives of over 12.5 million people. Between 1906 and 1914, the Plague Research Commission was created, and published supplements to the \"Journal of Hygiene\".\n\nMeasles is a highly contagious airborne virus spread by contact with infected oral and nasal fluids. When a person with measles coughs or sneezes, he releases microscopic particles into the air. During the 4- to 12-day incubation period, an infected individual shows no symptoms, but as the disease progresses, the following symptoms appear: runny nose, cough, red eyes, extremely high fever and a rash.\n\nMeasles is an endemic disease, meaning that it has been continually present in a community, and many people developed resistance. In populations that have not been exposed to measles, exposure to the new disease can be devastating. In 1529, a measles outbreak in Cuba killed two-thirds of the natives who had previously survived smallpox. Two years later measles was responsible for the deaths of half the indigenous population of Honduras, and ravaged Mexico, Central America, and the Inca civilization.\n\nHistorically, measles was very prevalent throughout the world, as it is highly contagious. According to the National Immunization Program, 90% of people were infected with measles by age 15, acquiring immunity to further outbreaks. Until a vaccine was developed in 1963, measles was considered to be deadlier than smallpox. Vaccination reduced the number of reported occurrences by 98%. Major epidemics have predominantly occurred in unvaccinated populations, particularly among nonwhite Hispanic and African American children under 5 years old. In 2000 a group of experts determined that measles was no longer endemic in the United States. The majority of cases that occur are among immigrants from other countries.\n\nTyphus is caused by \"rickettsia\", which is transmitted to humans through lice. The main vector for typhus is the rat flea. Flea bites and infected flea feaces in the respiratory tract are the two most common methods of transmission. In areas where rats are not common, typhus may also be transmitted through cat and opossum fleas. The incubation period of typhus is 7–14 days. The symptoms start with a fever, then headache, rash, and eventually stupor. Spontaneous recovery occurs in 80–90% of victims.\n\nThe first outbreak of typhus was recorded in 1489. Historians believe that troops from the Balkans, hired by the Spanish army, brought it to Spain with them. By 1490 typhus traveled from the eastern Mediterranean into Spain and Italy, and by 1494, it had swept across Europe. From 1500–1914, more soldiers were killed by typhus than from all the combined military actions during that time. It was a disease associated with the crowded conditions of urban poverty and refugees as well. Finally, during World War I, governments instituted preventative delousing measures among the armed forces and other groups, and the disease began to decline. The creation of antibiotics has allowed disease to be controlled within two days of taking a 200 mg dose of tetracycline.\n\nSyphilis is a sexually transmitted disease that causes open sores, delirium and rotting skin, and is characterized by genital ulcers. Syphilis can also do damage to the nervous system, brain and heart. The disease can be transmitted from mother to child.\n\nThe origins of syphilis are unknown, and some historians argue that it descended from a twenty-thousand-year-old African zoonosis. Other historians place its emergence in the New World, arguing that the crews of Columbus’s ships first brought the disease to Europe. The first recorded case of syphilis occurred in Naples in 1495, after King Charles VIII of France besieged the city of Naples, Italy. The soldiers, and the prostitutes who followed their camps, came from all corners of Europe. When they went home, they took the disease with them and spread it across the continent.\n\nSmallpox is a highly contagious disease caused by the Variola virus. There are four variations of smallpox; variola major, variola minor, haemorrhagic, and malignant, with the most common being variola major and variola minor. Symptoms of the disease including hemorrhaging, blindness, back ache, vomiting, which generally occur shortly after the 12- to 17-day incubation period. The virus begins to attack skin cells, and eventually leads to an eruption of pimples that cover the whole body. As the disease progresses, the pimples fill up with pus or merge. This merging results in a sheet that can detach the bottom layer from the top layer of skin. The disease is easily transmitted through airborne pathways (coughing, sneezing, and breathing), as well as through contaminated bedding, clothing or other fabrics,\n\nIt is believed that smallpox first emerged over 3000 years ago, probably in India or Egypt. There have been numerous recorded devastating epidemics throughout the world, with high losses of life.\n\nSmallpox was a common disease in Eurasia in the 15th century, and was spread by explorers and invaders. After Columbus landed on the island of Hispaniola during his second voyage in 1493, local people started to die of a virulent infection. Before the smallpox epidemic started, more than one million indigenous people had lived on the island; afterward, only ten thousand had survived.\n\nDuring the 16th century, Spanish soldiers introduced smallpox by contact with natives of the Aztec capital Tenochtitlan. A devastating epidemic broke out among the indigenous people, killing thousands.\n\nIn 1617, smallpox reached Massachusetts, probably brought by earlier explorers to Nova Scotia, Canada.” By 1638 the disease had broken out among people in Boston, Massachusetts. In 1721 people fled the city after an outbreak, but the residents spread the disease to others throughout the thirteen colonies. Smallpox broke out in six separate epidemics in the United States through 1968.\n\nThe smallpox vaccine was developed in 1798 by Edward Jenner. By 1979 the disease had been completely eradicated, with no new outbreaks. The WHO stopped providing vaccinations and by 1986, vaccination was no longer necessary to anyone in the world except in the event of future outbreak.\n\nLeprosy, also known as Hansen’s Disease, is caused by a bacillus, \"Mycobacterium leprae\". It is a chronic disease with an incubation period of up to five years. Symptoms often include irritation or erosion of the skin, and effects on the peripheral nerves, mucosa of the upper respiratory tract and eyes. The most common sign of leprosy are pale reddish spots on the skin that lack sensation.\n\nLeprosy probably originated in India, more than four thousand years ago. It was prevalent in ancient societies in China, Egypt and India, and was transmitted throughout the world by various traveling groups, including Roman Legionnaires, Crusaders, Spanish conquistadors, Asian seafarers, European colonists, and Arab, African, and American slave traders. Some historians believe that Alexander the Great's troops brought leprosy from India to Europe during the 3rd century BC. With the help of the crusaders and other travelers, leprosy reached epidemic proportions by the 13th century.\n\nOnce detected, leprosy can be cured using multi-drug therapy, composed of two or three antibiotics, depending on the type of leprosy. In 1991 the World Health Assembly began an attempt to eliminate leprosy. By 2005 116 of 122 countries were reported to be free of leprosy.\n\nOn Nov. 6, 1880 Alphonse Laveran discovered that malaria (then called \"Marsh Fever\") was a protozoan parasite, and that mosquitoes carry and transmit malaria. Malaria is a protozoan infectious disease that is generally transmitted to humans by mosquitoes between dusk and dawn. The European variety, known as \"vivax\" after the \"Plasmodium vivax\" parasite, causes a relatively mild, yet chronically aggravating disease. The west African variety is caused by the sporozoan parasite, \"Plasmodium falciparum\", and results in a severely debilitating and deadly disease.\n\nMalaria was common in parts of the world where it has now disappeared, as the vast majority of Europe (disease of African descent are particularly diffused in the Empire romain) and North America . In some parts of England, mortality due to malaria was comparable to that of sub-Saharan Africa today. Although William Shakespeare was born at the beginning of a colder period called the \"Little Ice Age\", he knew enough ravages of this disease to include in eight parts. Plasmodium vivax lasted until 1958 in the polders of Belgium and the Netherlands.\nIn the 1500s, it was the European settlers and their slaves who probably brought malaria on the American continent (we know that Columbus was suffering from this disease before his arrival in the new land). The Spanish Jesuit missionaries saw the Indians bordering on Lake Loxa Peru used the Cinchona bark powder to treat fevers. However, there is no reference to malaria in the medical literature of the Maya or Aztecs. The use of the bark of the \"fever tree\" was introduced into European medicine by Jesuit missionaries whose Barbabe Cobo who experimented in 1632 and also exports, so that the precious powder s' also called \"Jesuit powder\" . A study in 2012 of thousands of genetic markers for Plasmodium falciparum samples confirmed the African origin of the parasite in South America (Europeans themselves have been affected by this disease through Africa): it borrowed from the mid-sixteenth century and the mid-nineteenth the two main roads of the slave trade, the first leading to the north of South America (Colombia) by the Spanish, the second most leading south (Brazil) by Portugueses.\n\nParts of the Third World are more affected by malaria than the rest of the world. For instance, many inhabitants of sub-Saharan Africa are affected by recurring attacks of malaria throughout their lives. In many areas of Africa, there is limited running water. The residents' use of wells and cisterns provides many sites for the breeding of mosquitoes and spread of the disease. Mosquitoes use areas of standing water like marshes, wetlands, and water drums to breed.\n\nThe bacterium that causes tuberculosis, \"Mycobacterium tuberculosis\", is generally spread when an infected person coughs and another person inhales the bacteria. Once inhaled TB frequently grows in the lungs, but can spread to any part of the body. Although TB is highly contagious, in most cases the human body is able to fend off the bacteria. But, TB can remain dormant in the body for years, and become active unexpectedly. If and when the disease does become active in the body, it can multiply rapidly, causing the person to develop many symptoms including cough (sometimes with blood), night sweats, fever, chest pains, loss of appetite and loss of weight. This disease can occur in both adults and children and is especially common among those with weak or undeveloped immune systems.\n\nTuberculosis (TB) has been one of history’s greatest killers, taking the lives of over 3 million people annually. It has been called the \"white plague\". According to the WHO, approximately fifty percent of people infected with TB today live in Asia. It is the most prevalent, life-threatening infection among AIDS patients. It has increased in areas where HIV seroprevalence is high.\n\nAir travel and the other methods of travel which have made global interaction easier, have increased the spread of TB across different societies. Luckily, the BCG vaccine was developed, which prevents TB meningitis and miliary TB in childhood. But, the vaccine does not provide substantial protection against the more virulent forms of TB found among adults. Most forms of TB can be treated with antibiotics to kill the bacteria. The two antibiotics most commonly used are rifampicin and isoniazid. There are dangers, however, of a rise of antibiotic-resistant TB. The TB treatment regimen is lengthy, and difficult for poor and disorganized people to complete, increasing resistance of bacteria. Antibiotic-resistant TB is also known as \"multidrug-resistant tuberculosis.\" \"Multidrug-resistant tuberculosis\" is a pandemic that is on the rise. Patients with MDR-TB are mostly young adults who are not infected with HIV or have other existing illness. Due to the lack of health care infrastructure in underdeveloped countries, there is a debate as to whether treating MDR-TB will be cost effective or not. The reason is the high cost of \"second-line\" antituberculosis medications. It has been argued that the reason the cost of treating patients with MDR-TB is high is because there has been a shift in focus in the medical field, in particular the rise of AIDS, which is now the world's leading infectious cause of death. Nonetheless, it is still important to put in the effort to help and treat patients with \"multidrug-resistant tuberculosis\" in poor countries.\n\nHIV and AIDS are among the newest and deadliest diseases. According to the World Health Organization, it is unknown where the HIV virus originated, but it appeared to move from animals to humans. It may have been isolated within many groups throughout the world. It is believed that HIV arose from another, less harmful virus, that mutated and became more virulent. The first two AIDS/HIV cases were detected in 1981. As of 2013, an estimated 1.3 million persons in the United States were living with HIV or AIDS, almost 110,000 in the UK and an estimated 35 million people worldwide are living with HIV”.\n\nDespite efforts in numerous countries, awareness and prevention programs have not been effective enough to reduce the numbers of new HIV cases in many parts of the world, where it is associated with high mobility of men, poverty and sexual mores among certain populations. Uganda has had an effective program, however. Even in countries where the epidemic has a very high impact, such as Swaziland and South Africa, a large proportion of the population do not believe they are at risk of becoming infected. Even in countries such as the UK, there is no significant decline in certain at-risk communities. 2014 saw the greatest number of new diagnoses in gay men, the equivalent of nine being diagnosed a day.\n\nInitially, HIV prevention methods focused primarily on preventing the sexual transmission of HIV through behaviour change. The ABC Approach - \"Abstinence, Be faithful, Use a Condom\". However, by the mid-2000s, it became evident that effective HIV prevention requires more than that and that interventions need to take into account underlying socio-cultural, economic, political, legal and other contextual factors.\n\nThe Ebola outbreak, which was the 26th outbreak since 1976, started in Guinea in March 2014. The WHO warned that the number of Ebola patients could rise to 20,000, and said that it used $489m (£294m) to contain Ebola within six to nine months. The outbreak was accelerating. Medecins sans Frontieres has just opened a new Ebola hospital in Monrovia, and after one week it is already a capacity of 120 patients. It said that the number of patients seeking treatment at its new Monrovia centre was increasing faster than they could handle both in terms of the number of beds and the capacity of the staff, adding that it was struggling to cope with the caseload in the Liberian capital. Lindis Hurum, MSF's emergency coordinator in Monrovia, said that it was humanitarian emergency and they needed a full-scale humanitarian response. Brice de la Vinge, MSF director of operations, said that it was not until five months after the declaration of the Ebola outbreak that serious discussions started about international leadership and coordination, and said that it was not acceptable.\n\nLeptospirosis, also known as field fever is an infection caused by \"Leptospira\". Symptoms can range from none to mild such as headaches, muscle pains, and fevers; to severe with bleeding from the lungs or meningitis. \"Leptospira\" is transmitted by both wild and domestic animals, most commonly by rodents. It is often transmitted by animal urine or by water or soil containing animal urine coming into contact with breaks in the skin, eyes, mouth, or nose.\nThe countries with the highest reported incidence are located in the Asia-Pacific region (Seychelles, India, Sri Lanka and Thailand) with incidence rates over 10 per 1000,000 people s well as in Latin America and the Caribbean (Trinidad and Tobago, Barbados, Jamaica, El Salvador, Uruguay, Cuba, Nicaragua and Costa Rica) However, the rise in global travel and eco-tourism has led to dramatic changes in the epidemiology of leptospirosis, and travelers from around the world have become exposed to the threat of leptospirosis. Despite decreasing prevalence of leptospirosis in endemic regions, previously non-endemic countries are now reporting increasing numbers of cases due to recreational exposure International travelers engaged in adventure sports are directly exposed to numerous infectious agents in the environment and now comprise a growing proportion of cases worldwide.\n\nGlobalization can benefit people with non-communicable diseases such as heart problems or mental health problems. Global trade and rules set forth by the World Trade Organization can actually benefit the health of people by making their incomes higher, allowing them to afford better health care. While it has to be admitted making many non-communicable diseases more likely as well. Also the national income of a country, mostly obtained by trading on the global market, is important because it dictates how much a government spends on health care for its citizens. It also has to be acknowledged that an expansion in the definition of disease often accompanies development, so the net effect is not clearly beneficial due to this and other effects of increased affluence. Metabolic syndrome is one obvious example. Although poorer countries have not yet experienced this and are still suffering from diseases listed above.\n"}
{"id": "1831787", "url": "https://en.wikipedia.org/wiki?curid=1831787", "title": "HIV/AIDS in Latin America", "text": "HIV/AIDS in Latin America\n\nIn Latin America, the Central American countries of Guatemala, and Honduras, and the Caribbean countries of Haiti, and the Dominican Republic have national HIV prevalence of over 1%. In these countries, HIV-infected men outnumber HIV-infected women by roughly 3:1. Low prevalence in other countries disguises serious, localized epidemics. In Mexico, Brazil, Colombia and Argentina, intravenous drug use and homosexual activity are the main modes of transmission, and there is concern that heterosexual activity may soon become a primary method of spreading the virus.\n\n\n"}
{"id": "3292213", "url": "https://en.wikipedia.org/wiki?curid=3292213", "title": "Hazard analysis", "text": "Hazard analysis\n\nNote: Parts of this article are written from the perspective of aircraft safety analysis techniques and definitions; these may not represent current best practice and the article needs to be updated to represent a more generic description of hazard analysis and discussion of more modern standards and techniques.\n\nA hazard analysis is used as the first step in a process used to assess risk. The result of a hazard analysis is the identification of different type of hazards. A hazard is a potential condition and exists or not (probability is 1 or 0). It may in single existence or in combination with other hazards (sometimes called events) and conditions become an actual Functional Failure or Accident (Mishap). The way this exactly happens in one particular sequence is called a scenario. This scenario has a probability (between 1 and 0) of occurrence. Often a system has many potential failure scenarios. It also is assigned a classification, based on the worst case severity of the end condition. Risk is the combination of probability and severity. Preliminary risk levels can be provided in the hazard analysis. The validation, more precise prediction (verification) and acceptance of risk is determined in the Risk assessment (analysis). The main goal of both is to provide the best selection of means of controlling or eliminating the risk. The term is used in several engineering specialties, including avionics, chemical process safety, safety engineering, reliability engineering and food safety.\n\nA hazard is defined as a \"Condition, event, or circumstance that could lead to or contribute to an unplanned or undesirable event.\" Seldom does a single hazard cause an accident or a functional failure. More often an accident or operational failure occurs as the result of a sequence of causes. A hazard analysis will consider system state, for example operating environment, as well as failures or malfunctions.\n\nWhile in some cases, safety or reliability risk can be eliminated, in most cases a certain degree of risk must be accepted. In order to quantify expected costs before the fact, the potential consequences and the probability of occurrence must be considered. Assessment of risk is made by combining the severity of consequence with the likelihood of occurrence in a matrix. Risks that fall into the \"unacceptable\" category (e.g., high severity and high probability) must be mitigated by some means to reduce the level of safety risk.\n\nIEEE STD-1228-1994 Software Safety Plans prescribes industry best practices for conducting software safety hazard analyses to help ensure safety requirements and attributes are defined and specified for inclusion in software that commands, controls or monitors critical functions. When software is involved in a system, the development and design assurance of that software is often governed by DO-178B. The severity of consequence identified by the hazard analysis establishes the criticality level of the software. Software criticality levels range from A to E, corresponding to the severity of Catastrophic to No Safety Effect. Higher levels of rigor are required for level A and B software and corresponding functional tasks and work products is the system safety domain are used as objective evidence of meeting safety criteria and requirements.\n\nRecently a leading edge commercial standard was promulgated based on decades of proven system safety processes in DoD and NASA. ANSI/GEIA-STD-0010-2009 (Standard Best Practices for System Safety Program Development and Execution) is a demilitarized commercial best practice that uses proven holistic, comprehensive and tailored approaches for hazard prevention, elimination and control. It is centered around the hazard analysis and functional based safety process.\n\n\n\n"}
{"id": "2596739", "url": "https://en.wikipedia.org/wiki?curid=2596739", "title": "Health geography", "text": "Health geography\n\nHealth geography is the application of geographical information, perspectives, and methods to the study of health, disease, and health care.\n\nThe study of health geography has been influenced by repositioning medical geography within the field of social geography due to a shift towards a social model in health care, rather than a medical model. This advocates for the redefinition of health and health care away from prevention and treatment of illness only to one of promoting well-being in general. Under this model, some previous illnesses (e.g., mental ill health) are recognized as behavior disturbances only, and other types of medicine (e.g., complementary or alternative medicine and traditional medicine) are studied by the medicine researchers, sometimes with the aid of health geographers without medical education. This shift changes the definition of care, no longer limiting it to spaces such as hospitals or doctor's offices. Also, the social model gives priority to the intimate encounters performed at non-traditional spaces of medicine and healthcare as well as to the individuals as health consumers.\n\nThis alternative methodological approach means that medical geography is broadened to incorporate philosophies such as Marxian political economy, structuralism, social interactionism, humanism, feminism and queer theory.\n\nThe relationship between space and health dates back to Hippocrates, who stated that \"airs, waters, places\" all played significant roles impacting human health and history. A classic piece of research in health geography was done in 1854 as a cholera outbreak gripped a neighborhood in London. Death tolls rang around the clock and the people feared that they were being infected by vapors coming from the ground. John Snow predicted that if he could locate the source of the disease, it could be contained. He drew maps demonstrating the homes of people who had died of cholera and the locations of water pumps. He found that one pump, the public pump on Broad Street, was central to most of the victims. He concluded that infected water from the pump was the culprit. He instructed the authorities to remove the handle to the pump, making it unusable. As a result, the number of new cholera cases decreased.\n\nHealth geography is considered to be divided into two distinct elements. The first of which is focused on geographies of disease and ill health, involving descriptive research quantifying disease frequencies and distributions, and analytic research concerned with finding what characteristics make an individual or population susceptible to disease. This requires an understanding of epidemiology. The second component of health geography is the geography of health care, primarily facility location, accessibility, and utilization. This requires the use of spatial analysis and often borrows from behavioral economics.\n\nHsenting health risks, from natural disasters, to interpersonal violence, stress, and other potential dangers.\n\nAlthough healthcare is a public good, it is not equally available to all individuals. Demand for public services is continuously increasing. People need advance knowledge and the latest prediction technology, that health geography offers. The latest example of such technology is Telemedicine. Many people in the United States are not able to access proper healthcare because of inequality in health insurance and the means to afford medical care.\n\nMobility and Disease Tracking\n\nWith the advent of mobile technology and its spread, it is now possible to track individual mobility. By correlating the movement of individuals through tracking the devices using access towers or other tracking systems, it is now possible to determine and even control disease spread. While privacy laws question the legality of tracking individuals, the commercial mobile service providers are using covert techniques or obtaining government waivers to allow permission to track people .\n\nNotable health geographers include:\n\n\n\n"}
{"id": "48566508", "url": "https://en.wikipedia.org/wiki?curid=48566508", "title": "Healthcare in Madagascar", "text": "Healthcare in Madagascar\n\nMedical centers, dispensaries and hospitals are found throughout the island of Madagascar, although they are concentrated in urban areas and particularly in Antananarivo. Access to medical care remains beyond the reach of many Malagasy. \n\nIn addition to the high expense of medical care relative to the average Malagasy income, the prevalence of trained medical professionals remains extremely low. In 2010 Madagascar had an average of three hospital beds per 10,000 people and a total of 3,150 doctors, 5,661 nurses, 385 community health workers, 175 pharmacists and 57 dentists for a population of 22 million. The only hospital with a cancer ward is Joseph Ravoahangy Andrianavalona Hospital. 14.6 percent of government spending in 2008 was directed toward the health sector. Approximately 70% of spending on health was contributed by the government, while 30% originated with international donors and other private sources. The government provides at least one basic health center per commune. Private health centers are concentrated within urban areas and particularly those of the central highlands.\n\nDespite these barriers to access, health services have shown a trend toward improvement over the past twenty years. Child immunizations against such diseases as hepatitis B, diphtheria and measles increased an average of 60% in this period, indicating low but increasing availability of basic medical services and treatments. The Malagasy fertility rate in 2009 was 4.6 children per woman, declining from 6.3 in 1990. Teen pregnancy rates of 14.8% in 2011, much higher than the African average, are a contributing factor to rapid population growth. In 2010 the maternal mortality rate was 440 per 100,000 births, compared to 373.1 in 2008 and 484.4 in 1990, indicating a decline in perinatal care following the 2009 coup. The infant mortality rate in 2011 was 41 per 1,000 births, with an under-five mortality rate at 61 per 1,000 births. Schistosomiasis, malaria and sexually transmitted diseases are common in Madagascar, although infection rates of AIDS remain low relative to many countries in mainland Africa, at only 0.2% of the adult population. The malaria mortality rate is also among the lowest in Africa at 8.5 deaths per 100,000 people, in part due to the highest frequency use of insecticide treated nets in Africa. Adult life expectancy in 2009 was 63 years for men and 67 years for women.\n"}
{"id": "13915509", "url": "https://en.wikipedia.org/wiki?curid=13915509", "title": "Healthy development measurement tool", "text": "Healthy development measurement tool\n\nThe Healthy Development Measurement Tool (HDMT), developed by the San Francisco Department of Public Health, provides an approach for evaluating land-use planning and urban development with regards to the achievement of human health needs. The HDMT provides a set of baseline data on community health metrics for San Francisco and development targets to assess the extent to which urban development projects and plans can improve community health. The HDMT also provides a range of policy and design strategies that can advance health conditions and resources via the development process.\n\nIn the San Francisco Bay Area, between the mid- and late- 1990’s, the bustling information economy brought multitudes of young people to the Bay Area and Silicon Valley’s technology-inspired new economy. Housing was notoriously difficult to find, with vacancy rates at less than 2%. During this period, average rents increased by 30% and the cost to buy increased dramatically. Although the economic recession triggered by the dot-com bubble brought the city’s vacancy rates to pre-boom levels, the Bay Area continued to encounter pressure for new housing development due to extraordinary levels of unmet demand and its high profitability. This phenomenon occurred elsewhere in California and throughout the country, in both urban and suburban settings.\n\nHistorically, health inequities were associated with differences in health behaviors and health care access and utilization. Today, however, many believe that these inequities result from differences in access to the social, economic, and environmental resources necessary for health. Increasingly, inter-disciplinary research demonstrates that the root causes of disease and illness, as well as strategies to improve health and well-being are dependent on community design, land use, and transportation. Changes in societal conditions can affect many individuals simultaneously, and have broad and diverse impacts on multiple health outcomes.\n\nThe value of this tool is that it focuses on broadening the range of social, economic, and environmental resources needed for health on a population level. It does so by recognizing a range of resources needed for optimal health at the societal level and identifying measurable and actionable ways to meet those needs through urban development. It combines quantitative analysis of health indicators with a qualitative assessment of whether plans and projects meet tool development targets.\n\nSFDPH has recognized that it has a legitimate agency interest in integrating health considerations into land use decision-making. While SFDPH does not have formal decision-making authority regarding land use and development decisions, a number of drivers brought SFDPH to understand that it has a potentially important role. Drivers include:\nThe first driver for SFDPH to be involved in land use was that community groups were struggling with the pace of development in their neighborhoods. In addition, they were dissatisfied with the responsiveness of the Planning Department to address neighborhood needs and concerns, including displacement of existing residents and jobs, and an overall lack of infrastructure to support a complete neighborhood. Many groups called for community planning processes and specifically community, social, and economic impact assessments of land use changes to be conducted as part of or complementary to the environmental impact report required by CEQA.\n\nThe second driver was that SFDPH increasingly recognized that environmental health and justice issues in San Francisco had roots in land use and transportation planning decisions. For example, SFDPH environmental health inspectors frequently observed that families lived in housing conditions that caused a variety of health outcomes such as asthma and lead poisoning.\nHowever, because of the high costs associated with improving these conditions, landlords often would not take action. In addition, the high cost of housing made it difficult or families to leave their homes and find new places to live.\n\nCumulatively, SFDPH also observed the disproportionate share of unwanted land uses (such as power plants, sewage treatment facilities, substandard public housing, and poor public infrastructure) in places like Bayview/Hunters Point as contributing to significant disparities in life expectancy for residents. Finally, SFDPH also witnessed residential development in historically industrial areas generating noise, traffic emissions, and pedestrian hazards for residents and workers in these areas.\n\nThird, on a national scale, the public health and urban planning communities were increasingly calling attention to the connections between the built environment (that is, land use, transportation systems, and community design) and health, particularly focusing on the contribution of the land use patterns (for example, sprawl) to physical inactivity, pedestrian safety, and air quality. Findings illustrated that urban design and land use regulations could accomplish the complementary goals of preventing illness and ensuring environmental quality. For example, creating higher density, mixed-use developments closer to transit and job centers would enhance public safety, prevent motor-vehicle injuries, increase access to goods and services, encourage walking or bicycling, reduce air pollution, and limit global warming.\n\nFinally, on an international scale, public health practitioners were also developing methods and tools for Health Impact Assessment. The goal of HIA was to bring to light information on how diverse public policy decisions might affect health as well as the social and environmental resources required for good health. While HIA was novel in the United States, it presented a potential way to gain consideration more pro-actively of both root causes of poor health and community needs in the land use development process.\n\nBy 2001, SFDPH had already begun using HIA methods to increase the inclusion of health considerations in policy-making. In a study examining the health impacts of increasing the city’s living wage, SFDPH found that adoption of an increased living wage would result in decreases in the risk of premature death by 5% for adults 24–44 years of age in households whose current income was around $20,000. For the offspring of these workers, a living wage would result in an increase of a quarter of a year of completed education, a 34% increase in the odds of high school completion, and a 22% decrease in the risk of early childbirth.\n\nSFDPH also conducted exploratory workshops with community members on the health impacts of housing subsidies, farmers’ markets, and green schoolyards. In 2002, SFDPH began using HIA more specifically in local land use planning, policy making, and project review. For example, SFDPH conducted HIAs of:\n\nIn response to development pressures, many communities called on public health officials to evaluate the ensuing health impacts and to advocate for healthy environments. For this reason, the San Francisco Department of Public Health initiated the Eastern Neighborhoods Community Health Impact Assessment (ENCHIA) to analyze how development affects social determinants of health within several San Francisco neighborhoods.\n\nThe HDMT was created through collaboration among development stakeholders and public agencies in San Francisco as a result of the ENCHIA. The process was guided by the principles of \"health impact assessment\" and designed to act on growing scientific understanding that optimal health cannot be achieved by improving health services or individual behavior change alone, but requires advancing healthful neighborhood conditions. Such conditions include:\n\nFacilitated and staffed by SFDPH, the eighteen-month ENCHIA process was guided by a multi-stakeholder Community Council of over twenty diverse organizations including: \n\nThis tool is not a new form of environmental regulation or a set of enforceable standards. The tool does not mandate the achievement of specific development targets. Similar to tools such as Leadership in Energy and Environmental Design (LEED), this tool is intended to encourage voluntary efforts to improve health-oriented development.\n\nHDMT provides a systematic assessment approach to simultaneously consider effects of development on six overarching domains:\n\nThese six elements are divided into 27 objectives, which themselves are further divided into 107 measurable indicators. Most indicators are paired with specific, actionable development targets, usually as a benchmark and/or minimum. As of November 1, 2007, fourteen of these indicators are in process.\n\nThe San Francisco Planning Department has received and reviewed the HDMT and other ENCHIA products and has committed to using the indicators and development criteria, where possible, in screening the content of its Eastern Neighborhood plans.\n\nThis tool comprises a set of metrics to evaluate the extent to which urban development is meeting the needs of human health. \nMeasurable indicators and development targets provide information about both the positive and negative ways in which health is impacted by a proposed development project and focuses attention on ways that development can improve population health. By providing measures and criteria for development, it allows those involved in policy- and decision-making to make more informed choices between trade-offs. As a result, the tool may provide an additional means to support greater transparency in development processes.\n\nThis diagram depicts the relationship of many of the tool's components.\nThus far, SFDPH has applied the HDMT to evaluate San Francisco's Eastern neighborhoods. Pilot applications for smaller geographical areas are ongoing.\n\n\n"}
{"id": "39127332", "url": "https://en.wikipedia.org/wiki?curid=39127332", "title": "High-altitude adaptation in humans", "text": "High-altitude adaptation in humans\n\nHigh-altitude adaptation in humans is an instance of evolutionary modification in certain human populations, including those of Tibet in Asia, the Andes of the Americas, and Ethiopia in Africa, who have acquired the ability to survive at extremely high altitudes. This adaptation means irreversible, long-term physiological responses to high-altitude environments, associated with heritable behavioural and genetic changes.\n\nWhile the rest of the human population would suffer serious health consequences, the indigenous inhabitants of these regions thrive well in the highest parts of the world. These people have undergone extensive physiological and genetic changes, particularly in the regulatory systems of oxygen respiration and blood circulation, when compared to the general lowland population.\n\nThis special adaptation is now recognised as an example of natural selection in action. The adaptation account of the Tibetans has become the fastest case of human evolution in the scientific record, as it is estimated to have occurred in less than 3,000 years.\n\nModern humans dispersed from Africa less than 100,000 years ago, and eventually colonized the rest of the world, including the harshest environments of extreme cold and high mountains. The abundance of oxygen in the atmosphere is inversely related to altitude from the sea level; hence, the highest mountain ranges of the world are considered unsuitable for human habitation.\n\nNevertheless, around 140 million people, just under 2% of the world's human population, live permanently at high altitudes, that is, at heights above in South America, East Africa, and South Asia. These populations have done so for millennia without apparent complications. The overwhelming majority, over 98% of humans from other parts of the world, normally suffer symptoms of altitude sickness in these regions, often resulting in life-threatening trauma and even death.\n\nStudies on the detail biological mechanism have revealed that adaptation of the Tibetans, Andeans and Ethiopians is indeed an observable instance of the process of natural selection in acting on favourable characters such as enhanced respiratory mechanisms in humans.\n\nHumans are naturally adapted to lowland environment where oxygen is abundant. When people from the general lowlands go to altitudes above , with atmospheric pressure 74% of normal they experience mountain sickness, which is a type of hypoxia, a clinical syndrome of severe lack of oxygen. Complications include fatigue, dizziness, breathlessness, headaches, insomnia, malaise, nausea, vomiting, body pain, loss of appetite, ear-ringing, blistering and purpling of the hands and feet, and dilated veins.\n\nThe sickness is compounded by related symptoms such as cerebral oedema (swelling of brain) and pulmonary oedema (fluid accumulation in lungs). For several days, they breathe excessively and burn extra energy even when the body is relaxed. The heart rate then gradually decreases. Hypoxia, in fact, is one of the principal causes of death among mountaineers. In women, pregnancy can be severely affected, such as development of high blood pressure, called preeclampsia, which causes premature labour, low birth weight of babies, and often complicated with profuse bleeding, seizures, and death of the mother.\n\nMore than 140 million people worldwide are estimated to live at an elevation higher than above sea level, of which 13 million are in Ethiopia, 1.7 million in Tibet (total of 78 million in Asia), 35 million in the South American Andes, and 0.3 million in Colorado Rocky Mountains. Certain natives of Tibet, Ethiopia, and the Andes have been living at these high altitudes for generations and are protected from hypoxia as a consequence of genetic adaptation. It is estimated that at , every lungful of air only has 60% of the oxygen molecules that people at sea level have. At elevations above , lack of oxygen becomes seriously lethal. That is, these highlanders are constantly exposed to an intolerably low oxygen environment, yet they live without any debilitating problems. Basically, the shared adaptation is the ability to maintain relatively low levels of haemoglobin, which is the chemical complex for transporting oxygen in the blood. One of the best documented effects of high altitude is a progressive reduction in birth weight. It has been known that women of long-resident high-altitude population are not affected. These women are known to give birth to heavier-weight infants than women of lowland inhabitants. This is particularly true among Tibetan babies, whose average birth weight is 294-650 (~470) g heavier than the surrounding Chinese population; and their blood-oxygen level is considerably higher.\n\nThe first scientific investigations of high-altitude adaptation was done by A. Roberto Frisancho of the University of Michigan in the late 1960s among the Quechua people of Peru. Paul T. Baker, Penn State University, (in the Department of Anthropology) also conducted a considerable amount of research into human adaptation to high altitudes, and mentored students who continued this research. One of these students went on to conduct research on high altitude adaptation among the Tibetans in the early 1980s through to today, anthropologist Cynthia Beall of the Case Western Reserve University.\n\nScientists started to notice the extraordinary physical performance of Tibetans since the beginning of Himalayan climbing era in the early 20th century. The hypothesis of a possible evolutionary genetic adaptation makes sense. The Tibetan plateau has an average elevation of above sea level, and covering more than 2.5 million km, it is the highest and largest plateau in the world. In 1990, it was estimated that 4,594,188 Tibetans live on the plateau, with 53% living at an altitude over . Fairly large numbers (about 600,000) live at an altitude exceeding in the Chantong-Qingnan area. Where the Tibetan highlanders live, the oxygen level is only about 60% of that at sea level. The Tibetans, who have been living in this region for 3,000 years, do not exhibit the elevated haemoglobin concentrations to cope with oxygen deficiency as observed in other populations who have moved temporarily or permanently at high altitudes. Instead, the Tibetans inhale more air with each breath and breathe more rapidly than either sea-level populations or Andeans. Tibetans have better oxygenation at birth, enlarged lung volumes throughout life, and a higher capacity for exercise. They show a sustained increase in cerebral blood flow, lower haemoglobin concentration, and less susceptibility to chronic mountain sickness than other populations, due to their longer history of high-altitude habitation.\n\nGeneral people can develop short-term tolerance with careful physical preparation and systematic monitoring of movements, but the biological changes are quite temporary and reversible when they return to lowlands. Moreover, unlike lowland people who only experience increased breathing for a few days after entering high altitudes, Tibetans retain this rapid breathing and elevated lung-capacity throughout their lifetime. This enables them to inhale larger amounts of air per unit of time to compensate for low oxygen levels. In addition, they have high levels (mostly double) of nitric oxide in their blood, when compared to lowlanders, and this probably helps their blood vessels dilate for enhanced blood circulation. Further, their haemoglobin level is not significantly different (average 15.6 g/dl in males and 14.2 g/dl in females), from those of people living at low altitude. (Normally, mountaineers experience >2 g/dl increase in Hb level at Mt. Everest base camp in two weeks.) In this way they are able to evade both the effects of hypoxia and mountain sickness throughout life. Even when they climbed the highest summits like Mt. Everest, they showed regular oxygen uptake, greater ventilation, more brisk hypoxic ventilatory responses, larger lung volumes, greater diffusing capacities, constant body weight and a better quality of sleep, compared to people from the lowland.\n\nIn contrast to the Tibetans, the Andean highlanders, who have been living at high-altitudes for no more than 11,000 years, show different pattern of haemoglobin adaptation. Their haemoglobin concentration is higher compared to those of lowlander population, which also happens to lowlanders moving to high altitude. When they spend some weeks in the lowland their haemoglobin drops to average of other people. This shows only temporary and reversible acclimatisation. However, in contrast to lowland people, they do have increased oxygen level in their haemoglobin, that is, more oxygen per blood volume than other people. This confers an ability to carry more oxygen in each red blood cell, making a more effective transport of oxygen in their body, while their breathing is essentially at the same rate. This enables them to overcome hypoxia and normally reproduce without risk of death for the mother or baby. The Andean highlanders are known from the 16th-century missionaries that their reproduction had always been normal, without any effect in the giving birth or the risk for early pregnancy loss, which are common to hypoxic stress. They have developmentally acquired enlarged residual lung volume and its associated increase in alveolar area, which are supplemented with increased tissue thickness and moderate increase in red blood cells. Though the physical growth in body size is delayed, growth in lung volumes is accelerated. An incomplete adaptation such as elevated haemoglobin levels still leaves them at risk for mountain sickness with old age. \n\nAmong the Quechua people of the Altiplano, there is a significant variation in \"NOS3\" (the gene encoding endothelial nitric oxide synthase, eNOS), which is associated with higher levels of nitric oxide in high altitude. Nuñoa children of Quechua ancestry exhibit higher blood-oxygen content (91.3) and lower heart rate (84.8) than their counterpart school children of different ethnicity, who have an average of 89.9 blood-oxygen and 88-91 heart rate. High-altitude born and bred females of Quechua origins have comparatively enlarged lung volume for increased respiration. \n\nBlood profile comparisons show that among the Andeans, Aymaran highlanders are better adapted to highlands than the Quechuas. Among the Bolivian Aymara people, the resting ventilation and hypoxic ventilatory response were quite low (roughly 1.5 times lower), in contrast to those of the Tibetans. The intrapopulation genetic variation was relatively less among the Aymara people. Moreover, unlike the Tibetans, the blood haemoglobin level is quite normal among Aymarans, with an average of 19.2 g/dl for males and 17.8 g/dl for females. Among the different native highlander populations, the underlying physiological responses to adaptation are quite different. For example, among four quantitative features, such as are resting ventilation, hypoxic ventilatory response, oxygen saturation, and haemoglobin concentration, the levels of variations are significantly different between the Tibetans and the Aymaras. The Andeans, in general are the most poorly adapted, as particularly shown by their frequent mountain sickness and loss of adaptative characters when they move to lowlands.\n\nThe peoples of the Ethiopian highlands also live at extremely high altitudes, around to . Highland Ethiopians exhibit elevated haemoglobin levels, like Andeans and lowlander peoples at high altitudes, but do not exhibit the Andean’s increased in oxygen-content of haemoglobin. Among healthy individuals, the average haemoglobin concentrations are 15.9 and 15.0 g/dl for males and females respectively (which is lower than normal, almost similar to the Tibetans), and an average oxygen saturation of haemoglobin is 95.3% (which is higher than average, like the Andeans). Additionally, Ethiopian highlanders do not exhibit any significant change in blood circulation of the brain, which has been observed among the Peruvian highlanders (and attributed to their frequent altitude-related illnesses). Yet, similar to the Andeans and Tibetans, the Ethiopian highlanders are immune to the extreme dangers posed by high-altitude environment, and their pattern of adaptation is definitely unique from that of other highland peoples.\n\nThe underlying molecular evolution of high-altitude adaptation has been explored and understood fairly recently. Depending on the geographical and environmental pressures, high-altitude adaptation involves different genetic patterns, some of which\nhave evolved quite recently. For example, Tibetan adaptations became prevalent in the past 3,000 years, a rapid example of recent human evolution. At the turn of the 21st century, it was reported that the genetic make-up of the respiratory components of the Tibetan and the Ethiopian populations are significantly different.\n\nSubstantial evidence in Tibetan highlanders suggests that variation in haemoglobin and blood-oxygen levels are adaptive as Darwinian fitness. It has been documented that Tibetan women with a high likelihood of possessing one to two alleles for high blood-oxygen content (which is odd for normal women) had more surviving children; the higher the oxygen capacity, the lower the infant mortality. In 2010, for the first time, the genes responsible for the unique adaptive traits were identified following genome sequences of 50 Tibetans and 40 Han Chinese from Beijing. Initially, the strongest signal of natural selection detected was a transcription factor involved in response to hypoxia, called endothelial Per-Arnt-Sim (PAS) domain protein 1 (\"EPAS1\"). It was found that one single-nucleotide polymorphism (SNP) at \"EPAS1\" shows a 78% frequency difference between Tibetan and mainland Chinese samples, representing the fastest genetic change observed in any human gene to date. Hence, Tibetan adaptation to high altitude becomes the fastest process of phenotypically observable evolution in humans, which is estimated to occur in less than 3,000 years ago, when the Tibetans split up from the mainland Chinese population. Mutations in \"EPAS1\", at higher frequency in Tibetans than their Han neighbours, correlate with decreased haemoglobin concentrations among the Tibetans, which is the hallmark of their adaptation to hypoxia. Simultaneously, two genes, egl nine homolog 1 (\"EGLN1\") (which inhibits haemoglobin production under high oxygen concentration) and peroxisome proliferator-activated receptor alpha (\"PPARA\"), were also identified to be positively selected in relation to decreased haemoglobin nature in the Tibetans.\n\nSimilarly, the Sherpas, known for their Himalayan hardiness, exhibit similar patterns in the \"EPAS1\" gene, which further fortifies that the gene is under selection for adaptation to the high-altitude life of Tibetans. A study in 2014 indicates that the mutant \"EPAS1\" gene could have been inherited from archaic hominins, the Denisovans. \"EPAS1\" and \"EGLN1\" are definitely the major genes for unique adaptive traits when compared with those of the Chinese and Japanese. Comparative genome analysis in 2014 revealed that the Tibetans inherited an equal mixture of genomes from the Nepalese-Sherpas and Hans, and they acquired the adaptive genes from the sherpa-lineage. Further, the population split was estimated to occur around 20,000 to 40,000 years ago, a range of which support archaeological, mitochondria DNA and Y chromosome evidence for an initial colonisation of the Tibetan plateau around 30,000 years ago.\n\nThe genes (\"EPAS1\", \"EGLN1\", and \"PPARA\") function in concert with another gene named hypoxia inducible factors (\"HIF\"), which in turn is a principal regulator of red blood cell production (erythropoiesis) in response to oxygen metabolism. The genes are associated not only with decreased haemoglobin levels, but also in regulating energy metabolism. \"EPAS1\" is significantly associated with increased lactate concentration (the product of anaerobic glycolysis), and \"PPARA\" is correlated with decrease in the activity of fatty acid oxidation. \"EGLN1\" codes for an enzyme, prolyl hydroxylase 2 (PHD2), involved in erythropoiesis. Among the Tibetans, mutation in \"EGLN1\" (specifically at position 12, where cytosine is replaced with guanine; and at 380, where G is replaced with C) results in mutant PHD2 (aspartic acid at position 4 becomes glutamine, and cysteine at 127 becomes serine) and this mutation inhibits erythropoiesis. The mutation is estimated to occur about 8,000 years ago. Further, the Tibetans are enriched for genes in the disease class of human reproduction (such as genes from the \"DAZ\", \"BPY2\", \"CDY\", and \"HLA-DQ\" and \"HLA-DR\" gene clusters) and biological process categories of response to DNA damage stimulus and DNA repair (such as \"RAD51\", \"RAD52\", and \"MRE11A\"), which are related to the adaptive traits of high infant birth weight and darker skin tone and are most likely due to recent local adaptation.\n\nThe patterns of genetic adaptation among the Andeans are largely distinct from those of the Tibetan, with both populations showing evidence of positive natural selection in different genes or gene regions. However, \"EGLN1\" appears to be the principal signature of evolution, as it shows evidence of positive selection in both Tibetans and Andeans. Even then, the pattern of variation for this gene differs between the two populations. Among the Andeans, there are no significant associations between \"EPAS1\" or \"EGLN1\" SNP genotypes and haemoglobin concentration, which has been the characteristic of the Tibetans. The whole genome sequences of 20 Andeans (half of them having chronic mountain sickness) revealed that two genes, SENP1 (an erythropoiesis regulator) and ANP32D (an oncogene) play vital roles in their weak adaptation to hypoxia.\n\nThe adaptive mechanism of Ethiopian highlanders is quite different. This is probably because their migration to the highland was relatively early; for example, the Amhara have inhabited altitudes above for at least 5,000 years and altitudes around to for more than 70,000 years. Genomic analysis of two ethnic groups, Amhara and Oromo, revealed that gene variations associated with haemoglobin difference among Tibetans or other variants at the same gene location do not influence the adaptation in Ethiopians. Identification of specific genes further reveals that several candidate genes are involved in Ethiopians, including \"CBARA1\", \"VAV3\", \"ARNT2\" and \"THRB\". Two of these genes (\"THRB\" and \"ARNT2\") are known to play a role in the HIF-1 pathway, a pathway implicated in previous work reported in Tibetan and Andean studies. This supports the concept that adaptation to high altitude arose independently among different highlanders as a result of convergent evolution.\n\n\n"}
{"id": "19771103", "url": "https://en.wikipedia.org/wiki?curid=19771103", "title": "IPP-SHR", "text": "IPP-SHR\n\nInternational Program of Psycho-Social Health Research (IPP-SHR) is an Australian research program based in Queensland which explores the psycho-social dimension of health through examining and reporting on the human experience of serious physical and mental illnesses.\n\nThe International Program for Psycho Social Health was established in 2006 by A/Pr Pam McGrath as a research initiative funded by the National Health and Medical Research Council (NHMRC) and CQUniversity. In 2011, the International Program of Psycho-Social Health Research moved to Griffith Health Institute, Griffith University.\n\nIPP-SHR explores the psycho-social dimension of health through a wide range of topics including: palliative care; haematology/oncology; mental health; acute medicine; bioethics; rural and remote health; Indigenous health; spirituality; paediatrics; birth studies; and service delivery evaluation.\n\nIPP-SHR also produces two industry focused publications including a quarterly review and a weekly podcast.\n"}
{"id": "18766553", "url": "https://en.wikipedia.org/wiki?curid=18766553", "title": "Isolation (health care)", "text": "Isolation (health care)\n\nIn health care facilities, isolation represents one of several measures that can be taken to implement infection control: the prevention of contagious diseases from being spread from a patient to other patients, health care workers, and visitors, or from outsiders to a particular patient (reverse isolation). Various forms of isolation exist, in some of which contact procedures are modified, and others in which the patient is kept away from all others. In a system devised, and periodically revised, by the U.S. Centers for Disease Control and Prevention (CDC), various levels of patient isolation comprise application of one or more formally described \"precaution\".\n\nIsolation is most commonly used when a patient is known to have a contagious (transmissible person-to-person) viral or bacterial illness. Special equipment is used in the management of patients in the various forms of isolation. These most commonly include items of personal protective equipment (gowns, masks, and gloves) and engineering controls (positive pressure rooms, negative pressure rooms, laminar air flow equipment, and various mechanical and structural barriers). Dedicated isolation wards may be pre-built into hospitals, or isolation units may be temporarily designated in facilities in the midst of an epidemic emergency.\n\nIsolation is defined as the voluntary or compulsory separation and confinement of those known or suspected to be infected with a contagious disease agent (whether ill or not) to prevent further infections. (In this form of isolation, transmission-based precautions are imposed.) In contrast, quarantine is the compulsory separation and confinement, with restriction of movement, of healthy individuals or groups who have potentially been exposed to an agent to prevent further infections should infection occur. Biocontainment refers to laboratory biosafety in microbiology laboratories in which the physical containment (BSL-3, BSL-4) of highly pathogenic organisms is accomplished through built-in engineering controls.\n\n\"Universal precautions\" refer to the practice, in medicine, of avoiding contact with patients' bodily fluids, by means of the wearing of nonporous articles such as medical gloves, goggles, and face shields. The practice was widely introduced in 1985–88. In 1987, the practice of universal precautions was adjusted by a set of rules known as body substance isolation. In 1996, both practices were replaced by the latest approach known as standard precautions. Use of personal protective equipment is now recommended in all health settings.\n\n\"Transmission-based precautions\" are additional infection control precautions — over and above universal/standard precautions — and the latest routine infection prevention and control practices applied for patients who are known or suspected to be infected or colonized with infectious agents, including certain epidemiologically important pathogens. The latter require additional control measures to effectively prevent transmission.\n\nThere are three types of transmission-based precaution:\n\n\nStrict isolation is used for diseases spread through the air and in some cases by contact. Patients must be placed in isolation to prevent the spread of infectious diseases. Those who are kept in strict isolation are often kept in a special room at the facility designed for that purpose. Such rooms are equipped with a special lavatory and caregiving equipment, and a sink and waste disposal are provided for workers upon leaving the area.\n\nContact isolation is used to prevent the spread of diseases that can be spread through contact with open wounds. Health care workers making contact with a patient on contact isolation are required to wear gloves, and in some cases, a gown.\n\nRespiratory isolation is used for diseases that are spread through particles that are exhaled. Those having contact with or exposure to such a patient are required to wear a mask.\n\nReverse isolation is a way to prevent a patient in a compromised health situation from being contaminated by other people or objects. It often involves the use of laminar air flow and mechanical barriers (to avoid physical contact with others) to isolate the patient from any harmful pathogens present in the external environment.\n\nHigh isolation is used to prevent the spread of unusually highly contagious, or high consequence, infectious diseases (e.g., smallpox, Ebola virus). It stipulates mandatory use of: (1) gloves (or double gloves if appropriate), (2) protective eyewear (goggles or face shield), (3) a waterproof gown (or total body Tyvek suit, if appropriate), and (4) a respirator (at least FFP2 or N95 NIOSH equivalent), \"not\" simply a surgical mask. Sometimes negative pressure rooms or powered air-purifying respirators (PAPRs) are also used.\n\nIsolation can have the following negative effects on patients and staff:\n\nHealth care workers who become infected with certain contagious agents may not be permitted to work with patients for a period of time. While facility rules and laws vary from place to place, a common guideline that has been set is 48 hours of removal. Technically, however, this form of infection control is not considered \"isolation\".\n\n\n"}
{"id": "2522519", "url": "https://en.wikipedia.org/wiki?curid=2522519", "title": "Macroorchidism", "text": "Macroorchidism\n\nMacroorchidism is a disorder found in males where a subject has abnormally large testes. The condition is commonly inherited in connection with fragile X syndrome, which is also the second most common genetic cause of mental disabilities. The opposite side of the spectrum is called microorchidism, which is the condition of abnormally small testes.\n\nOther possible causes of macroorchidism are long-standing primary hypothyroidism, adrenal remnants in congenital adrenal hyperplasia, follicle stimulating hormone (FSH)-secreting pituitary macroadenomas, local tumors, lymphomas, or aromatase deficiency.\n\n"}
{"id": "8977613", "url": "https://en.wikipedia.org/wiki?curid=8977613", "title": "Mad Pride", "text": "Mad Pride\n\nMad Pride is a mass movement of the users of mental health services, former users, and the aligned, and that individuals with mental illness should be proud of their 'mad' identity. It was formed in 1993 in response to local community prejudices towards people with a psychiatric history living in boarding homes in the Parkdale area of Toronto, Ontario, Canada, and an event has been held every year since then in the city except for 1996. A similar movement began around the same time in the United Kingdom. By the late 1990s similar events were being organized under the Mad Pride name around the globe, including Australia, Ireland, Portugal, Brazil, Madagascar, South Africa and the United States. Events draw thousands of participants, according to MindFreedom International, a United States mental health advocacy organization that promotes and tracks events spawned by the movement.\n\nMad Pride activists seek to reclaim terms such as \"mad\", \"nutter\", and \"psycho\" from misuse, such as in tabloid newspapers and in order to switch it from a negative view into a positive view. Through a series of mass media campaigns, Mad Pride activists seek to re-educate the general public on such subjects as the causes of mental disabilities, the experiences of those using the mental health system, and the global suicide pandemic. One of Mad Pride's founding activists in the UK was Pete Shaughnessy, who later died by suicide. Mark Roberts, Robert Dellar (who died in 2016) and Simon Barnet were among the other founders of the movement. \"Mad Pride: A celebration of mad culture\" records the early Mad Pride movement. \"On Our Own: Patient-Controlled Alternatives to the Mental Health System\", published in 1978 by Judi Chamberlin, is a foundational text in the Mad Pride movement, although it was published before the movement was launched.\n\nThe first known event, specifically organized as a Pride event by people who identified as survivors, consumers or ex-patients of psychiatric practices, was held on 18 September 1993, when it was called \"Psychiatric Survivor Pride Day\".\n\nMad Studies grew out of Mad Pride and survivor thinking, and focuses on developing scholarly thinking around \"mental health\" by academics who self-identify as mad. As noted in \"Mad matters: a critical reader in Canadian mad studies\" (LeFrançois, Menzies and Reaume, 2013), \"Mad Studies can be defined in general terms as a project of inquiry, knowledge production, and political action devoted to the critique and transcendence of psy-centred ways of thinking, behaving, relating, and being\". As a book, Mad Matters' offers a critical discussion of mental health and madness in ways that demonstrate the struggles, oppression, resistance, agency and perspectives of Mad people to challenge dominant understandings of ‘mental illness’\". \"\"Mad Studies\" is a growing, evolving, multi-voiced and interdisciplinary field of activism, theory, praxis and scholarship.\"\n\nMad Pride was launched alongside a book of the same name, \"Mad Pride: A celebration of mad culture\", published in 2000. On May 11, 2008, Gabrielle Glaser documented Mad Pride in \"The New York Times\". Glaser stated, \"Just as gay-rights activists reclaimed the word queer as a badge of honor rather than a slur, these advocates proudly call themselves mad; they say their conditions do not preclude them from productive lives.\"\n\nElizabeth Packard (1816-1897) was deemed insane by her husband as she did not agree with his conservative political views. In Illinois at the time, involuntary admission to an asylum did not require a public hearing so long as it was a husband admitting his wife. Due to this, Packard was institutionalized though she saw herself to be sane. In Packard’s lifetime to be labeled as ‘mad’ was a form of social disapproval. However, she felt solidarity among Mad people due to her experience in the institution. Though she did not personally identify as Mad and had to identify as ‘sane’ in order to be an activist, it is here that we see early forms of organizing from ex-patients.\n\nThe Mad Pride movement has spawned recurring cultural events in Toronto, London, Paris and other cities around the world. These events often include music, poetry readings, film screenings, and street theatre, such as \"bed push\" protests, which aim to raise awareness about the poor levels of choice of treatments and the widespread use of force in psychiatric hospitals. Past events have included British journalist Jonathan Freedland, and popular novelist Clare Allan. Mad Pride cultural events take a variety of forms, such as the South London collective Creative Routes, the Chipmunka Publishing enterprise, and the many works of Dolly Sen.\n\nA Bed Push is a method of activism employed by multiple mental health agencies and advocates as a method of raising awareness about psychiatric care. Activists wheel a gurney through public spaces to provoke discussion about mental health care. Mind Freedom has a recipe for a successful Bed Push on their website, urging participants to remain peaceful but also be seen by blowing horns, slightly disrupting traffic and playing music. Often patients in psychiatric care feel silenced and powerless, showing resilience in the face of that and securing visibility is a method of regaining dignity.\n\nMad Pride Week in Toronto is proclaimed as such by the city itself. The festivities surrounding this week are highlighted by the Mad Pride Bed Push, typically on the 14th of July. The event takes place Toronto’s Queen Street West “to raise public awareness about the use of force and lack of choice for people ensnared in the Ontario mental health system” This week is officially run by Toronto Mad Pride which partners a number of mental health agencies in the city. In recent years, some advocates have pushed for Parkdale, Toronto to be renamed MAD! Village, to reclaim pride in its surrounding communities' long history of struggle with mental health and addictions \n\nA series of bed push events take place around London each year.\n\nThe Psychiatric Patient Built Wall Tours take place in Toronto, ON at the CAMH facility on Queen St West. The tours show the patient built walls from the 19th century that are located at present day CAMH. The purpose of the tours is to give a history on the lives of the patients who built the walls, and bring attention to the harsh realities of psychiatry.\n\nGeoffrey Reaume and Heinz Klein first came up with the idea of walking tours as part of a Mad Pride event in 2000. The first wall tour occurred on what is now known as Mad Pride Day, on July 14, 2000, with an attendance of about fifty people. Reaume solely leads the tours, and they have grown from annual events for Mad Pride, to occurring several times throughout the year in all non-winter months.\n\n\n"}
{"id": "258979", "url": "https://en.wikipedia.org/wiki?curid=258979", "title": "Malnutrition", "text": "Malnutrition\n\nMalnutrition is a condition that results from eating a diet in which one or more nutrients are either not enough or are too much such that the diet causes health problems. It may involve calories, protein, carbohydrates, vitamins or minerals. Not enough nutrients is called undernutrition or undernourishment while too much is called overnutrition. Malnutrition is often used to specifically refer to undernutrition where an individual is not getting enough calories, protein, or micronutrients. If undernutrition occurs during pregnancy, or before two years of age, it may result in permanent problems with physical and mental development. Extreme undernourishment, known as starvation, may have symptoms that include: a short height, thin body, very poor energy levels, and swollen legs and abdomen. People also often get infections and are frequently cold. The symptoms of micronutrient deficiencies depend on the micronutrient that is lacking.\nUndernourishment is most often due to not enough high-quality food being available to eat. This is often related to high food prices and poverty. A lack of breastfeeding may contribute, as may a number of infectious diseases such as: gastroenteritis, pneumonia, malaria, and measles, which increase nutrient requirements. There are two main types of undernutrition: protein-energy malnutrition and dietary deficiencies. Protein-energy malnutrition has two severe forms: marasmus (a lack of protein and calories) and kwashiorkor (a lack of just protein). Common micronutrient deficiencies include: a lack of iron, iodine, and vitamin A. During pregnancy, due to the body's increased need, deficiencies may become more common. In some developing countries, overnutrition in the form of obesity is beginning to present within the same communities as undernutrition. Other causes of malnutrition include anorexia nervosa and bariatric surgery.\nEfforts to improve nutrition are some of the most effective forms of development aid. Breastfeeding can reduce rates of malnutrition and death in children, and efforts to promote the practice increase the rates of breastfeeding. In young children, providing food (in addition to breastmilk) between six months and two years of age improves outcomes. There is also good evidence supporting the supplementation of a number of micronutrients to women during pregnancy and among young children in the developing world. To get food to people who need it most, both delivering food and providing money so people can buy food within local markets are effective. Simply feeding students at school is insufficient. Management of severe malnutrition within the person's home with ready-to-use therapeutic foods is possible much of the time. In those who have severe malnutrition complicated by other health problems, treatment in a hospital setting is recommended. This often involves managing low blood sugar and body temperature, addressing dehydration, and gradual feeding. Routine antibiotics are usually recommended due to the high risk of infection. Longer-term measures include: improving agricultural practices, reducing poverty, improving sanitation, and the empowerment of women.\nThere were 815 million undernourished people in the world in 2017 (11% of the total population). This is a reduction of 176 million people since 1990 when 23% were undernourished. In 2012 it was estimated that another billion people had a lack of vitamins and minerals. In 2015, protein-energy malnutrition was estimated to have resulted in 323,000 deaths—down from 510,000 deaths in 1990. Other nutritional deficiencies, which include iodine deficiency and iron deficiency anemia, result in another 83,000 deaths. In 2010, malnutrition was the cause of 1.4% of all disability adjusted life years. About a third of deaths in children are believed to be due to undernutrition, although the deaths are rarely labelled as such. In 2010, it was estimated to have contributed to about 1.5 million deaths in women and children, though some estimate the number may be greater than 3 million. An additional 165 million children were estimated to have stunted growth from malnutrition in 2013. Undernutrition is more common in developing countries. Certain groups have higher rates of undernutrition, including women—in particular while pregnant or breastfeeding—children under five years of age, and the elderly. In the elderly, undernutrition becomes more common due to physical, psychological, and social factors.\n\nUnless specifically mentioned otherwise, the term malnutrition refers to undernutrition for the remainder of this article. Malnutrition can be divided into two different types, SAM and MAM. SAM refers to children with severe acute malnutrition. MAM refers to moderate acute malnutrition.\n\nMalnutrition is caused by eating a diet in which nutrients are \"not enough\" or is \"too much\" such that it causes health problems. It is a category of diseases that includes undernutrition and overnutrition. Overnutrition can result in obesity and being overweight. In some developing countries, overnutrition in the form of obesity is beginning to present within the same communities as undernutrition.\n\nHowever, the term malnutrition is commonly used to refer to undernutrition only. This applies particularly to the context of development cooperation. Therefore, \"malnutrition\" in documents by the World Health Organization, UNICEF, Save the Children or other international non-governmental organizations (NGOs) usually is equated to undernutrition.\n\nUndernutrition is sometimes used as a synonym of protein–energy malnutrition (PEM). While other include both micronutrient deficiencies and protein energy malnutrition in its definition. It differs from calorie restriction in that calorie restriction may not result in negative health effects. The term hypoalimentation means underfeeding.\n\nThe term \"severe malnutrition\" or \"severe undernutrition\" is often used to refer specifically to PEM. PEM is often associated with micronutrient deficiency. Two forms of PEM are kwashiorkor and marasmus, and they commonly coexist.\n\nKwashiorkor is mainly caused by inadequate protein intake. The main symptoms are edema, wasting, liver enlargement, hypoalbuminaemia, steatosis, and possibly depigmentation of skin and hair. Kwashiorkor is further identified by swelling of the belly, which is deceiving of actual nutritional status. The term means ‘displaced child’ and is derived from a Ghana language of West Africa, means \"the sickness the older one gets when the next baby is born,\" as this is when the older child is deprived of breast feeding and weaned to a diet composed largely of carbohydrates.\n\nMarasmus (‘to waste away’) is caused by an inadequate intake of protein and energy. The main symptoms are severe wasting, leaving little or no edema, minimal subcutaneous fat, severe muscle wasting, and non-normal serum albumin levels. Marasmus can result from a sustained diet of inadequate energy and protein, and the metabolism adapts to prolong survival. It is traditionally seen in famine, significant food restriction, or more severe cases of anorexia. Conditions are characterized by extreme wasting of the muscles and a gaunt expression.\n\nUndernutrition encompasses stunted growth (stunting), wasting, and deficiencies of essential vitamins and minerals (collectively referred to as micronutrients). The term hunger, which describes a feeling of discomfort from not eating, has been used to describe undernutrition, especially in reference to food insecurity.\n\nIn 1956, Gómez and Galvan studied factors associated with death in a group of malnourished (undernourished) children in a hospital in Mexico City, Mexico and defined categories of malnutrition: first, second, and third degree. The degrees were based on weight below a specified percentage of median weight for age. The risk of death increases with increasing degree of malnutrition. An adaptation of Gomez's original classification is still used today. While it provides a way to compare malnutrition within and between populations, the classification has been criticized for being \"arbitrary\" and for not considering overweight as a form of malnutrition. Also, height alone may not be the best indicator of malnutrition; children who are born prematurely may be considered short for their age even if they have good nutrition.\n\nJohn Conrad Waterlow established a new classification for malnutrition. Instead of using just weight for age measurements, the classification established by Waterlow combines weight-for-height (indicating acute episodes of malnutrition) with height-for-age to show the stunting that results from chronic malnutrition. One advantage of the Waterlow classification over the Gomez classification is that weight for height can be examined even if ages are not known.\n\nThese classifications of malnutrition are commonly used with some modifications by WHO.\n\nMalnutrition increases the risk of infection and infectious disease, and moderate malnutrition weakens every part of the immune system. For example, it is a major risk factor in the onset of active tuberculosis. Protein and energy malnutrition and deficiencies of specific micronutrients (including iron, zinc, and vitamins) increase susceptibility to infection. Malnutrition affects HIV transmission by increasing the risk of transmission from mother to child and also increasing replication of the virus. In communities or areas that lack access to safe drinking water, these additional health risks present a critical problem. Lower energy and impaired function of the brain also represent the downward spiral of malnutrition as victims are less able to perform the tasks they need to in order to acquire food, earn an income, or gain an education.\n\nVitamin-deficiency-related diseases (such as scurvy and rickets).\n\nHypoglycemia (low blood sugar) can result from a child not eating for 4 to 6 hours. Hypoglycemia should be considered if there is lethargy, limpness, convulsion, or loss of consciousness. If blood sugar can be measured immediately and quickly, perform a finger or heel stick.\n\nIn those with malnutrition some of the signs of dehydration differ. Children; however, may still be interested in drinking, have decreased interactions with the world around them, have decreased urine output, and may be cool to touch.\n\nProtein-calorie malnutrition can cause cognitive impairments. For humans, \"critical period varies from the final third of gestation to the first 2 years of life\". Iron deficiency anemia in children under two years of age likely affects brain function acutely and probably also chronically. Folate deficiency has been linked to neural tube defects.\n\nMalnutrition in the form of iodine deficiency is \"the most common preventable cause of mental impairment worldwide.\" \"Even moderate deficiency, especially in pregnant women and infants, lowers intelligence by 10 to 15 I.Q. points, shaving incalculable potential off a nation's development. The most visible and severe effects — disabling goiters, cretinism and dwarfism — affect a tiny minority, usually in mountain villages. But 16 percent of the world's people have at least mild goiter, a swollen thyroid gland in the neck.\"\n\nMajor causes of malnutrition include poverty and food prices, dietary practices and agricultural productivity, with many individual cases being a mixture of several factors. Clinical malnutrition, such as cachexia, is a major burden also in developed countries. Various scales of analysis also have to be considered in order to determine the sociopolitical causes of malnutrition. For example, the population of a community that is within poor governments, may be at risk if the area lacks health-related services, but on a smaller scale certain households or individuals may be at an even higher risk due to differences in income levels, access to land, or levels of education.\n\nMalnutrition can be a consequence of health issues such as gastroenteritis or chronic illness, especially the HIV/AIDS pandemic. Diarrhea and other infections can cause malnutrition through decreased nutrient absorption, decreased intake of food, increased metabolic requirements, and direct nutrient loss. Parasite infections, in particular intestinal worm infections (helminthiasis), can also lead to malnutrition. A leading cause of diarrhea and intestinal worm infections in children in developing countries is lack of sanitation and hygiene.\n\nPeople may become malnourished due to abnormal nutrient loss (due to diarrhea or chronic illness affecting the small bowel). This conditions may include Crohn's disease or untreated coeliac disease. Malnutrition may also occur due to increased energy expenditure (secondary malnutrition).\n\nA lack of adequate breastfeeding leads to malnutrition in infants and children, associated with the deaths of an estimated one million children annually. Illegal advertising of breast milk substitutes contributed to malnutrition and continued three decades after its 1981 prohibition under the \"WHO International Code of Marketing Breast Milk Substitutes\".\n\nMaternal malnutrition can also factor into the poor health or death of a baby. Over 800,000 neonatal death have occurred because of deficient growth of the fetus in the mother's womb.\n\nDeriving too much of one's diet from a single source, such as eating almost exclusively corn or rice, can cause malnutrition. This may either be from a lack of education about proper nutrition, or from only having access to a single food source.\n\nIt is not just the total amount of calories that matters but specific nutritional deficiencies such as vitamin A deficiency, iron deficiency or zinc deficiency can also increase risk of death.\n\nOvernutrition caused by overeating is also a form of malnutrition. In the United States, more than half of all adults are now overweight — a condition that, like hunger, increases susceptibility to disease and disability, reduces worker productivity, and lowers life expectancy. Overeating is much more common in the United States, where for the majority of people, access to food is not an issue. Many parts of the world have access to a surplus of non-nutritious food, in addition to increased sedentary lifestyles. Yale psychologist Kelly Brownell calls this a \"toxic food environment\" where fat and sugar laden foods have taken precedence over healthy nutritious foods.\n\nThe issue in these developed countries is choosing the right kind of food. More fast food is consumed per capita in the United States than in any other country. The reason for this mass consumption of fast food is its affordability and accessibility. Often fast food, low in cost and nutrition, is high in calories and heavily promoted. When these eating habits are combined with increasingly urbanized, automated, and more sedentary lifestyles, it becomes clear why weight gain is difficult to avoid.\n\nNot only does obesity occur in developed countries, problems are also occurring in developing countries in areas where income is on the rise. Overeating is also a problem in countries where hunger and poverty persist. In China, consumption of high-fat foods has increased while consumption of rice and other goods has decreased.\n\nOvereating leads to many diseases, such as heart disease and diabetes, that may result in death.\n\nIn Bangladesh, poor socioeconomic position was associated with chronic malnutrition since it inhibits purchase of nutritious foods such as milk, meat, poultry, and fruits. As much as food shortages may be a contributing factor to malnutrition in countries with lack of technology, the FAO (Food and Agriculture Organization) has estimated that eighty percent of malnourished children living in the developing world live in countries that produce food surpluses. The economist Amartya Sen observed that, in recent decades, famine has always been a problem of food distribution and/or poverty, as there has been sufficient food to feed the whole population of the world. He states that malnutrition and famine were more related to problems of food distribution and purchasing power.\n\nIt is argued that commodity speculators are increasing the cost of food. As the real estate bubble in the United States was collapsing, it is said that trillions of dollars moved to invest in food and primary commodities, causing the 2007–2008 food price crisis.\n\nThe use of biofuels as a replacement for traditional fuels raises the price of food. The United Nations special rapporteur on the right to food, Jean Ziegler proposes that agricultural waste, such as corn cobs and banana leaves, rather than crops themselves be used as fuel.\n\nLocal food shortages can be caused by a lack of arable land, adverse weather, lower farming skills such as crop rotation, or by a lack of technology or resources needed for the higher yields found in modern agriculture, such as fertilizers, pesticides, irrigation, machinery and storage facilities. As a result of widespread poverty, farmers cannot afford or governments cannot provide the resources necessary to improve local yields. The World Bank and some wealthy donor countries also press nations that depend on aid to cut or eliminate subsidized agricultural inputs such as fertilizer, in the name of free market policies even as the United States and Europe extensively subsidized their own farmers. Many, if not most, farmers cannot afford fertilizer at market prices, leading to low agricultural production and wages and high, unaffordable food prices.\nReasons for the unavailability of fertilizer include moves to stop supplying fertilizer on environmental grounds, cited as the obstacle to feeding Africa by the Green Revolution pioneers Norman Borlaug and Keith Rosenberg.\n\nThere are a number of potential disruptions to global food supply that could cause widespread malnutrition.\n\nGlobal warming is of importance to food security, with 95 percent of all malnourished peoples living in the relatively stable climate region of the sub-tropics and tropics. According to the latest IPCC reports, temperature increases in these regions are \"very likely.\" Even small changes in temperatures can lead to increased frequency of extreme weather conditions. Many of these have great impact on agricultural production and hence nutrition. For example, the 1998–2001 central Asian drought brought about an 80 percent livestock loss and 50 percent reduction in wheat and barley crops in Iran. Similar figures were present in other nations. An increase in extreme weather such as drought in regions such as Sub-Saharan Africa would have even greater consequences in terms of malnutrition. Even without an increase of extreme weather events, a simple increase in temperature reduces the productivity of many crop species, also decreasing food security in these regions.\n\nColony collapse disorder is a phenomenon where bees die in large numbers. Since many agricultural crops worldwide are pollinated by bees, this represents a threat to the supply of food.\n\nThe effort to bring modern agricultural techniques found in the West, such as nitrogen fertilizers and pesticides, to Asia, called the Green Revolution, resulted in increased food production and corresponding decreases in prices and malnutrition similar to those seen earlier in Western nations. This was possible because of existing infrastructure and institutions that are in short supply in Africa, such as a system of roads or public seed companies that made seeds available. Investments in agriculture, such as subsidized fertilizers and seeds, increases food harvest and reduces food prices. For example, in the case of Malawi, almost five million of its 13 million people used to need emergency food aid. However, after the government changed policy and subsidies for fertilizer and seed were introduced against World Bank strictures, farmers produced record-breaking corn harvests as production leaped to 3.4 million in 2007 from 1.2 million in 2005, making Malawi a major food exporter. This lowered food prices and increased wages for farm workers. Such investments in agriculture are still needed in other African countries like the Democratic Republic of the Congo. The country has one of the highest prevalence of malnutrition even though it is blessed with great agricultural potential John Ulimwengu explains in his article for D+C. Proponents for investing in agriculture include Jeffrey Sachs, who has championed the idea that wealthy countries should invest in fertilizer and seed for Africa’s farmers.\n\nIn Nigeria, the use of imported Ready to Use Therapeutic Food (RUTF) has been used to treat malnutrition in the North. \"Soy Kunu\", a locally sourced and prepared blend consisting of peanut, millet and soya beans may also be used.\n\nNew technology in agricultural production also has great potential to combat undernutrition. By improving agricultural yields, farmers could reduce poverty by increasing income as well as open up area for diversification of crops for household use. The World Bank itself claims to be part of the solution to malnutrition, asserting that the best way for countries to succeed in breaking the cycle of poverty and malnutrition is to build export-led economies that will give them the financial means to buy foodstuffs on the world market.\n\nThere is a growing realization among aid groups that giving cash or cash vouchers instead of food is a cheaper, faster, and more efficient way to deliver help to the hungry, particularly in areas where food is available but unaffordable. The UN's World Food Program, the biggest non-governmental distributor of food, announced that it will begin distributing cash and vouchers instead of food in some areas, which Josette Sheeran, the WFP's executive director, described as a \"revolution\" in food aid. The aid agency Concern Worldwide is piloting a method through a mobile phone operator, Safaricom, which runs a money transfer program that allows cash to be sent from one part of the country to another.\n\nHowever, for people in a drought living a long way from and with limited access to markets, delivering food may be the most appropriate way to help. Fred Cuny stated that \"the chances of saving lives at the outset of a relief operation are greatly reduced when food is imported. By the time it arrives in the country and gets to people, many will have died.\" U.S. law, which requires buying food at home rather than where the hungry live, is inefficient because approximately half of what is spent goes for transport. Cuny further pointed out \"studies of every recent famine have shown that food was available in-country — though not always in the immediate food deficit area\" and \"even though by local standards the prices are too high for the poor to purchase it, it would usually be cheaper for a donor to buy the hoarded food at the inflated price than to import it from abroad.\"\nFood banks and soup kitchens address malnutrition in places where people lack money to buy food. A basic income has been proposed as a way to ensure that everyone has enough money to buy food and other basic needs; it is a form of social security in which all citizens or residents of a country regularly receive an unconditional sum of money, either from a government or some other public institution, in addition to any income received from elsewhere.\n\nEthiopia has been pioneering a program that has now become part of the World Bank's prescribed method for coping with a food crisis and had been seen by aid organizations as a model of how to best help hungry nations. Through the country's main food assistance program, the Productive Safety Net Program, Ethiopia has been giving rural residents who are chronically short of food, a chance to work for food or cash. Foreign aid organizations like the World Food Program were then able to buy food locally from surplus areas to distribute in areas with a shortage of food. Ethiopia been pioneering a program, and Brazil has established a recycling program for organic waste that benefits farmers, urban poor, and the city in general. City residents separate organic waste from their garbage, bag it, and then exchange it for fresh fruit and vegetables from local farmers. As a result, the country's waste is reduced and the urban poor get a steady supply of nutritious food.\n\nRestricting population size is a proposed solution. Thomas Malthus argued that population growth could be controlled by natural disasters and voluntary limits through \"moral restraint.\" Robert Chapman suggests that an intervention through government policies is a necessary ingredient of curtailing global population growth. The interdependence and complementarity of population growth with poverty and malnutrition (as well as the environment) is also recognised by the United Nations. More than 200 million women worldwide do not have adequate access to family planning services. According to the World Health Organisation, \"Family planning is key to slowing unsustainable population growth and the resulting negative impacts on the economy, environment, and national and regional development efforts\".\n\nHowever, there are many who believe that the world has more than enough resources to sustain its population. Instead, these theorists point to unequal distribution of resources and under- or unutilized arable land as the cause for malnutrition problems. For example, Amartya Sen advocates that, \"no matter how a famine is caused, methods of breaking it call for a large supply of food in the public distribution system. This applies not only to organizing rationing and control, but also to undertaking work programmes and other methods of increasing purchasing power for those hit by shifts in exchange entitlements in a general inflationary situation.\" \n\nOne suggested policy framework to resolve access issues is termed food sovereignty—the right of peoples to define their own food, agriculture, livestock, and fisheries systems, in contrast to having food largely subjected to international market forces. Food First is one of the primary think tanks working to build support for food sovereignty. Neoliberals advocate for an increasing role of the free market.\n\nAnother possible long term solution would be to increase access to health facilities to rural parts of the world. These facilities could monitor undernourished children, act as supplemental food distribution centers, and provide education on dietary needs. These types of facilities have already proven very successful in countries such as Peru and Ghana.\n\nAs of 2016 is estimated that about 821,000 deaths of children less than five years old could be prevented globally per year through more widespread breastfeeding. In addition to reducing infant death, breast milk feeding provides an important source of micronutrients, clinically proven to bolster the immune system of children, and provide long-term defenses against non-communicable and allergic diseases. Breastfeeding has also been shown to improve cognitive abilities in children, with a strong correlation to individual educational achievements. As previously noted, lack of proper breastfeeding is a major factor in child mortality rates, and a primary determinant of disease development for children. The medical community recommends exclusively breastfeeding infants for 6 months, with nutritional whole food supplementation and continued breastfeeding up to 2 years or older for overall optimal health outcomes. Exclusive breastfeeding is defined as only giving an infant breast milk for six months as a source of food and nutrition. This means no other liquids, including water or semi-solid foods.\n\nBreastfeeding is noted as one of the most cost effective medical interventions for providing beneficial child health. While there are considerable differences within developed and developing countries: income, employment, social norms, and access to healthcare were found to be universal determinants of whether a mother breast or formula fed their children. Community based healthcare workers have helped alleviate financial barriers faced by newly made mothers, and provided a viable alternative to traditional and expensive hospital based medical care. Recent studies based upon surveys conducted from 1995-2010 shows exclusive breastfeeding rates have gone up globally, from 33% to 39%. Despite the growth rates, medical professionals acknowledge the need for improvement given the importance of exclusive breastfeeding.\n\nThe EndingHunger campaign is an online communication campaign aimed at raising awareness of the hunger problem. It has many worked through viral videos depicting celebrities voicing their anger about the large number of hungry people in the world.\n\nFood security and global malnutrition has long been a topic of international concern, with one of the first official global documents addressing it being the 1948 Universal Declaration of Human Rights(UDHR). Within this document it stated that access to food was part of an adequate right to a standard of living. The Right to food was asserted in the International Covenant on Economic, Social and Cultural Rights, a treaty adopted by the United Nations General Assembly on December 16, 1966. The Right to food is a human right for people to feed themselves in dignity, be free from hunger, food insecurity, and malnutrition. As of 2018, the treaty has been signed by 166 countries, by signing states agreed to take steps to the maximum of their available resources to achieve the right to adequate food.\n\nHowever, after the 1966 International Covenant the global concern for the access to sufficient food only became more present, leading to the first ever World Food Conference that was held in 1974 in Rome, Italy. The Universal Declaration on the Eradication of Hunger and Malnutrition was a UN resolution adopted November 16, 1974 by all 135 countries that attended the 1974 World Food Conference. This non-legally binding document set forth certain aspirations for countries to follow to sufficiently take action on the global food problem. Ultimately this document outline and provided guidance as to how the international community as one could work towards fighting and solving the growing global issue of malnutrition and hunger.\n\nAdoption of the right to food was included in the Additional Protocol to the American Convention on Human Rights in the area of Economic, Social, and Cultural Rights, this 1978 document was adopted by many countries in the Americas, the purpose of the document is, \"to consolidate in this hemisphere, within the framework of democratic institutions, a system of personal liberty and social justice based on respect for the essential rights of man.\"\n\nThe next document in the timeline of global inititaves for malnutrition was the 1996 Rome Declaration on World Food Security, organized by the Food and Agriculture Organization. This document reaffirmed the right to have access to safe and nutritous food by everyone, also considering that everyone gets sufficient food, and set the goals for all nations to improve their commitment to food security by halfing their amount of undernourished people by 2015. In 2004 the Food and Agriculture Organization adopted the Right to Food Guidelines, which offered states a framework of how to increase the right to food on a national basis.\n\nIn April 2012, the Food Assistance Convention was signed, the world's first legally binding international agreement on food aid. The May 2012 Copenhagen Consensus recommended that efforts to combat hunger and malnutrition should be the first priority for politicians and private sector philanthropists looking to maximize the effectiveness of aid spending. They put this ahead of other priorities, like the fight against malaria and AIDS.\n\nThe main global policy to reduce hunger and poverty are the Sustainable Development Goals, approved through the UN in 2015. In particular Goal 2: Zero hunger sets globally agreed targets to end hunger, achieve food security and improved nutrition and promote sustainable agriculture. The partnership Compact2025, led by IFPRI with the involvement of UN organisations, NGOs and private foundations develops and disseminates evidence-based advice to politicians and other decision-makers aimed at ending hunger and undernutrition in the coming 10 years, by 2025.\n\nIn June 2015, the European Union and the Bill & Melinda Gates Foundation have launched a partnership to combat undernutrition especially in children. The program will initiatilly be implemented in Bangladesh, Burundi, Ethiopia, Kenya, Laos and Niger and will help these countries to improve information and analysis about nutrition so they can develop effective national nutrition policies.\n\nThe Food and Agriculture Organization of the UN has created a partnership that will act through the African Union's CAADP framework aiming to end hunger in Africa by 2025. It includes different interventions including support for improved food production, a strengthening of social protection and integration of the right to food into national legislation.\n\nThe EndingHunger campaign is an online communication campaign aimed at raising awareness of the hunger problem. It has many worked through viral videos depicting celebrities voicing their anger about the large number of hungry people in the world.\n\nIn response to child malnutrition, the Bangladeshi government recommends ten steps for treating severe malnutrition. They are to prevent or treat dehydration, low blood sugar, low body temperature, infection, correct electrolyte imbalances and micronutrient deficiencies, start feeding cautiously, achieve catch-up growth, provide psychological support, and prepare for discharge and follow-up after recovery.\n\nAmong those who are hospitalized, nutritional support improves protein, calorie intake and weight.\n\nThe evidence for benefit of supplementary feeding is poor. This is due to the small amount of research done on this treatment.\n\nSpecially formulated foods do however appear useful in those from the developing world with moderate acute malnutrition. In young children with severe acute malnutrition it is unclear if ready-to-use therapeutic food differs from a normal diet. They may have some benefits in humanitarian emergencies as they can be eaten directly from the packet, do not require refrigeration or mixing with clean water, and can be stored for years.\n\nIn those who are severely malnourished, feeding too much too quickly can result in refeeding syndrome. This can result regardless of route of feeding and can present itself a couple of days after eating with heart failure, dysrhythmias and confusion that can result in death.\n\nManufacturers are trying to fortify everyday foods with micronutrients that can be sold to consumers such as wheat flour for Beladi bread in Egypt or fish sauce in Vietnam and the iodization of salt.\n\nFor example, flour has been fortified with iron, zinc, folic acid and other B vitamins such as thiamine, riboflavin, niacin and vitamin B12.\n\nTreating malnutrition, mostly through fortifying foods with micronutrients (vitamins and minerals), improves lives at a lower cost and shorter time than other forms of aid, according to the World Bank. The Copenhagen Consensus, which look at a variety of development proposals, ranked micronutrient supplements as number one.\n\nIn those with diarrhea, once an initial four-hour rehydration period is completed, zinc supplementation is recommended. Daily zinc increases the chances of reducing the severity and duration of the diarrhea, and continuing with daily zinc for ten to fourteen days makes diarrhea less likely recur in the next two to three months.\n\nIn addition, malnourished children need both potassium and magnesium. This can be obtained by following the above recommendations for the dehydrated child to continue eating within two to three hours of starting rehydration, and including foods rich in potassium as above. Low blood potassium is worsened when base (as in Ringer's/Hartmann's) is given to treat acidosis without simultaneously providing potassium. As above, available home products such as salted and unsalted cereal water, salted and unsalted vegetable broth can be given early during the course of a child's diarrhea along with continued eating. Vitamin A, potassium, magnesium, and zinc should be added with other vitamins and minerals if available.\n\nFor a malnourished child with diarrhea from any cause, this should include foods rich in potassium such as bananas, green coconut water, and unsweetened fresh fruit juice.\n\nThe World Health Organization (WHO) recommends rehydrating a severely undernourished child who has diarrhea relatively slowly. The preferred method is with fluids by mouth using a drink called oral rehydration solution (ORS). The oral rehydration solution is both slightly sweet and slightly salty and the one recommended in those with severe undernutrition should have half the usual sodium and greater potassium. Fluids by nasogastric tube may be use in those who do not drink. Intravenous fluids are recommended only in those who have significant dehydration due to their potential complications. These complications include congestive heart failure. Over time, ORS developed into ORT, or oral rehydration therapy, which focused on increasing fluids by supplying salts, carbohydrates, and water. This switch from type of fluid to amount of fluid was crucial in order to prevent dehydration from diarrhea.\n\nBreast feeding and eating should resume as soon as possible. Drinks such as soft drinks, fruit juices, or sweetened teas are not recommended as they contain too much sugar and may worsen diarrhea. Broad spectrum antibiotics are recommended in all severely undernourished children with diarrhea requiring admission to hospital.\n\nTo prevent dehydration readily available fluids, preferably with a modest amount of sugars and salt such as vegetable broth or salted rice water, may be used. The drinking of additional clean water is also recommended. Once dehydration develops oral rehydration solutions are preferred. As much of these drinks as the person wants can be given, unless there are signs of swelling. If vomiting occurs, fluids can be paused for 5–10 minutes and then restarting more slowly. Vomiting rarely prevents rehydration as fluid are still absorbed and the vomiting rarely last long. A severely malnourished child with what appears to be dehydration but who has not had diarrhea should be treated as if they have an infection.\n\nFor babies a dropper or syringe without the needle can be used to put small amounts of fluid into the mouth; for children under 2, a teaspoon every one to two minutes; and for older children and adults, frequent sips directly from a cup. After the first two hours, rehydration should be continued at the same or slower rate, determined by how much fluid the child wants and any ongoing diarrheal loses. After the first two hours of rehydration it is recommended that to alternate between rehydration and food.\n\nIn 2003, WHO and UNICEF recommended a reduced-osmolarity ORS which still treats dehydration but also reduced stool volume and vomiting. Reduced-osmolarity ORS is the current standard ORS with reasonably wide availability. For general use, one packet of ORS (glucose sugar, salt, potassium chloride, and trisodium citrate) is added to one liter of water; however, for malnourished children it is recommended that one packet of ORS be added to two liters of water along with an extra 50 grams of sucrose sugar and some stock potassium solution.\n\nMalnourished children have an excess of body sodium. Recommendations for home remedies agree with one liter of water (34 oz.) and 6 teaspoons sugar and disagree regarding whether it is then one teaspoon of salt added or only 1/2, with perhaps most sources recommending 1/2 teaspoon of added salt to one liter water.\n\nHypoglycemia, whether known or suspected, can be treated with a mixture of sugar and water. If the child is conscious, the initial dose of sugar and water can be given by mouth. If the child is unconscious, give glucose by intravenous or nasogastric tube. If seizures occur after despite glucose, rectal diazepam is recommended. Blood sugar levels should be re-checked on two hour intervals.\n\nHypothermia can occur. To prevent or treat this, the child can be kept warm with covering including of the head or by direct skin-to-skin contact with the mother or father and then covering both parent and child. Prolonged bathing or prolonged medical exams should be avoided. Warming methods are usually most important at night.\n\nThe figures provided in this section on epidemiology all refer to \"undernutrition\" even if the term malnutrition is used which, by definition, could also apply to too much nutrition.\n\nThe Global Hunger Index (GHI) is a multidimensional statistical tool used to describe the state of countries’ hunger situation. The GHI measures progress and failures in the global fight against hunger. The GHI is updated once a year. The data from the 2015 report shows that Hunger levels have dropped 27% since 2000. Fifty two countries remain at serious or alarming levels. In addition to the latest statistics on Hunger and Food Security, the GHI also features different special topics each year. The 2015 report include an article on conflict and food security.\n\nThere were 815 million undernourished people in the world in 2017. This was 176 million fewer people than in 1990 when it was 991 million undernourished people. This is despite the world's farmers producing enough food to feed around 12 billion people – almost double the current world population.\n\nMalnutrition, as of 2010, was the cause of 1.4% of all disability adjusted life years.\n\nMortality due to malnutrition accounted for 58 percent of the total mortality in 2006: \"In the world, approximately 62 million people, all causes of death combined, die each year. One in twelve people worldwide is malnourished and according to the Save the Children 2012 report, one in four of the world’s children are chronically malnourished. In 2006, more than 36 million died of hunger or diseases due to deficiencies in micronutrients\".\n\nIn 2010 protein-energy malnutrition resulted in 600,000 deaths down from 883,000 deaths in 1990. Other nutritional deficiencies, which include iodine deficiency and iron deficiency anemia, result in another 84,000 deaths. In 2010 malnutrition caused about 1.5 million deaths in women and children.\n\nAccording to the World Health Organization, malnutrition is the biggest contributor to child mortality, present in half of all cases. Six million children die of hunger every year. Underweight births and intrauterine growth restrictions cause 2.2 million child deaths a year. Poor or non-existent breastfeeding causes another 1.4 million. Other deficiencies, such as lack of vitamin A or zinc, for example, account for 1 million. Malnutrition in the first two years is irreversible. Malnourished children grow up with worse health and lower education achievement. Their own children tend to be smaller. Malnutrition was previously seen as something that exacerbates the problems of diseases such as measles, pneumonia and diarrhea, but malnutrition actually causes diseases, and can be fatal in its own right.\n\nThroughout history, portions of the world's population have often experienced sustained periods of hunger. In many cases, this resulted from food supply disruptions caused by war, plagues, or adverse weather. For the first few decades after World War II, technological progress and enhanced political cooperation suggested it might be possible to substantially reduce the number of people suffering from hunger. While progress was uneven, by 2000 the threat of extreme hunger subsided for many of the world's people. According to the WFP some statistics are that, \"Some 795 million people in the world do not have enough food to lead a healthy active life. That's about one in nine people on earth. The vast majority of the world's hungry people live in developing countries, where 12.9 percent of the population is undernourished.\"\n\nUntil 2006, the average international price of food had been largely stable for several decades. In the closing months of 2006, however, prices began to rise rapidly. By 2008, rice had tripled in price in some regions, and this severely affected developing countries. Food prices fell in early 2009, but rose to another record high in 2011, and have since decreased slightly. The 2008 worldwide financial crisis further increased the number of people suffering from hunger, including dramatic increases even in advanced economies such as Great Britain, the Eurozone and the United States.\n\nThe Millennium Development Goals included a commitment to a further 50% reduction in the proportion of the world's population who have extreme hunger by 2015. As of 2012, this target appeared difficult to achieve, due in part to persistent inflation in food prices. However, in late 2012 the UN's Food and Agriculture Organization (FAO) stated it is still possible to hit the target with sufficient effort. In 2013, the \"FAO\" estimated that 842 million people are undernourished (12% of the global population). Malnutrition is a cause of death for more than 3.1 million children under 5 every year. UNICEF estimates 300 million children go to bed hungry each night; and that 8000 children under the age of 5 are estimated to die of malnutrition every day.\n\nThroughout history, the need to aid those suffering from hunger has been commonly, though not universally, recognized.\n\nThe philosopher Simone Weil wrote that feeding the hungry when you have resources to do so is the most obvious of all human obligations. She says that as far back as Ancient Egypt, many believed that people had to show they had helped the hungry in order to justify themselves in the afterlife. Weil writes that Social progress is commonly held to be first of all, \"...a transition to a state of human society in which people will not suffer from hunger.\" Social historian Karl Polanyi wrote that before markets became the world's dominant form of economic organization in the 19th century, most human societies would either starve all together or not at all, because communities would invariably share their food.\n\nFrom the first age of globalization, which began in the 19th century, it became more common for people to consider problems like hunger in global terms. However, as early globalization largely coincided with the high peak of influence for classical liberalism, there was relatively little call for politicians to address world hunger.\n\nIn the late nineteenth and early twentieth century, the view that politicians ought not to intervene against hunger was increasingly challenged by campaigning journalists, with some academics and politicians also calling for or organizing intervention against world hunger, such as U.S. President Woodrow Wilson.\n\nHunger as an academic and social topic came to prominence during the Great Depression. As many individuals struggled for food, the same agricultural industries were suddenly producing large surpluses as means of increased production to counter the drop in demand from the European markets. This increased output was meant to ease the growing debt levels, however domestic demand could not keep up with prices. Instead, what is often called \"the paradox of want amid plenty,\" agricultural surpluses and large demand simply did not fit together, causing the Hoover administration to buy large amounts of product, such as grain, to stabilize prices. Initially refusing to further compromise the distressed price levels, political pressure from starving families across the country forced Congress to reconsider. With large deposits of grain already wasting away in government possession, the only political move left was to begin a process of donations to the hungry from the Farm Board, a federal oversight created in 1929 to promote the sale and stabilization of agricultural products. Instead of hunger being a reason for the allocation of large grain surpluses, waste became the eventual driving force.\n\nAfter World War II, a new international politico-economic order came into being, which was later described as Embedded liberalism.\n\nFor at least the first decade after the war, the United States, by far the period's most dominant national actor, was strongly supportive of efforts to tackle world hunger and to promote international development. It heavily funded the United Nation's development programmes, and later the efforts of other multilateral organizations like the International Monetary Fund (IMF) and the World Bank (WB).\n\nThe newly established United Nations became a leading player in co-ordinating the global fight against hunger. The UN has three agencies that work to promote food security and agricultural development: the Food and Agriculture Organization (FAO), the World Food Programme (WFP) and the International Fund for Agricultural Development (IFAD). FAO is the world's agricultural knowledge agency, providing policy and technical assistance to developing countries to promote food security, nutrition and sustainable agricultural production, particularly in rural areas.\n\nWFP's key mission is to deliver food into the hands of the hungry poor. The agency steps in during emergencies and uses food to aid recovery after emergencies. Its longer term approaches to hunger helps the transition from recovery to development. IFAD, with its knowledge of rural poverty and exclusive focus on poor rural people, designs and implements programmes to help those people access the assets, services and opportunities they need to overcome poverty.\n\nFollowing successful post WWII reconstruction of Germany and Japan, the IMF and WB began to turn their attention to the developing world. A great many civil society actors were also active in trying to combat hunger, especially after the late 1970s when global media began to bring the plight of starving people in places like Ethiopia to wider attention. Most significant of all, especially in the late 1960s and 70s, the Green revolution helped improved agricultural technology propagate throughout the world.\n\nThe United States began to change its approach to the problem of world hunger from about the mid 1950s. Influential members of the administration became less enthusiastic about methods they saw as promoting an over reliance on the state, as they feared that might assist the spread of communism. Despite this view, during the 1960s postwar era hunger within the United States was overshadowed by hunger in Europe and Asia. John F. Kennedy did in fact use Executive Order to double the amount of commodities available from the surplus commodity program as well as initiated the pilot Food Stamp Program which later became permanent in 1964.\n\nBy the 1980s, the previous consensus in favour of moderate government intervention had been displaced across the western world. The IMF and World Bank in particular began to promote market-based solutions. In cases where countries became dependent on the IMF, they sometimes forced national governments to prioritize debt repayments and sharply cut public services. This sometimes had a negative effect on efforts to combat hunger.\n\nOrganizations such as Food First raised the issue of food sovereignty and claimed that every country on earth (with the possible minor exceptions of some city-states) has sufficient agricultural capacity to feed its own people, but that the \"free trade\" economic order, which from the late 1970s to about 2008 had been associated with such institutions as the IMF and World Bank, had prevented this from happening.\n\nThe World Bank itself claimed it was part of the solution to hunger, asserting that the best way for countries to break the cycle of poverty and hunger was to build export-led economies that provide the financial means to buy foodstuffs on the world market. However, in the early 21st century the World Bank and IMF became less dogmatic about promoting free market reforms. They increasingly returned to the view that government intervention does have a role to play, and that it can be advisable for governments to support food security with policies favourable to domestic agriculture, even for countries that do not have a Comparative advantage in that area. As of 2012, the World Bank remains active in helping governments to intervene against hunger.\n\nUntil at least the 1980s—and, to an extent, the 1990s—the dominant academic view concerning world hunger was that it was a problem of demand exceeding supply. Proposed solutions often focused on boosting food production, and sometimes on birth control. There were exceptions to this, even as early as the 1940s, Lord Boyd-Orr, the first head of the UN's FAO, had perceived hunger as largely a problem of distribution, and drew up comprehensive plans to correct this. Few agreed with him at the time, however, and he resigned after failing to secure support for his plans from the US and Great Britain. In 1998, Amartya Sen won a Nobel Prize in part for demonstrating that hunger in modern times is not typically the product of a lack of food. Rather, hunger usually arises from food distribution problems, or from governmental policies in the developed and developing world. It has since been broadly accepted that world hunger results from issues with the distribution as well as the production of food. Sen's 1981 essay \"Poverty and Famines: An Essay on Entitlement and Deprivation\" played a prominent part in forging the new consensus.\n\nIn 2007 and 2008, rapidly increasing food prices caused a \"global food crisis\", increasing the numbers suffering from hunger by over a hundred million. Food riots erupted in several dozen countries; in at least two cases, Haiti and Madagascar, this led to the toppling of governments. A second \"global food crisis\" unfolded due to the spike in food prices of late 2010 and early 2011. Fewer food riots occurred, due in part to greater availability of food stock piles for relief. However, several analysts argue the food crisis was one of the causes of the Arab Spring.\nAs of 2008 roughly $300 million of aid went to basic nutrition each year, less than $2 for each child below two in the 20 worst affected countries. In contrast, at that time HIV/AIDS, which caused fewer deaths than child malnutrition, received $2.2 billion—$67 per person with HIV in all countries. In 2008 UN estimated that ending world hunger could cost about 30 billion.\n\nThe International Crops Research Institute for the Semi-Arid Tropics (ICRISAT), a member of the CGIAR consortium, partners with farmers, governments, researchers and NGOs to help farmers grow nutritious crops, such as chickpea, groundnut, pigeonpea, millet and sorghum. This helps their communities have more balanced diets and become more resilient to pests and drought. The Harnessing Opportunities for Productivity Enhancement of Sorghum and Millets in Sub-Saharan Africa and the Indian-Subcontinent (HOPE) project, for example, is increasing yields of finger millet in Tanzania by encouraging farmers to grow improved varieties. Finger millet is very high in calcium, rich in iron and fiber, and has a better energy content than other cereals. These characteristics make it ideal for feeding to infants and the elderly.\n\nSome organizations have begun working with teachers, policymakers, and managed food service contractors to mandate improved nutritional content and increased nutritional resources in school cafeterias from primary to university-level institutions. Health and nutrition have been proven to have close links with overall educational success.\n\nIn the early 21st century, there was relatively little awareness of hunger from leaders of advanced nations such as those that form the G8. Prior to 2009, efforts to fight hunger were mainly undertaken by governments of the worst affected countries, by civil society actors, and by multilateral and regional organizations. In 2009, Pope Benedict published his third encyclical, Caritas in Veritate, which emphasised the importance of fighting against hunger. The encyclical was intentionally published immediately before the July 2009 G8 Summit to maximise its influence on that event. At the Summit, which took place at L'Aquila in central Italy, the \"L'Aquila Food Security Initiative\" was launched, with a total of US$22 billion committed to combat hunger.\n\nFood prices did fall sharply in 2009 and early 2010, though analysts credit this much more to farmers increasing production in response to the 2008 spike in prices, than to the fruits of enhanced government action. However, since the 2009 G8 summit, the fight against hunger has remained a high-profile issue among the leaders of the worlds major nations, and was a prominent part of the agenda for the 2012 G-20 summit. \n\nIn April 2012, the Food Assistance Convention was signed, the world's first legally binding international agreement on food aid. The May 2012 Copenhagen Consensus recommended that efforts to combat hunger and malnutrition should be the first priority for politicians and private sector philanthropists looking to maximize the effectiveness of aid spending. They put this ahead of other priorities, like the fight against malaria and AIDS. Also in May 2012, U.S. President Barack Obama launched a \"new alliance for food security and nutrition\"—a broad partnership between private sector, governmental and civil society actors—that aimed to \"...achieve sustained and inclusive agricultural growth and raise 50 million people out of poverty over the next 10 years.\" The UK's prime minister David Cameron held a hunger summit on 12 August, the last day of the 2012 Summer Olympics.\n\nThe fight against hunger has also been joined by an increased number of regular people. While folk throughout the world had long contributed to efforts to alleviate hunger in the developing world, there has recently been a rapid increase in the numbers involved in tackling domestic hunger even within the economically advanced nations of the Global North.\n\nThis had happened much earlier in North America than it did in Europe. In the US, the Reagan administration scaled back welfare the early 1980s, leading to a vast increase of charity sector efforts to help Americans unable to buy enough to eat. According to a 1992 survey of 1000 randomly selected US voters, 77% of Americans had contributed to efforts to feed the hungry, either by volunteering for various hunger relief agencies such as food banks and soup kitchens, or by donating cash or food. \nEurope, with its more generous welfare system, had little awareness of domestic hunger until the food price inflation that began in late 2006, and especially as austerity-imposed welfare cuts began to take effect in 2010. Various surveys reported that upwards of 10% of Europe's population had begun to suffer from food insecurity. Especially since 2011, there has been a substantial increase in grass roots efforts to help the hungry by means of food banks, within both the UK and continental Europe. \n\nBy July 2012, the 2012 US drought had already caused a rapid increase in the price of grain and soy, with a knock on effect on the price of meat. As well as affecting hungry people in the US, this caused prices to rise on the global markets; the US is the world's biggest exporter of food. This led to much talk of a possible third 21st century global food crisis. The \"Financial Times\" reported that the BRICS may not be as badly affected as they were in the earlier crises of 2008 and 2011. However, smaller developing countries that must import a substantial portion of their food could be hard hit. The UN and G20 has begun contingency planning so as to be ready to intervene if a third global crisis breaks out.\nBy August 2013 however, concerns had been allayed, with above average grain harvests expected from major exporters, including Brazil, Ukraine and the U.S. 2014 also saw a good worldwide harvest, leading to speculation that grain prices could soon begin to fall.\nIn an April 2013 summit held in Dublin concerning Hunger, Nutrition, Climate Justice, and the post 2015 MDG framwework for global justice, Ireland's President Higgins said that only 10% of deaths from hunger are due to armed conflict and natural disasters, with ongoing hunger being both the \"greatest ethical failure of the current global system\" and the \"greatest ethical challenge facing the global community.\"\n$4.15 billion of new commitments were made to tackle hunger at a June 2013 Hunger Summit held in London, hosted by the governments of Britain and Brazil, together with The Children's Investment Fund Foundation.\n\nUndernutrition is an important determinant of maternal and child health, accounting for more than a third of child deaths and more than 10 percent of the total global disease burden according to 2008 studies.\n\nThe World Health Organization estimates that malnutrition accounts for 54 percent of child mortality worldwide, about 1 million children. Another estimate also by WHO states that childhood underweight is the cause for about 35% of all deaths of children under the age of five years worldwide.\n\nAs underweight children are more vulnerable to almost all infectious diseases, the \"indirect\" disease burden of malnutrition is estimated to be an order of magnitude higher than the disease burden of the \"direct\" effects of malnutrition. The combination of direct and indirect deaths from malnutrition caused by unsafe water, sanitation and hygiene (WASH) practices is estimated to lead to 860,000 deaths per year in children under five years of age.\n\nOlder sources sometimes claim this phenomenon is unique to developing countries, due to greater sexual inequality. More recent findings suggested that mothers often miss meals in advanced economies too. For example, a 2012 study undertaken by Netmums in the UK found that one in five mothers sometimes misses out on food to save their children from hunger.\n\nIn several periods and regions, gender has also been an important factor determining whether or not victims of hunger would make suitable examples for generating enthusiasm for hunger relief efforts. James Vernon, in his \"Hunger: A Modern History\", wrote that in Britain before the 20th century, it was generally only women and children suffering from hunger who could arouse compassion. Men who failed to provide for themselves and their families were often regarded with contempt. This changed after World War I, where thousands of men who had proved their manliness in combat found themselves unable to secure employment. Similarly, female gender could be advantageous for those wishing to advocate for hunger relief, with Vernon writing that being a woman helped Emily Hobhouse draw the plight of hungry people to wider attention during the Second Boer War.\nResearchers from the Centre for World Food Studies in 2003 found that the gap between levels of undernutrition in men and women is generally small, but that the gap varies from region to region and from country to country. These small-scale studies showed that female undernutrition prevalence rates exceeded male undernutrition prevalence rates in South/Southeast Asia and Latin America and were lower in Sub-Saharan Africa. Datasets for Ethiopia and Zimbabwe reported undernutrition rates between 1.5 and 2 times higher in men than in women; however, in India and Pakistan, datasets rates of undernutrition were 1.5-2 times higher in women than in men. Intra-country variation also occurs, with frequent high gaps between regional undernutrition rates. Gender inequality in nutrition in some countries such as India is present in all stages of life.\n\nStudies on nutrition concerning gender bias within households look at patterns of food allocation, and one study from 2003 suggested that women often receive a lower share of food requirements than men. Gender discrimination, gender roles, and social norms affecting women can lead to early marriage and childbearing, close birth spacing, and undernutrition, all of which contribute to malnourished mothers.\n\nWithin the household, there may be differences in levels of malnutrition between men and women, and these differences have been shown to vary significantly from one region to another, with problem areas showing relative deprivation of women. Samples of 1000 women in India in 2008 demonstrated that malnutrition in women is associated with poverty, lack of development and awareness, and illiteracy. The same study showed that gender discrimination in households can prevent a woman's access to sufficient food and healthcare. How socialization affects the health of women in Bangladesh, Najma Rivzi explains in an article about a research program on this topic. In some cases, such as in parts of Kenya in 2006, rates of malnutrition in pregnant women were even higher than rates in children.\n\nWomen in some societies are traditionally given less food than men since men are perceived to have heavier workloads. Household chores and agricultural tasks can in fact be very arduous and require additional energy and nutrients; however, physical activity, which largely determines energy requirements, is difficult to estimate.\n\nWomen have unique nutritional requirements, and in some cases need more nutrients than men; for example, women need twice as much calcium as men.\n\nDuring pregnancy and breastfeeding, women must ingest enough nutrients for themselves and their child, so they need significantly more protein and calories during these periods, as well as more vitamins and minerals (especially iron, iodine, calcium, folic acid, and vitamins A, C, and K). In 2001 the FAO of the UN reported that iron deficiency afflicted 43 percent of women in developing countries and increased the risk of death during childbirth. A 2008 review of interventions estimated that universal supplementation with calcium, iron, and folic acid during pregnancy could prevent 105,000 maternal deaths (23.6 percent of all maternal deaths). Malnutrition has been found to affect three quarters of UK women aged 16-49 indicated by them having less folic acid than the WHO recommended levels.\n\nFrequent pregnancies with short intervals between them and long periods of breastfeeding add an additional nutritional burden.\n\nAccording to the FAO, women are often responsible for preparing food and have the chance to educate their children about beneficial food and health habits, giving mothers another chance to improve the nutrition of their children.\n\nMalnutrition and being underweight are more common in the elderly than in adults of other ages. If elderly people are healthy and active, the aging process alone does not usually cause malnutrition. However, changes in body composition, organ functions, adequate energy intake and ability to eat or access food are associated with aging, and may contribute to malnutrition. Sadness or depression can play a role, causing changes in appetite, digestion, energy level, weight, and well-being. A study on the relationship between malnutrition and other conditions in the elderly found that malnutrition in the elderly can result from gastrointestinal and endocrine system disorders, loss of taste and smell, decreased appetite and inadequate dietary intake. Poor dental health, ill-fitting dentures, or chewing and swallowing problems can make eating difficult. As a result of these factors, malnutrition is seen to develop more easily in the elderly.\n\nRates of malnutrition tend to increase with age with less than 10 percent of the \"young\" elderly (up to age 75) malnourished, while 30 to 65 percent of the elderly in home care, long-term care facilities, or acute hospitals are malnourished. Many elderly people require assistance in eating, which may contribute to malnutrition. However, the mortality rate due to undernourishment may be reduced. Because of this, one of the main requirements of elderly care is to provide an adequate diet and all essential nutrients. Providing the different nutrients such as protein and energy keeps even small but consistent weight gain.\n\nIn Australia malnutrition or risk of malnutrition occurs in 80 percent of elderly people presented to hospitals for admission. Malnutrition and weight loss can contribute to sarcopenia with loss of lean body mass and muscle function. Abdominal obesity or weight loss coupled with sarcopenia lead to immobility, skeletal disorders, insulin resistance, hypertension, atherosclerosis, and metabolic disorders. A paper from the \"Journal of the American Dietetic Association\" noted that routine nutrition screenings represent one way to detect and therefore decrease the prevalence of malnutrition in the elderly.\n\n\n"}
{"id": "23339740", "url": "https://en.wikipedia.org/wiki?curid=23339740", "title": "Mass drug administration", "text": "Mass drug administration\n\nThe administration of drugs to whole populations irrespective of disease status is referred to as mass drug administration (MDA).\n\nThis article describes the administration of antimalarial drugs to whole populations an intervention which has been used as a malaria-control measure for more than 70 years. Recent proposals to eliminate or even to eradicate malaria have led to a renewed interest in mass drug administrations in areas with very high malaria endemicity. Drugs have been administered either directly as a full therapeutic course of treatment or indirectly through the fortification of salt. Mass drug administrations were generally unsuccessful in interrupting transmission but, in some cases, had a marked effect on parasite prevalence and on the incidence of clinical malaria. MDAs are likely to encourage the spread of drug-resistant parasites and so have only a limited role in malaria control. They may have a part to play in the management of epidemics and in the control of malaria in areas with a very short transmission season. In order to reduce the risk of spreading drug resistance, MDAs should use more than one drug and, preferably include a drug, such as an artemisinin, which has an effect on gametocytes. MDAs have low acceptance in areas with low malaria endemicity.\n\nAnother example of mass drug administration is mass deworming of children to remove helminth infections (intestinal worms).\n\nReports of attempts to control malaria through mass treatment with antimalarial drugs date back to at least 1932. In the 1950s, the WHO included mass drug administration (MDA) of antimalarial drugs as a tool for malaria eradication ‘in exceptional conditions when conventional control techniques have failed. In 1971, the WHO expert committee on malaria still recommended MDA in special circumstances. Subsequently, MDA was linked to the emergence of drug resistance and its overall benefit was questioned. Concomitantly, the goal of malaria eradication was replaced by one of prevention of malaria morbidity and mortality through the provision of effective treatment. Considering the short lasing benefit of mass drug administration one modification has been to repeat mass drug administrations which has led to the development of intermittent preventive therapy.\n\nTwo methods of MDA, direct and indirect, have been used. In direct MDA, also referred to as ‘Mass drug treatment’, a therapeutic dose of the antimalarial drug, usually in the form of tablets, is given to an entire population. In indirect MDA, the antimalarial drug is added to food stuff, usually salt.\n\nThe first, well documented use of direct MDA took place in a rubber plantation in Liberia in 1931. Two doses of the 8-aminoquinoline plasmoquine were given weekly to workers and their families in two camps. The prevalences of malaria parasite infections in humans and anopheline mosquitoes before and after treatment were studied. The authors concluded that ‘the fall in the mosquito infection rate of the two plasmoquine treated camps was so large as to indicate a local disappearance, or at least a great reduction, in gametocyte carriers in the treated population’. No long-term follow up data were provided for this study or most of the trials reported subsequently. The next documented use of MDA in sub-Saharan Africa took place in 1948 and 1949 in tea estates in Kericho, Kenya. Ten thousand inhabitants of the tea estates received twice weekly proguanil from April to July 1948. The intervention was supplemented with DDT spraying in March and June of the following year. Before the intervention the mean malaria incidence in July, the peak of the malaria transmission season, was 56 cases per 1000 population. Following the intervention 4 malaria cases were detected in July 1949. The author therefore recommended continuation of twice weekly proguanil prophylaxis on the estates.\nThe Nandi district of Kenya was the scene of a large MDA in 1953 and 1954. The target population of 83,000 received a single dose of pyrimethamine at the beginning of the malaria season in 1953 and 1954. The coverage was estimated to be around 95%. Before the intervention severe malaria epidemics had been reported in the area. Following the intervention the parasite prevalence dropped from 23% to 2.3%. The author states that in a control area parasite prevalence rose over the same period to over 50%. It was felt that the MDA was effective in curbing severe malaria epidemics. In the following three years, 1955 to 1957, pyrimethamine administration was replaced with Dieldrin spraying to consolidate malaria control, which makes an assessment of the long-term effect of this MDA impossible.\n\nDuring a pilot programme in Uganda in 1959 mass administration of chloroquine / pyrimethamine was combined with spraying of residual insecticides (DDT). The success of the pilot programme led to a larger study targeted at a population of 16,000. Because of logistic problems, only half of the target population received the first round of MDA. According to the investigators, the intervention resulted in the eradication of the vector and rapid elimination of malaria from the area.\n\nTwo large trials of MDA combined with household spraying with DDT were conducted in Cameroun and Upper Volta (Burkina Faso) in 1960–1961. In both trials, substantial reductions in the prevalence of parasitaemia were achieved but transmission was not interrupted. In Bobo-Dioulasso, where primaquine was used in combination with either chloroquine or amodiaquine, the prevalence of gametocytes and Anopheles gambiae sporozoites were reduced substantially. A MDA was also combined with DDT spraying in Zanzibar (Dola 1974). After the MDA, the parasite prevalence in children decreased, but the overall parasite prevalence increased slightly, thus failing to deplete the reservoir of infection.\n\nTwo trials in Northern Nigeria combined multiple rounds of MDA and insecticide spraying. The first trial, in Kankiya, included 11 rounds of MDA combined with 8 rounds of DDT indoor spraying. The study was based on computer-aided models that showed that MDA could eradicate malaria in the study area if combined with an appropriate ‘insecticide attack’. Following MDAs, parasite prevalence dropped from 19% to 1%. The investigators did not consider this a success because parasite prevalence increased again after the interventions were stopped. Entomological indices also showed only a temporary reduction in transmission, which was completely reversed after the control measures ceased. Because the investigators felt that the failure of the trial to interrupt transmission was due to operational inadequacies, they recommended a much larger and more sophisticated evaluation of insecticide spraying combined with MDA. This recommendation helped to launch the Garki project, also in Northern Nigeria, in 1969. In the Garki project, all 164 study villages in the catchment area were sprayed with propoxur, a residual insecticide. In addition, in 60 villages, MDA with sulfalene / pyrimethamine was given at 10-week intervals for two years. In two small village clusters, house spraying was supplemented with larvicide and MDA every two weeks. With biweekly MDA, parasite prevalence fell to 1% in the dry season and to 5% in the rainy season. MDA given every 10 weeks resulted in a parasite prevalence of 2% in the dry season and 28% in the rainy season. Transmission was not interrupted with either MDA regime. The authors concluded that spraying of residual insecticides and MDA did not result in a sustainable interruption of malaria transmission.\n\nIn 1999 in The Gambia residents living in 33 of 42 villages in the catchment area received a single dose of sulfadoxine / pyrimethamine (SP) combined with artesunate while the residents of nine control villages received placebo. Following the MDA, 1388 children ≤10 years of age living in nine control villages and in nine matched villages which had been allocated active treatment were kept under surveillance for clinical malaria throughout the transmission season. Initially, during July and August, the mean malaria incidence rate in treated villages was significantly lower than in the control villages. In subsequent months, the incidence was slightly higher in the MDA villages. The difference between the two groups was not statistically significant. Overall no benefit of the mass drug administration was detected over the course of the malaria transmission season.\n\nA mass drug administration campaign using S/P, artesunate and primaquine was completed in Moshi district, Tanzania in 2008. The findings have yet to be published.\n\nOutside of sub-Saharan Africa one of the larger reported malaria-control projects using MDA took place in Nicaragua in 1981 following the overthrow of the Somoza regime. An estimated 70% of Nicaragua’s total population (1.9 million people) received chloroquine and primaquine during the peak period of disease transmission (November). An estimated 9200 cases of malaria were prevented. The campaign had better results in preventing and curing malaria infections than in interrupting transmission. However, the mass administration of antimalarials was not sustainable and, as with other malaria-control efforts, collapsed following the return of politically conservative forces.\n\nIn three malaria-control projects conducted in the Indian states of Andhra Pradesh, Uttar Pradesh, and Orissa in the early 1960s, MDA had an ancillary role and was mentioned only briefly in reports on these interventions. More detailed information is available following a focal outbreak in two villages in Gujarat State during 1978-1979. Here a mass administration of chloroquine was part of a programme of intensified surveillance, case management, health education, and residual spraying. The incidence of malaria decreased so that, by the end of 1979, the authors considered the intervention to be a success. In 1980, in areas of Andhra Pradesh State in India, residual spraying was combined with a MDA. During the period of lowest malaria incidence a single dose of chloroquine plus primaquine was distributed to the whole population in eight villages. A second dose was given after an interval of 2–3 months. This project failed to reduce malaria incidence and was considered to be a failure.\n\nIn 1984, MDA was added to the distribution of insecticide-impregnated bed nets (ITNs) in Sabah (Malaysia), but this failed to interrupt malaria transmission. A MDA in Sumatra, Indonesia in 1987 focused on schoolchildren. Eight months after the MDA, \"Plasmodium falciparum\" prevalence had decreased from 14% to 1%.\n\nThe only reported project with an MDA component which succeeded in permanently interrupting malaria transmission took place on the island of Aneityum, Vanuatu. Starting in September 1991, three malaria-control activities were employed – permethrin-impregnated bednets, larvivorous fish and the administration of three antimalarials. This MDA comprised 300 mg chloroquine base and 45 mg pyrimethamine weekly for nine weeks. An additional 300 mg chloroquine and 75 mg pyrimethamine plus 1500 mg sulfadoxine was added to this regimen in the first, fifth, and ninth week. Children received an adjusted equivalent of the adult dose. Follow-up consisted of yearly parasite surveillance. During the seven surveillance years following the MDA, no \"P.falciparum\" infections were detected.\n\nMDA is included in the malaria-control policy of the People’s Republic of China. Following the first malaria-control phase from 1955 to 1962, which was mostly focused on malaria surveys, mass administrations were added to vector control measures and improved case management in 10 of China’s 33 provinces. The drugs used in the administrations, mostly chloroquine and piperaquine, were provided free of charge by the central government. The economic reforms instituted by Deng Xiaoping, which ultimately put an end to the provision of free health care through the central government and the emergence of resistance against the most widely used antimalarials modified the use of mass drug administrations after 1980. MDAs are now targeted at high-risk populations, specifically non-immune migratory workers who receive repeated courses during the high transmission season. According to government guidelines, piperaquine, chloroquine, or sulfadoxine combined with primaquine can be used for mass administrations. The artemisinin derivatives are not used in mass drug administrations and are reserved for treatment failures. Malaria burden and control measures are shown in Table 1. Between 1990 and 2000 the malaria prevalence dropped from 10.6 to 1.9 / 100,000, the number of reported malaria cases dropped from 117,359 to 24,088 while the number of reported deaths attributable to malaria remained stable. These data, reported to the national government, depend on reporting from health care providers and like all data depending on passive surveillance tend to underestimate the true disease burden. However, there is no reason to think that the level of underreporting has changed over the last decade. Therefore, the proportional reduction in malaria disease burden is likely to be true. Malaria-control measures, including MDA, as well as major ecologic changes during the second half of the last century are likely to have been responsible for the more than 100-fold reduction in malaria burden in China since the initial surveys in 1955. The widespread use of antimalarials has been followed by the emergence of drug resistance especially in regions with high drug use. By 1995 more than 95% of \"P.falciparum\" strains isolated in the South of Yunnan province were found to be resistant to chloroquine, and piperaquine while in the remainder of Yunnan and Hainan province the resistance rates were 85%and 38% respectively.\n\nA different approach to MDA consists of adding an antimalarial to an essential foodstuff, usually salt. Chloroquinized salt for malaria suppression was introduced by Pinotti in 1952 and gave promising results in a number of field trials and malaria-control programmes in Brazil.\n\nIn 1959, the WHO conducted a trial in West New Guinea (later known as Irian Jaya). Salt crystals were mixed with pyrimethamine so as to provide a 0.07% pyrimethamine salt. As there were no shops in the catchment area, each family unit received fortnightly a quantity of salt from the local teacher or another member of the village community. Within three and a half months of the onset of the campaign, clinically significant levels of pyrimethamine resistance were reported. It was then decided to mix the remaining stock of pyrimethaminized salt with chloroquine powder. The chloroquine base content was 0.04% or 140 mg per adult per week based on a 5g per day salt consumption. The emergence of chloroquine resistance was investigated, but this was not detected. The distribution of medicated salts otherwise had no effect and it was concluded that \"‘Pinotti’s method holds no prospect of malaria eradication…’\". The explanation for this finding given by the author is that \"‘the salt consumption by children was too small to reduce significantly the parasite reservoir of the younger age groups’\".\n\nBetween 1961 and 1965, the use of chloroquinized salt was made compulsory over an area of 109,000km2 in Guyana, covering a population of 48,500 individuals. The chloroquinized salt was prepared at a state salt plant so as to provide a 0.43% chloroquine concentration. The salt was sold in two pound plastic bags. The state held the monopoly for the salt. The only alternative source was salt smuggled from Brazil. Although the chloroquinized salt was used, its popularity was limited by the occurrence of a photo-allergic dermatitis popularly called ‘salt itch’ noted in all treatment areas. Chloroquine resistance was first observed in 1962 in the area with the lowest relative uptake of chloroquinized salt. In the course of the following months, a complete replacement of the susceptible strains with resistant \"P. falciparum\" strains was observed. Following the reintroduction of DDT spraying, the prevalence of \"P. falciparum\" declined.\n\nIn Southeast Asia, the medicated salt project at Pailin, on the Kampuchea-Thai border, demonstrated how drug resistance can develop when a large population of \"P. falciparum\" undergoing high transmission rates is exposed to intense drug pressure. The project was launched in 1960 and covered a population of approximately 20,000. Sea salt was mixed with pyrimethamine at a concentration of 0.05%. Between 1960 and 1961, 77 tons of medicated salt were distributed in the area. After widespread pyrimethamine resistance was reported, pyrimethamine was replaced by chloroquine. From 1961 to 1962, 75 tons of chloroquine were distributed. In two indicator districts, the parasite rates decreased from 40% to 7% and from 27% to 14%. Chloroquine resistant \"P.falciparum\" isolates were first detected in Pailin in 1962 which appeared to be widespread by 1966. However no survey was undertaken to document the prevalence in the area. The factors leading to the emergence and spread of drug resistance appear to have been the continuous introduction of non-immune migrants, attracted by the promise of quick wealth from mining of precious stones, and prolonged drug pressure resulting from individual drug consumption and mass drug administration. Unrelated to MDAs the emergence of artemisinin resistant \"P.falciparum\" strains was reported in Pailin in 2008, this may have been related to overuse of artemisinin derivateves including counterfeit drugs but was not related to programmatic MDAs.\n\nFurther malaria-control projects have used MDA, but have never been published, or have been published as technical reports.\n\nWhether MDAs can be considered successful or not depends on the expectation of what they might achieve; many studies do not define whether their main aim was to interrupt transmission or to control disease. When MDAs were used as part of an attempt to interrupt transmission completely, they almost always failed. Only one project, conducted on Aneityum, a small isolated island in the Pacific, succeeded in permanently interrupting transmission using MDA as one of several malaria-control strategies. However, although unable to interrupt transmission, many MDA projects led to a marked reduction in parasite prevalence and probably had a marked also transient effect on malaria-related morbidity and mortality. Most of the early trials used study designs which would now be considered inadequate to provide a definitive answer on study outcome. For example, before-and-after comparisons were used frequently. Such comparisons are especially unreliable for vector-borne diseases which may show marked variations in incidence from season to season as well as from year to year. Furthermore, in several studies only a single intervention and control area or group were compared despite the fact a single control group cannot provide statistically interpretable results (see \"n\" = 1 fallacy).\n\nThe deficiencies in the study designs mentioned above reflect the evolution of research methodology over the last 50 years. The evaluation of an intervention such as MDA is complicated by the fact that the effect of the intervention on transmission can only be measured at the community and not at the individual level. Trial methods which use a community, a village, or a cluster as unit of inference have taken longer to evolve than those used for individually randomized trials. There are, with some notable exceptions, few properly designed and analysed cluster randomized trials conducted by health care researchers prior to 1978. One major handicap for researchers who need to use the cluster approach, besides the need for a large sample size, is the need to use statistical methods that differ from the familiar methods used in individually randomized trials. Significant progress has been made in the development of statistical methods for the analysis of correlated data.\n\nThe present unpopularity of MDA is not only due to doubts regarding the health benefit of this intervention but to the fear that MDAs will facilitate the spread of drug resistance. Concern that MDA would cause pyrimethamine and later chloroquine resistance was first raised in the early 1960s. Circumstantial evidence linked the use of medicated salts to the emergence of chloroquine resistance in the 1980s: Chloroquine resistance emerged first in three foci, namely South America (Colombia, Venezuela, Brazil), Southeast Asia (Thailand/Kampuchea), and Africa (Tanzania/Kenya). Payne has argued that the one common factor between these three epidemiologically diverse areas was widespread distribution of medicated salts prior to the emergence of chloroquine resistance.\n\nIn contrast to indirect MDA, emergence of drug resistance has not been linked to the administration of therapeutic doses of antimalarials through direct MDA programmes. The likely explanation lies in the different pharmacokinetic profiles that result from these two methods of drug administration. The administration of therapeutically dosed antimalarial drugs results in a single peak drug level which kills all susceptible strains. Only during the terminal half life of the drug when the concentration drops below the C, the inhibitory concentration which kills the large majority of a parasite population, will new infections with more resistant strains have a survival advantage. Thus drugs with a very short terminal half-life, including artemisinin derivatives, carry a lower risk of selecting resistant parasites than longer acting drugs. In contrast, the administration of medicated salts is likely to result in drug levels undulating in the sub-lethal range, which reach a steady state after several doses have been administered. The situation is worse if drugs such as chloroquine are used which accumulate progressively. This situation, a steady increase in drug concentration, is identical to the experimental design used for the in vitro induction of drug resistance. Medicated salt projects can be considered as large scale \"in vivo\" experiments designed to select resistant parasites.\n\nThe administration of antimalarials to large numbers of individuals with little or no preliminary screening could result in significant toxicity as nearly all antimalarials in common use can occasionally cause serious adverse events. For example, the widespread use of 8-aminoquinolines in areas where Glucose-6-phosphate dehydrogenase deficiency is common carries the risk of precipitating episodes of haemolysis. Few MDA projects have reported specifically on adverse events. No life-threatening outcomes have been reported as a result of an MDA but a rare serious adverse event such as a blood dyscrasia would probably not have been detected without active surveillance for adverse events which was not reported in any of the studies. There is a theoretical risk that administration of antimalarial drugs during the course of MDAs to women in the first trimester of pregnancy, some of whom may not know that they are pregnant, could lead to foetal abnormalities. The benefit of malaria control has to be weighed against potential problems. Hence MDA is likely to be only used in areas with very high malaria endemicity.\n\n"}
{"id": "1148672", "url": "https://en.wikipedia.org/wiki?curid=1148672", "title": "Micronutrient", "text": "Micronutrient\n\nMicronutrients are essential elements required by organisms in small quantities throughout life to orchestrate a range of physiological functions to maintain health. Micronutrient requirements differ between organisms; for example, humans and other animals require numerous vitamins and dietary minerals, whereas plants require specific minerals. For human nutrition, micronutrient requirements are in amounts generally less than 100 milligrams per day, whereas macronutrients are required in gram quantities daily. \n\nThe minerals for humans and other animals include 13 elements that originate from Earth's soil and are not synthesized by living organisms, such as calcium and iron. Micronutrient requirements for animals also include vitamins, which are organic compounds required in microgram or milligram amounts. Since plants are the primary origin of nutrients for humans and animals, some micronutrients may be in low levels and deficiencies can occur when dietary intake is insufficient, as occurs in malnutrition, implying the need for initiatives to deter inadequate micronutrient supply in plant foods.\n\nAt the 1990 World Summit for Children, the gathered nations identified deficiencies in two microminerals and one micronutrient – iodine, iron, and vitamin A – as being particularly common and posing public health risks in developing countries. The Summit set goals for elimination of these deficiencies. The Ottawa-based Micronutrient Initiative was formed in response to this challenge with the mission to undertake research and fund and implement micronutrient programming.\n\nAs programming around these micronutrients grew, new research in the 1990s led to the implementation of folate and zinc supplementation programmes as well.\n\nPriority programs include supplementation with vitamin A for children 6–59 months, zinc supplementation as a treatment for diarrhoeal disease, iron and folate supplementation for women of child-bearing age, salt iodization, staple food fortification, multiple micronutrient powders, biofortification of crops and behaviour-centred nutrition education.\n\nSalt iodization is the recommended strategy for ensuring adequate human iodine intake. To iodize salt, potassium iodate is added to salt after it is refined and dried and before it is packed. Although large-scale iodization is most efficient, given the proliferation of small scale salt producers in developing countries, technology for small-scale iodization has also been developed. International organizations work with national governments to identify and support small salt producers in adopting iodization activity.\n\nIn 1990, less than 20 percent of households in developing countries were consuming iodized salt. By 1994, international partnerships had formed in a global campaign for Universal Salt Iodization. By 2008, it was estimated that 72 per cent of households in developing countries were consuming iodized salt and the number of countries in which iodine deficiency disorders were a public health concern reduced by more than half from 110 to 47 countries.\n\nIn 1997, national vitamin A supplementation programming received a boost when experts met to discuss rapid scale-up of supplementation activity and the Micronutrient Initiative, with support from the Government of Canada, began to ensure vitamin A supply to UNICEF.\n\nIn areas with vitamin A deficiency, it is recommended that children aged 6–59 months receive two doses annually. In many countries, vitamin A supplementation is combined with immunization and campaign-style health events.\n\nGlobal vitamin A supplementation efforts have targeted 103 priority countries. In 1999, 16 per cent of children in these countries received two annual doses of vitamin A. By 2007, the rate increased to 62 per cent.\n\nThe Micronutrient Initiative, with funding from the Government of Canada, supplies 75 per cent of the vitamin A required for supplementation in developing countries.\n\nDouble-fortified salt (DFS) is a public health tool for delivering nutritional iron. DFS is fortified with both iodine and iron. It was developed by Venkatesh Mannar, Executive Director of the Micronutrient Initiative and University of Toronto Professor Levente Diosady, who discovered a process for coating iron particles with a vegetable fat to prevent the negative interaction of iodine and iron.\n\nIn India, Tata Salt Plus, priced at an economical rate of Rs 20 per kg, is an iodine plus iron fortified salt, developed by the National Institute of Nutrition, Hyderabad through double fortification technology. This technology was offered to Tata Chemicals under a long-term MoU after due studies on bio-availability across the population strata conducted and published by NIN.\n\nIt was first used in public programming in 2004. In September 2010 DFS was produced in the Indian State of Tamil Nadu and distributed through a state school feeding program. DFS has also been used to combat Iron Deficiency Anemia (IDA) in the Indian state of Bihar. In September 2010, Venkatesh Mannar was named a Laureat of the California-based Tech Awards for his work in developing Double-Fortified Salt.\n\nThe returns of applying micronutrient-enriched fertilizers could be huge for human health, social and economic development. Research has shown that enriching fertilizers with micronutrients had not only an impact on plant deficiencies but also on humans and animals, through the food chain. A 1994 report by the World Bank estimated that micronutrient malnutrition costs developing economies at least 5 percent of gross domestic product. The Asian Development Bank has summarized the benefits of eliminating micronutrient deficiencies as follows:\n\nAlong with a growing understanding of the extent and impact of micronutrient malnutrition, several interventions have demonstrated the feasibility and benefits of correction and prevention. Distributing inexpensive capsules, diversifying to include more micronutrient-rich foods, or fortifying commonly consumed foods can make an enormous difference. Correcting iodine, vitamin A, and iron deficiencies can improve the population-wide intelligence quotient by 10-15 points, reduce maternal deaths by one fourth, decrease infant and child mortality by 40 percent, and increase people’s work capacity by almost half. The elimination of these deficiencies will reduce health care and education costs, improve work capacity and productivity, and accelerate equitable economic\ngrowth and national development. Improved nutrition is essential to sustained economic growth. Micronutrient deficiency elimination is as cost effective as the best public health interventions and fortification is the most cost-effective strategy.\n\nExperiments show that soil and foliar application of zinc fertilizer can effectively reduce the phytate zinc ratio in grain. People who eat bread prepared from zinc enriched wheat show a significant increase in serum zinc, suggesting that the zinc fertilizer strategy is a promising approach to address zinc deficiencies in humans.\n\nWhere zinc deficiency is a limiting factor, zinc fertilization can increase crop yields. Balanced crop nutrition supplying all essential nutrients, including zinc, is a cost effective management strategy. Even with zinc-efficient varieties, zinc fertilizers are needed when the available zinc in the topsoil becomes depleted.\n\nThere are about seven nutrients essential to plant growth and health that are only needed in very small quantities. Though these are present in only small quantities, they are all necessary:\n\n\nMicronutrient deficiencies are widespread. 51% of world cereal soils are deficient in zinc and 30% of cultivated soils globally are deficient in iron. Steady growth of crop yields during recent decades (in particular through the Green Revolution) compounded the problem by progressively depleting soil micronutrient pools.\n\nIn general, farmers only apply micronutrients when crops show deficiency symptoms, while micronutrient deficiencies decrease yields before symptoms appear. Some common farming practices (such as liming acid soils) contribute to widespread occurrence of micronutrient deficiencies in crops by decreasing the availability of the micronutrients present in the soil. \n\nBiofortification of crop plants – improvement of vitamin and mineral levels through plant biotechnology – is being used in many world regions to address micronutrient deficiencies in regions of poverty and malnutrition.\n\n\n"}
{"id": "1664254", "url": "https://en.wikipedia.org/wiki?curid=1664254", "title": "Most livable cities", "text": "Most livable cities\n\nThe world's most livable cities is an informal name given to any list of cities as they rank on an annual survey of living conditions. Regions with cities commonly ranked in the top 50 include the United States, Canada, Western Europe, Australia, and New Zealand. Three examples of such surveys are Monocle's \"Most Liveable Cities Index\", the Economist Intelligence Unit's \"Global Liveability Ranking\", and \"Mercer Quality of Living Survey\". Numbeo has the largest statistics and survey data based on cities and countries. Livability rankings may be used by employers assigning hardship allowances as part of job relocation.\n\nThe Economist Intelligence Unit's (EIU) publishes an annual Global Liveability Ranking, which ranks 140 cities for their urban quality of life based on assessments of their stability, healthcare, culture and environment, education and infrastructure.\n\nMelbourne, Australia, had been ranked by the EIU as the world's most livable city for seven years in a row, from 2011 to 2017. Between 2004 and 2010, Vancouver, Canada, was ranked the EIU's most livable city, with Melbourne sharing first place in the inaugural 2002 report. Vancouver has ranked third since 2015, while Vienna, Austria, ranked second until 2018 when it ranked first.\n\nThe Syrian capital, Damascus, was ranked the least livable city of the 140 assessed in 2016.\n\nThe EIU also publishes a Worldwide Cost of Living Survey that compares the cost of living in a range of global cities.\n\nAmerican global human resources and related financial services consulting firm Mercer annually releases its Mercer Quality of Living Survey, comparing 221 cities based on 39 criteria. New York City is given a baseline score of 100 and other cities are rated in comparison. Important criteria are safety, education, hygiene, health care, culture, environment, recreation, political-economic stability, public transport and access to goods and services. The list is intended to help multinational companies decide where to open offices or plants, and how much to pay employees. For nine consecutive years (2009–2017), Mercer ranked Austria's capital Vienna first in its annual \"Quality of Living\" survey, a title the city still held in 2016.\nSince 2006, the lifestyle magazine \"Monocle\" has published an annual list of livable cities. The list in 2008 was named \"The Most Liveable Cities Index\" and presented 25 top locations for quality of life.\n\nImportant criteria in this survey are safety/crime, international connectivity, climate/sunshine, quality of architecture, public transport, tolerance, environmental issues and access to nature, urban design, business conditions, pro-active policy developments and medical care.\n\nThe 2018 Monocle Survey determined the world's most livable city was Munich, followed by Tokyo, Vienna and Zurich. A total of four German cities were on the list of the 25 most livable cities, 15 out of the 25 were European, 3 each from Japan and Australia, and one from North America (Vancouver). No cities from South America, South Asia, or Africa made it into the list.\n\n"}
{"id": "51173373", "url": "https://en.wikipedia.org/wiki?curid=51173373", "title": "National Farm Safety &amp; Health Week", "text": "National Farm Safety &amp; Health Week\n\nNational Farm Safety & Health Week is a week of commemoration, recognized annually on the third week of September in the United States.\n\nIn 1944, President Franklin D. Roosevelt signed the first proclamation for farm safety due to the high injury rate in agriculture. Agriculture is one of the most hazardous industries with a death rate of 23.2 deaths per 100,000 workers annually according to U.S. Bureau of Labor Statistics in 2013. However, many injuries are preventable through education. Serious injuries and death can be prevented by cautiously approaching field adjustments or repairs, taking precautions to avoid slips and falls, making smart decisions while assigning tasks to youth, using and maintaining the slow moving vehicle emblem correctly, and retrofitting tractors with rollover structures.\n\nIn September 2015, President Obama gave an official proclamation to accentuate the importance of agriculture for our society and economy as well as affirm farm safety and health programs \n\"America's farmers and ranchers have played a critical role in shaping our progress and forging a better future for coming generations. Through centuries of hard work, they have supplied our Nation with products and services essential to the economic and physical well-being of our society.\n\"Across our country, those who work on farms bolster our economy and nourish our people by providing what we need at a most human level, helping to uphold America's founding creed: Out of many, we are one. This week, let us recognize the steadfast dedication and commitment of agricultural producers and their families, and let us reaffirm our resolve to promote their health and safety.\"\n\nOver the years, the development and dissemination of National Farm Safety & Health Week materials shifted from the National Safety Council to National Education Center for Ag Safety (NECAS). NECAS is the agricultural partner for the National Safety Council and has been serving the agricultural family and business community since 1997. Each year they provide farmers with programs and materials to promote farmer safety and health.\n\nPresident Donald Trump proclaimed \"National Farm Safety & Health Week\" on September 15, 2017.\n"}
{"id": "33973815", "url": "https://en.wikipedia.org/wiki?curid=33973815", "title": "Online doctor", "text": "Online doctor\n\nOnline doctor is a term that emerged during the 2000s, used by both the media and academics, to describe a generation of physicians and health practitioners who deliver healthcare, including drug prescription, over the internet.\n\nIn the 2000s, many people came to treat the internet as a first, or at least a major, source of information and communication. Health advice is now the second-most popular topic, after pornography, that people search for on the internet. With the advent of broadband and videoconferencing, many individuals have turned to online doctors to receive online consultations and purchase prescription drugs. Use of this technology has many advantages for both the doctor and the patient, including cost savings, convenience, accessibility, and improved privacy and communication.\n\nIn the US, a 2006 study found that searching for information on prescription or over-the-counter drugs was the fifth most popular search topic, and a 2004 study found that 4% of Americans had purchased prescription medications online. A 2009 survey conducted by Geneva-based Health On the Net Foundation found one-in-ten Europeans buys medicines from websites and one-third claim to use online consultation. In Germany, approximately seven million people buy from mail-order pharmacies, and mail-order sales account for approximately 8–10% of total pharmaceutical sales. In 2008, the Royal Pharmaceutical Society of Great Britain reported that approximately two million people in Great Britain were regularly purchasing pharmaceuticals online (both with a prescription from registered online UK doctors and without prescriptions from other websites). A recent survey commissioned by Pfizer, the Medicines and Healthcare products Regulatory Agency, RPSGB, the Patients Association and HEART UK found that 15% of the British adults asked had bought a prescription-only medicine online.\n\nIn developed countries, many online doctors prescribe so-called ‘lifestyle drugs’, such as for weight loss, hair loss or erectile dysfunction. The RPSGB has identified the most popular products prescribed online as Prozac (an antidepressant), Viagra (for erectile dysfunction), Valium (a tranquiliser), Ritalin (a psychostimulant), Serostim (a synthetic growth hormone) and Provigil (a psychostimulant). A study in the USA has also shown that antibiotics are commonly available online without prescription.\n\nTraditionalist critics of online doctors argue that an online doctor cannot provide proper examinations or diagnosis either by email or video call. Such consultations, they argue, will always be dangerous, with the potential for serious disease to be missed. There are also concerns that the absence of proximity leads to treatment by unqualified doctors or patients using false information to secure dangerous drugs.\n\nProponents argue there is little difference between an e-mail consultation and the sort of telephone assessment and advice that doctors regularly make out of hours or in circumstances where doctors cannot physically examine a patient (e.g., jungle medicine).\n\nLaurence Buckman, chairman of the British Medical Association’s GPs’ committee, says that online consultations make life easier for doctors and patients when used properly. \"Many GPs will be very happy with it and it could be useful. When it’s a regular patient you know well, it follows on from telephone consulting. Voice is essential, vision is desirable. The problem comes when I don’t know the patient\".\nNiall Dickson, chief executive of the General Medical Council, says: \"We trust doctors to use their judgement to decide whether they should see a patient in person. Online consultations will be appropriate for some patients, whereas other patients will need a physical examination or may benefit from seeing their doctor in person\".\n\nThe first medical consulting website in the US was WebMD, founded in 1996 by Jim Clark (one of the founders of Netscape) and Pavan Nigam as Healthscape. Currently, its website carries information regarding health and health care, including a symptom checklist, pharmacy information, drug information, blogs of physicians with specific topics, and a place to store personal medical information. As of February 2011, WebMD’s network of sites reaches an average of 86.4 million visitors per month and is the leading health portal in the United States.\n\nOther popular US healthcare and medical consulting sites include NIH.gov, MSN Health, Doctorspring, MdLive, Justdoc, Yahoo! Health, EverydayHealth, WomensHealth.gov, MayoClinic, Doctor Vista, and many have experienced dramatic growth. (Healthline, launched in 2005, grew by 269% to 2.7 million average monthly unique visitors in Q1 2007 from 0.8 million average monthly unique visitors in Q1 2006). Niche consulting sites are also popular including SeniorNet, which deals with age-related syndromes and 4collegewomen.org and GirlsHealth.gov, which target young women. Several American online doctor companies, including Sherpaa, MDlive, Teladoc, First Stop Health, American Well, WebDoc247, MeMD, and Ringadoc, provide consultations with doctors over the phone or the Internet. Prominent San Francisco-based venture capital firm Founders Fund called such services \"extraordinarily fast\" and predicted that they will \"bring relief to thousands of people with immediate medical needs\".\n\nIn the UK, e-med was the first online health site to offer both a diagnosis and prescriptions to patients over the Internet. It was established in March 2000 by Dr. Julian Eden, In 2010, DrThom claimed to have 100,000 patients visit their site. NHS Direct (currently NHS Choices) is the free health advice and information service provided by the National Health Service (NHS) for residents and visitors in the UK, with advice offered 24 hours a day via telephone and web contact. Over 1.5 million patients visit the website every month. More recently, a number of online doctors have emerged in the country, firms such as Now Healthcare Group, Dr Fox Pharmacy, Push Doctor and Lloyds Pharmacy offer consultation and prescriptions via the Internet.\n\nIn Australia HealthDirect is the free health advice and information service provided by the government with advice offered 24 hours a day via telephone. Medicare began funding online consultations for specialists on 1 July 2011 which has seen a slow but steady increase in volumes.\n\nNew advances in digital information technology mean that in future online doctors and healthcare websites may offer advanced scanning and diagnostic services over the internet. The Nuffield Council on Bioethics identifies such services as direct-to-consumer body imaging (such as CT and MRI scans) and personal genetic profiling for individual susceptibility to disease. Professor Sir Bruce Keogh, the medical director of the UK NHS, is drawing up plans to introduce online consultations via Skype and has said IT will \"completely change the way [doctors] deliver medicine\". This concept is gaining more importance and there are some companies who started cashing on it, few companies like Zocdoc or Ziffi have started providing online doctor booking service.\n\n\n"}
{"id": "7981926", "url": "https://en.wikipedia.org/wiki?curid=7981926", "title": "Orchialgia", "text": "Orchialgia\n\nOrchialgia is long-term pain of the testes. It is considered chronic if it has persisted for more than 3 months. Orchialgia may be caused by injury, infection, surgery, cancer or testicular torsion and is a possible complication after vasectomy. IgG4-related disease is a more recently identified cause of chronic orchialgia.\n\nOne author describes the syndromes of chronic testicular pain thus:\"The complaint is of a squeezing deep ache in the testis like the day after you got kicked there, often bilateral or alternating from one side to the other, intermittent, and, most commonly, associated with lower back pain. Sometimes it feels like the testicle is pinched in the crotch of the underwear but trouser readjustment does not help. There may also be pain in the inguinal area but no nausea or other symptoms. Back pain may be concurrent or absent and some patients have a long history of low back pain. Onset of pain is commonly related to activity that would stress the low back such as lifting heavy objects. Other stresses that might cause low back pain are imaginative coital positions, jogging, sitting hunched over a computer, long car driving, or other such positions of unsupported seating posture that flattens the normal lumbar lordosis curve.\"\n\nTesting for gonorrhea and chlamydia should be routinely performed.\n\nTreatment is often with NSAIDs and antibiotics however, this is not always effective.\n\n"}
{"id": "25982869", "url": "https://en.wikipedia.org/wiki?curid=25982869", "title": "Oxford Brookes Centre for Nutrition and Health", "text": "Oxford Brookes Centre for Nutrition and Health\n\nThe Oxford Brookes Centre for Nutrition and Health is the first research centre in the United Kingdom dedicated to researching functional foods.\n\nThe Oxford Brookes Centre for Nutrition and Health opened as the Functional Food Centre at Oxford Brookes University in early 2009 with a £300,000 grant from the Higher Education Funding Council for England. It was formerly known as the Nutrition and Food Research Group, which had been in existence since 1984. Its founding director, Jeya Henry, is a consultant to the World Health Organization, UNICEF and the Food and Agriculture Organisation of the United Nations on nutrition assessment, food safety and nutrient requirements. The centre offers research and consultancy services to the food industry, the United Nations and various Government agencies.\n\nThe Oxford Brookes Centre for Nutrition and Health has the goal of providing scientific information about food and health to consumers, government and the food industry. It tests popular claims about food, for example that genetically modified crops will feed the world, that substances such as omega-3 in fish oil will make children more intelligent, or that antioxidants can reduce cancer by removing free radicals. It develops new food products such as low glycemic index bread, which reduces cholesterol and blood sugar levels and help weight loss. It researches lesser-known foods such as breadfruit, cassava, sorghum and millet. The health issues that are its research priorities are diabetes, obesity and the impact of food on age-related problems.\n\n\n"}
{"id": "40738710", "url": "https://en.wikipedia.org/wiki?curid=40738710", "title": "Patient Reported Outcome Indices for Multiple Sclerosis", "text": "Patient Reported Outcome Indices for Multiple Sclerosis\n\nThe Patient Reported Outcome Indices for Multiple Sclerosis (PRIMUS) is a disease specific patient-reported outcome questionnaire which measures the quality of life (QoL) of patients suffering from multiple sclerosis.\n\nThe measure contains an assessment of quality of life, activity limitations and symptoms. A higher score on any or all of these scales indicates a lower quality of life due to the disease.\n\nFirst published in 2009 by Galen Research and funded by Novartis Pharmaceuticals, the PRIMUS was developed in order to provide a more holistic view of the impact of MS on a patient.\n\nThe measure has three scales: quality of life, symptoms and activity limitations, which are designed to be used together or as standalone measures. The QoL and symptom scales are based on simple statements with dichotomous response options. Each scale has a total score which ranges from 0 to 22. The activity scale is based on 15 statements describing tasks. Patients are asked to rate their ability to perform these tasks on a scale from 1 to 3. The total score of the activities section ranges from 0 to 30.\n\nSince the development of the PRIMUS, it has been translated into several languages including Canadian English and French, French, German, Italian, Spanish, Swedish, US English, Australian and New Zealand English and US Spanish. This has allowed researchers to conduct studies for specific populations, such as Spain and Europe.\n\nThe PRIMUS has been utilized in clinical trials which assess the efficacy of a treatment or medication. If a patient’s score on the PRIMUS changes after a trial has taken place, it is inferred that the trial has had an effect on the patient’s quality of life. PRIMUS has been used to assess the efficacy of fingolimod and rivastigmine.\n"}
{"id": "40749890", "url": "https://en.wikipedia.org/wiki?curid=40749890", "title": "Psoriatic Arthritis Quality of Life", "text": "Psoriatic Arthritis Quality of Life\n\nThe Psoriatic Arthritis Quality of Life (PsAQoL) measure is a disease specific patient-reported outcome measure which measures the effect that psoriatic arthritis has on a patient’s quality of life.\n\nIt is a self-administered, 20-item questionnaire that takes about three minutes to complete. The answers are restricted to true or false.\n\nThe PsAQoL was first published in 2004 by Galen Research. The development of the measure was a joint effort between Galen Research, the University of Leeds and St. Vincent’s University Hospital in Dublin. The study was funded by the Arthritis Research Campaign in the UK.\n\nQualitative interviews were conducted with 48 patients, after which a 51-item questionnaire was created. A follow up survey with 94 patients was then conducted which reduced the items to 35. An additional 286 patients were surveyed and Rasch analysis was performed, which finalized the PsAQoL to 20 items. A high score on the PsAQoL indicates a lower quality of life.\n\nThe Psoriatic Arthritis Quality of Life measure has been translated into 30 languages, other than UK English. These languages include Dutch and Swedish.\nThe PsAQoL has also been utilized in clinical research studies in order to determine whether a medication or treatment is effective in treating psoriatic arthritis. If the scores on the PsAQoL change after treatment, this means that the given treatment has had an effect on quality of life. The PsAQoL has been used to evaluate infliximab and adalimumab.\n"}
{"id": "5953219", "url": "https://en.wikipedia.org/wiki?curid=5953219", "title": "Refeeding syndrome", "text": "Refeeding syndrome\n\nRefeeding syndrome is a syndrome consisting of metabolic disturbances that occur as a result of reinstitution of nutrition to patients who are starved, severely malnourished or metabolically stressed due to severe illness. When too much food and/or liquid nutrition supplement is consumed during the initial four to seven days of refeeding, this triggers synthesis of glycogen, fat and protein in cells, to the detriment of serum (blood) concentrations of potassium, magnesium, and phosphorus. Cardiac, pulmonary and neurological symptoms can be signs of refeeding syndrome. The low serum minerals, if severe enough, can be fatal.\n\nAny individual who has had negligible nutrient intake for many consecutive days and/or is metabolically stressed from a critical illness or major surgery is at risk of refeeding syndrome. Refeeding syndrome usually occurs within four days of starting to re-feed. Patients can develop fluid and electrolyte disorders, especially hypophosphatemia, along with neurologic, pulmonary, cardiac, neuromuscular, and hematologic complications.\n\nDuring fasting the body switches its main fuel source from carbohydrates to fat tissue fatty acids and amino acids as the main energy sources. The spleen decreases its rate of red blood cell breakdown thus conserving red blood cells. Many intracellular minerals become severely depleted during this period, although serum levels remain normal. Importantly, insulin secretion is suppressed in this fasted state and glucagon secretion is increased.\n\nDuring refeeding, insulin secretion resumes in response to increased blood sugar, resulting in increased glycogen, fat and protein synthesis. This process requires phosphates, magnesium and potassium which are already depleted and the stores rapidly become used up. Formation of phosphorylated carbohydrate compounds in the liver and skeletal muscle depletes intracellular ATP and 2,3-diphosphoglycerate in red blood cells, leading to cellular dysfunction and inadequate oxygen delivery to the body's organs. Refeeding increases the basal metabolic rate. Intracellular movement of electrolytes occurs along with a fall in the serum electrolytes, including phosphorus and magnesium. Levels of serum glucose may rise and the B vitamin thiamine may fall. Abnormal heart rhythms are the most common cause of death from refeeding syndrome, with other significant risks including confusion, coma and convulsions and cardiac failure.\n\nThis syndrome can occur at the beginning of treatment for anorexia nervosa when patients have an increase in calorie intake and can be fatal. It can also occur after the onset of a severe illness or major surgery. The shifting of electrolytes and fluid balance increases cardiac workload and heart rate. This can lead to acute heart failure. Oxygen consumption is also increased which strains the respiratory system and can make weaning from ventilation more difficult.\n\nRefeeding syndrome can be fatal if not recognized and treated properly. An awareness of the condition and a high index of suspicion are required in order to make the diagnosis. The electrolyte disturbances of the refeeding syndrome can occur within the first few days of refeeding. Close monitoring of blood biochemistry is therefore necessary in the early refeeding period. \n\nIn critically ill patients admitted to an intensive care unit, if phosphate drops to below 0.65 mmol/L (2.0 mg/dL) from a previously normal level within three days of starting enteral or parenteral nutrition, caloric intake should be reduced to 480 kcals per day for at least two days whilst electrolytes are replaced. Prescribing thiamine, vitamin B complex (strong) and a multivitamin and mineral preparation is recommended. Biochemistry should be monitored regularly until it is stable. Although clinical trials are lacking in patients other than those admitted to an intensive care, it is commonly recommended that energy intake should remain lower than that normally required for the first 3–5 days of treatment of refeeding syndrome.\n\nSee NICE Clinical guideline CG32, section 6.6. On first aid and preliminary medical management, see for example the guidance by HMS Monmouth medical officer.\nA common error, repeated in multiple papers, is that \"The syndrome was first \ndescribed after World War II in Americans who, held by the Japanese as prisoners of war, had become malnourished during captivity and who were then released to the care of United States personnel in the Philippines.\" However, closer inspection of the 1951 paper by Schnitker reveals the prisoners under study were not American POWs but Japanese soldiers who, already malnourished, surrendered in the Philippines during 1945, after the war was over. \n\nIt is difficult to ascertain when the syndrome was first discovered and named, but it is likely the associated electrolyte disturbances were identified well before 1951, perhaps in Holland during the closing months of World War II, before Victory Day in Europe. There are also anecdotal eyewitness reports from a still earlier time of Polish prisoners in Iran who were freed from Soviet camps in 1941–1942 under an amnesty to form an army under General Anders and were given food whilst in a state of starvation which caused many to die.\n\nThe Roman Historian Flavius Josephus writing in first century the described classic symptoms of the syndrome among survivors of the siege of Jerusalem. He described the death of those who over indulged in food after famine, whereas those who ate a more restrained pace survived.\n\nIn his 5th century BCE work 'On Fleshes' (De Carnibus), Hippocrates writes, \"if a person goes\nseven days without eating or drinking anything, in this period most die; but there are some who survive that time\nbut still die, and others are persuaded not to starve themselves to death but to eat and drink: however, the cavity\nno longer admits anything because the jejunum (nêstis) has grown together in that many days, and these people\ntoo die.\" Though Hippocrates misidentifies the exact cause of death, this passage likely represents an early description of Refeeding Syndrome.\n\n"}
{"id": "2778149", "url": "https://en.wikipedia.org/wiki?curid=2778149", "title": "Seattle 500 Study", "text": "Seattle 500 Study\n\nThe Seattle 500 Study is a University of Washington study that tracks individuals from birth. It is a longitudinal prospective study of the effects of prenatal health habits on human development. Beginning in 1974, this study has continuously followed a birth cohort of approximately 500 offspring. Current data collection is aimed at studying the development of mental health problems and problems of alcohol/drug abuse and dependence and their pre and post-natal antecedents.\n\nThe data which Seattle 500 collects is the basis of other research.\n"}
{"id": "44996080", "url": "https://en.wikipedia.org/wiki?curid=44996080", "title": "Social quality", "text": "Social quality\n\nSocial quality is a way of understanding society which is also relevant for social and public policy. Social quality looks at elements that should constitute a good or decent society. It contributes to the body of work concerned with understanding social progress going beyond GDP, taking into account the work of the Organisation for Economic Co-operation and Development and the European Foundation for the Improvement of Living and Working Conditions. Whilst most approaches have concentrated on the economics or psychology of well-being, social quality can help understanding the social conditions that enable human flourishing.\n\nThere are four elements shown in the diagram on the right. \"Economic security\" refers to the material conditions that enable people to have a long term perspective on their lives, and their confidence for the future. This can include social policies, such as pensions and social security, that ensure this. It also includes economic conditions that enable people to feel secure because they have enough income to cover their basic needs. \"Social cohesion\" refers to the bonds that link societies together, deriving from the work of Émile Durkheim. Émile Durkheim's work considers the moral and social relationships between members of society and social groups, including factors such as trust in others, trust in social institutions, a sense of identity, a collective consciousness, solidarity and commitment to the common good of society. \"Social inclusion\" refers to the way in which individuals and social groups are included into society, either through employment, networks and family or through access to social support of various kinds, this can come through support from other people or from social policies. Social policies that foster social inclusion, reduce poverty and tackle other forms of social exclusion can be important in this respect. \"Social empowerment\" refers to the conditions that enable people to participate in their society, such as having good health, education and democratic opportunities for participation, such as voting and joining civil society. However, it can also mean the feelings of being empowered, being able to take control of one's circumstances. It derives from the work of Amartya Sen, who suggests that people need to have the capability to be able to do the things that they wish to do.\n\nHowever, the social quality approach is fundamentally relational. As such, it emphasises a complex field of conditional (i), constitutional (ii) and normative factors (iii). These are (i) Socio-Economic Security, Social Cohesion, Social Inclusion, Social Empowerment; (ii) Person/human security, Social Recognition, Social Responsiveness, Personal/human Capacity; and (iii) Social Justice (Equity), Solidarity, Equal Valuation, Human Dignity. On the one hand this rejects the Durkheimian view on society as an independent social fact sui generis; on the other hand it qualifies Sen's approach as social quality is not primarily focusing on the subjective side and individuals within society but on the constitution of society by way of people producing and reproducing their daily life and with this society.\n\nThe idea of social quality began as a European Union project among a group of social scientists, concerned to develop a better way forward for social policies in Europe. Later the concept underwent some fundamental development. Empirically it had been developed amongst others by Pamela Abbott, Claire Wallace and Roger Sapsford, who looked at how the model can be developed empirically in Europe, Eastern Europe, Central Asia, China and Rwanda. They have used a different variation, considering the impact on well being, measured through life satisfaction and human flourishing in order to understand how social quality might work in fostering the well being of people in society. \nThis empirical work had been to a large extent emerging from the work undertaken by colleagues in Asia, in particular by Ka Lin.\nAnother development is by a marked shift towards questions around 'overall sustainability'. Laurent van der Maesen is the main proponent of this strand contends. In a working paper we read that \"international and national (and urban) strategies and policies for development toward 'overall' sustainability not only have to have a strong emphasis on development toward sustainability of cities themselves (internal sustainability) but also – simultaneously – address the development toward sustainability of the cities' surroundings ('Hinterland')\".\n\nA more traditional line is maintained by Peter Herrmann who had been since the end of the 1990s advisor of the then European Foundation, later International Association of Social Quality. He claims that \n\"[w]e have to reconceptualize ... indicators. They are not measurement instruments sui generis. Rather they are instruments to develop an understanding of complex issues and trends. As such they need to be guided by a sound conceptual reflection of what they are looking for\".\nHerrmann is on the one hand reclaiming the methodological and theoretical importance of Social Quality thinking, at least qualifying the empirical orientation. On the other hand, though this does not contradict the orientation on sustainability, Herrmann emphasises the need to reorient debates on \"the kind of economy in place\" instead of maintaining dichotomies, separations and hierarchies between different areas of societal development. This had been elaborated in a book on defining social policy as matter of producing society.\n\n\n"}
{"id": "44441540", "url": "https://en.wikipedia.org/wiki?curid=44441540", "title": "Targeted immunization strategies", "text": "Targeted immunization strategies\n\nTargeted immunization strategies are approaches designed to increase the immunization level of populations and decrease the chances of epidemic outbreaks. Though often in regards to use in healthcare practices and the administration of vaccines to prevent biological epidemic outbreaks, these strategies refer in general to immunization schemes in complex networks, biological, social or artificial in nature. Identification of at-risk groups and individuals with higher odds of spreading the disease often plays an important role in these strategies.\n\nThe success of vaccines and anti-virus software in preventing major outbreaks relies on the mechanism of herd immunity, also known as community immunity, where the immunization of individuals provides protection for not only the individuals, but also the community at large. In cases of biological contagions such as influenza, measles, and chicken pox, immunizing a critical community size can provide protection against the disease for members who cannot be vaccinated themselves (infants, pregnant women, and immunocompromised individuals). Often however these vaccine programmes require the immunization of a large majority of the population to provide herd immunity. A few successful vaccine programmes have led to the eradication of infectious diseases like small pox and rinderpest, and the near eradication of polio, which plagued the world before the second half of the 20th century.\n\nMore recently researchers have looked at exploiting network connectivity properties to better understand and design immunization strategies to prevent major epidemic outbreaks. Many real networks like the Internet, world wide web, and even sexual contact networks have been shown to be scale-free networks and as such exhibit a power-law distribution for the degree distribution. In large networks this results in the vast majority of nodes (individuals in social networks) having few connections or low degree \"k\", while a few \"hubs\" have many more connections than the average <\"k\">. This wide variability (heterogeneity) in degree offers immunization strategies based on targeting members of the network according to their connectivity rather than random immunization of the network. In epidemic modeling on scale-free networks, targeted immunization schemes can considerably lower the vulnerability of a network to epidemic outbreaks over random immunization schemes. Typically these strategies result in the need for far fewer nodes to be immunized in order to provide the same level of protection to the entire network as in random immunization. In circumstances then where vaccines are scarce, efficient immunization strategies become necessary to preventing infectious outbreaks.\n\nExamples\n\nA common approach for targeted immunization studies in scale-free networks focuses on targeting the highest degree nodes for immunization. These nodes are the most highly connected in the network, making them more likely to spread the contagion if infected. Immunizing this segment of the network can drastically reduce the impact of the disease on the network and requires the immunization of far fewer nodes compared to randomly selecting nodes. However, this strategy relies on knowing the global structure of the network, which may not always be practical.\n\nAnother strategy, \"acquaintance immunization\", tries to target all of the most highly connected nodes for immunization by going through their neighbors without knowing the full network topology. In this approach a random group of nodes are chosen and then a random set of their neighbors are selected for immunization. The most highly connected nodes are far more likely to be in this group of neighbors so immunizing this group results in targeting the most highly connected but requiring far less information about the network. Another variant of this strategy again calls for the random selection of nodes but instead asks for one of their neighbors with a higher degree, or at least more than a given threshold degree and immunizes them. These degree based strategies consistently require fewer nodes to be immunized and as such improve a network's chances against epidemic attacks.\n\nA recent centrality measure, Percolation Centrality, introduced by Piraveenan et al. is particularly useful in identifying nodes for vaccination based on the network topology. Unlike node degree which depends on topology alone, however, percolation centrality takes into account the topological importance of a node as well as its distance from infected nodes in deciding its overall importance. Piraveenan et al. has shown that percolation centrality-based vaccination is particularly effective when the proportion of people already infected is on the same order of magnitude as the number of people who could be vaccinated before the disease spreads much further. If infection spread is at its infancy, then ring-vaccination surrounding the source of infection is most effective, whereas if the proportion of people already infected is much higher than the number of people that could be vaccinated quickly, then vaccination will only help those who are vaccinated and herd immunity cannot be achieved. Percolation centrality-based vaccination is most effective in the critical scenario where the infection has already spread too far to be completely surrounded by ring-vaccination, yet not spread wide enough so that it cannot be contained by strategic vaccination. Nevertheless, Percolation Centrality also needs full network topology to be computed, and thus is more useful in higher levels of abstraction (for example, networks of townships rather than social networks of individuals), where the corresponding network topology can more readily be obtained.\n\nMillions of children worldwide do not receive all of the routine vaccinations as per their national schedule. As immunization is a powerful public health strategy for improving child survival, it is important to determine what strategies work best to increase coverage. A Cochrane review assessed the effectiveness of intervention strategies to boost and sustain high childhood immunization coverage in low- and middle income countries. Fourteen trials were included but most of the evidence was of low quality. Providing parents and other community members with information on immunization, health education at facilities in combination with redesigned immunization reminder cards, regular immunization outreach with and without household incentives, home visits, and integration of immunization with other services may improve childhood immunization coverage in low-and middle income countries.\n\n"}
{"id": "12514501", "url": "https://en.wikipedia.org/wiki?curid=12514501", "title": "Testicular rupture", "text": "Testicular rupture\n\nTesticular rupture is a rip or tear in the tunica albuginea resulting in extrusion of the testicular contents, including the seminiferous tubules. It is a rare complication of testicular trauma, and can result from blunt or penetrating trauma, though blunt trauma is more likely to cause rupture. Testicular rupture typically results from trauma sustained during a motor vehicle crash or sports play, mainly affects those from the ages of 10-30. The main symptoms of testicular rupture are scrotal swelling and severe pain, which can make diagnosis difficult. Testicular rupture should be suspected whenever blunt trauma to the scrotum has been sustained. Treatment consists of surgical exploration with repair of the injury.\n\nA potential testicular rupture should be evaluated with ultrasound imaging. Testicular rupture is treated with surgery, though the procedure performed depends on the magnitude of the injury and the salvageability of the tissue. An orchiectomy - removal of the affected testis - is done when the testis is not salvageable and leads to reduced semen quality and higher rates of endocrine dysfunction than repair of salvageable tissue.\n\n90% of ruptured testes are successfully repaired when treated surgically within 72 hours; the percentage of successful treatment drops to 45% after this period. Though not typically fatal, testicular rupture can cause hypogonadism, low self-esteem, and infertility.\n"}
{"id": "6606537", "url": "https://en.wikipedia.org/wiki?curid=6606537", "title": "Tony Mills (physician)", "text": "Tony Mills (physician)\n\nTony Mills, is an American physician who specializes in treatment of HIV and AIDS. \nHe is one of the leading clinician in the fields of Men's Health and HIV disease. Mills has served as the primary care provider for over 2,000 patients, including approximately half living with HIV. Mills received both his undergraduate and medical degrees from Duke University. He completed an internship in Internal Medicine, a residency in Anesthesiology and a fellowship in Cardiovascular Anesthesiology, all at the University of California, San Francisco. Mills is a member of many professional societies including; the Infectious Disease Society of America, International AIDS Society, IAS-USA, and the American Academy of HIV Medicine. He is the executive director of SoCal Men's Medical Group, the clinical research director of Mills Clinical Research, and the president of the Men's Health Foundation.\n\nIn May 1998, he won the title of International Mister Leather, publicly coming out as HIV-positive one day later.\n\n\nMills graduated from Duke University School of Medicine and was awarded both the Stanley Sarnoff Fellowship Award in Cardiovascular Research and the Eugene Stead Research Award. He began his clinical practice in 1991 at Columbia Presbyterian Medical Center in New York City, concentrating on heart transplantation and cardiovascular research.\n\nIn 1994, Mills was named Chief of Pediatric Cardiac Anesthesiology at the University of Miami, where he was actively involved in both the recovery community and in the gay community as an advocate for people living with HIV.\n\nIn 1999, he moved to Los Angeles and opened a general medical practice specializing in HIV care. He was certified as an HIV specialist by the American Academy of HIV Medicine in 2000 and currently serves on both the California Board and the National Board of the AAHIVM. In 2002, Mills joined the Clinical Medicine Faculty at UCLA where he works actively with residents and fellows and is a frequent lecturer.\n\nHe is the current editor of \"HIV Treatment News\" and is a frequent contributor to other HIV-related publications.\n\nOn May 5, 1998, having earned the regional title of Mister Mid-Atlantic Leather, Mills entered and won the International Mister Leather contest, competing against 61 contestants from 7 countries.\n\nSince winning the contest, Mills has been featured in the documentaries \"Beyond Vanilla\" and \"Mr. Leather\". He has also been a model for the COLT Studio Group.\n"}
{"id": "25384450", "url": "https://en.wikipedia.org/wiki?curid=25384450", "title": "Uni Health", "text": "Uni Health\n\nUni Research Health is a department in Uni Research, one of the largest research companies in Norway. Research Director of Uni Research Health is Professor Hege R. Eriksen.\n\nUni research health has approximately 125 employees, most of them located in Bergen, Norway.\n\nThe research and educational activities of Uni Health are concentrated in the following research units:\n\nCentre for Child and Adolescent Mental Health Research\nChild Protection Research Unit\nDental Biomaterials: Adverse Reaction Unit\nGAMUT (the Grieg Academy Music Therapy Research Centre)\nHEMIL Centre (Research Centre for Health Promotion)\nNational Centre for Emergency Primary Health Care\nOccupational and Environmental Medicine\nResearch Unit for General Practice in Bergen\nResearch Centre for Sick Leave and Rehabilitation\nStress, Health and Rehabilitation (formerly the Research Unit of the Norwegian Network for Back Pain)\n\n"}
{"id": "54398615", "url": "https://en.wikipedia.org/wiki?curid=54398615", "title": "Wound assessment", "text": "Wound assessment\n\nWound assessment is a component of wound management that collects information about the patient, and the wound before a treatment plan is prescribed.\n\nWound assessment includes observation of the wound, surveying the patient, as well as identifying relevant clinical data from physical examination and patient’s health history. Clinical data recorded during an initial assessment serves as a baseline for prescribing the appropriate treatment.\n\nTo assist clinicians in standardizing the wound assessment and preparation of wound bed for treatment, the TIME framework has been developed in 2002 by a group of wound care experts.\nThe TIME acronym stands for Tissue, Infection/Inflammation, Moisture, and Edge – components that, per the TIME recommendation, should be thoroughly assessed to optimize the treatment. Depending on the clinical findings for each component, TIME recommends certain clinical actions aimed at correcting the issues and facilitating the healing.\n\nA recent global anthropological study has prompted clinicians to review the TIME framework and resulted in a 2016 development of a new comprehensive tool for wound assessment – the Triangle of Wound Assessment (TWA). Based on the study’s findings, TWA identifies three zones (wound bed, wound edge, and periwound skin) that must be included in wound assessment to arrive at clinical decisions that will help heal the wound in the most efficient way. TIME framework components are integrated into the assessment of each zone.\n\nThe introduction of periwound skin as a component of wound assessment identifies a significant departure from traditional methods; it emphasizes the importance of addressing periwound skin during treatment in the same measure as wound bed and wound edge.\n\nWound assessment is a holistic process that considers the patient’s current state of health, the factors that may impede wound healing, and the cause, duration and state of the wound. As such, this process is applicable to any wound.\n\nPatient’s health history may include disorders that affect the body’s ability to heal itself. These disorders are called comorbidities and may interfere with circulatory and metabolic body functions, levels of various physiological assessment components (sugar, albumin, etc.), and induce other factors that negatively affect the healing. Common co-morbidities are: diabetes, venous insufficiency or peripheral arterial disease, respiratory and cardiovascular disorders, malignancies and autoimmune disorders.\n\nAmong other factors that may impede the healing of a wound are:\n\nIf the wound is chronic, is it the result of: an underlying illness (diabetic, venous and arterial ulcers), poor handling of the patient (pressure injuries, deep tissue injuries, wounds with cavities and undermining), poor previous treatment choices that slowed down the healing (untreated infection, inappropriate wound care product choice, lack of necessary procedures).\nIf the wound is acute, is it the result of: traumatic injury, burn, or surgery.\n\nFor chronic wounds: time the current wound has been present, is it a recurring wound, how many times it has recurred in the past, how long it took to heal each time.\nFor acute wounds: when the wound was first acquired before the clinician visit.\n\nWound bed, wound edge and periwound skin should be examined before the initial treatment plan is devised. It should also be re-assessed at each visit or each dressing change.\n\nFor wound bed, the following parameters are assessed:\n\nWound edge must be examined to detect:\n\nFor periwound skin, the following conditions should be diagnosed or ruled out:\n"}
{"id": "44457141", "url": "https://en.wikipedia.org/wiki?curid=44457141", "title": "Youth and disability", "text": "Youth and disability\n\nWorldwide, there are between 180 and 220 million youth with disabilities. Eighty percent of disabled youth live in developing countries, and therefore have even less access to education, health care, jobs and general rights Disabilities include physical, mental disabilities or mental illness. Many youth live normal and stable lives, however those with disabilities may experience more obstacles than those without due to potential limitations, those created by physical limitations and social limitations. \n\nBefore the 1970s, there were no major federal laws that protected the civil or constitutional rights of Americans with disabilities. The civil rights movement started off the \"disability rights movement\", which focused on social and therapeutic services for those with disabilities, and in 1975 the Individuals with Disabilities Education Act (IDEA) was created. This law establishes the rights of children with disabilities to attend public schools, to receive services designed to meet their needs free of charge, and to receive instruction in regular education classrooms alongside non-disabled children.\n\nThe IDEA also authorized federal grants to states to cover some of the costs of special education services for youth aged three to twenty-one. Additions to the law focused on improving access to general education and curriculum (inclusion programs), developing appropriate assessments, implementing appropriate disciplinary procedures and alternative placements, and creating transition services for youth leaving the education system. This transition can be difficult for youth with a disability if it is too sudden- many of these youth struggle with the independence that graduation allows. In 2004, additions were made to promote better accountability for results, enhance parent involvement, encourage the use of proven practices and materials (increase research on current practices), and reduce administrative burdens for teachers, states and local school districts. Before 1975, only one in five children with identified disabilities attended public school, and many states explicitly excluded children with certain types of disabilities from school; these included children who were blind or deaf, and children labeled \"emotionally disturbed\" or \"mentally retarded\".\n\nThe current special education system can offer many different supports and services including transportation, speech-language pathology and audiology services, psychological services, physical and occupational therapy, therapeutic recreation, counseling services including rehabilitation counseling, orientation, and mobility services, medical services for diagnostic or evaluation purposes, school health services, social work services in school, and parent counseling and training children. Each unique student and their unique symptoms of their disability is going to require different supports from another youth with a disability (even if it is the same disability). Within the classroom, there are many different ways that teachers and administration can adjust their work to meet the needs of the youth in their classroom with disabilities. Just a few of these adaptations are curriculum modification, small-group or individual instruction, and teachers who are especially skilled in motivating students, adapting instructional materials, teaching reading skills and language arts, and managing student behaviors.Specific accommodations might include tutors or aides, more time for students to take tests, alternative tests or assessments, modified grading standards, slower-paced instruction, shorter or different assignments, more frequent feedback, a reader or interpreter, a peer tutor, or special behavior management approaches and programs.\n\nBeing a youth with a disability can create a financial burden on the individual, as well as to those who provide care and support. Their families also incur extra direct and indirect costs.. Families with disabled yout spend money on health care, therapeutic, behavioral, or educational services; transportation; caregivers; and other special needs services. Indirect costs include reductions in parents' ability to work because of additional time that is required to care for a child with a disability combined with high costs or unavailability of adequate child care. This is a similar problem to one that many families face, but disabled youth may live at home longer require more attention. These costs alone can decrease the financial stability of a family. Having a child with disabilities increases the likelihood that the mother (or less often the father) will either curtail hours of work or stop working altogether. One study showed that a decline in employment of 9% for mothers with a disabled child relative to all mothers, with a resultant estimate of approximately $3,150 in lost pay. In addition, mothers who continue to work are estimated to reduce time worked by around two hours a week, with a range of between half an hour and five hours a week.\n\nFamilies with more resources may be able to maintain financial stability, even with the financial strain of having a youth with a disability. These resources may provide treatment, affordable housing, therapy, etc. for a youth with special needs. As a conclusion from the research listed above, these children are more likely to be able to function independently from their family, and live on their own at an earlier age, than those from families that may not be able to afford these extra resources.\n\nEighty percent of people with disabilities live in resource-poor societies. They are often considered to be a burden, and carry a very negative social stigma. Many are unable to contribute to society, attend school, or find work.\n\nIn the justice system, youth are disproportionately male, poor and have significant learning or behavioral disabilities to the extent that they require services listed under the IDEA. There are 1345,000 youth incarcerated in the U.S system, and 30%-70% of these individuals are youth with disabilities.\nPsychiatric disorders occur more often among prisoners than among those outside of prison. Such illnesses relate to at least some of the difficulties former inmates experience after they are released. Those who were diagnosed with a mental disability may have a harder time readjusting to life outside of prison after being released; this often consists of repetition of crime, or difficulty maintaining independent stability financially or emotionally.\n\n"}
