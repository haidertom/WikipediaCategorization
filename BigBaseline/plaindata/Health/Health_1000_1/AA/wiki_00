{"id": "43318121", "url": "https://en.wikipedia.org/wiki?curid=43318121", "title": "ADIME", "text": "ADIME\n\nADIME, or Assessment, Diagnosis, Intervention, and Monitoring/Evaluation, is a process used to ensure high quality nutrition care to patients and clients from nutrition professionals, such as Registered Dietitians (RD) or Registered Dietitian Nutritionist (RDN). ADIME is used as a means of charting patient progress and to encourage a universal language amongst nutrition professionals.\n\nThe ADIME process consists of four steps:\n"}
{"id": "17700269", "url": "https://en.wikipedia.org/wiki?curid=17700269", "title": "Adventist Health Studies", "text": "Adventist Health Studies\n\nAdventist Health Studies (AHS) is a series of long-term medical research projects of Loma Linda University with the intent to measure the link between lifestyle, diet, disease and mortality of Seventh-day Adventists.\n\nSeventh-day Adventists have a lower risk than other Americans of certain diseases, and many researchers hypothesize that this is due to dietary and other lifestyle habits.\nThis provides a special opportunity to answer scientific questions about how diet and other health habits affect the risk of suffering from many chronic diseases.\n\nTwo studies on Adventist health involving 24,000 and 34,000 Californian Adventists were conducted over the last 40 years.\nAlthough not sponsored by the Adventist church itself, the church is supportive of the studies.\nThese studies have been the subject of significant national media coverage on programs such as \"ABC News: World News Tonight\", \"Good Morning America\" and in the \"National Geographic\" feature article \"Longevity: The Secrets of a Long Life\".\n\nThere is a third larger ongoing study that includes Adventists throughout the United States and Canada.\n\nThe first major study of Adventists began in 1960, and has become known as the Adventist Mortality Study. Consisting of 22,940 California Adventists, it entailed an intensive 5-year follow-up and a more informal 25-year follow-up.\n\n\"...[The] Adventist Mortality Study (1960–1965) did indicate that Adventist men lived 6.2 years longer than non-Adventist men in the concurrent American Cancer Society Study and Adventist women had a 3.7-year advantage over their counterparts. These statistics were based on life table analyses.\"\nSpecifically, comparing death rates of Adventist compared to other Californians:\n\nAn additional study (1974–1988) involved approximately 34,000 Californian Adventists over 25 years of age. Unlike the mortality study, the purpose was to find out which components of the Adventist lifestyle give protection against disease.\n\nThe data from the study have been studied for more than a decade and the findings are numerous – linking diet to cancer and coronary heart disease.\n\nSpecifically: \n\nThis is a sub-study of AHS-1. It began in 1976 and is still being conducted. It includes 6,328 Adventists from California.\nThe study was funded by the Environmental Protection Agency.\n\nThe study linked the effects of various indoor and outdoor pollutants with respiratory diseases and lung cancer.\n\nThe current study which began in 2002 with a goal of 125,000 Adventists continues to explore the links between lifestyle, diet and disease among the broader base of Seventh-day Adventists in America and Canada. As of May 2006 it had an enrollment of 96,741.\n\nDr. Gary Fraser with a team of researchers from the School of Public Health at Loma Linda University is conducting the study which is funded by the National Cancer Institute.\nIn July 2011, National Institutes of Health awarded AHS-2 a $5.5 million 5-year grant to continue the study.\n\nWhile the study is on-going, some findings have been reported:\n\nThis sub-study of AHS-2 began in 2006 and is funded by the National Institute on Aging.\nIt is also known as the Biopsychosocial Religion and Health Study (BRHS).\n\nThe study has exceeded its goal of 10,000 participants with 11,835 subjects as of 2008.\n\n\n"}
{"id": "11101687", "url": "https://en.wikipedia.org/wiki?curid=11101687", "title": "Bachelor of Science in Public Health", "text": "Bachelor of Science in Public Health\n\nThe Bachelor of Science in Public Health (BSPH) (or Bachelor of Public Health) is an undergraduate degree that prepares students to pursue careers in the public, private, or non-profit sector in areas such as public health, environmental health, health administration, epidemiology, nutrition, biostatistics, or health policy and planning. Postbaccalaureate training is available in public health, health administration, public affairs, and related areas.\n\nThe University of California at Irvine, Program in Public Health, Department of Population Health and Disease Prevention, has the largest enrollment of undergraduate majors in Public Health, with about 1,500 students including ~1,000 in the Bachelor of Science in Public Health Sciences, and another ~500 students in the Bachelor of Arts in Public Health Policy (2014). UC Irvine also offers a minor in Public Health for students of other majors.\n\nThe Council on Education for Public Health includes undergraduate public health degrees in the accreditation review of public health programs and schools.\n\n\n"}
{"id": "6580964", "url": "https://en.wikipedia.org/wiki?curid=6580964", "title": "Centre for Reviews and Dissemination", "text": "Centre for Reviews and Dissemination\n\nThe Centre for Reviews and Dissemination (CRD) is a health services research centre based at the University of York, England. CRD was established in January 1994, and aims to provide research-based information for evidence-based medicine. CRD carries out systematic reviews and meta-analyses of healthcare interventions, and disseminates the results of research to decision-makers in the NHS. \n\nCRD produces three databases:\n\n\nThese are freely available from the CRD database website and as part of the Cochrane Library.\n\nCRD also publishes a number of regular reports including \"Effective Health Care\" and \"Effectiveness Matters\".\n\nCRD is funded by the UK Department of Health's NHS Research and Development Programme, as well as from a number of other sources.\n\nCRD was established in 1994. Along with the UK Cochrane Centre, the \nCentre was originally created as part of the Information Systems \nStrategy of the NHS Research and Development Programme.\n\nThe original aims of the centre were:\n\n\nProfessor Trevor Sheldon established and directed CRD from 1994 to 1998. He was followed as director by Professor Jos Kleijnen from 1998 to 2005. The current director is Professor Lesley Stewart who took up appointment in 2006.\n\nThe Centre for Reviews and Dissemination (CRD) is part of the National Institute for Health Research (NIHR) and is a department of the University of York. CRD is one of the largest groups in the world engaged exclusively in evidence synthesis in the health field. The Centre comprises health researchers, medical information specialists, health economists and a dissemination team.\n\nCRD undertakes systematic reviews evaluating the research evidence on health and public health questions. The findings of CRD reviews are widely disseminated and have impacted on health care policy and practice, both in the UK and internationally.\n\nCRD produce the DARE, NHS EED and HTA databases which are used by health professionals, policy makers and researchers. \n\nCRD also undertake methods research and produce internationally accepted guidelines for undertaking systematic reviews.\n\nThe Centre's role in developing research evidence to support decision making in policy and practice was highlighted in the Cooksey report on UK health research funding and subsequently in the Government national research strategy Best Research for Best Health. CRD was also recognised by the Lancet as part of NHS R&D's most important contribution to the UK science base, namely building systematic review capacity and promulgating systematic reviews as a global public good.\n\nCRD receives core funding through the NIHR. This funding enables the centre to function as a national resource and provides the necessary infrastructure to respond to requests from policy makers and healthcare professionals, and to support the provision and promotion of the online databases (DARE, NHS EED and HTA ).\n\nIn addition to the core funding, the Centre has undertaken independent research for a number of different agencies including:\n\n\nCRD is one of seven independent academic centres currently undertaking reviews commissioned by NICE. CRD collaborate with the Centre for Health Economics at the University of York to undertake technology assessment reviews that inform NICE Technology Appraisals on the use of new and existing medicines and treatments within the NHS.\n\nCRD also has close links with the UK Cochrane Centre and contributes to the work of several of the Cochrane Collaboration’s Review and Methods groups. The Centre also has representation on the Steering and User groups of the Campbell Collaboration and is a member of the International Network of Agencies for Health Technology Assessment (INAHTA).\n\nCRD is a founder member of the Public Health Research Consortium (PHRC) with brings together researchers from 11 UK institutions. The PHRC aims to strengthen the evidence base for public health, with a strong emphasis on tackling socioeconomic inequalities in health. The centre is collaborating on a number of projects and is providing support for information retrieval and the knowledge transfer activities of the Consortium. \n\n"}
{"id": "39536478", "url": "https://en.wikipedia.org/wiki?curid=39536478", "title": "Children Without Worms", "text": "Children Without Worms\n\nChildren Without Worms (CWW) is a global collaborative health programme among two pharmaceutical giants, Johnson & Johnson and GlaxoSmithKline, and a nonprofit organisation, the Task Force for Global Health. The cooperative goal is to support the treatment and prevention of parasitic infection with soil-transmitted helminths, which are the major cause of morbidity in school-age children, especially those living in Africa, Asia and South America.\n\nCWW is an effort to make the world's children free of soil-transmitted helminthiasis so that they can grow, play, learn normally and enrich their communities. To accomplish the mission, CWW works closely with the World Health Organization, regional government ministries and nongovernmental organizations. It partners with Helen Keller International to work in Cambodia, World Wildlife Fund in Cameroon, and Save the Children in Bangladesh.\n\nSoil-transmitted helminthiasis is a neglected tropical disease as a result of infection of intestinal parasites such as roundworm (\"Ascaris lumbricoides\"), whipworm (\"Trichuris trichiura\"), hookworms (\"Ancylostoma duodenale\" and \"Necator americanus\"), and pinworm/threadworm (\"Strongyloides stercoralis\"). Most prevalent in the impoverished tropical and subtropical regions of Subsaharan Africa, Latin America, Southeast Asia and China, where sanitation and hygiene are poor, the disease is an enormous burden on humanity, amounting to 135,000 deaths every year, and persistent infection of more than two billion people. The long-term impact is even worse. In these regions, the disease is the single most debilitating cause of intellectual and physical retardation. Thus it remains a relentless factor of backwardness in socio-economic and human development.\n\nCWW basically is an aim to control STH in children by:\n\nCWW targets children of school age through a slogan called the WASHED Framework, which includes systematic promotion of provision of water, sanitation, hygiene education, and deworming. Since helminth infections are not easily controlled by a simple treatment regime at a specific target level, CWW made a coordinated plan to improve:\n\nCWW oversees the donation of anthelmintics to the Ministries of Health and Education in eight recipient countries, including Bangladesh, Cambodia, Cameroon, Cape Verde, Lao PDR, Nicaragua, Uganda and Zambia. The annual donation consists of 200 million tablets of mebendazole from Johnson & Johnson, and 400 million tablets of albendazole from GlaxoSmithKline, and these are estimated for the treatment of up to 300 million children twice a year.\n\nCWW joined forces with the World Health Organization, the World Bank, the Bill & Melinda Gates Foundation, the world's 13 leading pharmaceutical companies, and governments of US, UK, United Arab Emirate, Bangladesh, Brazil, Mozambique and Tanzania, to implement the largest global health programme called the \"London Declaration on Neglected Tropical Diseases\" on 30 January 2012 in London. The ambitious project is to control or eradicate the major neglected tropical diseases. Under this programme, CWW will continue to disseminate drugs through for the projected eradication by 2020.\n\n"}
{"id": "4857593", "url": "https://en.wikipedia.org/wiki?curid=4857593", "title": "Clinical research", "text": "Clinical research\n\nClinical research is a branch of healthcare science that determines the safety and effectiveness (efficacy) of medications, devices, diagnostic products and treatment regimens intended for human use. These may be used for prevention, treatment, diagnosis or for relieving symptoms of a disease. Clinical research is different from clinical practice. In clinical practice established treatments are used, while in clinical research evidence is collected to establish a treatment.\n\nThe term \"clinical research\" refers to the entire bibliography of a drug/device/biologic, in fact any test article from its inception in the lab to its introduction to the consumer market and beyond. Once the promising candidate or the molecule is identified in the lab, it is subjected to pre-clinical studies or animal studies where different aspects of the test article (including its safety toxicity if applicable and efficacy, if possible at this early stage) are studied.\n\nIn the United States, when a test article is unapproved or not yet cleared by the Food and Drug Administration (FDA), or when an approved or cleared test article is used in a way that may significantly increase the risks (or decreases the acceptability of the risks), the data obtained from the pre-clinical studies or other supporting evidence, case studies of off label use, etc. are submitted in support of an Investigational New Drug (IND) application to the FDA for review prior to conducting studies that involve even one human and a test article if the results are intended to be submitted to or held for inspection by the FDA at any time in the future (in the case of an already approved test article, if intended to submit or hold for inspection by the FDA in support of a change in labeling or advertising). Where devices are concerned the submission to the FDA would be for an Investigational Device Exemption (IDE) application if the device is a significant risk device or is not in some way exempt from prior submission to the FDA. In addition, clinical research may require Institutional Review Board (IRB) or Research Ethics Board (REB) and possibly other institutional committee reviews, Privacy Board, Conflict of Interest Committee, Radiation Safety Committee, Radioactive Drug Research Committee, etc. approval whether or not the research requires prior submission to the FDA. Clinical research review criteria will depend on which federal regulations the research is subject to (e.g., (Department of Health and Human Services (DHHS) if federally funded, FDA as already discussed) and will depend on which regulations the institutions subscribe to, in addition to any more stringent criteria added by the institution possibly in response to state or local laws/policies or accreditation entity recommendations. This additional layer of review (IRB/REB in particular) is critical to the protection of human subjects especially when you consider that often research subject to the FDA regulation for prior submission is allowed to proceed, by those same FDA regulations, 30 days after submission to the FDA unless specifically notified by the FDA not to initiate the study.\n\nClinical research is often conducted at academic medical centers and affiliated research study sites. These centers and sites provide the prestige of the academic institution as well as access to larger metropolitan areas, providing a larger pool of medical participants. These academic medical centers often have their internal Institutional Review Boards that oversee the ethical conduct of medical research.\n\nThe clinical research ecosystem involves a complex network of sites, pharmaceutical companies and academic research institutions. This has led to a growing field of technologies used for managing the data and operational factors of clinical research. Clinical research management is often aided by eClinical systems to help automate the management and conducting of clinical trials.\n\nIn the European Union, the European Medicines Agency (EMA) acts in a similar fashion for studies conducted in their region. These human studies are conducted in four phases in research subjects that give consent to participate in the clinical trials.\n\nClinical trials involving new drugs are commonly classified into four phases. Each phase of the drug approval process is treated as a separate clinical trial. The drug-development process will normally proceed through all four phases over many years. If the drug successfully passes through Phases I, II, and III, it will usually be approved by the national regulatory authority for use in the general population. Phase IV are 'post-approval' studies.\n\nPhase I includes 20 to 100 healthy volunteers or individuals with the disease/condition. This study typically lasts several months and its purpose is safety and dosage. Phase II includes larger number of individual participants ranging 100-300, and phase III includes Approximately 1000-3000 participants to collect more data about the drug. 70% of drugs advance to the next phase.\n\nBefore pharmaceutical companies start clinical trials on a drug, they conduct extensive pre-clinical studies.\n\n\n"}
{"id": "20902512", "url": "https://en.wikipedia.org/wiki?curid=20902512", "title": "Code of safe working practices", "text": "Code of safe working practices\n\nThe Code of Safe Working Practices (COSWP) is published by the Maritime and Coastguard Agency (MCA) of the UK. The code details the regulatory framework for health and safety aboard ship, safety management and statutory duties underlying to the advice in the code and the areas that should be covered when introducing a new recruit to safety procedures on board.\n\nThe code arose from the Health and Safety at Work act (1974).\n"}
{"id": "766544", "url": "https://en.wikipedia.org/wiki?curid=766544", "title": "Construction site safety", "text": "Construction site safety\n\nConstruction work is a hazardous land-based job. Some construction site jobs include: building houses, roads, tree forts, workplaces and repair and maintain infrastructures. This work includes many hazardous task and conditions such as working with height, excavation, noise, dust, power tools and equipment. The most common fatalities are caused by the fatal four: falls, being struck by an object, electrocutions, and being caught in between two objects. Construction work has been increasing in developing and undeveloped countries over the past few years. With an increase in this type of work occupational fatalities have increased. Occupational fatalities are individuals who die while on the job or performing work related tasks. Within the field of construction it is important to have safe construction sites.\n\nIn deciding the risk precautions, the employer has to provide different degrees of protection. Where one task happens to be more dangerous than another, a greater degree of care has to be taken, but where the employer cannot eliminate the dangerous task, reasonable precautions are needed to reduce the risk according to Nguyen Van Vinh v Cheung Ying Construction Engineering Ltd (2008). This does not, however, imply that an employer is required to remove every risk. The Lord Oaksey commented in Winter v Cardiff Rural District Council (1950) stated that “but this does not mean that an employer must decide on every detail of the system of work or mode of operation. There is a sphere in which the employer must exercise his discretion and there are other spheres in which foremen and workmen must exercise theirs...With regard to the decision how safety precaution has to be taken frequently, it should be left to the foreman or workmen on the site. Whilst the immediate employer of the employee is liable for safety, Morris v. Breaveglen (1993) ruled that the principal contractor cannot escape from his liability. The general employers argued that they should not be liable for the injuries as they were not exercising direct control over the workers. However, judges invalidated such contention in Rainfield Design & Associates Ltd v Siu Chi Moon (2000), “[t]he purpose of the Regulations was clearly to provide for the safety of workman and the primary responsibility for this must rest with the contractor responsible for the site. Even where a subcontractor had a contractual duty to provide plant and equipment, the contractor responsible for the site would not be relieved from its duty under the Regulations.” \n\nLi (2019) proposes that there are three generations of construction safety informatics which are relevant to construction safety enhancement:\n\n1. The first generation of construction safety informatics consisted of technologies that relied completely on control by human beings; for example, structural equation modelling requires the work of an analyst.\n\n2. The second generation of construction safety informatics included smart features such as the Internet of Things which can send information to human operators, without human intervention — from sensors, etc. Yet, these “smart” tools cannot learn and improve on their own capabilities.\n\n3. The third generation of construction safety informatics uses state-of-the-art AI, to mimic human behavior and think, act, learn and improve on its own decision making. All that is required is that the relevant information is fed to these systems, so that they can be ‘taught’ \n\nIn 2014, the United States had 4,679 fatal occupational injuries, an incidence rate of 3.3 per 100,000 full-time employed workers. In the same year, fatal work injuries in construction and extraction occupations increased 5%. One in five deaths of workers in 2014 were construction related. Construction has about 6% of U.S. workers, but 17% of the fatalities - the largest number of fatalities reported for any industry sector. In the United Kingdom, the construction industry is responsible for 31% of fatalities at work and 10% of major workplace injuries. In South Africa there are 150 fatalities and approximately 400 injuries each year related to construction sites. In Brazil, the incidence rate for all occupational fatalities is 3.6 per 100,000. (Little to no information regarding construction fatalities could be found in Asia, South American, Africa, and the Antarctic.) The chart below contains more countries and the rate of construction site fatalities.\n\nThe leading safety hazards on construction sites include falls, being caught between objects, electrocutions, and being struck by objects. These hazards have caused injuries and deaths on construction sites throughout the world. Failures in hazard identification are often due to limited or improper training and supervision of workers. Areas where there is limited training include tasks in design for safety, safety inspection, and monitoring safety. Failure in any of these areas can result in an increased risk in exposing workers to harm in the construction environment.\n\nFalls are the leading cause of injury in the construction industry, in particularly for elder and untrained construction workers. In the Occupational Safety and Health Administration (OSHA) Handbook (29 CFR) used by the United States, fall protection is needed in areas including but not limited to ramps, runways, and other walkways; excavations; hoist areas; holes; form-work; leading edge work; unprotected sides and edges; overhand bricklaying and related work; roofing; precast erection; wall openings; floor openings such as holes; residential construction; and other walking/working surfaces. Other countries have regulations and guidelines for fall protections to prevent injuries and deaths.\n\nMotor vehicle crashes are another major safety hazard on construction sites. It is important to be cautious while operating motor vehicles or equipment on the site. A motor vehicle should have a service brake system, emergency brake system, and a parking brake system. All vehicles must be equipped with an audible warning system if the operator chooses to use it. Vehicles must have windows and doors, power windshield wipers, and a clear view of site from the rear window. All employees should be properly trained before using motor vehicles and equipment.\n\nEmployees on construction sites also need to be aware of dangers on the ground. Cables running across roadways were often seen until cable ramp equipment was invented to protect hoses and other equipment which had to be laid out. Another common hazard that workers may face is overexposure to heat and humidity in the environment. Overexertion in this type of weather can lead to serious heat-related illnesses such as heat stroke, heat exhaustion, and heat cramps. Other hazards found on construction site include asbestos, solvents, noise, and manual handling activities. \n\nConstruction workers need to be properly trained and educated on the task or job before working, which will assist in preventing injuries and deaths. There are many methods of training construction workers. One method is coaching construction site foremen to include safety in their daily verbal exchanges with workers to reduce work-related accidents. It is important that the workers use the same language to assure the best communication. In recent years, apart from traditional face to face safety knowledge sharing, mobile apps also make knowledge sharing possible. \n\nAnother method is ensuring that all workers know how to properly use electronics, conveyors, skid-steer, trucks, aerial lifts, and other equipment on the construction site. Equipment on the job site must be properly maintained and inspected regularly before and after each shift. The equipment inspection system will help the operator make sure that a machine is mechanically sound and in safe operating conditions. An employee should be assigned to inspect equipment to insure proper safety. Equipment should have lights and reflectors if intended for night use. The glass in the cab of the equipment must be safety glass in some countries. The equipment must be used for its intended task at all times on the job site to insure workers' safety.\n\nEach construction site should have a construction site manager. This is an occupational health and safety specialist who designs and implements safety regulations to minimize injuries and accidents. He or she also is in charge of conducting daily safety audits and inspections to ensure compliance with government regulations. Most construction site managers have an entry level experience or higher degree.\n\nBefore any excavation takes place, the contractor is responsible for notifying all applicable companies that excavation work is being performed. During excavation, the contractor is responsible for providing a safe work environment for employees and pedestrians. \n\nAccess and egress are also important parts of excavation safety. Ramps used by equipment must be designed by a person qualified in structural design. No person is allowed to cross underneath or stand underneath any loading or digging equipment. Employees are to remain at a safe distance from all equipment while it is operational. Employees who have training and education in the above areas will benefit their co-workers and themselves on the construction site.\n\nThe American Recovery and Reinvestment Act of 2009 created over 12,600 road construction projects, over 10,000 of which are currently in progress. Workers in highway work zones are exposed to a variety of hazards and face risk of injury and death from construction equipment as well as passing motor vehicles. Workers on foot are exposed to passing traffic, often at high speeds, while workers who operate construction vehicles are at risk of injury due to overturn, collision, or being caught in running equipment. Regardless of the task assigned, construction workers work in conditions in poor lighting, poor visibility, inclement weather, congested work areas, high volume traffic and speeds. In 2011, there were a total of 119 fatal occupation fatalities in road construction sites. In 2010 there were 37,476 injuries in work zones; about 20,000 of those were to construction workers. Causes of road work site injuries included being struck by objects, trucks or mobile equipment (35%), falls or slips (20%), overexertion (15%), transportation incidents (12%), and exposure to harmful substances or environments (5%). Causes of fatalities included getting hit by trucks (58%), mobile machinery (22%), and automobiles (13%).\n\nRoad construction safety remains a priority among workers. Several states have implemented campaigns addressing construction zone dangers and encouraging motorists to use caution when driving through work zones. \n\nNational Work Zone Safety Awareness Week is held yearly. The national event began in 1999 and has gained popularity and media attention each year since. The purpose of the event is to draw national attention to motorist and worker safety issues in work zones.\n\nSite preparation aids in preventing injury and death on construction sites. Site preparation includes removing debris, leveling the ground, filling holes, cutting tree roots, and marking gas, water, and electric pipelines. Another prevention method on the construction site is to provide a scaffold that is rigid and sufficient to carry its own weight plus four times the maximum intended load without settling or displacement.\n\nWays to prevent injuries and improve safety include:\n\nThe employees or employers are responsible for providing fall protection systems and to ensure the use of systems. Fall protection can be provided by guardrail systems, safety net systems, personal fall arrest systems, positioning device systems, and warning line systems. Making sure that ladders are long enough to safely reach the work area to prevent injury. Stairway, treads, and walkways must be free of dangerous objects, debris and materials. A registered professional engineer should design a protective system for trenches 20 feet deep or greater for safety reasons. To prevent injury with cranes, they should be inspected for any damage. The operator should know the maximum weight of the load that the crane is to lift. All operators should be trained and certified to ensure that they operate forklifts safely.\n\nOperational Excellence Model to improve safety for construction organizations\n\nThere are 13 safety drivers associated with this model to improve safety for construction organizations:\n\n\nEach safety driver mentioned above has some sub-elements attributed to it.\n\nHard hats, steel-toe boots and reflective safety vests are perhaps the most common personal protective equipment worn by construction workers around the world. A risk assessment may deem that other protective equipment is appropriate, such as gloves, goggles, or high-visibility clothing.\n\nMany construction sites cannot completely exclude non-workers. Road construction sites must often allow traffic to pass through. This places non-workers at some degree of risk.\n\nRoad construction sites are blocked off and traffic is redirected. The sites and vehicles are protected by signs and barricades. However, sometimes even these signs and barricades can be a hazard to vehicle traffic. For example, improperly designed barricades can cause cars that strike them to roll over or even be thrown into the air. Even a simple safety sign can penetrate the windshield or roof of a car if it strikes from certain angles.\n\nThe majority of deaths in construction are caused by hazards relating to construction activity. However, many deaths are also caused by non construction activities, such as electrical hazards. A notable example of this occurred when Andy Roberts, a father of four, was killed in 1988 in New York while changing a light bulb at a construction site when he came into contact with a loose bare wire that was carrying two thousand volts of electricity and died.. Events like this have motivated the passing of further safety laws relating to non construction activities such as electrical work laws.\n\nUnder European Union Law, there are European Union Directives in place to protect workers, notably Directive 89/391 (the Framework Directive) and Directive 92/57 (the Temporary and Mobile Sites Directive). This legislation is transposed into the Member States and places requirements on employers (and others) to assess and protect workers health and safety.\n\nIn the United Kingdom, the Health and Safety Executive (HSE) is responsible for standards enforcement, while in Northern Ireland, the Health and Safety Executive for Northern Ireland (HSENI) is responsible. In Ireland, the Health and Safety Authority (HSA) is responsible for standards and enforcement.\n\nIn Europe, the European Agency for Safety and Health at Work coordinates actions at the EU and national levels and the Directorate-General for Employment, Social Affairs and Inclusion is responsible for regulation at the EU level.\n\nIn the United States, the Occupational Safety and Health Administration (OSHA) sets and enforces standards concerning workplace safety and health. Efforts have been made in the first decade of the 21st century to improve safety for both road workers and drivers in construction zone. In 2004, Title 23 Part 630 Subpart J of the Code of Federal Regulations was updated by Congress to include new regulations that direct state agencies to systematically create and adopt comprehensive plans to address safety in road construction zones that receive federal funding.\n\nOSHA implemented the Final Rule to Improve Tracking of Workplace Injuries and Illnesses, which went into effect January 1, 2017. It requires employers to submit incident data electronically to OSHA. This data will enable OSHA to use enforcement and compliance assistance resources more efficiently. The amount of data required varies by company and industry.\n\n\n"}
{"id": "19639356", "url": "https://en.wikipedia.org/wiki?curid=19639356", "title": "Debunkify", "text": "Debunkify\n\nDebunkify is a campaign established in July 2006 aimed at dispelling tobacco and secondhand smoke misconceptions in the state of Ohio. A mobile marketing tour, complete with a Debunkify-branded vehicle and a team of brand ambassadors, canvassed Ohio on a 10-month run, with the aim of debunking tobacco myths and correcting tobacco misconceptions at each of its stops.\n\nThroughout June 2007, 17 participating standTunz artists battled it out online for a chance to play at the Myth Farewell Tour Main Event.\n\n"}
{"id": "51850608", "url": "https://en.wikipedia.org/wiki?curid=51850608", "title": "Digital therapeutics", "text": "Digital therapeutics\n\nDigital therapeutics, a subset of digital health, is a health discipline and treatment option that utilizes a digital and often online health technologies to treat a medical or psychological condition. The treatment relies on behavioral and lifestyle changes usually spurred by a collection of digital impetuses. Because of the digital nature of the methodology, data can be collected and analyzed as both a progress report and a preventative measure. Treatments are being developed for the prevention and management of a wide variety of diseases and conditions, including type II diabetes, congestive heart failure, obesity, Alzheimer's disease, dementia, asthma, substance abuse, ADHD, panic attacks, anxiety, depression, and several others. Digital therapeutics often employ strategies rooted in cognitive behavioral therapy.\n\nAlthough digital therapeutics can be employed in numerous ways, the term can broadly be defined as a treatment or therapy that utilizes digital and often Internet-based health technologies to spur changes in patient behavior. The use of digital products to improve health outcomes dates as far back as 2000. The term itself has been in use since around 2012. The first mention of the term in a peer-reviewed research publication was in 2015, in which Dr. Cameron Sepah formally defined the field as: \"Digital therapeutics are evidence-based behavioral treatments delivered online that can increase accessibility and effectiveness of health care.\" Digital therapeutics can be used as a standalone therapy or in conjunction with more conventional treatments like pharmacological or in-person therapy. As of 2018, digital therapeutics continues to be an evolving field that medical professionals, students, and patients are beginning to utilize.\n\nIt is often used as a preventive measure for patients who are at risk of developing more serious conditions. For instance, a patient with prediabetes may be prescribed digital therapeutics as a method to change their diet and behavior that could otherwise lead to a diabetes diagnosis. Digital therapeutics can also be used as a treatment option for existing conditions. For instance, a patient with type II diabetes can use digital therapeutics to manage the disease more effectively.\n\nThe methodology uses a variety of digital implements to help manage, monitor, and prevent illnesses in at-risk patients. These include mobile devices and technologies, apps, sensors, desktop computers, and various Internet of Things devices. These implements can collect a wide variety of data, ranging from big to small. Digital therapeutics can theoretically collect a high volume of data from a variety of sources. It also collects \"smaller\" data, \"capturing personalized physiological parameters, behavior patterns and social and geographical patterns that can be recorded from multiple digital sources.\"\n\nDigital therapeutics can be used for a variety of conditions. There is no single methodology used in the practice of digital therapeutics. It uses methods rooted in cognitive behavioral therapy to spur patients to make lifestyle changes. The method can be used to manage and prevent numerous conditions, including type II diabetes, Alzheimer's disease, dementia, congestive heart failure, chronic obstructive pulmonary disease, asthma, lung disease, obesity, substance abuse, ADHD, insomnia, panic attacks, anxiety, depression, and others.\n\nMethodologies can be as simple as sending notifications designed to alter behavior to patients who are at risk of obesity or diabetes and as complex as administering an ingestible radio tag that communicates with an external sensor to monitor the efficacy of a given medication. Diabetes and obesity prevention and management is a major focus in the field of digital therapeutics. Connected devices like insulin pumps, blood glucose meters, and wearable gadgets can all send data to a unified system. The therapy also uses self-reported data like diet or other lifestyle factors. It is also often used to monitor the potential for heart and lung conditions and change behaviors like smoking, poor diet, or a lack of exercise.\n\nDigital therapeutics can also be used to treat patients with psychological and neurological disorders. For example, patients with disorders like ADHD, depression, and anxiety can receive cognitive behavioral therapy via their mobile devices. One study looked at the efficacy of avatar-based therapeutic interventions to reduce depressive symptoms. Another study demonstrated that a 4-week at-home treatment eliminated or reduced panic attacks.\n\nThe general consensus among researchers in the field of digital therapeutics is that the discipline requires more clinical data and investigation to be fully evaluated. A variety of studies have been conducted to evaluate the efficacy and impact of behavior change techniques that utilize a digital platform, however. In a meta-analysis of 85 such studies comprising a total sample size of over 43,000 participants, researchers discovered that digital therapeutics have a \"statistically small but significant effect on health-related behavior.\" The study also showed that a broader use of theory, behavior change techniques, and modes of delivery (especially regular notifications) improved the efficacy of a given program.\n\nIndividual studies have also showed some benefits for patients. For instance, a diabetes prevention program using digital therapeutics saw participants lose an average of 4.7% of baseline body weight after 1 year (4.2% after 2 years) and undergo a 0.38% reduction in A1c levels after 1 year (0.43% after 2 years). Another weight loss pilot program using digital therapeutics reported a mean weight loss of 13.5 pounds (or 7.3% of baseline) with a significant average drop in both systolic and diastolic blood pressure (18.6 mmHg and 6.4 mmHg respectively). The study also saw a slight but statistically insignificant drop in total cholesterol, LDL, triglycerides, and A1c.\n\n"}
{"id": "37445789", "url": "https://en.wikipedia.org/wiki?curid=37445789", "title": "Disability and poverty", "text": "Disability and poverty\n\nThe world's poor are significantly more likely to have or incur a disability within their lifetime compared to more financially privileged populations. The rate of disability within impoverished nations is notably higher than that found in more developed countries. Though no one explanation entirely accounts for this connection, recently there has been a substantial amount of research illustrating the cycle by which poverty and disability are mutually reinforcing. Physical, cognitive, mental, emotional, sensory, or developmental impairments independently or in tandem with one another may increase one's likelihood of becoming impoverished, while living in poverty may increase one's potential of having or acquiring special needs in some capacity.\n\nA multitude of studies have been shown to demonstrate a significant rate of disability among individuals living in poverty. People with disabilities were shown by the World Bank to comprise 15 to 20 percent of the poorest individuals in developing countries. Former World Bank President James Wolfensohn has stated that this connection reveals a link that should be broken. He stated, “People with disabilities in developing countries are over-represented among the poorest people. They have been largely overlooked in the development agenda so far, but the recent focus on poverty reduction strategies is a unique change to rethink and rewrite that agenda.” The link between disability and development has been further stressed by Judith Heumann, the World Bank's first advisor for international disability rights, who indicated that of the 650 million people living with disabilities today eighty percent live in developing countries. Additionally, some research investigations with proved social impact are opening venues that lead to establish enabling factors to break the cycle of deprivation faced by poor people with disabilities. According to the United Kingdom Department for International Development, 10,000 individuals with disabilities die each day as a result of extreme poverty, showing that the connection between these two constructs is especially problematic and deep-seated. This connection is also present in developed countries, with the Disability Funders Network reporting that in the United States alone those with disabilities are twice as likely to live below the poverty line than those without special needs.\n\nAccording to the World Bank, “Persons with disabilities on average as a group experience worse socioeconomic outcomes than persons without disabilities, such as less education, worse health outcomes, less employment, and higher poverty rates.” Researchers have demonstrated that these reduced outcomes may be attributed to a myriad of institutional barriers and other factors. Furthermore, the prevalence of disabilities in impoverished populations has been predicted to follow a cyclical pattern by which those who live in poverty are more likely to acquire a disability and those who have a disability are more likely to become impoverished.\n\nExperts from the United Kingdom Disabled Persons Council attribute the connection between disability and poverty to many systemic factors that promote a “vicious circle.” Statistics affirm the mutually reinforcing nature of special needs and low socioeconomic status, showing that people with disabilities are significantly more likely to become impoverished and people who are impoverished are significantly more likely to become disabled. Barriers presented for those with disabilities can lead individuals to be deprived of access to essential resources, such as opportunities for education and employment, thus causing them to fall into poverty. Likewise, poverty places individuals at a much greater risk of acquiring a disability due to the general lack of health care, nutrition, sanitation, and safe working conditions that the poor are subject to.\n\nExperts assert that this cycle is perpetuated mainly by the lack of agency afforded to those living in poverty. The few options available to the poor often necessitate that these individuals put themselves in harms way, consequently resulting in an increase in the acquisition of preventable impairments. Living in poverty is also shown to decrease an individual's access to preventative health services, which results in an increase in the acquisition of potentially preventable disabilities. In a study by Oxfam, the organization found that well over half of the instances of childhood blindness and hearing impairment in Africa and Asia were considered preventable or treatable. Another estimate released by Oxfam provides further evidence of this vicious circle, finding that 100 million people living in poverty suffer from impairments acquired due to malnutrition and lack of proper sanitation.\n\nDiscrimination\nPrejudice held against individuals with disabilities, otherwise termed ableism, is shown to be a significant detriment to the successful outcomes of persons in this population. According to one study following the lives of children with disabilities in South Africa, the children in the sample described \"discrimination from other metal children and adults in the community as their most significant daily problem.\"\n\nAdditional forms of discrimination may lead disability to be more salient in already marginalized populations. Women and individuals belonging to certain ethnic groups who have disabilities have been found to more greatly suffer from discrimination and endure negative outcomes. Some researchers attribute this to what they believe is a “double rejection” of girls and women who are disabled on the basis of their sex in tandem with their special needs. The stereotypes that accompany both of these attributes lead females with disabilities to be seen as particularly dependent upon others and serve to amplify the misconception of this population as burdensome. In a study done by Oxfam, the societal consequences of having a disability while belonging to an already marginalized population were highlighted, stating, “A disabled women suffers a multiple handicap. Her chances of marriage are very slight, and she is most likely to be condemned to a twilight existence as a non-productive adjunct to the household of her birth… it is small wonder that many disabled female babies do not survive.” Additionally, women with disabilities are particularly susceptible to abuse. A 2004 UN survey in Orissa, India, found that every women with disabilities in their sample had experienced some form of physical abuse. This double discrimination is also shown to be prevalent in more industrialized nations. In the United States, for example, 72 percent of women with disabilities live below the poverty line. The intensified discrimination individuals with disabilities may face due to their sex is especially important to consider when taking into account that, according to the Organisation for Economic Co-operation and Development, women report higher incidences of disability than men. Furthermore, the connection between disability and poverty holds particular significance for the world's women, with females accounting for roughly 70 percent of all individuals living in poverty.\n\nInstitutional discrimination also exists as there are policies existing in organizations that result in inequality between a disabled person and non-disabled person. Some of these organizations systematically ignore the needs of disabled people and some interfere in their lives as a means of social control.\n\nAnother reason individuals living with disabilities are often impoverished is the high medical costs associated with their needs. One study, conducted in villages in South India, demonstrated that the annual cost of treatment and equipment needed for individuals with disabilities in the area ranged from three days of income to upwards of two years' worth, with the average amount spent on essential services totaling three months worth of income. This figure does not take into account the unpaid work of caregivers who must provide assistance after these procedures and the opportunity costs leading to a loss of income during injury, surgery, and rehabilitation.\nStudies reported by medical anthropologists Benedicte Ingstad and Susan Reynolds Whyte have also shown that access to medical care is significantly impaired when one lacks mobility. They report that in addition to the direct medical costs associated with special needs, the burden of transportation falls most heavily on those with disabilities. This is especially true for the rural poor whose distance from urban environments necessitates extensive movement in order to obtain health services. Due to these barriers, both economic and physical, it is estimated that only 2 percent of individuals with disabilities have access to adequate rehabilitation services.\n\nThe inaccessibility of health care for those living in poverty has a substantial impact on the rate of disability within this population. Individuals living in poverty face higher health risks and are often unable to obtain proper treatment, leading them to be significantly more likely to acquire a disability within their lifetime. Financial barriers are not the only obstacles those living in poverty are confronted with. Research shows that matters of geographic inaccessibility, availability, and cultural limitations all provide substantial impediments to the acquisition of proper care for the populations of developing countries. Sex-specific ailments are particularly harmful for women living in poverty. The World Health Organization estimates that each year 20 million women acquire disabilities due to complications during pregnancy and childbirth that could be significantly mitigated with proper pre-natal, childbirth, and post-natal medical care.\nOther barriers to care are present in the lack of treatments developed to target diseases of poverty. Experts assert that the diseases most commonly affecting those in poverty attract the least research funding. This discrepancy, known as the 10/90 gap, reveals that only 10 percent of global health research focuses on conditions that account for 90 percent of the global disease burden. Without a redistribution in research capital, it is likely that many of the diseases known to cause death and disability in impoverished populations will persist.\n\nResearchers assert that institutional barriers play a substantial role in the incidence of poverty in those with disabilities.\n\nPhysical environment may be a large determinant in one's ability to access ladders of success or even basic sustenance. Professor of urban planning Rob Imrie concluded that most spaces contain surmountable physical barriers that unintentionally create an “apartheid by design,” whereby individuals with disabilities are excluded from areas because of the inaccessible layout of these spaces. This \"apartheid\" has been seen by some, such as the United Kingdom Disabled Persons Council, as especially concerning with regard to public transportation, education and health facilities, and perhaps most relevantly places of employment. Physical barriers are also commonly found in the home, with those in poverty more likely to occupy tighter spaces inaccessible to wheelchairs. Beyond physical accessibility, other potential excluding agents include a lack of Braille, sign language and shortage of audio tape availability for those who are blind and deaf.\n\nThe roots of unemployment are speculated to begin with discrimination at an early age. UNESCO reports that 98 percent of children with disabilities in developing countries are denied access to formal education. According to the World Bank, at least 40 million children with disabilities do not receive an education thus barring them from obtaining knowledge essential to gainful employment and forcing them to grow up to be financially dependent upon others. This is also reflected in a finding obtained by the World Development Report that 77 percent of persons with disabilities are illiterate. This statistic is even more jarring for women with disabilities, with the United Nations Development Program reporting that the global literacy rate for this population is a mere 1 percent. This may be attributed to the fact that, according to the World Health Organization, boys with disabilities are significantly more likely to receive an education than similarly abled girls. Beyond simply the skills obtained, experts such as former World Bank advisor Judith Heumann speculate that the societal value of education and the inability of schools to accommodate special needs children substantially contributes to the discrimination of these individuals. It is important to note that the deprivation of education to individuals with special needs may not be solely an issue of discrimination, but an issue of resources. Children with disabilities often require specialized educational resources and teaching practices largely unavailable in developing countries.\n\nSome sociologists have found a number of barriers to employment for individuals with disabilities. These may be seen in employer discrimination, architectural barriers within the workplace, pervasive negative attitudes regarding skill, and the adverse reactions of customers. According to sociologist Edward Hall, \"More disabled people are unemployed, in lower status occupations, on low earnings, or out of the labour market altogether, than non-disabled people.\" The International Labour Organization estimates that roughly 386 million of the world's working age population have some form of disability, however, up to eighty percent of these employable individuals with disabilities are unable to find work. Statistics show that individuals with disabilities in both industrialized and developing countries are generally unable to obtain formal work. In India, only 100,000 of the country's 70 million individuals with disabilities are employed. In the United States, 14.3 of a projected 48.9 million people with disabilities were employed, with two-thirds of those unemployed reporting that they were unable to find work. Similarly in Belgium, only 30 percent of persons with disabilities were able to find gainful employment. In the United Kingdom, 45 percent of adults with disabilities were found to live below the poverty line. Reliable data on the rate of unemployment for persons with disabilities has yet to be determined in most developing countries.\n\nSociologists Colin Barnes and Geof Mercer demonstrated that this exclusion of persons with disabilities from the paid labor market is a primary reason why the majority of this population experiences far greater levels of poverty and are more reliant on the financial support of others. In addition to the economic gains associated with employment, researchers have shown that participation in the formal economic sector reduces discrimination of persons with disabilities. One anthropologist who chronicled the lives of persons with disabilities in Botswana noted that individuals who were able to find formal employment “will usually obtain a position in society equal to that of non-disabled citizens.” Because the formal workplace is such a social space, the exclusion of individuals with disabilities from this realm is seen by some sociologists to be a significant impediment to social inclusion and equality.\n\nEquity in employment has been strategized by some, such as sociologists Esther Wilder and William Walters, to depend on heightened awareness of current barriers, wider use of assistive technologies that can make workplaces and tasks more accessible, more accommodating job development, and most importantly deconstructing discrimination.\n\nCreating inclusive employment that better facilitates the participation of individuals with disabilities is demonstrated to have a significantly positive impact on not only the lives of these individuals, but also the economies of nations who implement such measures. The International Labour Organization estimates that the current exclusion of employable individuals with special needs is costing countries possible gains of 1 to 7 percent of their GDP.\n\nThe relationship between disability and poverty is seen by many to be especially problematic given that it places those with the greatest needs in a position where they have access to the fewest resources. Researchers from the United Nations and the Yale School of Public Health refer to the link between disability and poverty as a manifestation of a self-fulfilling prophecy where the assumption that this population is a drain of resources leads society to deny them access to avenues of success. Such exclusion of individuals on the basis of their special needs in turn denies them the opportunity to make meaningful contributions that disprove these stereotypes. Oxfam asserts that this negative cycle is largely due to a gross underestimation of the potential held by individuals with disabilities and a lack of awareness of the possibilities that each person may hold if the proper resources were present.\n\nThe early onset of preventable deaths has been demonstrated as a significant consequence of disability for those living in poverty. Researchers show that families who lack adequate economic agency are unable to care for children with special medical needs, resulting in preventable deaths. In times of economic hardship studies show families may divert resources from children with disabilities because investing in their livelihood is often perceived as an investment caretakers cannot afford to make. Benedicte Ingstad, an anthropologist who studied families with a member with disabilities, asserted that what some may consider neglect of individuals with disabilities “was mainly a reflection of the general hardship that the household was living under.\" A study conducted by Oxfam found that the rejection of a child with disabilities was not uncommon in areas of extreme poverty. The report went on to show that neglect of children with disabilities was far from a deliberate choice, but rather a consequence of a lack of essential resources. The study also demonstrated that services necessary to the well being of these children “are seized upon” when they are made available. The organization thus concludes that if families had the capacity to care for children with special needs they would do so willingly, but often the inability to access crucial resources bars them from administering proper care.\n\nInitiatives on the local, national, and transnational levels addressing the connection between poverty and disability are exceedingly rare. According to the UN, only 45 countries throughout the world have anti-discrimination and other disability-specific laws. Additionally, experts point to the Western world as a demonstration that the association between poverty and disability is not naturally dissolved through the development process. Instead, a conscious effort toward inclusive development is seen by theorists, such as Disability Policy expert Mark Priestley, as essential in the remediation process.\n\nDisability rights advocate James Charlton asserts that it is crucial to better incorporate the voices of individuals with disabilities into the decision making process. His literature on disability rights made popular the slogan, “Nothing about us without us,” evidencing the need to ensure those most affected by policy have an equitable hand in its creation. This need for agency is an issue particularly salient for those with special needs who are often negatively stereotyped as dependent upon others. Furthermore, many who are part of the disability rights movement argue that there is too little emphasis on aid designed to eliminate the physical and social barriers those with disabilities face. The movement asserts that unless these obstacles are rectified, the connection between disability and poverty will persist.\n\nEmployment is seen as a critical agent in reducing stigma and increasing capacity in the lives of individuals with disabilities. The lack of opportunities currently available is shown to perpetuate the vicious cycle, causing individuals with disabilities to fall into poverty. To address these concerns many recent initiatives have begun to develop more inclusive employment structures. One example of this is the Ntiro Project for Supported and Inclusive Employment. Located in South Africa, the project aims to eliminate the segragationist models prevalent in the country through coordinated efforts between districts, NGOs, and community organizations. The model stresses education and pairs individuals with intellectual disabilities with mentors until they have developed the skills necessary to perform their roles independently. The program then matches individuals with local employers. This gradualist model ensures that people who may have been deprived of the resources necessary to acquire essential skills are able to build their expertise and enter the workforce.\n\nThe United Nations has been at the forefront of initiating legislation that aims to deter the current toll disabilities take on individuals in society, especially those in poverty. In 1982 the UN published the World Programme of Action Concerning Disabled Persons, which explicitly states \"Particular efforts should be made to integrate the disabled in the development process and that effective measures for prevention, rehabilitation and equalization of opportunities are therefore essential.\" This doctrine set stage for the UN Decade of the Disabled Person from 1983 to 1992, where, at its close, the General Assembly adopted the Standard Rules of the Equalization of Opportunities for Persons with Disabilities. The Standard Rules encourages states to remove social, cultural, economic, educational, and political barriers that bar individuals with disabilities from participating equally in society. Proponents claim that these movements on behalf of the UN helped facilitate more inclusive development policy and brought disability rights to the forefront.\n\nCritics assert that the relationship between disability and poverty may be overstated. Cultural differences in the definition of disability, bias leading to more generous estimates on behalf of researchers, and the variability in incidences that are not accounted for between countries are all speculated to be part of this mischaracterization. These factors lead some organizations to conclude that the projection asserting 10 percent of the global population belongs to the disabled community is entirely too broad. Speculation over the projection of a 10 percent disability rate has led other independent studies to collect varying results. The World Health Organization updated their estimate to 4 percent for developing countries and 7 percent for industrialized countries. USAID maintains the initial 10 percent figure, while the United Nations works off half of that rate with a projection of 5 percent. The percentage of the world's population with disabilities remains a highly contested matter.\n\nThe argument that development should be channeled to better the agency of individuals with disabilities has been contested on several grounds. First, critics argue that development is enacted to harness potential that most individuals in this population do not possess. Second, the case that health care costs for many persons with special needs are simply too great to be shouldered by the government or NGO's has been made, especially with regard to emerging economies. Furthermore, there is no guarantee that investing in an individual's rehabilitation will result in substantial change in their agency. Lastly is the proposition of priorities. It is argued that most countries in need of extensive development must focus on health ails such as infant mortality, diarrhea, and malaria that are widespread killers not limited to a specific population.\n\nCritique with respect to potential solutions has also been made. In regards to implementing change through policy, critics have noted that the weak legal standing of United Nations' documents and the lack of resources available to aid in their implementation have resulted in a struggle to achieve the goals set forth by the General Assembly. Other studies have shown that policy on a national level has not necessarily equated to marked improvements within these countries. One such example is the United States where sociologists Esther Wilder and William Walters purport that “the employment of disabled individuals has increased only marginally since the Americans with Disabilities Act was passed.” The smaller than anticipated impact of the ADA and other policy-based initiatives is seen as a critical flaw in legislation. This is because many issues surrounding disability, namely employment discrimination, are generally reconciled through the legal system necessitating that individuals engage in the often expensive process of litigation.\n\n"}
{"id": "199433", "url": "https://en.wikipedia.org/wiki?curid=199433", "title": "Early Warning and Response System", "text": "Early Warning and Response System\n\nThe Early Warning and Response System (EWRS) for communicable diseases in the European Union was created by the European Commission to \"ensure a rapid and effective response by the EU to events (including emergencies) related to communicable diseases.\"\n\n"}
{"id": "50504513", "url": "https://en.wikipedia.org/wiki?curid=50504513", "title": "Ergonomic hazard", "text": "Ergonomic hazard\n\nErgonomic hazards are physical conditions that may pose risk of injury to the musculoskeletal system, such as the muscles or ligaments of the lower back, tendons or nerves of the hands/wrists, or bones surrounding the knees. Ergonomic hazards include things such as awkward or extreme postures, whole-body or hand/arm vibration, poorly designed tools, equipment, or workstations, repetitive motion, and poor lighting. Ergonomic hazards occur in both occupational and non-occupational settings such as in workshops, building sites, offices, home, school, or public spaces and facilities.\n"}
{"id": "5975230", "url": "https://en.wikipedia.org/wiki?curid=5975230", "title": "Ethnic bioweapon", "text": "Ethnic bioweapon\n\nAn ethnic bioweapon (\"biogenetic weapon\") is a type of theoretical bioweapon that aims to harm only or primarily people of specific ethnicities or genotypes.\n\nOne of the first modern fictional discussions of ethnic weapons is in Robert A. Heinlein's 1942 novel \"Sixth Column\" (republished as \"The Day After Tomorrow\"), in which a race-specific radiation weapon is used against a so-called \"Pan-Asian\" invader.\n\nIn 1997, U.S. Secretary of Defense William Cohen referred to the concept of an ethnic bioweapon as a possible risk. In 1998 some biological weapon experts considered such a \"genetic weapon\" plausible, and believed the former Soviet Union had undertaken some research on the influence of various substances on human genes.\n\nIn its 2000 policy paper Rebuilding America's Defenses, think-tank Project for the New American Century (PNAC) described ethnic bioweapons as a potentially \"politically useful tool\". PNAC went on to provide substantial staffing for the Bush Jr administration.\n\nThe possibility of a \"genetic bomb\" is presented in Vincent Sarich's and Frank Miele's book, \"\", published in 2004. These authors view such weapons as technically feasible but not very likely to be used. (page 248 of paperback edition.)\n\nIn 2004, \"The Guardian\" reported that the British Medical Association (BMA) considered bioweapons designed to target certain ethnic groups as a possibility, and highlighted problems that advances in science for such things as \"treatment to Alzheimer's and other debilitating diseases could also be used for malign purposes\".\n\nIn 2005, the official view of the International Committee of the Red Cross was \"The potential to target a particular ethnic group with a biological agent is probably not far off. These scenarios are not the product of the ICRC's imagination but have either occurred or been identified by countless independent and governmental experts.\"\n\nIn 2008, the US government held a congressional committee, ‘Genetics and other human modification technologies: sensible international regulation or a new kind of arms race?’, during which it was discussed how “we can anticipate a world where rogue (and even not-so-rogue) states and non-state actors attempt to manipulate human genetics in ways that will horrify us”.\n\nIn 2012, \"The Atlantic\" wrote that a specific virus that targets individuals with a specific DNA sequence is within possibility in the near future. The magazine put forward a hypothetical scenario of a virus which caused mild flu to the general population but deadly symptoms to the President of the United States. They cite advances in personalized gene therapy as evidence.\n\nIn 2016, \"Foreign Policy\" magazine suggested the possibility of a virus used as an ethnic bioweapon that could sterilize a \"genetically-related ethnic population.\"\n\nIn November 1998, \"The Sunday Times\" reported that Israel was attempting to build an \"ethno-bomb\" containing a biological agent that could specifically target genetic traits present amongst Arab populations. \"Wired News\" also reported the story, as did \"Foreign Report\".\n\nMicrobiologists and geneticists were skeptical towards the scientific plausibility of such a biological agent. The \"New York Post\", describing the claims as \"blood libel\", reported that the likely source for the story was a work of science fiction by Israeli academic Doron Stanitsky. Stanitsky had sent his completely fictional work about such a weapon to Israeli newspapers two years before. The article also noted the views of genetic researchers who claimed the idea as \"wholly fantastical\", with others claiming that the weapon was theoretically possible.\n\nA planned second installment of the article never appeared, and no sources were ever identified. Neither of the authors of the \"Sunday Times\" story, Uzi Mahnaimi and Marie Colvin, have spoken publicly on the matter.\n\nIn May 2007, a Russian newspaper \"Kommersant\" reported that the Russian government banned all exports of human biosamples.\nThe report claims that the reason for the ban was a secret FSB report about on-going development of \"genetic bioweapons\" targeting Russian population by Western institutions. The report mentions the Harvard School of Public Health, American International Health Alliance, Department of Medical Biotechnology of Jagiellonian University, United States Department of Justice Environment and Natural Resources Division, Institute of Genetics and Biotechnology Warsaw University, and United States Agency for International Development.\n\n\n"}
{"id": "54932601", "url": "https://en.wikipedia.org/wiki?curid=54932601", "title": "Femalia", "text": "Femalia\n\nFemalia is a book of 32 full-color photographs of human vulvas, edited by Joani Blank and first published by Down There Press in 1993. A reprint edition was published by Last Gasp in 2011. The photographs were taken by Tee Corinne, Michael Perry, Jill Posener, and Michael A. Rosen. The photographs are presented without commentary, except for Blank's brief introduction to the volume as a whole.\n\nThe word used as the book's title, \"\", was taken from the novel \"Vox\" by Nicholson Baker. The photographs by Corinne and Perry had been taken years before the book's original publication in 1993; those by Posener and Rosen were taken specifically for inclusion in the first edition of \"Femalia\".\n\n\"Femalia\" grew out of Blank's long-term work as a feminist sex educator. She felt that medical and pornographic images of the female genitals were inadequate to her purposes. In her introduction to the first edition, Blank lamented the absence of readily available photographic representations of the vulva other than heavily edited images in male-oriented pornography, and the resulting feeling on the part of a majority of women that \"in one way or another, their genitals are not quite ‘normal’\".\n\nFeminist authors have sharply contrasted the portrayals of vulvas in \"Femalia\" with those in typical male-oriented pornography and in biomedical sources. \"Femalia\"'s portrayals are characterized as accurate, honest, open, and truthful, as exhibiting \"stark reality\"; as promoting a positive view of the vulva; as emphasizing the diversity of the vulva in different women, as well as the diversity of opinions and perspectives about the vulva on the part of both men and women; and as emphasizing female autonomy. By contrast, portrayals of the vulva in pornography and in biomedical science are characterized as stylized and uniform, excluding women whose genitalia do not match their models. Pornographic portrayals are further characterized as commodified, and medical portrayals as sterile. Feminist sex educators have advocated perusal of the images in \"Femalia\" as an exercise to help women to regard their genitals in a more positive light.\n\nLibrarian Sanford Berman has cited \"Femalia\" as an example to illustrate his thesis that libraries engage in inappropriate self-censorship, often motivated by concerns about controversial sexual content, in deciding which books to stock. Berman comments, \"A detailed, artistic picture of a seashell adorns the cover. Were the contents strictly shell photos, the book might make it into at least some libraries. Shells, yes. Vulvas, no.\"\n\nIn a study of systematic differences in the depiction of female genitals in online pornography, anatomy textbooks, and feminist publications, \"Femalia\" was used as one of three sources of sample depictions in the feminist-publications category. This study found a statistically significant difference between online pornography and feminist publications in depicted protuberance of the labia minora, with greater mean protuberance shown in the feminist publications. It also found greater variation in measured genital proportions shown in the feminist publications than in the other two categories of sources.\n\n\"Femalia\" was used as one of two sources of sample depictions of female genitals (the other was \"Penthouse\") in a psychological study of the relationship between women's aesthetic perceptions of female genitals and their attitudes toward gynecological examinations. More specifically, the examinations in question were Pap smears, and the relevant attitudes were anxiety, embarrassment, and likelihood of making or keeping an appointment for a Pap smear.\n\nThe Royal Australian College of General Practitioners (RACGP) has published a guideline document, authored by Dr. Magdalena Simonis under authority of the RACGP, intended to inform healthcare professionals about female genital cosmetic surgery (FGCS), such as labiaplasty, and to advise them about management of patient requests for FGCS. In this document, Dr. Simonis identifies lack of appreciation of female genital diversity, not only on the part of the public but also on the part of healthcare professionals, as a contributing factor to the demand for FGCS. She advocates the use of \"Femalia\" as a tool for patient education about genital diversity, in part because it depicts female genitals without digital enhancement. Dr. Simonis has further referenced this educational use of \"Femalia\" in slide and poster presentations intended to promote better management of the demand for FGCS on the part of healthcare professionals.\n\nMedical anthropologist Eric Plemons has stated that:\nPlemons documents the use of \"Femalia\" as a resource to demonstrate the existence of female genital diversity, and to educate both clinicians and patients as to the range of normal vulval appearance. He attributes its widespread use by healthcare professionals to their belief that \"it is one of very few photographic collections of ‘normal’ vulvas that exists\".\n\n\"Femalia\" has been used as a way of assessing preferences for perineal and genital cosmetic appearance, to improve cosmesis in male-to-female transsexuals (transwomen) undergoing genital sex reassignment surgery (GSRS). Beginning in the year 2000, surgeon Neal Wilson began showing photographs from \"Femalia\" to his prospective GSRS patients and asking them to indicate which vulvas they found most aesthetically pleasing, as well as which ones they would choose for themselves. Dr. Wilson attempted to approximate through surgery the appearance of the photographs from \"Femalia\" selected by his prospective patients, even though he held that they set \"impossible standards\" because of the limitations of early 21st-century surgical technique. Dr. Wilson has republished, in an online journal article, the three photographs most often selected by his patients. He has also provided summary statistics concerning his patients′ choices of vulval photographs from \"Femalia\", as well as a short narrative summary of the specific anatomical features that he believed to be characteristic of the most popular photographs.\n\n"}
{"id": "33292706", "url": "https://en.wikipedia.org/wiki?curid=33292706", "title": "Health 3.0", "text": "Health 3.0\n\nHealth 3.0 is a health-related extension of the concept of Web 3.0 whereby the users' interface with the data and information available on the web is personalized to optimize their experience. This is based on the concept of the Semantic Web, wherein websites' data is accessible for sorting in order to tailor the presentation of information based on user preferences. Health 3.0 will use such data access to enable individuals to better retrieve and contribute to personalized health-related information within networked electronic health records, and social networking resources.\n\nHealth 3.0 has also been described as the idea of semantically organizing electronic health records to create an Open Healthcare Information Architecture. Health care could also make use of social media, and incorporate virtual tools for enhanced interactions between health care providers and consumers/patients.\n\n\nSocial networking is a popular and powerful tool for engaging patients in their health care. These virtual communities provide a real-time resource for obtaining health-related knowledge and counselling. Pew Internet and American Life Project report that greater than 90% of young adults and nearly three quarters of all Americans access the internet on a regular basis. Greater than 60% of online adults regularly access social networking resources. In addition, 80% of internet users search for health-related information. Definitive evidence of health benefit from interaction with health-related virtual communities is currently lacking as further research needs to be performed.\n\n"}
{"id": "46810902", "url": "https://en.wikipedia.org/wiki?curid=46810902", "title": "Health Care for Women International", "text": "Health Care for Women International\n\nHealth Care for Women International is a monthly peer-reviewed healthcare journal covering health care and related topics that concern women around the globe.\n\nIt is the official journal for Women's Health Issues and it is published by Taylor & Francis. Its editor-in-chief is Eleanor Krassen Covan (University of North Carolina at Wilmington).\n\nThe journal was originally titled \"Issues in Health Care of Women\" (1978–1983).\n\nThe editor-in-chief from 1983 to 2001 was Phyllis Stern (University of Pennsylvania School of Nursing).\n\nAccording to the \"Journal Citation Reports\", the journal has a 2015 impact factor of 0.950, ranking it 20th out of 40 journals in the category \"Women's Studies\" and 115th out of 153 journals in the category \"Public, Environmental & Occupational Health\".\n\n\n"}
{"id": "41384660", "url": "https://en.wikipedia.org/wiki?curid=41384660", "title": "Health information on the Internet", "text": "Health information on the Internet\n\nHealth information on the Internet refers to all communication related to health done on the Internet.\n\nAs a general communication channel, various Internet products serve every type of general health information.\n\nIn the late 1990s researchers began to note that huge numbers of people were using the Internet to seek health information, despite various problems with the quality of information or inefficiencies in accessing it. Various problems have been identified for those using search engines to seek health information.\n\nThere is a premise that patients with access to their personal health information presented in a form they can understand will be able to interpret and learn from this information in a way that benefits them. Physicians worry that patients using their own medical history records as a starting point for personal research on the Internet are at risk for being overwhelmed and misinformed when seeking health information on the Internet.\n\nAs of 2013 opinions about the relationship health care providers should have with online health information were still being established. According to one 2014 study, \"The flow of information has fundamentally changed, and physicians have less control over health information relayed to patients. Not surprisingly, this paradigm shift has elicited varied and sometimes conflicting views about the value of the Internet as a tool to improve health care.\"\n\nSocial media channels have been noted as places which physicians can visit to get insight on patient thoughts. Patients have increasingly turned to social media for health information, sometimes of dubious quality. Several studies have used social media to gather data on patients' adverse drug reactions (ADRs), with generally promising results.\n\nVarious commercial organizations use health information gathered from the Internet. The use of health information gathered from social media has been described as raising serious ethical and privacy concerns, including the risk of accidental violations of patient privacy by health care providers on social media.\n\nPhysicians have difficulty explaining complicated medical concepts to their patients and patients have difficulty understanding complicated things which physicians tell them. One reasons for this is that a patient's visit to a physician is likely to be less than 15 minutes, and in any case, physicians are unable to spend the amount of time which patients that patients typically desire. Physicians use medical terms which patients do not understand, but which they would like to learn. There is consensus that patients should have shared decision making, which means that they make informed decisions about the direction of their health care in collaboration with their physician. Rich, educated, socially advantaged patients enjoy many more benefits of shared decision making than patients who have disadvantages in getting healthcare, including lower socioeconomic class or having a minority status.\n\nLack of patient understanding of health contributes to a range of problems including tendency to not adhere to the physician's medical advice and missing medical appointments. Patients without access to health information are also more likely to use complementary and alternative medicine which is not evidence-based medicine and to fail to inform their physician that they are doing so. While some benefits can be gained by training physicians to be more efficient in serving patients, there are also benefits in training patients to be more efficient in getting benefits from physicians and there are arguments that encouraging efficient patient behavior is a powerful strategy for improving health care processes.\n\nVarious social forums exist in which anyone can have conversations about health with their peers. Such forums are especially popular among patients who seek to have conversations with other patients with a shared medical concern. Those who participate in online communities which discuss health issues report feeling relief about their health worries, perceiving more control over their health and medical condition, having more medical knowledge, and having more personal agency overall.\n\nSome research has failed to find evidence to validate physicians' concerns that patients typically receive misinformation online or using health information to inappropriately conduct self diagnosis. Patients with chronic diseases who use the Internet to get health information often acquire good skills to judge the quality of information which they find.\n\nThe written record of medical consensus is stored in scientific journals. Since the advent of electronic publishing there has been academic journal publishing reform which had a range of effects, including more researchers and physicians having greater access to professional information in medical journals through the Internet.\n\nThrough various sources both publicly and privately available, datasets containing health information about large numbers of patients are available on the Internet to an extent which was impossible to manage before the Internet.\n\nMeasures and standards for ensuring quality control on the Internet have been criticized and no one standard is universally accepted. Regardless of what kind of measure is used, much health information on the Internet is of dubious quality. Among all sources there is a wide variance in quality of health information on the Internet.\nThe nature and quality of online health-related information seeking on the web is challenging and complex and many researchers investigated this issue benefiting from a wide range of theories from different disciplines.</ref> Among all sources there is a wide variance in quality of health information on the Internet.\n\nFor many applications people wish to use health information on the Internet to give insight about a personal health concern. Because of this, the goal is often to use the Internet to find information described in a person's medical record. Since the advent of electronic media, medical records have been increasingly kept as electronic medical records. If electronic medical records were shared online then it would be easy to match those with information and conduct a range of research. However, all medical records are protected health information because sharing personal health information exposes an individual to a range of harms which result from violation of their expectation of privacy.\n\nThere is currently broad international debate about how to balance patient and commercial medicine demands for personal health information with individual's needs for safety and respect.\n\nAn electronic medical record is a medical record stored as electronic media.\n\nDe-identification is an attempt to divide a collection of information about a particular person so that all information which identifies the person is removed, and with intent to distribute whatever information is left. The closer the data is to anonymization the less valuable the data is to those who want it, so in general, data is only de-identified somewhat and rarely anonymized. There are many controversies in de-identification.\n\nThere is a large demand for access to large collections of various types of personal health information. Almost all of this demand is commercial.\n\nVarious groups have expressed worry over danger to the public which results from the distribution of collections of personal health information.\n\nIn 2014 National Health Service in the United Kingdom proposed to sell datasets of personal health information.\n\nPubMed is a free search engine accessing primarily the MEDLINE database of references and abstracts on life sciences and biomedical topics. The United States National Library of Medicine at the National Institutes of Health maintains the database.\n\nIn 2014 Wikipedia was described as \"the leading single source of healthcare information for patients\nand healthcare professionals\".\n\nIn the United States the Food and Drug Administration offers guidance for health industry organizations which share information online.\n\n medicaltipsblog.com\n\n"}
{"id": "3845894", "url": "https://en.wikipedia.org/wiki?curid=3845894", "title": "Human male sexuality", "text": "Human male sexuality\n\nHuman male sexuality covers physiological, psychological, social, cultural, and political aspects of the human male sexual response and related phenomena. It encompasses a broad range of topics involving male sexual desires and behavior that have also been addressed by ethics, morality, and religion.\n\nThere are a number of factors that influence male sexuality and sexual behaviour, including expected parental investment, and paternal presence during development.\n\nElizabeth Cashdan proposed that mate strategies among both genders differ depending on how much parental investment is expected of the male, and provided research support for her hypotheses. When men expect to provide a high level of parental investment, they will attempt to attract women by emphasising their ability to invest. In addition, men who expect to invest will be more likely to highlight their chastity and fidelity than men who expect not to invest. Men with the expectation of low parental investment will flaunt their sexuality to women. Cashdan argues the fact the research supports the idea that men expecting to invest emphasise their chastity and fidelity, which is a high cost strategy (because it lowers reproductive opportunities), suggests that that type of behaviour must be beneficial, or the behaviour would not have been selected.\n\nA relationship between the early experiences and environment of boys, and their later sexual behaviour, has been drawn by several studies. Research suggests that father absence can lead to an increase in rape behaviour. Research conducted by Malamuth found that men raised in the absence of their father (or where resources were scarce) reported more use of sexual coercion in the past, and were more likely to indicate being more willing to rape, in the event that there was no chance of them getting caught. Research has also found that parental divorce and rape correlate positively.\n\nMales who are in a committed relationship, in other words have a restricted sociosexual orientation, will have different sexual strategies compared to males who have an unrestricted sociosexual orientation. Males with a restricted sociosexual orientation will be less willing to have sex outside of their committed relationship, and adjust their strategies according to their desire for commitment and emotional closeness with their partner.\n\nIt has been found that such males are less likely to approach attractive females who have greater waist-to-hip ratios (0.68–0.72).\n\nOne study has several factors that influence the age of first sexual intercourse among both genders. Those from families with both parents present, from high socioeconomic backgrounds, who performed better at school, were more religious, who had higher parental expectations, and felt like their parents care, showed lower levels of sexual activity across all age groups in the study (age 13–18). In contrast, those with higher levels of body pride, showed higher levels of sexual activity.\n\nThere are many sexual strategies that males can employ in order to gain mates. This includes sexual coercion.\n\nSexual coercion is forcing mate choice against a partner's will or preference. Sexual coercion functions to increase the chance of a female mating with a male, and decrease the chance that the female will mate with another male. There are several strategies by which sexual coercion can be achieved. These are harassment, intimidation, and forced copulation (rape).\n\nThornhill and Palmer's \"A Natural History of Rape\" investigates the evolutionary causes of sexual coercion, particularly of rape, and suggest that such behaviour is a result of sexual selection, rather than Darwinian natural selection.\n\nOf ten listed hypotheses, they accepted two reasonable hypotheses:\n\nThornhill and Palmer argue that these two theories are the strongest of the ten for several reasons. For example, both hypothesis argue rape exists because it functions to increase matings, thus improving reproductive success. Because rape can be a costly behaviour for the male – he risks injury inflicted by the victim, or punishment by her social allies, it must have strong reproductive benefits for the behaviour to survive and be demonstrated today. Thornhill and Palmer also use several facts to support the idea that the two evolutionary based hypotheses are the most reasonable. They argue that the fact that most rape victims are of childbearing age, that married women and women of childbearing age suffer more psychological distress after rape than single or postmenopausal women, and that rape takes place in a variety of other species, all point towards an evolutionary heritage for rape behaviour.\n\nThe 'rape as a by-product' explanation holds that rape behaviour evolved as a by-product of other psychological adaptations in men to obtain many mates. This adaptation not only leads to rape but a number of other behaviours including overrating female sexual interest, a desire for sexual variety, coercion, and sexual arousal which is not dependent on the consent of mate.\n\nThe rape specific adaptation hypothesis suggests that rape is an evolved behaviour because it provides direct benefits to the rapist. In this case, the benefit would be a higher chance of reproductive success through increasing mate number. The hypothesis suggests that rape behaviour is the result of psychological mechanisms designed specifically to influence males to rape, unlike in the by-product hypothesis. This theory suggests that rape by a man which offers no chance of reproductive success, i.e. the rape of any other person who is not a female of reproductive age, is a maladaptive byproduct of this evolutionary adaptation.\n\nSupport for the idea that rape provides males with a way to increase their reproductive success comes from a study by Barbaro and Shackelford, who found that men in committed heterosexual relationships who had committed at least one act of violence/coercion towards their partner in the last month had more in-pair copulations per week.\n\nSome potential specific psychological adaptations that Thornhill and Palmer suggest might be present in men to induce rape include the evolution of a mechanism that helps males evaluate the vulnerability of potential victims, or mechanism that motivates men with a lack of sexual access to females, to rape – the mate deprivation hypothesis.\n\nThe mate deprivation hypothesis alludes to the concept that the threshold for rape is lowered in males that lack alternative reproductive options. This idea is supported by the fact that rape is disproportionately committed by men with a lower socioeconomic status. However, Malamuth found a relationship between low socioeconomic status and a rearing environment in which social relationships were not committed, which in turn resulted in a male's reduced ability to form enduring relationships in later life. This subsequently results in less alternative reproductive options. Therefore, while there is indeed a relationship between a lack of alternative reproductive options and rape behaviour, there are likely to be a number of co-morbid factors affecting this correlation, leading Thornhill and Palmer to conclude that the idea of a specific psychological adaptation that motivated men with a lack of sexual access to females is unlikely, and that further research need be conducted.\n\nOne of Thornhill and Palmer's rejected hypotheses for why men rape implicates violent pornography. Subscribers to the social science theory of rape purport that one of the main reasons why the human male learns to rape is via learning imitative behaviour when watching violent pornography. However, this fails to explain why if males are likely to imitate behaviour witnessed in violent pornography they would not also imitate the actions of human males in other videos. Furthermore, no explanation is offered into why this behaviour is inspired in some men and not others. It is also limited in its ability to predict valuable variables surrounding why rape occurs (such as who, when or where). For this reason, Thornhill and Palmer argued that \"although the removal of violent pornography may be desirable in its own right, it is very unlikely to solve the problem of rape\".\n\nAnother of their rejected hypotheses is the 'choosing victim' rape-adaptation hypothesis which suggests that there is an evolved victim-preference mechanism to maximise the reproductive benefits of rape. This hypothesis suggests that men would be most likely to rape reproductive-age females. Research shows that the age of US rape victims correlates slightly better with age of peak fertility than age of peak reproductive potential. However, this explanation does not explain the rape of those with no chance of reproductive success e.g. girls, boys, adult males, and postmenopausal women.\n\nThough it is a widely held view that sexually coercive behaviour occurs as a result of sexual selection, Smuts and Smuts (1993) proposed that sexual coercion is best described as a third type of sexual selection, rather than attempting to fit it into either of the other two forms: mate choice and intrasex competition. While sexual coercion certainly interacts with the other two forms of sexual selection, its conceptual distinction lies under the fact that a sexually coercive male may succeed in the competition for mates using coercion, despite losing in male-male competition for females, and despite not being chosen by females as a mate.\n\nCoercive behaviour of men towards women can be argued to be a result of male sexual entitlement. Gender stereotypes view men and boys as being the more typically aggressive sex. Subsequently, a man may act aggressively towards women and girls in order to increase his chances of submission from them. This is known as male sexual entitlement – the belief that women and girls owe men sex due to society viewing their sexual gratification as more important. This can result in men being more likely than women to view pressuring a woman or girl into sex as acceptable behavior. Examples of men's sexual entitlement include harassing women with thick breasts and their refusal to perform oral sex on women. Non-consensual condom removal has been described as \"a threat to [a victim's] bodily agency and as a dignitary harm\", and men who do this \" justify their actions as a natural male instinct\".\n\nMale sexual entitlement, which consequently can predict sexual entitlement due to societal norms, has been found to predict rape-related attitudes and behaviors. If men feel that their own sexual needs are more important, it is likely that they will have rape-related attitudes, as such, attitudes reinforce their own sexual entitlement as being the more dominant sex.\n\nSexual strategies are essential to males when pursuing a mate in order to maximize reproductive potential, in order for their genes to be passed on to future generations. However, in order for a male's sexual strategy to succeed with a female, it is the male who must compromise his own sexual strategies, typically because of uncertainty over the paternity of a child, whereas maternity is essentially certain.\n\nWomen have higher levels of parental investment because they carry the developing child, and higher confidence in their maternity since they witness giving birth to the child. Hence women have reason to accept greater responsibility for raising their children. By comparison, males have no objective way of being certain that the child they are raising is biologically theirs. Because of this difference, males have to adapt their own sexual strategies to accommodate the strategies of the females around them.\n\nAmong other behaviors, this means that men are more likely to favour chastity in a woman, as this way a male can be more certain that her offspring are his own. Such a strategy is seen in males, and maternity is never doubted by the female, and so a chaste male is not highly valued by women. However, for men, female chastity confirms paternity, causing the male to compromise his sexual strategies in order to select a chaste mate.\n\nHomoerotic behaviour differs from homosexuality (see below) in that it is purely same-sex sexual behaviour that occurs for pleasure, whereas homosexuality is the sexual orientation or enduring sexual preference for the same-sex. Due to its universality, history and perceived functions it has been theorised that homoerotic behaviour has origins in evolution.\n\nThere is evidence of the long-standing existence of homoeroticism, dating back to early human history. From cave paintings of men engaging in sexual acts to modern history, homoerotic behaviour is still prevalent today.\n\nFrom an evolutionary perspective homoeroticism is seen as counter-productive as it doesn't directly contribute to successfully producing offspring. However, male-male sexual behaviour has been argued to have served an adaptive function and an indirect reproductive advantage for males. Evidence suggests that male-male sexual relations in early human periods often occurred between younger adolescent boys and older males. Sexual acts have been viewed as a psychological factor in societies used for bonding. These same-sex relations between young adolescent boys and older men brought many benefits to the younger males, such as access to food, protection from aggression and overall helping them attain personal survival and an increased social standing. These direct effects on survival also led to indirect effects of reproductive success. The advantages the young males would obtain from their sexual relations with older men made them a more desired mating choice amongst females. The age and status difference between the men involved, suggests that a dominance-submission dynamic was an important factor in these relations.\n\nThe Alliance theory perspective of male-male sexual behaviour in early humans states that this behaviour was a feature that developed to reduce aggression between different males and to enforce alliances. It is believed that young adult males and adolescents were segregated from society and living on the outskirts of communities due to their perceived sexual threat by the older men. Therefore, same-sex behaviour allowed younger men to have reinforced alliances with other older males, which later gained them access to resources and females which were both scarce at the time. Similarly, Kirkpatrick states male-male sexual behaviour has occurred in part because of the reciprocal-altruism hypothesis. The older male receives sexual gratification from the relationship whilst the younger male has to bear the cost of engaging in non-reproductve sex. However, the younger male is able to later receive the social benefits discussed, through this same-sex alliance. This relationship can be viewed as a resource exchange.\n\nIn support of the evolutionary perspective, much of modern history demonstrates higher and lower status roles between two men involved in sexual relations. There is evidence of males seducing each other for social gain as well as sexual pleasure. Examples of this in modern history include Roman Emperors such as Augustus Caesar, who supposedly acquired the throne in part due to their sexual relations with their predecessors. Additionally, the ancient Greek custom of pederasty provides additional support for the evolutionary account. It was very common for adult males and adolescent males in ancient Greece to engage in sexual relations. Similarly to relationships found in early humans who displayed homoeroticism, the relationship dynamic between males involved in pederasty in the ancient Greek period was unequal. These young males also received benefits such as increased social networks and educational development.\n\nHomoerotic behaviour has been thought to be maintained by indirect selection, since it does not encourage reproduction. The kin-selection hypothesis, which argued that homosexuals contribute to their nephews' and nieces' survival, and the female fertility hypothesis, were both findings which support the idea that homoerotic behaviour is an evolutionary by-product that serves no beneficial function by itself (for discussion see the section on homosexuality, below).\n\nRelatively newer studies suggest that similar to how heterosexual bonds provide non-conceptive benefits, including the maintenance of long-term bonds, homoerotic behaviour aid in same-sex alliances that help in resource competition or defense. Emotions that are homosexual in nature could help to foster and reinforce supportive relationships, one example of which would be the Azande society in which homosexual relationships were very common, and the Sambia, who engage in homoerotic behaviour between the initiates in their militia, and their behavior buttress bonds that were important in survival.\n\nIn various societies, many individuals exhibit homoerotic behaviour during certain stages of their life, notably during adolescence, and generally before their heterosexual marriage, possibly because that same-sex alliances are more important in one's early life than later, when the concern for sexual reproduction comes into play, and individuals who engage in homoerotic acts obtain benefits applicable to their reproductive lives. Before that period of their life, same-sex alliances are important in aiding survival, and among the Q'eqchi' of Belize, significantly more children survive past six months for men with same-sex alliance due to the increase in productivity of agricultural labour.\n\nSame-sex alliances do not need to be sexual in nature, although when competition for partners is especially severe the sexualisation of same-sex alliances occurs more often. Displays of commitment between partners are adaptive because of the cost in terms of efforts invested in maintaining the alliance. Sex could be argued as a type of currency in long-term relationships, and signify to an individual's partner and to others a prominent level of connection and commitment. Homosexual/homoerotic behaviour would be therefore a significant representation of one's loyalty and affiliation in a same-sex alliance. Ultimately, homoerotic behaviour is not selectively disadvantaged, as homoerotic behaviour does not result in a net decrease to an individual's reproductive success, and the attraction to other individuals of same sex and the behaviour as result of that attraction is not contrary or alternative to the attraction to people of the other sex.\n\nSubsequent research in the role of homoerotic behaviour further supports the \"affiliation hypothesis\". A study published in 2014 sought to measure homoerotic motivation, and to investigate how an affiliative context would affect homoerotic motivation in men, and found that men in an affiliative priming condition are more open to engaging in homoerotic behaviour. This effect is most pronounced in men with high progesterone, a hormone that is associated with affiliative motivation in humans. In spite of the opportunity costs homoerotic behaviour and motivation were thought to incur, the results provide data constituting evidence that homoerotic motivation, and subsequently homoerotic behaviour, holds the adaptive function of encouraging alliance formation and bonding.\n\nThe Western \"homosexual\" category has been related to the non-Western \"third gender\" category, being cast as a redefinition and expansion of the latter category to include all biological males who acknowledge having same-sex attractions (instead of only effeminate males). This extension of \"third gender\" is due to various factors that were unique to the Western world, including the widespread influence of Christianity and the resultant encouragement of opposite-sex relationships. Before the concept of sexual orientation was developed in the modern West, only effeminate males who sought to be anally penetrated by men (oral sex was far less common than today) were seen as a belonging to a different gender category. The Western equivalent of the third-genders (and not all men with same-sex attractions) were the ones who started and propagated the Western concept of a homosexual identity.\n\nMany non-Western societies show hostility towards the concept of homosexuality, which they view as a pernicious Western practice and a legacy of colonialism and (Western) sexual tourism. However, and strangely to Western eyes, such societies do accept both men who have sex with men and third-genders who have sex with men as an unremarkable part of society, so long as they're not called \"homosexuals\".\n\nIn the West, a man often cannot acknowledge or display sexual attraction for another man without the homosexual or bisexual label being attached to him. The same pattern of shunning the homosexual identity, while still having sex with men, is prevalent in the non-West, where sexual attraction between men is often seen as a universal male phenomenon—and practiced, either quietly or openly—even if held morally wrong in the larger society, sexual attraction between men being seen as a universal male quality, not something limited to a minority.\n\nIn the 1860s, German third-gender Karl Heinrich Ulrichs coined a new term for third-genders that he called \"urnings\", which was supposed to mean \"men who like men\". These \"urnings\" were \"females inside male bodies\", who were emotionally or sexually attracted to men. Ulrichs and most self-declared members of the third sex thought that masculine men can never have sexual desires for other men. Hence, to be attracted to men, a male necessarily had to be feminine-gendered – had to have a female inside him. This was supported by Ulrichs' own experience, as well as by the fact that men only had sex with men secretively, due to the cultural climate. Ulrichs termed ordinary men (as opposed to third-genders) as \"diones\", meaning \"men who like women.\"\n\nLater, Austrian third-gender and human rights activist Karl Maria Kertbeny coined the terms \"homosexual\" and \"heterosexual\". For most of this period, these terms were popular only amongst the third-gender and scientific communities, the latter of which was developing the concept of homosexuality as a mental disorder.\n\nThus was born the idea of \"men who like men\" being different from \"men who like women\", and of differentiating male sexuality between \"heterosexuality\" and \"homosexuality\". The basis for the division, however, remained gender orientation (masculinity and femininity). Men who were now decidedly \"heterosexual\", however, rarely related to these terms; they saw themselves as neither heterosexual or homosexual. Even in 2010, \"straight\" men in the West, quite like men in the East, seldom relate strongly to sexual identities. These identities, however, remain a strong focus within the LGBT community.\n\n"}
{"id": "31094752", "url": "https://en.wikipedia.org/wiki?curid=31094752", "title": "Hypersexual disorder", "text": "Hypersexual disorder\n\nHypersexual disorder is a pattern of behavior involving intense preoccupation with sexual fantasies, urges and activities, leading to adverse consequences and clinically significant distress or impairment in social, occupational or other important functions. It was proposed in 2010 for inclusion in the Diagnostic and Statistical Manual of Mental Disorders Fifth Edition (DSM-5) of the American Psychiatric Association (APA). \n\nPeople with hypersexual disorder experience multiple, unsuccessful attempts to control or diminish the amount of time spent engaging in sexual fantasies, urges, and behaviors in response to dysphoric mood states or stressful life events. \n\nFor a valid diagnosis of hypersexual disorder to be established, symptoms must persist for a period of at least 6 months and occur independently of a use mania or a medical condition.\n\nHypersexual disorder was recommended for inclusion in the DSM-5 (Diagnostic and Statistical Manual of Mental Disorders, Fifth Edition) by the Sexual and Gender Identity Disorders Workgroup (Emerging Measures and Models, Conditions for Further Study). It was ultimately not approved. The term \"hypersexual disorder\" was reportedly chosen because it did not imply any specific theory for the causes of hypersexuality, which remain unknown. A proposal to add sexual addiction to the DSM system had been previously rejected by the APA, as not enough evidence suggested to them that the condition is analogous to substance addictions, as that name would imply.\n\nRory Reid, a research psychologist in the Department of Psychiatry at the University of California Los Angeles (UCLA), led a team of researchers to investigate the proposed criteria for Hypersexual Disorder. Their findings were published in the \"Journal of Sexual Medicine\" where they concluded that the given criteria is valid and the disorder could be reliably diagnosed. \n\nThe DSM-IV-TR, published in 2000, includes an entry called \"Sexual Disorder—Not Otherwise Specified\" (Sexual Disorder NOS), for disorders that are clinically significant but do not have code. The DSM-IV-TR notes that Sexual Disorder NOS would apply to, among other conditions, \"distress about a pattern of repeated sexual relationships involving a succession of lovers who are experienced by the individual only as things to be used\".\n\n"}
{"id": "220255", "url": "https://en.wikipedia.org/wiki?curid=220255", "title": "Hypersexuality", "text": "Hypersexuality\n\nHypersexuality is a clinical diagnosis used by mental healthcare professionals for extremely frequent or suddenly increased libido. Nymphomania and satyriasis were terms previously used for the condition, in women and men respectively.\n\nHypersexuality may be a primary condition, or the symptom of another medical disease or condition, for example Klüver-Bucy syndrome or bipolar disorder. Hypersexuality may also present as a side effect of medication such as drugs used to treat Parkinson's disease, or through the administering of hormones such as testosterone and estrogen during hormone therapy.\nClinicians have yet to reach a consensus over how best to describe hypersexuality as a primary condition, or to determine the appropriateness of describing such behaviors and impulses as a separate pathology.\n\nHypersexual behaviours are viewed variously by clinicians and therapists as a type of obsessive-compulsive disorder (OCD) or \"OCD-spectrum disorder\", an addiction, or a disorder of impulsivity. A number of authors do not acknowledge such a pathology and instead assert that the condition merely reflects a cultural dislike of exceptional sexual behavior.\n\nConsistent with there not being any consensus over what causes hypersexuality, authors have used many different labels to refer to it, sometimes interchangeably, but often depending on which theory they favor or which specific behavior they were studying. Contemporary names include compulsive masturbation, compulsive sexual behavior, cybersex addiction, erotomania, \"excessive sexual drive\", hyperphilia, hypersexuality, hypersexual disorder, problematic hypersexuality, sexual addiction, sexual compulsivity, sexual dependency, sexual impulsivity, \"out of control sexual behavior\", and paraphilia-related disorder.\n\nThe \"Merriam-Webster Dictionary\" defines \"hypersexual\" as \"exhibiting unusual or excessive concern with or indulgence in sexual activity\". Sexologists have been using the term \"hypersexuality\" since the late 1800s, when Krafft-Ebing described several cases of extreme sexual behaviours in his seminal 1886 book, \"Psychopathia Sexualis.\" The author used the term \"hypersexuality\" to describe conditions that would now be termed premature ejaculation. Terms to describe males with the condition include \"donjuanist\", \"satyromaniac\",\"satyriac\" and \"satyriasist\", for women \"clitoromaniac\", \"nympho\" and \"nymphomaniac\", for teleiophilic heterosexual women \"andromaniac\", while \"hypersexualist\", \"sexaholic\", \"onanist\", \"hyperphiliac\" and \"erotomaniac\" are gender neutral terms.\n\nOther, mostly historical, names include Don Juanism, the Messalina complex, sexaholism hyperlibido and furor uterinus.\n\nThere is little consensus among experts as to the causes of hypersexuality. Some research suggests that some cases can be linked to biochemical or physiological changes that accompany dementia. Psychological needs also complicate the biological explanation, which identifies the temporal/frontal lobe of the brain as the area for regulating libido. Persons suffering from injuries to this part of the brain are at increased risk for aggressive behavior and other behavioral problems including personality changes and socially inappropriate sexual behavior such as hypersexuality. The same symptom can occur after unilateral temporal lobotomy. There are other biological factors that are associated with hypersexuality such as premenstrual changes, and the exposure to virilising hormones in childhood or in utero.\n\nIn research involving use of antiandrogens to reduce undesirable sexual behaviour such as hypersexuality, testosterone has been found to be necessary, but not sufficient, for sexual drive. Other proposed factors include a lack of physical closeness, and forgetfulness of the recent past.\n\nPathogenic overactivity of the dopaminergic mesolimbic pathway in the brain—forming either psychiatrically, during mania, or pharmacologically, as a side effect of dopamine agonists, specifically D-preferring agonists—is associated with various addictions and has been shown to result among some in overindulgent, sometimes hypersexual, behavior.\n\nThe American Association for Sex Addiction Therapy acknowledges biological factors as contributing causes of sex addiction. Other associated factors include psychological components (which affect mood and motivation as well as psychomotoric and cognitive functions), spiritual control, mood disorders, sexual trauma, and intimacy anorexia as causes or type of sex addiction.\n\nHypersexuality is known to present itself as a symptom in connection to a number of mental and neurological disorders. Some people with borderline personality disorder (sometimes referred to as BPD) can be markedly impulsive, seductive, and extremely sexual. Sexual promiscuity, sexual obsessions, and hypersexuality are very common symptoms for both men and women with BPD. On occasion for some there can be extreme forms of paraphilic drives and desires. \"Borderline\" patients, due in the opinion of some to the use of splitting, experience love and sexuality in unstable ways.\n\nPeople with bipolar disorder may often display tremendous swings in sex drive depending on their mood. As defined in the DSM-IV-TR, hypersexuality can be a symptom of hypomania or mania in bipolar disorder or schizoaffective disorder. Pick's disease causes damage to the temporal/frontal lobe of the brain; people with Pick's disease show a range of socially inappropriate behaviors.\n\nSeveral neurological conditions such as Alzheimer's disease, autism, various types of brain injury, Klüver–Bucy syndrome, Kleine–Levin syndrome, and many more neurodegenerative diseases can cause hypersexual behavior. Sexually inappropriate behavior has been shown to occur in 7-8% of Alzheimer's patients living at home, at a care facility or in a hospital setting. Hypersexuality has also been reported to result as a side-effect of some medications used to treat Parkinson's disease. Some street drugs, such as methamphetamine, may also contribute to hypersexual behavior.\n\nA positive link between the severity of dementia and occurrence of inappropriate behavior has also been found. Hypersexuality can be caused by dementia in a number of ways, including disinhibition due to organic disease, misreading of social cues, understimulation, the persistence of learned sexual behaviour after other behaviours have been lost, and the side-effects of the drugs used to treat dementia. Other possible causes of dementia-related hypersexuality include an inappropriately expressed psychological need for intimacy and forgetfulness of the recent past. As this illness progresses, increasing hypersexuality has been theorized to sometimes compensate for declining self-esteem and cognitive function.\n\nSymptoms of hypersexuality are also similar to those of sexual addiction in that they embody similar traits. These symptoms include the inability to be intimate (intimacy anorexia), depression and bipolar disorders. The resulting hypersexuality may have an impact in the person's social and occupational domains if the underlying symptoms have a large enough systemic influence.\n\n, a proposal to add \"Sexual Addiction\" to the Diagnostic and Statistical Manual of Mental Disorders (DSM) system has failed to get support of the American Psychiatric Association (APA). The DSM does include an entry called Sexual Disorder Not Otherwise Specified (Sexual Disorder NOS) to apply to, among other conditions, \"distress about a pattern of repeated sexual relationships involving a succession of lovers who are experienced by the individual only as things to be used\".\n\nThe International Statistical Classification of Diseases and Related Health Problems (ICD-10) of the World Health Organization (WHO), includes two relevant entries. One is \"Excessive Sexual Drive\" (coded F52.7), which is divided into satyriasis for males and nymphomania for females. The other is \"Excessive Masturbation\" or \"Onanism (excessive)\" (coded F98.8).\n\nSome authors have questioned whether it makes sense to discuss hypersexuality at all, arguing that labeling sexual urges \"extreme\" merely stigmatizes people who do not conform to the norms of their culture or peer group.\n\nThe ICD-11 has created a new condition classification, compulsive sexual behavior, to cover \" a persistent pattern of failure to control intense, repetitive sexual impulses or urges resulting in repetitive sexual behaviour\".\nHypersexuality may negatively impact an individual. The concept of hypersexuality as an addiction was started in the 1970s by former members of Alcoholics Anonymous who felt they experienced a similar lack of control and compulsivity with sexual behaviors as with alcohol. Multiple 12-step style self-help groups now exist for people who identify as sex addicts, including Sex Addicts Anonymous, Sexaholics Anonymous, Sex and Love Addicts Anonymous, and Sexual Compulsives Anonymous. Some hypersexuals may treat their condition with the usage of medication (such as Cyproterone acetate) or any foods considered to be anaphrodisiacs. Other hypersexuals may choose a route of consultation, such as psychotherapy, self-help groups or counselling.\n\n"}
{"id": "54779466", "url": "https://en.wikipedia.org/wiki?curid=54779466", "title": "Infections associated with diseases", "text": "Infections associated with diseases\n\nInfections associated with diseases are those that are associated with possible infectious etiologies, that meet the requirements of Koch's postulates. Other methods of causation are described by the Bradford Hill criteria and Evidence-based medicine. Koch's postulates have been altered by some epidemiologists based upon sequence-based detection of distinctive pathogenic nucleic acid sequences in tissue samples. Using this method, absolute statements are not always possible regarding causation. Since this is true, higher amounts of distinctive pathogenic nucleic acid sequences would be in those exhibiting disease compared to controls since inoculating those without the pathogen is unethical. In addition, the DNA load should drop or become lower with the resolution of the disease. The distinctive pathogenic nucleic acid sequences load should also increase upon recurrence.\n\nOther conditions are met to establish cause or association including studies in disease transmission. This means that there should be a high disease occurrence in those carrying an pathogen, evidence of a serologicalresponse to the pathogen, and the success of vaccination prevention. Direct visualization of the pathogen, the identification of different strains, immunological responses in the host, how the infection is spread and, the combination of these should all be taken into account to determine the probability that an infectious agent is the cause of the disease. A conclusive determination of a causal role of an infectious agent for in a particular disease using Koch's postulates is desired yet this might not be possible.\n\nThe leading cause of death worldwide is cardiovascular disease, but infectious diseases are the second leading cause of death worldwide and the leading cause of death in infants and children.\n\nOther causes or associations of disease are: a compromised immune system, environmental toxins, radiation exposure, diet and lifestyle choices, stress, and genetics. Diseases may also be multifactorial, requiring multiple factors to induce disease. For example: in a murine model, Crohn's disease can be precipitated by a norovirus, but only when both a specific gene variant is present and a certain toxin has damaged the gut.\n\nA list of the more common and well-known diseases associated with infectious pathogens is provided and is not intended to be a complete listing.\n\nInfectious pathogen-associated diseases include many of the most common and costly chronic illnesses. The treatment of chronic diseases accounts for 75% of all US healthcare costs (amounting to $1.7 trillion in 2009).\n\nOne of first examples of systematic study of disease causation was Avicenna, in the tenth century. The history of infection and disease were observed in the 1800s and related to the one of the tick-borne diseases, Rocky Mountain spotted fever. The cause of viral encephalitis was discovered in Russia based upon epidemiological clustering of cases. The virus causing this illness was isolated in 1937. The rash typical of Lyme borreliosis was identified the early 1900s. Historically, some chronic diseases were linked or associated with infectious pathogens.\n"}
{"id": "22862133", "url": "https://en.wikipedia.org/wiki?curid=22862133", "title": "Institutional syndrome", "text": "Institutional syndrome\n\nIn clinical and abnormal psychology, institutionalization or institutional syndrome refers to deficits or disabilities in social and life skills, which develop after a person has spent a long period living in mental hospitals, prisons, or other remote institutions. In other words, individuals in institutions may be deprived (whether unintentionally or not) of independence and of responsibility, to the point that once they return to \"outside life\" they are often unable to manage many of its demands; it has also been argued that institutionalized individuals become psychologically more prone to mental health problems.\n\nThe term \"institutionalization\" can also be used to describe the process of committing an individual to a mental hospital or prison or to institutional syndrome; thus the phrase \"X is institutionalized\" may mean either that X has been placed in an institution, or that X is suffering the psychological effects of having been in an institution for an extended period of time.\n\nIn Europe and North America, the trend of putting the mentally ill into mental hospitals began as early as the 17th century, and hospitals often focused more on \"restraining\" or controlling inmates than on curing them, although hospital conditions improved somewhat with movements for human treatment, such as moral management. By the mid-20th century, overcrowding in institutions, the failure of institutional treatment to cure most mental illnesses, and the advent of drugs such as Thorazine prompted many hospitals to begin discharging patients in large numbers, in the beginning of the deinstitutionalization movement (the process of gradually moving people from inpatient care in mental hospitals, to outpatient care).\n\nDeinstitutionalization did not always result in better treatment, however, and in many ways it helped reveal some of the shortcomings of institutional care, as discharged patients were often unable to take care of themselves, and many ended up homeless or in jail. In other words, many of these patients had become \"institutionalized\" and were unable to adjust to independent living. One of the first studies to address the issue of institutionalization directly was Russell Barton's 1959 book \"Institutional Neurosis,\" which claimed that many symptoms of mental illness (specifically, psychosis) were not physical brain defects as once thought, but were consequences of institutions' \"stripping\" (a term probably first used in this context by Erving Goffman) away the \"psychological crutches\" of their patients.\n\nSince the middle of the 20th century, the problem of institutionalization has been one of the motivating factors for the increasing popularity of deinstitutionalization and the growth of community mental health services, since some mental healthcare providers believe that institutional care may create as many problems as it solves.\n\nRomanian children who suffered from severe neglect at a young age were adopted by families. Research reveals that the post-institutional syndrome occurring in these children gave rise to symptoms of autistic behavior. Studies done on eight Romanian adoptees living in the Netherlands revealed that about one third of the children exhibited behavioral and communication problems resembling that of autism.\n\nIndividuals who suffer from institutional syndrome can face several kinds of difficulties upon returning to the community. The lack of independence and responsibility for patients within institutions, along with the 'depressing' and 'dehumanizing' environment, can make it difficult for patients to live and work independently. Furthermore, the experience of being in an institution may often have exacerbated individuals' illness: proponents of labeling theory claim that individuals who are socially \"labeled\" as mentally ill suffer stigmatization and alienation that lead to psychological damage and a lessening of self-esteem, and thus that being placed in a mental health institution can actually cause individuals to become more mentally ill.\n\n"}
{"id": "31974159", "url": "https://en.wikipedia.org/wiki?curid=31974159", "title": "List of nutrition guides", "text": "List of nutrition guides\n\nThis is a list of nutrition guides. A nutrition guide is a reference that provides nutrition advice for general health, typically by dividing foods into food groups and recommending servings of each group. Nutrition guides can be presented in written or visual form, and are commonly published by government agencies, health associations and university health departments.\n\nMost countries also have nutrition facts labels which are not listed here; many of those reference specific target amounts for various nutrients.\n\nThe Hippocratic Corpus of Ancient Greece contains one of the earliest known nutrition guides. It recommends a seasonal diet. For winter, it advises eating a heavy diet of bread and roasted meat and fish, while avoiding vegetables and restricting liquids to, if anything, strong wine. It then recommends a lighter summer diet of soft barley cake, vegetables, boiled meat, and large quantities of diluted wine. Gradual transitions between these two diets are advised in the intervening months.\n\nDuring the Tang Dynasty, Chinese physician Sun Simiao is believed to have written the first nutrition guide in traditional Chinese medicine. In his book, \"Precious Prescriptions for Emergencies\" (), the chapter \"Dietary Treatment\" () contains sections describing the effects of eating fruits, vegetables, grains and animals.\n\nAmid high food prices in 1972, Sweden's National Board of Health and Welfare developed the idea of \"basic foods\" that were both cheap and nutritious, and \"supplemental foods\" that added nutrition missing from the basic foods. KF, a consumer co-op that worked with the Board, sought to illustrate these food groups. KF developed a food pyramid because it could depict basic foods as its base, and introduced the guide to the public in 1974 in their magazine, \"\". At the base were bread, cereals, potatoes, milk, cheese and margarine; above it was a large section of supplemental vegetables and fruit; and at the top was an apex of supplemental meat, fish and eggs. The pyramid competed with the National Board's \"dietary circle,\" which KF saw as problematic for resembling a cake divided into seven slices, and for not indicating how much of each food should be eaten. While the Board distanced itself from the pyramid, KF continued to promote it, and food pyramids were developed in other Scandinavian countries, plus West Germany, Japan and Sri Lanka. The United States later developed its first food pyramid in 1992.\n\nToday, both the Swedish government and KF have moved to the Plate Model.\n\nThe Australian Department of Health and Ageing publishes The Australian Guide to Healthy Eating, which features a wheel divided into five sections: approximately 40 percent bread, cereals, rice, pasta and noodles; 30 percent vegetables and legumes; 10 percent fruit; 10 percent milk, yogurt and cheese; and 10 percent lean meat, fish, poultry, eggs, nuts and legumes. Below the wheel are reminders to drink plenty of water and eat fats and sweets occasionally or in small amounts. More specific recommendations are provided based on age, gender, life stage and activity level.\n\nThe Austrian Federal Ministry of Health uses The Austrian Food Pyramid (), which is divided into 25 blocks, each block representing a daily serving from a food group. Starting at the base, there are six servings of non-alcoholic beverages (preferably low-energy drinks like water, tea, unsweetened fruit juice and vegetable juice); three servings of vegetables and legumes; two of fruit; four of cereals, bread, pasta, rice and potatoes (preferably whole grains); three of milk and dairy (two \"white,\" like milk, yogurt or cream cheese, and one of yellow cheese); one of fats and oil (preferably olive or canola oil); three of meat, fish, sausage and eggs (preferably fish or lean meat); and three of fatty, sweet and salty foods (though not nutritionally recommended and to be consumed rarely).\n\nThe Voedingsdriehoek is a (mostly in Flanders) widely used tool for dietitians, health educators, schools, etc. to explain a balanced diet.\nThe triangle is constructed as a pyramid with five different layers. The bottom layer is the largest in area, so the most important, the top is narrow and there represents little food, although it is an essential ingredient. The layers are (from top to bottom):\n- Oil / fats\n- Protein: meat, fish, eggs, dairy products, meat substitutes\n- Fruits and vegetables\n- Starch or carbohydrates (bread, potatoes, rice, pasta, etc.)\n- Drinks\nIn 2005 a (rather symbolically) layer was added at the bottom: sports and exercise. This is not a food, but an important part of a healthy diet and lifestyle. The information services use the \"active\" food pyramid.\n\nBrazil's Ministry of Health publishes the Food Guide (), which features a map of Brazil depicting each of Brazil's five regions as puzzle pieces representing a food group. However, Brazil's food recommendations differ from the map, and feature eight food groups: rice, bread, pasta, potato and cassavas (5 servings a day); vegetables and legumes (3 servings); fruits (3); milk, cheese and yogurt (3); beans (1); meat, fish and eggs (1); oils and fats (1); and sugars and sweets (no more than one).\n\nCanada's Food Guide from Health Canada is illustrated with a rainbow of four food groups: vegetables and fruit (7 to 10 servings a day for adults, depending on biological sex), grain products (6 to 8), milk and alternatives (2), and meat and alternatives (2 to 3). More specific recommendations are provided based on age, gender and life stage.\n\nCanada developed its first nutrition guide in 1942 as part of its wartime nutrition program.\n\nChina's Ministry of Health uses the Balanced Diet Pagoda (), which is divided into five stories ascending from largest to smallest. Cereals are at the large base; topped by vegetables and fruits; then fish, poultry, meat, eggs and other animal foods; followed by milk and soy foods; and topped with fats and oils in the small spire. Beside the pagoda are images representing water and exercise.\n\nDenmark's Food Administration uses the Diet Compass () to depict its \"Dietary 8\" () guidelines, with an image on each compass point representing a guideline. Those guidelines are: \"Eat 6 fruits and vegetables a day,\" \"Eat fish and seafood several times a week,\" \"Eat potatoes, rice or pasta and whole wheat bread every day,\" \"Cut back on sugar,\" \"Cut back on greasy foods,\" \"Eat a varied diet and maintain a normal weight,\" \"Quench your thirst with water,\" and, \"Be physically active at least 30 minutes a day.\"\n\nFrance's has 25 separate food guides under the title The Food Guide For All (). Each guide is based on a consumer's personality, habits and lifestyle. There are guides for people who: are health-conscious, don't care about food, prepare family meals, are poor, eat at restaurants, skip meals, don't cook, have large appetites, are on diets, prefer certain foods, avoid various foods (including meat), are sedentary, are athletic, and more. Each guide features a \"portrait\" in the image of a happy face designed out of foods representing that consumer's type. The portraits are billed as nutrition recommendations, though they are more decorative than informative.\n\nGermany's state-funded uses the Food Pyramid (), which divides a pyramid into 22 blocks, each block representing a hand-sized serving from a food group. Starting at the base, there are six blocks for beverages, three for vegetables, two for fruit, four for grains, three for dairy, one for meat and fish, one for oils, one for fats, and one for sweets and alcohol. Beverages, vegetables, fruit and grains are marked green for \"free travel\"; meat and dairy are marked yellow for \"caution\"; and oils, fats, sweets and alcohol are marked red for \"brake lights\". Aid also collaborated with the German Nutrition Society to create a 3D pyramid model.\n\nIndia's National Institute of Nutrition publishes the Dietary Guidelines for Indians, which, among other diagrams, includes the Food Pyramid. The pyramid has a base of beans and legumes to eat adequately, a second layer of vegetables and fruit to eat liberally, a third layer of meat, fish, eggs and oils to eat moderately, and an apex of fatty, salty and sugary foods to eat sparingly. Accompanying the pyramid is a recommendation of regular exercise and physical activity, as well as warnings against drinking alcohol and smoking.\n\nThe island of Ireland's Food Safety Promotion Board uses The Food Pyramid, which is divided into five levels: bread, cereals and potatoes at the large base (6 or more servings); then fruit and vegetables (5); followed by milk, cheese and yogurt (3); then meat, fish, eggs and alternatives (2); and finally fats, high fat/sugar snacks, foods and drinks at the apex (small amounts). At least 8 cups of water a day are also recommended.\n\nThe Food Pyramid () from Israel's Ministry of Health is divided into six levels. At the wide base is water; followed by starches, including pasta, bread, corn and yams; then fruits and vegetables; then meat, fish, eggs and dairy; then fats and oils; and finally sugary foods at the small apex. Images around the pyramid represent exercise.\n\nThe Italian Ministry of Health uses the Italian Food Pyramid (), which is divided into squares, triangles, trapezoids and rectangles that represent one, one-half, one-and-a-half, and two servings, respectively. Beginning from the base, the divisions are: fruit (2 servings), water (1), vegetables (2), bread (1.5), biscotti (0.5), potatoes (0.5), rice and pasta (1.5), meat (1), eggs (0.5), fish (0.5), legumes (0.5), cold cut meats (0.5), milk (0.5), dairy (0.5), oils and fats (1), sweets (0.5) and wine or beer (0.5). Along the edges of the pyramid are triangles representing physical activity. The pyramid is intended to represent the variety of foods eaten over an entire week, averaged into daily portions. Also provided is an alternative, more traditional pyramid for a single day, divided into six layers representing six food groups; as well as two additional pyramids for children.\n\nThe Food Guide (), from Japan's Health and Agriculture ministries, is depicted as a spinning top with a wide upper layer tapering to a narrow bottom. At the large upper level is a staple meal of carbohydrates, including rice, bread and noodles (5 to 7 servings a day); followed below by a side dish of vegetables, potatoes, beans (except soybeans), mushrooms and seaweed (5 to 6); then a smaller main course of protein, including meat, fish, eggs and soy (3 to 5); and finally the narrow point, divided between dairy (2) and fruit (2). The top's handle is filled with \"essential\" water and tea, and running in circles around the handle is a figure representing moderate exercise. A small motion line beside the top represents sweet beverages in moderation.\n\nMexico's Department of Nutrition and Health Promotion uses The Plate of Good Eating (), which is divided into thirds: vegetables and fruits (in equal proportion); cereals; and legumes and animal products. The guide further recommends eating \"many\" vegetables and fruits and \"enough\" cereals. The legume and animal products section is subdivided, and \"few\" animal products are recommended, while the guide recommends combining the larger legume section with cereals. Additional recommendations are to include each of the three food groups in each of the three main meals of the day, to eat the greatest possible variety of food, to eat according to individual needs and conditions, and to eat the least possible fats, oils, sugar and salt.\n\nThe Netherlands Nutrition Center uses the Wheel of Five (), which is divided into five groups: approximately 30 percent vegetables and fruit; 30 percent bread, cereals, potatoes, rice, pasta, couscous, and legumes; 16 percent dairy, meat, fish, eggs and meat substitutes; 16 percent beverages; and 8 percent fats and oils. More specific recommendations are provided based on age and gender.\n\nSouth Korea's Korean Nutrition Society uses the Food Bicycle (), with a small front wheel filled with water and a large rear wheel composed of approximately one-third grains; 20 percent meat, fish, eggs and beans; 20 percent vegetables; 12 percent fruits; 12 percent dairy; and 3 percent oils and sugars. A person is pedaling the bicycle, representing exercise. More specific recommendations are provided based on age, gender and life stage.\n\nBefore 2010, South Korea used a very similar to China's current guide.\n\nSpain's Ministry of Health, Social Policy and Equality uses the NAOS Pyramid (), which promotes a Mediterranean diet as well as plenty of physical activity, and is drawn as a sailboat on water. It is divided into beverages and water at the base; an equal division between physical activity and a combination of grains, vegetables, tubers, fruit, olive oil and dairy in the second level, which is labeled \"several times a day\" and color-coded green; an equal division between sports and a combination of meat, fish, eggs, legumes and nuts in the third level, which is labeled \"several times a week\" and is color-coded orange; and an apex of saturated fats, sugars, salt, and sedentary activity labeled \"occasionally\" and color-coded red.\n\nSweden's Food Agency uses the Plate Model (), which divides a plate into approximately 40 percent potatoes, rice, pasta and bread; 40 percent vegetables and fruit; and 20 percent meat, fish, eggs and legumes. People requiring more energy are allowed a larger share of carbohydrates, while people who are overweight can make up to half their plate vegetables and fruit.\n\nPreviously, Sweden used the Dietary Circle created in 1963, while Swedish consumer cooperative KF created the world's first food pyramid in 1974.\n\nSwitzerland's Federal Office of Public Health uses the Food Pyramid () developed by the Swiss Society for Nutrition. At the base is 1 to 2 liters of liquids, preferably non-sugared beverages; then three servings of vegetables and two servings of fruit of different colors; followed by whole grains, beans, cereals and potatoes to be eaten with each meal; then three servings of milk or dairy; one serving of meat, fish, eggs, cheese or another protein; oils, fats and nuts daily in moderation; and an apex of sweet/salty snacks and sweetened/alcoholic drinks in moderation for pleasure.\n\nTurkey's Ministry of Health uses the Basic Food Groups (), a four-part division of milk and dairy; meat, eggs, fish, legumes and seeds; vegetables and fruit; and bread and cereal. Each food group is accompanied by bullet points, such as serving recommendations or advice to eat more raw vegetables and whole grains.\n\nThe United Kingdom's Department of Health published Dietary Reference Values. These are equivalent to the easier to understand Eatwell plate used by the National Health Service. This consists of roughly one-third fruit and vegetables (\"at least 5 portions\"); one-third bread, rice, potatoes and pasta (\"plenty\"); a smaller amount of milk and dairy (\"some\"); meat, fish, eggs and beans in the same proportion as dairy (\"some\"); and a small wedge of fatty and sugary foods.\n\nIn the United States, the Department of Agriculture uses MyPlate, a plate icon divided into approximately 30 percent grains, 30 percent vegetables, 20 percent fruits and 20 percent protein, accompanied by a smaller circle representing dairy. Additional recommendations include to enjoy food but eat less, avoid oversized portions, make at least half of grains whole, switch to fat-free or low-fat milk, choose foods with less sodium, and drink water instead of sugary drinks.\n\nPrevious USDA food guides include the Basic 7 (1943–1956), the Basic Four (1956–1992), the Food Guide Pyramid (1992–2005), and MyPyramid (2005–2011).\n\nThe National Institutes of Health uses the Dietary Approaches to Stop Hypertension (DASH) Eating Plan for people seeking to lower their blood pressure. DASH differs from MyPlate in that the protein category is replaced by a smaller proportion of lean meats, poultry, and fish; there are separate sections for fats and oils, legumes, and sweets; and fruits and vegetables don't constitute half of the diet.\n\nThe Center for Nutrition Policy and Promotion in the USDA and the United States Department of Health and Human Services jointly release a longer textual document called \"Dietary Guidelines for Americans\", updated in 2015 with the next scheduled revision in 2012.\n\nDietary Reference Intake values are published by the Institute of Medicine for general use, but nutrition facts labels in the U.S. use the older Reference Daily Intake standards instead.\n\nThe World Health Organization uses The 3 Fives, a guide featuring five key points in each of three categories. The three categories are safer food, a healthy diet, and appropriate physical activity. In the healthy diet category, the five keys are: \"Give your baby only breast milk for the first 6 months of life,\" \"Eat a variety of food,\" \"Eat plenty of vegetables and fruit,\" \"Eat moderate amounts of fats and oil,\" and \"Eat less salt and sugar.\" Each key includes bullet points with further recommendations. The 3 Fives was originally developed for distribution at major sporting events like the Olympics and the World Cup, but it can also be used for general audiences.\n\nBefore The 3 Fives, the WHO used the CINDI dietary guide, which included the CINDI pyramid. The guide was intended to help European health professionals and policymakers develop their own national nutrition guides. The pyramid has a large green base representing approximately two-thirds of the triangle's area, which is filled with vegetables, fruits, grains and starches. A middle layer shaded orange for \"caution\" is divided into two equal sections: low-fat milk and dairy; and beans, lentils, legumes, fish, eggs, poultry and lean meat. At the top is a small \"red zone\" of fats and sweets.\n\nFAO provides technical assistance to countries for developing, revising and implementing food-based dietary guidelines (FBDG) and food guides in line with current scientific evidence. FAO also carries out periodic reviews on progress made in the development and use of dietary guidelines. FAO's food based dietary guidelines website collects more than 80 national guidelines.\n\n"}
{"id": "23463344", "url": "https://en.wikipedia.org/wiki?curid=23463344", "title": "Lone worker monitoring", "text": "Lone worker monitoring\n\nLone worker monitoring is the practice of monitoring the safety of employees who may be exposed to risk due to work conditions in which they are located out of sight and sound from a person who may be able to offer aid in the event of an emergency.\n\nIn some areas including the United Kingdom Australian States and certain Provinces in Canada, legislation has driven the adoption of lone worker policies as well as methods of monitoring the safety of these employees. In the United States, no explicit legislation exists regarding an employer's obligation to monitor the safety of its lone or isolated employees except in the shipbuilding industry.\n\nNumerous methods have been developed and are currently in use by companies world-wide. These methods include:\n\n\n"}
{"id": "19051", "url": "https://en.wikipedia.org/wiki?curid=19051", "title": "Manganese", "text": "Manganese\n\nManganese is a chemical element with symbol Mn and atomic number 25. It is not found as a free element in nature; it is often found in minerals in combination with iron. Manganese is a metal with important industrial metal alloy uses, particularly in stainless steels.\n\nHistorically, manganese is named for pyrolusite and other black minerals from the region of Magnesia in Greece, which also gave its name to magnesium and the iron ore magnetite. By the mid-18th century, Swedish-German chemist Carl Wilhelm Scheele had used pyrolusite to produce chlorine. Scheele and others were aware that pyrolusite (now known to be manganese dioxide) contained a new element, but they were unable to isolate it. Johan Gottlieb Gahn was the first to isolate an impure sample of manganese metal in 1774, which he did by reducing the dioxide with carbon.\n\nManganese phosphating is used for rust and corrosion prevention on steel. Ionized manganese is used industrially as pigments of various colors, which depend on the oxidation state of the ions. The permanganates of alkali and alkaline earth metals are powerful oxidizers. Manganese dioxide is used as the cathode (electron acceptor) material in zinc-carbon and alkaline batteries.\n\nIn biology, manganese(II) ions function as cofactors for a large variety of enzymes with many functions. Manganese enzymes are particularly essential in detoxification of superoxide free radicals in organisms that must deal with elemental oxygen. Manganese also functions in the oxygen-evolving complex of photosynthetic plants. While the element is a required trace mineral for all known living organisms, it also acts as a neurotoxin in larger amounts. Especially through inhalation, it can cause manganism, a condition in mammals leading to neurological damage that is sometimes irreversible.\n\nManganese is a silvery-gray metal that resembles iron. It is hard and very brittle, difficult to fuse, but easy to oxidize. Manganese metal and its common ions are paramagnetic. Manganese tarnishes slowly in air and oxidizes (\"rusts\") like iron in water containing dissolved oxygen.\n\nNaturally occurring manganese is composed of one stable isotope, Mn. Eighteen radioisotopes have been isolated and described, ranging in atomic weight from 46 u (Mn) to 65 u (Mn). The most stable are Mn with a half-life of 3.7 million years, Mn with a half-life of 312.3 days, and Mn with a half-life of 5.591 days. All of the remaining radioactive isotopes have half-lives of less than three hours, and the majority of less than one minute. The primary decay mode before the most abundant stable isotope, Mn, is electron capture and the primary mode after is beta decay. Manganese also has three meta states.\n\nManganese is part of the iron group of elements, which are thought to be synthesized in large stars shortly before the supernova explosion. Mn decays to Cr with a half-life of 3.7 million years. Because of its relatively short half-life, Mn is relatively rare, produced by cosmic rays impact on iron. Manganese isotopic contents are typically combined with chromium isotopic contents and have found application in isotope geology and radiometric dating. Mn–Cr isotopic ratios reinforce the evidence from Al and Pd for the early history of the solar system. Variations in Cr/Cr and Mn/Cr ratios from several meteorites suggest an initial Mn/Mn ratio, which indicates that Mn–Cr isotopic composition must result from \"in situ\" decay of Mn in differentiated planetary bodies. Hence, Mn provides additional evidence for nucleosynthetic processes immediately before coalescence of the solar system.\n\nThe most common oxidation states of manganese are +2, +3, +4, +6, and +7, though all oxidation states from −3 to +7 have been observed. Mn often competes with Mg in biological systems. Manganese compounds where manganese is in oxidation state +7, which are mostly restricted to the unstable oxide MnO, compounds of the intensely purple permanganate anion MnO, and a few oxyhalides (MnOF and MnOCl), are powerful oxidizing agents. Compounds with oxidation states +5 (blue) and +6 (green) are strong oxidizing agents and are vulnerable to disproportionation.\nThe most stable oxidation state for manganese is +2, which has a pale pink color, and many manganese(II) compounds are known, such as manganese(II) sulfate (MnSO) and manganese(II) chloride (MnCl). This oxidation state is also seen in the mineral rhodochrosite (manganese(II) carbonate). Manganese(II) most commonly exists with a high spin, S = 5/2 ground state because of the high pairing energy for manganese(II). However, there are a few examples of low-spin, S =1/2 manganese(II). There are no spin-allowed d–d transitions in manganese(II), explaining why manganese(II) compounds are typically pale to colorless.\n\nThe +3 oxidation state is known in compounds like manganese(III) acetate, but these are quite powerful oxidizing agents and also prone to disproportionation in solution, forming manganese(II) and manganese(IV). Solid compounds of manganese(III) are characterized by its strong purple-red color and a preference for distorted octahedral coordination resulting from the Jahn-Teller effect.\n\nThe oxidation state +5 can be produced by dissolving manganese dioxide in molten sodium nitrite. Manganate (VI) salts can be produced by dissolving Mn compounds, such as manganese dioxide, in molten alkali while exposed to air. Permanganate (+7 oxidation state) compounds are purple, and can give glass a violet color. Potassium permanganate, sodium permanganate, and barium permanganate are all potent oxidizers. Potassium permanganate, also called Condy's crystals, is a commonly used laboratory reagent because of its oxidizing properties; it is used as a topical medicine (for example, in the treatment of fish diseases). Solutions of potassium permanganate were among the first stains and fixatives to be used in the preparation of biological cells and tissues for electron microscopy.\n\nThe origin of the name manganese is complex. In ancient times, two black minerals from Magnesia (located within modern Greece) were both called \"magnes\" from their place of origin, but were thought to differ in gender. The male \"magnes\" attracted iron, and was the iron ore now known as lodestone or magnetite, and which probably gave us the term magnet. The female \"magnes\" ore did not attract iron, but was used to decolorize glass. This feminine \"magnes\" was later called \"magnesia\", known now in modern times as pyrolusite or manganese dioxide. Neither this mineral nor elemental manganese is magnetic. In the 16th century, manganese dioxide was called \"manganesum\" (note the two Ns instead of one) by glassmakers, possibly as a corruption and concatenation of two words, since alchemists and glassmakers eventually had to differentiate a \"magnesia negra\" (the black ore) from \"magnesia alba\" (a white ore, also from Magnesia, also useful in glassmaking). Michele Mercati called magnesia negra \"manganesa\", and finally the metal isolated from it became known as \"manganese\" (German: \"Mangan\"). The name \"magnesia\" eventually was then used to refer only to the white magnesia alba (magnesium oxide), which provided the name magnesium for the free element when it was isolated much later.\n\nSeveral colorful oxides of manganese, for example manganese dioxide, are abundant in nature and have been used as pigments since the Stone Age. The cave paintings in Gargas that are 30,000 to 24,000 years old contain manganese pigments.\n\nManganese compounds were used by Egyptian and Roman glassmakers, either to add to, or remove color from glass. Use as \"glassmakers soap\" continued through the Middle Ages until modern times and is evident in 14th-century glass from Venice.\n\nBecause it was used in glassmaking, manganese dioxide was available for experiments by alchemists, the first chemists. Ignatius Gottfried Kaim (1770) and Johann Glauber (17th century) discovered that manganese dioxide could be converted to permanganate, a useful laboratory reagent. By the mid-18th century, the Swedish chemist Carl Wilhelm Scheele used manganese dioxide to produce chlorine. First, hydrochloric acid, or a mixture of dilute sulfuric acid and sodium chloride was made to react with manganese dioxide, later hydrochloric acid from the Leblanc process was used and the manganese dioxide was recycled by the Weldon process. The production of chlorine and hypochlorite bleaching agents was a large consumer of manganese ores.\n\nScheele and other chemists were aware that manganese dioxide contained a new element, but they were not able to isolate it. Johan Gottlieb Gahn was the first to isolate an impure sample of manganese metal in 1774, by reducing the dioxide with carbon.\n\nThe manganese content of some iron ores used in Greece led to speculations that steel produced from that ore contains additional manganese, making the Spartan steel exceptionally hard. Around the beginning of the 19th century, manganese was used in steelmaking and several patents were granted. In 1816, it was documented that iron alloyed with manganese was harder but not more brittle. In 1837, British academic James Couper noted an association between miners' heavy exposure to manganese with a form of Parkinson's disease. In 1912, United States patents were granted for protecting firearms against rust and corrosion with manganese phosphate electrochemical conversion coatings, and the process has seen widespread use ever since.\n\nThe invention of the Leclanché cell in 1866 and the subsequent improvement of batteries containing manganese dioxide as cathodic depolarizer increased the demand for manganese dioxide. Until the development of batteries with nickel-cadmium and lithium, most batteries contained manganese. The zinc-carbon battery and the alkaline battery normally use industrially produced manganese dioxide because naturally occurring manganese dioxide contains impurities. In the 20th century, manganese dioxide was widely used as the cathodic for commercial disposable dry batteries of both the standard (zinc-carbon) and alkaline types.\n\nManganese comprises about 1000 ppm (0.1%) of the Earth's crust, the 12th most abundant of the crust's elements. Soil contains 7–9000 ppm of manganese with an average of 440 ppm. Seawater has only 10 ppm manganese and the atmosphere contains 0.01 µg/m. Manganese occurs principally as pyrolusite (MnO), braunite, (MnMn)(SiO), psilomelane (Ba,HO)MnO, and to a lesser extent as rhodochrosite (MnCO).\n\nThe most important manganese ore is pyrolusite (MnO). Other economically important manganese ores usually show a close spatial relation to the iron ores. Land-based resources are large but irregularly distributed. About 80% of the known world manganese resources are in South Africa; other important manganese deposits are in Ukraine, Australia, India, China, Gabon and Brazil. According to 1978 estimate, the ocean floor has 500 billion tons of manganese nodules. Attempts to find economically viable methods of harvesting manganese nodules were abandoned in the 1970s. The CIA once used mining manganese modules on the ocean floor as a cover story for recovering a sunken Soviet submarine.\n\nIn South Africa, most identified deposits are located near Hotazel in the Northern Cape Province, with a 2011 estimate of 15 billion tons. In 2011 South Africa produced 3.4 million tons, topping all other nations.\n\nManganese is mainly mined in South Africa, Australia, China, Gabon, Brazil, India, Kazakhstan, Ghana, Ukraine and Malaysia. US Import Sources (1998–2001): Manganese ore: Gabon, 70%; South Africa, 10%; Australia, 9%; Mexico, 5%; and other, 6%. Ferromanganese: South Africa, 47%; France, 22%; Mexico, 8%; Australia, 8%; and other, 15%. Manganese contained in all manganese imports: South Africa, 31%; Gabon, 21%; Australia, 13%; Mexico, 8%; and other, 27%.\n\nFor the production of ferromanganese, the manganese ore is mixed with iron ore and carbon, and then reduced either in a blast furnace or in an electric arc furnace. The resulting ferromanganese has a manganese content of 30 to 80%. Pure manganese used for the production of iron-free alloys is produced by leaching manganese ore with sulfuric acid and a subsequent electrowinning process.\nA more progressive extraction process involves directly reducing manganese ore in a heap leach. This is done by percolating natural gas through the bottom of the heap; the natural gas provides the heat (needs to be at least 850 °C) and the reducing agent (carbon monoxide). This reduces all of the manganese ore to manganese oxide (MnO), which is a leachable form. The ore then travels through a grinding circuit to reduce the particle size of the ore to between 150–250 μm, increasing the surface area to aid leaching. The ore is then added to a leach tank of sulfuric acid and ferrous iron (Fe) in a 1.6:1 ratio. The iron reacts with the manganese dioxide to form iron hydroxide and elemental manganese. This process yields approximately 92% recovery of the manganese. For further purification, the manganese can then be sent to an electrowinning facility.\n\nIn 1972 the CIA's Project Azorian, through billionaire Howard Hughes, commissioned the ship \"Hughes Glomar Explorer\" with the cover story of harvesting manganese nodules from the sea floor. That triggered a rush of activity to collect manganese nodules, which was not actually practical. The real mission of \"Hughes Glomar Explorer\" was to raise a sunken Soviet submarine, the K-129, with the goal of retrieving Soviet code books.\n\nManganese has no satisfactory substitute in its major applications in metallurgy. In minor applications, (e.g., manganese phosphating), zinc and sometimes vanadium are viable substitutes.\n\nManganese is essential to iron and steel production by virtue of its sulfur-fixing, deoxidizing, and alloying properties, as first recognized by the British metallurgist Robert Forester Mushet (1811–1891) who, in 1856, introduced the element, in the form of Spiegeleisen, into steel for the specific purpose of removing excess dissolved oxygen, sulfur, and phosphorus in order to improve its malleability. Steelmaking, including its ironmaking component, has accounted for most manganese demand, presently in the range of 85% to 90% of the total demand. Manganese is a key component of low-cost stainless steel. Often ferromanganese (usually about 80% manganese) is the intermediate in modern processes.\n\nSmall amounts of manganese improve the workability of steel at high temperatures by forming a high-melting sulfide and preventing the formation of a liquid iron sulfide at the grain boundaries. If the manganese content reaches 4%, the embrittlement of the steel becomes a dominant feature. The embrittlement decreases at higher manganese concentrations and reaches an acceptable level at 8%. Steel containing 8 to 15% of manganese has a high tensile strength of up to 863 MPa. Steel with 12% manganese was discovered in 1882 by Robert Hadfield and is still known as Hadfield steel (mangalloy). It was used for British military steel helmets and later by the U.S. military.\n\nThe second largest application for manganese is in aluminium alloys. Aluminium with roughly 1.5% manganese has increased resistance to corrosion through grains that absorb impurities which would lead to galvanic corrosion. The corrosion-resistant aluminium alloys 3004 and 3104 (0.8 to 1.5% manganese) are used for most beverage cans. Before 2000, more than 1.6 million tonnes of those alloys were used; at 1% manganese, this consumed 16,000 tonnes of manganese.\n\nMethylcyclopentadienyl manganese tricarbonyl is used as an additive in unleaded gasoline to boost octane rating and reduce engine knocking. The manganese in this unusual organometallic compound is in the +1 oxidation state.\n\nManganese(IV) oxide (manganese dioxide, MnO) is used as a reagent in organic chemistry for the oxidation of benzylic alcohols (where the hydroxyl group is adjacent to an aromatic ring). Manganese dioxide has been used since antiquity to oxidize and neutralize the greenish tinge in glass from trace amounts of iron contamination. MnO is also used in the manufacture of oxygen and chlorine and in drying black paints. In some preparations, it is a brown pigment for paint and is a constituent of natural umber.\n\nManganese(IV) oxide was used in the original type of dry cell battery as an electron acceptor from zinc, and is the blackish material in carbon–zinc type flashlight cells. The manganese dioxide is reduced to the manganese oxide-hydroxide MnO(OH) during discharging, preventing the formation of hydrogen at the anode of the battery.\n\nThe same material also functions in newer alkaline batteries (usually battery cells), which use the same basic reaction, but a different electrolyte mixture. In 2002, more than 230,000 tons of manganese dioxide was used for this purpose.\n\nThe metal is occasionally used in coins; until 2000, the only United States coin to use manganese was the from 1942 to 1945. An alloy of 75% copper and 25% nickel was traditionally used for the production of nickel coins. However, because of shortage of nickel metal during the war, it was substituted by more available silver and manganese, thus resulting in an alloy of 56% copper, 35% silver and 9% manganese. Since 2000, dollar coins, for example the Sacagawea dollar and the Presidential $1 coins, are made from a brass containing 7% of manganese with a pure copper core. In both cases of nickel and dollar, the use of manganese in the coin was to duplicate the electromagnetic properties of a previous identically sized and valued coin in the mechanisms of vending machines. In the case of the later U.S. dollar coins, the manganese alloy was intended to duplicate the properties of the copper/nickel alloy used in the previous Susan B. Anthony dollar. \n\nManganese compounds have been used as pigments and for the coloring of ceramics and glass. The brown color of ceramic is sometimes the result of manganese compounds. In the glass industry, manganese compounds are used for two effects. Manganese(III) reacts with iron(II) to induce a strong green color in glass by forming less-colored iron(III) and slightly pink manganese(II), compensating for the residual color of the iron(III). Larger quantities of manganese are used to produce pink colored glass.\n\nTetravalent manganese is used as an activator in red-emitting phosphors. While many compounds are known which show luminescence the majority is not used in commercial application due to low efficiency or deep red emission. However, several Mn aktivated fluorides were reported as potential red emitting phorshors for warm-white LEDs. But to this day, only KSiF:Mn is commercially available for use in warm-white LEDs.\n\nThe manganese oxide is also used in portland cement mixtures.\n\nManganese is an important element for human health, essential for development, metabolism, and the antioxidant system. Nevertheless, excessive exposure or intake may lead to a condition known as manganism, a neurodegenerative disorder that causes dopaminergic neuronal death and symptoms similar to Parkinson's disease. The classes of enzymes that have manganese cofactors is large and includes oxidoreductases, transferases, hydrolases, lyases, isomerases, ligases, lectins, and integrins. The reverse transcriptases of many retroviruses (though not lentiviruses such as HIV) contain manganese. The best-known manganese-containing polypeptides may be arginase, the diphtheria toxin, and Mn-containing superoxide dismutase (Mn-SOD).\n\nMn-SOD is the type of SOD present in eukaryotic mitochondria, and also in most bacteria (this fact is in keeping with the bacterial-origin theory of mitochondria). The Mn-SOD enzyme is probably one of the most ancient, for nearly all organisms living in the presence of oxygen use it to deal with the toxic effects of superoxide (), formed from the 1-electron reduction of dioxygen. The exceptions, which are all bacteria, include \"Lactobacillus plantarum\" and related lactobacilli, which use a different nonenzymatic mechanism with manganese (Mn) ions complexed with polyphosphate, suggesting a path of evolution for this function in aerobic life.\n\nThe human body contains about 12 mg of manganese, mostly in the bones. The soft tissue remainder is concentrated in the liver and kidneys. In the human brain, the manganese is bound to manganese metalloproteins, most notably glutamine synthetase in astrocytes.\n\nManganese is also important in photosynthetic oxygen evolution in chloroplasts in plants. The oxygen-evolving complex (OEC) is a part of photosystem II contained in the thylakoid membranes of chloroplasts; it is responsible for the terminal photooxidation of water during the light reactions of photosynthesis, and has a metalloenzyme core containing four atoms of manganese. To fulfill this requirement, most broad-spectrum plant fertilizers contain manganese.\n\nThe U.S. Institute of Medicine (IOM) updated Estimated Average Requirements (EARs) and Recommended Dietary Allowances (RDAs) for minerals in 2001. For manganese there was not sufficient information to set EARs and RDAs, so needs are described as estimates for Adequate Intakes (AIs). As for safety, the IOM sets Tolerable upper intake levels (ULs) for vitamins and minerals when evidence is sufficient. In the case of manganese the adult UL is set at 11 mg/day. Collectively the EARs, RDAs, AIs and ULs are referred to as Dietary Reference Intakes (DRIs). Manganese deficiency is rare.\n\nThe European Food Safety Authority (EFSA) refers to the collective set of information as Dietary Reference Values, with Population Reference Intake (PRI) instead of RDA, and Average Requirement instead of EAR. AI and UL defined the same as in United States. For people ages 15 and older the AI is set at 3.0 mg/day. AIs for pregnancy and lactation is 3.0 mg/day. For children ages 1–14 years the AIs increase with age from 0.5 to 2.0 mg/day. The adult AIs are higher than the U.S. RDAs. The EFSA reviewed the same safety question and decided that there was insufficient information to set a UL.\n\nFor U.S. food and dietary supplement labeling purposes the amount in a serving is expressed as a percent of Daily Value (%DV). For manganese labeling purposes 100% of the Daily Value was 2.0 mg, but as of May 27, 2016 it was revised to 2.3 mg to bring it into agreement with the RDA. A table of the old and new adult Daily Values is provided at Reference Daily Intake. Food and supplement companies have until January 1, 2020 to comply with the change.\n\nManganese compounds are less toxic than those of other widespread metals, such as nickel and copper. However, exposure to manganese dusts and fumes should not exceed the ceiling value of 5 mg/m even for short periods because of its toxicity level. Manganese poisoning has been linked to impaired motor skills and cognitive disorders.\n\nThe permanganate exhibits a higher toxicity than the manganese(II) compounds. The fatal dose is about 10 g, and several fatal intoxications have occurred. The strong oxidative effect leads to necrosis of the mucous membrane. For example, the esophagus is affected if the permanganate is swallowed. Only a limited amount is absorbed by the intestines, but this small amount shows severe effects on the kidneys and on the liver.\n\nManganese exposure in United States is regulated by the Occupational Safety and Health Administration (OSHA). People can be exposed to manganese in the workplace by breathing it in or swallowing it. OSHA has set the legal limit (permissible exposure limit) for manganese exposure in the workplace as 5 mg/m over an 8-hour workday. The National Institute for Occupational Safety and Health (NIOSH) has set a recommended exposure limit (REL) of 1 mg/m over an 8-hour workday and a short term limit of 3 mg/m. At levels of 500 mg/m, manganese is immediately dangerous to life and health.\n\nGenerally, exposure to ambient Mn air concentrations in excess of 5 μg Mn/m3 can lead to Mn-induced symptoms. Increased ferroportin protein expression in human embryonic kidney (HEK293) cells is associated with decreased intracellular Mn concentration and attenuated cytotoxicity, characterized by the reversal of Mn-reduced glutamate uptake and diminished lactate dehydrogenase leakage.\n\nWaterborne manganese has a greater bioavailability than dietary manganese. According to results from a 2010 study, higher levels of exposure to manganese in drinking water are associated with increased intellectual impairment and reduced intelligence quotients in school-age children. It is hypothesized that long-term exposure due to inhaling the naturally occurring manganese in shower water puts up to 8.7 million Americans at risk. However, data indicates that the human body can recover from certain adverse effects of overexposure to manganese if the exposure is stopped and the body can clear the excess.\n\nMethylcyclopentadienyl manganese tricarbonyl (MMT) is a gasoline additive used to replace lead compounds for unleaded gasolines to improve the octane rating of low octane petroleum distillates. It reduces engine knock agent through the action of the carbonyl groups. Fuels containing manganese tend to form manganese carbides, which damage exhaust valves. Compared to 1953, levels of manganese in air have dropped. Many racing competitions specifically ban manganese compounds in racing fuel for carts and minibikes. MMT contains 24.4–25.2% manganese. Elevated atmospheric manganese concentrations are strongly correlated with automobile traffic density. The Level of manganese emitted by MMT fuels has been found to be safe for the general population and vulnerable groups such as infants and the elderly by the EPA and European and Canadian environmental agencies.\n\nThe tobacco plant readily absorbs and accumulates heavy metals such as manganese from the surrounding soil into its leaves. These are subsequently inhaled during tobacco smoking. While manganese is a constituent of tobacco smoke, studies have largely concluded that concentrations are not hazardous for human health.\n\nManganese overexposure is most frequently associated with manganism, a rare neurological disorder associated with excessive manganese ingestion or inhalation. Historically, persons employed in the production or processing of manganese alloys have been at risk for developing manganism; however, current health and safety regulations protect workers in developed nations. The disorder was first described in 1837 by British academic John Couper, who studied two patients who were manganese grinders.\n\nManganism is a biphasic disorder. In its early stages, an intoxicated person may experience depression, mood swings, compulsive behaviors, and psychosis. Early neurological symptoms give way to late-stage manganism, which resembles Parkinson's disease. Symptoms include weakness, monotone and slowed speech, an expressionless face, tremor, forward-leaning gait, inability to walk backwards without falling, rigidity, and general problems with dexterity, gait and balance. Unlike Parkinson's disease, manganism is not associated with loss of the sense of smell and patients are typically unresponsive to treatment with L-DOPA. Symptoms of late-stage manganism become more severe over time even if the source of exposure is removed and brain manganese levels return to normal.\n\nChronic manganese exposure has been shown to produce a parkinsonism-like illness characterized by movement abnormalities. This condition is not responsive to typical therapies used in the treatment of PD, suggesting an alternative pathway than the typical dopaminergic loss within the substantia nigra. Manganese may accumulate in the basal ganglia, leading to the abnormal movements. A mutation of the SLC30A10 gene, a manganese efflux transporter necessary for decreasing intracellular Mn, has been linked with the development of this Parkinsonism-like disease. The Lewy bodies typical to PD are not seen in Mn-induced parkinsonism.\n\nAnimal experiments have given the opportunity to examine the consequences of manganese overexposure under controlled conditions. In (non-aggressive) rats, manganese induces mouse-killing behavior.\n\nSeveral recent studies attempt to examine the effects of chronic low-dose manganese overexposure on child development. The earliest study was conducted in the Chinese province of Shanxi. Drinking water there had been contaminated through improper sewage irrigation and contained 240–350 µg Mn/L. Although Mn concentrations at or below 300 µg Mn/L were considered safe at the time of the study by the US EPA and 400 µg Mn/L by the World Health Organization, the 92 children sampled (between 11 and 13 years of age) from this province displayed lower performance on tests of manual dexterity and rapidity, short-term memory, and visual identification, compared to children from an uncontaminated area. More recently, a study of 10-year-old children in Bangladesh showed a relationship between Mn concentration in well water and diminished IQ scores. A third study conducted in Quebec examined school children between the ages of 6 and 15 living in homes that received water from a well containing 610 µg Mn/L; controls lived in homes that received water from a 160 µg Mn/L well. Children in the experimental group showed increased hyperactive and oppositional behavior.\n\nThe current maximum safe concentration under EPA rules is 50 µg Mn/L.\n\nA protein called DMT1 is the major transporter in manganese absorption from the intestine, and may be the major transporter of manganese across the blood–brain barrier. DMT1 also transports inhaled manganese across the nasal epithelium. The proposed mechanism for manganese toxicity is that dysregulation leads to oxidative stress, mitochondrial dysfunction, glutamate-mediated excitoxicity, and aggregation of proteins.\n\n\n"}
{"id": "3057007", "url": "https://en.wikipedia.org/wiki?curid=3057007", "title": "Medical care ratio", "text": "Medical care ratio\n\nMedical care ratio (MCR), also known as medical cost ratio, medical loss ratio, and medical benefit ratio, is a metric used in managed health care and health insurance to measure medical costs as a percentage of premium revenues. It is a type of loss ratio, which is a common metric in insurance measuring the percentage of premiums paid out in claims rather than expenses and profit provision. It is calculated by dividing those premiums allocated for fully insured or self-funded health care coverage into the total expenses for inpatient, professional (physicians and other licensed providers), outpatient, and pharmacy. (Briefly, MCR = Costs/Premiums.) As a general rule, a medical cost ratio of 85% or less is desirable. Some insurers now call MCR \"benefit–cost ratio\" (BCR). In the United States, the term is \"medical loss ratio\".\n"}
{"id": "12571217", "url": "https://en.wikipedia.org/wiki?curid=12571217", "title": "Medical food", "text": "Medical food\n\nMedical foods are foods that are specially formulated and intended for the dietary management of a disease that has distinctive nutritional needs that cannot be met by normal diet alone. In the United States they were defined in the Food and Drug Administration's 1988 Orphan Drug Act Amendments and are subject to the general food and safety labeling requirements of the Federal Food, Drug, and Cosmetic Act. In Europe the European Food Safety Authority established definitions for \"foods for special medical purposes\" (FSMPs) in 2015.\n\nMedical foods, called \"food for special medical purposes\" in Europe, are distinct from the broader category of foods for special dietary use, from traditional foods that bear a health claim, and from dietary supplements. In order to be considered a medical food the product must, at a minimum:\n\nMedical foods can be classified into the following categories:\n\nMedical foods are regulated by the US Food and Drug Administration under the Food Drug and Cosmetic Act regulations. 21 CFR 101.9(j) (8).\n\nThe term medical food, as defined in section 5(b) of the Orphan Drug Act (21 U.S.C. 360ee (b) (3)) is \"a food which is formulated to be consumed or administered enterally under the supervision of a physician and which is intended for the specific dietary management of a disease or condition for which distinctive nutritional requirements, based on recognized scientific principles, are established by medical evaluation.\"\n\nMedical foods are not required to undergo premarket review or approval by FDA. Additionally, they are exempted from the labeling requirements for health claims and nutrient content claims under the Nutrition Labeling and Education Act of 1990. In 2016 the FDA published an update: Guidance for Industry: Frequently Asked Questions About Medical Foods; Second Edition. Definitions and labeling requirements are included.\n\n\n"}
{"id": "32315242", "url": "https://en.wikipedia.org/wiki?curid=32315242", "title": "Mental health in the Middle East", "text": "Mental health in the Middle East\n\nThe study of mental health in the Middle East is an area of research that continues to grow in its scope and content. In 1998, the World Mental Health Survey Initiative was conducted by the World Federation for Mental Health. The (WFMH) was originally created in 1948 and works in concert with the World Health Organization (WHO). The 1998 survey sought to help define and clarify mental health Issues across many societies.\n\nTo accurately evaluate and understand the mental health issues of the Middle East, one must take into account the geographic, historical, cultural, and social influences of that part of the world. While each of the many countries commonly considered part of the \"Middle East\" is unique, there is a binding ethnic fabric that should be considered. Firstly, the Middle East is the origin of many of the major world religions. Christianity, Judaism and Islam, all began in this region. Of these many religions, Islam has had the most lasting and culturally significant influence on the region, encompassing well over ninety percent (90%) of the population by some measurements. The tenets of the Islamic faith, and its strict purpose, certainly have served as both a guide and a hindrance to the practices' of mental health care providers in the Middle East. There is a conflict between ancient religious teachings and the modern day or \"Western world\" approach to the issues of mental health.\n\nMental health in the Middle East, from Pharaonic times through to the Islamic Renaissance, has a rich and complex history. During Pharaonic times, soma and psyche were terms used to define mental disorders, and such disorders were described as problems of the heart or uterus, as stated in Eber's and Kahoun's Papyri. While mystical culture predominated at that time, mental disorders were treated on a somatic basis. In the Islamic era, those with mental illness were not known to have endured any forms of torture, nor were they ostracized; this was due to the belief that possession by a good Muslim genie was possible. The first mental hospital in Europe was located in Spain, as discussed by author Paul Ghalioungui, following the Moorish invasions. Mental hospitals then expanded to other European countries. The occurrence of anxiety, schizophrenia, depression, suicide, conversion disorders, and obsessive compulsive disorders had been reported, and these disorders were treated with some success.\n\nMorality and culture most likely arose from a collective choice of communal decisions to provide an accepted structure of communal living. Geography, tradition, and the specific environment may have been most influential in these decisions. The development or evolution of a social milieu integrated with the culmination of culture. Cultural morality has provided a way of managing conflict within a societal group. Cultural morality and the required behaviors that are cooperative in nature and a cognizance of others within the group, ostracized unhealthy behaviors, and therefore encouraged emotions and actions beneficial to society. These realized constructs provided an outlet and model of motivations and accepted actions within a social group.\nIn a 1992 study, Schwartz, Roccas and Sagiv evaluated how priorities are displayed and altered by a \"social experience\", and how those priorities affect \"behavioral orientation and choices\". The study surmised that the majority of cultures prioritize some of ten particular value types: self-determination, stimulation, hedonism, achievement, power, security, conformity, tradition, benevolence, and universalism. Among these there were values that cultures prioritize to varying degrees.\n\nSchwartz used these findings in developing his theory of specific types of values, allowing varying cultures to be contrasted with one another. Data was collected from 49 countries and applied to a construct of seven value types. This was done according to the specific priority given by a society to its communal value set. Schwartz selected the seven types of values based on their compatibilities and contradictions to one another. The value types he identified were: \"conservatism vs. autonomy\", \"hierarchy vs. egalitarianism\", and \"mastery vs. harmony\". The value types were then used to examine cultures that were closely\nrelated to each other. The theory is based on cultural dimensions so that conclusions can be defined to a degree, while accounting for the entire social matrix. This theory has helped researchers to assess the cultural implications of values on regional cultures. The research also found a correlation between geographical proximity and shared cultural values. Schwartz attributed these relationships to the \"shared history\", religion, level of development, culture contact, and other factors\".\n\nZār (زار) known as possession by a spirit, is exhibited by some Middle Eastern cultures. Specific ceremonies are performed to placate the zār and relieve the symptoms of the afflicted individual. These ritualized ceremonies, organized and facilitated by a leader, include the affected individual and a person previously affected by the zār, and involve \"incense, music, and movement\". The details of the ceremony vary by region. Some leaders may recommend that the patient first seek the help of a doctor, while others believe that interventions by the doctor, such as using needles for injections, may further agitate the zār, creating more problems for the patient. Those who choose traditional treatment for zār remain in isolation for up to seven days. This syndrome has been reported in North African and Middle Eastern countries, including Ethiopia, Egypt, Iran, and Sudan. Signs and symptoms of zār may include dissociative episodes, unexpected laughing, yelling, or singing, or even patients hitting their heads against a wall. Clients may exhibit apathy and be reclusive. Those under the influence of the zār may refuse to eat or carry out activities of daily living, and may develop an extended interaction with the possessing spirit.\n\nThe \"evil eye\" is also known as ʿayn al-ḥasūd (عين الحسود)\" and \"Mal De Ojo\", and has been described by a number of sources. It is also described in Mexican culture The late Professor Alan Dundes theorized that belief in the evil eye had a Middle Eastern, Mediterranean, and Indo European pattern of distribution, and was completely unknown in the Americas, Pacific Islands, Asia, and sub-Saharan Africa until the introduction of European and possibly Moorish culture. He based his ideas upon the ancient underlying belief that water represented life and dryness represented death. Dundes suggested that evil caused by the evil eye came from its power to cause living beings to \"dry up\", specifically referring to infants, lactating mammals, and even young fruit trees. Symptoms included sudden vomiting, diarrhea, and loss of potency in men. Dundes contended that the evil eye dries up liquids, which explained its Middle Eastern and desert origins. This unique syndrome is also mentioned in the DSM-IV TR, Appendix I. The syndrome is also seen in many other Middle Eastern Countries.\n\nPost-traumatic stress syndrome (PTSD) is, unfortunately, common in the Middle East due to the myriad conflicts experienced by people in the region. Diagnostic symptoms for PTSD include recurrent experiencing of the initial traumatic event(s) through flashbacks or recurrent night sweats and nightmares. Those afflicted with post-traumatic stress syndrome often seek to avoid others as well as any stimulus similar to the traumatic occurrence. They may also exhibit increased arousal. Difficulty in falling or staying asleep, unexpected episodic fits of anger, and hypervigilance may also occur. The formal diagnostic criteria for both the DSM-IV-TR and ICD-10) indicate that symptoms last for more than one month and cause significant impairment in social, occupational, and/or other important areas of functioning. There is also some evidence that children suffering from PTSD in the Middle East may experience accelerated aging.\n\nDepression in the Middle East has been specifically studied at Namazi Hospital Shiraz, in Iran. In a 2006 study of nurses, depressive symptoms were seen in 26.9% of the individuals studied. In this cross-sectional survey, the rate of depression in 130 nurses was investigated using the 21-item Beck Depression Inventory. Data collection also involved individual interviews and follow-up by the research team.\n\n\n\n"}
{"id": "49494992", "url": "https://en.wikipedia.org/wiki?curid=49494992", "title": "National Survey on Drug Use and Health", "text": "National Survey on Drug Use and Health\n\nThe National Survey on Drug Use and Health, often abbreviated NSDUH, is an annual nationwide survey on the use of legal and illegal drugs, as well as mental disorders, that has been conducted by the United States federal government since 1971. It is funded by the Substance Abuse and Mental Health Services Administration (SAMHSA), and is supervised by the SAMHSA's Center for Behavioral Health Statistics and Quality. The survey interviews about 70,000 Americans aged 12 and older, through face-to-face interviews conducted where the respondent lives. In 1988, RTI International started conducting the survey, and they have been contracted by SAMHSA to continue doing so through 2017. It was originally called the National Household Survey on Drug Abuse, but was renamed in 2002 to its current name. The NSDUH, along with the Monitoring the Future, is one of the two main ways the National Institute on Drug Abuse measures drug use in the United States.\n"}
{"id": "29718054", "url": "https://en.wikipedia.org/wiki?curid=29718054", "title": "Nutritional epidemiology", "text": "Nutritional epidemiology\n\nNutritional epidemiology is a relatively new field of medical research that studies the relationship between nutrition and health. Diet and physical activity are difficult to measure accurately, which may partly explain why nutrition has received less attention than other risk factors for disease in epidemiology.\n"}
{"id": "22720886", "url": "https://en.wikipedia.org/wiki?curid=22720886", "title": "Occupational stress", "text": "Occupational stress\n\nOccupational stress is stress related to one's job. Occupational stress often stems from unexpected responsibilities and pressures that do not align with a person's knowledge, skills, or expectations, inhibiting one's ability to cope. Occupational stress can increase when workers do not feel supported by supervisors or colleagues, or feel as if they have little control over work processes.\n\nBecause stress results from the complex interactions between a large system of interrelated variables, there are several psychological theories and models that address occupational stress.\n\nPerson-environment fit model: This model \"suggests that the match between a person and their work environment is key in influencing their health. For healthy conditions, it is necessary that employees’ attitudes, skills, abilities and resources match the demands of their job, and that work environments should meet workers’ needs, knowledge, and skills potential. Lack of fit in either of these domains can cause problems, and the greater the gap or misfit (either subjective or objective) between the person and their environment, the greater the strain as demands exceed abilities, and need exceeds supply. These strains can relate to health related issues, lower productivity, and other work problems. Defense mechanisms, such as denial, reappraisal of needs, and coping, also operate in the model, to try and reduce subjective misfit\".\n\nJob characteristics model: This model \"focuses on important aspects of job characteristics, such as skill variety, task identity, task significance, autonomy, and feedback. These characteristics are proposed to lead to ‘critical psychological states’ of experienced meaningfulness, and experienced responsibility and knowledge of outcomes. It is proposed that positive or negative work characteristics give rise to mental states which lead to corresponding cognitive and behavioral outcomes, e.g. motivation, satisfaction, absenteeism, etc. In conjunction with the model, Hackman and Oldham (1980) developed the Job Diagnostic Survey, a questionnaire for job analysis, which implies key types of job-redesign including combining tasks, creating feedback methods, job enrichment, etc.\"\n\nDiathesis–stress model: This model looks at behaviors as a susceptibility burden together with stress from life experiences. It is useful to distinguish stressful job conditions or stressors from an individual's reactions or strains. Strains can be mental, physical or emotional. Occupational stress can occur when there is a discrepancy between the demands of the environment/workplace and an individual’s ability to carry out and complete these demands. Often a stressor can lead the body to have a physiological reaction that can strain a person physically as well as mentally. A variety of factors contribute to workplace stress such as excessive workload, isolation, extensive hours worked, toxic work environments, lack of autonomy, difficult relationships among coworkers and management, management bullying, harassment and lack of opportunities or motivation to advancement in one’s skill level.\n\nJob demands-resources model: This model posits that strain are a response to imbalance between demands of one's job and the resources he or she has to deal with those demands.\n\n\nEffort-reward imbalance model: This model focuses on the reciprocal relationship between efforts and rewards at work. \"More specifically, the ERI Model claims that work characterized by both high efforts and low rewards represents a reciprocity deficit between high ‘costs’ and low ‘gains’, which could elicit negative emotions in exposed employees. The accompanying feelings may cause sustained strain reactions. So, working hard without receiving adequate appreciation or being treated fairly are examples of a stressful imbalance. Another assumption of the ERI Model concerns individual differences in the experience of effort-reward imbalance. It is assumed that employees characterized by a motivational pattern of excessive job-related commitment and a high need for approval (i.e., overcommitment) will respond with more strain reactions to an effort-reward imbalance, in comparison with less overcommitted people.\"\n\nSources of occupational stress come from:\n\nThese individual sources demonstrate that stress can occur specifically when a conflict arises from the job demands of the employee and the employee itself. If not handled properly, the stress can become distress.\n\nDistress is a prevalent and costly problem in today's workplace. About one-third of workers report high levels of stress. 20–30% of workers in different sectors of the European Union reported in 2007 that they believed work-related stress was potentially affecting their health. Three-quarters of employees believe the worker has more on-the-job stress than a generation ago. In Great Britain, a sixth of the workforce experiences occupational stress every year. Evidence also suggests that distress is the major cause of turnover in organizations. With continued distress at the workplace, workers will develop psychological and physiological dysfunctions and decreased motivation in excelling in their position. Increased levels of job stress are determined by the awareness of having little control but lots of demands in the work area. Occupational stress and its sequelae represent the majority of work-related illnesses causing missed work days. Those in the protective services, transportation and materials moving, building grounds cleaning and maintenance, and healthcare are more susceptible to both work injuries and illnesses, as well as work-related stress.\n\nStress-related disorders encompass a broad array of conditions, including psychological disorders (e.g., depression, anxiety, post-traumatic stress disorder) and other types of emotional strain (e.g., dissatisfaction, fatigue, tension, etc.), maladaptive behaviors (e.g., aggression, substance abuse), and cognitive impairment (e.g., concentration and memory problems). In turn, these conditions may lead to poor work performance, higher absenteeism, less work productivity or even injury. \"If untreated, consistently high stress can become a chronic condition, which can exacerbate existing mental health conditions and chronic physical conditions (diabetes, hypertension, weak immune system). These conditions not only diminish the well-being of workers and increase the employer's health benefits expenses, they contribute to increased injury incidence. Consistently high levels of stress increase the risk of occupational injury. A study of light/short haul truckers, a group that experiences high rates of injury and mental health issues, found that frequent stress increased the odds of occupational injury by 350%.\"\n\nJob stress is also associated with various biological reactions that may lead ultimately to compromised health, such as cardiovascular disease, or in extreme cases death. Due to the high pressure and demands in the work place the demands have been shown to be correlated with increased rates of heart attack, hypertension and other disorders. In New York, Los Angeles, and London, among other municipalities, the relationship between job stress and heart attacks is acknowledged.\n\nProblems at work are more strongly associated with health complaints than are any other life stressor-more so than even financial problems or family problems. Occupational stress accounts for more than 10% of work-related health claims. Many studies suggest that psychologically demanding jobs that allow employees little control over the work process increase the risk of cardiovascular disease. Research indicates that job stress increases the risk for development of back and upper-extremity musculoskeletal disorders. Other disorders that can be caused or exacerbated by occupational stress include sleep disorders, headache, mood disorders, upset stomach, hypertension, high cholesterol, autoimmune disease, cardiovascular disease, depression, and anxiety. Stress at work can also increase the risk of acquiring an infection and the risk of accidents at work.\n\nHigh levels of stress are associated with substantial increases in health service utilization. Workers who report experiencing stress at work also show excessive health care utilization. In a 1998 study of 46,000 workers, health care costs were nearly 50% greater for workers reporting high levels of stress in comparison to “low risk” workers. The increment rose to nearly 150%, an increase of more than $1,700 per person annually, for workers reporting high levels of both stress and depression. Health care costs increase by 200% in those with depression and high occupational stress. Additionally, periods of disability due to job stress tend to be much longer than disability periods for other occupational injuries and illnesses.\n\nPhysiological reactions to stress can have consequences for health over time. Researchers have been studying how stress affects the cardiovascular system, as well as how work stress can lead to hypertension and coronary artery disease. These diseases, along with other stress-induced illnesses tend to be quite common in American work-places. There are four main physiological reactions to stress:\n\nMen and women are exposed to many of the same stressors. Although men and women might not differ in overall strains, women are more likely to experience psychological distress, whereas men experience more physical strain. Desmarais and Alksnis suggest two explanations for the greater psychological distress of women. First, the genders may differ in their awareness of negative feelings, leading women to express and report strains, whereas men deny and inhibit such feelings. Second, the demands to balance work and family result in more overall stressors for women that leads to increased strain.\n\nThe Kenexa Research Institute released a global survey of almost 30,000 workers which showed that females suffered more workplace distress than their male counterparts. According to the survey, women's stress level were 10% higher for those in supervisory positions, 8% higher stress in service and production jobs than men, and 6% higher in middle and upper management than men in the same position.\n\nJob stress results from various interactions of the worker and the environment of the work they perform their duties. Location, gender, environment, and many other factors contribute to the buildup of stress. Job stress results from the interaction of the worker and the conditions of work. Views differ on the importance of worker characteristics versus working conditions as the primary cause of job stress. The differing viewpoints suggest different ways to prevent stress at work. Differences in individual characteristics such as personality and coping skills can be very important in predicting whether certain job conditions will result in stress. In other words, what is stressful for one person may not be a problem for someone else. This viewpoint underlies prevention strategies that focus on workers and ways to help them cope with demanding job conditions. In general, occupational stress is caused by a mismatch between perceived effort and perceived reward, and/or a sense of low control in a job with high demands. Low social support at work and job insecurity can also increase occupational stress. Psychosocial stressors are a major cause of occupational stress.\n\nAlthough the importance of individual differences cannot be ignored, scientific evidence suggests that certain working conditions are stressful to most people. Such evidence argues for a greater emphasis on working conditions as the key source of job stress, and for job redesign as a primary prevention strategy. Large surveys of working conditions, including conditions recognized as risk factors for job stress, were conducted in member states of the European Union in 1990, 1995, and 2000. Results showed a time trend suggesting an increase in work intensity. In 1990, the percentage of workers reporting that they worked at high speeds at least one-quarter of their working time was 48%, increasing to 54% in 1995 and to 56% in 2000. Similarly, 50% of workers reported they work against tight deadlines at least one-fourth of their working time in 1990, increasing to 56% in 1995 and 60% in 2000. However, no change was noted in the period 1995–2000 (data not collected in 1990) in the percentage of workers reporting sufficient time to complete tasks.\n\nIn an occupational setting, dealing with workload can be stressful and serve as a stressor for employees. There are three aspects of workload that can be stressful.\n\nWorkload as a work demand is a major component of the demand-control model of stress. This model suggests that jobs with high demands can be stressful, especially when the individual has low control over the job. In other words, control serves as a buffer or protective factor when demands or workload is high. This model was expanded into the demand-control-support model that suggests that the combination of high control and high social support at work buffers the effects of high demands.\n\nAs a work demand, workload is also relevant to the job demands-resources model of stress that suggests that jobs are stressful when demands (e.g., workload) exceed the individual's resources to deal with them.\n\nA substantial percentage of Americans work very long hours. By one estimate, more than 26% of men and more than 11% of women worked 50 hours per week or more in 2000. These figures represent a considerable increase over the previous three decades, especially for women. According to the Department of Labor, there has been a rise in the number of hours in the work place by employed women, an increase in extended work weeks (>40 hours) by men, and a considerable increase in combined working hours among working couples, particularly couples with young children.\n\nA person's status in the workplace can also affect levels of stress. While workplace stress has the potential to affect employees of all categories; those who have very little influence to those who make major decisions for the company. However, less powerful employees (that is, those who have less control over their jobs) are more likely to suffer stress than powerful workers. Managers as well as other kinds of workers are vulnerable to work overload.\n\nEconomic factors that employees face in the 21st century have been linked to increased stress levels. Researchers and social commentators have pointed out that the computer and communications revolutions have made companies more efficient and productive than ever before. This increase in productivity, however, has caused higher expectations and greater competition, putting more stress on the employee.\n\nThe following economic factors may lead to workplace stress: \n\nBullying in the workplace can also contribute to stress. This can be broken down into five categories:\n\nThis can create a hostile work environment for employees, which in turn can affect their work ethic and contribution to the organization.\n\nThomas suggests that there tends to be a higher level of stress with people who work or interact with a narcissist, which in turn increases absenteeism and staff turnover. Boddy finds the same dynamic where there is a corporate psychopath in the organisation.\n\nInterpersonal conflict among people at work has been shown to be one of the most frequently noted stressors for employees. Conflict has been noted to be an indicator of the broader concept of workplace harassment. It relates to other stressors that might co-occur, such as role conflict, role ambiguity, and workload. It also relates to strains such as anxiety, depression, physical symptoms, and low levels of job satisfaction.\n\nWomen are more likely than men to experience sexual harassment, especially for those working in traditionally masculine occupations. In addition, a study indicated that sexual harassment negatively affects workers' psychological well-being. Another study found that level of harassment at workplaces lead to differences in performance of work related tasks. High levels of harassment were related to the worst outcomes, and no harassment was related to least negative outcomes. In other words, women who had experienced a higher level of harassment were more likely to perform poorly at workplaces.\n\nLower occupational groups are at higher risk of work-related ill health than higher occupational groups. This is in part due to adverse work and employment conditions. Furthermore, such conditions have greater effects on ill-health to those in lower socio-economic positions.\n\nStressful working conditions can lead to three types of strains: Behavioral (e.g., absenteeism or poor performance), physical (e.g., headaches or coronary heart disease), and psychological (e.g., anxiety or depressed mood). Physical symptoms that may occur because of occupational stress include fatigue, headache, upset stomach, muscular aches and pains, weight gain or loss, chronic mild illness, and sleep disturbances. Psychological and behavioral problems that may develop include anxiety, irritability, alcohol and drug use, feeling powerless and low morale. The spectrum of effects caused by occupational stress includes absenteeism, poor decision making, lack of creativity, accidents, organizational breakdown or even sabotage. If exposure to stressors in the workplace is prolonged, then chronic health problems can occur including stroke. An examination was of physical and psychological effects of workplace stress was conducted with a sample of 552 female blue collar employees of a microelectronics facility. It was found that job-related conflicts were associated with depressive symptoms, severe headaches, fatigue, rashes, and other multiple symptoms. Studies among the Japanese population specifically showed a more than 2-fold increase in the risk of total stroke among men with job strain (combination of high job demand and low job control). Those in blue-collar or manual labor jobs are more likely to develop heart disease compared to those in white-collar jobs. Along with the risk of stroke, stress can raise the risk of high blood pressure, immune system dysfunction, coronary artery disease. Prolonged occupational stress can lead to occupational burnout. Occupational stress can also disrupt relationships.\n\nThe effects of job stress on chronic diseases are more difficult to ascertain because chronic diseases develop over relatively long periods of time and are influenced by many factors other than stress. Nonetheless, there is some evidence that stress plays a role in the development of several types of chronic health problems—including cardiovascular disease, musculoskeletal disorders, and psychological disorders. Job stress and strain has been associated with poor mental health and wellbeing over a 12-year period.\n\nOccupational stress has negative effects for organizations and employers. Occupational stress is the cause of approximately 40% of turnover and 50% of workplace absences. The annual cost of occupational stress and its effects in the US is estimated to be over $60 billion to employers and $250–300 billion to the economy.\n\nA combination of organizational change and stress management is often the most useful approach for preventing stress at work. Both organizations and employees can employ strategies at organizational and individual levels. Generally, organizational level strategies include job procedure modification and employee assistance programs (EAP). Individual level strategies include taking vacation. Getting a realistic job preview to understand the normal workload and schedules of the job will also help people to identify whether or not the job fit them.\n\nHow an Organization Can Prevent Job Stress\n\nAn insurance company conducted several studies on the effects of stress prevention programs in hospital settings. Program activities included (1) employee and management education on job stress, (2) changes in hospital policies and procedures to reduce organizational sources of stress, and (3) the establishment of employee assistance programs. In one study, the frequency of medication errors declined by 50% after prevention activities were implemented in a 700-bed hospital. In a second study, there was a 70% reduction in malpractice claims in 22 hospitals that implemented stress prevention activities. In contrast, there was no reduction in claims in a matched group of 22 hospitals that did not implement stress prevention activities.\n\nTelecommuting is another way organizations can help reduce stress for their workers. Employees defined telecommuting as \"an alternative work arrangement in which employees perform tasks elsewhere that are normally done in a primary or central workplace, for at least some portion of their work schedule, using electronic media to interact with others inside and outside the organization.\" One reason that telecommuting gets such high marks is that it allows employees more control over how they do their work. Telecommuters reported more job satisfaction and less desire to find a new job. Employees that worked from home also had less stress, improved work/life balance and higher performance rating by their managers.\n\nA systematic review of stress-reduction techniques among healthcare workers found that cognitive behavioral training lowered emotional exhaustion and feelings of lack of personal accomplishment.\n\nSigns and symptoms of excessive job and workplace stress include:\nBoth yoga and mindful-based stress reduction have been shown to reduce work-related stress. Nurses who participated in cognitive behavioral interventions had less perceived stress, a greater sense of coherence, and increased mood.\n\nExpanding research on stress: Contemporary opinions hold that jobs designed to support skill variety, task identity, significance, autonomy, and feedback, while providing for existence and growth needs, will sustain a healthier, greater satisfied workforce.\n\nFor team-oriented work environments, Patrick Lencioni's \"The Five Dysfunctions of a Team\" profiles the behavior of cohesive teams:\n\n\nFor immediate individual stress management, rudimentary mental coping strategies may be adopted in the work environment.\n\n"}
{"id": "501376", "url": "https://en.wikipedia.org/wiki?curid=501376", "title": "Oligosaccharide", "text": "Oligosaccharide\n\nAn oligosaccharide (/ˌɑlɪgoʊˈsækəˌɹaɪd/; from the Greek ὀλίγος \"olígos\", \"a few\", and σάκχαρ \"sácchar\", \"sugar\") is a saccharide polymer containing a small number (typically three to ten) of monosaccharides (simple sugars). Oligosaccharides can have many functions including cell recognition and cell binding. For example, glycolipids have an important role in the immune response.\n\nThey are normally present as glycans: oligosaccharide chains linked to lipids or to compatible amino acid side chains in proteins, by \"N\"- or \"O\"-glygosidic bonds. N-linked oligosaccharides are always pentasaccharides attached to asparagine via a beta linkage to the amine nitrogen of the side chain. Alternately, O-linked oligosaccharides are generally attached to threonine or serine on the alcohol group of the side chain. Not all natural oligosaccharides occur as components of glycoproteins or glycolipids. Some, such as the raffinose series, occur as storage or transport carbohydrates in plants. Others, such as maltodextrins or cellodextrins, result from the microbial breakdown of larger polysaccharides such as starch or cellulose.\n\nIn biology, glycosylation is the process by which a carbohydrate is covalently attached to an organic molecule, creating structures such as glycoproteins and glycolipids.\n\nN-linked glycosylation involves oligosaccharide attachment to asparagine via a beta linkage to the amine nitrogen of the side chain. The process of N-linked glycosylation occurs cotranslationally, or concurrently while the proteins is being translated. Since it is added cotranslationally, it is believed that N-linked glycosylation helps determine the folding of polypeptides due to the hydrophilic nature of sugars. All N-linked Oligosaccharides are pentasaccharides: five monosaccharides long.\n\nIn N-glycosylation for eukaryotes, the oligosaccharide substrate is assembled right at the membrane of the endoplasmatic reticulum. For prokaryotes, this process occurs at the plasma membrane. In both cases, the acceptor substrate is an asparagine residue. The asparagine residue linked to an N-linked oligosaccharide usually occurs in the sequence Asn-X-Ser/Thr, where X can be any amino acid except for proline, although it is rare to see Asp, Glu, Leu, or Trp in this position.\n\nOligosaccharides that participate in O-linked glycosylation are attached to threonine or serine on the hydroxyl group of the side chain. O-linked glycosylation occurs in the golgi apparatus, where monosaccharide units are added to a complete polypeptide chain. Cell surface proteins and extracellular proteins are O-glycosylated. Glycosylation sites in O-linked oligosaccharides are determined by the secondary and tertiary structures of the polypeptide, which dictate where glycosyltransferases will add sugars.\n\nGlycoproteins and glycolipids are by definition covalently bonded to carbohydrates. They are very abundant on the surface of the cell, and their interactions contribute to the overall stability of the cell.\n\nGlycoproteins have distinct Oligosaccharide structures which have significant effects on many of their properties, affecting critical functions such as antigenicity, solubility, and resistance to proteases. Glycoproteins are relevant as cell-surface receptors, cell-adhesion molecules, immunoglobulins, and tumor antigens.\n\nGlycolipids are important for cell recognition, and are important for modulating the function of membrane proteins that act as receptors. Glycolipids are lipid molecules bound to oligosaccharides, generally present in the lipid bilayer. Additionally, they can serve as receptors for cellular recognition and cell signaling. The head of the oligosaccharide serves as a binding partner in receptor activity. The binding mechanisms of receptors to the oligosaccharides depends on the composition of the oligosaccharides that are exposed or presented above the surface of the membrane. There is great diversity in the binding mechanisms of glycolipids, which is what makes them such an important target for pathogens as a site for interaction and entrance. For example, the chaperone activity of glycolipids has been studied for its relevance to HIV infection.\n\nAll cells are coated in either glycoproteins or glycolipids, both of which help determine cell types. Lectins, or proteins that bind carbohydrates, can recognize specific oligosaccharides and provide useful information for cell recognition based on oligosaccharide binding.\n\nAn important example of oligosaccharide cell recognition is the role of glycolipids in determining blood types. The various blood types are distinguished by the glycan modification present on the surface of blood cells. These can be visualized using mass spectrometry. The oligosaccharides found on the A, B, and H antigen occur on the non-reducing ends of the oligosaccharide. The H antigen (which indicates an O blood type) serves as a precursor for the A and B antigen. Therefore, a person with A blood type will have the A antigen and H antigen present on the glycolipids of the red blood cell plasma membrane. A person with B blood type will have the B and H antigen present. A person with AB blood type will have A, B, and H antigens present. And finally, a person with O blood type will only have the H antigen present. This means all blood types have the H antigen, which explains why the O blood type is known as the \"universal donor\".\n\nMany cells produce specific carbohydrate-binding proteins known as lectins, which mediate cell adhesion with oligosaccharides. Selectins - a family of lectins - mediate certain cell-cell adhesion processes, including those of leukocytes to endothelial cells. In an immune response, endothelial cells can express certain selectins transiently in response to damage or injury to the cells. In response, a reciprocal selectin-oligosaccharide interaction will occur between the two molecules which allows the white blood cell to help eliminate the infection or damage. Protein-Carbohydrate bonding is often mediated by hydrogen bonding and van der Waals forces.\n\nFructo-oligosaccharides (FOS), which are found in many vegetables, are short chains of fructose molecules. They differ from inulin, which has a much higher degree of polymerization than FOS and is therefore a polysaccharide, but like inulin, they are considered soluble dietary fibre. Galactooligosaccharides (GOS), which also occur naturally, consist of short chains of galactose molecules. These compounds cannot be digested in the human small intestine, and instead pass through to the large intestine, where they promote the growth of \"Bifidobacteria\", which are beneficial to gut health.\n\nMannan oligosaccharides (MOS) are widely used in animal feed to improve gastrointestinal health. They are normally obtained from the yeast cell walls of \"Saccharomyces cerevisiae\". Mannan oligosaccharides differ from other oligosaccharides in that they are not fermentable and their primary mode of actions include agglutination of type-1 fimbria pathogens and immunomodulation\n\nOligosaccharides are a component of fibre from plant tissue. FOS and inulin are present in Jerusalem artichoke, burdock, chicory, leeks, onions, and asparagus. Inulin is a significant part of the daily diet of most of the world’s population. FOS can also be synthesized by enzymes of the fungus \"Aspergillus niger\" acting on sucrose. GOS is naturally found in soybeans and can be synthesized from lactose. FOS, GOS, and inulin are also sold as nutritional supplements.\n\n"}
{"id": "7981926", "url": "https://en.wikipedia.org/wiki?curid=7981926", "title": "Orchialgia", "text": "Orchialgia\n\nOrchialgia is long-term pain of the testes. It is considered chronic if it has persisted for more than 3 months. Orchialgia may be caused by injury, infection, surgery, cancer or testicular torsion and is a possible complication after vasectomy. IgG4-related disease is a more recently identified cause of chronic orchialgia.\n\nOne author describes the syndromes of chronic testicular pain thus:\"The complaint is of a squeezing deep ache in the testis like the day after you got kicked there, often bilateral or alternating from one side to the other, intermittent, and, most commonly, associated with lower back pain. Sometimes it feels like the testicle is pinched in the crotch of the underwear but trouser readjustment does not help. There may also be pain in the inguinal area but no nausea or other symptoms. Back pain may be concurrent or absent and some patients have a long history of low back pain. Onset of pain is commonly related to activity that would stress the low back such as lifting heavy objects. Other stresses that might cause low back pain are imaginative coital positions, jogging, sitting hunched over a computer, long car driving, or other such positions of unsupported seating posture that flattens the normal lumbar lordosis curve.\"\n\nTesting for gonorrhea and chlamydia should be routinely performed.\n\nTreatment is often with NSAIDs and antibiotics however, this is not always effective.\n\n"}
{"id": "261773", "url": "https://en.wikipedia.org/wiki?curid=261773", "title": "Parenteral nutrition", "text": "Parenteral nutrition\n\nParenteral nutrition (PN) is the feeding of specialist nutritional products to a person intravenously, bypassing the usual process of eating and digestion. The products are made by specialist pharmaceutical compounding companies and is considered to be the highest risk pharmaceutical preparation available as the products cannot undergo any form of terminal sterilization. The person receives highly complex nutritional formulae that contain nutrients such as glucose, salts, amino acids, lipids and added vitamins and dietary minerals. It is called total parenteral nutrition (TPN) or total nutrient admixture (TNA) when no significant nutrition is obtained by other routes, and partial parenteral nutrition (PPN) when nutrition is also partially enteric. It may be called peripheral parenteral nutrition (PPN) when administered through vein access in a limb rather than through a central vein as central venous nutrition (CVN).\n\nTotal parenteral nutrition (TPN) is provided when the gastrointestinal tract is nonfunctional because of an interruption in its continuity (it is blocked, or has a leak - a fistula) or because its absorptive capacity is impaired. It has been used for comatose patients, although enteral feeding is usually preferable, and less prone to complications. Parenteral nutrition is used to prevent malnutrition in patients who are unable to obtain adequate nutrients by oral or enteral routes. The Society of Critical Care Medicine (SCCM) and American Society for Parenteral and Enteral Nutrition recommends waiting until hospital day number seven.\n\nAbsolute indications for TPN:\n\n\nTPN may be the only feasible option for providing nutrition to patients who do not have a functioning gastrointestinal tract or who have disorders requiring complete bowel rest, including bowel obstruction, short bowel syndrome, gastroschisis, prolonged diarrhea regardless of its cause, very severe Crohn's disease or ulcerative colitis, and certain pediatric GI disorders including congenital GI anomalies and necrotizing enterocolitis.\n\nThe benefit of TPN to cancer patients is largely debated, and studies to date have generally showed minimal long term benefit.\n\nShort-term PN may be used if a person's digestive system has shut down (for instance by peritonitis), and they are at a low enough weight to cause concerns about nutrition during an extended hospital stay. Long-term PN is occasionally used to treat people suffering the extended consequences of an accident, surgery, or digestive disorder. PN has extended the life of children born with nonexistent or severely deformed organs.\n\nApproximately 40,000 people use TPN at home in the United States, and because TPN requires anywhere from 10–16 hours to be administered, daily life can be affected. Although daily lifestyle can be changed, most patients agree that these changes are better than staying at the hospital. Many different types of pumps exist to limit the time the patient is “hooked-up”. Usually a backpack pump is used, allowing for mobility. The time required to be connected to the IV is dependent on the situation of each patient; some require once a day, or five days a week.\n\nIt is important for patients to avoid as much TPN related change as possible in their lifestyles. This allows for the best possible mental health situation; constantly being held down can lead to resentment and depression. Physical activity is also highly encouraged, but patients must avoid contact sports (equipment damage) and swimming (infection). Many teens find it difficult to live with TPN due to issues regarding body image and not being able to participate in activities and events.\n\nTPN fully bypasses the GI tract and normal methods of nutrient absorption. Possible complications, which may be significant, are listed below. Other than those listed below, other common complications of TPN include hypophosphatemia, hypokalemia, hyperglycemia, hypercapnia, decreased copper and zinc levels, elevated prothrombin time (if associated with liver injury), hyperchloremic metabolic acidosis and decreased gastrointestinal motility.\n\nTPN requires a chronic IV access for the solution to run through, and the most common complication is infection of this catheter. Infection is a common cause of death in these patients, with a mortality rate of approximately 15% per infection, and death usually results from septic shock. When using central venous access, the subclavian (or axillary) vein is preferred due to its ease of access and lowest infectious complications compared to the jugular and femoral vein insertions.\n\nChronic IV access leaves a foreign body in the vascular system, and blood clots on this IV line are common. Death can result from pulmonary embolism wherein a clot that starts on the IV line breaks off and travels to the lungs, blocking blood flow. \nPatients on TPN who have such clots occluding their catheter may receive a thrombolytic flush to dissolve the clots and prevent further complications.\n\nFatty liver is usually a more long term complication of TPN, though over a long enough course it is fairly common. The pathogenesis is due to using linoleic acid (an omega-6 fatty acid component of soybean oil) as a major source of calories. TPN-associated liver disease strikes up to 50% of patients within 5–7 years, correlated with a mortality rate of 2–50%. Onset of this liver disease is the major complication that leads TPN patients to requiring an intestinal transplant.\n\nIntralipid (Fresenius-Kabi), the US standard lipid emulsion for TPN nutrition, contains a 7:1 ratio of n-6/n-3 ratio of polyunsaturated fatty acids (PUFA). By contrast, Omegaven has a 1:8 ratio and showed promise in multiple clinical studies. Therefore n-3-rich fat may alter the course of parenteral nutrition associated liver disease.\n\nBecause patients are being fed intravenously, the subject does not physically eat, resulting in intense hunger pangs (pains). The brain uses signals from the mouth (taste and smell), the stomach/gastrointestinal tract (fullness) and blood (nutrient levels) to determine conscious feelings of hunger. In cases of TPN, the taste, smell and physical fullness requirements are not met, and so the patient experiences hunger, despite the fact that the body is being fully nourished.\n\nPatients who eat food despite the inability can experience a wide range of complications.\n\nTotal parenteral nutrition increases the risk of acute cholecystitis due to complete disuse of gastrointestinal tract, which may result in bile stasis in the gallbladder. Other potential hepatobiliary dysfunctions include steatosis, steatohepatitis, cholestasis, and cholelithiasis. Six percent of patients on TPN longer than 3 weeks and 100% of patients on TPN longer than 13 weeks develop biliary sludge. The formation of sludge is the result of stasis due to lack of enteric stimulation and is not due to changes in bile composition. Gallbladder sludge disappears after 4 weeks of normal oral diet. Administration of exogenous cholecystokinin (CCK) or stimulation of endogenous CCK by periodic pulse of large amounts of amino acids have been shown to help prevent sludge formation. These therapies are not routinely recommended. Such complications are suggested to be the main reason for mortality in people requiring long-term total parenteral nutrition, such as in short bowel syndrome. In newborn infants with short bowel syndrome with less than 10% of expected intestinal length, thereby being dependent upon total parenteral nutrition, 5 year survival is approximately 20%.\n\nInfants who are sustained on TPN without food by mouth for prolonged periods are at risk for developing gut atrophy.\n\nOther complications are either related to catheter insertion, or metabolic, including refeeding syndrome. Catheter complications include pneumothorax, accidental arterial puncture, and catheter-related sepsis. The complication rate at the time of insertion should be less than 5%. Catheter-related infections may be minimised by appropriate choice of catheter and insertion technique. Metabolic complications include the refeeding syndrome characterised by hypokalemia, hypophosphatemia and hypomagnesemia. Hyperglycemia is common at the start of therapy, but can be treated with insulin added to the TPN solution. Hypoglycaemia is likely to occur with abrupt cessation of TPN. Liver dysfunction can be limited to a reversible cholestatic jaundice and to fatty infiltration (demonstrated by elevated transaminases). Severe hepatic dysfunction is a rare complication. Overall, patients receiving TPN have a higher rate of infectious complications. This can be related to hyperglycemia.\n\nPregnancy can cause major complications when trying to properly dose the nutrient mixture. Because all of the baby’s nourishment comes from the mother’s blood stream, the doctor must properly calculate the dosage of nutrients to meet both recipients’ needs and have them in usable forms. Incorrect dosage can lead to many adverse, hard-to-guess effects, such as death, and varying degrees of deformation or other developmental problems.\n\nIt is recommended that parenteral nutrition administration begin after a period of natural nutrition so doctors can properly calculate the nutritional needs of the fetus. Otherwise, it should only be administered by a team of highly skilled doctors who can accurately assess the fetus’ needs.\n\nSolutions for total parenteral nutrition may be customized to individual patient requirements, or standardized solutions may be used. The use of standardized parenteral nutrition solutions is cost effective and may provide better control of serum electrolytes. \nIdeally each patient is assessed individually before commencing on parenteral nutrition, and a team consisting of specialised doctors, nurses, clinical pharmacists and registered dietitians evaluate the patient's individual data and decide what PN formula to use and at what infusion rate.\n\nFor energy only, intravenous sugar solutions with dextrose or glucose are generally used. This is not considered to be parenteral nutrition as it does not prevent malnutrition when used on its own. Standardized solutions may also differ between developers. Following are some examples of what compositions they may have. The solution for normal patients may be given both centrally and peripherally.\n\nPrepared solutions generally consist of water and electrolytes; glucose, amino acids, and lipids; essential vitamins, minerals and trace elements are added or given separately. Previously lipid emulsions were given separately but it is becoming more common for a \"three-in-one\" solution of glucose, proteins, and lipids to be administered.\n\nIndividual nutrient components may be added to more precisely adjust the body contents of it. That individual nutrient may, if possible, be infused individually, or it may be injected into a bag of nutrient solution or intravenous fluids (volume expander solution) that is given to the patient.\n\nAdministration of individual components may be more hazardous than administration of pre-mixed solutions such as those used in total parenteral nutrition, because the latter are generally already balanced in regard to e.g. osmolarity and ability to infuse peripherally. Incorrect IV administration of concentrated potassium can be lethal, but this is not a danger if the potassium is mixed in TPN solution and diluted.\n\nVitamins may be added to a bulk premixed nutrient immediately before administration, since the additional vitamins can promote spoilage of stored product. Vitamins can be added in two doses, one fat-soluble, the other water-soluble. There are also single-dose preparations with both fat- and water-soluble vitamins such as \"Cernevit\".\n\nMinerals and trace elements for parenteral nutrition are available in prepared mixtures, such as \"Addaven\".\n\nOnly a limited number of emulsifiers are commonly regarded as safe to use for parenteral administration, of which the most important is lecithin. Lecithin can be biodegraded and metabolized, since it is an integral part of biological membranes, making it virtually non-toxic. Other emulsifiers can only be excreted via the kidneys, creating a toxic load. The emulsifier of choice for most fat emulsions used for parenteral nutrition is a highly purified egg lecithin, due to its low toxicity and complete integration with cell membranes.\n\nUse of egg-derived emulsifiers is not recommended for people with an egg allergy due to the risk of reaction. In situations where there is no suitable emulsifying agent for a person at risk of developing essential fatty acid deficiency, cooking oils may be spread upon large portions of available skin for supplementation by transdermal absorption.\n\nAnother type of fat emulsion Omegaven is being used experimentally within the US primarily in the pediatric population. It is made of fish oil instead of the egg based formulas more widely in use. Research has shown use of Omegaven may reverse and prevent liver disease and cholestasis.\n\nDeveloped in the 1960s by Dr. Stanley J. Dudrick, who as a surgical resident in the University of Pennsylvania, working in the basic science laboratory of Dr. Jonathan Rhoads, was the first to successfully nourish initially Beagle puppies and subsequently newborn babies with catastrophic gastrointestinal malignancies. Dr. Dudrick collaborated with Dr. Willmore and Dr. Vars to complete the work necessary to make this nutritional technique safe and successful.\n\n"}
{"id": "23453324", "url": "https://en.wikipedia.org/wiki?curid=23453324", "title": "Patient participation", "text": "Patient participation\n\nPatient participation is a trend that arose in answer to perceived physician paternalism. However, only rarely can unchecked physician paternalism be justified, and unlimited patient autonomy would presumably be equally abhorrent.\n\nIn recent years, the term \"patient participation\" has been used in many different contexts. These include, for example: participatory medicine, health consumerism, \nand patient-centered care. For the latter context, i.e. patient-centered care, a more nuanced definition was proposed in 2009 by the president of the Institute for Healthcare Improvement Donald Berwick: \"The experience (to the extent the informed, individual patient desires it) of transparency, individualization, recognition, respect, dignity, and choice in all matters, without exception, related to one's person, circumstances, and relationships in health care\" are concepts closely related to patient participation. In the UK over the course of 2016 two new relevant terms have expanded in usage: Patient and Public Involvement (PPI) and Engagement (PPIE) in the sense of the older term coproduction (public services).\n\nPatient participation is a generic term, and thus no list can be exhaustive; nonetheless, the following description shall subdivide it into several areas where patients and/or their advocates have a role.\n\nPatient participation, as it pertains to the formation of health policy, is a process that involves patients as stakeholders, advisors and shared decision makers. The practice of engaging patients in health policy originated from the consumer advocacy movement, which prioritized consumer safety, access to information and public participation in public health programs. Depending on the context, patient participation in health policy can refer to informed decision making, health advocacy, program development, policy implementation, and evaluation of services. Patient participation in health policy can affect many different levels of the health care system. Hospitalized individuals may participate in their own medical care in an effort to make shared decisions. In other areas, patients act as advocates by serving as members of organizational and governmental policy committees.\n\nIncreased patient participation in health policy can lead to improvements in patient satisfaction, quality and safety, cost savings and population health outcomes. Involving patient participation in health policy research can also ensure that public health needs are accurately incorporated into policy proposals. When solicited for participation by policymakers and industry leaders, patients can influence health policy, and both groups benefit from collaboration on goal setting and outcome measurement. By providing feedback in the form of survey responses, patients give community health officials and hospital leaders helpful feedback on the perceived quality and accessibility of health care services. Furthermore, patient satisfaction scores from these surveys have become an important metric by which hospitals are evaluated and compared to one another.\n\nPatient participation has driven the development of a variety of health policies, ranging from the expansion of hospital visitation hours to the implementation of patient-centered bedside rounding by hospital medical teams. Patient participation has contributed to improvements in the nurse-to-nurse handoff process by engaging with staff to discuss change-of-shift information at the patient's bedside. Patient participation in care coordination has also lead to the utilization of electronic medical records that patients can access and edit. By engaging with patients and patient advocacy groups, policymakers can support patients to shape public policy. Examples include facilitation of public participation in research, town hall meetings, public information sessions, internet and mobile-based surveys and open comment periods on proposed legislation. Hospitals promote patient participation by empowering patients to serve as advisers and decision makers, including on quality improvement teams, patient safety committees, and family-centered care councils. Similarly, foundations, nonprofit organizations, and government agencies can create funding mechanisms requiring and supporting patient participation in societal decisions and priority setting. \n\nSome aspects of PPI have been seen critically; in addition to those under HTA below, examples of general critical voices include a group of U.S. researchers presenting a framework in 2013 and a young Canadian speaker in 2018. The former warn that clinicians, delivery systems, and policy makers cannot assume that patients have certain capabilities, interests, or goals, nor can they dictate the pathway to achieving a patient's goals. The latter sees multiple potential conflicts of interest in the current arena of PPI.\n\nPatient participation in health technology assessment (HTA) is an approach to HTA which aims to include patients in the process of HTA. It is sometimes called consumer or patient engagement or consumer or patient involvement, although in HTA the latter term has been defined to include research into patients' needs, preferences and experiences as well as participation per se. In HTA, patient participation is also often used to include the participation of patient groups, patient advocates, and patients' families and carers in the process.\n\nAs HTA aims to help healthcare funders, such as governments, make decisions about health policy, it often involves the question as to whether to use broadly defined health \"technologies\", and if so, how and when; then patients comprise a key stakeholder in the HTA process. Additionally, because HTA seeks to assess if a heath technology produces useful outcomes for patients in real world settings (clinical effectiveness) that are good value for money (cost effectiveness), understanding patients' needs, preferences and experiences is essential.\n\nWhen patients take part in HTA, their knowledge gained from living with a condition and using treatments and services can add value to an HTA. Sometimes they are called experience-based experts or lay experts.\nPatients can add value to HTAs by providing real world insights (e.g. implications of benefits and side effects, variation in clinical practice) highlighting outcomes that matter, addressing gaps and uncertainties in the published literature, and contributing to the value construct that shapes assessments and decisions.\n\nIn 2017, a book was published on patient involvement in HTA (Eds Facey KM, Hansen HP, Single ANV) bringing together research, approaches, methods and case studies prepared by 80 authors. It demonstrates that practices vary between HTA bodies, and patients can potentially contribute at every stage of an HTA from scoping the questions asked about the health technology, providing input, interpreting the evidence, and drafting and communicating recommendations. It suggests that patient participation in HTA depends on two-way communication and is a dialogue for shared learning and problem solving. The approach taken should be driven by the goal of participation.\nThe most common way that patients take part in HTA is by providing written submissions and participating in expert meetings (for example as an equal member of an expert group or by attending an expert meeting to present information and answer questions).\n\nAlthough patient participation has been adopted and developed by a variety of HTA bodies around the world, there are limitations and criticisms of its use. These include concern about how and when to involve patients, the burden of participation for patients, representativeness of patients, the potential for conflict of interest (in relation to patient groups receiving funding from manufacturers), and lack of evaluation of patient participation. Facey et al. (2017) published the book on patient involvement in HTA to establish consistent terminology in the field and demonstrate a range of recognised approaches and methods found in published literature. Authors also highlighted the challenges of evaluation, rapid (short HTAs) and the problem of HTA bodies confusing patient input (information provided by patients and patient groups taking part in HTA) with patient-based evidence (robust research into patients' needs, preferences and experiences). The book itself is not open access, but the lead editor also published a paper on the topic six years before the larger collection. \nOne of the issues for patient participation in HTA is that HTA has often been constructed as a scientific process which must remain free from the subjective input of patients. \nLikewise, Gauvin et al. report that their \"analysis reveals that HTA agencies' role as bridges or boundary organizations situated at the frontier of research and policymaking causes the agencies to struggle with the idea of public involvement\" (page 1518).\n\nHowever, HTA would be better understood as a policy tool which critically reviews scientific evidence for a local context and this review is shaped by those involved in the process. There are many ways that public participation in HTA, including patients, can be implemented. In fact, an entire \"typology of issues\" has been developed by Gauvin et al., in which each type is \"related to the most appropriate public involvement methods\". Facey (2017) built on this work in Chapter 5 to describe it in detail for patient participation in HTA.\n\nSociologist Andrew Webster sees the problem as \"a failure to recognise that evaluation is a contested terrain involving different sorts of evidence related to different sorts of context (such as the \"experimental\" derived from clinical trials, \"evidential\", derived from existing clinical practice, and \"experiential\", based on patients' experiences of an intervention\".\n\nA further issue for patient participation in HTA is that of the individual versus the group. The HTAi's list available for endorsement on values for patient involvement express this issue as 'involvement ... contributes to equity by seeking to understand the diverse needs of patients with a particular health issue, balanced against the requirements of a health system that seeks to distribute resources fairly amongst all users'.\n\nKelly et al. explain (with their original citations shown here in brackets): \"From the moment Archie Cochrane linked questions of clinical effectiveness to cost effectiveness [17] and cost utility analysis was chosen as the basis for assessing value for money, [Evidence-Based Medicine] EBM and HTA have been framed within the utilitarian philosophical tradition. Utilitarianism is premised on the view that actions are good insofar as they maximize benefit for the greatest number [51]. This is not necessarily congruent with what is in the best interest of an individual patient [34].\"\n\nWorkshops in Denmark and Austria have resulted in calls to action to reinforce patients' role in SDM and health advocacy. The Danish workshop recommended the new Toolbox of resources for patient participation from the European Patients' Academy on Therapeutic Innovation (EUPATI). Furthermore the Danish workshop reported that the European Medicines Agency would be \"measuring the impact of patient involvement\", this being crucial to establishing credibility. And indeed, measurement is provided for in the place cited.\n\nIn Austria, a bibliography \"inter alia\" resulted from the most recent event in a workshop series continuing through 2019 entitled \"Toward a Shared Culture of Health: Enriching and Charting the Patient-Clinician Relationship held from 10–16 March 2017.\n\nIn the Netherlands, there is a debate about the relative value of patient participation versus decisions without explicitly empowering patients. Bovenkamp is one of the most vocal opponents challenging patients as stakeholders in clinical guideline development. \nAdonis is more positive in her shorter paper. Caron-Flinterman goes into more detail in her dissertation. She is cited in a more recent open-access survey laying out researchers' various views, especially on the ethical dimensions of engaging patients as partners within research teams. \n\nIn Norway, Nilsen et al. were critical of patients' role in health policy and clinical guideline development in their Cochrane Intervention Review. Two other Norwegian researchers, though, in unison with the workshop findings above, expand the list of areas where patients' views matter: \"The central arena for patient participation is the meeting between patient and health professional, but other important areas of involvement include decisions at the system and policy levels\".\n\nIn the United States there are several trends emerging with potential international implications: Health 2.0, artificial intelligence in healthcare (AI), the role of entrepreneurs, the value of patient participation in precision medicine and mobile health or MHealth, which will be dealt with in greater detail below.\n\nPrior to the recent advances in technology, patient participation was limited to shared decision making (SDM), a form of participation that occurred specifically between a patient and their physician in clinical practice, but can be regarded as a step forward. Changes in modern technology now allow computers to play an increasingly important role in healthcare decision making. Examples of artificial intelligence (AI) technology that is being used in healthcare include IBM's Watson Health, which is intended to assist in the diagnosis and treatment of difficult illnesses or disease. One of the Watson's objectives is to highlighting findings developed by Watson's computing skills and access to everyday information and give concrete suggestions that are tailored to the expertise of the physician, type of ailment, and needed level of care. Physicians can use ailment specific programs such as the Watson for Oncology app, which is aimed at the detection and treatment of tumors. Artificial intelligence is being used more frequently in patient participatory healthcare.\n\nEntrepreneurs have led the challenge to conventional health thinking since Craig Venter took on the NIH with the 1000 Genomes Project in 2008. Mike Milken, another entrepreneur and stock trader, founded the Santa Monica, California based Milken Institute. Following the institute's inception, Milken launched the Fastercures program, which \"brings together patient advocates, researchers, investors and policymakers from every sector of the medical research and development system to eliminate the roadblocks that get in the way of a faster cure\". The Fastercures program proposes patient-center improvements and advancements in the modern healthcare arena.\n\nIn 2010, the U.S. Government boosted patient participation by launching its own Patient-Centered Outcomes Research Institute. PCORI is striving to systematize its evaluation metrics to prove where results show improvement. PCORI was created by provisions in the Patient Protection and Affordable Care Act of 2010. The 501(c) organization has faced a great deal of scrutiny over funding, specifically when it was revealed PCORI was funded by a new tax originating from the Affordable Care Act.\n\nFour years after the spectacular 1000 Genomes Project hoped to call in a new era of precision medicine (PM), some opinion leaders have spoken up for reassessing the value of patient participation to be seen as a driver of PM. The Chancellor of the University of California, San Francisco, for instance, wrote an editorial in \"Science Translational Medicine\" calling for an amendment to the social contract to boost patient participation, citing a historical precedent: \"We need only look back to the human immunodeficiency virus (HIV)/AIDS epidemic during the 1980s to experience the power of patient advocacy combined with the dogged pursuit of scientific discovery and translation; clearly, motivated patients and scientists as well as their advocates can influence political, scientific, and regulatory agendas to drive advances in health.\"\n\nIn 2011 a seminal paper on the topic came out written by Hood and Friend.\n\nA second success story is that of the patient John W. Walsh, who founded Alphanet, which has funnelled tens of millions of dollars into research on chronic obstructive pulmonary disease or COPD.\n\nA more ethically ambivalent development involving patient-funded research involves so-called named patient programs and expanded access.\n\nTaking its name from the \"Web 2.0\" label given to describe the social-networking emphasis of the Internet since 2004, Health 2.0 is the use of web and social networking technologies to facilitate patient and physician interaction and engagement, usually through an online web platform or mobile application. Health 2.0 is sometimes used interchangeably with the term \"Medicine 2.0,\" however, both terms refer to a personalized health care experience that aims to increase collaboration between patients and providers, while increasing patient participation in their own health care. In addition to increased patient-physician interactions, Health 2.0 platforms seek to educate and empower patients through increased accessibility of their own health care information, such as lab reports or diagnoses. Some Health 2.0 platforms are also designed with remote medicine or telemedicine in mind, such as Hello Health. The advent of this communication method between patients and their medical providers is thought to change the way medicine is delivered, evidenced by a growing focus on innovating health technology, such as the annual Health 2.0 conference. One way Health 2.0 technologies can increase patient participation by actively engaging patients with their doctors is through the use of electronic health records, which are electronic versions of a physician's after-visit summaries. Electronic health records can also include the ability for patients to communicate to their physicians electronically for scheduling appointments or reaching out with questions. Other ways electronic health records can enhance patient participation include electronic health records that alert physicians to potentially dangerous drug interactions, reducing time to review a patient's medical history in an emergency situation, enhanced ability for managing chronic conditions like hypertension, and reducing costs through increased medical practice efficiency.\n\nMHealth is bringing promising solutions to meet the growing demand for care. With more and more evidence suggesting that the most effective treatment models involve specialized, multi-faceted approaches, and require a variety of materials and effort on both the physician's and patient's end. Mobile applications serve as both a method for increasing health literacy, and as a bridge for patient-physician communication (thus increasing patient participation). There are a broad number of ways to increase participation through the use of web-based and mobile applications. Live videoconferencing appointments have proven effective, especially in the field of mental health, and can be especially significant in providing services to low resource, rural communities. Patient reminders have increased patient participation in attending preventative screenings, and it is possible that similar reminders distributed automatically via web-based applications, such as patient portals, have the potential to provide similar benefits at a potentially lower cost.\n\nTo meet this demand for materials, production of patient-centered health applications is occurring at a rapid pace, with estimates of over 100,000 mobile applications available for use already. This boom in production has led to a developing concern regarding the amount of research and testing the application undergoes before going live, while others see promise in patient's having greater access to treatment materials. Some of that concern includes whether or not the patient will continue to use the mobile application specific for their treatment needs over time.\n\nPatient participation can include a broad spectrum of activities for human subjects during clinical trials and has become associated with several other words such as \"patient engagement\" or \"decision making\". This includes agenda-setting, clinical guideline development and clinical trial design. \n\nAccording to early career researchers working in the field of patient engagement in research, this research approach is still in its infancy and will become mainstream in 2023.\n\nIn the US, trends in patient participation have been influenced by a variety of sources and previous political movements. One such source for patient participation in clinical research was the AIDS epidemic in the 1980s and 1990s. During the epidemic, the AIDS activists argued not only for new clinical trial models, but for the importance of additional social service groups to support a wider range of potential human subjects. Since then, the FDA has taken several steps to include patients earlier in the drug development process. The authorization of Prescription Drug User Fee Act (PDUFA) V in 2012 included the Patient-Focused Drug Development (PFDD) initiative to provide the FDA with a way of hearing the patients' perspectives and concerns. Similarly, the European Medicine Agency (EMA) has been attempting to incorporate patient perspectives during the evaluation of medicinal products by the EMA scientific committees.\n\nThere has been an increased interest among healthcare providers, such as nurses, in cultivating patient participation. Due to this increased interest, studies have been done to assess the benefits and risks of patient participation and engagement in research. For benefits, patient engagement improves patient outcomes as well as clinical trial enrollment and retention. For risks, it has been proposed that the inclusion of patient participation may lead to extended research times and increased funding for clinical trials.\n\nTwo of the reasons to cultivate patient participation in clinical research have been the growth of patient organizations along with the development of databases and the concept of a patient or disease registry. Computer databases allow for the mass collection and dissemination of data. Registries, specifically, not only allow patients to access personal information but also allow physicians to review the outcomes and experiences of multiple patients who have received treatment with medicinal products . Furthermore, registries and patient participation have been particularly important to the development of rare-disease medicines. In the US, the Rare Diseases Clinical Research Network (RDCRN) was created in 2003 which includes a registry for patients afflicted with a rare-disease. This registry provides information to the patients and allows physicians to contact potential patients for enrollment in clinical trials. \"Patient registry\" is a developing term, and this 2013 open-access book provides a comprehensive description of the trend toward registries and their networks, i.e. the \"broader research collaboratives that connect individual registries\".\n\nPatients have a new resource to help them navigate the clinical trials landscape and find lay summaries of mediTal research in the OpenTrials database launched by the AllTrials campaign in 2016 as part of open data in medicine.\n\nPrecision medicine will change the conduct of clinical trials, and thus the role of patients as subjects. \"Key to making precision medicine mainstream is the ongoing shift in the relationship between patients and physicians\" comments N.J. Schork from the Venter Institute in \"Nature\". He cites as reasons for this development a growing interest in 'omics' assays and cheap and efficient devices that collect health data.\n\nA newer article in \"Nature\" outlines the conditions with which patent participation can be optimized, and it is co-authored by Jennifer Wagner et al.\n\nWith cancer being a prime target for precision medicine, Dr. Jennifer Carter, founder of the start-up N-of-One, recruits patients to \"participate\" in clinical trials to help find a cure for it.\n\n\n"}
{"id": "429542", "url": "https://en.wikipedia.org/wiki?curid=429542", "title": "Preterm birth", "text": "Preterm birth\n\nPreterm birth, also known as premature birth, is the birth of a baby at fewer than 37 weeks' gestational age. These babies are known as preemies or premies. Symptoms of preterm labor include uterine contractions which occur more often than every ten minutes or the leaking of fluid from the vagina. Premature infants are at greater risk for cerebral palsy, delays in development, hearing problems and sight problems. These risks are greater the earlier a baby is born.\nThe cause of preterm birth is often not known. Risk factors include diabetes, high blood pressure, being pregnant with more than one baby, being either obese or underweight, a number of vaginal infections, tobacco smoking and psychological stress, among others. It is recommended that labor not be medically induced before 39 weeks unless required for other medical reasons. The same recommendation applies to cesarean section. Medical reasons for early delivery include preeclampsia.\nIn those at risk, the hormone progesterone, if taken during pregnancy, may prevent preterm birth. Evidence does not support the usefulness of bed rest. It is estimated that at least 75% of preterm infants would survive with appropriate treatment, and the survival rate is highest among the infants born the latest. In women who might deliver between 24 and 37 weeks, corticosteroids improve outcomes. A number of medications, including nifedipine, may delay delivery so that a mother can be moved to where more medical care is available and the corticosteroids have a greater chance to work. Once the baby is born, care includes keeping the baby warm through skin to skin contact, supporting breastfeeding, treating infections and supporting breathing.\nPreterm birth is the most common cause of death among infants worldwide. About 15 million babies are preterm each year (5% to 18% of all deliveries). Approximately 0.5% of births are extremely early periviable births, and these account for most of the deaths. In many countries, rates of premature births have increased between the 1990s and 2010s. Complications from preterm births resulted in 0.81 million deaths in 2015 down from 1.57 million in 1990. The chance of survival at 22 weeks is about 6%, while at 23 weeks it is 26%, 24 weeks 55% and 25 weeks about 72%. The chances of survival without any long-term difficulties are lower.\n\nIn humans, the usual definition of preterm birth is birth before a gestational age of 37 complete weeks. In the normal human fetus, several organ systems mature between 34 and 37 weeks, and the fetus reaches adequate maturity by the end of this period. One of the main organs greatly affected by premature birth is the lungs. The lungs are one of the last organs to mature in the womb; because of this, many premature babies spend the first days and weeks of their lives on ventilators. Therefore, a significant overlap exists between preterm birth and prematurity. Generally, preterm babies are premature and term babies are mature. Preterm babies born near 37 weeks often have no problems relating to prematurity if their lungs have developed adequate surfactant, which allows the lungs to remain expanded between breaths. Sequelae of prematurity can be reduced to a small extent by using drugs to accelerate maturation of the fetus, and to a greater extent by preventing preterm birth.\n\nPreterm birth causes a range of problems.\n\nThe main categories of causes of preterm birth are preterm labor induction and spontaneous preterm labor. Signs and symptoms of preterm labor include four or more uterine contractions in one hour. In contrast to false labour, true labor is accompanied by cervical dilatation and effacement. Also, vaginal bleeding in the third trimester, heavy pressure in the pelvis, or abdominal or back pain could be indicators that a preterm birth is about to occur. A watery discharge from the vagina may indicate premature rupture of the membranes that surround the baby. While the rupture of the membranes may not be followed by labor, usually delivery is indicated as infection (chorioamnionitis) is a serious threat to both fetus and mother. In some cases, the cervix dilates prematurely without pain or perceived contractions, so that the mother may not have warning signs until very late in the birthing process.\n\nA review into using uterine monitoring at home to detect contractions and possible preterm births in women at higher risk of having a preterm baby found that it did not reduce the number of preterm births. The research included in the review was poor quality but it showed that home monitoring may increase the number of unplanned antenatal visits and may reduce the number of babies admitted to special care when compared with women receiving normal antenatal care.\n\nIn the U.S. where many neonatal infections and other causes of neonatal death have been markedly reduced, prematurity is the leading cause of neonatal mortality at 25%. Prematurely born infants are also at greater risk for having subsequent serious chronic health problems as discussed below.\n\nThe earliest gestational age at which the infant has at least a 50% chance of survival is referred to as the limit of viability. As NICU care has improved over the last 40 years, the limit of viability has reduced to approximately 24 weeks. Most newborns who die, and 40% of older infants who die, were born between 20 and 25.9 weeks (gestational age), during the second trimester.\n\nAs risk of brain damage and developmental delay is significant at that threshold even if the infant survives, there are ethical controversies over the aggressiveness of the care rendered to such infants. The limit of viability has also become a factor in the abortion debate.\n\nPreterm infants usually show physical signs of prematurity in reverse proportion to the gestational age. As a result, they are at risk for numerous medical problems affecting different organ systems.\n\nA study of 241 children born between 22 and 25 weeks who were currently of school age found that 46 percent had severe or moderate disabilities such as cerebral palsy, vision or hearing loss and learning problems. Thirty-four percent were mildly disabled and 20 percent had no disabilities, while 12 percent had disabling cerebral palsy.\n\nThe exact cause of preterm birth is difficult to determine and it may be multi-factorial. The cause of 50% of preterm births is never determined. Labor is a complex process involving many factors. Four different pathways have been identified that can result in preterm birth and have considerable evidence: precocious fetal endocrine activation, uterine overdistension (placental abruption), decidual bleeding, and intrauterine inflammation/infection.\n\nIdentifying women at high risk of giving birth early would enable the health services to provide specialized care for these women to delay the birth or make sure they are in the best place to give birth (for example a hospital with a special care baby unit). Risk scoring systems have been suggested as a possible way of identifying these women. However, there is no research in this area so it is unclear whether the risk scoring systems would prolong pregnancy and reduce the numbers of preterm births or not.\n\nA number of factors have been identified that are linked to a higher risk of a preterm birth such as being less than 18 years of age. Maternal height and weight can play a role.\n\nFurther, in the U.S. and the UK, black women have preterm birth rates of 15–18%, more than double than that of the white population. Filipinos are also at high risk of premature birth, and it is believed that nearly 11–15% of Filipinos born in the U.S. (compared to other Asians at 7.6% and whites at 7.8%) are premature. Filipinos being a big risk factor is evidenced with the Philippines being the 8th highest ranking in the world for preterm births, the only non-African country in the top 10. This discrepancy is not seen in comparison to other Asian groups or Hispanic immigrants and remains unexplained.\n\nPregnancy interval makes a difference as women with a six-month span or less between pregnancies have a two-fold increase in preterm birth. Studies on type of work and physical activity have given conflicting results, but it is opined that stressful conditions, hard labor, and long hours are probably linked to preterm birth.\n\nA history of spontaneous (i.e., miscarriage) or surgical abortion has been associated with a small increase in the risk of preterm birth, with an increased risk with increased number of abortions, although it is unclear whether the increase is caused by the abortion or by confounding risk factors (e.g., socioeconomic status). Increased risk has not been shown in women who terminated their pregnancies medically. Pregnancies that are unwanted or unintended are also a risk factor for preterm birth.\n\nAdequate maternal nutrition is important. Women with a low BMI are at increased risk for preterm birth. Further, women with poor nutrition status may also be deficient in vitamins and minerals. Adequate nutrition is critical for fetal development and a diet low in saturated fat and cholesterol may help reduce the risk of a preterm delivery. Obesity does not directly lead to preterm birth; however, it is associated with diabetes and hypertension which are risk factors by themselves. To some degree those individuals may have underlying conditions (i.e., uterine malformation, hypertension, diabetes) that persist.\n\nWomen with celiac disease have an increased risk of the development of preterm birth. The risk of preterm birth is more elevated when celiac disease remains undiagnosed and untreated.\n\nMarital status is associated with risk for preterm birth. A study of 25,373 pregnancies in Finland revealed that unmarried mothers had more preterm deliveries than married mothers (P=0.001). Pregnancy outside of marriage was associated overall with a 20% increase in total adverse outcomes, even at a time when Finland provided free maternity care. A study in Quebec of 720,586 births from 1990 to 1997 revealed less risk of preterm birth for infants with legally married mothers compared with those with common-law wed or unwed parents.\n\nGenetic make-up is a factor in the causality of preterm birth. Genetics has been a big factor into why Filipinos have a high risk of premature birth as the Filipinos have a large prevalence of mutations that help them be predisposed to premature births. An intra- and transgenerational increase in the risk of preterm delivery has been demonstrated. No single gene has been identified.\n\nSubfertility is associated with preterm birth. Couples who have tried more than 1 year versus those who have tried less than 1 year before achieving a spontaneous conception have an adjusted odds ratio of 1.35 (95% confidence interval 1.22-1.50) of preterm birth. Pregnancies after IVF confers a greater risk of preterm birth than spontaneous conceptions after more than 1 year of trying, with an adjusted odds ratio of 1.55 (95% CI 1.30-1.85).\n\nThe use of fertility medication that stimulates the ovary to release multiple eggs and of IVF with embryo transfer of multiple embryos has been implicated as an important factor in preterm birth. Maternal medical conditions increase the risk of preterm birth. Often labor has to be induced for medical reasons; such conditions include high blood pressure, pre-eclampsia, maternal diabetes, asthma, thyroid disease, and heart disease.\n\nIn a number of women anatomical issues prevent the baby from being carried to term. Some women have a weak or short cervix (the strongest predictor of premature birth) Women with vaginal bleeding during pregnancy are at higher risk for preterm birth. While bleeding in the third trimester may be a sign of placenta previa or placental abruption – conditions that occur frequently preterm – even earlier bleeding that is not caused by these conditions is linked to a higher preterm birth rate. Women with abnormal amounts of amniotic fluid, whether too much (polyhydramnios) or too little (oligohydramnios), are also at risk.\nThe mental status of the women is of significance. Anxiety and depression have been linked to preterm birth.\n\nFinally, the use of tobacco, cocaine, and excessive alcohol during pregnancy increases the chance of preterm delivery. Tobacco is the most commonly abused drug during pregnancy and contributes significantly to low birth weight delivery. Babies with birth defects are at higher risk of being born preterm.\n\nPassive smoking and/or smoking before the pregnancy influences the probability of a preterm birth. The World Health Organization published an international study in March 2014.\n\nPresence of anti-thyroid antibodies is associated with an increased risk preterm birth with an odds ratio of 1.9 and 95% confidence interval of 1.1–3.5.\n\nA 2004 systematic review of 30 studies on the association between intimate partner violence and birth outcomes concluded that preterm birth and other adverse outcomes, including death, are higher among abused pregnant women than among non-abused women.\n\nThe Nigerian cultural method of abdominal massage has been shown to result in 19% preterm birth among women in Nigeria, plus many other adverse outcomes for the mother and baby. This ought not be confused with massage conducted by a fully trained and licensed massage therapist or by significant others trained to provide massage during pregnancy, which has been shown to have numerous positive results during pregnancy, including the reduction of preterm birth, less depression, lower cortisol, and reduced anxiety.\n\nThe frequency of infection in preterm birth is inversely related to the gestational age. \"Mycoplasma genitalium\" infection is associated with increased risk of preterm birth, and spontaneous abortion.\n\nInfectious microorganisms can be ascending, hematogeneous, iatrogenic by a procedure, or retrograde through the Fallopian tubes. From the deciduas they may reach the space between the amnion and chorion, the amniotic fluid, and the fetus. A chorioamnionitis also may lead to sepsis of the mother. Fetal infection is linked to preterm birth and to significant long-term handicap including cerebral palsy.\n\nIt has been reported that asymptomatic colonization of the decidua occurs in up to 70% of women at term using a DNA probe suggesting that the presence of micro-organism alone may be insufficient to initiate the infectious response.\n\nAs the condition is more prevalent in black women in the US and the UK, it has been suggested to be an explanation for the higher rate of preterm birth in these populations. It is opined that bacterial vaginosis before or during pregnancy may affect the decidual inflammatory response that leads to preterm birth. The condition known as aerobic vaginitis can be a serious risk factor for preterm labor; several previous studies failed to acknowledge the difference between aerobic vaginitis and bacterial vaginosis, which may explain some of the contradiction in the results.\n\nUntreated yeast infections are associated with preterm birth.\n\nA review into prophylactic antibiotics (given to prevent infection) in the second and third trimester of pregnancy (13–42 weeks of pregnancy) found a reduction in the number of preterm births in women with bacterial vaginosis. These antibiotics also reduced the number of waters breaking before labor in full-term pregnancies, reduced the risk of infection of the lining of the womb after delivery (endometritis), and rates of gonococcal infection. However, the women without bacterial vaginosis did not have any reduction in preterm births or pre-labor preterm waters breaking. Much of the research included in this review lost participants during follow-up so did not report the long-term effects of the antibiotics on mothers or babies. More research in this area is needed to find the full effects of giving antibiotics throughout the second and third trimesters of pregnancy.\n\nA number of maternal bacterial infections are associated with preterm birth including pyelonephritis, asymptomatic bacteriuria, pneumonia, and appendicitis. A review into giving antibiotics in pregnancy for asymptomatic bacteriuria (urine infection with no symptoms) found the research was of very low quality but that it did suggest that taking antibiotics reduced the numbers of preterm births and babies with low birth weight. Another review found that one dose of antibiotics did not seem as effective as a course of antibiotics but fewer women reported side effects from one dose. This review recommended that more research is needed to discover the best way of treating asymptomatic bacteriuria.\n\nA different review found that preterm births happened less for pregnant women who had routine testing for low genital tract infections than for women who only had testing when they showed symptoms of low genital tract infections. The women being routinely tested also gave birth to fewer babies with a low birth weight. Even though these results look promising, the review was only based on one study so more research is needed into routine screening for low genital tract infections.\n\nAlso periodontal disease has been shown repeatedly to be linked to preterm birth. In contrast, viral infections, unless accompanied by a significant febrile response, are considered not to be a major factor in relation to preterm birth.\n\nThere is believed to be a maternal genetic component in preterm birth. Estimated heritability of timing-of-birth in women was 34%. However, the occurrence of preterm birth in families does not follow a clear inheritance pattern, thus supporting the idea that preterm birth is a non-Mendelian trait with a polygenic nature.\n\nPlacental alpha microglobulin-1 (PAMG-1) has been the subject of several investigations evaluating its ability to predict imminent spontaneous preterm birth in women with signs, symptoms, or complaints suggestive of preterm labor. In one investigation comparing this test to fetal fibronectin testing and cervical length measurement via transvaginal ultrasound, the test for PAMG-1 (commercially known as the PartoSure test) has been reported to be the single best predictor of imminent spontaneous delivery within 7 days of a patient presenting with signs, symptoms, or complaints of preterm labor. Specifically, the PPV, or positive predictive value, of the tests were 76%, 29%, and 30% for PAMG-1, fFN and CL, respectively (P < 0.01).\n\nFetal fibronectin (fFN) has become an important biomarker—the presence of this glycoprotein in the cervical or vaginal secretions indicates that the border between the chorion and deciduas has been disrupted. A positive test indicates an increased risk of preterm birth, and a negative test has a high predictive value. It has been shown that only 1% of women in questionable cases of preterm labor delivered within the next week when the test was negative.\n\nObstetric ultrasound has become useful in the assessment of the cervix in women at risk for premature delivery. A short cervix preterm is undesirable: A cervical length of less than 25 mm at or before 24 weeks of gestational age is the most common definition of cervical incompetence.\n\nHistorically efforts have been primarily aimed to improve survival and health of preterm infants (tertiary intervention). Such efforts, however, have not reduced the incidence of preterm birth. Increasingly primary interventions that are directed at all women, and secondary intervention that reduce existing risks are looked upon as measures that need to be developed and implemented to prevent the health problems of premature infants and children. Smoking bans are effective in decreasing preterm births.\n\nAdoption of specific professional policies can immediately reduce risk of preterm birth as the experience in assisted reproduction has shown when the number of embryos during embryo transfer was limited.\nMany countries have established specific programs to protect pregnant women from hazardous or night-shift work and to provide them with time for prenatal visits and paid pregnancy-leave. The EUROPOP study showed that preterm birth is not related to type of employment, but to prolonged work (over 42 hours per week) or prolonged standing (over 6 hours per day). Also, night work has been linked to preterm birth. Health policies that take these findings into account can be expected to reduce the rate of preterm birth.\nPreconceptional intake of folic acid is recommended to reduce birth defects. There is significant evidence that long-term (> one year) use of folic acid supplement preconceptionally may reduce premature birth. Reducing smoking is expected to benefit pregnant women and their offspring.\n\nHealthy eating can be instituted at any stage of the pregnancy including nutritional adjustments, use of vitamin supplements, and smoking cessation. Calcium supplementation in women who have low dietary calcium reduces the number of negative outcomes including preterm birth, pre-eclampsia, and maternal death. The World Health Organization (WHO) suggests 1.5–2 g of calcium supplements daily, for pregnant women who have low levels calcium in their diet. Supplemental intake of C and E vitamins have not been found to reduce preterm birth rates. Different strategies are used in the administration of prenatal care, and future studies need to determine if the focus can be on screening for high-risk women, or widened support for low-risk women, or to what degree these approaches can be merged. While periodontal infection has been linked with preterm birth, randomized trials have not shown that periodontal care during pregnancy reduces preterm birth rates.\n\nScreening for asymptomatic bacteriuria followed by appropriate treatment reduces pyelonephritis and reduces the risk of preterm birth. Extensive studies have been carried out to determine if other forms of screening in low-risk women followed by appropriate intervention are beneficial, including: Screening for and treatment of Ureaplasma urealyticum, group B streptococcus, Trichomonas vaginalis, and bacterial vaginosis did not reduce the rate of preterm birth. Routine ultrasound examination of the length of the cervix identifies patients at risk, but cerclage is not proven useful, and the application of a progestogen is under study. Screening for the presence of fibronectin in vaginal secretions is not recommended at this time in women at low risk.\n\nSelf-care methods to reduce the risk of preterm birth include proper nutrition, avoiding stress, seeking appropriate medical care, avoiding infections, and the control of preterm birth risk factors (e.g. working long hours while standing on feet, carbon monoxide exposure, domestic abuse, and other factors). Self-monitoring vaginal pH followed by yogurt treatment or clindamycin treatment if the pH was too high all seem to be effective at reducing the risk of preterm birth. Additional support during pregnancy does not appear to prevent low birthweight or preterm birth.\n\nWomen are identified to be at increased risk for preterm birth on the basis of their past obstetrical history or the presence of known risk factors. Preconception intervention can be helpful in selected patients in a number of ways. Patients with certain uterine anomalies may have a surgical correction (i.e. removal of a uterine septum), and those with certain medical problems can be helped by optimizing medical therapies prior to conception, be it for asthma, diabetes, hypertension and others.\n\nIn multiple pregnancies, which often result from use of assisted reproductive technology, there is a high risk of preterm birth. Selective reduction is used to reduce the number of fetuses to two or three.\n\nA number of agents have been studied for the secondary prevention of indicated preterm birth. Trials using low-dose aspirin, fish oil, vitamin C and E, and calcium to reduce preeclampsia demonstrated some reduction in preterm birth only when low-dose aspirin was used. Even if agents such as calcium or antioxidants were able to reduce preeclampsia, a resulting decrease in preterm birth was not observed.\n\nReduction in activity by the mother—pelvic rest, limited work, bed rest—may be recommended although there is no evidence it is useful with some concerns it is harmful. Increasing medical care by more frequent visits and more education has not been shown to reduce preterm birth rates. Use of nutritional supplements such as omega-3 polyunsaturated fatty acids is based on the observation that populations who have a high intake of such agents are at low risk for preterm birth, presumably as these agents inhibit production of proinflammatory cytokines. A randomized trial showed a significant decline in preterm birth rates, and further studies are in the making.\n\nWhile antibiotics can get rid of bacterial vaginosis in pregnancy, this does not appear to change the risk of preterm birth. It has been suggested that chronic chorioamnionitis is not sufficiently treated by antibiotics alone (and therefore they cannot ameliorate the need for preterm delivery in this condition).\n\nProgestogens, often given in the form of progesterone or hydroxyprogesterone caproate, relaxes the uterine musculature, maintains cervical length, and has anti-inflammatory properties, and thus exerts activities expected to be beneficial in reducing preterm birth. Two meta-analyses demonstrated a reduction in the risk of preterm birth in women with recurrent preterm birth by 40–55%.\n\nProgestogen supplementation also reduces the frequency of preterm birth in pregnancies where there is a short cervix. However, progestogens are not effective in all populations, as a study involving twin gestations failed to see any benefit.\n\nIn preparation for childbirth, the woman's cervix shortens. Preterm cervical shortening is linked to preterm birth and can be detected by ultrasonography. Cervical cerclage is a surgical intervention that places a suture around the cervix to prevent its shortening and widening. Numerous studies have been performed to assess the value of cervical cerclage and the procedure appears helpful primarily for women with a short cervix and a history of preterm birth. Instead of a prophylactic cerclage, women at risk can be monitored during pregnancy by sonography, and when shortening of the cervix is observed, the cerclage can be performed.\n\nAbout 75% of nearly a million deaths due to preterm deliver would survive if provided warmth, breastfeeding, treatments for infection, and breathing support. If a baby has cardiac arrest at birth and is before 23 weeks or less than 400 g attempts at resuscitation are not indicated.\n\nTertiary interventions are aimed at women who are about to go into preterm labor, or rupture the membranes or bleed preterm. The use of the fibronectin test and ultrasonography improves the diagnostic accuracy and reduces false-positive diagnosis. While treatments to arrest early labor where there is progressive cervical dilatation and effacement will not be effective to gain sufficient time to allow the fetus to grow and mature further, it may defer delivery sufficiently to allow the mother to be brought to a specialized center that is equipped and staffed to handle preterm deliveries. In a hospital setting women are hydrated via intravenous infusion (as dehydration can lead to premature uterine contractions).\n\nSeverely premature infants may have underdeveloped lungs because they are not yet producing their own surfactant. This can lead directly to respiratory distress syndrome, also called hyaline membrane disease, in the neonate. To try to reduce the risk of this outcome, pregnant mothers with threatened premature delivery prior to 34 weeks are often administered at least one course of glucocorticoids, a steroid that crosses the placental barrier and stimulates the production of surfactant in the lungs of the baby. Steroid use up to 37 weeks is also recommended by the American Congress of Obstetricians and Gynecologists. Typical glucocorticoids that would be administered in this context are betamethasone or dexamethasone, often when the pregnancy has reached viability at 23 weeks.\n\nIn cases where premature birth is imminent, a second \"rescue\" course of steroids may be administered 12 to 24 hours before the anticipated birth. There are still some concerns about the efficacy and side effects of a second course of steroids, but the consequences of RDS are so severe that a second course is often viewed as worth the risk. A 2015 Cochrane review supports the use of repeat dose(s) of prenatal corticosteroids for women still at risk of preterm birth seven days or more after an initial course.\n\nBeside reducing respiratory distress, other neonatal complications are reduced by the use of glucocorticosteroids, namely intraventricular bleeding, necrotising enterocolitis, and patent ductus arteriosus. A single course of antenatal corticosteroids could be considered routine for preterm delivery, but there are some concerns about applicability of this recommendation to low-resource settings with high rates of infections. It remains unclear whether one corticosteroid (or one particular regimen) has advantages over another.\n\nConcerns about adverse effects of prenatal corticosteroids include increased risk for maternal infection, difficulty with diabetic control, and possible long-term effects on neurodevelopmental outcomes for the infants. There is ongoing discussion about when steroids should be given (i.e. only antenatally or postnatally too) and for how long (i.e. single course or repeated administration). Despite these unknowns, there is a consensus that the benefits of a single course of prenatal glucocorticosteroids vastly outweigh the potential risks.\n\nThe routine administration of antibiotics to all women with threatened preterm labor reduces the risk of the baby to get infected with group B streptococcus and has been shown to reduce related mortality rates.\n\nWhen membranes rupture prematurely, obstetrical management looks for development of labor and signs of infection. Prophylactic antibiotic administration has been shown to prolong pregnancy and reduced neonatal morbidity with rupture of membranes at less than 34 weeks. Because of concern about necrotizing enterocolitis, amoxicillin or erythromycin has been recommended, but not amoxicillin + clavulanic acid (co-amoxiclav).\n\nA number of medications may be useful to delay delivery including: nonsteroidal anti-inflammatory drugs, calcium channel blockers, beta mimetics, and atosiban. Tocolysis rarely delays delivery beyond 24–48 hours. This delay, however, may be sufficient to allow the pregnant woman to be transferred to a center specialized for management of preterm deliveries and give administered corticosteroids to reduce neonatal organ immaturity. Meta-analyses indicate that calcium-channel blockers and an oxytocin antagonist can delay delivery by 2–7 days, and β2-agonist drugs delay by 48 hours but carry more side effects. Magnesium sulfate does not appear to be useful to prevent preterm birth. Its use before delivery, however, does appear to decrease the risk of cerebral palsy.\n\nThe routine use of caesarean section for early delivery of infants expected to have very low birth weight is controversial, and a decision concerning the route and time of delivery probably needs to be made on a case by case basis.\n\nAfter delivery, plastic wraps or warm mattresses are useful to keep the infant warm on their way to the neonatal intensive care unit (NICU). In developed countries premature infants are usually cared for in an NICU. The physicians who specialize in the care of very sick or premature babies are known as neonatologists. In the NICU, premature babies are kept under radiant warmers or in incubators (also called isolettes), which are bassinets enclosed in plastic with climate control equipment designed to keep them warm and limit their exposure to germs. Modern neonatal intensive care involves sophisticated measurement of temperature, respiration, cardiac function, oxygenation, and brain activity. Treatments may include fluids and nutrition through intravenous catheters, oxygen supplementation, mechanical ventilation support, and medications. In developing countries where advanced equipment and even electricity may not be available or reliable, simple measures such as \"kangaroo care\" (skin to skin warming), encouraging breastfeeding, and basic infection control measures can significantly reduce preterm morbidity and mortality. Bili lights may also be used to treat newborn jaundice (hyperbilirubinemia).\n\nWater can be carefully provided to prevent dehydration but no so much to increase risks of side effects.\n\nIn a 2012 policy statement, the American Academy of Pediatrics recommended feeding preterm infants human milk, finding \"significant short- and long-term beneficial effects,\" including lower rates of necrotizing enterocolitis (NEC).\n\nThe chance of survival at 22 weeks is about 6%, while at 23 weeks it is 26%, 24 weeks 55% and 25 weeks about 72%. The chances of survival without long-term difficulties is less. In the developed world overall survival is about 90% while in low-income countries survival rates are about 10%.\n\nSome children will adjust well during childhood and adolescence, although disability is more likely nearer the limits of viability. A large study followed children born between 22 and 25 weeks until the age of 6 years old. Of these children, 46 percent had moderate to severe disabilities such as cerebral palsy, vision or hearing loss and learning disabilities, 34 percent had mild disabilities, and 20 percent had no disabilities. Twelve percent had disabling cerebral palsy.\n\nAs survival has improved, the focus of interventions directed at the newborn has shifted to reduce long-term disabilities, particularly those related to brain injury. Some of the complications related to prematurity may not be apparent until years after the birth. A long-term study demonstrated that the risks of medical and social disabilities extend into adulthood and are higher with decreasing gestational age at birth and include cerebral palsy, intellectual disability, disorders of psychological development, behavior, and emotion, disabilities of vision and hearing, and epilepsy. Standard intelligence tests showed that 41 percent of children born between 22 and 25 weeks had moderate or severe learning disabilities when compared to the test scores of a group of similar classmates who were born at full-term. It is also shown that higher levels of education were less likely to be obtained with decreasing gestational age at birth. People born prematurely may be more susceptible to developing depression as teenagers.\nSome of these problems can be described as being within the executive domain and have been speculated to arise due to decreased myelinization of the frontal lobes. Studies of people born premature and investigated later with MRI brain imaging, demonstrate qualitative anomalies of brain structure and grey matter deficits within temporal lobe structures and the cerebellum that persist into adolescence. Throughout life they are more likely to require services provided by physical therapists, occupational therapists, or speech therapists.\n\nDespite the neurosensory, mental and educational problems studied in school age and adolescent children born extremely preterm, the majority of preterm survivors born during the early years of neonatal intensive care are found to do well and to live fairly normal lives in young adulthood. Young adults born preterm seem to acknowledge that they have more health problems than their peers, yet feel the same degree of satisfaction with their quality of life.\n\nBeyond the neurodevelopmental consequences of prematurity, infants born preterm have a greater risk for many other health problems. For instance, children born prematurely have an increased risk for developing chronic kidney disease.\n\nPreterm birth complicates the births of infants worldwide affecting 5% to 18% of births. In Europe and many developed countries the preterm birth rate is generally 5–9%, and in the USA it has even risen to 12–13% in the last decades.\n\nAs weight is easier to determine than gestational age, the World Health Organization tracks rates of low birth weight (< 2,500 grams), which occurred in 16.5 percent of births in less developed regions in 2000. It is estimated that one third of these low birth weight deliveries are due to preterm delivery. Weight generally correlates to gestational age, however, infants may be underweight for other reasons than a preterm delivery. Neonates of low birth weight (LBW) have a birth weight of less than 2500 g (5 lb 8 oz) and are mostly but not exclusively preterm babies as they also include \"small for gestational age\" (SGA) babies. Weight-based classification further recognizes \"Very Low Birth Weight\" (VLBW) which is less than 1,500 g, and \"Extremely Low Birth Weight\" (ELBW) which is less than 1,000 g. Almost all neonates in these latter two groups are born preterm.\n\nComplications from preterm births resulted in 740,000 deaths in 2013, down from 1.57 million in 1990.\n\nPreterm birth is a significant cost factor in healthcare, not even considering the expenses of long-term care for individuals with disabilities due to preterm birth. A 2003 study in the US determined neonatal costs to be $224,400 for a newborn at 500–700 g versus $1,000 at over 3,000 g. The costs increase exponentially with decreasing gestational age and weight.\nThe 2007 Institute of Medicine report \"Preterm Birth\" found that the 550,000 premature babies born each year in the U.S. run up about $26 billion in annual costs, mostly related to care in neonatal intensive care units, but the real tab may top $50 billion.\n\nJames Elgin Gill (born on 20 May 1987 in Ottawa, Ontario, Canada) was the earliest premature baby in the world, until that record was broken in 2014. He was 128 days premature (21 weeks and 5 days' gestation) and weighed . He survived.\n\nIn 2014, a baby girl born in San Antonio, Texas, U.S. became the youngest premature baby in the world. The daughter of Courtney Stensrud was born at 21 weeks 4 days and weighed 410 grams (less than a pound). Kaashif Ahmad resuscitated the baby after she was born. As of 2017, the girl was in preschool and on par with her peers.\n\nAmillia Taylor is also often cited as the most premature baby. She was born on 24 October 2006 in Miami, Florida, U.S. at 21 weeks and 6 days' gestation. This report has created some confusion as her gestation was measured from the date of conception (through \"in vitro\" fertilization) rather than the date of her mother's last menstrual period, making her appear 2 weeks younger than if gestation was calculated by the more common method. At birth, she was long and weighed . She suffered digestive and respiratory problems, together with a brain hemorrhage. She was discharged from the Baptist Children's Hospital on 20 February 2007.\n\nThe record for the smallest premature baby to survive was held for a considerable amount of time by Madeline Mann, who was born in 1989 at 26 weeks, weighing and measuring long. This record was broken in September 2004 by Rumaisa Rahman, who was born in the same hospital at 25 weeks' gestation. At birth, she was long and weighed . Her twin sister was also a small baby, weighing at birth. During pregnancy their mother had suffered from pre-eclampsia, which causes dangerously high blood pressure putting the baby into distress and requiring birth by caesarean section. The larger twin left the hospital at the end of December, while the smaller remained there until 10 February 2005 by which time her weight had increased to . Generally healthy, the twins had to undergo laser eye surgery to correct vision problems, a common occurrence among premature babies.\n\nThe world's smallest premature boy to survive was born in February 2009 at Children's Hospitals and Clinics of Minnesota in Minneapolis, Minnesota, U.S.. Jonathon Whitehill was born at 25 weeks' gestation with a weight of . He was hospitalized in a neonatal intensive care unit for five months, and then discharged.\n\nHistorical figures who were born prematurely include Johannes Kepler (born in 1571 at seven months' gestation), Isaac Newton (born in 1642, small enough to fit into a quart mug, according to his mother), Winston Churchill (born in 1874 at seven months' gestation), and Anna Pavlova (born in 1885 at seven months' gestation),\n\nThe transformation of medical care means that extremely premature and very ill babies have better chances of survival than ever before. But it is difficult to predict which babies will die and which will live, though possibly with severe disabilities. As a consequence, families and health professionals have to make complex decisions about how much intervention is necessary or justifiable.\n\nThe most difficult decisions are about whether or not to resuscitate a premature baby and admit him or her to neonatal intensive care, or whether to withdraw intensive care and give the child palliative care.\n\nThis is discussed at great length in a report \"Critical care decisions in fetal and neonatal medicine: ethical issues\" produced by the London-based Nuffield Council for Bioethics.\n"}
{"id": "53912950", "url": "https://en.wikipedia.org/wiki?curid=53912950", "title": "Preventive nutrition", "text": "Preventive nutrition\n\nPreventive nutrition is a branch of nutrition science with the goal of preventing or delaying or reducing the impacts of disease and disease-related complications. It is concerned with a high level of personal well-being, disease prevention, and diagnosis of recurring health problems or symptoms of discomfort which are often precursors to health issues.\n\n\n"}
{"id": "463734", "url": "https://en.wikipedia.org/wiki?curid=463734", "title": "Public health", "text": "Public health\n\nPublic health is \"the science and art of preventing disease, prolonging life and promoting human health through organized efforts and informed choices of society, organizations, public and private, communities and individuals\". Analyzing the health of a population and the threats is the basis for public health. The \"public\" in question can be as small as a handful of people, an entire village or it can be as large as several continents, in the case of a pandemic. \"Health\" takes into account physical, mental and social well-being. It is not merely the absence of disease or infirmity, according to the World Health Organization. Public health is interdisciplinary. For example, epidemiology, biostatistics and health services are all relevant. Environmental health, community health, behavioral health, health economics, public policy, mental health and occupational safety, gender issues in health, sexual and reproductive health are other important subfields.\n\nPublic health aims to improve the quality of life through prevention and treatment of disease, including mental health. This is done through the surveillance of cases and health indicators, and through the promotion of healthy behaviors. Common public health initiatives include promoting handwashing and breastfeeding, delivery of vaccinations, suicide prevention and distribution of condoms to control the spread of sexually transmitted diseases.\n\nModern public health practice requires multidisciplinary teams of public health workers and professionals. Teams might include epidemiologists, biostatisticians, medical assistants, public health nurses, midwives, medical microbiologists, economists, sociologists, geneticists and data managers. Depending on the need environmental health officers or public health inspectors, bioethicists, and even veterinarians, gender experts, sexual and reproductive health specialists might be called on.\n\nAccess to health care and public health initiatives are difficult challenges in developing countries. Public health infrastructures are still forming in those countries.\n\nThe focus of a public health intervention is to prevent and manage diseases, injuries and other health conditions through surveillance of cases and the promotion of healthy behaviors, communities and environments. Many diseases are preventable through simple, nonmedical methods. For example, research has shown that the simple act of handwashing with soap can prevent the spread of many contagious diseases. In other cases, treating a disease or controlling a pathogen can be vital to preventing its spread to others, either during an outbreak of infectious disease or through contamination of food or water supplies. Public health communications programs, vaccination programs and distribution of condoms are examples of common preventive public health measures. Measures such as these have contributed greatly to the health of populations and increases in life expectancy.\n\nPublic health plays an important role in disease prevention efforts in both the developing world and in developed countries through local health systems and non-governmental organizations. The World Health Organization (WHO) is the international agency that coordinates and acts on global public health issues. Most countries have their own government public health agencies, sometimes known as ministries of health, to respond to domestic health issues. For example, in the United States, the front line of public health initiatives are state and local health departments. The United States Public Health Service (PHS), led by the Surgeon General of the United States, and the Centers for Disease Control and Prevention, headquartered in Atlanta, are involved with several international health activities, in addition to their national duties. In Canada, the Public Health Agency of Canada is the national agency responsible for public health, emergency preparedness and response, and infectious and chronic disease control and prevention. The Public health system in India is managed by the Ministry of Health & Family Welfare of the government of India with state-owned health care facilities.\n\nMost governments recognize the importance of public health programs in reducing the incidence of disease, disability, and the effects of aging and other physical and mental health conditions. However, public health generally receives significantly less government funding compared with medicine. Public health programs providing vaccinations have made strides in promoting health, including the eradication of smallpox, a disease that plagued humanity for thousands of years.\nThe World Health Organization (WHO) identifies core functions of public health programs including:\n\nIn particular, public health surveillance programs can:\n\nPublic health surveillance has led to the identification and prioritization of many public health issues facing the world today, including HIV/AIDS, diabetes, waterborne diseases, zoonotic diseases, and antibiotic resistance leading to the reemergence of infectious diseases such as tuberculosis. Antibiotic resistance, also known as drug resistance, was the theme of World Health Day 2011. Although the prioritization of pressing public health issues is important, Laurie Garrett argues that there are following consequences. When foreign aid is funnelled into disease-specific programs, the importance of public health in general is disregarded. This public health problem of stovepiping is thought to create a lack of funds to combat other existing diseases in a given country.\n\nFor example, the WHO reports that at least 220 million people worldwide suffer from diabetes. Its incidence is increasing rapidly, and it is projected that the number of diabetes deaths will double by the year 2030. In a June 2010 editorial in the medical journal \"The Lancet\", the authors opined that \"The fact that type 2 diabetes, a largely preventable disorder, has reached epidemic proportion is a public health humiliation.\" The risk of type 2 diabetes is closely linked with the growing problem of obesity. The WHO’s latest estimates as of June 2016 highlighted that globally approximately 1.9 billion adults were overweight in 2014, and 41 million children under the age of five were overweight in 2014. The United States is the leading country with 30.6% of its population being obese. Mexico follows behind with 24.2% and the United Kingdom with 23%. Once considered a problem in high-income countries, it is now on the rise in low-income countries, especially in urban settings. Many public health programs are increasingly dedicating attention and resources to the issue of obesity, with objectives to address the underlying causes including healthy diet and physical exercise.\n\nSome programs and policies associated with public health promotion and prevention can be controversial. One such example is programs focusing on the prevention of HIV transmission through safe sex campaigns and needle-exchange programmes. Another is the control of tobacco smoking. Changing smoking behavior requires long-term strategies, unlike the fight against communicable diseases, which usually takes a shorter period for effects to be observed. Many nations have implemented major initiatives to cut smoking, such as increased taxation and bans on smoking in some or all public places. Proponents argue by presenting evidence that smoking is one of the major killers, and that therefore governments have a duty to reduce the death rate, both through limiting passive (second-hand) smoking and by providing fewer opportunities for people to smoke. Opponents say that this undermines individual freedom and personal responsibility, and worry that the state may be emboldened to remove more and more choice in the name of better population health overall.\n\nSimultaneously, while communicable diseases have historically ranged uppermost as a global health priority, non-communicable diseases and the underlying behavior-related risk factors have been at the bottom. This is changing, however, as illustrated by the United Nations hosting its first General Assembly Special Summit on the issue of non-communicable diseases in September 2011.\n\nMany health problems are due to maladaptive personal behaviors. From an evolutionary psychology perspective, over consumption of novel substances that are harmful is due to the activation of an evolved reward system for substances such as drugs, tobacco, alcohol, refined salt, fat, and carbohydrates. New technologies such as modern transportation also cause reduced physical activity. Research has found that behavior is more effectively changed by taking evolutionary motivations into consideration instead of only presenting information about health effects. The marketing industry has long known the importance of associating products with high status and attractiveness to others. Films are increasingly being recognized as a public health tool. In fact, film festivals and competitions have been established to specifically promote films about health. Conversely, it has been argued that emphasizing the harmful and undesirable effects of tobacco smoking on other persons and imposing smoking bans in public places have been particularly effective in reducing tobacco smoking.\n\nAs well as seeking to improve population health through the implementation of specific population-level interventions, public health contributes to medical care by identifying and assessing population needs for health care services, including:\n\nTo improve public health, one important strategy is to promote modern medicine and scientific neutrality to drive the public health policy and campaign, which is recommended by Armanda Solorzana, through a case study of the Rockefeller Foundation's hookworm campaign in Mexico in the 1920s. Soloranza argues that public health policy can't concern only politics or economics. Political concerns can lead government officials to hide the real numbers of people affected by disease in their regions, such as upcoming elections. Therefore, scientific neutrality in making public health policy is critical; it can ensure treatment needs are met regardless of political and economic conditions.\n\nThe history of public health care clearly shows the global effort to improve health care for all. However, in modern-day medicine, real, measurable change has not been clearly seen, and critics argue that this lack of improvement is due to ineffective methods that are being implemented. As argued by Paul E. Farmer, structural interventions could possibly have a large impact, and yet there are numerous problems as to why this strategy has yet to be incorporated into the health system. One of the main reasons that he suggests could be the fact that physicians are not properly trained to carry out structural interventions, meaning that the ground level health care professionals cannot implement these improvements. While structural interventions can not be the only area for improvement, the lack of coordination between socioeconomic factors and health care for the poor could be counterproductive, and end up causing greater inequity between the health care services received by the rich and by the poor. Unless health care is no longer treated as a commodity, global public health will ultimately not be achieved. This being the case, without changing the way in which health care is delivered to those who have less access to it, the universal goal of public health care cannot be achieved.\n\nAnother reason why measurable changes may not be noticed in public health is because agencies themselves may not be measuring their programs' efficacy. Perrault et al. analyzed over 4,000 published objectives from Community Health Improvement Plans (CHIPs) of 280 local accredited and non-accredited public health agencies in the U.S., and found that the majority of objectives - around two-thirds - were focused on achieving agency outputs (e.g., developing communication plans, installing sidewalks, disseminating data to the community). Only about one-third focused on seeking measurable changes in the populations they serve (i.e., changing people's knowledge, attitudes, behaviors). What this research showcases is that if agencies are only focused on accomplishing tasks (i.e., outputs) and do not have a focus on measuring actual changes in their populations with the activities they perform, it should not be surprising when measurable changes are not reported. Perrault et al. advocate for public health agencies to work with those in the discipline of Health Communication to craft objectives that are measurable outcomes, and to assist agencies in developing tools and methods to be able to track more proximal changes in their target populations (e.g., knowledge and attitude shifts) that may be influenced by the activities the agencies are performing.\n\n\"Public Health 2.0\" is a movement within public health that aims to make the field more accessible to the general public and more user-driven. The term is used in three senses. In the first sense, \"Public Health 2.0\" is similar to \"Health 2.0\" and describes the ways in which traditional public health practitioners and institutions are reaching out (or could reach out) to the public through social media and health blogs.\n\nIn the second sense, \"Public Health 2.0\" describes public health research that uses data gathered from social networking sites, search engine queries, cell phones, or other technologies. A recent example is the proposal of statistical framework that utilizes online user-generated content (from social media or search engine queries) to estimate the impact of an influenza vaccination campaign in the UK.\n\nIn the third sense, \"Public Health 2.0\" is used to describe public health activities that are completely user-driven. An example is the collection and sharing of information about environmental radiation levels after the March 2011 tsunami in Japan. In all cases, Public Health 2.0 draws on ideas from Web 2.0, such as crowdsourcing, information sharing, and user-centred design. While many individual healthcare providers have started making their own personal contributions to \"Public Health 2.0\" through personal blogs, social profiles, and websites, other larger organizations, such as the American Heart Association (AHA) and United Medical Education (UME), have a larger team of employees centered around online driven health education, research, and training. These private organizations recognize the need for free and easy to access health materials often building libraries of educational articles.\n\nThere is a great disparity in access to health care and public health initiatives between developed nations and developing nations. In the developing world, public health infrastructures are still forming. There may not be enough trained health workers, monetary resources or, in some cases, sufficient knowledge to provide even a basic level of medical care and disease prevention. As a result, a large majority of disease and mortality in the developing world results from and contributes to extreme poverty. For example, many African governments spend less than US$10 per person per year on health care, while, in the United States, the federal government spent approximately US$4,500 per capita in 2000. However, expenditures on health care should not be confused with spending on public health. Public health measures may not generally be considered \"health care\" in the strictest sense. For example, mandating the use of seat belts in cars can save countless lives and contribute to the health of a population, but typically money spent enforcing this rule would not count as money spent on health care.\n\nLarge parts of the developing world remained plagued by largely preventable or treatable infectious diseases. In addition to this however, many developing countries are also experiencing an epidemiological shift and polarization in which populations are now experiencing more of the effects of chronic diseases as life expectancy increases with, the poorer communities being heavily affected by both chronic and infectious diseases. Another major public health concern in the developing world is poor maternal and child health, exacerbated by malnutrition and poverty. The WHO reports that a lack of exclusive breastfeeding during the first six months of life contributes to over a million avoidable child deaths each year. Intermittent preventive therapy aimed at treating and preventing malaria episodes among pregnant women and young children is one public health measure in endemic countries.\n\nEach day brings new front-page headlines about public health: emerging infectious diseases such as SARS, rapidly making its way from China (see Public health in China) to Canada, the United States and other geographically distant countries; reducing inequities in health care access through publicly funded health insurance programs; the HIV/AIDS pandemic and its spread from certain high-risk groups to the general population in many countries, such as in South Africa; the increase of childhood obesity and the concomitant increase in type II diabetes among children; the social, economic and health effects of adolescent pregnancy; and the public health challenges related to natural disasters such as the 2004 Indian Ocean tsunami, 2005's Hurricane Katrina in the United States and the 2010 Haiti earthquake.\n\nSince the 1980s, the growing field of population health has broadened the focus of public health from individual behaviors and risk factors to population-level issues such as inequality, poverty, and education. Modern public health is often concerned with addressing determinants of health across a population. There is a recognition that our health is affected by many factors including where we live, genetics, our income, our educational status and our social relationships; these are known as \"social determinants of health\". The upstream drivers such as environment, education, employment, income, food security, housing, social inclusion and many others effect the distribution of health between and within populations and are often shaped by policy. A social gradient in health runs through society. The poorest generally suffer the worst health, but even the middle classes will generally have worse health outcomes than those of a higher social stratum. The new public health advocates for population-based policies that improve health in an equitable manner.\n\nHealth aid to developing countries is an important source of public health funding for many developing countries. Health aid to developing countries has shown a significant increase after World War II as concerns over the spread of disease as a result of globalization increased and the HIV/AIDS epidemic in sub-Saharan Africa surfaced. From 1990 to 2010, total health aid from developed countries increased from 5.5 billion to 26.87 billion with wealthy countries continuously donating billions of dollars every year with the goal of improving population health. Some efforts, however, receive a significantly larger proportion of funds such as HIV which received an increase in funds of over $6 billion dollars between 2000 and 2010 which was more than twice the increase seen in any other sector during those years. Health aid has seen an expansion through multiple channels including private philanthropy, non-governmental organizations, private foundations such as the Bill & Melinda Gates Foundation, bilateral donors, and multilateral donors such as the World Bank or UNICEF. In 2009 health aid from the OECD amounted to $12.47 billion which amounted to 11.4% of its total bilateral aid. In 2009, Multilateral donors were found to spend 15.3% of their total aid on bettering public healthcare. Recent data, however, shows that international health aid has plateaued and may begin to decrease.\n\nDebates exist questioning the efficacy of international health aid. Proponents of aid claim that health aid from wealthy countries is necessary in order for developing countries to escape the poverty trap. Opponents of health aid claim that international health aid actually disrupts developing countries' course of development, causes dependence on aid, and in many cases the aid fails to reach its recipients. For example, recently, health aid was funneled towards initiatives such as financing new technologies like antiretroviral medication, insecticide-treated mosquito nets, and new vaccines. The positive impacts of these initiatives can be seen in the eradication of smallpox and polio; however, critics claim that misuse or misplacement of funds may cause many of these efforts to never come into fruition.\n\nEconomic modeling based on the Institute for Health Metrics and Evaluation and the World Health Organization has shown a link between international health aid in developing countries and a reduction in adult mortality rates. However, a 2014-2016 study suggests that a potential confounding variable for this outcome is the possibility that aid was directed at countries once they were already on track for improvement. That same study, however, also suggests that 1 billion dollars in health aid was associated with 364,000 fewer deaths occurring between ages 0 and 5 in 2011.\n\nTo address current and future challenges in addressing health issues in the world, the United Nations have developed the Sustainable Development Goals building off of the Millennium Development Goals of 2000 to be completed by 2030. These goals in their entirety encompass the entire spectrum of development across nations, however Goals 1-6 directly address health disparities, primarily in developing countries. These six goals address key issues in global public health: Poverty, Hunger and food security, Health, Education, Gender equality and women's empowerment, and water and sanitation. Public health officials can use these goals to set their own agenda and plan for smaller scale initiatives for their organizations. These goals hope to lessen the burden of disease and inequality faced by developing countries and lead to a healthier future.\n\nThe links between the various sustainable development goals and public health are numerous and well established:\n\nThe U.S. Global Health Initiative was created in 2009 by President Obama in an attempt to have a more holistic, comprehensive approach to improving global health as opposed to previous, disease-specific interventions. The Global Health Initiative is a six-year plan, \"to develop a comprehensive U.S. government strategy for global health, building on the President's Emergency Plan for AIDS Relief (PEPFAR) to combat HIV as well as U.S. efforts to address tuberculosis (TB) and malaria, and augmenting the focus on other global health priorities, including neglected tropical diseases (NTDs), maternal, newborn and child health (MNCH), family planning and reproductive health (FP/RH), nutrition, and health systems strengthening (HSS)\". The GHI programs are being implemented in more than 80 countries around the world and works closely with the United States Agency for International Development, the Centers for Disease Control and Prevention, the United States Deputy Secretary of State.\n\nThere are seven core principles:\n\n\nThe aid effectiveness agenda is a useful tool for measuring the impact of these large scale programs such as The Global Fund to Fight AIDS, Tuberculosis and Malaria and the Global Alliance for Vaccines and Immunization (GAVI) which have been successful in achieving rapid and visible results. The Global Fund claims that its efforts have provided antiretroviral treatment for over three million people worldwide. GAVI claims that its vaccination programs have prevented over 5 million deaths since it began in 2000.\n\nEducation and training of public health professionals is available throughout the world in Schools of Public Health, Medical Schools, Veterinary Schools, Schools of Nursing, and Schools of Public Affairs. The training typically requires a university degree with a focus on core disciplines of biostatistics, epidemiology, health services administration, health policy, health education, behavioral science, gender issues, sexual and reproductive health, public health nutrition and environmental and occupational health. In the global context, the field of public health education has evolved enormously in recent decades, supported by institutions such as the World Health Organization and the World Bank, among others. Operational structures are formulated by strategic principles, with educational and career pathways guided by competency frameworks, all requiring modulation according to local, national and global realities. It is critically important for the health of populations that nations assess their public health human resource needs and develop their ability to deliver this capacity, and not depend on other countries to supply it.\n\nIn the United States, the Welch-Rose Report of 1915 has been viewed as the basis for the critical movement in the history of the institutional schism between public health and medicine because it led to the establishment of schools of public health supported by the Rockefeller Foundation. The report was authored by William Welch, founding dean of the Johns Hopkins Bloomberg School of Public Health, and Wickliffe Rose of the Rockefeller Foundation. The report focused more on research than practical education. Some have blamed the Rockefeller Foundation's 1916 decision to support the establishment of schools of public health for creating the schism between public health and medicine and legitimizing the rift between medicine's laboratory investigation of the mechanisms of disease and public health's nonclinical concern with environmental and social influences on health and wellness.\n\nEven though schools of public health had already been established in Canada, Europe and North Africa, the United States had still maintained the traditional system of housing faculties of public health within their medical institutions. A $25,000 donation from businessman Samuel Zemurray instituted the School of Public Health and Tropical Medicine at Tulane University in 1912 conferring its first doctor of public health degree in 1914. The Yale School of Public Health was founded by Charles-Edward Avory Winslow in 1915. The Johns Hopkins School of Hygiene and Public Health became an independent, degree-granting institution for research and training in public health, and the largest public health training facility in the United States, when it was founded in 1916. By 1922, schools of public health were established at Columbia and Harvard on the Hopkins model. By 1999 there were twenty nine schools of public health in the US, enrolling around fifteen thousand students.\n\nOver the years, the types of students and training provided have also changed. In the beginning, students who enrolled in public health schools typically had already obtained a medical degree; public health school training was largely a second degree for medical professionals. However, in 1978, 69% of American students enrolled in public health schools had only a bachelor's degree.\n\nSchools of public health offer a variety of degrees which generally fall into two categories: professional or academic. The two major postgraduate degrees are the Master of Public Health (MPH) or the Master of Science in Public Health (MSPH). Doctoral studies in this field include Doctor of Public Health (DrPH) and Doctor of Philosophy (PhD) in a subspeciality of greater Public Health disciplines. DrPH is regarded as a professional degree and PhD as more of an academic degree.\n\nProfessional degrees are oriented towards practice in public health settings. The Master of Public Health, Doctor of Public Health, Doctor of Health Science (DHSc) and the Master of Health Care Administration are examples of degrees which are geared towards people who want careers as practitioners of public health in health departments, managed care and community-based organizations, hospitals and consulting firms, among others. Master of Public Health degrees broadly fall into two categories, those that put more emphasis on an understanding of epidemiology and statistics as the scientific basis of public health practice and those that include a more eclectic range of methodologies. A Master of Science of Public Health is similar to an MPH but is considered an academic degree (as opposed to a professional degree) and places more emphasis on scientific methods and research. The same distinction can be made between the DrPH and the DHSc. The DrPH is considered a professional degree and the DHSc is an academic degree.\n\nAcademic degrees are more oriented towards those with interests in the scientific basis of public health and preventive medicine who wish to pursue careers in research, university teaching in graduate programs, policy analysis and development, and other high-level public health positions. Examples of academic degrees are the Master of Science, Doctor of Philosophy, Doctor of Science (ScD), and Doctor of Health Science (DHSc). The doctoral programs are distinct from the MPH and other professional programs by the addition of advanced coursework and the nature and scope of a dissertation research project.\n\nIn the United States, the Association of Schools of Public Health represents Council on Education for Public Health (CEPH) accredited schools of public health. Delta Omega is the honor society for graduate studies in public health. The society was founded in 1924 at the Johns Hopkins School of Hygiene and Public Health. Currently, there are approximately 68 chapters throughout the United States and Puerto Rico.\n\nPublic health has early roots in antiquity. From the beginnings of human civilization, it was recognized that polluted water and lack of proper waste disposal spread communicable diseases (theory of miasma). Early religions attempted to regulate behavior that specifically related to health, from types of food eaten, to regulating certain indulgent behaviors, such as drinking alcohol or sexual relations. Leaders were responsible for the health of their subjects to ensure social stability, prosperity, and maintain order.\n\nBy Roman times, it was well understood that proper diversion of human waste was a necessary tenet of public health in urban areas. The ancient Chinese medical doctors developed the practice of variolation following a smallpox epidemic around 1000 BC. An individual without the disease could gain some measure of immunity against it by inhaling the dried crusts that formed around lesions of infected individuals. Also, children were protected by inoculating a scratch on their forearms with the pus from a lesion.\n\nIn 1485 the Republic of Venice established a permanent Venetian Magistrate for Health comprising supervisors of health with special attention to the prevention of the spread of epidemics in the territory from abroad. The three supervisors were initially appointed by the Venetian Senate. In 1537 it was assumed by the Grand Council, and in 1556 added two judges, with the task of control, on behalf of the Republic, the efforts of the supervisors.\n\nHowever, according to Michel Foucault, the plague model of governmentality was later controverted by the cholera model. A Cholera pandemic devastated Europe between 1829 and 1851, and was first fought by the use of what Foucault called \"social medicine\", which focused on flux, circulation of air, location of cemeteries, etc. All those concerns, born of the miasma theory of disease, were mixed with urbanistic concerns for the management of populations, which Foucault designated as the concept of \"biopower\". The German conceptualized this in the \"Polizeiwissenschaft\" (\"Police science\").\n\nThe 18th century saw rapid growth in voluntary hospitals in England. The latter part of the century brought the establishment of the basic pattern of improvements in public health over the next two centuries: a social evil was identified, private philanthropists brought attention to it, and changing public opinion led to government action.\nThe practice of vaccination became prevalent in the 1800s, following the pioneering work of Edward Jenner in treating smallpox. James Lind's discovery of the causes of scurvy amongst sailors and its mitigation via the introduction of fruit on lengthy voyages was published in 1754 and led to the adoption of this idea by the Royal Navy. Efforts were also made to promulgate health matters to the broader public; in 1752 the British physician Sir John Pringle published \"Observations on the Diseases of the Army in Camp and Garrison\", in which he advocated for the importance of adequate ventilation in the military barracks and the provision of latrines for the soldiers.\n\nWith the onset of the Industrial Revolution, living standards amongst the working population began to worsen, with cramped and unsanitary urban conditions. In the first four decades of the 19th century alone, London's population doubled and even greater growth rates were recorded in the new industrial towns, such as Leeds and Manchester. This rapid urbanisation exacerbated the spread of disease in the large conurbations that built up around the workhouses and factories. These settlements were cramped and primitive with no organized sanitation. Disease was inevitable and its incubation in these areas was encouraged by the poor lifestyle of the inhabitants. Unavailable housing led to the rapid growth of slums and the per capita death rate began to rise alarmingly, almost doubling in Birmingham and Liverpool. Thomas Malthus warned of the dangers of overpopulation in 1798. His ideas, as well as those of Jeremy Bentham, became very influential in government circles in the early years of the 19th century.\n\nThe first attempts at sanitary reform and the establishment of public health institutions were made in the 1840s. Thomas Southwood Smith, physician at the London Fever Hospital, began to write papers on the importance of public health, and was one of the first physicians brought in to give evidence before the Poor Law Commission in the 1830s, along with Neil Arnott and James Phillips Kay. Smith advised the government on the importance of quarantine and sanitary improvement for limiting the spread of infectious diseases such as cholera and yellow fever.\n\nThe Poor Law Commission reported in 1838 that \"the expenditures necessary to the adoption and maintenance of measures of prevention would ultimately amount to less than the cost of the disease now constantly engendered\". It recommended the implementation of large scale government engineering projects to alleviate the conditions that allowed for the propagation of disease. The Health of Towns Association was formed in Exeter on 11 December 1844, and vigorously campaigned for the development of public health in the United Kingdom. Its formation followed the 1843 establishment of the Health of Towns Commission, chaired by Sir Edwin Chadwick, which produced a series of reports on poor and insanitary conditions in British cities.\n\nThese national and local movements led to the Public Health Act, finally passed in 1848. It aimed to improve the sanitary condition of towns and populous places in England and Wales by placing the supply of water, sewerage, drainage, cleansing and paving under a single local body with the General Board of Health as a central authority. The Act was passed by the Liberal government of Lord John Russell, in response to the urging of Edwin Chadwick. Chadwick's seminal report on \"The Sanitary Condition of the Labouring Population\" was published in 1842 and was followed up with a supplementary report a year later.\n\nVaccination for various diseases was made compulsory in the United Kingdom in 1851, and by 1871 legislation required a comprehensive system of registration run by appointed vaccination officers.\n\nFurther interventions were made by a series of subsequent Public Health Acts, notably the 1875 Act. Reforms included latrinization, the building of sewers, the regular collection of garbage followed by incineration or disposal in a landfill, the provision of clean water and the draining of standing water to prevent the breeding of mosquitoes.\n\nThe Infectious Disease (Notification) Act 1889 mandated the reporting of infectious diseases to the local sanitary authority, which could then pursue measures such as the removal of the patient to hospital and the disinfection of homes and properties.\n\nIn the United States, the first public health organization based on a state health department and local boards of health was founded in New York City in 1866.\n\nThe science of epidemiology was founded by John Snow's identification of a polluted public water well as the source of an 1854 cholera outbreak in London. Dr. Snow believed in the germ theory of disease as opposed to the prevailing miasma theory. He first publicized his theory in an essay, \"On the Mode of Communication of Cholera\", in 1849, followed by a more detailed treatise in 1855 incorporating the results of his investigation of the role of the water supply in the Soho epidemic of 1854.\n\nBy talking to local residents (with the help of Reverend Henry Whitehead), he identified the source of the outbreak as the public water pump on Broad Street (now Broadwick Street). Although Snow's chemical and microscope examination of a water sample from the Broad Street pump did not conclusively prove its danger, his studies of the pattern of the disease were convincing enough to persuade the local council to disable the well pump by removing its handle.\n\nSnow later used a dot map to illustrate the cluster of cholera cases around the pump. He also used statistics to illustrate the connection between the quality of the water source and cholera cases. He showed that the Southwark and Vauxhall Waterworks Company was taking water from sewage-polluted sections of the Thames and delivering the water to homes, leading to an increased incidence of cholera. Snow's study was a major event in the history of public health and geography. It is regarded as the founding event of the science of epidemiology.\n\nWith the pioneering work in bacteriology of French chemist Louis Pasteur and German scientist Robert Koch, methods for isolating the bacteria responsible for a given disease and vaccines for remedy were developed at the turn of the 20th century. British physician Ronald Ross identified the mosquito as the carrier of malaria and laid the foundations for combating the disease. Joseph Lister revolutionized surgery by the introduction of antiseptic surgery to eliminate infection. French epidemiologist Paul-Louis Simond proved that plague was carried by fleas on the back of rats, and Cuban scientist Carlos J. Finlay and U.S. Americans Walter Reed and James Carroll demonstrated that mosquitoes carry the virus responsible for yellow fever. Brazilian scientist Carlos Chagas identified a tropical disease and its vector.\n\nWith onset of the epidemiological transition and as the prevalence of infectious diseases decreased through the 20th century, public health began to put more focus on chronic diseases such as cancer and heart disease. Previous efforts in many developed countries had already led to dramatic reductions in the infant mortality rate using preventative methods. In Britain, the infant mortality rate fell from over 15% in 1870 to 7% by 1930.\n\nFrance 1871-1914 followed well behind Bismarckian Germany, as well as Great Britain, in developing the welfare state including public health. Tuberculosis was the most dreaded disease of the day, especially striking young people in their 20s. Germany set up vigorous measures of public hygiene and public sanatoria, but France let private physicians handle the problem, which left it with a much higher death rate. The French medical profession jealously guarded its prerogatives, and public health activists were not as well organized or as influential as in Germany, Britain or the United States. For example, there was a long battle over a public health law which began in the 1880s as a campaign to reorganize the nation's health services, to require the registration of infectious diseases, to mandate quarantines, and to improve the deficient health and housing legislation of 1850. However the reformers met opposition from bureaucrats, politicians, and physicians. Because it was so threatening to so many interests, the proposal was debated and postponed for 20 years before becoming law in 1902. Success finally came when the government realized that contagious diseases had a national security impact in weakening military recruits, and keeping the population growth rate well below Germany's.\n\nModern public health began developing in the 19th century, as a response to advances in science that led to the understanding of, the source and spread of disease. As the knowledge of contagious diseases increased, means to control them and prevent infection were soon developed. Once it became understood that these strategies would require community-wide participation, disease control began being viewed as a public responsibility. Various organizations and agencies were then created to implement these disease preventing strategies.\n\nMost of the Public health activity in the United States took place at the municipal level before the mid-20th century. There was some activity at the national and state level as well.\n\nIn the administration of the second president of the United States John Adams, the Congress authorized the creation of hospitals for mariners. As the U.S. expanded, the scope of the governmental health agency expanded.\n\nIn the United States, public health worker Sara Josephine Baker, M.D. established many programs to help the poor in New York City keep their infants healthy, leading teams of nurses into the crowded neighborhoods of Hell's Kitchen and teaching mothers how to dress, feed, and bathe their babies.\n\nAnother key pioneer of public health in the U.S. was Lillian Wald, who founded the Henry Street Settlement house in New York. The Visiting Nurse Service of New York was a significant organization for bringing health care to the urban poor.\n\nDramatic increases in average life span in the late 19th century and 20th century, is widely credited to public health achievements, such as vaccination programs and control of many infectious diseases including polio, diphtheria, yellow fever and smallpox; effective health and safety policies such as road traffic safety and occupational safety; improved family planning; tobacco control measures; and programs designed to decrease non-communicable diseases by acting on known risk factors such as a person's background, lifestyle and environment.\n\nAnother major public health improvement was the decline in the \"urban penalty\" brought about by improvements in sanitation. These improvements included chlorination of drinking water, filtration and sewage treatment which led to the decline in deaths caused by infectious waterborne diseases such as cholera and intestinal diseases.\nThe federal Office of Indian Affairs (OIA) operated a large-scale field nursing program. Field nurses targeted native women for health education, emphasizing personal hygiene and infant care and nutrition.\n\nPublic health issues were important for the Spanish empire during the colonial era. Epidemic disease was the main factor in the decline of indigenous populations in the era immediately following the sixteenth-century conquest era and was a problem during the colonial era. The Spanish crown took steps in eighteenth-century Mexico to bring in regulations to make populations healthier.\n\nIn the late nineteenth century, Mexico was in the process of modernization, and public health issues were again tackled from a scientific point of view. Even during the Mexican Revolution (1910–20), public health was an important concern, with a text on hygiene published in 1916. During the Mexican Revolution, feminist and trained nurse Elena Arizmendi Mejia founded the Neutral White Cross, treating wounded soldiers no matter for what faction they fought.\n\nIn the post-revolutionary period after 1920, improved public health was a revolutionary goal of the Mexican government.\nThe Mexican state promoted the health of the Mexican population, with most resources going to cities. Concern about disease conditions and social impediments to the improvement of Mexicans' health were important in the formation of the Mexican Society for Eugenics. The movement flourished from the 1920s to the 1940s. Mexico was not alone in Latin America or the world in promoting eugenics. Government campaigns against disease and alcoholism were also seen as promoting public health.\n\nThe Mexican Social Security Institute was established in 1943, during the administration of President Manuel Avila Camacho to deal with public health, pensions, and social security.\n\nSince the 1959 Cuban Revolution the Cuban government has devoted extensive resources to the improvement of health conditions for its entire population via universal access to health care. Infant mortality has plummeted. Cuban medical internationalism as a policy has seen the Cuban government sent doctors as a form of aid and export to countries in need in Latin America, especially Venezuela, as well as Oceania and Africa countries.\n\nPublic health was important elsewhere in Latin America in consolidating state power and integrating marginalized populations into the nation-state. In Colombia, public health was a means for creating and implementing ideas of citizenship. In Bolivia, a similar push came after their 1952 revolution.\n\nThough curable and preventative, malaria remains a huge public health problem and is the third leading cause of death in Ghana. In the absence of a vaccine, mosquito control, or access to anti-malaria medication, public health methods become the main strategy for reducing the prevalence and severity of malaria. These methods include reducing breeding sites, screening doors and windows, insecticide sprays, prompt treatment following infection, and usage of insecticide treated mosquito nets. Distribution and sale of insecticide-treated mosquito nets is a common, cost-effective anti-malaria public health intervention; however, barriers to use exist including cost, hosehold and family organization, access to resources, and social and behavioral determinants which have not only been shown to affect malaria prevalence rates but also mosquito net use.\n\n"}
{"id": "47789", "url": "https://en.wikipedia.org/wiki?curid=47789", "title": "Quality of life", "text": "Quality of life\n\nQuality of life (QOL) is the general well-being of individuals and societies, outlining negative and positive features of life. It observes life satisfaction, including everything from physical health, family, education, employment, wealth, safety, security to freedom, religious beliefs, and the environment. QOL has a wide range of contexts, including the fields of international development, healthcare, politics and employment. It is important not to mix up the concept of QOL with a more recent growing area of health related QOL (HRQOL). An assessment of HRQOL is effectively an evaluation of QOL and its relationship with health.\n\nQuality of life should not be confused with the concept of standard of living, which is based primarily on income.\n\nStandard indicators of the quality of life include not only wealth and employment but also the built environment, physical and mental health, education, recreation and leisure time, and social belonging. According to the World Health Organization (WHO), quality of life is defined as “the individual’s perception of their position in life in the context of the culture and value systems in which they live and in relation to their goals.” In comparison to WHO's definitions, the Wang-Baker Faces scale defines quality of life as “life quality (in this case, physical pain) at a precise moment in time.”\n\nAccording to ecological economist Robert Costanza:\nOne approach, called engaged theory, outlined in the journal of \"Applied Research in the Quality of Life\", posits four domains in assessing quality of life: ecology, economics, politics and culture. In the domain of culture, for example, it includes the following subdomains of quality of life:\n\n\nAlso frequently related are concepts such as freedom, human rights, and happiness. However, since happiness is subjective and difficult to measure, other measures are generally given priority. It has also been shown that happiness, as much as it can be measured, does not necessarily increase correspondingly with the comfort that results from increasing income. As a result, standard of living should not be taken to be a measure of happiness. Also sometimes considered related is the concept of human security, though the latter may be considered at a more basic level and for all people.\n\nUnlike \"per capita\" GDP or standard of living, both of which can be measured in financial terms, it is harder to make objective or long-term measurements of the quality of life experienced by nations or other groups of people. Researchers have begun in recent times to distinguish two aspects of personal well-being: \"Emotional well-being\", in which respondents are asked about the quality of their everyday emotional experiences—the frequency and intensity of their experiences of, for example, joy, stress, sadness, anger, and affection— and \"life evaluation\", in which respondents are asked to think about their life in general and evaluate it against a scale. Such and other systems and scales of measurement have been in use for some time. Research has attempted to examine the relationship between quality of life and productivity. There are many different methods of measuring quality of life in terms of health care, wealth and materialistic goods. However, it is much more difficult to measure meaningful expression of one's desires. One way to do so is to evaluate the scope of how individuals have fulfilled their own ideals. Quality of life can simply mean happiness, the subjective state of mind. By using that mentality, citizens of a developing country appreciate more since they are content with the basic necessities of health care, education and child protection.\n\nPerhaps the most commonly used international measure of development is the Human Development Index (HDI), which combines measures of life expectancy, education, and standard of living, in an attempt to quantify the options available to individuals within a given society. The HDI is used by the United Nations Development Programme in their Human Development Report.\n\nThe World Happiness Report is a landmark survey on the state of global happiness. It ranks 156 countries by their happiness levels, reflecting growing global interest in using happiness and substantial well-being as an indicator of the quality of human development. Its growing purpose has allowed governments, communities and organizations to use appropriate data to record happiness in order to enable policies to provide better lives. The reports review the state of happiness in the world today and show how the science of happiness explains personal and national variations in happiness. Also developed by the United Nations and published recently along with the HDI, this report combines both objective and subjective measures to rank countries by happiness, which is deemed as the ultimate outcome of a high quality of life. It uses surveys from Gallup, real GDP per capita, healthy life expectancy, having someone to count on, perceived freedom to make life choices, freedom from corruption, and generosity to derive the final score. Happiness is already recognised as an important concept in global public policy. The World Happiness Report indicates that some regions have in recent years been experiencing progressive inequality of happiness. Without life, there is no happiness to be realised.\n\nThe Physical Quality of Life Index (PQLI) is a measure developed by sociologist Morris David Morris in the 1970s, based on basic literacy, infant mortality, and life expectancy. Although not as complex as other measures, and now essentially replaced by the Human Development Index, the PQLI is notable for Morris's attempt to show a \"less fatalistic pessimistic picture\" by focusing on three areas where global quality of life was generally improving at the time and ignoring gross national product and other possible indicators that were not improving.\n\nThe Happy Planet Index, introduced in 2006, is unique among quality of life measures in that, in addition to standard determinants of well-being, it uses each country's ecological footprint as an indicator. As a result, European and North American nations do not dominate this measure. The 2012 list is instead topped by Costa Rica, Vietnam and Colombia.\n\nGallup researchers trying to find the world's happiest countries found Denmark to be at the top of the list. uSwitch publishes an annual quality of life index for European countries. France has topped the list for the last three years.\n\nA 2010 study by two Princeton University professors looked at 1,000 randomly selected U.S. residents over an extended period. It concludes that their \"life evaluations\" – that is, their considered evaluations of their life against a stated scale of one to ten – rise steadily with income. On the other hand, their reported quality of \"emotional daily experiences\" (their reported experiences of joy, affection, stress, sadness, or anger) levels off after a certain income level (approximately $75,000 per year); income above $75,000 does not lead to more experiences of happiness nor to further relief of unhappiness or stress. Below this income level, respondents reported decreasing happiness and increasing sadness and stress, implying the pain of life's misfortunes, including disease, divorce, and being alone, is exacerbated by poverty.\n\nGross national happiness and other subjective measures of happiness are being used by the governments of Bhutan and the United Kingdom. The World Happiness report, issued by Columbia University is a meta-analysis of happiness globally and provides an overview of countries and grassroots activists using GNH. The OECD issued a guide for the use of subjective well-being metrics in 2013. In the U.S., cities and communities are using a GNH metric at a grassroots level.\n\nThe Social Progress Index measures the extent to which countries provide for the social and environmental needs of their citizens. Fifty-two indicators in the areas of basic human needs, foundations of wellbeing, and opportunity show the relative performance of nations. The index uses outcome measures when there is sufficient data available or the closest possible proxies.\n\nDay-Reconstruction Method was another way of measuring happiness, in which researchers asked their subjects to recall various things they did on the previous day and describe their mood during each activity. Being simple and approachable, this method required memory and the experiments have confirmed that the answers that people give are similar to those who repeatedly recalled each subject. The method eventually declined as it called for more effort and thoughtful responses, which often included interpretations and outcomes that do not occur to people who are asked to record every action in their daily lives.\n\nThe term \"quality of life\" is also used by politicians and economists to measure the livability of a given city or nation. Two widely known measures of livability are the Economist Intelligence Unit's Where-to-be-born Index and Mercer's Quality of Living Reports. These two measures calculate the livability of countries and cities around the world, respectively, through a combination of subjective life-satisfaction surveys and objective determinants of quality of life such as divorce rates, safety, and infrastructure. Such measures relate more broadly to the population of a city, state, or country, not to individual quality of life. Livability has a long history and tradition in urban design, and neighborhoods design standards such as LEED-ND are often used in an attempt to influence livability.\n\nSome crimes against property (e.g., graffiti and vandalism) and some \"victimless crimes\" have been referred to as \"quality-of-life crimes.\" American sociologist James Q. Wilson encapsulated this argument as the broken windows theory, which asserts that relatively minor problems left unattended (such as litter, graffiti, or public urination by homeless individuals) send a subliminal message that disorder in general is being tolerated, and as a result, more serious crimes will end up being committed (the analogy being that a broken window left broken shows an image of general dilapidation).\n\nWilson's theories have been used to justify the implementation of zero tolerance policies by many prominent American mayors, most notably Oscar Goodman in Las Vegas, Richard Riordan in Los Angeles, Rudolph Giuliani in New York City and Gavin Newsom in San Francisco. Such policies refuse to tolerate even minor crimes; proponents argue that this will improve the quality of life of local residents. However, critics of zero tolerance policies believe that such policies neglect investigation on a case-by-case basis and may lead to unreasonably harsh penalties for crimes.\n\nThe popsicle index is a quality-of-life measurement coined by Catherine Austin Fitts as the percentage of people in a community who believe that a child in their community can leave their home alone, go to the nearest possible location to buy a popsicle or other snack, and return home safely.\n\nWithin the field of healthcare, quality of life is often regarded in terms of how a certain ailment affects a patient on an individual level. This may be a debilitating weakness that is not life-threatening; life-threatening illness that is not terminal; terminal illness; the predictable, natural decline in the health of an elder; an unforeseen mental/physical decline of a loved one; or chronic, end-stage disease processes. Researchers at the University of Toronto's Quality of Life Research Unit define quality of life as \"The degree to which a person enjoys the important possibilities of his or her life\" (UofT). Their Quality of Life Model is based on the categories \"being\", \"belonging\", and \"becoming\"; respectively who one is, how one is not connected to one's environment, and whether one achieves one's personal goals, hopes, and aspirations.\n\nExperience sampling studies show substantial between-person variability in within-person associations between somatic symptoms and quality of life. Hecht and Shiel measure quality of life as “the patient’s ability to enjoy normal life activities” since life quality is strongly related to wellbeing without suffering from sickness and treatment. There are multiple assessments available that measure Health-Related Quality of Life, e.g., AQoL-8D, EQ5D - Euroqol, 15D, SF-36, SF-6D, HUI.\n\nQuality of life is an important concept in the field of international development since it allows development to be analyzed on a measure broader than standard of living. Within development theory, however, there are varying ideas concerning what constitutes desirable change for a particular society, and the different ways that quality of life is defined by institutions therefore shapes how these organizations work for its improvement as a whole.\n\nOrganisations such as the World Bank, for example, declare a goal of \"working for a world free of poverty\", with poverty defined as a lack of basic human needs, such as food, water, shelter, freedom, access to education, healthcare, or employment. In other words, poverty is defined as a low quality of life. Using this definition, the World Bank works towards improving quality of life through the stated goal of lowering poverty and helping people afford a better quality of life.\n\nOther organizations, however, may also work towards improved global quality of life using a slightly different definition and substantially different methods. Many NGOs do not focus at all on reducing poverty on a national or international scale, but rather attempt to improve quality of life for individuals or communities. One example would be sponsorship programs that provide material aid for specific individuals. Although many organizations of this type may still talk about fighting poverty, the methods are significantly different.\n\nImproving quality of life involves action not only by NGOs but also by governments. Global health has the potential to achieve greater political presence if governments were to incorporate aspects of human security into foreign policy. Stressing individuals’ basic rights to health, food, shelter, and freedom addresses prominent inter-sectoral problems negatively impacting today's society and may lead to greater action and resources. Integration of global health concerns into foreign policy may be hampered by approaches that are shaped by the overarching roles of defense and diplomacy.\n\n\n\n\n\n"}
{"id": "57335546", "url": "https://en.wikipedia.org/wiki?curid=57335546", "title": "Real world evidence", "text": "Real world evidence\n\nReal world evidence (RWE) in medicine means evidence obtained from real world data (RWD), which are observational data obtained outside the context of randomized controlled trials (RCTs) and generated during routine clinical practice. In order to assess patient outcomes and to ensure that patients get treatment that is right for them, real world data needs to be utilized. RWE is generated by analyzing data which is stored in electronic health records (EHR), medical claims or billing activities databases, registries, patient-generated data, mobile devices, etc. It may be derived from retrospective or prospective observational studies and observational registries. In the USA the 21st Century Cures Act required the FDA to expand the role of real world evidence.\nReal World Evidence comes into play when clinical trials cannot really account for the entire patient population of a particular disease. Patients suffering from comorbidities or belonging to a distant geographic region or age limit who did not participate in any clinical trial may not respond to the treatment in question as expected. RWE provides answers to these problems and also to analyze effects of drugs over a longer period of time. Pharmaceutical companies and Health Insurance Payers study RWE to understand patient pathways to deliver appropriate care for appropriate individuals and to minimize their own financial risk by investing on drugs that work for patients.\n\n\n\n"}
{"id": "8917125", "url": "https://en.wikipedia.org/wiki?curid=8917125", "title": "Religion and divorce", "text": "Religion and divorce\n\nThe relationship between religion and divorce is complicated and varied. This article attempts to summarize the dominant views in a number of major world faiths.\n\nThe great majority of Christian denominations affirm that marriage is intended as a lifelong covenant, but vary in their response to its dissolubility through divorce. The Roman Catholic Church treats all consummated sacramental marriages as permanent during the life of the spouses, and therefore does not allow remarriage after a divorce if the other spouse still lives and the marriage has not been annulled. However, divorced Catholics are still welcome to participate fully in the life of the church so long as they have not remarried against church law, and the Catholic Church generally requires civil divorce or annulment procedures to have been completed before it will consider annulment cases. Annulment is not the same as divorce - it is a declaration that the marriage was never valid to begin with. Other Christian denominations, including the Eastern Orthodox Church and many Protestant churches, will allow both divorce and remarriage even with a surviving former spouse, at least under certain conditions. For example, the Allegheny Wesleyan Methodist Connection, in its 2014 Discipline, teaches:\n\nIn societies that practised Puritanism, divorce was allowed if one partner in the marriage was not completely satisfied with the other, and remarriage was also allowed. The Church of England also took an indissolublist line until 2002, when it agreed to allow a divorced person to remarry in church under exceptional circumstances.\n\nBible commentary on divorce comes primarily from the gospels of Matthew, Mark, Luke, and the epistles of Paul. Jesus taught on the subject of divorce in three of the Gospels, and Paul gives a rather extensive treatment of the subject in his First Epistle to the Corinthians chapter 7: \"Let not the wife depart from her husband...let not the husband put away his wife\" (1 Corinthians 7:10-11), but he also includes the Pauline privilege. He again alludes to his position on divorce in his Epistle to the Romans, albeit an allegory, when he states \"For the woman which hath an husband is bound by the law to her husband so long as he liveth. . . . So then if, while her husband liveth, she be married to another man, she shall be called an adulteress\" (Romans 7:2-3).\n\nIn , and , Jesus came into conflict with the Pharisees over divorce concerning their well-known controversy between Hillel and Shammai about —as evidenced in Nashim Gittin 9:10 of the Mishnah. Do Jesus’ answers to the Pharisees also pertain to Christians? Are Christians who adopt these teachings Judaizers? The differences in opinions about these questions usually arise over whether Jesus opposed the Law of Moses or just some of the viewpoints of the Pharisees, and whether Jesus just addressed a Jewish audience or expanded his audience to include Christians, for example \"all nations\" as in the Great Commission. \nSince Deuteronomy 24:1-4 did not give Jewish women the right to directly initiate a divorce (See \"Agunah\"), did Jesus' answers \"in the house\" to his disciples expand the rights of women or did they merely acknowledge that some Jewish women, such as Herodias who divorced Herod Boethus, were wrongfully taking rights because Jewish women were being assimilated by other cultures? (See , .) In other words, did Jesus confine his remarks to the Pharisaical questions, and did he appeal to his own authority by refuting the oral authority of the Pharisees with the formula \"You have heard...But I say to you\" in ? Expressions used by Jesus such as \"you have heard\", \"it hath been said\", \"it is written\", \"have you never read\", \"keep the commandments\", \"why do you break the commandments with your traditions?\" and \"what did Moses Command you?\" seem to indicate that Jesus generally respected the Hebrew Bible and sometimes opposed Pharisaical Opinions. He was critical of the Pharisees.\n\nBuddhism has no religious concept of marriage (see Buddhist view of marriage). In Buddhism, marriage is a secular affair, subject to local customs.\n\nAccording to the Quran, marriage is intended to be unbounded in time, but when marital harmony cannot be attained, the Quran allows the spouses to bring the marriage to an end (2:231). Divorce in Islam can take a variety of forms, some initiated by the husband and some initiated by the wife. The main traditional legal categories are \"talaq\" (repudiation), \"khulʿ\" (mutual divorce), judicial divorce and oaths. The theory and practice of divorce in the Islamic world have varied according to time and place. Historically, the rules of divorce were governed by sharia, as interpreted by traditional Islamic jurisprudence, and they differed depending on the legal school. Historical practice sometimes diverged from legal theory. In modern times, as personal status (family) laws were codified, they generally remained \"within the orbit of Islamic law\", but control over the norms of divorce shifted from traditional jurists to the state.\n\nJudaism has always accepted divorce as a fact of life, though an unfortunate one: e.g., see Deuteronomy chapters 22 and 24. Judaism generally maintains that it is better for a couple to divorce than to remain together in a state of bitterness and strife. It is said that \"shalom bayit\" (domestic harmony) is a desirable state.\n\nIn general, it is accepted that for a Jewish divorce to be effective the husband must hand to the wife, and not vice versa, a bill of divorcement, called a get, which also acts as proof of the divorce. From ancient times, the get was considered to be very important to show all those who needed to have proof that the woman was in fact free from the previous marriage and free to remarry. In Jewish law, besides other things, the consequences of a woman remarrying and having a child while still legally married to another is profound: the child would be a mamzer, to be avoided at any cost. Also, the woman would be committing adultery should she remarry while still legally married to another. An enactment called Herem de-Rabbenu Gershom (literally, the proscription of Rabbenu Gershom)--accepted universally throughout European Jewish communities—prohibited a husband from divorcing his wife against her will.\n\nIn Jewish law divorce is an act of the parties to the marriage, which is different from the approach adopted by many other legal systems. That is, a Jewish divorce does not require a decree from a court. The function of the court, in the absence of agreement between the parties, is to decide whether the husband should be compelled to give the \"get\" or for the wife to accept the \"get\". But, notwithstanding any such ruling, the parties remain married until such time as the husband actually delivers the \"get\".\n\nJewish law, in effect, does not require proof or even an allegation of moral or other fault by either party. In the first place, as noted above, if both parties agree to a divorce and follow the prescribed procedure, then the court would not need to establish responsibility for the marriage break-down. In the second place, if either party does not wish to continue co-habiting with the other is sufficient grounds for divorce. (Anything else, it is said, would amount to rape of the woman.) In this sense it is a \"no-fault\" approach to divorce. This approach has been accepted for thousands of years . It was the approach advocated by followers of Hillel, a very influential school of thought in ancient Judea, which predated the current era. This is the approach which is now generally accepted in most, if not all, Jewish communities around the world.\n\nOn the other hand, the refusal of a husband to give his wife a get (the document) can be for purely vindictive or even extortion motives. This situation has resulted in numerous social problems in modern times. For example, where pre-nuptial agreements are enforceable in civil courts, appropriate provisions may be made to compel the giving of the get by the husband in the event of a civil divorce being obtained. A woman who has been refused a get is typically referred to as an \"agunah\".\n\nA wife can initiate a divorce process on several grounds (including lack of satisfaction in her sexual life). However, this right extends only so far as petitioning a court to force her husband to divorce her. Also see \"Jewish Attitude Toward Divorce\".\nand Get in the Conflict of Laws.\n\nFurthermore, from the philosophical and mystical point of view, divorce is a unique procedure of tremendous importance and complexity, because it nullifies the holiest of connections that can exist in the Universe (similar to a connection between a person and God). Because of the danger of the birth of illegitimate children (mamzerim) if the process is not performed properly, and because divorce law is extraordinarily complex, the process is generally supervised by experts.\n\nIn some Jewish mythologies, Adam had a wife before Eve named Lilith who left him. The earliest historically documentation of this legend appears in the 8th-10th centuries \"Alphabet of Ben Sira\". Whether this particular tradition is older is not known.\n\nAfter finding he intended to marry Glauce; for what Jason said was political ties; Medea murdered Glauce and her father with a burning dress; than proceeded to kill her own children Tisander and Alcimenes fearing they would be imprisoned. Afterwords she left to Athens on a chariot of dragons given to her by her grandfather Helios.\n\nThe Wiccan equivalent of a divorce is described as a \"handparting\". Wiccans traditionally see either a high priest or high priestess to discuss things out before a divorce. However a handfasting (marriage) that falls apart peacefully does not necessarily need a handparting.\n\nIn Unitarian Universalism, divorce is allowed and should be a decision by the individual person and is seen as ending a rite of passage. Such divorces have sometimes taken the form of divorce rituals as far back as the 1960s. Divorces are largely seen as a life choice.\n\nhttp://www.bibleissues.org, https://web.archive.org/web/20091027092358/http://geocities.com/dcheddie/divorce1.html, http://students.eng.fiu.edu/~denver/divorce1.html \n"}
{"id": "77668", "url": "https://en.wikipedia.org/wiki?curid=77668", "title": "Scar", "text": "Scar\n\nA scar is an area of fibrous tissue that replaces normal skin after an injury. Scars result from the biological process of wound repair in the skin, as well as in other organs and tissues of the body. Thus, scarring is a natural part of the healing process. With the exception of very minor lesions, every wound (e.g., after accident, disease, or surgery) results in some degree of scarring. An exception to this are animals with complete regeneration, which regrow tissue without scar formation.\n\nScar tissue is composed of the same protein (collagen) as the tissue that it replaces, but the fiber composition of the protein is different; instead of a random basketweave formation of the collagen fibers found in normal tissue, in fibrosis the collagen cross-links and forms a pronounced alignment in a single direction. This collagen scar tissue alignment is usually of inferior functional quality to the normal collagen randomised alignment. For example, scars in the skin are less resistant to ultraviolet radiation, and sweat glands and hair follicles do not grow back within scar tissues. A myocardial infarction, commonly known as a heart attack, causes scar formation in the heart muscle, which leads to loss of muscular power and possibly heart failure. However, there are some tissues (e.g. bone) that can heal without any structural or functional deterioration.\n\nAll scarring is composed of the same collagen as the tissue it has replaced, but the composition of the scar tissue, compared to the normal tissue, is different. Scar tissue also lacks elasticity unlike normal tissue which distributes fiber elasticity. Scars differ in the amounts of collagen overexpressed. Labels have been applied to the differences in overexpression. Two of the most common types are hypertrophic and keloid scarring, both of which experience excessive stiff collagen bundled growth overextending the tissue, blocking off regeneration of tissues. Another form is atrophic scarring (sunken scarring), which also has an overexpression of collagen blocking regeneration. This scar type is sunken, because the collagen bundles do not overextend the tissue. Stretch marks (striae) are regarded as scars by some.\n\nHigh melanin levels and either African or Asian ancestry may make adverse scarring more noticeable.\n\nHypertrophic scars occur when the body overproduces collagen, which causes the scar to be raised above the surrounding skin. Hypertrophic scars take the form of a red raised lump on the skin. They usually occur within 4 to 8 weeks following wound infection or wound closure with excess tension and/or other traumatic skin injuries.\n\nKeloid scars are a more serious form of excessive scarring, because they can grow indefinitely into large, tumorous (although benign) neoplasms.\n\nHypertrophic scars are often distinguished from keloid scars by their lack of growth outside the original wound area, but this commonly taught distinction can lead to confusion.\n\nKeloid scars can occur on anyone, but they are most common in dark-skinned people. They can be caused by surgery, accident, acne or, sometimes, body piercings. In some people, keloid scars form spontaneously. Although they can be a cosmetic problem, keloid scars are only inert masses of collagen and therefore completely harmless and not cancerous. However, they can be itchy or painful in some individuals. They tend to be most common on the shoulders and chest. Hypertrophic scars and keloids tend to be more common in wounds closed by secondary intention. Surgical removal of keloid is risky and may excerbate the condition and worsening of the keloid.\n\nAn atrophic scar takes the form of a sunken recess in the skin, which has a pitted appearance. These are caused when underlying structures supporting the skin, such as fat or muscle, are lost. This type of scarring is often associated with acne, chickenpox, other diseases (especially \"Staphylococcus\" infection), surgery, certain insect and spider bites, or accidents. It can also be caused by a genetic connective tissue disorder, such as Ehlers–Danlos syndrome.\n\nStretch marks (technically called \"striae\") are also a form of scarring. These are caused when the skin is stretched rapidly (for instance during pregnancy,<ref name=\"doi10.1002/14651858.CD000066\"></ref> significant weight gain, or adolescent growth spurts), or when skin is put under tension during the healing process, (usually near joints). This type of scar usually improves in appearance after a few years.\n\nElevated corticosteroid levels are implicated in striae development.\n\nHumans and other placental mammals have an umbilical scar (commonly referred to as to a navel) which starts to heal when the umbilical cord is cut after birth. Egg-laying animals have an umbilical scar which, depending on the species, may remain visible for life or disappear within a few days after birth.\n\nA scar is the product of the body's repair mechanism after tissue injury. If a wound heals quickly within two weeks with new formation of skin, minimal collagen will be deposited and no scar will form. When the extracellular matrix senses elevated mechanical stress loading, tissue will scar, and scars can be limited by stress shielding wounds. Small full thickness wounds under 2mm reepithilize fast and heal scar free. Deep second-degree burns heal with scarring and hair loss. Sweat glands do not form in scar tissue, which impairs the regulation of body temperature. Elastic fibers are generally not detected in scar tissue younger than 3 months old. In scars rete pegs are lost; through a lack of rete pegs scars tend to shear easier than normal tissue.\n\nThe endometrium, the inner lining of the uterus, is the only adult tissue to undergo rapid cyclic shedding and regeneration without scarring; shedding and restoring roughly inside a 7-day window on a monthly basis. All other adult tissues, upon rapid shedding or injury, can scar.\n\nProlonged inflammation, as well as the fibroblast proliferation can occur. Redness that often follows an injury to the skin is not a scar, and is generally not permanent (see wound healing). The time it takes for this redness to dissipate may, however, range from a few days to, in some serious and rare cases, a few years. \n\nScars form differently based on the location of the injury on the body and the age of the person who was injured. \n\nThe worse the initial damage is, the worse the scar will generally be. \n\nSkin scars occur when the dermis (the deep, thick layer of skin) is damaged. Most skin scars are flat and leave a trace of the original injury that caused them. \n\nWounds allowed to heal secondarily tend to scar worse than wounds from primary closure.\n\nAny injury does not become a scar until the wound has completely healed; this can take many months, or years in the worst pathological cases, such as keloids. To begin to patch the damage, a clot is created; the clot is the beginning process that results in a provisional matrix. In the process, the first layer is a provisional matrix and is not scar. Over time, the wounded body tissue then overexpresses collagen inside the provisional matrix to create a collagen matrix. This collagen overexpression continues and crosslinks the fiber arrangement inside the collagen matrix, making the collagen dense. This densely packed collagen, morphing into an inelastic whitish collagen scar wall, blocks off cell communication and regeneration; as a result, the new tissue generated will have a different texture and quality than the surrounding unwounded tissue. This prolonged collagen-producing process results in a fortuna scar.\n\nThe scarring is created by fibroblast proliferation, a process that begins with a reaction to the clot.\n\nTo mend the damage, fibroblasts slowly form the collagen scar. The fibroblast proliferation is circular and cyclically, the fibroblast proliferation lays down thick, whitish collagen inside the provisional and collagen matrix, resulting in the abundant production of packed collagen on the fibers giving scars their uneven texture. Over time, the fibroblasts continue to crawl around the matrix, adjusting more fibers and, in the process, the scarring settles and becomes stiff. This fibroblast proliferation also contracts the tissue. In unwounded tissue, these fibers are not overexpressed with thick collagen and do not contract.\n\nThe fibroblast involved in scarring and contraction is the myofibroblast, which is a specialized contractile fibroblast. These cells express α-smooth muscle actin (α-SMA).\n\nThe myofibroblasts are absent in the first trimester in the embryonic stage where damage heals scar free; in small incisional or excision wounds less than 2 mm that also heal without scarring; and in adult unwounded tissues where the fibroblast in itself is arrested; however, the myofibroblast is found in massive numbers in adult wound healing which heals with a scar.\n\nThe myofibroblasts make up a high proportion of the fibroblasts proliferating in the postembryonic wound at the onset of healing. In the rat model, for instance, myofibroblasts can constitute up to 70% of the fibroblasts, and is responsible for fibrosis on tissue.\nGenerally, the myofibroblasts disappear from the wound within 30 days, but can stay around in pathological cases in hypertrophy, such as keloids. Myofibroblasts have plasticity and in mice can be transformed into fat cells, instead of scar tissue, via the regeneration of hair follicles.\n\nEarly and effective treatment of acne scarring can prevent severe acne and the scarring that often follows. no prescription drugs for the treatment or prevention of scars were available.\n\nChemical peels are chemicals which destroy the epidermis in a controlled manner, leading to exfoliation and the alleviation of certain skin conditions, including superficial acne scars. Various chemicals can be used depending upon the depth of the peel, and caution should be used, particularly for dark-skinned individuals and those individuals susceptible to keloid formation or with active infections.\n\nFiller injections of collagen can be used to raise atrophic scars to the level of surrounding skin. Risks vary based upon the filler used, and can include further disfigurement and allergic reaction.\n\nNonablative lasers, such as the 585 nm pulsed dye laser, 1064 nm and 1320 nm , or the 1540 nm are used as laser therapy for hypertrophic scars and keloids. For burn scars they improve the appearance.\n\nAblative lasers such as the carbon dioxide laser (CO) or offer the best results for atrophic and acne scars. Like dermabrasion, ablative lasers work by destroying the epidermis to a certain depth. Healing times for ablative therapy are much longer and the risk profile is greater compared to nonablative therapy; however, nonablative therapy offers only minor improvements in cosmetic appearance of atrophic and acne scars.\n\nLow-dose, superficial radiotherapy is sometimes used to prevent recurrence of severe keloid and hypertrophic scarring. It is thought to be effective despite a lack of clinical trials, but only used in extreme cases due to the perceived risk of long-term side effects.\n\nSilicone scar treatments are commonly used in preventing scar formation and improving existing scar appearance. A meta-study by the Cochrane collaboration found weak evidence that silicone gel sheeting helps prevent scarring. However, the studies examining it were of poor quality and susceptible to bias.\n\nPressure dressings are commonly used in managing burn and hypertrophic scars, although supporting evidence is lacking. Care providers commonly report improvements, however, and pressure therapy has been effective in treating ear keloids. The general acceptance of the treatment as effective may prevent it from being further studied in clinical trials.\n\nA long-term course of corticosteroid injections into the scar may help flatten and soften the appearance of keloid or hypertrophic scars.\n\nTopical steroids are ineffective. However, clobetasol propionate can be used as an alternative treatment for keloid scars.\n\nScar revision is a process of cutting the scar tissue out. After the excision, the new wound is usually closed up to heal by primary intention, instead of secondary intention. Deeper cuts need a multilayered closure to heal optimally, otherwise depressed or dented scars can result.\n\nSurgical excision of hypertrophic or keloid scars is often associated to other methods, such as pressotherapy or silicone gel sheeting. Lone excision of keloid scars, however, shows a recurrence rate close to 45%. A clinical study is currently ongoing to assess the benefits of a treatment combining surgery and laser-assisted healing in hypertrophic or keloid scars.\n\n\"Subcision\" is a process used to treat deep rolling scars left behind by acne or other skin diseases. It is also used to lessen the appearance of severe glabella lines, though its effectiveness in this application is debatable. Essentially the process involves separating the skin tissue in the affected area from the deeper scar tissue. This allows the blood to pool under the affected area, eventually causing the deep rolling scar to level off with the rest of the skin area. Once the skin has leveled, treatments such as laser resurfacing, microdermabrasion or chemical peels can be used to smooth out the scarred tissue. \n\nResearch shows the use of vitamin E and onion extract (sold as Mederma) as treatments for scars is ineffective. Vitamin E causes contact dermatitis in up to 33% of users and in some cases it may worsen scar appearance and could cause minor skin irritations, but Vitamin C and some of its esters fade the dark pigment associated with some scars.\n\n\nThe permanence of scarring has led to its intentional use as a form of body art within some cultures and subcultures. These forms of ritual and non-ritual scarring practices can be found in many groups and cultures around the world.\n\nFirst attested in English in the late 14th century, the word \"scar\" derives from a conflation of Old French \"escharre\", from Late Latin \"eschara\", which is the latinisation of the Greek ἐσχάρα (\"eskhara\"), meaning \"hearth, fireplace\", but in medicine \"scab, eschar on a wound caused by burning or otherwise, and Middle English \"skar\" (\"cut, crack, incision\"), which is from Old Norse \"skarð\" (\"notch, gap\"). . The conflation helped to form the English meaning. Compare Scarborough for evolution of \"skarð\" to \"scar\".\n\nAn intradermal injection of transforming growth factor beta 3 (TGFβ3) is being tested. The results of three trials already completed were published in the \"Lancet\" along with an editorial commentary.\n\nA study implicated the protein ribosomal s6 kinase (RSK) in the formation of scar tissue and found the introduction of a chemical to counteract RSK could halt the formation of cirrhosis. This treatment also has the potential to reduce or even prevent altogether other types of scarring.\n\nResearch has also implicated osteopontin in scarring.\n\n"}
{"id": "25010568", "url": "https://en.wikipedia.org/wiki?curid=25010568", "title": "Sex Roles (journal)", "text": "Sex Roles (journal)\n\nSex Roles is a peer-reviewed scientific journal published by Springer. Articles appearing in \"Sex Roles\" are written from a feminist perspective, and topics span gender role socialization, gendered perceptions and behaviors, gender stereotypes, body image, violence against women, gender issues in employment and work environments, sexual orientation and identity, and methodological issues in gender research. The Editor-in-Chief is Janice D. Yoder.\n\n\"Sex Roles\" is abstracted/indexed in:\n\nAccording to the \"Journal Citation Reports\", the journal has a 2016 impact factor of 1.954, ranking it 1st out of 41 journals in the category \"Women's Studies\" and 11th out of 62 journals in the category, \"Social Psychology\".\n\n"}
{"id": "26875658", "url": "https://en.wikipedia.org/wiki?curid=26875658", "title": "Studies on intercessory prayer", "text": "Studies on intercessory prayer\n\nSome religions claim that praying for somebody who is sick can have positive effects on the health of the person being prayed for.\n\nMeta-studies of the literature in the field have been performed showing evidence only for no effect or a potentially small effect. For instance, a 2006 meta analysis on 14 studies concluded that there is \"no discernible effect\" while a 2007 systemic review of intercessory prayer reported inconclusive results, noting that 7 of 17 studies had \"small, but significant, effect sizes\" but the review noted that the most methodologically rigorous studies failed to produce significant findings.\n\nIn comparison to other fields that have been scientifically studied, carefully monitored studies of prayer are relatively few. The field remains tiny, with about $5 million spent worldwide on such research. If and when more studies of prayer are done, the issue of prayer's efficacy may be further clarified.\n\nThe third party studies discussed here have all been performed using Christian prayers. Some have reported null results, some have reported correlations between prayer and health, and some have reported contradictory results in which beneficiaries of prayer had worsened health outcomes. The parameters used within the study designs have varied, for instance, daily or weekly prayers, whether to provide patient photographs, with full or partial names, measuring levels of belief in prayer, and whether patients underwent surgery.\n\nIn 1872, the Victorian scientist Francis Galton made the first statistical analysis of third-party prayer. He hypothesized, partly as satire, that if prayer were effective, members of the British Royal Family would live longer than average, given that thousands prayed for their well-being every Sunday, and he prayed over randomized plots of land to see whether the plants would grow any faster, and found no correlation in either case.\n\nA 1988 study by Randolph C. Byrd used 393 patients at the San Francisco General Hospital coronary care unit (CCU). Measuring 29 health outcomes using three-level (good, intermediate, or bad) scoring, the prayer group suffered fewer newly diagnosed ailments on only six of them. Byrd concluded that \"Based on these data there seemed to be an effect, and that effect was presumed to be beneficial\", and that \"intercessory prayer to the Judeo-Christian God has a beneficial therapeutic effect in patients admitted to a CCU.\" The reaction from the scientific community concerning this study was mixed. Several reviewers considered Byrd’s study to be well-designed and well-executed, while others remained skeptical. A criticism of Byrd's study, which also applies to most other studies, is the fact that he did not limit prayers by the friends and family of patients, hence it is unclear which prayers, if any, may have been measured.\n\nThe Byrd study had an inconsistent pattern of only six positive outcomes amongst 26 specific problem conditions. A systematic review suggested this indicates possible Type I errors.\n\nA 1999 follow-up by William S. Harris et al. attempted to replicate Byrd's findings under stricter experimental conditions, noting that the original research was not completely blinded and was limited to only \"prayer-receptive\" individuals (57 of the 450 patients invited to participate in the study refused to give consent \"for personal reasons or religious convictions\"). Using a different, continuous weighted scoring system – which admittedly was, like Byrd's scoring, \"an unvalidated measure of CCU outcomes\" – Harris et al. concluded that \"supplementary, remote, blinded, intercessory prayer produced a measurable improvement in the medical outcomes of critically ill patients\", and suggested that \"prayer be an\neffective adjunct to standard medical care.\" However, when they applied Byrd’s scores to their data, they could not document an effect of prayer using his scoring method. Critics have suggested that both Byrd's and Harris's results can be explained by chance. Richard P. Sloan compared the Byrd and Harris studies with the sharpshooter fallacy, \"searching through the data until a significant effect is found, then drawing the bull's-eye.\"\n\nA 1997 study by O'Laoire measured the effects on the agents performing daily prayers and reported benefits not only for the beneficiaries, but also for the agents, and the benefit levels correlated with the belief levels of agents and beneficiaries in some cases. The study measured anxiety and depression. This study used beneficiary names as well as photographs.\n\nIn 1998 Fred Sicher et al. performed a small scale double-blind randomized study of 40 patients with advanced AIDS. The patients were in category C-3 with CD4 cell counts below 200 and each had at least one case of AIDS-defining illness. The patients were randomly assigned to receive distant intercessory healing or none at all. The intercession took place by people in different parts of the United States who never had any contact with the patients. Both patients and physicians were blind to who received or did not receive intercession. Six months later the prayer group had significantly fewer AIDS illnesses, less frequent doctor visits, and fewer days in the hospital. However, CD4 counts and scores on other physiological tests had no significant variation between the two groups of patients.\n\nA 2001 double-blind study at the Mayo Clinic randomized 799 discharged coronary surgery patients into a control group and an intercessory prayer group, which received prayers at least once a week from 5 intercessors per patient. Analyzing \"primary end points\" (death, cardiac arrest, rehospitalization, etc.) after 26 weeks, the researchers concluded \"intercessory prayer had no significant effect on medical outcomes after hospitalization in a coronary care unit.\"\n\nIn 2001 the \"Journal of Reproductive Medicine\" published an experimental study by three Columbia University researchers indicating that prayer for women undergoing \"in vitro\" fertilization-embryo transfer (IVF-ET) resulted in a success rate (50%) of pregnancy double that of women who did not receive prayer. Columbia University issued a news release saying that the study had been carefully designed to eliminate bias. The most vocal skeptic was Bruce Flamm, a clinical professor of gynecology and obstetrics at the University of California at Irvine, who found that the experimental procedures were flawed. One of the study's authors, Cha, responded to criticism of the study in the November 2004 issue of JRM. In December 2001, the U.S. Department of Health and Human Services' (DHHS) office for Human Research Protections (OHRP) confirmed a report by the Columbia University Health Sciences Division that one of the study’s authors, Rogerio Lobo, only learned of the study six to twelve months after the study was completed, and that he had only provided editorial assistance. The name of Columbia University and Lobo were retracted from the study.\n\nA 2001 study by Leonard Leibovici used records of 3,393 patients who had developed blood infections at the Rabin Medical Center between 1990 and 1996 to study \"retroactive\" intercessory prayer. To compound the alleged miraculous power of prayer itself, the prayers were performed \"after\" the patients had already left the hospital. All 3,393 patients were those in the hospital between 1990 and 1996, and the prayers were conducted in 2000. Two of the outcomes, length of stay in the hospital and duration of fever, were found to be significantly improved in the intervention group, implying that prayer can even change events \"in the past.\" However, the \"mortality rate was lower in the intervention group, but the difference between the groups was not significant.\" Leibovici concluded that \"Remote, retroactive intercessory prayer was associated with a shorter stay in hospital and a shorter duration of fever in patients with a bloodstream infection.\" Leibovici goes on to note that in the past, people knew the way to prevent diseases (he cites scurvy) without understanding why it worked. In saying so, he suggests that if prayer truly does have a positive effect on patients in hospital, then there may be a naturalist explanation for it that we do not yet understand. After many scientists and scholars criticized this retroactive study, Leibovici stated in 2002 that the \"article has nothing to do with religion. I believe that prayer is a real comfort and help to a believer. I do not believe it should be tested in controlled trials.\" The study has been summarised as being \"intended lightheartedly to illustrate the importance of asking research questions that fit with scientific models.\"\n\nIn 2003, Larry Dossey, the executive editor of the journal \"\" and an advocate of faith healing co-authored a paper responding to Leibovici which discussed possible mechanisms to explain the results reported. Olshansky and Dossey invoked quantum mechanics to explain not only the benefits of intercessory prayer, but also how it might operate retroactively, drawing strong criticisms from physicist Victor Stenger and physician Jeffrey Bishop. The observer effect is regularly used to suggest that conscious control of physical reality is predicted by quantum mechanics, but this misconception \"can be traced to a misinterpretation of wave-particle duality.\" In relation to backwards causality, Stenger noted that \"the results of some quantum experiments may be interpreted as evidence for events in the future affecting events in the past at the quantum level, [but] no theoretical basis exists for applying this notion on the macroscopic scale of human experience.\" He concluded that while \"the atoms in biological systems are quantum in nature ... their collective behaviour does not exhibit any quantum effects. ... What is more, even if the brain were a quantum system, that would not imply that it can break the laws of physics any more than electrons or photons, which are inarguably quanta.\" One further point which illustrates that Dossey and Olshansky do not understand the physics they are using is seen in their invocation of quantum nonlocality in explaining backward causation, stating that \"[r]etroactive prayer may be less absurd than [Leibovici] supposes, in the light of the discovery of non-local phenomena.\" Unfortunately, the two are mutually incompatible: in allowing reverse causality in a model, the phenomenon of nonlocality ceases. Dossey has written in \"Explore\" about coining the term \"nonlocal mind\" in 1987, though quantum nonlocality goes back to a 1935 paper by Einstein, Podolsky, and Rosen. Olshansky and Dossey defended their work from various critics in the \"British Medical Journal's rapid response section\n\nOlensky and Dossey are not alone amongst alternative medicine proponents in having missed the point which Leibovici was making. In 2004, Stephen Wright described the Olshansky and Dossey contribution as a \"thoughtful essay,\" and it was praised by an editorial in the \"Journal of Alternative and Complementary Medicine\" the same year. In 2005, Olshansky and Dossey's work was included in a critical review published in \"Explore\" which concluded that \"Religious activity may improve health outcomes.\" Their work was also defended in the \"British Medical Journal\" itself in 2004. Dossey authored an \"Explore\" paper defending experiments on the medical effects of prayer in 2005.\n\nA 2005 MANTRA (Monitoring and Actualisation of Noetic Trainings) II study conducted a three-year clinical trial led by Duke University comparing intercessory prayer and MIT (Music, Imagery, and Touch) therapies for 748 cardiology patients. The study is regarded as the first time rigorous scientific protocols were applied on a large scale to assess the feasibility of intercessory prayer and other healing practices. The study produced null results and the authors concluded, \"Neither masked prayer nor MIT therapy significantly improved clinical outcome after elective catheterization or percutaneous coronary intervention.\" Neither study specified whether photographs were used or whether belief levels were measured in the agents or those performing the prayers.\n\nHarvard professor Herbert Benson performed a \"Study of the Therapeutic Effects of Intercessory Prayer (STEP)\" in 2006. The STEP, commonly called the \"Templeton Foundation prayer study\" or \"Great Prayer Experiment\", used 1,802 coronary artery bypass surgery patients at six hospitals. Using double-blind protocols, patients were randomized into three groups, individual prayer receptiveness was not measured. The members of the experimental and control Groups 1 and 2 were informed they might or might not receive prayers, and only Group 1 received prayers. Group 3, which served as a test for possible psychosomatic effects, was informed they would receive prayers and subsequently did. Unlike some other studies, STEP attempted to standardize the prayer method. Only first names and last initial for patients were provided and no photographs were supplied. The congregations of three Christian churches who prayed for the patients \"were allowed to pray in their own manner, but they were instructed to include the following phrase in their prayers: \"for a successful surgery with a quick, healthy recovery and no complications\". Some participants complained that this mechanical way they were told to pray as part of the experiment was unusual for them. Complications of surgery occurred in 52 percent of those who received prayer (Group 1), 51 percent of those who did not receive it (Group 2), and 59 percent of patients who knew they would receive prayers (Group 3). There were no statistically significant differences in major complications or thirty-day mortality. In \"The God Delusion,\" evolutionary biologist Richard Dawkins wrote, \"It seems more probable that those patients who knew they were being prayed for suffered additional stress in consequence: performance anxiety', as the experimenters put it. Dr Charles Bethea, one of the researchers, said, \"It may have made them uncertain, wondering am I so sick they had to call in their prayer team?'\" Study co-author Jeffery Dusek stated that: \"Each study builds on others, and STEP advanced the design beyond what had been previously done. The findings, however, could well be due to the study limitations.\" Team leader Benson stated that STEP was not the last word on the effects of intercessory prayer and that questions raised by the study will require additional answers.\n\nA meta-analysis of several studies related to distant intercessory healing was published in the \"Annals of Internal Medicine\" in 2000. The authors analyzed 23 trials of 2,774 patients. Five of the trials were for prayer as the distant healing method, 11 were with noncontact touch, and 7 were other forms. Of these trials, 13 showed statistically significant beneficial treatment results, 9 showed no effect, and 1 showed a negative result. The authors concluded that it is difficult to draw conclusions regarding distant healing and suggested further studies.\n\nA 2003 levels of evidence review found \"some\" evidence for the hypothesis that \"Being prayed for improves physical recovery from acute illness\". It concluded that although \"a number of studies\" have tested this hypothesis, \"only three have sufficient rigor for review here\" (Byrd 1988, Harris et al. 1999, and Sicher et al. 1998). In all three, \"the strongest findings were for the variables that were evaluated most subjectively. This raises concerns about the possible inadvertent unmasking of the outcomes assessors. Moreover, the absence of a clearly plausible biological mechanism by which such a treatment could influence hard medical outcome results in the inclination to be skeptical of results.\" This 2003 review was performed before the 2005 MANTRA study and the 2006 STEP project, neither of which were conclusive in establishing the efficacy of prayer.\n\nVarious broader meta-studies of the literature in the field have been performed showing evidence only for no effect or a potentially small effect. For instance, a 2006 meta analysis on 14 studies concluded that \"There is no scientifically discernable effect for intercessory prayer as assessed in controlled studies\". However, a 2007 systemic review of 17 intercessory prayer studies found \"small, but significant, effect sizes for the use of intercessory prayer\" in 7 studies, but \"prayer was unassociated with positive improvement in the condition of client\" in the other 10, concluding that based upon the American Psychology Association's Division 12 (clinical psychology) criteria for evidence-based practice, intercessory prayer \"must be classified as an experimental intervention.\" The review noted that the most methodologically rigorous studies had failed to produce significant findings.\n\n\n"}
{"id": "14753861", "url": "https://en.wikipedia.org/wiki?curid=14753861", "title": "Unwarranted variation", "text": "Unwarranted variation\n\nUnwarranted variation (or geographic variation) in health care service delivery refers to medical practice pattern variation that cannot be explained by illness, medical need, or the dictates of evidence-based medicine.\n\nUnwarranted variation (or geographic variation) in health care service delivery refers to differences that cannot be explained by illness, medical need, or the dictates of evidence-based medicine. The term was coined by Dr. John Wennberg. \nIt can be caused by shortfalls in three areas:\n\n\nIn 1967, while working in the Regional Medical Program created with a $350,000 grant from President Lyndon Johnson, Wennberg analyzed Medicare data to determine how well hospitals and doctors were serving their communities. He found 4 types of variation: the underuse of effective care, variations in outcomes attributable to the quality of care, the misuse of preference-sensitive treatments and overuse of supply-sensitive services.\n\nAccording to Health Dialog, a privately held, for-profit disease-management company which was established to address unwarranted variation:\nIf you live in northern Idaho, and you develop back pain, chances are good that you’ll undergo surgery to treat your pain. Move to the southern tip of Texas, however, and the chances that you’ll undergo that same surgery will drop by a factor of 6. The surgery is no more effective in Idaho than it is in Texas. It’s just that doctors in the northwest are more likely than those in southern Texas to recommend surgery. This phenomenon, in which doctors practice medicine differently depending on where they’re from, is called practice pattern variation. And it isn’t limited to treating back pain, or even surgical decisions. There is also variation in treatment for chronic conditions, such as use of beta blockers for individuals with Congestive Heart Failure (CHF) or lipid testing for those with diabetes.\n\nWennberg and colleagues at the Dartmouth Center for Evaluative Clinical Sciences documented these wide variations in how healthcare is practiced around the United States. They have asserted that most of this variation is unwarranted. Health Dialog was built to address unwarranted variation in healthcare: the overuse, underuse and misuse of medical care. Wennberg and his colleagues concluded that if unwarranted variation in the US healthcare system could be reduced, the quality of care would go up and healthcare costs would go down. Studies have shown that if unwarranted variation could be reduced in the Medicare population, quality of care would rise dramatically and costs could be lowered by as much as 30%.\nUnwarranted variation in medical practice is costly and deadly as noted by Martin Sipkoff in \"9 Ways To Reduce Unwarranted Variation\". Analysis of Medicare data revealed that per-capita spending per enrollee in Miami was almost 2.5 times as much as in Minneapolis, even after adjusting data for age, sex, and race. According to a 2003 report from the National Committee for Quality Assurance 57,000 lives were lost annually because US physicians have not been using evidence-based medicine to guide their care.\n\n\"We're literally dying, waiting for the practice of medicine to catch up with medical knowledge,\" said Margaret O'Kane, president of NCQA. The report, \"The State of Health Care Quality 2003,\" says that the deaths \"should not be confused with those attributable to medical errors or lack of access to health care. This report shows that a thousand Americans die each week because the care they get is not consistent with the care that medical science tells us they should get.\"\n\nStudies show that individuals with diabetes should have blood lipids monitored regularly, yet patients in Chicago are 50% less likely to receive these tests than patients in Fort Lauderdale. A patient with heart disease in Bloomington, Indiana, is three times more likely to have bypass surgery than a similar patient in Albuquerque. In Miami, where medical services are abundant, Medicare pays more than twice as much per person per year as it does in Minneapolis, with no discernible difference in overall health or life expectancy.\n\nIn November 2010 the Department of Health QIPP Right Care programme published the first NHS Atlas of Variation in Healthcare, inspired by the work of Wennberg. clinicians selected 34 topics, as being important to their speciality, which were mapped by Primary Care Trust area, then the healthcare commissioning body. The Atlas was published to challenge commissioners to maximise health outcome and minimise inequalities by addressing unwarranted variation.\n\"Awareness is the first important step in identifying and addressing unwarranted variation; if the existence of variation is unknown, the debate about whether it is unwarranted cannot take place.\"\n\nThe 2010 Atlas revealed widespread variations in outcome, quality, cost and activity:\n\nA further extended Atlas was published in November 2011, mapping variation across 71 indicators and a follow-on series of Atlases focussing on specific themes in more depth like children and young people, diabetes, kidney disease and respiratory disease.\nA forthcoming atlas will be about liver disease, diagnostics, organ donation and transplantation. \nPublication of the Atlases has been well-received within the NHS and by patient groups and clinical societies.\nIn 2012, the British Department of Health published a mandate for the new NHS Commissioning Board. On variation in healthcare, the Mandate charged the Board with the responsibility to \"shine a light on variation\" and \"to make significant progress... in reducing unjustified variation... Success will be measured not only by the average level of improvement but also by progress in reducing health inequalities and unjustified variation.\"\n\nIn April 2016, Jane Cummings, Chief Nursing Officer (CNO) for England, launched a national strategic framework for nurses, midwives and care staff in England called Leading Change, Adding Value. This framework sets out the 10 commitments for nurses, midwives and care staff in England towards identifying and addressing unwarranted variation in care practice. The framework builds on the previous CNO strategy 'Compassion in Practice' and identifies the nursing, midwifery and care staff approach to meeting the triple aims of 'improving health outcomes, reducing the care quality gap and effective use of resources' as set out in the Department of Health's Five Year Forward View. Actions to address unwarranted variation in nursing, midwifery and care provision are underpinned by the values of the 6cs, and a skills and knowledge framework is being developed to support staff in delivering on the 10 commitments set out in the framework.\n\n\n\n\n"}
{"id": "31863", "url": "https://en.wikipedia.org/wiki?curid=31863", "title": "Uterus", "text": "Uterus\n\nThe uterus (from Latin \"uterus\", plural \"uteri\") or womb is a major female hormone-responsive secondary sex organ of the reproductive system in humans and most other mammals. In the human, the lower end of the uterus, the cervix, opens into the vagina, while the upper end, the fundus, is connected to the fallopian tubes. It is within the uterus that the fetus develops during gestation. In the human embryo, the uterus develops from the paramesonephric ducts which fuse into the single organ known as a simplex uterus. The uterus has different forms in many other animals and in some it exists as two separate uteri known as a duplex uterus.\n\nIn English, the term \"uterus\" is used consistently within the medical and related professions, while the Germanic-derived term \"womb\" is also commonly used in everyday contexts.\n\nThe uterus is located within the pelvic region immediately behind and almost overlying the bladder, and in front of the sigmoid colon. The human uterus is pear-shaped and about long, broad (side to side), and thick. A typical adult uterus weighs about 60 grams. The uterus can be divided anatomically into four regions: The fundus – the uppermost rounded portion of the uterus, the corpus (body), the cervix and the cervical canal. The cervix protrudes into the vagina. The uterus is held in position within the pelvis by ligaments, which are called endopelvic fascia. These ligaments include the pubocervical, transverse cervical ligaments or cardinal ligaments, and the uterosacral ligaments. It is covered by a sheet-like fold of peritoneum, the broad ligament.\nFrom outside to inside, regions of the uterus include:\n\nThe uterus has three layers, which together form the \"uterine wall\". From innermost to outermost, these layers are as follows:\n\nThe uterus is primarily supported by the pelvic diaphragm, perineal body and the urogenital diaphragm. Secondarily, it is supported by ligaments and the peritoneal ligament the broad ligament of uterus.\n\nIt is held in place by several peritoneal ligaments, of which the following are the most important (there are two of each):\n\nNormally the uterus lies in anteversion & anteflexion. In most women, the long axis of the uterus is bent forward on the long axis of the vagina, against the urinary bladder. This position is referred to as anteversion of the uterus. Furthermore, the long axis of the body of the uterus is bent forward at the level of the internal os with the long axis of the cervix. This position is termed anteflexion of the uterus. Uterus assumes anteverted position in 50% women, retroverted position in 25% women and rest have midposed uterus.\n\nThe uterus is in the middle of the pelvic cavity in frontal plane (due to ligamentum latum uteri). The fundus does not surpass the linea terminalis, while the vaginal part of the cervix does not extend below interspinal line. The uterus is mobile and moves posteriorly under the pressure of a full bladder, or anteriorly under the pressure of a full rectum. If both are full, it moves upwards. Increased intra-abdominal pressure pushes it downwards. The mobility is conferred to it by musculo-fibrous apparatus that consists of suspensory and sustentacular part. Under normal circumstances the suspensory part keeps the uterus in anteflexion and anteversion (in 90% of women) and keeps it \"floating\" in the pelvis. The meaning of these terms are described below:\n\nThe sustentacular part supports the pelvic organs and comprises the larger pelvic diaphragm in the back and the smaller urogenital diaphragm in the front.\n\nThe pathological changes of the position of the uterus are:\n\nIn cases where the uterus is \"tipped\", also known as retroverted uterus, women may have symptoms of pain during sexual intercourse, pelvic pain during menstruation, minor incontinence, urinary tract infections, fertility difficulties, and difficulty using tampons. A pelvic examination by a doctor can determine if a uterus is tipped.\n\nThe uterus is supplied by arterial blood both from the uterine artery and the ovarian artery. Another anastomotic branch may also supply the uterus from anastomosis of these two arteries.\n\nAfferent nerves supplying the uterus are T11 and T12. Sympathetic supply is from hypogastric plexus and ovarian plexus. Parasympathetic supply is from second, third and fourth sacral nerves.\n\nBilateral Müllerian ducts form during early fetal life. In males, anti-müllerian hormone (AMH) secreted from the testes leads to their regression. In females, these ducts give rise to the Fallopian tubes and the uterus. In humans the lower segments of the two ducts fuse to form a single uterus, however, in cases of uterine malformations this development may be disturbed. The different uterine forms in various mammals are due to various degrees of fusion of the two Müllerian ducts.\n\nVarious congenital conditions of the uterus can develop in utero. Though uncommon some of these are a double uterus, didelphic uterus, bicornate uterus and others.\n\nThe reproductive function of the uterus is to accept a fertilized ovum which passes through the utero-tubal junction from the fallopian tube (uterine tube). The fertilized ovum divides to become a blastocyst, which implants into the endometrium, and derives nourishment from blood vessels which develop exclusively for this purpose. The fertilized ovum becomes an embryo, attaches to a wall of the uterus, creates a placenta, and develops into a fetus (gestates) until childbirth. Due to anatomical barriers such as the pelvis, the uterus is pushed partially into the abdomen due to its expansion during pregnancy. Even during pregnancy the mass of a human uterus amounts to only about a kilogram (2.2 pounds).\n\nThe uterus also plays a role in sexual response, by directing blood flow to the pelvis and ovaries, and to the external genitals, including the vagina, labia, and clitoris.\n\nA hysterectomy is the surgical removal of the uterus which may be carried out for a number of reasons including the ridding of tumours both benign and malignant. A complete hysterectomy involves the removal of the body, fundus, and cervix of the uterus. A partial hysterectomy may just involve the removal of the uterine body while leaving the cervix intact. It is the most commonly performed gynecological surgical procedure. \n\nDuring pregnancy the growth rate of the fetus can be assessed by measuring the fundal height.\n\nSome pathological states include:\n\n\nMost animals that lay eggs, such as birds and reptiles, including most ovoviviparous species, have an oviduct instead of a uterus. However, recent research into the biology of the viviparous (not merely ovoviviparous) skink \"Trachylepis ivensi\" has revealed development of a very close analogue to eutherian mammalian placental development.\n\nIn monotremes, mammals which lay eggs, namely the platypus and the echidnas, either the term \"uterus\" or \"oviduct\" is used to describe the same organ, but the egg does not develop a placenta within the mother and thus does not receive further nourishment after formation and fertilization.\n\nMarsupials have two uteri, each of which connect to a lateral vagina and which both use a third, middle \"vagina\" which functions as the birth canal. Marsupial embryos form a choriovitelline placenta (which can be thought of as something between a monotreme egg and a \"true\" placenta), in which the egg's yolk sac supplies a large part of the embryo's nutrition but also attaches to the uterine wall and takes nutrients from the mother's bloodstream. However, bandicoots also have a rudimentary chorioallantoic placenta, similar to those of placental mammals.\n\nThe fetus usually develops fully in placental mammals and only partially in marsupials including kangaroos and opossums. In marsupials the uterus forms as a duplex organ of two uteri. In monotremes (egg-laying mammals) such as the platypus, the uterus is duplex and rather than nurturing the embryo, secretes the shell around the egg. It is essentially identical with the shell gland of birds and reptiles, with which the uterus is homologous.\n\nIn mammals, the four main forms of the uterus are: duplex, bipartite, bicornuate and simplex.\n\nTwo uteri usually form initially in a female and usually male fetus, and in placental mammals they may partially or completely fuse into a single uterus depending on the species. In many species with two uteri, only one is functional. Humans and other higher primates such as chimpanzees, usually have a single completely fused uterus, although in some individuals the uteri may not have completely fused.\n\n\n"}
{"id": "56502322", "url": "https://en.wikipedia.org/wiki?curid=56502322", "title": "Vaginal epithelium", "text": "Vaginal epithelium\n\nThe vaginal epithelium is the aglandular inner lining of the vagina consisting of multiple layers of (squamous) cells. The basal membrane provides the support for the first layer of the epithelium-the basal layer. The intermediate layers lie upon the basal layer and the superficial layer is the outermost layer of the epithelium. Anatomists have described the epithelium as consisting of as many as 40 distinct layers. The mucous found on the epithelium is secreted by the cervix and uterus. The rugae of the epithelium create a involuted surface and result in a large surface area that covers 360 cm. This large surface area allows the trans-epithelial absorption of some medications via the vaginal route.\n\nIn the course of the reproductive cycle, the vaginal epithelium is subject to normal, cyclic changes, that are influenced by estrogen: with increasing circulating levels of the hormone, there is proliferation of epithelial cells along with an increase in the number of cell layers. As cells proliferate and mature, they undergo partial cornification. Although hormone induced changes occur in the other tissues and organs of the female reproductive system, the vaginal epithelium is more sensitive and its structure is an indicator of estrogen levels. Some Langerhans cells and melanocytes are also present in the epithelium. The epithelium of the ectocervix is contiguous with that of the vagina, possessing the same properties and function. The vaginal epithelium is divided into layers of cells, including the basal cells, the parabasal cells, the superficial squamous flat cells, and the intermediate cells. The superficial cells exfoliate continuously and basal cells replace the superficial cells that die and slough off from the stratum corneum. Under the stratus corneum is the stratum granulosum and stratum spinosum. The cells of the vaginal epithelium retain an usually high level of glycogen compared to other epithelial tissue in the body. The surface patterns on the cells themselves are circular and arranged in longitudinal rows. The epithelial cells of the uterus possess some of the same characteristics of the vaginal epithelium.\n\nVaginal epithelium forms transverse ridges or rugae that are most prominent in the lower third of the vagina. This structure of the epithelium results in an increased surface area that allows for stretching. This layer of epithelium is protective and its uppermost surface of cornified (dead) cells that are unique in that they are permeable to microorganisms that are part of the vaginal flora. The lamina propria of connective tissue is under the epithelium.\n\nThe basal layer of the epithelium is the most mitotically active and reproduces new cells. This layer is composed of one layer of cuboidal cells laying on top of the basal membrane.\n\nThe parabasal cells include the stratum granulousum and the stratum spinosum. In these two layers, cells from the lower basal layer transition from active metabolic activity to death (apoptosis). In these mid-layers of the epithelia, the cells begin to lose their mitochondria and other cell organelles. The multiple layers of parabasal cells are polyhedral in shape with prominent nuclei.\n\nIntermediate cells make abundant glycogen and store it. Estrogen induces the intermediate and superficial cells to fill with glycogen. The intermediate cells contain nuclei and are larger than the parabasal cells and more flattened. Some have identified a transitional layer of cells above intermediate layer\n\nEstrogen induces the intermediate and superficial cells to fill with glycogen. Several layers of superficial cells exist that consist large, flattened cells with indistinct nuclei. The superficial cells are exfoliated continuously.\n\nThe junctions between epithelial cells regulate the passage of molecules, bacteria and viruses by functioning as a physical barrier. The three types of structural adhesions between epithelial cells are: tight junctions, adherens junctions, and desmosomes. \"Tight junctions (zonula occludens) are composed of transmembrane proteins that make contact across the intercellular space and create a seal to restrict transmembrane proteins difusion. of molecules across the epithelial sheet. Tight junctions also have an organizing role in epithelial polarization by limiting the mobility of membrane-bound molecules between the apical and basolateral domains of the plasma membrane of each epithelial cell. Adherens junctions (zonula adherens) connect bundles of actin filaments from cell to cell to form a continuous adhesion belt, usually just below the microfilaments.\" Junction integrity changes as the cells move to the upper layers of the epidermis.\n\nThe vagina itself does not contain mucous glands. Though mucous is not produced by the vaginal epithelium, mucous originates from the cervix. The cervical mucous that is located inside the vagina can be used to assess fertility in ovulating women. The Bartholin's glands and Skene's glands located at the entrance of the vagina do produce mucous.\n\nThe epithelium of the vagina originates from three different precursors during embryonic and fetal development. These are the vaginal squamous epithelium of the lower vagina, the columnar epithelium of the endocervix, and the squamous epithelium of the upper vagina. The distinct origins of vaginal epithelium may impact the understanding of vaginal anomalies. Vaginal adenosis is a vaginal anomaly traced to displacement of normal vaginal tissue by other reproductive tissue within the muscular layer and epithelium of the vaginal wall. This displaced tissue often contains glandular tissue and appears as a raised, red surface.\n\nDuring the luteal and follicular phases of the estrous cycle the structure of the vaginal epithelium varies. The number of cell layers vary during the days of the estrous cycle:\n\nDay 10, 22 layers\n\nDays 12-14, 46 layers\n\nDay 19, 32 layers\n\nDay 24, 24 layers\n\nThe glycogen levels in the cells is at its highest immediately before ovulation.\n\nWithout estrogen, the vaginal epithelium is only a few layers thick. Only small round cells are seen that originate directly from the basal layer (basal cells) or the cell layers (parabasal cells) above it. The parabasal cells, which are slightly larger than the basal cells, form a five- to ten-layer cell layer. The parabasal cells can also differentiate into histiocytes or glandular cells. Estrogen also influences the changing ratios of nuclear constituents to cytoplasm. As a result of cell aging, cells with shrunken, seemingly foamy cell nuclei (intermediary cells ) develop from the parabasal cells. These can be categorized by means of the nuclear-plasma relation into \"upper\" and \"deep\" intermediate cells. Intermediate cells make abundant glycogen and store it. The further nuclear shrinkage and formation of mucopolysaccharides are distinct characteristics of superficial cells. The mucopolysaccharides form a keratin-like cell scaffold. Fully keratinized cells without a nucleus are called 'floes'. Intermediate and superficial cells are constantly exfoliated from the epithelium. The glycogen from these cells is converted to sugars and then fermented by the bacteria of the vaginal flora to lactic acid. The cells progress through the cell cycle and then decompose (cytolysis) within a week's time. Cytolysis occurs only in the presence of glycogen-containing cells, that is, when the epithelium is degraded to the upper intermediate cells and superficial cells. In this way, the cytoplasm is dissolved, while the cell nuclei remain.\n\nLow pH is necessary to control vaginal microbiota. Vaginal epithelial cells have a relatively high concentration of glycogen compared to other epithelial cells of the human body. The metabolism of this complex sugar by the lactobacillus dominated microbiome is responsible for vaginal acidity.\n\nThe cellular junctions of the vaginal epithelium help prevent pathogenic microorganisms from entering the body though some are still able to penetrate this barrier. Cells of the cervix and vaginal epithelium generate a mucous barrier (glycocalyx) in which immune cells reside. In addition, white blood cells provide additional immunity and are able to infiltrate and move through the vaginal epithelium. The epithelium is permeable to antibodies, other immune system cells, and macromolecules. The permeability of epithelium thus provides access for these immune system components to prevent the passage of invading pathogens into deeper vaginal tissue. The epithelium further provides a barrier to microbes the synthesis of antimicrobial peptides (beta-defensins and cathelicidins) and immunoglobulins. Terminally differentiated, superficial keratinocytes extrude the contents of lamellar bodies out of the cell to form a specialized, intercellular lipid envelope that encases the cells of the epidermis and provides a physical barrier to microorganisms.\n\nSexually transmitted infections, including HIV are rarely transmitted across intact and healthy epithelium. These protective mechanisms are due to: frequent exfoliation of the superficial cells, low pH, and innate and acquired immunity in the tissue. Research into the protective nature of the vaginal epithelium has been recommended as it would help in the design of topical medication and microbicides.\n\nThere are very rare malignant growths that can originate in the vaginal epithelium. Some are only known through case studies. They are more common in older women.\n\n\n\nThe vaginal epithelium changes significantly when estrogen levels decrease at menopause. Atrophic vaginitis usually causes scant odorless discharge with no odor\n\nThe vaginal epithelium has been studied since 1910 by a number of histologists.\n\nThe used of nanoprticles that can penetrate the cervical mucous (present in the vagina) and vaginal epithelium has been investigated to determine if medication can be administered in this manner to provide protection from infection of the Herpes simplex virus. Nanoparticle drug administration into and through the vaginal epithelium to treat HIV infection is also being investigated.\n\n"}
{"id": "32943530", "url": "https://en.wikipedia.org/wiki?curid=32943530", "title": "Work–life interface", "text": "Work–life interface\n\nWork–life interface is the intersection of work and private life. There are many aspects of one's personal life that can intersect with work including family, leisure, and health. Work–life interface is bidirectional; for instance, work can interfere with private life, and private life can interfere with work. This interface can be adverse in nature (e.g., work-life conflict) or can be beneficial (e.g., work-life enrichment) in nature.\n\nSeveral theories explain different aspects of the relationship between the work and family life. Boundary theory and border theory are the two fundamental theories that researchers have used to study these role conflicts. Other theories are built on the foundations of these two theories.\n\nSeven dominant theories have been utilized to explain this relationship on the boundary-border spectrum; These theories are: structural functioning, segmentation, compensation, supplemental and reactive compensation, role enhancement, spillover, and work enrichment model.\n\nThe roots of this theory can be traced back to the early 20th century, when industrial revolution was separating economic work from the family home. The 19th century’s technological advancements in machinery and manufacturing initiated the separation of work from family. However, it was not until the early 20th century that the first view of work-family theories started to shape. Structural-functionalism as one of the dominant sociology theories of early 20th century was a natural candidate.\nThe structural functionalism theory, which emerged following WWII, was largely influenced from the industrial revolution and the changes in the social role of men and women during this period. This theory implies that the life is concerned mainly with two separate spheres: productive life which happens in the workplace and affective life which is at home. Structural functionalism theory believes in the existence of radical separation between work (institution, workplace, or market) and families. According to this theory, these two (workplace and family) work best \"when men and women specialize their activities in separate spheres, women at home doing expressive work and men in the workplace performing instrumental tasks” (Kingsbury & Scanzoni, 1993; as cited in MacDermid, 2005: 18).\n\nIt has been argued that the work-family conflicts, in particular role conflicts, can be interpreted in terms of Lewis A. Coser's concept of \"greedy institutions\". These institutions are called \"greedy\" in the sense that they make all-encompassing demands on the commitment and loyalty of individuals, and tend to discourage involvement in other social spheres. Institutions such as religious orders, sects, academia, top level sports, the military and senior management have been interpreted as greedy institutions. On the other hand, also the family has been interpreted as a greedy institution in consideration of the demands placed on a caretaker. When a person is involved in two greedy institutions – be it child care and university, or family and the military, or others − task and role conflicts arise.\n\nBased on this theory work and family do not affect each other, since they are segmented and independent from each other. The literature also reports the usage of the terms compartmentalization, independence, separateness, disengagement, neutrality, and detachment to describe this theory.\n\nIn 1979, Piotrkowski argued that according to this theory employees “look to their homes as havens, [and] look to their families as sources of satisfaction lacking in the occupational sphere.\" What distinguishes compensation theory from the previous theories is that, in compensation theory, for the first time, the positive effect of work to family has been recognized.\n\nSupplemental and reactive compensation theories are two dichotomies of compensation theory which were developed during the late 1980s and the early 1990s. While compensation theory describes the behavior of employees in pursuing an alternative reward in the other sphere, supplemental and reactive compensation theories try to describe the reason behind the work-family compensation behavior of employees.\n\nAccording to this theory, the combination of certain roles has a positive, rather than a negative effect on well-being. This theory states that participation in one role is made better or easier by virtue of participation in the other role. Moreover, this theory acknowledges the negative effect of the work-family relationship, in which, only beyond a certain upper limit may overload and distress occur, however, the central focus of this perspective is mainly on the positive effects of work and family relationship, such as resource enhancement.\n\nSpillover is a process by which an employee’s experience in one domain affects their experience in another domain. Theoretically, spillover is perceived to be one of two types: positive or negative. Spillover as the most popular view of relationship between work and family, considers multidimensional aspects of work and family relationship.\n\nThis theory is one of the recent models for explaining the relationship between work and family. According to this model, experience in one role (work or family) will enhance the quality of life in the other role. In other words, this model tries to explain the positive effects of the work-family relationship.\n\nWork and family studies historically focus on studying the conflict between different roles that individuals have in their society, specifically their roles at work, and their roles as a family member.\n\nWork–family conflict is defined as interrole conflict where the participation in one role interfere with the participation in another. Greenhaus and Beutell (1985) differentiate three sources for conflict between work and family: \nConceptually, the conflict between work and family is bi-directional. Scholars distinguish between what is termed work-to-family conflict (WFC), and what is termed family-to-work conflict (FWC). This bi-directional view is displayed in the figure on the right.\nAccordingly, WFC might occur when experiences at work interfere with family life like extensive, irregular, or inflexible work hours. Family-to-work conflict occurs when experiences in the family interfere with work life. For example, a parent may take time off from work in order to take care of a sick child. Although these two forms of conflict — WFC and FWC — are strongly correlated with each other, more attention has been directed at WFC. This may because family demands are more elastic than the boundaries and responsibilities of the work role. Also, research has found that work roles are more likely to interfere with family roles than family roles are likely to interfere with work roles.\n\nAllen, Herst, Bruck, and Sutton (2000) describe in their paper three categories of consequences related to WFC: work-related outcomes (e.g., job satisfaction or job performance), nonwork-related outcomes (e.g., life or family satisfaction), and stress-related outcomes (e.g., depression or substance abuse). For example, WFC has been shown to be negatively related to job satisfaction whereas the association is more pronounced for females.\n\nThe vast majority of studies investigating the consequences of WFC were interrogating samples from Western countries, such as U.S. Therefore, the generalizability of their findings is in question. Fortunately, there is also literature studying WFC and its consequences in other cultural contexts, such as Taiwan and India. Lu, Kao, Cooper, Allen, Lapierre, O`Driscoll, Poelmans, Sanchez, and Spector (2009) could not find any cultural difference related in work-related and nonwork-related outcomes of WFC when they compared Great Britain and Taiwan. Likewise, Pal and Saksvik (2008) also did not detect specific cultural differences between employees from Norway and India. Nevertheless, more cross-cultural research is needed to understand the cultural dimensions of the WFC construct.\n\nThe research concerning interventions to reduce WFC is currently still very limited. As an exception, Nielson, Carlson, and Lankau (2001) showed that having a supportive mentor on the job correlates negatively with the employee’s WFC. However, other functions of mentoring, like the role model aspect, appear to have no effect on WFC. Therefore, the mechanisms how having a mentor influences the work–family interface remain unclear.\n\nIn terms of primary and secondary intervention there are some results. Hammer, Kossek, Anger, Bodner, and Zimmerman (2011) conducted a field study and showed that training supervisors to show more family supportive behavior, led to increased physical health in employees that were high in WFC. At the same time, employees having low WFC scores even decreased in physical health. This shows that even though interventions can help, it is important to focus on the right persons. Otherwise, the intervention damages more than it helps.\nAnother study (Wilson, Polzer-Debruyne, Chen, & Fernandes, 2007) showed that training employees helps to reduce shift work related WFC. Additionally, this training is more effective, if the partner of the focal person is also participating. Therefore, integrating the family into the intervention seems to be helpful too.\nThere are various additional factors that might influence the effectiveness of WFC interventions. For example, some interventions seem more adequate to reduce family-to-work conflict (FWC) than WFC (Hammer et al., 2011). More research is still needed, before optimal treatments against WFC can be derived.\n\nWork–family enrichment or work–family facilitation is a form of positive spillover, defined as a process whereby involvement in one domain establishes benefits and/or resources which then may improve performance or involvement in another domain (Greenhaus & Powell, 2006). For example, involvement in the family role is made easier by participation in the work role (Wayne, Musisca, & Fleeson, 2004).\n\nIn contrast to work–family conflict which is associated with several negative consequences, work–family enrichment is related to positive organizational outcomes such as job satisfaction and effort (Wayne et al., 2004). There are several potential sources enrichment can arise from. Examples are that resources (e.g., positive mood) gained in one role lead to better functioning in the other role (Sieber, 1974) or skills and attitudes that are acquired in one role are useful in the other role (Crouter, 1984).\n\nConceptually, enrichment between work and family is bi-directional. Most researchers make the distinction between what is termed work–family enrichment, and what is termed family–work enrichment. Work–family enrichment occurs, when ones involvement in work provides skills, behaviors, or positive mood which influences the family life in a positive way. Family-work enrichment, however, occurs when ones involvement in the family domain results in positive mood, feeling of success or support that help individuals to cope better with problems at work, feel more confident and in the end being more productive at work (Wayne, et al., 2004).\n\nSeveral antecedents of work–family enrichment have been proposed. Personality traits, such as extraversion and openness for experience have been shown to be positively related to work–family enrichment (Wayne et al., 2004). Next to individual antecedents, organizational circumstances such as resources and skills gained at work foster the occurrence of work–family enrichment (Voydanoff, 2004). For example, abilities such as interpersonal communication skills are learned at work and may then facilitate constructive communication with family members at home.\n\n\"Our review suggests that most of what is known about Work-Family issues is based on the experiences of heterosexual, Caucasian, managerial and professional employees in family arrangements\" (Casper et al., 2007, p. 10).\n\nResearch has focused especially on the role of the organization and the supervisor in the reduction of WFC. Results provide evidence for the negative association between the availability of family friendly resources provided by the work place and WFC. General support by the organization aids the employees to deal with work family issues so that organizational support is negatively connected to WFC (Kossek, Pichler, Bodner, & Hammer, 2011). Furthermore, Kossek et al. (2011) showed that work family specific support has a stronger negative connection with work family conflict. Interesting results by other researchers show that family friendly organizational culture also has an indirect effect on WFC via supervisor support and coworker support (Dolcoy & Daley, 2009). Surprisingly, some research also shows that the utilization of provided resources such as child care support or flexible work hours has no longitudinal connection with WFC (Hammer, Neal, Newson, Brockwood, & Colton, 2005). This result speaks against common assumptions. Also, the supervisor has a social-support function for his/her subordinates. As Moen and Yu (2000) showed supervisor support is an indicator for lower levels of WFC. Further support for this hypothesis stems from a study conducted by Thompson and Prottas (2005). Keeping in mind the support function, organizations should provide trainings for the supervisors and conduct the selection process of new employees. Similar as for organizational support, the meta-analysis by Kossek et al. (2011) showed that general supervisor is negatively connected to WFC. Again, work–family-specific supervisor support has a stronger negative connection with WFC. Aside from support by the organization and the supervisor, research points out a third source of work-place support: The coworker. The informal support by the coworker not only correlates with positive aspects such as job satisfaction, but is also negatively associated with negative variables such as WFC (Dolcos & Doley, 2009; Thompson & Prottas, 2005).\n\nIn terms of work–family enrichment, supervisors and organizations are also relevant, since they are able to provide with important resource (e.g., skills and financial benefits) and positive affect.\n\nA methodological review by Casper, Eby, Bordeaux, Lockwood, and Lambert (2007) summarizes the research methods used in the area of work-family research from 1980 to 2003. Their main findings are:\n\n· The descriptions of sample characteristics are often inconsistent and leave out essential information necessary to evaluate if generalization is appropriate or not.\n· Samples are mostly homogenous, neglecting diversity regarding racial, ethnic, cultural aspects, and non-traditional families (e.g., single or homosexual parents).\n· The research design of most studies is cross-sectional and correlational. Field settings are predominant (97%). Only 2% use experimental designs.\n· Surveys are mostly used for data collection (85%) whereas qualitative methods are used less often. Measures are mainly derived from one single person (76%) and focus on the individual level of analysis (89%). In this respect, research on, for example, dyads and groups have been neglected.\n· Simple inferential statistics are preferred (79%) instead of, for example, structural equation modeling (17%).\n· Regarding reliability aspects, coefficient alpha is often provided (87%), thereby reaching .79 on average. Pre-existing scales are often used (69%) containing multi-item measures (79%).\n\nIn light of these results, Casper, et al. (2007) give several recommendations. They suggest, for example, that researchers should use more longitudinal and experimental research designs, more diverse samples, data sources and levels of analysis.\n\nWork–life balance\n"}
{"id": "54729058", "url": "https://en.wikipedia.org/wiki?curid=54729058", "title": "World Mental Health survey initiative", "text": "World Mental Health survey initiative\n\nThe World Mental Health Survey Initiative is a collaborative project by World Health Organization, Harvard University, University of Michigan, and country-based researchers worldwide to coordinate the analysis and implementation of epidemiological surveys of mental and behavioral disorders and substance abuse in all WHO Regions.\n\nIt is estimated that the burden of mental and addictive disorders are among the highest in the world with expected increase over the next decades. However, those estimations are not based on cross-sectional epidemiological surveys, rather, they are mainly based on literature reviews and isolated studies.\nThe WMH Survey Initiative aim is to accurately address the global burden of mental disorders by obtaining accurate cross-sectional information about the prevalences and correlates of mental and behavioral disorders as well as substance abuse, allowing for evaluation of risk factors and study of patterns of service use in order to target appropriate interventions.\n\nCollaborators in this survey come from all WHO regions of the world, with 27 participating countries.\n"}
