{"id": "24016510", "url": "https://en.wikipedia.org/wiki?curid=24016510", "title": "Airborne disease", "text": "Airborne disease\n\nAn airborne disease is any disease that is caused by pathogens that can be transmitted through the air. Such diseases include many of considerable importance both in human and veterinary medicine. The relevant pathogens may be viruses, bacteria, or fungi, and they may be spread through breathing, talking, coughing, sneezing, raising of dust, spraying of liquids, toilet flushing or any activities which generates aerosol particles or droplets. Human airborne diseases do not include conditions caused by air pollution such as volatile organic compounds (VOCs), gasses and any airborne particles, though their study and prevention may help inform the science of airborne disease transmission.\n\nAirborne diseases include any that are caused via transmission through the air. Many airborne diseases are of great medical importance. The pathogens transmitted may be any kind of microbe, and they may be spread in aerosols, dust or liquids. The aerosols might be generated from sources of infection such as the bodily secretions of an infected animal or person, or biological wastes such as accumulate in lofts, caves, garbage and the like. Such infected aerosols may stay suspended in air currents long enough to travel for considerable distances, though the rate of infection decreases sharply with the distance between the source and the organism infected.\n\nAirborne pathogens or allergens often cause inflammation in the nose, throat, sinuses and the lungs. This is caused by the inhalation of these pathogens that affect a person's respiratory system or even the rest of the body. Sinus congestion, coughing and sore throats are examples of inflammation of the upper respiratory air way due to these airborne agents. Air pollution plays a significant role in airborne diseases which is linked to asthma. Pollutants are said to influence lung function by increasing air way inflammation.\n\nMany common infections can spread by airborne transmission at least in some cases, including: Anthrax (inhalational), Chickenpox, Influenza, Measles, Smallpox, Cryptococcosis, and Tuberculosis.\n\nAirborne diseases can also affect non-humans. For example, Newcastle disease is an avian disease that affects many types of domestic poultry worldwide which is transmitted via airborne contamination.\nOften, airborne pathogens or allergens cause inflammation in the nose, throat, sinuses, and the upper airway lungs. Upper airway inflammation causes coughing congestion, and sore throat. This is caused by the inhalation of these pathogens that affect a person's respiratory system or even the rest of the body. Sinus congestion, coughing and sore throats are examples of inflammation of the upper respiratory air way due to these airborne agents.\n\nAn airborne disease can be caused by exposure to a source: an infected patient or animal, by being transferred from the infected person or animal’s mouth, nose, cut, or needle puncture. People receive the disease through a portal of entry: mouth, nose, cut, or needle puncture.\n\nAirborne transmission of disease depends on several physical variables endemic to the infectious particle. Environmental factors influence the efficacy of airborne disease transmission; the most evident environmental conditions are temperature and relative humidity. The sum of all the factors that influence temperature and humidity, either meteorological (outdoor) or human (indoor), as well as other circumstances influencing the spread of the droplets containing the infectious particles, as winds, or human behavior, sum up the factors influencing the transmission of airborne diseases.\n\n\n\nSome ways to prevent airborne diseases include washing hands, using appropriate hand disinfection, getting regular immunizations against diseases believed to be locally present, wearing a respirator and limiting time spent in the presence of any patient likely to be a source of infection.\nExposure to a patient or animal with an airborne disease does not guarantee receiving the disease. Because of the changes in host immunity and how much the host was exposed to the particles in the air makes a difference to how the disease affects the body.\n\nAntibiotics are not prescribed for patients to control viral infections. They may however be prescribed to a flu patient for instance, to control or prevent bacterial secondary infections. They also may be used in dealing with air-borne bacterial primary infections, such as pneumonic plague.\n\nAdditionally the Centers for Disease Control and Prevention (CDC) has told consumers about vaccination and following careful hygiene and sanitation protocols for airborne disease prevention. Consumers also have access to preventive measures like UV Air purification devices that FDA and EPA-certified laboratory test data has verified as effective in inactivating a broad array of airborne infectious diseases. Many public health specialists recommend social distancing to reduce the transmission of airborne infections.\n\n"}
{"id": "1298976", "url": "https://en.wikipedia.org/wiki?curid=1298976", "title": "Asbestos and the law", "text": "Asbestos and the law\n\nLitigation related to asbestos injuries and property damages has been claimed to be the longest-running mass tort in U.S. history. Since asbestos-related disease has been identified by the medical profession in the late 1920s, workers' compensation cases were filed and resolved in secrecy, with a flood of litigation starting in the United States in the 1970s, and culminating in the 1980s and 1990s. A massive multi-district litigation (MDL) complex filing has remained pending in the Eastern District of Pennsylvania for over 20 years. As many of the scarring-related injury cases have been resolved, asbestos litigation continues to be hard-fought among the litigants, mainly in individually brought cases for terminal cases of asbestosis, mesothelioma, and other cancers.\n\nIn the late 19th century and early 20th century, asbestos was considered an ideal material for use in the construction industry. It was known to be an excellent fire retardant, to have high electrical resistance, and was inexpensive and easy to use.\n\nThe dangers related to asbestos arise mainly when the fibers become airborne and are inhaled. Because of the size of the fibers, the lungs cannot expel them. These fibers are also sharp and penetrate internal tissues.\n\nHealth problems attributed to asbestos include:\n\n\nConsiderable international controversy exists regarding the perceived rights and wrongs associated with litigation on compensation claims related to asbestos exposure and alleged subsequent medical consequences. Some measure of the vast range of views expressed in legal and political circles can perhaps be exemplified by the two quotes below, the first from Prof. Lester Brickman, an American legal ethicist writing in the \"Pepperdine Law Review\", and second, Michael Wills, a British Member of Parliament, speaking in the House of Commons on July 13. 2006:\n\n\"A review of the scholarly literature indicates a substantial degree of indifference to the causes of this civil justice system failure. Many of the published articles on asbestos litigation focus on transactional costs and ways in which the flow of money from defendants to plaintiffs and their lawyers can be expeditiously and efficiently prioritized and routed. The failure to acknowledge, let alone analyze, the overriding reality of specious claiming and meritless claims demonstrates a disconnect between the scholarship and the reality of the litigation that is nearly as wide as the disconnect between rates of disease claiming and actual disease manifestation\".\n\n\"Many of those who I see in my surgeries have worked in a number of workplaces and they could have been exposed to asbestos in each of them, but medical science is such that no one can identify which of them it is. As a result, there has been a long and complex history of legal discussion on how to apportion liability. The lawyers and the judiciary have wrestled, rightly and valiantly, with complex and difficult law, but it has created despair for the families whom we represent. Many of my constituents’ families have been riven by the consequences of litigation in trying to get some compensation for a disease that has been contracted through no fault of theirs. That is cruel and unacceptable.\"\n\nWorldwide, 60 countries (including those in the European Union) have banned the use of asbestos, in whole or in part. Some examples follow.\n\nAsbestos is listed as a category of controlled waste under Annex I of the Basel Convention on the Control of Transboundary Movements of Hazardous Wastes and their Disposal [1992]. Specifically, any waste streams having asbestos (dust and fibres) as constituents are controlled (Item Y36). In general terms, Parties to the Convention are required to prohibit and not permit the export of hazardous wastes to the Parties which have prohibited the import of such wastes via the notification procedure in Article 13 of the Convention.\n\nA nationwide ban on importing and using all forms asbestos took effect on 31 December 2003. Reflecting the ban, the National Occupational Health and Safety Commission (NOHSC) revised asbestos-related material to promote a consistent approach to controlling exposure to workplace asbestos and to introduce best-practice health and safety measures for asbestos management, control and removal. The ban does not cover asbestos materials or products already in use at the time the ban was implemented.\n\nAlthough Australia has only a third of the UK's population, its asbestos disease fatalities approximate Britain's of more than 3,000 people per year.\n\nWestern Australia' center of blue asbestos mining was Wittenoom. The mine was run by CSR Limited (a company that had been the Colonial Sugar Refinery). The main manufacturer of asbestos products was James Hardie, which set up a minor fund for its workers, then transferred operations to the Netherlands where it would be out of reach of the workers when the fund expired.\n\nThe São Paulo State law 12.684/07 prohibits the use of any product which utilizes asbestos. This legislation has been formally upheld by the Brazilian Supremo Tribunal Federal.\n\nSince the 1980s, Canada has not permitted crocidolite asbestos to be used and has had limitations on certain uses of other types of asbestos, notably in some construction materials and textiles.\n\nIn late 2011, Canada's remaining two asbestos mines, both located in the Province of Quebec, halted operations. The following year, Quebec's government announced a halt to asbestos mining and the federal government announced that it would end its opposition to adding chrysotile asbestos to the list of hazardous substances under the international Rotterdam Convention. \n\nIn 2018, the Canadian federal government posted proposed regulations planned for implementation later that year, which would prohibit use, sale, import, and export of all forms of asbestos.\n\nFrance banned the use of asbestos in 1997, and the WTO upheld France's right to the ban in 2000. In addition, France has called for a worldwide ban.\n\nThe import, shipment, supply of, and use of all forms of asbestos is banned in Hong Kong under the Air Pollution Control (Amendment) Ordinance 2014.\n\nBefore the 1980s, use of the material was common in construction, manufacturing, and shipping. The government banned the use of \"most asbestos products\" in public areas in 1978. The Factories and Industrial Undertakings (Asbestos) Special Regulation (Cap. 59X), which came into force in 1986, and the subsequent Factories and Industrial Undertakings (Asbestos) Regulation (Cap. 59AD) implemented controls on the use of asbestos in the workplace and banned the use of amphibole asbestos completely.\n\nThe import of amosite or crocidolite to Hong Kong was banned in 1996 by the Air Pollution Control Ordinance (Cap. 311). On 4 April 2014, the Air Pollution Control (Amendment) Ordinance 2014 came into force, completely banning the \"import, transhipment, supply and use of all forms of asbestos and asbestos containing materials\" in Hong Kong.\n\nThe Vision Statement of the Environment and Human Health of The Government of India clearly states \"\"Alternatives to asbestos may be used to the extent possible and use of asbestos may be phased out.\"\n\nIn Case No.693/30/97-98, National Human Rights Commission (NHRC) has clearly directed to “\"Replace all asbestos sheets roofing with roofing made up of some other material that would not be harmful to inmates\".”\n\nThe Secretary of the Post Graduate Institute of Medical Education & Research (PGIMER, Chandigarh also voiced its opinion “\"Asbestos is definitely a harmful material, it causes cancer and other related diseases\".” In their complete statement and recommendation to NHRC they have clearly expressed their concern: “\"White Asbestos (Chrysotile Asbestos) is implicated in so many studies with the following diseases:-Mesothelioma (Cancer of Pleura), Lung Cancer, Peritoneal Cancer, Asbestosis, and is also considered as a cause for Ovarian Cancer, Laryngeal Cancer, Other Cancererous Diseases are produced in the person involved in Asbestos Industry\".” It concludes its position by specifying, “\"Hence, Use of White Asbestos should be completely banned in India and the same may be replaced by some safer alternative material\".”\n\nThe Joint Secretary, Government of Uttarakhand in Case No.2951/30/0/2011, submitted to the NHRC that “\"There is no cure for Asbestos Diseases. Stopping all exposure to Asbestos is only essential.\"”\n\nThe Union Ministry of Labour’s concept paper declared, \"The Government of India is considering the ban on use of chrysotile asbestos in India to protect the workers and the general population against primary and secondary exposure to Chrysotile form of Asbestos.\" The Concept paper further notes, \"Asbestosis is yet another occupational disease of the Lungs which is on an increase under similar circumstances warranting concentrated efforts of all stake holders to evolve strategies to curb this menace\"\".\n\nThe Indian Factory Act and Bureau of Indian Standard already have rules and regulations for safe usage of asbestos contaminated products such as:\n\nHowever, there is no enforcement of the rules at ground level, hence asbestos usage is prevalent without following even the simplest basic safety rules.\n\nThe Centre for Pollution Control Board (CPCB) struggles to enforce their own guidelines for Asbestos as Hazardous Waste and relies on Industries and Companies to volunteer themselves to follow Safety Regulations. \nThere continues to be a high usage of friable or dust based asbestos in Compressed Asbestos Fibre (CAF) Gaskets, Ropes, Cloth, Gland Packings, Millboards, Insulation and Brake Liners in Factories and Industries within India as well as inadvertently exported by Equipment Manufacturers without adequate knowledge and information to the other countries.\n\nOn 21 January 2011, the Supreme Court of India banned the usage of asbestos in India.\n\nIn spite of the health hazards, asbestos is widely used in India without any restriction. Activists in India and abroad have tried to persuade the governments of Canada and Quebec to stop asbestos mining in Quebec and Exporting to India. The Canadian government has repeatedly blocked asbestos being listed as a hazardous chemical by the UN even though it spends massive amounts of money to remove it from Canadian homes and offices. While India recognizes it as a hazardous substance and has banned asbestos mining and its waste trade, it keeps its price low through patronage.\n\nOn 15 August 2016, in the strongest statement till date, the Hon Mr. Anil Madhav Dave, Union Minister of Environment, Forest & Climate Change categorically specified “\"Since the use of asbestos is affecting human health, its usage should gradually be minimised and ended. As far as I know, its use is declining, but it must end\"” \n\nThe Ban Asbestos Network of India (BANI) has been working towards an Asbestos Free India to safeguard the health of the present and future generations amidst the misinformation campaign of the White Chrysotile Industry.\n\nOn 5th May 2017, India opposed listing of asbestos under the Prior Informed Consent (PIC) list of hazardous substances during the 2017 United Nations Rotterdam Convention. becoming the second Democracy (After Russian Federation) to have such a stand in the world. Besides India, Russia, Kazakhstan, Kyrgyzstan, Syria, and Zimbabwe also opposed the listing. While, all the other countries opposing are producers of asbestos; India (\"where mining is banned\") is the sole (\"and largest\") consumer and importer of asbestos still opposing its inclusion on the PIC List.\n\nThe National Asbestos Profile of India made in cooperation by Peoples Training and Research Centre, Vadodara, Occupational & Environmental Health Network of India, New Delhi and Asia Monitor Resource Centre, Hong Kong is the first such attempt and resource for identifying total Asbestos usage in India.\n. This extensive profile documents the Manufacturers, Consumers, Health and Disease statistics of Asbestos usage in India till date. An extensive list of companies manufacturing or distributing Cancerous Asbestos Fiber (CAF) Containing Products is also highlighted as part of this Profile.\n\nItaly fully banned the use of asbestos in 1992 with law 257/92 art. 1 and set up a comprehensive plan for asbestos decontamination in industry and housing.\n\nJapan did not fully ban asbestos until 2004. Its government has been held responsible for related diseases.\n\nAsbestos was banned in South Africa in 2008. Prior to this, the country was one of the global leaders in asbestos production, and consequently had one of the highest rates of mesothelioma. Regulations to ban asbestos in South Africa occurred in March 2008 under the leadership of Environmental Affairs and Tourism Minister Marthinus van Schalkwyk. The first draft regulations were announced in November 2005 for public comment and again in September 2007. The regulations prohibited the use, processing, manufacturing, and import and export of any asbestos or asbestos-containing materials (ACMs). A grace period of 120 days was allowed to give people or traders currently dealing in asbestos or ACMs to clear their stock. Exemptions would be granted under strict control. The regulations did not prohibit the continued use of asbestos-containing materials that were already in place such as asbestos-cement roof sheets or ceilings, the department was satisfied that there was “no undue risk” and they would be replaced in due time. Penalties for the continued use of asbestos include a fine which would not exceed R100 000 and/or imprisonment of less than ten years. Prior to the regulations' implementation, asbestos had been in the process of being phased out from as early as 2003. Requests from Zimbabwe and Canada to be excepted from the prohibition were denied. South also terminated all import of asbestos or ACMs from Zimbabwe. South Africa would allow products to pass through its borders while in transit under strict conditions and if registered with the department of Environmental Affairs and Tourism. Everite, a building company, supported the government’s ban on imports from Zimbabwe It became an offence to acquire, process, package or repackage, manufacture or distribute these products from after July 28, 2008. Kgalagadi Relief Trust (KRT) chair Brian Gibson stated that asbestos may still be imported into South Africa for research or analysis. Asbestos may also be imported into the country for disposal from Southern African Development Community (SADC) countries that were unable to dispose of the waste themselves.\n\nIn May 1997, the manufacture and use of crocidolite and amosite, commonly known as blue and brown asbestos, were fully banned in South Korea. In January 2009, a full-fledged ban on all types of asbestos occurred when the government banned the manufacture, import, sale, storage, transport or use of asbestos or any substance containing more than 0.1% of asbestos. In 2011, South Korea became the world's sixth country to enact an asbestos harm aid act, which entitles any Korean citizen to free lifetime medical care as well as monthly income from the government if he or she is diagnosed with an asbestos-related disease.\n\nIn 1984, the import of raw amphibole (blue and brown) asbestos into New Zealand was banned. In 2002, the import of chrysotile (white) asbestos was banned.\n\nThe British Government's Health and Safety Executive (HSE) has promoted rigorous controls on asbestos handling, based on reports linking exposure to asbestos dust or fibres with thousands of annual deaths from mesothelioma and asbestos-related lung cancer.\n\nThe HSE does not assume that any minimum threshold exists for exposure to asbestos below which a person is at zero risk of developing mesothelioma, since they consider that it cannot currently be quantified for practical purposes; they cite evidence from epidemiological studies of asbestos exposed groups to argue that even if any such threshold for mesothelioma does exist, it must be at a very low level.\n\nPreviously it was possible to claim compensation for pleural plaques caused by negligent exposure to asbestos, on the grounds that although it is in itself asymptomatic, it is linked to development of diffuse pleural thickening, which causes lung impairment. It has been highly contentious, however, as to the probability of pleural plaques developing into pleural thickening or other asbestos-related illnesses. On October 17, 2007 this point was clarified by the Law Lords' ruling that workers who have pleural plaques as a result of asbestos exposure will no longer be able to seek compensation as it does not in itself constitute a disease. This ruling was, however, superseded, so far as sufferers of pleural plaques in Scotland are concerned, by the passing of the Damages (Asbestos-related Conditions)(Scotland) Act 2009, which provides that in Scots law pleural plaques are to be considered an actionable type of personal injury.\n\nThe Control of Asbestos Regulations were introduced in the UK in November 2006 and are an amalgamation of three previous sets of legislation (Asbestos Prohibition, Asbestos Licensing and the Control of Asbestos at Work Regulations) aimed at minimising the use and disturbance of asbestos containing materials within British workplaces. Essentially this legislation bans the import and use of most asbestos products and sets out guidelines on how best to manage those currently in situ.\n\nDutyholders of all non-domestic properties within the UK must establish an asbestos register and a management plan. The definition of \"non-domestic\" is \"a property or structure (commercial, domestic or residential) where work is carried out\" the obligation of the dutyholder is that such operatives are not exposed to any asbestos-based materials during the course of the work, the Asbestos Register states the presence or non-presence of asbestos related to the inside and outside of the structure. The exception is where the property age (post-1999 when chrysotile asbestos was banned) would indicate that such products will not have been used during the construction of the building.\n\nThe removal of high-risk asbestos products from non-domestic properties is tightly controlled by the HSE and high-risk products such as thermal insulation must be removed under controlled conditions by licensed contractors. Further guidance on which products this applies to can be found on the HSE website along with a list of licensees.\n\nThe Control of Asbestos Regulations were amended and came into force on 6 April 2012 to take account of the European Commission's view that the UK had not fully implemented the EU Directive on exposure to asbestos (Directive 2009/148/EC). These changes were relatively minor and included additional requirements for non-licensed asbestos work. These changes mean that some non-licensed asbestos work now requires notification, and has additional requirements for managing this work (e.g. record keeping and health surveillance).\n\nAccording to the Environmental Working Group Action Fund, 10,000 people die each year from asbestos-caused diseases in the United States, including one out of every 125 American men who die over the age of 50. The Environmental Protection Agency (EPA) has no general ban on the use of asbestos. However, asbestos was one of the first hazardous air pollutants regulated under Section 112 of the Clean Air Act of 1970, and many applications have been forbidden by the Toxic Substances Control Act (TSCA). The United States has extensive laws regulating the use of asbestos at the federal, state, and local level.\n\nOne of the major issues relating to asbestos in civil procedure is the latency of asbestos-related diseases. Most countries have limitation periods to bar actions that are taken long after the cause of action has lapsed. For example, in Malaysia the time period to file a tort action is six years from the time the tort occurred. Due to several asbestos-related actions, countries such as Australia have amended their laws relating to limitations to accumulate starting from time of discovery rather than time when the cause of action accrued.\n\nThe first employee claims for injury from exposure to asbestos in the workplace were made in 1927, and the first lawsuit against an asbestos manufacturer was filed in 1929. Since then, many lawsuits have been filed. As a result of the litigation, manufacturers sold off subsidiaries, diversified, produced asbestos substitutes, and started asbestos removal businesses.\n\nGuardian Unlimited reported a test-case ruling in 2005, that allowed thousands of workers to be compensated for pleural plaques. Diffuse or localised fibrosis of the pleura, or pleural plaques, is less serious than asbestosis or mesothelioma, but is also considered a disease closely linked to the inhalation of asbestos. However, insurers claimed the plaques are \"simply a marker for asbestos exposure rather than an injury.\" Mr Justice Holland rejected the insurers' arguments, and counsel for workers hailed the decision as a \"victory that puts people before profits.\" However this decision was reversed by the Court of Appeal. On 17 October 2007, the House of Lords confirmed the Court of Appeal's decision. Pleural plaques no longer constitute actionable injury in England, Wales and Northern Ireland. The Scottish government introduced legislation in 2009 to preserve the status of pleural plaques as an actionable injury in Scotland and there are proposals to introduce similar legislation in Northern Ireland.\n\nInsurance companies allege that asbestos litigation has taken too heavy a toll on insurance and industry. A 2002 article in the British \"Daily Telegraph\"'s associate quoted Equitas, the reinsurance vehicle which assumed Lloyd's of London's liabilities, which argued that asbestos claims were the \"greatest single threat\" to Lloyd's of London's existence. Of note is that Lloyd's of London had been sued for fraud by its investors, who claimed Lloyd's misrepresented pending losses from asbestos claims.\n\nIn May 2006, the House of Lords ruled that compensation for asbestos injuries should be reduced where responsibility could not be attached to a single employer. Critics, including trade unions, asbestos groups and Jim Wallace, former justice minister, have condemned the ruling. They said it overturned the traditional Scottish law to such cases, and was a breach of natural justice. As a result of this outcry, the ruling has been overturned by section three of the Compensation Act 2006.\n\nIn February 2010 a court ruling set a new precedent for asbestosis claims. The case, in which widow Della Sabin attempted to claim compensation following her husband's death from asbestosis, hinged on the issue of how many asbestos fibres must be present in the lungs for a claim to be valid. A research team based at Llandough Hospital initially reported that the minimum amount of fibres that needed to be present for a claim to be valid was 20 million (only 7 million were found in the sample taken from Mrs Sabin's husband Leslie). However, a subsequent US study suggested that, due to the fact that Leslie had lived for more than forty years after his exposure, a large number of fibres would have cleared from his body naturally; had he died twenty years earlier the asbestos count in his lungs would have been about 35 million fibres per gram. The judge preferred this evidence, and ruled in favour of Mrs Sabin.\n\nIn June 2008, the Brazilian Supremo Tribunal Federal (STF), voted to maintain the law (12.684/07) which prohibits the use of any product which utilizes asbestos in São Paulo State. It is expected that the decision will be extended to include the whole country.\n\nLubbe v Cape plc -The Lubbe v Cape Plc case [2000] UKHL 41 is a conflict of laws case, which is also highly significant for the question of lifting the corporate veil in relation to tort victims.\n\nThe Richard Meeran-run Cape Plc case was settled in 2003\n\nThe Richard Spoor-run Gencor case was settled in 2003\n\n400 Swaziland ARD victims from the Havelock \nThe Havelock chrysotile mine cases were suspended in 2003 because Turner and Newall, the company that owned the mine, had filed for bankruptcy in 2001. \nSwiss Eternit Group\nThis was a voluntary agreement which was reached in 2006. The agreement enabled ex-miners of the Kuruman and Danielskuil Cape Blue Asbestos (KCBA and DCBA) mines in the Northern Cape province to apply under similar conditions as the open settlement of the ART. The Kgalagadi Relief Trust (KRT) was\nthus created. The terms of the trust are not clarified however in practice R136 million was paid over for compensation purposes, for payouts until 2026. The trustees of the KRT requested the ART to administer the KRT settlement as the two trusts were very similarly structured.\n\nSeventy-five percent of the claimants in the Cape Plc case came from Limpopo province and the remaining twenty-five percent from the Prieska Koegas area in the Northern Cape province. The majority of the claimants in the ART settlement (around 78%) were exposed in the Kuruman area in the Northern Cape province, with the balance exposed at Penge in Limpopo province and Msauli in Mpumalanga province, which were equally proportioned.\n\nStatistically, mesothelioma and asbestos related lung cancer sufferers receiving the highest payments of R71 500 each. \nDue to Gencor's significant contribution settlements, it was prohibited for those who had received compensation under the Cape agreement to later be paid by the ART, even if the worker had worked on the\nKuruman or Penge mines when under Gencor control.\n\nIn 2006, Cape plc started a trust to compensate those who have suffered from asbestos related diseases as a result of Cape’s historical activities. To date, this Trust has paid out over £30m to those who have become sick or to their dependents. The Scheme of Arrangement was approved by the High Court and is separately funded. Its funds being administered by two independent trustees.\n\nAsbestos Relief Trust (ART) is regarded as a model of efficient occupational disease compensation in South Africa. Gencor was a major contributor to the Richard Meeran-run Cape Plc case and the Richard Spoor-run Gencor case. Glencor provided 29% of the R138 million that went to the Cape Plc’s set of claimants, and 96% of the R381 million that formed the ART. An additional sum of R35 million went to environmental rehabilitation, and about R20 million was added to the ART to contribute to supplementary and additional payments. After some time and publicity claims against The Cape Plc list had grown from 2 000 in January 1999 and to 7 500 in August 2001. The ART settlement was open, and made provision for compensation to any person who met the compensation criteria set out in the Trust deed, until the year 2028. Many companies agreed to compensate the workers which were exposed to asbestos in addition to the compensation payable under the Occupational Diseases in Mines and Works Act (ODMWA) The settlement included environmentally expose victims of ARDS. This settlement model was achieved by personal communication, Georgina Jephson, attorney at Richard Spoor Inc. Attorneys.\n\nThe Trust provides compensation for people in these four categories related to Acute respiratory distress syndrome (ARD) namely:\n\n\nA model by the ART estimated that about 16 800 individuals would submit claims to the Trust, of which approximately 5 036 (30%) would be successful. This was subsequently revised to 5 162. Of these, 219 (4.2%) would be environmental claimants, 150 (2.9%) would have lung cancer and 556 (10.8%) would have mesothelioma; the balance would have asbestosis and/or pleural thickening. No definitive figures were provided for the expected ARD1/\nARD2 ratio. The\namounts payable for compensation vary, but the average amount of compensation since 2003 has been about R40 000, R80 000, R170 000 and R350 000 for each of the categories ARD 1-4 described above. These amounts are paid over and above any compensation that the claimants might receive under the ODMWA. In order for a case to be compensable, a victim needs to show that he/she was both exposed to asbestos from one of the operations run by the funders of the ART, and has a compensable disease.\n\nA lack of facilities for terminally ill mesothelioma sufferers results in a larger burden of service, according to Sister Phemelo Magabanyane, a palliative care nurse who has cared for over 100 mesothelioma and lung cancer sufferers in the greater Kuruman district in the Northern Cape. Mesothelioma is a life-threatening cancer of the pleura or peritoneum which can be diagnosed up to 40 years after exposure to asbestos.\nSouth Africa has the highest prevalence of mesothelioma in the world. Richard Spoor, a lawyer who represented the claimants against Gencor says,:“The environmental scale of the disaster we are seeing unfold in the Northern Cape is on a level with the nuclear disaster at Chernobyl, in terms of impact, spread and longevity,” he also says that children are particularly vulnerable to mesothelioma. Since September 2016, five of the 1 600 claimants he represents in the Northern Cape have died. Internationally renowned photographer David Goldblatt started photographing victims after a friend died of mesothelioma despite never being in close proximity to a mine. It has been reported that she picked up the disease from rubbing a blue asbestos rock ornament that she kept in her home.\n\nAsbestos litigation is the longest, most expensive mass tort in U.S. history, involving more than 8,000 defendants and 700,000 claimants. Current trends indicate that the rate at which people are diagnosed with the disease will likely increase through the next decade. Analysts have estimated that the total costs of asbestos litigation in the USA alone will eventually reach $200 to $275 billion. The amounts and method of allocating compensation have been the source of many court cases, and government attempts at resolution of existing and future cases.\n\nIn June 1982, a retired boilermaker, James Cavett, won a record award of $2.3 million compensatory and $1.5 million in punitive damages against Johns-Manville. The Manville Corporation, formerly the Johns-Manville Corporation, filed for reorganization and protection under the United States Bankruptcy Code in August 1982. At the time, it was the largest company ever to file bankruptcy, and was one of the richest. Manville was then 181st on the Fortune 500, but was the defendant of 16,500 lawsuits related to the health effects of asbestos. The company was described by Ron Motley, a South Carolina attorney, as \"the greatest corporate mass murderer in history.\" Court documents show that the corporation had a long history of hiding evidence of the ill effects of asbestos from its workers and the public. One of many examples is a memo from Manville's medical director to corporate headquarters:\n\nBy the early 1990s, \"more than half of the 25 largest asbestos manufacturers in the US, including Amatex, Carey-Canada, Celotex, Eagle-Picher, Forty-Eight Insulations, Manville Corporation, National Gypsum, Standard Insulation, Unarco, and UNR Industries had declared bankruptcy. Filing for bankruptcy protects a company from its creditors.\"\n\nAsbestos-related cases increased significantly on the U.S. Supreme Court docket after 1980. The court has dealt with several asbestos-related cases since 1986. Two large class action settlements, designed to limit liability, came before the court in 1997 and 1999. Both settlements were ultimately rejected by the court because they would exclude future claimants, or those who later developed asbestos-related illnesses. These rulings addressed the 20-50 year latency period of serious asbestos-related illnesses.\n\nIn 1988, the United States Environmental Protection Agency (USEPA) issued regulations requiring certain U.S. companies to report the asbestos used in their products.\n\nSeveral legislative remedies have been considered by the U.S. Congress but each time rejected for a variety of reasons. In 2005, Congress considered but did not pass legislation entitled the \"Fairness in Asbestos Injury Resolution Act of 2005\". The act would have established a $140 billion trust fund in lieu of litigation, but as it would have proactively taken funds held in reserve by bankruptcy trusts, manufacturers and insurance companies, it was not widely supported either by victims or corporations.\n\nOn April 26, 2005, Dr. Philip J. Landrigan, professor and chair of the Department of Community and Preventive Medicine at Mount Sinai Medical Center in New York City, testified before the US Senate Committee on the Judiciary against this proposed legislation. He testified that many of the bill's provisions were unsupported by medicine and would unfairly exclude a large number of people who had become ill or died from asbestos: \"The approach to the diagnosis of disease caused by asbestos that is set forth in this bill is not consistent with the diagnostic criteria established by the American Thoracic Society. If the bill is to deliver on its promise of fairness, these criteria will need to be revised.\" Also opposing the bill were the American Public Health Association and the Asbestos Workers' Union.\n\nOn June 14, 2006, the Senate Judiciary Committee approved an amendment to the act which would have allowed victims of mesothelioma $1.1M within 30 days of their claim's approval. This version would have also expanded eligible claimants to people exposed to asbestos from the September 11, 2001 attacks on the World Trade Center, and to construction debris in hurricanes Katrina and Rita. Ultimately, the bill's reliance on funding from private entities large and small, as well as debate over a sunset provision and the impact on the U.S. budgetary process caused the bill to fail to leave committee.\n\nSince the bankruptcy filing of Johns-Manville in 1984, many U.S. and U.K. asbestos manufacturers have escaped litigation by filing bankruptcy. Once in bankruptcy, these companies typically are required to fund special \"bankruptcy trusts\" that pay pennies on the dollar to injured parties. However, these trusts do permit larger numbers of claimants to receive some kind of compensation, even if greatly reduced from potential recoveries in the tort system.\n\nSince 2002, asbestos lawsuits in the U.S. have included the following as defendants: (1) manufacturers of machinery that are alleged to have required asbestos-containing parts to function properly; (2) owners of premises at which asbestos-containing products were installed (which includes virtually anyone who owned a building prior to 1980); (3) banks that financed ships or buildings where asbestos was installed (on the grounds that no rational lender would take a security interest in an asset without studying the risks involved); (4) retailers of asbestos-containing products (including hardware, home improvement and automotive parts stores); (5) corporations that allegedly conspired with asbestos manufacturers to deliberately conceal the dangers of asbestos (e.g., MetLife, a well-known insurance company which worked with Johns-Manville); (6) manufacturers of tools which were used to cut or shape asbestos-containing parts; and (7) manufacturers of respiratory protective equipment.\n\nDefendants in the first category have contested liability on the grounds that nearly all of them either did not ship asbestos-containing parts with their products at all (that is, asbestos was installed only by end users) or did not sell replacement parts for their own products (in cases where the plaintiff was allegedly exposed well after any factory-original asbestos-containing parts would have been replaced), and either way cannot be responsible for toxic third-party parts that they did not manufacture, distribute, or sell. In 2008, the Washington Supreme Court, the first to reach the issue, decided in favor of the defense. On January 12, 2012, the Supreme Court of California also decided in favor of the defense in \"O'Neil v. Crane Co.\" This is significant as a 2007 study found that California and Washington were the two most influential state supreme courts in the United States in the period from 1940 to 2005.\n\nIn a decision from January 2014, Gray v. Garlock Sealing Technologies had entered into bankruptcy proceedings, and discovery in the case uncovered evidence of fraud that led to a reduction in estimated future liability to a tenth of what was estimated.\n\nAnother area of dispute remains the so-called chrysotile-defense. Manufacturers of some products containing only chrysotile fibers claim that these are not as harmful as amphibole-containing products. As 95% of the products used in the United States historically were mostly chrysotile, this claim is widely disputed by health officials and medical professionals. The World Health Organization recognizes that exposure to all types of asbestos fibers, including chrysotile, can cause cancer of the lung, larynx, and ovary, mesothelioma, and asbestosis.\n\nOn February 20, 1973 a federal grand jury in Detroit, Michigan indicted Adamo Wrecking Company (\"Adamo\") for violating provisions of the Clean Air Act by knowingly causing the emission of asbestos by failure to wet and remove friable asbestos materials from demolitions.\n\nAdamo was one of a number of demolition contractors indicted throughout the country for the alleged violation of the Clean Air Act. The United States District Court for the Eastern District of Michigan dismissed the criminal indictment on the ground that it was not an \"emission standard,\" but a \"work practice standard,\" which under the terms of the statute, did not carry criminal liability.\n\nThe government appealed and the Sixth Circuit Court of Appeals reversed the decision of the trial court, stating that it erred in determining that it had jurisdiction to review the validity of the standard in a criminal proceeding. Adamo's attorneys appealed to the Supreme Court.\n\nOn January 10, 1978, the Supreme Court ruled in favor of Adamo when it held that the trial court did have jurisdiction to review the standard in a criminal proceeding and also agreed with the trial court that the requirements in the act were \"not standards\" but \"procedures\" and therefore the proceedings were properly dismissed.\n\nA federal grand jury indicted W. R. Grace and Company and seven top executives on February 5, 2005, for its operations of a vermiculite mine in Libby, Montana. The indictment accused Grace of wire fraud, knowing endangerment of residents by concealing air monitoring results, obstruction of justice by interfering with an Environmental Protection Agency (EPA) investigation, violation of the Clean Air Act, providing asbestos materials to schools and local residents, and conspiracy to release asbestos and cover up health problems from asbestos contamination. The Department of Justice said 1,200 residents had developed asbestos-related diseases and some had died, and there could be many more injuries and deaths.\n\nW. R. Grace and Company faced fines of up to $280 million for polluting the town of Libby, Montana. Libby was declared a Superfund disaster area in 2002, and the EPA has spent $54 million in cleanup. Grace was ordered by a court to reimburse the EPA for cleanup costs, but the bankruptcy court must approve any payments.\n\nOn June 8, 2006, a federal judge dismissed the conspiracy charge of \"knowing endangerment\" because some of the defendant officials had left the company before the five-year statute of limitations had begun to run. The wire fraud charge was dropped by prosecutors in March.\n\nAsbestos abatement (removal of asbestos) has become a thriving industry in the United States. Strict removal and disposal laws have been enacted to protect the public from airborne asbestos. The Clean Air Act requires that asbestos be wetted during removal and strictly contained, and that workers wear safety gear and masks. The federal government has prosecuted dozens of violations of the act and violations of Racketeer Influenced and Corrupt Organizations Act (RICO) related to the operations. Often these involve contractors who hire undocumented workers without proper training or protection to illegally remove asbestos.\n\nOn April 2, 1998, three men were indicted in a conspiracy to use homeless men for illegal asbestos removal from an aging Wisconsin manufacturing plant. Then-US Attorney General Janet Reno said, \"Knowingly removing asbestos improperly is criminal. Exploiting the homeless to do this work is cruel.\"\n\nOn December 12, 2004, owners of New York asbestos abatement companies were sentenced to the longest federal jail sentences for environmental crimes in U.S. history, after they were convicted on 18 counts of conspiracy to violate the Clean Air Act and the Toxic Substances Control Act, and actual violations of the Clean Air Act and Racketeer-Influenced and Corrupt Organizations Act. The crimes involved a 10-year scheme to illegally remove asbestos. The RICO counts included obstruction of justice, money laundering, mail fraud and bid rigging, all related to the asbestos cleanup.\n\nOn January 11, 2006, San Diego Gas & Electric Co., two of its employees, and a contractor were indicted by a federal grand jury on charges that they violated safety standards while removing asbestos from pipes in Lemon Grove, California. The defendants were charged with five counts of conspiracy, violating asbestos work practice standards and making false statements.\n\n\n\n"}
{"id": "3085232", "url": "https://en.wikipedia.org/wiki?curid=3085232", "title": "Baxter Althane disaster", "text": "Baxter Althane disaster\n\nThe Baxter Althane disaster in autumn 2001 was a series of 53 sudden deaths of renal failure patients in Spain, Croatia, Italy, Germany, Taiwan, Colombia and the USA (mainly Nebraska and Texas). All had received hospital treatment with Althane hemodialysis equipment, a product range manufactured by Baxter International, USA.\n\nAlthough official investigations initially found no link between the cases, Baxter Co. eventually published its own findings, admitting that a perfluorohydrocarbon-based cleaning fluid was not properly removed from the tubings during manufacture. Baxter also announced discontinuation and permanent recall of all Althane equipment. Families of most non-US victims were compensated by Baxter voluntarily, while US plaintiffs settled via a class action lawsuit. The company continues to manufacture dialysis machines of a newer design.\n\n"}
{"id": "10193102", "url": "https://en.wikipedia.org/wiki?curid=10193102", "title": "Certified safety professional", "text": "Certified safety professional\n\nThe Certified Safety Professional (CSP) is a certification offered by the Board of Certified Safety Professionals (BCSP). The CSP is accredited in the United States by the National Commission for Certifying Agencies and internationally by the International Organization for Standardization/International Electrotechnical Commission (ISO/IEC 17024) (see ANSI) and 193 Countries Consortium .\n\nThe requirements to become a CSP are:\n\nCSPs are further required to provide BCSP with proof that they are maintaining a high level of competency in safety work by recertifying every five years.\n\nSimilar to the CSP in the US, Canada offers the CRSP (Canadian Registered Safety Professional) designation, through the Board of Canadian Registered Safety Professionals\n\nApplicants must meet some prerequisites prior to submitting a formal application:\n\n\n1. Application: Once the applicant has met the prerequisites, they must complete the \"Application for Canadian Registered Safety Professional Designation\", obtained from the BCRSP board. The lengthy application requires a compilation of all the applicant’s course and job descriptions.\n\n2. Evaluation: The BCRSP Board reviews the application. The applicant is then advised if they meet the minimum requirements or not. If the application is accepted, then the applicant is advised that they will soon be contacted for an interview.\n\n3. Interview: The interview is set up with a member of the BCRSP’s Regional Screening Centre personnel in the applicant’s geographical area. It’s an informal meeting, where the applicant’s work and educational Occupational Health & Safety experience are discussed.\n\n4. Examination: The applicant has to write a comprehensive 3.5 hour multiple choice exam that covers Accident Theory, Environmental Practices, Ergonomics, Fire Prevention and Protection, Health Promotion, HSE Auditing, Law and Ethics, Occupational Health Safety and Environment Systems, Occupational Hygiene, Risk Management, and Safety Techniques and Technology. Study guides and books are recommended by the Board, and numerous preparatory courses are available through:\n\n4. Approval: If the applicant minimally achieves the passing score on the examination, they receive notification of such.\n\n5. Confirmation: The applicant revives the CRSP Designation along with their designation registration number.\n\nIn Order to maintain a professional safety designation continuing education is often required with most designations. To be in good standing with the certification body, continuing education units (CEU) or professional developmental conferences (PDC) must be completed within a designated time frame and approved by the certification body. Some provincial trade associations and safety associations have their industry designations such as:\n\nInternationally, other countries have set up similar programs. In the UK the highest professional standing is that of a chartered safety and health practitioner or fellow of IIRSM. The standards are maintained by both the UK’s largest body for safety professionals Institution of Occupational Safety and Health (IOSH) and also the International Institute of Risk and Safety Management (IIRSM). Like North American safety professional programs, to achieve these grades the applicant must be professionally qualified and have relevant experience. Continuing professional development (CPD) is also a strong requirement of both memberships. IIRSM also offer Recognised Safety Professional (not to be confused with 'registered safety practitioners' of OSHCR) these honorary post nominal letters given by IIRSM to recognised safety practitioners.\n\n\n"}
{"id": "51491107", "url": "https://en.wikipedia.org/wiki?curid=51491107", "title": "Circle of Health International", "text": "Circle of Health International\n\nCircle of Health International, known as COHI for short, is a US based non-governmental organization founded in 2004 with the mission to work with women and their communities with a community based approach in times of crisis. As of 2016, COHI has responded to eighteen humanitarian emergencies and served over three million women globally. COHI has worked with midwives and public health professionals in Sri Lanka, Louisiana, Tibet, Tanzania, Israel, the Philippines, Palestine, Jordan, Syria, Oklahoma, Nicaragua, Sudan, Haiti, and Afghanistan.\n\nAs of 2016 COHI supports maternal and child health clinics in Haiti, midwives in an indigenous women's forum in Nicaragua, midwifery students and sexual health advocates in Nepal, a clinic for refugees in the Rio Grande Valley on the Mexico/US border, and works with survivors of human trafficking globally. COHI is also engaged in Austin's social enterprise community through a program, known as the COHI Cloth Network, to address women's poverty through income generation initiatives.\n\nIn 2004, COHI partnered with a local host organization, Tibetan Healing Fund, in Tongren, also known as Repkong, Eastern Tibet to aid with the training of midwives in order to create a more sustainable maternal health care system.\n\nIn 2004, COHI and their partners worked with Israeli and Palestinian woman to address midwifery and gender based violence (GBV). Conducted an assessment based on three main categories\n\n\nThe assessment included in depth interviews and recommendations for each sector of Israel and the West Bank's diverse populations: religious and secular Jews, immigrant populations (Ethiopian, Russian, Congolese), the Bedouin, and the Palestinian populations of Israel and the West Bank, both Muslim and Christian\n\nThe results from the assessment were used to advocate for the needs of Israeli and Palestinian women such as better postpartum care.\n\n"}
{"id": "51753034", "url": "https://en.wikipedia.org/wiki?curid=51753034", "title": "Compensation scheme for radiation-linked diseases", "text": "Compensation scheme for radiation-linked diseases\n\nThe Compensation scheme for radiation-linked diseases is a workers compensation scheme administered by the UK government. It was established in November 1982 by British Nuclear Fuels Limited and its trade unions following legal actions brought against the company by nuclear industry workers in the late 1970s. At the time of its establishment, BNFL and its trade unions agreed that the causation of cancer by radiation was sufficiently well understood that \"it should be possible to construct a scheme which would evaluate the probability that a diagnosed cancer may have been caused by radiation exposure at work.\" Initially the scheme only accepted claims in which a worker had died from a radiation-linked disease. In 1987 this was expanded to allow morbidity claims. The list of participating member employers and trade unions has grown through the 1990s and 2000s. As of December 2015, 1525 claims have been made out of which 156 have been successful.\n\nIn order to be eligible for compensation, a worker must have been employed by a listed company, and have received an occupational radiation dose. Then the claimant must have developed cancer of the bladder, bone, brain and central nervous system, breast or uterus (for female workers), colon, liver, oesophagus, respiratory or lung, prostate, ovary, skin (non-Melanoma), thyroid or other tissues. Other compensable diseases include cataracts and leukaemias (with two exceptions). Some diseases are excluded on the basis that there is no convincing epidemiological evidence to link them with ionising radiation exposure. Excluded diseases include: Hodgkin’s disease, hairy cell leukaemia, chronic lymphatic leukaemia (CLL), malignant melanoma and mesothelioma.\n\nIn 1982, trade union members of the scheme included Transport and General Workers Union, Institution of Professional Civil Servants, Amalgamated Engineering Union, GMB and PCS. Other trades unions joined as the scheme expanded. Later members include the EETPU and MSF, UCATT, Engineers and Managers Association, Unison, the First Division Association of Civil Servants, the AEA Constabulary Federation (now the Civil Nuclear Police Federation) and the Defence Police Federation.\n\nClaims may be lodged by the worker, his or her wife, husband or partner (including same sex), the worker's \"first line children\" (those born to or legally adopted by the claimant). Once diagnosed with or deceased from an eligible disease, the worker or surviving family members have 30 years in which to make a claim. Claims can be made online, by post, by phone or through the worker's trade union. Once made, claims are assessed on consideration of medical, employment and dosimetry histories and data. These are used to calculate the probability that the cancer could have been caused by occupational exposure to radiation.\n\nThe payment received following a successful claim varies depending on the actual loss (in earnings and pension) suffered by the claimant and sums for pain and suffering, loss of amenity and number of dependents. The level of payment awarded to a claimant (or estate if deceased) is determined by the \"causation probability\". The greater the causation probability, the greater the payment. Top tier payments are awarded if the causation probability is 50% or more. The Scheme publishes an Annual statement which is approved by the Scheme Council. The report includes a paragraph updating the number of claims and payments awarded.\n\nBy 22 February 2005, 1000 claims had been made, of which 97 had been awarded to varying degrees along a sliding scale.\n\nAs of December 2015, 1525 cases had been considered since the compensation scheme for radiation-linked diseases was established. 156 of these claims were successful, and payments totaling £8.24 million had been dispatched.\n"}
{"id": "9422422", "url": "https://en.wikipedia.org/wiki?curid=9422422", "title": "Degreasing", "text": "Degreasing\n\nDegreasing, often called defatting or fat trimming, is the removal of fatty acids from an object. In culinary science, degreasing is done with the intention of reducing the fat content of a meal.\n\nDegreasing is often used by dieters, particularly those following low-fat diets to reduce their fat consumption to induce weight loss. The energy content of 1 g of fat is 9 calories, while that of carbohydrates and proteins are 4 calories. Hence, dieters often view decreasing fat consumption as an efficient way of losing weight without greatly sacrificing total volume of food. Degreasing during meal preparation is used to reduce the energy content of the food being prepared, and degreasing of a prepared food is also often used in social situations where one may have to consume restaurant food which is rich in fat.\n\nThose people who wish to reduce their cholesterol level or fat intake, in particular people with hypercholesterolemia often use degreasing to reduce their fat consumption.\n\nFat trimming of a meal can be done during preparation by a variety of methods. The most common methods involving substituting food items or removal of naturally occurring fat and conservative addition of fat.\n\nSubstituting fats is a method in which a certain ingredient is substituted by another ingredient. A common way of doing this is substituting saturated fatty acids with unsaturated fatty acids while cooking. For instance, olive oil can be used instead of butter for seasoning vegetables.\n\nFood items are also substituted to reduce fat content. For instance, instead of using eggs by using a whole egg, where the egg yolks are high in fat levels, egg whites can instead be used. Alternatively, skimmed milk or similar low-fat products can be used as ingredients for cooking.\n\nMany different foods can be degreased after preparation. Liquid foods that are high in fat, such as braising liquids, roasting juices and broths may have floating oil on top throughout and after the cooking process. Fat can be skimmed off the liquid with a small ladle, spoon, or cup as the liquid simmers and then discarded. This is done by placing the saucepan with only half of it on the heat source so that the liquid simmers only on one side. This pushes the fat to the opposite side and makes it easier to lift off with the ladle. The fat can then be skimmed off by holding the ladle so that the top of its bowl is almost level with the liquid and then tilting it slightly and in a circular motion toward the edge of the saucepan where the fat accumulates. \n\nAs braising liquids and broths simmer, they typically throw off more fat and protein and sometimes froth, so it is easier to wait at least a few minutes until the liquid has cooled before degreasing. In making slowly reduced liquids it may be ideal to wait for it to cool even longer. \n\nAs an alternative to a ladle, a degreasing cup can be used to skim the fat off hot liquids. This is a clear plastic or glass cup that looks similar to a teapot, with a spout that comes out of the bottom. The liquid and juices are poured into the cup then poured out leaving the fat behind; the liquid comes from the bottom of the cup instead of the top. Degreasing cups come in various sizes.\nRefrigerating or freezing liquids until the fat congeals and solidifies can make the fat easier to remove with a spoon. If, after removing the congealed fat, the liquid still appears cloudy, it can be made clearer by skimming it again with a ladle after bringing it to a gentle simmer to allow for the release of more fat and insoluble proteins.\n\nSome items, such as roasted foods, and other cooked food items, such as pizza, can sometimes be oily and greasy. Degreasing them after preparation may provide a solution to this problem. Fried foods, such as French fries, can often be degreased without decreasing the crispiness of the food item by simply blotting out the oil in the food item with a tissue paper.\n\nMany food manufacturing companies such as Weight Watchers have been developed for people who wish to reduce their fat intake. The customer need of defatting has also led to the production of low fat products, e.g., low fat cheese.\n\nSolvent degreasing is a process used to prepare a part for further operations such as electroplating or painting. Typically it uses petroleum, chlorine, or alcohol based solvents to dissolve the machining fluids and other contaminants that might be on the part.\n\n"}
{"id": "2267876", "url": "https://en.wikipedia.org/wiki?curid=2267876", "title": "Employee assistance program", "text": "Employee assistance program\n\nAn employee assistance program (EAP) is an employee benefit program that assists employees with personal problems and/or work-related problems that may impact their job performance, health, mental and emotional well-being. EAPs generally offer free and confidential assessments, short-term counseling, referrals, and follow-up services for employees and their household members. EAP counselors also work in a consultative role with managers and supervisors to address employee and organizational challenges and needs. Many corporations, academic institution and/or government agencies are active in helping organizations prevent and cope with workplace violence, trauma, and other emergency response situations. There is a variety of support programs offered for employees. Even though EAPs are mainly aimed at work-related problems, there are a variety of programs that can assist with problems outside of the workplace. EAPs have grown over the years, and are more desirable economically and socially.\n\nEAPs have been traced back to the late 1930s, and were formed out of programs that dealt with occupational alcoholism. During a time when drinking on the job was the norm, people began to notice the effects it had on job performance and productivity. This became a major issue for industrial jobs and would become the main focus for correction with job-based alcoholism programs. By 1939, the Alcoholics Anonymous (AA) movement had begun to spread throughout the Midwestern and Northeastern United States. People in “recovery\" began to eagerly share their experiences with other workers. This would be the start of the EAP movement. Businesses also started to see the effectiveness of the programs through the rehabilitation of their workers and the rise of productivity. These improvements sparked the thought of what other types of problems this program could address.\n\nIn 1962, The Kemper Group introduced a program to address alcoholic rehabilitation and later expanded the program to address the needs of the families of their employees as well. Including the families broaden the programs services to deal with marital, emotional, financial, legal, and drug abuse problems. In 1969, Senator Harold Hughes would introduce a bill called The Hughes Act. Sen. Hughes felt that there was a great lack of federal and state involvement in the treatment of alcoholism. In 1970, Congress would pass the Federal Comprehensive Alcohol Abuse and Alcoholism Prevention Treatment and Rehabilitation Act creating the National Institute on Alcoholism and Alcohol Abuse (NIAAA). States would then soon begin to follow suit and denounce public intoxication and began treating alcoholism as a disease. The NIAAA priority would be to begin researching and treating alcoholism. They were also focused on providing states with grants to hire and train EAP specialist.\n\nIn the 1970s, the Occupational Alcoholism Bureau formed by the National Council on Alcoholism (NCA) and the Association of Labor and Management Administrators and Consultants on Alcoholism (ALMACA) helped to spread EAP concepts. They did this by distributing information, giving conferences and seminars, increasing the knowledge of professionals and the community. A number of treatment centers would also grow after the passing of the Hughes Act. These centers have EAP specialist on site to help in the rehabilitation processes. It is not known the exact amount of treatment centers in the United States.\n\nEmployee Assistance Programs would see a significant shift during the economic crisis of the 1980s. During this time, the government was forced to create cutbacks for programs. This would cause for mental health public agencies, treatment centers, and private counseling firms to survive by partnering with industry wanting to enter the EAP field. This though would also cause the effectiveness of the programs to come into question. The cutbacks began to affect the training of the EAP specialist and their effectiveness. The situations of workers also began to change at this time. People were also having to wait in lines, and were having to search for work due to the crisis.\n\nIn most recent years, the services provided by EAPs have changed in their direction. With events occurring nationally and around the world has caused for EAPs to rise and the need for them greater in the United States. EAPs have also been affected by technology, terrorism attacks, natural disasters, disabilities act, and workplace violence. Since the events of September 11, 2001, EAP specialists have become more involved in incident debriefing and implementing plans during emergencies Providers began to report more on the workforce experiencing Post Traumatic Stress Disorder (PTSD), and an increase in stress and depression. The continued threat of terrorism, people are reporting that they are more anxious about the thought of an attack occurring again.\n\nSome studies indicate that offering EAPs may result in various benefits for employers, including lower medical costs, reduced turnover and absenteeism, and higher employee productivity. Critics of these studies question the scientific validity of their findings, noting small sample sizes, lack of experimental control groups, and lack of standardized measures as primary concerns. Proponents, however, argue that the consistency of positive findings across studies in different service sectors denote at least some positive effect of programs, even if the most effective components of such programs have not been determined. EAPs may also provide other services to employers, such as supervisory consultations, support to troubled work teams, training and education programs, and critical incident services.\n\nThe provision of employee assistance services has established business benefits, including increased productivity of employees (termed \"presenteeism\") and decreased absenteeism. Employees typically have access to an EAP hotline 24 hours a day, so there is no need to wait to seek assistance. If an appointment with a medical professional or counselor is necessary, the employee can arrange to see one in just a few days. Because the employee can call anytime, they do not have to worry about calling from a work phone. You may also be able to minimize the cost of your health insurance plan, because employees can use the EAP to ward off stress-related illnesses, meaning fewer trips to the doctor.\n\nSmall businesses can especially benefit from EAP programs. Even though they may have fewer employees and may not see the need to implement an EAP program, small business owner's bottom line can suffer quicker due to decreased performance and productivity and workplace negativity. A small company could be severely damaged if they do not seek effective measure to rectify such issues. An employer that provides an effective, full-service EAP can help both themselves and employees by lowering risk and liability, improving employee satisfaction, and especially decreasing the stress small business owners experience when managing numerous responsibilities with little support.\n\nBenefits of EAP are:\n\nEmployees and their household members may use EAPs to help manage issues in their personal lives. EAP counselors typically provide assessment, support, and referrals to additional resources such as counselors for a limited number of program-paid counseling sessions. The issues for which EAPs provide support vary, but examples include:\n\nAn EAP's services are usually free to the employee and their household members, having been prepaid by the employer. In most cases, an employer contracts with a third-party company to manage its EAP. Some of these companies rely upon other vendors or contracted employees for specialized services to supplement their own services, such as: financial advisors, attorneys, travel agents, elder/child care specialists, and the like.\n\nConfidentiality is maintained in accordance with privacy laws and ethical standards.\n\nIn the United States, California requires EAP providers who deliver actual counseling services on a pre-paid basis for more than 3 sessions within any six-month period to have a Knox-Keene license. This is a specialty license for psychological services and is mandated by the Knox-Keene Health Care Service Plan Act of 1975. The state's Department of Managed Health Care regulates these licensed plans and assists consumers with regard to grievances, access to quality care, and ensuring that the EAP has an appropriate level of tangible net equity to deliver services to plan members. Title 28, Rule 1300.43.14 of the California Code of Regulations allows EAPs without a Knox-Keene license to request an exemption if they solely refer callers to external services and do not provide the actual services themselves.\n\nEach Federal Executive Branch agency has an Employee Assistance Program (EAP). An EAP is a voluntary, confidential program that helps employees (including management) work through various life challenges that may adversely affect job performance, health, and personal well-being to optimize an organization's success. EAP services include assessments, counseling, and referrals for additional services to employees with personal and/or work-related concerns, such as stress, financial issues, legal issues, family problems, office conflicts, and alcohol and substance abuse. EAPs also often work with management and supervisors providing advanced planning for situations, such as organizational changes, legal considerations, emergency planning, and response to unique traumatic events. EAP’s can reap benefits for agencies, employees, families and communities. Some of those aspects that we will be focusing on are: the improvement of productivity and employee engagement, improving employees’ and dependents’ abilities to successfully respond to challenges, developing employee and manager competencies in managing workplace stress, reducing workplace absenteeism and unplanned absences, supporting employees and managers during workforce restructuring, reduction-in-forces, or other workforce change events, reducing workplace accidents, reducing the likelihood of workplace violence or other safety risks, supporting disaster and emergency preparedness, managing the effect of disruptive incidents, such as workplace, injury, or other crises,facilitating safe, timely, and effective return-to-work for employees short-term and extended absences, reducing healthcare costs associated with stress, depression, and other mental health issues, reducing employee turnover and related replacement costs.\n\nThere are a variety of employee assistance programs in the military, ranging from financial assistance programs, family counseling, depression, and transitional assistance programs. One of the largest military employee assistance organizations is Military One Source. Military One Source offers both services directly to service members and their families, but can also help send people to places in the community if there is a specific need. If someone is in need of assistance and is: an active duty service member, family member of a service member, a veteran, or family member of a veteran, they can find additional information about the assistance programs offered by Military One Source on their website.\n\nIn the last couple decades military employee assistance programs have expanded greatly. What started as alcohol assistance programs has grown to help people with financial issues, physical health, family health, and since 2000 mental health programs have expanded rapidly as more has become known about Post Traumatic Stress Disorder (PTSD), and Traumatic Brain Injuries (TBI). There are several programs for families, and increasing assistance for dual military families, families in which both spouses are active duty military, as they have grown dramatically in number. Financial programs have been expanded as well, as payday loan establishments grew in popularity in the 2000s; and as a result, debt among military members has grown as well. The Navy-Marine Corps Relief Society (NMCRS), is a program started by military members to help other members that have fallen into debt, and has expanded to also help service members reach educational goals. This society has helped many service members, as a service member can borrow up to 300 dollars without a reason, and more with a valid reason for either no interest or low interest depending on how much and how long they borrow for. The Navy-Marine Corps Relief Society has helped many members of the military when unforeseen problems arise. More information on the Navy-Marine Corps Relief Society can be found at: www.nmcrs.org.\n\nOne of the largest veteran employee assistance programs is the Veterans Affairs (VA). The VA provides a variety of services to veterans including: health care, education assistance, transitional housing assistance, mental health, financial assistance, women veterans assistance, and career search assistance. The Veteran Affairs is a nationwide system that offers their services to all veterans who attained any discharge other than dishonorable. More information on Veterans Affairs can be found at: www.va.gov.\n\n\nMasi, D.A. (2011). Employee assistance programs. Retrieved April 12, 2016 from http://www.socialwelfarehistory.com/programs/employee-assistance-programs/.\n\nRichard, Michael A., William S. Hutchison, and William G. Emener. 2009. Employee Assistance Programs: Wellness/Enhancement Programming. 4th ed. Springfield: Charles C Thomas, eBook Collection.\n\nProfessional associations in the employee assistance program industry:\n"}
{"id": "28009780", "url": "https://en.wikipedia.org/wiki?curid=28009780", "title": "Fall protection", "text": "Fall protection\n\nFall protection is the use of controls designed to protect personnel from falling or in the event they do fall, to stop them without causing severe injury. Typically, fall protection is implemented when working at height, but may be relevant when working near any edge, such as near a pit or hole, or performing work on a steep surface.\n\nThere are four generally accepted categories of fall protection: fall elimination, fall prevention, fall arrest and administrative controls. According to the US Department of Labor, falls account for 8% of all work-related trauma injuries leading to death. Federal statutes, standards and regulations in the United States pertaining to the requirements for employers to provide fall protection are administered by OSHA.\n\nFalls from elevations were the fourth leading cause of workplace death from 1980 through 1994, with an average of 540 deaths per year accounting for 10% of all occupational fatalities.\n\nFalls are a concern for oil and gas workers, many of whom must work high on a derrick. A study of falls over the period 2005–2014 found that in 86% of fatal falls studied, fall protection was required by regulation, but it was not used, was used improperly, or the equipment failed. Many of the fatalities were because, although the workers were wearing harnesses, they neglected to attach them to an anchor point.\n\nIn most work-at-height environments, multiple fall protection measures are used concurrently.\n\nFall elimination is often the preferred way of providing fall protection. This entails finding ways of completing tasks without working at heights.\n\n\nFall arrest is the form of fall protection that stops a person who has fallen.\n\nAdministrative controls are used along with other measures, but they do not physically prevent a worker from going over an edge. Examples of administrative controls include placing a safety observer or warning line near an edge, or enforcing a safety policy which trains workers and requires them to adhere to other fall protection measures, or prohibiting any un-restrained worker from approaching an edge.\n"}
{"id": "3433583", "url": "https://en.wikipedia.org/wiki?curid=3433583", "title": "Folk healer", "text": "Folk healer\n\nA folk healer is an unlicensed person who practices the art of healing using traditional practices, herbal remedies and even the power of suggestion. A folk healer may be a highly trained person who pursues their specialties, learning by study, observation and imitation. In some cultures a healer might be considered to be a person who has inherited the \"gift\" of healing from his or her parent. The ability to set bones or the power to stop bleeding may be thought of as hereditary powers.\n\nGranny women are purported to be healers and midwives in Southern Appalachia and the Ozarks, claimed by a few academics as practicing from the 1880s to the 1930s. They are theorized to be usually elder women in the community and may have been the only practitioners of health care in the poor rural areas of Southern Appalachia. They are often thought to not have expected or received payment, and were respected as authorities on herbal healing and childbirth. They are mentioned by John C. Campbell in \"The Southern Highlander and His Homeland\":\n\nWhite witch and good witch are qualifying terms in English used to distinguish practitioners of folk magic for benevolent purposes (i.e. white magic) from practitioners of malevolent witchcraft or black magic. Related terms are \"cunning-folk\", \"witch doctor\", and the French \"devins-guérisseurs\", \"seer-healers\".\n\nDuring the witch trials of Early Modern Europe, many practitioners of folk magic who did not see themselves as witches, but as healers or seers, were convicted of witchcraft (Éva Pócs' \"sorcerer witches\"): many English \"witches\" convicted of consorting with demons seem to have been cunning folk whose fairy familiars had been demonised, and over half the accused witches in Hungary seem to have been healers.\n\nSome of the healers and diviners historically accused of witchcraft have considered themselves mediators between the mundane and spiritual worlds, roughly equivalent to shamans. Such people described their contacts with fairies, spirits, or the dead, often involving out-of-body experiences and travelling through the realms of an \"other-world\". Beliefs of this nature are implied in the folklore of much of Europe, and were explicitly described by accused witches in central and southern Europe. Repeated themes include participation in processions of the dead or large feasts, often presided over by a female divinity who teaches magic and gives prophecies; and participation in battles against evil spirits, \"vampires\", or \"witches\" to win fertility and prosperity for the community.\n\n\n\n"}
{"id": "48369072", "url": "https://en.wikipedia.org/wiki?curid=48369072", "title": "Food and diet in ancient medicine", "text": "Food and diet in ancient medicine\n\nModern understanding of disease is very different from the way it was understood in ancient Greece and Rome. The way modern physicians approach healing of the sick differs greatly from the methods used by early general healers or elite physicians like Hippocrates or Galen. In modern medicine, the understanding of disease stems from the “germ theory of disease”, a concept that emerged in the second half of the 19th century, such that a disease is the result of an invasion of a microorganism into a living host. Therefore, when a person becomes ill, modern treatments “target” the specific pathogen or bacterium in order to “beat” or “kill” the disease. In Ancient Greece and Rome, disease was literally understood as dis-ease, or physical imbalance. Medical intervention, therefore, was purposed with goal of restoration of harmony rather than waging a war against disease. Surgery was regarded by Greek and Roman physicians as extreme and damaging while prevention was seen as the crucial first step to healing almost all ailments. In both prevention and treatment of disease in classical medicine, food and diet was central. The eating of correctly-balanced foods made up the majority of preventative treatment as well as to restore harmony to the body after it encountered disease.\n\nAncient Greek Medicine is described as rational, ethical and based upon observation, conscious learning and experience. Superstition and religious dogmatism are often excluded from descriptions of ancient Greek medicine. It is important, however, to note that this rational approach to medicine did not always exist in the ancient Greek medical world, nor was it the only popular method of healing. Along with rational Greek medicine, disease was also thought of as being of supernatural origin, resulting from the unhappiness of the gods or from demonic possession. Exorcists and religious healers were among the ‘doctors’ that patients sought out when they became ill. Sacrifices, exorcisms, spells and prayers were then carried out in order to reconcile with the gods and restore health to the patient. It was not until the time of Hippocrates, between 450 and 350 BC, that rational, observation and the humoral theory of medicine began to become highly influential. The theory of the humours or “Humorism” understood the human body to be composed of fluid (humours) and regarded disease as a result of an imbalance of the four humors: yellow bile, black bile, phlegm and blood. These humours contain qualities such as hot, cool, moist, dry, etc., which must also remain in balance. Foods can be heating, cooling, or generative of one humour. Some foods produce good juices and others bad juices and often times cooking and preparation of the foods can change or improve the juices of the foods. In addition, foods may be easy to assimilate (easy to pass through the body), easily excreted, nourishing or not nourishing. In Hippocratic medicine, the qualities in foods are analogous to the four humors in the body: too much of a single one is bad, a proper mixture is ideal. Therefore, the consumption of correctly-balanced foods and life-style of the patient was crucial to the prevention and treatment of disease in Ancient Greece.\n\nThis brings us to the way in which seasonal food played an important role in the treatment of ancient disease. According to the Hippocratic author of “Airs, Waters and Places” (there remains debate as to whether Hippocrates himself wrote the Hippocratic Corpus), it is important that a physician learn astronomy because, “the changes of the seasons produce changes in diseases,”. In the same Hippocratic text, the author goes on to explain that villages facing east and that are exposed to winds from the north-east, south-east and west tended to be healthy and, “The climate in such a district may be compared with the spring in that there are no extremes of heat and cold. As a consequence, diseases in such a district are few and not severe,”. As an example of the importance of seasonal food on maintaining balance of the humours and preventing disease is given by Hippocrates in “On Regimen” when the authors state that, “in winter, to secure a dry and hot body it is better to eat wheaten bread, roast meat, and few vegetables; whereas in summer it is appropriate to eat barley cake, barley meat and softer foods,” (qtd. in Wilkins et al., p. 346).\n\nFood and diet feature prominently in the aphorisms of the Hippocratic Corpus. For example, in one aphorism in the first section, Hippocrates states, “Things which are growing have the greatest natural warmth and, accordingly, need most nourishment. Failing this the body becomes exhausted. Old men have little warmth and they need little food which produces warmth; too much only extinguishes the warmth they have. For this reason, fevers are not so acute in old people for then the body is cold”. Another aphorism says, “It is better to be full of drink than full of food”. And finally, an aphorism that generally sums up treatment of disease in Hippocratic times states, “Disease which results from over-eating is cured by fasting; disease following fasting, by a surfeit. So with other things; cures may be effected by opposites,”. This concept of treating diseases opposite to the way it manifests in the individual is concept that is carried over into Roman medicine.\n\nGout was called podagra in ancient Greek medicine and is a common arthritis caused by deposition of monosodium urate crystals within the joints. Gout usually affects the first metatarsophalangeal joint of the big toe and later the other joints of the feet and hands. Hippocrates considered gout to be the result of an accumulation of one of the body humours that distended the joint and caused pain. Hippocrates also believed gout to be a result from sexual excess or too rich a diet as alluded to in three of his aphorisms “Eunuchs do not take the gout nor become bald”, “A woman does not take the gout unless her menses has stopped”, and “A young man does not take the gout until he indulges coitus”. As with other diseases, physicians in antiquity believed that diet was the best way to manage gout. Hippocrates recommended high doses of white hellebore because he believed that the best and most natural relief for gout was dysentery. However, purging with white hellebore was probably for the more chronic cases due to the fact that wine and barleywater drinks were very strongly recommended.\n\nThe importance of legumes in ancient Greek diet and medical practice is often disregarded. However, legumes improved the quality of the soil and were considered very important to the agriculturalists of the time. Additionally, legumes contain a high amount of albumen, which led them to be a critical dietary supplement in countries where meat was in short supply and difficult to store. Such was the case with Greece. People in the Graeco-Roman world consumed less meat than we do today and therefore, legumes were a necessary source of protein. Of all legumes, the lentil appears most frequently in Greek and Roman literature. Medicinally, Hippocrates recommends lentils as a remedy for ulcers and hemorrhoids. Bitter vetch, or Vicia ervilia, was also an important legume in ancient Greek medicine. The extensive medicinal qualities of the bitter vetch were thought reliable enough to later administer to Roman emperors such as Augustus. Bitter vetch was thought to heal pimples, prevent sores from spreading and soothe spots or sores when they appear on the breasts. It was also reported to relieve painful urination, flatulence, liver problems, and indigestion when roasted and mixed with honey and Hippocrates cautioned that when eaten boiled or raw, the consumption of bitter vetch may cause more flatulence or pain.\n\nAt the heart of Roman Medicine and central to the development of Western Medicine is Galen of Pergamum (AD 129–c. AD 210). Galen was a prolific writer from whose surviving works comes what Galen believed to be the definitive guide to a healthy diet, based on the theory of the four humours. Galen understood the humoral theory in a dynamic sense rather than static sense such that yellow bile is hot and dry like fire; black bile is dry and cold like earth; phlegm is cold and moist like water; and blood is moist and hot. He also understood the humours to be produced by food through digestion and it is with digestion, and respiration, that Galen applies his knowledge of anatomy. According to Galen, digestion begins in the mouth because this where food comes into contact with the saliva. The chewed food is then pulled into the stomach where the heat of the stomach cooks the food into chyle. The chyle is then carried to the liver where the nutrients are converted to blood and transported throughout the body. With this understanding of the humours as being dynamic, and his knowledge of anatomy, Galen was able to categorize illnesses as hot, cold, dry or moist and attribute the causes of these illnesses to specific types of foods. For example, in Galen’s own “On the Causes of Disease”, as cited by Mark Grant, Galen says when describing hot diseases that, “[one cause of excessive heat] lies in foods that have hot and harsh powers, such as garlic, leeks, onions, and so on. Immoderate use of these foods sometimes sparks a fever,”. Galen believed that a good physician must also be a good cook. Therefore, in Galen’s dietary treatise “On the Powers of Foods”, recipes are often given in addition to descriptions of foods as being salty or sweet, sour or watery, difficult or easy to digest, costive or laxative, cooling or heating, etc. Galen insists that the balance of the four humours can be beneficially or adversely effected by ‘diet’ which in Galenic medicine includes not just food and drink but also physical exercise, baths, massage, and climate\n\nAs mentioned, certain types of food can affect the balance of the humours in different ways. According to Galen’s “On Humours,” as referenced by Wilkins et al., beef, camel, and goat meat, snails, cabbage, and soft cheeses produce black bile; brains, fungi, and hard apples cause phlegm; bitter almonds and garlic reduce phlegm. Additionally, for the treatment of gout, Galen suggested a range of dressings to be applied to the affected area made of mandrake, caper, and henbane and for the acute phase, creams made of seeds of conium, mushroom, and deer brain were administered.\n"}
{"id": "38544252", "url": "https://en.wikipedia.org/wiki?curid=38544252", "title": "Global Oncology", "text": "Global Oncology\n\nGlobal Oncology (GO) is an American academic community-based healthcare organization. The organization was established at Harvard University in 2012.\n\nGlobal Oncology was founded in 2012, by Ami S. Bhatt and Franklin W. Huang. The organization is composed of physicians, staff, students, and members of the oncology community and other skilled professional volunteers. The mission of the organization is to improve cancer care and research in resource-limited settings through a variety of programs and efforts, including the development of training and research opportunities in global oncology. The organization supports the efforts of other leading organizations working in global oncology and cancer care including the Global Task Force on Expanded Access to Cancer Care and Control in Developing Countries.\n\nThe steering committee that advises the organization is composed of academic and faculty experts in cancer care and oncology drawn from Harvard-affiliated institutions and hospitals, including Harvard Medical School, Harvard School of Public Health, Beth Israel Deaconess Medical Center, Brigham and Women's Hospital, Children's Hospital Boston, Dana-Farber Cancer Institute, and Massachusetts General Hospital.\n\nA primary effort of GO has been the creation of the GO! Talks, a unique bi-monthly Global Oncology seminar series that seeks to educate the larger community on global oncology issues and catalyze collaborations in the field. Speakers to date have included Paul Farmer and Eric Krakauer.\nGlobal Oncology developed a first-of-its-kind Global Cancer Project Map that launched in 2015 as a resource for the global cancer community to identify projects and needs in global cancer work.\n"}
{"id": "2607178", "url": "https://en.wikipedia.org/wiki?curid=2607178", "title": "Glucosinolate", "text": "Glucosinolate\n\nThe glucosinolates are natural components of many pungent plants such as mustard, cabbage, and horseradish. The pungency of those plants is due to mustard oils produced from glucosinolates when the plant material is chewed, cut, or otherwise damaged. These natural chemicals most likely contribute to plant defence against pests and diseases, and impart a characteristic bitter flavor property of cruciferous vegetables.\n\nGlucosinolates occur as secondary metabolites of almost all plants of the order Brassicales. Ordered in the Brassicales are for example the economically important family Brassicaceae as well as Capparaceae and Caricaceae.\nOutside of the Brassicales, the genera \"Drypetes\" and \"Putranjiva\" in the family Putranjivaceae are the only other known occurrence of glucosinolates.\nGlucosinolates occur in various edible plants such as cabbage (white cabbage, Chinese cabbage, broccoli) watercress, horseradish, capers and radishes where the breakdown products often contribute a significant part of the distinctive taste. The glucosinolates are also found in seeds of these plants.\n\nGlucosinolates constitute a natural class of organic compounds that contain sulfur and nitrogen and are derived from glucose and an amino acid. They are water-soluble anions and belong to the glucosides. Every glucosinolate contains a central carbon atom, which is bound via a sulfur atom to the thioglucose group and via a nitrogen atom to a sulfate group (making a sulfated aldoxime). In addition, the central carbon is bound to a side group; different glucosinolates have different side groups, and it is variation in the side group that is responsible for the variation in the biological activities of these plant compounds. \nThe semisystematic naming of glucosinolates consists of the chemical name of that side chain followed by \"glucosinolate\". Spelling glucosinolate names in one or two words (e.g. allylglucosinolate versus allyl glucosinolate) are both in use and has equivalent meaning. Isothiocyanates must be spelled in two words.\n\nThe essence of glucosinolate chemistry is the ability of a glucosinolate to convert into an isothiocyanate (a \"mustard oil\") upon hydrolysis of the thioglucoside bond by the enzyme myrosinase.\n\nSome glucosinolates:\n\nAbout 132 different glucosinolates are known to occur naturally in plants. They are synthesized from certain amino acids: So-called aliphatic glucosinolates derived from mainly methionine, but also alanine, leucine, isoleucine, or valine. (Most glucosinolates are actually derived from chain-elongated homologues of these amino acids, e.g. glucoraphanin is derived from dihomomethionine, which is methionine chain-elongated twice). Aromatic glucosinolates include indolic glucosinolates, such as glucobrassicin, derived from tryptophan and others from phenylalanine, its chain-elongated homologue homophenylalanine, and sinalbin derived from tyrosine.\n\nThe plants contain the enzyme myrosinase, which, in the presence of water, cleaves off the glucose group from a glucosinolate. The remaining molecule then quickly converts to an isothiocyanate, a nitrile, or a thiocyanate; these are the active substances that serve as defense for the plant. Glucosinolates are also called mustard oil glycosides. The standard product of the reaction is the isothiocyanate (mustard oil); the other two products mainly occur in the presence of specialised plant proteins that alter the outcome of the reaction.\n\nIn the chemical reaction illustrated above, the red curved arrows in the left side of figure are simplified compared to reality, as the role of the enzyme myrosinase is not shown. However, the mechanism shown is fundamentally in accordance with the enzyme-catalyzed reaction.\n\nIn contrast, the reaction illustrated by red curved arrows at the right side of the figure, depicting the rearrangement of atoms resulting in the isothiocyanate, is expected to be non-enzymatic. This type of rearrangement can be named a Lossen-reaarrangement, or a Lossen-\"like\" rearrangement, since this name was first used for the analogous reaction leading to an organic isocyanate (R-N=C=O).\n\nTo prevent damage to the plant itself, the myrosinase and glucosinolates are stored in separate compartments of the cell or in different cells in the tissue, and come together only or mainly under conditions of physical injury.\n\nThe use of glucosinolate-containing crops as primary food source for animals can have negative effects if the concentration of glucosinolate is higher than what is acceptable for the animal in question, because some glucosinolates have been shown to have toxic effects (mainly as goitrogens) in both humans and animals at high doses. However, tolerance level to glucosinolates varies even within the same genus (e.g. \"Acomys cahirinus\" and \"Acomys russatus\").\n\nThe glucosinolate sinigrin, among others, was shown to be responsible for the bitterness of cooked cauliflower and Brussels sprouts. Glucosinolates may alter animal eating behavior.\n\nGlucosinolates are studied for their potential to affect human health, as well as plant breeding, physiology, genetics, and food applications. , studies on possible anticancer mechanisms have been conducted, but there is no clinical evidence to indicate the safety and efficacy of using glucosinolates for treating cancer or any human disease. The compositions of glucosinolates and their hydrolysis products vary by vegetable.\n\nGlucosinolates and their products have a negative effect on many insects, resulting from a combination of deterrence and toxicity. In an attempt to apply this principle in an agronomic context, some glucosinolate-derived products can serve as antifeedants, i.e., natural pesticides. \n\nIn contrast, the diamondback moth, a pest of cruciferous plants, may recognize the presence of glucosinolates, allowing it to identify the proper host plant. Indeed, a characteristic, specialised insect fauna is found on glucosinolate-containing plants, including butterflies, such as large white, small white, and orange tip, but also certain aphids, moths, such as the southern armyworm, sawflies, and flea beetles. For instance, the large white butterfly deposits its eggs on these glucosinolate-containing plants, and the larvae survive even with high levels of glucosinolates and eat plant material containing glucosinolates. The whites and orange tips all possess the so-called nitrile specifier protein, which diverts glucosinolate hydrolysis toward nitriles rather than reactive isothiocyanates. In contrast, the diamondback moth possesses a completely different protein, glucosinolate sulfatase, which desulfates glucosinolates, thereby making them unfit for degradation to toxic products by myrosinase.\n\nOther kinds of insects (specialised sawflies and aphids) sequester glucosinolates. In specialised aphids, but not in sawflies, a distinct animal myrosinase is found in muscle tissue, leading to degradation of sequestered glucosinolates upon aphid tissue destruction. This diverse panel of biochemical solutions to the same plant chemical plays a key role in the evolution of plant-insect relationships.\n\n\n"}
{"id": "32890301", "url": "https://en.wikipedia.org/wiki?curid=32890301", "title": "Health crisis", "text": "Health crisis\n\nA health crisis or public health crisis is a difficult situation or complex health system that affects humans in one or more geographic areas (mainly occurred in natural hazards), from a particular locality to encompass the entire planet. Health crises generally have significant impacts on community health, loss of life, and on the economy. They may result from disease, industrial processes or poor policy.\n\nIts severity is often measured by the number of people affected by its geographical extent, or the disease or death of the pathogenic process which it originates.\n\nGenerally there are three key components in health crises:\n\n\n\n\n\n\n"}
{"id": "27900272", "url": "https://en.wikipedia.org/wiki?curid=27900272", "title": "Health humanities", "text": "Health humanities\n\nHealth humanities refers to the application of the creative or fine arts (including visual arts, music, performing arts) and humanities disciplines (including literary studies, languages, law, history, philosophy, religion, etc.) to discourse about, express, and/or promote dimensions of human health and well being. This applied capacity of the humanities is not itself a novel idea; however, the construct of the health humanities has only recently begun to emerge over the first decade of the 21st Century. Historically, the roots informing the health humanities can be traced back to, and can now be considered to include, such multidisciplinary areas as the medical humanities and the expressive therapies/creative arts therapies.\n\nIn the health humanities, health (and the promotion of health) is understood according to the constructivist (and other non-positivist) principles indigenous to the humanities, as opposed to the positivism of science. The health humanities are rooted in dialogical (negotiated, intersubjective voices of multiple truths), versus monological (a singular, authoritative voice of \"the\" truth) perspectives on health. As such, evidence upon which health practices are based is generally considered axiological (based in meanings, values, and aesthetics), versus epistemological (based in factual knowledge), in orientation. The health humanities are not an alternative to the health sciences, but rather offer a contrasting paradigm and pragmatic approach with respect to health and its promotion, and can function in a manner that is complementary and simultaneous relative to the health sciences. \n\nThe health humanities are a growing movement internationally. A conference on the health humanities was held October 13–15, 2006, at Green College, University of British Columbia. The conference was co-organized by Judy Segal (UBC English) and Alan Richardson (UBC Philosophy) and featured presentations by Jacalyn Duffin, Carl Elliott, Nicholas King, Lorelei Lingard, Robert Proctor, Susan Squier, Andrea Tone, and Kathleen Woodward. In January 2009, Paul Crawford became the world's first Professor of Health Humanities at The University of Nottingham, and with Dr Victoria Tischler, Charley Baker, Dr Brian Brown, Dr Lisa Mooney-Smith and Professor Ronald Carter created an international health humanities initiative that included the AHRC-funded International Health Humanities Conference (IHHC). The first field description for Health Humanities was presented in the key article \"Health Humanities: The future of Medical Humanities\" (Crawford, Brown, Tischler, & Baker, 2010), published in the Mental Health Review Journal.\n\nThe 1st International Health Humanities Conference was held 6–8 August 2010, at The University of Nottingham, United Kingdom. The conference opened with Professor Crawford's address entitled ‘Health humanities: Literature and Madness’ and included keynote lectures by Professor Kay Redfield Jamison and Professor Elaine Showalter. Mark A. Radcliffe, who also spoke at the conference, reported on 'health humanities' in his weekly column for the Nursing Times. The conference was also reported in the Bethlem Blog. The 2nd International Health Humanities Conference was hosted in the USA, 9–11 August 2012, at Montclair State University in New Jersey, with the theme of \"Music, Health, and Humanity.\" The 3rd International Health Humanities Conference was held 5-7 September 2014, once again at the University of Nottingham, and featured the theme of \"Traumatextualities: Trauma in the Clinical, Arts and Humanities Contexts.\" The 4th International Health Humanities Conference was held 30 April and 1-2 May 2015, at the Center for Bioethics and Humanities, Anschutz Medical Campus, University of Colorado, Denver, featuring the theme of \"Health Humanities: The Next Decade (Pedagogies, Practices, Politics).\" The 5th International Health Humanities Conference will be held 15-17 September 2016, at the University of Seville, Seville, Spain, featuring the theme of \"Arts and Humanities for Improving Social Inclusion, Education, and Health: Creative Practice and Mutuality.\" \n\nTextbooks on the health humanities include \"Health Humanities Reader\" and \"Health Humanities\".\n\nIn 2015 a Health Humanities Centre was established at University College London, dedicated to research and teaching in the Health Humanities, including an MA in Health Humanities.\n\n\nJones T, Blackie M, Garden R, and Wear D. (2016) The Almost Right Word: The Move From Medical to Health Humanities. Academic Medicine http://journals.lww.com/academicmedicine/Abstract/publishahead/The_Almost_Right_Word___The_Move_From_Medical_to.98313.aspx\n\nSCOPE: The Health Humanities Learning Lab\n\nhttp://sites.fhi.duke.edu/healthhumanitieslab/\n"}
{"id": "6199182", "url": "https://en.wikipedia.org/wiki?curid=6199182", "title": "Health literacy", "text": "Health literacy\n\nHealth literacy is the ability to obtain, read, understand, and use healthcare information in order to make appropriate health decisions and follow instructions for treatment. There are multiple definitions of health literacy, in part, because health literacy involves both the context (or setting) in which health literacy demands are made (e.g., health care, media, internet or fitness facility) and the skills that people bring to that situation.\n\nSince health literacy is a primary contributing factor to health disparities, it is a continued and increasing concern for health professionals. The 2003 National Assessment of Adult Literacy (NAAL) conducted by the US Department of Education found that 36% of participants scored as either \"basic\" or \"below basic\" in terms of their health literacy and concluded that approximately 80 million Americans have limited health literacy. These individuals have difficulty with common health tasks including reading the label of a prescribed drug. Several factors may influence health literacy. However, the following factors have been shown to strongly increase this risk: age (especially patients 65 years and older), limited English language proficiency or English as a second language, less education, and lower socioeconomic status. Patients with low health literacy understand less about their medical conditions and treatments and overall report worse health status.\n\nVarious interventions, such as simplifying information and illustrations, avoiding jargon, using \"teach-back\" methods, and encouraging patients' questions, have improved health behaviors in persons with low health literacy. The proportion of adults aged 18 and over in the U.S., in the year 2010, who reported that their health care providers always explained things so they could understand them was about 60.6%. This number increased 1% from 2007 to 2010. The Healthy People 2020 initiative of the United States Department of Health and Human Services has included health literacy as a pressing new topic, with objectives for improving it in the decade to come.\n\nSociety as a whole is responsible for improving health literacy. Most importantly, improving health literacy is the responsibility of healthcare and public health professionals and systems.\n\nIn order to have a patient that understands health terms and can make proper health decisions, the language used by health professionals has to be at a level that others who are not in the medical field can understand. Health professionals must know their audience in order to better serve their patients. The language used by these professionals should be plain language. Plain language is a strategy for making written and oral information easier to understand; it is communication that users can understand the first time they read or hear it.\n\nSome key elements of plain language include: \n\nThe National Institute of Health (NIH) recommends that patient education materials should be not written higher than a 6th-7th grade reading level; further recommendations provided by the NIH Office of Communications and Public Liaison are published in their \"Clear Communication\" Initiative.\n\nMany factors determine the health literacy level of health education materials or interventions: readability of the text, the patient's current state of health, language barriers of the patient, cultural appropriateness of the materials, format and style, sentence structure, use of illustrations, and numerous other factors.\n\nA study of 2,600 patients conducted in 1995 by two US hospitals found that between 26% and 60% of patients could not understand medication directions, a standard informed consent form, or materials about scheduling an appointment. The 2003 National Assessment of Adult Literacy (NAAL) conducted by the US Department of Education found that 36% of participants scored as either \"basic\" or \"below basic\" in terms of their health literacy and concluded that approximately 80 million Americans have limited health literacy.\n\nThe young and multidisciplinary field of health literacy emerged from two groups of experts: physicians, health providers such as nurses, and health educators; and Adult Basic Education (ABE) and English as a second language (ESL) practitioners in the field of education. Physicians and nurses are a source of patient comprehension and compliance studies. Adult Basic Education / English for Speakers of Languages Other Than English (ABE/ESOL) specialists study and design interventions to help people develop reading, writing, and conversation skills and increasingly infuse curricula with health information to promote better health literacy. A range of approaches to adult education brings health literacy skills to people in traditional classroom settings, as well as where they work and live.\n\nThe biomedical approach to health literacy that became dominant (in the U.S.) during the 1980s and 1990s often depicted individuals as lacking health literacy or \"suffering\" from low health literacy. This approach assumed that recipients are passive in their possession and reception of health literacy and believed that models of literacy and health literacy are politically neutral and universally applicable. This approach is found lacking when placed in the context of broader ecological, critical, and cultural approaches to health. This approach has produced, and continues to reproduce, numerous correlational studies.\n\nLevel of health literacy is considered adequate when the population has sufficient knowledge, skills, and confidence to guide their own health, and people are able to stay healthy, recover from illness, and/or live with disability or disease.\n\nMcMurray states that health literacy is important in a community because it addresses health inequities. It is no coincidence that individuals with lower levels of health literacy live, disproportionally, in communities with lower socio-economic standing. A barrier to achieving adequate health literacy for these individuals is a lack of awareness, or understanding of, information and resources relevant to improving their health. This knowledge gap arises from both patients being unable to understand information presented to them and hospitals' inadequate efforts and materials to address these literacy gaps.\n\nA more robust view of health literacy includes the ability to understand scientific concepts, content, and health research; skills in spoken, written, and online communication; critical interpretation of mass media messages; navigating complex systems of health care and governance; knowledge and use of community capital and resources; and using cultural and indigenous knowledge in health decision making. This integrative view sees health literacy as a social determinant of health that offers a powerful opportunity to reduce inequities in health.\n\nThis perspective defines health literacy as the wide range of skills, and competencies that people develop over their lifetimes to seek out, comprehend, evaluate, and use health information and concepts to make informed choices, reduce health risks, and increase quality of life. While various definitions vary in wording, they all fall within this conceptual framework.\n\nDefining health literacy in that manner builds the foundation for a multi-dimensional model of health literacy built around four central domains:\n\n\nThere are several tests, which have verified reliability in the academic literature that can be administered in order to test one's health literacy. Some of these tests include the Medical Term Recognition Test (METER), which was developed in the United States (2 minute administration time) for the clinical setting. The METER includes many words from the Rapid Estimate of Adult Literacy in Medicine (REALM) test. The Short Assessment of Health Literacy in Spanish and English populations (SAHL-S&E) uses word recognition and multiple choice questions to test a person's comprehension. The CHC-Test measures Critical Health Competencies and consists of 72 items designed to test a person's understanding of medical concepts, literature searching, basic statistics, and design of experiments and samples.\n\nAccording to an Institute of Medicine (2004) report, low health literacy negatively affects the treatment outcome and safety of care delivery. The lack of health literacy affects all segments of the population. However, it is disproportionate in certain demographic groups, such as the elderly, ethnic minorities, recent immigrants, and persons with low general literacy. These populations have a higher risk of hospitalization, longer hospital stays, are less likely to comply with treatment, are more likely to make errors with medication, and are more ill when they initially seek medical care.\nThe mismatch between a clinician's communication of content and a patient's ability to understand that content can lead to medication errors and adverse medical outcomes. Health literacy skills are not only a problem in the general population. Health care professionals (doctors, nurses, public health workers) can also have poor health literacy skills, such as a reduced ability to clearly explain health issues to patients and the public.\nIn addition to tailoring the content of what health professionals communicate to their patients, a well arranged layout, pertinent illustrations, and intuitive format of written materials can improve the usability of health care literature. This in turn can help in effective communication between healthcare providers and their patients.\n\nIdentifying a patient as having low health literacy is essential for a healthcare professional to conform their health intervention in a way that the patient will understand. When patients with low health literacy receive care that is tailored to their more limited medical knowledge base, results have shown that health behaviors drastically improve. This has been seen with: correct medication use and dosage, utilizing health screenings, as well as increased exercise and smoking cessation. Effective visual aids have shown to help supplement the information communicated by the doctor in the office. In particular, easily readable brochures and videos have shown to be very effective. Healthcare professionals can use many methods to attain patients' health literacy. A multitude of tests used during research studies and three minute assessments commonly used in doctors offices are examples of the variety of tests healthcare professionals can use to better understand their patients' health literacy.\n\nThe American Medical Association showed that asking simple single item questions, such as \"How confident are you in filling out medical forms by yourself?\", is a very effective and direct way to understand from a patient's point of view how they feel about interacting with their healthcare provider and understanding their health condition.\n\nIn order to be understood by patients with insufficient health literacy, health professionals must intervene to provide clear and concise information that can be more easily understood. Avoidance of medical jargon, illustrations of important concepts, and confirming information by a \"teach back\" method have shown to be effective tools to communicating essential health topics with health illiterate patients. A program called \"Ask Me 3\" is designed to bring public and physician attention to this issue, by letting patients know that they should ask three questions each time they talk to a doctor, nurse, or pharmacist:\n\nThere have also been large-scale efforts to improve health literacy. For example, a public information program by the US Department of Health and Human Services encourages patients to improve healthcare quality and avoid errors by asking questions about health conditions and treatment. Additionally, the IROHLA (Intervention Research on Health Literacy of the Ageing population) project, funded by the European Union (EU), seeks to develop evidence-based guidelines for policy and practice to improve health literacy of the ageing population in EU member states. The project has developed a framework and identified and validated interventions which together constitute a comprehensive approach of addressing health literacy needs of the elderly.\n\nDiabetes is a rapidly growing health problem among immigrants—affecting approximately 10 percent of Asian-Americans. It is the fifth-leading cause of death in Asian-Americans between the ages of 45 and 64. In addition, type 2 diabetes is the most common form of the disease. Those who are diagnosed with type 2 diabetes have high levels of blood glucose because the body does not effectively respond to insulin. It is a lifelong disease with no known cure. Diabetes is a chronic, debilitating, and costly social burden—costing healthcare systems about $100 billion annually.\n\nDiabetes disproportionately affects underserved and ethnically diverse populations, such as Vietnamese-American communities. The relationship between the disease and health literacy level is in part because of an individual's ability to read English, evaluate blood glucose levels, and communicate with medical professionals. Other studies also suggest lack in knowledge of diabetes symptoms and complications. According to an observational cross-sectional study conducted, many Vietnamese-American diabetic patients show signs of poor blood glucose control and adherence due to inadequate self-management knowledge and experience. Diabetes health literacy research is needed to fully understand the burden of the chronic disease in Vietnamese-American communities, with respect to language and culture, health literacy, and immigrant status. Ethnic minority groups and immigrant communities have less knowledge of health promoting behavior, face considerable obstacles to health services, and experience poor communication with medical professionals. According to a recent review, studies have supported an independent relationship between literacy and knowledge of diabetes management and glucose control, but its impact on patients has not been sufficiently described. With the demand of chronic disease self-management (e.g., diabetic diet, glucose monitoring, etc.), a call for cultural-specific patient education is needed to achieve the control of diabetes and its adverse health outcomes in low- to middle-income Vietnamese-American immigrant communities.\n\nThe problem of low oral health literacy (OHL) is often neglected which may lead to poor oral health outcomes and under utilization of oral care services. A cross-sectional survey of school teachers working in schools at Mangalore, India was undertaken. Details regarding demographics, medical, and dental history, oral hygiene practices and habits, diet history, and decay promoting the potential of school teachers were obtained using face-to-face interview method. The Rapid Estimate of Adult Literacy in Dentistry-99 (REALD-99) was used to assess their OHL.The OHL was high in the school teachers with the REALD-99 scores ranging from 45 to 95 with a mean score of 75.83 ± 9.94. Th This study found that there was a statistically significant difference between OHL and education, frequency of brushing and the filled teeth. Although this study indicated high OHL levels among school teachers in Mangalore, India the magnitude of dental caries in this population was also relatively high and very few had a healthy periodontium.\n\neHealth literacy describes the relatively modern concept of an individual's ability to search for, successfully access, comprehend, and appraise desired health information from electronic sources and to then use such information to attempt to address a particular health problem. Due to the increasing influence of the internet for information-seeking and health information distribution purposes, eHealth literacy has become an important topic of research in recent years. Stellefson (2011) states, \"8 out of 10 Internet users report that they have at least once looked online for health information, making it the third most popular Web activity next to checking email and using search engines in terms of activities that almost everybody has done.\" Though in recent years, individuals may have gained access to a multitude of health information via the Internet, access alone does not ensure that proper search skills and techniques are being used to find the most relevant online and electronic resources. As the line between a reputable medical source and an amateur opinion can often be blurred, the ability to differentiate between the two is important.\n\nHealth literacy requires a combination of several different literacy skills in order to facilitate eHealth promotion and care. Six core skills are delineated by an eHealth literacy model referred to as the Lily model. The Lily Model's six literacies are organized into two central types: analytic and context-specific. Analytic type literacies are those skills that can be applied to a broad range of sources, regardless of topic or content (i.e., skills that can also be applied to shopping or researching a term paper in addition to health) whereas context-specific skills are those that are contextualized within a specific problem domain (can solely be applied to health). The six literacies are listed below, the first three of the analytic type and the latter three of the context-specific:\n\nAccording to Norman (2006), both analytical and context-specific literacy skills are \"required to fully engage with electronic health resources.\" As the World Wide Web and technological innovations are more and more becoming a part of the healthcare environment, it is important for information technology to be properly utilized to promote health and deliver health care effectively. Furthermore, it was argued by Hayat Brainin & Neter (2017), that digital media fosters the creation of interpersonal ties, that can supplement eHealth literacy. According to Hayat Brainin & Neter (2017), individuals with low eHealth literacy who were able to recruit help when performing online activities demonstrated higher health outcomes compared to similar individuals who did not find help. Also relating to the proliferation of digital media is the fact that many individuals now can create their own ‘media content’ (user-generated content). This means that the boundary between “information” and “media” content, as proposed by Norman in 2006, now is increasingly blurred, creating additional challenges for health practitioners (Holmberg, 2016).\n\nIt has also been suggested that the move towards patient-centered care and the greater use of technology for self-care and self-management requires higher health literacy on the part of the patient. This has been noted in several research studies, for example among adolescent patients with obesity.\n\nThe United States Department of Health and Human Services created a National Action Plan to Improve Health Literacy. One of the goals of the National Action Plan is to incorporate health and science information in childcare and education through the university level. The target is to educate people at an early stage; that way individuals are raised with health literacy and will have a better quality of life. The earlier an individual is exposed to health literacy skills the better for the person and the community.\n\nPrograms such as Head Start and Women, Infants, and Children (WIC) have impacted our society, especially the low income population. Head Start provides low-income children and their families early childhood education, nutrition, and health screenings. Health literacy is integrated in the program for both children and parents through the education given to the individuals. WIC serves low-income pregnant women and new mothers by supplying them with food, health care referrals, and nutrition education. Programs like these help improve the health literacy of both the parent and the child, creating a more knowledgeable community with health education.\n\nAlthough programs like Head Start and WIC have been working with the health literacy of a specific population, much more can be done with the education of children and young adults. Now, more and more adolescents are getting involved with their health care. It is crucial to educate these individuals in order for them to make informed decisions.\n\nMany schools in the country incorporate a health class in their curriculum. These classes provided an excellent opportunity to facilitate and develop health literacy in today's children and adolescents. The skills of how to read food labels, the meaning of common medical terms, the structure of the human body, and education on the most prevalent diseases in the United States should be taught in both private and public schools. This way new generations will grow with health literacy and would hopefully make knowledgeable health decisions.\n\nThe National Library of Medicine defines health literacy as:\n\nBased on this clinical definition, health literacy gives\nindividuals the skills that they need to both understand and effectively\ncommunicate information and concerns. Bridging that gap between literacy skills\nand the ability of the individual in health contexts, the \"Health Literacy Framework\" highlights the health outcomes and costs\nassociated with health contexts including cognitive abilities, social skills,\nemotional state, and physical conditions such as visual and auditory contributions.\n\n\"Potential Intervention Points\" are illustrated in reflection of the \"Health Literacy Framework.\"\nWhile these potential intervention points include interactions such as those of individuals and the education systems\nthat they are engaged with, their health systems, and societal factors as they\nrelate to health literacy, these points are not \ncomponents of a causal model. The three\npotential intervention points are culture and society, the health system, and\nthe education system. Health outcomes and costs are the products of the health\nliteracy developed during diversity of exposure to these three potential intervention\npoints.\n\nReferring to shared ideas, meanings, and values that\ninfluence an individual's beliefs and attitudes, cultural and societal\ninfluences are a significant intervention point for health literacy\ndevelopment. As interactions with healthcare systems often first occur at the\nfamily level, deeply rooted beliefs and values can shape the significance of\nthe experience. Included components that reflect the development of health\nliteracy both culturally and societally are native language, socioeconomic status,\ngender, race, and ethnicity, as well as mass media exposure. These\nare pathways to understanding American life paralleling conquests for a health\nliterate America.\n\nThe health system is an intervention point in the \"Health Literacy Framework\". For the\npurposes of this framework, health literacy refers to an individual's\ninteraction with people performing health-related activities in settings such\nas hospitals, clinics, physician's offices, home health care, public health agencies,\nand insurers.\n\nIn the United States, the education system consists of K-12 curricula. In addition to this standard educational\nsetting, adult education programs are also environments in which individuals\ncan develop traditional literacy skills founded in comprehension and real-world\napplication of knowledge via reading and writing. Tools for educational development provided by\nthese systems impact an individual's capacity to obtain specific knowledge\nregarding health. Reflecting components of traditional literacy such as\ncultural and conceptual knowledge, oral literacy (listening and speaking,)\nprint literacy (reading and writing,) and numeracy, education systems are also\npotential intervention points for health literacy development.\n\nA successful health literacy program will have many goals that all work together to improve health literacy. Many people assume these goals should communicate health information to the general public, however in order to be successful the goals should not only communicate with people but also take into account social and environmental factors that influence lifestyle choices. A good example of this is the movement to end smoking. When a health literacy program is put into place where only the negative side effects of smoking are told to the general public it is doomed to fail. However, when there is a larger program put in – one that includes strategies outlining how to quit smoking, raises tobacco prices, reduces access to tobacco by minors, and reflect social a social unacceptability of smoking – it will be much more effective.\n\nThe U.S. Department of Health and Human Services suggests a National Action Plan to implement a comprehensive Health Literacy Program. They include 7 goals:\nThese goals should be taken into account when implementing a health literacy program.\n\nThere are also goals for the outcomes of a Health Literacy Program.\n\n\"Health Related Goals\"\n\nLibraries have increasingly recognised that they can play a role in health literacy since the 2000s, influenced by the Medical Library Association. Library initiatives have included running education programs, fostering partnerships with health organisations, and using outreach efforts.\n\n\n\n"}
{"id": "59004987", "url": "https://en.wikipedia.org/wiki?curid=59004987", "title": "Holistic Lifelong Learning", "text": "Holistic Lifelong Learning\n\nHolistic Lifelong Learning is a traditional approach to healing which has been practiced by some of the indigenous nations in North America in particular Métis Nations of Canada. It observes a \"Tree Model\" of lifelong learning. The aim is to restore balance in the four realms of the physical, mental, spiritual, and emotional health of an individual acting as a person, as well as a member of a family, community and nation.\n\n"}
{"id": "46328497", "url": "https://en.wikipedia.org/wiki?curid=46328497", "title": "Home health care software", "text": "Home health care software\n\nHome health care software sometimes referred to as home care software or home health software falls under the broad category of health care information technology (HIT). HIT is “the application of information processing involving both computer hardware and software that deals with the storage, retrieval, sharing, and use of health care information, data, and knowledge for communication and decision making” Home health software is designed specifically for companies employing home health providers, as well as government entities who track payments to home health care providers.\n\nThe first use of home health care software was in the 1990s, with companies making software based on the Omaha system of software. The first software was made available for public health departments, nurse-managed centers, and community clinics. The use of home health care software increased with the technologies of cloud computing, telehealth and business analytical tools. Implementing these technologies with home health software is attributed to the enhancement of quality care at home, and reduction of fraud. During the early stages of 2000s, the home health care software industry expanded from simple databases to agencies being able to transmit electronic health records. There was also an increase in the available types of software as a result of software vendors working directly with health care providers.\n\nThere are clinical and non-clinical applications of home health care software. Including types such as agency software, hospice solutions, clinical management systems, telehealth solutions, and electronic visit verification. Depending on the type of software used, companies can track health care employee visits to patients, verify payroll, and document patient care. Governments can also use home health care software to verify visits from providers who bill them for services. Use of some software is mandated by government agencies such as OASIS assessment information that must be transmitted electronically by home health care providers.\n\nAgency software is used by home health care providers for office use and is a subset of medical practice management software used by inpatient clinics and doctors offices. Agency software is used for billing, paying vendors, staff scheduling, and maintaining records associated with the business. Agency software can be standalone or part of software packages, that include electronic visit verification to track hours of employees and time spent on home visits and patient care. Agency software can be purchased or leased through various vendors.\n\nElectronic visit verification (often referred to as EVV) is a method used to verify home healthcare visits to ensure patients are not neglected and to cut down on fraudulently documented home visits. EVV monitors locations of caregivers, and is mandated by certain states, including Texas and Illinois. Other states do not mandate it, but use it as part of its Medicaid fraud oversight, created by the passing of the Affordable Care Act in 2010. It is also widely used by employers of home healthcare providers to verify employee's locations and hours of work as well as document patient care.\n\nHome health care providers that participate in Medicaid are required to report specific data about patient care known as Outcome and Assessment Information Set-C (OASIS-C). Data includes health status, functional status, and support system information. The data is used to establish a measurement of patient home health care options. Home health care software allows health care providers to obtain and transmit such data while on location with a patient. Data collection is mandated by the Centers for Medicare and Medicaid Services, a division of the United States Department of Health and Human Services. Software for collecting and transmitting data is free through CMM and can also be purchased through private vendors as an add-on to other home health care software.\n\nThe software is hosted on servers located and maintained on-site at the agency. The localization of the data provides the agency direct access to the computers hosting their software and may also require the agency to maintain the technology that hosts the software.\n\nThe software is deployed on a cloud system maintained by the software vendor. The off-site maintenance of the cloud system results in a lower cost to the home health agency and provides access to the agency's data from off-site locations.\n\n"}
{"id": "31094752", "url": "https://en.wikipedia.org/wiki?curid=31094752", "title": "Hypersexual disorder", "text": "Hypersexual disorder\n\nHypersexual disorder is a pattern of behavior involving intense preoccupation with sexual fantasies, urges and activities, leading to adverse consequences and clinically significant distress or impairment in social, occupational or other important functions. It was proposed in 2010 for inclusion in the Diagnostic and Statistical Manual of Mental Disorders Fifth Edition (DSM-5) of the American Psychiatric Association (APA). \n\nPeople with hypersexual disorder experience multiple, unsuccessful attempts to control or diminish the amount of time spent engaging in sexual fantasies, urges, and behaviors in response to dysphoric mood states or stressful life events. \n\nFor a valid diagnosis of hypersexual disorder to be established, symptoms must persist for a period of at least 6 months and occur independently of a use mania or a medical condition.\n\nHypersexual disorder was recommended for inclusion in the DSM-5 (Diagnostic and Statistical Manual of Mental Disorders, Fifth Edition) by the Sexual and Gender Identity Disorders Workgroup (Emerging Measures and Models, Conditions for Further Study). It was ultimately not approved. The term \"hypersexual disorder\" was reportedly chosen because it did not imply any specific theory for the causes of hypersexuality, which remain unknown. A proposal to add sexual addiction to the DSM system had been previously rejected by the APA, as not enough evidence suggested to them that the condition is analogous to substance addictions, as that name would imply.\n\nRory Reid, a research psychologist in the Department of Psychiatry at the University of California Los Angeles (UCLA), led a team of researchers to investigate the proposed criteria for Hypersexual Disorder. Their findings were published in the \"Journal of Sexual Medicine\" where they concluded that the given criteria is valid and the disorder could be reliably diagnosed. \n\nThe DSM-IV-TR, published in 2000, includes an entry called \"Sexual Disorder—Not Otherwise Specified\" (Sexual Disorder NOS), for disorders that are clinically significant but do not have code. The DSM-IV-TR notes that Sexual Disorder NOS would apply to, among other conditions, \"distress about a pattern of repeated sexual relationships involving a succession of lovers who are experienced by the individual only as things to be used\".\n\n"}
{"id": "52822695", "url": "https://en.wikipedia.org/wiki?curid=52822695", "title": "Infection rate", "text": "Infection rate\n\nAn infection rate (or incident rate) is the probability or risk of an infection in a population. It is used to measure the frequency of occurrence of new instances of infection within a population during a specific time period.\n\nThe number of infections equals the cases identified in the study or observed. An example might by HIV infection during a specific time period in the defined population. The population at risk are the cases appearing in the population during the same time period. An example would be all the people in a city during a specific time period. The constant, or K is assigned a value of 100 to represent a percentage. An example would be to find the percentage of people in a city who are infected with HIV: 6,000 cases in March divided by the population of a city (one million) multiplied by the constant (K) would give an infection rate of 0.6%.\nCalculating the infection rate is used to analyze trends for the purpose of infection and disease control.\n\nAn online infection rate calculator has been developed by the Centers for Disease Control and Prevention that allows the determination of the Streptococcal A infection rate in a population.\n\nHealth care facilities routinely track their infection rates according to the guidelines issued by the Joint Commission. The healthcare-associated infection (HAI) rates measure infection of patients in a particular hospital. This allows rates to compared with other hospitals. These infections can often be prevented when healthcare facilities follow guidelines for safe care. To get payment from Medicare, hospitals are required to report data about some infections to the Centers for Disease Control and Prevention’s (CDC’s) National Healthcare Safety Network (NHSN). Hospitals currently submit information on central line-associated bloodstream infections (CLABSIs), catheter-associated urinary tract infections (CAUTIs), surgical site infections (SSIs), MRSA Bacteremia, and \"C. difficile\" laboratory-identified events. The public reporting of these data is an effort by the Department of Health and Human Services.\n\nFor meaningful comparisons of infection rates, populations must be very similar between the two or more assessments. However, a problem with mean rates is that they cannot reflect differences in risk between populations,\n\n"}
{"id": "5816458", "url": "https://en.wikipedia.org/wiki?curid=5816458", "title": "Insulin index", "text": "Insulin index\n\nThe insulin index of a food represents how much it elevates the concentration of insulin in the blood during the two-hour period after the food is ingested. The index is similar to the glycemic index (GI) and glycemic load (GL), but rather than relying on blood glucose levels, the Insulin Index is based upon blood insulin levels. The Insulin Index represents a comparison of food portions with equal overall caloric content (250 kcal or 1000 kJ), while GI represents a comparison of portions with equal digestible carbohydrate content (typically 50 g) and the GL represents portions of a typical serving size for various foods. The Insulin Index can be more useful than either the glycemic index or the glycemic load because certain foods (e.g., lean meats and proteins) cause an insulin response despite there being no carbohydrates present, and some foods cause a disproportionate insulin response relative to their carbohydrate load.\n\nHolt et al. have noted that the glucose and insulin scores of most foods are highly correlated, but high-protein foods and bakery products that are rich in fat and refined carbohydrates \"elicit insulin responses that were disproportionately higher than their glycemic responses.\" They also conclude that insulin indices may be useful for dietary management and avoidance of non-insulin-dependent diabetes mellitus and hyperlipidemia.\n\nThe Insulin Index is not the same as a glycemic index (GI), which is based exclusively on the digestible carbohydrate content of a food, and represents a comparison of foods in amounts with equal digestible carbohydrate content (typically 50 g). The insulin index compares foods in amounts with equal overall caloric content (250 kcal or 1000 kJ). Insulin indexes are scaled relative to white bread, while glycemic index scores nowadays are usually scaled with respect to pure glucose, although in the past white bread has been a reference point for GI measurements as well.\nIn the chart below, glycemic and insulin scores show the increase in the blood concentration of each. A higher satiety score indicates how much less was eaten from a buffet after participants ate the listed food.\n\nGlucose (glycemic) and insulin scores were determined by feeding 1000 kilojoules (239 kilocalories) of the food to the participants and recording the area under the glucose/insulin curve for 120 minutes then dividing by the area under the glucose/insulin curve for white bread. The result being that all scores are relative to white bread. The satiety score was determined by comparing how satiated participants felt within two hours after being fed a fixed number of calories (240 kilocalories) of a particular food while blindfolded (to ensure food appearance was not a factor), then dividing that number by how satiated the participants felt after eating white bread.\nWhite bread serves as the baseline of 100. In other words, foods scoring higher than 100 are more satisfying than white bread and those under 100 are less satisfying. The satiety score was negatively correlated to the amount eaten by participants at a subsequent buffet.\n\n± indicate uncertainty in the data. For example 60 ± 12 means that there's a 95% chance the score is between 60-12 (48) and 60+12 (72), 60 being the highest probability assuming a bell curve. In practice this means that if two foods have large uncertainty and have values close together then you don't really know which score is the higher.\n\n\n\n"}
{"id": "40136014", "url": "https://en.wikipedia.org/wiki?curid=40136014", "title": "Invoke Image Display", "text": "Invoke Image Display\n\nInvoke Image Display (IID) is an IHE Integration Profile that simplifies the task of integrating non-image-aware systems like EHRs, EMRs, PHRs, RIS and other information systems with image-aware systems like PACS, VNA and Image Sharing solutions, by providing a standard mechanism to request that imaging studies be displayed.\n\nIID defines a simple HTTP GET request that contains in the URL either a patient identifier or a list of study identifiers, and a small set of additional parameters to dictate display behavior.\n\nThe profile specifies requirements for the display to provide interactive viewing of a complete set of diagnostic images, if requested. The interactivity required includes windowing, zooming and panning as well as image navigation.\n\nSecurity is addressed through normal HTTP mechanisms.\n\nThe display can take the form of a browser based viewer, a separate applet, plugin or thick client based viewer or even a separate physical machine; the invocation remains the same and is agnostic to the viewer implementation mechanism.\n\nThe profile is defined as part of the IHE Radiology Technical Framework.\n\n\n"}
{"id": "19025616", "url": "https://en.wikipedia.org/wiki?curid=19025616", "title": "John E. Fogarty International Center", "text": "John E. Fogarty International Center\n\nThe John E. Fogarty International Center was founded in 1968 by US President Lyndon Johnson at the National Institutes of Health to support international medical and behavioral research and to train international researchers.\n\nOn July 1, 1968 President Lyndon Johnson issued an Executive Order establishing the John E. Fogarty International Center for Advanced Study in the Health Sciences at the National Institutes of Health (NIH) in order to support international medical and behavioral research and to train international researchers..\n\nIn March 2017, the Trump Administration proposed cuts to the NIH budget, including elimination of the Fogarty Center, saving $69 million.\n"}
{"id": "46310601", "url": "https://en.wikipedia.org/wiki?curid=46310601", "title": "LTIFR", "text": "LTIFR\n\nLTIFR refers to Lost Time Injury Frequency Rate, the number of lost time injuries occurring in a workplace per 1 million hours worked. An LTIFR of 7, for example, shows that 7 lost time injuries occur on a jobsite every 1 million hours worked. The formula gives a picture of how safe a workplace is for its workers.\n\nLost time injuries (LTI) include all on-the-job injuries that require a person to stay away from work more than 24 hours, or which result in death or permanent disability. This definition comes from the Australian standard 1885.1– 1990 Workplace Injury and Disease Recording Standard.\n"}
{"id": "2076669", "url": "https://en.wikipedia.org/wiki?curid=2076669", "title": "Leukorrhea", "text": "Leukorrhea\n\nLeukorrhea or (leucorrhoea British English) is a thick, whitish or yellowish vaginal discharge. There are many causes of leukorrhea, the usual one being estrogen imbalance. The amount of discharge may increase due to vaginal infection or STDs, and it may disappear and reappear from time to time. This discharge can keep occurring for years, in which case it becomes more yellow and foul-smelling. It is usually a non-pathological symptom secondary to inflammatory conditions of vagina or cervix.\n\nLeukorrhea can be confirmed by finding >10 WBC under a microscope when examining vaginal fluid.\n\nVaginal discharge is not abnormal, and causes of change in discharge include infection, malignancy, and hormonal changes. It sometimes occurs before a girl has her first period, and is considered a sign of puberty.\n\nIt is not a major issue but is to be resolved as soon as possible. It can be a natural defense mechanism that the vagina uses to maintain its chemical balance, as well as to preserve the flexibility of the vaginal tissue. The term \"physiologic leukorrhea\" is used to refer to leukorrhea due to estrogen stimulation.\n\nLeukorrhea may occur normally during pregnancy. This is caused by increased bloodflow to the vagina due to increased estrogen. Female infants may have leukorrhea for a short time after birth due to their in-uterine exposure to estrogen.\n\nLeukorrhea can also be caused by sexual stimulation.\n\nIt may also result from inflammation or congestion of the vaginal mucosa. In cases where it is yellowish or gives off an odor, a doctor should be consulted since it could be a sign of several disease processes, including an organic bacterial infection (aerobic vaginitis) or STD.\n\nAfter delivery, leukorrhea accompanied by backache and foul-smelling lochia (post-partum vaginal discharge, containing blood, mucus, and placental tissue) may suggest the failure of involution (the uterus returning to pre-pregnancy size) due to infection. A number of investigation such as wet smear, Gram stain, culture, pap smear and biopsy are suggested to diagnose the condition.\n\nLeukorrhea is also caused by trichomonads, a group of parasitic protozoan, specifically \"Trichomonas vaginalis\". Common symptoms of this disease are burning sensation, itching and discharge of frothy substance, thick, white or yellow mucous.\n\nLeukorrhea may be caused by sexually transmitted diseases; therefore, treating the STD will help treat the leukorrhea.\n\nTreatment may include antibiotics, such as metronidazole. Other antibiotics common for the treatment of STDs include clindamycin or trinidazole.\n\nThe word leukorrhea comes from Greek λευκός (leukós, “white”) + ῥοία (rhoía, “flow, flux”). In Latin leukorrhea is \"fluor albus\".\n\n"}
{"id": "20009077", "url": "https://en.wikipedia.org/wiki?curid=20009077", "title": "List of countries by ecological footprint", "text": "List of countries by ecological footprint\n\nThis is a list of countries by ecological footprint. The table is based on data spanning from 1961 to 2013 from the\nGlobal Footprint Network's National Footprint Accounts published in 2016. Numbers are given in global hectares per capita. The\nworld-average ecological footprint in 2012 was 2.84 global hectares per person (22.1 billion in total). With a world-average biocapacity of 1.73 global hectares (gha) per person (9.2 billion in total), this leads to a global ecological deficit of 1.1 global hectares per person (7.8 billion in total).\n\nFor humanity, having a footprint smaller than the planet's biocapacity is a necessary condition for sustainability. After all, ecological overuse is only possible temporarily. A country that consumes more, on average, than 1.73 gha per person has a resource demand that is not replicable world-wide. Vice versa, with a footprint below 1.73 gha per person, is not necessarily sustainable, because the quality of the footprint may still lead to ecological destruction. If a country does not have enough ecological resources within its own territory to cover its population's footprint, then it runs an ecological deficit and the country is termed an \"ecological debtor\". Otherwise, it has an ecological reserve and it is called a \"creditor\".\n\n\" This table below is based on 2012 results (National Footprint Accounts edition 2016). The latest edition (2018) is available on Global Footprint Network's website at http://data.footprintnetwork.org. Note that this list contains only 188 countries, covering most of the countries with more than one million inhabitants.\"\n\n"}
{"id": "56656329", "url": "https://en.wikipedia.org/wiki?curid=56656329", "title": "List of vaginal tumors", "text": "List of vaginal tumors\n\nVaginal tumors are neoplasms (tumors) found in the vagina. They can be benign or malignant. A neoplasm is an abnormal growth of tissue that usually forms a tissue mass.\nVaginal neoplasms may be solid, cystic or of mixed type.\n\nVaginal cancers arise from vaginal tissue, with vaginal sarcomas develop from bone, cartilage, fat, muscle, blood vessels or other connective or supportive tissue. Tumors in the vagina may also be metastases (malignant tissue that has spread to the vagina from other parts of the body).\nSome neoplastic growths of the vagina are sufficiently rare as to be only described in case studies.\n\nSigns and symptoms may include a feeling of pressure, painful intercourse or bleeding. Most vaginal tumors are located during a pelvic exam. Ultrasonography, CT and MRI imaging is used to establish the location and presence or absence of fluid in a tumor. Biopsy provides a more definitive diagnosis.\n\n\nVaginal tumors also can be found in domesticated animals:\n\n\n"}
{"id": "8977613", "url": "https://en.wikipedia.org/wiki?curid=8977613", "title": "Mad Pride", "text": "Mad Pride\n\nMad Pride is a mass movement of the users of mental health services, former users, and the aligned, and that individuals with mental illness should be proud of their 'mad' identity. It was formed in 1993 in response to local community prejudices towards people with a psychiatric history living in boarding homes in the Parkdale area of Toronto, Ontario, Canada, and an event has been held every year since then in the city except for 1996. A similar movement began around the same time in the United Kingdom. By the late 1990s similar events were being organized under the Mad Pride name around the globe, including Australia, Ireland, Portugal, Brazil, Madagascar, South Africa and the United States. Events draw thousands of participants, according to MindFreedom International, a United States mental health advocacy organization that promotes and tracks events spawned by the movement.\n\nMad Pride activists seek to reclaim terms such as \"mad\", \"nutter\", and \"psycho\" from misuse, such as in tabloid newspapers and in order to switch it from a negative view into a positive view. Through a series of mass media campaigns, Mad Pride activists seek to re-educate the general public on such subjects as the causes of mental disabilities, the experiences of those using the mental health system, and the global suicide pandemic. One of Mad Pride's founding activists in the UK was Pete Shaughnessy, who later died by suicide. Mark Roberts, Robert Dellar (who died in 2016) and Simon Barnet were among the other founders of the movement. \"Mad Pride: A celebration of mad culture\" records the early Mad Pride movement. \"On Our Own: Patient-Controlled Alternatives to the Mental Health System\", published in 1978 by Judi Chamberlin, is a foundational text in the Mad Pride movement, although it was published before the movement was launched.\n\nThe first known event, specifically organized as a Pride event by people who identified as survivors, consumers or ex-patients of psychiatric practices, was held on 18 September 1993, when it was called \"Psychiatric Survivor Pride Day\".\n\nMad Studies grew out of Mad Pride and survivor thinking, and focuses on developing scholarly thinking around \"mental health\" by academics who self-identify as mad. As noted in \"Mad matters: a critical reader in Canadian mad studies\" (LeFrançois, Menzies and Reaume, 2013), \"Mad Studies can be defined in general terms as a project of inquiry, knowledge production, and political action devoted to the critique and transcendence of psy-centred ways of thinking, behaving, relating, and being\". As a book, Mad Matters' offers a critical discussion of mental health and madness in ways that demonstrate the struggles, oppression, resistance, agency and perspectives of Mad people to challenge dominant understandings of ‘mental illness’\". \"\"Mad Studies\" is a growing, evolving, multi-voiced and interdisciplinary field of activism, theory, praxis and scholarship.\"\n\nMad Pride was launched alongside a book of the same name, \"Mad Pride: A celebration of mad culture\", published in 2000. On May 11, 2008, Gabrielle Glaser documented Mad Pride in \"The New York Times\". Glaser stated, \"Just as gay-rights activists reclaimed the word queer as a badge of honor rather than a slur, these advocates proudly call themselves mad; they say their conditions do not preclude them from productive lives.\"\n\nElizabeth Packard (1816-1897) was deemed insane by her husband as she did not agree with his conservative political views. In Illinois at the time, involuntary admission to an asylum did not require a public hearing so long as it was a husband admitting his wife. Due to this, Packard was institutionalized though she saw herself to be sane. In Packard’s lifetime to be labeled as ‘mad’ was a form of social disapproval. However, she felt solidarity among Mad people due to her experience in the institution. Though she did not personally identify as Mad and had to identify as ‘sane’ in order to be an activist, it is here that we see early forms of organizing from ex-patients.\n\nThe Mad Pride movement has spawned recurring cultural events in Toronto, London, Paris and other cities around the world. These events often include music, poetry readings, film screenings, and street theatre, such as \"bed push\" protests, which aim to raise awareness about the poor levels of choice of treatments and the widespread use of force in psychiatric hospitals. Past events have included British journalist Jonathan Freedland, and popular novelist Clare Allan. Mad Pride cultural events take a variety of forms, such as the South London collective Creative Routes, the Chipmunka Publishing enterprise, and the many works of Dolly Sen.\n\nA Bed Push is a method of activism employed by multiple mental health agencies and advocates as a method of raising awareness about psychiatric care. Activists wheel a gurney through public spaces to provoke discussion about mental health care. Mind Freedom has a recipe for a successful Bed Push on their website, urging participants to remain peaceful but also be seen by blowing horns, slightly disrupting traffic and playing music. Often patients in psychiatric care feel silenced and powerless, showing resilience in the face of that and securing visibility is a method of regaining dignity.\n\nMad Pride Week in Toronto is proclaimed as such by the city itself. The festivities surrounding this week are highlighted by the Mad Pride Bed Push, typically on the 14th of July. The event takes place Toronto’s Queen Street West “to raise public awareness about the use of force and lack of choice for people ensnared in the Ontario mental health system” This week is officially run by Toronto Mad Pride which partners a number of mental health agencies in the city. In recent years, some advocates have pushed for Parkdale, Toronto to be renamed MAD! Village, to reclaim pride in its surrounding communities' long history of struggle with mental health and addictions \n\nA series of bed push events take place around London each year.\n\nThe Psychiatric Patient Built Wall Tours take place in Toronto, ON at the CAMH facility on Queen St West. The tours show the patient built walls from the 19th century that are located at present day CAMH. The purpose of the tours is to give a history on the lives of the patients who built the walls, and bring attention to the harsh realities of psychiatry.\n\nGeoffrey Reaume and Heinz Klein first came up with the idea of walking tours as part of a Mad Pride event in 2000. The first wall tour occurred on what is now known as Mad Pride Day, on July 14, 2000, with an attendance of about fifty people. Reaume solely leads the tours, and they have grown from annual events for Mad Pride, to occurring several times throughout the year in all non-winter months.\n\n\n"}
{"id": "1294685", "url": "https://en.wikipedia.org/wiki?curid=1294685", "title": "Medical guideline", "text": "Medical guideline\n\nA medical guideline (also called a clinical guideline or clinical practice line) is a document with the aim of guiding decisions and criteria regarding diagnosis, management, and treatment in specific areas of healthcare. Such documents have been in use for thousands of years during the entire history of medicine. However, in contrast to previous approaches, which were often based on tradition or authority, modern medical guidelines are based on an examination of current evidence within the paradigm of evidence-based medicine. They usually include summarized consensus statements on best practice in healthcare. A healthcare provider is obliged to know the medical guidelines of his or her profession, and has to decide whether to follow the recommendations of a guideline for an individual treatment.\n\nModern clinical guidelines identify, summarize and evaluate the highest quality evidence and most current data about prevention, diagnosis, prognosis, therapy including dosage of medications, risk/benefit and cost-effectiveness. Then they define the most important questions related to clinical practice and identify all possible decision options and their outcomes. Some guidelines contain decision or computation algorithms to be followed. Thus, they integrate the identified decision points and respective courses of action with the clinical judgment and experience of practitioners. Many guidelines place the treatment alternatives into classes to help providers in deciding which treatment to use.\n\nAdditional objectives of clinical guidelines are to standardize medical care, to raise quality of care, to reduce several kinds of risk (to the patient, to the healthcare provider, to medical insurers and health plans) and to achieve the best balance between cost and medical parameters such as effectiveness, specificity, sensitivity, resolutiveness, etc. It has been demonstrated repeatedly that the use of guidelines by healthcare providers such as hospitals is an effective way of achieving the objectives listed above, although they are not the only ones.\n\nGuidelines are usually produced at national or international levels by medical associations or governmental bodies, such as the United States Agency for Healthcare Research and Quality. Local healthcare providers may produce their own set of guidelines or adapt them from existing top-level guidelines.\n\nSpecial computer software packages known as guideline execution engines have been developed to facilitate the use of medical guidelines in concert with an electronic medical record system. \nThe Guideline Interchange Format (GLIF) is a computer representation format for clinical guidelines that can be used with such engines.\n\nThe USA and other countries maintain medical guideline clearinghouses. In the USA, the National Guideline Clearinghouse maintains a catalog of high-quality guidelines published by various health and medical associations. In the United Kingdom, clinical practice guidelines are published primarily by the National Institute for Health and Care Excellence (NICE). In The Netherlands, two bodies—the (CBO) and (NHG)—have published specialist and primary care guidelines, respectively. In Germany, the German Agency for Quality in Medicine (ÄZQ) coordinates a national program for disease management guidelines. All these organisations are now members of the Guidelines International Network (G-I-N), an international network of organisations and individuals involved in clinical practice guidelines.\n\nChecklists have been used in medical practice to attempt to ensure that clinical practice guidelines are followed. An example is the Surgical Safety Checklist developed for the World Health Organization by Dr. Atul Gawande. According to a meta-analysis after introduction of the checklist mortality dropped by 23% and all complications by 40%, but further high-quality studies are required to make the meta-analysis more robust. In the UK, a study on the implementation of a checklist for provision of medical care to elderly patients admitting to hospital found that the checklist highlighted limitations with frailty assessment in acute care and motivated teams to review routine practices, but that work is needed to understand whether and how checklists can be embedded in complex multidisciplinary care.\n\nGuidelines may lose their clinical relevance as they age and newer research emerges. Even 20% of strong recommendations, especially when based on opinion rather than trials, from practice guidelines may be retracted.\n\n\"The New York Times\" reported in 2004 that some simple clinical practice guidelines are not routinely followed to the extent they might be. It has been found that providing a nurse or other medical assistant with a checklist of recommended procedures can result in the attending physician being reminded in a timely manner regarding procedures that might have been overlooked.\n\nGuidelines may have both methodological problems and conflict of interest. As such, the quality of guidelines may vary substantially, especially for guidelines that are published on-line and have not had to follow methodological reporting standards often required by reputable clearinghouses.\n\nGuidelines may make recommendations that are stronger than the supporting evidence.\n\nIn response to many of these problems with traditional guidelines, the BMJ created a new series of trustworthy guidelines focused on the most pressing medical issues called \"BMJ\" Rapid Recommendations.\n\n\n\n"}
{"id": "361024", "url": "https://en.wikipedia.org/wiki?curid=361024", "title": "Men's rights movement", "text": "Men's rights movement\n\nThe men's rights movement (MRM) is a part of the larger men's movement. It branched off from the men's liberation movement in the early 1970s. The men's rights movement is notably anti-feminist and made up of a variety of groups and individuals who focus on numerous social issues (including family law, parenting, reproduction, domestic violence against men and opposition to circumcision) and government services (including education, compulsory military service, social safety nets, and health policies), which men's rights advocates say discriminate against men.\n\nScholars have described the men's rights movement or parts of the movement as a backlash against feminism.\n\nClaims and activities associated with the men's rights movement have been criticized and labeled hateful and violent by the Southern Poverty Law Center and Political Research Associates. In 2018, the Southern Poverty Law Center categorized some men's rights groups as being part of a hate ideology under the umbrella of patriarchy and male supremacy. The movement and sectors of the movement have been described as misogynistic, and the perceived disadvantage some men feel is argued as often being due to loss of entitlement and privilege.\n\nThe term \"men's rights\" was used at least as early as February 1856 when it appeared in \"Putnam's Magazine\".\n\nThree loosely connected men's rights organizations formed in Austria in the interwar period. The \"League for Men's Rights\" was founded in 1926 with the goal of \"combatting all excesses of women's emancipation\". In 1927, the \"Justitia League for Family Law Reform\" and the \"Aequitas World's League for the Rights of Men\" split from the \"League of Men's Rights\". The three men's rights groups opposed women's entry into the labor market and what they saw as the corrosive influence of the women's movement on social and legal institutions. They criticized marriage and family laws, especially the requirement to pay spousal and child support to former wives and illegitimate children, and supported the use of blood tests to determine paternity. \"Justitia\" and \"Aequitas\" issued their own short-lived journals \"Men's Rightists Newspaper\" and \"Self-Defense\" where they expressed their views that were heavily influenced by the works of Heinrich Schurtz, Otto Weininger, and Jörg Lanz von Liebenfels. The organizations ceased to exist before 1939.\n\nThe modern men's rights movement emerged from the men's liberation movement, which appeared in the first half of the 1970s when scholars began to study feminist ideas and politics. The men's liberation movement acknowledged men's institutionalized power while critically examining the consequences of hegemonic masculinity. In the late 1970s, the men's liberation movement split into two separate strands with opposing views: the pro-feminist men's movement and the anti-feminist men's rights movement. Men's rights activists have rejected feminist principles and focused on areas in which they believe men are disadvantaged, oppressed, or discriminated against. Masculinities studies scholar Michael Kimmel notes that their critiques of gender roles \"morphed into a celebration of all things masculine and a near infatuation with the traditional masculine role itself.\"\n\nIn the 1980s and 1990s, men's rights activists opposed societal changes sought by feminists and defended the patriarchal gender order in the family, schools and the workplace. Some men's rights activists view men as an oppressed group and believe that society and men have been \"feminized\" by the women's movement. Sarah Maddison, an Australian author, has said that Warren Farrell and Herb Goldberg \"argue that, for most men, power is an illusion, and that women are the true power holders in society through their roles as the primary carers and nurturers of children\".\n\nOne of the first major men's rights organizations was the \"Coalition of American Divorce Reform Elements\", founded by Richard Doyle in 1971, from which the \"Men's Rights Association\" spun off in 1973. Free Men Inc. was founded in 1977 in Columbia, Maryland, spawning several chapters over the following years, which eventually merged to form the \"National Coalition of Free Men\" (now known as the National Coalition for Men). \"Men's Rights, Inc.\" was also formed in 1977. \"Fathers and Families\" was formed in 1994. In the United Kingdom, a men's rights group calling itself the \"UK Men's Movement\" began to organize in the early 1990s. The Save Indian Family Foundation (SIFF) was founded in 2005, and in 2010 claimed to have over 30,000 members.\n\nMen's rights groups have formed in some European countries during periods of shifts toward conservatism and policies supporting patriarchal family and gender relations. In the United States, the men's rights movement has ideological ties to neoconservatism. Men's rights activists have received lobbying support from conservative organizations and their arguments have been covered extensively in neoconservative media.\n\nThe men's rights movement has become more vocal and more organized since the development of the internet. The manosphere emerged and men's rights websites and forums have proliferated on the internet. Activists mostly organize online. The most popular men's rights site is A Voice for Men. Other sites dedicated to men's rights issues are the Fathers Rights Foundation, MGTOW (Men Going Their Own Way), and subreddits /r/MensRights. Men's rights proponents often use the red pill and blue pill metaphor from a scene in \"The Matrix\" to identify each other online and in reference to the moment they came to believe that men are oppressed. There tends to be much hostility between the different subgroups. Critics say that r/TheRedpill is a subreddit dedicated to men's rights. However, others from within the subreddit claim they focus on personal and interpersonal improvement. Some critics, outside the subreddit, say r/TheRedPill does not really care for the men’s rights movement and that MGTOW (Men Going Their Own Way) are men who have no patience for either /r/TheRedPill or men's rights.\n\nFringe political parties focusing on men's rights have been formed including, but not limited to, the Australian Non-Custodial Parents Party (Equal Parenting), the Israeli Man's Rights in the Family Party, and the Justice for Men and Boys party in the UK.\n\nMost men's rights activists in the United States are white, middle-class, heterosexual men. Prominent advocates include Warren Farrell, Herb Goldberg, Richard Doyle, and Asa Baber. Several women have emerged as leading voices of the MRM, including Helen Smith, Christina Hoff Sommers and Erin Pizzey.\n\nMany scholars consider the men's rights movement a backlash or countermovement to feminism. Bob Lingard and Peter Douglas suggest that the conservative wing of the men's rights movement, rather than the men's rights position in general, is an antifeminist backlash. Masculinities scholar Jonathan A. Allan described the men's rights movement as a reactionary movement that is defined by its opposition to women and feminism but has not yet formulated its own theories and methodologies outside of antifeminism. Scholar Michael Messner notes that the early men's rights movement \"appropriates the symmetrical language of sex roles\" first used by feminists, which implies a false balance of institutional power between men and women.\n\nThe men's rights movement generally incorporates points of view that reject feminist and profeminist ideas. Men's rights activists have said that they believe feminism has radicalized its objective and harmed men. They believe that rights have been taken away from men and that men are victims of feminism and 'feminizing' influences in society. They dispute that men as a group have institutional power and privilege and believe that men are victimized and disadvantaged relative to women. Men's rights groups generally reject the notion that feminism is interested in men's problems, and some men's rights activists have viewed the women's movement as a plot to deliberately conceal discrimination against men and promote gynocentrism.\n\nMen's rights proponents are concerned with a wide variety of matters, some of which have spawned their own groups or movements, such as the fathers' rights movement, concerned specifically with divorce and child custody issues. Some, if not all, men's rights issues stem from gender roles and, according to sociologist Allan Johnson, patriarchy.\n\nMen's rights activists seek to expand the rights of unwed fathers in case of their child's adoption. Warren Farrell argues that in failing to inform the father of a pregnancy, an expectant mother deprives an adopted child of a relationship with the biological father. He proposes that women be legally required to make every reasonable effort to notify the father of her pregnancy within four to five days. In response, philosopher James P. Sterba agrees that, for moral reasons, a woman should inform the father of the pregnancy and adoption, but this should not be imposed as a legal requirement as it might result in undue pressure, for example, to have an abortion.\n\nMen's rights organizations such as Save Indian Family Foundation (SIFF) say that women misuse legislation meant to protect them from dowry death and bride burnings. SIFF is a men's rights organization in India that focuses on abuse of anti-dowry laws against men. SIFF has campaigned to abolish Section 498A of the Indian Penal Code, which penalizes cruelty by husbands (and the husband's family) in pursuit of dowry or for driving a wife to suicide. SIFF states anti-dowry laws are regularly being abused to settle petty disputes in marriage and that they regularly receive calls from many men who allege their wives have used false dowry claims to imprison them.\n\nFamily law is an area of deep concern among men's rights groups. Men's rights adherents argue that the legal system and family courts discriminate against men, especially in regards to child custody after divorce. They believe that men do not have the same contact rights or equitable shared parenting rights as their ex-spouse and use statistics on custody awards as evidence of judicial bias against men. Men's rights advocates seek to change the legal climate for men through changes in family law, for example by lobbying for laws that make joint custody the default custody arrangement except in cases where one parent is unfit or unwilling to parent. They appropriated the feminist rhetoric of \"rights\" and \"equality\" in their discourse, framing child custody as a matter of basic civil rights as opposed to children's rights. Some men's rights activists argue that the lack of contact with their children makes fathers less willing to pay child support. Others cite parental alienation syndrome (PAS) as a reason to grant custody to fathers. Scholarly consensus has rejected PAS as an empirically-supported syndrome.\n\nFamily law scholars state that father's rights proponents fail to credit feminism as being the revolutionary social force to bring awareness of fathers' equally important parental roles. Scholars and critics assert that empirical research does not support the notion of a judicial bias against men and that men's rights advocates distort statistics in a way that ignores the fact that the majority of men do not seek custody. 90% of custody disputes are agreed upon without family court involvement and studies have found fair assessment in child custody decisions and that legal appointees are more likely to award custody to parents with interpersonally sensitive traits such as warmth and compassion regardless of gender. \n\nAcademics critique the rhetorical framing of custody decisions, stating that men's rights advocates appeal for \"equal rights\" without ever specifying the legal rights they believe have been violated. Scholars and critics assert that the men's rights rhetoric of children's \"needs\" that accompanies their plea for fathers' rights is merely to deflect criticism that they are motivated by self-interest and masks men's rights advocates' own claims. Deborah Rhode argues that contrary to the claims of some men's rights activists, research shows that joint legal custody does \"not\" increase the likelihood that fathers will pay child support or remain involved parents. Michael Flood argues that the father's rights movement seems to prioritize re-establishing paternal authority over actual involvement with the children, and that they prioritize formal principles of equality over positive parenting and well-being of the children.\n\nLundy Bancroft states that family courts repeatedly fail to protect battered women and their children by frequently granting joint or sole custody to abusive fathers.\n\nObservers have noted that the 'intactivist' movement, an anti-circumcision movement, has some overlap with the men's rights movement. Most men's rights activists object to routine neonatal circumcision and say that female genital mutilation has received more attention than male circumcision.\n\nThe controversy around non-consensual circumcision of children for non-therapeutic reasons is not exclusive to the men's rights movement, and involves concerns of feminists and medical ethics. Some doctors and academics have argued that circumcision is a violation of the right to health and bodily integrity, while others have disagreed.\n\nWarren Farrell claims there are criminal defenses that are only available to women. N. Quintin Woolf has argued that the over-representation of men as both murderers and the victims of murder is evidence that men are being harmed by outmoded cultural attitudes.\n\nMen's rights groups in the United States began organizing in opposition to divorce reform and custody issues around the 1960s. Up until this time, husbands held legal power and control over wives and children. The men involved in the early organization claimed that family and divorce law discriminated against them and favored their wives. Men's rights leader Rich Doyle likened divorce courts to slaughterhouses, considering their judgements unsympathetic and unreasonable.\n\nMen's rights adherents say that men are consciously or unconsciously opting out of marriage and engaging in a \"marriage strike\" as a result of a perceived lack of benefits in marriage, and the emotional and financial consequences of divorce, including alimony, child custody and support. Men's rights activists have argued that divorce and custody laws violate men's individual rights to equal protection. Law professor Gwendolyn Leachman writes that this sort of framing \"downplays the systemic biases that women face that justify protective divorce and custody laws\".\n\nAcross several countries (including the US and the UK), men file less than a third of opposite-sex divorce cases, and women file over two thirds.\n\nMen's rights groups describe domestic violence committed by women against men as a problem that goes ignored and under-reported, in part because men are reluctant to label themselves as victims. They say that women are as aggressive or more aggressive than men in relationships and that domestic violence is gender-symmetrical. They cite controversial family conflict research by Murray Straus and Richard Gelles as evidence of gender symmetry. Men's rights advocates argue that judicial systems too easily accept false allegations of domestic violence by women against male partners. Men's rights advocates have been critics of legal, policy and practical protections for abused women, campaigning for domestic violence shelters for battered men and for the legal system to be educated about women's violence against men.\n\nIn response to such claims, family violence scholar Richard Gelles published an article entitled \"Domestic Violence: Not An Even Playing Field\" and accused the men's rights movement of distorting his research findings on men's and women's violence to promote a misogynistic agenda. Domestic violence scholars and advocates have rejected the flawed research cited by men's rights activists and dispute their claims that such violence is gender symmetrical, suggesting that their focus on women's violence stems from a political agenda to minimize the severity of the problem of men's violence against women and children and to undermine services to abused women. Men's rights groups have lobbied to block state funding to battered women's shelters, despite the high rates of intimate partner femicide justifying greater funding.\n\nMen's rights adherents describe the education of boys as being in crisis, with boys having reduced educational achievement and motivation compared to girls. Advocates blame the influence of feminism on education for what they believe is discrimination against and systematic oppression of boys in the education system. They critique what they describe as the \"feminization\" of education, stating that the predominance of female teachers, a focus on girls' needs, as well as a curricula and assessment methods that supposedly favour girls, have proved repressive and restrictive to men and boys.\n\nAnother study has also found gender differences in academic achievement not being reliably linked to gender policies, and that female academic achievement is greater than boys' in 70% of studied countries around the world. However, this is contradicted in Australia by ACARA's independent NAPLAN findings that \"Since their introduction, a subtle but consistent pattern of gender differences in performance on the NAPLAN tests has emerged, with boys regularly outperforming girls in numeracy, and girls consistently outperforming boys in the reading, writing, spelling, and grammar and punctuation components\". To further add to the confusion with other evidence presented, there has been an obvious, unexplained and significant drop in the number of boys at university in most countries. Australia, for example, has seen a drop from 61% to 46% since 1974. Similar trends have been observed in the UK, which is thought to be influenced primarily by teacher policies and attitudes.\n\nMen's rights groups call for increased recognition of masculinity, greater numbers of male role models, more competitive sports, and the increased responsibilities for boys in the school setting. They have also advocated clearer school routines, more traditional school structures, including gender-segregated classrooms, and stricter discipline.\n\nCritics suggest that men's rights groups view boys as a homogeneous group sharing common experiences of schooling and that they fail to account for how responses to educational approaches may differ by age, disability, culture, ethnicity, sexuality, religion, and class.\n\nIn Australia, men's rights discourse has influenced government policy documents. Compared to Australia, less impact has been noted in the United Kingdom, where feminists have historically had less influence on educational policy. However, Mary Curnock Cook, the UCAs chief executive, argued that in Britain \"despite the clear evidence and despite the press coverage, there is a deafening policy silence on the issue. Has the women’s movement now become so normalised that we cannot conceive of needing to take positive action to secure equal education outcomes for boys?”\n\nThe men's rights movement rejects the concept that men are privileged relative to women. The movement is divided into two groups: those who consider men and women to be harmed equally by sexism, and those who view society as endorsing the degradation of men and upholding what they term \"female privilege\".\n\nConnor et al. have rejected the idea of female privilege, as research since the early 90s on ambivalent sexism across the world has found that these \"privileges\" are benevolent sexism, which contributes to women's oppression.\n\nMen's rights groups have called for male-majority governmental structures to address issues specific to men and boys including education, health, work and marriage. Men's rights groups in India have called for the creation of a Men's Welfare Ministry and a National Commission for Men, or for the abolition of the National Commission for Women. In the United Kingdom, the creation of a Minister for Men analogous to the existing Minister for Women, has been proposed by David Amess, MP and Lord Northbourne, but was rejected by the government headed by Prime Minister Tony Blair. In the United States, Warren Farrell heads a commission focused on the creation of a White House Council on Boys and Men as a counterpart to the White House Council on Women and Girls, which was formed in March 2009.\n\nMen's rights groups view the health issues faced by men and their shorter life spans compared to women globally, as evidence of discrimination and oppression. They claim that feminism has led to women's health issues being privileged at the expense of men's. They highlight certain disparities in funding of men's health issues as compared to women's, noting that, for example, prostate cancer research receives less funding than breast-cancer research. David Benatar has argued that putting more money into health research on males may reduce the disparity between men's and women's life expectancy. However, women and minorities had typically been excluded from medical research until the 1990s. Vivianna Simon notes, \"Most biomedical and clinical research has been based on the assumption that the male can serve as representative of the species.\" Medical scholars warn that such false assumptions are still prevalent. Contrary to antifeminist assertions, empirical findings suggest that gender bias against females remains the norm in medicine. Farrell argues that industrialization raised the stress level of men while lowering the stress-level of women by pulling men away from the home and the family, and pushing women closer to home and family. He cites this an explanation why men are more likely to die from all 15 leading causes of death than women at all ages. He argues that the U.S. government having an Office of Research on Women's Health but no Office of Research on Men's Health, along with the U.S. federal government spending twice as much money on Women's health, shows that society considers men more disposable than women.\n\nScholars have critiqued these claims, stating, as Michael Messner puts it, that the poorer health outcomes are the heavy costs paid by men \"for conformity with the narrow definitions of masculinity that promise to bring them status and privilege\" and that these costs fall disproportionately on men who are marginalized socially and economically. According to Michael Flood, men's health would best be improved by \"tackling destructive notions of manhood, an economic system which values profit and productivity over workers' health, and the ignorance of service providers\", instead of blaming a feminist health movement. Genevieve Creighton & John L Oliffe have stated that men engage in positive health practices, such as reducing fat intake and alcohol, to conform to positive masculine ideals. Some have argued that biology contributes to the life-expectancy gap. For example, it has been found that females consistently outlive males among primates. Eunuchs, castrated before puberty, have shown to live with varying differences, more than other males, pointing to testosterone levels playing a role in the life-expectancy gap. Luy M. and Gast K. found that the female-male life expectancy gap is primarily due to higher mortality rates among specific sub-populations of men (e.g., gay, trans, racial minorities). They therefore state that social programs should be narrowly targeted to those sub-populations, rather than to men as a whole.\n\nMen's rights advocates argue that homelessness is a gendered issue. In Britain, most homeless people are male. A 2018 study focused on three Pennsylvania emergency departments found little difference in the number of men and women who self-reported as homeless; however, the study did not claim to reflect the homeless population in the United States as a whole. For information on the homeless population of the United States as a whole, see Homelessness in the United States.\n\nMen's rights activists say differential prison terms for men and women are evidence of discrimination against men. Farrell cites evidence that men receive harsher prison sentences and are more likely sentenced to death in the United States. He believes society considers women to be naturally more innocent and credible, and criticizes battered woman and infanticide defenses. He also criticizes conditions in men's prisons and the lack of attention to prison male-to-male rape by authorities.\n\nMen's rights adherents have argued that sole military conscription of men is an example of discrimination against men.\n\nIn 1971, draft resisters in the United States initiated a class-action suit alleging that male-only conscription violated men's rights to equal protection under the US constitution. When the case, \"Rostker v. Goldberg\", reached the Supreme Court in 1981, they were supported by a men's rights group and multiple feminist groups, including the National Organization for Women. However, the Supreme Court upheld the Military Selective Service Act, stating that \"the argument for registering women was based on considerations of equity, but Congress was entitled, in the exercise of its constitutional powers, to focus on the question of military need, rather than 'equity'\". The 2016 decision by Defense Secretary Ash Carter to make all combat positions open to women relaunched debate over whether or not women should be required to register for the Selective Service System.\n\nMen's and fathers' rights groups have said that there are high levels of mistaken paternity or \"paternity fraud\", where men are parenting and/or financially supporting children who are not biologically their own. They hold biological views of fatherhood, emphasizing the imperative of the genetic foundation of paternity rather than social aspects of fatherhood. They state that men should not be forced to support children fathered by another man, and that men are harmed because a relationship is created between a man and non-biological children while denying the children and their biological father of that experience and knowledge of their genetic history. In addition, they say non-biological fathers are denied the resources to have their own biological children in another relationship.\n\nMen's rights activists support the use of nonconsensual paternity testing to reassure presumed fathers about the child's paternity; men's and fathers' rights groups have called for compulsory paternity testing of all children. They have campaigned vigorously in support of men who have been shown by genetic testing not to be the biological father, but who are nevertheless required to be financially responsible for them. Prompted by these concerns, legislators in certain jurisdictions have supported this biological view and have passed laws providing relief from child support payments when a man is proved not to be the father. Australian men's rights groups have opposed the recommendations of a report by the Australian Law Reform Commission and the National Health and Medical Research Council that would require the informed consent of both parents for paternity testing of young children, and laws that would make it illegal to obtain a sample for DNA testing without the individual's informed consent. Sociologist Michael Gilding asserts that men's rights activists have exaggerated the rate and extent of misattributed paternity, which he estimates at about 1–3%. He opposed unnecessary calls for mandatory paternity testing of all children.\n\nMen's rights activists are significantly concerned with false accusations of rape and sexual assault, and desire to protect men from the negative consequences of false accusations.\n\nMen's rights proponents believe that the naming of the accused while providing the accuser (victim) with anonymity encourages abuse of this kind. Men's rights advocates have also claimed that rape \"has been used as a scam\". Other international studies from Australia, Britain and the Federal Bureau of Investigation have found the percentage of estimated false or unsubstantiated rape allegations to be around 2% to 8%. Quoting research including those by Eugene Kanin and the U.S. Air Force, they assert that 40–50% or more of rape allegations may be false.\n\nTo argue the issue of false accusations of rape, the categories of 'false' and 'unsubstantiated' are often conflated, such as the National Coalition for Men citing reports like the 1996 FBI summary that finds a rate of 8% for unsubstantiated forcible rape, which is four times higher than the average for all index crimes as a whole. Experts emphasize that \"verified\" false allegations are a distinct category from unsubstantiated allegations, and conflating the two is fallacious. These figures are widely debated due to the questionable methodology and small sample sizes - see the False accusation of rape page for wider survey estimates.\n\nLegislation and judicial decisions criminalizing marital rape are opposed by some men's rights groups in the United Kingdom, the United States and India. The reasons for opposition include concerns about false allegations related to divorce proceedings, and the belief that sex within marriage is an irrevocable part of the institution of marriage. In India, there has been anxiety about relationships and the future of marriage that such laws have given women \"grossly disproportional rights\". Virag Dhulia of the Save Indian Family Foundation, a men's rights organization, has opposed recent efforts to criminalize marital rape in India, arguing that \"no relationship will work if these rules are enforced\".\n\nFeminist scholars Lise Gotell and Emily Dutton argue that content on the manosphere reveals anti-feminist pro-rape arguments, including that sexual violence is a gender-neutral problem, feminists are responsible for erasing men's experiences of victimization, false allegations are widespread, and that rape culture is a feminist-produced moral panic. They contend it is important to engage [this topic] as there is a real danger that MRA (Men's Rights Activism) claims could come to define the popular conversation about sexual violence.\n\nIn 2006, the American National Center for Men backed a lawsuit known as \"Dubay v. Wells\". The case concerned whether men should have the opportunity to decline all paternity rights and responsibilities in the event of an unplanned pregnancy. Supporters argued that this would allow the woman time to make an informed decision and give men the same reproductive rights as women. The case and the appeal were dismissed, with the U.S. Court of Appeals (Sixth Circuit) stating that neither parent has the right to sever their financial responsibilities for a child and that \"Dubay's claim that a man's right to disclaim fatherhood would be analogous to a woman's right to abortion rests upon a false analogy\".\n\nMen's rights groups argue that women are given superior social security and tax benefits than men. Warren Farrell states that men in the United States pay more into social security, but in total, women receive more in benefits, and that discrimination against men in insurance and pensions have gone unrecognized.\n\nMen's rights activists point to higher suicide rates in men compared to women. In the United States for example, the male-to-female suicide death ratio varies, approximately, between 3:1 and 10:1. However, studies have found an over-representation of women in attempted or incomplete suicides and men in complete suicide. This phenomenon, termed the \"gender paradox of suicide\", is argued to derive from a tendency for females to use less lethal methods and greater male access and use of lethal methods.\n\nThe men's rights movement has been criticized for exhibiting misogynistic tendencies. The Southern Poverty Law Center has stated that while some of the websites, blogs and forums related to the movement \"voice legitimate and sometimes disturbing complaints about the treatment of men, what is most remarkable is the misogynistic tone that pervades so many\". After further research into the movement, the SPLC elaborated: \"A thinly veiled desire for the domination of women and a conviction that the current system oppresses men in favor of women are the unifying tenets of the male supremacist worldview.\" Other studies have pointed towards men's rights groups in India trying to change or completely abolish important legal protections for women as a form of patriarchal anxiety as well as being hostile towards women.\n\nProfessor Ruth M. Mann of the University of Windsor in Canada suggests that men's rights groups fuel an international rhetoric of hatred and victimization by disseminating misinformation via online forums and websites containing constantly-updated \"diatribes against feminism, ex-wives, child support, shelters, and the family law and criminal justice systems\". According to Mann, these stories reignite their hatred and reinforce their beliefs that the system is biased against men and that feminism is responsible for a large scale and ongoing \"cover-up\" of men's victimization. Mann says that although existing legislation in Canada acknowledges that men are \"also\" victims of domestic violence, men's rights advocates demand government recognition that men are equally or \"more\" victimized by domestic violence, claims not supported by the data. Mann also states that in contrast to feminist groups, who have advocated for domestic violence services on behalf of other historically oppressed groups in addition to women, such as individuals impacted by poverty, ethnicity, disability, sexual orientation, etc., men's rights groups have attempted to achieve their goals by actively opposing and attempting to dismantle services and supports put in place to protect abused women and children.\n\nDomestic abuse expert Lundy Bancroft has called men's rights \"the abuser's crusade\" and said that the attitudes of the movement contribute to abuse of women.\n\nOther researchers such as Michael Flood have accused the men's rights movement, particularly the father's rights groups in Australia, of endangering women, children, and even men who are at greater risk of abuse and violence. Flood states that the men's rights/father's rights groups in Australia pursues \"equality with a vengeance\" or equal policies with negative outcomes and motives in order to re-establish paternal authority over the well-being of children and women as well as positive parenting.\n\n\n\n\n"}
{"id": "14885874", "url": "https://en.wikipedia.org/wiki?curid=14885874", "title": "Mental health literacy", "text": "Mental health literacy\n\nMental health literacy has been defined as \"knowledge and beliefs about mental disorders which aid their recognition, management or prevention. Mental health literacy includes the ability to recognize specific disorders; knowing how to seek mental health information; knowledge of risk factors and causes, of self-treatments, and of professional help available; and attitudes that promote recognition and appropriate help-seeking\". The concept of mental health literacy was derived from health literacy, which aims to increase patient knowledge about physical health, illnesses, and treatments.\n\nMental health literacy has three major components: recognition, knowledge, and attitudes. A conceptual framework of mental health literacy illustrates the connections between components, and each is conceptualized as an area to target for measurement or intervention. While some researchers have focused on a single component, others have focused on multiple and/or the connection between components. For example, a researcher may focus solely on improving recognition of disorders through an education program, whereas another researcher may focus on integrating all three components into one program.\n\nRecognition can be broken down into symptom or illness recognition. Symptom recognition is the ability to detect beliefs, behaviors, and other physical manifestations of mental illness, without knowing explicitly which disorder they link to. Specific illness recognition is the ability to identify the presentation of a disorder, such as major depressive disorder.\n\nThe recognition of difference between knowledge and attitudes is a crucial part of the mental health literacy framework. While some efforts have focused on promoting knowledge, other researchers have argued that changing attitudes by reducing stigma is a more prolific way of creating meaningful change in mental healthcare utilization. Overall, both approaches have benefits for improving outcomes.\n\nKnowledge is the largest component of mental health literacy, and is divided into four sub-components:\n\nAttitudes are studied in two sub-components: attitudes about mental disorders, or persons with mental disorders, and attitudes about seeking professional help or treatment. Attitudes can vary greatly by individual, and can often be difficult to measure or target with intervention. Nonetheless, a large body of research literature exists on both sub-components, though not always explicitly tied to the mental health literacy.\n\nSurveys of the public have been carried out in a number of countries to investigate mental health literacy. These surveys show that recognition of mental disorders is lacking and reveal negative beliefs about some standard psychiatric treatments, particularly medications. On the other hand, psychological, complementary and self-help methods are viewed much more positively. Negative attitudes towards people with mental disorders have been found, such as viewing them as having a weak character. These beliefs and attitudes are potential barriers to seeking optimal professional help and being supportive of others.\n\nResearchers have measured aspects of mental health literacy in several ways. Popular methodologies include vignette studies and achievement tests. Vignette studies measure mental health literacy by providing a brief, detailed story of an individual (or individuals) with a mental health problem, and asks participants questions to identify what problem the individual is experiencing, and at times, additional questions about how the individual can help themselves.\n\nAchievement tests measure mental health literacy on a continuum, such that higher scores on a test indicate greater overall knowledge or understanding of a concept. Achievement tests can be formatted using multiple-choice, true/false, or other quantitative scales.\n\nVarious scales have been created to measure the various components of mental health literacy, though not all are validated. Mental health literacy has been measured across several populations, varying in age range, culture, and profession. Most studies have focused on adult and young adult populations, though improving literacy in children has been a focus of prevention efforts.\n\nA number of approaches have been tried to improve mental health literacy, many of which have evidence of effectiveness. These include:\n"}
{"id": "516838", "url": "https://en.wikipedia.org/wiki?curid=516838", "title": "Micro-g environment", "text": "Micro-g environment\n\nThe term micro-g environment (also µg, often referred to by the term microgravity) is more or less a synonym for \"weightlessness\" and \"zero-g\", but indicates that g-forces are not quite zero—just very small. The symbol for microgravity, \"µg\", was used on the insignias of Space Shuttle flights STS-87 and STS-107, because these flights were devoted to microgravity research in low Earth orbit.\n\nA \"stationary\" micro-g environment would require travelling far enough into deep space so as to reduce the effect of gravity by attenuation to almost zero. This is the simplest in conception, but requires traveling an enormous distance, rendering it most impractical. For example, to reduce the gravity of the Earth by a factor of one million, one needs to be at a distance of 6 million kilometers from the Earth, but to reduce the gravity of the Sun to this amount one has to be at a distance of 3.7 billion kilometers. (On Earth the gravity due to the rest of the Milky Way is already attenuated by a factor greater than one million, so we do not need to move away further from its center). Thus it is not impossible, but it has only been achieved so far by four interstellar probes (Voyager 1 and 2 of the Voyager program, and Pioneer 10 and 11 of the Pioneer program) and they did not return to Earth. To reduce the gravity to one thousandth of that on Earth's surface, one needs to be at a distance of 200,000 km.\n\nAt a distance relatively close to Earth (less than 3000 km), gravity is only slightly reduced. As an object orbits a body such as the Earth, gravity is still attracting objects towards the Earth and the object is accelerated downward at almost 1g. Because the objects are typically moving laterally with respect to the surface at such immense speeds, the object will not lose altitude because of the curvature of the Earth. When viewed from an orbiting observer, other close objects in space appear to be floating because everything is being pulled towards Earth at the same speed, but also moving forward as the Earth's surface \"falls\" away below. All these objects are in free fall, not zero gravity.\n\nCompare the gravitational potential at some of these locations.\n\nWhat remains is a micro-g environment moving in free fall, i.e. there are no forces other than gravity acting on the people or objects in this environment. To prevent air drag making the free fall less perfect, objects and people can free-fall in a capsule that itself, while not necessarily in free fall, is accelerated as in free fall. This can be done by applying a force to compensate for air drag. Alternatively free fall can be carried out in space, or in a vacuum tower or shaft.\n\nTwo cases can be distinguished: Temporary micro-g, where after some time the Earth's surface is or would be reached, and indefinite micro-g.\n\nA temporary micro-g environment exists in a drop tube (in a tower or shaft), a sub-orbital spaceflight, e.g. with a sounding rocket, and in an airplane such as used by NASA's Reduced Gravity Research Program, aka the Vomit Comet, and by the Zero Gravity Corporation. A temporary micro-g environment is applied for training of astronauts, for some experiments, for filming movies, and for recreational purposes.\n\nA micro-g environment for an indefinite time, while also possible in a spaceship going to infinity in a parabolic or hyperbolic orbit, is most practical in an Earth orbit. This is the environment commonly experienced in the International Space Station, Space Shuttle, etc. While this scenario is the most suitable for scientific experimentation and commercial exploitation, it is still quite expensive to operate in, mostly due to launch costs.\n\nObjects in orbit are not perfectly weightless due to several effects:\n\n\nIn a shot tower (now obsolete), molten metal (such as lead or steel) was dripped through a sieve into free fall. With sufficient height (several hundred feet), the metal would be solid enough to resist impact (usually in a water bath) at the bottom of the tower. While the shot may have been slightly deformed by its passage through the air and by impact at the bottom, this method produced metal spheres of sufficient roundness to be used directly in shotgun shells or to be refined by further processing for applications requiring higher accuracy.\n\nWhile not yet a commercial application, there has been interest in growing crystals in micro-g, as in a space station or automated artificial satellite, in an attempt to reduce crystal lattice defects. Such defect-free crystals may prove useful for certain microelectronic applications and also to produce crystals for subsequent X-ray crystallography.\n\nSpace Motion Sickness (SMS) is thought to be a subtype of motion sickness that plagues nearly half of all astronauts who venture into space. SMS, along with facial stuffiness from headward shifts of fluids, headaches, and back pain, is part of a broader complex of symptoms that comprise Space Adaptation Syndrome (SAS). SMS was first described in 1961 during the second orbit of the fourth manned spaceflight when the Cosmonaut, Gherman Titov aboard the Vostok 2, described feeling disoriented with physical complaints mostly consistent with motion sickness. It is one of the most studied physiological problems of spaceflight but continues to pose a significant difficulty for many astronauts. In some instances, it can be so debilitating that astronauts must sit out from their scheduled occupational duties in space – including missing out on a spacewalk they have spent months training to perform. In most cases, however, astronauts will work through the symptoms even with degradation in their performance.\n\nDespite their experiences in some of the most rigorous and demanding physical maneuvers on earth, even the most seasoned astronauts may be affected by SMS, resulting in symptoms of severe nausea, projectile vomiting, fatigue, malaise (feeling sick), and headache. These symptoms may occur so abruptly and without any warning that space travelers may vomit suddenly without time to contain the emesis, resulting in strong odors and liquid within the cabin which may affect other astronauts. Symptoms typically last anywhere from one to three days upon entering weightlessness, but may recur upon reentry to Earth’s gravity or even shortly after landing. SMS differs from terrestrial motion sickness in that sweating and pallor are typically minimal or absent and gastrointestinal findings usually demonstrate absent bowel sounds indicating reduced gastrointestinal motility.\n\nEven when the nausea and vomiting resolve, some central nervous system symptoms may persist which may degrade the astronaut’s performance. Graybiel and Knepton proposed the term “sopite syndrome” to describe symptoms of lethargy and drowsiness associated with motion sickness in 1976. Since then, their definition has been revised to include “…a symptom complex that develops as a result of exposure to real or apparent motion and is characterized by excessive drowsiness, lassitude, lethargy, mild depression, and reduced ability to focus on an assigned task.” Together, these symptoms may pose a substantial threat (albeit temporary) to the astronaut who must remain attentive to life and death issues at all times.\n\nSMS is most commonly thought to be a disorder of the vestibular system that occurs when sensory information from the visual system (sight) and the proprioceptive system (posture, position of the body) conflicts with misperceived information from the semicircular canals and the otoliths within the inner ear. This is known as the ‘neural mismatch theory’ and was first suggested in 1975 by Reason and Brand. Alternatively, the fluid shift hypothesis suggests that weightlessness reduces the hydrostatic pressure on the lower body causing fluids to shift toward the head from the rest of the body. These fluid shifts are thought to increase cerebrospinal fluid pressure (causing back aches), intracranial pressure (causing headaches), and inner ear fluid pressure (causing vestibular dysfunction).\n\nDespite a multitude of studies searching for a solution to the problem of SMS, it remains an ongoing problem for space travel. Most non-pharmacological countermeasures such as training and other physical maneuvers have offered minimal benefit. Thornton and Bonato noted, “Pre- and inflight adaptive efforts, some of them mandatory and most of them onerous, have been, for the most part, operational failures.” To date, the most common intervention is promethazine, an injectable antihistamine with antiemetic properties, but sedation can be a problematic side effect. Other common pharmacological options include metaclopromide, as well as oral and transdermal application of scopolamine, but drowsiness and sedation are common side effects for these medications as well.\n\nIn the space (or microgravity) environment the effects of unloading varies significantly among individuals, with sex differences compounding the variability. Differences in mission duration, and the small sample size of astronauts participating in the same mission also adds to the variability to the musculoskeletal disorders that are seen in space. In addition to muscle loss, microgravity leads to increased bone resorption, decreased bone mineral density, and increased fracture risks. Bone resorption leads to increased urinary levels of calcium, which can subsequently lead to an increased risk of nephrolithiasis.\n\nIn the first two weeks that the muscles are unloaded from carrying the weight of the human frame during space flight, whole muscle atrophy begins. Postural muscles contain more slow fibers, and are more prone to atrophy than non-postural muscle groups. The loss of muscle mass occurs because of imbalances in protein synthesis and breakdown. The loss of muscle mass is also accompanied by a loss of muscle strength, which was observed after only 2–5 days of spaceflight during the Soyuz-3 and Soyuz-8 missions. Decreases in the generation of contractile forces and whole muscle power have also been found in response to microgravity.\n\nTo counter the effects of microgravity on the musculoskeletal system, aerobic exercise is recommended. This often takes the form of in-flight cycling. A more effective regimen includes resistive exercises or the use of a penguin suit (contains sewn-in elastic bands to maintain a stretch load on antigravity muscles), centrifugation, and vibration. Centrifugation recreates Earth’s gravitational force on the space station, in order to prevent muscle atrophy. Centrifugation can be performed with centrifuges or by cycling along the inner wall of the space station. Whole body vibration has been found to reduce bone resorption through mechanisms that are unclear. Vibration can be delivered using exercise devices that use vertical displacements juxtaposed to a fulcrum, or by using a plate that oscillates on a vertical axis. The use of beta-2 adrenergic agonists to increase muscle mass, and the use of essential amino acids in conjunction with resistive exercises have been proposed as pharmacologic means of combating muscle atrophy in space.\n\nMicrogravity can also lead to height increases; in 2018 a Japanese astronout reported growing 2 centimetres in just three weeks of micro-gravity on board the International Space Station.\n\nNext to the skeletal and muscular system, the cardiovascular system is less strained in weightlessness than on Earth and is de-conditioned during longer periods spent in space. In a regular environment, gravity exerts a downward force, setting up a vertical hydrostatic gradient. When standing, some 'excess' fluid resides in vessels and tissues of the legs. In a micro-g environment, with the loss of a hydrostatic gradient, some fluid quickly redistributes toward the chest and upper body; sensed as 'overload' of circulating blood volume. In the micro-g environment, the newly sensed excess blood volume is adjusted by expelling excess fluid into tissues and cells (12-15% volume reduction) and red blood cells are adjusted downward to maintain a normal concentration (relative anemia). In the absence of gravity, venous blood will rush to the right atrium because the force of gravity is no longer pulling the blood down into the vessels of the legs and abdomen, resulting in increased stroke volume. These fluid shifts become more dangerous upon returning to a regular gravity environment as the body will attempt to adapt to the reintroduction of gravity. The reintroduction of gravity again will pull the fluid downward, but now there would be a deficit in both circulating fluid and red blood cells. The decrease in cardiac filling pressure and stroke volume during the orthostatic stress due to a decreased blood volume is what causes orthostatic intolerance. Orthostatic intolerance can result in temporary loss of consciousness and posture, due to the lack of pressure and stroke volume. More chronic orthostatic intolerance can result in additional symptoms such as nausea, sleep problems, and other vasomotor symptoms as well.\n\nMany studies on the physiological effects of weightlessness on the cardiovascular system are done in parabolic flights. It is one of the only feasible options to combine with human experiments, making parabolic flights the only way to investigate the true effects of the micro-g environment on a body without traveling into space. Parabolic flight studies have provided a broad range of results regarding changes in the cardiovascular system in a micro-g environment. Parabolic flight studies have increased the understanding of orthostatic intolerance and decreased peripheral blood flow suffered by Astronauts returning to Earth. Due to the loss of blood to pump, the heart can atrophy in a micro-g environment. A weakened heart can result in low blood volume, low blood pressure and affect the body's ability to send oxygen to the brain without the individual becoming dizzy. Heart rhythm disturbances have also been seen among astronauts, but it is not clear whether this was due to pre-existing conditions of effects of a micro-g environment. One current countermeasure includes drinking a salt solution, which increases the viscosity of blood and would subsequently increase blood pressure which would mitigate post micro-g environment orthostatic intolerance. Another countermeasure includes administration of midodrine, which is a selective alpha-1 adrenergic agonist. Midodrine produces arterial and venous constriction resulting in an increase in blood pressure by baroreceptor reflexes.\n\nSpace Motion Sickness can lead to degraded astronaut performance. SMS threatens operational requirements, reduces situational awareness, and threatens the safety of those exposed to micro-g environments. Lost muscle mass leads to difficulty with movement, especially when astronauts return to earth. This can pose a safety issue if the need for emergency egress were to arise. Loss of muscle power makes it extremely difficult, if not impossible, for astronauts to climb through emergency egress hatches or create unconventional exit spaces in the case of a crash upon landing. Additionally, bone resorption and inadequate hydration in space can lead to the formation of kidney stones, and subsequent sudden incapacitation due to pain. If this were to occur during critical phases of flight, a capsule crash leading to worker injury and/or death could result. Short-term and long-term health effects have been seen in the cardiovascular system from exposure to the micro-g environment that would limit those exposed after they return to Earth or a regular gravity environment. Steps need to be taken to ensure proper precautions are taken into consideration when dealing a micro-g environment for worker safety. Orthostatic intolerance can lead to temporary loss of consciousness due to the lack of pressure and stroke volume. This loss of consciousness inhibits and endangers those affected and can lead to deadly consequences.\n\n\n"}
{"id": "20381", "url": "https://en.wikipedia.org/wiki?curid=20381", "title": "Mining", "text": "Mining\n\nMining is the extraction of valuable minerals or other geological materials from the earth, usually from an orebody, lode, vein, seam, reef or placer deposit. These deposits form a mineralized package that is of economic interest to the miner.\n\nOres recovered by mining include metals, coal, oil shale, gemstones, limestone, chalk, dimension stone, rock salt, potash, gravel, and clay. Mining is required to obtain any material that cannot be grown through agricultural processes, or created artificially in a laboratory or factory. Mining in a wider sense includes extraction of any non-renewable resource such as petroleum, natural gas, or even water.\n\nMining of stones and metal has been a human activity since pre-historic times. Modern mining processes involve prospecting for ore bodies, analysis of the profit potential of a proposed mine, extraction of the desired materials, and final reclamation of the land after the mine is closed. De Re Metallica, Georgius Agricola, 1550, Book I, Para. 1\n\nMining operations usually create a negative environmental impact, both during the mining activity and after the mine has closed. Hence, most of the world's nations have passed regulations to decrease the impact. Work safety has long been a concern as well, and modern practices have significantly improved safety in mines.\n\nLevels of metals recycling are generally low. Unless future end-of-life recycling rates are stepped up, some rare metals may become unavailable for use in a variety of consumer products. Due to the low recycling rates, some landfills now contain higher concentrations of metal than mines themselves.\n\nSince the beginning of civilization, people have used stone, ceramics and, later, metals found close to the Earth's surface. These were used to make early tools and weapons; for example, high quality flint found in northern France, southern England and Poland was used to create flint tools. Flint mines have been found in chalk areas where seams of the stone were followed underground by shafts and galleries. The mines at Grimes Graves and Krzemionki are especially famous, and like most other flint mines, are Neolithic in origin (ca 4000–3000 BC). Other hard rocks mined or collected for axes included the greenstone of the Langdale axe industry based in the English Lake District.\n\nThe oldest-known mine on archaeological record is the Ngwenya Mine in Swaziland, which radiocarbon dating shows to be about 43,000 years old. At this site Paleolithic humans mined hematite to make the red pigment ochre. Mines of a similar age in Hungary are believed to be sites where Neanderthals may have mined flint for weapons and tools.\n\nAncient Egyptians mined malachite at Maadi. At first, Egyptians used the bright green malachite stones for ornamentations and pottery. Later, between 2613 and 2494 BC, large building projects required expeditions abroad to the area of Wadi Maghareh in order to secure minerals and other resources not available in Egypt itself. Quarries for turquoise and copper were also found at Wadi Hammamat, Tura, Aswan and various other Nubian sites on the Sinai Peninsula and at Timna.\n\nMining in Egypt occurred in the earliest dynasties. The gold mines of Nubia were among the largest and most extensive of any in Ancient Egypt. These mines are described by the Greek author Diodorus Siculus, who mentions fire-setting as one method used to break down the hard rock holding the gold. One of the complexes is shown in one of the earliest known maps. The miners crushed the ore and ground it to a fine powder before washing the powder for the gold dust.\n\nMining in Europe has a very long history. Examples include the silver mines of Laurium, which helped support the Greek city state of Athens. Although they had over 20,000 slaves working them, their technology was essentially identical to their Bronze Age predecessors. At other mines, such as on the island of Thassos, marble was quarried by the Parians after they arrived in the 7th century BC. The marble was shipped away and was later found by archaeologists to have been used in buildings including the tomb of Amphipolis. Philip II of Macedon, the father of Alexander the Great, captured the gold mines of Mount Pangeo in 357 BC to fund his military campaigns. He also captured gold mines in Thrace for minting coinage, eventually producing 26 tons per year.\n\nHowever, it was the Romans who developed large scale mining methods, especially the use of large volumes of water brought to the minehead by numerous aqueducts. The water was used for a variety of purposes, including removing overburden and rock debris, called hydraulic mining, as well as washing comminuted, or crushed, ores and driving simple machinery.\n\nThe Romans used hydraulic mining methods on a large scale to prospect for the veins of ore, especially a now-obsolete form of mining known as hushing. They built numerous aqueducts to supply water to the minehead. There, the water stored in large reservoirs and tanks. When a full tank was opened, the flood of water sluiced away the overburden to expose the bedrock underneath and any gold veins. The rock was then worked upon by fire-setting to heat the rock, which would be quenched with a stream of water. The resulting thermal shock cracked the rock, enabling it to be removed by further streams of water from the overhead tanks. The Roman miners used similar methods to work cassiterite deposits in Cornwall and lead ore in the Pennines.\n\nThe methods had been developed by the Romans in Spain in 25 AD to exploit large alluvial gold deposits, the largest site being at Las Medulas, where seven long aqueducts tapped local rivers and sluiced the deposits. Spain was one of the most important mining regions, but all regions of the Roman Empire were exploited. In Great Britain the natives had mined minerals for millennia, but after the Roman conquest, the scale of the operations increased dramatically, as the Romans needed Britannia's resources, especially gold, silver, tin, and lead.\n\nRoman techniques were not limited to surface mining. They followed the ore veins underground once opencast mining was no longer feasible. At Dolaucothi they stoped out the veins and drove adits through bare rock to drain the stopes. The same adits were also used to ventilate the workings, especially important when fire-setting was used. At other parts of the site, they penetrated the water table and dewatered the mines using several kinds of machines, especially reverse overshot water-wheels. These were used extensively in the copper mines at Rio Tinto in Spain, where one sequence comprised 16 such wheels arranged in pairs, and lifting water about . They were worked as treadmills with miners standing on the top slats. Many examples of such devices have been found in old Roman mines and some examples are now preserved in the British Museum and the National Museum of Wales.\n\nMining as an industry underwent dramatic changes in medieval Europe. The mining industry in the early Middle Ages was mainly focused on the extraction of copper and iron. Other precious metals were also used, mainly for gilding or coinage. Initially, many metals were obtained through open-pit mining, and ore was primarily extracted from shallow depths, rather than through deep mine shafts. Around the 14th century, the growing use of weapons, armour, stirrups, and horseshoes greatly increased the demand for iron. Medieval knights, for example, were often laden with up to of plate or chain link armour in addition to swords, lances and other weapons. The overwhelming dependency on iron for military purposes spurred iron production and extraction processes.\n\nThe silver crisis of 1465 occurred when all mines had reached depths at which the shafts could no longer be pumped dry with the available technology. Although an increased use of banknotes, credit and copper coins during this period did decrease the value of, and dependence on, precious metals, gold and silver still remained vital to the story of medieval mining.\n\nDue to differences in the social structure of society, the increasing extraction of mineral deposits spread from central Europe to England in the mid-sixteenth century. On the continent, mineral deposits belonged to the crown, and this regalian right was stoutly maintained. But in England, royal mining rights were restricted to gold and silver (of which England had virtually no deposits) by a judicial decision of 1568 and a law in 1688. England had iron, zinc, copper, lead, and tin ores. Landlords who owned the base metals and coal under their estates then had a strong inducement to extract these metals or to lease the deposits and collect royalties from mine operators. English, German, and Dutch capital combined to finance extraction and refining. Hundreds of German technicians and skilled workers were brought over; in 1642 a colony of 4,000 foreigners was mining and smelting copper at Keswick in the northwestern mountains.\n\nUse of water power in the form of water mills was extensive. The water mills were employed in crushing ore, raising ore from shafts, and ventilating galleries by powering giant bellows. Black powder was first used in mining in Selmecbánya, Kingdom of Hungary (now Banská Štiavnica, Slovakia) in 1627. Black powder allowed blasting of rock and earth to loosen and reveal ore veins. Blasting was much faster than fire-setting and allowed the mining of previously impenetrable metals and ores. In 1762, the world's first mining academy was established in the same town there.\n\nThe widespread adoption of agricultural innovations such as the iron plowshare, as well as the growing use of metal as a building material, was also a driving force in the tremendous growth of the iron industry during this period. Inventions like the arrastra were often used by the Spanish to pulverize ore after being mined. This device was powered by animals and used the same principles used for grain threshing.\n\nMuch of the knowledge of medieval mining techniques comes from books such as Biringuccio’s \"De la pirotechnia\" and probably most importantly from Georg Agricola's \"De re metallica\" (1556). These books detail many different mining methods used in German and Saxon mines. A prime issue in medieval mines, which Agricola explains in detail, was the removal of water from mining shafts. As miners dug deeper to access new veins, flooding became a very real obstacle. The mining industry became dramatically more efficient and prosperous with the invention of mechanical and animal driven pumps.\n\nMining in the Philippines began around 1000 BC. The early Filipinos worked various mines of gold, silver, copper and iron. Jewels, gold ingots, chains, calombigas and earrings were handed down from antiquity and inherited from their ancestors. Gold dagger handles, gold dishes, tooth plating, and huge gold ornamets were also used. In Laszlo Legeza's \"Tantric elements in pre-Hispanic Philippines Gold Art\", he mentioned that gold jewelry of Philippine origin was found in Ancient Egypt. According to Antonio Pigafetta, the people of Mindoro possessed great skill in mixing gold with other metals and gave it a natural and perfect appearance that could deceive even the best of silversmiths. The natives were also known for the jewelries made of other precious stones such as carnelian, agate and pearl. Some outstanding examples of Philippine jewelry included necklaces, belts, armlets and rings placed around the waist.\n\nDuring prehistoric times, large amounts of copper was mined along Lake Superior's Keweenaw Peninsula and in nearby Isle Royale; metallic copper was still present near the surface in colonial times. Indigenous peoples used Lake Superior copper from at least 5,000 years ago; copper tools, arrowheads, and other artifacts that were part of an extensive native trade network have been discovered. In addition, obsidian, flint, and other minerals were mined, worked, and traded. Early French explorers who encountered the sites made no use of the metals due to the difficulties of transporting them, but the copper was eventually traded throughout the continent along major river routes. \n\nIn the early colonial history of the Americas, \"native gold and silver was quickly expropriated and sent back to Spain in fleets of gold- and silver-laden galleons,\" the gold and silver originating mostly from mines in Central and South America. Turquoise dated at 700 AD was mined in pre-Columbian America; in the Cerillos Mining District in New Mexico, estimates are that \"about 15,000 tons of rock had been removed from Mt. Chalchihuitl using stone tools before 1700.\"\n\nIn 1727, Louis Denys (Denis) (1675 - 1741), sieur de La Ronde– brother of Simon-Pierre Denys de Bonaventure and the son-in-law of René Chartier –took command of Fort La Pointe at Chequamegon Bay; where natives informed him of an island of copper. La Ronde obtained permission from the French crown to operate mines in 1733, becoming \"the first practical miner on Lake Superior\"; seven years later, mining was halted by an outbreak between Sioux and Chippewa tribes.\n\nMining in the United States became prevalent in the 19th century, and the General Mining Act of 1872 was passed to encourage mining of federal lands. As with the California Gold Rush in the mid-19th century, mining for minerals and precious metals, along with ranching, was a driving factor in the Westward Expansion to the Pacific coast. With the exploration of the West, mining camps were established and \"expressed a distinctive spirit, an enduring legacy to the new nation;\" Gold Rushers would experience the same problems as the Land Rushers of the transient West that preceded them. Aided by railroads, many traveled West for work opportunities in mining. Western cities such as Denver and Sacramento originated as mining towns.\n\nWhen new areas were explored, it was usually the gold (placer and then lode) and then silver that were taken into possession and extracted first. Other metals would often wait for railroads or canals, as coarse gold dust and nuggets do not require smelting and are easy to identify and transport.\n\nIn the early 20th century, the gold and silver rush to the western United States also stimulated mining for coal as well as base metals such as copper, lead, and iron. Areas in modern Montana, Utah, Arizona, and later Alaska became predominate suppliers of copper to the world, which was increasingly demanding copper for electrical and households goods. Canada's mining industry grew more slowly than did the United States' due to limitations in transportation, capital, and U.S. competition; Ontario was the major producer of the early 20th century with nickel, copper, and gold.\n\nMeanwhile, Australia experienced the Australian gold rushes and by the 1850s was producing 40% of the world's gold, followed by the establishment of large mines such as the Mount Morgan Mine, which ran for nearly a hundred years, Broken Hill ore deposit (one of the largest zinc-lead ore deposits), and the iron ore mines at Iron Knob. After declines in production, another boom in mining occurred in the 1960s. Now, in the early 21st century, Australia remains a major world mineral producer.\n\nAs the 21st century begins, a globalized mining industry of large multinational corporations has arisen. Peak minerals and environmental impacts have also become a concern. Different elements, particularly rare earth minerals, have begun to increase in demand as a result of new technologies.\n\nThe process of mining from discovery of an ore body through extraction of minerals and finally to returning the land to its natural state consists of several distinct steps. The first is discovery of the ore body, which is carried out through prospecting or exploration to find and then define the extent, location and value of the ore body. This leads to a mathematical resource estimation to estimate the size and grade of the deposit.\n\nThis estimation is used to conduct a pre-feasibility study to determine the theoretical economics of the ore deposit. This identifies, early on, whether further investment in estimation and engineering studies is warranted and identifies key risks and areas for further work. The next step is to conduct a feasibility study to evaluate the financial viability, the technical and financial risks, and the robustness of the project.\n\nThis is when the mining company makes the decision whether to develop the mine or to walk away from the project. This includes mine planning to evaluate the economically recoverable portion of the deposit, the metallurgy and ore recoverability, marketability and payability of the ore concentrates, engineering concerns, milling and infrastructure costs, finance and equity requirements, and an analysis of the proposed mine from the initial excavation all the way through to reclamation. The proportion of a deposit that is economically recoverable is dependent on the enrichment factor of the ore in the area.\n\nTo gain access to the mineral deposit within an area it is often necessary to mine through or remove waste material which is not of immediate interest to the miner. The total movement of ore and waste constitutes the mining process. Often more waste than ore is mined during the life of a mine, depending on the nature and location of the ore body. Waste removal and placement is a major cost to the mining operator, so a detailed characterization of the waste material forms an essential part of the geological exploration program for a mining operation.\n\nOnce the analysis determines a given ore body is worth recovering, development begins to create access to the ore body. The mine buildings and processing plants are built, and any necessary equipment is obtained. The operation of the mine to recover the ore begins and continues as long as the company operating the mine finds it economical to do so. Once all the ore that the mine can produce profitably is recovered, reclamation begins to make the land used by the mine suitable for future use.\n\nMining techniques can be divided into two common excavation types: surface mining and sub-surface (underground) mining. Today, surface mining is much more common, and produces, for example, 85% of minerals (excluding petroleum and natural gas) in the United States, including 98% of metallic ores.\n\nTargets are divided into two general categories of materials: \"placer deposits\", consisting of valuable minerals contained within river gravels, beach sands, and other unconsolidated materials; and \"lode deposits\", where valuable minerals are found in veins, in layers, or in mineral grains generally distributed throughout a mass of actual rock. Both types of ore deposit, placer or lode, are mined by both surface and underground methods.\n\nSome mining, including much of the rare earth elements and uranium mining, is done by less-common methods, such as in-situ leaching: this technique involves digging neither at the surface nor underground. The extraction of target minerals by this technique requires that they be soluble, e.g., potash, potassium chloride, sodium chloride, sodium sulfate, which dissolve in water. Some minerals, such as copper minerals and uranium oxide, require acid or carbonate solutions to dissolve.\n\nSurface mining is done by removing (stripping) surface vegetation, dirt, and, if necessary, layers of bedrock in order to reach buried ore deposits. Techniques of surface mining include: open-pit mining, which is the recovery of materials from an open pit in the ground, quarrying, identical to open-pit mining except that it refers to sand, stone and clay; strip mining, which consists of stripping surface layers off to reveal ore/seams underneath; and mountaintop removal, commonly associated with coal mining, which involves taking the top of a mountain off to reach ore deposits at depth. Most (but not all) placer deposits, because of their shallowly buried nature, are mined by surface methods. Finally, landfill mining involves sites where landfills are excavated and processed. Landfill mining has been thought of as a solution to dealing with long-term methane emissions and local pollution\n\nSub-surface mining consists of digging tunnels or shafts into the earth to reach buried ore deposits. Ore, for processing, and waste rock, for disposal, are brought to the surface through the tunnels and shafts. Sub-surface mining can be classified by the type of access shafts used, the extraction method or the technique used to reach the mineral deposit. Drift mining utilizes horizontal access tunnels, slope mining uses diagonally sloping access shafts, and shaft mining utilizes vertical access shafts. Mining in hard and soft rock formations require different techniques.\n\nOther methods include shrinkage stope mining, which is mining upward, creating a sloping underground room, long wall mining, which is grinding a long ore surface underground, and room and pillar mining, which is removing ore from rooms while leaving pillars in place to support the roof of the room. Room and pillar mining often leads to retreat mining, in which supporting pillars are removed as miners retreat, allowing the room to cave in, thereby loosening more ore. Additional sub-surface mining methods include hard rock mining, which is mining of hard rock (igneous, metamorphic or sedimentary) materials, bore hole mining, drift and fill mining, long hole slope mining, sub level caving, and block caving.\n\nHighwall mining is another form of surface mining that evolved from auger mining. In Highwall mining, the coal seam is penetrated by a continuous miner propelled by a hydraulic Pushbeam Transfer Mechanism (PTM). A typical cycle includes sumping (launch-pushing forward) and shearing (raising and lowering the cutterhead boom to cut the entire height of the coal seam). As the coal recovery cycle continues, the cutterhead is progressively launched into the coal seam for 19.72 feet (6.01 m). Then, the Pushbeam Transfer Mechanism (PTM) automatically inserts a 19.72-foot (6.01 m) long rectangular Pushbeam (Screw-Conveyor Segment) into the center section of the machine between the Powerhead and the cutterhead. The Pushbeam system can penetrate nearly 1,000 feet (300 m) into the coal seam. One patented Highwall mining system uses augers enclosed inside the Pushbeam that prevent the mined coal from being contaminated by rock debris during the conveyance process. Using a video imaging and/or a gamma ray sensor and/or other Geo-Radar systems like a coal-rock interface detection sensor (CID), the operator can see ahead projection of the seam-rock interface and guide the continuous miner's progress. Highwall mining can produce thousands of tons of coal in contour-strip operations with narrow benches, previously mined areas, trench mine applications and steep-dip seams with controlled water-inflow pump system and/or a gas (inert) venting system.\n\nHeavy machinery is used in mining to explore and develop sites, to remove and stockpile overburden, to break and remove rocks of various hardness and toughness, to process the ore, and to carry out reclamation projects after the mine is closed. Bulldozers, drills, explosives and trucks are all necessary for excavating the land. In the case of placer mining, unconsolidated gravel, or alluvium, is fed into machinery consisting of a hopper and a shaking screen or trommel which frees the desired minerals from the waste gravel. The minerals are then concentrated using sluices or jigs.\n\nLarge drills are used to sink shafts, excavate stopes, and obtain samples for analysis. Trams are used to transport miners, minerals and waste. Lifts carry miners into and out of mines, and move rock and ore out, and machinery in and out, of underground mines. Huge trucks, shovels and cranes are employed in surface mining to move large quantities of overburden and ore. Processing plants utilize large crushers, mills, reactors, roasters and other equipment to consolidate the mineral-rich material and extract the desired compounds and metals from the ore.\n\nOnce the mineral is extracted, it is often then processed. The science of extractive metallurgy is a specialized area in the science of metallurgy that studies the extraction of valuable metals from their ores, especially through chemical or mechanical means.\n\nMineral processing (or mineral dressing) is a specialized area in the science of metallurgy that studies the mechanical means of crushing, grinding, and washing that enable the separation (extractive metallurgy) of valuable metals or minerals from their gangue (waste material). Processing of placer ore material consists of gravity-dependent methods of separation, such as sluice boxes. Only minor shaking or washing may be necessary to disaggregate (unclump) the sands or gravels before processing. Processing of ore from a lode mine, whether it is a surface or subsurface mine, requires that the rock ore be crushed and pulverized before extraction of the valuable minerals begins. After lode ore is crushed, recovery of the valuable minerals is done by one, or a combination of several, mechanical and chemical techniques.\n\nSince most metals are present in ores as oxides or sulfides, the metal needs to be reduced to its metallic form. This can be accomplished through chemical means such as smelting or through electrolytic reduction, as in the case of aluminium. Geometallurgy combines the geologic sciences with extractive metallurgy and mining.\n\nIn 2018, led by Chemistry and Biochemistry professor Bradley D. Smith, University of Notre Dame researchers \"invented a new class of molecules whose shape and size enable them to capture and contain precious metal ions,\" reported in a study published by the Journal of the American Chemical Society. The new method \"converts gold-containing ore into chloroauric acid and extracts it using an industrial solvent. The container molecules are able to selectively separate the gold from the solvent without the use of water stripping.\" The newly developed molecules can eliminate water stripping, whereas mining traditionally \"relies on a 125-year-old method that treats gold-containing ore with large quantities of poisonous sodium cyanide... this new process has a milder environmental impact and that, besides gold, it can be used for capturing other metals such as platinum and palladium,\" and could also be used in urban mining processes that remove precious metals from wastewater streams.\n\nEnvironmental issues can include erosion, formation of sinkholes, loss of biodiversity, and contamination of soil, groundwater and surface water by chemicals from mining processes. In some cases, additional forest logging is done in the vicinity of mines to create space for the storage of the created debris and soil. Contamination resulting from leakage of chemicals can also affect the health of the local population if not properly controlled. Extreme examples of pollution from mining activities include coal fires, which can last for years or even decades, producing massive amounts of environmental damage.\n\nMining companies in most countries are required to follow stringent environmental and rehabilitation codes in order to minimize environmental impact and avoid impacting human health. These codes and regulations all require the common steps of environmental impact assessment, development of environmental management plans, mine closure planning (which must be done before the start of mining operations), and environmental monitoring during operation and after closure. However, in some areas, particularly in the developing world, government regulations may not be well enforced.\n\nFor major mining companies and any company seeking international financing, there are a number of other mechanisms to enforce good environmental standards. These generally relate to financing standards such as the Equator Principles, IFC environmental standards, and criteria for Socially responsible investing. Mining companies have used this oversight from the financial sector to argue for some level of industry self-regulation. In 1992, a Draft Code of Conduct for Transnational Corporations was proposed at the Rio Earth Summit by the UN Centre for Transnational Corporations (UNCTC), but the Business Council for Sustainable Development (BCSD) together with the International Chamber of Commerce (ICC) argued successfully for self-regulation instead.\n\nThis was followed by the Global Mining Initiative which was begun by nine of the largest metals and mining companies and which led to the formation of the International Council on Mining and Metals, whose purpose was to \"act as a catalyst\" in an effort to improve social and environmental performance in the mining and metals industry internationally. The mining industry has provided funding to various conservation groups, some of which have been working with conservation agendas that are at odds with an emerging acceptance of the rights of indigenous people – particularly the right to make land-use decisions.\n\nCertification of mines with good practices occurs through the International Organization for Standardization (ISO). For example, ISO 9000 and ISO 14001, which certify an \"auditable environmental management system\", involve short inspections, although they have been accused of lacking rigor. Certification is also available through Ceres' Global Reporting Initiative, but these reports are voluntary and unverified. Miscellaneous other certification programs exist for various projects, typically through nonprofit groups.\n\nThe purpose of a 2012 EPS PEAKS paper was to provide evidence on policies managing ecological costs and maximise socio-economic benefits of mining using host country regulatory initiatives. It found existing literature suggesting donors encourage developing countries to:\n\n\nOre mills generate large amounts of waste, called tailings. For example, 99 tons of waste are generated per ton of copper, with even higher ratios in gold mining - because only 5.3 g of gold is extracted per ton of ore, a ton of gold produces 200,000 tons of tailings. (As time goes on and richer deposits are exhausted - and technology improves to permit - this number is going down to .5 g and less.) These tailings can be toxic. Tailings, which are usually produced as a slurry, are most commonly dumped into ponds made from naturally existing valleys. These ponds are secured by impoundments (dams or embankment dams). In 2000 it was estimated that 3,500 tailings impoundments existed, and that every year, 2 to 5 major failures and 35 minor failures occurred; for example, in the Marcopper mining disaster at least 2 million tons of tailings were released into a local river. In central Finland, Talvivaara Terrafame polymetal mine waste effluent since 2008 and numerous leaks of saline mine water has resulted in ecological collapse of nearby lake. Subaqueous tailings disposal is another option. The mining industry has argued that submarine tailings disposal (STD), which disposes of tailings in the sea, is ideal because it avoids the risks of tailings ponds; although the practice is illegal in the United States and Canada, it is used in the developing world.\n\nThe waste is classified as either sterile or mineralised, with acid generating potential, and the movement and storage of this material forms a major part of the mine planning process. When the mineralised package is determined by an economic cut-off, the near-grade mineralised waste is usually dumped separately with view to later treatment should market conditions change and it becomes economically viable. Civil engineering design parameters are used in the design of the waste dumps, and special conditions apply to high-rainfall areas and to seismically active areas. Waste dump designs must meet all regulatory requirements of the country in whose jurisdiction the mine is located. It is also common practice to rehabilitate dumps to an internationally acceptable standard, which in some cases means that higher standards than the local regulatory standard are applied.\n\nMany mining sites are remote and not connected to the grid. Electricity is typically generated with diesel generators. Due to high transportation cost and theft during transportation the cost for generating electricity is normally high. Renewable energy applications are becoming an alternative or amendment. Both solar and wind power plants can contribute in saving diesel costs at mining sites. Renewable energy applications have been built at mining sites.\nCost savings can reach up to 70%.\n\nMining exists in many countries. London is known as the capital of global \"mining houses\" such as Rio Tinto Group, BHP Billiton, and Anglo American PLC. The US mining industry is also large, but it is dominated by the coal and other nonmetal minerals (e.g., rock and sand), and various regulations have worked to reduce the significance of mining in the United States. In 2007 the total market capitalization of mining companies was reported at US$962 billion, which compares to a total global market cap of publicly traded companies of about US$50 trillion in 2007. In 2002, Chile and Peru were reportedly the major mining countries of South America. The mineral industry of Africa includes the mining of various minerals; it produces relatively little of the industrial metals copper, lead, and zinc, but according to one estimate has as a percent of world reserves 40% of gold, 60% of cobalt, and 90% of the world's platinum group metals. Mining in India is a significant part of that country's economy. In the developed world, mining in Australia, with BHP Billiton founded and headquartered in the country, and mining in Canada are particularly significant. For rare earth minerals mining, China reportedly controlled 95% of production in 2013.\n\nWhile exploration and mining can be conducted by individual entrepreneurs or small businesses, most modern-day mines are large enterprises requiring large amounts of capital to establish. Consequently, the mining sector of the industry is dominated by large, often multinational, companies, most of them publicly listed. It can be argued that what is referred to as the 'mining industry' is actually two sectors, one specializing in exploration for new resources and the other in mining those resources. The exploration sector is typically made up of individuals and small mineral resource companies, called \"juniors\", which are dependent on venture capital. The mining sector is made up of large multinational companies that are sustained by production from their mining operations. Various other industries such as equipment manufacture, environmental testing, and metallurgy analysis rely on, and support, the mining industry throughout the world. Canadian stock exchanges have a particular focus on mining companies, particularly junior exploration companies through Toronto's TSX Venture Exchange; Canadian companies raise capital on these exchanges and then invest the money in exploration globally. Some have argued that below juniors there exists a substantial sector of illegitimate companies primarily focused on manipulating stock prices.\n\nMining operations can be grouped into five major categories in terms of their respective resources. These are oil and gas extraction, coal mining, metal ore mining, nonmetallic mineral mining and quarrying, and mining support activities. Of all of these categories, oil and gas extraction remains one of the largest in terms of its global economic importance. Prospecting potential mining sites, a vital area of concern for the mining industry, is now done using sophisticated new technologies such as seismic prospecting and remote-sensing satellites. Mining is heavily affected by the prices of the commodity minerals, which are often volatile. The 2000s commodities boom (\"commodities supercycle\") increased the prices of commodities, driving aggressive mining. In addition, the price of gold increased dramatically in the 2000s, which increased gold mining; for example, one study found that conversion of forest in the Amazon increased six-fold from the period 2003–2006 (292 ha/yr) to the period 2006–2009 (1,915 ha/yr), largely due to artisanal mining.\n\nMining companies can be classified based on their size and financial capabilities:\n\n\nNew regulations and a process of legislative reforms aim to improve the harmonization and stability of the mining sector in mineral-rich countries. New legislation for mining industry in African countries still appears to be an issue, but has the potential to be solved, when a consensus is reached on the best approach. By the beginning of the 21st century the booming and increasingly complex mining sector in mineral-rich countries was providing only slight benefits to local communities, especially in given the sustainability issues. Increasing debate and influence by NGOs and local communities called for a new approahes which would also include disadvantaged communities, and work towards sustainable development even after mine closure (including transparency and revenue management). By the early 2000s, community development issues and resettlements became mainstream concerns in World Bank mining projects. Mining-industry expansion after mineral prices increased in 2003 and also potential fiscal revenues in those countries created an omission in the other economic sectors in terms of finances and development. Furthermore, this highlighted regional and local demand for mining revenues and an inability of sub-national governments to effectively use the revenues. The Fraser Institute (a Canadian think tank) has highlighted the environmental protection laws in developing countries, as well as voluntary efforts by mining companies to improve their environmental impact.\n\nIn 2007 the Extractive Industries Transparency Initiative (EITI) was mainstreamed in all countries cooperating with the World Bank in mining industry reform. The EITI operates and was implemented with the support of the EITI multi-donor trust fund, managed by the World Bank. The EITI aims to increase transparency in transactions between governments and companies in extractive industries by monitoring the revenues and benefits between industries and recipient governments. The entrance process is voluntary for each country and is monitored by multiple stakeholders including governments, private companies and civil society representatives, responsible for disclosure and dissemination of the reconciliation report; however, the competitive disadvantage of company-by company public report is for some of the businesses in Ghana at least, the main constraint. Therefore, the outcome assessment in terms of failure or success of the new EITI regulation does not only \"rest on the government's shoulders\" but also on civil society and companies.\n\nOn the other hand, implementation has issues; inclusion or exclusion of artisanal mining and small-scale mining (ASM) from the EITI and how to deal with \"non-cash\" payments made by companies to subnational governments. Furthermore, the disproportionate revenues the mining industry can bring to the comparatively small number of people that it employs, causes other problems, like a lack of investment in other less lucrative sectors, leading to swings in government revenuebecause of volatility in the oil markets. Artisanal mining is clearly an issue in EITI Countries such as the Central African Republic, D.R. Congo, Guinea, Liberia and Sierra Leone – i.e. almost half of the mining countries implementing the EITI. Among other things, limited scope of the EITI involving disparity in terms of knowledge of the industry and negotiation skills, thus far flexibility of the policy (e.g. liberty of the countries to expand beyond the minimum requirements and adapt it to their needs), creates another risk of unsuccessful implementation. Public awareness increase, where government should act as a bridge between public and initiative for a successful outcome of the policy is an important element to be considered.\n\nThe World Bank has been involved in mining since 1955, mainly through grants from its International Bank for Reconstruction and Development, with the Bank's Multilateral Investment Guarantee Agency offering political risk insurance. Between 1955 and 1990 it provided about $2 billion to fifty mining projects, broadly categorized as reform and rehabilitation, greenfield mine construction, mineral processing, technical assistance, and engineering. These projects have been criticized, particularly the Ferro Carajas project of Brazil, begun in 1981. The World Bank established mining codes intended to increase foreign investment; in 1988 it solicited feedback from 45 mining companies on how to increase their involvement.\n\nIn 1992 the World Bank began to push for privatization of government-owned mining companies with a new set of codes, beginning with its report \"The Strategy for African Mining\". In 1997, Latin America's largest miner Companhia Vale do Rio Doce (CVRD) was privatized. These and other developments such as the Philippines 1995 Mining Act led the bank to publish a third report (\"Assistance for Minerals Sector Development and Reform in Member Countries\") which endorsed mandatory environment impact assessments and attention to the concerns of the local population. The codes based on this report are influential in the legislation of developing nations. The new codes are intended to encourage development through tax holidays, zero custom duties, reduced income taxes, and related measures. The results of these codes were analyzed by a group from the University of Quebec, which concluded that the codes promote foreign investment but \"fall very short of permitting sustainable development\". The observed negative correlation between natural resources and economic development is known as the resource curse.\n\nSafety has long been a concern in the mining business, especially in sub-surface mining. The Courrières mine disaster, Europe's worst mining accident, involved the death of 1,099 miners in Northern France on March 10, 1906. This disaster was surpassed only by the Benxihu Colliery accident in China on April 26, 1942, which killed 1,549 miners. While mining today is substantially safer than it was in previous decades, mining accidents still occur. Government figures indicate that 5,000 Chinese miners die in accidents each year, while other reports have suggested a figure as high as 20,000. Mining accidents continue worldwide, including accidents causing dozens of fatalities at a time such as the 2007 Ulyanovskaya Mine disaster in Russia, the 2009 Heilongjiang mine explosion in China, and the 2010 Upper Big Branch Mine disaster in the United States. Mining has been identified by the National Institute for Occupational Safety and Health (NIOSH) as a priority industry sector in the National Occupational Research Agenda (NORA) to identify and provide intervention strategies regarding occupational health and safety issues. The Mining Safety and Health Administration (MSHA) was established in 1978 to \"work to prevent death, illness, and injury from mining and promote safe and healthful workplaces for US miners.\" Since its implementation in 1978, the number of miner fatalities has decreased from 242 miners in 1978 to 28 miners in 2015.\n\nThere are numerous occupational hazards associated with mining, including exposure to rockdust which can lead to diseases such as silicosis, asbestosis, and pneumoconiosis. Gases in the mine can lead to asphyxiation and could also be ignited. Mining equipment can generate considerable noise, putting workers at risk for hearing loss. Cave-ins, rock falls, and exposure to excess heat are also known hazards. The current NIOSH Recommended Exposure Limit (REL) of noise is 85 dBA with a 3 dBA exchange rate and the MSHA Permissible Exposure Limit (PEL) is 90 dBA with a 5 dBA exchange rate as an 8-hour time-weighted average. NIOSH has found that 25% of noise-exposed workers in Mining, Quarrying, and Oil and Gas Extraction have hearing impairment. The prevalence of hearing loss increased by 1% from 1991-2001 within these workers.\n\nNoise studies have been conducted in several mining environments. Stageloaders (84-102 dBA), shearers (85-99 dBA), auxiliary fans (84-120 dBA), continuous mining machines (78-109 dBA), and roof bolters (92-103 dBA) represent some of the noisiest equipment in underground coal mines. Dragline oilers, dozer operators, and welders using air arcing were occupations with the highest noise exposures among surface coal miners. Coal mines had the highest hearing loss injury likelihood.\n\nProper ventilation, hearing protection, and spraying equipment with water are important safety practices in mines.\n\nAs of 2008, the deepest mine in the world is TauTona in Carletonville, South Africa at , replacing the neighboring Savuka Mine in the North West Province of South Africa at . East Rand Mine in Boksburg, South Africa briefly held the record at , and the first mine declared the deepest in the world was also TauTona when it was at .\n\nThe Moab Khutsong gold mine in North West Province (South Africa) has the world's longest winding steel wire rope, able to lower workers to in one uninterrupted four-minute journey.\n\nThe deepest mine in Europe is the 16th shaft of the uranium mines in Příbram, Czech Republic at , second is Bergwerk Saar in Saarland, Germany at . \n\nThe deepest open-pit mine in the world is Bingham Canyon Mine in Bingham Canyon, Utah, United States at over . The largest and second deepest open-pit copper mine in the world is Chuquicamata in Chuquicamata, Chile at , 443,000 tons of copper and 20,000 tons of molybdenum produced annually. \n\nThe deepest open-pit mine with respect to sea level is Tagebau Hambach in Germany, where the base of the pit is below sea level.\n\nThe largest underground mine is Kiirunavaara Mine in Kiruna, Sweden. With of roads, 40 million tonnes of ore produced yearly, and a depth of , it is also one of the most modern underground mines. The deepest borehole in the world is Kola Superdeep Borehole at . This, however, is not a matter of mining but rather related to scientific drilling.\n\nDuring the 20th century, the variety of metals used in society grew rapidly. Today, the development of major nations such as China and India and advances in technologies are fueling an ever-greater demand. The result is that metal mining activities are expanding and more and more of the world’s metal stocks are above ground in use rather than below ground as unused reserves. An example is the in-use stock of copper. Between 1932 and 1999, copper in use in the US rose from to per person.\n\n95% of the energy used to make aluminium from bauxite ore is saved by using recycled material. However, levels of metals recycling are generally low. In 2010, the International Resource Panel, hosted by the United Nations Environment Programme (UNEP), published reports on metal stocks that exist within society and their recycling rates.\n\nThe report's authors observed that the metal stocks in society can serve as huge mines above ground. However, they warned that the recycling rates of some rare metals used in applications such as mobile phones, battery packs for hybrid cars, and fuel cells are so low that unless future end-of-life recycling rates are dramatically stepped up these critical metals will become unavailable for use in modern technology.\n\nAs recycling rates are low and so much metal has already been extracted, some landfills now contain a higher concentrations of metal than mines themselves. This is especially true of aluminium, used in cans, and precious metals, found in discarded electronics. Furthermore, waste after 15 years has still not broken down, so less processing would be required when compared to mining ores. A study undertaken by Cranfield University has found £360 million of metals could be mined from just 4 landfill sites. There is also up to 20MJ/kg of energy in waste, potentially making the re-extraction more profitable. However, although the first landfill mine opened in Tel Aviv, Israel in 1953, little work has followed due to the abundance of accessible ores.\n\n\n"}
{"id": "763780", "url": "https://en.wikipedia.org/wiki?curid=763780", "title": "Occupational noise", "text": "Occupational noise\n\nOccupational noise is the amount of acoustic energy received by an employee's auditory system when they are working in the industry. Occupational noise, or industrial noise, is often a term used in occupational safety and health, as sustained exposure can cause permanent hearing damage.\n\n\"Twenty-two million workers are exposed to potentially damaging noise at work each year. Last year, U.S. business paid more than $1.5 million in penalties for not protecting workers from noise.\" - OSHA\n\nOccupational noise is considered an occupational hazard traditionally linked to loud industries such as ship-building, mining, railroad work, welding, and construction, but can be present in any workplace where hazardous noise is present.\n\nIn the United States, the National Institute for Occupational Safety and Health (NIOSH) and the Occupational Safety and Health Administration (OSHA) work together to provide standards and regulations for noise in the workplace.\n\nNational Institute for Occupational Safety and Health (NIOSH), Occupational Safety and Health Administration (OSHA), Mine Safety and Health Administration (MSHA), Federal Railroad Administration (FRA) have all set standards on hazardous occupational noise in their respective industries. Each industry is different, as workers' tasks and equipment differ, but most regulations agree that noise becomes hazardous when it exceeds 85 decibels, for an 8-hour time exposure (typical work shift). This relationship between allotted noise level and exposure time is known as an Exposure action value (EAV) or Permissible exposure limit (PEL). The EAV or PEL can be seen as equations which manipulate the allotted exposure time according to the intensity of the industrial noise. This equation works as an inverse, exponential, relationship. As the industrial noise intensity increases, the allotted exposure tie, to still remain safe, decreases.\n\nThe above calculations of PEL and EAV are based on measurements taken to determine the intensity of that particular industrial noise. A-weighted measurements are commonly used to determine noise levels that can cause harm to the human ear. There are also special exposure meters available that integrate noise over a period of time to give an value (equivalent sound pressure level), defined by standards.\n\nOccupational noise, if experienced repeatedly, at high intensity, for an extended period of time, can cause noise-induced hearing loss (NIHL) which is then classified as occupational hearing loss.\n\nNoise, in the context of industrial noise, is hazardous to a person's hearing because of its loud intensity through repeated long-term exposure. In order for noise to cause hearing impairment for the worker, the noise has to be close enough, loud enough, and sustained long enough to damage the hair cells in the auditory system. Please see Occupational hearing loss or Noise-induced hearing loss for more information regarding the physiology of hearing loss. These factors have been taken into account by the governing occupational health and safety organization to determine the unsafe noise exposure levels and durations for their respective industries.\n\nNoise can also affect the safety of the employee and others. Noise can be a causal factor in work accidents as it may mask hazards and warning signals and impede concentration. High intensity noise interferes with vital workplace communication which increases the chance of accidents and decreases productivity.\n\nNoise may also act synergistically with other hazards to increase the risk of harm to workers. In particular, toxic materials (e.g. some solvents, metals, asphyxiants and pesticides) have some ototoxic properties that may affect hearing function.\n\nModern thinking in occupational safety and health further identifies noise as hazardous to workers' safety and health. This hazard is experienced in various places of employment and through a variety of sources.\n\nNoise, in the context of industrial noise, is hazardous to a persons hearing because of its loud intensity through repeated long-term exposure. In order for Noise to cause Hearing impairment for the worker, the noise has to be close enough, loud enough and the listener has to be exposed for long enough. These factors have been taken into account by the governing occupational health and safety organizations as they determine the unsafe noise exposure levels and durations for their respective industries.\n\nNational Institute for Occupational Safety and Health (NIOSH), Occupational Safety and Health Administration (OSHA), Mine Safety and Health Administration (MSHA), Federal Railroad Administration (FRA) have all set standards on hazardous occupational noise in their respective industries. Each industry is different, as workers tasks and equipment differ, but most regulations agree that noise becomes hazardous when it exceeds 85 Decibel, for an 8-hour exposure (typical work shift). This relationship between allotted noise level and exposure time is known as an Exposure action value (EAV) or Permissible exposure limit (PEL). The EAV or PEL can be seen as equations which manipulate the allotted exposure time according to the intensity of the industrial noise. This equation works as an inverse relationship. As the industrial noise intensity increases, the allotted exposure time, to still remain safe, decreases.\n\nThese above calculations of PEL and EAV are based on measurements taken to determine the intensity of that particular industrial noise. A-weighted measurements are commonly used to determine noise levels that can cause harm to the human ear. There are also special exposure meters available that integrate noise over a period of time to give an value (equivalent sound pressure level), defined by standards.\n\nThere are several ways to limit your exposure to hazardous occupational noise. The hierarchy of controls is a guideline for reducing hazardous noise. First, the company can eliminate the noise source. If the noise source cannot be eliminated, the company must try to reduce the noise with alternative methods. This process is called acoustic quieting. \n\nAcoustic quieting is the process of making machinery quieter by damping vibrations to prevent them from reaching the observer. The company can isolate the certain piece of machinery by placing materials on the machine or in between the machine and the worker to decreases the signal intensity that reaches the worker's ear.\n\nTo decrease an employee's exposure to hazardous noise, the company can also take administrative control by limiting the employee's exposure time. This can be done by changing work shifts and switching employees out from the noise exposure area. Lastly, to decrease occupational noise exposure, hearing protection should be used. There are several types of earplugs and earmuffs that can be used to attenuate the noise to a safe level.\n\nFor a more detailed description of the hierarchy of controls, please see Occupational hearing loss. \n\nSince the hazards of occupational noise exposure were realized, programs and initiatives such as the US Buy Quiet program have been set up to regulate or discourage noise exposure. The Buy Quiet initiative promotes the purchase of quieter tools and equipment and encourages manufacturers to design quieter machines. Additionally, the Safe-In-Sound Award was created to recognize successes in hearing loss prevention programs or initiatives.\n\n\nGeneral:\n\n"}
{"id": "759377", "url": "https://en.wikipedia.org/wiki?curid=759377", "title": "Orchitis", "text": "Orchitis\n\nOrchitis or orchiditis (from the Ancient Greek ὄρχις meaning \"testicle\"; same root as \"orchid\") is inflammation of the testes. It can also involve swelling, heavy pains and frequent infection, particularly of the epididymis, and is more rarely known as didymitis (as in \"epididymis\").\n\nSymptoms of orchitis are similar to those of testicular torsion. These can include:\n\nOrchitis can be related to epididymitis infection that has spread to the testicles (then called \"epididymo-orchitis\"), sometimes caused by the sexually transmitted diseases chlamydia and gonorrhea. It has also been reported in cases of males infected with brucellosis. Orchitis can also be seen during active mumps, particularly in adolescent boys.\n\nIschemic orchitis may result from damage to the blood vessels of the spermatic cord during inguinal herniorrhaphy, and may in the worst event lead to testicular atrophy.\n\n\nIn most cases where orchitis is caused by epididymitis, treatment is an oral antibiotic such as cefalexin or ciprofloxacin until infection clears up. In both causes non-steroidal anti-inflammatory drugs such as naproxen or ibuprofen are recommended to relieve pain. Sometimes stronger pain medications in the opiate category are called for and are frequently prescribed by experienced emergency department physicians.\n\nOrchitis is not rare in bulls and rams.\n\nIt has also been described in roosters.\n\n\n"}
{"id": "24714189", "url": "https://en.wikipedia.org/wiki?curid=24714189", "title": "Outline of smoking", "text": "Outline of smoking\n\nThe following outline is provided as an overview of and topical guide to smoking:\n\nSmoking – activity of intentionally burning and inhaling a small quantity of a substance, most often tobacco.\n\nSmoking can be described as all of the following:\n\n\n\nHistory of smoking\n\n"}
{"id": "43846467", "url": "https://en.wikipedia.org/wiki?curid=43846467", "title": "Partnership for Maternal, Newborn &amp; Child Health", "text": "Partnership for Maternal, Newborn &amp; Child Health\n\nThe Partnership for Maternal, Newborn & Child Health (PMNCH) is a multi-constituency partnership hosted by the World Health Organization and chaired by Graça Machel. PMNCH seeks to achieve universal access to comprehensive, high-quality reproductive, maternal, newborn and child health care. PMNCH works with more than 650 members in the reproductive, maternal, newborn and child health (RMNCH) communities across seven constituencies:\n\n\nPMNCH describes itself as \"a platform for knowledge, advocacy and accountability to improve women and children’s health\". The Partnership is governed by a Board chaired by Graça Machel.\n\nThe Partnership plays a central role in facilitating joint action on many fronts, mainly progress towards the United Nations Millennium Development Goals (MDGs) 4 and 5, to\nreduce child mortality and improve maternal health, as tracked by the Countdown to 2015\ninitiative, and through support for the Global Strategy for Women’s and Children’s Health (Global Strategy) and Every Woman Every Child. PMNCH enables members to share\nstrategies, align objectives and resources, and collectively agree on policy interventions.\n\nPMNCH’s work and support to partners is focused on three key Strategic Objectives: \n\nThe Partnership issues annual reports on progress of commitments from more than 250 stakeholders and estimates that the Global Strategy has leveraged over US$18 billion in new and additional money for women’s and children’s health.\n\nPMNCH was launched in September 2005 when the world’s three leading maternal, newborn and child health alliances joined forces under the new name of The Partnership for Maternal, Newborn & Child Health. 80 original members joined together from the three organizations, which included: the Partnership for Safe Motherhood and Newborn Health, hosted by the World Health Organization in Geneva; the Healthy Newborn Partnership, based at Save the Children USA; and the Child Survival Partnership, hosted by UNICEF in New York. All three partnerships focused on accelerating action by countries—both donor and\ndeveloping countries—to achieve Millennium Development Goals (MDGs) 4 (reduce child\nmortality) and 5 (improve maternal health).\n\nAs of July 2014, the following bodies are represented on the PMNCH Board. The Board is chaired by Mrs Graça Machel and co-chaired by Mr CK Mishra, Government of India and Dr Flavia Bustreo, the World Health Organization.\n\n\n\n\n\n\n\n\n\n"}
{"id": "14321988", "url": "https://en.wikipedia.org/wiki?curid=14321988", "title": "Patient portal", "text": "Patient portal\n\nPatient portals are healthcare-related online applications that allow patients to interact and communicate with their healthcare providers, such as physicians and hospitals. Typically, portal services are available on the Internet at all hours of the day and night. Some patient portal applications exist as stand-alone web sites and sell their services to healthcare providers. Other portal applications are integrated into the existing web site of a healthcare provider. Still others are modules added onto an existing electronic medical record (EMR) system. What all of these services share is the ability of patients to interact with their medical information via the Internet. Currently, the lines between an EMR, a personal health record, and a patient portal are blurring. For example, Intuit Health and Microsoft HealthVault describe themselves as personal health records (PHRs), but they can interface with EMRs and communicate through the Continuity of Care Record standard, displaying patient data on the Internet so it can be viewed through a patient portal.\n\nThe central feature that makes any system a patient portal is the ability to expose individual patient health information in a secure manner through the Internet. In addition, virtually all patient portals allow patients to interact in some way with health care providers. Patient portals benefit both patients and providers by increasing efficiency and productivity. Patient portals are also regarded as a key tool to help physicians meet \"meaningful use\" requirements in order to receive federal incentive checks, especially for providing health information to patients.\nSome patient portal applications enable patients to register and complete forms online, which can streamline visits to clinics and hospitals. Many portal applications also enable patients to request prescription refills online, order eyeglasses and contact lenses, access medical records, pay bills, review lab results, and schedule medical appointments. Patient portals also typically allow patients to communicate directly with healthcare providers by asking questions, leaving comments, or sending e-mail messages.\n\nThe major shortcoming of most patient portals is their linkage to a single health organization. If a patient uses more than one organization for healthcare, the patient normally needs to log on to each organization’s portal to access information. This results in a fragmented view of individual patient data.\n\nPortal applications for individual practices typically exist in tandem with patient portals, allowing access to patient information and records, as well as schedules, payments, and messages from patients. Most patient portals require the practice to have some type of electronic medical record or patient management system, as the patient data needs to be stored in a data repository then retrieved by the patient portal.\nWhile lauding its ease-of-use, some physicians note that it is hard to encourage patients to utilize online portals to benefit both themselves and the medical practice staff.\n\nHealth care providers in the US are bound to comply with HIPAA regulations. These regulations specify what patient information must be held in confidence. Something as seemingly trivial as a name is viewed by HIPAA as protected health information. For this reason, security has always been a top concern for the industry when dealing with the adoption of patient portals. While there may be systems that are not HIPAA compliant, certainly most patient and practice portals are secure and compliant with HIPAA regulations. The use of SSL and access control patterns are commonplace in the industry. Patient access is typically validated with a user name and password.\n\nInternet portal technology has been in common use since the 1990s. The financial industry has been particularly adept at using the Internet to grant individual users access to personal information. Possibly because of the strictness of HIPAA regulations, or the lack of financial incentives for the health care providers, the adoption of patient portals has lagged behind other market segments.\n\nThe American Recovery and Reinvestment Act of 2009 (ARRA), in particular the HITECH Act within ARRA, sets aside approximately $19 billion for health information technology. This funding will potentially offset the costs of electronic medical record systems for practicing physicians. Because the conversion to electronic medical records is typically complex, systems often transition to patient portals first and then follow with a complete implementation of electronic medical records.\n\nTo attest to Meaningful Use Stage 2, eligible professionals must have 5 percent of their patients view, transmit or download their health information. Additionally, providers must implement notifications for follow up appointments and identify clinically relevant health information for more than 10 percent of their patients with two or more appointments in the preceding two years. \n\nConsequently, personal health record systems are becoming more common and available. In 2012, 57 percent of providers already had a patient portal in place. At present, individual health data are located primarily on paper in physicians' files. Patient portals have been developed to give patients better access to their information. Given the patient mobility and the development of clear interoperable standards, the best documentation of patient medical history may involve data stored outside physician offices.\n\nE-visits (remote use of medical services) may soon become one of the most commonly used options of patient portals. The most likely demographic for uptake of e-visits are patients who live in remote rural areas, far from clinical services. An Internet session would be much cheaper and more convenient than traveling a long distance, especially for simple questions or minor medical complaints.\n\nProviding a route that does not require in-person patient visits to a clinic may potentially benefit both patients and providers. Many organizations find that overall utilization drops when e-visits are implemented, in some places by as much as 25%. This makes e-visits a very interesting proposition for insurance companies, although few actually re-imburse for them currently. E-visits, with the proper functionality, also allow the patient to update their allergies, vital signs, and history information.\n\nProviding e-visits allows the standard healthcare organization to offer a product that can compete on price with the retail clinics that are popping up in strip malls and Wal-mart.\n\nSome vendors, such as athenahealth, Epic Systems and Cerner offer patient portals as one module of a complete Electronic Health Record (EHR) system. Other vendors, such as Allscripts and MedFusion, offer patient portals that can be integrated with any EHR.\n\nRecent market surveys have highlighted best of breed, or applications that excel at one or two functions, are losing ground to portals provided by large vendors. While best of breed portals are better equipped for interoperability, portals supplied by larger vendors may be overall better equipped to handle the patient engagement requirements of Meaningful Use Stage 2.\n\n\n"}
{"id": "17968101", "url": "https://en.wikipedia.org/wiki?curid=17968101", "title": "Position (obstetrics)", "text": "Position (obstetrics)\n\nIn obstetrics, position is the orientation of the fetus in the womb, identified by the location of the presenting part of the fetus relative to the pelvis of the mother. Conventionally, it is the position assumed by the fetus before the process of birth, as the fetus assumes various positions and postures during the course of childbirth.\n\nDepending upon which part of the fetus is expected to be delivered first (fetal presentation), there are many possible positions:\n\n\n\n\n"}
{"id": "7057945", "url": "https://en.wikipedia.org/wiki?curid=7057945", "title": "Positive deviance", "text": "Positive deviance\n\nPositive deviance (PD) is an approach to behavioral and social change based on the observation that in any community there are people whose uncommon but successful behaviors or strategies enable them to find better solutions to a problem than their peers, despite facing similar challenges and having no extra resources or knowledge than their peers. These individuals are referred to as positive deviants.\n\nThe concept first appeared in nutrition research in the 1970s. Researchers observed that despite the poverty in a community, some poor families had well nourished children. Some suggested using information gathered from these outliers to plan nutrition programs.\n\nPositive deviance is a strength-based approach which is applied to problems requiring behavior and social change. It is based on the following principles:\n\nThe PD approach was first operationalized and applied in programming in the field by Jerry and Monique Sternin through their work with Save the Children in Vietnam in the 1990s (Tuhus-Dubrow, Sternin, Sternin & Pascale).\n\nAt the start of the pilot 64% of children weighed in the pilot villages were malnourished. Through a PD inquiry, the villagers found poor peers in the community that through their uncommon but successful strategies, had well-nourished children. These families collected foods typically considered inappropriate for children (sweet potato greens, shrimp, and crabs), washed their children's hands before meals, and actively fed them three to four times a day instead of the typical two meals a day provided to children.\n\nWithout knowing it, PDs had incorporated foods already found in their community that provided important nutrients: protein, iron, and calcium. A nutrition program based on these insights was created. Instead of simply telling participants what to do differently, they designed the program to help them act their way into a new way of thinking. To attend a feeding session, parents were required to bring one of the newly identified foods. They brought their children and while sharing nutritious meals, learned to cook the new foods.\n\nAt the end of the two year pilot, malnutrition fell by 85%. Results were sustained, and transferred to the younger siblings of participants.\n\nThis approach to programming was different in important ways. Based on a community's own assets, the positive deviance approach operates within the specific cultural context of a given community (village, business, schools, ministry, department, hospital) and is therefore always appropriate. It provides to community members the \"social proof\" that an uncommon behavior can be adopted by all because it is already practiced by a few within the community. The solutions come from the community, therefore avoid the \"immune response\" that can occur when outside experts enter a community with best practices that are often unsuccessful in promoting sustained change. (Sternin)\n\nSince it was first applied in Vietnam, PD has been used to inform nutrition programs in over 40 countries by USAID, World Vision, Mercy Corps, Save the Children, CARE, Plan International, Indonesian Ministry of Health, Peace Corps, Food for the Hungry, among others.\n\nA positive deviance approach may follow a series of steps.\n\nA PD inquiry begins with an invitation from a community that wishes to address an important problem they face. This is an important first step of community ownership of a process that they will lead.\n\nThis process occurs with the community at the center of defining the problem for themselves. This will often lead to a problem definition that differs from the outside \"expert\" opinion of the situation. \nA quantitative baseline is established by the community. This baseline provides an opportunity for the community to reflect on the problem given the evidence at hand, and also measure the progress toward their goals.\nThis is also the beginning of the process to identify stakeholder and decision-makers regarding the issue at hand. Additional stakeholders and decision-makers will be pulled in throughout the process as they are identified.\n\nThrough the use of data and observation, the community establishes that there are Positive Deviants in their midst.\n\nThis is the Positive Deviance Inquiry. The community, having identified positive deviants, sets out to find the behaviors, attitudes, or beliefs that allow the PD to be successful. The focus is on the successful strategies of the PD, not on making a hero of the person using the strategy. This self-discovery of people/groups just like them who have found successful solutions provide \"social proof\" that this problem can be overcome now, without outside resources.\n\nNow that the community has identified successful strategies, they decide what strategies they would like to adopt, and design activities to help others access and practice these uncommon and other beneficial. Program design is not focused on spreading \"best practices\" but helping community members \"act their way into a new way of thinking\" through hands-on activities.\n\nPD-informed projects are monitored their programs and evaluated through a participatory process. As the monitoring will be decided on and performed by the community, the tools they create will be appropriate to the setting. This can allow even illiterate community members to participate through pictorial monitoring forms or other appropriate tools.\nEvaluation allows the community to see the progress they are making towards their goals and reinforces the changes they are making in behaviors, attitudes, and beliefs.\n\nThe scaling up of a PD project may happen through many mechanisms: the \"ripple effect\" of other communities observing the success and engaging in a PD project of their own, through the coordination of NGOs, or organizational development consultants. However the project is scaled up, the process of community discovery of PDs in their midst remains vital to the acceptance of new behaviors, attitudes, and knowledge.\n\nThe PD approach has been applied in hospitals in the United States, Brazil, Canada, Mexico, Colombia, and England to stop the spread of hospital acquired infections such as c-diff and Methicillin-resistant \"Staphylococcus aureus\" (MRSA). The Centers for Disease Control and Prevention (CDC) evaluated pilot programs in the U.S. and found units using the approach decreased their infections by 30-73%.\n\nAdditionally, it has been used in health care setting increase the incidence of hand washing, and improving care for patients immediately after a heart attack.\n\nTermed \"Bright Spotting\", instead of positive deviance, the primary care pilot initiative first took place in rural New Hampshire and is still ongoing. The outpatient clinic identified a complex patient population, from the clinics perspective, studied the risk factors of that population, then identified measures that would signify that a patient has become healthy and sustained health. Once these measures were identified, using both data and the practices knowledge of patient's, \"Bright Spots\" were identified as those that meet both criteria of high risk and achieved health Finding positive deviant patients through predictive analytics has also be suggested as a possible tool in discovery. Once these patients were identified the care team performed qualitative research to discover their patterns of behavior. The results were then shown to the bright spots and their families who then designed a peer learning experience with the results in mind. The community meetings were then facilitated using both positive deviance facilitation techniques as well as applying the \"Citizen Health Care Model\" which is very similar to positive deviance approaches.\n\nA PD project helped prisoners in a New South Wales prison stop smoking. Projects in Burkino Faso, Guatemala, Ivory Coast, and Rwanda addressed reproductive health in adolescents. PD maternal and newborn health projects in Myanmar, Pakistan, Egypt, and India have improved women's access to prenatal care, delivery preparation, and antenatal care for mothers and babies.\n\nPD projects to prevent the spread of HIV/AIDS took place in 2002 with motorbike taxi drivers in Vietnam and in 2004 with sex workers in Indonesia. A PD project to enhance psychological resilience amongst adolescents vulnerable to depression and anxiety was implemented in the Netherlands.\n\nA five-year PD project starting in 2003 to prevent girl trafficking in Indonesia with Save the Children and a local Indonesian NGO, helped them find viable economic options to stay in their communities.\n\nA PD project to stop Female Genital Mutilation/Cutting in Egypt began in 1998 with CEDPA (Center for Development and Population Activities), COST (Coptic Organization for Services and Training), Caritas in Minya, Community Development Agency (CDA), Monshaat Nasser in Beni Suef governorate, and the Center for Women's Legal Assistance (CEWLA). Efforts have already shown a reduction in the practice.\n\nIn Uganda, a project with the Oak Foundation and Save the Children helped girls who were child soldiers with the Lords Resistance Army in Sudan reintegrate into their communities.\n\nPD projects in New Jersey, California, Argentina, Ethiopia, and Burkina Faso have addressed drop out rates and keeping girls in school.\n\nProponents of PD within management science argue that in any population, even in such seemingly mundane groups as service personnel in a fast food environment, the positive deviants have attitudes, cognitive processes and behavioral patterns that lead to significantly improved performance in key metrics such as speed-of-service and profitability. Studies claim that widespread adoption of the positive deviant approaches consistently leads to significant performance improvement.\n\nPD had been significantly extended to the private sector, by William Seidman and Michael McCauley. Their extensions include methodologies and technologies for:\n\nPositive deviance was further extended to groups or organizations by Gary Hamel. Hamel looks to Positive Deviant companies to set the example for \"management innovation.\"\n\n\n"}
{"id": "651370", "url": "https://en.wikipedia.org/wiki?curid=651370", "title": "Precocious puberty", "text": "Precocious puberty\n\nIn medicine, precocious puberty is puberty occurring at an unusually early age. In most cases, the process is normal in every aspect except the unusually early age, and simply represents a variation of normal development. In a minority of children, the early development is triggered by a disease such as a tumor or injury of the brain. Even when there is no disease, unusually early puberty can have adverse effects on social behavior and psychological development, can reduce adult height potential, and may shift some lifelong health risks. Central precocious puberty can be treated by suppressing the pituitary hormones that induce sex steroid production. The opposite condition is delayed puberty.\n\nThe term is used with several slightly different meanings that are usually apparent from the context. In its broadest sense, and often simplified as early puberty, \"precocious puberty\" sometimes refers to any physical sex hormone effect, due to any cause, occurring earlier than the usual age, especially when it is being considered as a medical problem. Stricter definitions of \"precocity\" may refer only to central puberty starting before a statistically specified age based on percentile in the population (e.g., 2.5 standard deviations below the population mean), on expert recommendations of ages at which there is more than a negligible chance of discovering an abnormal cause, or based on opinion as to the age at which early puberty may have adverse effects. A common definition for medical purposes is onset before 8 years in girls or 9 years in boys.\n\n\"Pubertas praecox\" is the Latin term used by physicians in the 19th century. Early pubic hair, breast, or genital development may result from natural early maturation or from several other conditions.\n\nIf the cause can be traced to the hypothalamus or pituitary, the cause is considered central. Other names for this type are \"complete\" or \"true precocious\" puberty.\n\nCauses of central precocious puberty can include:\n\nCentral precocious puberty can be caused by intracranial neoplasm, infection (most commonly central nervous system tuberculosis especially in developing countries), trauma, hydrocephalus, and Angelman syndrome. Precocious puberty is associated with advancement in bone age, which leads to early fusion of epiphyses, thus resulting in reduced final height and short stature.\n\nPrecocious puberty can make a child fertile when very young, with the youngest mother on record being Lina Medina, who gave birth at the age of 5 years, 7 months and 17 days, in one report and at 6 years 5 months in another.\n\n\"Central precocious puberty (CPP) was reported in some patients with suprasellar arachnoid cysts (SAC), and SCFE (slipped capital femoral epiphysis) occurs in patients with CPP because of rapid growth and changes of growth hormone secretion.\"\n\nIf no cause can be identified, it is considered idiopathic or constitutional.\n\nSecondary sexual development induced by sex steroids from other abnormal sources is referred to as \"peripheral precocious puberty\" or \"precocious pseudopuberty.\" It typically presents as a severe form of disease with children. Symptoms are usually as a sequelae from adrenal insufficiency (because of 21-hydroxylase deficiency or 11-beta hydroxylase deficiency, the former being more common), which includes but is not limited to hypertension, hypotension, electrolyte abnormalities, ambiguous genitalia in females, signs of virilization in females. Blood tests will typically reveal high level of androgens with low levels of cortisol.\n\nCauses can include:\n\nGenerally, patients with precocious puberty develop phenotypically appropriate secondary sexual characteristics. This is called \"isosexual\" precocity.\n\nSometimes a patient may develop in the opposite direction. For example, a male may develop breasts and other feminine characteristics, while a female may develop a deepened voice and facial hair. This is called \"heterosexual\" or \"contrasexual\" precocity. It is very rare in comparison to isosexual precocity and is usually the result of unusual circumstances. As an example, children with a very rare genetic condition called aromatase excess syndrome in which exceptionally high circulating levels of estrogen are present usually develop precocious puberty. Males and females are hyperfeminized by the syndrome.\n\nMany causes of early puberty are somewhat unclear, though girls who have a high-fat diet and are not physically active or are obese are more likely to physically mature earlier. \"Obese girls, defined as at least 10 kilograms (22 pounds) overweight, had an 80 percent chance of developing breasts before their ninth birthday and starting menstruation before age 12 – the western average for menstruation is about 12.7 years.\" Exposure to chemicals that mimic estrogen (known as xenoestrogens) is a possible cause of early puberty in girls. Bisphenol A, a xenoestrogen found in hard plastics, has been shown to affect sexual development. \"Factors other than obesity, however, perhaps genetic and/or environmental ones, are needed to explain the higher prevalence of early puberty in black versus white girls.\" While more girls are increasingly entering puberty at younger ages, new research indicates that some boys are actually starting later (delayed puberty). \"Increasing rates of obese and overweight children in the United States may be contributing to a later onset of puberty in boys, say researchers at the University of Michigan Health System.\"\n\nHigh levels of beta-hCG in serum and cerebrospinal fluid observed in a 9-year-old boy suggest a pineal gland tumor. The tumor is called a \"chorionic gonadotropin secreting pineal tumor\". Radiotherapy and chemotherapy reduced tumor and beta-hCG levels normalized.\n\nIn a study using neonatal melatonin on rats, results suggest that elevated melatonin could be responsible for some cases of early puberty.\n\nFamilial cases of idiopathic central precocious puberty (ICPP) have been reported, leading researchers to believe there are specific genetic modulators of ICPP. Mutations in genes such as LIN28, and LEP and LEPR, which encode leptin and the leptin receptor, have been associated with precocious puberty. The association between LIN28 and puberty timing was validated experimentally in vivo, when it was found that mice with ectopic overexpression of LIN28 show an extended period of pre-pubertal growth and a significant delay in puberty onset.\n\nMutations in the kisspeptin (KISS1) and its receptor, KISS1R (also known as GPR54), involved in GnRH secretion and puberty onset, are also thought to be the cause for ICPP However, this is still a controversial area of research, and some investigators found no association of mutations in the LIN28 and KISS1/KISS1R genes to be the common cause underlying ICPP.\n\nThe gene MKRN3, which is a maternally imprinted gene, was first cloned by Jong et al in 1999. MKRN3 was originally named Zinc finger protein 127. It is located on human chromosome 15 on the long arm in the Prader-Willi syndrome critical region2, and has since been identified as a cause of premature sexual development or CPP. The identification of mutations in MKRN3 leading to sporadic cases of CPP has been a significant contribution to better understanding the mechanism of puberty. MKRN3 appears to act as a \"brake\" on the central hypothalamic-pituitary access. Thus, loss of function mutations of the protein allow early activation of the GnRH pathway and cause phenotypic CPP. Patients with a MKRN3 mutation all display the classic signs of CCP including early breast and testes development, increased bone aging and elevated hormone levels of GnRH and LH.\n\nStudies indicate that breast development in girls and the appearance of pubic hair in girls and boys are starting earlier than in previous generations. As a result, \"early puberty\" in children, particularly girls, as young as 9 and 10 is no longer considered abnormal, although it may be upsetting to parents and can be harmful to children who mature physically at a time when they are immature mentally.\n\nNo age reliably separates normal from abnormal processes in children, but the following age thresholds for evaluation are thought to minimize the risk of missing a significant medical problem:\n\nMedical evaluation is sometimes necessary to recognize the few children with serious conditions from the majority who have entered puberty early but are still medically normal. Early sexual development warrants evaluation because it may:\n\nEarly puberty is believed to put girls at higher risk of sexual abuse, unrelated to pedophilia because the child has developed secondary sex characteristics; however, a causal relationship is, as yet, inconclusive. Early puberty also puts girls at a higher risk for teasing or bullying, mental health disorders and short stature as adults. Helping children control their weight is suggested to help delay puberty. Early puberty additionally puts girls at a \"far greater\" risk for breast cancer later in life. Girls as young as 8 are increasingly starting to menstruate, develop breasts and grow pubic and underarm hair; these \"biological milestones\" typically occurred only at 13 or older in the past. African-American girls are especially prone to early puberty. There are theories debating the trend of early puberty, but the exact causes are not known.\n\nThough boys face fewer problems upon early puberty than girls, early puberty is not always positive for boys; early sexual maturation in boys can be accompanied by increased aggressiveness due to the surge of hormones that affect them. Because they appear older than their peers, pubescent boys may face increased social pressure to conform to adult norms; society may view them as more emotionally advanced, although their cognitive and social development may lag behind their appearance. Studies have shown that early maturing boys are more likely to be sexually active and are more likely to participate in risky behaviours.\n\nOne possible treatment is with anastrozole. Histrelin, triptorelin, or leuprorelin, any GnRH agonists, may be used. Non-continuous usage of GnRH agonists stimulates the pituitary gland to release follicle stimulating hormone (FSH) and luteinizing hormone (LH). However, when used regularly, GnRH agonists cause a decreased release of FSH and LH. Prolonged use has a risk of causing osteoporosis. After stopping GnRH agonists, pubertal changes resume within 3 to 12 months.\n\n\n"}
{"id": "19454783", "url": "https://en.wikipedia.org/wiki?curid=19454783", "title": "Prevention through design", "text": "Prevention through design\n\nPrevention through design (PtD), also called safety by design usually in Europe, is the concept of applying methods to minimize occupational hazards early in the design process, with an emphasis on optimizing employee health and safety throughout the life cycle of materials and processes. It is a concept and movement that encourages construction or product designers to \"design out\" health and safety risks during design development. The concept supports the view that along with quality, programme and cost; safety is determined during the design stage. It increases the cost-effectiveness of enhancements to occupational safety and health.\n\nThis method for reducing workplace safety risks lessens workers' reliance on personal protective equipment, which is the least effective of the hierarchy of hazard control.\n\nEach year in the U.S., 55,000 people die from work-related injuries and diseases, 294,000 are made sick, and 3.8 million are injured. The annual direct and indirect costs have been estimated to range from $128 billion to $155 billion. Recent studies in Australia indicate that design is a significant contributor in 37% of work-related fatalities; therefore, the successful implementation of prevention through design concepts can have substantial impacts on worker health and safety.\n\nThe National Institute for Occupational Safety and Health (NIOSH) in the United States is a major contributor and promoter of PtD policy and guidelines. NIOSH considers PtD to be \"the most effective and reliable type\" of prevention of occupational injuries. A core tenet of PtD philosophy the concept of addressing workplace hazards using methods at the top of the Hierarch of Controls, namely elimination and substitution.\n\nWithin Europe, construction designers are legally bound to design out risks during design development to reduce hazards in the construction and end use phases via the Mobile Worksite Directive (also known as CDM regulations in the UK). The concept supports this legal requirement. Some Notified Bodies provide testing and design verification services to ensure compliance with the safety standards defined in regulation codes such as the American Society of Mechanical Engineers. Many non-governmental organizations have been established to support this aim, principally in the UK, Australia and the United States.\n\nWhile engineering as a rule factors human safety into the design process, a modern appraisal of specific links to design and workers' safety can be seen in efforts beginning in the 1800s. Trends included the widespread implementation of guards for machinery, controls for elevators, and boiler safety practices. This was followed by enhanced design for ventilation, enclosures, system monitors, lockout/tagout controls, and hearing protectors. More recently, there has been the development of chemical process safety, ergonomically engineered tools, chairs, and work stations, lifting devices, retractable needles, latex-free gloves, and a parade of other safety devices and processes.\n\nIn 2007, the National Institute for Occupational Health and Safety began its National Initiative on Prevention through Design with the goal of promoting prevention through design philosophy, practice, and policy.\n\nPrevention through design represents a shift in approach for on-the-job safety. It involves evaluating potential risks associated with processes, structures, equipment, and tools. It takes into consideration the construction, maintenance, decommissioning, and disposal or recycling of waste material.\nThe idea of redesigning job tasks and work environments has begun to gain momentum in business and government as a cost-effective means to enhance occupational safety and health. Many U.S. companies openly support PtD concepts and have developed management practices to implement them. Other countries are actively promoting PtD concepts as well. The United Kingdom began requiring construction companies, project owners, and architects to address safety and health during the design phase of projects in 1994. Australia developed the Australian National OHS Strategy 2002–2012, which set \"eliminating hazards at the design stage\" as one of five national priorities. As a result, the Australian Safety and Compensation Council (ASCC) developed the Safe Design National Strategy and Action Plans for Australia encompassing a wide range of design areas.\n\nThe National Institute for Occupational Safety and Health is a large contributor to prevention through design efforts in the United States. Several NIOSH initiatives and guidelines directly or indirectly advocate for PtD practices. Through NIOSH efforts, the U.S. Green Building Council posted new PtD credits available for Leadership in Energy and Environmental Design (LEED) certification for construction. Additionally, they provide a wide variety of educational and guidance materials on the topic of PtD The NIOSH \"Buy Quiet\" initiative uses elements of prevention through design to encourage companies to buy quieter machinery, thereby reducing occupational hearing loss for their workers.\n\n\n"}
{"id": "8543665", "url": "https://en.wikipedia.org/wiki?curid=8543665", "title": "Pseudonymization", "text": "Pseudonymization\n\nPseudonymization is a data management and de-identification procedure by which personally identifiable information fields within a data record are replaced by one or more artificial identifiers, or pseudonyms. A single pseudonym for each replaced field or collection of replaced fields makes the data record less identifiable while remaining suitable for data analysis and data processing. \n\nPseudonymization can be one way to comply with the European Union's new General Data Protection Regulation demands for secure data storage of personal information. Pseudonymized data can be restored to its original state with the addition of information which then allows individuals to be re-identified, while anonymized data can never be restored to its original state.\n\nThe choice of which data fields are to be pseudonymized is partly subjective. Less selective fields, such as Birth Date or Postal Code are often also included because they are usually available from other sources and therefore make a record easier to identify. Pseudonymizing these less identifying fields removes most of their analytic value and is therefore normally accompanied by the introduction of new derived and less identifying forms, such as year of birth or a larger postal code region.\n\nData fields that are less identifying, such as date of attendance, are usually not pseudonymized. It is important to realize that this is because too much statistical utility is lost in doing so, not because the data cannot be identified. For example, given prior knowledge of a few attendance dates it is easy to identify someone's data in a pseudonymized dataset by selecting only those people with that pattern of dates. This is an example of an inference attack.\n\nThe weakness of pseudonymized data to inference attacks is commonly overlooked. A famous example is the AOL search data scandal.\n\nProtecting statistically useful pseudonymized data from re-identification requires:\n\nThe pseudonym allows tracking back of data to its origins, which distinguishes pseudonymization from anonymization, where all person-related data that could allow backtracking has been purged. Pseudonymization is an issue in, for example, patient-related data that has to be passed on securely between clinical centers.\n\nThe application of pseudonymization to e-health intends to preserve the patient's privacy and data confidentiality. It allows primary use of medical records by authorized health care providers and privacy preserving secondary use by researchers. However, plain pseudonymization for privacy preservation often reaches its limits when genetic data are involved (see also genetic privacy). Due to the identifying nature of genetic data, depersonalization is often not sufficient to hide the corresponding person. Potential solutions are the combination of pseudonymization with fragmentation and encryption.\n\nAn example of application of pseudonymization procedure is creation of datasets for de-identification research by replacing identifying words with words from the same category (e.g. replacing a name with a random name from the names dictionary), however, in this case it is in general not possible to track data back to its origins.\n\n"}
{"id": "50066385", "url": "https://en.wikipedia.org/wiki?curid=50066385", "title": "Reproductive health care for incarcerated women in the United States", "text": "Reproductive health care for incarcerated women in the United States\n\nIn the United States, prisons are obligated to provide health care to prisoners. Such health care is sometimes called \"correctional medicine\". In women's prisons, correctional medicine includes attention to reproductive health.\n\nThe number of women incarcerated in the United States has increased greatly over the last few decades, and at a faster rate than the number of incarcerated men. Many of the same factors that increase women's likelihood of incarceration also put them at a higher risk for contracting HIV/AIDS and other sexually transmitted infections, and for having high-risk pregnancies. The majority of incarcerated women are economically disadvantaged and poorly educated, and have not had adequate access to preventive healthcare prior to their imprisonment, such as Pap tests, STI screening, and pregnancy counseling.\n\nEstelle v. Gamble obligates prisons to provide for the serious medical needs of their inmates. However, it also requires that an incarcerated person must demonstrate that they had a serious medical need, and that they didn't receive adequate medical care because officials showed \"deliberate indifference.\" Due to the difficulty inherent in proving that a prison official knew about a medical condition, yet failed to respond to it, this standard makes it difficult to hold correctional facilities accountable for their mistakes.\n\nTodaro v. Ward argued that women within a New York prison did not have adequate, constitutional access to healthcare. Since Todaro v. Ward was the first major court case that called into question incarcerated women's actual access to health care, it spurred organizations such as the American Medical Association, American Correctional Association, and the American Public Health Association to start creating standards for health care within prisons.\n\nWith this ruling, the court determined that the medical care provided to prisoners must only be \"reasonable,\" not necessarily \"perfect, the best obtainable, or even very good.\"\n\nCurrently, no national agency monitors the treatment of prisoners, although a few governmental and non-governmental agencies do provide monitoring standards that facilities may use if they wish to become accredited (about thirty percent of prisons are accredited), and some federal legislation has been passed regarding correctional health care.\n\nThis legislation required that incarcerated individuals pay for some of their health care bill while in prison.\n\nThe Prison Litigation Reform Act made it much more difficult for prisoners to file class action and individual lawsuits against a prison. The Prison Litigation Reform Act (PRLA) requires that prisoners exhaust a facility's own administrative resources and solutions before attempting to file a lawsuit. This can be detrimental because prisoners' health problems are often time-sensitive. PRLA also includes a cap on attorney fees, which has made fewer attorneys willing to represent prisoners. Although PRLA applies to all lawsuits filed by incarcerated people, it is particularly relevant for women's health because most lawsuits filed by incarcerated women concern substandard health care.\n\nAs of 2005, about five to ten percent of incarcerated women were pregnant (most upon intake), and about 2,000 incarcerated women give birth every year. Incarcerated women already tend to have more high-risk pregnancies, due to possible complications by drug and alcohol abuse and STIs, histories of victimization and abuse, and poor support networks, so they are particularly in need of quality prenatal care as compared to non-incarcerated women.\n\nStudies have found mixed results when it comes to the effects of incarceration on pregnancy outcomes. Some studies have found that incarceration correlates to lower birth weight, and an increased chance of complications in the pregnancy; others have found the opposite: that incarceration is associated with higher birth weight and a decreased likelihood of premature delivery. However, the latter finding may simply speak to the low level of medical care available to women in poverty in the free world.\n\nA 1996 study conducted by the National Council on Crime and Delinquency (NCCD) on women in California, Connecticut, and Florida state prisons found a lack of adequate prenatal and postnatal medical care, prenatal nutrition, level of methadone maintenance for opiate-dependent pregnant inmates, education regarding childbirth and parenting, and preparation for the mother's separation from her child. Often, inmates report receiving no regular pelvic exams or sonograms, and little to no information about proper prenatal care and nutrition. Prison diets often lack proper nutrition, particularly for the changing and special dietary needs of pregnant women, and many women prisoners report not being allowed to alter their diets during their pregnancy.\n\nDespite the lack or low quality of prenatal care in most institutions, many correctional facilities have made strides to provide adequate care. For example, Washington State has a program called the Birth Attendants Prison Doula project, which provides support to incarcerated pregnant and postpartum women. When asked about her prison's response to pregnancy, Boo, an inmate in an Arizona correctional facility, said:\n\nIn general, pregnant inmates are transported to outside medical facilities to give birth, because most correctional facilities are not medically equipped to provide such services. These transports often result in complications due to the risk of injury to both the mother and the child, and due to the additional stress.\n\nThe practice of shackling inmates, both in transit to the hospital and during labor, is also common at many facilities. Forty-one states permit the use of restraints during transport to the hospital, and twenty-three states and the federal government permit the use of restraints during labor. These restraints may include belly chains, shackles, handcuffs, or nylon \"soft restraints.\" The use of restraints on women so far into their pregnancy and during labor poses many health risks. Restraints inhibit the movement of a woman, something that aids the progression of the labor and alleviates some of her discomfort. They may also hinder the ability of health care professionals to respond quickly to emergencies during the labor. Organizations such as Amnesty International have pressured correctional facilities to stop the use of shackles on pregnant women, pointing out the fact that most pregnant women are incarcerated for nonviolent offenses and pose no risk (especially during childbirth), so the restraints are unnecessary. Maria Jones, a pregnant inmate, described her experience being shackled during labor:\n\nIncarcerated women are 15 times more likely to be infected with HIV than are women who are free, and are also more than twice as likely than incarcerated men to be infected with HIV. For example, in New York, a state which does blind testing, 14.6% of incarcerated women and 7.3% of incarcerated men tested positive for HIV. The 1996 NCCD study found that female African American and Hispanic/Latina inmates were significantly more likely than their White counterparts to report testing positive for HIV. Many of the same social factors that increase the likelihood of incarceration for women, such as poverty, race, gender, and a history of victimization, are also correlated to HIV infection.\n\nA 2000 study by the American Correctional Association found that mandatory HIV testing is conducted upon intake in 23 states, and that a few also provide 6-month follow-up testing. Most prisons test inmates who either ask to be tested, or display symptoms of HIV (this is the policy in 44 out of 51 jurisdictions), and fifteen states also specifically test inmates that are in high-risk groups. Three states, the District of Columbia, and the Federal Bureau of Prisons test inmates upon release. The same American Correctional Association study also found that most prisons provide HIV-positive individuals with medications while they are in the prison, may provide them with a small supply after they are released, and may also direct them towards community resources where they can receive more medication. Treatment of inmates who have tested positive for HIV varies greatly from state to state. For example, in New Jersey, female HIV positive prisoners were shackled to their beds for up to six months after being diagnosed. However, Ohio and New York, among other states, have infirmaries specifically adapted to attend to the needs of inmates with HIV/AIDS, and allow some inmates to reside in a hospital for treatment. In addition to this, New York State's Division of Health Services department does regular evaluations of the state prisons' services for AIDS patients, and also provides space for support groups to meet, and patients to be counseled on their illness.\n\nWomen's symptoms and treatment needs for HIV infection are quite different from those of men, but the treatment resources for incarcerated women are often limited. The treatment of HIV requires a specialist, and generally, prison doctors don't have adequate training to treat women effectively. Or, prisons often do not have adequate facilities, staff, or a follow-up treatment system. For example, the 1996 NCCD study also found that within the entire California women's prison system, one of the largest in the nation, there was only one full-time specialist able to provide \"gender-specific treatment\" to women with HIV. Male prisoners, however, did have access to gender-specific HIV treatment.\n\nFederal courts have established, according to constitutional law, that prisoners retain the right to have an abortion once they are incarcerated. However, state standards regarding abortion for incarcerated women are unclear - actions are often left up to the discretion of prison officials on a case-by-case basis.\n\nMost state abortion policies are written and approved without going through the administrative process which other policies are generally subject to; thus, they are frequently incomplete. In fact, fourteen state DOCs have no official written abortion policies, and others simply will not release or publish their policies. Alaska has a provision which bans abortion funding in its policy on general medical services, but no specific policy guidelines on abortion. Other states may not define the exact type of abortions they pay for. Most states require that women pay for all of the costs of an abortion procedure, including transportation to the clinic (which is often far due to prisons' rural locations, and the urban locations of most abortion clinics), security, and the actual surgery. Minnesota and Wisconsin are the only two states which explicitly mention in their policies that they pay for abortions if a woman has been raped. The policy of the Federal Bureau of Prisons since 1987 has been to pay for abortion only in the cases of rape or life endangerment (although they do pay for transportation to the clinic). Women's experiences and perceptions of their own access to abortion reflect these policies. In a nationwide survey of correctional health care and inmates' access to abortion, 68% of respondents said that women within their prison were allowed to have an abortion if they requested one. However, many respondents also stated that although their prison did allow access to abortion, women receive little to no help with arranging an appointment, paying for the procedure, and getting themselves to the clinic.\n\nAccording to the U.S. Department of Justice Bureau of Justice Statistics, cancer is one of the leading causes of death for women (both inside and outside prison), and one-quarter of deaths of incarcerated women due to cancer are from breast, cervical, ovarian, and uterine cancer. In particular, women in prison are at a high risk for contracting cervical cancer due to their high rates of substance abuse and diseases such as hepatitis C, HIV, and other STIs. However, reproductive health care reform in prisons generally focuses on pregnancy or HIV.\n\nIncarcerated women have higher rates of STIs and gynecologic infections than non-incarcerated women. In fact, one study estimated that about 9% of incarcerated women tested positive for gonococcus infection, and between 11 and 17% of incarcerated women are infected with chlamydia. However, not every infected woman will be diagnosed and given treatment, because many prisons only test women who request to be tested, or show symptoms.\n\nStates often have their own legislation regarding correctional health care, but it doesn't always completely take into consideration the complexities of women's reproductive health. For example, the New York State Department of Corrections and Community Supervision (DOCCS) has issued two main documents which outline their institutional health care policies: the Health Services Policy Manual, and the Women's Health Primary Care Practice Guideline (first published in 2000, and updated in 2008 and 2011). Within these documents, the Patient Bill of Rights includes the right of a patient to respectful care, the right to refuse treatment, and the right to complete information regarding a diagnosis; the Professional Code of Ethics outlines the standards that staff must follow, including having a respect for human dignity, and a professional relationship with the patient. The DOCCS does not monitor how well facilities actually adhere to these standards, and there are no consequences for those who do not. Also, many women are not informed of their patient's rights, so are unaware when standards have been breached.\n\nThe DOCCS has no written policies on pregnancy tests, pregnancy options counseling, abortion, ectopic pregnancy, miscarriage, stillbirth, nutrition for pregnant and nursing women, or hysterectomies, and has incomplete policies regarding menopause, vitamins, health care for pregnant women, women in labor, or women who have recently given birth. The DOCCS policies stray from community standards in the areas of the starting age for yearly GYN check-ups, the frequency of breast exams and Pap smears, follow-up for abnormal Pap smears, frequency of prenatal visits and ultrasounds, and the time frame for postpartum check-ups for women who have a Caesarean section.\n\nThe Federal Bureau of Prisons' policy is to provide each inmate with a complete medical exam (which includes gynecological and obstetrical history) within 30 days of admission. The BOP currently adheres to the standards for yearly exams put forth by the American College of Obstetrics and Gynecology. According to a 1997 survey, approximately 90 percent of the inmates in women's state prisons reported having received a gynecologic exam from their institution upon intake. Additionally, an American Correctional Association study found that most women's correctional institutions provided OB/GYN services, prenatal and postpartum care, mammography, and Pap smears upon request. Fewer provided counseling for inmates regarding their reproductive health, many correctional facilities do not provide follow-up exams, and screenings often do not continue on the recommended schedule. In a New York city juvenile detention facility, the 5,000 youth who went through the system annually were all served by a single physician. Less than one-third of the inmates received a Pap smear test, and one in five were tested for gonorrhea, chlamydia, and syphilis.\n\nWomen often report long delays and wait times in signing up for appointments with a GYN doctor, and in getting treated. For example, more than half of respondents in a survey of incarcerated women in New York state prisons said that they were not able to see the GYN when necessary, and 47% of respondents stated that their problems became worse in the time that they had to wait. In one extreme case, Sara, a woman incarcerated in New York, had to wait seven months before she was finally diagnosed with an aggressive cancer. Another woman had to wait four months to receive a colposcopy, a follow-up to her Pap test. Another woman at the same prison wrote:\n\nIn a qualitative study on the experiences of incarcerated women with the Papanicolaou test (a test which screens for cervical cancer) in California state prisons, researchers found a lack of communication between medical providers within the prison, which tended to result in long delays or cancellations of treatment, no standardized process for scheduling a Pap test, and a lack of education and explanation regarding both the test itself, and how to fill out the medical forms associated with it.\n\nIn addition to this, women who aren't called in for a Pap test are sometimes required to pay a $5 fee if they request one. This can be a significant barrier to requesting a test, considering an average prisoner's wage is around 7 to 13 cents per hour.\n\nIn the same study, women brought attention to the discomfort suffered by inmates with personal histories of sexual abuse and victimization when they were forced to be examined by a male physician. Women interviewed in New York State prisons had similar complaints about their physicians' seeming lack of awareness of female prisoners' past traumas, and the lack of explanation of the procedures. One woman stated:\n\nAnother consideration in correctional reproductive health care is the relationship between the prisoner-patients and the physicians. Studies have shown that female inmates often report distrust of physicians and disappointment with their interactions with providers.\n\nResearchers have found that most of the women interviewed in the 2005 California state prison study had negative perceptions of their gynecologic tests and treatment. According to the women interviewed, the prison doctors who performed their tests were often unprofessional and disrespectful. One woman described her experience, saying: \"They expect us to give them respect, but they don't respect us. They treat us like we are animals just because we are incarcerated.\" The California prison system employs corrections officers who are also trained to be licensed nurses (called medical technical assistants). Similarly, in Chicago, the county jails train corrections officers as doulas and birth attendants. Inmates generally must request medical treatment through either these officers, or through other, non-medically trained correctional staff, and studies have shown that female patients' complaints and requests for medical assistance are often not taken as seriously as those of male patients. The simultaneous positions that these officers hold as both security personnel and medical caretakers or advocates may contribute to female inmates' distrust of them.\n\nA 1999 study found that female inmates were more likely than their male counterparts to think that their access to health care was inadequate and that their quality of care was low compared to the care male prisoners were given. Women in this study also visited the health care facility more often than their male counterparts and reported being healthy less often than men did.\n\nA lack of resources, specifically of adequate staff, within facilities is largely what contributes to the substandard care of inmates. For example, although California's prison system is the largest in the country, an obstetrician/gynecologist was not hired at the California Institution for Women (CIW) until pregnant prisoners in California filed a class action lawsuit (Harris v. McCarthy) against CIW. The Valley State Prison for Women (VSPW), another prison in California, had just two OB/GYN doctors on staff in 2000 (one of whom was indicted on four counts of sexual misconduct and eventually fired). Albion, a women's prison in New York State, has 1,000 prisoners, but only one GYN doctor, who works only 16 hours per week. Taconic, another facility in New York, which holds about 370 women, has no GYN doctor on site. In these cases, routine GYN care falls to the general nurse practitioner or Medical Director. In addition to the lack of staff, medical doctors are often not on duty on the weekends and during evening hours. This can be dangerous if medical emergencies arise during non-business hours. For example, a prisoner in CIW in 1997 went into labor over the weekend, when there was only one nurse on duty. The nurse strapped the woman into a gurney, but refused to help with her labor. When the baby was born not breathing, the nurse had to call paramedics, because she was unable to activate the breathing apparatus. By the time the newborn was transported to the hospital, he was declared brain dead.\n\nThe difficulty with recruiting qualified staff can be attributed to both the physical isolation of women's prisons (often placed in areas that are considered undesirable to live in, where there are few available medical professionals), and to the relative lack of medical resources and low salaries that prisons offer to medical staff. Dr. Valda Chijide, a former HIV doctor in an Alabama prison, for instance, resigned from her position due to inadequate support. In this case, the HIV unit was rat-infested, with broken windows covered in plastic.\n\nThere is also a trend towards privatizing health care by hiring outside, for-profit health care companies to provide medical care to inmates. According to a 1996 survey by the National Institute of Corrections (NIC), forty-four state Departments of Corrections contract out at least some of their medical care to private vendors - in 1996 this amounted to $706 million.\n\nThis can result in a lack of accountability, as proper monitoring makes a contract more expensive. Oversight is often lacking. In Alabama, the same official who previously held a high level position at Prison Health Services, one of the nation's largest private vendors, now works in a position within the Alabama Department of Corrections which requires her to oversee Prison Health's compliance to the contract. Prison Health Services, which has cared for 237,000 inmates, has had to pay millions of dollars in fines and settlements due to inadequate care. In another example, according to a former supervisory nurse of a jail operated by Correctional Medical Services (CMS), a large private contractor, the jail would often release pregnant women when they went into labor, and then arrest them again after they gave birth, in order to avoid having to pay for the inmates' medical expenses.\n\nLack of accountability also results due to varying state and federal laws on private contractors' liability for medical abuse and neglect. Private contractors can be challenged under the same federal civil rights law that applies to state and local governments, but these corporations cannot be sued for unconstitutional medical practices.\n\n"}
{"id": "5584134", "url": "https://en.wikipedia.org/wiki?curid=5584134", "title": "Safe bottle lamp", "text": "Safe bottle lamp\n\nThe Safe bottle lamp, called sudeepa or sudipa for \"good lamp\", is a safer kerosene lamp designed by Wijaya Godakumbura of Sri Lanka. The safety comes from heavier glass, a secure screw-on metal lid, and two flat sides which prevent it from rolling if knocked over.\n\nAs surgeon Dr. Godakumbura saw many burn cases caused by kerosene lamp fires. Over 1 million homes in Sri Lanka do not have electricity, and rely on kerosene lamps for illumination, often improvised lamps made from bottles. These tall lamps tip easily, and when they do, the wick holder often falls out and starts a sudden, intense fire. Often the fuel falls on a nearby person, setting them ablaze and resulting in severe burns, often fatal.\n\nIn 1992, Dr. Godakumbura set out to design a new lamp that was both safer, and inexpensive enough to be affordable by the impoverished Sri Lankans at risk for these fires. The resulting lamp is a small, flattened sphere, which resists tipping and rolling. It is made of thick glass to resist breaking, and has a screw-on metal cap that holds the wick in place and prevents spilling.\n\nIn 1993, with contributions from numerous sources, including Science Fiction writer and Sri Lanka resident Arthur C. Clarke, and the Canadian High Commission, the lamp was put into production.\n\nAvailable for a cost of less than US$0.25 each, over half a million of the new lamps have been sold, and Dr. Godakumbura hopes to continue producing the new lamps until use of improvised lamps drops to a small percentage of lamp use in Sri Lanka.\n\nHaving received a Rolex Award for Enterprise in 1998, Dr. Godakumbura established the Safe Bottle Lamp Foundation (SBLF), a non-profit organization. The Foundation is governed by a board of directors and employs two full-time staff .\n\nIn addition to the Rolex Award, the foundation and Dr. Godakumbura have received a range of other local and international awards and grants. Among these are a Lindbergh Foundation Grant and a BBC World Challenge Award. The project has been featured in many international publications such as TIME, Newsweek, Science and Nature, National Geographic and La Figaro.\n\nDr. Godakumbura has represented the foundation in many international conferences on burn and accident prevention as a speaker or as a participant. The foundation and the Sudeepa lamp have been promoted as a replicable solution for other developing countries where accidental burns due to unsafe lamps is prevalent.\n\n\n"}
{"id": "1571927", "url": "https://en.wikipedia.org/wiki?curid=1571927", "title": "Social support", "text": "Social support\n\nSocial support is the perception and actuality that one is cared for, has assistance available from other people, and most popularly, that one is part of a supportive social network. These supportive resources can be emotional (e.g., nurturance), tangible (e.g., financial assistance), informational (e.g., advice), or companionship (e.g., sense of belonging) and intangible (e.g., personal advice).\n\nSocial support can be measured as the perception that one has assistance available, the actual received assistance, or the degree to which a person is integrated in a social network. Support can come from many sources, such as family, friends, pets, neighbors, coworkers, organizations, etc. Government-provided social support is often referred to as public aid in foreign nations. \n\nSocial support is studied across a wide range of disciplines including psychology, medicine, sociology, nursing, public health, education, rehabilitation, and social work. Social support has been linked to many benefits for both physical and mental health, but \"social support\" (e.g., gossiping about friends) is not always beneficial.\n\nSocial support theories and models were prevalent as intensive academic studies in the 1980s and 1990s , and are linked to the development of caregiving and payment models, and community delivery systems in the US and around the world. \n\nTwo main models have been proposed to describe the link between social support and health: the buffering hypothesis and the direct effects hypothesis. Gender and cultural differences in social support have also been found in fields such as education \"which may not control for age, disability, income and social status, ethnic and racial, or other significant factors\".\n\nSocial support can be categorized and measured in several different ways.\n\nThere are four common functions of social support:\n\n\nResearchers also commonly make a distinction between perceived and received support. \"Perceived support\" refers to a recipient’s subjective judgment that providers will offer (or have offered) effective help during times of need. \"Received support\" (also called enacted support) refers to specific supportive actions (e.g., advice or reassurance) offered by providers during times of need.\n\nFurthermore, social support can be measured in terms of structural support or functional support. \"Structural support\" (also called \"social integration\") refers to the extent to which a recipient is connected within a social network, like the number of social ties or how integrated a person is within his or her social network. Family relationships, friends, and membership in clubs and organizations contribute to social integration. \"Functional support\" looks at the specific functions that members in this social network can provide, such as the emotional, instrumental, informational, and companionship support listed above.\nData suggests that emotional support may play a more significant role in protecting individuals from the deleterious effects of stress than structural means of support, such as social involvement or activity.\n\nThese different types of social support have different patterns of correlations with health, personality, and personal relationships. For example, perceived support is consistently linked to better mental health whereas received support and social integration are not. In fact, research indicates that perceived social support that is untapped can be more effective and beneficial than utilized social support. Some have suggested that invisible support, a form of support where the person has support without his or her awareness, may be the most beneficial.\n\nSocial support can come from a variety of sources, including (but not limited to): family, friends, romantic partners, pets, community ties, and coworkers. Sources of support can be natural (e.g., family and friends) or more formal (e.g., mental health specialists or community organizations). \nThe source of the social support is an important determinant of its effectiveness as a coping strategy. Support from a romantic partner is associated with health benefits, particularly for men. However, one study has found that although support from spouses buffered the negative effects of work stress, it did not buffer the relationship between marital and parental stresses, because the spouses were implicated in these situations.However, work-family specific support worked more to alleviate work-family stress that feeds into marital and parental stress. Employee humor is negatively associated with burnout, and positively with, stress, health and stress coping effectiveness. Additionally, social support from friends did provide a buffer in response to marital stress, because they were less implicated in the marital dynamic.\n\nEarly familial social support has been shown to be important in children’s abilities to develop social competencies, and supportive parental relationships have also had benefits for college-aged students. Teacher and school personnel support have been shown to be stronger than other relationships of support. This is hypothesized to be a result of family and friend social relationships to be subject to conflicts whereas school relationships are more stable.\n\nSocial support is also available among social media sites. As technology advances, the availability for online support increases. Social support can be offered through social media websites such as blogs, Facebook groups, health forums, and online support groups. According to Hwang, the support is similar to face-to-face social support, but also offers the unique aspects of convenience, anonymity, and non-judgmental interactions. Support sought through social media also provides users with emotional comfort that relates them to others while creating awareness about particular health issues.\n\nResearch conducted by Winzelberg et al. evaluated an online support group for women with breast cancer finding participants were able to form fulfilling supportive relationships in an asynchronous format and this form of support proved to be effective in reducing participants' scores on depression, perceived stress, and cancer-related trauma measures. This type of online communication can increase the ability to cope with stress. Social support through social media is available to everyone with internet access and allows users to create relationships and receive encouragement for whatever issue they may be facing.\n\nCoulson claims online support groups provide a unique opportunity for health professionals to learn about the experiences and views of individuals. This type of social support can also benefit users by providing them with a variety of information. Seeking informational social support allows users to access suggestions, advice, and information regarding health concerns or recovery. Many need social support, and with its emergence on social media access can be obtained from a wider range of people in need. Wong and Ma (2016) have done research that shows online social support affects users' online subjective well-being. \n\nSocial support profile is associated with increased psychological well-being in the workplace and in response to important life events.\nIn stressful times, social support helps people reduce psychological distress (e.g., anxiety or depression). Social support can simultaneously function as a problem-focused (e.g. receiving tangible information that helps resolve an issue) and emotion-focused coping strategy (e.g. used to regulate emotional responses that arise from the stressful event) Social support ≤has been found to promote psychological adjustment in conditions with chronic high stress like HIV, rheumatoid arthritis, cancer, stroke, and coronary artery disease. Additionally, social support has been associated with various acute and chronic pain variables (for more information, see Chronic pain).\n\nPeople with low social support report more sub-clinical symptoms of depression and anxiety than do people with high social support. In addition, people with low social support have higher rates of major mental disorder than those with high support. These include posttraumatic stress disorder, panic disorder, social phobia, major depressive disorder, dysthymic disorder, and eating disorders. Among people with schizophrenia, those with low social support have more symptoms of the disorder. In addition, people with low support have more suicidal ideation, and more alcohol and (illicit and prescription) drug problems. Similar results have been found among children. Religious coping has especially been shown to correlate positively with positive psychological adjustment to stressors with enhancement of faith-based social support hypothesized as the likely mechanism of effect. However, more recent research reveals the role of religiosity/spirituality in enhancing social support may be overstated and in fact disappears when the personality traits of \"agreeableness\" and \"conscientiousness\" are also included as predictors.\n\nIn a 2013 study, Akey et al. did a qualitative study of 34 men and women diagnosed with an eating disorder and used the Health Belief Model (HBM) to explain the reasons for which they forgo seeking social support. Many people with eating disorders have a low perceived susceptibility, which can be explained as a sense of denial about their illness. Their perceived severity of the illness is affected by those to whom they compare themselves to, often resulting in people believing their illness is not severe enough to seek support. Due to poor past experiences or educated speculation, the perception of benefits for seeking social support is relatively low. The number of perceived barriers towards seeking social support often prevents people with eating disorders from getting the support they need to better cope with their illness. Such barriers include fear of social stigma, financial resources, and availability and quality of support. Self-efficacy may also explain why people with eating disorders do not seek social support, because they may not know how to properly express their need for help. This research has helped to create a better understanding of why individuals with eating disorders do not seek social support, and may lead to increased efforts to make such support more available. Eating disorders are classified as mental illnesses but can also have physical health repercussions. Creating a strong social support system for those affected by eating disorders may help such individuals to have a higher quality of both mental and physical health.\n\nVarious studies have been performed examining the effects of social support on psychological distress. Interest in the implications of social support were triggered by a series of articles published in the mid-1970s, each reviewing literature examining the association between psychiatric disorders and factors such as change in marital status, geographic mobility, and social disintegration. Researchers realized that the theme present in each of these situations is the absence of adequate social support and the disruption of social networks. This observed relationship sparked numerous studies concerning the effects of social support on mental health.\n\nOne particular study documented the effects of social support as a coping strategy on psychological distress in response to stressful work and life events among police officers. Talking things over among coworkers was the most frequent form of coping utilized while on duty, whereas most police officers kept issues to themselves while off duty. The study found that the social support between co-workers significantly buffered the relationship between work-related events and distress.\n\nOther studies have examined the social support systems of single mothers. One study by D'Ercole demonstrated that the effects of social support vary in both form and function and will have drastically different effects depending upon the individual. The study found that supportive relationships with friends and co-workers, rather than task-related support from family, was positively related to the mother's psychological well-being. D'Ercole hypothesizes that friends of a single parent offer a chance to socialize, match experiences, and be part of a network of peers. These types of exchanges may be more spontaneous and less obligatory than those between relatives. Additionally, co-workers can provide a community away from domestic life, relief from family demands, a source of recognition, and feelings of competence. D'Ercole also found an interesting statistical interaction whereby social support from co-workers decreased the experience of stress only in lower income individuals. The author hypothesizes that single women who earn more money are more likely to hold more demanding jobs which require more formal and less dependent relationships. Additionally, those women who earn higher incomes are more likely to be in positions of power, where relationships are more competitive than supportive.\n\nMany studies have been dedicated specifically to understanding the effects of social support in individuals with posttraumatic stress disorder (PTSD). In a study by Haden et al., when victims of severe trauma perceived high levels of social support and engaged in interpersonal coping styles, they were less likely to develop severe PTSD when compared to those who perceived lower levels of social support. These results suggest that high levels of social support alleviate the strong positive association between level of injury and severity of PTSD, and thus serves as a powerful protective factor. In general, data shows that the support of family and friends has a positive influence on an individual's ability to cope with trauma. In fact, a meta-analysis by Brewin et al. found that social support was the strongest predictor, accounting for 40%, of variance in PTSD severity. However, perceived social support may be directly affected by the severity of the trauma. In some cases, support decreases with increases in trauma severity.\n\nCollege students have also been the target of various studies on the effects of social support on coping. Reports between 1990 and 2003 showed college stresses were increasing in severity. Studies have also shown that college students' perceptions of social support have shifted from viewing support as stable to viewing them as variable and fluctuating. In the face of such mounting stress, students naturally seek support from family and friends in order to alleviate psychological distress. A study by Chao found a significant two-way correlation between perceived stress and social support, as well as a significant three-way correlation between perceived stress, social support, and dysfunctional coping. The results indicated that high levels of dysfunctional coping deteriorated the association between stress and well-being at both high and low levels of social support, suggesting that dysfunctional coping can deteriorate the positive buffering action of social support on well-being. Students who reported social support were found more likely to engage in less healthy activities, including sedentary behavior, drug and alcohol use, and too much or too little sleep. Lack of social support in college students is also strongly related to life dissatisfaction and suicidal behavior.\n\nSocial support has a clearly demonstrated link to physical health outcomes in individuals, with numerous ties to physical health including mortality. People with low social support are at a much higher risk of death from a variety of diseases (e.g., cancer or cardiovascular disease). Numerous studies have shown that people with higher social support have an increased likelihood for survival.\n\nIndividuals with lower levels of social support have: more cardiovascular disease, more inflammation and less effective immune system functioning, more complications during pregnancy, and more functional disability and pain associated with rheumatoid arthritis, among many other findings. Conversely, higher rates of social support have been associated with numerous positive outcomes, including faster recovery from coronary artery surgery, less susceptibility to herpes attacks, a lowered likelihood to show age-related cognitive decline, and better diabetes control. People with higher social support are also less likely to develop colds and are able to recover faster if they are ill from a cold. There is sufficient evidence linking cardiovascular, neuroendocrine, and immune system function with higher levels of social support. Social support predicts less atherosclerosis and can slow the progression of an already diagnosed cardiovascular disease. There is also a clearly demonstrated link between social support and better immune function, especially in older adults. While links have been shown between neuroendocrine functionality and social support, further understanding is required before specific significant claims can be made. Social support is also hypothesized to be beneficial in the recovery from less severe cancers. Research focuses on breast cancers, but in more serious cancers factors such as severity and spread are difficult to operationalize in the context of impacts of social support. The field of physical health often struggles with the confoundment of variables by external factors that are difficult to control, such as the entangled impact of life events on social support and the buffering impact these events have. There are serious ethical concerns involved with controlling too many factors of social support in individuals, leading to an interesting crossroads in the research.\n\nSocial support is integrated into service delivery schemes and sometimes are a primary service provided by governmental contracted entities (e.g., companionship, peer services, family caregiving). Community services known by the nomenclature community support, and workers by a similar title, Direct Support Professional, have a base in social and community support \"ideology\". All supportive services from supported employment to supported housing, family support, supported education, and supportive living are based upon the relationship between \"informal and formal\" supports, and \"paid and unpaid caregiving\". Inclusion studies, based upon affiliation and friendship, or the conversely, have a similar theoretical basis as do \"person-centered support\" strategies.\n\nSocial support theories are often found in \"real life\" in cultural, music and arts communities, and as might be expected within religious communities. Social support is integral in theories of aging, and the \"social care systems\" have often been challenged (e.g., creativity throughout the lifespan, extra retirement hours). Ed Skarnulis' (state director) adage, \"Support, don't supplant the family\" applies to other forms of social support networks.\n\nAlthough there are many benefits to social support, it is not always beneficial. It has been proposed that in order for social support to be beneficial, the social support desired by the individual has to match the support given to him or her; this is known as the matching hypothesis. \nPsychological stress may increase if a different type of support is provided than what the recipient wishes to receive (e.g., informational is given when emotional support is sought). Additionally, elevated levels of perceived stress can impact the effect of social support on health-related outcomes.\n\nOther costs have been associated with social support. For example, received support has not been linked consistently to either physical or mental health; perhaps surprisingly, received support has sometimes been linked to worse mental health. Additionally, if social support is overly intrusive, it can increase stress. It is important when discussing social support to always consider the possibility that the social support system is actually an antagonistic influence on an individual.\n\nThere are two dominant hypotheses addressing the link between social support and health: the buffering hypothesis and the direct effects hypothesis. The main difference between these two hypotheses is that the direct effects hypothesis predicts that social support is beneficial all the time, while the buffering hypothesis predicts that social support is mostly beneficial during stressful times. Evidence has been found for both hypotheses.\n\nIn the buffering hypothesis, social support protects (or \"buffers\") people from the bad effects of stressful life events (e.g., death of a spouse, job loss). Evidence for stress buffering is found when the correlation between stressful events and poor health is weaker for people with high social support than for people with low social support. The weak correlation between stress and health for people with high social support is often interpreted to mean that social support has protected people from stress. Stress buffering is more likely to be observed for perceived support than for social integration or received support. The theoretical concept or construct of resiliency is associated with coping theories. \n\nIn the direct effects (also called main effects) hypothesis, people with high social support are in better health than people with low social support, regardless of stress. In addition to showing buffering effects, perceived support also shows consistent direct effects for mental health outcomes. Both perceived support and social integration show main effects for physical health outcomes. However, received (enacted) support rarely shows main effects.\n\nSeveral theories have been proposed to explain social support’s link to health. Stress and coping social support theory dominates social support research and is designed to explain the buffering hypothesis described above. According to this theory, social support protects people from the bad health effects of stressful events (i.e., stress buffering) by influencing how people think about and cope with the events. An example in 2018 are the effects of school shootings on the well being and future of children and children's health. According to stress and coping theory, events are stressful insofar as people have negative thoughts about the event (appraisal) and cope ineffectively. Coping consists of deliberate, conscious actions such as problem solving or relaxation. As applied to social support, stress and coping theory suggests that social support promotes adaptive appraisal and coping. Evidence for stress and coping social support theory is found in studies that observe stress buffering effects for perceived social support. One problem with this theory is that, as described previously, stress buffering is not seen for social integration, and that received support is typically not linked to better health outcomes.\n\nRelational regulation theory (RRT) is another theory, which is designed to explain main effects (the direct effects hypothesis) between perceived support and mental health. As mentioned previously, perceived support has been found to have both buffering and direct effects on mental health. RRT was proposed in order to explain perceived support’s main effects on mental health which cannot be explained by the stress and coping theory. RRT hypothesizes that the link between perceived support and mental health comes from people regulating their emotions through ordinary conversations and shared activities rather than through conversations on how to cope with stress. This regulation is relational in that the support providers, conversation topics and activities that help regulate emotion are primarily a matter of personal taste. This is supported by previous work showing that the largest part of perceived support is relational in nature.\n\nLife-span theory is another theory to explain the links of social support and health, which emphasizes the differences between perceived and received support. According to this theory, social support develops throughout the life span, but especially in childhood attachment with parents. Social support develops along with adaptive personality traits such as low hostility, low neuroticism, high optimism, as well as social and coping skills. Together, support and other aspects of personality (\"psychological theories\") influence health largely by promoting health practices (e.g., exercise and weight management) and by preventing health-related stressors (e.g., job loss, divorce). Evidence for life-span theory includes that a portion of perceived support is trait-like, and that perceived support is linked to adaptive personality characteristics and attachment experiences. Lifespan theories are popular from their origins in Schools of Human Ecology at the universities, aligned with family theories, and researched through federal centers over decades (e.g., University of Kansas, Beach Center for Families; Cornell University, School of Human Ecology). \n\nOf the Big Five Personality Traits, agreeableness is associated with people receiving the most social support and having the least-strained relationships at work and home. Receiving support from a supervisor in the workplace is associated with alleviating tensions both at work and at home, as are interdependency and idiocentrism of an employee.\n\nMany studies have tried to identify biopsychosocial pathways for the link between social support and health. Social support has been found to positively impact the immune, neuroendocrine, and cardiovascular systems. Although these systems are listed separately here, evidence has shown that these systems can interact and affect each other.\n\n\nThough many benefits have been found, not all research indicates positive effects of social support on these systems. For example, sometimes the presence of a support figure can lead to increased neuroendocrine and physiological activity.\n\nSocial support groups can be a source of informational support, by providing valuable educational information, and emotional support, including encouragement from people experiencing similar circumstances. Studies have generally found beneficial effects for social support group interventions for various conditions, including Internet support groups. These groups may be termed \"self help\" groups in nation-states, may be offered by non-profit organizations, and in 2018, may be paid for as part of governmental reimbursement schemes.\n\nThere are both costs and benefits to providing support to others. Providing long-term care or support for someone else is a chronic stressor that has been associated with anxiety, depression, alterations in the immune system, and increased mortality. Thus, family caregivers and \"university personnel\" alike have advocated for both respite or relief, and higher payments related to ongoing, long-term caregiving. However, providing support has also been associated with health benefits. In fact, providing instrumental support to friends, relatives, and neighbors, or emotional support to spouses has been linked to a significant decrease in the risk for mortality. Also, a recent neuroimaging study found that giving support to a significant other during a distressful experience increased activation in reward areas of the brain.\n\nIn 1959 Isabel Menzies Lyth identified that threat to a person’s identity in a group where they share similar characteristics develops a defence system inside the group which stems from emotions experienced by members of the group, which are difficult to articulate, cope with and finds solutions to. Together with an external pressure on efficiency a collusive and injunctive system develops that is resistant to change, supports their activities and prohibit others from performing their major tasks.\n\nGender differences have been found in social support research. Women provide more social support to others and are more engaged in their social networks. Evidence has also supported the notion that women may be better providers of social support. In addition to being more involved in the giving of support, women are also more likely to seek out social support to deal with stress, especially from their spouses. However, one study indicates that there are no differences in the extent to which men and women seek appraisal, informational, and instrumental types of support. Rather, the big difference lies in seeking emotional support. Additionally, social support may be more beneficial to women. Shelley Taylor and her colleagues have suggested that these gender differences in social support may stem from the biological difference between men and women in how they respond to stress (i.e., flight or fight versus tend and befriend). Married men are less likely to be depressed compared to non-married men after the presence of a particular stressor because men are able to delegate their emotional burdens to their partner, and women have been shown to be influenced and act more in reaction to social context compared to men. It has been found that men’s behaviors are overall more antisocial, with less regard to the impact their coping may have upon others, and women more prosocial-active with importance stressed on how their coping affects people around them. This may explain why women are more likely to experience negative psychological problems such as depression and anxiety based on how women receive and process stressors. In general, women are likely to find situations more stressful than males are. It is important to note that when the perceived stress level is the same, men and women have much fewer differences in how they seek and use social support.\n\nAlthough social support is thought to be a universal resource, cultural differences exist in social support. In many Asian cultures, the person is seen as more of a collective unit of society, whereas Western cultures are more individualistic and conceptualize social support as a transaction in which one person seeks help from another. In more interdependent Eastern cultures, people are less inclined to enlist the help of others. For example, European Americans have been found to call upon their social relationships for social support more often than Asian Americans or Asians during stressful occasions, and Asian Americans expect social support to be less helpful than European Americans. These differences in social support may be rooted in different cultural ideas about social groups. It is important to note that these differences are stronger in emotional support than instrumental support. Additionally, ethnic differences in social support from family and friends have been found.\n\nCultural differences in coping strategies other than social support also exist. One study shows that Koreans are more likely to report substance abuse than European Americans are. Further, European Americans are more likely to exercise in order to cope than Koreans. Some cultural explanations are that Asians are less likely to seek it from fear of disrupting the harmony of their relationships and that they are more inclined to settle their problems independently and avoid criticism. However, these differences are not found among Asian Americans relative to their Europeans American counterparts.\n\n"}
{"id": "1874234", "url": "https://en.wikipedia.org/wiki?curid=1874234", "title": "Species dysphoria", "text": "Species dysphoria\n\nSpecies dysphoria is the experience of dysphoria associated with the feeling that one's body is of the wrong species. Outside of psychological literature, the term is common within the otherkin and therian communities to describe their experiences.\n\nEarls and Lalumière (2009) describe it as \"the sense of being in the wrong [species'] body...a desire to be an animal\". A term that has also been used is \"transspecies\", described by Phaedra and Isaac Bonewits as \"people who believe themselves to be part animal, or animal souls that have been incarnated in human bodies\".\n\nSpecies dysphoria may include:\n\n\nMany find comfort in a form of transition, whether physical or social. There is little research in surgeries towards the end of looking like another species, amongst what does exist, a 2008 paper by Samuel Poore in \"The Journal of Hand Surgery - American Volume\" details how a wing similar to that of a flightless bird could be constructed from a human arm.\n\nIn a critical discussion of the work of Gerbasi \"et al.\" (2008), Fiona Probyn-Rapsey (2011) proposes that if \"Species Identity Disorder\" were to be treated, it may follow paths towards encouraging desistance, mirroring aims for desistance in the treatment of gender dysphoria in children; perhaps \"redirecting a child’s attention away from cross-dressing as an animal\" or \"limiting the influence of humanimal creatures like stuffed toys\". She proposes alternatively that treatments \"might involve counseling to learn to tolerate “atypical” humanimal development for those bothered by furries [with Species Identity Disorder]\".\n\nThough not all people with species dysphoria have gender dysphoria, and vice versa, overlap exists. Some people experience both gender dysphoria and species dysphoria and consider them to be related in that they believe them to be similar dysphoric experiences.\n\n\"Species dysphoria\" is informally used mainly in psychological literature to compare the experiences of some individuals to those in the transgender community.\n\nIn a 2008 study by Gerbasi \"et al.\", amongst other things pursuing the potential of a condition termed \"Species Identity Disorder\", 46% of people surveyed who identified as being in the furry fandom, answered \"yes\" to the question \"Do you consider yourself to be less than 100% human?\" and 41% answered \"yes\" to the question \"If you could become 0% human, would you?\"\n\nQuestions that Gerbasi states as being deliberately designed to draw parallels with gender dysphoria, specifying \"a persistent feeling of discomfort\" about the human body and the feeling that the person was the \"non-human species trapped in a human body\", were answered \"yes\" by 24% and 29% of respondents, respectively.\n\nThough not all people with species dysphoria are zoophilic, it has been proposed that there is comorbidity in some zoophiles by Beetz (2004), citing Miletski (2002) as evidence of this.\n\nMiletski (2002), in a study contained in their book, involving 67 male and nine female zoophiles, found that 16 (24%) of the men reported that it was ‘‘completely or mostly true’’ that they began having sex with animals because they ‘‘identiﬁed with the animal of [their] gender’’, 0 women reported this, with 1 woman reporting it as \"sometimes true\" against 14 of the men reporting as \"sometimes true\".\n\nA 2003 study by Williams and Weinberg surveyed 114 self identified zoophilic men and wrote that some admitted to \"believing they had animal characteristics or that they felt like they were an animal.\"\n\nIn 2007, Los Angeles artist Micha Cárdenas created \"Becoming Dragon\", a \"mixed-reality performance\" in which a virtual reality experience was created to allow a person to completely experience life through the eyes of a dragon avatar in the virtual world, Second Life. After the performance, Cárdenas reported that \"some of these people call themselves Otherkin, and feel deeply, truly, painfully that they were born as the wrong species, that they are foxes, dragons and horses. I would refer to them as transspecies.\"\n\nJ M Barrie's \"Peter Pan\" has been described as experiencing species dysphoria by Garber (1997).\n\nJean Dutourd's short novel \"Une Tête de Chien\" features a spaniel-headed human protagonist described by Giffney and Hird as suffering from species dysphoria.\n\n"}
{"id": "1177467", "url": "https://en.wikipedia.org/wiki?curid=1177467", "title": "Telehealth", "text": "Telehealth\n\nTelehealth involves the distribution of health-related services and information via electronic information and telecommunication technologies. It allows long distance patient/clinician contact and care, advice, reminders, education, intervention, monitoring and remote admissions. As well as provider distance-learning; meetings, supervision, and presentations between practitioners; online information and health data management and healthcare system integration. Telehealth could include two clinicians discussing a case over video conference; a robotic surgery occurring through remote access; physical therapy done via digital monitoring instruments, live feed and application combinations; tests being forwarded between facilities for interpretation by a higher specialist; home monitoring through continuous sending of patient health data; client to practitioner online conference; or even videophone interpretation during a consult.\n\nAs the population grows and ages, and medical advances are made which prolong life, demands increase on the healthcare system. Healthcare providers are also being asked to do more, with no increase in funding, or are encouraged to move to new models of funding and care such as patient-centered or outcomes based, rather than fee-for-service. Some specific health professions already have a shortage (i.e. Speech-language pathologists). When rural settings, lack of transport, lack of mobility (i.e. In the elderly or disabled), decreased funding or lack of staffing restrict access to care, telehealth can bridge the gap.\n\nTelehealth is sometimes discussed interchangeably with telemedicine. The Health Resources and Services Administration (HRSA) distinguishes telehealth from telemedicine in its scope. According to HRSA, telemedicine only describes remote clinical services; such as diagnosis and monitoring, while telehealth includes preventative, promotive and curative care delivery. This includes the above-mentioned non-clinical applications like administration and provider education which make telehealth the preferred modern terminology.\n\nThe development and history of telehealth or telemedicine (terms used interchangeably in literature) is deeply rooted in the history and development in not only technology but also society itself. Humans have long sought to relay important messages through torches, optical telegraphy, electroscopes, and wireless transmission. In the 21st century, with the advent of the internet, portable devices and other such digital devices are taking a transformative role in healthcare and its delivery.\n\nAlthough, traditional medicine relies on in-person care, the need and want for remote care has existed from the Roman and pre-Hippocratic periods in antiquity. The elderly and infirm who could not visit temples for medical care sent representatives to convey information on symptoms and bring home a diagnosis as well as treatment. In Africa, villagers would use smoke signals to warn neighbouring villages of disease outbreak. The beginnings of telehealth have existed through primitive forms of communication and technology.\n\nAs technology developed and wired communication became increasingly commonplace, the ideas surrounding telehealth began emerging. The earliest telehealth encounter can be traced to Alexander Graham Bell in 1876, when he used his early telephone as a means of getting help from his assistant Mr. Watson after he spilt acid on his trousers. Another instance of early telehealth, specifically telemedicine was reported in \"The Lancet\" in 1879. An anonymous writer described a case where a doctor successfully diagnosed a child over the telephone in the middle of the night. This Lancet issue, also further discussed the potential of Remote Patient Care in order to avoid unnecessary house visits, which were part of routine health care during the 1800s. Other instances of telehealth during this period came from the American Civil War, during which telegraphs were used to deliver mortality lists and medical care to soldiers.\n\nFrom the late 1800s to the early 1900s the early foundations of wireless communication were laid down. Radios provided an easier and near instantaneous form of communication. The use of radio to deliver healthcare became accepted for remote areas. The Royal Flying Doctor Service of Australia is an example of the early adoption of radios in telehealth.\n\nIt was during the mid-1900s well into the 1980s that a lot of the momentum and foundations of telehealth were founded. When the American National Aeronautics and Space Administration (NASA), began plans to send astronauts into space, the need for Telemedicine became all too clear. In order to monitor their astronauts in space, telemedicine capabilities were built into the spacecraft as well as the first spacesuits. Additionally, during this period, telehealth and Telemedicine were promoted in different countries especially the United States. Different projects were funded across North America and Canada in order to realise the exciting potential of this new innovation.\n\nIn 1964, the Nebraska Psychiatric Institute began using television links to form two-way communication with the Norfolk State Hospital which was 112 miles away for the education and consultation purposes between clinicians in the two locations. The Logan International Airport in Boston established in-house medical stations in 1967. These stations were linked to Massachusetts General Hospital. Clinicians at the hospital would provide consultation services to patients who were at the airport. Consultations were achieved through microwave audio as well as video links.\n\nIn 1972, there was a key emphasis on telemedicine so much so that the Department of Health, Education and Welfare in the United States approved funding for seven telemedicine projects across different states. This funding was renewed and two further projects were funded the following year.\n\nAlthough the excitement of telehealth and telemedicine remained, enthusiasm waned in the 1980s. Telehealth projects underway before and during the 1980s would take off but fail to proliferate mainstream healthcare. This put a halt on various projects and reduced opportunities for funding. As a result, this period of telehealth history is called the \"maturation\" stage and made way for sustainable growth.\n\nSustained growth happened most notably in North America. Although State funding was beginning to run low, different hospitals in various states began to launch their own telehealth initiatives. Additionally, NASA started experimenting with their ATS-3 satellite. Eventually, NASA started their SateLife/HealthNet programme which tried to increase the health services connectivity in developing countries.\n\nThe combination of sustained growth, the advent of the internet and the increasing adoption of ICT in traditional methods of care spurred the revival or \"renaissance\" of telehealth into the early 2000s and onwards.\n\nThe early 2000s were characterised by accelerated development in both science and technology. The early adoption of technology in society made way for widespread adoption in society. The diffusion of portable devices like laptops and mobile devices in everyday life made ideas surrounding telehealth more plausible. This continuing trend of better and innovative technology in homes, schools and organisations is contributing to the growing research in telehealth. Telehealth is no longer bound within the realms of telemedicine but has expanded itself to promotion, prevention and education.\n\nTelehealth requires a strong, reliable broadband connection. The broadband signal transmission infrastructure includes wires, cables, microwaves and optic fibre, which must be maintained for the provision of telehealth services. The better the connection (bandwidth quality), the more data can be sent and received. Historically this has priced providers or patients out of the service, but as the infrastructure improves and becomes more accessible, telehealth usage can grow.\n\nWhen a healthcare service decides to provide telehealth to its patients, there are steps to consider, besides just whether the above resources are available. A needs assessment is the best way to start, which includes assessing the access the community currently has to the proposed specialists and care, whether the organisation currently has underutilized equipment which will make them useful to the area they are trying to service, and the hardships they are trying to improve by providing the access to their intended community (i.e. Travel time, costs, time off work). A service then needs to consider potential collaborators. Other services may exist in the area with similar goals who could be joined to provide a more holistic service, and/or they may already have telehealth resources available. The more services involved, the easier to spread the cost of IT, training, workflow changes and improve buy-in from clients. Services need to have the patience to wait for the accrued benefits of providing their telehealth service and cannot necessarily expect community-wide changes reflected straight away.\n\nOnce the need for a Telehealth service is established, delivery can come within four distinct domains. They are live video (synchronous), store-and-forward (asynchronous), remote patient monitoring, and mobile health. Live video involves a real-time two-way interaction, such as patient/caregiver-provider or provider-provider, over a digital (i.e. broadband) connection. This often is used to substitute a face to face meeting such as consults, and saves time and cost in travel. Store-and-forward is when data is collected, recorded, and then sent on to a provider. For example, a patient's' digital health history file including x-rays and notes, being securely transmitted electronically to evaluate the current case. Remote patient monitoring includes patients' medical and health data being collected and transferred to a provider elsewhere who can continue to monitor the data and any changes that may occur. This may best suit cases that require ongoing care such as rehabilitation, chronic care, or elderly clients trying to stay in the community in their own homes as opposed to a care facility. Mobile health includes any health information, such as education, monitoring and care, that is present on and supported by mobile communication devices such as cell phones or tablet computers. This might include an application, or text messaging services like appointment reminders or public health warning systems.\n\nTelehealth is a modern form of health care delivery. Telehealth breaks away from traditional health care delivery by using modern telecommunication systems including wireless communication methods. Traditional health is legislated through policy to ensure the safety of medical practitioners and patients. Consequently, since telehealth is a new form of health care delivery that is now gathering momentum in the health sector, many organizations have started to legislate the use of telehealth into policy. In New Zealand, the Medical Council has a statement about telehealth on their website. This illustrates that the medical council has foreseen the importance that telehealth will have on the health system and have started to introduce telehealth legislation to practitioners along with government.\n\nTraditional use of telehealth services has been for specialist treatment. However, there has been a paradigm shift and telehealth is no longer considered a specialist service. This development has ensured that many access barriers are eliminated, as medical professionals are able to use wireless communication technologies to deliver health care. This is evident in rural communities. For individuals living in rural communities, specialist care can be some distance away, particularly in the next major city. Telehealth eliminates this barrier, as health professionals are able to conduct a medical consultation through the use of wireless communication technologies. However, this process is dependent on both parties having Internet access.\n\nTelehealth allows the patient to be monitored between physician office visits which can improve patient health. Telehealth also allows patients to access expertise which is not available in their local area. This remote patient monitoring ability enables patients to stay at home longer and helps avoid unnecessary hospital time. In the long-term, this could potentially result in less burdening of the healthcare system and consumption of resources.\n\nThe technological advancement of wireless communication devices is a major development in telehealth. This allows patients to self-monitor their health conditions and to not rely as much on health care professionals. Furthermore, patients are more willing to stay on their treatment plans as they are more invested and included in the process, decision-making is shared. Technological advancement also means that health care professionals are able to use better technologies to treat patients for example in surgery. Technological developments in telehealth are essential to improve health care, especially the delivery of healthcare services, as resources are finite along with an ageing population that is living longer.\n\nTelehealth allows multiple, different disciplines to merge and deliver a much more uniform level of care using the efficiency and accessibility of everyday technology. As telehealth proliferates mainstream healthcare and challenges notions of traditional healthcare delivery, different populations are starting to experience better quality, access and personalised care in their lives.\n\n\"See Also: Health Promotion\" Telehealth can also increase health promotion efforts. These efforts can now be more personalised to the target population and professionals can extend their help into homes or private and safe environments in which patients of individuals can practice, ask and gain health information. Health promotion using telehealth has become increasingly popular in underdeveloped countries where there are very poor physical resources available. There has been a particular push toward mHealth applications as many areas, even underdeveloped ones have mobile phone coverage.\n\nIn developed countries, health promotion efforts using telehealth have been met with some success. The Australian hands-free breastfeeding Google Glass application reported promising results in 2014. This application made in collaboration with the Australian Breastfeeding Association and a tech startup called Small World Social, helped new mothers learn how to breastfeed. Breastfeeding is beneficial to infant health and maternal health and is recommended by the World Health Organisation and health organisations all over the world. Widespread breastfeeding can prevent 820,000 infant deaths globally but the practice is often stopped prematurely or intents to do are disrupted due to lack of social support, know-how or other factors. This application gave mother's hands-free information on breastfeeding, instructions on how to breastfeed and also had an option to call a lactation consultant over Google Hangout. When the trial ended, all participants were reported to be confident in breastfeeding.\n\nTheoretically, the whole health system stands to benefit from telehealth. In a UK telehealth trial done in 2011, it was reported that the cost of health could be dramatically reduced with the use of telehealth monitoring. The usual cost of in vitro fertilisation (IVF) per cycle would be around $15,000, with telehealth it was reduced to $800 per patient. In Alaska the Federal Health Care Access Network which connects 3,000 healthcare providers to communities, engaged in 160,000 telehealth consultations from 2001 and saved the state $8.5 million in travel costs for just Medicaid patients. There are indications telehealth consumes fewer resources and requires fewer people to operate it with shorter training periods to implement initiatives.\n\nHowever, whether or not the standard of health care quality is increasing is quite debatable, with literature refuting such claims. Research is increasingly reporting that clinicians find the process difficult and complex to deal with. Furthermore, there are concerns around informed consent, legality issues as well as legislative issues. Although health care may become affordable with the help of technology, whether or not this care will be \"good\" is the issue.\n\nDue to its digital nature it is often assumed that telehealth saves the health system money. However, the evidence to support this is varied. When conducting economic evaluations of telehealth services, the individuals evaulating them need to be aware of potential outcomes and extraclinical benefits of the telehealth service. \n\n\nWhile many branches of medicine have wanted to fully embrace telehealth for a long time, there are certain risks and barriers which bar the full amalgamation of telehealth into best practice. For a start, it is dubious as to whether a practitioner can fully leave the \"hands-on\" experience behind. Although it is predicted that telehealth will replace many consultations and other health interactions, it cannot yet fully replace a physical examination, this is particularly so in diagnostics, rehabilitation or mental health.\n\nThe benefits posed by telehealth challenge the normative means of healthcare delivery set in both legislation and practice. Therefore, the growing prominence of telehealth is starting to underscore the need for updated regulations, guidelines and legislation which reflect the current and future trends of healthcare practices. Telehealth enables timely and flexible care to patients wherever they may be; although this is a benefit, it also poses threats to privacy, safety, medical licensing and reimbursement. When a clinician and patient are in different locations, it is difficult to determine which laws apply to the context. Once healthcare crosses borders different state bodies are involved in order to regulate and maintain the level of care that is warranted to the patient or telehealth consumer. As it stands, telehealth is complex with many grey areas when put into practice especially as it crosses borders. This effectively limits the potential benefits of telehealth.\n\nAn example of these limitations include the current American reimbursement infrastructure, where Medicare will reimburse for telehealth services only when a patient is living in an area where specialists are in shortage, or in particular rural counties. The area is defined by whether it is a medical facility as opposed to a patient's' home. The site that the practitioner is in, however, is unrestricted. Medicare will only reimburse live video (synchronous) type services, not store-and-forward, mhealth or remote patient monitoring (if it does not involve live-video). Some insurers currently will reimburse telehealth, but not all yet. So providers and patients must go to the extra effort of finding the correct insurers before continuing. Again in America, states generally tend to require that clinicians are licensed to practice in the surgery' state, therefore they can only provide their service if licensed in an area that they do not live in themselves.\n\nMore specific and widely reaching laws, legislations and regulations will have to evolve with the technology. They will have to be fully agreed upon, for example, will all clinicians need full licensing in every community they provide telehealth services too, or could there be a limited use telehealth licence? Would the limited use licence cover all potential telehealth interventions, or only some? Who would be responsible if an emergency was occurring and the practitioner could not provide immediate help – would someone else have to be in the room with the patient at all consult times? Which state, city or country would the law apply in when a breach or malpractice occurred? \n\nA major legal action prompt in telehealth thus far has been issues surrounding online prescribing and whether an appropriate clinician-patient relationship can be established online to make prescribing safe, making this an area that requires particular scrutiny. It may be required that the practitioner and patient involved must meet in person at least once before online prescribing can occur, or that at least a live-video conference must occur, not just impersonal questionnaires or surveys to determine need.\n\n\"Informed consent\" is another issue – should the patient give informed consent to receive online care before it starts? Or will it be implied if it is care that can only practically be given over distance? When telehealth includes the possibility for technical problems such as transmission errors or security breaches or storage which impact on ability to communicate, it may be wise to obtain informed consent in person first, as well as having backup options for when technical issues occur. In person, a patient can see who is involved in their care (namely themselves and their clinician in a consult), but online there will be other involved such as the technology providers, therefore consent may need to involve disclosure of anyone involved in the transmission of the information and the security that will keep their information private, and any legal malpractice cases may need to involve all of those involved as opposed to what would usually just be the practitioner.\n\nThe rate of adoption of telehealth services in any jurisdiction is frequently influenced by factors such as the adequacy and cost of existing conventional health services in meeting patient needs; the policies of governments and/or insurers with respect to coverage and payment for telehealth services; and medical licensing requirements that may inhibit or deter the provision of telehealth second opinions or primary consultations by physicians.\n\nProjections for the growth of the telehealth market are optimistic, and much of this optimism is predicated upon the increasing demand for remote medical care. According to a recent survey, nearly three-quarters of U.S. consumers say they would use telehealth. At present, several major companies along with a bevvy of startups are working to develop a leading presence in the field.\n\nIn the UK, the Government's Care Services minister, Paul Burstow, has stated that telehealth and telecare would be extended over the next five years (2012–2017) to reach three million people.\n\n\n\n"}
{"id": "716048", "url": "https://en.wikipedia.org/wiki?curid=716048", "title": "Thorotrast", "text": "Thorotrast\n\nThorotrast is a suspension containing particles of the radioactive compound thorium dioxide, ThO, that was used as a radiocontrast agent in medical radiography in the 1930s and 1940s. Use in some countries, such as the U.S., continued into the 1950s.\n\nThorium compounds produce excellent images because of thorium's high opacity to X-rays (it has a high cross section for absorption). However, thorium is retained in the body, and it is radioactive, emitting harmful alpha radiation as it decays. Because the suspension offered high image quality and had virtually no immediate side-effects compared to the alternatives available at the time, Thorotrast became widely used after its introduction in 1931. António Egas Moniz contributed to its development. About 2 to 10 million patients worldwide have been treated with Thorotrast. However, today it has shown an increase risk in certain cancers such as cholangiocarcinomas and angiosarcomas of the liver.\n\nEven at the time of introduction, there was concern about the safety of Thorotrast. Following injection, the drug is distributed to the liver, spleen, lymph nodes, and bone, where it is absorbed. After this initial absorption, redistribution takes place at a very slow pace. Specifically, the biological half-life is estimated to be 22 years. This means that the organs of patients who have been given Thorotrast will be exposed to internal alpha radiation for the rest of their lives. The significance of this long-term exposure was not fully understood at the time of Thorotrast's introduction in 1931.\n\nDue to the release of alpha particles, Thorotrast was found to be extremely carcinogenic. There is a high over-incidence of various cancers in patients who have been treated with Thorotrast. The cancers occur some years (usually 20-30) after injection of Thorotrast. The risk of developing liver cancer (or bile duct cancer) in former Thorotrast patients has been measured to be well above 100 times the risk of the rest of the population. The risk of leukemia appears to be 20 times higher in Thorotrast patients. Thorotrast exposure has also been associated with the development of angiosarcoma. German patients exposed to Thorotrast had their median life-expectancy shortened by 14 years in comparison to a similar non-exposed control group.\n\nThorium is no longer used in X-ray contrast agents. Today, iodinated hydrophilic (water-soluble) molecules are universally used as injected contrast agents in X-ray procedures.\n\nThe Danish director Nils Malmros's movie, \"Facing the Truth\" (original Danish title \"At Kende Sandheden\") from 2002, portrays the dilemma that faced Malmros's father, Richard Malmros, when treating his patients in the 1940s. Richard Malmros was deeply concerned about the persistence of Thorotrast in the body but was forced to use Thorotrast, because the only available alternative (per-abrodil) had serious immediate side-effects, suffered from image quality problems and was difficult to obtain during the Second World War. The use of Thorotrast in Denmark ended in 1947 when safer alternatives became available.\n\nThorotrast has also been used in research to stain neural tissue samples for examination by historadiography.\n"}
