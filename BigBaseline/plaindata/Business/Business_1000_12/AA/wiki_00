{"id": "13663006", "url": "https://en.wikipedia.org/wiki?curid=13663006", "title": "4–4–5 calendar", "text": "4–4–5 calendar\n\nThe 4–4–5 calendar is a method of managing accounting periods, and is a common calendar structure for some industries such as retail and manufacturing.\n\nThe 4–4–5 calendar divides a year into four quarters of 13 weeks grouped into two 4-week \"months\" and one 5-week \"month\". The grouping of 13 weeks may also be set up as 5–4–4 weeks or 4–5–4.\n\nWhen this type of calendar is in use, reports with month-by-month comparisons or trends are flawed as one month is 25% longer than the other two. It could still compare a period to the same period in the prior year, or use week by week data comparisons.\n\nIts major advantage over a regular calendar is that the end date of the period is always the same day of the week, which is useful for shift or manufacturing planning as every period is the same length.\n\nA disadvantage of the 4–4–5 calendar is that it has only 364 days (7 days x 52 weeks), meaning a 53rd week will need to be added every five or six years: this can make year-on-year comparison difficult.\n\nThe 52–53-week fiscal year is a variation on the 4–4–5 calendar. It is used by companies that desire that their fiscal year always end on the same day of the week. Any day of the week may be used, and Saturday and Sunday are common because the business may more easily be closed for counting inventory and other end-of-year accounting activities. There are two methods in use:\n\nUnder this method the company's fiscal year is defined as the final Saturday (or other day selected) in the fiscal year end month. For example, if the fiscal year end month is August, the company's year end could fall on any date from August 25 to August 31. In particular, the last fiscal week is the one that includes August 25 and the first fiscal week of the following year is the one that includes September 1. In this scenario, fiscal years would end on the following days:\n\nThe end of the fiscal year moves one day earlier on the calendar each year (or two days when there is an intervening leap day) until it would otherwise reach the date seven days before the end of the month (August 24 in this case) or earlier. At that point it resets to the end of the month (August 31) or earlier and the fiscal year has 53 weeks instead of 52. In this example the fiscal years ending in 2013, 2019, and 2024 have 53 weeks.\n\nUnder this method the company's fiscal year is defined as the Saturday (or other day selected) that falls closest to the last day of the fiscal year end month. For example, if the fiscal year end month is August, the company's year end could fall on any date from August 28 to September 3. In particular, the last fiscal week is the one that includes August 28 and the first fiscal week of the following year is the one that includes September 4. In this scenario, fiscal years would end on the following days:\n\nThe end of the fiscal year moves one day earlier on the calendar each year (or two days when there is an intervening leap day) until it would otherwise reach the date four days before the end of the month (August 27 in this case) or earlier. At that point the first Saturday in the following month (September 3 or earlier in this case) becomes the date closest to the end of August and it resets to that date and the fiscal year has 53 weeks instead of 52. In this example the fiscal years ending in 2016, 2022, and 2028 have 53 weeks.\n\nThe 52–53 week method is permitted by generally accepted accounting principles in the United States, by US Internal Revenue Code Regulation 1.441-2 (IRS Publication 538)., as well as the International Financial Reporting Standards.\n\n"}
{"id": "14417025", "url": "https://en.wikipedia.org/wiki?curid=14417025", "title": "Accounting ethics", "text": "Accounting ethics\n\nAccounting ethics is primarily a field of applied ethics and is part of business ethics and human ethics, the study of moral values and judgments as they apply to accountancy. It is an example of professional ethics. Accounting introduced by Luca Pacioli, and later expanded by government groups, professional organizations, and independent companies. Ethics are taught in accounting courses at higher education institutions as well as by companies training accountants and auditors.\n\nDue to the diverse range of accounting services and recent corporate collapses, attention has been drawn to ethical standards accepted within the accounting profession. These collapses have resulted in a widespread disregard for the reputation of the accounting profession. To combat the criticism and prevent fraudulent accounting, various accounting organizations and governments have developed regulations and remedies for improved ethics among the accounting profession.\n\nThe nature of the work carried out by accountants and auditors requires a high level of ethics. Shareholders, potential shareholders, and other users of the financial statements rely heavily on the yearly financial statements of a company as they can use this information to make an informed decision about investment. They rely on the opinion of the accountants who prepared the statements, as well as the auditors that verified it, to present a true and fair view of the company. Knowledge of ethics can help accountants and auditors to overcome ethical dilemmas, allowing for the right choice that, although it may not benefit the company, will benefit the public who relies on the accountant/auditor's reporting.\n\nMost countries have differing focuses on enforcing accounting laws. In Germany, accounting legislation is governed by \"tax law\"; in Sweden, by \"accounting law\"; and in the United Kingdom, by the \"company law\". In addition, countries have their own organizations which regulate accounting. For example, Sweden has the Bokföringsnämden (BFN - Accounting Standards Board), Spain the Instituto de Comtabilidad y Auditoria de Cuentas (ICAC), and the United States the Financial Accounting Standards Board (FASB).\n\nLuca Pacioli, the \"Father of Accounting\", wrote on accounting ethics in his first book , published in 1494. Ethical standards have since then been developed through government groups, professional organizations, and independent companies. These various groups have led accountants to follow several codes of ethics to perform their duties in a professional work environment. Accountants must follow the code of ethics set out by the professional body of which they are a member. United States accounting societies such as the Association of Government Accountants, Institute of Internal Auditors, and the National Association of Accountants all have codes of ethics, and many accountants are members of one or more of these societies.\n\nIn 1887, the American Association of Public Accountants (AAPA) was created; it was the first step in developing professionalism in the United States accounting industry. By 1905, the AAPA's first ethical codes were formulated to educate its members. During its twentieth anniversary meeting in October 1907, ethics was a major topic of the conference among its members. As a result of discussions, a list of professional ethics was incorporated into the organization's bylaws. However, because membership to the organization was voluntary, the association could not require individuals to conform to the suggested behaviors. Other accounting organizations, such as the Illinois Institute of Accountants, also pursued discussion on the importance of ethics for the field. The AAPA was renamed several times throughout its history, before becoming the American Institute of Certified Public Accountants (AICPA) as it's named today. The AICPA developed five divisions of ethical principles that its members should follow: \"independence, integrity, and objectivity\"; \"competence and technical standards\"; \"responsibilities to clients\"; \"responsibilities to colleagues\"; as well as \"other responsibilities and practices\". Each of these divisions provided guidelines on how a Certified Public Accountant (CPA) should act as a professional. Failure to comply with the guidelines could have caused an accountant to be barred from practicing. When developing the ethical principles, the AICPA also considered how the profession would be viewed by those outside of the accounting industry.\n\nCourses on this subject have grown significantly in the last couple of decades. Teaching accountants about ethics can involve role playing, lectures, case studies, guest lectures, as well as other mediums. Recent studies indicate that nearly all accounting textbooks touch on ethics in some way. In 1993, the first United States center that focused on the study of ethics in the accounting profession opened at Binghamton University. Starting in 1999, several U.S. states began requiring ethics classes prior to taking the CPA exam.\n\nIn 1988, Stephen E. Loeb proposed that accounting ethics education should include seven goals (adapted from a list by Daniel Callahan). To implement these goals, he pointed out that accounting ethics could be taught throughout accounting curriculum or in an individual class tailored to the subject. Requiring it be taught throughout the curriculum would necessitate all accounting teachers to have knowledge on the subject (which may require training). A single course has issues as to where to include the course in a student's education (for example, before preliminary accounting classes or near the end of a student's degree requirements), whether there is enough material to cover in a semester class, and whether most universities have room in a four-year curriculum for a single class on the subject.\n\nThere has been debate on whether ethics should be taught in a university setting. Supporters point out that ethics are important to the profession, and should be taught to accountants entering the field. In addition, the education would help to reinforce students' ethical values and inspire them to prevent others from making unethical decisions. Critics argue that an individual is ethical or not, and that teaching an ethics course would serve no purpose. Despite opposition, instruction on accounting ethics by universities and conferences, has been encouraged by professional organizations and accounting firms. The Accounting Education Change Commission (AECC) has called for students to \"know and understand the ethics of the profession and be able to make value-based judgments.\"\n\nPhillip G. Cottel argued that in order to uphold strong ethics, an accountant \"must have a strong sense of values, the ability to reflect on a situation to determine the ethical implications, and a commitment to the well-being of others.\" Iris Stuart recommends an ethics model consisting of four steps: the accountant must recognize that an ethical dilemma is occurring; identify the parties that would be interested in the outcome of the dilemma; determine alternatives and evaluate its effect on each alternative on the interested parties; and then select the best alternative.\n\nAccounting ethics has been deemed difficult to control as accountants and auditors must consider the interest of the public (which relies on the information gathered in audits) while ensuring that they remained employed by the company they are auditing. They must consider how to best apply accounting standards even when faced with issues that could cause a company to face a significant loss or even be discontinued. Due to several accounting scandals within the profession, critics of accountants have stated that when asked by a client \"what does two plus two equal?\" the accountant would be likely to respond \"what would you like it to be?\". This thought process along with other criticisms of the profession's issues with conflict of interest, have led to various increased standards of professionalism while stressing ethics in the work environment.\n\nThe role of accountants is critical to society. Accountants serve as financial reporters and\nintermediaries in the capital markets and owe their primary obligation to the public interest. The\ninformation they provide is crucial in aiding managers, investors and others in making critical economic\ndecisions. Accordingly, ethical improprieties by accountants can be detrimental to society, resulting in\ndistrust by the public and disruption of efficient capital market operations.\nFrom the 1980s to the present there have been multiple accounting scandals that were widely reported on by the media and resulted in fraud charges, bankruptcy protection requests, and the closure of companies and accounting firms. The scandals were the result of creative accounting, misleading financial analysis, as well as bribery. Various companies had issues with fraudulent accounting practices, including Nugan Hand Bank, Phar-Mor, WorldCom, and AIG. One of the most widely reported violation of accounting ethics involved Enron, a multinational company, that for several years had not shown a true or fair view of their financial statements. Their auditor Arthur Andersen, an accounting firm considered one of the \"Big Five\", signed off on the validity of the accounts despite the inaccuracies in the financial statements. When the unethical activities were reported, not only did Enron dissolve but Arthur Andersen also went out of business. Enron's shareholders lost $25 billion as a result of the company's bankruptcy. Although only a fraction of Arthur Anderson's employees were involved with the scandal, the closure of the firm resulted in the loss of 85,000 jobs.\n\nFraudulent accounting can arise from a variety of issues. These problems usually come to light eventually and could ruin not only the company but also the auditors for not discovering or revealing the misstatements. Several studies have proposed that a firm's corporate culture as well as the values it stresses may negatively alter an accountant's behavior. This environment could contribute to the degradation of ethical values that were learned from universities.\n\nUntil 1977, ethics rules prevented accounting and auditing firms from advertising to clients. When the rules were lifted, spending by the largest CPA firms on advertisements rose from US$4 million in the 1980s to more than $100 million in the 2000s. Critics claimed that, by allowing the firms to advertise, the business side overstepped the professional side of the profession, which led to a conflict of interest. This focus allowed for occurrences of fraud, and caused the firms, according to Apj , \"... to offer services that made them more consultants and business advisers than auditors.\" As accounting firms became less interested in the lower-paying audits due to more focus on higher earning services such as consulting, problems arose. This disregard for the lack of time spent on audits resulted in a lack of attention to catching creative and fraudulent accounting.\n\nA 2007 article in \"Managerial Auditing Journal\" determined the top nine factors that contributed to ethical failures for accountants based on a survey of 66 members of the International Federation of Accountants. The factors include (in order of most significant): \"self-interest, failure to maintain objectivity and independence, inappropriate professional judgment, lack of ethical sensitivity, improper leadership and ill-culture, failure to withstand advocacy threats, lack of competence, lack of organizational and peer support, and lack of professional body support.\" The main factor, self-interest, is the motivation by an accountant to act in his/her best interest or when facing a conflict of interest. For example, if an auditor has an issue with an account he/she is auditing, but is receiving financial incentives to ignore these issues, the auditor may act unethically.\n\nThe International Financial Reporting Standards (IFRS) are standards and interpretations developed by the International Accounting Standards Board, which are principle-based. IFRS are used by over 115 countries or areas including the European Union, Australia, and Hong Kong. The United States Generally Accepted Accounting Principles (GAAP), the standard framework of guidelines for financial accounting, is largely rule-based. Critics have stated that the rules-based GAAP is partly responsible for the number of scandals that the United States has suffered. The principles-based approach to monitoring requires more professional judgment than the rules-based approach.\n\nThere are many stakeholders in many countries such as The United States who report several concerns in the usage of rules-based accounting. According to recent studies, many believe that the principles-based approach in financial reporting would not only improve but would also support an auditor upon dealing with client’s pressure. As a result, financial reports could be viewed with fairness and transparency. When the U.S. switched to International accounting standards, they are composed that this would bring change. However, as a new chairperson of the SEC takes over the system, the transition brings a stronger review about the pros and cons of rules- based accounting. While the move towards international standards progresses, there are small amount of research that examines the effect of principle- based standards in an auditor’s decision- making process. According to 114 auditing experts, most are willing to allow clients to manage their net income based on rules- based standards. These results offers insight to the SEC, IASB and FASB in weighing the arguments in the debate of principles- vs. rules based- accounting.\n\nIFRS is based on \"understandability, relevance, materiality, reliability, and comparability\". Since IFRS has not been adopted by all countries, these practices do not make the international standards viable in the world domain. In particular, the United States has not yet conformed and still uses GAAP which makes comparing principles and rules difficult. In August 2008, the Securities and Exchange Commission (SEC) proposed that the United States switch from GAAP to IFRS, starting in 2014.\n\nSince the major accounting scandals, new reforms, regulations, and calls for increased higher education have been introduced to combat the dangers of unethical behavior. By educating accountants on ethics before entering the workforce, such as through higher education or initial training at companies, it is believed it will help to improve the credibility of the accounting profession. Companies and accounting organizations have expanded their assistance with educators by providing education materials to assist professors in educating students.\n\nNew regulations in response to the scandals include the Corporate Law Economic Reform Program Act 2004 in Australia as well as the Sarbanes-Oxley Act of 2002, developed by the United States. Sarbanes-Oxley limits the level of work which can be carried out by accounting firms. In addition, the Act put a limit on the fee which a firm can receive from one client as a percentage of their total fees. This ensures that companies are not wholly reliant on one firm for its income, in the hope that they do not need to act unethically to keep a steady income. The act also protects whistleblowers and requires senior management in public companies to sign off on the accuracy of its company's accounting records. In 2002, the five members of the Public Oversight Board (POB), which oversaw ethics within the accounting profession, resigned after critics deemed the board ineffective and the SEC proposed developing a new panel, the Public Company Accounting Oversight Board (PCAOB). The PCAOB was developed through the Act, and replaced the POB.\n\nIn 2003, the International Federation of Accountants (IFAC) released a report entitled \"Rebuilding Public Confidence in Financial Reporting: An International Perspective\". By studying the international company collapses as a result of accounting issues, it determined areas for improvement within organizations as well as recommendations for companies to develop more effective ethics codes. The report also recommended that companies pursue options that would improve training and support so accountants could better handle ethical dilemmas.\nA collaborative effort by members of the international financial regulatory community led by Michel Prada, Chairman of the French Financial Markets Authority, resulting in establishment of the Public Interest Oversight Board (PIOB) on 1 March 2005.\nThe PIOB provides oversight of the IFAC standards-setting boards: the International Auditing and Assurance Standards Board (IAASB), the International Accounting Education Standards Board (IAESB) and the International Ethics Standards Board for Accountants (IESBA).\n\nThe most recent reform came into effect in July 2010 when President Obama signed \"The Dodd-Frank Wall Street Reform and Consumer Protection Act\". The act covers a broad range of changes. The highlights of the legislation are consumer protections with authority and independence, ends too big to fail bail outs, advance warning system, transparency and accountability for exotic instruments, executive compensation and corporate governance, protects investors, and enforces regulations on the books. The legislation also resulted in the Office of the Whistleblower, which was established to administer the SEC's whistleblower program. Congress authorized the SEC to provide monetary awards to whistleblowers who come forward with information that results in a minimum of a $1,000,000 sanction. The rewards are between 10% and 30% of the dollar amount collected. Whistleblowers help identify fraud and other unethical behaviors early on. The result is less harm to investors, quickly holding offenders responsible, and to maintain the integrity of the U.S. markets.\n\n\n"}
{"id": "5593083", "url": "https://en.wikipedia.org/wiki?curid=5593083", "title": "Action item", "text": "Action item\n\nIn management, an action item is a documented event, task, activity, or action that needs to take place. Action items are discrete units that can be handled by a single person.\n\nAction items are usually created during a discussion by a group of people who are meeting about one or more topics and during the discussion it is discovered that some kind of action is needed. The act required is then documented as an action item and usually assigned to someone, usually a member of the group. The person to whom the action is assigned is then obligated to perform the action and report back to the group on the results.\n\nAction items are usually documented in the meeting minutes and are recorded in the task list of the group. As people complete action items, the items are documented as being completed and the item is removed from the list of outstanding action items.\n\nThere are many attributes that can be associated with an action item - e.g.:\n\nA usual format for an action item is to record an action item number, the date when the action item is identified or created, the name of the person to whom the action is assigned, a title description of the action, a more detailed description of the action and its outcomes, and its deliverables.\n\nAn action item can be considered a more general form of various types of action/issue/defect tracking methods. For instance, a bug report is a form of action item as is a service report created by a service company to track a problem reported by a customer or an RMA number.\n\nFollow up in a consistent and timely manner with the responsible individual(s) to track their progress and drive the action item to resolution.\n\nIt is important to understand the distinction here that we track to resolution and not to completion. While it is the goal to have every action item completed, there are numerous scenarios in which the action item may no longer be relevant or completion may not be possible.\n\nThere are a number of software applications for tracking action items or service reports. These applications are used within software development organizations as well as in customer support functions. Because these software applications facilitate group communication, they are often referred to as Collaborative software or groupware.. Various such software are available in the market. They are known to increase the productivity of Managers.\n\nMany information technology support groups use some kind of service report software so that when a trouble report is called in, the person answering the telephone will create a trouble ticket to track the issue. The trouble ticket is assigned to a member of the IT services organization who then visits the person who has reported a problem and resolves the problem. The IT services person will then close out the trouble ticket indicating the issue has been resolved.\n\nRecent research at the Computational Semantics Lab at Stanford University seeks to automatically identify action items and extract their properties (the \"what\", \"who\", and \"when\" of the action item) using automatic speech recognition transcripts from spontaneous, multi-party conversations during meetings.\n\n"}
{"id": "2621934", "url": "https://en.wikipedia.org/wiki?curid=2621934", "title": "Agent-owned company", "text": "Agent-owned company\n\nAn agent-owned company is a private company, controlled by its agents, for which it provides common marketing and business coordination. It is common in the moving company sector, where moves are performed by local agents, under a national brand.\n\n"}
{"id": "30171297", "url": "https://en.wikipedia.org/wiki?curid=30171297", "title": "Aisle411", "text": "Aisle411\n\naisle411 Inc. is a St. Louis based company that has developed a consumer service called aisle411, which allows customers to use their phones to find products in stores. Founded in 2008 by Nathan Pettyjohn (Founder) and Matthew Kulig (Co-Founder), aisle411 entered the market in August 2009 with a mobile service that allowed consumers to search retail stores for product locations inside stores using their mobile phones.\n\nIn August 2009, aisle411 launched its original product location service in St. Louis, MO area Ace Hardware Ace Hardware locations and Springfield, MO area Price Cutter grocery stores. In 2010, aisle411 launched its mobile smartphone service with Shop 'n Save, a grocery chain owned by Supervalu Supervalu and Schnucks Schnucks grocery stores in the St. Louis, MO area. In September 2011, WinCo Foods WinCo Foods partnered with aisle411 to deploy its technology chainwide to its 79 stores. In December 2011, Hyvee Hy-Vee, a grocery chain of 235 store locations launched the aisle411 platform within their mobile iPhone and Android applications allowing shoppers to map products, lists, and weekly ads. In September 2012, Walgreens Walgreens Co. launched the aisle411 platform in its iPhone and Android mobile applications, making Walgreens in-store inventory searchable and mappable in over 7,800 locations.\n\nIn September 2012, aisle411 acquired the technology assets of WiLocate, adding indoor mobile device positioning technology to its service offering.\n\nIn September 2013, aisle411 raised $6.3 million in its first round of financing - led by St. Louis-based Cultivation Capital - to help the company scale, meet demand and expand retail partnerships. Another company that has invested in aisle411 includes Billiken Angels Network.\n\naisle411 allows consumers with Smart Phones to pull up a map pinpointing the aisle and location of the object of their desire in a particular store. The aisle411 platform is made available for licensing to retailers, and 3rd parties through an API and Map SDK. Shoppers can also use the app to scan barcodes to read product reviews and find out about in-store discounts and promotions. aisle411 went live in the iTunes Store around Thanksgiving.\n\nOfficial website www.aisle411.com\n"}
{"id": "44133735", "url": "https://en.wikipedia.org/wiki?curid=44133735", "title": "Alteryx", "text": "Alteryx\n\nAlteryx is an American computer software company based in Irvine, California, with a development center in Broomfield, Colorado. The company's products are used for data science and analytics. The software is designed to make advanced analytics accessible to any data worker.\n\nSRC LLC, the predecessor to Alteryx, was founded in 1997 by Dean Stoecker, Olivia Duane Adams and Ned Harding. SRC developed the first online data engine for delivering demographic-based mapping and reporting shortly after being founded. In 1998, SRC released Allocate, a data engine incorporating geographically organized U.S. Census data that allows users to manipulate, analyze and map data. Solocast was developed in 1998, which was software that allowed customers to do customer segmentation analysis.\n\nIn 2000, SRC LLC entered into a contract with the U.S. Census Bureau that resulted in a modified version of its Allocate software being included on CD-ROMs of Census Data sold by the Bureau.\n\nIn 2006, the software product Alteryx was released, which was a unified spatial and non-spatial data environment for building analytical processes and applications.\n\nIn 2010, SRC LLC changed its name to that of its core product, Alteryx.\n\nIn 2011, Alteryx raised $6 million in venture funding from the Palo Alto investment arm of SAP AG, SAP Ventures. In 2013, Alteryx raised $12 million from SAP Ventures and Toba Capital. In 2014, the company raised $60 million in Round B funding from Insight Venture Partners, SAP Ventures and Toba Capital, and announced plans for a 30% workforce expansion.\n\nIn 2015, Iconiq Capital led a $85 million investment in Alteryx, with Insight Venture Partners and Meritech Capital Partners also participating. Alteryx announced plans to use the new capital to expand internationally, invest in research and development, and increase its sales and marketing efforts.\n\nAlteryx announced in 2015 a new relationship with Microsoft in order to enable faster and easier data analysis through Power BI.\n\nIn 2016, Alteryx was ranked #24 on the Forbes Cloud 100 list.\n\nOn March 24, 2017, Alteryx went public launching their IPO at the NYSE.\n\nOn February 22, 2018, Alteryx was named a leader in the 2018 Magic Quadrant for Data Science and Machine Learning Platforms. \n\nAlteryx currently offers four main products as part of an analytics platform: Alteryx Connect, Alteryx Designer, Alteryx Promote and Alteryx Server. Alteryx also hosts a cloud-based website known as the Alteryx Analytics Gallery.\n\nAlteryx was recognized by research firm Gartner as a leader in the 2018 Magic Quadrant for Data Science and Machine Learning Platforms. In addition, Alteryx was named the Gold winner in \"The Best Business Intelligence and Analytics Software of 2017, as Reviewed by Customers” by Gartner Peer Insights, a comprehensive platform that provides unfiltered, first-hand product and service ratings and reviews by experienced Enterprise Technology Buyers.\n\nAlteryx has also been named one of Deloitte’s Technology Fast 500 and a Top 20 AI All-Stars in Technology by KeyBanc Capital Markets.\n\nIn 2017, co-founder and CEO Dean Stoecker received the Ernst and Young Entrepreneur of The Year 2017 Award in the Orange County Region, which recognizes entrepreneurs who excel in areas such as innovation, financial performance and personal commitment to their businesses and communities.\n\nAlteryx was also named as one of the Best Places to Work in Orange County three years in a row. 2016, 2017, and 2018, ranking within the top ten in the large employer category.\n\nKevin Rubin was recognized as the CFO of the Year by the Orange County Business Journal, an annual award that recognizes Orange County CFOs who demonstrated superior leadership and corporate stewardship in the preceding fiscal year.\n\nIn October 2017, it was discovered that Alteryx was subject to a data breach of partially anonymized data records for approximately 123 million U.S. households. While no names were attached, telephone numbers and physical addresses were among the 248 fields per household involved in the breach. Also included was \"consumer demographics, life event, direct response, property, and mortgage information for more than 235 million consumers\" according to the company. Alteryx assembled information from Experian and public sources like the U.S. Census Bureau to create their product which sold for 39,000 per license. Alteryx's hosting on Amazon Web Services had been unsecured (its sources had no breach).\n"}
{"id": "31074815", "url": "https://en.wikipedia.org/wiki?curid=31074815", "title": "Artifact-centric business process model", "text": "Artifact-centric business process model\n\nArtifact-centric business process model represents an operational model of business processes in which the changes and evolution of business data, or business entities, are considered as the main driver of the processes. The artifact-centric approach, also called activity-centric or process-centric business process modeling, focuses on describing how business data is change/updated, by a particular action or task, throughout the process.\n\nIn general, a process model describes activities conducted in order to achieve business goals, informational structures, and organizational resources. Workflows, as a typical process modeling approach, often emphasize the sequencing of activities (i.e., control flows), but ignore the informational perspective or treat it only within the context of single activities. Without a complete view of the informational context, business actors often focus on what should be done instead of what can be done, hindering operational innovations.\n\nBusiness process modeling is a foundation for design and management of business processes. Two key aspects of business process modeling are a formal framework that integrates both control flow and data, and a set of tools to assist all aspects of a business process life cycle. A typical business process life cycle includes at least a design phase, concerned with the “correct” realization of business logic in a resource-constrained environment, and an operational phase, concerned with optimizing and improving execution (operations). Traditional business process models emphasize a procedural and/or graph-based paradigm (i.e., control flow). Thus, methodologies to design workflow in those models are typically process-centric. It has been argued that a data-centric perspective is more useful for designing business processes in the modern era.\n\nIntuitively, business artifacts (or simply artifacts) are data objects whose manipulations define the underlying processes in a business model. Recent engineering and development efforts have adopted the artifact approach for design and analysis of business models. An important distinction between artifact-centric models and traditional data flow (computational) models is that the notion of the life cycle of the data objects is prominent in the former, while not existing in the latter.\n\nArtifact-centric modeling is an area of growing interest. Nigam and Caswell introduced the concept of business artifacts and information-centric processing of artifact lifecycles. Kumaran et al.'s further studies on artifact-centric business processes can be found here. Bhattacharya described a successful business engagement which applies business artifact techniques to industrialize discovery processes in pharmaceutical research. Liu et al. formulated nine commonly used patterns in information-centric business operation models and developed a computational model based on Petri Nets. Bhattacharya, K., et al. provides a formal model for artifact-centric business processes with complexity results concerning static analysis of the semantics of such processes. Kumaran et al. presented the formalized information-centric approach to discovering business entities from activity-centric process models and transforming such models into artifact-centric business process models. An algorithm was provided to achieve this transformation automatically.\n\nOther approaches related to artifact-centric modelling can be found in. Van der Aalst et al. provides a case-handling approach where a process is driven by the presence of data objects instead of control flows. A case is similar to the business entity concept in many respects. Wang and Kumar proposed the document-driven workflow systems which is designed based on data dependencies without the need for explicit control flows. Muller et al. also introduced the framework for the data-driven modelling of large process structures, namely COREPRO. The approach reduces modelling efforts significantly and provides mechanisms for maintaining data-driven process structures.\n\nAnother related thread of work is the use of state machines to model object lifecycles. Industries often define data objects and standardize their lifecycles as state machines to facilitate interoperability between industry partners and enforce legal regulations. Redding et al. and Küster et al. give techniques to generate business processes which are compliant with predefined object lifecycles. In addition, event-driven process modelling, for example, Event-driven Process Chains (EPC), also describes object lifecycles glued by events.\n\nMore recent and closely related work on artifact-centric process model can be found in. Gerede and Su developed a specification language ABSL to specify artifact behaviours in artifact-centric process models. The authors showed decidability results of our language for different cases and provided key insights on how artifact-centric view can affect the specification of desirable business properties. Gerede et al. identified important classes of properties on artifact-centric operational models focusing on persistence, uniqueness and arrival properties. They proposed a formal model for artifact-centric operational models to enable a static analysis of these properties and showed that the formal model guarantees persistence and uniqueness.\n\nFritz, Hull, and Su formulated the technical problem of goal-directed workflow construction in the context of declarative artifact-centric workflow, and develop results concerning the general setting, design time analysis, and the synthesis of workflow schemas from goal specifications. The work is among the important initial steps along the path towards eventual support for tools that enable substantial automation for workflow design, analysis, and modification. Deutsch et al. introduced the artifact system model, which formalizes a business process modelling paradigm that has recently attracted the attention of both the industrial and research communities. The problem of automatic verification of artifact systems, with the goal of increasing confidence in the correctness of such business processes is also studied.\n\nSira and Chengfei proposed a novel view framework for artifact-centric business processes. It consists of artifact-centric process model, process view model, a set of consistency rules, and the construction approach for building process views. The formal model of artifact-centric business processes and views, namely ACP, is defined and used to describe artifacts, services, business rules that control the processes, as well as views. They developed a bottom-up abstraction mechanism for process view construction to derive views from underlying process models according to view requirements. Consistency rules are also defined to preserve the consistency between constructed view and its underlying process. This work can be considered as one approach to the abstraction, i.e., generalization of artifact-centric business processes. The framework has also been extended to address modelling and change validation of inter-organizational business processes.\n\n"}
{"id": "149833", "url": "https://en.wikipedia.org/wiki?curid=149833", "title": "Association of MBAs", "text": "Association of MBAs\n\nThe Association of MBAs (AMBA) is a global organisation founded in 1967 which focuses primarily on international business school accreditation and membership.\n\nBased in London, AMBA is one of the three main global accreditation bodies in business education (see Triple Accreditation) and styles itself as the world's impartial authority on postgraduate management education. It differs from AACSB in the US and EQUIS in Brussels as it accredits a school's portfolio of postgraduate management programmes but does not accredit undergraduate programmes. AMBA accredits approximately 2% of the world's business schools, and is the most international of the three organisations having accredited schools headquartered in 54 countries, compared with the 52 for AACSB and 38 for EQUIS.\n\nBusiness schools can become associated with AMBA in two ways: by applying for accreditation, or by applying for membership of the AMBA Development Network (ADN), which confers institutional membership similar to EFMD or AACSB membership. Schools that cannot meet all of the AMBA accreditation criteria usually join the ADN, which gives them time to prepare for accreditation with support from AMBA and mentoring from an AMBA-accredited school.\n\nAll MBA students and alumni of the 257 accredited member schools join AMBA as individual members free of charge. AMBA also accredits generalist MBM programmes and DBA programmes, and admits as members students and graduates thereof.\n\nAMBA's long-serving president until 2017 was the late Sir Paul Judge, the founding benefactor of Cambridge Judge Business School in Cambridge, UK. AMBA's current Chief Executive is Andrew Main Wilson, who joined the organisation from the Institute of Directors in 2013. Bodo Schlegelmilch was elected Chairman of the AMBA Board of Trustees in 2018.\n\nThe Association of MBAs was founded in 1967 as an MBA alumni club by eight UK graduates from Harvard Business School, Wharton, Stanford and Columbia, and two graduates from the first intake of London Business School. The founders saw a lack of awareness in Europe of the value of the MBA degree, which at that time was primarily an American qualification. They decided to form a lobby and membership group to promote the benefits of postgraduate business education, under the name of Business Graduates Association (BGA). The organisation's development helped shape the growth of management education in Europe and the UK and coincided with the setting up and growth of London Business School and Manchester Business School in Britain.\n\nThe Association's first Director General was Vice-Admiral David Clutterbuck who assumed this position in 1969. In 1983 BGA began to accredit the growing number of MBA programmes, while preserving its functions as a membership organization. BGA was renamed Association of MBAs in 1987. Until 2017, AMBA's president was the late Sir Paul Judge, who helped establish one of the two business schools in Cambridge, UK.\n\nThe Association of MBAs accredits MBA, MBM and DBA degree programmes. When a school applies for accreditation for its MBA programmes, AMBA requires that the entire portfolio of MBA programmes be put up for consideration and will award accreditation only if all programmes meet its criteria (though the school pays the same fee regardless of the number of programmes being reviewed).\n\nThe Association's process of accrediting a school's MBA programmes portfolio includes reviewing compliance AMBA's criteria, most of them qualitative rather than quantitative. The criteria fall into seven dimensions: history and development of the institution; facilities and libraries; teaching faculty, teaching standards and research track record; programme administration, career and alumni services; student admission standards, diversity and cohort size; curriculum content, programme mode and duration; and learning outcomes.\n\nSome of the key AMBA criteria for the accreditation of an MBA programme include:\n\nAMBA holds three annual conferences for business school deans and directors: a Global Conference, an Asia Pacific Conference, and a Latin America Conference. Participation is open to both accredited and non-accredited schools. AMBA also hosts an annual Gala Dinner in London, which is open only to accredited schools.\n\nAMBA organises two annual global forums with the purpose of development and training for specific functions within AMBA-accredited business schools such as accreditation managers; programme managers; marketing, admissions, alumni and development staff.\n\nAMBA also organises webinars, lectures and networking events on a regular basis catering to MBA alumni, current MBA students, prospective MBA students and business school admissions departments. These on-campus events are held at accredited business schools and often feature distinguished speakers and practitioners in fields such as leadership, entrepreneurship and innovation.\n\n\n"}
{"id": "12215218", "url": "https://en.wikipedia.org/wiki?curid=12215218", "title": "Bates Australia", "text": "Bates Australia\n\nBates Australia is a leading saddle manufacturer originally established in 1934. Bates Saddlery was formed when Mr. George Bates borrowed $100 from his sister, bought a sewing machine and began to make saddles on the veranda of his home in Perth, Western Australia. Bates Australia is the parent company to Bates Saddles and Wintec. \n\nThe family-owned business, located in Perth, Western Australia, has expanded into many international markets and can now be found in 36 countries around the world.\n\n"}
{"id": "53108275", "url": "https://en.wikipedia.org/wiki?curid=53108275", "title": "Blue Prism", "text": "Blue Prism\n\nBlue Prism is the trading name of the Blue Prism Group, a UK multinational software corporation that pioneered and makes enterprise robotic process automation software to eliminate low-return, high-risk, manual data entry and processing work. Blue Prism is headquartered in Newton-le-Willows, Merseyside, UK with regional offices in the US and Australia. The company is listed on the London Stock Exchange AIM market.\n\nBlue Prism was founded in 2001 by a group of process automation experts to develop technology that could be used to improve the efficiency and effectiveness of organisations. Initially their focus was on the white collar back office where they recognised an enormous unfulfilled need for automation. The company was co-founded by Alastair Bathgate and David Moss to provide a new business-led, more granular and economic approach that today is known as Robotic Process Automation, or RPA. In doing so Blue Prism created a digital workforce. In 2003 Blue Prism’s first commercial product was launched. Co-operative Financial Services began using Blue Prism software in 2005, to automate manual processes in customer services.\n\nRobotic Process Automation (RPA) is the application of technology that provides organizations with a digital workforce that follows rule-based business processes and interacts with the organisations systems in the same way that existing users currently do. Blue Prism coined the term \"robotic automation\" in 2012.\n\nOn March 18, 2016, the company floated on the London Stock Exchange AIM market, becoming a public company. With a market capitalization of £48.5 million, Blue Prism underwent an IPO in March 2016. The company's shares rose 44 percent on the first day of trading on AIM, under CEO Alastair Bathgate. Customers include O2, Co-operative Bank and Fidelity Investment Management. By November 2016, it had offices in Chicago and Miami, as well as the United Kingdom.\n\nOn January 6, 2017, Blue Prism announced it would open new offices in Austin, Texas, while remaining based in London. At the time, it employed 86 people worldwide. In March 2017, a group of shareholders sold stakes in Blue Prism. At the time, Blue Prism remained based in Merseyside. In June 2017, Blue Prism announced that a new version of its software would run on public clouds such as Amazon Web Services, Microsoft Azure, and Google Cloud Platform. Previously, the software had run on customers' own servers.\n\nBlue Prism is built on the Microsoft .NET Framework. It automates any application and supports any platform (mainframe, Windows, WPF, Java, web, etc.) presented in a variety of ways (terminal emulator, thick client, thin client, web browser, Citrix and web services). It has been designed for a multi-environment deployment model (development, test, staging, and production) with both physical and logical access controls. Blue Prism RPA software includes a centralized release management interface and process change distribution model providing high levels of visibility and control. Additional control is provided to the business via a centralised model for process development and re-use. The software supports regulatory contexts such as PCI-DSS, HIPAA and SOX, with a large number of controls in place to provide the necessary security and governance.\n\nIn 2016, Blue Prism received one of the top honors at the AIconics Awards, named as The Best Enterprise Application of AI.\n\nBlue Prism supports only \"back-office\" unattended robots and does not offer attended \"front-office\" robots.\n\nThe main competitors in the Robotic Process Automation market include UIPath and Automation Anywhere. \nThe independent market research company Forrester Research identified these three companies as leaders both in terms of their market presence as well as the quality of their offering in a 2017 study.\nIn April 2018, industry analyst firm Wikibon attempted to test Robotic Process Automation platforms noting: \"we contacted three RPA vendors — Automation Anywhere, Blue Prism, and UiPath — to test and validate each of these propositions. Only one vendor, UIPath, responded.\"\n\nBlue Prism’s digital workforce is built, managed and owned by the user or customer, spanning operations and technology, adhering to an enterprise-wide robotic operating model. It is code-free and can automate any software in a non-invasive way. The digital workforce can be applied to automate processes in any department where clerical or administrative work is performed across an organization.\n\nBlue Prism has been deployed in a number of industries including banking, finance and insurance, consumer package goods, legal services, public sector, professional services, healthcare and utilities.\n"}
{"id": "1434997", "url": "https://en.wikipedia.org/wiki?curid=1434997", "title": "Business Planning and Control System", "text": "Business Planning and Control System\n\nBusiness Planning and Control System (BPCS) is an Enterprise Resource Planning (ERP) software designed for an OS/400 system. BPCS is a series of software programs that are the largest software supplier for AS/400.\n\n\"BPCS\", the acronym for the software, is pronounced as \"Bee picks\".\n\nBPCS was developed by Chicago-based System Software Associates (SSA), which later became SSA Global Technologies (which was then acquired by Infor Global Solutions and rebranded as Infor LX), and is used to control the operations of manufacturing companies. BPCS includes MRP logic to manufacturing operations, provided there are high standards of data validity such as engineering specifications and inventory accuracy. It runs on several systems, with the IBM System i (previously named IBM AS/400 and IBM eServer iSeries), being most popular. It is written in AS/SET CASE tool, RPG, SQL and other IBM languages supported on the System i.\n\nMany of the BPCS modules are stand-alone, in that companies can choose to implement only the financial applications for example, and none of the manufacturing. \n\nSSA began developing BPCS in the early 1980s; by the mid 1990s the BPCS programs were used internationally. \"Inc. magazine\" ranked SSA as the 23rd fastest growing small public company in 1988 and \"Business Week\" named it as the 25th best small company.\n\nBPCS Applications are very dependent on BPCS software version release, because SSA enters into partnerships with different specialty suppliers of applications such as Data Mining, Bar Coding, etc. and suppliers that integrated with a particular version.\n\nThe BPCS Application suite includes:\n\n\nMost planning functions can be used in either Distribution or Manufacturing.\n\nMost planning functions are used in both Distribution and Manufacturing.\n\nMost planning functions are used in both Distribution and Manufacturing.\n\n\n"}
{"id": "33171468", "url": "https://en.wikipedia.org/wiki?curid=33171468", "title": "Business operating system (management)", "text": "Business operating system (management)\n\nThe term business operating system (BOS) refers to standard, enterprise-wide collection of business processes used in many diversified industrial companies. The definition has also been extended to include the common structure, principles and practices necessary to drive the organization.\n\nDiversified industrial companies like Ingersoll Rand, Honeywell, and Danaher have adopted a standard, common collection of business processes and/or business process improvement methodologies which they use to manage strategy development and execution. In the case of Danaher, the business system is a core part of the company's culture, is seen as one of the key drivers of corporate performance, and is therefore a differentiator for shareholders and prospective employees.\n\nThe objectives of such systems are to ensure daily work is focused on the organisation's strategic objectives and is done in the most efficient way. The systems deal with the questions \"why\" (purpose of the work), \"what\" (specific objectives of the work) and \"how\" (the processes used to do the work). The Toyota Production System is focused on both how to make cars, and how to improve the way cars are made. A third objective can also be added, which is to improve the business system itself by identifying or improving the component tools and techniques.\n\nTerms used to describe such systems include:\n\n\nMany of business operating systems share common features. This is because the systems are derived from other known systems, and from established methods and practices for business management. The following is a list of features that appear in several systems.\n"}
{"id": "1155664", "url": "https://en.wikipedia.org/wiki?curid=1155664", "title": "Business process automation", "text": "Business process automation\n\nBusiness process automation (BPA), also known as business automation or digital transformation, is the technology-enabled automation of complex business processes. It can streamline a business for simplicity, achieve digital transformation, increase service quality, improve service delivery or contain costs. It consists of integrating applications, restructuring labor resources and using software applications throughout the organization. Robotic process automation is an emerging field within BPA and uses artificial intelligence.\n\nBPAs can be implemented in a number of business areas including marketing, sales and workflow. Toolsets vary in sophistication, but there is an increasing trend towards the use of artificial intelligence technologies that can understand natural language and unstructured data sets, interact with human beings, and adapt to new types of problems without human-guided training. BPA providers tend to focus on different industry sectors but their underlying approach tends to be similar in that they will attempt to provide the shortest route to automation by exploiting the user interface layer rather than going deeply into the application code or databases sitting behind them. They also simplify their own interface to the extent that these tools can be used directly by non-technically qualified staff. The main advantage of these toolsets is therefore their speed of deployment, the drawback is that it brings yet another IT supplier to the organization.\n\nThe market is, however, evolving in this area. In order to automate these processes, connectors are needed to fit these systems/solutions together with a data exchange layer to transfer the information. A process driven messaging service is an option for optimizing your data exchange layer. By mapping your end-to-end process workflow, you can build an integration between individual platforms using a process driven messaging platform. Process driven messaging service gives you the logic to build your process by using triggers, jobs and workflows. Some companies uses an API where you build workflow/s and then connect various systems or mobile devices. You build the process, creating workflows in the API where the workflow in the API acts as a data exchange layer.\n\nA business process management system is quite different from BPA. However, it is possible to build automation on the back of a BPM implementation. The actual tools to achieve this vary, from writing custom application code to using specialist BPA tools. The advantages and disadvantages of this approach are inextricably linked – the BPM implementation provides an architecture for all processes in the business to be mapped, but this in itself delays the automation of individual processes and so benefits may be lost in the meantime.\n\nThe practice of performing robotic process automation results in the deployment of attended or unattended software agents to an organization's environment. These software agents, or robots, are deployed to perform pre-defined structured and repetitive sets of business tasks or processes. Artificial intelligence software robots are deployed to handle unstructured data sets and are deployed after performing and deploying robotic process automation. Robotic process automation is the leading gateway for the adoption of artificial intelligence in business environments.\n\n\nBusiness Process Automation LTI Canada\n"}
{"id": "2254416", "url": "https://en.wikipedia.org/wiki?curid=2254416", "title": "Business record", "text": "Business record\n\nA business record is a document (hard copy or digital) that records a business dealing. Business records include meeting minutes, memoranda, employment contracts, and .\n\nIt must be retrievable at a later date so that the business dealings can be accurately reviewed as required. Since business is dependent upon confidence and trust, not only must the record be accurate and easily retrieved, the processes surrounding its creation and retrieval must be perceived by customers and the business community to consistently deliver a full and accurate record with no gaps or additions.\n\nMost business records have specified retention periods based on legal requirements and / or internal company policies. This is important because in many countries (including the United States) many documents \"may\" be required by law to be disclosed to government regulatory agencies or to the general public. Likewise, they \"may\" be discoverable if the business is sued. Under the business records exception in the Federal Rules of Evidence, certain types of business records, particularly those made and kept with regularity, may be considered admissible in court despite containing hearsay.\n\n\n"}
{"id": "4032051", "url": "https://en.wikipedia.org/wiki?curid=4032051", "title": "Business simulation", "text": "Business simulation\n\nBusiness simulation is simulation used for business training, education or analysis. It can be scenario-based or numeric-based.\n\nMost business simulations are used for business acumen training and development. Learning objectives include: strategic thinking, decision making, problem solving, financial analysis, market analysis, operations, teamwork and leadership.\n\nThe business gaming community seems lately to have adopted the term business simulation game instead of just gaming or just simulation. The word simulation is sometimes considered too mechanistic for educational purposes. Simulation also refers to activities where an optimum for some problem is searched for, while this is not usually the aim of an educational game. On the other hand, the word game can imply time wasting, not taking things too seriously and engaging in an exercise designed purely for fun. The concept of simulation gaming seems to offer the right combination and balance between the two. Simulation gaming is also the term that the educational gaming community has adopted.\n\nPartly, the terminology of business simulation games is not well established. The most common term used is business game but several other terms are also in use. Here we will define the most common terms used in context of (computer-based) business learning environments.\n\nKlabbers (1999) notes that gaming is sometimes associated with something that is frivolous, just for the fun of it. This hampers its scientific endeavor and the more serious connotations of gaming in the scientific arena. The term game is used to describe activities in which some or all of these characteristics are prominent:\n\nGames are played when one or more players compete or cooperate for payoffs, according to an agreed set of rules. Players behave as themselves though they may well display exceptional behavior. Games are social systems and they include actors (players), rules and resources, which are the basic building blocks of social systems. In each game, the players (actors) interact with one another, while applying different rules, and utilizing different resources.\n\nTsuchiya and Tsuchiya note that the simulation gaming community is still struggling to establish itself as a discipline, although 35 years have passed since the International Simulation and Gaming Association (ISAGA) was established. To be a discipline, simulation gaming needs a theory, methodology, and application and validation. Of these, forming a theory is the most difficult challenge. Similar comments come from Wolfe and Crookall. Referring to prior research they conclude that the educational simulation gaming field has been unable to create a generally accepted typology, let alone taxonomy, of the nature of simulation gaming. According to them this is unfortunate because the basis of any science is its ability to discriminate and classify phenomena within its purview, based on underlying theory and precepts. Without this, the field has been stuck, despite its age, at a relatively low level of development.\n\nIn most cases, the terms business (simulation) game and management (simulation) game can be used interchangeably and there is no well-established difference between these two terms. Greenlaw et al. determine a business game (or business simulation) as \"a sequential decision-making exercise structure around a model of a business operation, in which participants assume the role of managing the simulated operation\". The descriptions given for a management game, for example, by Forrester and Naylor do not differ from the previous. However, Elgood determines that in a management game profit is not the dominant measure of success. Keys and Wolfe define a management game as a simplified simulated experiential environment that contains enough verisimilitude, or illusion of reality, to include real world-like responses by those participating in the exercise.\n\nGredler divides experiential simulations into the following four categories:\nBusiness simulation games are most often of the first kind. A participant in a data management simulation typically functions as a member of a team of managers or planners. Each team is managing a company allocating economic resources to any of several variables in order to achieve a particular goal.\n\nBusiness strategy games are intended to enhance students' decision-making skills, especially under conditions defined by limited time and information. They vary in focus from how to undertake a corporate takeover to how to expand a company's share of the market. Typically, the player feeds information into a computer program and receives back a series of optional or additional data that are conditional upon the player's initial choices. The game proceeds through several series of these interactive, iterative steps. As can be noted, this definition does not consider continuous (real-time) processing an alternative.\n\nIn business simulation games players receive a description of an imaginary business and an imaginary environment and make decisions – on price, advertising, production targets, etc. – about how their company should be run. A business game may have an industrial, commercial or financial background (Elgood, 1996). Ju and Wagner mention that the nature of business games can include decision-making tasks, which pit the player against a hostile environment or hostile opponents. These simulations have a nature of strategy or war games, but usually are very terse in their user interface. Other types of managerial simulations are resource allocation games, in which the player or players have to allocate resources to areas such as plant, production, operations, marketing, and human resources, in order to produce and sell goods.\n\nAccording to Senge and Lannon in managerial microworlds – like business simulation games – unlike in the actual world, managers are free to experiment with policies and strategies without fear of jeopardizing the company. This process includes the kind of reflection and inquiry for which there is no time in the hectic everyday world. Thus, Senge and Lannon argue, managers learn about the long-term, systemic consequences of their actions. Such \"virtual worlds\" are particularly important in team learning. Managers can learn to think systemically if they can uncover the subtle interactions that thwart their efforts.\n\nNaylor in 1971 gives quite a detailed view of the contents, structure, and operating of management games. Today, this description by Naylor is still valid for most of the business simulation games. Business simulation games are built around a hypothetical oligopolistic industry consisting of three to six firms, whose decision-makers or managers are the participants of the game. Each firm or team is allocated a specific amount of resources in the form of cash, inventories, raw materials, plant and equipment, and so forth. Before each operating period the players make decisions. Naylor mentions that these decisions can concern, e.g., price, output, advertising, marketing, raw material acquisition, changes in plant capacity, and wage rate. This information is read into a computer that has been programmed on the basis of a set of mathematical models that provide a link between the operating results and operating decisions of the individual firms, as well as the external environment (the market). On the basis of (a) a set of behavioral equations, such as demand and cost functions, and a set of accounting formulas that have been programmed into the computer, and (b) the individual decisions of each firm, operating results are generated by the computer in the form of printed reports – for example, profit and loss statements, balance sheets, production reports, sales reports, and total industry reports – at the end of each operating period. Usually the environment can be changed by the administrator of the game by altering the parameters of the operating characteristics of the game. In each case, the firms find it necessary to react according to the magnitude and the nature of the change imposed by the external environment. Naylor mentions that some of the more complicated and more realistic games even permit multiple products, plants, and marketing areas, stochastic production periods, stochastic demand, labor negotiations, and the sale of common stock. For more information about this topic see Lainema (2003).\n\nThe first use of games for education and development was the war game simulations in China in about 3,000 B.C. These games bore a vague similarity to the early 17th century chess. In the Western world, war games date back to at least the German Kriegspiel of the mid-nineteenth century (Faria and Dickinson). Faria and Dickinson note that different war games have also been conducted in Japan before the Second World War and war games have been long used by the British and the Americans to test battle strategies. Military officers trained with war games in the 1930s and 1940s started to use their military training to manage civilian businesses. Some of the business game evolution can be traced to a 1955 Rand Corporation game, which simulated the U.S. Air Force inventory management within its supply system. Greenlaw et al. state that business simulation exercises may be considered an outgrowth of earlier developments in three fields: military war gaming, operations research, and educational role-playing.\n\nAccording to Naylor, the use of games in business and economics goes back to 1956 when the American Management Association developed the first so-called management decision-making game, called the Top Management Decision Game. Faria and Dickinson and Greenlaw et al. also find this the first widely known business decision-making simulation, although Greenlaw et al. date the origin of the game to 1957 and further specify that it was the first non-military competitive business game. Greenlaw et al. note that the Top Management Decision Simulation stimulated the design and use of dozens of other games. In this simulation five teams of players operated firms competing in a hypothetical, one-product industry. Teams made quarterly decisions covering price, production volume, budgets, research and development, advertising, and sales force and could request selected marketing research information. During the period 1955-1957 only one or two new games appeared each year (Faria, 1990).\n\nA rapid growth in the number of business games occurred over the years from 1958 to 1961. Greenlaw et al. had made a summary of some business games available by the beginning of the 1960s. The summary includes 89 different business games or different versions of a certain business game developed by industrial firms, business associations, educational institutes, or governmental units. Naylor mentions already in 1971 that hundreds of management games have been developed by various universities, business firms, and research organizations. These management games have been used both for research purposes and for training people in diverse disciplines such as management, business operation, economics, organization theory, psychology, production management, finance, accounting, and marketing. Also Faria (1990) and Dickinson note that the number of simulation games grew rapidly in the 1960s. McRaith and Goeldner list 29 marketing games, of which 20 had been developed by business firms and nine by academians for university teaching. In 1969 Graham and Gray listed nearly 200 business games of different varieties. Horn and Cleaves provided a description of 228 business games. Faria (1989) mentions that over 200 simulations were in use in the United States in over 1,700 business schools. Overall, taking advantage of computer games in education increased enormously through the 1960s to the 1980s, see for example Ju and Wagner.\n\nAt the end of the 1980s Faria (1990) estimated that there were approximately 228 games available in the United States, and that there were around 8,500 instructors using business games. At that point Faria also believes that there is a large and growing number of business schools instructors and business firm users of simulation games. Still, Faria estimated that only 12.5% of all US business firms with training and development managers used computerized business games.\n\nThe penetration of business gaming in academia is fuelled by the following factors: the increase in student numbers, the increase in new courses, increased adoption of methods supporting diverse learning styles, and the increasing availability of technology. Dickinson and Faria state that in US over 200 business games are being used by nearly 9,000 teachers at over 1,700 colleges offering business programmes.\n\nLarsen and Lomi describe the shift of the objectives of management gaming. They state that until the early 1980s simulation was used to forecast the behavior of a variety of sub-system level variables, ranging from the cash flow and financial performance of a company, to the inflation and unemployment rates of an economy. They state further, that during the last 15 years a new way of thinking about simulation emerged. Instead of focusing on predicting, simulation progressively became a tool to help management teams understand their company and industry's problems and opportunities. Simulations could prepare for the future and reduce the sensitivity of possible strategies to changes in alternative frames of reference – or mental models. Larsen and Lomi further note, that the emphasis of computer-based simulation models has shifted:\n\nIn the late 1990s, training and consulting companies began designing and customizing business simulations for individual companies to augment their corporate leadership development programs. The business simulations often focused on strategy and business acumen. The business simulations allowed participants to test their decision-making skills, make mistakes, and safely learn from their experience. Some refer to this type of employee education as \"experiential learning\". By 2000, business simulations were available that blended the traditional business acumen (financial) skills with the softer – interpersonal – skills required for effective leadership development.\n\nIn a business game or business simulation game, a scenario is played out in a simulated environment and the learner or user is asked to make individual or team based decisions on how to act in the simulations. Often multiple choice alternatives are used and the scenario is played out following a branching tree based on which decisions the learner makes. Throughout or at certain intervals feedback is provided. These are similar to role-play simulations.\n\nA numeric simulation can mimic a whole company on a high level or it can be more detailed and mimic specific organizational units or processes. In a numeric simulation the learner or user makes decisions by pulling levers and dialers as well as through inputting numbers. The decisions are processed and the outcomes are calculated and shown in reports and graphs, e.g. price and volume as well as number of employees can be decisions and the outcome can be viewed in e.g. an income statement, a balance sheet and a cash flow statement. Feedback is given throughout the simulation or at certain intervals, such as when a year has passed. Many numeric business simulations include elements of competition against other participants or against computer generated competitors.\n\nBusiness simulation games can be classified according to several properties. The first taxonomies were introduced already in the beginning of the 1960s (see e.g. Greenlaw et al., 1962). Here we introduce the taxonomy from Biggs, which is practically identical with the taxonomy from Greenlaw et al.\nBusiness simulation game developers regard their artifacts to be learning environments. When arguing for this, they most often refer to David A. Kolb's influential work in the field of experiential learning. During the last decades ideas from constructivism and authentic e-learning have also provided new perspectives for considering the role of business simulations in learning.\nThe activities carried out during a simulation game training session are:\n\nThe last phase in the list above is usually called debriefing. Debriefing is the most important part of the simulation/gaming experience. We all learn from experience, but without reflecting on this experience the learning potential may be lost. Simulation gaming needs to be seen as contrived experiences in the learning cycle, which require special attention at the stages of reflection and generalization.\n\nThiagarajan lists six phases of debriefing, presented as a flexible suggestion and not as rigid requirements:\n\nVan Ments notes that the aim of debriefing is to: deal with factual errors and to tie up loose ends (including scoring); draw out general conclusions about the session; and deduce general lessons which can be extrapolated to the real world. Furthermore, the participants should not be allowed to conclude what was learned without receiving feedback (Gentry, 1990). The participants need to articulate their perception of what was learned, and the instructor needs to put things into a broader perspective. Gentry also expresses that process feedback is much more valuable than outcome feedback. As games are less-than-perfect representations of the real world, it should be the decision process used that needs to be applauded or critiqued, not the gaming outcome.\nThe importance of reflection, debriefing and feedback highlight the need for business simulations to supported by carefully considered learning outcomes, pedagogy and assessment tasks. Student factors such as low motivation to engage and prior skill weaknesses can undermine the ability of authentic assessment regimes to achieve the purported learning benefits. The Online Business Simulations Project funded by the Australian Government Office for Learning and Teaching has developed a range of resources to help educators embed simulations into their classes.\n\n\n"}
{"id": "28724157", "url": "https://en.wikipedia.org/wiki?curid=28724157", "title": "Centre for India &amp; Global Business", "text": "Centre for India &amp; Global Business\n\nThe Centre for India & Global Business (CIGB) was launched in March 2009 as part of Cambridge Judge Business School at the University of Cambridge, England, to support the university’s growing engagement with India. As a research centre and an engagement platform CIGB is dedicated to the study of India's rapidly expanding role in the global knowledge economy.\n\nCIGB was established with initial funding from the BP foundation. The academic director of CIGB is Professor Jaideep Prabhu, the first Jawaharlal Nehru Professor of Indian Business and Enterprise at the University of Cambridge, who took up the post in September 2008. The founding Executive Director of CIGB was Navi Radjou, who served between January 2009 and June 2011. The Centre has 14 other academic members, including Cambridge Judge Business School faculty and PhD students.\n\nCIGB conducts research in collaboration with a global network of academic partners around three complementary themes:\nA major issue underlying these three themes is how India is emerging as a learning laboratory for affordable and sustainable innovation to deal with the global challenges of scarcity and diversity.\n\nThe academic projects undertaken by CIGB involve field research in India which CIGB-related faculty visit frequently in order to collect data and conduct interviews with local policy-makers, entrepreneurs, and corporate executives.\n\nCIGB produces case studies that feed into its outreach and networks activities. Its research has been featured in leading newspapers and magazines worldwide.\n\nCIGB also hosts conferences and seminars that bring together academics, corporate leaders, and policy-makers from all over the world to discuss and disseminate latest research findings as well as policy and industry best practices. These events are hosted both in Cambridge as well as in major Indian cities. For instance, in March 2009, Professor Jaideep Prabhu, Director of CIGB, delivered his inaugural lecture in New Delhi, which was chaired by Mr Montek Singh Ahluwalia, Deputy Chairman of the Indian Planning Commission. In May 2009, CIGB hosted in Cambridge a major conference titled “\"Innovation in India and China: How to Create Value from Emerging Markets\",” which explored the rise of India and China as both fast-growing global markets and world-class sources of innovation.\n\nSince its inception, CIGB has also hosted a number of speakers including: Nandan Nilekani, Chairman of the new Unique Identification Authority of India (UIDAI) and former Co-Chairman of Infosys Technologies (April 2009); former Indian president Dr A. P. J. Abdul Kalam(June 2009); Ravi Kant, Vice-Chairman of Tata Motors; and Dr Ramesh Mashelkar, former Director General of the Council of Scientific and Industrial Research, who delivered the inaugural BP Annual Lecture at Cambridge Judge Business Business on 21 June 2010.\n\nIn November 2009, at the World Economic Forum’s India Economic Summit, CIGB launched Indovations.Net, an interactive, multimedia website that educates users worldwide about innovations in India across five domains: business, social, energy/environment, art/culture, and science & technology. This website addresses a fundamental questions such as, What can the world learn from India-inspired innovation — or Indovation — and how people can benefit from it, etc.\n\nProf Jaideep Prabhu and Navi Radjou write blog posts on innovation, emerging markets, and global leadership on the \"Harvard Business Review\" website.\n\n"}
{"id": "31529005", "url": "https://en.wikipedia.org/wiki?curid=31529005", "title": "Communication quotient", "text": "Communication quotient\n\nCommunication quotient, communication intelligence, or CQ is a theory that communication is a behaviour based skill that can be measured and trained. CQ measures the ability of people to communicate effectively with one another. The first scholarly article referring to CQ was by Robert Service in CQ: the Communication Quotient for IS professionals. The article was published in 2005 in the Journal of Information Science. In 2010 at TED Women, Clare Munn spoke about the importance of our Communication Quotient in an increasingly digital world.\n\nThe development of CQ as a theory and a concept can be traced back to the challenging of IQ as fully explaining cognitive ability in 1983, by Howard Gardner with his Theory of multiple intelligences. In Gardner's view, traditional types of intelligence, such as IQ, fail to fully explain cognitive ability. The development of CQ is part of the trend to analyse and fully understand human intelligence, a trend led by Daniel Goleman’s emotional intelligence and social intelligence.\n\n\"The Times of India\" in 2005 , in an article entitled A Shift from IQ, referred to CQ as an ability multi-national corporations were testing for amongst Indian graduates. In 2005 Craig Harrison in Improving Your Communication Quotient described CQ skills specifically in terms of workplace communication. In 2007 Clare Munn and Maria Bello defined CQ as \"Expressive & Receptive Intelligence\" the communication bridge between IQ and EQ. In 2011 Alistair Gordon and Steve Kimmens in \"The CQ Manifesto\" defined CQ as \"saying the right thing in the right way to the right people at the right time in a such a way that the message is received and understood as it was intended\".\n\nIn Robert Service’s 2005 article he presented the communication quotient as a measurable and improvable type of intelligence, specifically for IT and IS professionals. Service argues that the improvement of communication ability will allow individuals the opportunity to move up in the organisational ranks. The article presents two models to explain communication, the first is the model of two-way communications and the second is the CQ measurement and improvement matrix.\n\n"}
{"id": "56840493", "url": "https://en.wikipedia.org/wiki?curid=56840493", "title": "Competent person", "text": "Competent person\n\nIn the United Kingdom, a competent person is designated by a company to ensure that the company's health and safety responsibilities are being met. This may be a legal obligation required of the company, to ensure that the business understands, and can act on, the health and safety risks that might occur during their particular type of work. \n\nRecognition of competence is defined as requiring \"sufficient training and experience or knowledge and other qualities\" to be able to effectively oversee health and safety in the business, although specialised industries may have additional, more specific requirements.\n\nFor building trades, there are various Competent Person Schemes including:\n"}
{"id": "1706589", "url": "https://en.wikipedia.org/wiki?curid=1706589", "title": "Competitive intelligence", "text": "Competitive intelligence\n\nCompetitive intelligence (CI) is the action of defining, gathering, analyzing, and distributing intelligence about products, customers, competitors, and any aspect of the environment needed to support executives and managers in strategic decision making for an organization.\n\nCI means understanding and learning what is happening in the world outside the business to increase one's competitivity. It means learning as much as possible, as soon as possible, about one's external environment including one's industry in general and relevant competitors.\n\nKey points:\n\nAnother definition of CI regards it as the organizational function responsible for the early identification of risks and opportunities in the market before they become \"obvious\" (\"early signal analysis\"). This definition focuses attention on the difference between dissemination of widely available factual information (such as market statistics, financial reports, newspaper clippings) performed by functions such as libraries and information centers, and competitive intelligence which is a \"perspective\" on developments and events aimed at yielding a competitive edge.\n\nThe term CI is often viewed as synonymous with competitor analysis, but competitive intelligence is more than analyzing competitors; it embraces the entire environment and stakeholders: customers, competitors, distributors, technologies, and macroeconomic data.\n\nCI literature is best exemplified by the bibliographies that were published in the Society of Competitive Intelligence Professionals' academic journal \"The Journal of Competitive Intelligence and Management\". Although elements of organizational intelligence collection have been a part of business for many years, the history of competitive intelligence arguably began in the U.S. in the 1970s, although the literature on the field pre-dates this time by at least several decades. In 1980, Michael Porter published the study \"Competitive-Strategy: Techniques for Analyzing Industries and Competitors\" which is widely viewed as the foundation of modern competitive intelligence. This has since been extended most notably by the pair of Craig Fleisher and Babette Bensoussan, who through several popular books on competitive analysis have added 48 commonly applied competitive intelligence analysis techniques to the practitioner's tool box. In 1985, Leonard Fuld published his best selling book dedicated to competitor intelligence. However, the institutionalization of CI as a formal activity among American corporations can be traced to 1988, when Ben and Tamar Gilad published the first organizational model of a formal corporate CI function, which was then adopted widely by US companies. The first professional certification program (CIP) was created in 1996 with the establishment of The Fuld-Gilad-Herring Academy of Competitive Intelligence in Cambridge, Massachusetts.\n\nIn 1986 the \"Society of Competitive Intelligence Professionals\" (SCIP) was founded in the United States and grew in the late 1990s to around 6,000 members worldwide, mainly in the United States and Canada, but with large numbers especially in the UK and Australia. Due to financial difficulties in 2009, the organization merged with Frost & Sullivan under the Frost & Sullivan Institute. SCIP has since been renamed \"Strategic & Competitive Intelligence Professionals\" to emphasise the strategic nature of the subject, and also to refocus the organisation's general approach, while keeping the existing SCIP brandname and logo. A number of efforts have been made to discuss the field's advances in post-secondary (university) education, covered by several authors including Blenkhorn & Fleisher, Fleisher, Fuld, Prescott, and McGonagle. Although the general view would be that competitive intelligence concepts can be readily found and taught in many business schools around the globe, there are still relatively few dedicated academic programs, majors, or degrees in the field, a concern to academics in the field who would like to see it further researched. These issues were widely discussed by over a dozen knowledgeable individuals in a special edition of the Competitive Intelligence Magazine that was dedicated to this topic. In France, a Specialized Master in Economic Intelligence and Knowledge Management was created in 1995 within the CERAM Business School, now SKEMA Business School, in Paris, with the objective of delivering a full and professional training in Economic Intelligence. A Centre for Global Intelligence and Influence was created in September 2011 in the same School.\n\nOn the other hand, practitioners and companies regard professional accreditation as especially important in this field. In 2011, SCIP recognized the Fuld-Gilad-Herring Academy of Competitive Intelligence's CIP certification process as its global, dual-level (CIP-I and CIP-II) certification program.\n\nGlobal developments have also been uneven in competitive intelligence. Several academic journals, particularly the \"Journal of Competitive Intelligence and Management\" in its third volume, provided coverage of the field's global development. For example, in 1997 the \" (\"School of economic warfare\") was founded in Paris, France. It is the first European institution which teaches the tactics of economic warfare within a globalizing world. In Germany, competitive intelligence was unattended until the early 1990s. The term \"competitive intelligence\" first appeared in German literature in 1997. In 1995 a German SCIP chapter was founded, which is now second in terms of membership in Europe. In summer 2004 the Institute for Competitive Intelligence was founded, which provides a postgraduate certification program for Competitive Intelligence Professionals. Japan is currently the only country that officially maintains an economic intelligence agency (JETRO). It was founded by the Ministry of International Trade and Industry (MITI) in 1958.\n\nAccepting the importance of competitive intelligence, major multinational corporations, such as ExxonMobil, Procter & Gamble, and Johnson and Johnson, have created formal CI units. Importantly, organizations execute competitive intelligence activities not only as a safeguard to protect against market threats and changes, but also as a method for finding new opportunities and trends.\n\nOrganizations use competitive intelligence to compare themselves to other organizations (\"competitive benchmarking\"), to identify risks and opportunities in their markets, and to pressure-test their plans against market response (business wargaming), which enable them to make informed decisions. Most firms today realize the importance of knowing what their competitors are doing and how the industry is changing, and the information gathered allows organizations to understand their strengths and weaknesses.\n\nOne of the major activities involved in corporate competitive intelligence is use of ratio analysis, using key performance indicators (KPI). Organizations compare annual reports of their competitors on certain KPI and ratios, which are intrinsic to their industry. This helps them track their performance, vis-a-vis their competitors.\n\nThe actual importance of these categories of information to an organization depends on the contestability of its markets, the organizational culture, the personality and biases of its top decision makers, and the reporting structure of competitive intelligence within the company.\n\n\"Strategic Intelligence\" (SI) focuses on the longer term, looking at issues affecting a company's competitiveness over the course of a couple of years. The actual time horizon for SI ultimately depends on the industry and how quickly it's changing. The general questions that SI answers are, ‘Where should we as a company be in X years?' and 'What are the strategic risks and opportunities facing us?' This type of intelligence work involves among others the identification of weak signals and application of methodology and process called Strategic Early Warning (SEW), first introduced by Gilad, followed by Steven Shaker and Victor Richardson, Alessandro Comai and Joaquin Tena, and others. According to Gilad, 20% of the work of competitive intelligence practitioners should be dedicated to strategic early identification of weak signals within a SEW framework.\n\n\"Tactical Intelligence\": the focus is on providing information designed to improve shorter-term decisions, most often related with the intent of growing market share or revenues. Generally, it is the type of information that you would need to support the sales process in an organization. It investigates various aspects of a product/product line marketing:\n\n\nWith the right amount of information, organizations can avoid unpleasant surprises by anticipating competitors' moves and decreasing response time. Examples of competitive intelligence research is evident in daily newspapers, such as the \"Wall Street Journal\", \"Business Week\", and \"Fortune\". Major airlines change hundreds of fares daily in response to competitors' tactics. They use information to plan their own marketing, pricing, and production strategies.\n\nResources, such as the Internet, have made gathering information on competitors easy. With a click of a button, analysts can discover future trends and market requirements. However competitive intelligence is much more than this, as the ultimate aim is to lead to competitive advantage. As the Internet is mostly public domain material, information gathered is less likely to result in insights that will be unique to the company. In fact there is a risk that information gathered from the Internet will be misinformation and mislead users, so competitive intelligence researchers are often wary of using such information.\n\nAs a result, although the Internet is viewed as a key source, most CI professionals should spend their time and budget gathering intelligence using primary research—networking with industry experts, from trade shows and conferences, from their own customers and suppliers, and so on. Where the Internet is used, it is to gather sources for primary research as well as information on what the company says about itself and its online presence (in the form of links to other companies, its strategy regarding search engines and online advertising, mentions in discussion forums and on blogs, etc.). Also, important are online subscription databases and news aggregation sources which have simplified the secondary source collection process. Social media sources are also becoming important—providing potential interviewee names, as well as opinions and attitudes, and sometimes breaking news (e.g., via Twitter).\n\nOrganizations must be careful not to spend too much time and effort on old competitors without realizing the existence of any new competitors. Knowing more about your competitors will allow your business to grow and succeed. The practice of competitive intelligence is growing every year, and most companies and business students now realize the importance of knowing their competitors.\n\nAccording to Arjan Singh and Andrew Beurschgens in their 2006 article in the \"Competitive Intelligence Review\", there are four stages of development of a competitive intelligence capability with a firm. It starts with \"stick fetching\", where a CI department is very reactive, up to \"world class\", where it is completely integrated in the decision-making process.\n\nThe technical advances in massive parallel processing offered by the Hadoop \"big data\" architecture has allowed the creation of multiple platforms for named-entity recognition such as the Apache Projects OpenNLP and Apache Stanbol. The former includes pre-trained statistical parsers that can discern elements key to establishing trends and evaluating competitive position and responding appropriately. Public information mining from \"SEC.gov\", Federal Contract Awards, social media (Twitter, Reddit, Facebook, and others), vendors, and competitor websites now permit real-time counterintelligence as a strategy for horizontal and vertical market expansion and product positioning. This occurs in an automated fashion on massive marketplaces such as Amazon.com and their classification and prediction of product associations and purchase probability.\n\nCompetitive intelligence has been influenced by national strategic intelligence. Although national intelligence was researched 50 years ago, competitive intelligence was introduced during the 1990s. Competitive-intelligence professionals can learn from national-intelligence experts, especially in the analysis of complex situations. Competitive intelligence may be confused with (or seen to overlap) environmental scanning, business intelligence and market research. Craig Fleisher questions the appropriateness of the term, comparing it to business intelligence, competitor intelligence, knowledge management, market intelligence, marketing research and strategic intelligence.\n\nFleisher suggests that business intelligence has two forms. Its narrow (contemporary) form is more focused on information technology and internal focus than CI, while its broader (historical) definition is more inclusive than CI. Knowledge management (KM), when improperly achieved, is seen as an information-technology driven organizational practice relying on data mining, corporate intranets and mapping organizational assets to make it accessible to organization members for decision-making. CI shares some aspects of KM; they are human-intelligence- and experience-based for a more-sophisticated qualitative analysis. km is essential for effective change. A key effective factor is a powerful, dedicated IT system executing the full intelligence cycle.\n\nMarket intelligence (MI) is industry-targeted intelligence developed in real-time aspects of competitive events taking place among the four Ps of the marketing mix (pricing, place, promotion and product) in the product (or service) marketplace to better understand the market's attractiveness. A time-based competitive tactic, MI is used by marketing and sales managers to respond to consumers more quickly in the marketplace. Fleisher suggests it is not distributed as widely as some forms of CI, which are also distributed to non-marketing decision-makers. Market intelligence has a shorter time horizon than other intelligence areas, and is measured in days, weeks, or (in slower-moving industries) months.\n\nMarket research is a tactical, method-driven field consisting of neutral, primary research of customer data (beliefs and perceptions) gathered in surveys or focus groups, and is analyzed with statistical-research techniques. CI draws on a wider variety (primary and secondary) of sources from a wider range of stakeholders (suppliers, competitors, distributors, substitutes and media) to answer existing questions, raise new ones and guide action.\n\nBen Gilad and Jan Herring lay down a set of prerequisites defining CI, distinguishing it from other information-rich disciplines such as market research or business development. They show that a common body of knowledge and a unique set of tools (key intelligence topics, business war games and blindspots analysis) distinguish CI; while other sensory activities in a commercial firm focus on \"one\" segment of the market (customers, suppliers or acquisition targets), CI synthesizes data from all high-impact players (HIP).\n\nGilad later focused his delineation of CI on the difference between information and intelligence. According to him, the common denominator among organizational sensory functions (whether they are called market research, business intelligence or market intelligence) is that they deliver information rather than intelligence. Intelligence, says Gilad, is a perspective on facts rather than the facts themselves. Unique among corporate functions, competitive intelligence has a perspective of risks and opportunities for a firm's performance; as such, it (not information activities) is part of an organization's risk-management activity.\n\nEthics has been a long-held issue of discussion among CI practitioners. The questions revolve around what is and is not allowable in terms of CI activity. A number of scholarly treatments have been generated on this topic, most prominently addressed through Society of Competitive Intelligence Professionals publications. The book \"Competitive Intelligence Ethics: Navigating the Gray Zone\" provides nearly twenty separate views about ethics in CI, as well as another 10 codes used by various individuals or organizations. Combining that with the over two dozen scholarly articles or studies found within the various CI bibliographic entries, it is clear that no shortage of study has gone into better classifying, understanding and addressing CI ethics.\n\nCompetitive information may be obtained from public or subscription sources, from networking with competitor staff or customers, disassembly of competitor products or from field research interviews. Competitive intelligence research is distinguishable from industrial espionage, as CI practitioners generally abide by local legal guidelines and ethical business norms.\nOutsourcing has become a big business for competitive intelligence professionals. There a many different companies in this field, including market research and consulting firms.\n\n\n"}
{"id": "33454478", "url": "https://en.wikipedia.org/wiki?curid=33454478", "title": "Consignment agreement", "text": "Consignment agreement\n\nA consignment agreement is an agreement between a consignee and consignor for the storage, transfer, sale or resale and use of the commodity. The consignee may take goods from the consignment stock for use or resale subject to payment to the consignor agreeably to the terms bargained in the consignment agreement. The unsold goods will normally be returned by the consignee to the consignor.\n\nIt may be accompanied by a consignment agreement (Franchising, distributorship or OEM).\nGoods are stored at the premises of the distributor, or premises of a third party, at distributor's disposal, but remain the property of the exporter.\n\nThis agreement decreases the exporter's risk because he remains the owner of the commodities in storage. The distributor does not need to pay until he has sold the commodities, so he improves his cash flow. \nBoth parties must ensure that the consignment agreement is formulated very carefully, so no room for doubt remains with regard to third parties, specifically the distributor's creditors in case of his bankruptcy. \nThe distributor and the exporter have incompatible interests. The distributor's interest will be to increase the amount of the stock in consignment because this has no effect on his cash situation. Therefore the parties should agree on reasonable stock rolling adapted to the market demand, and consider how quickly the exporter may produce and deliver additional goods in order to avoid stock disruption.\n\nFulfillment of certain conditions is requested by the customs and VAT authorities. Due to the European VAT rules, it is easier to have a consignment stock between EU countries. The distributor is required to keep accurate accounts, but dispensable to have a bonded warehouse.\n\n\n"}
{"id": "9810095", "url": "https://en.wikipedia.org/wiki?curid=9810095", "title": "Convention of conservatism", "text": "Convention of conservatism\n\nIn accounting, the convention of conservatism, also known as the doctrine of prudence, is a policy of anticipating possible future losses but not future gains. This policy tends to understate rather than overstate net assets and net income, and therefore lead companies to . When given a choice between several outcomes where the probabilities of occurrence are equally likely, you should recognize that transaction resulting in the lower amount of profit, or at least the deferral of a profit. .\nIn accounting, it states that when choosing between two solutions, the one that will be least likely to overstate assets and income should be selected. Essentially, \"expected losses are losses but expected gains are not gains\".\n\nThe conservatism principle is the foundation for the lower of cost or market rule, which states that you should record inventory at the lower of either its acquisition cost or its current market value.\n\nConservatism plays an important role in a number of accounting rules, including the allowance for doubtful debts and the lower of cost or market rule.\n\n\n"}
{"id": "25335381", "url": "https://en.wikipedia.org/wiki?curid=25335381", "title": "Cottage and small scale industries in Pakistan", "text": "Cottage and small scale industries in Pakistan\n\nIn Pakistan, cottage or household industries hold an important position in rural set-up. Most villages are self-sufficient in the basic necessities of life. They have their own carpenters, cobblers, potters, craftsmen and cotton weavers. Many families depend on cottage industries for income.\n\nCottage industries have also gained immense importance in cities and towns. There is a great demand for hand-woven [carpet]s, embroidered work, brassware, rugs and traditional bangles. These are also considered important export items and are in good demand in international markets.\n\nThere are meagre resources to develop large-scale industries. However, a program for developing and promoting small-scale industries both in rural and urban areas is more feasible figure 1 shows the advantages of establishing such industries.\n\n\nThere is a web of cottage and books industries. In almost every village, there are a number of such industries depending upon the size of the village and the demand for the products. The establishment of such industry is closely related to the availability of raw material traditional skills, climatic conditions and, in several cases, the local specialization in the organized factory sector.\n\nThere is a large variety of handicrafts available in Pakistan. They are not only aesthetically pleasing items, but they also serve the needs of local people.\n\nSome of these industries produce important export items. Recently exports of non-cotton products have faced increasing trade barriers as public opinion in industrialized countries has expressed growing concern about child labor, environmental and health standards. These concerns are being addressed now.\n\nIn the small scale industries, the most important is the Carpet weaving and its center are located almost all over the Pakistan. It is also significant in economic terms and they make valuable contribution in exports. Cotton is the raw material required for this industry. They also employ women for the production of fine hand woven carpets and for the production of wool silk or a mixture of the two, as the carpets are of great significance which generates equal economic opportunities.It is valuable for gross domestic product of country.\n\nTextiles are found throughout the country with a variety of design and techniques. The most famous among them are Khadar, Susi, Khes, Chunri, Boski, Karandi, Shaal, and Ajrak. The designs are invariably brightly colored with traditional emphasis on blue and red.\n\nEmbroidery has developed to a fine art with distinctive regional designs and patterns.\n\nGold and silversmiths are one of the largest communities of craftsmen. Much of the jewelry made and sold in the cities is intricately fashioned and delicate.\n\nClay and terracotta pottery and utensils continue to be of great practical importance. Many of the designs of urns, pitchers, bowls, jugs, plates, and pots seen today are almost identical to those un covered at archaeological sites around the country. Distinctive glazed blue tiles are used to decorate many of the great mosques in Pakistan.\n\nWazirabad is the city of cutlery industry in Pakistan . This industry is growing day by day and has share of 65 million US dollars in Export for 2010. High Quality Damascus Steel ) is manufactured in this city and 95% of world needs are produced here.\n\nThe Swat Valley is perhaps the most famous for its intricately carved architectural woodwork and furniture, although wood-carving is common throughout the northern mountains.\n\nSports goods earn about 3.7% of our total exports. The main raw material for the sports goods industry are leather and mulberry wood that are available in Punjab, but also imported PVC. Football, hockey ball, hockey sticks, cricket bats and rackets are mostly manufactured by hand. The skilled workers are available in Sialkot and Lahore. In the industry large and medium size factories contract work out to small-scale and cottage concerns.The local sports goods manufacturing industry is one of the major source of foreign exchange earnings of Pakistan. It is centralised in and around the city of Sialkot, where it has flourished as a cottage industry with most of its production by generations of skilled craftsmen. At the time of independence, this industry was in an infant stage with a nominal export of Rs. 0.82 million. The Government took immediate steps to develop this industry by providing loans and subsidies to the manufacturers and arrangements were made to market the manufactured goods. Since then, the industry has flourished locally and enjoys good reputation in the international markets as well.mostly these goods are provided to fatima syed productions\n\nProduction\nAt present, there are more than 2000 units, mostly on small scale in operation with an installed capacity of Rs. 20 billion per annum. The units are operating on single-shift basis.\n\nPakistan produces a wide range of sports goods, accessories, games and athletic equipment generally following the British, American and German specifications.\n\nThe Government is also enforcing on a compulsory basis, minimum quality standards for sports goods manufacture. The Pakistan Standards Institute, a government agency, has devised specific standards for different types of sports goods. The important items being produced are tennis rackets, hockey sticks, hokey balls, polo sticks, cricket bats and balls, footballs, (complete) and numerous goods used in both in-door as well out-door games.\n\nAt present, Pakistan's sports goods enjoy a world-wide recognition mainly because of the care that goes into their designing, manufacturing and selecting of the finest raw materials. The basic raw materials required for the production of sports goods, are leather, wood, glue, nylon guts, rubber and chemicals. Out of these, leather and various kinds of wood are abundantly available in Pakistan. The industry annually utilises materials worth Rs. 8 billion including imported raw material.\n\nExports\nThis industry is one of the major foreign exchange earners for Pakistan and is, therefore, receiving full government backing in its development. It is estimated that more than 75 per cent of the total production is exported every year.\n\nIn fact, the export demand has acted as the main stimulus for the rapid growth of this industry because of care that goes into designing, manufacturing and selecting of raw materials. There are two factors which are responsible of this.\n\n(i) Low price as compared to general price level\n\n(ii) Durability plus good workmanship\n\nExport of sports goods increased from $136 million in 1990-91 to $384 million in 1997-98. Showing an average increase of 23 per cent annum as evident from table-1. The export market for sports goods is fairly diversified. More and more countries are being added to the list of their imports. In 1990-91 there were in all 50 countries importing these good from Pakistan. Thereafter, the list has continuously expanded so that during the 1992-98 period, Pakistan exported sports goods to 90 countries. However, the principal importing countries are Germany, USA, UK, France and Italy. Others were Spain, Netherlands, Hong Kong, Denmark, Canada, Belgium, Dubai and Chile. Country-wise export of sports goods is given in table-2.\n\nIn the international market, India, Japan, Taiwan and South Korea are the main competitors of Pakistan. They are supplying their products at lower prices. While India has an advantage of cheap labour and raw material Taiwan, Japan, and South Korea have semi-automotive and mechanised units and are always engaged in introducing cheap sports goods such as metal rackets and cricket bats etc.\n\nIn order to encourage the export of sports goods, the Government has taken many positive steps and has offered various incentives. Customs duty, sales tax and excise duty rebates on f.o.b. value of exported various types of sports goods are available.\n\nAnother incentive is that import of restricted and tanned raw materials are also allowed on cash licenses against export of sports goods.\n\nThis industry is facing severe competition from Taiwan, India and South Korea. Although the Government has provided various incentives and facilities to modernise and mechanise the industry, the opportunity has not been availed. The improvement in quality and consequently in exports earnings has been due to the improved availability for leather for manufacture of footballs which constitutes about 75 per cent of the total exports. Keeping in view the trends during 1991-98 about 23 per cent growth rate, improved quality available and competition faced in the international markets the future demand is expected to growth the rate of 15 per cent during 1999-2000.\n\nSialkot and Lahore are also noted for the manufacture and export of surgical instruments. The most important raw material is stainless steel which has to be imported. In this industry, also, medium scale factories contract work out to small-scale and cottage concerns. Also like the sports goods industry, most of the output is exported.Sialkot is the biggest surgical maker in the world and India, America, Australia and many other countries are importers of surgical instruments from this city. Sialkot has the major role in making surgical instruments\n\nOther small-scale industries include electric fans, cutlery and general engineering.\n\n″small scale industries are those which generally employed less than workers and they run with or without electric powers, in or outside the home but there assets do not exceed rs.2 million.for example; carpet industry, poultry forming, hand and power loom industry, manufacturing of sports and leather goods, toy industry, agriculture implements etc.″\n\nCottage and small-scale industries contribution to the GDP is only 5%. There are certain problems associated with these industries that are mentioned below.\n\n\nThe government is fully aware of the potential of cottage and small-scale industries for industrial development. The following organizations have been established to develop this sector of economy.\n\n\nThe above organizations are taking the following measures:\n\n"}
{"id": "1152274", "url": "https://en.wikipedia.org/wiki?curid=1152274", "title": "Cultural industry", "text": "Cultural industry\n\nAccording to international organizations such as UNESCO and the General Agreement on Tariffs and Trade (GATT), cultural industries (sometimes also known as \"creative industries\") combine the creation, production, and distribution of goods and services that are cultural in nature and usually protected by intellectual property rights.\n\nThe notion of cultural industries generally includes textual, music, television, and film production and publishing, as well as crafts and design. For some countries, architecture, the visual and performing arts, sport, advertising, and cultural tourism may be included as adding value to the content and generating values for individuals and societies. They are knowledge-based and labour-intensive, creating employment and wealth. By nurturing creativity and fostering innovation societies will maintain cultural diversity and enhance economic performance.\n\nCultural industries worldwide have adapted To The New digital technologies and to the arrival of national, regional and international (de)regulatory policies. These factors have radically altered the context in which cultural goods, services, and investments flow between countries and, consequently, these industries have undergone a process of internationalization and progressive concentration, resulting in the formation of a few big conglomerates: a new global oligopoly.\n\n\n1. \"Exploring The Cultural and Creative Industries Debate\". Culture Action Europe. Retrieved 2013-07-07.\n[nonexistent/incorrect reference]\n"}
{"id": "21687181", "url": "https://en.wikipedia.org/wiki?curid=21687181", "title": "Customer involvement management", "text": "Customer involvement management\n\nCustomer involvement management, \"CIM\", is a marketing management method that takes customer orientation further than customer relationship management. CIM identifies and develops ways to involve customers in the business and product development process, such as design, marketing, sales, customer service, etc. The degree of involvement can be as far as to make the customer a part of the product, experience, and delivery.\n\nWithin CIM, the product is considered a subset in what meets the customer's need of identification, problem solving, and consumption. The possibility to influence the design and the consumption itself is assumed to be of great importance for the customers buying decision and loyalty.\n\nOne example of a company that uses CIM is Nike. With the concept NikeiD they let the customers design their own sport shoes.\n\nMore recently companies have started to build Web portals that involve customers in the idea generation, selection, development, and commercialization.\n"}
{"id": "47449116", "url": "https://en.wikipedia.org/wiki?curid=47449116", "title": "Customer success", "text": "Customer success\n\nCustomer success is the function at a company responsible for managing the relationship between a vendor and its customers. The goal of customer success is to make the customer as successful as possible, which in turn, improves customer lifetime value (CLTV) for the company. \n\nCustomer success is an emerging role in business. It is known as CS. The function is most commonly used in the software world and most prevalent among software as a service (SaaS) companies. Because customer success is a nascent field of business, its organizational alignment and activities are still evolving.\nThere is still a large amount of variance with respect to its scope of responsibilities, reporting structure, terminology used for describing its activities, metrics used for measuring its performance and more. While deviations exist in the specifics of the function, it always refers to the customer relationship management after the initial sale.\n\nAs customer success is a nascent and fast emerging field, variance is still high with respect to its scope, reporting structure, terminology used for describing its activities, metrics used for measuring its performance and more.\nHowever, the key functions the CS team is often responsible for include:\n\nPresently, the customer success function within most organizations is embodied in the customer success manager (CSM) job title.\n\nThe CSM acts as the main point of contact and as a trusted advisor for the customer from the vendor side as they are the one ultimately responsible and accountable for that customer's success. The function may share many of the same functions of traditional account managers, relationship managers, project managers, and technical account managers, but their mode of operations tend to be much more focused on long-term value-generation to the customer. At its heart, it is about maximizing the value the customer generates from utilizing the solutions of the vendor, while enabling the vendor the ability to derive high return from the customer value. To enable that, the CSM must monitor the customer's usage of and satisfaction from the solutions of the vendor, identify opportunities and challenges from the way the customer engages with the solution and take action to help resolve challenges and foster expansion of the usage as well as the value from the solutions (to both sides) over time.\n\nAs a consequence, relentlessly monitor and manage the customer health is a key success factor for every CSM as well as the need to deeply understand the drivers of value the customer gains from the solutions provided by the vendor. Without such deep and timely understanding of these two aspects of the customer, the CSM will not be able to act effectively.\n\nIn young organizations, where the total number of employees (and customers) is small, the CSM may be the first employee of the customer success team. As such, they will be responsible for most of the functions described above, which over time may be fulfilled by more specialized team members. Ownership of commercial responsibilities by CSM vary among companies. While some believe a CSM's neutrality from sales or commercial conversations may make a customer more likely to respond to and engage with a CSM, other view the ownership of the commercial relations natural to a long-term relations between a vendor and a customer and more empowering to the CSM.\n\nFor CSMs to fulfill the responsibilities of their role, they must be empowered by an organization's executive team to navigate freely among all parts of an organization. This maintains the CSMs credibility with the customer as an effective resource. In organizations where CSMs are just another level of abstraction or a \"screen\" between the customer and the resources they need, the credibility of the CSM is compromised and the customer experience eroded which may result in a customer not renewing or expanding their business with the vendor. Furthermore, lacking the top-down support, will deprive the CSM the ability to garner the right resources needed by them to complete their jobs.\nVirtual customer success managers are remote points of contact for customers and monitor the success of customers, providing important feedback.\n\nEvery company that sells its products and/or services to customers has functions responsible for managing the customer fulfillment and relations. In traditional businesses, those functions are referred to most commonly as \"Fulfillment\", \"Post Sales\" or \"Professional Services\".\n\nIn the world of technology, companies have been developing, selling and enabling software solutions for many years. At most of those companies the function responsible for managing the relations with customers used to be called \"Account Management\", \"Operations\" or \"Services\".\n\nAs the business world is changing and evolving into new fields, the method by which software is enabled to customers changes. One of the most significant changes in recent years has been the emergence of software as a service (aka SaaS).\n\nSaaS is a method of enabling a software solution to customers in a subscription model that departs from the \"old\" model of granting a perpetual license that enables the customer to own the solution and therefore use it as they see fit (but also be responsible for its operation). Rather, when enabling a solution to customers as a SaaS, companies offer their products as services instead of physical objects, moving the economy to a subscription model. The customer \"rents\" the solution and is able to use it only for the period they have rented it for. The vendor enabling the solution provides not only the solution itself, but also the infrastructure that supports it.\n\nThe key implication of this new model is a fundamental shift in the engagement model between the software vendor and its customers. If in the traditional \"enterprise software\" model, a customer buys the license to the software and pays the vendor then, regardless of its actual usage, in a SaaS model, the customer pays a (much smaller) rent for the software every month. The software vendor must therefore ensure the customer is using the solution and seeing value from it if they wanted to ensure the customer continues to pay their rent. This fundamental shift in the software industry's operating model revealed a need for a function at the company to own and ensure that success of its customers.\n\nThe emergence of this function is what is now being referred to as customer success (CS).\n\nWhile the trend towards SaaS has been going on since the beginning of the 21st century, the understanding of the need for much stronger focus on customer success and therefore the creation of the field of customer success only began around 2010–2012. On LinkedIn, the world's largest repository of professional bios, only a very small number of people had titles that included \"customer success\" in 2010 or 2012. By 2015, this number was huge and growing rapidly.\n\nThe reason is that the impact of customer success on the performance and value of a SaaS company is huge. The CS function is responsible for retaining and growing the business that the sales team has secured. Case studies show that companies with strong CS teams outperform peers with weak or no CS teams in a multitude of financial criteria including customer retention (also measured by \"churn\", which is the opposite of retention), revenue growth rates, gross margin, customer satisfaction, and referrals. In fact, customer experience is the greatest untapped source of both decreased costs and increased revenue in most industries, but only if companies take the time to understand what underpins it and how they can benefit financially from improving it.\n"}
{"id": "43574082", "url": "https://en.wikipedia.org/wiki?curid=43574082", "title": "Daigou", "text": "Daigou\n\nDaigou (Chinese: 代购 dàigòu (); also 海外代购 hǎiwài dàigòu), Overseas personal shopper\nis a channel of commerce in which a person outside of China purchases commodities (mainly luxury goods but also groceries) for a customer in mainland China, \nsince prices for luxury goods can be 30 to 40 percent higher in China than abroad. \nThe phrase means \"buying on behalf of\". \n\"Daigou\" sales across sectors total $15 billion annually. \nIn 2014 the value of the \"daigou\" business just in luxury goods increased from CN¥55 billion to CN¥75 billion yuan (USD $8.8 billion to $12 billion).\n\n\"Daigou\" purchases are often from luxury brand boutiques in major fashion cities like Paris, London, New York City, Hong Kong, Tokyo and Seoul. Some \"daigou\" operators use Weibo and WeChat to communicate with their clients. \nThe large demand for \"daigou\" service is due to concern over unsafe products, especially food safety problems, \nand China's high import tariffs on luxury goods. \nSome \"daigou\" service providers intentionally sell counterfeit made-in-China products that have been altered to appear purchased abroad. \nA 2015 survey of Chinese online luxury shoppers found that 35% have used \"daigou\" to purchase luxury goods online, while only 7% used the website of the brand they are buying, or think they are buying.\nApproximately 80% of Chinese luxury purchases are made abroad.\nAsian-American sales associates at Macy's Herald Square sued Macy's for racial discrimination in September 2017, alleging that store managers instructed sales associates not to sell more than one unit to any single Asian customer, and that they were fired when they spoke up about the alleged discrimination.\n\nThe daigou business is thriving in Australia and the government has also shown its support to recognize the job-creating potential of this grey industry.\n\n\n"}
{"id": "45087591", "url": "https://en.wikipedia.org/wiki?curid=45087591", "title": "EOdisha Summit", "text": "EOdisha Summit\n\nThe eOdisha Summit is a Conference, Exhibition and Awards Summit held every year with themes of eGovernance, Information Technology and Healthcare. The eOdisha Summits of 2013 and 2014 aimed at active knowledge sharing, showcasing of existing governance, education, healthcare, and information technology (IT) initiatives in Odisha.\n\nPowered by: eGov <br>\nOrganisers: eLets Technomedia, Centre for Science, Development and Media Studies <br>\nHosting Partners: Government of Odisha, Odisha Computer Application Centre\n\n"}
{"id": "5586148", "url": "https://en.wikipedia.org/wiki?curid=5586148", "title": "Equity of redemption", "text": "Equity of redemption\n\nThe equity of redemption refers to the right of a mortgagor in law to redeem his or her property once the debt secured by the mortgage has been discharged.\n\nHistorically, a mortgagor (the borrower) and a mortgagee (the lender) executed a conveyance of legal title to the property in favour of the mortgagee as security for the loan. If the loan was repaid, then the mortgagee would return the property; if the loan was not repaid, then the mortgagee would keep the property in satisfaction of the debt. The equity of redemption was the right to petition the courts of equity to compel the mortgagee to transfer the property back to the mortgagor once the secured obligation had been performed. Today, most mortgages are granted by statutory charge rather than by a formal conveyance, although theoretically there is usually nothing to stop two parties from executing a mortgage in the more traditional manner.\n\nTraditionally, the courts have been astute to ensure that the mortgagee did not introduce any artificial stipulations into the contractual arrangements to impede a mortgagor's ability to satisfy obligations and reclaim the property. Such impediments are \"clogs\" on the equity of redemption, and the courts of equity were particularly astute in striking down any provision which was, or in later cases, which may be, a clog.\n\nWhere collateral is pledged in prime brokerage transactions, it is common for the broker to rehypothecate the collateral. Concerns remained, however, that because the rehypothecation might theoretically mean that the lender could lose title to the collateral, and thereby possibly be unable to reconvey it to the primary customer, it was speculated that such rehypothecation is possibly unlawful.\n\nThe tide has for some years now turned against striking down every clause in a mortgage document that might conceivably impede the right to redeem.\n\nThe equity of redemption is itself recognised as a separate species of property, and can be bought, sold or even itself mortgaged by the holder.\n\nHistorically the equity of redemption would naturally expire upon the mortgagor breaching the terms of repayment. However, in modern times, extinguishing the equity of redemption (and leaving the mortgagee with absolute title to the property) ordinarily requires a court order in most jurisdictions. For both legal and practical reasons, the use of foreclosure as a remedy has fallen into disuse. Even where a mortgagee seeks an order for foreclosure from the courts, the courts will frequently order judicial sale of the property instead.\n\n"}
{"id": "43697811", "url": "https://en.wikipedia.org/wiki?curid=43697811", "title": "Hungarian Industrial and Commercial Bank", "text": "Hungarian Industrial and Commercial Bank\n\nThe Hungarian Industrial and Commercial Bank (Hungarian: Magyar Ipar-és Kereskedelmi Bank) was an important Hungarian bank in the late 19th and early 20th century.\n\nIt was a mediocre bank until the appointment of István Tisza - a prominent businessman and politician - as its president. Under his direction, it became the largest bank of Hungary within a decade.\n"}
{"id": "369692", "url": "https://en.wikipedia.org/wiki?curid=369692", "title": "Index of international trade topics", "text": "Index of international trade topics\n\nThis is a list of international trade topics.\n\n\n"}
{"id": "29683255", "url": "https://en.wikipedia.org/wiki?curid=29683255", "title": "LSE Alternative Investment Conference", "text": "LSE Alternative Investment Conference\n\nThe LSE SU Alternative Investments Conference (also known as the LSE AIC) is an international conference on hedge funds, private equity and venture capital held annually in London, United Kingdom by the Alternative Investments Society (AIS), a Student’s Union society at the London School of Economics and Political Science (LSE).\n\nThe Alternative Investments Society was founded in 2006 by a group of undergraduate students at the LSE, and continues to be run by students from the university. The first Alternative Investments Conference was a one-day event hosted on the LSE campus in 2007, and was attended by 200 students who heard from several notable investors. The AIC has evolved considerably over its twelve-year history, having changed venues and ultimately becoming the world’s largest student conference on hedge funds, private equity and venture capital.\n\nThe Conference takes place over two days at the five-star London Marriott Hotel Grosvenor Square, bringing together over 40 senior level industry leaders (see notable past speakers) and 320 student delegates from many of the world’s leading educational institutions. Delegates come from a range of backgrounds, including both undergraduates and postgraduates studying finance, law, engineering, history as well as many other disciplines. Admission to the Conference is competitive, with only 4.3% of applicants to AIC 2017 being selected to attend. The Conference is international in character, with applicants coming from over 200 universities located across more than 120 countries world wide.\n\nThe format of the Conference sees delegates attend keynote speeches, panel discussions and workshop sessions designed to allow for them to learn from and interact with experienced professionals from within the investment community. Networking events are also held during the Conference by the AIC’s partner firms, as well as a dinner for all attending delegates hosted during the evening of the first day of the event. Sponsor firms of the AIC include Point72 Asset Management, Dartmouth Partners, Canada Pension Plan Investment Board, Terra Firma Capital Partners, GAM, Amicus, Bain & Company, Bain Capital, 3i, Patron Capital, Lansdowne Partners, Stable Asset Management, and Dechert.\n\nThe Conference is well regarded on campus, as well as outside of the LSE, having been cited in the media a number of times and described by the Financial Times as 'a chance for the best brains in asset management to meet'.\n\nSeveral keynote sessions from past conferences are also available to watch online.\n\nSeveral notable past speakers at the AIC include:\n\nThe Alternative Investments Conference 2019 will be held at the London Marriott Hotel Grosvenor Square, on January 21 & 22 2019. This will be the thirteenth annual edition of the Conference, and will mark over 3,300 students having attended the AIC since its inception.\n\n"}
{"id": "6731231", "url": "https://en.wikipedia.org/wiki?curid=6731231", "title": "Lex mercatoria", "text": "Lex mercatoria\n\nLex mercatoria (from the Latin for \"merchant law\"), often referred to as \"the Law Merchant\" in English, is the body of commercial law used by merchants throughout Europe during the medieval period. It evolved similar to English common law as a system of custom and best practice, which was enforced through a system of merchant courts along the main trade routes. It functioned as the international law of commerce. It emphasised contractual freedom and alienability of property, while shunning legal technicalities and deciding cases \"ex aequo et bono\". A distinct feature was the reliance by merchants on a legal system developed and administered by them. States or local authorities seldom interfered, and did not interfere a lot in internal domestic trade. Under \"lex mercatoria\" trade flourished and states took in large amounts of taxation.\n\nIn the last years new theories had changed the understanding of this medieval treatise considering it as proposal for legal reform or a document used for instructional purposes. These theories consider that the treatise cannot be described as a body of laws applicable in its time, but the desire of a legal scholar to improve and facilitate the litigation between merchants. The text is composed by 21 sections and an annex. The sections described procedural matters such as the presence of witnesses and the relation between this body of law and common law. It has been considered as a false statement to define this as a system exclusively based in custom, when there are structures and elements from the existent legal system, such as Ordinances and even concepts proper of the Romano-canonical procedure.\n\nThe \"lex mercatoria\" was originally a body of rules and principles laid down by merchants to regulate their dealings. It consisted of rules and customs common to merchants and traders in Europe, with some local variation. It originated from the need for quick and effective jurisdiction, administered by specialised courts. The guiding spirit of the merchant law was that it ought to derive from commercial practice, respond to the needs of the merchants, and be comprehensible and acceptable to the merchants who submitted to it. International commercial law today owes some of its fundamental principles to the \"lex mercatoria\". This includes choice of arbitration institutions, procedures, applicable law and arbitrators, and the goal to reflect customs, usage and good practice among the parties.\n\nGoods and services flowed freely during the medieval merchant law, thus generating more wealth for all involved. It is debated whether the law was uniform in nature, was spontaneous as a method of dispute resolution, or applied equally to everyone who subordinated to it. The \"lex mercatoria\" was also a means for local communities to protect their own markets. Local kings or lords extracted taxes and set trade restrictions. In 1303 Edward I issued the Carta Mercatoria, a charter to foreign merchants in England, which guaranteed them freedom to trade, with certain protections and exemption under the law. Although the charter was revoked by Edward II, due to complaints by English merchants, foreign merchants retained most of their rights in practice, but these would vary widely with the march of time, events and changes to state policy.\n\nThe \"lex mercatoria\" was the product of customs and practices among traders, and could be enforced through the local courts. However, the merchants needed to solve their disputes rapidly, sometimes on the hour, with the least costs and by the most efficient means. Public courts did not provide this. A trial before the courts would delay their business, and that meant losing money. The \"lex mercatoria\" provided quick and effective justice. This was possible through informal proceedings, with liberal procedural rules. The \"lex mercatoria\" rendered proportionate judgements over the merchants’ disputes, in light of \"fair price\", good commerce, and equity.\n\nJudges were chosen according to their commercial background and practical knowledge. Their reputation rested upon their perceived expertise in merchant trade and their fair-mindedness. Gradually, a professional judiciary developed through the merchant judges. Their skills and reputation would however still rely upon practical knowledge of merchant practice. These characteristics serve as important measures in the appointment of international commercial arbitrators today.\n\nThe \"lex mercatoria\" owed its origin to the fact that the civil law was not sufficiently responsive to the growing demands of commerce, as well as to the fact that trade in pre-medieval times was practically in the hands of those who might be termed cosmopolitan merchants, who wanted a prompt and effective jurisdiction. It was administered for the most part in special courts, such as those of the guilds in Italy, or the fair courts of Germany and France, or as in England, in courts of the Staple or Piepowder.\n\nThe \"lex mercatoria\" was composed of such usages and customs as were common to merchants and traders in all parts of Europe, varied slightly in different localities by special peculiarities. Less procedural formality meant speedier dispensation of justice, particularly when it came to documentation and proof. Out of practical need, the medieval \"lex mercatoria\" originated the “writing obligatory”. By this, creditors could freely transfer the debts owed to them. The “writing obligatory” displaced the need for more complex forms of proof, as it was valid as a proof of debt, without further proof of; transfer of the debt; powers of attorney; or a formal bargain for sale. The \"lex mercatoria\" also strengthened the concept of party autonomy: whatever the rules of the \"lex mercatoria\" were, the parties were always free to choose whether to take a case to court, what evidence to submit and which law to apply.\n\nMerchant law declined as a cosmopolitan and international system of merchant justice towards the end of medieval times. This was to a large extent due to the adoption of national commercial law codes. It was also connected with an increasing modification of local customs to protect the interests of local merchants. The result of the replacement of \"lex mercatoria\" codes with national governed codes was the loss of autonomy of merchant tribunals to state courts. The main reason for this development was the protection of state interests.\n\nThe nationalisation of the \"lex mercatoria\" did not neglect the practises of merchants or their trans-border trade. Some institutions continued to function, and state judges also were appointed for their merchant expertise, just as modern commercial arbitrators. The laws of the merchants were not eradicated, but simply codified. National codes built on the principles laid down by trade commercial practise and to a large extent they embodied \"lex mercatoria\" substantial rules. This was for example the case in France. The Code Commercial was issued in 1807, where \"lex mercatoria\" rules were preserved to govern formation, performance and termination of contracts. In effect, the nation states reconstituted the \"lex mercatoria\" in their image.\n\nEnglish courts applied merchant customs only if they were \"certain\" in nature, \"consistent with law\" and \"in existence since time immemorial.\" English judges also required that merchant customs were proven before the court. But even as early as 1608, Chief Justice Edward Coke described \"lex mercatoria\" as \"a part of the common law,\" and William Blackstone would later concur. The tradition continued especially under Lord Mansfield, who is said to be the father of English commercial law. Precepts of the \"lex mercatoria\" were also kept alive through equity and the admiralty courts in maritime affairs. In the US, traditions of the \"lex mercatoria\" prevailed in the general principles and doctrines of commercial jurisprudence.\n\nThe history of the \"lex mercatoria\" in England is divided into three stages: the first prior to the time of Coke, when it was a special kind of law – as distinct from the common law – administered in special courts for a special class of the community (i.e. the mercantile); the second stage was one of transition, the \"lex mercatoria\" being administered in the common law courts, but as a body of customs, to be proved as a fact in each individual case of doubt; the third stage, which has continued to the present day, dates from the presidency over the king's bench of Lord Mansfield (q.v.), under whom it was moulded into the mercantile law of to-day. To the \"lex mercatoria\" modern English law owes the fundamental principles in the law of partnership, negotiable instruments and trade marks.\nSir John Holt (Chief Justice 1689 to 1710) and Lord Mansfield (Chief Justice, 1756 to 1788) were the leading proponents of incorporating the \"lex mercatoria\" into the common law. Holt did not complete the task, possibly out of his own conservatism (see \"Clerke v Martin\") and it was Lord Mansfield that became known as the 'founder of the commercial law of this country\" (Great Britain). Whilst sitting in Guildhall, Lord Mansfield created,\na body of substantive commercial law, logical, just, modern in character and at the same time in harmony with the principles of the common law. It was due to Lord Mansfield's genius that the harmonisation of commercial custom and the common law was carried out with an almost complete understanding of the requirements of the commercial community, and the fundamental principles of the old law and that that marriage of idea proved acceptable to both merchants and lawyers.\n\n\n\"Lex mercatoria\" precepts have been reaffirmed in new international mercantile law. National trade barriers are torn down in order to induce commerce. The new commercial law is grounded on commercial practice directed at market efficiency and privacy. Dispute resolution has also evolved, and functional methods like international commercial arbitration is now available. These developments have also attracted the interest of empirical sociology of law The principles of the medieval \"lex mercatoria\" – efficiency, party autonomy, and choice of arbitrator – are applied, and arbitrators often render judgements based on customs. The new merchant law encompasses a huge body of international commercial law.\n\nIn summary, nation states somewhat fragmented the medieval \"lex mercatoria\" but it is far from destroyed. Local interests triumphed in the medieval ages, just as national interests do today. A modern variant of the \"lex mercatoria\" is the evolving law and dispute resolution in cyberspace. Internet traders are the fastest growing body of merchants in history. Parties can solve domain-name disputes online expeditiously and quickly. In a virtual court documents are filed and examined online, arguments are made online and decisions are published online – seldom challenged before traditional courts of law. ICANN's UDRP (and its proposals for Rapid Suspension) and Nominet's DRS are examples of this. The medieval, the modern and cyberspace merchant laws face comparable issues of enforceability. They solve the problems somewhat differently, but the reaction of the market is the main incentive to comply with a ruling.\n\nFurther, \"lex mercatoria\" is sometimes used in international disputes between commercial entities. Most often those disputes are decided by arbitrators which sometimes are allowed (explicitly of implied) to apply \"lex mercatoria\" principles. Therefore, some legal practitioners assume that there is a whole set of legal principles named \"lex mercatoria\" in international or transnational commercial law. The most recent and constantly updated set of rules are the \"TransLex Principles\" collected and formulated by Klaus Peter Berger (University of Cologne) and his Center for Transnational Law.\n\nWhat remains of \"lex mercatoria\" precepts today is a qualified faith in self-regulation by merchants, and a reluctance to surrender the efficiencies of merchant practice to state confinement.\n\n\n\n"}
{"id": "1742837", "url": "https://en.wikipedia.org/wiki?curid=1742837", "title": "List of Australian and New Zealand advertising characters", "text": "List of Australian and New Zealand advertising characters\n\nMany advertising characters used as mascots and characters by companies in Australia and New Zealand are similar to those used in the United States and the United Kingdom. There are, however, quite a number that are unique to these two nations.\n\nMany advertisements shown on New Zealand television are made in Australia, and many Australian and New Zealand companies operate similar businesses on both sides of the Tasman Sea. As such, there is considerable overlap in advertising characters and mascots found in the two countries.\n\nThe following is a list of notable mascots and characters created specifically for advertising purposes in Australia and New Zealand, listed alphabetically by the product they represent.\n\n\n"}
{"id": "1302778", "url": "https://en.wikipedia.org/wiki?curid=1302778", "title": "List of privatizations by country", "text": "List of privatizations by country\n\nThis list of privatizations provides links to notable and/or major privatizations.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n1,150 public companies, including banks, railroads, the telephone company, mines, roads, TV stations, ports, airports, airlines, sugar mills, and retirement funds.\n\n\n\n\n\n\n\n\n\nA wide-scale privatization program was launched in 1992-1994, using a voucher privatization scheme; from 1995, a monetary scheme was used.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "11988019", "url": "https://en.wikipedia.org/wiki?curid=11988019", "title": "Managerial psychology", "text": "Managerial psychology\n\nManagerial psychology is a sub-discipline of industrial and organizational psychology, which focuses on the efficacy of individuals, groups and organizations in the workplace. Its purpose is to specifically aid managers in gaining a better understanding of the psychological patterns common among individuals and groups within any given organisation. Managerial psychology can be used to predict and prevent harmful psychological patterns within the workplace and can also be implemented to control psychological patterns among individuals and groups in a way that will benefit the organisation long term. (Robbins, SP et al.2010).\n\nIf in the early stages managerial psychologists used to study the problems of fatigue, boredom, and other working conditions that could impede efficient work performance. More recently, their contributions have expanded to include learning, perception, personality, emotions, training, leadership, effectiveness, needs and motivational forces, job satisfaction, decision-making processes, performance appraisals, attitude measurement, employee-selection techniques, work design, and job stress(Robbins, SP et al.2010). This means that they apply psychology principles to the workplace and use their skills to study workplace productivity, morale, employee screening or organizational development. Apart from this, they can also train and screen job applicants, assist with organizational development, and consult with corporations on a problem-solving basis.\n\n\nHerzberg et al.’s (1959) seminal two-factor theory of motivation theorized that satisfaction and dissatisfaction were not two opposite extremes of the same sequence, but two separate entities caused by quite different facets of work – these were labelled as “hygiene factors” and “motivators”. Hygiene factors are characterized as extrinsic components of job design that contribute to employee dissatisfaction if they are not met. Examples include: supervision, working conditions, company policies, salary, and relations with co-workers. Motivators, however, are intrinsic to the job itself and include aspects such as achievement, development, responsibility and recognition. On the other hand, intrinsic factors have long been acknowledged as important determinants of motivation. There is a longstanding debate as to whether hygiene factors really contribute to job satisfaction (Furnham et al., 1999; Warr, 1987). Most job satisfaction and motivation research literature is concerned with organisational or situational predictors (such as pay and supervision) (Locke, 1976) while neglecting individual differences (Staw and Ross, 1985). O’Reilly et al. (1980) discovered that individuals’ significantly differ in the way they perceive their jobs, even if the job description and the tasks they had to perform remained constant, thus suggesting that some individual differences must have an effect on work attitudes.\n\nStaw et al. (1986) argued that individual disposition may have a profound influence over how the working world is perceived (i.e. what is important to the individual), and this is likely to affect the type of jobs that are sought.\n\nIt was first introduced in Gosling et al., (2003) and the ten items of this measure are scored using a seven-point scale, with two statements (one reversed) used to measure each personality variable. The authors report extensive data showing good reliability and validity of this instrument.\n\nThis inventory introduced in Furnham et al., (2005) consists of 37 items and requires individuals to report the extent to which intrinsic (e.g.responsibility and personal growth) and extrinsic (e.g. pay and benefits) components are important to them on a six-point scale. The WVQ is a revised version of Mantech’s(1983) questionnaire. Previous studies have indicated that between two and four factors tend to be extracted, and that these often correspond to Herzberg et al.’s (1959) hygiene and motivator factors .\n\nThis scale introduced in Warr et al., (1979) consists of 15 items, seven of which measure intrinsic satisfaction, whilst the remaining eight measure extrinsic job satisfaction. Responses are given on a seven-point scale and can be summed to create and overall satisfaction score as well as an intrinsic and extrinsic value.\n\nIn a recent issue of Journal of Managerial Psychology published in 2009 is presented an experiment with 202 full-time employees (81 males, mean age=38.3 and 121 females, mean age= 28.4) working in very different jobs in the retail, manufacturing and healthcare to investigate the extent to which personality and demographic factors explain variance in motivation and job satisfaction as defined by Herzberg et al.’s (1959) two-factor theory. Every person was given 3 questionnaires ( The ten item personality inventory, The work values questionnaire (WVQ), The job satisfaction scale) and had to complete them via a website.\n\nAs predicted, personality and demographic variables were significant correlates of the extracted factors, accounting for between 9 and 15.2 per cent of the variance. Similarly, personality and demographic variables were also significantly related to all three job satisfaction scores and accounted for between 10.5 and 12.7 per cent of the variance. As expected, conscientiousness was a significant correlate of job satisfaction scores in both correlational and regressional analyses. Contrary to expectations, age, job tenure and years working full-time were not significantly related to job satisfaction scores; however, in line with predictions and the two-factor theory, job status was significantly associated with these scores.\n\nNegative relationships were observed between the security and conditions factor and job status, as well as years in full-time employment. These results suggest that individuals with low job status (e.g. graduate positions and non-managerial roles) are more concerned with working conditions and clarity in their work than those of a higher status and individuals who have been working for longer periods.\n\nThese results further validate the contention that work attitudes are not the product of situational factors alone, and that both literature and organisations should further investigate the variables that contribute to these values with the intention of increasing job satisfaction and performance, through effective selection methods and pervasive job interventions.\n\nAbraham Maslow developed the Hierarchy of Needs model in 1940-50s USA, and the Hierarchy of Needs theory remains valid today for understanding human motivation, management training, and personal development. Maslow's ideas surrounding the Hierarchy of Needs concern the responsibility of employers to provide a workplace environment that encourages and enables employees to fulfill their own unique potential (self-actualization).\n\nWhile Maslow referred to various additional aspects of motivation, he expressed the Hierarchy of Needs in these five clear stages.\n\n\nDouglas McGregor, an American social psychologist, proposed his famous X-Y theory in his 1960 book 'The Human Side Of Enterprise'. Theory X and Theory Y are still referred to commonly in the field of management and motivation. McGregor's ideas suggest that there are two fundamental approaches to managing people. Many managers tend towards theory x, and generally get poor results. Enlightened managers use theory y, which produces better performance and results, and allows people to grow and develop.\n\n\n\nSee also Need theory\n\nDavid McClelland in his 1961 book, \"The Achieving Society \" identified three motivators that he believed we all have: a need for achievement, a need for affiliation, and a need for power. People will have different characteristics depending on their dominant motivator. According to McClelland, these motivators are learned (which is why this theory is sometimes called the Learned Needs Theory).\n\nMcClelland says that, regardless of our gender, culture, or age, we all have three motivating drivers, and one of these will be our dominant motivating driver. This dominant motivator is largely dependent on our culture and life experiences.\n\nPeople motivated by achievement need challenging, but not impossible, projects. They thrive on overcoming difficult problems or situations, so make sure you keep them engaged this way. People motivated by achievement work very effectively either alone or with other high achievers.\n\nWhen providing feedback, give achievers a fair and balanced appraisal. They want to know what they're doing right – and wrong – so that they can improve.\n\nPeople motivated by affiliation work best in a group environment, so try to integrate them with a team (versus working alone) whenever possible. They also don't like uncertainty and risk. Therefore, when assigning projects or tasks, save the risky ones for other people.\n\nWhen providing feedback to these people, be personal. It's still important to give balanced feedback, but if you start your appraisal by emphasizing their good working relationship and your trust in them, they'll likely be more open to what you say. Remember that these people often don't want to stand out, so it might be best to praise them in private rather than in front of others.\n\nThose with a high need for power work best when they're in charge. Because they enjoy competition, they do well with goal-oriented projects or tasks. They may also be very effective in negotiations or in situations in which another party must be convinced of an idea or goal.\n\nWhen providing feedback, be direct with these team members. And keep them motivated by helping them further their career goals\n\n"}
{"id": "49611486", "url": "https://en.wikipedia.org/wiki?curid=49611486", "title": "Mid-market awards", "text": "Mid-market awards\n\nThe CEO Connection Mid-Market Awards are a critical piece of a larger strategy to ensure the mid-market receives the attention it warrants as a job creator, a force for economic growth, and a driver of social impact. The Mid-Market Awards honor three mid-market leaders and one company that have demonstrated leadership, creativity, generosity and other qualities that represent the true spirit of the mid-market.\n\nAwardees are honored during a dedicated dinner held in conjunction with CEO Connection’s Mid-Market Convention, which is held at the Wharton School of the University of Pennsylvania.\n\nThe award categories include Mid-Market Company of the Year, Mid-Market CEO of the Year, Mid-Market Young Leader, and Mid-Market Social Impact Award. The Mid-Market Awards also includes presentation of a list of honorees of the Most Influential Women of the Mid-Market.\n\n\n\n\n\n\n"}
{"id": "4087550", "url": "https://en.wikipedia.org/wiki?curid=4087550", "title": "Operating agreement", "text": "Operating agreement\n\nAn operating agreement is a key document used by LLCs because it outlines the business' financial and functional decisions including rules, regulations and provisions. The purpose of the document is to govern the internal operations of the business in a way that suits the specific needs of the business owners. Once the document is signed by the members of the limited liability company, it acts as an official contract binding them to its terms. Many states in the United States require an LLC to have an operating agreement. LLCs operating without an operating agreement are governed by the state's default rules contained in the relevant statute and developed through state court decisions. An operating agreement is similar in function to corporate by-laws, or analogous to a partnership agreement in multi-member LLCs. In single-member LLCs, an operating agreement is a declaration of the structure that the member has chosen for the company and sometimes used to prove in court that the LLC structure is separate from that of the individual owner and thus necessary so that the owner has documentation to prove that he or she is indeed separate from the entity itself.\n\nMost states do not require operating agreements. However, an operating agreement is highly recommended for multi-member LLCs because it structures an LLC's finances and organization, and provides rules and regulations for smooth operation. The operating agreement usually includes percentage of interests, allocation of profits and losses, member's rights and responsibilities and other provisions.\nOperating agreements are legally significant. Having a correctly drafted operating agreement is crucial in case legal disputes arise between business owners.\n\n"}
{"id": "45075900", "url": "https://en.wikipedia.org/wiki?curid=45075900", "title": "PandaDoc", "text": "PandaDoc\n\nPandaDoc is a document automation software as a service with built-in electronic signatures, workflow management, a document builder, and CPQ functionality. Its developer company was founded in 2013 by Mikita Mikado and Sergey Barysiuk, and is based in San Francisco, California. PandaDoc currently has offices in Minsk, Belarus and St. Petersburg, Florida.\n\nPandaDoc was founded in 2013 by Mikita Mikado and Sergey Barysiuk. Mikita Mikado and Sergey Barysiuk initially created Quote Roller in 2011.\n\nIn 2015, PandaDoc raised US $5 million in Series A funding, and a Series B round worth US $15 million in 2017 led by Rembrandt Venture Partners, which also included funding from Microsoft Ventures, HubSpot, EBRD and Altos Ventures.\n\nAs of 2018, the company reached its 8,000th paying customer.\n\nPandaDoc proposal and contract software is a SaaS product for sales processes. The software is available in English.\n\nPandaDoc includes features to create, track and execute documents, as well as functionality for electronic signatures. It consists of features in the following categories: proposals, quotes, team management, content management, branding, tracking, workflow, productivity, etc. It integrates with several CRMs, as well as ERP, payment, cloud storage, and other systems.\n\nPandaDoc is available as native Android and iOS apps.\n\nPandaDoc integrates with a number of other enterprise systems. It also offers an API and JavaScript SDK so that developers can build their own integrations.\n\n\n"}
{"id": "1175262", "url": "https://en.wikipedia.org/wiki?curid=1175262", "title": "Performance indicator", "text": "Performance indicator\n\nA performance indicator or key performance indicator (KPI) is a type of performance measurement. KPIs evaluate the success of an organization or of a particular activity (such as projects, programs, products and other initiatives) in which it engages. \n\nOften success is simply the repeated, periodic achievement of some levels of operational goal (e.g. zero defects, 10/10 customer satisfaction, etc.), and sometimes success is defined in terms of making progress toward strategic goals. Accordingly, choosing the right KPIs relies upon a good understanding of what is important to the organization. What is deemed important often depends on the department measuring the performance – e.g. the KPIs useful to finance will differ from the KPIs assigned to sales. \n\nSince there is a need to understand well what is important, various techniques to assess the present state of the business, and its key activities, are associated with the selection of performance indicators. These assessments often lead to the identification of potential improvements, so performance indicators are routinely associated with 'performance improvement' initiatives. A very common way to choose KPIs is to apply a management framework such as the balanced scorecard.\n\nKey performance indicators define a set of values against which to measure. These raw sets of values, which can be fed to systems that aggregate the data, are called \"indicators\". There are two categories of measurements for KPIs. \n\nAn 'indicator' can only measure what 'has' happened, in the past tense, so the only type of measurement is descriptive or lagging. Any KPI that attempts to measure something in a future state as predictive, diagnostic or prescriptive is no longer an 'indicator' it is a 'prognosticator' - at this point its analytics (possibly based on a KPI).\n\n\"Performance\" focuses on measuring a particular \"element\" of an \"activity\". An activity can have four elements: input, output, control, and mechanism. At a minimum, an activity is required to have at least an input and an output. Something goes into the activity as an \"input\"; the activity transforms the input by making a change to its \"state\"; and the activity produces an \"output\". An activity can also have enabling \"mechanisms\" that are typically separated into \"human\" and \"system\" mechanisms. It can also be constrained in some way by a \"control\". Lastly, its actions can have a temporal construct of \"time\".\n\n\nPerformance indicators differ from business drivers and aims (or goals). A school might consider the failure rate of its students as a key performance indicator which might help the school understand its position in the educational community, whereas a business might consider the percentage of income from returning customers as a potential KPI.\n\nThe key stages in identifying KPIs are:\n\nKey performance indicators (KPIs) are ways to periodically assess the performances of organizations, business units, and their division, departments and employees. Accordingly, KPIs are most commonly defined in a way that is understandable, meaningful, and measurable. They are rarely defined in such a way such that their fulfillment would be hampered by factors seen as non-controllable by the organizations or individuals responsible. Such KPIs are usually ignored by organizations.\n\nKPIs should follow the SMART criteria. This means the measure has a Specific purpose for the business, it is Measurable to really get a value of the KPI, the defined norms have to be Achievable, the improvement of a KPI has to be Relevant to the success of the organization, and finally it must be Time phased, which means the value or outcomes are shown for a predefined and relevant period.\n\nIn order to be evaluated, KPIs are linked to target values, so that the value of the measure can be assessed as meeting expectations or not.\n\nKey performance indicators are the non-financial measures of a company's performance - they do not have a monetary value but they do contribute to the company's profitability. \n\nSome examples are:\n\n\nMany of these customer KPIs are developed and managed with customer relationship management software.\n\nFaster availability of data is a competitive issue for most organizations. For example, businesses which have higher operational/credit risk (involving for example credit cards or wealth management) may want weekly or even daily availability of KPI analysis, facilitated by appropriate IT systems and tools.\n\nOverall equipment effectiveness is a set of broadly accepted non-financial metrics which reflect manufacturing success.\n\nMost professional services firms (for example: management consultancies, systems integration firms, or digital marketing agencies) use three key performance indicators to track the health of their businesses. They typically use professional services automation (PSA) software to keep track of and manage these metrics.\n\n\n\nBusinesses can utilize KPIs to establish and monitor progress toward a variety of goals, including lean manufacturing objectives, minority business enterprise and diversity spending, environmental \"green\" initiatives, cost avoidance programs and low-cost country sourcing targets.\n\nAny business, regardless of size, can better manage supplier performance with the help of KPIs robust capabilities, which include:\n\nMain SCM KPIs will detail the following processes:\n\nSuppliers can implement KPIs to gain an advantage over the competition. Suppliers have instant access to a user-friendly portal for submitting standardized cost savings templates. Suppliers and their customers exchange vital supply chain performance data while gaining visibility to the exact status of cost improvement projects and cost savings documentation.\n\nThe provincial government of Ontario, Canada has been using KPIs since 1998 to measure the performance of higher education institutions in the province. All post secondary schools collect and report performance data in five areas – graduate satisfaction, student satisfaction, employer satisfaction, employment rate, and graduation rate.\n\n\n\nHuman Resource Management\n\n\nIn practice, overseeing key performance indicators can prove expensive or difficult for organizations. Some indicators such as staff morale may be impossible to quantify. As such, dubious KPIs can be adopted that can be used as a rough guide rather than a precise benchmark. \n\nKey performance indicators can also lead to perverse incentives and unintended consequences as a result of employees working to the specific measurements at the expense of the actual quality or value of their work. \n\nSometimes the collecting of statistics can become a substitute for a better understanding of the problems so the use of dubious KPIs can result in progress in aims and measured effectiveness becoming different. For example, US soldiers during the Vietnam War were shown to be effective in kill ratios and high body counts, but this was misleading when used to measure aims as it did not show the lack of progress towards the US goal of increasing South Vietnamese government control of its territory. Another example would be to measure the productivity of a software development team in terms of lines of source code written. This approach can easily result in large amounts of dubious code being added, thereby inflating the line count but adding little of value in terms of systemic improvement. A similar problem arises when a footballer kicks a ball uselessly in a match in order to build up his statistics.\n\n\n"}
{"id": "18920944", "url": "https://en.wikipedia.org/wiki?curid=18920944", "title": "Personal offshoring", "text": "Personal offshoring\n\nPersonal Offshoring or Smallsourcing is a business model where individual consumers or small businesses (often sole proprietorships) directly outsource their work. It is a form of Offshoring. According to the Wall Street Journal, \"[t]he approach relies on the same model that drives corporate outsourcing: labor arbitrage, or benefiting from the wage differential between U.S. workers and those in developing countries.\" It can potentially be a dangerous business model if not managed appropriately. Problems can include language barriers and communications issues due to differences in time zones.\n"}
{"id": "53971331", "url": "https://en.wikipedia.org/wiki?curid=53971331", "title": "Stakeholder approach", "text": "Stakeholder approach\n\nIn management, a stakeholder approach suggests that managers should formulate and implement processes which satisfy stakeholders' needs in order to ensure the long-term success of the firm. According to the degree of participation of the different groups, the company can take advantage of market imperfections in order to create valuable opportunities. It emphasizes active management of the business environment, relationships and the promotion of shared interests. This approach is based on the stakeholder theory which arises as a counterpart to the dominant way of understanding business and management that is focused on shareholders satisfaction. The implementation of this approach can reinforce the firm values and create competitive advantage. However, it has been criticized for overvaluing stakeholders and its difficulty to reach consensus. \n\nThis approach is more able to create competitive advantage because it creates a link between the firm and stakeholders. The latter will perceive the coherent application of the organizational values and will relate those values with his own. In that way, the company will have the needed information about stakeholders in order to treat them well and develop important initiatives. By doing that, the firm’s reputation and loyalty will be reinforced among customers and other stakeholders alike, it will create stronger brand recognition and will increase trust in the firm. Even if there are limits in loyalty and reputation can be damaged, those two key elements can make a big difference creating barriers to other companies that may want to have information about stakeholder utility functions. A firm that follows the stakeholder approach will get the information needed to work for satisfying the stakeholders’ needs, making it easier to develop expertise. Those acquired skills can be transmitted, promoted and reinforced across the business operation of the firm creating core competencies. Over time, this approach can become an indispensable issue in the organizational culture.\n\nFirms that manage for stakeholders are more able to attract a higher-quality workforce. Employees’ job satisfaction has an impact on the firm’s ability to foster innovation. Workers who are satisfied with their jobs are more likely to engage in long-term thinking and generate potentially valuable ideas. Those firms can use information about stakeholder to devise new ways of satisfying them. Reciprocity is a key aspect in this approach: when stakeholders stand to benefit, they are more likely to reveal information about their utility function. That is why firms and firm managers can better meet consumers' needs by understanding their own customers and suppliers and using this information strategically and flexibly.\n\nThe implication of all the stakeholders may produce divergent views making it difficult to reach consensus. Each stakeholder may care mostly about its own benefits or self-interests. Trying to satisfy a large number of players will complicate governance. It can result in time consuming in engaging all the parties. Moreover, in this approach, an equality of stakeholders and business is implied in negotiating issues of mutual interest. That assumption has been critiqued in terms of an inequality of resources, negotiating power and time required. The identification of prices and opportunity costs for the different stakeholders difficult and reduce the operability of this approach.\n\nIt has been suggested that obtaining information about stakeholders' utility functions may produce costs that can exceed the benefits. Therefore, in its intention to create value, managing for stakeholders can end up allocating too many resources to stakeholders. Also, having into account that the power among stakeholders is not equal, some powerful actors can get much of the firm’s profitability. And with that distribution of value, shareholders cannot expect a maximization of returns.\n\n"}
{"id": "20273569", "url": "https://en.wikipedia.org/wiki?curid=20273569", "title": "Stakeholder management", "text": "Stakeholder management\n\nStakeholder management is a critical component to the successful delivery of any project, programme or activity. A stakeholder is any individual, group or organization that can affect, be affected by, or perceive itself to be affected by a programme. Stakeholder management creates positive relationships with stakeholders through the appropriate management of their expectations and agreed objectives. Stakeholder management is a process and control that must be planned and guided by underlying principles. Stakeholder management within businesses, organizations, or projects prepares a strategy using information (or intelligence) gathered during the following common processes.\n\nThe first step in your stakeholder analysis is to brainstorm who your stakeholders are. As part of this, think of all the people who are affected by your work, who have influence or power over it, or have an interest in its successful or unsuccessful conclusion. Remember that although stakeholders may be both organizations and people, ultimately you must communicate with people. Make sure that you identify the correct individual stakeholders within a stakeholder organization. \n\nSo in a nutshell, the stakeholder management comprises four steps:\n\n\nYou may now have a long list of people and organizations that are affected by your work. Some of these may have the power either to block or advance. Some may be interested in what you are doing, others may not care. Map out your stakeholders on a Power/Interest Grid, and classify them by their power over your work and by their interest in your work. There are other tools available to map out your stakeholders and how best to influence them.\n\nFor example, your boss is likely to have high power and influence over your projects and high interest. Your family may have high interest, but are unlikely to have power over it. Someone's position on the grid shows you the actions you have to take with them:\n\nYou now need to know more about your key stakeholders. You need to know how they are likely to feel about and react to your project. You also need to know how best to engage them in your project and how best to communicate with them. Key questions that can help you understand your stakeholders are:\n\n\nSource: \n\nIt is well acknowledged that any given organization will have multiple stakeholders including, but not limited to, customers, shareholders, employees, suppliers, and so forth. Within the field of marketing, it is believed that customers are one of the most important stakeholders for managing its long-term value, with a firm's major objective being the management of customer satisfaction.\n\nWith a clear understanding of your Stakeholders, engaging and communicating can be achieved through a variety of channels based upon who the stakeholder is.\n\n\n"}
{"id": "56830751", "url": "https://en.wikipedia.org/wiki?curid=56830751", "title": "Teal organisation", "text": "Teal organisation\n\nA teal organisation is an emerging organisational paradigm that advocates a level of consciousness including all previous world views within the operations of an organisation. The concept of teal organisation refers to the next stage in the evolution of consciousness and was introduced in 2014 by Frederic Laloux in his book on Reinventing Organizations. It also rests on previous studies done by evolutionary and social psychologists including Jean Gebser, Clare W. Graves, Don Edward Beck, Chris Cowan and Ken Wilber who explored the stages of development and impact of human consciousness.\n\nA teal organisation transcends and integrates the existing limitations of current static organisational paradigms including Amber, Orange and Green organisations. It is characterized by three breakthroughs in human collaboration, specific to this evolutionary level: \"self-management\" suggests a system based on peer relationships with no need for hierarchy, consensus, nor central command and control; \"wholeness\" is about a consistent set of practices that invite members to reclaim their inner wholeness and bring on the workplace “all of who they are”; \"evolutionary purpose\" introduces a teal organisation as a living organism with a direction of its own where its members are invited to listen and take note of the purpose it wants to serve. These breakthroughs overcome the limitations of previous organisational models in that they welcome the emotional, intuitive, and spiritual elements in lieu of the usual display of rationality, determination and strength; and concealment of doubts or vulnerability. During its evolution, a teal organisation exhibits properties similar to complex adaptive systems because the interactions and relationships between its elements are nonlinear and based on few simple rules or guiding principles. These elements learn from the past and their immediate environment and then adapt accordingly for the survival of the system. A number of notable organisations around the world has adopted and operates on the teal organisation model including The Morning Star Company (food processing, United States), Patagonia (apparel, United States), Sounds True (media, United States), AES (energy sector, international), Buurtzorg Nederland (health care, Netherlands), ESBZ (K–12 school, Germany), Heiligenfelde (mental health hospitals, Germany), Nucor (steel manufacturing, international).\n\n\n\n"}
{"id": "4142888", "url": "https://en.wikipedia.org/wiki?curid=4142888", "title": "Telligent Systems", "text": "Telligent Systems\n\nTelligent, A Verint Company is an enterprise collaboration and community software business founded in 2004 by Rob Howard. The company changed its name to Zimbra, Inc in September 2013 after completing the acquisition of Zimbra from VMWare. In August 2015 Zimbra's Telligent business was acquired by Verint Systems, Inc. Verint continues to operate Telligent as an independent business unit. Also in August 2015 the remaining assets of Zimbra, Inc were acquired by Synacor.\n\nTelligent was founded by Rob Howard in 2004. Howard was previously a founding member of Microsoft's ASP.NET team and helped build and run the Microsoft ASP.NET community. \n\nTelligent introduced its first product, Community Server, in the fall of 2004. Community Server was an integrated community platform that brought together blogs, wikis, forums, user profiles, etc. Community Server was based on the work done by Rob Howard on the ASP.NET Forums, Jason Alexander on nGallery, and Scott Watermasysk on .Text.\n\nDell and MySpace both became Telligent customers in 2006. Dell started a blog on Telligent's platform in response to Jeff Jarvis' post about his dissatisfaction with a Dell laptop.\n\nAt the end of 2007 Telligent introduced a social analytics tool called Harvest Reporting Server.\n\nIn 2008, Intel Capital became Telligent's first capital partner.\n\nIn 2009, Patrick Brandt took over as CEO and Rob Howard became CTO. Patrick Brandt was previously CEO of Skywire Software, which was acquired by Oracle for an undisclosed amount. Telligent also re-branded its product offering as follows: Community Server became Telligent Community, Harvest Reporting Server became Telligent Analytics, and Community Server Evolution became Telligent Enterprise. Telligent also formally introduced the Telligent Evolution platform upon which Telligent Community and Telligent Enterprise were based.\n\nAs part of the company re-positioning, Telligent discontinued development and support for Graffiti CMS and made it available as an open source project. Telligent also discontinued BlogMailr, a free email-to-blog service.\n\nIn 2010 David Mitchell joined Telligent's board. David is currently CEO of Global 360 and was previously CEO of WebMethods. Telligent additionally added Wendy Gibson as Chief Marketing Officer.\n\nOn December 19, 2011, Telligent acquired Leverage Software.\n\nOn July 15, 2013, Telligent acquired Zimbra from Vmware. \n\nOn August 1, 2015, Telligent was acquired by Verint Systems, Inc.\n\nTelligent's products are built on the Microsoft .NET and Microsoft SQL Server platform. They are primarily used as on-premises, white label software solutions.\n\nTelligent Community (formerly Community Server), built on the Telligent Evolution platform, is Telligent's flagship product. It was first introduced in 2004 and the most recent version is 5.6 as of October 2010.\n\nTelligent Community is designed to support external facing communities and the primary use cases are: digital marketing, support communities, and networking.\n\nTelligent Enterprise, built on the Telligent Evolution platform, was first introduced in 2008 in response to users of Telligent Community asking Telligent to provide an employee-focused solution. Telligent Enterprise version 2.6 was released October 2010.\n\nTelligent Enterprise is designed to support enterprise 2.0 / internal communities, private business-to-business communities, and private networking communities. An emphasis on integration with enterprise email systems, such as Microsoft Exchange Server, and enterprise identity management systems, such as Microsoft Active Directory, are examples of how Telligent Enterprise differs from Telligent Community.\n\nTelligent introduced Telligent Analytics in 2007 as Harvest Reporting Server. Telligent Analytics is designed to analyze people and information created within the Telligent Evolution platform. This includes both Telligent Community and Telligent Enterprise. It additionally includes any data created on applications that run on the Telligent Evolution platform.\n\n"}
{"id": "750398", "url": "https://en.wikipedia.org/wiki?curid=750398", "title": "Trading while insolvent", "text": "Trading while insolvent\n\nTrading while insolvent is unlawful in a number of legal systems, and may result in the directors becoming personally liable for a company's debts.\n\nUnder UK insolvency law trading once a company is legally insolvent can trigger several provisions of the Insolvency Act 1986, including:\n\n\nA limited company becomes insolvent when it can no longer pay its bills when due, or its liabilities—including contingent liabilities such as redundancy payments—outweigh the company’s assets. This is a critical point in the lifespan of a company as it denotes when the directors' responsibilities change from the shareholders to the creditors. It also means that the directors need to be extremely careful when considering whether to continue to trade, or not. Any director who knows that the company is insolvent and makes the decision to continue to trade, and in doing so increases the debts of the company can be made liable for the company debts.\n\nIn most legal systems, the liability in respect of other transactions only extends for a certain period of time prior to the company going into liquidation. In the UK, directors are exposed in respect of transaction at an undervalue, preferences, and extortionate credit transactions if the transaction occurred: a) while the company was insolvent; and b) within 2 years before the onset of liquidation if the transaction was with a connected person, and 6 months if the transaction was with an unconnected person.\n\nDirectors who continue to trade while insolvent may face disqualification under the Company Directors Disqualification Act 1986. Under the provision of this act, when a company goes into liquidation, the liquidator must make a report to the Disqualification Unit of the Department for Business, Innovation and Skills on the conduct of all directors. If the liquidator has come across any conduct which makes the director unfit to be involved in the management of a company in the future (which things would include trading while insolvent) the Department for Business, Innovation and Skills will apply to the Court for an order disqualifying the director or directors from acting as a company director for a certain period of time.\n\nMany other countries have similar laws, often referred to as 'insolvent trading' or wrongful trading.\n\n"}
{"id": "38500550", "url": "https://en.wikipedia.org/wiki?curid=38500550", "title": "Travel management company", "text": "Travel management company\n\nTravel Management Companies (TMCs) are organizations that manage organizations' corporate or business travel program. They will often provide an end-user online booking tool, mobile application, program management, and consulting teams, executive travel services, meetings and events support, reporting functionality, and more. These companies use Global Distribution Systems (GDS) to book flights for their clients. This allows the travel consultant to compare different itineraries and costs by displaying availability in real-time, allowing users to access fares for air tickets, hotel rooms and rental cars simultaneously.\n\nSome major TMCs include: American Express Global Business Travel, BCD Group and Carlson Wagonlit Travel.\n"}
{"id": "372478", "url": "https://en.wikipedia.org/wiki?curid=372478", "title": "Video game industry", "text": "Video game industry\n\nThe video game industry is the economic sector involved in the development, marketing, and monetization of video games. It encompasses dozens of job disciplines and its component parts employ thousands of people worldwide.\n\nThe computer and video game industry has grown from focused markets to mainstream. They took in about US$9.5 billion in the US in 2007, 11.7 billion in 2008, and 25.1 billion in 2010 (ESA annual report).\n\nModern personal computers owe many advancements and innovations to the game industry: sound cards, graphics cards and 3D graphic accelerators, faster CPUs, and dedicated co-processors like PhysX are a few of the more notable improvements.\n\nSound cards were developed for addition of digital-quality sound to games and only later improved for music and audiophiles. Early on, graphics cards were developed for more colors. Later, graphic cards were developed for graphical user interfaces (GUIs) and games; GUIs drove the need for high resolution, and games began using 3D acceleration. They also are one of the only pieces of hardware to allow multiple hookups (such as with SLI or CrossFire graphics cards). CD- and DVD-ROMs were developed for mass distribution of media in general; however the ability to store more information on cheap easily distributable media was instrumental in driving their ever-higher speeds.\n\nBen Sawyer of Digitalmill observes that the game industry value chain is made up of six connected and distinctive layers:\n\nThe game industry employs those experienced in other traditional businesses, but some have experience tailored to the game industry. Some of the disciplines specific to the game industry include: game programmer, game designer, level designer, game producer, game artist and game tester. Most of these professionals are employed by video game developers or video game publishers. However, many hobbyists also produce computer games and sell them commercially. Game developers and publishers sometimes employ those with extensive or long-term experience within the modding communities.\n\nPrior to the 1970s, there was no significant commercial aspect of the video game industry, but many advances in computing would set the stage for the birth of the industry.\n\nMany early publicly-available interactive computer-based game machines used or other mechanisms to mimic a display; while technically not \"video games\", they had elements of interactivity between the player and the machine. Some examples of these included the 1940 \"Nimatron\", an electromagentic relay-based Nim-playing device designed by Edward Condon and built by Westinghouse Electric for the New York World's Fair, \"Bertie the Brain\", an arcade game of tic-tac-toe, built by Josef Kates for the 1950 Canadian National Exhibition, and Nimrod created by engineering firm Ferranti for the 1951 Festival of Britain,\n\nThe development of cathode ray tube—the core technology behind televisions—created several of the first true video games. In 1947 Thomas T. Goldsmith Jr. and Estle Ray Mann filed a patent for a \"cathode ray tube amusement device\". Their game, which uses a cathode ray tube hooked to an oscilloscope display, challenges players to fire a gun at target.\n\nBetween the 1950s and 1960s, with mainframe computers becoming available to campus colleges, students and others started to develop games that could be played at terminals that accessed the mainframe. One of the first known examples is \"Spacewar!\", developed by Harvard and MIT employees Martin Graetz, Steve Russell, and Wayne Wiitanen. The introduction of easy-to-program languages like BASIC for mainframes allowed for more simplistic games to be developed.\n\nIn 1971, the arcade game, \"Computer Space was released.\" The following year, Atari, Inc. released the first commercially successful video game, \"Pong\", the original arcade version of which sold over 19,000 arcade cabinets. That same year saw the introduction of video games to the home market with the release of the early video game console, the Magnavox Odyssey. However, both the arcade and home markets would be dominated by \"Pong\" clones, which flooded the market and led to the video game crash of 1977. The crash eventually came to an end with the success of Taito's \"Space Invaders\", released in 1978, sparking a renaissance for the video game industry and paving the way for the golden age of video arcade games. The game's success inspired arcade machines to become prevalent in mainstream locations such as shopping malls, traditional storefronts, restaurants and convenience stores during the golden age. \"Space Invaders\" would go on to sell over 360,000 arcade cabinets worldwide, and by 1982, generate a revenue of $2 billion in quarters, equivalent to $4.6 billion in 2011.\n\nSoon after, \"Space Invaders\" was licensed for the Atari VCS (later known as Atari 2600), becoming the first \"killer app\" and quadrupling the console's sales. The success of the Atari 2600 in turn revived the home video game market during the second generation of consoles, up until the North American video game crash of 1983. By the end of the 1970s, the personal computer game industry began forming from a hobby culture.\n\nThe early 1980s saw the golden age of video arcade games reach its zenith. The total sales of arcade video game machines in North America increased significantly during this period, from $50 million in 1978 to $900 million by 1981, with the arcade video game industry's revenue in North America tripling to $2.8 billion in 1980. By 1981, the arcade video game industry was generating an annual revenue of $5 billion in North America, equivalent to $12.3 billion in 2011. In 1982, the arcade video game industry reached its peak, generating $8 billion in quarters, equivalent to over $18.5 billion in 2011, surpassing the annual gross revenue of both pop music ($4 billion) and Hollywood films ($3 billion) combined at that time. This was also nearly twice as much revenue as the $3.8 billion generated by the home video game industry that same year; both the arcade and home markets combined add up to a total revenue of $11.8 billion for the video game industry in 1982, equivalent to over $27.3 billion in 2011. The arcade video game industry would continue to generate an annual revenue of $5 billion in quarters through to 1985. The most successful game of this era was Namco's \"Pac-Man\", released in 1980, which would go on to sell over 350,000 cabinets, and within a year, generate a revenue of more than $1 billion in quarters; in total, \"Pac-Man\" is estimated to have grossed over 10 billion quarters ($2.5 billion) during the 20th century, equivalent to over $3.4 billion in 2011.\n\nThe early part of the decade saw the rise of 8-bit home computing, and home-made games, especially in Europe (with the ZX Spectrum and Commodore 64) and Asia (with the NEC PC-88 and MSX). This time also saw the rise of video game journalism, which was later expanded to include covermounted cassettes and CDs.\nIn 1983, the North American industry crashed due to the production of too many badly developed games (quantity over quality), resulting in the fall of the North American industry. The industry would eventually be revitalized by the release of the Nintendo Entertainment System, which resulted in the home console market being dominated by Japanese companies such as Nintendo, while a professional European computer game industry also began taking shape with companies such as Ocean Software and Gremlin Interactive. The latter part of the decade saw the rise of the Game Boy handheld system. In 1987, Nintendo lost a legal challenge against Blockbuster Entertainment, which enabled games rentals in the same way as movies.\n\nThe 1990s saw advancements in game related technology. Among the significant advancements were:\n\nAside from technology, in the early part of the decade, licensed games became more popular, as did video game sequels.\n\nThe video game industry generated worldwide sales of $19.8 billion in 1993 (equivalent to $31 billion in 2011), $20.8 billion in 1994 (equivalent to $32 billion in 2011), and an estimated $30 billion in 1998 (equivalent to $41.5 billion in 2011). In the United States alone, in 1994, arcades were generating $7 billion in quarters (equivalent to $11 billion in 2011) while home console game sales were generating revenues of $6 billion (equivalent to $9 billion in 2011). Combined, this was nearly two and a half times the $5 billion revenue generated by movies in the United States at the time.\n\nIn 2000s, the video game industry is a juggernaut of development; profit still drives technological advancement which is then used by other industry sectors. Technologies such as Smartphones, virtual reality and augmented reality are major drivers for game hardware and gameplay development. Though maturing, the video game industry was still very volatile, with third-party video game developers quickly cropping up, and just as quickly, going out of business. Nevertheless, many casual games and indie games were developed and become popular and successful, such as \"Braid\" and \"Limbo\". Game development for mobile phones (such as iOS and Android devices) and social networking sites emerged. For example, a Facebook game developer, Zynga, has raised in excess of $300 million.\n\nThough not the main driving force, indie games continue to have a significant impact on the industry, with sales of some of these titles such as \"Spelunky\", \"Fez\", \"Don't Starve\", \"Castle Crashers\", and \"Minecraft\", exceeding millions of dollars and over a million users. The 2010s have seen a larger shift to casual and mobile gaming; in 2016, the mobile gaming market is estimated to have taken $38 billion in revenues, compared to $6 billion for the console market and $33 billion for personal computing gaming. Games centered on virtual reality and augmented reality equipment also arose during this decade. As of 2014, newer game companies arose that vertically integrate live operations and publishing such as crowdfunding and other direct-to-consumer efforts, rather than relying on a traditional publishers, and some of these have grown to substantial size. Spurred by some initial events in the late 2000s, eSports centered around professional players in organized competitions and leagues for prize money, grew greatly over this decade, drawing hundreds of millions of viewers and reaching nearly $500 million in revenue by 2016 and expected to break $1 billion by 2019.\n\nEarly on, development costs were minimal, and video games could be quite profitable. Games developed by a single programmer, or by a small team of programmers and artists, could sell hundreds of thousands of copies each. Many of these games only took a few months to create, so developers could release multiple titles per year. Thus, publishers could often be generous with benefits, such as royalties on the games sold. Many early game publishers started from this economic climate, such as Origin Systems, Sierra Entertainment, Capcom, Activision and Electronic Arts.\n\nAs computing and graphics power increased, so too did the size of development teams, as larger staffs were needed to address the ever-increasing technical and design complexities. The larger teams consist of programmers, artists, game designers, and producers. Their salaries can range anywhere from $50,000 to $120,000 generating large labor costs for firms producing videogames which can often take between one and three years to develop. Now budgets typically reach millions of dollars despite the growing popularity of middleware and pre-built game engines. In addition to growing development costs, marketing budgets have grown dramatically, sometimes consisting of two to three times of the cost of development.\n\nThe game development team has to select a profitable and suitable method to sell or earn money from the finished game. Traditionally, the game monetization method is to sell hard copies in retail store. Now some developers are turning to alternative production and distribution methods, such as online distribution, to reduce costs and increase revenue.\n\nToday, the video game industry has a major impact on the economy through the sales of major systems and games such as \"\", which took in over $650 USD million of sales in the game's first five days and which set a five-day global record for a movie, book or videogame. The game's income was more than the opening weekend of \"Spider-Man 3\" and the previous title holder for a video game \"Halo 3\". Many individuals have also benefited from the economic success of video games including the former chairman of Nintendo and Japan's third richest man: Hiroshi Yamauchi. Today the global video game market is valued at over $93 billion.\n\nThe industry wide adoption of high-definition graphics during the seventh generation of consoles greatly increased development teams' sizes and reduced the number of high-budget, high-quality titles under development. In 2013 Richard Hilleman of Electronic Arts estimated that only 25 developers were working on such titles for the eighth console generation, compared to 125 at the same point in the seventh generation-console cycle seven or eight years earlier.\n\nThe games industry's shift from brick and mortar retail to digital downloads led to a severe sales decline at video game retailers such as GameStop, following other media retailers superseded by Internet delivery, such as Blockbuster, Tower Records, and Virgin Megastores. GameStop diversified its services by purchasing chains that repair wireless devices and expanding its trade-in program through which customers trade used games for credit towards new games. The company began to produce its own merchandise and games. In Britain, the games retailer Game revamped its stores so customers would spend time playing games there. It built a gaming arena for events and tournaments. The shift to digital marketplaces, especially for smartphones, led to an influx of inexpensive and disposable titles, as well as lower engagement among gamers who otherwise purchased new games from retail. Customers also shifted away from the tradition of buying games on their first day of release.\n\nPublishers often funded trade-in deals to encourage consumers to purchase new games. Trade-in customers at the Australia retailer Game would purchase twice the games per year as non-trade-in customers. The sale of pre-owned games kept retailers in business, and composed about a third of Game's revenue. Retailers also saved on the UK's value-added tax, which only taxed the retailer's profit on pre-owned games, rather than the full sale on regular games. The former trade-in retail executives behind the trade-in price comparison site Trade In Detectives estimated that the United Kingdom's trade-in industry was about a third of the size of its new games business. They figured that sites such as eBay, which convert used games into cash, compose about a quarter of the UK's trade-in market, but do not keep the credit within the industry. While consumers might appear to receive better offers on these sites, they also take about 15 percent of the selling price in fees. Alternatively, some retailers will match the trade-in values offered by their competitors. Microsoft's original plan for the Xbox One attempted to translate trade-in deals for the digital marketplace, with a database of product licenses that shops would be able to resell with publisher permission, though the plan was poorly received or poorly sold.\n\nVideo game industry practices are similar to those of other entertainment industries (e.g., the music recording industry), but the video game industry in particular has been accused of treating its development talent poorly. This promotes independent development, as developers leave to form new companies and projects. In some notable cases, these new companies grow large and impersonal, having adopted the business practices of their forebears, and ultimately perpetuate the cycle.\n\nHowever, unlike the music industry, where modern technology has allowed a fully professional product to be created extremely inexpensively by an independent musician, modern games require increasing amounts of manpower and equipment. This dynamic makes publishers, who fund the developers, much more important than in the music industry.\n\nIn the video game industry, it is common for developers to leave their current studio and start their own. A particularly famous case is the \"original\" independent developer Activision, founded by former Atari developers. Activision grew to become the world's second largest game publisher. In the mean time, many of the original developers left to work on other projects. For example, founder Alan Miller left Activision to start another video game development company, Accolade (now Atari née Infogrames).\n\nActivision was popular among developers for giving them credit in the packaging and title screens for their games, while Atari disallowed this practice. As the video game industry took off in the mid-1980s, many developers faced the more distressing problem of working with fly-by-night or unscrupulous publishers that would either fold unexpectedly or run off with the game profits.\n\nThe industry claims software piracy to be a big problem, and take measures to counter this. \nDigital rights management have proved to be the most unpopular with gamers, as a measure to counter piracy.\nThe most popular and effective strategy to counter piracy is to change the business model to Freemium, where gamers pay for their in-game needs or service. Strong server-side security is required for this, to properly distinguish authentic transactions from hacked (faked) transactions.\n\nOn various Internet forums, some gamers have expressed disapproval of publishers having creative control since publishers are more apt to follow short-term market trends rather than invest in risky but potentially lucrative ideas. On the other hand, publishers may know better than developers what consumers want. The relationship between video game developers and publishers parallels the relationship between recording artists and record labels in many ways. But unlike the music industry, which has seen flat or declining sales in the early 2000s, the video game industry continues to grow. Also, personal computers have made the independent development of music almost effortless, while the gap between an independent game developer and the product of a fully financed one grows larger.\n\nIn the computer games industry, it is easier to create a startup, resulting in many successful companies. The console games industry is a more closed one, and a game developer must have up to three licenses from the console manufacturer:\n\nIn addition, the developer must usually buy development systems from the console manufacturer in order to even develop a game for consideration, as well as obtain concept approval for the game from the console manufacturer. Therefore, the developer normally has to have a publishing deal in place before starting development on a game project, but in order to secure a publishing deal, the developer must have a track record of console development, something which few startups will have.\n\nAn alternative method for publishing video games is to self-publish using the shareware or open source model over the Internet.\n\nGaming conventions are an important showcase of the industry. The major annual gaming conventions include gamescom in Cologne (Germany), the E3 in Los Angeles (USA), the Penny Arcade Expo, and .\n\nAs with other forms of media, video games have often been released in different world regions at different times. The practice has been used where localization is not done in parallel with the rest of development or where the game must be encoded differently, as in PAL vs. NTSC. It has also been used to provide price discrimination in different markets or to focus limited marketing resources. Developers may also stagger digital releases so as not to overwhelm the servers hosting the game.\n\nInternational video game revenue is estimated to be $81.5B in 2014. This is more than double the revenue of the international film industry in 2013. In 2015, it was estimated at .\n\nThe largest nations by estimated video game revenues in 2016 are China ($24.4B), the United States ($23.5B) and Japan ($12.4B). The largest regions in 2015 were Asia-Pacific ($43.1B), North America ($23.8B), and Western Europe ($15.6B).\n\nGaming conventions are an important showcase of the industry. The annual gamescom in Cologne (Germany) is a major expo for video games. The E3 in Los Angeles (USA) is also of global importance, but is an event for industry insiders only.\n\nOther notable conventions and trade fairs include Tokyo Game Show (Japan), Brasil Game Show (Brazil), EB Games Expo (Australia), KRI (Russia), ChinaJoy (China) and the annual Game Developers Conference. Some publishers, developers and technology producers also have their own regular conventions, with BlizzCon, QuakeCon, Nvision and the X shows being prominent examples.\n\nVideo gaming is still in its infancy throughout the African continent, but due to the continent's young population and increasing technological literacy, the sector is growing rapidly. African countries such as South Africa, Nigeria and Kenya have been making rapid advances in mobile game development, both within their country and internationally, but due to limited funding and a market over-crowded with western games, success has thus far been minimal.\n\nCanada has the third largest video game industry in terms of employment numbers. The video game industry has also been booming in Montreal since 1997, coinciding with the opening of Ubisoft Montreal. Recently, the city has attracted world leading game developers and publishers studios such as Ubisoft, EA, Eidos Interactive, Artificial Mind and Movement, BioWare, Warner Bros. Interactive Entertainment and Strategy First, mainly because video games jobs have been heavily subsidized by the provincial government. Every year, this industry generates billions of dollars and thousands of jobs in the Montreal area. Vancouver has also developed a particularly large cluster of video game developers, the largest of which, Electronic Arts, employs over two thousand people. The \"Assassin's Creed\" series, along with the \"Tom Clancy\" series have all been produced in Canada and have achieved worldwide success. For consumers, the largest video games convention in Canada is the Enthusiast Gaming Live Expo (EGLX).\n\nChina is the largest country by game revenue, and has a gaming public that exceeds the population of the entire United States. It is home to Asia Game Show, the largest game convention in the world by attendance. In 2014, the Xbox One became the first new game console sold since China's ban on consoles in 2000.\n\nGermany has the largest video games market in Europe, with revenues of $4.1 billion forecast for 2017. The annual gamescom in Cologne is Europe's largest gaming expo.\n\nOne of the earliest internationally successful video game companies was Gütersloh-based Rainbow Arts (founded in 1984) who were responsible for publishing the popular \"Turrican\" series of games. The Anno series and The Settlers series are globally popular strategy game franchises since the 1990s. The Gothic series, and Risen are established RPG franchises. The X series by Egosoft is the best-selling space simulation. The FIFA Manager series was also developed in Germany. The German action game (2012) was successful in the markets and received largely positive reviews. One of the most famed titles to come out of Germany is \"Far Cry\" (2004) by Frankfurt-based Crytek, who also produced the topseller \"Crysis\" and its sequels later.\n\nOther well-known current and former developers from Germany include Ascaron, Blue Byte, Deck13, Phenomic, Piranha Bytes, Radon, Related, Spellbound and Yager Development. Publishers include Deep Silver (Koch Media), dtp entertainment, Kalypso and Nintendo Europe. Bigpoint Games, Gameforge, Goodgame Studios and Wooga are among the world's leading browser game and social network game developers/distributors.\n\nThe Japanese video game industry is markedly different from the industry in North America, Europe and Australia.\nJapanese companies have created some of the largest and most lucrative titles ever made, such as the \"Mario\", \"Final Fantasy\", \"Metal Gear\", \"Pokémon\" and \"Resident Evil\" series of games.\nIn recent years, consoles and arcade games have both been overtaken by downloadable free-to-play games on the PC and mobile platforms.\n\nThe UK industry is the third largest in the World in terms of developer success and sales of hardware and software by country alone but fourth behind Canada in terms of people employed. The size of the UK game industry is comparable to its film or music industries. In recent years some of the studios have become defunct or been purchased by larger companies such as \"LittleBigPlanet\" developer, Media Molecule and Codemasters. The country is home to some of the world's most successful video game franchises, such as \"Tomb Raider\", \"Grand Theft Auto\", \"Fable\", \"\" and \"Total War\".\n\nThe country also went without tax relief until March 21, 2012 when the British government changed its mind on tax relief for UK developers, which without, meant most of the talented development within the UK may move overseas for more profit, along with parents of certain video game developers which would pay for having games developed in the UK. The industry trade body TIGA estimates that it will increase the games development sector's contribution to UK GDP by £283 million, generate £172 million in new and protected tax receipts to HM Treasury, and could cost just £96 million over five years. Before the tax relief was introduced there was a fear that the UK games industry could fall behind other leading game industries around the world such as France and Canada, of which Canada overtook the UK in terms of job numbers in the industry in 2010.\n\nThe United States has the largest video games presence in the world in terms of total industry employees. In 2004, the U.S. game industry as a whole was worth US$10.3 billion. U.S. gaming revenue is forecast to reach $23.5 billion in 2017, making it the second largest market behind China.\n\nMagnavox is credited for releasing the first video game console, the Odyssey. Activision was the first developer to create independent/third-party video games. Once the fastest-growing American company, Atari crashed in spectacular fashion, resulting in the North American video game crash of 1983. This resulted in the domination of the Japanese video game industry worldwide throughout the 1980s and 1990s.\n\nThe U.S. is home to major game development companies such as Activision Blizzard (\"Call of Duty\", \"World of Warcraft\"), Electronic Arts (\"FIFA\", \"Battlefield\", \"Mass Effect\"), and Take-Two Interactive (Civilization, \"NBA 2K\" series, \"Grand Theft Auto\"). ZeniMax Media (\"Doom\", \"Fallout\", \"The Elder Scrolls\") is the world's largest privately held video game company. Niantic (\"Ingress\", \"Pokémon Go\") is a notable mobile game developer.\n\nValve Corporation operates Steam, the largest computer gaming platform, with an active user base (~125 million) that rivals the combined user bases of the current console generation (~150 million). While not specifically focused on games, the largest mobile gaming platforms are operated by Google (Google Play), and Apple Inc. (App Store), with the majority of mobile revenue coming from Asia. Microsoft operates Xbox, the only major game console hardware franchise not controlled by a Japanese company. Sony established Sony Interactive Entertainment in Silicon Valley to run PlayStation, the world's largest and longest-running video game console franchise.\n\nIntel and Nvidia are the largest makers of PC graphics chips. Advanced Micro Devices has become the most important console processor vendor, with all three of the eighth generation home consoles using AMD GPUs, and two of them use AMD CPUs. Microsoft, Nintendo, and Sony were not aware that they were all using AMD processors until all their consoles were announced, underscoring the secrecy found within the game industry. Notable game engine developers include Epic Games (Unreal Engine) and Unity Technologies (Unity).\n\nThe West Coast is home to important video game conventions, such as Electronic Entertainment Expo (E3), one of the largest video game industry-only events in the world, and Penny Arcade Expo (PAX West), the largest public video game convention in North America. The West Coast is also home to many of the major American video game industry companies, particularly the regions of Los Angeles, San Francisco Bay Area, and Seattle. Major game development regions outside of the West Coast include the Northeast and Texas.\n\nOver 150 million Americans play video games, with an average age of 35 and a gender breakdown of 59 percent male and 41 percent female. American gamers are more likely to vote than non-gamers, feel that the economy is the most important political issue, and lean conservative.\n\nSeveral important game development personalities were born in or moved to the United States. Notable developers include Ralph H. Baer (Magnavox Odyssey, the \"Father of Video Games\"), Jonathan Blow (\"Braid\"), John D. Carmack (\"Doom\", \"Quake\"), and Alexey Pajitnov (\"Tetris\"). Mario is named after Mario Segale, a former landlord of Nintendo of America. Some right-wing and left-wing activists, including Jack Thompson and Anita Sarkeesian, have met extreme resistance from the gaming public in response to the perceived politicizing of their art form.\n\nThe United States recognizes eSports players as professional athletes. Major League Gaming has eSports arenas and studios across the nation. Robert Morris University has a League of Legends varsity team, whose members are eligible for scholarships.\n\nPlayers become fourth-party developers, allowing for more open source models of game design, development and engineering. Players also create modifications (mods), which in some cases become just as popular as the original game for which they were created. An example of this is the game \"Counter-Strike\", which began as a mod of the video game \"Half-Life\" and eventually became a very successful, published game in its own right.\n\nWhile this \"community of modifiers\" may only add up to approximately 1% of a particular game's user base, the number of those involved will grow as more games offer modifying opportunities (such as, by releasing source code) and the video user base swells. According to Ben Sawyer, as many as 600,000 established online game community developers existed as of 2012. This effectively added a new component to the game industry value chain and if it continues to mature, it will integrate itself into the overall industry.\n\nThe industry has seen a shift towards games with multiplayer facilities. A larger percentage of games on all types of platforms include some type of competitive online multiplayer capability.\n\nIn addition, the industry is experiencing further significant change driven by convergence, with technology and player comfort being the two primary reasons for this wave of industry convergence. Video games and related content can now be accessed and played on a variety of media, including: cable television, dedicated consoles, handheld devices and smartphones, through social networking sites or through an ISP, through a game developer's website, and online through a game console and/or home or office personal computer. In fact, 12% of U.S. households already make regular use of game consoles for accessing video content provided by online services such as Hulu and Netflix. In 2012, for the first time, entertainment usage passed multiplayer game usage on Xbox, meaning that users spent more time with online video and music services and applications than playing multiplayer games. This rapid type of industry convergence has caused the distinction between video game console and personal computers to disappear. A game console with high-speed microprocessors attached to a television set is, for all intents and purposes, a computer and monitor.\n\nAs this distinction has been diminished, players' willingness to play and access content on different platforms has increased. The growing video gamer demographic accounts for this trend, as former president of the Entertainment Software Association Douglas Lowenstein explained at the 10th E3 expo, \"Looking ahead, a child born in 1995, E3's inaugural year, will be 19 years old in 2014. And according to Census Bureau data, by the year 2020, there will be 174 million Americans between the ages of 5 and 44. That's 174 million Americans who will have grown up with PlayStations, Xboxes, and GameCubes from their early childhood and teenage years...What this means is that the average gamer will be both older and, given their lifetime familiarity with playing interactive games, more sophisticated and discriminating about the games they play.\"\n\nEvidence of the increasing player willingness to play video games across a variety of media and different platforms can be seen in the rise of casual gaming on smartphones, tablets, and social networking sites as 92% of all smartphone and tablet owners play games at least once a week, 45% play daily, and industry estimates predict that, by 2016, one-third of all global mobile gaming revenue will come from tablets alone. Apple's App Store alone has more than 90,000 game apps, a growth of 1,400% since it went online. In addition, game revenues for iOS and Android mobile devices now exceed those of both Nintendo and Sony handheld gaming systems combined.\n\n\n"}
{"id": "5545841", "url": "https://en.wikipedia.org/wiki?curid=5545841", "title": "Voting trust", "text": "Voting trust\n\nA voting trust is an arrangement whereby the shares in a company of one or more shareholders and the voting rights attached thereto are legally transferred to a trustee, usually for a specified period of time (the \"trust period\"). In some voting trusts, the trustee may also be granted additional powers (such as to sell or redeem the shares). At the end of the trust period, the shares would ordinarily be re-transferred to the beneficiary(ies), although in practice many voting trusts contain provisions for them to re-vested on the voting trusts with identical terms.\n\nVoting trusts were made popular in Delaware corporate law, but they have since been adopted widely by other states in the United States. They have also been extensively adopted in offshore jurisdictions.\n\nThere are several reasons why shareholders may wish to put a voting trust arrangement in place.\n\nSample Voting trust agreement\n"}
{"id": "76656", "url": "https://en.wikipedia.org/wiki?curid=76656", "title": "Workflow", "text": "Workflow\n\nA workflow consists of an orchestrated and repeatable pattern of business activity enabled by the systematic organization of resources into processes that transform materials, provide services, or process information. It can be depicted as a sequence of operations, the work of a person or group, the work of an organization of staff, or one or more simple or complex mechanisms.\n\nFrom a more abstract or higher-level perspective, workflow may be considered a view or representation of real work. The flow being described may refer to a document, service, or product that is being transferred from one step to another.\n\nWorkflows may be viewed as one fundamental building block to be combined with other parts of an organization's structure such as information technology, teams, projects and hierarchies.\n\nThe development of the concept of workflow occurred above a series of loosely defined, overlapping eras.\n\nThe modern history of workflows can be traced to Frederick Taylor and Henry Gantt, although the term \"workflow\" was not in usage as such during their lifetimes. One of the earliest usages of the term \"work flow\" was in a railway engineering journal from 1921.\n\nTaylor and Gantt launched the study of the deliberate, rational organization of work, primarily in the context of manufacturing. This gave rise to time and motion studies. Related concepts include job shops and queuing systems (Markov chains).\n\nThe 1948 book \"Cheaper by the Dozen\" introduced the emerging concepts to the context of family life.\n\nThe invention of the typewriter and the copier helped spread the study of the rational organization of labor from the manufacturing shop floor to the office. Filing systems and other sophisticated systems for managing physical information flows evolved. Several events likely contributed to the development of formalized information workflows. First, the field of optimization theory matured and developed mathematical optimization techniques. For example, Soviet mathematician and economist Leonid Kantorovich developed the seeds of linear programming in 1939 through efforts to solve a plywood manufacturer's production optimization issues. Second, World War II and the Apollo program drove process improvement forward with their demands for the rational organization of work.\n\nIn the post-war era, the work of W. Edwards Deming and Joseph M. Juran led to a focus on quality, initially in Japanese companies, and from the 1980s on a more global level, giving rise to a variety of movements ranging from total quality management to Six Sigma, then to more qualitative notions of business process re-engineering. Under the influence of the quality movement, workflows, in knowledge economy sectors as well as in manufacturing, became the subject of further scrutiny and optimization efforts. Acknowledgement of the dynamic and changing nature of the demands on workflows came in the form of recognition of the phenomena associated with critical paths and moving bottlenecks.\n\nA workflow management system (WfMS) is a software system for setting up, performing, and monitoring of a defined sequence of processes and tasks, with the broad goals of increasing productivity, reducing costs, becoming more agile, and improving information exchange within an organization. These systems may be process-centric or data-centric, and they may represent the workflow as graphical maps. The workflow management system may also include an extensible interface so that external software applications can be integrated and provide support for wide area workflows that provide faster response times and improved productivity.\n\nThe concept of workflow is closely related to several fields in operations research and other areas that study the nature of work, either quantitatively or qualitatively, such as artificial intelligence (in particular, the sub-discipline of AI planning) and ethnography. The term \"workflow\" is more commonly used in particular industries, such as in printing or professional domains such as clinical laboratories, where it may have particular specialized meanings.\n\n\nThe following examples illustrate the variety of workflows seen in various contexts:\n\n\n\nSeveral workflow improvement theories have been proposed and implemented in the modern workplace. These include:\n\n\nEvaluation of resources, both physical and human, is essential to evaluate hand-off points and potential to create smoother transitions between tasks.\n\nA workflow can usually be described using formal or informal flow diagramming techniques, showing directed flows between processing steps. Single processing steps or components of a workflow can basically be defined by three parameters:\n\nComponents can only be plugged together if the output of one previous (set of) component(s) is equal to the mandatory input requirements of the following component(s). Thus, the essential description of a component actually comprises only input and output that are described fully in terms of data types and their meaning (semantics). The algorithms' or rules' descriptions need only be included when there are several alternative ways to transform one type of input into one type of output – possibly with different accuracy, speed, etc.\n\nWhen the components are non-local services that are invoked remotely via a computer network, such as Web services, additional descriptors (such as QoS and availability) also must be considered.\n\nMany software systems exist to support workflows in particular domains. Such systems manage tasks such as automatic routing, partially automated processing, and integration between different functional software applications and hardware systems that contribute to the value-addition process underlying the workflow. There are also software suppliers using the technology process driven messaging service based upon three elements:\n\n\n\n\n"}
