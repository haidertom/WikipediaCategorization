{"id": "31137615", "url": "https://en.wikipedia.org/wiki?curid=31137615", "title": "Andrea Elizabeth Michaels", "text": "Andrea Elizabeth Michaels\n\nAndrea Elizabeth Michaels (née Gerson; born December 14, 1943) is an American meeting and event producer, author and speaker. As founder of an international event production company, Extraordinary Events, Michaels and her company produce large corporate events. She is an original member of the special events industry Hall of Fame and the author of a business biography, and is featured in three anthologies.\n\nAndrea Michaels was born in the Arbe concentration camp (today Rab, Croatia, on the northern shore of the Adriatic Sea) to Lelja Kauders (b. Zagreb, Yugoslavia, August 25, 1923 – d. Los Angeles California, May 30, 2009). Michaels and her mother, along with her mother's Jewish parents, Paul (Palvo) Kauders and Elza Ackerman Kauders, escaped in a small boat across the Adriatic to Italy. Michaels lived in Italy to the age of four with her grandparents until her mother, who had married an American pilot, sent for her to come live in America. The marriage ended, and after Michaels arrived in America, her mother married Peter Gerson, who adopted Andrea and moved the family from Silverlake, California, to Hollywood. When Michaels was ten, the family moved to Burbank, California.\n\nAn only child Michaels graduated from Burbank High School in 1961. Michaels attended the University of California at Berkeley in 1961 and 1962, transferring to the University of California at Los Angeles (UCLA) before marrying Bob Spritz on February 1, 1964. The marriage lasted ten years, and they had a son, Jon (b. July 24, 1970). After her divorce from Spritz, she changed her last name as well as that of her son's to \"Michaels.\" Michaels returned to UCLA in 1970 and earned a degree, with honors, in Psychology.\n\nInstead of pursuing a career in Psychology, Michaels began working with the Ron Rubin Orchestras in 1973. During that time, she became interested in the event planning business when there was no formalized industry. She produced her first event in 1978—-a bicentennial celebration for Marsh McClellan Insurance Company. By 1982, the company had been restructured and renamed Michaels, Rubin and Associates. Michaels and Rubin dissolved their relationship in December 1988, and on that same day, Extraordinary Events was launched. At that time, Michaels began to bring to the corporate world the spectaculars that had been produced on football fields, Disneyland and in entertainment venues by the late Tommy Walker and the late Robert (Bob) Jani.\n\nExtraordinary Events has specialized in planning and producing corporate events globally including Europe, Asia, Canada, Mexico, the Caribbean, Africa, Australia and New Zealand. In 2006, \"Special Events\" magazine listed Extraordinary Events as one of the \"Top 50 Event Planning Companies.\" The Los Angeles Business Journal named it Number 28 on its list of the \"Fastest Growing Private Companies\" in 2006, jumping from Number 80 in 2005 and then to Number 48 in 2005 and then to Number 48 in 2007.\n\nMichaels and her company began to incorporate charitable activities into its events with the rebuilding of an elementary school in Leona Vicario, Mexico, with client CEMEX, a concrete manufacturer, in 2005. The \"Los Angeles Business Journal\" and \"Financial and Insurance Meetings \" magazine covered the event.\n\nMichaels and Extraordinary Events have won the following awards:\n\nIn addition to her business biography, \"Reflections of a Successful Wallflower—Lessons in Business; Lessons in Life\", Michaels contributed and was featured in three business anthologies in 2010. They are: \"Stepping Stones to Success\", featuring Deepak Chopra, Jack Canfield, Andrea Michaels and Dr. Denis Waitley; \"Yes You Can! Reaching Your Potential While Achieving Greatness\", featuring Dr. Warren Bennis, Andrea Michaels and Jim Rohn and \"Bushido Business —The Fine Art of the Modern Professional\" with Tom Hopkins, Brian Tracy, Andrea Michaels and Stephen M. R. Covey.\n\nMichaels has authored several magazine articles. Two of the most recent ones were published in \"WE Magazine for Women\" and are entitled \"Growing a Business with Creative Thinking\", Part 1 and Part 2. Michaels is a regular contributor to \"Special Events\" magazine writing about her experiences speaking internationally. Michaels was also highlighted in Anaheim University's CEO Video Interview Series.\n\nMichaels has been the subject of numerous newspaper and magazine articles. A few include \"Smart Business,\" \"Antelope Valley Proz,\" \"Special Events\" magazine, \"Los Angeles Times,\" \"MICE Exchange,\" \"Southern California Meetings & Events,\" The \"Los Angeles Business Journal,\" \"Antelope Valley Proz,\" \"Special Events\" magazine, \"Los Angeles Times,\" \"MICE Exchange,\" \"Southern California Meetings & Events,\" The \"Los Angeles Business Journal,\" \"ExhibitorOnline,\" \"Event Solutions,\" \"All Business/Successful Meetings,\" and \"Meetings & Conventions.\"\n\nIn 2007, Michaels developed a mentor program to offer an in-the-field special event education and international exchange program to assist event newcomers. Michaels is currently a faculty member at California State University, Long Beach College of Continuing and Professional Education and teaches event management.\n\n\n"}
{"id": "488743", "url": "https://en.wikipedia.org/wiki?curid=488743", "title": "Carbon monoxide poisoning", "text": "Carbon monoxide poisoning\n\nCarbon monoxide poisoning typically occurs from breathing in carbon monoxide (CO) at excessive levels. Symptoms are often described as \"flu-like\" and commonly include headache, dizziness, weakness, vomiting, chest pain, and confusion. Large exposures can result in loss of consciousness, arrhythmias, seizures, or death. The classically described \"cherry red skin\" rarely occurs. Long term complications may include feeling tired, trouble with memory, and movement problems. In those exposed to smoke, cyanide toxicity should also be considered.\nCarbon monoxide poisoning can occur accidentally or as an attempt to end one's life. CO is a colorless and odorless gas which is initially non-irritating. It is produced during incomplete burning of organic matter. This can occur from motor vehicles, heaters, or cooking equipment that run on carbon-based fuels. It can also occur from exposure to methylene chloride. Carbon monoxide primarily causes adverse effects by combining with hemoglobin to form carboxyhemoglobin (HbCO) preventing the blood from carrying oxygen. Additionally, myoglobin and mitochondrial cytochrome oxidase are affected. Diagnosis is based on a HbCO level of more than 3% among nonsmokers and more than 10% among smokers.\nEfforts to prevent poisoning include carbon monoxide detectors, proper venting of gas appliances, keeping chimneys clean, and keeping exhaust systems of vehicles in good repair. Treatment of poisoning generally consists of giving 100% oxygen along with supportive care. This should generally be carried out until symptoms are no longer present and the HbCO level is less than 10%. While hyperbaric oxygen therapy is used for severe poisonings, the benefit over standard oxygen delivery is unclear. The risk of death among those affected is between 1 and 30%.\nCarbon monoxide poisoning is relatively common, resulting in more than 20,000 emergency department visits a year in the United States. It is the most common type of fatal poisoning in many countries. In the United States non-fire related cases results in more than 400 deaths a year. Poisonings occur more often in the winter, particularly from the use of portable generators during power outages. The toxic effects of CO have been known since ancient history. The realization that hemoglobin was affected by CO was determined in 1857.\n\nCarbon monoxide is not toxic to all forms of life. Its harmful effects are due to binding with hemoglobin so its danger to organisms that do not use this compound is doubtful. It thus has no effect on photosynthesising plants. It is easily absorbed through the lungs. Inhaling the gas can lead to hypoxic injury, nervous system damage, and even death. Different people and populations may have different carbon monoxide tolerance levels. On average, exposures at 100 ppm or greater is dangerous to human health. In the United States, the OSHA limits long-term workplace exposure levels to less than 50 ppm averaged over an 8-hour period; in addition, employees are to be removed from any confined space if an upper limit (\"ceiling\") of 100 ppm is reached. Carbon monoxide exposure may lead to a significantly shorter life span due to heart damage. The carbon monoxide tolerance level for any person is altered by several factors, including activity level, rate of ventilation, a pre-existing cerebral or cardiovascular disease, cardiac output, anemia, sickle cell disease and other hematological disorders, barometric pressure, and metabolic rate.\n\nThe main manifestations of carbon monoxide poisoning develop in the organ systems most dependent on oxygen use, the central nervous system and the heart. The initial symptoms of acute carbon monoxide poisoning include headache, nausea, malaise, and fatigue. These symptoms are often mistaken for a virus such as influenza or other illnesses such as food poisoning or gastroenteritis. Headache is the most common symptom of acute carbon monoxide poisoning; it is often described as dull, frontal, and continuous. Increasing exposure produces cardiac abnormalities including fast heart rate, low blood pressure, and cardiac arrhythmia; central nervous system symptoms include delirium, hallucinations, dizziness, unsteady gait, confusion, seizures, central nervous system depression, unconsciousness, respiratory arrest, and death. Less common symptoms of acute carbon monoxide poisoning include myocardial ischemia, atrial fibrillation, pneumonia, pulmonary edema, high blood sugar, lactic acidosis, muscle necrosis, acute kidney failure, skin lesions, and visual and auditory problems.\n\nOne of the major concerns following acute carbon monoxide poisoning is the severe delayed neurological manifestations that may occur. Problems may include difficulty with higher intellectual functions, short-term memory loss, dementia, amnesia, psychosis, irritability, a strange gait, speech disturbances, Parkinson's disease-like syndromes, cortical blindness, and a depressed mood. Depression may occur in those who did not have pre-existing depression. These delayed neurological sequelae may occur in up to 50% of poisoned people after 2 to 40 days. It is difficult to predict who will develop delayed sequelae; however, advanced age, loss of consciousness while poisoned, and initial neurological abnormalities may increase the chance of developing delayed symptoms.\n\nOne classic sign of carbon monoxide poisoning is more often seen in the dead rather than the living – people have been described as looking red-cheeked and healthy (see below). However, since this \"cherry-red\" appearance is common only in the deceased, and is unusual in living people, it is not considered a useful diagnostic sign in clinical medicine. In pathological (autopsy) examination the ruddy appearance of carbon monoxide poisoning is notable because unembalmed dead persons are normally bluish and pale, whereas dead carbon-monoxide poisoned persons may simply appear unusually lifelike in coloration. The colorant effect of carbon monoxide in such postmortem circumstances is thus analogous to its use as a red colorant in the commercial meat-packing industry.\n\nChronic exposure to relatively low levels of carbon monoxide may cause persistent headaches, lightheadedness, depression, confusion, memory loss, nausea, hearing disorders and vomiting. It is unknown whether low-level chronic exposure may cause permanent neurological damage. Typically, upon removal from exposure to carbon monoxide, symptoms usually resolve themselves, unless there has been an episode of severe acute poisoning. However, one case noted permanent memory loss and learning problems after a 3-year exposure to relatively low levels of carbon monoxide from a faulty furnace. Chronic exposure may worsen cardiovascular symptoms in some people. Chronic carbon monoxide exposure might increase the risk of developing atherosclerosis. Long-term exposures to carbon monoxide present the greatest risk to persons with coronary heart disease and in females who are pregnant. In experimental animals, carbon monoxide appears to worsen noise-induced hearing loss at noise exposure conditions that would have limited effects on hearing otherwise. In humans, hearing loss has been reported following carbon monoxide poisoning. Unlike the findings in animal studies, noise exposure was not a necessary factor for the auditory problems to occur.\n\nCarbon monoxide is a product of combustion of organic matter under conditions of restricted oxygen supply, which prevents complete oxidation to carbon dioxide (CO). Sources of carbon monoxide include cigarette smoke, house fires, faulty furnaces, heaters, wood-burning stoves, internal combustion vehicle exhaust, electrical generators, propane-fueled equipment such as portable stoves, and gasoline-powered tools such as leaf blowers, lawn mowers, high-pressure washers, concrete cutting saws, power trowels, and welders. Exposure typically occurs when equipment is used in buildings or semi-enclosed spaces.\n\nRiding in the back of pickup trucks has led to poisoning in children. Idling automobiles with the exhaust pipe blocked by snow has led to the poisoning of car occupants. Any perforation between the exhaust manifold and shroud can result in exhaust gases reaching the cabin. Generators and propulsion engines on boats, especially houseboats, has resulted in fatal carbon monoxide exposures.\n\nPoisoning may also occur following the use of a self-contained underwater breathing apparatus (SCUBA) due to faulty diving air compressors.\n\nIn caves carbon monoxide can build up in enclosed chambers due to the presence of decomposing organic matter. In coal mines incomplete combustion may occur during explosions resulting in the production of afterdamp. The gas is up to 3% CO and may be fatal after just a single breath. Following an explosion in a colliery, adjacent interconnected mines may become dangerous due to the afterdamp leaking from mine to mine. Such an incident followed the Trimdon Grange explosion which killed men in the Kelloe mine.\n\nAnother source of poisoning is exposure to the organic solvent dichloromethane, found in some paint strippers, as the metabolism of dichloromethane produces carbon monoxide.\n\nThe precise mechanisms by which the effects of carbon monoxide are induced upon bodily systems, are complex and not yet fully understood. Known mechanisms include carbon monoxide binding to hemoglobin, myoglobin and mitochondrial cytochrome oxidase and restricting oxygen supply, and carbon monoxide causing brain lipid peroxidation.\n\nCarbon monoxide has a higher diffusion coefficient compared to oxygen, and the only enzyme in the human body that produces carbon monoxide is heme oxygenase, which is located in all cells and breaks down heme. Under normal conditions, carbon monoxide levels in the plasma are approximately 0 mmHg because it has a higher diffusion coefficient and the body easily gets rid of any CO made. When CO is not ventilated it binds to hemoglobin, which is the principal oxygen-carrying compound in blood; this produces a compound known as carboxyhemoglobin. The traditional belief is that carbon monoxide toxicity arises from the formation of carboxyhemoglobin, which decreases the oxygen-carrying capacity of the blood and inhibits the transport, delivery, and utilization of oxygen by the body. The affinity between hemoglobin and carbon monoxide is approximately 230 times stronger than the affinity between hemoglobin and oxygen so hemoglobin binds to carbon monoxide in preference to oxygen.\n\nHemoglobin is a tetramer with four oxygen binding sites. The binding of carbon monoxide at one of these sites increases the oxygen affinity of the remaining three sites, which causes the hemoglobin molecule to retain oxygen that would otherwise be delivered to the tissue. This situation is described as carbon monoxide shifting the oxygen dissociation curve to the left. Because of the increased affinity between hemoglobin and oxygen during carbon monoxide poisoning, little oxygen will actually be released in the tissues. This causes hypoxic tissue injury. Hemoglobin acquires a bright red color when converted into carboxyhemoglobin, so poisoned cadavers and even commercial meats treated with carbon monoxide acquire an unnatural reddish hue.\n\nCarbon monoxide also binds to the hemeprotein myoglobin. It has a high affinity for myoglobin, about 60 times greater than that of oxygen. Carbon monoxide bound to myoglobin may impair its ability to utilize oxygen. This causes reduced cardiac output and hypotension, which may result in brain ischemia. A delayed return of symptoms have been reported. This results following a recurrence of increased carboxyhemoglobin levels; this effect may be due to a late release of carbon monoxide from myoglobin, which subsequently binds to hemoglobin.\n\nAnother mechanism involves effects on the mitochondrial respiratory enzyme chain that is responsible for effective tissue utilization of oxygen. Carbon monoxide binds to cytochrome oxidase with less affinity than oxygen, so it is possible that it requires significant intracellular hypoxia before binding. This binding interferes with aerobic metabolism and efficient adenosine triphosphate synthesis. Cells respond by switching to anaerobic metabolism, causing anoxia, lactic acidosis, and eventual cell death. The rate of dissociation between carbon monoxide and cytochrome oxidase is slow, causing a relatively prolonged impairment of oxidative metabolism.\n\nThe mechanism that is thought to have a significant influence on delayed effects involves formed blood cells and chemical mediators, which cause brain lipid peroxidation (degradation of unsaturated fatty acids). Carbon monoxide causes endothelial cell and platelet release of nitric oxide, and the formation of oxygen free radicals including peroxynitrite. In the brain this causes further mitochondrial dysfunction, capillary leakage, leukocyte sequestration, and apoptosis. The result of these effects is lipid peroxidation, which causes delayed reversible demyelinization of white matter in the central nervous system known as Grinker myelinopathy, which can lead to edema and necrosis within the brain. This brain damage occurs mainly during the recovery period. This may result in cognitive defects, especially affecting memory and learning, and movement disorders. These disorders are typically related to damage to the cerebral white matter and basal ganglia. Hallmark pathological changes following poisoning are bilateral necrosis of the white matter, globus pallidus, cerebellum, hippocampus and the cerebral cortex.\n\nCarbon monoxide poisoning in pregnant women may cause severe adverse fetal effects. Poisoning causes fetal tissue hypoxia by decreasing the release of maternal oxygen to the fetus. Carbon monoxide also crosses the placenta and combines with fetal hemoglobin, causing more direct fetal tissue hypoxia. Additionally, fetal hemoglobin has a 10 to 15% higher affinity for carbon monoxide than adult hemoglobin, causing more severe poisoning in the fetus than in the adult. Elimination of carbon monoxide is slower in the fetus, leading to an accumulation of the toxic chemical. The level of fetal morbidity and mortality in acute carbon monoxide poisoning is significant, so despite mild maternal poisoning or following maternal recovery, severe fetal poisoning or death may still occur.\n\nAs many symptoms of carbon monoxide poisoning also occur with many other types of poisonings and infections (such as the flu), the diagnosis is often difficult. A history of potential carbon monoxide exposure, such as being exposed to a residential fire, may suggest poisoning, but the diagnosis is confirmed by measuring the levels of carbon monoxide in the blood. This can be determined by measuring the amount of carboxyhemoglobin compared to the amount of hemoglobin in the blood.\n\nThe ratio of carboxyhemoglobin to hemoglobin molecules in an average person may be up to 5%, although cigarette smokers who smoke two packs per day may have levels up to 9%. In symptomatic poisoned people they are often in the 10–30% range, while persons who die may have postmortem blood levels of 30–90%.\n\nAs people may continue to experience significant symptoms of CO poisoning long after their blood carboxyhemoglobin concentration has returned to normal, presenting to examination with a normal carboxyhemoglobin level (which may happen in late states of poisoning) does not rule out poisoning.\n\nCarbon monoxide may be quantitated in blood using spectrophotometric methods or chromatographic techniques in order to confirm a diagnosis of poisoning in a person or to assist in the forensic investigation of a case of fatal exposure.\n\nA CO-oximeter can be used to determine carboxyhemoglobin levels. Pulse CO-oximeters estimate carboxyhemoglobin with a non-invasive finger clip similar to a pulse oximeter. These devices function by passing various wavelengths of light through the fingertip and measuring the light absorption of the different types of hemoglobin in the capillaries. The use of a regular pulse oximeter is not effective in the diagnosis of carbon monoxide poisoning as people with carbon monoxide poisoning may have a normal oxygen saturation level on a pulse oximeter. This is due to the carboxyhemoglobin being misrepresented as oxyhemoglobin.\n\nBreath CO monitoring offers an alternative to pulse CO-oximetry. Carboxyhemoglobin levels have been shown to have a strong correlation with breath CO concentration. However, many of these devices require the user to inhale deeply and hold their breath to allow the CO in the blood to escape into the lung before the measurement can be made. As this is not possible in people who are unresponsive, these devices may not appropriate for use in on-scene emergency care detection of CO poisoning.\n\nThere are many conditions to be considered in the differential diagnosis of carbon monoxide poisoning. The earliest symptoms, especially from low level exposures, are often non-specific and readily confused with other illnesses, typically flu-like viral syndromes, depression, chronic fatigue syndrome, chest pain, and migraine or other headaches. Carbon monoxide has been called a \"great mimicker\" due to the presentation of poisoning being diverse and nonspecific. Other conditions included in the differential diagnosis include acute respiratory distress syndrome, altitude sickness, lactic acidosis, diabetic ketoacidosis, meningitis, methemoglobinemia, or opioid or toxic alcohol poisoning.\n\nPrevention remains a vital public health issue, requiring public education on the safe operation of appliances, heaters, fireplaces, and internal-combustion engines, as well as increased emphasis on the installation of carbon monoxide detectors. Carbon monoxide is tasteless and odourless, and therefore can not be detected by visual cues or smell.\n\nThe United States Consumer Product Safety Commission has stated, \"carbon monoxide detectors are as important to home safety as smoke detectors are,\" and recommends each home have at least one carbon monoxide detector, and preferably one on each level of the building. These devices, which are relatively inexpensive and widely available, are either battery- or AC-powered, with or without battery backup. In buildings, carbon monoxide detectors are usually installed around heaters and other equipment. If a relatively high level of carbon monoxide is detected, the device sounds an alarm, giving people the chance to evacuate and ventilate the building. Unlike smoke detectors, carbon monoxide detectors do not need to be placed near ceiling level.\n\nThe use of carbon monoxide detectors has been standardized in many areas. In the US, NFPA 720-2009, the carbon monoxide detector guidelines published by the National Fire Protection Association, mandates the placement of carbon monoxide detectors/alarms on every level of the residence, including the basement, in addition to outside sleeping areas. In new homes, AC-powered detectors must have battery backup and be interconnected to ensure early warning of occupants at all levels. NFPA 720-2009 is the first national carbon monoxide standard to address devices in non-residential buildings. These guidelines, which now pertain to schools, healthcare centers, nursing homes and other non-residential buildings, include three main points:\n\nGas organizations will often recommend to get gas appliances serviced at least once a year.\n\nThe NFPA standard is not necessarily enforced by law. As of April 2006, the US state of Massachusetts requires detectors to be present in all residences with potential CO sources, regardless of building age and whether they are owner-occupied or rented. This is enforced by municipal inspectors, and was inspired by the death of 7-year-old Nicole Garofalo in 2005 due to snow blocking a home heating vent. Other jurisdictions may have no requirement or only mandate detectors for new construction or at time of sale.\n\nDespite similar deaths in vehicles with clogged exhaust pipes (for example in the Northeastern United States blizzard of 1978 and February 2013 nor'easter) and the commercial availability of the equipment, there is no legal requirement for automotive CO detectors.\n\nThe following guideline values (ppm values rounded) and periods of time-weighted average exposures have been determined in such a way that the carboxyhaemoglobin (COHb) level of 2.5% is not exceeded, even when a normal subject engages in light or moderate exercise:\n\nFor indoor air quality 7 mg/m3 (6 ppm) for 24 h (so as not to exceed 2% COHb for chronic exposure)\n\nInitial treatment for carbon monoxide poisoning is to immediately remove the person from the exposure without endangering further people. Those who are unconscious may require CPR on site. Administering oxygen via non-rebreather mask shortens the half-life of carbon monoxide from 320 minutes, when breathing normal air, to only 80 minutes. Oxygen hastens the dissociation of carbon monoxide from carboxyhemoglobin, thus turning it back into hemoglobin. Due to the possible severe effects in the fetus, pregnant women are treated with oxygen for longer periods of time than non-pregnant people.\n\nHyperbaric oxygen is also used in the treatment of carbon monoxide poisoning, as it may hasten dissociation of CO from carboxyhemoglobin and cytochrome oxidase to a greater extent than normal oxygen. Hyperbaric oxygen at three times atmospheric pressure reduces the half life of carbon monoxide to 23 (~80/3 minutes) minutes, compared to 80 minutes for oxygen at regular atmospheric pressure. It may also enhance oxygen transport to the tissues by plasma, partially bypassing the normal transfer through hemoglobin. However, it is controversial whether hyperbaric oxygen actually offers any extra benefits over normal high flow oxygen, in terms of increased survival or improved long-term outcomes. There have been randomized controlled trials in which the two treatment options have been compared; of the six performed, four found hyperbaric oxygen improved outcome and two found no benefit for hyperbaric oxygen. Some of these trials have been criticized for apparent flaws in their implementation. A review of all the literature concluded that the role of hyperbaric oxygen is unclear and the available evidence neither confirms nor denies a medically meaningful benefit. The authors suggested a large, well designed, externally audited, multicentre trial to compare normal oxygen with hyperbaric oxygen.\n\nFurther treatment for other complications such as seizure, hypotension, cardiac abnormalities, pulmonary edema, and acidosis may be required. Increased muscle activity and seizures should be treated with dantrolene or diazepam; diazepam should only be given with appropriate respiratory support. Hypotension requires treatment with intravenous fluids; vasopressors may be required to treat myocardial depression. Cardiac dysrhythmias are treated with standard advanced cardiac life support protocols. If severe, metabolic acidosis is treated with sodium bicarbonate. Treatment with sodium bicarbonate is controversial as acidosis may increase tissue oxygen availability. Treatment of acidosis may only need to consist of oxygen therapy. The delayed development of neuropsychiatric impairment is one of the most serious complications of carbon monoxide poisoning. Brain damage is confirmed following MRI or CAT scans. Extensive follow up and supportive treatment is often required for delayed neurological damage. Outcomes are often difficult to predict following poisoning, especially people who have symptoms of cardiac arrest, coma, metabolic acidosis, or have high carboxyhemoglobin levels. One study reported that approximately 30% of people with severe carbon monoxide poisoning will have a fatal outcome. It has been reported that electroconvulsive therapy (ECT) may increase the likelihood of delayed neuropsychiatric sequelae (DNS) after carbon monoxide (CO) poisoning.\n\nThe true number of cases of carbon monoxide poisoning is unknown, since many non-lethal exposures go undetected. From the available data, carbon monoxide poisoning is the most common cause of injury and death due to poisoning worldwide. Poisoning is typically more common during the winter months. This is due to increased domestic use of gas furnaces, gas or kerosene space heaters, and kitchen stoves during the winter months, which if faulty and/or used without adequate ventilation, may produce excessive carbon monoxide. Carbon monoxide detection and poisoning also increases during power outages, when electric heating and cooking appliances become inoperative and residents may temporarily resort to fuel-burning space heaters, stoves, and grills (some of which are safe only for outdoor use but nonetheless are errantly burned indoors).\n\nIt has been estimated that more than 40,000 people per year seek medical attention for carbon monoxide poisoning in the United States. 95% of carbon monoxide poisoning deaths in the United States are due to gas space heaters. In many industrialized countries carbon monoxide is the cause of more than 50% of fatal poisonings. In the United States, approximately 200 people die each year from carbon monoxide poisoning associated with home fuel-burning heating equipment. Carbon monoxide poisoning contributes to the approximately 5613 smoke inhalation deaths each year in the United States. The CDC reports, \"Each year, more than 500 Americans die from unintentional carbon monoxide poisoning, and more than 2,000 commit suicide by intentionally poisoning themselves.\" For the 10-year period from 1979 to 1988, 56,133 deaths from carbon monoxide poisoning occurred in the United States, with 25,889 of those being suicides, leaving 30,244 unintentional deaths. A report from New Zealand showed that 206 people died from carbon monoxide poisoning in the years of 2001 and 2002. In total carbon monoxide poisoning was responsible for 43.9% of deaths by poisoning in that country. In South Korea, 1,950 people had been poisoned by carbon monoxide with 254 deaths from 2001 through 2003. A report from Jerusalem showed 3.53 per 100,000 people were poisoned annually from 2001 through 2006. In Hubei, China, 218 deaths from poisoning were reported over a 10-year period with 16.5% being from carbon monoxide exposure.\n\nThe earliest description of carbon monoxide poisoning dates to at least 200 BC by Aristotle. Documented cases of carbon monoxide being used as a method of suicide date to at least 100 BC in ancient Rome. In the AD 350s, the Roman emperor Julian suffered from carbon monoxide poisoning in Paris, and later described it in his work \"Misopogon\": \"though the winter weather prevailed and continually increased in severity, even so I did not allow my servants to heat the house, because I was afraid of drawing out the dampness in the walls; but I ordered them to carry in fire that had burned down and to place in the room a very moderate number of hot coals. But the coals, though there were not very many of them, brought out from the walls quantities of steam and this made me fall asleep. And since my head was filled with the fumes I was almost choked. Then I was carried outside.\" This misunderstanding of the causes of carbon monoxide poisoning may have caused the death of Julian's successor, Jovian.\n\nJohn Scott Haldane identified carbon monoxide as the lethal constituent of afterdamp, the gas created by combustion, after examining many bodies of miners killed in pit explosions. Their skin was coloured cherry-pink from carboxyhaemoglobin, the stable compound formed in the blood by reaction with the gas. As a result of his research, he was able to design respirators for rescue workers. He tested the effect of carbon monoxide on his own body in a closed chamber, describing the results of his slow poisoning. In the late 1890s, he introduced the use of small animals for miners to detect dangerous levels of carbon monoxide underground, either white mice or canaries. With a faster metabolism, they showed the effects of poisoning before gas levels became critical for the workers, and so gave an early warning of the problem. The canary in British pits was replaced in 1986 by the electronic gas detector.\n\nAs part of the Holocaust during World War II, German Nazis used gas vans at Chelmno extermination camp and elsewhere to kill an estimated over 700,000 prisoners by carbon monoxide poisoning. This method was also used in the gas chambers of several death camps such as Treblinka, Sobibor and Belzec. Gassing with carbon monoxide started in action T4, the programme developed by the Nazis in Germany to murder the mentally ill and disabled people before the war started in earnest. The gas was supplied by IG Farben in pressurized cylinders and fed by tubes into the gas chambers built at various mental hospitals, such as Hartheim Euthanasia Centre. Many key personnel were recruited from the T4 programme to murder much larger numbers of people in the gas vans and the special gas chambers used in the death camps such as Treblinka. Exhaust fumes from tank engines for example, were used to supply the gas to the chambers.\n\nThe use of oxygen as treatment began in 1868. The use of hyperbaric oxygen in rats following poisoning was studied by Haldane in 1895 while its use in humans began in the 1960s.\n\nCarbon monoxide is produced naturally by the body as a byproduct of converting protoporphyrin into bilirubin. This carbon monoxide also combines with hemoglobin to make carboxyhemoglobin, but not at toxic levels.\n\nSmall amounts of CO are beneficial and enzymes exist that produce it at times of oxidative stress. Drugs are being developed to introduce small amounts of CO during certain kinds of surgery, these drugs are called carbon monoxide-releasing molecules.\n\n"}
{"id": "17866864", "url": "https://en.wikipedia.org/wiki?curid=17866864", "title": "Carol McCain", "text": "Carol McCain\n\nCarol Shepp McCain (born February 19, 1938) is a former political aide and event planner who was director of the White House Visitors Office during the Reagan administration. She was the first wife of United States Senator John McCain.\n\nCarol Shepp was born in Pennsylvania to Joseph, an insurance agent, and Mary Shepp.\nShepp grew up in Lansdowne, Pennsylvania, outside Philadelphia. She graduated from Lansdowne-Aldan High School in 1955, winning a scholarship award.\n\nShepp attended Centenary Junior College for Women in Hackettstown, New Jersey, beginning in 1956. There she majored in English.\n\nShepp first met John McCain while he was attending the United States Naval Academy in Annapolis from 1954 to 1958. But, in 1958, she married one of his midshipman classmates, Alasdair E. Swanson, who had been a football and basketball star there. She and Swanson, who became a Navy pilot, had two children, Douglas (born 1959) and Andrew (born 1962), and lived in Pensacola, Florida. The Swansons divorced in June 1964, after she sued him for infidelity.\n\nShepp met John McCain again when he was stationed at the Naval Air Basic Training Command at Pensacola in 1964, and after her divorce from Swanson, the two began dating. Her future husband frequently took training flights from Florida up to Philadelphia to see her on weekends.\n\nOn July 3, 1965, Shepp and McCain married in Philadelphia. The ceremony was held at the home of the family that owned the well-known Old Original Bookbinder's seafood restaurant in Philadelphia; one of the Bookbinder family members was a close friend of Shepp from college.\n\nFollowing the wedding, McCain's new husband adopted her two children. Together, they had a daughter, Sidney, born in September 1966.\n\nJohn McCain was shot down over North Vietnam on October 26, 1967; he would remain a prisoner of war for five and a half years. During her husband's captivity, McCain raised their children in Orange Park, Florida, with the assistance of friends and neighbors in the Navy-oriented community. She sent frequent letters and packages to him, few of which the North Vietnamese let through. She became active in the POW/MIA movement, while those around her wore POW bracelets with her husband's name and capture date engraved on them.\n\nWhile visiting family and friends in the Philadelphia area on Christmas Eve 1969, McCain skidded and crashed into a telephone pole as she was navigating an icy, snowy, isolated portion of Pennsylvania Route 320 near Gulph Mills, Pennsylvania, driving alone. She was thrown from her car into the snow, going into shock; she thought she would never be seen and would die there. Hours later she was found and taken to Bryn Mawr Hospital. She suffered two smashed legs, a broken pelvis, broken arm, and a ruptured spleen. She spent six months in the hospital and underwent 23 operations over the following two years in order to rebuild her legs with rods and pins, and had extensive physical therapy. During this time her daughter stayed with her parents in Landsdowne while her sons stayed with friends in Florida.\n\nMcCain did not tell her husband about the accident in her letters, believing he already had enough to worry about. The U.S. State Department contacted her surgeon the next day with a warning; as the doctor later said: Businessman and POW advocate Ross Perot paid for McCain's medical care. She remained grateful to Perot, later remarking: \"The military families are in Ross's heart and in his soul...There are millions of us who are extremely grateful to Ross Perot\". Years after her husband found out about Perot's help, he said \"we loved him for it\". McCain was interviewed on \"CBS Evening News\" in 1970 and said Christmas had no meaning for her without her husband but that she carried on with it for their children.\n\nMcCain and her husband were reunited upon his release from captivity on March 14, 1973. She was now four inches (ten centimeters) shorter, in a wheelchair or on crutches, and substantially heavier than when he had last seen her. He was also visibly hampered by his injuries and the mistreatment he had endured from the North Vietnamese.\n\nFollowing her husband's return from Vietnam, the McCains became frequent guests of honor at dinners hosted by Governor of California Ronald Reagan and his wife Nancy Reagan, and the two couples became friendly. McCain was the Clay County director for Reagan's 1976 presidential campaign as he sought the Republican Party nomination. Her husband's assignments as executive officer, then commanding officer, of A-7 attack squadron VA-174 at NAS Cecil Field saw the couple leading an active social life. Such engagements included entertaining other naval personnel at their Orange Park home and Ponte Vedra beach house. McCain's marriage, however, began to falter due to her husband's partying away from home and extramarital affairs.\n\nHer husband's next assignment was to the Senate Liaison Office within the Navy's Office of Legislative Affairs. The McCains separated briefly, then reunited. His job was aided by the social life the couple conducted, entertaining Navy, government, and other persons three to four nights a week at their Alexandria, Virginia, home. During this time she worked for Congressman John H. Rousselot. By 1979, the McCains were still living together.\n\nIn April 1979, John McCain began a relationship with Cindy Lou Hensley, an Arizona special education teacher and Hensley & Co. heiress. McCain's husband pushed to end their marriage, and friends described her as being in shock. The McCains stopped cohabitating in January 1980; he filed for divorce in February 1980, which she accepted. When asked by a friend what had gone wrong, she said, \"It's just one of those things.\" The uncontested divorce became official in Fort Walton Beach on April 2, 1980.\n\nHer ex-husband would later state that he felt the demise of his marriage was due to his \"selfishness and immaturity more than it was to Vietnam, and I cannot escape blame by pointing a finger at the war. The blame was entirely mine.\" Regarding her divorce, McCain said, \"The breakup of our marriage was not caused by my accident or Vietnam or any of those things. I don't know that it might not have happened if John had never been gone. I attribute it more to John turning 40 and wanting to be 25 again than I do to anything else.\" John McCain's biographer Robert Timberg wrote, however, \"Vietnam did play a part, perhaps not the major part, but more than a walk-on.\" Ross Perot gave his own assessment of the McCain divorce: \"After he came home, he walked with a limp, she [Carol McCain] walked with a limp. So he threw her over for a poster girl with big money from Arizona [Cindy McCain] and the rest is history.\" McCain's three children were initially upset with their father about the divorce, but later reconciled with him.\n\nThe divorce settlement afforded McCain full custody of her three children as well as alimony, child support, college tuition for their children, houses in Virginia and Florida, and lifelong financial support for her continuing medical treatment. She was sued by her former mother-in-law, Roberta McCain, in 1980 for return of personal property, with the suit settled out of court in 1981.\n\nDespite the divorce, McCain remained on good terms with her ex-husband, supporting him in his subsequent political campaigns. She refused to discuss her marriage with an election opponent of her ex-husband's in 1982 who was seeking negative information about him, telling the opponent that \"a gentleman never would have called.\" During his 2008 presidential campaign, McCain said of her former husband: \"He's a good guy. We are still good friends. He is the best man for president.\"\n\nMcCain moved to La Mesa, California, where she lived for several months with the family of top Reagan associate Edwin Meese. She became a personal assistant to Nancy Reagan in the fall of 1979, working with her as a press assistant on Ronald Reagan's 1980 presidential campaign, and then worked on the 1980 Republican National Convention. She was director of the 1981 Reagan inaugural ball, and as the Reagan administration began, she handled scheduling for the First Lady and the Reagan children.\n\nIn 1981 she became Director of the White House Visitors Office. There she planned tours and dealt with the pleas of different groups for the limited slots available. She also dealt with demands from Washington officials, including a dispute about tour slots between Nancy Reagan and New York Congressman Thomas Downey. Regarding the pressures of her job, she said cheerfully, \"I'm always in tears, but I love the job. I'm really having a ball.\" During the early 1980s recession, she declared that the White House tours were fully booked even when other Washington attractions saw declining attendance; her office processed well over one million visits a year. She was a well-liked presence on the Washington social scene.\n\nBetween 1981 and 1986, she greatly expanded the annual White House Easter Egg Roll, adding participatory activities and doubling the size of the crowds attending. The \"Washington Post\" likened her \"extravaganza-loving\" event style to that of Cecil B. DeMille. She was also involved in planning South Lawn State Arrival Ceremonies, as well as a national Christmas celebration.\n\nShe left the White House Visitors Office position in January 1987 to join Philadelphia-based We the People 200, Inc., which was the organization planning the celebration for the 200th anniversary of the United States Constitution that year. She was named programming director, part of We the People 200's senior management team. The bicentennial project was already troubled by lack of corporate financial sponsorship and persistent internal conflicts; the high salaries of McCain and other senior staff came under some criticism, but were defended by the organization's president as justified based upon age and experience.\n\nBy 1990, she was a spokesperson for Washington, Inc., a large event planning company. During 1991, she was a spokesperson for the Desert Storm Homecoming Foundation, which held a $12 million victory celebration and memorial in Washington in June 1991 following the conclusion of the Gulf War and Operation Desert Storm. She later worked in press relations for the National Soft Drink Association in Washington.\n\nIn 2003, McCain retired and moved to a bungalow in Virginia Beach. While she has had romantic relationships since her divorce, McCain has not remarried. A friend of the family, who was interviewed by the \"Washington Post\" in 2008, recounted McCain's reasoning why she never remarried: \"She had a lot of boyfriends. She was going out with one fellow who was so terrific. And I said: 'He's so in love with you. You'll have a terrific life together.' She said, 'No, I don't think so.' She's never fallen in love with anyone else. [John McCain] was a hard act to follow.\"\n"}
{"id": "174883", "url": "https://en.wikipedia.org/wiki?curid=174883", "title": "Catastrophism", "text": "Catastrophism\n\nCatastrophism was the theory that the Earth had largely been shaped by sudden, short-lived, violent events, possibly worldwide in scope. This was in contrast to uniformitarianism (sometimes described as gradualism), in which slow incremental changes, such as erosion, created all the Earth's geological features. Uniformitarianism held that the present was the key to the past, and that all geological processes (such as erosion) throughout the past were like those that can be observed now. Since the early disputes, a more inclusive and integrated view of geologic events has developed, in which the scientific consensus accepts that there were some catastrophic events in the geologic past, but these were explicable as extreme examples of natural processes which can occur.\n\nCatastrophism held that geological epochs had ended with violent and sudden natural catastrophes such as great floods and the rapid formation of major mountain chains. Plants and animals living in the parts of the world where such events occurred were made extinct, being replaced abruptly by the new forms whose fossils defined the geological strata. Some catastrophists attempted to relate at least one such change to the Biblical account of Noah's flood.\n\nThe concept was first popularised by the early 19th-century French scientist Georges Cuvier, who proposed that new life forms had moved in from other areas after local floods, and avoided religious or metaphysical speculation in his scientific writings.\n\nIn the early development of geology, efforts were made in a predominantly Christian western society to reconcile biblical narratives of Creation and the universal flood with new concepts about the processes which had formed the Earth. The discovery of other ancient flood myths was taken as explaining why the flood story was \"stated in scientific methods with surprising frequency among the Greeks\", an example being Plutarch's account of the Ogygian flood.\n\nThe leading scientific proponent of catastrophism in the early nineteenth century was the French anatomist and paleontologist Georges Cuvier. His motivation was to explain the patterns of extinction and faunal succession that he and others were observing in the fossil record. While he did speculate that the catastrophe responsible for the most recent extinctions in Eurasia might have been the result of the inundation of low-lying areas by the sea, he did not make any reference to Noah's flood. Nor did he ever make any reference to divine creation as the mechanism by which repopulation occurred following the extinction event. In fact Cuvier, influenced by the ideas of the Enlightenment and the intellectual climate of the French revolution, avoided religious or metaphysical speculation in his scientific writings. Cuvier also believed that the stratigraphic record indicated that there had been several of these revolutions, which he viewed as recurring natural events, amid long intervals of stability during the history of life on earth. This led him to believe the Earth was several million years old.\n\nBy contrast in Britain, where natural theology was influential during the early nineteenth century, a group of geologists including William Buckland and Robert Jameson interpreted Cuvier's work differently. Cuvier had written an introduction to a collection of his papers on fossil quadrupeds, discussing his ideas on catastrophic extinction. Jameson translated Cuvier's introduction into English, publishing it under the title \"Theory of the Earth\". He added extensive editorial notes to the translation, explicitly linking the latest of Cuvier's revolutions with the biblical flood. The resulting essay was extremely influential in the English-speaking world. Buckland spent much of his early career trying to demonstrate the reality of the biblical flood using geological evidence. He frequently cited Cuvier's work, even though Cuvier had proposed an inundation of limited geographic extent and extended duration, whereas Buckland, to be consistent with the biblical account, was advocating a universal flood of short duration. Eventually, Buckland abandoned flood geology in favor of the glaciation theory advocated by Louis Agassiz, following a visit to the Alps where Agassiz demonstrated the effects of glaciation at first hand. As a result of the influence of Jameson, Buckland, and other advocates of natural theology, the nineteenth century debate over catastrophism took on much stronger religious overtones in Britain than elsewhere in Europe.\n\nUniformitarian explanations for the formation of sedimentary rock and an understanding of the immense stretch of geological time, or as the concept came to be known deep time, were found in the writing of James Hutton, sometimes known as the father of geology, in the late 18th century. The geologist Charles Lyell built upon Hutton's ideas during the first half of 19th century and amassed observations in support of the uniformitarian idea that the Earth's features had been shaped by same geological processes that could be observed in the present acting gradually over an immense period of time. Lyell presented his ideas in the influential three volume work, \"Principles of Geology\", published in the 1830s, which challenged theories about geological cataclysms proposed by proponents of catastrophism like Cuvier and Buckland.\n\nFrom around 1850 to 1980, most geologists endorsed uniformitarianism (\"The present is the key to the past\") and gradualism (\"geologic change occurs slowly over long periods of time\") and rejected the idea that cataclysmic events such as earthquakes, volcanic eruptions, or floods of vastly greater power than those observed at the present time, played any significant role in the formation of the Earth's surface. Instead they believed that the earth had been shaped by the long term action of forces such as volcanism, earthquakes, erosion, and sedimentation, that could still be observed in action today. In part, the geologists' rejection was fostered by their impression that the catastrophists of the early nineteenth century believed that God was directly involved in determining the history of Earth. Some of the theories about Catastrophism in the nineteenth and early twentieth centuries were connected with religion and catastrophic origins were sometimes considered miraculous rather than natural events.\n\nThe rise in uniformitarianism made the introduction of a new catastrophe theory very difficult. In 1923 J Harlen Bretz published a paper on the channeled scablands formed by glacial Lake Missoula in Washington State, USA. Bretz encountered resistance to his theories from the geology establishment of the day, kicking off an acrimonious 40 year debate. Finally in 1979 Bretz received the Penrose Medal; the Geological Society of America's highest award.\n\nIn the 1950s, Immanuel Velikovsky propounded catastrophism in several popular books. He speculated that the planet Venus is a former \"comet\" which was ejected from Jupiter and subsequently 3,500 years ago made two catastrophic close passes by Earth, 52 years apart, and later interacted with Mars, which then had a series of near collisions with Earth which ended in 687 BCE, before settling into its current orbit. Velikovsky used this to explain the biblical plagues of Egypt, the biblical reference to the \"Sun standing still\" for a day (Joshua 10:12 & 13, explained by changes in Earth's rotation), and the sinking of Atlantis. Scientists vigorously rejected Velikovsky's conjectures.\n\nNeocatastrophism is the explanation of sudden extinctions in the palaeontological record by high magnitude, low frequency events (such as asteroid impacts, super-volcanic eruptions, supernova gamma ray bursts, etc.), as opposed to the more prevalent geomorphological thought which emphasises low magnitude, high frequency events.\n\nOver the past 25 years, a scientifically based catastrophism has gained wide acceptance with regard to certain events in the distant past. One impetus for this change came from the publication of a historic paper by Walter and Luis Alvarez in 1980. This paper suggested that a asteroid struck Earth 66 million years ago at the end of the Cretaceous period. The impact wiped out about 70% of all species, including the dinosaurs, leaving behind the Cretaceous–Paleogene boundary (K–T boundary). In 1990, a candidate crater marking the impact was identified at Chicxulub in the Yucatán Peninsula of Mexico.\n\nSince then, the debate about the extinction of the dinosaurs and other mass extinction events has centered on whether the extinction mechanism was the asteroid impact, widespread volcanism (which occurred about the same time), or some other mechanism or combination. Most of the mechanisms suggested are catastrophic in nature.\n\nThe observation of the Shoemaker-Levy 9 cometary collision with Jupiter illustrated that catastrophic events occur as natural events.\n\nOne of the key differences between catastrophism and uniformitarianism is that uniformitarianism requires the assumption of vast timelines, whereas catastrophism does not. Today most geologists combine catastrophist and uniformitarianist standpoints, taking the view that Earth's history is a slow, gradual story punctuated by occasional natural catastrophic events that have affected Earth and its inhabitants.\n\nModern theories also suggest that Earth's anomalously large moon was formed catastrophically. In a paper published in \"Icarus\" in 1975, William K. Hartmann and Donald R. Davis proposed that a catastrophic near-miss by a large planetesimal early in Earth's formation approximately 4.5 billion years ago blew out rocky debris, remelted Earth and formed the Moon, thus explaining the Moon's lesser density and lack of an iron core. The impact theory does have some faults; some computer simulations show the formation of a ring or multiple moons post impact, and elements are not quite the same between the earth and moon.\n\n\n\n\n"}
{"id": "26241411", "url": "https://en.wikipedia.org/wiki?curid=26241411", "title": "Chase's Calendar of Events", "text": "Chase's Calendar of Events\n\nChase's Calendar of Events is an annual American publication, started in 1957 by brothers William (Bill) D. Chase (a journalist and publisher from Michigan), and Harrison V. Chase (a university social scientist from Florida). It includes special events, holidays, federal and state observances, historic anniversaries, and more unusual celebratory traditions. Bill Chase worked as a newspaper librarian and saw a need for \"a single reference source for calendar dates, and for authoritative and current information about various observances throughout the year\".\n\nThe brothers gathered information on events and the first edition of 2,000 copies was printed for 1958. \"It was 32 pages, contained 364 entries and sold for $1\", while recent editions are 752 pages and contain more than 12,000 entries. A promotion sponsored by the US Chamber of Commerce was added in 1958: a pamphlet listed commercial promotions as Special Days, Weeks and Months, and remained in future editions. Contemporary Books in Chicago, Illinois, took over publication in 1983 and the Chases retired in 1987 from compiling the calendar, which is now handled by an in-house staff of editors and researchers. Contemporary Books was acquired by Tribune in 1993 and sold to McGraw-Hill Companies in September 2000. McGraw-Hill sold the property to Rowman and Littlefield Publishing Group in January 2015, where it is now published by Bernan Press, an imprint of RLPG. \nSections of the calendar include \n\n"}
{"id": "20261829", "url": "https://en.wikipedia.org/wiki?curid=20261829", "title": "Chronology of the Great Famine", "text": "Chronology of the Great Famine\n\nThe Chronology of the Great Famine ( or \"An Drochshaol\", litt: \"The Bad Life\") documents a period of Irish history between 1845 and 1852 during which time the population of Ireland was reduced by 20 to 25 percent. The proximate cause was famine resulting from a potato disease commonly known as late blight. Although blight ravaged potato crops throughout Europe during the 1840s, the impact and human cost in Irelandwhere a third of the population was entirely dependent on the potato for foodwas exacerbated by a host of political, social and economic factors which remain the subject of historical debate.\n\nAn important and controversial point of debate, that is notable amongst Irish people, was that Irish food supplies were sufficient until it was taken away by the British the feed the British, and the landlords kept economic benefits to themselves. Some commentators and writers controversially compared this explanation to \"genocide\".\n\nAt the beginning of August, Sir Robert Peel, the British Prime Minister, received news of a potato disease in the South of England. This was the first recorded evidence that the 'blight' which had ravaged the potato crop in North America in 1843-1844 had crossed the Atlantic. Cecil Woodham-Smith would write that a failure in England would be serious, but for Ireland, it would be a disaster.\n\nFollowing earlier reports of incidences of the blight in England, on 13 September 1845 potato blight was first reported in Ireland. The crops at Dublin were suddenly perishing, it was reported in the \"Gardeners' Chronicle\", asking \"where will Ireland be in the event of a universal potato rot?\" The British Government were nevertheless optimistic through the next few weeks.\n\n\nThe principle of the Corn Laws had been to keep the price of home-grown grain up. Duties on imported grain assured English farmers a minimum and profitable price. The burden of a higher price for bread was carried by the labouring classes, in particular factory workers and operatives. It was claimed that if the Corn Laws were repealed all those connected with the land would be ruined and the established social organisation of the country destroyed.\n\nAccording to Cecil Woodham-Smith, the rising wrath of Tories and landlords ensured \"all interest in Ireland was submerged.\" She writes that the Tory Mayor of Liverpool refused to call a meeting for the relief of Irish distress. She continues that the Mansion House Committee in Dublin was accused of 'deluding the public with a false alarm', and the blight itself 'was represented as the invention of agitators on the other side of the water'. The entanglement of the Irish famine with the repeal of the Corn Laws, she says, was a key misfortune for Ireland. The potato failure was eclipsed by the domestic issue of Corn Law repeal. The Irish famine, she writes, \"slipped into the background.\"\n\n\nThe first deaths from hunger took place in early 1846. In March Peel set up a programme of public works in Ireland but was forced to resign as Prime Minister on 29 June. The new Whig administration under Lord Russell, influenced by their laissez-faire belief that the market would provide the food needed then halted government food and relief works, leaving many hundreds of thousands of people without any work, money or food. Grain continued to be exported from the country. Private initiatives such as The Central Relief Committee of the Society of Friends (Quakers) attempted to fill the gap caused by the end of government relief and eventually the government reinstated the relief works, although bureaucracy slowed the release of food supplies. The blight almost totally destroyed the 1846 crop and the Famine worsened considerably. By December a third of a million destitute people were employed in public works.\n\nThere were average crop yields in the 1847 harvest, but due to lack of seed potatoes to plant, the crop was low. Crowds began to throng the public works during the last months of 1846 and the start of 1847, which promoted exactly the social conditions for the spread of 'famine fever.' In late January and February, legislation called the Temporary Relief Act went through the British parliament; it became popularly known as the Soup Kitchen Act and occasionally as Burgoyne's Act. This system of relief was designed to deliver cheap food directly and gratuitously to the destitute masses. This system of relief would be terminated in September. The government also announced an additional change in the system of relief. After August 1847, the permanent Poor Law was to be extended and was to become responsible for providing relief and as a result, all relief would be financed by the local Poor Law rates. This put impossible loads on local poor rates, particularly in the rural west and south. With the mass emigration of the famine era, the horrors of the 'coffin ships' and 1847 have ever since been associated in the popular mind, according to James S. Donnelly.\n\nIn December 1847 The Crime and Outrage Bill (Ireland) 1847 was enacted due to growing Irish nationalist agitation that was causing the British government concern about a possible violent rebellion against British rule in Ireland.\n\nThe bill gave the Lord Lieutenant of Ireland the power to organise the island into districts and bring police forces into them at the districts' expense. It limited who could own guns, and required all of the men in the district between the ages of 16 and 60 to assist in apprehending suspected murderers when landlords were killed, or else be guilty of a misdemeanour themselves.\n\nThe blight returned in 1848 and outbreaks of cholera were reported. Evictions became common among the Irish who could not keep up with the demands of their British landlords. Famine victims on outdoor relief peaked in July at almost 840,000 people. On 29 July an uprising against the government was led by William Smith O'Brien. After a skirmish at \"Widow McCormack's house\" in the village of Ballingarry, County Tipperary the leaders of the rebellion fled to America or were sentenced to transportation.\n\nThe potato crop failed again in 1849 and famine was accompanied by cholera outbreaks. This deadly cholera epidemic killed one of Ireland's greatest poets: James Clarence Mangan.\n\nThe Famine ended.\n\nBy 1851 census figures showed that the population of Ireland had fallen to 6,575,000 – a drop of 1,600,000 in ten years. Cormac Ó Gráda and Joel Mokyr have described the 1851 census as a famous but flawed source. They contend that the combination of institutional and individuals figures gives \"an incomplete and biased count\" of fatalities during the famine. The famine left in its wake up to a million dead and another million emigrated. The famine caused a sense of lasting bitterness by the Irish towards the British government, whom many blamed — then and now — for the starvation of so many people. The fall-out of the famine continued for decades afterwards.\n\n\n\n"}
{"id": "19167840", "url": "https://en.wikipedia.org/wiki?curid=19167840", "title": "Chronology of the universe", "text": "Chronology of the universe\n\nThe chronology of the universe describes the history and future of the universe according to Big Bang cosmology. The earliest stages of the universe's existence are estimated as taking place 13.8 billion years ago, with an uncertainty of around 21 million years at the 68% confidence level.\n\nFor the purposes of this summary, it is convenient to divide the chronology of the universe since it originated, into five parts. It is generally considered meaningless or unclear whether time existed before this chronology:\n\nEarliest stages of chronology shown below (before neutrino decoupling) are an active area of research and based on ideas which are still speculative and subject to modification as scientific knowledge improves.\n\n\"Time\" column is based on extrapolation of observed metric expansion of space back in the past. For the earliest stages of chronology this extrapolation may be invalid. To give one example, eternal inflation theories propose that inflation lasts forever throughout most of the universe, making the notion of \"N seconds since Big Bang\" ill-defined.\n\nThe radiation temperature refers to the cosmic background radiation and is given by 2.725·(1+\"z\"), where \"z\" is the redshift.\n\nThe Planck epoch is an era in traditional (non-inflationary) Big Bang cosmology immediately after the event which began our known universe. During this epoch, the temperature and average energies within the universe were so inconceivably high compared to any temperature we can observe today, that everyday subatomic particles could not form, and even the four fundamental forces that shape our universe—electromagnetism, gravitation, weak nuclear interaction, and strong nuclear interaction—were combined and formed one fundamental force. Little is understood about physics at this temperature; different hypotheses propose different scenarios. Traditional big bang cosmology predicts a gravitational singularity before this time, but this theory relies on the theory of general relativity, which is thought to break down for this epoch due to quantum effects.\n\nIn inflationary models of cosmology, times before the end of inflation (roughly 10 second after the Big Bang) do not follow the same timeline as in traditional big bang cosmology. Models that aim to describe the universe and physics during the Planck epoch are generally speculative and fall under the umbrella of \"New Physics\". Examples include the Hartle–Hawking initial state, string landscape, string gas cosmology, and the ekpyrotic universe.\n\nAs the universe expanded and cooled, it crossed transition temperatures at which forces separated from each other. These phase transitions can be visualised as similar to condensation and freezing phase transitions of ordinary matter. At certain temperatures/energies, water molecules change their behaviour and structure, and they will behave completely differently. Like steam turning to water, the fields which define our universe's fundamental forces and particles also completely change their behaviors and structures when the temperature/energy falls below a certain point. This is not apparent in everyday life, because it only happens at much, much, higher temperatures than we usually see in our present universe.\n\nThese phase transitions are believed to be caused by a phenomenon of quantum fields called \"symmetry breaking\".\n\nIn everyday terms, as the universe cools, it becomes possible for the quantum fields that create the forces and particles around us, to settle at lower energy levels and with higher levels of stability. In doing so, they completely shift how they interact. Forces and interactions arise due to these fields, so the universe can behave very differently above and below a phase transition. For example, in a later epoch, a side effect of one phase transition is that suddenly, many particles that had no mass at all acquire a mass (they begin to interact with the Higgs boson), and a single force begins to manifest as two separate forces.\n\nThe grand unification epoch began with a phase transitions of this kind, when gravitation separated from the universal combined gauge force. This caused two forces to now exist: gravity, and an electrostrong interaction. There is no hard evidence yet, that such a combined force existed, but many physicists believe it did. The physics of this electrostrong interaction would be described by a so-called grand unified theory (GUT).\n\nThe grand unification epoch ended with a second phase transition, as the electrostrong interaction in turn separated, and began to manifest as two separate interactions, called the strong and electroweak interactions.\n\nDepending on how epochs are defined, and the model being followed, the electroweak epoch may be considered to start before or after the inflationary epoch. In some models it is described as including the inflationary epoch. In other models, the electroweak epoch is said to begin after the inflationary epoch ended, at roughly 10 seconds.\n\nAccording to traditional big bang cosmology, the electroweak epoch began 10 seconds after the Big Bang, when the temperature of the universe was low enough (10 K) for the Electronuclear Force to begin to manifest as two separate interactions, called the strong and the electroweak interactions. (The electroweak interaction will also separate later, dividing into the electromagnetic and weak interactions). The exact point where electrostrong symmetry was broken is not certain, because of the very high energies of this event.\n\nAt this point, the very early universe suddenly and very rapidly expanded to at least 10 times its previous volume (and possibly much more). This is equivalent to a linear increase of at least 10 times in every spatial dimension – equivalent to an object 1 nanometer (10 m, about half the width of a molecule of DNA) in length, expanding to one approximately 10.6 light years (about 62 trillion miles) long in a tiny fraction of a second. This change is known as inflation.\n\nAlthough light and objects within spacetime cannot travel faster than the speed of light, in this case it was the metric governing the size and geometry of spacetime itself that changed in scale. Changes to the metric are not limited by the speed of light.\n\nIn some models, it is thought to have been triggered by the separation of the strong and electroweak interactions which ended the grand unification epoch. One of the theoretical products of this phase transition was a scalar field called the inflaton field. As this field settled into its lowest energy state throughout the universe, it generated an enormous repulsive force that led to a rapid expansion of space itself. Inflation explains several observed properties of the current universe that are otherwise difficult to account for, including explaining how today's universe has ended up so exceedingly homogeneous (similar) on a very large scale, even though it was highly disordered in its earliest stages.\n\nIt is not known exactly when the inflationary epoch ended, but it is thought to have been between 10 and 10 seconds after the Big Bang. The rapid expansion of space meant that elementary particles remaining from the grand unification epoch were now distributed very thinly across the universe. However, the huge potential energy of the inflation field was released at the end of the inflationary epoch, as the inflaton field decayed into other particles, known as \"reheating\". This heating effect led to the universe being repopulated with a dense, hot mixture of quarks, anti-quarks and gluons. In other models, reheating is often considered to mark the start of the electroweak epoch, and some theories, such as warm inflation, avoid a reheating phase entirely.\n\nIn non-traditional versions of Big Bang theory (known as \"inflationary\" models), inflation ended at a temperature corresponding to roughly 10 second after the Big Bang, but this does \"not\" imply that the inflationary era lasted less than 10 second. To explain the observed homogeneity of the universe, the duration in these models must be longer than 10 second. Therefore, in inflationary cosmology, the earliest meaningful time \"after the Big Bang\" is the time of the \"end\" of inflation. \n\nAfter inflation ended, the universe continued to expand, but at a very slow rate. The slow expansion began to speed up after several billion years, believed to be due to dark energy, and is still expanding today.\n\nOn March 17, 2014, astrophysicists of the BICEP2 collaboration announced the detection of inflationary gravitational waves in the B-mode power spectrum which was interpreted as clear experimental evidence for the theory of inflation. However, on June 19, 2014, lowered confidence in confirming the cosmic inflation findings was reported and finally, on February 2, 2015, a joint analysis of data from BICEP2/Keck and Planck satellite concluded that the statistical \"significance [of the data] is too low to be interpreted as a detection of primordial B-modes\" and can be attributed mainly to polarized dust in the Milky Way.\n\nAs the universe's temperature continued to fall below a certain very high energy level, a third symmetry breaking occurs. So far as we currently know, it was the final symmetry breaking event in the formation of our universe. It is believed that below some energies unknown yet, the Higgs field spontaneously acquires a vacuum expectation value. When this happens, it breaks electroweak gauge symmetry. This has two related effects:\n\n\nAfter electroweak symmetry breaking, the fundamental interactions we know of – gravitation, electromagnetism, the strong interaction and the weak interaction – have all taken their present forms, and fundamental particles have mass, but the temperature of the universe is still too high to allow the formation of many fundamental particles we now see in the universe.\n\nIf supersymmetry is a property of our universe, then it must be broken at an energy that is no lower than 1 TeV, the electroweak scale. The masses of particles and their superpartners would then no longer be equal. This very high energy could explain why no superpartners of known particles have ever been observed.\n\nAfter cosmic inflation ends, the universe is filled with a hot quark–gluon plasma, the remains of reheating. From this point onwards the physics of the early universe is much better understood, and the energies involved in the Quark epoch are directly amenable to experiment.\n\nThe quark epoch began approximately 10 seconds after the Big Bang. This was the period in the evolution of the early universe immediately after electroweak symmetry breaking, when the fundamental interactions of gravitation, electromagnetism, the strong interaction and the weak interaction had taken their present forms, but the temperature of the universe was still too high to allow quarks to bind together to form hadrons.\n\nDuring the quark epoch the universe was filled with a dense, hot quark–gluon plasma, containing quarks, leptons and their antiparticles. Collisions between particles were too energetic to allow quarks to combine into mesons or baryons.\n\nThe quark epoch ended when the universe was about 10 seconds old, when the average energy of particle interactions had fallen below the binding energy of hadrons.\n\n\"Perhaps by 10 seconds.\"\n\nBaryons are subatomic particles such as protons and neutrons, that are composed of three quarks. It would be expected that both baryons, and particles known as antibaryons would have formed in equal numbers. However, this does not seem to be what happened – as far as we know, the universe was left with far more baryons than antibaryons. In fact, almost no antibaryons are observed in nature. It is not clear how this came about. Any explanation for this phenomenon must allow the Sakharov conditions related to baryogenesis to have been satisfied at some time after the end of cosmological inflation. Current particle physics suggests asymmetries under which these conditions would be met, but these asymmetries appear to be too small to account for the observed baryon-antibaryon asymmetry of the universe.\n\nThe quark–gluon plasma that composes the universe cools until hadrons, including baryons such as protons and neutrons, can form.\nInitially, hadron/anti-hadron pairs could form, so matter and anti-matter were in thermal equilibrium. However, as the temperature of the universe continued to fall, new hadron/anti-hadron pairs were no longer produced, and most of the newly formed hadrons and anti-hadrons annihilated each other, giving rise to pairs of high-energy photons. A comparatively small residue of hadrons remained at about 1 second of cosmic time, when this epoch ended. \n\nTheory predicts that about 1 neutron remained for every 7 protons. We believe this to be correct because, at a later stage, all the neutrons and some of the protons fused, leaving hydrogen, a hydrogen isotope called deuterium, helium and other elements, which we can measure. A 1:7 ratio of hadrons at the end of this epoch would indeed produce the observed element ratios in the early as well as current universe.\n\nAt approximately 1 second after the Big Bang neutrinos decouple and begin traveling freely through space. As neutrinos rarely interact with matter, these neutrinos still exist today, analogous to the much later cosmic microwave background emitted during recombination, around 377,000 years after the Big Bang. The neutrinos from this event have a very low energy, around 10 times smaller than is possible with present-day direct detection. Even high energy neutrinos are notoriously difficult to detect, so this cosmic neutrino background (CNB) may not be directly observed in detail for many years, if at all.\n\nHowever, Big Bang cosmology makes many predictions about the CNB, and there is very strong indirect evidence that the cosmic neutrino background exists, both from Big Bang nucleosynthesis predictions of the helium abundance, and from anisotropies in the cosmic microwave background. One of these predictions is that neutrinos will have left a subtle imprint on the cosmic microwave background (CMB). It is well known that the CMB has irregularities. Some of the CMB fluctuations were roughly regularly spaced, because of the effect of baryonic acoustic oscillations. In theory, the decoupled neutrinos should have had a very slight effect on the phase of the various CMB fluctuations.\n\nIn 2015, it was reported that such shifts had been detected in the CMB. Moreover, the fluctuations corresponded to neutrinos of almost exactly the temperature predicted by Big Bang theory ( compared to a prediction of 1.95K), and exactly three types of neutrino, the same number of neutrino flavours currently predicted by the Standard Model.\n\nPrimordial black holes are a hypothetical type of black hole proposed in 1966, that may have formed during the so-called \"radiation dominated era\", due to the high densities and inhomogeneous conditions within the first second of cosmic time. Random fluctuations could lead to some regions becoming dense enough to undergo gravitational collapse, forming black holes. Current understandings and theories place tight limits on the abundance and mass of these objects.\n\nTypically, primordial black hole formation requires density contrasts (regional variations in the Universe's density) of around formula_1 (10%), where formula_2 is the average density of the Universe. Several mechanisms could produce dense regions meeting this criterion during the early universe, including reheating, cosmological phase transitions and (in so-called \"hybrid inflation models\") axion inflation. Since primordial black holes didn't form from stellar gravitational collapse, their masses can be far below stellar mass (~2×10 g). Stephen Hawking calculated in 1971 that primordial black holes could weigh as little as 10 g. But they can have any size, so they could also be large, and may have contributed to the formation of galaxies.\n\nThe majority of hadrons and anti-hadrons annihilate each other at the end of the hadron epoch, leaving leptons (such as the electron, muons and certain neutrinos) and anti-leptons, dominating the mass of the universe. \n\nThe lepton epoch follows a similar path to the earlier hadron epoch. Initially leptons and anti-leptons are produced in pairs. About 10 seconds after the Big Bang the temperature of the universe falls to the point at which new lepton/anti-lepton pairs are no longer created and most remaining leptons and anti-leptons quickly annihilate each other, giving rise to pairs of high energy photons, and leaving a small residue of non-annihilated leptons.\n\nAfter most leptons and anti-leptons are annihilated at the end of the lepton epoch, most of the mass-energy in the universe is left in the form of photons. (Much of the rest of its mass-energy is in the form of neutrinos and other relativistic particles). Therefore the energy of the universe, and its overall behavior, is dominated by its photons. These photons continue to interact frequently with charged protons, electrons and (eventually) nuclei. They continue to do so for about the next 377,000 years.\n\nBetween about 2 and 20 minutes after the Big Bang, the temperature and pressure of the universe allow nuclear fusion to occur, giving rise to nuclei of a few light elements beyond hydrogen (\"Big Bang nucleosynthesis\"). About 25% of the protons, and all the neutrons fuse to form deuterium, a hydrogen isotope, and most of the deuterium quickly fuses to form helium-4. \n\nAtomic nuclei will easily unbind (break apart) above a certain temperature, related to their binding energy. From about 2 minutes, the falling temperature means that deuterium no longer unbinds, and is stable, and starting from about 3 minutes, helium and other elements formed by the fusion of deuterium also no longer unbind and are stable.\n\nThe short duration and falling temperature means that only the simplest and fastest fusion processes can occur. Only tiny amounts of nuclei beyond helium are formed, because nucleosynthesis of heavier elements is difficult and requires thousands of years even in stars. Small amounts of tritium (another hydrogen isotope) and beryllium-7 and -8 are formed, but these are unstable and are quickly lost again. A small amount of deuterium is left unfused because of the very short duration.\n\nTherefore, the only stable nuclides created by the end of Big Bang nucleosynthesis are protium (single proton/hydrogen nucleus), deuterium, helium-3, helium-4, and lithium-7. By mass, the resulting matter is about 75% hydrogen nuclei, 25% helium nuclei, and perhaps 10 by mass of Lithium-7. The next most common stable isotopes produced are lithium-6, beryllium-9, boron-11, carbon, nitrogen and oxygen (\"CNO\"), but these have predicted abundances of between 5 and 30 parts in 10 by mass, making them essentially undetectable and negligible.\n\nThe amounts of each light element in the early universe can be estimated from old galaxies, and is strong evidence for the Big Bang. For example, the Big Bang should produce about 1 neutron for every 7 protons, allowing for 25% of all nucleons to be fused into helium-4 (2 protons and 2 neutrons out of every 16 nucleons), and this is the amount we find today, and far more than can be easily explained by other processes. Similarly, deuterium fuses extremely easily; any alternative explanation must also explain how conditions existed for deuterium to form, but also left some of that deuterium unfused and not immediately fused again into helium. Any alternative must also explain the proportions of the various light elements and their isotopes. A few isotopes, such as lithium-7, were found to be present in amounts that differed from theory, but over time, these differences have been resolved by better observations.\n\nUntil now, the universe's large scale dynamics and behavior have been determined mainly by radiation – meaning, those constituents that move relativistically (at or near the speed of light), such as photons and neutrinos. As the universe cools, from around 47,000 years (z=3600), the universe's large scale behavior becomes dominated by matter instead. This occurs because the energy density of matter begins to exceed both the energy density of radiation and the vacuum energy density. Around or shortly after this time, the densities of non-relativistic matter (atomic nuclei) and relativistic radiation (photons) become equal, the Jeans length, which determines the smallest structures that can form (due to competition between gravitational attraction and pressure effects), begins to fall and perturbations, instead of being wiped out by free-streaming radiation, can begin to grow in amplitude.\n\nAccording to the Lambda-CDM model, by this stage, the matter in the universe is around 84.5% cold dark matter and 15.5% \"ordinary\" matter. (However the total matter in the universe is only 31.7%, much smaller than the 68.3% of dark energy). There is overwhelming evidence that dark matter exists and dominates our universe, but since the exact nature of dark matter is still not understood, Big Bang theory does not presently cover any stages in its formation.\n\nFrom this point on, and for several billion years to come, the presence of dark matter accelerates the formation of structure in our universe. In the early universe, dark matter gradually gathers in huge filaments under the effects of gravity. This amplifies the tiny inhomogeneities (irregularities) in the density of the universe which was left by cosmic inflation. Over time, slightly denser regions become denser and slightly rarefied (emptier) regions become more rarefied. Ordinary matter eventually gathers together faster than it would otherwise do, because of the presence of these concentrations of dark matter.\n\nAbout 377,000 years after the Big Bang, two connected events occurred: recombination and photon decoupling. Recombination describes the ionized particles combining to form the first neutral atoms, and decoupling refers to the photons released (\"decoupled\") as the newly formed atoms settle into more stable energy states.\n\nJust before recombination, the baryonic matter in the universe was at a temperature where it formed a hot ionized plasma. Most of the photons in the universe interacted with electrons and protons, and could not travel significant distances without interacting with ionized particles. As a result, the universe was opaque or \"foggy\". Although there was light, it was not possible to see, nor can we observe that light through telescopes.\n\nAt around 377,000 years, the universe has cooled to a point where free electrons can combine with the hydrogen and helium nuclei to form neutral atoms. This process is relatively fast (and faster for the helium than for the hydrogen), and is known as recombination. The name is slightly inaccurate and is given for historical reasons: in fact the electrons and atomic nuclei were combining for the first time.\n\nDirectly combining in a low energy state (ground state) is less efficient, so these hydrogen atoms generally form with the electrons still in a high energy state, and once combined, the electrons quickly release energy in the form of one or more photons as they transition to a low energy state. This release of photons is known as photon decoupling. Some of these decoupled photons are captured by other hydrogen atoms, the remainder remain free. By the end of recombination, most of the protons in the universe have formed neutral atoms. This change from charged to neutral particles means that the mean free path photons can travel before capture in effect becomes infinite, so any decoupled photons that have not been captured can travel freely over long distances (see Thomson scattering). The universe has become transparent to visible light, radio waves and other electromagnetic radiation for the first time in its history.\nThe photons released by these newly formed hydrogen atoms initially had a temperature/energy of around ~ 4000 K. This would have been visible to the eye as a pale yellow/orange tinted, or \"soft\", white color. Over billions of years since decoupling, as the universe has expanded, the photons have been red-shifted from visible light to radio waves (microwave radiation corresponding to a temperature of about 2.7 K). Red shifting describes the photons acquiring longer wavelengths and lower frequencies as the universe expanded over billions of years, so that they gradually changed from visible light to radio waves. These same photons can still be detected as radio waves today. They form the cosmic microwave background (\"CMB\"), and they provide crucial evidence of the early universe and how it developed.\n\nAround the same time as recombination, existing pressure waves within the electron-baryon plasma – known as baryon acoustic oscillations – became embedded in the distribution of matter as it condensed, giving rise to a very slight preference in distribution of large-scale objects. Therefore, the cosmic microwave background is a picture of the universe at the end of this epoch including the tiny fluctuations generated during inflation (see diagram), and the spread of objects such as galaxies in the universe is an indication of the scale and size of the universe as it developed over time.\n\nAfter recombination and decoupling, the universe was transparent and had cooled enough to allow light to travel long distances, but there were no light-producing structures such as stars and galaxies. Stars and galaxies are formed when dense regions of gas form due to the action of gravity, and this takes a long time within a near-uniform density of gas and on the scale required, so it is estimated that stars did not exist for perhaps hundreds of millions of years after recombination.\n\nThis period, known as the Dark Ages, began around 377,000 years after the Big Bang. During the Dark Ages, the temperature of the universe cooled from some 4000 K down to about 60 K, and only two sources of photons existed: the photons released during recombination/decoupling (as neutral hydrogen atoms formed), which we can still detect today as the cosmic microwave background (CMB), and photons occasionally released by neutral hydrogen atoms, known as the 21 cm spin line of neutral hydrogen. The hydrogen spin line is in the microwave range of frequencies, and within 3 million years, the CMB photons had redshifted out of visible light to infrared; from that time until the first stars, there were no visible light photons. Other than perhaps some rare statistical anomalies, the universe was truly dark.\n\nThe October 2010 discovery of UDFy-38135539, the first observed galaxy to have existed during the following reionization epoch, gives us a window into these times. The galaxy earliest in this period observed and thus also the most distant galaxy ever observed is currently on the record of Leiden University's Richard J. Bouwens and Garth D. Illingsworth from UC Observatories/Lick Observatory. They found the galaxy UDFj-39546284 to be at a time some 480 million years after the Big Bang or about halfway through the Cosmic Dark Ages at a distance of about 13.2 billion light-years. More recently, the UDFy-38135539, EGSY8p7 and GN-z11 galaxies were found to be around 380–550 million years after the Big Bang and at a distance of around 13.4 billion light-years. There is also currently an observational effort underway to detect the faint 21 cm spin line radiation, as it is in principle an even more powerful tool than the cosmic microwave background for studying the early universe.\n\nStructures may have begun to emerge from around 150 million years, and stars and early galaxies gradually emerged from around 400 to 700 million years. As they emerged, the Dark Ages gradually ended. Because this process was gradual, the Dark Ages only fully ended around 1 billion (1000 million) years, as the universe took its present appearance.\n\nFor about 6.6 million years, between about 10 to 17 million years after the Big Bang (redshift 137–100), the background temperature was between 373 K and 273 K, a temperature compatible with liquid water and common biological chemical reactions. Loeb (2014) speculated that primitive life might in principle have appeared during this window, which he called \"the Habitable Epoch of the Early Universe\". Loeb argues that carbon-based life might have evolved in a hypothetical pocket of the early universe that was dense enough both to generate at least one massive star that subsequently releases carbon in a supernova, and that was also dense enough to generate a planet. (Such dense pockets, if they existed, would have been extremely rare.) Life would also have required a heat differential, rather than just uniform background radiation; this could be provided by naturally-occurring geothermal energy. Such life would likely have remained primitive; it is highly unlikely that intelligent life would have had sufficient time to evolve before the hypothetical oceans freeze over at the end of the habitable epoch.\n\nThe matter in the universe is around 84.5% cold dark matter and 15.5% \"ordinary\" matter. Since the start of the matter-dominated era, the dark matter has gradually been gathering in huge spread out (diffuse) filaments under the effects of gravity. Ordinary matter eventually gathers together faster than it would otherwise do, because of the presence of these concentrations of dark matter. It is also slightly more dense at regular distances due to early baryon acoustic oscillations (BAO) which became embedded into the distribution of matter when photons decoupled. Unlike dark matter, ordinary matter can lose energy by many routes, which means that as it collapses, it can lose the energy which would otherwise hold it apart, and collapse more quickly, and into denser forms. Ordinary matter gathers where dark matter is denser, and in those places it collapses into clouds of mainly hydrogen gas. The first stars and galaxies form from these clouds. Where numerous galaxies have formed, galaxy clusters and superclusters will eventually arise. Large voids with few stars will develop between them, marking where dark matter became less common.\n\nStructure formation in the big bang model proceeds hierarchically, due to gravitational collapse, with smaller structures forming before larger ones. The earliest structures to form are the first stars (known as population III stars), dwarf galaxies, and quasars (which are thought to be bright, early active galaxies containing a supermassive black hole surrounded by a inward-spiralling accretion disk of gas). Before this epoch, the evolution of the universe could be understood through linear cosmological perturbation theory: that is, all structures could be understood as small deviations from a perfect homogeneous universe. This is computationally relatively easy to study. At this point non-linear structures begin to form, and the computational problem becomes much more difficult, involving, for example, \"N\"-body simulations with billions of particles. The Bolshoi Cosmological Simulation is a high precision simulation of this era.\n\nThese Population III stars are also responsible for turning the few light elements that were formed in the Big Bang (hydrogen, helium and small amounts of lithium) into many heavier elements. They can be huge as well as perhaps small – and non-metallic (no elements except hydrogen and helium). The larger stars have very short lifetimes compared to most Main Sequence stars we see today, so they commonly finish burning their hydrogen fuel and explode as supernovae after mere millions of years, seeding the universe with heavier elements over repeated generations. They mark the start of the Stelliferous (starry) era.\n\nAs yet, no Population III stars have been found, so our understanding of them is based on computational models of their formation and evolution. Fortunately, observations of the Cosmic Microwave Background radiation can be used to date when star formation began in earnest. Analysis of such observations made by the European Space Agency's Planck telescope in 2016 concluded that the first generation of stars formed 700 million years after the Big Bang.\n\nQuasars provides some additional evidence of early structure formation. Their light shows evidence of elements such as carbon, magnesium, iron and oxygen. This is evidence that by the time quasars formed, a massive phase of star formation had already taken place, including sufficient generations of population III stars to give rise to these elements.\n\nAs the first stars, dwarf galaxies and quasars gradually form, the intense radiation they emit reionizes much of the surrounding universe; splitting the neutral hydrogen atoms back into a plasma of free electrons and protons for the first time since recombination and decoupling.\n\nReionization is evidenced from observations of quasars. Quasars are a form of active galaxy, and the most luminous objects observed in the universe. Electrons in neutral hydrogen have a specific patterns of absorbing photons, related to electron energy levels and called the Lyman series. Ionized hydrogen does not have electron energy levels of this kind. Therefore, light travelling through ionized hydrogen and neutral hydrogen shows different absorption lines. In addition, the light will have travelled for billions of years to reach us, so any absorption by neutral hydrogen will have been redshifted by varied amounts, rather than by one specific amount, indicating when it happened. These features make it possible to study the state of ionization at many different times in the past. They show that reionization began as \"bubbles\" of ionized hydrogen which became larger over time. They also show that the absorption was due to the general state of the universe (the intergalactic medium) and not due to passing through galaxies or other dense areas. Reionization might have started as early as \"z\"=16 (250 million years of cosmic time) and was complete by around \"z\"=9 or 10 (500 million years). The epoch of reionization probably ended by around \"z\"=5 or 6 (1 billion years) as the era of Population III stars and quasars – and their intense radiation – came to an end, and the ionized hydrogen gradually reverted to neutral atoms.\n\nThese observations have narrowed down the period of time during which reionization took place, but the source of the photons that caused reionization is still not completely certain. To ionize neutral hydrogen, an energy larger than 13.6 eV is required, which corresponds to ultraviolet photons with a wavelength of 91.2 nm or shorter, implying that the sources must have produced significant amount of ultraviolet and higher energy. Protons and electrons will recombine if energy is not continuously provided to keep them apart, which also sets limits on how numerous the sources were and their longevity. With these constraints, it is expected that quasars and first generation stars and galaxies were the main sources of energy. The current leading candidates from most to least significant are currently believed to be population III stars (the earliest stars) (possibly 70%), dwarf galaxies (very early small high-energy galaxies) (possibly 30%), and a contribution from quasars (a class of active galactic nuclei).\n\nHowever, by this time, matter had become far more spread out due to the ongoing expansion of the universe. Although the neutral hydrogen atoms were again ionized, the plasma was much more thin and diffuse, and photons were much less likely to be scattered. Despite being reionized, the universe remained largely transparent during reionization. As the universe continued to cool and expand, reionization gradually ended.\n\nMatter continues to draw together under the influence of gravity, to form galaxies. The stars from this time period, known as Population II stars, are formed early on in this process, with more recent Population I stars formed later. Gravitational attraction also gradually pulls galaxies towards each other to form groups, clusters and superclusters. The Hubble Ultra Deep Field observatory has identified a number of small galaxies merging to form larger ones, at 800 million years of cosmic time (13 billion years ago) (this age estimate is now believed to be slightly overstated).\n\nJohannes Schedler's project has identified a quasar CFHQS 1641+3755 at 12.7 billion light-years away, when the universe was just 7% of its present age. On July 11, 2007, using the 10-metre Keck II telescope on Mauna Kea, Richard Ellis of the California Institute of Technology at Pasadena and his team found six star forming galaxies about 13.2 billion light years away and therefore created when the universe was only 500 million years old. Only about 10 of these extremely early objects are currently known. More recent observations have shown these ages to be shorter than previously indicated. The most distant galaxy observed as of October 2016, GN-z11, has been reported to be 32 billion light years away, a vast distance made possible through space-time expansion (redshift z=11.1; comoving distance of 32 billion light-years; lookback time of 13.4 billion years).\n\nThe universe has appeared much the same as it does now, for many billions of years. It will continue to look similar for many more billions of years into the future.\n\nBased upon the emerging science of nucleocosmochronology, the Galactic thin disk of the Milky Way is estimated to have been formed 8.8 ± 1.7 billion years ago.\n\nFrom about 9.8 billion years of cosmic time, the universe's large-scale behavior is believed to have gradually changed for the third time in its history. Its behavior had originally been dominated by radiation (relativistic constituents such as photons and neutrinos) for the first 47,000 years, and since about 377,000 years of cosmic time, its behavior had been dominated by matter. During its matter-dominated era, the expansion of the universe had begun to slow down, as gravity reigned in the initial outward expansion. But from about 9.8 billion years of cosmic time, observations show that that the expansion of the universe slowly stops decelerating, and gradually begins to accelerate again, instead. \n\nWhile the precise cause is not known, the observation is accepted as correct by the cosmologist community. By far the most accepted understanding is that this is due to an unknown form of energy which has been given the name \"dark energy\". \"Dark\" in this context means that it is not directly observed, but can currently only be studied by examining the effect it has on the universe. Research is ongoing to understand this dark energy. Dark energy is now believed to be the single largest component of the universe, as it constitutes about 68.3% of the entire mass-energy of the physical universe.\n\nDark energy is believed to act like a cosmological constant - a scalar field that exists throughout space. Unlike gravity, the effects of such a field do not diminish (or only diminish slowly) as the universe grows. While matter and gravity have a greater effect initially, their effect quickly diminishes as the universe continues to expand. Objects in the universe, which are initially seen to be moving apart as the universe expands, continue to move apart, but their outward motion gradually slows down. This slowing effect becomes smaller as the universe becomes more spread out. Eventually, the outward and repulsive effect of dark energy begins to dominate over the inward pull of gravity. Instead of slowing down and perhaps beginning to move inward under the influence of gravity, from about 9.8 billion years of cosmic time, the expansion of space starts to slowly accelerate \"outward\" at a gradually \"increasing\" rate.\n\nThe universe has existed for around 13.8 billion years, and we believe that we understand it well enough to predict its large-scale development for many billions of years into the future – perhaps as much as 100 billion years of cosmic time (about 86 billion years from now). Beyond that, we need to better understand the universe to make any accurate predictions. Therefore, the universe could follow a variety of different paths beyond this time.\n\nThere are several competing scenarios for the possible long-term evolution of the universe. Which of them will happen, if any, depends on the precise values of physical constants such as the cosmological constant, the possibility of proton decay, the energy of the vacuum (meaning, the energy of \"empty\" space itself), and the natural laws beyond the Standard Model.\nIf the expansion of the universe continues and it stays in its present form, eventually all but the nearest galaxies will be carried away from us by the expansion of space at such a velocity that our observable universe will be limited to our own gravitationally bound local galactic cluster. In the very long term (after many trillions – thousands of billions – of years, cosmic time), the Stelliferous Era will end, as stars cease to be born and even the longest-lived stars gradually die. Beyond this, all objects in the universe will cool and (with the possible exception of protons) gradually decompose back to their constituent particles and then into subatomic particles and very low level photons and other fundamental particles, by a variety of possible processes. But this will take a duration of time that is almost inconceivable to most people, compared to which the entire 13.8 billion years of the universe would be a tiny instant in time.\n\nUltimately, in the extreme future, the following scenarios have been proposed for the ultimate fate of the universe.\n\nIn this kind of extreme timescale, extremely rare quantum phenomena may also occur that are extremely unlikely to be seen on a timescale smaller than trillions of years. These may also lead to unpredictable changes to the state of the universe which would not be likely to be significant on any smaller timescale. For example, on a timescale of millions of trillions of years, black holes might appear to evaporate almost instantly, uncommon quantum tunneling phenomena would appear to be common, and quantum (or other) phenomena so unlikely that they might occur just once in a trillion years may occur many times.\n\n"}
{"id": "2154755", "url": "https://en.wikipedia.org/wiki?curid=2154755", "title": "Dates of Epoch-Making Events", "text": "Dates of Epoch-Making Events\n\nDates of Epoch-Making Events is an entry in The Nuttall Encyclopaedia for its listing of the most important turning points in history, particularly western history. The work's list illustrates western culture's turning points and James Wood's views from the early 20th century. The events are listed as in the original listing, with modern footnotes.\n\nThe events chosen, with few errors, are:\n\n\n"}
{"id": "53395686", "url": "https://en.wikipedia.org/wiki?curid=53395686", "title": "David Monn", "text": "David Monn\n\nDavid Monn is an American event planner, interior designer, author, and artist. He is the founder and CEO of David Monn, LLC, an event production firm based in New York City. As part of the firm, Monn has been responsible for organizing and producing a variety of prominent events at locations like the Metropolitan Museum of Art, the New York Public Library, Guggenheim Museum, among others.\n\nMonn designed a White House State Dinner for former United States President Barack Obama. Monn's most recent book, \"David Monn: The Art of Celebrating\", was released in November 2016.\n\nMonn was born and raised in Fayetteville, Pennsylvania as one of six children. As a child, he showed an early interest in fashion design with hopes of attending the Parsons School of Design. In 1981 at age 18, Monn earned a job at Butte Knit, a clothing factory in his town that was a part of the Jonathan Logan Corporation. This job soon led him to New York City where he rented a studio apartment near Mindy Grossman's. While in New York, Monn took up interior decorating for himself and his friends (including Grossman).\n\nPrior to beginning his event and interior design career, Monn worked in the jewelry business for 12 years. In 2003, Gayfryd Steinberg hired Monn as the event planner and designer for the Library Lions Gala at the New York Public Library's main campus. Since then, Monn has been designing events for a wide variety of occasions. In 2004, he managed the festivities for the grand opening of the Time Warner Center. The following year, Monn was responsible for the design of the Metropolitan Museum of Art's Costume Institute Gala. By 2007, his clients included the Guggenheim Museum, LVMH, and the Metropolitan Opera (among others). In November 2007, Monn designed the Frank Gehry-inspired Guggenheim International Gala at Pier 40. He also designed the 100th anniversary celebration for the Plaza Hotel that year.\n\nIn May 2010, Monn designed the White House State Dinner honoring then Mexican President Felipe Calderón. In November 2016, he released his book, \"David Monn: The Art of Celebrating\", which contains 360 pages of images and text about events he has planned and designed. The book's release date was marked with a party at the New York Public Library's main campus. In January 2017, Monn designed an opening candlelight dinner for the inauguration festivities of President Donald Trump.\n\n"}
{"id": "52454436", "url": "https://en.wikipedia.org/wiki?curid=52454436", "title": "Devendrar Jayanti", "text": "Devendrar Jayanti\n\nDevendrar Jayanthi is a festival celebrated annually on 11 September. It is organised by various political parties, community organisations and civil societies in Southern Tamil Nadu to commemorate the memory of Immanuvel Devendrar, a freedom fighter and Indian National Congress leader who took part in the Quit India movement during the British Raj era. He worked in the Indian Army in independent India and later he fought for the rights of depressed class people of Southern Tamil Nadu, particularly for upliftment of Pallar caste. He was a close associate of chief minister K. Kamaraj.\n\nImmanuel Devendrar was a member of the Indian National Congress and a close associate of its leader, K. Kamaraj. At that time, Congress was supported by influential communities of southern Tamil Nadu, such as the Pallars and Nadars, and opposed by the Forward Block, which was itself favoured by the Maravar community. The two sides clashed during elections. Violence erupted notably in the districts of Ramanathapuram and Madurai which claimed over 40 lives. Devendrar was killed at this time and the anniversary of his death is today celebrated by various political parties and community organisations and civil societies as Devendrar Jayanthi.\n\nDevendrar Jayanti is among the celebrations sometimes criticised as being designed mainly as expressions of caste pride and political power, which sometimes turn into inter-caste violence. The critics claim that political parties used them as a form of vote bank politics.\n\nThese critics are rebutted by people who say that every caste has its own leader and the people are celebrating the memories of their leaders. Despite of these arguments every year the government is struggling to maintain the law and order during these celebrations.\n"}
{"id": "23081696", "url": "https://en.wikipedia.org/wiki?curid=23081696", "title": "Digitas NewFront", "text": "Digitas NewFront\n\nThe Digitas NewFront is an annual marketing conference first held during Internet Week New York. Hosted by Digitas and its brand content entity, The Third Act, the event's stated purpose is to \"bring together content creators, distributors, talent and brands to harness creative media opportunities made possible in the age of digital marketing\". In 2009, it was broadcast via public webcast. The event was produced by Amanda Anderson.\n\nFirst launched in 2008, the event was titled \"Digital Content NewFront\". The event title was later shortened to \"The NewFront\" with the stated mission to \"explore and help evolve the brand content and distribution landscape\". The event showcased online video content companies like 60 Frames, MySpaceTV, MTV New Media, Generate, Next New Networks and Vuguru. Speakers and panelists included Michael Eisner, CEO of Tornate and Vuguru, former Disney CEO and Dmitry Shapiro, the founder of Veoh Networks.\n\nIn 2018, the NewFronts ran for one week rather than two.\n\n\n\n\n\n\n\n\n"}
{"id": "8137", "url": "https://en.wikipedia.org/wiki?curid=8137", "title": "Disaster", "text": "Disaster\n\nA disaster is a serious disruption, occurring over a relatively short time, of the functioning of a community or a society involving widespread human, material, economic or environmental loss and impacts, which exceeds the ability of the affected community or society to cope using its own resources.\n\nIn contemporary academia, disasters are seen as the consequence of inappropriately managed risk. These risks are the product of a combination of both hazards and vulnerability. Hazards that strike in areas with low vulnerability will never become disasters, as in the case of uninhabited regions.\n\nDeveloping countries suffer the greatest costs when a disaster hits – more than 95 percent of all deaths caused by hazards occur in developing countries, and losses due to natural hazards are 20 times greater (as a percentage of GDP) in developing countries than in industrialized countries.\n\nThe word \"disaster\" is derived from Middle French \"désastre\" and that from Old Italian \"disastro\", which in turn comes from the Ancient Greek pejorative prefix δυσ-, (\"dus-\") \"bad\" and ἀστήρ (\"aster\"), \"star\". The root of the word \"disaster\" (\"bad star\" in Greek) comes from an astrological sense of a calamity blamed on the position of planets.\n\nResearchers have been studying disasters for more than a century, and for more than forty years disaster research. The studies reflect a common opinion when they argue that all disasters can be seen as being human-made, their reasoning being that human actions before the strike of the hazard can prevent it developing into a disaster. All disasters are hence the result of human failure to introduce appropriate emergency management measures. Hazards are routinely divided into natural or human-made, although complex disasters, where there is no single root cause, are more common in developing countries. A specific disaster may spawn a secondary disaster that increases the impact. A classic example is an earthquake that causes a tsunami, resulting in coastal flooding.\n\nA natural disaster is a natural process or phenomenon that may cause loss of life, injury or other health impacts, property damage, loss of livelihoods and services, social and economic disruption, or environmental damage.\n\nVarious phenomena like earthquakes, landslides, volcanic eruptions, floods, hurricanes, tornadoes, blizzards, tsunamis, and cyclones are all natural hazards that kill thousands of people and destroy billions of dollars of habitat and property each year. However, the rapid growth of the world's population and its increased concentration often in hazardous environments has escalated both the frequency and severity of disasters. With the tropical climate and unstable landforms, coupled with deforestation, unplanned growth proliferation, non-engineered constructions make the disaster-prone areas more vulnerable. Developing countries suffer more or less chronically from natural disasters due to ineffective communication combined with insufficient budgetary allocation for disaster prevention and management. Asia tops the list of casualties caused by natural hazards.\n\nHuman-instigated disasters are the consequence of technological hazards. Examples include stampedes, fires, transport accidents, industrial accidents, oil spills and nuclear explosions/radiation. War and deliberate attacks may also be put in this category. As with natural hazards, man-made hazards are events that have not happened—for instance, terrorism. Man-made disasters are examples of specific cases where man-made hazards have become reality in an event.\n\n"}
{"id": "1986491", "url": "https://en.wikipedia.org/wiki?curid=1986491", "title": "Disaster area", "text": "Disaster area\n\nA disaster area is a region or a locale, heavily damaged by either natural, technological or social hazards. Disaster areas affect the population living in the community by dramatic increase in expense, loss of energy, food and services; and finally increase the risk of disease for citizens. An area that has been struck with a natural, technological or sociological hazard that opens the affected area for national or international aid.\n\nA natural hazard is a negative process of phenomena created naturally (tornadoes, hurricanes, tsunamis, floods, earthquakes) that will affect people or the environment.\n\nTornadoes are narrow aggressively rotating mixtures of air that come from the base of a thunderstorm, being the most violent of storms. Tornadoes are usually hard to see unless they form a condensations funnel made from:- dust, water droplets and debris. Tornadoes take place in several parts of the world, such as Australia, Europe, Africa but mostly occur in the United States, Argentina, and Bangladesh.\n\nHurricanes, cyclones, tropical storms and typhoons can be referred to as the same. They combine low pressure and strong winds that rotate counter clockwise in the northern hemisphere and clockwise in the southern hemisphere. Tropical cyclones have a low pressure center, rain and strong winds. They usually initiate over tropical or subtropical waters. Hurricanes can be predicted several days before they hit and can be very destructive and destroy homes and some other buildings. Cyclones, although they are the same type of storm, are harder to predict before a few days like hurricanes thus giving people only a few hours of notice to evacuate their homes. In this case there are far more deaths from flooding and high tides. A recent (2012) example of a hurricane is Hurricane Sandy, which was the most devastating storm in decades hitting the United States, leaving millions without power and a few homeless.\n\nFloods take place when water overflows or submerges land that is usually parched. The most common way is when rivers or streams overflow their banks. A floodplain is produced when water from a rivers spreads through the land from excessive rain, rapid ice melting, unfortunately placed beaver dam, and ruptured dam. There are two types of floods: general and flash floods. General floods are predicted well in advance and usually cause the destruction of housing, people and crops. Flash floods come without warning and are sudden and extreme: A large volume of water flows rapidly and people have to make quick movements if they do not want to be caught in the flood. They have to find high safe ground where the water will not reach them.\n\nWhen two blocks of the earth suddenly slip past each other in the fault of the earth, it is called an earthquake. Energy released in many forms moves in all directions and causes the ground to shake. Sometimes earthquakes may have foreshocks, which are smaller earthquakes that occur in the same region which is followed by a larger earthquake. The larger earthquake, called the mainshock, always has aftershocks that follow it. Aftershocks can continue of hours, weeks, months and sometimes even years depending on how big the mainshock was. Earthquakes usually occur on active faults which define major tectonic plates on the Earth. 90% of the world’s earthquakes occur along plate boundaries. Earthquakes can cause much damage, mainly from the ground shaking and leaving cracks in the ground. Sometimes it can also cause buildings to collapse and cause deaths.\n\nThere are several technical hazards one should be aware of as they pose a threat humans and their values. These hazards are measured in terms of the risk they pose to the community using them. Technical hazards are classified by a source, specifically speaking; they can fall under automotive emissions, medical radiation, explosions and air pollution (environmental) hazards. The quality of the hazard determines the safety precautions that are taken. For example, hazards can be a risk to an individual or a risk to the population. If the population is at risk with the hazard, there will be more priorities for the management of the hazardous material. These hazardous materials can cause illness or even death to an individual if they are not address accordingly, so it is crucial for them to inform the public about these technical hazards.\n\nThere are several thoughts that come to mind when one thinks of the word “nuclear”, whether it be basic chemistry or highly complex explosives; nuclear accidents, nuclear incidences or nuclear terrorism are definitely a threat to the community or the world in that matter. But what is nuclear terrorism exactly? A \"nuclear accident, incident, or act of terrorism is an unpredictable, unusual and unwanted event involving radiation and/or radioactive materials.\" To specifically distinguish between the three, nuclear accidents are not deliberate and viewed as acts of nature. Nuclear incidents on the other hand are causes that include deliberate actions but these are “generally non-malicious and non-violent; may be due to poor judgement [or] wrong information.” \n\nNuclear accidents are not deliberate and viewed as acts of nature. There are several examples of these nuclear accidents taken place around the world. To start off, a prime example of a nuclear accident would be Palo mares B-52 Accident. On January 17, 1966, a U.S. Air Force B-52 bomber collided with a mid-tanker airplane that was being refueled at 31000 feet. When the two airplanes collided, the tanker airplane exploded completely killing all crew members, while the B-52 bomber split in half killing a small proportion of the crew. When the airplane split in half, four bombs dropped from the sky. Two of them detonated, causing 2 kilometers of land to be contaminated by radioactive plutonium. The fourth explosive was recovered when it was discovered that it landed on the sea. The soil on the contaminated area had to be removed and placed in barrels, to reduce the amount of pollution having been caused by the nuclear explosive.\n\nNuclear incidents are causes that include deliberate actions but these are “generally non-malicious and non-violent; may be due to poor judgement [or] wrong information.” A Secondary example would be the Johnston Atoll Incident that occurred on July 25, 1962. They decided to conduct the experiment at Christmas Island on Johnson Atoll, where they set off 36 nuclear explosions. One of the missile launches went wrong as it malfunctioned when trying to launch. The leaders decided to detonate the missile before it launched. When it detonated, the entire island was covered with radioactive plutonium. The witnesses claimed that 85% of the people suffered from radioactive contamination that created cancer and other radioactive related diseases. Plus those who were present at the site of the explosion suffered from infertility, and other body deformations.\n\nSociological hazards that create a disaster area are riots, terrorism, and war.\n\nA riot is defined as a noisy, violent public disturbance caused by a group or a crowd (three or more people) usually protesting against another group or government policy in the streets. The UK London riot in August 2011, for example, was started due to the shooting of Mark Duggan by the London police. The rioters came together destroying neighborhoods and streets violently damaging property to protest the police's actions that happened a few days prior. Riots increase expenses to repair costly damages putting the city in distress.\n\nTerrorism is defined as acts of violence and threats by a group against people or property with the intention of intimidating or coercing societies or governments, often for ideological or political reasons. Terrorism occurs with an unexpected attack on non-combatants to create fear and panic having a detrimental consequence. Terrorist attacks create a massive, costly impact on the society. Not only is there large amount of property damage that may not be able to be repaired, there is also a large impact on citizens. People lose loved ones and suffer from their own health being impacted. The 9/11 terrorist attack on the twin towers in New York City marks one of the largest attacks on the United States. The Twin Towers were completely destroyed, damaging the surrounding buildings as well as causing the loss of many lives. People suffered from health problems from inhaling the sediments from the crumbling towers. The financial and social impact is still present thirteen years later in today's society. It is concluded that New York City still suffers from a post-disaster decline in financial services. New York City is a Disaster area due to the sociological hazard of terrorism.\n\nWar defined as a period where conflict is carried out as an act of hostility by armed forces between two or more nations or within a nation. Afghanistan, for example would be a sociological hazard causing disaster areas because it is considered a war zone. There has been an ongoing battle between United States and the Taliban in Afghanistan resulting in a war zone. Constant bombing and shooting marks up Afghanistan destructing property, land and causing threats to the civilians living in the area. Thousands have died and international aid for the cost of the war is implemented through tax payers. There is lack of resources due to the war area cutting off access to areas because of violence and danger.\n\nAn example of a technological disaster was the Fukushima disaster which was caused by a “massive 8.9-magnitude earthquake [that] hit northeast Japan”. This earthquake caused a nuclear explosion at a power plant; five reactors were damaged, causing the plant to go into an emergency state. All this occurred because there was a technological error in the system that cut down the regular and emergency power, causing the five reactors to destabilize and explode. This significant nuclear event had a big impact on the public health, as the area suffered nuclear contamination. The contamination caused all the crops such as milk, water or vegetables unsafe to eat. Hence all food grown in that area was banned from being sold. People in the “surroundings were moved to safe shelters,” and 3 people were affected by the radiation alone. Luckily, the “Japanese government [handled] the situation in the most efficient and amazing way that anyone can imagine.”\n\nA recent example of a disaster area caused by a natural hazard is Hurricane Sandy which hit October 27, 2012. It was the most devastating storm in decades hitting the United States. The storm killed about 50 people and many were also hit by falling trees. The hardest-hit state was New York, leaving millions without power and a few homeless. \n\nA disaster area caused by a sociological hazard would be the terrorist attack on September 11th, 2001, in New York City. Two airplanes struck the Twin Towers, causing them to crumble, killing many people in the process. The unexpected attack harmed many people and had a detrimental impact on New York City.\n\n\n"}
{"id": "2770842", "url": "https://en.wikipedia.org/wiki?curid=2770842", "title": "Event (philosophy)", "text": "Event (philosophy)\n\nIn philosophy, events are objects in time or instantiations of properties in objects.\n\nJaegwon Kim theorized that events are structured.<br>\nThey are composed of three things:\nEvents are defined using the operation [x, P, t].<br>\nA unique event is defined by two principles:<br>\nThe existence condition states “[x, P, t] exists if and only if object x exemplifies the n-adic P at time t”. This means a unique event exists if the above is met. The identity condition states “[x, P, t] is [y, Q, t`] if and only if x=y, P=Q and t=t`].\n\nKim uses these to define events under five conditions:\n\nOther problems exist within Kim’s theory, as he never specified what properties were (e.g. universals, tropes, natural classes, etc.). In addition, it is not specified if properties are few or abundant. The following is Kim’s response to the above.\n\nThere is also a major debate about the essentiality of a constitutive object. There are two major questions involved in this: If one event occurs, could it have occurred in the same manner if it were another person, and could it occur in the same manner if it would have occurred at a different time? Kim holds that neither are true and that different conditions (i.e. a different person or time) would lead to a separate event. However, some consider it natural to assume the opposite.\n\nDonald Davidson and John Lemmon proposed a theory of events that had two major conditions, respectively: a causal criterion and a spatiotemporal criterion.\n\nThe causal criterion defines an event as two events being the same if and only if they have the same cause and effect.\n\nThe spatiotemporal criterion defines an event as two events being the same if and only if they occur in the same space at the same time. Davidson however provided this scenario; if a metal ball becomes warmer during a certain minute, and during the same minute rotates through 35 degrees, must we say that these are the same event? However, one can argue that the warming of the ball and the rotation are possibly temporally separated and are therefore separate events.\n\nDavid Lewis theorized that events are merely spatiotemporal regions and properties (i.e. membership of a class). He defines an event as “e is an event only if it is a class of spatiotemporal regions, both thisworldly (assuming it occurs in the actual world) and otherworldly.” The only problem with this definition is it only tells us what an event could be, but does not define a unique event. This theory entails modal realism, which assumes possible worlds exist; worlds are defined as sets containing all objects that exist as a part of that set. However, this theory is controversial. Some philosophers have attempted to remove possible worlds, and reduce them to other entities. They hold that the world we exist in is the only world that actually exists, and that possible worlds are only possibilities.\n\nLewis’ theory is composed of four key points. Firstly, the non-duplication principle; it states that x and y are separate events if and only if there is one member of x that is not a member of y (or vice versa). Secondly, there exist regions that are subsets of possible worlds and thirdly, events are not structured by an essential time.\n\nIn \"Being and Event\", Alain Badiou writes that the event (\"événement\") is a multiple which basically does not make sense according to the rules of the \"situation,\" in other words existence. Hence, the event \"is not,\" and therefore, in order for there to be an event, there must be an \"intervention\" which changes the rules of the situation in order to allow that particular event to be (\"to be\" meaning to be a multiple which belongs to the multiple of the situation — these terms are drawn from or defined in reference to set theory). In his view, there is no \"one,\" and everything that a \"multiple.\" \"One\" happens when the situation \"counts,\" or accounts for, acknowledges, or defines something: it \"counts it as one.\" For the event to be counted as one by the situation, or counted in the one of the situation, an intervention needs to decide its belonging to the situation. This is because his definition of the event violates the prohibition against self-belonging (in other words, it is a set-theoretical definition which violates set theory's rules of consistency), thus does not count as extant on its own.\n\nGilles Deleuze lectured on the concept of \"event\" on March 10, 1987. A sense of the lecture is described by James Williams. Williams also wrote, \"From the point of view of the difference between two possible worlds, the event is all important\". He also stated, \"Every event is revolutionary due to an integration of signs, acts and structures through the whole event. Events are distinguished by the intensity of this revolution, rather than the types of freedom or chance.\" In 1988 Deleuze published a magazine article \"Signes et événements\"\n\nIn his book \"Nietszche and Philosophy\", he addresses the question \"Which one is beautiful?\" In the preface to the English translation he wrote:\n\nThe Danish philosopher Ole Fogh Kirkeby deserves mentioning, as he has written a comprehensive trilogy about the event, or in Danish \"begivenheden\". In the first work of the trilogy \"Eventum tantum – begivenhedens ethos\" (Eventum tantum - the ethos of the event) he distinguishes between three levels of the event, inspired from Nicola Cusanus: Eventum tantum as non aliud, the alma-event and the proto-event.\n\n\n"}
{"id": "10061279", "url": "https://en.wikipedia.org/wiki?curid=10061279", "title": "Event chain methodology", "text": "Event chain methodology\n\nEvent chain methodology is an uncertainty modeling and schedule network analysis technique that is focused on identifying and managing events and relationship between them (event chains) that affect project schedules. Event chain methodology is an extension of quantitative project risk analysis with Monte Carlo simulations. It is the next advance beyond critical path method and critical chain project management. Event chain methodology helps to mitigate the effect of motivational and cognitive biases in estimating and scheduling. It improves accuracy of risk assessment and helps to generate more realistic risk adjusted project schedules.\n\nEvent chain methodology is an extension of traditional Monte Carlo simulation of project schedules where uncertainties in task duration and costs are defined by statistical distribution. For example, task duration can be defined by three point estimates: low, base, and high. The results of analysis is a risk adjusted project schedule, crucial tasks, and probabilities that project will be completed on time and on budget. Defining uncertainties using statistical distribution provide accurate results if there is a reliable historical data about duration and cost of similar tasks in previous projects. Another approach is to define uncertainties using risk events or risk drivers, which can be assigned to different tasks or resources. Information about probabilities and impact of such events is easier to elicit, which improves accuracy of analysis. Risks can be recorded in the Risk register. Event chain methodology was first proposed in the period of 2002-2004. It is fully or partially implemented in a number of software application. Event Chain Methodology is based on six principles and has a number of outcomes.\n\nActivities (tasks) are not a continuous uniform procedure. Tasks are affected by external events, which transform an activity from one state to another. One of the important properties of an event is the moment when an event occurs during the course of an activity. This moment, when an event occurs, in most cases is probabilistic and can be defined using statistical distribution. The original state is called a ground state, other states are called excited states. For example, if the team completes their job on activity, they can move to other activities. The notion of an activity's state is important because certain events can or cannot occur when activity is in certain state. It means that the state of an activity is subscribed to the events. Events can be local, affecting particular tasks or resources, or global affecting all tasks or resources.\n\nEvents can be related to other events, which will create event chains. These event chains can significantly affect the course of the project. \nFor example, requirement changes can cause an activity to be delayed. To accelerate the activity, the project manager allocates a resource from another activity, which then leads to a missed deadline. Eventually, this can lead to the failure of the project. It could be different relationship between events. One event can trigger one or multiple events.\n\nEvents can be correlated with each other without one triggering another one. In this case if one risk has occurred, another one will occur and vice versa. One event assigned in one activity can execute another activity or group of activities. In many cases it the execution of risk response plans. For example, event “structural defect is discovered” can cause one or many activities “Repair”. Events can cause other events to occur either immediately or with a delay. The delay is a property of the event subscription. The delay can be deterministic, but in most cases, it is probabilistic. Also risks can be transferred from one activity to another. To define event chains, we need to identify a \"sender\", the event that initiates the chain of events. The sender event can cause one or more events that effect multiple activities. These are called \"receiver\" events. In turn, the receiver events can also act as sender events.\n\nEvent chain diagram is a visualization that shows the relationships between events and tasks and how the events affect each other. The simplest way to represent these chains is to depict them as arrows associated with certain tasks or time intervals on the Gantt chart. Here are a few important rules:\n\nBy using event chain diagrams to visualize events and event chains, the modeling and analysis of risks and uncertainties can be significantly simplified.\nAnother tool that can be used to simplify the definition of events is a state table. Columns in the state table represent events; rows represent the states of an activity. Information for each event in each state includes four properties of event subscription: probability, moment of event, excited state, and impact of the event.\n\nOnce events and event chains are defined, quantitative analysis using Monte Carlo simulation can be performed to quantify the cumulative effect of the events. Probabilities and impacts of risks assigned to activities are used as input data for Monte Carlo simulation of the project schedule. In most projects it is necessary to supplement the event based variance with uncertainties as distributions related to duration, start time, cost, and other parameters.\n\nIn Event chain methodology, risk can not only affect schedule and cost, but also other parameters such as safety, security, performance, technology, quality, and other objectives. In other words one event can belong to different categories. The result of the analysis would show risk exposure for different categories as well as integrated project risk score for all categories. This integrated project risk score is calculated based on relative weights for each risk category.\n\nMonte Carlo simulation provides the capability, through sensitivity analysis, to identify single or chains of events. These chains of events can be identified by analyzing the correlations between the main project parameters, such as project duration or cost, and the event chains. These are called “critical events” or “critical chains of events”. By identifying critical events or critical chains of events, we can identify strategies to minimize their negative effects: Avoid, Transfer, Mitigate, or Accept. Event and event chain ranking is performed for all risk categories (schedule-related and non-schedule) as part of one process. Integrated risk probability, impact and score can be calculated using weights for each risk category.\n\nMonitoring the activity's progress ensures that updated information is used to perform the analysis. During the course of the project, the probability and time of the events can be recalculated based on actual data. The main reason for performance tracking is forecasting an activity’s duration and cost if an activity is partially completed and certain events are assigned to the activity. Event chain methodology reduces the risk probability and impact automatically based on the percent of work completed. Advanced analysis can be performed using a Bayesian approach. It is possible to monitor the chance that a project will meet a specific deadline. This chance is constantly updated as a result of the Monte Carlo analysis. Critical events and event chains can be different at the various phases of the project\n\nSometimes events can cause the start of an activity that has already been completed. This is a very common scenario for real life projects; sometimes a previous activity must be repeated based on the results of a succeeding activity. Event chain methodology simplifies modeling of these scenarios The original project schedule does not need to be updated, all that is required is to define the event and assign it to an activity that points to the previous activity. In addition, a limit to the number of times an activity can be repeated must be defined.\n\nIf an event or event chain occurs during the course of a project, it may require some risk response effort. \nRisk response plans execution are triggered by events, which occur if an activity is in an excited state. Risk response events may attempt to transform the activity from the excited state to the ground state. Response plans are an activity or group of activities (small schedule) that augment the project schedule if a certain event occurs. The solution is to assign the response plan to an event or event chain. The same response plan can be used for one or more events.\n\nOne potential event is the reassignment of a resource from one activity to another, which can occur under certain conditions. For example, if an activity requires more resources to complete it within a fixed period, this will trigger an event to reallocate the resource from another activity. Reallocation \nof resources can also occur when activity duration reaches a certain deadline or the cost exceeds a certain value. Events can be used to model different situations with resources, e.g. temporary leave, illness, vacations, etc.\n\n\n\n"}
{"id": "8611599", "url": "https://en.wikipedia.org/wiki?curid=8611599", "title": "False ending", "text": "False ending\n\nA false ending has two contexts: in literature, film and video games it is a narrative device where the plot seems to be heading to its conclusion, but in reality, there's still more to the story; in a musical composition, however, it is a complete stop of the work or song for one or more seconds before continuing.\n\nThe presence of a false ending can be anticipated through a number of ways: the medium itself might betray that it isn't the true ending (i.e. it's only halfway into a book or a song, a film's listed running time hasn't fully elapsed, only half the world has been explored in a video game, etc.), making only stories with indeterminate running length or a multi-story structure able to pull this off effectively; another indicator is the feeling that too much of the story is incomplete when the false ending comes, making it feel like there has to be more.\n\nIn \"L.A. Confidential\" , it seems like the criminal case that the movie revolves around is completely closed with no loose ends until one of the witnesses admits that she lied about important details to give more importance towards the trial of the people who raped her, exposing a cover-up conspiracy. In \"\", the director keeps using editing techniques that are indicative of endings in scenes that could be used as such, but continues until the movie finally ends. \"Spider-Man 3\" has two false endings. Another example is in \"The Simpsons Movie\", where, at a very climatic stage in the film, the screen fades away and says \"To be continued\", which is then followed by the word \"Immediately.\"\n\nSome movies come to a formal ending, followed by the rolling of the credits, which is almost universally used to indicate that the film has ended, only to have the actors reappear in one or more mid-credits scenes. In comedy films, these sequences may be bloopers or outtakes. In other types of films, the mid-credit scenes may continue the narrative set out in the movie. The Marvel Cinematic Universe movies have become notorious for this, in some cases featuring a mid-credits scene and an end-credits scene in the same movie.\n\nIn the police comedy \"Hot Fuzz\" (2007), two police officers realize that a cult is controlling their town. After the two officers defeat the head of the Neighbourhood Watch Alliance and his members, who have been causing problems in the town, the pair are celebrating their success at the police station, and the movie appears to be wrapping up. Then a surviving member of the Neighbourhood Watch Alliance enters the station and shoots an old musket, which triggers an old-fashioned mine.\n\nSome examples in video games include \"Final Fantasy VI\" and \"Wild ARMs\". Both involve confrontations with the major antagonists at what seems like their final lairs, but instead a crisis occurs and the story continues. A third is in \"\", upon sinking the \"Druna Skass\" a second time (Which can only happen if the player plays though the game again, as the game resets itself to the beginning if you sink it once), the player is greeted by another supership, that looks just like the \"Druna Skass\". Yet another example is the survival horror game \"Obscure II\", in which the player must wait until the credits roll to their conclusion before gameplay resumes.\n\nRole-playing video games are notorious for having such plot devices. It usually involves the game's main antagonist being defeated, only for a previously mentioned character to be revealed as the \"real\" villain. One example is \"\", in which the main character is apparently about to have a boss fight with the former villain Zant, but Zant reveals he has been working for another Villain.\nAnother example is \"\" where after you defeat Medusa, credits roll but are quickly interrupted by Hades, the villain for the rest of the game.\n\nIn video games, it is difficult to use the false ending device effectively. Nevertheless, in the hands of a skilled designer, there are several methods that allow it to be done. In several video games, such as those with multiple playable characters and story lines, the game may appear to end after defeating a difficult boss, or clearing what appears to be the \"Final\" level, complete with credits, an outro, and a return to the start screen. These endings are different from bad endings, as everything may appear to be resolved. However, fulfilling conditions such as clearing all the storylines, reloading the save file, or reaching the \"ending\" in a New Game+ mode may give the player the option to continue on to the real ending.\n\nAn example of this is \"Sonic Adventure\", and its sequel \"Sonic Adventure 2\". In the former, while there are six stories to play, only the main character's, Sonic's is the most complete. The other characters' stories are simply side-stories. However, if \"all\" of the stories are completed, a final story appears that wraps up the game and acts as the \"true\" ending. In the latter, there are two stories to play, one for the heroes, and one for the villains. Of note is the plot device is hidden in a false Chaos Emerald being used that would destroy the space colony in which the villain Doctor Eggman is using as a base. It is at first implied that Eggman took the false Emerald, but in reality, when the last story is played, again, after the two normal stories are completed, a true conclusion is offered.\n\nAnother example could be the survival horror game \"Resident Evil 2\", where, depending on your choice, you get to play with one of the two characters and get a certain ending for one of them to later discover, when you finish playing the second path with the second character, you fight the real final boss and the \"true\" ending (That may vary depending of which character you have chosen first) is shown. The main difference between both of the \"true\" endings are that places and times are exchanged, as well as the final dialogue from the game.\n\n\"The Consequence\", the second Kidman DLC for 2014's \"The Evil Within\", features a secret false ending in a similar manner to those shown in the \"Resident Evil\" franchise. At one point, Juli Kidman encounters the brain of the game's main antagonist, Ruben Victoriano, sealed inside a glass container hooked up to the STEM system in Beacon Hospital. Should the player choose to shoot the container and break it, the game cuts to black, then begins the credits sequence, cutting short on a postcard depicting Juli, Joseph and Sebastian on a postcard, before promptly returning to the game as Kidman exclaims \"What the fuck?!\", indicating the whole sequence was a hallucination.\n\nFalse endings are a known device in classical music. Josef Haydn was fond of them, for example inducing applause at the wrong place in the finales of his String Quartet, Op. 33 No. 2 (nicknamed \"The Joke\") and Symphony No. 90. The first movement of Prokoviev's Classical Symphony contains false endings.\nIn music, a number of rock and pop songs use false endings in which the music is arranged so that the song appears to be coming to its ending (e.g., reaching a final cadence on the chord of the home key and then stopping), but then the music recommences. Examples include The Rascals' \"Good Lovin'\"; The Four Tops' \"Bernadette\", \"White Room\" by Cream; \"The Peace!\" by Morning Musume; \"Thank You\" by Led Zeppelin; \"Rain\" by The Beatles; \"Monday, Monday\" by The Mamas & the Papas; and \"But You Know I Love You\" by The First Edition. Another notable example of a musical false ending is \"(Everything I Do) I Do It for You\" by Bryan Adams - because the original song was six and a half minutes long, the false ending became the end of the single/video edit of the song (the album version had a fadeout ending).\n\nIn addition, several other songs have also had false endings, such as \"Angels\" by Amy Grant, a #1 Christian hit in 1984. Another example is \"Keep On Dancing\", a 1965 Top 10 hit for the group The Gentrys from Memphis, Tennessee. In both songs, there is a pause of two seconds before the music starts all over again. Also, \"When I Grow Up (To Be a Man)\" by The Beach Boys, has a sudden stop and resumes 1 second later as the band counts numbers of years of age until the song fades, counting from 19 to 30. The Alice in Chains track \"Rain When I Die\" has a fade-out false ending lasting about 20 seconds, then the music comes back, and then it fades once more, thus providing the real ending. \"Weather with You\" by Crowded House appears to be ending and then another small instrumental is followed by a final chorus.\n\n"}
{"id": "25724894", "url": "https://en.wikipedia.org/wiki?curid=25724894", "title": "Famous Impostors", "text": "Famous Impostors\n\nFamous Impostors is the fourth and final book of non-fiction by Bram Stoker (the author of \"Dracula\"), published in 1910. It is a book that deals with exposing various impostors and hoaxes.\n\n\n"}
{"id": "535329", "url": "https://en.wikipedia.org/wiki?curid=535329", "title": "Fictitious entry", "text": "Fictitious entry\n\nFictitious or fake entries are deliberately incorrect entries in reference works such as dictionaries, encyclopedias, maps, and directories. There are more specific terms for particular kinds of fictitious entry, such as Mountweazel, trap street, paper street, paper town, phantom settlement, and nihilartikel.\n\nFictitious entries are included either as a humorous hoax or as a copyright trap to reveal subsequent plagiarism or copyright infringement. \n\nThe neologism \"Mountweazel\" was coined by \"The New Yorker\" based on a fictitious biographical entry in the 1975 \"New Columbia Encyclopedia\".\n\nThe term \"nihilartikel\", combining the Latin \"nihil\" (\"nothing\") and German \"Artikel\" (\"article\"), is sometimes used.\n\nBy including a trivial piece of false information in a larger work, it is easier to demonstrate subsequent plagiarism if the fictitious entry is copied along with other material. An admission of this motive appears in the preface to Chambers' 1964 mathematical tables: \"those [errors] that are known to exist form an uncomfortable trap for any would-be plagiarist\". Similarly, trap streets may be included in a map, or invented phone numbers in a telephone directory.\n\nFictitious entries may be used to demonstrate copying, but to prove legal infringement, the material must also be shown to be eligible for copyright (see \"Feist v. Rural\", Fred Worth lawsuit or \"Nester's Map & Guide Corp. v. Hagstrom Map Co.\", 796 F.Supp. 729, E.D.N.Y., 1992.)\nMost listings of the members of the German parliament feature the fictitious politician Jakob Maria Mierscheid, allegedly a member of the parliament since 1979. Among other activities he is reported to have contributed to a major symposium on the equally fictitious stone louse in Frankfurt.\n\nFictitious entries in reference publications often occur in an attempt to catch plagiarism, such as: \n\n\nFictitious entries on maps may be called phantom settlements, trap streets, paper towns, cartographer's follies, or other names. They are intended to help unmask copyright infringements.\n\n\n\nOther examples of copyright infringement that do not fall under the above categories include: \n\n\nSome publications such as those published by Harvard biologist John Bohannon are used to detect lack of academic scrutiny, editorial oversight, fraud and/or p hacking on the part of authors or their publishers. Trap publications may be used by publishers to immediately reject articles citing them, or by academics to detect journals of ill repute (those that would publish them or works citing them).\n\nFictitious entries often occur in reference publications as a prank, or practical joke, in an attempt to be humorous, such as: \n\n\nFictitious entries occasionally feature in other publications in an attempt to be humorous, such as:\n\n\nMany publications have included false items and then challenged readers to identify it, including:\n\nFictitious entries are sometimes plot points in fiction, including:\n\n\nFictitious entries may be used to demonstrate copying, but to prove legal infringement, the material must also be shown to be eligible for copyright. However, due to the \"Feist v. Rural\", Fred Worth lawsuit where the Supreme Court ruled that \"information alone without a minimum of original creativity cannot be protected by copyright\", there are very few cases where copyright has been proven. Many of these cases that go to court are dismissed and the affected party is rewarded no compensation.\n\n\nOften times there will be errors in maps, dictionaries, and other publications, that are not deliberate and thus are not fictitious entries. For example, within dictionaries there are such mistakes known as ghost words, \"words which have no real existence […] being mere coinages due to the blunders of printers or scribes, or to the perfervid imaginations of ignorant or blundering editors.\" \n\n\n\n"}
{"id": "52443600", "url": "https://en.wikipedia.org/wiki?curid=52443600", "title": "Funke Bucknor-Obruthe", "text": "Funke Bucknor-Obruthe\n\nFunke Bucknor-Obruthe (born 27 June 1976) is a Nigerian entrepreneur and lawyer. She is the founder and CEO of Zapphaire Events and is regarded as one of Nigeria's pioneering event planners.\n\nBucknor-Obruthe was born to Segun and Shola Bucknor in Lagos State, Southwestern Nigeria. She began her basic and secondary school education at Fountain Nursery and Primary School, Lagos, and Nigeria Navy Secondary School, Lagos, before she proceeded to study Law at the University of Lagos. In 2000, she was called to bar after attending the Nigerian Law School in Abuja.\n\nAfter practising law briefly, Bucknor-Obruthe was employed at Tie Communications, an advertisement agency, where she worked for a short period. In 2003, her love for event planning made her start Zapphaire Events, an independent event planning enterprise. She has since gone on to plan and organize several high-profile events within and outside Nigeria and has won awards and recognition including being featured in CNN's \"Inside Africa\" for planning a Nigerian royal wedding. Some of these include the Future Award for \"Entrepreneur of the Year\" (2006), the \"Wedding Planner Magazine\" Award for \"Wedding Planner of the Year\" (2007), Go2girl Life Achievement Awards (2011), Nigerian Events Awards for Outstanding Contribution to the Events Industry (2012).\n\n\nIn 2014, Nigerian online magazine \"YNaija\" listed Bucknor-Obruthe in its \"10 Most Powerful Under-40s in Business\". In 2016, she was listed in the BBC's \"100 Women\" series.\n\nBucknor-Obruthe is married to Onome Obruthe, with whom she has two children. She is the elder sister of Nigerian media personality Tosyn Bucknor.\n"}
{"id": "1861353", "url": "https://en.wikipedia.org/wiki?curid=1861353", "title": "Humbug", "text": "Humbug\n\nA humbug is a person or object that behaves in a deceptive or dishonest way, often as a hoax or in jest. The term was first described in 1751 as student slang, and recorded in 1840 as a \"nautical phrase\". It is now also often used as an exclamation to mean nonsense or gibberish.\n\nWhen referring to a person, a humbug means a fraud or impostor, implying an element of unjustified publicity and spectacle. In modern usage, the word is most associated with the character Ebenezer Scrooge, created by Charles Dickens in his novella \"A Christmas Carol\". His famous reference to Christmas, \"Bah! Humbug!\", declaring Christmas to be a fraud, is commonly used in stage and television versions and also appeared frequently in the original book. The word is also prominently used in \"The Wizard of Oz\", in which the Scarecrow refers to the Wizard as a humbug, and the Wizard agrees.\n\nAnother use of the word was by John Collins Warren, a Harvard Medical School professor who worked at Massachusetts General Hospital. Dr. Warren performed the first public operation with the use of ether anesthesia, administered by William Thomas Green Morton, a dentist. To the stunned audience at the Massachusetts General Hospital, Warren declared, \"Gentlemen, this is no humbug.\"\n\nThe oldest known written uses of the word are in the book \"The Student\" (1750–1751), ii. 41, where it is called \"a word very much in vogue with the people of taste and fashion\", and in Ferdinando Killigrew's \"The Universal Jester\", subtitled \"a choice collection of many conceits ... bon-mots and humbugs\" from 1754; as mentioned in \"Encyclopædia Britannica\" from 1911, which further refers to the \"New English Dictionary\".\n\nThere are many theories as to the origin of the term, none of which has been proven:\n\nThe word has been used outside of anglophone countries for well over a century. For instance, in Germany it has been known since the 1830s, in Sweden since at least 1862, in France since at least 1875, in Hungary, and in Finland.\n"}
{"id": "40929468", "url": "https://en.wikipedia.org/wiki?curid=40929468", "title": "Hunchun incident", "text": "Hunchun incident\n\nThe Hunchun incident (October 2, 1920) was a reported raid on a Japanese consulate in Manchuria resulting in the death of thirteen Japanese. The Japanese government used this incident to justify sending thousands of Imperial Japanese troops into Manchuria on October 5, 1920. These escalations culminated with the Battle of Qingshanli (October 21–26, 1920) between Japan and the Korean Independence Army, where Korean rebels fought Japanese soldiers.\n\nFor more than a decade prior to Korea’s March 1st Movement (1919) nationalist groups of Korean rebels, many of whom were former soldiers in the Korean army, organized into various pro-independence factions in Manchuria. Due to its strategic location across the Korean border, guerilla fighters could effectively launch raids on Japanese consular police stations, and then retreat back to the Chinese side of the boundary. For example, Hong Pomdo (a previous Righteous Army leader) created the Korean Independence Army and trained so-called independence fighters in Yanji. Additionally, the Northern Route Military Headquarters was established under the leadership of So Il, with Kim Chwajin commanding more than four hundred independence fighters at its officer training school. Separately, Yi Tonghwi also trained over 3,000 independence fighters in Hunchun and armed them with weapons provided by the Red Bolshevik army.\n\nResponding to the March 1st Movement’s failure to secure independence and arouse international sentiments toward the Koreans’ plight under Japanese colonial rule, disaffected Koreans came together on April 13, 1919 in Shanghai to form a republican Korean provisional government with the hope of working together with the independence factions in Manchuria to eventually obtain freedom from Japan.\n\nAs the momentum behind Korean independence movements in Manchuria increased, Japanese consular police chief Suematsu Kichiji became increasingly concerned about increasing radical Bolshevism among Koreans. To try and suppress these movements, he ordered numerous illegal police raids on suspected radical Jiandao base camps, which were protested by local Chinese leaders. While it is clear that some of the Korean guerrilla fighters in Manchuria were influenced by leftist ideologies, the major factions primarily supported the Shanghai Provisional Government and were focused primarily on Korean independence and self-determination.\n\nAngered by the Japanese suppression of the March 1st Movement, Korean independence fighters in Manchuria began increasing their raids against Japanese border posts, killing numerous Japanese guards, with the eventual goal of advancing into Korea to remove the Japanese. During the early summer months of 1920, Korean rebels fought with Japanese troops in thirty-two battles along the border. After one particular Japanese counterattack, Hong Pomdo’s forces surrounded and killed 120 Japanese soldiers and wounded more than 200.\n\nIn effort to contain the Korean rebels, Japan petitioned both Tokyo and the Chinese government to help, but received little assistance. Subsequently, on October 2, 1920, a Japanese consulate in the Chinese city of Hunchun in Jilin Province was attacked and burned to the ground purportedly (according to Japanese sources) by the Korean Independence Army, killing thirteen Japanese people. It was further reported that the “bandits” carrying out this attack “committed indiscriminate acts of murder and pillage” and “looted local shops.”\n\nMany South Korean historians maintain, however, that the attack on Hunchun was not carried out by the Korean rebels, but rather was staged by the Japanese to justify incursion into Manchuria. Some South Korean sources further believe that the attack was coordinated with Chinese bandit leader Ch'ang-chiang-hao who had been bribed by the Japanese to carry out the attack with several bandits in order to incriminate the Koreans. These sources maintain that Chang went further than the Japanese had requested him to in the scale of his attack. Other South Korean scholars even maintain that the entire incident was a complete Japanese fabrication.\n\nNorth Korea is likewise skeptical about the Japanese narrative of the incident, with official sources recently asserting that “the Japanese imperialists cooked up the ‘Hunchun incident’ in which they hurled mounted bandits into attacking their consulate and kicked up a wholesale whirlwind of suppression against Koreans in Northeast China under that pretext.”\n\nThough it is difficult for historians to determine who was behind the attack, or whether the incident actually took place, this controversial event is historically significant because Japan used it to justify its escalated military intervention in Manchuria. Japan petitioned and received permission from China to send 15,000 troops from the 19th Division of the Chosen Army of Japan to contain the Korean rebel armies in Jilin province.\n\nIn reaction to the Hunchun incident, a Japanese punitive “Jiandao Expedition” was accordingly sent to Manchuria, and used “search and destroy” patrols to suppress the guerrilla fighters by carrying out numerous arrests and executions. By December 1920, a Korean Commission report described that Japanese soldiers had burned down thirty-two villages and killed “all the male inhabitants of the [Hunchun] district, and massacred 145 peaceful inhabitants.” One house was reportedly burned down with “women and children inside.”\n\nThough Korean independence forces in Manchuria were never effectively organized under the leadership of the Shanghai Provisional Government, they did achieve notable military victories against the Japanese brigades. The most significant of these was the Battle of Qingshanli, where about 400 Korean rebels were able to defeat the better-trained Japanese after four days of intense combat. In this battle the Koreans killed about 1,200 Japanese soldiers while losing only 60 of their own. However, according to Japanese records, 11 soldiers were killed in action, 24 wounded.\n\nIn early 1921, after a series of skirmishes and retreats on both sides, as well as criticism from local Chinese authorities and the international community, most members of the 19th Division withdrew from eastern Manchuria. Some of the socialist-leaning Korean rebels were then recruited by the Red Bolshevik army to assist in its civil war prior to the formation of the Soviet Union.\n"}
{"id": "2494205", "url": "https://en.wikipedia.org/wiki?curid=2494205", "title": "International Union of Exhibitions and Fairs", "text": "International Union of Exhibitions and Fairs\n\nInternational Union of Exhibitions and Fairs (IUEF) is an association of the leading exhibition centres and trade show related companies from Russia, Armenia, Belarus, Moldova, Ukraine, Lithuania, Kazakhstan, United Kingdom. It was founded in 1991.\n\n"}
{"id": "42691783", "url": "https://en.wikipedia.org/wiki?curid=42691783", "title": "Jennifer Gilbert", "text": "Jennifer Gilbert\n\nJennifer Gilbert (born December 24, 1968) is an American entrepreneur and TV personality. She is the founder and chief visionary officer of \"Save the Date\", a New York–based special events company. She was named one of the \"entrepreneurs of the year\" by Ernst & Young for her event planning company, \"Save the Date\" in 1998. She has also appeared on the TV series \"Real Housewives of New York City\".\n\nGilbert graduated from the University of Vermont in 1990 and received a Bachelor of Science in Consumer Marketing with a minor in Business. After graduation Gilbert moved to New York City. In 1991, at the age of 22, a stranger attacked her, stabbing her 37 times with a screwdriver before leaving her for dead.\n\nAfter recovering, she decided to become an event planner and started \"Save the Date\", a New York based event planning company, in 1994. In 2010 she sued a dating company with similar name called Save the Dating, the company went off business after lawsuit. The company has rendered services to many notable companies including several in Fortune 500 companies and has been retained by clients such as Bill and Melinda Gates Foundation, Oprah Winfrey and singer Jewel.\n\nShe is also the co-owner of Portamee, a baby carrier company. She has also authored a book, \"I Never Promised You A Goodie Bag\", in 2012. In her book, she discussed about how her life changed after being attacked at a young age and then how she got control on her life and started a successful company. She spoke at TEDx Barnard College Women sharing her entrepreneurial spirit and philosophy.\n\nShe appeared as herself in the Bravo's hit TV series \"Real Housewives of New York City\" in 2010. She was the party planner for Jill Zarin.\n\nIn 1994, Gilbert met her husband, a Wall Street Executive, Bennett Egeth and the two were married in New York City 7 years later.\n\nGilbert lives in New York City with her husband, their daughter and twin boys.\n"}
{"id": "21296236", "url": "https://en.wikipedia.org/wiki?curid=21296236", "title": "Let's Do It 2008", "text": "Let's Do It 2008\n\nLet's do it 2008 () is the largest campaign to activate civic society in Estonia since the Singing Revolution in 1988. Let's Do It! World is a global campaign for cleaning all countries that grew out of Let's do it 2008. \n\nOver 50,000 people, or approximately 4% of the population of 1.3 million, participated in the cleanup of the forests and countryside, which would be proportionally equal to 15.3 million people in the United States, or 57 million people in India. Momentum for the event was built up with a media campaign from October 2007 to April 2008. The action was carried out during one day on 3 May 2008. More than 10,000 tons of garbage were removed from the country's forest in about 5 hours for less than 500,000 euros. Under normal circumstances it would have taken the government 3 years and 22.5 million euros to accomplish a similar feat.\nThe initial team was:\n\nThe group met at 29 August 2007 as recorded in the first protocol.\n\nOther people of the main team:\n\n\nThis has led to the international action \"Let's Do It! World\" and the creation of World Cleanup Day.\n\n\n\n"}
{"id": "5971019", "url": "https://en.wikipedia.org/wiki?curid=5971019", "title": "List of Cuba–United States aircraft hijackings", "text": "List of Cuba–United States aircraft hijackings\n\nAircraft hijacking incidents between the United States and Cuba were at their height between 1968 and 1972. These incidents have variously been attributed to terrorism, extortion, flight for political asylum, mental illness, and transportation between the two countries as a result of the ongoing antagonistic Cuba-United States relations and the Communist government restrictions against Cubans attempting to leave Cuba. Subsequent measures by both governments contributed to a gradual reduction of reported incidents towards the mid-1970s. Governmental measures included an amendment to Cuban law which made hijacking a crime in 1970, the introduction of metal detectors in U.S. airports in 1973, and a joint agreement between the U.S. and Cuba signed in Sweden to return or prosecute hijackers.\n\nBelow is a non-comprehensive list of hijacking incidents of aircraft between Cuba and the United States.\nBefore the Cuban Revolution:\n\nAfter the Cuban Revolution:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "6155907", "url": "https://en.wikipedia.org/wiki?curid=6155907", "title": "List of MTV award shows", "text": "List of MTV award shows\n\nMTV, the first and most popular music television network in the U.S. has a long history of hosting live music events in which awards are handed out. Along with MTV's related channels around the world, the network produces over 20 award shows annually. This list of MTV award shows links to further information about each of these shows.\n\n\"NOTE: Includes all MTV networks.\"\n\n\n\n\n\n\n"}
{"id": "2281261", "url": "https://en.wikipedia.org/wiki?curid=2281261", "title": "List of historical reenactment events", "text": "List of historical reenactment events\n\nThis is a list of historical reenactment events.\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "487164", "url": "https://en.wikipedia.org/wiki?curid=487164", "title": "List of public lecture series", "text": "List of public lecture series\n\nRecurrent series of prestigious public lectures are presented in various countries.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "24912020", "url": "https://en.wikipedia.org/wiki?curid=24912020", "title": "List of royal weddings", "text": "List of royal weddings\n\nA royal wedding is a marriage ceremony involving members of a royal family. Weddings involving senior members of the royal family are often seen as important occasions of state and attract significant national and international attention. The following is a list of notable royal weddings:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "21396853", "url": "https://en.wikipedia.org/wiki?curid=21396853", "title": "List of science and engineering blunders", "text": "List of science and engineering blunders\n\nThis is a list of engineering blunders, i.e., gross errors or mistakes resulting from grave lack of proper consideration, such as stupidity, confusion, carelessness, or culpable ignorance, which resulted in notable incidents.\n\n\n"}
{"id": "14819280", "url": "https://en.wikipedia.org/wiki?curid=14819280", "title": "Marcy Blum", "text": "Marcy Blum\n\nMarcy Blum is an American author, event planner and owner of Marcy Blum Associates in New York City.\n\nBlum is a graduate of The Culinary Institute of America and began her career in the restaurant business, later creating a segue into the then developing field of event planning. In 1986 she became one of the first in the industry to develop the vocation into a successful career\n\nBlum has been a guest on the \"CBS Morning Show\", \"The Today Show\", \"Live with Regis and Kelly\", \"The Oprah Winfrey Show\" and \"The Nate Berkus Show.\"\n\nAmong her clients are Billy Joel and Katie Lee Joel, Kevin Bacon and Kyra Sedgwick, Salman Rushdie and Padma Lakshmi, Colin Hanks and Samantha Bryant, Lebron James and Savannah James, George Soros and Tomiko Soros, The Rockefeller family, Nate Berkus and Jeremiah Brent, and Andre Iguodala and Christina Iguodala. She was also the planner of The Knot's \"Dream Wedding\" in 2014 of Boston Marathon bombing survivors, Rebekah Gregory and Pete DiMartino. \"New York\" magazine described Blum as a \"desired wedding planner to the wealthy and famous.\"\n\n\"Vogue Magazine\" named Blum one of the top wedding planners in the world by, \"Harper's Bazaar\", and Martha Stewart Weddings.\n\nBlum has written and contributed to many articles, including \"The New York Times\", \"New York\" magazine, The Knot, \"Brides\" magazine, \"Elle\", \"Modern Bride\", \"InStyle\", and \"Town & Country\".\n\nShe coauthored, with Laura Kaiser, two books, \"Wedding Planning for Dummies\" and \"Wedding Kit for Dummies\".\n\nMarcy is the sister of American author and journalist Howard Blum.\n\n"}
{"id": "53370439", "url": "https://en.wikipedia.org/wiki?curid=53370439", "title": "Maxine Lewis", "text": "Maxine Lewis\n\nMaxine Lewis is a producer, talent scout, and event planner for Harlem's Amateur Night (Showtime at the Apollo). She was Percy Sutton's spokeswoman.\n\nShe has worked for the Apollo theatre for decades as a producer for Showtime at the Apollo, and Apollo Kids at the Apollo Theater in Harlem. She has been an organizer for over 21 years for the 154/153 streets' Extended Family Reunion in Harlem, an event that brings residents from the neighborhood and residents that used to live in the area in July of each year.\nShe was born and raised in Harlem, New York City.\n\nMaxine Lewis is a Special Events Producer for Diamonds in the Rough Independent Entertainment from 1986 to till the present (31 years).\n\nPercy Sutton was the founder of Inner City Broadcasting Corporation. Which bought the Apollo Theatre in 1981. They rescued the Apollo Theatre from bankruptcy Court. The theatre was refurbished. She served as Percy Sutton's Executive Assistant for 19 years.\n\nIn 1992, Maxine organized and produced a memorial tribute for tap dancer Charles Coles known as Charles \"Honi\" Coles at the Apollo Theatre. She began working for Inner City Broadcasting Corporation in 1982. In 1984 she became the talent coordinator for amateur segments for Showtime at the Apollo and later segment producer for \"Apollo Kids\" segments for over 14 years. She traveled across the country looking for talent to be showcased. Some well known artists that she showcased were: Lyfe Jennings, Jermaine Paul, Ne-Yo, Jazmine Sullivan,\nLauren Hill, Canton Jones, Jessica Care Moore, and more. She has a featured segment on 90.3 WHCR FM radio for Street Corner Resources giving information and updates about resources and activities for adults.\nShe has been a community volunteer serving food for years for Thanksgiving and Christmas at the National Action Network in Harlem. She is a member of Perfecting Faith Church in Freeport, Long Island, where she utilizes her skills for community service in special events organizing and public relations work.\n\n2015 - Senior Citizen's Day by the New York State Office for the Aging for her community work honored for Community Service.\n\n2015 - Senator Díaz of the Bronx, New York City presents a Senior Citizen of the Year proclamation to Maxine Lewis \n\n2014 - Fannie Lou Hamer Honoree: for her work at the National Action Network National Crisis Manager (North Jersey Chapter)\n"}
{"id": "30860671", "url": "https://en.wikipedia.org/wiki?curid=30860671", "title": "Museum of Hoaxes", "text": "Museum of Hoaxes\n\nThe Museum of Hoaxes is a website created by Alex Boese in 1997 in San Diego, California as a resource for reporting and discussing hoaxes and urban legends, both past and present.\n\nIn 2004, PC Magazine included the site as one of the \"Top 100 Sites You Didn't Know You Couldn't Live Without,\" and Sci Fi Weekly named it \"site of the week\" for the week beginning 7 February 2007.\n\nBoese has published two books on hoaxes: \"Museum of Hoaxes\" (E.P. Dutton, 2002, ) and \"Hippo Eats Dwarf: A Field Guide to Hoaxes and Other B.S.\" (Harvest Books, 2006, ). A third book by Boese, \"Elephants on Acid\" (Harvest Books, 2007, ), focuses on unusual scientific experiments. His latest book is \"Electrified Sheep\", published by Boxtree 2011, is also about strange scientific experiments.\n\n"}
{"id": "14208196", "url": "https://en.wikipedia.org/wiki?curid=14208196", "title": "News bureau", "text": "News bureau\n\nA news bureau is an office for gathering or distributing news. Similar terms are used for specialized bureaux, often to indicate geographic location or scope of coverage: a ‘Tokyo bureau’ refers to a given news operation's office in Tokyo; 'foreign bureau' is a generic term for a news office set up in a country other than the primary operations center; a ‘Washington bureau’ is an office, typically located in Washington, D.C., that covers news related to national politics in the United States. The person in charge of a news bureau is often called the bureau chief.\n\nThe term is distinct from a news desk, which refers to the editorial function of assigning reporters and other staff, and otherwise coordinating, news stories, and sometimes the physical desk where that occurs, but without regard to the geographic location or overall operation of the news organization. For example, a foreign bureau is located in a foreign country and refers to all creative and administrative operations that take place there, whereas a foreign desk describes only editorial functions and may be located anywhere, possibly as an organizational unit within the news organization's home office.\n\nA news bureau is traditionally operated out of an office by a single news outlet such as a radio, television, or newspaper news program. A single news company such as CNN or NPR may use a single bureau and office staff for all of its programs, and even those of subsidiary or other affiliated companies. For convenience, to save money and space, and to ensure the availability of necessary services (such as video feeds and studios), different companies may share an office space or co-locate at a single office building. News agencies may also operate news bureaux, and major public relations sources (such as governments, large companies, or advocacy groups) may operate news bureaux of their own to create, rather than simply report, news stories.\n\nTraditional news media, particularly television news and newspapers, have cut the number and size of news bureaux in recent decades for several reasons. They face declining profitability due to increasing competition from Internet news sources, and therefore have less money to spend on news-gathering.\n\nNewspapers rely increasingly on cooperative arrangements with counterparts elsewhere, and often will accept stories from their sister organizations rather than investigating stories themselves. Similarly, smaller newspapers may formally affiliate to sponsor cooperative bureaux that operate as press pools to serve more than one news organization (and sometimes a large number of organizations) from a single office. When news sources combine operations following a merger or other business consolidation, the surviving company often combines or eliminates redundant bureaux. Growing multiculturalism has facilitated this process: rather than demanding a reporter from their own country or locale who has been sent on assignment, news audiences have come to tolerate or even expect to see stories in remote locations covered by people who live locally; this empowers the audience to make their own judgments about any apparent cultural difference between themselves and the news subjects, rather than leaving the function of cultural interpretation entirely up to the reporter.\n\nThe often-criticized practice of parachute journalism allows News media to cover stories remotely using journalists who are generalists rather than more specialized field experts. Rather than leaving journalists in place waiting for breaking news to occur, smaller staff can be assigned as needed to wherever there are breaking stories, either by commuting to the physical location or by synthesizing reports from remote sources. An even more controversial practice, sometimes described as a reaction to declining resources rather than a legitimate cost-saving measure, is to rely on and reprint information from press releases written by public relations professionals working for people or companies that are the subject of an article, or have an interest in an article, without spending the resources to verify or conduct independent research on the matter. Another practice that limits news bureaux is embedded reporting, whereby war correspondents travel under the care of military units rather than at their own direction. The ability to quickly and safely travel throughout a war zone, and to obtain interviews with soldiers and coverage of important conflicts, appeals to news media, but at the cost of journalistic independence and, according to some, objectivity.\n\nThe interaction between professional journalists, witnesses, and news subjects has evolved considerably. Whereas news subjects and bystanders were once treated simply as witnesses to be interviewed for a news story, media have now accepted them as part of the news process. There are many antecedents to Citizen journalism. For example, meteorologists would count on amateurs to gather weather data to report, or interview willing subjects unrelated to a news story for \"man on the street\" interviews. As early as the 1930s the Soviet Union encouraged millions of amateur People's correspondents to expose corruption and otherwise report on news. Beginning in the 1970s, media, unable to respond quickly enough to obtain compelling coverage of natural disasters and weather phenomena such as tornadoes would count on hobbyists for photographs and film footage. With improvements in technology and as video cameras and video-equipped cell phones became widely available, they set up formal programs to gather material from nonprofessionals. For example, in August, 2006, CNN launched \"CNN Exchange\", by which the public is encouraged to submit \"I-Reports\" comprising photographs, videos, or news accounts. More recently newspapers have incorporated blogs, once seen as a threat to conventional news practice, either by creating blogs of their own (and deputizing local or field-specific bloggers as a second, lower-paid tier among their recognized staff of independent contractors) or by covering blogs as news sources.\n\nIn 2006 Reuters opened its first virtual news Bureau, staffing real-life reporters in a virtual office in Second Life. CNN followed suit in October 2007, but took a citizen journalism approach, allowing residents of Second Life to submit their own reportage. Although the news audience of Second World is relatively small, and declining, media consider it a training ground for themselves and participants, applicable to future virtual news projects.\n\n"}
{"id": "9610206", "url": "https://en.wikipedia.org/wiki?curid=9610206", "title": "Old Home Week", "text": "Old Home Week\n\nOld Home Week or Old Home Day is a practice that originated in the New England region of the United States similar to a harvest holiday or festival. In its beginning in the 19th–20th century it involved a municipal effort to invite former residents of a village, town, or city—usually individuals who grew up in the municipality as children and moved elsewhere in adulthood—to visit the \"Old Home\", the parental household and home town. Some municipalities celebrate the holiday annually, while others celebrate it every few years.\n\nIn the late 20th and early 21st century, the practice has spread to other parts of North America and has become a broader celebration with an emphasis on local culture and history. From the Wilmington, Vermont town web site:\n"}
{"id": "58763675", "url": "https://en.wikipedia.org/wiki?curid=58763675", "title": "The Fifth Column Podcast", "text": "The Fifth Column Podcast\n\nThe Fifth Column Podcast is an American politics, news and humor podcast founded in April, 2016. The show is hosted by Kmele Foster, with co-hosts Michael C. Moynihan, Matt Welch and produced by contributor Anthony L. Fisher and Daniel Bier.  The podcast is described as in each episode as an “...almost weekly thoughtful commentary on the news, those who make it and occasionally ourselves”.  The show is closely associated with independent-leaning, libertarian and anarcho-capitalist ideology as presented by the show hosts and guests, but is not explicitly so. \n\nThe Fifth Column Podcast first aired on iTunes April 2, 2016. The show markets itself as “Analysis. Commentary. Sedition” on Instagram and as “@MattWelch @mcmoynihan @Kmele pick apart the news, interrogate their guests and question just about everything #someidiotwrotethis” on Twitter.  The introduction to the podcast typically describes itself as:  “…your almost weekly rhetorical assault on the news cycle, the people who make it and occasionally ourselves…”.  Credit for the shows creation has been attributed to the encouragement of Kennedy (Lisa Kennedy Montgomery) and her husband Dave Lee, who are associates of the hosts.  The name of the podcast has been attributed to a suggestion by Katherine Mangu-Ward as revealed in Episode 13 (3:05 mark).  The original music that opens and closes the podcast includes a soundbite of F.D.R.’s speech given 1940/05/27 warning prejudices exploited by Fifth . (https:archive.org)\n\n\"Reason.com\" regularly promotes The Fifth Column in its \"Hit & Run\" segments. Other promotions come by way of wearelibertarians.com, rampantdiscourse.com, postlibertarian.com and freedombunker.com.\n"}
{"id": "40553209", "url": "https://en.wikipedia.org/wiki?curid=40553209", "title": "The Sound of Music Live!", "text": "The Sound of Music Live!\n\nThe Sound of Music Live! is a television special that was originally broadcast by NBC on December 5, 2013. Produced by Craig Zadan and Neil Meron, the special was an adaptation of Rodgers and Hammerstein's Broadway musical \"The Sound of Music\". The television special starred country singer and \"American Idol\" winner Carrie Underwood as Maria von Trapp, and was performed and televised live from Grumman Studios in Bethpage, New York.\n\nSpearheaded by NBC chairman Bob Greenblatt, the network positioned the special as being a live television \"event\". In preparing for the broadcast, Meron and Zadan emphasized the logistical challenges that they would face due to the live aspects of the special, and the fact that \"The Sound of Music Live!\" was an adaptation based on the musical itself and not the 1965 film version. Meron felt that if the telecast were successful, the concept could become \"another kind of entertainment that can exist on TV.\" By her request, Underwood's casting as Maria was personally endorsed by Julie Andrews, who starred in the 1965 film.\n\nThe production was met with mixed reviews; much of its criticism was directed towards the casting of Carrie Underwood to play Maria, whom critics (including the real-life von Trapp family) believed was not experienced enough in theatre to portray such an iconic role. While her vocal performance was praised, her acting was described by critics as \"amateur\", \"lifeless\" and lacking emotion. The production was a ratings success for NBC; with a total of 18.62 million live viewers, \"The Sound of Music Live!\" brought the network its highest Thursday night viewership for an entertainment program since the series finale of \"Frasier\" in 2004, and prompted NBC to sign Zadan and Meron on to produce more live musicals for the network in the future.\n\nMaria Rainer, a postulant at Nonnberg Abbey in Salzburg, is contemplating the day she has spent in the mountains (\"The Sound of Music\"). When she returns to the Abbey, she learns from the Mother Abbess that she is to be the new governess for the von Trappe children. Before she leaves the Abbey, Mother Abbess asks her to teach her the song that she's always singing (\"My Favorite Things\"). When Maria arrives at the von Trappe house, she is greeted coldly by the Captain and introduced to the children, who enter with military precision. Maria finds that the Captain has emotionally closed himself off since the death of his wife and decides to teach his children the basics of singing to gain their trust and acceptance.\n\nA month, later the captain returns home with Elsa Schraeder, whom he is courting, and their friend Max Detweiler, who is looking for the perfect local singing group to perform at the annual Kaltzberg Festival. When his children arrive dressed in clothes Maria had made from her old bedroom curtains he is outraged and embarrassed. Maria then confronts him and tells him how he does not know or understand his children and that they need him but this only upsets him more and he orders her to return to Nonnberg Abbey. However, upon hearing his children sing to Schraeder, his eyes are open to the truth Maria had been speaking and he embraces his children and ask Maria to stay on as governess.\n\nHe then throws a grand party for Schraeder and when the band plays the Ländler, the captain's youngest son asks Maria to teach him the dance and the captain steps in to help. As the two dance an unspoken attraction begins to arise in the two and Maria puts a stop to the dancing. However, this unspoken attraction did not go unnoticed by Brigitta who confronts Maria on this. Though Maria strongly denies it she begins to realize that Brigitta is telling the truth. Then, Schraeder calls the children out to say good night to the guests and Max is instantly smitten with the idea to have the children sing in the festival and during all the hustle and bustle Maria sneaks off unnoticed and returns to the Abbey, where she confides in the Mother Abbess that she has fallen in love with the captain but that she is ready to take the orders of poverty, obedience and chastity. The Mother Abbess denies her this and encourages her to take face her problem head-on and to find the life she was born to live.\n\nMaria then returns to the von Trapp home and is warmly greeted by the children, who no longer feel the joys of singing due to her sudden departure. When she finds out that the captain intends to marry Schraeder she decides to see her duties through until arrangements can be made for a new governess. However, the political differences between Schraeder and the captain cause the two to realize that they have no future together and she leaves. Meanwhile, the captain confronts Maria and the two admit their feelings for each other. The two agree to marry at the Abbey and while the two are on honeymoon, Germany invades Austria.\n\nWhen they return, the captain is ordered to accept a commission in the German Navy and report immediately to Bremerhaven. Maria, thinking quickly, hands the Admiral the program for the Kaltzberg Festival showing that the von Trapp Family Singers are scheduled to perform, so the captain couldn’t possibly leave right away. They are granted permission to perform. During the finale, Max announces that a guard of honor is waiting to escort the captain away as soon as the concert is over. Maria leads the family in one more song to which they escape to one by one and flee to the Abbey. The Nazi soldiers search the Abbey for the von Trapps to no avail, as the family decides to flee Austria over the mountains with Maria's help.\n\n\n\nBenanti had previously portrayed Maria von Trapp on Broadway in 1998. Craig Zadan and Neil Meron, who had previously worked with Borle on the NBC musical drama television series \"Smash\", served as executive producers. Other credits include Rob Ashford and Beth McCarthy-Miller as directors, Ashford also was the choreographer, Priscilla Taussig served as producer, David Chase as music director and Derek McLane as production designer. Catherine Zuber was costume designer and Bernie Telsey was the casting director. The production is taken from the book by Howard Lindsay and Russel Crouse and is based off the memoir, \"The Story of the Trapp Family Singers\" by Maria von Trapp.\n\nOn June 30, 2012, NBC's chairman Bob Greenblatt announced the network's plans to broadcast a live adaptation of the Broadway musical \"The Sound of Music\" in 2013. On November 30, 2012, the network announced that country music and fourth season of \"American Idol\", winner performer Carrie Underwood would star as Maria von Trapp in the production. In a statement, Greenblatt justified Underwood's involvement, saying that \"[Maria] was an iconic woman who will now be played by an iconic artist.\" On September 16, 2013, NBC revealed the full cast of the special, and released a promotional image depicting Underwood as Maria von Trapp, wearing blond braids and a dirndl, referencing one of Julie Andrews' \"most iconic moments\" from the film adaptation. Prior to being cast, Underwood personally asked Andrews for her endorsement of the role and the production, stating that \"whenever I do a cover of somebody's song or whatever, I always get permission of the artist first.\"\n\nProduced on a budget of around $9 million, \"The Sound of Music Live!\" was broadcast live from a soundstage built at Grumman Studios in Bethpage, New York. NBC, as well as producers Craig Zadan and Neil Meron, emphasized that the production was not a remake of the film, but an adaptation of the musical itself; Meron iterated that \"the audience will discover, within the first few minutes of watching the show, that they are not seeing a TV version of the movie. They'll know right away it's \"The Sound of Music\", but it's a different \"Sound of Music\" than they are accustomed to seeing on film.\" Meron felt that if successful, the broadcast could \"open the door to another kind of entertainment that can exist on TV.\" He also praised the involvement of Underwood as the star of the production, believing that she was a quick learner, and \"has all of the qualities of Maria.\" Greenblatt was highly supportive of the project, as he was, in the words of Zadan, a \"passionate devotee of theater\". Zadan considered the production to be \"one of the profoundly complicated, amazing experiences we’ve ever had\", noting the additional challenges created by the live broadcast.\n\nPromoted by NBC as a \"three-hour holiday event\", \"The Sound of Music Live!\" was aired as part of a push by NBC to air more live entertainment specials. Among its most popular programs in recent years have been those with live components (such as \"The Voice\" and \"Sunday Night Football\"); NBC's Jennifer Salke believed that the increased level of social network interaction possible in a live broadcast, along with the feeling of being part of an \"event\", would encourage viewers to watch the special live instead of on-demand or from a recording. NBC's previous attempt at event television, \"The Million Second Quiz\", was met with mixed reviews and viewership, but NBC did indicate that Subway's advertising throughout the series brought a higher level of awareness to the brand. The television special was also the first live musical special in almost fifty years on NBC.\n\nRetail chain Walmart served as the presenting sponsor for \"The Sound of Music Live!\". NBC also produced five themed Walmart commercials to air throughout the special, featuring scenes of a family using products from the store set to songs from \"The Sound of Music\". The five ads were timed to air during the commercial break following the scene where the song was featured; NBC's advertising chief Dan Lovinger considered the ads to be a way to \"enhance the excitement\" of the presentation for families.\n\nPrior to the broadcast, members of the real-life von Trapp family were critical of casting Underwood to play Maria, agreeing with the notion that she would be a good singer but a poor actor. They suggested that Anne Hathaway, who played Fantine in the 2012 film adaptation of \"Les Misérables\" for which she won the Academy Award for Best Supporting Actress, would have been a better choice. \"The Sound of Music Live!\" received mixed reviews from entertainment critics, commending the show for its scope and supporting cast while questioning Underwood's acting capabilities.\n\nKevin Fallon of \"The Daily Beast\" wrote: \"Naturally, Underwood sounded astounding, as alive as those damned hills, every time she was asked to stand on top of things and belt. But whether it was because of nerves or lack of experience, her acting was painfully lifeless and amateur throughout the first two thirds of the lengthy ordeal. The singer, it seems, is a proud graduate of the school of 'If I don't blink, they'll think I'm acting!'\" Verne Gay of \"Newsday\" liked Underwood's performance, commenting that not only could she sing, but \"she is a luminous stage presence who had the guts to take on one of the most iconic roles of the stage or screen.\"\n\nBrian Lowry of \"Variety\" described the production \"as lifeless as [its] alpine backdrops.\" Marc Bernardin of \"The Hollywood Reporter\" commended the production's aesthetics but called it a \"very expensive karaoke\", and he wrote that Underwood \"doesn’t acquit herself so well when it comes to the carrying the emotional weight of the production.\" He also panned Moyer, criticizing his vocal performance and describing \"his attempt at conveying an emotional hollowness\" as \"mildly constipated\" and \"clenched.\" However, he lauded Benanti, Borle and McDonald's \"strong\" performances.\n\n18.62 million viewers watched \"The Sound of Music Live!\", making it the most-watched program of the night. It had a 4.6 rating, 13 share in the 18–49 demographic, which led all networks. It attracted NBC's largest non-sports Thursday audience since the series finale of \"Frasier\" in 2004 (which averaged 22.6 million viewers), and NBC's largest non-sports audience on any night since the 2007 Golden Globe Awards. The special performed the best with women in the 25–54 demographic, reaching a household rating of 7.0 for that demographic during the primetime broadcast. It did particularly well in Oklahoma City, the capital of Underwood's home state, where it was watched by 28% of TV viewers. At least 38.69 million viewers watched a portion of \"The Sound of Music Live!\". Factoring in DVR viewership over the week following the broadcast, the special was viewed by 21.84 million, with 3.1 million within the first three days.\n\nFollowing its original airing, an encore presentation of \"The Sound of Music Live!\" aired on December 14, 2013, attracting 3.1 million viewers. It notably displaced one of NBC's two traditional airings of the film \"It's a Wonderful Life\" (the other being on Christmas Eve), which was bumped ahead to December 20, 2013.\n\nNBC's Bob Greenblatt considered the production to be a success, and signed Meron and Zadan to produce another live musical for the 2014 holiday season. Greenblatt felt that there were enough recognizable, family-friendly musicals to make events like \"The Sound of Music Live!\" an annual tradition, and he indicated that NBC received e-mails and phone calls from various theatrical rightsholders, expressing interest in having their musicals adapted in a similar fashion, In January 2014, NBC announced that it would broadcast a live version of \"Peter Pan\" in December 2014, and at NBC's upfronts in May 2014, Greenblatt announced that NBC had also obtained rights to produce an adaptation of \"The Music Man\", although the network did not announce any timeframe for the production. It was later revealed that \"The Music Man\" project was put on hold for a live performance of \"The Wiz\". Interest in the concept also spread to competing networks: in April 2014, Fox announced that it would produce a of \"Grease\", which aired in January 2016. NBC aired \"Hairspray Live!\" later that year.\n\nThe special also had an influence on viewership for ABC's annual broadcast of the 1965 film version of \"The Sound of Music\"; with 6.5 million viewers and a 1.3 share, it was ABC's highest-rated airing of the film since 2007, although it was beaten by NBC's \"Sunday Night Football\".\n\nThe broadcast has been nominated for four Emmy Awards for the 66th Primetime Emmy Awards for Outstanding Music Direction, Outstanding Directing for a Variety Special, Outstanding Special Class Program and Outstanding Technical Direction, Camerawork, Video Control for a Miniseries, Movie, or Special in 2014. It won the Emmy for Outstanding Technical Direction, Camerawork, Video Control for a Miniseries, Movie, or Special.\n\nA soundtrack for the broadcast was released on December 3, 2013 and consists of 22 studio recordings. Walmart outlets exclusively released a nine-track instrumental (sing-along) bonus CD with the soundtrack. It peaked at number 17 on the \"Billboard\" 200 and number 2 on the \"Billboard\" Soundtracks chart. The soundtrack had sold 103,000 copies as of January 2, 2014.\n\nA DVD of the special was released on December 17, 2013, through Universal Studios Home Entertainment. The DVD includes a behind-the-scenes look, titled \"The Making of \"The Sound of Music Live!\"\" and a preview of the soundtrack. It was released on DVD in Canada on January 7, 2014.\n\n\n"}
{"id": "19206107", "url": "https://en.wikipedia.org/wiki?curid=19206107", "title": "Third Eurovision Dance Contest", "text": "Third Eurovision Dance Contest\n\nThe third Eurovision Dance Contest was originally planned to be organised in Baku, Azerbaijan at the Heydar Aliyev Sports and Exhibition Complex on 26 September 2009. İctimai Television was planning to increase the number of participating countries as well as inviting a world-famous star to host the contest, listing Jennifer Lopez, Kylie Minogue and Shakira as candidates. An additional extravaganza open-air concert was planned to be held, bringing together ex-participants of the Eurovision, Junior Eurovision and Eurovision Dance Contests on one stage.\n\nOn the date of cancellation, two countries had finished selection of the participating couples for the contest:\n\n\nHowever, on 28 May 2009, European Broadcasting Union (EBU) had announced that the contest was postponed at least until Autumn 2010 as \"the number of broadcasters that signed up for participation had not reach the desired level\". According to a preliminary calculations, at least five countries that were taking part in Eurovision Dance Contest 2008, namely , , , and had announced withdrawal from the contest, with only confirming its participation as a début country.\n\nAccording to contest coordinator on behalf of EBU Tal Barnea \"concrete plans for a 2010 autumn event were being developed, with considerable changes to introduce a new programme proposal\". These plans were expected to be unveiled in the autumn of 2009. EBU also praised \"the commendable work on the next Eurovision Dance Contest already completed by our partners Ictimai Television and the Azerbaijani officials\" stating that 2010 edition of the contest was planned to take place in Baku, Azerbaijan as well. \n\nIn January 2010, EBU Eurovision coordinator, Svante Stockselius, announced that contest has been postponed again, and is now unlikely to happen at all, at least within the next couple of years. He explained this decision was the fact that the popularity of televised dance shows had calmed down recently.\n\n"}
{"id": "33761097", "url": "https://en.wikipedia.org/wiki?curid=33761097", "title": "Use error", "text": "Use error\n\nThe term use error has recently been introduced to replace the commonly used terms human error and user error. The new term, which has already been adopted by international standards organizations for medical devices (see #Use errors in health care below for references), suggests that accidents should be attributed to the circumstances, rather than to the human beings who happened to be there.\n\nThe term \"use error\" was first used in May 1995 in an MD+DI guest editorial, “The Issue Is ‘Use,’ Not ‘User,’ Error,” by William Hyman. Traditionally, human errors are considered as a special aspect of human factors. Accordingly, they are attributed to the human operator, or user.\nWhen taking this approach, we assume that the system design is perfect, and the only source for the use errors is the human operator. For example, the U.S. Department of Defense (DoD) HFACS \nclassifies use errors attributed to the human operator, disregarding improper design and configuration setting, which often result in missing alarms, or in inappropriate alerting.\n\nThe need for changing the term was due to a common malpractice of the stakeholders (the responsible organizations, the authorities, journalists) in cases of accidents. Instead of investing in fixing the error-prone design, management attributed the error to the users.\nThe need for the change has been pointed out by the accident investigators: \n\nA mishap is typically considered as either a use error or a force majeure:\n\n\nIn 1998, Cook, Woods and Miller presented the concept of hindsight bias, exemplified by celebrated accidents in medicine, by a workgroup on patient safety\nThe workgroup pointed at the tendency to attribute accidents in health care to isolated human failures.\nThey provide references to early research about the effect of knowledge of the outcome, which was unavailable beforehand, on later judgement about the processes that led up to that outcome. They explain that in looking back, we tend to oversimplify the situation that the actual practitioners faces. They conclude focusing on the hindsight knowledge prevents our understanding of the richer story, the circumstances of the human error.\n\nAccording to this position, the term Use Error is formally defined in several international standards, such as IEC 62366, ISO 14155 and ISO 14971, to describe\n\nISO standards about medical devices and procedures provide examples of use errors, which are attributed to human factors, include slips, lapses and mistakes. Practically, this means that they are attributed to the user, implying the user’s accountability.\nThe U.S. Food and Drug Administration glossary of medical devices provides the following explanation about this term:\n\nWith this interpretation by ISO and the FDA, the term ‘use error’ is actually synonymous with ‘user error’.\nAnother approach, which distinguishes ‘use errors’ from ‘user errors', is taken by IEC 62366. Annex A includes an explanation justifying the new term:\n\nThis explanation complies with “The New View”, which Sidney Dekker suggested as an alternative to “The Old View”. This interpretation favors investigations intended to understand the situation, rather than blaming the operators.\n\nIn a 2011 report draft on health IT usability, the U.S. National Institute of Standards and Technology (NIST) defines \"use error\" in healthcare IT this way: “Use error is a term used very specifically to refer to user interface designs that will engender users to make errors of commission or omission. It is true that users do make errors, but many errors are due not to user error per se but due to designs that are flawed, e.g., poorly written messaging, misuse of color-coding conventions, omission of information, etc.\".\n\nAn example of an accident due to a user error is the ecological disaster of 1967 caused by the Torrey Canyon supertanker. The accident was due to a combination of several exceptional events, the result of which was that the supertanker was heading directly to the rocks. At that point, the captain failed to change the course because the steering control lever was inadvertently set to the Control position, which disconnected the rudder from the wheel at the helm.\n\nExamples of the second type are the Three Mile Island accident described above, the NYC blackout following a storm and the chemical plant disaster in Bhopal, India (Bhopal Disaster).\n\nThe URM Model characterizes use errors in terms of the user’s failure to manage a system deficiency. Six categories of use errors are described in a URM document:\n\nErik Hollnagel argues that going from and 'old' view to a 'new' view is not enough. One should go all the way to a 'no' view. This means that the notion of error, whether user error or use error might be destructive rather than constructive. \nInstead, he proposes to focus on the performance variability of everyday actions, on the basis that this performance variability is both useful and necessary. In most cases the result is that things go right, in a few cases that things go wrong. But the reason is the same.\n\nHollnagel expanded on this in his writings about the efficiency–thoroughness trade-off principle\nof Resilience Engineering,\n\nand the Resilient Health Care Net.\n"}
{"id": "20116588", "url": "https://en.wikipedia.org/wiki?curid=20116588", "title": "Zero Waste Event", "text": "Zero Waste Event\n\nA Zero Waste Event (or \"ZeeWee\" as it has been nicknamed) is one in which event organizers plan ahead to reduce solid waste from the event, reuse various elements such as banners, and set up Zero-Waste Stations for those recyclable and compostable materials such as paper cups, food scraps, and plastic water bottles that are generated by the event. ZeeWees can range from large scale sports events to weddings and parties.\n\nAs sustainability becomes ever more accepted as a concept within organizations and communities across the nation, the idea of Zero Waste Events is spreading. Additionally, the steady expansion of recycling and commercial composting infrastructure makes it possible to offer recycling and food composting at events in many locations. Increasingly Zero Waste Events are sharing best practices and developing new ways to divert event materials from landfills.\n\nZeeWee's utilize many if not all of the following strategies.\n\nFront-end waste prevention\n\nReuse of event equipment\n\nRecycling event materials\n\nWhen the bid was submitted for the London 2012 Olympic and Paralympic Games, LOCOG, The London 2012 Organising Committee of the Olympic and Paralympic Games, pledged not just to host the biggest sporting event in the world but also to stage the first truly sustainable Games. Central to this was an ambitious target to send zero waste from Games-time venues directly to landfill – something that no Games has attempted before. \nIn February 2012 an initiative was launched to share the lessons learnt from delivering a zero waste games with the wider waste and events community.\n"}
