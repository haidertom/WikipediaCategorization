{"id": "57724143", "url": "https://en.wikipedia.org/wiki?curid=57724143", "title": "AMADEE-18", "text": "AMADEE-18\n\nAMADEE-18 is a Mars simulation project. It was launched in February 2018 in a desert in Oman. It is mission of the Austrian Space Forum (OeWF). \n\n"}
{"id": "847879", "url": "https://en.wikipedia.org/wiki?curid=847879", "title": "Age of the universe", "text": "Age of the universe\n\nIn physical cosmology, the age of the universe is the time elapsed since the Big Bang. The current measurement of the age of the universe is billion (10) years within the Lambda-CDM concordance model. The uncertainty has been narrowed down to 21 million years, based on a number of projects that all give extremely close figures for the age. These include studies of the microwave background radiation, and measurements by the Planck satellite, the Wilkinson Microwave Anisotropy Probe and other probes. Measurements of the cosmic background radiation give the cooling time of the universe since the Big Bang, and measurements of the expansion rate of the universe can be used to calculate its approximate age by extrapolating backwards in time.\n\nThe Lambda-CDM concordance model describes the evolution of the universe from a very uniform, hot, dense primordial state to its present state over a span of about 13.8 billion years of cosmological time. This model is well understood theoretically and strongly supported by recent high-precision astronomical observations such as WMAP. In contrast, theories of the origin of the primordial state remain very speculative. If one extrapolates the Lambda-CDM model backward from the earliest well-understood state, it quickly (within a small fraction of a second) reaches a singularity. This is known as the \"initial singularity\" or the \"Big Bang singularity\". This singularity is not understood as having a physical significance in the usual sense, but it is convenient to quote times measured \"since the Big Bang\" even though they do not correspond to a physically measurable time. For example, \"10 seconds after the Big Bang\" is a well-defined era in the universe's evolution. If one referred to the same era as \"13.8 billion years minus 10 seconds ago\", the precision of the meaning would be lost because the minuscule latter time interval is eclipsed by uncertainty in the former.\n\nThough the universe might in theory have a longer history, the International Astronomical Union presently use \"age of the universe\" to mean the duration of the Lambda-CDM expansion, or equivalently the elapsed time since the Big Bang in the current observable universe.\n\nSince the universe must be at least as old as the oldest things in it, there are a number of observations which put a lower limit on the age of the universe; these include the temperature of the coolest white dwarfs, which gradually cool as they age, and the dimmest turnoff point of main sequence stars in clusters (lower-mass stars spend a greater amount of time on the main sequence, so the lowest-mass stars that have evolved off of the main sequence set a minimum age).\n\nThe problem of determining the age of the universe is closely tied to the problem of determining the values of the cosmological parameters. Today this is largely carried out in the context of the ΛCDM model, where the universe is assumed to contain normal (baryonic) matter, cold dark matter, radiation (including both photons and neutrinos), and a cosmological constant. The fractional contribution of each to the current energy density of the universe is given by the density parameters Ω, Ω, and Ω. The full ΛCDM model is described by a number of other parameters, but for the purpose of computing its age these three, along with the Hubble parameter formula_1, are the most important.\n\nIf one has accurate measurements of these parameters, then the age of the universe can be determined by using the Friedmann equation. This equation relates the rate of change in the scale factor \"a\"(\"t\") to the matter content of the universe. Turning this relation around, we can calculate the change in time per change in scale factor and thus calculate the total age of the universe by integrating this formula. The age \"t\" is then given by an expression of the form\nwhere formula_1 is the Hubble parameter and the function \"F\" depends only on the fractional contribution to the universe's energy content that comes from various components. The first observation that one can make from this formula is that it is the Hubble parameter that controls that age of the universe, with a correction arising from the matter and energy content. So a rough estimate of the age of the universe comes from the Hubble time, the inverse of the Hubble parameter. With a value for formula_1 around , the Hubble time evaluates to formula_5 = billion years.\n\nTo get a more accurate number, the correction factor \"F\" must be computed. In general this must be done numerically, and the results for a range of cosmological parameter values are shown in the figure. For the Planck values (Ω, Ω) = (0.3086, 0.6914), shown by the box in the upper left corner of the figure, this correction factor is about \"F\" = 0.956. For a flat universe without any cosmological constant, shown by the star in the lower right corner, \"F\" = is much smaller and thus the universe is younger for a fixed value of the Hubble parameter. To make this figure, Ω is held constant (roughly equivalent to holding the CMB temperature constant) and the curvature density parameter is fixed by the value of the other three.\n\nApart from the Planck satellite, the Wilkinson Microwave Anisotropy Probe (WMAP) was instrumental in establishing an accurate age of the universe, though other measurements must be folded in to gain an accurate number. CMB measurements are very good at constraining the matter content Ω and curvature parameter Ω. It is not as sensitive to Ω directly, partly because the cosmological constant becomes important only at low redshift. The most accurate determinations of the Hubble parameter \"H\" come from Type Ia supernovae. Combining these measurements leads to the generally accepted value for the age of the universe quoted above.\n\nThe cosmological constant makes the universe \"older\" for fixed values of the other parameters. This is significant, since before the cosmological constant became generally accepted, the Big Bang model had difficulty explaining why globular clusters in the Milky Way appeared to be far older than the age of the universe as calculated from the Hubble parameter and a matter-only universe. Introducing the cosmological constant allows the universe to be older than these clusters, as well as explaining other features that the matter-only cosmological model could not.\n\nNASA's Wilkinson Microwave Anisotropy Probe (WMAP) project's nine-year data release in 2012 estimated the age of the universe to be years (13.772 billion years, with an uncertainty of plus or minus 59 million years).\n\nHowever, this age is based on the assumption that the project's underlying model is correct; other methods of estimating the age of the universe could give different ages. Assuming an extra background of relativistic particles, for example, can enlarge the error bars of the WMAP constraint by one order of magnitude.\n\nThis measurement is made by using the location of the first acoustic peak in the microwave background power spectrum to determine the size of the decoupling surface (size of the universe at the time of recombination). The light travel time to this surface (depending on the geometry used) yields a reliable age for the universe. Assuming the validity of the models used to determine this age, the residual accuracy yields a margin of error near one percent.\n\nIn 2015, the Planck Collaboration estimated the age of the universe to be billion years, slightly higher but within the uncertainties of the earlier number derived from the WMAP data. By combining the Planck data with external data, the best combined estimate of the age of the universe is old. \n\nCalculating the age of the universe is accurate only if the assumptions built into the models being used to estimate it are also accurate. This is referred to as strong priors and essentially involves stripping the potential errors in other parts of the model to render the accuracy of actual observational data directly into the concluded result. Although this is not a valid procedure in all contexts (as noted in the accompanying caveat: \"based on the fact we have assumed the underlying model we used is correct\"), the age given is thus accurate to the specified error (since this error represents the error in the instrument used to gather the raw data input into the model).\n\nThe age of the universe based on the best fit to Planck 2015 data alone is billion years (the estimate of billion years uses Gaussian priors based on earlier estimates from other studies to determine the combined uncertainty). This number represents an accurate \"direct\" measurement of the age of the universe (other methods typically involve Hubble's law and the age of the oldest stars in globular clusters, etc.). It is possible to use different methods for determining the same parameter (in this case – the age of the universe) and arrive at different answers with no overlap in the \"errors\". To best avoid the problem, it is common to show two sets of uncertainties; one related to the actual measurement and the other related to the systematic errors of the model being used.\n\nAn important component to the analysis of data used to determine the age of the universe (e.g. from Planck) therefore is to use a Bayesian statistical analysis, which normalizes the results based upon the priors (i.e. the model). This quantifies any uncertainty in the accuracy of a measurement due to a particular model used.\n\nIn the 18th century, the concept that the age of the Earth was millions, if not billions, of years began to appear. However, most scientists throughout the 19th century and into the first decades of the 20th century presumed that the universe itself was Steady State and eternal, with maybe stars coming and going but no changes occurring at the largest scale known at the time.\n\nThe first scientific theories indicating that the age of the universe might be finite were the studies of thermodynamics, formalized in the mid-19th century. The concept of entropy dictates that if the universe (or any other closed system) were infinitely old, then everything inside would be at the same temperature, and thus there would be no stars and no life. No scientific explanation for this contradiction was put forth at the time.\n\nIn 1915 Albert Einstein published the theory of general relativity and in 1917 constructed the first cosmological model based on his theory. In order to remain consistent with a steady state universe, Einstein added what was later called a cosmological constant to his equations. However, already in 1922, also using Einstein's theory, Alexander Friedmann, and independently five years later Georges Lemaître, showed that the universe cannot be static and must be either expanding or contracting. Einstein's model of a static universe was in addition proved unstable by Arthur Eddington.\n\nThe first direct observational hint that the universe has a finite age came from the observations of 'recession velocities', mostly by Vesto Slipher, combined with distances to the 'nebulae' (galaxies) by Edwin Hubble in a work published in 1929. Earlier in the 20th century, Hubble and others resolved individual stars within certain nebulae, thus determining that they were galaxies, similar to, but external to, our Milky Way Galaxy. In addition, these galaxies were very large and very far away. Spectra taken of these distant galaxies showed a red shift in their spectral lines presumably caused by the Doppler effect, thus indicating that these galaxies were moving away from the Earth. In addition, the farther away these galaxies seemed to be (the dimmer they appeared to us) the greater was their redshift, and thus the faster they seemed to be moving away. This was the first direct evidence that the universe is not static but expanding. The first estimate of the age of the universe came from the calculation of when all of the objects must have started speeding out from the same point. Hubble's initial value for the universe's age was very low, as the galaxies were assumed to be much closer than later observations found them to be.\n\nThe first reasonably accurate measurement of the rate of expansion of the universe, a numerical value now known as the Hubble constant, was made in 1958 by astronomer Allan Sandage. His measured value for the Hubble constant came very close to the value range generally accepted today.\n\nHowever Sandage, like Einstein, did not believe his own results at the time of discovery. His value for the age of the universe was too short to reconcile with the 25-billion-year age estimated at that time for the oldest known stars. Sandage and other astronomers repeated these measurements numerous times, attempting to reduce the Hubble constant and thus increase the resulting age for the universe. Sandage even proposed new theories of cosmogony to explain this discrepancy. This issue was finally resolved by improvements in the theoretical models used for estimating the ages of stars. As of 2013, using the latest models for stellar evolution, the estimated age of the oldest known star is billion years.\n\nThe discovery of microwave cosmic background radiation announced in 1965 finally brought an effective end to the remaining scientific uncertainty over the expanding universe. It was a chance result from work by two teams less than 60 miles apart. In 1964, Arno Penzias and Robert Wilson were trying to detect radio wave echoes with a supersensitive antenna. The antenna persistently detected a low, steady, mysterious noise in the microwave region that was evenly spread over the sky, and was present day and night. After testing, they became certain that the signal did not come from the Earth, the Sun, or our galaxy, but from outside our own galaxy, but could not explain it. At the same time another team, Robert H. Dicke, Jim Peebles, and David Wilkinson, were attempting to detect low level noise which might be left over from the Big Bang and could prove whether the Big Bang theory was correct. The two teams realized that the detected noise was in fact radiation left over from the Big Bang, and that this was strong evidence that the theory was correct. Since then, a great deal of other evidence has strengthened and confirmed this conclusion, and refined the estimated age of the universe to its current figure.\n\nThe space probes WMAP, launched in 2001, and Planck, launched in 2009, produced data that determines the Hubble constant and the age of the universe independent of galaxy distances, removing the largest source of error.\n\n"}
{"id": "19468941", "url": "https://en.wikipedia.org/wiki?curid=19468941", "title": "Balance of nature", "text": "Balance of nature\n\nThe balance of nature is a theory that proposes that ecological systems are usually in a stable equilibrium or homeostasis, which is to say that a small change in some particular parameter (the size of a particular population, for example) will be corrected by some negative feedback that will bring the parameter back to its original \"point of balance\" with the rest of the system. It may apply where populations depend on each other, for example in predator/prey systems, or relationships between herbivores and their food source. It is also sometimes applied to the relationship between the Earth's ecosystem, the composition of the atmosphere, and the world's weather.\n\nThe Gaia hypothesis is a balance of nature-based theory that suggests that the Earth and its ecology may act as co-ordinated systems in order to maintain the balance of nature.\n\nThe theory that nature is permanently in balance has been largely discredited by scientists working in ecology, as it has been found that chaotic changes in population levels are common, but nevertheless the idea continues to be popular in the general public. During the later half of the twentieth century the theory was superseded by catastrophe theory and chaos theory.\n\nThe concept that nature maintains its condition is of ancient provenance; Herodotus commented on the wonderful relationship between predator and prey species, which remained in a steady proportion to one another, with predators never excessively consuming their prey populations. The \"balance of nature\" concept once ruled ecological research, as well as once governing the management of natural resources. This led to a doctrine popular among some conservationists that nature was best left to its own devices, and that human intervention into it was by definition unacceptable. The validity of a \"balance of nature\" was already questioned in the early 1900s, but the general abandonment of the theory by scientists working in ecology only happened in the last quarter of that century when studies showed that it did not match what could be observed among plant and animal populations.\n\nPredator-prey populations tend to show chaotic behavior within limits, where the sizes of populations change in a way that may appear random, but is in fact obeying deterministic laws based only on the relationship between a population and its food source illustrated by the Lotka–Volterra equation. An experimental example of this was shown in an eight-year study on small Baltic Sea creatures such as plankton, which were isolated from the rest of the ocean. Each member of the food web was shown to take turns multiplying and declining, even though the scientists kept the outside conditions constant. An article in the journal \"Nature\" stated; \"Advanced mathematical techniques proved the indisputable presence of chaos in this food web ... short-term prediction is possible, but long-term prediction is not.\"\n\nAlthough some conservationist organizations argue that human activity is incompatible with a balanced ecosystem, there are numerous examples in history showing that several modern day habitats originate from human activity: some of Latin America's rain forests owe their existence to humans planting and transplanting them, while the abundance of grazing animals in the Serengeti plain of Africa is thought by some ecologists to be partly due to human-set fires that created savanna habitats.\n\nPossibly one of the best examples of an ecosystem fundamentally modified by human activity can be observed as a consequence of the Australian Aboriginal practice of \"Fire-stick farming\". The legacy of this practice over long periods has resulted in forests being converted to grasslands capable of sustaining larger populations of faunal prey, particularly in the northern and western regions of the continent. So thorough has been the effect of these deliberate regular burnings that many plant and tree species from affected regions have now completely adapted to the annual fire regime in that they require the passage of a fire before their seeds will even germinate. One school in Los Angeles states, \" “We have let our kids go to the forest area of the playground. However, five years later, we found that none of the flowers were growing, the natural damp soil had been hardened, and all of the beautiful grass had been plucked,”.\n\nDespite being discredited among ecologists, the theory is widely held to be true by the general public, with one authority calling it an \"enduring myth\". At least in Midwestern America, the \"balance of nature\" idea was shown to be widely held by both science majors and the general student population. In a study at the University of Patras, educational sciences students were asked to reason about the future of ecosystems which suffered human-driven disturbances. Subjects agreed that it was very likely for the ecosystems to fully recover their initial state, referring to either a 'recovery process' which restores the initial 'balance', or specific 'recovery mechanisms' as an ecosystem's inherent characteristic. In a 2017 study, Ampatzidis and Ergazaki discuss the learning objectives and design criteria that a learning environment for non-biology major students should meet to support them challenge the \"balance of nature\" idea.\n\n"}
{"id": "13662027", "url": "https://en.wikipedia.org/wiki?curid=13662027", "title": "Colloid vibration current", "text": "Colloid vibration current\n\nColloid vibration current is an electroacoustic phenomenon that arises when ultrasound propagates through a fluid that contains ions and either solid particles or emulsion droplets. \n\nThe pressure gradient in an ultrasonic wave moves particles relative to the fluid. This motion disturbs the double layer that exists at the particle-fluid interface. The picture illustrates the mechanism of this distortion. Practically all particles in fluids carry a surface charge. This surface charge is screened with an equally charged diffuse layer; this structure is called the double layer. Ions of the diffuse layer are located in the fluid and can move with the fluid. Fluid motion relative to the particle drags these diffuse ions in the direction of one or the other of the particle's poles. The picture shows ions dragged towards the left hand pole. As a result of this drag, there is an excess of negative ions in the vicinity of the left hand pole and an excess of positive surface charge at the right hand pole. As a result of this charge excess, particles gain a dipole moment. These dipole moments generate an electric field that in turn generates measurable electric current. This phenomenon is widely used for measuring zeta potential in concentrated colloids.\n\n"}
{"id": "40836275", "url": "https://en.wikipedia.org/wiki?curid=40836275", "title": "Cosmic age problem", "text": "Cosmic age problem\n\nThe cosmic age problem is a historical problem in astronomy concerning the age of the universe. The problem was that at various times in the 20th century, some objects in the universe were estimated to be older than the time elapsed since the Big Bang, as estimated from measurements of the expansion rate of the universe known as the Hubble constant, denoted H. (This is more correctly called the Hubble parameter, since it generally varies with time).\nIf so, this would represent a contradiction, since objects such as galaxies, stars and planets could not have existed in the extreme temperatures and densities shortly after the Big Bang.\n\nSince around 1997–2003, the problem is believed to be solved by most cosmologists: modern cosmological measurements lead to a precise estimate of the age of the universe (i.e. time since the Big Bang) of 13.8 billion years, and recent age estimates for the oldest objects are either younger than this, or consistent allowing for measurement uncertainties.\n\nFollowing theoretical developments of the Friedmann equations by Alexander Friedmann and Georges Lemaître in the 1920s, and the discovery of the expanding universe by Edwin Hubble in 1929, it was immediately clear that tracing this expansion backwards in time predicts that the universe had almost zero size at a finite time in the past. This concept, initially known as the \"Primeval Atom\" by Lemaitre, was later elaborated into the modern Big Bang theory. If the universe had expanded at a constant rate in the past, the age of the universe now (i.e. the time since the Big Bang) is simply the inverse of the Hubble constant, often known as the \"Hubble time\". For Big Bang models with zero cosmological constant and positive matter density, the actual age must be somewhat younger than this Hubble time; typically the age would be between 66% and 90% of the Hubble time, depending on the density of matter.\n\nHubble's early estimate of his constant was 550 (km/s)/Mpc, and the inverse of that is 1.8 billion years. It was believed by many geologists such as Arthur Holmes in the 1920s that the Earth was probably over 2 billion years old, but with large uncertainty. The possible discrepancy between the ages of the Earth and the universe was probably one motivation for the development of the Steady State theory in 1948 as an alternative to the Big Bang; in the (now obsolete) steady state theory, the universe is infinitely old and on average unchanging with time. The steady state theory postulated spontaneous creation of matter to keep the average density constant as the universe expands, and therefore most galaxies still have an age less than 1/H. However, if H had been 550 (km/s)/Mpc, our Milky Way galaxy would be exceptionally large compared to most other galaxies, so it could well be much older than an average galaxy, therefore eliminating the age problem.\n\nIn the 1950s, two substantial errors were discovered in Hubble's extragalactic distance scale: first in 1952, Walter Baade discovered there were two classes of Cepheid variable star. Hubble's sample comprised different classes nearby and in other galaxies, and correcting this error made all other galaxies twice as distant as Hubble's values, thus doubling the Hubble time. A second error was discovered by Allan Sandage and coworkers: for galaxies beyond the Local Group, Cepheids were too faint to observe with Hubble's instruments, so Hubble used the brightest stars as distance indicators. Many of Hubble's \"brightest stars\" were actually HII regions or clusters containing many stars, which caused another underestimation of distances for these more distant galaxies. Thus, in 1958 Sandage published the first reasonably accurate measurement of the Hubble constant, at 75 (km/s)/Mpc, which is close to modern estimates of 68–74 (km/s)/Mpc.\n\nThe age of the Earth (actually the Solar System) was first accurately measured around 1955 by Clair Patterson at 4.55 billion years, essentially identical to the modern value. For H ~ 75 (km/s)/Mpc, the inverse of H is 13.0 billion years; so after 1958 the Big Bang model age was comfortably older than the Earth.\n\nHowever, in the 1960s and onwards, new developments in the theory of stellar evolution enabled age estimates for large star clusters called globular clusters: these generally gave age estimates of around 15 billion years, with substantial scatter. Further revisions of the Hubble constant by Sandage and Gustav Tammann in the 1970s gave values around 50–60 (km/s)/Mpc, and an inverse of 16-20 billion years, consistent with globular cluster ages.\n\nHowever, in the late 1970s to early 1990s, the age problem re-appeared: new estimates of the Hubble constant gave higher values, with Gerard de Vaucouleurs estimating values 90–100 (km/s)/Mpc, while Marc Aaronson and co-workers gave values around 80-90  (km/s)/Mpc. Sandage and Tammann continued to argue for values 50-60, leading to a period of controversy sometimes called the \"Hubble wars\". The higher values for H appeared to predict a universe younger than the globular cluster ages, and gave rise to some speculations during the 1980s that the Big Bang model was seriously incorrect.\n\nThe age problem was eventually thought to be resolved by several developments between 1995-2003: firstly, a large program with the Hubble space telescope measured the Hubble constant at 72 (km/s)/Mpc with 10 percent uncertainty. Secondly, measurements of parallax by the Hipparcos spacecraft in 1995 revised globular cluster distances upwards by 5-10 percent; this made their stars brighter than previously estimated and therefore younger, shifting their age estimates down to around 12-13 billion years. Finally, from 1998-2003 a number of new cosmological observations including supernovae, cosmic microwave background observations and large galaxy redshift surveys led to the acceptance of dark energy and the establishment of the Lambda-CDM model as the standard model of cosmology. The presence of dark energy implies that the universe was expanding more slowly at around half its present age than today, which makes the universe older for a given value of the Hubble constant. The combination of the three results above essentially removed the discrepancy between estimated globular cluster ages and the age of the universe.\n\nMore recent measurements from WMAP and the Planck spacecraft lead to an estimate of the age of the universe of 13.80 billion years with only 0.3 percent uncertainty (based on the standard Lambda-CDM model), and modern age measurements for globular clusters and other objects are currently smaller than this value (within the measurement uncertainties). A substantial majority of cosmologists therefore believe the age problem is now resolved.\n\n"}
{"id": "1643492", "url": "https://en.wikipedia.org/wiki?curid=1643492", "title": "Cosmic latte", "text": "Cosmic latte\n\nCosmic latte is a name assigned to the average color of the universe, found by a team of astronomers from Johns Hopkins University. In 2001, Karl Glazebrook and Ivan Baldry determined that the average color of the universe was a greenish white, but they soon corrected their analysis in a 2002 paper in which they reported that their survey of the light from over 200,000 galaxies averaged to a slightly beigeish white. The hex triplet value for cosmic latte is #FFF8E7.\n\nFinding the average color of the universe was not the focus of the study. Rather, the study examined spectral analysis of different galaxies to study star formation. Like Fraunhofer lines, the dark lines displayed in the study's spectral ranges display older and younger stars and allow Glazebrook and Baldry to determine the age of different galaxies and star systems. What the study revealed is that the overwhelming majority of stars formed about 5 billion years ago. Because these stars would have been \"brighter\" in the past, the color of the universe changes over time shifting from blue to red as more blue stars change to yellow and eventually red giants.\n\nAs light from distant galaxies reaches the Earth, the average \"color of the universe\" (as seen from Earth) tends towards pure white, due to the light coming from the stars when they were much younger and bluer.\n\nThe corrected color was initially published on the Johns Hopkins News website and updated on the team's initial announcement. Multiple news outlets, including NPR and BBC, displayed the color in stories and some relayed the request by Glazebrook on the announcement asking for suggestions for names, jokingly adding all were welcome as long as they were not \"beige\".\n\nThese were the results of a vote of the scientists involved based on the new color:\nThough Drum's suggestion of \"cappuccino cosmico\" received the most votes, the researchers favored Drum's other suggestion, \"cosmic latte\". This is because the similar \"Latteo\" means \"Milky\" in Italian, Galileo's native language. It also leads to the similarity to the Italian term for the Milky Way, \"Via Lattea\", and they enjoyed the fact that the color would be similar to the Milky Way's average color as well, as it is part of the sum of the universe. They also claimed to be \"caffeine biased\".\n\nDrum came up with the name while sitting in a Starbucks drinking a latte and reading the \"Washington Post\". Drum noticed that the color of the universe as displayed in the newspaper was the same color as his latte.\n\n"}
{"id": "12527335", "url": "https://en.wikipedia.org/wiki?curid=12527335", "title": "Cosmic time", "text": "Cosmic time\n\nCosmic time (also known as time since the big bang) is the time coordinate commonly used in the Big Bang models of physical cosmology. It is defined for homogeneous, expanding universes as follows: Choose a time coordinate so that the universe has the same density everywhere at each moment in time (the fact that this is possible means that the universe is, by definition, homogeneous). Measure the passage of time using clocks moving with the Hubble flow. Choose the big bang singularity as the origin of the time coordinate. \n\nCosmic time formula_1 is a measure of time by a physical clock with zero peculiar velocity in the absence of matter over-/under-densities (to prevent time dilation due to relativistic effects or confusions caused by expansion of the universe). Unlike other measures of time such as temperature, redshift, particle horizon, or Hubble horizon, the cosmic time (similar and complementary to the comoving coordinates) is blind to the expansion of the universe. \n\nThere are two main ways for establishing a reference point for the cosmic time. The most trivial way is to take the present time as the cosmic reference point (sometimes referred to as the lookback time) or alternatively, take the Big Bang as formula_2 (also referred to as age of the universe). The big bang doesn't necessarily have to correspond to a physical event but rather it refers to the point at which the scale factor would vanish for a standard cosmological model such as ΛCDM. For instance, in the case of inflation, i.e. a non-standard cosmology, the hypothetical moment of big bang is still determined using the benchmark cosmological models which may coincide with the end of the inflationary epoch. For inflationary models, it is not possible to establish a well defined origin of time before the big bang since the universe does not require a beginning event in such models. For technical purposes, concepts such as the average temperature of the universe (in units of eV) or the particle horizon are used when the early universe is the objective of a study since understanding the interaction among particles is more relevant than their time coordinate or age. \n\nCosmic time is the standard time coordinate for specifying the Friedmann–Lemaître–Robertson–Walker solutions of Einstein's equations.\n\n"}
{"id": "42882", "url": "https://en.wikipedia.org/wiki?curid=42882", "title": "Cosmogony", "text": "Cosmogony\n\nCosmogony is any model concerning the origin of either the cosmos or universe. Developing a complete theoretical model has implications in both the philosophy of science and epistemology.\n\nThe word comes from the Koine Greek κοσμογονία (from κόσμος \"cosmos, the world\") and the root of γί(γ)νομαι / γέγονα (\"come into a new state of being\"). In astronomy, cosmogony refers to the study of the origin of particular astrophysical objects or systems, and is most commonly used in reference to the origin of the Universe, the Solar System, or the Earth–Moon system.\n\nThe Big Bang theory is the prevailing cosmological model of the early development of the universe. \nThe most commonly held view is that the universe originates in a gravitational singularity, which expanded extremely rapidly from its hot and dense state. \nCosmologist and science communicator Sean M. Carroll explains two competing types of explanations for the origins of the singularity which is the main disagreement between the scientists who study cosmogony and centers on the question of whether time existed \"before\" the emergence of our universe or not. One cosmogonical view sees time as fundamental and even eternal: The universe could have contained the singularity because the universe evolved or changed from a prior state (the prior state was \"empty space\", or maybe a state that could not be called \"space\" at all). The other view, held by proponents like Stephen Hawking, says that there was no change through time because \"time\" itself emerged along with this universe (in other words, there can be no \"prior\" to the universe). Thus, it remains unclear what combination of \"stuff\", space, or time emerged with the singularity and this universe.\n\nOne problem in cosmogony is that there is currently no theoretical model that explains the earliest moments of the universe's existence (during the Planck time) because of a lack of a testable theory of quantum gravity. Researchers in string theory and its extensions (for example, M theory), and of loop quantum cosmology, have nevertheless proposed solutions of the type just discussed.\n\nCosmogony can be distinguished from cosmology, which studies the universe at large and throughout its existence, and which technically does not inquire directly into the source of its origins. There is some ambiguity between the two terms. For example, the cosmological argument from theology regarding the existence of God is technically an appeal to cosmogonical rather than cosmological ideas. In practice, there is a scientific distinction between cosmological and cosmogonical ideas. Physical cosmology is the science that attempts to explain all observations relevant to the development and characteristics of the universe as a whole. Questions regarding why the universe behaves in such a way have been described by physicists and cosmologists as being extra-scientific (i.e., metaphysical), though speculations are made from a variety of perspectives that include extrapolation of scientific theories to untested regimes (i.e., at Planck scales), and philosophical or religious ideas.\n\nCosmogonists have only tentative theories for the early stages of the universe and its beginning. , no accelerator experiments probe energies of sufficient magnitude to provide any experimental insight into the behavior of matter at the energy levels that prevailed shortly after the Big Bang. \n\nProposed theoretical scenarios differ radically, and include string theory and M-theory, the Hartle–Hawking initial state, string landscape, brane inflation, the Big Bang, and the ekpyrotic universe. Some of these models are mutually compatible, whereas others are not.\n"}
{"id": "38737", "url": "https://en.wikipedia.org/wiki?curid=38737", "title": "Cosmos", "text": "Cosmos\n\nThe cosmos (, ) is the universe. Using the word \"cosmos\" rather than the word \"universe\" implies viewing the universe as a complex and orderly system or entity; the opposite of chaos.\nThe cosmos, and our understanding of the reasons for its existence and significance, are studied in cosmology - a very broad discipline covering any scientific, religious, or philosophical contemplation of the cosmos and its nature, or reasons for existing. Religious and philosophical approaches may include in their concepts of the cosmos various spiritual entities or other matters deemed to exist outside our physical universe.\n\nThe philosopher Pythagoras first used the term \"cosmos\" () for the order of the universe. The term became part of modern language in the 19th century when geographer–polymath Alexander von Humboldt resurrected the use of the word from the ancient Greek, assigned it to his five-volume treatise, \"Kosmos\", which influenced modern and somewhat holistic perception of the universe as one interacting entity.\n\nCosmology is the study of the cosmos, and in its broadest sense covers a variety of very different approaches: scientific, religious and philosophical. All cosmologies have in common an attempt to understand the implicit order within the whole of being. In this way, most religions and philosophical systems have a cosmology.\n\nWhen \"cosmology\" is used without a qualifier, it often signifies physical cosmology, unless the context makes clear that a different meaning is intended.\n\nPhysical cosmology (often simply described as 'cosmology') is the scientific study of the universe, from the beginning of its physical existence. It includes speculative concepts such as a multiverse, when these are being discussed. In physical cosmology, the term \"cosmos\" is often used in a technical way, referring to a particular spacetime continuum within a (postulated) multiverse. Our particular cosmos, the observable universe, is generally capitalized as \"the Cosmos\". \n\nIn physical cosmology, the uncapitalized term cosmic signifies a subject with a relationship to the universe, such as 'cosmic time' (time since the Big Bang), 'cosmic rays' (high energy particles or radiation detected from space), and 'cosmic microwave background' (microwave radiation detectable from all directions in space).\n\nAccording to in Sir William Smith \"Dictionary of Greek and Roman Biography and Mythology\" (1870, see book screenshot for full quote), Pythagoreans described the universe.\n\nCosmology is a branch of metaphysics that deals with the nature of the universe, a theory or doctrine describing the natural order of the universe. The basic definition of Cosmology is the science of the origin and development of the universe. In modern astronomy the Big Bang theory is the dominant postulation.\n\nIn theology, the cosmos is the created heavenly bodies (sun, moon, planets, and fixed stars). In Christian theology, the word is also used synonymously with \"aion\" to refer to \"worldly life\" or \"this world\" or \"this age\" as opposed to the afterlife or world to come.\n\nThe 1870 book \"Dictionary of Greek and Roman Biography and Mythology\" noted\n\nThe book \"The Works of Aristotle\" (1908, p. 80 \"Fragments\") mentioned\n\nBertrand Russell (1947) noted\n\n"}
{"id": "13566984", "url": "https://en.wikipedia.org/wiki?curid=13566984", "title": "Double layer (surface science)", "text": "Double layer (surface science)\n\nA double layer (DL, also called an electrical double layer, EDL) is a structure that appears on the surface of an object when it is exposed to a fluid. The object might be a solid particle, a gas bubble, a liquid droplet, or a porous body. The DL refers to two parallel layers of charge surrounding the object. The first layer, the surface charge (either positive or negative), consists of ions adsorbed onto the object due to chemical interactions. The second layer is composed of ions attracted to the surface charge via the Coulomb force, electrically screening the first layer. This second layer is loosely associated with the object. It is made of free ions that move in the fluid under the influence of electric attraction and thermal motion rather than being firmly anchored. It is thus called the \"diffuse layer\".\n\nInterfacial DLs are most apparent in systems with a large surface area to volume ratio, such as a colloid or porous bodies with particles or pores (respectively) on the scale of micrometres to nanometres. However, DLs are important to other phenomena, such as the electrochemical behaviour of electrodes.\n\nDLs play a fundamental role in many everyday substances. For instance, homogenized milk exists only because fat droplets are covered with a DL that prevents their coagulation into butter. DLs exist in practically all heterogeneous fluid-based systems, such as blood, paint, ink and ceramic and cement slurry.\n\nThe DL is closely related to electrokinetic phenomena and electroacoustic phenomena.\n\nWhen an \"electronic\" conductor is brought in contact with a solid or liquid \"ionic\" conductor (electrolyte), a common boundary (interface) among the two phases appears. Hermann von Helmholtz was the first to realize that charged electrodes immersed in electrolyte solutions repel the co-ions of the charge while attracting counterions to their surfaces. Two layers of opposite polarity form at the interface between electrode and electrolyte.\nIn 1853 he showed that an electrical double layer (DL) is essentially a molecular dielectric and stores charge electrostatically. Below the electrolyte's decomposition voltage, the stored charge is linearly dependent on the voltage applied.\n\nThis early model predicted a constant differential capacitance independent from the charge density depending on the dielectric constant of the electrolyte solvent and the thickness of the double-layer.\n\nThis model, with a good foundation for the description of the interface, does not consider important factors including diffusion/mixing of ions in solution, the possibility of adsorption onto the surface and the interaction between solvent dipole moments and the electrode.\n\nLouis Georges Gouy in 1910 and David Leonard Chapman in 1913 both observed that capacitance was not a constant and that it depended on the applied potential and the ionic concentration. The \"Gouy-Chapman model\" made significant improvements by introducing a diffuse model of the DL. In this model the charge distribution of ions as a function of distance from the metal surface allows Maxwell–Boltzmann statistics to be applied. Thus the electric potential decreases exponentially away from the surface of the fluid bulk.\n\nThe Gouy-Chapman model fails for highly charged DLs. In 1924 Otto Stern suggested combining the Helmholtz model with the Gouy-Chapman model: In Stern's model, some ions adhere to the electrode as suggested by Helmholtz, giving an internal Stern layer, while some form a Gouy-Chapman diffuse layer.\n\nThe Stern layer accounts for ions' finite size and consequently an ion's closest approach to the electrode is on the order of the ionic radius. The Stern model has its own limitations, namely that it effectively treats ions as point charges, assumes all significant interactions in the diffuse layer are Coulombic, and assumes dielectric permittivity to be constant throughout the double layer and that fluid viscosity is constant plane.\n\nD. C. Grahame modified the Stern model in 1947. He proposed that some ionic or uncharged species can penetrate the Stern layer, although the closest approach to the electrode is normally occupied by solvent molecules. This could occur if ions lose their solvation shell as they approach the electrode. He called ions in direct contact with the electrode \"specifically adsorbed ions\". This model proposed the existence of three regions. The inner Helmholtz plane (IHP) passes through the centres of the specifically adsorbed ions. The outer Helmholtz plane (OHP) passes through the centres of solvated ions at the distance of their closest approach to the electrode. Finally the diffuse layer is the region beyond the OHP.\n\nIn 1963 J. O'M. Bockris, M. A. V. Devanathan and Klaus Müller proposed the BDM model of the double-layer that included the action of the solvent in the interface. They suggested that the attached molecules of the solvent, such as water, would have a fixed alignment to the electrode surface. This first layer of solvent molecules displays a strong orientation to the electric field depending on the charge. This orientation has great influence on the permittivity of the solvent that varies with field strength. The IHP passes through the centers of these molecules. Specifically adsorbed, partially solvated ions appear in this layer. The solvated ions of the electrolyte are outside the IHP. Through the centers of these ions pass the OHP. The diffuse layer is the region beyond the OHP.\n\nFurther research with double layers on ruthenium dioxide films in 1971 by Sergio Trasatti and Giovanni Buzzanca demonstrated that the electrochemical behavior of these electrodes at low voltages with specific adsorbed ions was like that of capacitors. The specific adsorption of the ions in this region of potential could also involve a partial charge transfer between the ion and the electrode. It was the first step towards understanding pseudocapacitance.\n\nBetween 1975 and 1980 Brian Evans Conway conducted extensive fundamental and development work on ruthenium oxide electrochemical capacitors. In 1991 he described the difference between 'Supercapacitor' and 'Battery' behavior in electrochemical energy storage. In 1999 he coined the term supercapacitor to explain the increased capacitance by surface redox reactions with faradaic charge transfer between electrodes and ions.\n\nHis \"supercapacitor\" stored electrical charge partially in the Helmholtz double-layer and partially as the result of faradaic reactions with \"pseudocapacitance\" charge transfer of electrons and protons between electrode and electrolyte. The working mechanisms of pseudocapacitors are redox reactions, intercalation and electrosorption.\n\nThe physical and mathematical basics of electron charge transfer absent chemical bonds leading to pseudocapacitance was developed by Rudolph A. Marcus. Marcus Theory explains the rates of electron transfer reactions—the rate at which an electron can move from one chemical species to another. It was originally formulated to address outer sphere electron transfer reactions, in which two chemical species change only in their charge, with an electron jumping. For redox reactions without making or breaking bonds, Marcus theory takes the place of Henry Eyring's transition state theory which was derived for reactions with structural changes. Marcus received the Nobel Prize in Chemistry in 1992 for this theory.\n\nThere are detailed descriptions of the interfacial DL in many books on colloid and interface science and microscale fluid transport. There is also a recent IUPAC technical report on the subject of interfacial double layer and related electrokinetic phenomena.\n\nAs stated by Lyklema, \"...the reason for the formation of a \"relaxed\" (\"equilibrium\") double layer is the non-electric affinity of charge-determining ions for a surface...\" This process leads to the buildup of an electric surface charge, expressed usually in C/m. This surface charge creates an electrostatic field that then affects the ions in the bulk of the liquid. This electrostatic field, in combination with the thermal motion of the ions, creates a counter charge, and thus screens the electric surface charge. The net electric charge in this screening diffuse layer is equal in magnitude to the net surface charge, but has the opposite polarity. As a result, the complete structure is electrically neutral.\n\nThe diffuse layer, or at least part of it, can move under the influence of tangential stress. There is a conventionally introduced slipping plane that separates mobile fluid from fluid that remains attached to the surface. Electric potential at this plane is called electrokinetic potential or zeta potential (also denoted as ζ-potential).\n\nThe electric potential on the external boundary of the Stern layer versus the bulk electrolyte is referred to as Stern potential. Electric potential difference between the fluid bulk and the surface is called the electric surface potential.\n\nUsually zeta potential is used for estimating the degree of DL charge. A characteristic value of this electric potential in the DL is 25 mV with a maximum value around 100 mV (up to several volts on electrodes). The chemical composition of the sample at which the ζ-potential is 0 is called the point of zero charge or the iso-electric point. It is usually determined by the solution pH value, since protons and hydroxyl ions are the charge-determining ions for most surfaces.\n\nZeta potential can be measured using electrophoresis, electroacoustic phenomena, streaming potential, and electroosmotic flow.\n\nThe characteristic thickness of the DL is the Debye length, κ. It is reciprocally proportional to the square root of the ion concentration \"C\". In aqueous solutions it is typically on the scale of a few nanometers and the thickness decreases with increasing concentration of the electrolyte.\n\nThe electric field strength inside the DL can be anywhere from zero to over 10 V/m. These steep electric potential gradients are the reason for the importance of the DLs.\n\nThe theory for a flat surface and a symmetrical electrolyte is usually referred to as the Gouy-Chapman theory. It yields a simple relationship between electric charge in the diffuse layer σ and the Stern potential Ψ:\n"}
{"id": "57151017", "url": "https://en.wikipedia.org/wiki?curid=57151017", "title": "Drakoo wave energy converter", "text": "Drakoo wave energy converter\n\nThe Drakoo wave energy converter is a technological device that uses the motion of ocean surface waves to generate electricity.\n\nThe Drakoo WEC does not fall under any of the usual wave energy converter classifications: its working principle, based on a twin-chamber oscillating water column system, is to transform waves into a continuous water flow which drives a hydro turbine generator.\n\nafter being patented in 2008, the Drakoo technology has successfully been tested in NTU lab and, subsequently, in the deep wave flume of NAREC in 2012(UK).\n\nOn December 23, 2016, Hann Ocean Energy, the Singapore wave developer company which invented and patented the technology, announced the first successful power production during a trial in its testing facility in Nantong, with a peak power output of 3.8 kW at a wave height of 0.6m.\n\nThe technology continuously increased its performances along the years, reaching firstly a peak output power of 9.3 kW in November 2017 and of 11.2 kW in March 2018. Moreover, during the World Future Energy Summit 2018, in Abu Dhabi, Hann Ocean Energy reported about sales inquiries from the Persian Gulf for the application of the Drakoo WEC for wellhead platforms.\n"}
{"id": "36238152", "url": "https://en.wikipedia.org/wiki?curid=36238152", "title": "Driving factors", "text": "Driving factors\n\nIn energy monitoring and targeting, a driving factor is something recurrent and measurable whose variation explains variation in energy consumption. The term \"independent variable\" is sometimes used as a synonym.\n\nOne of the most common driving factors is the weather, expressed usually as heating or cooling degree days. In energy-intensive processes, production throughputs would usually be used. For electrical circuits feeding outdoor lighting, the number of hours of darkness can be employed. For a borehole pump, the quantity of water delivered would be used; and so on. What these examples all have in common is that on a weekly basis (say) numerical values can be recorded for each factor and one would expect particular streams of energy consumption to correlate with them either singly or in a multi-variate model.\n\nCorrelation is arguably more important than causality. Variation in the driving factor merely has to \"explain\" variation in consumption; it does not necessarily have to \"cause\" it, although that will in most scenarios be the case.\n\nDriving factors differ from \"static\" factors, such as building floor areas, which determine energy consumption but change only rarely (if at all).\n"}
{"id": "9228", "url": "https://en.wikipedia.org/wiki?curid=9228", "title": "Earth", "text": "Earth\n\nEarth is the third planet from the Sun and the only astronomical object known to harbor life. According to radiometric dating and other sources of evidence, Earth formed over 4.5 billion years ago. Earth's gravity interacts with other objects in space, especially the Sun and the Moon, Earth's only natural satellite. Earth revolves around the Sun in 365.26 days, a period known as an Earth year. During this time, Earth rotates about its axis about 366.26 times.\n\nEarth's axis of rotation is tilted with respect to its orbital plane, producing seasons on Earth. The gravitational interaction between Earth and the Moon causes ocean tides, stabilizes Earth's orientation on its axis, and gradually slows its rotation. Earth is the densest planet in the Solar System and the largest of the four terrestrial planets.\n\nEarth's lithosphere is divided into several rigid tectonic plates that migrate across the surface over periods of many millions of years. About 71% of Earth's surface is covered with water, mostly by oceans. The remaining 29% is land consisting of continents and islands that together have many lakes, rivers and other sources of water that contribute to the hydrosphere. The majority of Earth's polar regions are covered in ice, including the Antarctic ice sheet and the sea ice of the Arctic ice pack. Earth's interior remains active with a solid iron inner core, a liquid outer core that generates the Earth's magnetic field, and a convecting mantle that drives plate tectonics.\n\nWithin the first billion years of Earth's history, life appeared in the oceans and began to affect the Earth's atmosphere and surface, leading to the proliferation of aerobic and anaerobic organisms. Some geological evidence indicates that life may have arisen as much as 4.1 billion years ago. Since then, the combination of Earth's distance from the Sun, physical properties, and geological history have allowed life to evolve and thrive. In the history of the Earth, biodiversity has gone through long periods of expansion, occasionally punctuated by mass extinction events. Over 99% of all species that ever lived on Earth are extinct. Estimates of the number of species on Earth today vary widely; most species have not been described. Over 7.6 billion humans live on Earth and depend on its biosphere and natural resources for their survival. Humans have developed diverse societies and cultures; politically, the world has about 200 sovereign states.\n\nThe modern English word \"Earth\" developed from a wide variety of Middle English forms, which derived from an Old English noun most often spelled '. It has cognates in every Germanic language, and their proto-Germanic root has been reconstructed as *\"erþō\". In its earliest appearances, \"eorðe\" was already being used to translate the many senses of Latin ' and Greek (\"gē\"): the ground, its soil, dry land, the human world, the surface of the world (including the sea), and the globe itself. As with Terra and Gaia, Earth was a personified goddess in Germanic paganism: the Angles were listed by Tacitus as among the devotees of Nerthus, and later Norse mythology included Jörð, a giantess often given as the mother of Thor.\n\nOriginally, \"earth\" was written in lowercase, and from early Middle English, its definite sense as \"the globe\" was expressed as \"the earth\". By Early Modern English, many nouns were capitalized, and \"the earth\" became (and often remained) \"the Earth\", particularly when referenced along with other heavenly bodies. More recently, the name is sometimes simply given as \"Earth\", by analogy with the names of the other planets. House styles now vary: Oxford spelling recognizes the lowercase form as the most common, with the capitalized form an acceptable variant. Another convention capitalizes \"Earth\" when appearing as a name (e.g. \"Earth's atmosphere\") but writes it in lowercase when preceded by \"the\" (e.g. \"the atmosphere of the earth\"). It almost always appears in lowercase in colloquial expressions such as \"what on earth are you doing?\"\n\nThe oldest material found in the Solar System is dated to (Bya). By the primordial Earth had formed. The bodies in the Solar System formed and evolved with the Sun. In theory, a solar nebula partitions a volume out of a molecular cloud by gravitational collapse, which begins to spin and flatten into a circumstellar disk, and then the planets grow out of that disk with the Sun. A nebula contains gas, ice grains, and dust (including primordial nuclides). According to nebular theory, planetesimals formed by accretion, with the primordial Earth taking 10– (Mys) to form.\n\nA subject of research is the formation of the Moon, some 4.53 Bya. A leading hypothesis is that it was formed by accretion from material loosed from Earth after a Mars-sized object, named Theia, hit Earth. In this view, the mass of Theia was approximately 10 percent of Earth, it hit Earth with a glancing blow and some of its mass merged with Earth. Between approximately 4.1 and , numerous asteroid impacts during the Late Heavy Bombardment caused significant changes to the greater surface environment of the Moon and, by inference, to that of Earth.\n\nEarth's atmosphere and oceans were formed by volcanic activity and outgassing. Water vapor from these sources condensed into the oceans, augmented by water and ice from asteroids, protoplanets, and comets. In this model, atmospheric \"greenhouse gases\" kept the oceans from freezing when the newly forming Sun had only 70% of its current luminosity. By , Earth's magnetic field was established, which helped prevent the atmosphere from being stripped away by the solar wind.\n\nA crust formed when the molten outer layer of Earth cooled to form a solid. The two models that explain land mass propose either a steady growth to the present-day forms or, more likely, a rapid growth early in Earth history followed by a long-term steady continental area. Continents formed by plate tectonics, a process ultimately driven by the continuous loss of heat from Earth's interior. Over the period of hundreds of millions of years, the supercontinents have assembled and broken apart. Roughly (Mya), one of the earliest known supercontinents, Rodinia, began to break apart. The continents later recombined to form Pannotia , then finally Pangaea, which also broke apart .\n\nThe present pattern of ice ages began about and then intensified during the Pleistocene about . High-latitude regions have since undergone repeated cycles of glaciation and thaw, repeating about every . The last continental glaciation ended ago.\n\nChemical reactions led to the first self-replicating molecules about four billion years ago. A half billion years later, the last common ancestor of all current life arose. The evolution of photosynthesis allowed the Sun's energy to be harvested directly by life forms. The resultant molecular oxygen () accumulated in the atmosphere and due to interaction with ultraviolet solar radiation, formed a protective ozone layer () in the upper atmosphere. The incorporation of smaller cells within larger ones resulted in the development of complex cells called eukaryotes. True multicellular organisms formed as cells within colonies became increasingly specialized. Aided by the absorption of harmful ultraviolet radiation by the ozone layer, life colonized Earth's surface. Among the earliest fossil evidence for life is microbial mat fossils found in 3.48 billion-year-old sandstone in Western Australia, biogenic graphite found in 3.7 billion-year-old metasedimentary rocks in Western Greenland, and remains of biotic material found in 4.1 billion-year-old rocks in Western Australia. The earliest direct evidence of life on Earth is contained in 3.45 billion-year-old Australian rocks showing fossils of microorganisms.\n\nDuring the Neoproterozoic, , much of Earth might have been covered in ice. This hypothesis has been termed \"Snowball Earth\", and it is of particular interest because it preceded the Cambrian explosion, when multicellular life forms significantly increased in complexity. Following the Cambrian explosion, , there have been five mass extinctions. The most recent such event was , when an asteroid impact triggered the extinction of the non-avian dinosaurs and other large reptiles, but spared some small animals such as mammals, which at the time resembled shrews. Mammalian life has diversified over the past , and several million years ago an African ape-like animal such as \"Orrorin tugenensis\" gained the ability to stand upright. This facilitated tool use and encouraged communication that provided the nutrition and stimulation needed for a larger brain, which led to the evolution of humans. The development of agriculture, and then civilization, led to humans having an influence on Earth and the nature and quantity of other life forms that continues to this day.\n\nEarth's expected long-term future is tied to that of the Sun. Over the next , solar luminosity will increase by 10%, and over the next by 40%. The Earth's increasing surface temperature will accelerate the inorganic carbon cycle, reducing concentration to levels lethally low for plants ( for C4 photosynthesis) in approximately . The lack of vegetation will result in the loss of oxygen in the atmosphere, making animal life impossible. After another billion years all surface water will have disappeared and the mean global temperature will reach . From that point, the Earth is expected to be habitable for another , possibly up to if nitrogen is removed from the atmosphere. Even if the Sun were eternal and stable, 27% of the water in the modern oceans will descend to the mantle in one billion years, due to reduced steam venting from mid-ocean ridges.\n\nThe Sun will evolve to become a red giant in about . Models predict that the Sun will expand to roughly , about 250 times its present radius. Earth's fate is less clear. As a red giant, the Sun will lose roughly 30% of its mass, so, without tidal effects, Earth will move to an orbit from the Sun when the star reaches its maximum radius. Most, if not all, remaining life will be destroyed by the Sun's increased luminosity (peaking at about 5,000 times its present level). A 2008 simulation indicates that Earth's orbit will eventually decay due to tidal effects and drag, causing it to enter the Sun's atmosphere and be vaporized.\n\nThe shape of Earth is approximately oblate spheroidal. Due to rotation, the Earth is flattened at the poles and bulging around the equator. The diameter of the Earth at the equator is larger than the pole-to-pole diameter. Thus the point on the surface farthest from Earth's center of mass is the summit of the equatorial Chimborazo volcano in Ecuador. The average diameter of the reference spheroid is . Local topography deviates from this idealized spheroid, although on a global scale these deviations are small compared to Earth's radius: The maximum deviation of only 0.17% is at the Mariana Trench ( below local sea level), whereas Mount Everest ( above local sea level) represents a deviation of 0.14%.\n\nIn geodesy, the exact shape that Earth's oceans would adopt in the absence of land and perturbations such as tides and winds is called the geoid. More precisely, the geoid is the surface of gravitational equipotential at mean sea level.\n\nEarth's mass is approximately (5,970 Yg). It is composed mostly of iron (32.1%), oxygen (30.1%), silicon (15.1%), magnesium (13.9%), sulfur (2.9%), nickel (1.8%), calcium (1.5%), and aluminium (1.4%), with the remaining 1.2% consisting of trace amounts of other elements. Due to mass segregation, the core region is estimated to be primarily composed of iron (88.8%), with smaller amounts of nickel (5.8%), sulfur (4.5%), and less than 1% trace elements.\n\nThe most common rock constituents of the crust are nearly all oxides: chlorine, sulfur, and fluorine are the important exceptions to this and their total amount in any rock is usually much less than 1%. Over 99% of the crust is composed of 11 oxides, principally silica, alumina, iron oxides, lime, magnesia, potash, and soda.\n\nEarth's interior, like that of the other terrestrial planets, is divided into layers by their chemical or physical (rheological) properties. The outer layer is a chemically distinct silicate solid crust, which is underlain by a highly viscous solid mantle. The crust is separated from the mantle by the Mohorovičić discontinuity. The thickness of the crust varies from about under the oceans to for the continents. The crust and the cold, rigid, top of the upper mantle are collectively known as the lithosphere, and it is of the lithosphere that the tectonic plates are composed. Beneath the lithosphere is the asthenosphere, a relatively low-viscosity layer on which the lithosphere rides. Important changes in crystal structure within the mantle occur at below the surface, spanning a transition zone that separates the upper and lower mantle. Beneath the mantle, an extremely low viscosity liquid outer core lies above a solid inner core. The Earth's inner core might rotate at a slightly higher angular velocity than the remainder of the planet, advancing by 0.1–0.5° per year. The radius of the inner core is about one fifth of that of Earth.\n\nEarth's internal heat comes from a combination of residual heat from planetary accretion (about 20%) and heat produced through radioactive decay (80%). The major heat-producing isotopes within Earth are potassium-40, uranium-238, and thorium-232. At the center, the temperature may be up to , and the pressure could reach . Because much of the heat is provided by radioactive decay, scientists postulate that early in Earth's history, before isotopes with short half-lives were depleted, Earth's heat production was much higher. At approximately , twice the present-day heat would have been produced, increasing the rates of mantle convection and plate tectonics, and allowing the production of uncommon igneous rocks such as komatiites that are rarely formed today.\n\nThe mean heat loss from Earth is , for a global heat loss of . A portion of the core's thermal energy is transported toward the crust by mantle plumes, a form of convection consisting of upwellings of higher-temperature rock. These plumes can produce hotspots and flood basalts. More of the heat in Earth is lost through plate tectonics, by mantle upwelling associated with mid-ocean ridges. The final major mode of heat loss is through conduction through the lithosphere, the majority of which occurs under the oceans because the crust there is much thinner than that of the continents.\n\nEarth's mechanically rigid outer layer, the lithosphere, is divided into tectonic plates. These plates are rigid segments that move relative to each other at one of three boundaries types: At convergent boundaries, two plates come together; at divergent boundaries, two plates are pulled apart; and at transform boundaries, two plates slide past one another laterally. Along these plate boundaries, earthquakes, volcanic activity, mountain-building, and oceanic trench formation can occur. The tectonic plates ride on top of the asthenosphere, the solid but less-viscous part of the upper mantle that can flow and move along with the plates.\n\nAs the tectonic plates migrate, oceanic crust is subducted under the leading edges of the plates at convergent boundaries. At the same time, the upwelling of mantle material at divergent boundaries creates mid-ocean ridges. The combination of these processes recycles the oceanic crust back into the mantle. Due to this recycling, most of the ocean floor is less than old. The oldest oceanic crust is located in the Western Pacific and is estimated to be old. By comparison, the oldest dated continental crust is .\n\nThe seven major plates are the Pacific, North American, Eurasian, African, Antarctic, Indo-Australian, and South American. Other notable plates include the Arabian Plate, the Caribbean Plate, the Nazca Plate off the west coast of South America and the Scotia Plate in the southern Atlantic Ocean. The Australian Plate fused with the Indian Plate between . The fastest-moving plates are the oceanic plates, with the Cocos Plate advancing at a rate of and the Pacific Plate moving . At the other extreme, the slowest-moving plate is the Eurasian Plate, progressing at a typical rate of .\n\nThe total surface area of Earth is about . Of this, 70.8%, or , is below sea level and covered by ocean water. Below the ocean's surface are much of the continental shelf, mountains, volcanoes, oceanic trenches, submarine canyons, oceanic plateaus, abyssal plains, and a globe-spanning mid-ocean ridge system. The remaining 29.2%, or , not covered by water has terrain that varies greatly from place to place and consists of mountains, deserts, plains, plateaus, and other landforms. Tectonics and erosion, volcanic eruptions, flooding, weathering, glaciation, the growth of coral reefs, and meteorite impacts are among the processes that constantly reshape the Earth's surface over geological time.\n\nThe continental crust consists of lower density material such as the igneous rocks granite and andesite. Less common is basalt, a denser volcanic rock that is the primary constituent of the ocean floors. Sedimentary rock is formed from the accumulation of sediment that becomes buried and compacted together. Nearly 75% of the continental surfaces are covered by sedimentary rocks, although they form about 5% of the crust. The third form of rock material found on Earth is metamorphic rock, which is created from the transformation of pre-existing rock types through high pressures, high temperatures, or both. The most abundant silicate minerals on Earth's surface include quartz, feldspars, amphibole, mica, pyroxene and olivine. Common carbonate minerals include calcite (found in limestone) and dolomite.\n\nThe elevation of the land surface varies from the low point of at the Dead Sea, to a maximum altitude of at the top of Mount Everest. The mean height of land above sea level is about .\n\nThe pedosphere is the outermost layer of Earth's continental surface and is composed of soil and subject to soil formation processes. The total arable land is 10.9% of the land surface, with 1.3% being permanent cropland. Close to 40% of Earth's land surface is used for agriculture, or an estimated of cropland and of pastureland.\n\nThe abundance of water on Earth's surface is a unique feature that distinguishes the \"Blue Planet\" from other planets in the Solar System. Earth's hydrosphere consists chiefly of the oceans, but technically includes all water surfaces in the world, including inland seas, lakes, rivers, and underground waters down to a depth of . The deepest underwater location is Challenger Deep of the Mariana Trench in the Pacific Ocean with a depth of .\n\nThe mass of the oceans is approximately 1.35 metric tons or about 1/4400 of Earth's total mass. The oceans cover an area of with a mean depth of , resulting in an estimated volume of . If all of Earth's crustal surface were at the same elevation as a smooth sphere, the depth of the resulting world ocean would be .\n\nAbout 97.5% of the water is saline; the remaining 2.5% is fresh water. Most fresh water, about 68.7%, is present as ice in ice caps and glaciers.\n\nThe average salinity of Earth's oceans is about 35 grams of salt per kilogram of sea water (3.5% salt). Most of this salt was released from volcanic activity or extracted from cool igneous rocks. The oceans are also a reservoir of dissolved atmospheric gases, which are essential for the survival of many aquatic life forms. Sea water has an important influence on the world's climate, with the oceans acting as a large heat reservoir. Shifts in the oceanic temperature distribution can cause significant weather shifts, such as the El Niño–Southern Oscillation.\n\nThe atmospheric pressure at Earth's sea level averages , with a scale height of about . A dry atmosphere is composed of 78.084% nitrogen, 20.946% oxygen, 0.934% argon, and trace amounts of carbon dioxide and other gaseous molecules. Water vapor content varies between 0.01% and 4% but averages about 1%. The height of the troposphere varies with latitude, ranging between at the poles to at the equator, with some variation resulting from weather and seasonal factors.\n\nEarth's biosphere has significantly altered its atmosphere. Oxygenic photosynthesis evolved , forming the primarily nitrogen–oxygen atmosphere of today. This change enabled the proliferation of aerobic organisms and, indirectly, the formation of the ozone layer due to the subsequent conversion of atmospheric into. The ozone layer blocks ultraviolet solar radiation, permitting life on land. Other atmospheric functions important to life include transporting water vapor, providing useful gases, causing small meteors to burn up before they strike the surface, and moderating temperature. This last phenomenon is known as the greenhouse effect: trace molecules within the atmosphere serve to capture thermal energy emitted from the ground, thereby raising the average temperature. Water vapor, carbon dioxide, methane, nitrous oxide, and ozone are the primary greenhouse gases in the atmosphere. Without this heat-retention effect, the average surface temperature would be , in contrast to the current , and life on Earth probably would not exist in its current form. In May 2017, glints of light, seen as twinkling from an orbiting satellite a million miles away, were found to be reflected light from ice crystals in the atmosphere.\n\nEarth's atmosphere has no definite boundary, slowly becoming thinner and fading into outer space. Three-quarters of the atmosphere's mass is contained within the first of the surface. This lowest layer is called the troposphere. Energy from the Sun heats this layer, and the surface below, causing expansion of the air. This lower-density air then rises and is replaced by cooler, higher-density air. The result is atmospheric circulation that drives the weather and climate through redistribution of thermal energy.\n\nThe primary atmospheric circulation bands consist of the trade winds in the equatorial region below 30° latitude and the westerlies in the mid-latitudes between 30° and 60°. Ocean currents are also important factors in determining climate, particularly the thermohaline circulation that distributes thermal energy from the equatorial oceans to the polar regions.\n\nWater vapor generated through surface evaporation is transported by circulatory patterns in the atmosphere. When atmospheric conditions permit an uplift of warm, humid air, this water condenses and falls to the surface as precipitation. Most of the water is then transported to lower elevations by river systems and usually returned to the oceans or deposited into lakes. This water cycle is a vital mechanism for supporting life on land and is a primary factor in the erosion of surface features over geological periods. Precipitation patterns vary widely, ranging from several meters of water per year to less than a millimeter. Atmospheric circulation, topographic features, and temperature differences determine the average precipitation that falls in each region.\n\nThe amount of solar energy reaching Earth's surface decreases with increasing latitude. At higher latitudes, the sunlight reaches the surface at lower angles, and it must pass through thicker columns of the atmosphere. As a result, the mean annual air temperature at sea level decreases by about per degree of latitude from the equator. Earth's surface can be subdivided into specific latitudinal belts of approximately homogeneous climate. Ranging from the equator to the polar regions, these are the tropical (or equatorial), subtropical, temperate and polar climates.\n\nThis latitudinal rule has several anomalies:\n\nThe commonly used Köppen climate classification system has five broad groups (humid tropics, arid, humid middle latitudes, continental and cold polar), which are further divided into more specific subtypes. The Köppen system rates regions of terrain based on observed temperature and precipitation.\n\nThe highest air temperature ever measured on Earth was in Furnace Creek, California, in Death Valley, in 1913. The lowest air temperature ever directly measured on Earth was at Vostok Station in 1983, but satellites have used remote sensing to measure temperatures as low as in East Antarctica. These temperature records are only measurements made with modern instruments from the 20th century onwards and likely do not reflect the full range of temperature on Earth.\n\nAbove the troposphere, the atmosphere is usually divided into the stratosphere, mesosphere, and thermosphere. Each layer has a different lapse rate, defining the rate of change in temperature with height. Beyond these, the exosphere thins out into the magnetosphere, where the geomagnetic fields interact with the solar wind. Within the stratosphere is the ozone layer, a component that partially shields the surface from ultraviolet light and thus is important for life on Earth. The Kármán line, defined as 100 km above Earth's surface, is a working definition for the boundary between the atmosphere and outer space.\n\nThermal energy causes some of the molecules at the outer edge of the atmosphere to increase their velocity to the point where they can escape from Earth's gravity. This causes a slow but steady loss of the atmosphere into space. Because unfixed hydrogen has a low molecular mass, it can achieve escape velocity more readily, and it leaks into outer space at a greater rate than other gases. The leakage of hydrogen into space contributes to the shifting of Earth's atmosphere and surface from an initially reducing state to its current oxidizing one. Photosynthesis provided a source of free oxygen, but the loss of reducing agents such as hydrogen is thought to have been a necessary precondition for the widespread accumulation of oxygen in the atmosphere. Hence the ability of hydrogen to escape from the atmosphere may have influenced the nature of life that developed on Earth. In the current, oxygen-rich atmosphere most hydrogen is converted into water before it has an opportunity to escape. Instead, most of the hydrogen loss comes from the destruction of methane in the upper atmosphere.\n\nThe gravity of Earth is the acceleration that is imparted to objects due to the distribution of mass within the Earth. Near the Earth's surface, gravitational acceleration is approximately . Local differences in topography, geology, and deeper tectonic structure cause local and broad, regional differences in the Earth's gravitational field, known as gravity anomalies.\n\nThe main part of Earth's magnetic field is generated in the core, the site of a dynamo process that converts the kinetic energy of thermally and compositionally driven convection into electrical and magnetic field energy. The field extends outwards from the core, through the mantle, and up to Earth's surface, where it is, approximately, a dipole. The poles of the dipole are located close to Earth's geographic poles. At the equator of the magnetic field, the magnetic-field strength at the surface is , with global magnetic dipole moment of . The convection movements in the core are chaotic; the magnetic poles drift and periodically change alignment. This causes secular variation of the main field and field reversals at irregular intervals averaging a few times every million years. The most recent reversal occurred approximately 700,000 years ago.\n\nThe extent of Earth's magnetic field in space defines the magnetosphere. Ions and electrons of the solar wind are deflected by the magnetosphere; solar wind pressure compresses the dayside of the magnetosphere, to about 10 Earth radii, and extends the nightside magnetosphere into a long tail. Because the velocity of the solar wind is greater than the speed at which waves propagate through the solar wind, a supersonic bowshock precedes the dayside magnetosphere within the solar wind. Charged particles are contained within the magnetosphere; the plasmasphere is defined by low-energy particles that essentially follow magnetic field lines as Earth rotates; the ring current is defined by medium-energy particles that drift relative to the geomagnetic field, but with paths that are still dominated by the magnetic field, and the Van Allen radiation belt are formed by high-energy particles whose motion is essentially random, but otherwise contained by the magnetosphere.\n\nDuring magnetic storms and substorms, charged particles can be deflected from the outer magnetosphere and especially the magnetotail, directed along field lines into Earth's ionosphere, where atmospheric atoms can be excited and ionized, causing the aurora.\n\nEarth's rotation period relative to the Sun—its mean solar day—is of mean solar time (). Because Earth's solar day is now slightly longer than it was during the 19th century due to tidal deceleration, each day varies between longer.\n\nEarth's rotation period relative to the fixed stars, called its \"stellar day\" by the International Earth Rotation and Reference Systems Service (IERS), is of mean solar time (UT1), or Earth's rotation period relative to the precessing or moving mean vernal equinox, misnamed its \"sidereal day\", is of mean solar time (UT1) . Thus the sidereal day is shorter than the stellar day by about 8.4 ms. The length of the mean solar day in SI seconds is available from the IERS for the periods 1623–2005 and 1962–2005.\n\nApart from meteors within the atmosphere and low-orbiting satellites, the main apparent motion of celestial bodies in Earth's sky is to the west at a rate of 15°/h = 15'/min. For bodies near the celestial equator, this is equivalent to an apparent diameter of the Sun or the Moon every two minutes; from Earth's surface, the apparent sizes of the Sun and the Moon are approximately the same.\n\nEarth orbits the Sun at an average distance of about every 365.2564 mean solar days, or one sidereal year. This gives an apparent movement of the Sun eastward with respect to the stars at a rate of about 1°/day, which is one apparent Sun or Moon diameter every 12 hours. Due to this motion, on average it takes 24 hours—a solar day—for Earth to complete a full rotation about its axis so that the Sun returns to the meridian. The orbital speed of Earth averages about , which is fast enough to travel a distance equal to Earth's diameter, about , in seven minutes, and the distance to the Moon, , in about 3.5 hours.\n\nThe Moon and Earth orbit a common barycenter every 27.32 days relative to the background stars. When combined with the Earth–Moon system's common orbit around the Sun, the period of the synodic month, from new moon to new moon, is 29.53 days. Viewed from the celestial north pole, the motion of Earth, the Moon, and their axial rotations are all counterclockwise. Viewed from a vantage point above the north poles of both the Sun and Earth, Earth orbits in a counterclockwise direction about the Sun. The orbital and axial planes are not precisely aligned: Earth's axis is tilted some 23.44 degrees from the perpendicular to the Earth–Sun plane (the ecliptic), and the Earth–Moon plane is tilted up to ±5.1 degrees against the Earth–Sun plane. Without this tilt, there would be an eclipse every two weeks, alternating between lunar eclipses and solar eclipses.\n\nThe Hill sphere, or the sphere of gravitational influence, of the Earth is about in radius. This is the maximum distance at which the Earth's gravitational influence is stronger than the more distant Sun and planets. Objects must orbit the Earth within this radius, or they can become unbound by the gravitational perturbation of the Sun.\n\nEarth, along with the Solar System, is situated in the Milky Way and orbits about 28,000 light-years from its center. It is about 20 light-years above the galactic plane in the Orion Arm.\n\nThe axial tilt of the Earth is approximately 23.439281° with the axis of its orbit plane, always pointing towards the Celestial Poles. Due to Earth's axial tilt, the amount of sunlight reaching any given point on the surface varies over the course of the year. This causes the seasonal change in climate, with summer in the Northern Hemisphere occurring when the Tropic of Cancer is facing the Sun, and winter taking place when the Tropic of Capricorn in the Southern Hemisphere faces the Sun. During the summer, the day lasts longer, and the Sun climbs higher in the sky. In winter, the climate becomes cooler and the days shorter. In northern temperate latitudes, the Sun rises north of true east during the summer solstice, and sets north of true west, reversing in the winter. The Sun rises south of true east in the summer for the southern temperate zone and sets south of true west.\n\nAbove the Arctic Circle, an extreme case is reached where there is no daylight at all for part of the year, up to six months at the North Pole itself, a polar night. In the Southern Hemisphere, the situation is exactly reversed, with the South Pole oriented opposite the direction of the North Pole. Six months later, this pole will experience a midnight sun, a day of 24 hours, again reversing with the South Pole.\n\nBy astronomical convention, the four seasons can be determined by the solstices—the points in the orbit of maximum axial tilt toward or away from the Sun—and the equinoxes, when the direction of the tilt and the direction to the Sun are perpendicular. In the Northern Hemisphere, winter solstice currently occurs around 21 December; summer solstice is near 21 June, spring equinox is around 20 March and autumnal equinox is about 22 or 23 September. In the Southern Hemisphere, the situation is reversed, with the summer and winter solstices exchanged and the spring and autumnal equinox dates swapped.\n\nThe angle of Earth's axial tilt is relatively stable over long periods of time. Its axial tilt does undergo nutation; a slight, irregular motion with a main period of 18.6 years. The orientation (rather than the angle) of Earth's axis also changes over time, precessing around in a complete circle over each 25,800 year cycle; this precession is the reason for the difference between a sidereal year and a tropical year. Both of these motions are caused by the varying attraction of the Sun and the Moon on Earth's equatorial bulge. The poles also migrate a few meters across Earth's surface. This polar motion has multiple, cyclical components, which collectively are termed quasiperiodic motion. In addition to an annual component to this motion, there is a 14-month cycle called the Chandler wobble. Earth's rotational velocity also varies in a phenomenon known as length-of-day variation.\n\nIn modern times, Earth's perihelion occurs around 3 January, and its aphelion around 4 July. These dates change over time due to precession and other orbital factors, which follow cyclical patterns known as Milankovitch cycles. The changing Earth–Sun distance causes an increase of about 6.9% in solar energy reaching Earth at perihelion relative to aphelion. Because the Southern Hemisphere is tilted toward the Sun at about the same time that Earth reaches the closest approach to the Sun, the Southern Hemisphere receives slightly more energy from the Sun than does the northern over the course of a year. This effect is much less significant than the total energy change due to the axial tilt, and most of the excess energy is absorbed by the higher proportion of water in the Southern Hemisphere.\n\nA study from 2016 suggested that Planet Nine tilted all Solar System planets, including Earth's, by about six degrees.\n\nA planet that can sustain life is termed habitable, even if life did not originate there. Earth provides liquid water—an environment where complex organic molecules can assemble and interact, and sufficient energy to sustain metabolism. The distance of Earth from the Sun, as well as its orbital eccentricity, rate of rotation, axial tilt, geological history, sustaining atmosphere, and magnetic field all contribute to the current climatic conditions at the surface.\n\nA planet's life forms inhabit ecosystems, whose total is sometimes said to form a \"biosphere\". Earth's biosphere is thought to have begun evolving about . The biosphere is divided into a number of biomes, inhabited by broadly similar plants and animals. On land, biomes are separated primarily by differences in latitude, height above sea level and humidity. Terrestrial biomes lying within the Arctic or Antarctic Circles, at high altitudes or in extremely arid areas are relatively barren of plant and animal life; species diversity reaches a peak in humid lowlands at equatorial latitudes.\n\nIn July 2016, scientists reported identifying a set of 355 genes from the last universal common ancestor (LUCA) of all organisms living on Earth.\n\nEarth has resources that have been exploited by humans. Those termed non-renewable resources, such as fossil fuels, only renew over geological timescales.\n\nLarge deposits of fossil fuels are obtained from Earth's crust, consisting of coal, petroleum, and natural gas. These deposits are used by humans both for energy production and as feedstock for chemical production. Mineral ore bodies have also been formed within the crust through a process of ore genesis, resulting from actions of magmatism, erosion, and plate tectonics. These bodies form concentrated sources for many metals and other useful elements.\n\nEarth's biosphere produces many useful biological products for humans, including food, wood, pharmaceuticals, oxygen, and the recycling of many organic wastes. The land-based ecosystem depends upon topsoil and fresh water, and the oceanic ecosystem depends upon dissolved nutrients washed down from the land. In 1980, of Earth's land surface consisted of forest and woodlands, was grasslands and pasture, and was cultivated as croplands. The estimated amount of irrigated land in 1993 was . Humans also live on the land by using building materials to construct shelters.\n\nLarge areas of Earth's surface are subject to extreme weather such as tropical cyclones, hurricanes, or typhoons that dominate life in those areas. From 1980 to 2000, these events caused an average of 11,800 human deaths per year. Many places are subject to earthquakes, landslides, tsunamis, volcanic eruptions, tornadoes, sinkholes, blizzards, floods, droughts, wildfires, and other calamities and disasters.\n\nMany localized areas are subject to human-made pollution of the air and water, acid rain and toxic substances, loss of vegetation (overgrazing, deforestation, desertification), loss of wildlife, species extinction, soil degradation, soil depletion and erosion.\n\nThere is a scientific consensus linking human activities to global warming due to industrial carbon dioxide emissions. This is predicted to produce changes such as the melting of glaciers and ice sheets, more extreme temperature ranges, significant changes in weather and a global rise in average sea levels.\n\nCartography, the study and practice of map-making, and geography, the study of the lands, features, inhabitants and phenomena on Earth, have historically been the disciplines devoted to depicting Earth. Surveying, the determination of locations and distances, and to a lesser extent navigation, the determination of position and direction, have developed alongside cartography and geography, providing and suitably quantifying the requisite information.\n\nEarth's human population reached approximately seven billion on 31 October 2011. Projections indicate that the world's human population will reach 9.2 billion in 2050. Most of the growth is expected to take place in developing nations. Human population density varies widely around the world, but a majority live in Asia. By 2020, 60% of the world's population is expected to be living in urban, rather than rural, areas.\n\nIt is estimated that one-eighth of Earth's surface is suitable for humans to live on – three-quarters of Earth's surface is covered by oceans, leaving one-quarter as land. Half of that land area is desert (14%), high mountains (27%), or other unsuitable terrains. The northernmost permanent settlement in the world is Alert, on Ellesmere Island in Nunavut, Canada. (82°28′N) The southernmost is the Amundsen–Scott South Pole Station, in Antarctica, almost exactly at the South Pole. (90°S)\nIndependent sovereign nations claim the planet's entire land surface, except for some parts of Antarctica, a few land parcels along the Danube river's western bank, and the unclaimed area of Bir Tawil between Egypt and Sudan. , there are 193 sovereign states that are member states of the United Nations, plus two observer states and 72 dependent territories and states with limited recognition. Earth has never had a sovereign government with authority over the entire globe, although some nation-states have striven for world domination and failed.\n\nThe United Nations is a worldwide intergovernmental organization that was created with the goal of intervening in the disputes between nations, thereby avoiding armed conflict. The U.N. serves primarily as a forum for international diplomacy and international law. When the consensus of the membership permits, it provides a mechanism for armed intervention.\n\nThe first human to orbit Earth was Yuri Gagarin on 12 April 1961. In total, about 487 people have visited outer space and reached orbit , and, of these, twelve have walked on the Moon. Normally, the only humans in space are those on the International Space Station. The station's crew, made up of six people, is usually replaced every six months. The farthest that humans have traveled from Earth is , achieved during the Apollo 13 mission in 1970.\n\nThe Moon is a relatively large, terrestrial, planet-like natural satellite, with a diameter about one-quarter of Earth's. It is the largest moon in the Solar System relative to the size of its planet, although Charon is larger relative to the dwarf planet Pluto. The natural satellites of other planets are also referred to as \"moons\", after Earth's.\n\nThe gravitational attraction between Earth and the Moon causes tides on Earth. The same effect on the Moon has led to its tidal locking: its rotation period is the same as the time it takes to orbit Earth. As a result, it always presents the same face to the planet. As the Moon orbits Earth, different parts of its face are illuminated by the Sun, leading to the lunar phases; the dark part of the face is separated from the light part by the solar terminator.\nDue to their tidal interaction, the Moon recedes from Earth at the rate of approximately . Over millions of years, these tiny modifications—and the lengthening of Earth's day by about 23 µs/yr—add up to significant changes. During the Devonian period, for example, (approximately ) there were 400 days in a year, with each day lasting 21.8 hours.\n\nThe Moon may have dramatically affected the development of life by moderating the planet's climate. Paleontological evidence and computer simulations show that Earth's axial tilt is stabilized by tidal interactions with the Moon. Some theorists think that without this stabilization against the torques applied by the Sun and planets to Earth's equatorial bulge, the rotational axis might be chaotically unstable, exhibiting chaotic changes over millions of years, as appears to be the case for Mars.\n\nViewed from Earth, the Moon is just far enough away to have almost the same apparent-sized disk as the Sun. The angular size (or solid angle) of these two bodies match because, although the Sun's diameter is about 400 times as large as the Moon's, it is also 400 times more distant. This allows total and annular solar eclipses to occur on Earth.\n\nThe most widely accepted theory of the Moon's origin, the giant-impact hypothesis, states that it formed from the collision of a Mars-size protoplanet called Theia with the early Earth. This hypothesis explains (among other things) the Moon's relative lack of iron and volatile elements and the fact that its composition is nearly identical to that of Earth's crust.\n\nEarth has at least five co-orbital asteroids, including 3753 Cruithne and . A trojan asteroid companion, , is librating around the leading Lagrange triangular point, L4, in the Earth's orbit around the Sun.\n\nThe tiny near-Earth asteroid makes close approaches to the Earth–Moon system roughly every twenty years. During these approaches, it can orbit Earth for brief periods of time.\n\n, there are 1,886 operational, human-made satellites orbiting Earth. There are also inoperative satellites, including Vanguard 1, the oldest satellite currently in orbit, and over 16,000 pieces of tracked space debris. Earth's largest artificial satellite is the International Space Station.\n\nThe standard astronomical symbol of Earth consists of a cross circumscribed by a circle, , representing the four corners of the world.\n\nHuman cultures have developed many views of the planet. Earth is sometimes personified as a deity. In many cultures it is a mother goddess that is also the primary fertility deity, and by the mid-20th century, the Gaia Principle compared Earth's environments and life as a single self-regulating organism leading to broad stabilization of the conditions of habitability. Creation myths in many religions involve the creation of Earth by a supernatural deity or deities.\n\nScientific investigation has resulted in several culturally transformative shifts in people's view of the planet. Initial belief in a flat Earth was gradually displaced in the Greek colonies of southern Italy during the late 6th century BC by the idea of spherical Earth, which was attributed to both the philosophers Pythagoras and Parmenides. By the end of the 5th century BC, the sphericity of Earth was universally accepted among Greek intellectuals. Earth was generally believed to be the center of the universe until the 16th century, when scientists first conclusively demonstrated that it was a moving object, comparable to the other planets in the Solar System. Due to the efforts of influential Christian scholars and clerics such as James Ussher, who sought to determine the age of Earth through analysis of genealogies in Scripture, Westerners before the 19th century generally believed Earth to be a few thousand years old at most. It was only during the 19th century that geologists realized Earth's age was at least many millions of years.\n\nLord Kelvin used thermodynamics to estimate the age of Earth to be between 20 million and 400 million years in 1864, sparking a vigorous debate on the subject; it was only when radioactivity and radioactive dating were discovered in the late 19th and early 20th centuries that a reliable mechanism for determining Earth's age was established, proving the planet to be billions of years old. The perception of Earth shifted again in the 20th century when humans first viewed it from orbit, and especially with photographs of Earth returned by the Apollo program.\n\n</math>, where \"m\" is the mass of Earth, \"a\" is an astronomical unit, and \"M\" is the mass of the Sun. So the radius in AU is about formula_1.</ref>\n\n"}
{"id": "40159918", "url": "https://en.wikipedia.org/wiki?curid=40159918", "title": "Ecosystem health", "text": "Ecosystem health\n\nEcosystem health is a metaphor used to describe the condition of an ecosystem. Ecosystem condition can vary as a result of fire, flooding, drought, extinctions, invasive species, climate change, mining, overexploitation in fishing, farming or logging, chemical spills, and a host of other reasons. There is no universally accepted benchmark for a healthy ecosystem, rather the apparent health status of an ecosystem can vary depending upon which health metrics are employed in judging it and which societal aspirations are driving the assessment. Advocates of the health metaphor argue for its simplicity as a communication tool. \"Policy-makers and the public need simple, understandable concepts like health.\" Critics worry that ecosystem health, a \"value-laden construct\", is often \"passed off as science to unsuspecting policy makers and the public.\"\n\nThe health metaphor applied to the environment has been in use at least since the early 1800s and the great American conservationist Aldo Leopold (1887–1948) spoke metaphorically of land health, land sickness, mutilation, and violence when describing land use practices. The term \"ecosystem management\" has been in use at least since the 1950s. The term \"ecosystem health\" has become widespread in the ecological literature, as a general metaphor meaning something good, and as an environmental quality goal in field assessments of rivers, lakes, seas, and forests.\n\nRecently however this metaphor has been subject of quantitative formulation using complex systems concepts such as criticality, meaning that a healthy ecosystem is in some sort of balance between adaptability (randomness) and robustness (order) . Nevertheless the universality of criticality is still under examination and is known as the Criticality Hypothesis, which states that systems in a dynamic regime shifting between order and disorder, attain the highest level of computational capabilities and achieve an optimal trade-off between robustness and flexibility. Recent results in cell and evolutionary biology, neuroscience and computer science have great interest in the criticality hypothesis, emphasizing its role as a viable candidate general law in the realm of adaptive complex systems (see and references therein).\n\nThe term ecosystem health has been employed to embrace some suite of environmental goals deemed desirable. Edward Grumbine's highly cited paper \"What is ecosystem management?\" surveyed ecosystem management and ecosystem health literature and summarized frequently encountered goal statements:\n\nGrumbine describes each of these goals as a \"value statement\" and stresses the role of human values in setting ecosystem management goals.\n\nIt is the last goal mentioned in the survey, accommodating humans, that is most contentious. \"We have observed that when groups of stakeholders work to define … visions, this leads to debate over whether to emphasize ecosystem health or human well-being … Whether the priority is ecosystems or people greatly influences stakeholders' assessment of desirable ecological and social states.\" and, for example, \"For some, wolves are critical to ecosystem health and an essential part of nature, for others they are a symbol of government overreach threatening their livelihoods and cultural values.\"\n\nMeasuring ecosystem health requires extensive goal-driven environmental sampling. For example, a vision for ecosystem health of Lake Superior was developed by a public forum and a series of objectives were prepared for protection of habitat and maintenance of populations of some 70 indigenous fish species. A suite of 80 lake health indicators was developed for the Great Lakes Basin including monitoring native fish species, exotic species, water levels, phosphorus levels, toxic chemicals, phytoplankton, zooplankton, fish tissue contaminants, etc.\n\nSome authors have attempted broad definitions of ecosystem health, such as benchmarking as healthy the historical ecosystem state \"prior to the onset of anthropogenic stress.\" A difficulty is that the historical composition of many human-altered ecosystems is unknown or unknowable. Also, fossil and pollen records indicate that the species that occupy an ecosystem reshuffle through time, so it is difficult to identify one snapshot in time as optimum or \"healthy.\".\n\nA commonly cited broad definition states that a healthy ecosystem has three attributes:\n\nWhile this captures significant ecosystem properties, a generalization is elusive as those properties do not necessarily co-vary in nature. For example, there is not necessarily a clear or consistent relationship between productivity and species richness. Similarly, the relationship between resilience and diversity is complex, and ecosystem stability may depend upon one or a few species rather than overall diversity. And some undesirable ecosystems are highly productive.\n\n\"Resilience is not desirable per se. There can be highly resilient states of ecosystems which are very undesirable from some human perspectives , such as algal-dominated coral reefs.\" Ecological resilience is a \"capacity\" that varies depending upon which properties of the ecosystem are to be studied and depending upon what kinds of disturbances are considered and how they are to be quantified. Approaches to assessing it \"face high uncertainties and still require a considerable amount of empirical and theoretical research.\"\n\nOther authors have sought a numerical index of ecosystem health that would permit quantitative comparisons among ecosystems and within ecosystems over time. One such system employs ratings of the three properties mentioned above: Health = system vigor x system organization x system resilience. Ecologist Glenn Suter argues that such indices employ \"nonsense units,\" the indices have \"no meaning; they cannot be predicted, so they are not applicable to most regulatory problems; they have no diagnostic power; effects of one component are eclipsed by responses of other components, and the reason for a high or low index value is unknown.\"\n\nHealth metrics are determined by stakeholder goals, which drive ecosystem definition. An ecosystem is an abstraction. \"Ecosystems cannot be identified or found in nature. Instead, they must be delimited by an observer. This can be done in many different ways for the same chunk of nature, depending on the specific perspectives of interest.\"\n\nEcosystem definition determines the acceptable range of variability (reference conditions) and determines measurement variables. The latter are used as indicators of ecosystem structure and function, and can be used as indicators of \"health\".\n\nAn indicator is a variable, such as a chemical or biological property, that when measured, is used to infer trends in another (unmeasured) environmental variable or cluster of unmeasured variables (the indicandum). For example, rising mortality rate of canaries in a coal mine is an indicator of rising carbon monoxide levels. Rising chlorophyll-a levels in a lake may signal eutrophication.\n\nEcosystem assessments employ two kinds of indicators, descriptive indicators and normative indicators. \"Indicators can be used descriptively for a scientific purpose or normatively for a political purpose.\"\n\nUsed descriptively, high chlorophyll-a is an indicator of eutrophication, but it may also be used as an ecosystem health indicator. When used as a normative (health) indicator, it indicates a rank on a health scale, a rank that can vary widely depending on societal preferences as to what is desirable. A high chlorophyll-a level in a natural successional wetland might be viewed as healthy whereas a human-impacted wetland with the \"same\" indicator value may be judged unhealthy.\n\nEstimation of ecosystem health has been criticized for intermingling the two types of environmental indicators. A health indicator is a normative indicator, and if conflated with descriptive indicators \"implies that normative values can be measured objectively, which is certainly not true. Thus, implicit values are insinuated to the reader, a situation which has to be avoided.\"\n\nIt can be argued that the very act of selecting indicators of any kind is biased by the observer's perspective but separation of goals from descriptions has been advocated as a step toward transparency: \"A separation of descriptive and normative indicators is essential from the perspective of the philosophy of science … Goals and values cannot be deduced directly from descriptions … a fact that is emphasized repeatedly in the literature of environmental ethics … Hence, we advise always specifying the definition of indicators and propose clearly distinguishing ecological indicators in science from policy indicators used for decision-making processes.\"\n\nAnd integration of multiple, possibly conflicting, normative indicators into a single measure of \"ecosystem health\" is problematic. Using 56 indicators, \"determining environmental status and assessing marine ecosystems health in an integrative way is still one of the grand challenges in marine ecosystems ecology, research and management\"\n\nAnother issue with indicators is validity. Good indicators must have an independently validated high predictive value, that is high sensitivity (high probability of indicating a significant change in the indicandum) and high specificity (low probability of wrongly indicating a change). The reliability of various health metrics has been questioned and \"what combination of measurements should be used to evaluate ecosystems is a matter of current scientific debate.\" Most attempts to identify ecological indicators have been correlative rather than derived from prospective testing of their predictive value and the selection process for many indicators has been based upon weak evidence or has been lacking in evidence.\n\nIn some cases no reliable indicators are known: \"We found no examples of invertebrates successfully used in [forest] monitoring programs. Their richness and abundance ensure that they play significant roles in ecosystem function but thwart focus on a few key species.\" And, \"Reviews of species-based monitoring approaches reveal that no single species, nor even a group of species, accurately reflects entire communities. Understanding the response of a single species may not provide reliable predictions about a group of species even when the group is a few very similar species.\"\n\nA trade-off between human health and the \"health\" of nature has been termed the \"health paradox\" and it illuminates how human values drive perceptions of ecosystem health.\n\nHuman health has benefited by sacrificing the \"health\" of wild ecosystems, such as dismantling and damming of wild valleys, destruction of mosquito-bearing wetlands, diversion of water for irrigation, conversion of wilderness to farmland, timber removal, and extirpation of tigers, whales, ferrets, and wolves.\n\nThere has been an acrimonious schism among conservationists and resource managers over the question of whether to \"ratchet back human domination of the biosphere\" or whether to embrace it. These two perspectives have been characterized as utilitarian vs protectionist.\n\nThe utilitarian view treats human health and well-being as criteria of ecosystem health. For example, destruction of wetlands to control malaria mosquitoes \"resulted in an improvement in ecosystem health.\"\nThe protectionist view treats humans as an invasive species: \"If there was ever a species that qualified as an invasive pest, it is \"Homo sapiens\",\"\n\nProponents of the utilitarian view argue that \"healthy ecosystems are characterized by their capability to sustain healthy human populations,\" and \"healthy ecosystems must be economically viable,\" as it is \"unhealthy\" ecosystems that are likely to result in increases in contamination, infectious diseases, fires, floods, crop failures and fishery collapse.\n\nProtectionists argue that privileging of human health is a conflict of interest as humans have demolished massive numbers of ecosystems to maintain their welfare, also disease and parasitism are historically normal in pre-industrial nature. Diseases and parasites promote ecosystem functioning, driving biodiversity and productivity, and parasites may constitute a significant fraction of ecosystem biomass.\n\nThe very choice of the word \"health\" applied to ecology has been questioned as lacking in neutrality in a BioScience article on responsible use of scientific language: \"Some conservationists fear that these terms could endorse human domination of the planet … and could exacerbate the shifting cognitive baseline whereby humans tend to become accustomed to new and often degraded ecosystems and thus forget the nature of the past.\"\n\nCriticism of ecosystem health largely targets the failure of proponents to explicitly distinguish the normative dimension from the descriptive dimension, and has included the following:\n\nAlternatives have been proposed for the term ecosystem health, including more neutral language such as ecosystem status, ecosystem prognosis, and ecosystem sustainability. Another alternative to the use of a health metaphor is to \"express exactly and clearly the public policy and the management objective\", to employ habitat descriptors and real properties of ecosystems. An example of a policy statement is \"The maintenance of viable natural populations of wildlife and ecological functions always takes precedence over any human use of wildlife.\" An example of a goal is \"Maintain viable populations of all native species in situ.\" An example of a management objective is \"Maintain self-sustaining populations of lake whitefish within the range of abundance observed during 1990-99.\"\n\nKurt Jax presented an ecosystem assessment format that avoids imposing a preconceived notion of normality, that avoids the muddling of normative and descriptive, and that gives serious attention to ecosystem definition. (1) Societal purposes for the ecosystem are negotiated by stakeholders, (2) a functioning ecosystem is defined with emphasis on phenomena relevant to stakeholder goals, (3) benchmark reference conditions and permissible variation of the system are established, (4) measurement variables are chosen for use as indicators, and (5) the time scale and spatial scale of assessment are decided.\n\nEcological health has been used as a medical term in reference to human allergy and multiple chemical sensitivity and as a public health term for programs to modify health risks (diabetes, obesity, smoking, etc.). Human health itself, when viewed in its broadest sense, is viewed as having ecological foundations. It is also an urban planning term in reference to \"green\" cities (composting, recycling), and has been used loosely with regard to various environmental issues, and as the condition of human-disturbed environmental sites. Ecosystem integrity implies a condition of an ecosystem exposed to a minimum of human influence. Ecohealth is the relationship of human health to the environment, including the effect of climate change, wars, food production, urbanization, and ecosystem structure and function. Ecosystem management and ecosystem-based management refer to the sustainable management of ecosystems and in some cases may employ the terms ecosystem health or ecosystem integrity as a goal.\n"}
{"id": "46488727", "url": "https://en.wikipedia.org/wiki?curid=46488727", "title": "Energoland", "text": "Energoland\n\nEnergoland is an information centre for energy and electricity generation which was opened by Slovenské elektrárne on 14 October 2014 at Mochovce, Slovakia, at the site of the nuclear power plant. It is situated between Levice and Nitra. The centre mainly serves the schools and public. The entry is free of charge. The exposition was awarded as an excellent communication project at the PIME conference and evaluated as a five-star training centre by Laura Elizabeth Kennedy, governor of the United States of America in the Board of Governors of the International Atomic Energy Agency in Vienna and chargé d'affaires of permanent representation of the USA in international organisation in Vienna.\n\nIn Energoland, there are more than thirty objects, applications, and interactive expositions. Edutainment (blending education and entertainment) at Mochovce offers information on energy, electricity generation, global warming, and carbon burden. In addition, visitors learn about the energy mix, electricity grid dispatching, or about the radiation around us. The exposition also focuses on power industry, not only with respect to safety and radioactive waste but also dealing with the fuel cycle, nanowolrd of atoms, and Cherenkov radiation. Some of the specialities include the 3D cinema with its own movie Energy Odyssey, a model of emission-free motorcycle, interactive LED floor or thermal mirror, and mobile application using augmented reality, \"Energoland\", downloadable from Google Play or AppStore. Through the Energy Map, various facts and statistics can be searched which are directly shown in the map of the world with the highlighted countries.\nEnergoland also wants its visitors to make their own opinion of the electricity generation and its sources. Therefore, it does not provide information only about nuclear energy but it also brings knowledge about other sources: water, sun, wind, geothermal energy, or fossil fuels.\n\nThe design of the building is the work of Ing. arch. Viktor Šabík (BARAK Architekti) who, together with the QEX Company and Italian agency Piter Design, designed the interior of Energoland, as well.[2] The total area of the building, including the training rooms and offices is 2,000 square metres; the info centre itself takes up the area of 600 m2.\n\nEnergoland is not only a visitor and information centre; it also serves as a training centre for the employees of Slovenské elektrárne and provides office areas. There are many events for the employees of Slovenské elektrárne or public taking place here, such as the 90 reactor-years anniversary, programme within the national round of Olympics in Physics, Science Talks during the Science and Technology Week, Sustainable Development Conference in May 2015, etc.\n\n"}
{"id": "1610231", "url": "https://en.wikipedia.org/wiki?curid=1610231", "title": "Energy density", "text": "Energy density\n\nEnergy density is the amount of energy stored in a given system or region of space per unit volume. Colloquially it may also be used for energy per unit mass, though the accurate term for this is specific energy. Often only the \"useful\" or extractable energy is measured, which is to say that inaccessible energy (such as rest mass energy) is ignored. In cosmological and other general relativistic contexts, however, the energy densities considered are those that correspond to the elements of the stress–energy tensor and therefore do include mass energy as well as energy densities associated with the pressures described in the next paragraph.\n\nEnergy per unit volume has the same physical units as pressure, and in many circumstances is a synonym: for example, the energy density of a magnetic field may be expressed as (and behaves as) a physical pressure, and the energy required to compress a compressed gas a little more may be determined by multiplying the difference between the gas pressure and the external pressure by the change in volume. In short, pressure is a measure of the enthalpy per unit volume of a system. A pressure gradient has the potential to perform work on the surroundings by converting enthalpy to work until equilibrium is reached.\n\nThere are many different types of energy stored in materials, and it takes a particular type of reaction to release each type of energy. In order of the typical magnitude of the energy released, these types of reactions are: nuclear, chemical, electrochemical, and electrical.\n\nNuclear reactions are used by stars and nuclear power plants, both of which derive energy from the binding energy of nuclei. Chemical reactions are used by animals to derive energy from food, and by automobiles to derive energy from gasoline. Liquid hydrocarbons (fuels such as gasoline, diesel and kerozene) are today the most dense way known to economically store and transport chemical energy at a very large scale (1 kg of diesel fuel burns with the oxygen contained in ~15 kg of air). Electrochemical reactions are used by most mobile devices such as laptop computers and mobile phones to release the energy from batteries.\n\nThe following is a list of the thermal energy densities (that is to say: the amount of heat energy that can be extracted) of commonly used or well-known energy storage materials; it doesn't include uncommon or experimental materials. Note that this list does not consider the mass of reactants commonly available such as the oxygen required for combustion or the energy efficiency in use. An extended version of this table is found at Energy density#Extended Reference Table. Major reference = .\n\nThe following unit conversions may be helpful when considering the data in the table: 3.6 MJ = 1 kWh ≈ 1.34 HPh.\n\nIn energy storage applications the energy density relates the mass of an energy store to the volume of the storage facility, e.g. the fuel tank. The higher the energy density of the fuel, the more energy may be stored or transported for the same amount of volume. The energy density of a fuel per unit mass is called the specific energy of that fuel. In general an engine using that fuel will generate less kinetic energy due to inefficiencies and thermodynamic considerations—hence the specific fuel consumption of an engine will always be greater than its rate of production of the kinetic energy of motion.\n\nThe greatest energy source by far is mass itself. This energy, \"E = mc\", where \"m = ρV\", \"ρ\" is the mass per unit volume, \"V\" is the volume of the mass itself and \"c\" is the speed of light. This energy, however, can be released only by the processes of nuclear fission (0.1%), nuclear fusion (1%), or the annihilation of some or all of the matter in the volume \"V\" by matter-antimatter collisions (100%). Nuclear reactions cannot be realized by chemical reactions such as combustion. Although greater matter densities can be achieved, the density of a neutron star would approximate the most dense system capable of matter-antimatter annihilation possible. A black hole, although denser than a neutron star, does not have an equivalent anti-particle form, but would offer the same 100% conversion rate of mass to energy in the form of Hawking radiation. In the case of relatively small black holes (smaller than astronomical objects) the power output would be tremendous.\n\nThe highest density sources of energy aside from antimatter are fusion and fission. Fusion includes energy from the sun which will be available for billions of years (in the form of sunlight) but so far (2018), sustained fusion power production continues to be elusive. \n\nPower from fission of uranium and thorium in nuclear power plants will be available for many decades or even centuries because of the plentiful supply of the elements on earth, though the full potential of this source can only be realised through breeder reactors, which are, apart from the BN-600 reactor, not yet used commercially. Coal, gas, and petroleum are the current primary energy sources in the U.S. but have a much lower energy density. Burning local biomass fuels supplies household energy needs (cooking fires, oil lamps, etc.) worldwide. \n\nThe density of thermal energy contained in the core of a light water reactor (PWR or BWR) of typically 1 GWe (1 000 MW electrical corresponding to ~3 000 MW thermal) is in the range of 10 to 100 MW of thermal energy per cubic meter of cooling water depending on the location considered in the system (the core itself (~30 m), the reactor pressure vessel (~50 m), or the whole primary circuit (~300 m)). This represents a considerable density of energy which requires under all circumstances a continuous water flow at high velocity in order to be able to remove the heat from the core, even after an emergency shutdown of the reactor. The incapacity to cool the cores of three boiling water reactors (BWR) at Fukushima in 2011 after the tsunami and the resulting loss of the external electrical power and of the cold source was the cause of the meltdown of the three cores in only a few hours. Meanwhile, the three reactors were correctly shut down just after the Tōhoku earthquake. This extremely high power density distinguishes nuclear power plants (NPP's) from any thermal power plants (burning coal, fuel or gas) or any chemical plants and explains the large redundancy required to permanently control the neutron reactivity and to remove the residual heat from the core of NPP's.\n\nEnergy density differs from energy conversion efficiency (net output per input) or embodied energy (the energy output costs to provide, as harvesting, refining, distributing, and dealing with pollution all use energy). Large scale, intensive energy use impacts and is impacted by climate, waste storage, and environmental consequences.\n\nNo single energy storage method boasts the best in specific power, specific energy, and energy density. Peukert's Law describes how the amount of useful energy that can be obtained (for a lead-acid cell) depends on how quickly we pull it out. To maximize both specific energy and energy density, one can compute the specific energy density of a substance by multiplying the two values together, where the higher the number, the better the substance is at storing energy efficiently.\n\nAlternative options are discussed for energy storage to increase energy density and decrease charging time.\n\nGravimetric and volumetric energy density of some fuels and storage technologies (modified from the Gasoline article):\n\nThis table lists energy densities of systems that require external components, such as oxidisers or a heat sink or source. These figures do not take into account the mass and volume of the required components as they are assumed to be freely available and present in the atmosphere. Such systems cannot be compared with self-contained systems. These values may not be computed at the same reference conditions.\n\nDivide joule/m by 10 to get MJ/L. Divide MJ/L by 3.6 to get kWh/L.\n\nElectric and magnetic fields store energy. In a vacuum, the (volumetric) energy density is given by\n\nwhere E is the electric field and B is the magnetic field. The solution will be (in SI units) in Joules per cubic metre. In the context of magnetohydrodynamics, the physics of conductive fluids, the magnetic energy density behaves like an additional pressure that adds to the gas pressure of a plasma.\n\nIn normal (linear and nondispersive) substances, the energy density (in SI units) is\n\nwhere D is the electric displacement field and H is the magnetizing field.\n\nIn the case of absence of magnetic fields, by exploting Fröhlich's relationships it is also possible to extend these equations to anisotropy and nonlinearity dielectrics, as well as to calculate the correlated Helmholtz free energy and entropy densities.\n\n\n\n"}
{"id": "321382", "url": "https://en.wikipedia.org/wiki?curid=321382", "title": "Energy flow (ecology)", "text": "Energy flow (ecology)\n\nIn ecology, energy flow, also called the calorific flow, refers to the flow of energy through a food chain, and is the focus of study in ecological energetics. In an ecosystem, ecologists seek to quantify the relative importance of different component species and feeding relationships.\n\nA general energy flow scenario follows:\n\nThe energy is passed on from trophic level to trophic level and each time about 90% of the energy is lost, with some being lost as heat into the environment (an effect of respiration) and some being lost as incompletely digested food (egesta). Therefore, primary consumers get about 10% of the energy produced by autotrophs, while secondary consumers get 1% and tertiary consumers get 0.1%. This means the top consumer of a food chain receives the least energy, as a lot of the food chain's energy has been lost between trophic levels. This loss of energy at each level limits typical food chains to only four to six links.\n\nEcological energetics appears to have grown out of the Age of Enlightenment and the concerns of the Physiocrats. It began in the works of Sergei Podolinsky in the late 1800s, and subsequently was developed by the Soviet ecologist Vladmir Stanchinsky, the Austro-American Alfred J. Lotka, and American limnologists, Raymond Lindeman and G. Evelyn Hutchinson. It underwent substantial development by Howard T. Odum and was applied by systems ecologists, and radiation ecologists.\n\n\n"}
{"id": "4772620", "url": "https://en.wikipedia.org/wiki?curid=4772620", "title": "Energy monitoring and targeting", "text": "Energy monitoring and targeting\n\nEnergy monitoring and targeting (M&T) is an energy efficiency technique based on the standard management axiom stating that “you cannot manage what you cannot measure”. M&T techniques provide energy managers with feedback on operating practices, results of energy management projects, and guidance on the level of energy use that is expected in a certain period. Importantly, they also give early warning of unexpected excess consumption caused by equipment malfunctions, operator error, unwanted user behaviours, lack of effective maintenance and the like.\n\nThe foundation of M&T lies in determining the normal relationships of energy consumptions to relevant driving factors (HVAC equipment, production though puts, weather, occupancy available daylight, etc.) and the goal is to help business managers:\n\n\nThe ultimate goal is to reduce energy costs through improved energy efficiency and energy management control. Other benefits generally include increased resource efficiency, improved production budgeting and reduction of greenhouse gas (GHG) emissions.\n\nM&T is an established technique that was first launched as a national program in the UK in 1980, and has since then spread throughout Europe. These techniques are now also rapidly growing in America.\n\nThroughout the numerous M&T projects implemented since the 1980s, a certain number of benefits have proved to be recurrent:\n\nMonitoring and Targeting techniques rely on three main principles, which form a constant feedback cycle, therefore improving control of energy use.\n\nMonitoring information of energy use, in order to establish a basis for energy management and explain deviations from an established pattern. Its primary goal is to maintain said pattern, by providing all the necessary data on energy consumption, as well as certain driving factors, as identified during preliminary investigation (production, weather, etc.)\n\nThe final principle is the one which enables ongoing control of energy use, achievement of targets and verification of savings: reports must be issued to the appropriate managers. This in turn allows decision-making and actions to be taken in order to achieve the targets, as well as confirmation or denial that the targets have been reached.\n\nBefore the M&T measures themselves are implemented, a few preparatory steps are necessary. First of all, key energy consumers on the site must be identified. Generally, most of the energy consumption is concentrated in a small number of processes, like heating, or certain machinery. This normally requires a certain survey of the building and the equipment to estimate their energy consumption level.\n\nIt is also necessary to assess what other measurements will be required to analyze the consumption appropriately. This data will be used to chart against the energy consumption: these are underlying factors which influence the consumption, often production (for industry processes) or exterior temperature (for heating processes), but may include many other variables.\n\nOnce all variables to be measured have been established, and the necessary meters installed, it is possible to initiate the M&T procedures.\n\nThe first step is to compile the data from the different meters. Low-cost energy feedback displays have become available. The frequency at which the data is compiled varies according to the desired reporting interval, but can go once every 30 seconds to once every 15 minutes. Some measurements can be taken directly from the meters, others must be calculated. These different measurements are often called streams or channels.\n\nDriving factors such as production or degree days also constitute streams and must be collected at intervals to match.\n\nThe data compiled must then be plotted on a graph in order to define the general consumption base-line. Consumption rates are plotted in a scatter plot against production or any other variable previously identified, and the best fit line is identified. This graph is the image of the business’ average energy performance, and conveys a lot of information:\n\nThe slope is not used quite as often for M&T purposes. However, a high y-intercept can mean that there is a fault in the process, causing it to use too much energy with no performance, unless there are specific distinctive features which lead to high base loads. Very scattered points, on the other hand, may reflect other significant factors playing in the variation of the energy consumption, other than the one plotted in the first place, but it can also be the illustration of a lack of control over the process.\n\nThe next step is to monitor the difference between the expected consumption and the actual measured consumption. One of the tools most commonly used for this is the CUSUM, which is the CUmulative SUM of differences. This consists in first calculating the difference between the expected and actual performances (the best fit line previously identified and the points themselves).\n\nThe CUSUM can then be plotted against time on a new graph, which then yields more information for the energy efficiency specialist. Variances scattered around zero usually mean that the process is operating normally. Marked variations, increasing or decreasing steadily usually reflect a modification in the conditions of the process.\n\nIn the case of the CUSUM graph, the slope becomes very important, as it is the main indicator of the savings achieved. A slope going steadily down indicates steady savings. Any variation in the slope indicates a change in the process. For example, in the graph on the right, the first section indicated no savings. However, in September (beginning of the yellow line), an energy efficiency measure must have been implemented, as savings start to occur. The green line indicates an increase in the savings (as the slope is becoming steeper), whereas the red line must reflect a modification in the process having occurred in November, as savings have decreased slightly.\n\nEnergy efficiency specialists, in collaboration with building managers, will decipher the CUSUM graph and identify the causes leading to variations in the consumption. This can be a change in behaviour, a modification to the process, different exterior conditions, etc. These changes must be monitored and the causes identified in order to promote and enhance good behaviour, and discourage bad ones.\n\nOnce the base line has been established, and causes for variations in energy consumption have been identified, it is time to set targets for the future. Now with all this information in hand, the targets are more realistic, as they are based on the building’s actual consumption.\nTargeting consists in two main parts: the measure to which the consumption can be reduced, and the timeframe during which the compression will be achieved.\n\nA good initial target is the best fit line identified during step 2. This line represents the average historical performance. Therefore, keeping all consumption below or equal to the historical average is an achievable target, yet remains a challenge as it involves eliminating high consumption peaks.\n\nSome companies, as they improve their energy consumption, might even decide to bring their average performance down to their historical best. This is considered a much more challenging target.\n\nThis brings us back to step 1: measure consumption. One of the specificities of M&T is that it is an ongoing process, requiring constant feedback in order to consistently improve performance. Once the targets are set and the desired measures are implemented, repeating the procedure from the start ensures that the managers are aware of the success or failure of the measures, and can then decide on further action.\n\nAn example with some features of an M&T application is the ASU Campus Metabolism, which provides real-time and historic energy use and generation data for facilities of Arizona State University on a public web site. Many utilities also offer customers electric interval data monitoring services. Xcel Energy is an example of an investor owned utility that offers its customer electric and natural gas monitoring services under the product name InfoWise from Xcel Energy which is administered by Power TakeOff, a third party partner.\n\n"}
{"id": "43699304", "url": "https://en.wikipedia.org/wiki?curid=43699304", "title": "Energy rate density", "text": "Energy rate density\n\nEnergy rate density is the amount of free energy per unit time per unit mass (in CGS metric units erg/s/g; in MKS units joule/s/kg). It is terminologically (but not always numerically) equivalent to power density when measured in SI units of W/kg. Regardless of the units used, energy rate density describes the flow of energy through any system of given mass, and has been proposed as a measure of system complexity. The more complex the system, the more energy flows per second through each gram. \n\nEnergy rate density is actually a general term that is equivalent to more specialized terms used by many different disciplinary scientists. For example, in astronomy it is called the luminosity-to-mass ratio (the inverse of the mass-luminosity ratio), in physics the power density, in geology the specific radiant flux (where “specific” denotes per unit mass), in biology the specific metabolic rate, and in engineering the power-to-weight ratio. Interdisciplinary researchers prefer to use the general term, energy rate density, not only to stress the intuitive notion of energy flow (in contrast to more colloquial connotations of the word \"power\"), but also to unify its potential application among all the natural sciences, as in the cosmology of cosmic evolution. When the energy rate density for systems including our galaxy, sun, earth, plants, animals, society are plotted according to when, in historical time, they first emerged, a clear increase in energy rate density over time is observed. \n\nThis term has in recent years gained many diverse applications in various disciplines, including history, cosmology, economics, philosophy, and behavioral biology. \n\n\n"}
{"id": "4329772", "url": "https://en.wikipedia.org/wiki?curid=4329772", "title": "Exertion", "text": "Exertion\n\nExertion is the physical or perceived use of energy. Exertion traditionally connotes a strenuous or costly \"effort,\"resulting in generation of force, initiation of motion, or in the performance of work. It often relates to muscularactivity and can be quantified, empirically and by measurable metabolic response.\n\nIn physics, \"exertion\" is the expenditure of energy against, or inductive of, inertia as described by Isaac Newton's third law of motion. In physics, force exerted equivocates work done. The ability to do work can be either positive or negative depending on the direction of exertion relative to gravity. For example, a force exerted upwards, like lifting an object, creates positive work done on that object.\n\nExertion often results in force generated, a contributing dynamic of general motion. In mechanics it describes the use of force against a body in the direction of its motion (see vector).\n\nExertion, physiologically, can be described by the initiation of exercise, or, intensive and exhaustive physical activity that causes cardiovascular stress or a sympathetic nervous response. This can be continuous or intermittent exertion.\n\nExertion requires, of the body, modified oxygen uptake, increased heart rate, and autonomic monitoring of blood lactate concentrations. Mediators of physical exertion include cardio-respiratory and musculoskeletal strength, as well as metabolic capability. This often correlates to an output of force followed by a refractory period of recovery. Exertion is limited by cumulative load and repetitive motions.\n\nMuscular energy reserves, or stores for biomechanical exertion, stem from metabolic, immediate production of ATP and increased O2 consumption. Muscular exertion generated depends on the muscle length and the velocity at which it is able to shorten, or contract.\n\nPerceived exertion can be explained as subjective, perceived experience that mediates response to somatic sensations and mechanisms. A rating of perceived exertion, as measured by the \"RPE-scale\", or Borg scale, is a quantitative measure of physical exertion.\n\nOften in health, exertion of oneself resulting in cardiovascular stress showed reduced physiological responses, like cortisol levels and mood, to stressors. Therefore, biological exertion is effective in mediating psychological exertion, responsive to environmental stress.\n\nOverexertion causes more than 3.5 million injuries a year. An overexertion injury can include sprains or strains, the stretching and tear of ligaments, tendons, or muscles caused by a load that exceeds the human ability to perform the work. Overexertion, besides causing acute injury, implies physical exertion beyond the person's capacity which leads to symptoms such as dizziness, irregular breathing and heart rate, and fatigue. Preventative measures can be taken based on biomechanical knowledge to limit possible overexertion injuries.\n\n\n"}
{"id": "1686779", "url": "https://en.wikipedia.org/wiki?curid=1686779", "title": "Fusion energy gain factor", "text": "Fusion energy gain factor\n\nThe fusion energy gain factor, usually expressed with the symbol Q, is the ratio of fusion power produced in a nuclear fusion reactor to the power required to maintain the plasma in steady state. The condition of \"Q\" = 1, when the power being released by the fusion reactions is equal to the required heating power, is referred to as breakeven.\n\nThe power given off by the fusion reactions may be captured within the fuel, leading to \"self-heating\". Most fusion reactions release at least some of their energy in a form that cannot be captured within the plasma, so a system at \"Q\" = 1 will cool without external heating. With typical fuels, self-heating in fusion reactors is not expected to match the external sources until at least \"Q\" = 5. If \"Q\" increases past this point, increasing self-heating eventually removes the need for external heating. At this point the reaction becomes self-sustaining, a condition called ignition. Ignition corresponds to infinite \"Q\", and is generally regarded as highly desirable for a practical reactor design.\n\nOver time, several related terms have entered the fusion lexicon. As a reactor does not cover its own heating losses until about \"Q\" = 5, the term engineering breakeven is sometimes used to describe a reactor that produces enough electricity to provide that heating. Above engineering breakeven a machine would produce more electricity than it uses, and could sell that excess. A machine that can sell enough electricity to cover its operating costs, estimated to require at least \"Q\" = 20, is sometimes known as economic breakeven.\n\n, the record for \"Q\" is held by the JET tokamak in the UK, at \"Q\" = (16 MW)/(24 MW) ≈ 0.67, first attained in 1997. ITER was originally designed to reach ignition, but is currently designed to reach \"Q\" = 10, producing 500 MW of fusion power from 50 MW of injected thermal power.\n\n\"Q\" is simply the comparison of the power being released by the fusion reactions in a reactor, \"P\", to the constant heating power being supplied, \"P\". However, there are several definitions of breakeven that consider additional power losses.\n\nIn 1955, John Lawson was the first to explore the energy balance mechanisms in detail, initially in classified works but published openly in a now-famous 1957 paper. In this paper he considered and refined work by earlier researchers, notably Hans Thirring, Peter Thonemann, and a review article by Richard Post. Expanding on all of these, Lawson's paper made detailed predictions for the amount of power that would be lost through various mechanisms, and compared that to the energy needed to sustain the reaction. This balance is today known as the Lawson criterion.\n\nIn a successful fusion reactor design, the fusion reactions generate an amount of power designated \"P\". Some amount of this energy, \"P\", is lost through a variety of mechanisms, mostly convection of the fuel to the walls of the reactor chamber and various forms of radiation that cannot be captured to generate power. In order to keep the reaction going, the system has to provide heating to make up for these losses, where \"P\" = \"P\" to maintain thermal equilibrium.\n\nThe most basic definition of breakeven is when \"Q\" = 1, that is, \"P\" = \"P\".\n\nSome works refer to this definition as scientific breakeven, to contrast it with similar terms. However, this usage is rare outside certain areas, specifically the inertial confinement fusion field, where the term is much more widely used.\n\nSince the 1950s, most commercial fusion reactor designs have been based on a mix of deuterium and tritium as their primary fuel; others fuels have been studied for a variety of reasons but are much harder to ignite. As tritium is radioactive, highly bioactive and highly mobile, it represents a significant safety concern and adds to the cost of designing and operating such a reactor.\n\nIn order to lower costs, many experimental machines are designed to run on test fuels of hydrogen or deuterium alone, leaving out the tritium. In this case, the term extrapolated breakeven is used to define the expected performance of the machine running on D-T fuel based on the performance when running on hydrogen or deuterium alone.\n\nThe records for extrapolated breakeven are slightly higher than the records for scientific breakeven. Both JET and JT-60 have reached values around 1.25 (see below for details) while running on D-D fuel. When running on D-T, only possible in JET, the maximum performance is about half the extrapolated value.\n\nAnother related term, engineering breakeven, considers the need to extract the energy from the reactor, turn that into electrical energy, and feed that back into the heating system. This closed loop is known as \"recirculation\". In this case, the basic definition changes by adding additional terms to the \"P\" side to consider the efficiencies of these processes.\n\nMost fusion reactions release energy in a variety of forms, mostly neutrons and a variety of charged particles like alpha particles. Neutrons are electrically neutral and will travel out of any magnetic confinement fusion (MFE) design, and in spite of the very high densities found in inertial confinement fusion (ICF) designs, they tend to easily escape the fuel mass in these designs as well. This means that only the charged particles from the reactions can be captured within the fuel mass and give rise to self-heating. If the fraction of the energy being released in the charged particles is \"f\", then the power in these particles is \"P\" = \"f\"\"P\". If this self-heating process is perfect, that is, all of \"P\" is captured in the fuel, that means the power available for generating electricity is the power that is not released in that form, or (1 − \"f\")\"P\".\n\nIn the case of neutrons carrying most of the practical energy, as is the case in the D-T fuel studied in most designs, this neutron energy is normally captured in a \"blanket\" of lithium that produces more tritium that is used to fuel the reactor. Due to various exothermic and endothermic reactions, the blanket may have a power gain factor a few percent higher or lower than 100%, but that will be neglected here. The blanket is then cooled and the cooling fluid used in a heat exchanger driving conventional steam turbines. These have an efficiency η which is around 35 to 40%.\n\nConsider a system that uses external heaters to heat the fusion fuel, then extracts the power from those reactions to generate electrical power. Some fraction of that power, \"f\", is needed to recirculate back into the heaters to close the loop. This is not the same as the \"P\" because the self-heating processes are providing some of the required energy. While the system as a whole requires additional power for building climate control, lighting, and the confinement system, these are generally much smaller than the plasma heating system requirements.\nConsidering all of these factors, the heating power can thus be related to the fusion power by the following equation:\n\nformula_1\n\nwhere formula_2 is the efficiency that power supplied to the heating systems is turned into heat in the fuel, as opposed to lost in the equipment itself, and formula_3 is the efficiency achieved when turning the heat into electrical power, for instance, through the Rankine cycle.\n\nThe fusion energy gain factor is then defined as:\n\nformula_4\n\nAs the temperature of the plasma increases, the rate of fusion reactions grows rapidly, and with it, the rate of self heating. In contrast, the non-capturable energy losses like x-rays do not grow at the same rate. Thus, in overall terms, the self-heating process becomes more efficient as the temperature increases, and less energy is needed from external sources to keep it hot.\n\nEventually \"P\" reaches zero, that is, all of the energy needed to keep the plasma at the operational temperature is being supplied by self-heating, and the amount of external energy that needs to be added drops to zero. This point is known as ignition.\n\nIgnition, by definition, corresponds to an infinite \"Q\", but it does not mean that \"f\" drops to zero as the other power sinks in the system, like the magnets and cooling systems, still need to be powered. Generally, however, these are much smaller than the energy in the heaters, and require a much smaller \"f\". More importantly, this number is more likely to be near constant, meaning that further improvements in plasma performance will result in more energy that can be directly used for commercial generation, as opposed to recirculation.\n\nThe final definition of breakeven is commercial breakeven, which occurs when the economic value of any net energy left over after recirculation is enough to finance the construction of the reactor. This value depends both on the reactor and the spot price of electrical power.\n\nCommercial breakeven relies on factors outside the technology of the reactor itself, and it is possible that even a reactor with a fully ignited plasma will not generate enough energy to pay for itself. Whether any of the mainline concepts like ITER can reach this goal is being debated in the field.\n\nMost fusion reactor designs being studied are based on the D-T reaction, as this is by far the easiest to ignite, and is energy dense. However, this reaction also gives off most of its energy in the form of a single highly energetic neutron, and only 20% of the energy in the form of an alpha. Thus, for the D-T reaction, \"f\" = 0.2. This means that self-heating does not become equal to the external heating until at least \"Q\" = 5. \n\nEfficiency values depend on design details but may be in the range of η = 0.7 (70%) and η = 0.4 (40%). The purpose of a fusion reactor is to produce power, not to recirculate it, so a practical reactor must have \"f\" = 0.2 approximately. Lower would be better but will be hard to achieve. Using these values we find for a practical reactor \"Q\" = 22.\n\nMany early fusion devices operated for microseconds, using some sort of pulsed power source to feed their magnetic confinement system and used the confinement as the heating source. Lawson defined breakeven in this context as the total energy released by the entire reaction cycle compared to the total energy supplied to the machine during the same cycle.\n\nOver time, as performance increased by orders of magnitude, the reaction times have extended from microseconds to seconds, and in ITER, on the order of minutes. In this case definition of \"the entire reaction cycle\" becomes blurred. In the case of an ignited plasma, for instance, P may be quite high while the system is being set up, and then drop to zero when it is fully developed, so one may be tempted to pick an instant in time when it is operating at its best to determine \"Q\". A better solution in these cases is to use the original Lawson definition averaged over the reaction to produce a similar value as the original definition.\n\nHowever, there is a complication. During the heating phase when the system is being brought up to operational conditions, some of the energy released by the fusion reactions will be used to heat the surrounding fuel, and thus not be released. This is no longer true when the plasma reaches its operational temperature and enters thermal equilibrium. Thus, if one averages over the entire cycle, this energy will be included as part of the heating term, that is, some of the energy that was captured for heating would otherwise have been released in P and is therefore not indicative of an operational \"Q\".\n\nOperators of the JET reactor argued that this input should be removed from the total:\n\nformula_5\n\nwhere:\n\nformula_6\n\nThat is, P is the amount of energy needed to raise the internal energy of the plasma. It is this definition that was used when reporting JET's record 0.67 value.\n\nSome debate over this definition continues. In 1998, the operators of the JT-60 claimed to have reached \"Q\" = 1.25 running on D-D fuel, thus reaching extrapolated breakeven. However, this measurement was based on the JET definition of Q*. Using this definition, JET had also reached extrapolated breakeven some time earlier. If one considers the energy balance in these conditions, and the analysis of previous machines, it is argued the original definition should be used, and thus both machines remain well below break-even of any sort.\n\nAlthough most fusion experiments use some form of magnetic confinement, another major branch is inertial confinement fusion (ICF) that mechanically presses together the fuel mass (the \"target\") to increase its density. This greatly increases the rate of fusion events and lowers the need to confine the fuel for long periods. This compression is accomplished by heating a lightweight capsule holding the fuel so rapidly that it explodes outwards, driving the fuel mass on the inside inward in accordance with Newton's third law. There are a variety of proposed \"drivers\" to cause the implosion process, but to date most experiments have used lasers. \n\nUsing the traditional definition of \"Q\", \"P\" / \"P\", ICF devices have extremely low \"Q\". This is because the laser is extremely inefficient; whereas formula_2 for the heaters used in magnetic systems might be on the order of 70%, lasers are on the order of 1.5%. For this reason, Lawrence Livermore National Laboratory (LLNL), the leader in ICF research, has proposed another modification of \"Q\" that defines \"P\" as the energy delivered by the driver, as opposed to the energy put into the driver. This definition produces much higher \"Q\" values, and changes the definition of breakeven to be \"P\" / \"P\" = 1. On occasion, they referred to this definition as \"scientific breakeven\". This term was not universally used, other groups adopted the redefinition of \"Q\" but continued to refer to \"P\" = \"P\" simply as breakeven. \n\nOn 7 October 2013, the BBC announced that LLNL had achieved scientific breakeven in the National Ignition Facility (NIF) on 29 September. In this experiment, \"P\" was approximately 14 kJ, while the laser output was 1.8 MJ. By their previous definition, this would be a \"Q\" of 0.0077. However, for this press release, they re-defined \"Q\" once again, this time equating \"P\" to be only the amount energy delivered to \"the hottest portion of the fuel\", calculating that only 10 kJ of the original laser energy reached the part of the fuel that was undergoing fusion reactions. This release has been heavily criticized in the field.\n\n"}
{"id": "7371308", "url": "https://en.wikipedia.org/wiki?curid=7371308", "title": "Graphical timeline from Big Bang to Heat Death", "text": "Graphical timeline from Big Bang to Heat Death\n\nThis is the timeline of the Universe from Big Bang to Heat Death scenario. The different eras of the universe are shown. The heat death will occur in 10</sup> years, if protons decay.\n\nUsually the logarithmic scale is used for such timelines but it compresses the most interesting Stelliferous Era too much as this example shows. Therefore, a double-logarithmic scale \"s\" (\"s*100\" in the graphics) is used instead. The minimum of it is only 1, not 0 as needed, and the negative outputs for inputs smaller than 10 are useless. Therefore, the time from 0.1 to 10 years is collapsed to a single point 0, but that does not matter in this case because nothing special happens in the history of the universe during that time.\n\nThe seconds in the timescale have been converted to years by formula_2 using the Julian year.\n"}
{"id": "1471036", "url": "https://en.wikipedia.org/wiki?curid=1471036", "title": "Graphical timeline of the universe", "text": "Graphical timeline of the universe\n\nThis more than 20-billion-year timeline of our universe shows the best estimates of major events from the universe's beginning to anticipated future events. Zero on the scale is the present day. A large step on the scale is one billion years; a small step, one hundred million years. The past is denoted by a minus sign: e.g., the oldest rock on Earth was formed about four billion years ago and this is marked at -4e+09 years, where 4e+09 represents 4 times 10 to the power of 9. The \"Big Bang\" event most likely happened 13.8 billion years ago; see age of the universe.\n\n"}
{"id": "180236", "url": "https://en.wikipedia.org/wiki?curid=180236", "title": "Greisen–Zatsepin–Kuzmin limit", "text": "Greisen–Zatsepin–Kuzmin limit\n\nThe Greisen–Zatsepin–Kuzmin limit (GZK limit) is a theoretical upper limit on the energy of cosmic ray protons traveling from other galaxies through the intergalactic medium to our galaxy. The limit is , or about 8 joules. The limit is set by slowing-interactions of the protons with the microwave background radiation over long distances (~160 million light-years). The limit is at the same order of magnitude as the upper limit for energy at which cosmic rays have experimentally been detected. For example, one extreme-energy cosmic ray has been detected which appeared to possess a record (50 joules) of energy (about the same as the kinetic energy of a 35 mph baseball).\n\nThe GZK limit is derived under the assumption that ultra-high energy cosmic rays are protons. Measurements by the largest cosmic-ray observatory, the Pierre Auger Observatory, suggest that most ultra-high energy cosmic rays are heavier elements. In this case, the argument behind the GZK limit does not apply in the originally simple form and there is no fundamental contradiction in observing cosmic rays with energies that violate the limit.\n\nIn the past, the apparent violation of the GZK limit has inspired cosmologists and theoretical physicists to suggest other ways that circumvent the limit. These theories propose that ultra-high energy cosmic rays are produced nearby our galaxy or that Lorentz covariance is violated in such a way that protons do not lose energy on their way to our galaxy.\n\nThe limit was independently computed in 1966 by Kenneth Greisen, Vadim Kuzmin, and Georgiy Zatsepin, based on interactions between cosmic rays and the photons of the cosmic microwave background radiation (CMB). They predicted that cosmic rays with energies over the threshold energy of would interact with cosmic microwave background photons formula_1, relatively blueshifted by the speed of the cosmic rays, to produce pions via the formula_2 resonance,\n\nor\n\nPions produced in this manner proceed to decay in the standard pion channels—ultimately to photons for neutral pions, and photons, positrons, and various neutrinos for positive pions. Neutrons decay also to similar products, so that ultimately the energy of any cosmic ray proton is drained off by production of high energy photons plus (in some cases) high energy electron/positron pairs and neutrino pairs.\n\nThe pion production process begins at a higher energy than ordinary electron-positron pair production (lepton production) from protons impacting the CMB, which starts at cosmic ray proton energies of only about . However, pion production events drain 20% of the energy of a cosmic ray proton as compared with only 0.1% of its energy for electron positron pair production. This factor of 200 is from two sources: the pion has only about ~130 times the mass of the leptons, but the extra energy appears as different kinetic energies of the pion or leptons, and results in relatively more kinetic energy transferred to a heavier product pion, in order to conserve momentum. The much larger total energy losses from pion production result in the pion production process becoming the limiting one to high energy cosmic ray travel, rather than the lower-energy light-lepton production process.\n\nThe pion production process continues until the cosmic ray energy falls below the pion production threshold. Due to the mean path associated with this interaction, extragalactic cosmic rays traveling over distances larger than () and with energies greater than this threshold should never be observed on Earth. This distance is also known as GZK horizon.\n\nA number of observations have been made by the largest cosmic ray experiments Akeno Giant Air Shower Array, High Resolution Fly's Eye Cosmic Ray Detector, the Pierre Auger Observatory and Telescope Array Project that appeared to show cosmic rays with energies above this limit (called extreme-energy cosmic rays, or EECRs). The observation of these particles was the so-called GZK paradox or cosmic ray paradox.\n\nThese observations appear to contradict the predictions of special relativity and particle physics as they are presently understood. However, there are a number of possible explanations for these observations that may resolve this inconsistency.\n\nAnother suggestion involves ultra-high energy weakly interacting particles (for instance, neutrinos) which might be created at great distances and later react locally to give rise to the particles observed. In the proposed Z-burst model, an ultra-high energy cosmic neutrino collides with a relic anti-neutrino in our galaxy and annihilates to hadrons. This process proceeds via a (virtual) Z-boson:\n\nformula_8\n\nThe cross section for this process becomes large if the center of mass energy of the neutrino antineutrino pair is equal to the Z-boson mass (such a peak in the cross section is called \"resonance\"). Assuming that the relic anti-neutrino is at rest, the energy of the incident cosmic neutrino has to be:\n\nformula_9\n\nwhere formula_10 is the mass of the Z-boson and formula_11 the mass of the neutrino.\n\nA number of exotic theories have been advanced to explain the AGASA observations, including doubly special relativity. However, it is now established that standard doubly special relativity does not predict any GZK suppression (or GZK cutoff), contrary to models of Lorentz symmetry violation involving an absolute rest frame. Other possible theories involve a relation with dark matter, decays of exotic super-heavy particles beyond those known in the Standard Model.\n\nA suppression of the cosmic ray flux which can be explained with the GZK limit has been confirmed by the latest generation of cosmic ray observatories. A former claim by the AGASA experiment that there is no suppression was overruled. It remains controversial, whether the suppression is due to the GZK effect. The GZK limit only applies if ultra-high energy cosmic rays are mostly protons.\n\nIn July 2007, during the 30th International Cosmic Ray Conference in Mérida, Yucatán, México, the High Resolution Fly's Eye Experiment (HiRes) and the Pierre Auger Observatory (Auger) presented their results on ultra-high-energy cosmic rays. HiRes observed a suppression in the UHECR spectrum at just the right energy, observing only 13 events with an energy above the threshold, while expecting 43 with no suppression. This was interpreted as the first observation of the GZK limit. Auger confirmed the flux suppression, but did not claim it to be the GZK limit: instead of the 30 events necessary to confirm the AGASA results, Auger saw only two, which are believed to be heavy nuclei events. The flux suppression was previously brought into question when the AGASA experiment found no suppression in their spectrum. According to Alan Watson, spokesperson for the Auger Collaboration, AGASA results have been shown to be incorrect, possibly due to the systematic shift in energy assignment.\n\nIn 2010 and the following years, both the Pierre Auger Observatory and HiRes confirmed again a flux suppression, in case of the Pierre Auger Observatory the effect is statistically significant at the level of 20 standard deviations.\n\nAfter the flux suppression was established, a heated debate ensued whether cosmic rays that violate the GZK limit are protons. The Pierre Auger Observatory, the world's largest observatory, found with high statistical significance that ultra-high energy cosmic rays are not purely protons, but a mixture of elements which is getting heavier with increasing energy.\nThe Telescope Array Project, a joint effort from members of the HiRes and AGASA collaborations, agrees with the former HiRes result that these cosmic rays look like protons. The claim is based on data with lower statistical significance, however. The area covered by Telescope Array is about one third of the area covered by the Pierre Auger Observatory, and the latter has been running for a longer time.\n\nThe controversy was partially resolved in 2017, when a joint working group formed by members of both experiments presented a report at the 35th International Cosmic Ray Conference. According to the report, the raw experimental results are not in contradiction with each other. The different interpretations are mainly based on the use of different theoretical models (Telescope Array uses an outdated model for its interpretation), and the fact that Telescope Array has not collected enough events yet to distinguish the pure proton hypothesis from the mixed-nuclei hypothesis.\n\nEUSO, which was scheduled to fly on the International Space Station (ISS) in 2009, was designed to use the atmospheric-fluorescence technique to monitor a huge area and boost the statistics of UHECRs considerably. EUSO is to make a deep survey of UHECR-induced extensive air showers (EASs) from space, extending the measured energy spectrum well beyond the GZK-cutoff. It is to search for the origin of UHECRs, determine the nature of the origin of UHECRs, make an all-sky survey of the arrival direction of UHECRs, and seek to open the astronomical window on the extreme-energy universe with neutrinos. The fate of the EUSO Observatory is still unclear since NASA is considering early retirement of the ISS.\n\nLaunched in June 2008, the Fermi Gamma-ray Space Telescope (formerly GLAST) will also provide data that will help resolve these inconsistencies.\n\nIn November 2007, researchers at the Pierre Auger Observatory announced that they had evidence that UHECRs appear to come from the active galactic nuclei (AGNs) of energetic galaxies powered by matter swirling onto a supermassive black hole. The cosmic rays were detected and traced back to the AGNs using the Véron-Cetty-Véron catalog. These results are reported in the journal \"Science\". Nevertheless, the strength of the correlation with AGNs from this particular catalog for the Auger data recorded after 2007 has been slowly diminishing.\n\n"}
{"id": "3588836", "url": "https://en.wikipedia.org/wiki?curid=3588836", "title": "Hardness", "text": "Hardness\n\nHardness is a measure of the resistance to localized plastic deformation induced by either mechanical indentation or abrasion. Some materials (e.g. metals) are harder than others (e.g. plastics, wood). Macroscopic hardness is generally characterized by strong intermolecular bonds, but the behavior of solid materials under force is complex; therefore, there are different measurements of hardness: \"scratch hardness\", \"indentation hardness\", and \"rebound hardness\".\n\nHardness is dependent on ductility, elastic stiffness, plasticity, strain, strength, toughness, viscoelasticity, and viscosity.\n\nCommon examples of hard matter are ceramics, concrete, certain metals, and superhard materials, which can be contrasted with soft matter.\n\nThere are three main types of hardness measurements: \"scratch\", \"indentation\", and \"rebound\". Within each of these classes of measurement there are individual measurement scales. For practical reasons conversion tables are used to convert between one scale and another.\n\nScratch hardness is the measure of how resistant a sample is to fracture or permanent plastic deformation due to friction from a sharp object. The principle is that an object made of a harder material will scratch an object made of a softer material. When testing coatings, scratch hardness refers to the force necessary to cut through the film to the substrate. The most common test is Mohs scale, which is used in mineralogy. One tool to make this measurement is the sclerometer.\n\nAnother tool used to make these tests is the pocket hardness tester. This tool consists of a scale arm with graduated markings attached to a four-wheeled carriage. A scratch tool with a sharp rim is mounted at a predetermined angle to the testing surface. In order to use it a weight of known mass is added to the scale arm at one of the graduated markings, the tool is then drawn across the test surface. The use of the weight and markings allows a known pressure to be applied without the need for complicated machinery.\n\nIndentation hardness measures the resistance of a sample to material deformation due to a constant compression load from a sharp object. Tests for indentation hardness are primarily used in engineering and metallurgy fields. The tests work on the basic premise of measuring the critical dimensions of an indentation left by a specifically dimensioned and loaded indenter.\n\nCommon indentation hardness scales are Rockwell, Vickers, Shore, and Brinell, amongst others.\n\nRebound hardness, also known as \"dynamic hardness\", measures the height of the \"bounce\" of a diamond-tipped hammer dropped from a fixed height onto a material. This type of hardness is related to elasticity. The device used to take this measurement is known as a scleroscope.\n\nTwo scales that measures rebound hardness are the Leeb rebound hardness test and Bennett hardness scale.\n\nThere are five hardening processes: Hall-Petch strengthening, work hardening, solid solution strengthening, precipitation hardening, and martensitic transformation.\n\nIn solid mechanics, solids generally have three responses to force, depending on the amount of force and the type of material:\n\nStrength is a measure of the extent of a material's elastic range, or elastic and plastic ranges together. This is quantified as compressive strength, shear strength, tensile strength depending on the direction of the forces involved. Ultimate strength is an engineering measure of the maximum load a part of a specific material and geometry can withstand.\n\nBrittleness, in technical usage, is the tendency of a material to fracture with very little or no detectable plastic deformation beforehand. Thus in technical terms, a material can be both brittle and strong. In everyday usage \"brittleness\" usually refers to the tendency to fracture under a small amount of force, which exhibits both brittleness and a lack of strength (in the technical sense). For perfectly brittle materials, yield strength and ultimate strength are the same, because they do not experience detectable plastic deformation. The opposite of brittleness is ductility.\n\nThe toughness of a material is the maximum amount of energy it can absorb before fracturing, which is different from the amount of force that can be applied. Toughness tends to be small for brittle materials, because elastic and plastic deformations allow materials to absorb large amounts of energy.\n\nHardness increases with decreasing particle size. This is known as the Hall-Petch relationship. However, below a critical grain-size, hardness decreases with decreasing grain size. This is known as the inverse Hall-Petch effect.\n\nHardness of a material to deformation is dependent on its microdurability or small-scale shear modulus in any direction, not to any rigidity or stiffness properties such as its bulk modulus or Young's modulus. Stiffness is often confused for hardness. Some materials are stiffer than diamond (e.g. osmium) but are not harder, and are prone to spalling and flaking in squamose or acicular habits.\n\nThe key to understanding the mechanism behind hardness is understanding the metallic microstructure, or the structure and arrangement of the atoms at the atomic level. In fact, most important metallic properties critical to the manufacturing of today’s goods are determined by the microstructure of a material. At the atomic level, the atoms in a metal are arranged in an orderly three-dimensional array called a crystal lattice. In reality, however, a given specimen of a metal likely never contains a consistent single crystal lattice. A given sample of metal will contain many grains, with each grain having a fairly consistent array pattern. At an even smaller scale, each grain contains irregularities.\n\nThere are two types of irregularities at the grain level of the microstructure that are responsible for the hardness of the material. These irregularities are point defects and line defects. A point defect is an irregularity located at a single lattice site inside of the overall three-dimensional lattice of the grain. There are three main point defects. If there is an atom missing from the array, a vacancy defect is formed. If there is a different type of atom at the lattice site that should normally be occupied by a metal atom, a substitutional defect is formed. If there exists an atom in a site where there should normally not be, an interstitial defect is formed. This is possible because space exists between atoms in a crystal lattice. While point defects are irregularities at a single site in the crystal lattice, line defects are irregularities on a plane of atoms. Dislocations are a type of line defect involving the misalignment of these planes. In the case of an edge dislocation, a half plane of atoms is wedged between two planes of atoms. In the case of a screw dislocation two planes of atoms are offset with a helical array running between them.\n\nIn glasses, hardness seems to depend linearly on the number of topological constraints acting between the atoms of the network. Hence, the rigidity theory has allowed predicting hardness values with respect to composition.\n\nDislocations provide a mechanism for planes of atoms to slip and thus a method for plastic or permanent deformation. Planes of atoms can flip from one side of the dislocation to the other effectively allowing the dislocation to traverse through the material and the material to deform permanently. The movement allowed by these dislocations causes a decrease in the material's hardness.\n\nThe way to inhibit the movement of planes of atoms, and thus make them harder, involves the interaction of dislocations with each other and interstitial atoms. When a dislocation intersects with a second dislocation, it can no longer traverse through the crystal lattice. The intersection of dislocations creates an anchor point and does not allow the planes of atoms to continue to slip over one another A dislocation can also be anchored by the interaction with interstitial atoms. If a dislocation comes in contact with two or more interstitial atoms, the slip of the planes will again be disrupted. The interstitial atoms create anchor points, or pinning points, in the same manner as intersecting dislocations.\n\nBy varying the presence of interstitial atoms and the density of dislocations, a particular metal's hardness can be controlled. Although seemingly counter-intuitive, as the density of dislocations increases, there are more intersections created and consequently more anchor points. Similarly, as more interstitial atoms are added, more pinning points that impede the movements of dislocations are formed. As a result, the more anchor points added, the harder the material will become.\n\n\n\n\n"}
{"id": "40111102", "url": "https://en.wikipedia.org/wiki?curid=40111102", "title": "Intelligent Energy", "text": "Intelligent Energy\n\nIntelligent Energy is a fuel cell engineering company focused on the development and commercialisation of its PEM fuel cell technologies for a range of markets including automotive, stationary power and UAVs. We are headquartered in the UK, with offices and representation in the US, Japan, India, and China.\n\nThe origins of Intelligent Energy began at Loughborough University in the UK during the late 1980s, when the University became one of Europe’s first research and development centres for proton exchange membrane (PEM) fuel cell technology. In 1995, the UK’s first kW-level PEM fuel cell stack was produced by the R&D team. In June of that year, Advanced Power Sources (APS) Ltd was founded as a spin-out from Loughborough University by Paul Adcock, Phil Mitchell, Jon Moore and Anthony Newbold, and was the first company in the UK formed specifically to address the development and commercialisation of PEM fuel cells.\n\nFounded by Harry Bradbury, Intelligent Energy was established in 2001, acquiring Advanced Power Sources Ltd, together with its personnel and fuel cell related intellectual property that originated from research conducted by both APS and Loughborough University into PEM fuel cell technology. This triggered investment and enabled the company to grow its business activities.\nIn March 2005, it launched the ENV, the world’s first purpose-built fuel cell motorbike which gained the company recognition as a Technology Pioneer by the World Economic Forum in 2006. The ENV incorporated the company’s air-cooled fuel cell technology hybridised with a battery pack to provide 6 kW peak load to the motor to improve performance during spikes in power demand i.e. acceleration.\n\nIn 2007, a partnership was announced with Suzuki Motor Corporation to develop hydrogen fuel cells for a range of vehicles. In 2008, Intelligent Energy established the company, IE-CHP in a joint venture with SSE plc, to develop fuel cells and other technologies for CHP (Combined Heat and Power) applications. In the same year, Intelligent Energy also produced the power system for the first fuel cell powered manned flight in conjunction with Boeing.\nIn 2010, its fuel-cell taxi received The Engineer Technology and Innovation Award.\n\nIn March 2011, the Suzuki Burgman fuel cell scooter, equipped with Intelligent Energy’s fuel cell system, became the first fuel cell vehicle to achieve European Whole Vehicle Type Approval.\n\nIn 2012, SMILE FC System Corporation, a joint venture between Intelligent Energy and Suzuki Motor Corporation, was established to develop and manufacture air-cooled fuel cell systems for the automotive and a range of industry sectors.\nDuring the same year, a fleet of fuel cell taxis incorporating Intelligent Energy’s technology was used during the 2012 London Olympics. Part of the European Union-funded HyTEC (Hydrogen Technologies in European Cities) project launched in 2011, the taxis were used to transport VIP guests of the Mayor of London around the city.\nIn 2013, SMILE FC Corporation announced that it had established a ready-to-scale production line for its fuel cell systems, utilising Intelligent Energy’s semi-automated production technology. IE-CHP also received CE certification for its first-generation product, a 10 kWe/12 kWth combined heat and power (CHP) fuel cell. The certification allows the product to be sold in the European Economic Area, confirming that the product satisfies all the EU regulatory and conformity assessment procedures covering the design, manufacture, and testing of the system.\n\nIntelligent Energy was acquired by Meditor Energy, part of the Meditor Group, in October 2017.\n\nIntelligent Energy's fuel-cell technology is divided into two platforms: air-cooled (AC) and evaporatively-cooled (EC). The air-cooled fuel cell systems use low-power fans to provide cooling and the oxidant supply for operation. Heat from the fuel cell stack is conducted to cooling plates and removed through airflow channels, a simplified and cost-effective system for the power range from a few watts to several kilowatts. They are used in a wide range of UAV, stationary power and automotive applications for two-wheel and small car range extender applications.\n\nEvaporatively-cooled (EC) fuel cell systems provide power generation from a few kilowatts up to 200 kW. Efficient thermal management of the EC fuel cell stack reduces system complexity, mass and cost. These systems are designed for high-volume, low-cost manufacturing, and use modular architecture that can be quickly modified to suit the application.\n\nThe firm's fuel cell stacks have been developed for small and large cars, scooters and motorbikes. \nIn 2010, the company was involved in the development of the report entitled “A portfolio of power-trains for Europe: a fact-based analysis. The role of Battery Electric Vehicles, Plug-In Hybrids and Fuel Cell Electric Vehicles”, produced by McKinsey & Company with input from car manufacturers, oil and gas suppliers, utilities and industrial gas companies, wind turbine and electrolyser companies as well as governmental and non-governmental organisations. The report concluded, amongst other findings, that fuel cell vehicles are technology ready, and cost competitive, and that decarbonisation targets for Europe are unlikely to be met without the introduction of fuel cell powertrains.\n\nThe firm provides fuel cells to power UAVs and aerial drones. Its UAV Fuel Cell Modules run on hydrogen and ambient air to produce DC power in a lightweight package providing extended flight times when compared to battery systems.\n\nThe company’s fuel cell systems are used to provide diesel replacement and backup power initially for telecom towers but also for other sectors. The company has field proven its fuel cell products in the Indian telecommunications market with a tower uptime of close to 100%.\n\nThe company is a founding member of UKH Mobility, a government and industry group aiming to accelerate the commercial roll out of hydrogen vehicles in 2014/15;\nIt is also a member of the Fuel Cell and Hydrogen Energy Association (FCHEA), the US-based trade association for the fuel cell and hydrogen energy industry, dedicated to the commercialisation of fuel cells and hydrogen energy technologies.\n\n"}
{"id": "3591456", "url": "https://en.wikipedia.org/wiki?curid=3591456", "title": "Interface (matter)", "text": "Interface (matter)\n\nIn the physical sciences, an interface is the boundary between two spatial regions occupied by different matter, or by matter in different physical states. The interface between matter and air, or matter and vacuum, is called a surface, and studied in surface science. In thermal equilibrium, the regions in contact are called phases, and the interface is called a phase boundary. An example for an interface out of equilibrium is the grain boundary in polycrystalline matter.\n\nThe importance of the interface depends on the type of system: the bigger the quotient area/volume, the greater the effect the interface will have. Consequently, interfaces are very important in systems with large interface area-to-volume ratios, such as colloids.\n\nInterfaces can be flat or curved. For example, oil droplets in a salad dressing are spherical but the interface between water and air in a glass of water is mostly flat.\n\nSurface tension is the physical property which rules interface processes involving liquids. For a liquid film on flat surfaces, the liquid-vapor interface keeps flat to minimize interfacial area and system free energy. For a liquid film on rough surfaces, the surface tension tends to keep the meniscus flat, while the disjoining pressure makes the film conformal to the substrate. The equilibrium meniscus shape is a result of the competition between the capillary pressure and disjoining pressure.\n\nInterfaces may cause various optical phenomena, such as refraction. Optical lenses serve as an example of a practical application of the interface between glass and air.\n\nOne topical interface system is the gas-liquid interface between aerosols and other atmospheric molecules.\n\n"}
{"id": "985963", "url": "https://en.wikipedia.org/wiki?curid=985963", "title": "Lambda-CDM model", "text": "Lambda-CDM model\n\nThe ΛCDM (Lambda cold dark matter) or Lambda-CDM model is a parametrization of the Big Bang cosmological model in which the universe contains a cosmological constant, denoted by Lambda (Greek Λ), associated with dark energy, and cold dark matter (abbreviated CDM). It is frequently referred to as the standard model of Big Bang cosmology because it is the simplest model that provides a reasonably good account of the following properties of the cosmos:\n\nThe model assumes that general relativity is the correct theory of gravity on cosmological scales.\nIt emerged in the late 1990s as a concordance cosmology, after a period of time when disparate observed properties of the universe appeared mutually inconsistent, and there was no consensus on the makeup of the energy density of the universe.\n\nThe ΛCDM model can be extended by adding cosmological inflation, quintessence and other elements that are current areas of speculation and research in cosmology.\n\nSome alternative models challenge the assumptions of the ΛCDM model. Examples of these are modified Newtonian dynamics, modified gravity, theories of large-scale variations in the matter density of the universe, and scale invariance of empty space.\n\nMost modern cosmological models are based on the cosmological principle, which states that our observational location in the universe is not unusual or special; on a large-enough scale, the universe looks the same in all directions (isotropy) and from every location (homogeneity).\n\nThe model includes an expansion of metric space that is well documented both as the red shift of prominent spectral absorption or emission lines in the light from distant galaxies and as the time dilation in the light decay of supernova luminosity curves. Both effects are attributed to a Doppler shift in electromagnetic radiation as it travels across expanding space. Although this expansion increases the distance between objects that are not under shared gravitational influence, it does not increase the size of the objects (e.g. galaxies) in space. It also allows for distant galaxies to recede from each other at speeds greater than the speed of light; local expansion is less than the speed of light, but expansion summed across great distances can collectively exceed the speed of light.\n\nThe letter formula_1 (lambda) represents the cosmological constant, which is currently associated with a vacuum energy or dark energy in empty space that is used to explain the contemporary accelerating expansion of space against the attractive effects of gravity. A cosmological constant has negative pressure, formula_2, which contributes to the stress-energy tensor that, according to the general theory of relativity, causes accelerating expansion. The fraction of the total energy density of our (flat or almost flat) universe that is dark energy, formula_3, is currently (2015) estimated to be 0.692 ± 0.012, or even 0.6911 ± 0.0062 based on Planck satellite data.\n\nDark matter is postulated in order to account for gravitational effects observed in very large-scale structures (the \"flat\" rotation curves of galaxies; the gravitational lensing of light by galaxy clusters; and enhanced clustering of galaxies) that cannot be accounted for by the quantity of observed matter. Cold dark matter is \"non-baryonic\", i.e. it consists of matter other than protons and neutrons (and electrons, by convention, although electrons are not baryons); \"cold\", i.e. its velocity is far less than the speed of light at the epoch of radiation-matter equality (thus neutrinos are excluded, being non-baryonic but not cold); \"dissipationless\", i.e. it cannot cool by radiating photons; and \"collisionless\", i.e. the dark matter particles interact with each other and other particles only through gravity and possibly the weak force. The dark matter component is currently (2013) estimated to constitute about 26.8% of the mass-energy density of the universe.\n\nThe remaining 4.9% (2013) comprises all ordinary matter observed as atoms, chemical elements, gas and plasma, the stuff of which visible planets, stars and galaxies are made. The great majority of ordinary matter in the universe is unseen, since visible stars and gas inside galaxies and clusters account for less than 10% of the ordinary matter contribution to the mass-energy density of the universe.\n\nAlso, the energy density includes a very small fraction (~ 0.01%) in cosmic microwave background radiation, and not more than 0.5% in relic neutrinos. Although very small today, these were much more important in the distant past, dominating the matter at redshift > 3200.\n\nThe model includes a single originating event, the \"Big Bang\", which was not an explosion but the abrupt appearance of expanding space-time containing radiation at temperatures of around 10 K. This was immediately (within 10 seconds) followed by an exponential expansion of space by a scale multiplier of 10 or more, known as cosmic inflation. The early universe remained hot (above 10,000 K) for several hundred thousand years, a state that is detectable as a residual cosmic microwave background, or CMB, a very low energy radiation emanating from all parts of the sky. The \"Big Bang\" scenario, with cosmic inflation and standard particle physics, is the only current cosmological model consistent with the observed continuing expansion of space, the observed distribution of lighter elements in the universe (hydrogen, helium, and lithium), and the spatial texture of minute irregularities (anisotropies) in the CMB radiation. Cosmic inflation also addresses the \"horizon problem\" in the CMB; indeed, it seems likely that the universe is larger than the observable particle horizon.\n\nThe model uses the Friedmann–Lemaître–Robertson–Walker metric, the Friedmann equations and the cosmological equations of state to describe the observable universe from right after the inflationary epoch to present and future.\n\nThe expansion of the universe is parametrized by a dimensionless scale factor formula_4 (with time formula_5 counted from the birth of the universe), defined relative to the present day, so formula_6; the usual convention in cosmology is that subscript 0 denotes present-day values, so formula_7 is the current age of the universe. The scale factor is related to the observed redshift formula_8 of the light emitted at time formula_9 by\n\nThe expansion rate is described by the time-dependent Hubble parameter, formula_11, defined as\nwhere formula_13 is the time-derivative of the scale factor. The first Friedmann equation gives the expansion rate in terms of the matter+radiation density the curvature and the cosmological constant \n\nwhere as usual is the speed of light and is the gravitational constant. \nA critical density formula_15 is the present-day density, which gives zero curvature formula_16, assuming the cosmological constant formula_1 is zero, regardless of its actual value. Substituting these conditions to the Friedmann equation gives\n\nwhere formula_19 is the reduced Hubble constant.\nIf the cosmological constant were actually zero, the critical density would also mark the dividing line between eventual recollapse of the universe to a Big Crunch, or unlimited expansion. For the Lambda-CDM model with a positive cosmological constant (as observed), the universe is predicted to expand forever regardless of whether the total density is slightly above or below the critical density; though other outcomes are possible in extended models where the dark energy is not constant but actually time-dependent.\n\nIt is standard to define the present-day density parameter formula_20 for various species as the dimensionless ratio\nwhere the subscript formula_22 is one of formula_23 for baryons, formula_24 for cold dark matter, formula_25 for radiation (photons plus relativistic neutrinos), and formula_26 or formula_1 for dark energy.\n\nSince the densities of various species scale as different powers of formula_28, e.g. formula_29 for matter etc.,\nthe Friedmann equation can be conveniently rewritten in terms of the various density parameters as\nwhere w is the equation of state of dark energy, and assuming negligible neutrino mass (significant neutrino mass requires a more complex equation). The various formula_31 parameters add up to formula_32 by construction.\nIn the general case this is integrated by computer to give\nthe expansion history formula_33 and also observable distance-redshift relations for any chosen values of the cosmological parameters, which can then be compared with observations such as supernovae and baryon acoustic oscillations.\n\nIn the minimal 6-parameter Lambda-CDM model, it is assumed that curvature formula_34 is zero and formula_35, so this simplifies to\n\nObservations show that the radiation density is very small today, formula_37; if this term is neglected\nthe above has an analytic solution\nwhere formula_39\nthis is fairly accurate for formula_40 or formula_41million years.\nSolving for formula_42 gives the present age of the universe formula_43 in terms of the other parameters.\n\nIt follows that the transition from decelerating to accelerating expansion (the second derivative formula_44 crossing zero) occurred when\n\nwhich evaluates to formula_46 or formula_47 for the best-fit parameters estimated from the Planck spacecraft.\n\nThe discovery of the Cosmic Microwave Background (CMB) in 1964 confirmed a key prediction of the Big Bang cosmology. From that point on, it was generally accepted that the universe started in a hot, dense state and has been expanding over time. The rate of expansion depends on the types of matter and energy present in the universe, and in particular, whether the total density is above or below the so-called critical density. During the 1970s, most attention focused on pure-baryonic models, but there were serious challenges explaining the formation of galaxies, given the small anisotropies in the CMB (upper limits at that time). In the early 1980s, it was realized that this could be resolved if cold dark matter dominated over the baryons, and the theory of cosmic inflation motivated models with critical density. During the 1980s, most research focused on cold dark matter with critical density in matter, around 95% CDM and 5% baryons: these showed success at forming galaxies and clusters of galaxies, but problems remained; notably, the model required a Hubble constant lower than preferred by observations, and observations around 1988-1990 showed more large-scale galaxy clustering than predicted. These difficulties sharpened with the discovery of CMB anisotropy by COBE in 1992, and several modified CDM models, including ΛCDM and mixed cold and hot dark matter, came under active consideration through the mid-1990s. The ΛCDM model then became the leading model following the observations of accelerating expansion in 1998, and was quickly supported by other observations: in 2000, the BOOMERanG microwave background experiment measured the total (matter–energy) density to be close to 100% of critical, whereas in 2001 the 2dFGRS galaxy redshift survey measured the matter density to be near 25%; the large difference between these values supports a positive Λ or dark energy. Much more precise spacecraft measurements of the microwave background from WMAP in 2003 – 2010 and Planck in 2013 - 2015 have continued to support the model and pin down the parameter values, most of which are now constrained below 1 percent uncertainty.\n\nThere is currently active research into many aspects of the ΛCDM model, both to refine the parameters and possibly detect deviations. In addition, ΛCDM has no explicit physical theory for the origin or physical nature of dark matter or dark energy; the nearly scale-invariant spectrum of the CMB perturbations, and their image across the celestial sphere, are believed to result from very small thermal and acoustic irregularities at the point of recombination. A large majority of astronomers and astrophysicists support the ΛCDM model or close relatives of it, but Milgrom, McGaugh, and Kroupa are leading critics, attacking the dark matter portions of the theory from the perspective of galaxy formation models and supporting the alternative MOND theory, which requires a modification of the Einstein field equations and the Friedmann equations as seen in proposals such as MOG theory or TeVeS theory. Other proposals by theoretical astrophysicists of cosmological alternatives to Einstein's general relativity that attempt to account for dark energy or dark matter include f(R) gravity, scalar–tensor theories such as galileon theories, brane cosmologies, the DGP model, and massive gravity and its extensions such as bimetric gravity.\n\nIn addition to explaining pre-2000 observations,\nthe model has made a number of successful predictions: notably the existence of the\nbaryon acoustic oscillation feature, discovered in 2005 in the predicted location; and the statistics of weak gravitational lensing, first observed in 2000 by several teams. The polarization of the CMB, discovered in 2002 by DASI is now a dramatic success: in the 2015 Planck data release, there are seven observed peaks in the temperature (TT) power spectrum, six peaks in the temperature-polarization (TE) cross spectrum, and five peaks in the polarization (EE) spectrum. The six free parameters can be well constrained by the TT spectrum alone, and then the TE and EE spectra can be predicted theoretically to few-percent precision with no further adjustments allowed: comparison of theory and observations shows an excellent match.\n\nExtensive searches for dark matter particles have so far shown no well-agreed detection;\nthe dark energy may be almost impossible to detect in a laboratory, and its value is unnaturally small compared to naive theoretical predictions.\n\nComparison of the model with observations is very successful on large scales (larger than galaxies, up to the observable horizon), but may have some problems on sub-galaxy scales, possibly predicting too many dwarf galaxies and too much dark matter in the innermost regions of galaxies. This problem is called the \"small scale crisis\". These small scales are harder to resolve in computer simulations, so it is not yet clear whether the problem is the simulations, non-standard properties of dark matter, or a more radical error in the model.\n\nIt has been argued that the ΛCDM model is built upon a foundation of conventionalist stratagems, rendering it unfalsifiable in the sense defined by Karl Popper.\n\nThe simple ΛCDM model is based on six parameters: physical baryon density parameter; physical dark matter density parameter; the age of the universe; scalar spectral index; curvature fluctuation amplitude; and reionization optical depth. In accordance with Occam's razor, six is the smallest number of parameters needed to give an acceptable fit to current observations; other possible parameters are fixed at \"natural\" values, e.g. total density parameter = 1.00, dark energy equation of state = −1. (See below for extended models that allow these to vary.)\n\nThe values of these six parameters are mostly not predicted by current theory (though, ideally, they may be related by a future \"Theory of Everything\"), except that most versions of cosmic inflation predict the scalar spectral index should be slightly smaller than 1, consistent with the estimated value 0.96. The parameter values, and uncertainties, are estimated using large computer searches to locate the region of parameter space providing an acceptable match to cosmological observations. From these six parameters, the other model values, such as the Hubble constant and the dark energy density, can be readily calculated.\n\nCommonly, the set of observations fitted includes the cosmic microwave background anisotropy, the brightness/redshift relation for supernovae, and large-scale galaxy clustering including the baryon acoustic oscillation feature. Other observations, such as the Hubble constant, the abundance of galaxy clusters, weak gravitational lensing and globular cluster ages, are generally consistent with these, providing a check of the model, but are less precisely measured at present.\n\nParameter values listed below are from the Planck Collaboration Cosmological parameters 68% confidence limits for the base ΛCDM model from Planck CMB power spectra, in combination with lensing reconstruction and external data (BAO + JLA + H). See also Planck (spacecraft).\n\nMassimo Persic and Paolo Salucci firstly estimated the baryonic density today present in ellipticals, spirals, groups and clusters of galaxies.\nThey performed an integration of the baryonic mass-to-light ratio over luminosity (in the following formula_48), weighted with the luminosity function formula_49 over the previously mentioned classes of astrophysical objects: \n\nThe result was:\n\nwhere formula_52.\n\nNote that this value is much lower than the prediction of standard cosmic nucleosynthesis formula_53, so that stars and gas in galaxies and in galaxy groups and clusters account for less than 10% of the primordially synthesized baryons. This issue is known as the problem of the \"missing baryons\".\n\nExtended models allow one or more of the \"fixed\" parameters above to vary, in addition to the basic six; so these models join smoothly to the basic six-parameter model in the limit that the additional parameter(s) approach the default values. For example, possible extensions of the simplest ΛCDM model allow for spatial curvature (formula_54 may be different from 1); or quintessence rather than a cosmological constant where the equation of state of dark energy is allowed to differ from −1. Cosmic inflation predicts tensor fluctuations (gravitational waves). Their amplitude is parameterized by the tensor-to-scalar ratio (denoted formula_55), which is determined by the unknown energy scale of inflation. Other modifications allow hot dark matter in the form of neutrinos more massive than the minimal value, or a running spectral index; the latter is generally not favoured by simple cosmic inflation models.\n\nAllowing additional variable parameter(s) will generally \"increase\" the uncertainties in the standard six parameters quoted above, and may also shift the central values slightly. The Table below shows results for each of the possible \"6+1\" scenarios with one additional variable parameter; this indicates that, as of 2015, there is no convincing evidence that any additional parameter is different from its default value.\n\nSome researchers have suggested that there is a running spectral index, but no statistically significant study has revealed one. Theoretical expectations suggest that the tensor-to-scalar ratio formula_55 should be between 0 and 0.3, and the latest results are now within those limits.\n\n\n"}
{"id": "9332507", "url": "https://en.wikipedia.org/wiki?curid=9332507", "title": "Leibniz–Clarke correspondence", "text": "Leibniz–Clarke correspondence\n\nThe Leibniz–Clarke correspondence was a scientific, theological and philosophical debate conducted in an exchange of letters between the German thinker Gottfried Wilhelm Leibniz and Samuel Clarke, an English supporter of Isaac Newton during the years 1715 and 1716. The exchange began because of a letter Leibniz wrote to Caroline of Ansbach, in which he remarked that Newtonian physics was detrimental to natural theology. Eager to defend the Newtonian view, Clarke responded, and the correspondence continued until the death of Leibniz in 1716.\n\nAlthough a variety of subjects is touched on in the letters, the main interest for modern readers is in the dispute between the absolute theory of space favoured by Newton and Clarke, and Leibniz's relational approach. Also important is the conflict between Clarke's and Leibniz's opinions on free will and whether God must create the best of all possible worlds.\n\nLeibniz had published only a book on moral matters, the \"Theodicée\" (1710), and his more metaphysical views had never been exposed to a sufficient extent, so the collected letters were met with interest by their contemporaries. The priority dispute between Leibniz and Newton about the calculus was still fresh in the public's mind and it was taken as a matter of course that it was Newton himself who stood behind Clarke's replies.\n\nThe Leibniz-Clarke letters were first published under Clarke's name in the year following Leibniz' death. He wrote a preface, took care of the translation from French, added notes and some of his own writing. In 1720 Pierre Desmaizeaux published a similar volume in a French translation, including quotes from Newton's work. It is quite certain that for both editions the opinion of Newton himself has been sought and Leibniz left at a disadvantage. However the German translation of the correspondence published by Kohler, also in 1720, contained a reply to Clarke's last letter which Leibniz had not been able to answer. The letters have been reprinted in most collections of Leibniz' works and regularly published in stand alone editions.\n\n\n\n"}
{"id": "14997569", "url": "https://en.wikipedia.org/wiki?curid=14997569", "title": "Location of Earth", "text": "Location of Earth\n\nKnowledge of the location of Earth has been shaped by 400 years of telescopic observations, and has expanded radically in the last century. Initially, Earth was believed to be the center of the Universe, \nwhich consisted only of those planets visible with the naked eye and an outlying sphere of fixed stars. After the acceptance of the heliocentric model in the 17th century, observations by William Herschel and others showed that the Sun lay within a vast, disc-shaped galaxy of stars. By the 20th century, observations of spiral nebulae revealed that our galaxy was one of billions in an expanding universe, grouped into clusters and superclusters. By the end of the 20th century, the overall structure of the visible universe was becoming clearer, with superclusters forming into a vast web of filaments and voids. Superclusters, filaments and voids are the largest coherent structures in the Universe that we can observe. At still larger scales (over 1000 megaparsecs) the Universe becomes homogeneous meaning that all its parts have on average the same density, composition and structure.\n\nSince there is believed to be no \"center\" or \"edge\" of the Universe, there is no particular reference point with which to plot the overall location of the Earth in the universe. Because the observable universe is defined as that region of the Universe visible to terrestrial observers, Earth is, by definition, the center of Earth's observable universe. Reference can be made to the Earth's position with respect to specific structures, which exist at various scales. It is still undetermined whether the Universe is infinite. There have been numerous hypotheses that our universe may be only one such example within a higher multiverse; however, no direct evidence of any sort of multiverse has ever been observed, and some have argued that the hypothesis is not falsifiable.\n\n"}
{"id": "3595285", "url": "https://en.wikipedia.org/wiki?curid=3595285", "title": "Maximum power principle", "text": "Maximum power principle\n\nThe maximum power principle or Lotka's principle has been proposed as the fourth principle of energetics in open system thermodynamics, where an example of an open system is a biological cell. According to Howard T. Odum, \"The maximum power principle can be stated: During self-organization, system designs develop and prevail that maximize power intake, energy transformation, and those uses that reinforce production and efficiency.\"\n\nChen (2006) has located the origin of the statement of maximum power as a formal principle in a tentative proposal by Alfred J. Lotka (1922a, b). Lotka's statement sought to explain the Darwinian notion of evolution with reference to a physical principle. Lotka's work was subsequently developed by the systems ecologist Howard T. Odum in collaboration with the Chemical Engineer Richard C. Pinkerton, and later advanced by the Engineer Myron Tribus.\n\nWhile Lotka's work may have been a first attempt to formalise evolutionary thought in mathematical terms, it followed similar observations made by Leibniz and Volterra and Ludwig Boltzmann, for example, throughout the sometimes controversial history of natural philosophy. In contemporary literature it is most commonly associated with the work of Howard T. Odum.\n\nThe significance of Odum's approach was given greater support during the 1970s, amid times of oil crisis, where, as Gilliland (1978, pp. 100) observed, there was an emerging need for a new method of analysing the importance and value of energy resources to economic and environmental production. A field known as energy analysis, itself associated with net energy and EROEI, arose to fulfill this analytic need. However, in energy analysis intractable theoretical and practical difficulties arose when using the energy unit to understand, a) the conversion among concentrated fuel types (or energy types), b) the contribution of labour, and c) the contribution of the environment.\n\nLotka said (1922b: 151): \nGilliland noted that these difficulties in analysis in turn required some new theory to adequately explain the interactions and transactions of these different energies (different concentrations of fuels, labour and environmental forces). Gilliland (Gilliland 1978, p. 101) suggested that Odum's statement of the maximum power principle (H.T.Odum 1978, pp. 54–87) was, perhaps, an adequate expression of the requisite theory:\nThis theory Odum called maximum power theory. In order to formulate maximum power theory Gilliland observed that Odum had added another law (the maximum power principle) to the already well established laws of thermodynamics. In 1978 Gilliland wrote that Odum's new law had not yet been validated (Gilliland 1978, p. 101). Gilliland stated that in maximum power theory the second law efficiency of thermodynamics required an additional physical concept: \"the concept of second law efficiency under maximum power\" (Gilliland 1978, p. 101):\nIn this way the concept of maximum power was being used as a principle to quantitatively describe the selective law of biological evolution. Perhaps H.T.Odum's most concise statement of this view was (1970, p. 62):\n\nThe Odum–Pinkerton approach to Lotka's proposal was to apply Ohm's law – and the associated maximum power theorem (a result in electrical power systems) – to ecological systems. Odum and Pinkerton defined \"power\" in electronic terms as the rate of work, where Work is understood as a \"useful energy transformation\". The concept of maximum power can therefore be defined as the \"maximum rate of useful energy transformation\". Hence the underlying philosophy aims to unify the theories and associated laws of electronic and thermodynamic systems with biological systems. This approach presupposed an analogical view which sees the world as an ecological-electronic-economic engine.\n\nOdum et al. viewed the maximum power theorem as a principle of power-efficiency reciprocity selection with wider application than just electronics. For example, Odum saw it in open systems operating on solar energy, like both photovoltaics and photosynthesis (1963, p. 438). Like the maximum power theorem, Odum's statement of the maximum power principle relies on the notion of 'matching', such that high-quality energy maximizes power by matching and amplifying energy (1994, pp. 262, 541): \"in surviving designs a matching of high-quality energy with larger amounts of low-quality energy is likely to occur\" (1994, p. 260). As with electronic circuits, the resultant rate of energy transformation will be at a maximum at an intermediate power efficiency. In 2006, T.T. Cai, C.L. Montague and J.S. Davis said that, \"The maximum power principle is a potential guide to understanding the patterns and processes of ecosystem development and sustainability. The principle predicts the selective persistence of ecosystem designs that capture a previously untapped energy source.\" (2006, p. 317). In several texts H.T. Odum gave the Atwood machine as a practical example of the 'principle' of maximum power.\n\nThe mathematical definition given by H.T. Odum is formally analogous to the definition provided on the maximum power theorem article. (For a brief explanation of Odum's approach to the relationship between ecology and electronics see Ecological Analog of Ohm's Law)\n\nWhether or not the principle of maximum power efficiency can be considered the fourth law of thermodynamics and the fourth principle of energetics is moot. Nevertheless, H.T. Odum also proposed a corollary of maximum power as the organisational principle of evolution, describing the evolution of microbiological systems, economic systems, planetary systems, and astrophysical systems. He called this corollary the maximum empower principle. This was suggested because, as S.E. Jorgensen, M.T. Brown, H.T. Odum (2004) note,\n\nC. Giannantoni may have confused matters when he wrote \"The \"Maximum Em-Power Principle\" (Lotka–Odum) is generally considered the \"Fourth Thermodynamic Principle\" (mainly) because of its practical validity for a very wide class of physical and biological systems\" (C. Giannantoni 2002, § 13, p. 155). Nevertheless, Giannantoni has proposed the Maximum Em-Power Principle as the fourth principle of thermodynamics (Giannantoni 2006).\n\nThe preceding discussion is incomplete. The \"maximum power\" was discovered several times independently, in physics and engineering, see: Novikov (1957), El-Wakil (1962), and Curzon and Ahlborn (1975). The incorrectness of this analysis and design evolution conclusions was demonstrated by Gyftopoulos (2002).\n\n\n"}
{"id": "558685", "url": "https://en.wikipedia.org/wiki?curid=558685", "title": "Natural environment", "text": "Natural environment\n"}
{"id": "14389994", "url": "https://en.wikipedia.org/wiki?curid=14389994", "title": "Natural landscape", "text": "Natural landscape\n\nA natural landscape is the original landscape that exists before it is acted upon by human culture. The natural landscape and the cultural landscape are separate parts of the landscape. However, in the twenty-first century landscapes that are totally untouched by human activity no longer exist, so that reference is sometimes now made to degrees of naturalness within a landscape.\n\nIn \"Silent Spring\" (1962) Rachel Carson describes a roadside verge as it used to look: \"Along the roads, laurel, viburnum and alder, great ferns and wildflowers delighted the traveler’s eye through much of the year\" and then how it looks now following the use of herbicides: \"The roadsides, once so attractive, were now lined with browned and withered vegetation as though swept by fire\". Even though the landscape before it is sprayed is biologically degraded, and may well contains alien species, the concept of what might constitute a natural landscape can still be deduced from the context.\n\nThe phrase \"natural landscape\" was first used in connection with landscape painting, and landscape gardening, to contrast a formal style with a more natural one, closer to nature. Alexander von Humboldt (1769 – 1859) was to further conceptualize this into the idea of a natural landscape \"separate\" from the cultural landscape. Then in 1908 geographer Otto Schlüter developed the terms original landscape (\"Urlandschaft\") and its opposite cultural landscape (\"Kulturlandschaft\") in an attempt to give the science of geography a subject matter that was different from the other sciences. An early use of the actual phrase \"natural landscape\" by a geographer can be found in Carl O. Sauer's paper \"The Morphology of Landscape\" (1925).\n\nThe concept of a natural landscape was first developed in connection with landscape painting, though the actual term itself was first used in relation to landscape gardening. In both cases it was used to contrast a formal style with a more natural one, that is closer to nature. Chunglin Kwa suggests, \"that a seventeenth-century or early-eighteenth-century person could experience natural scenery ‘just like on a painting,’ and so, with or without the use of the word itself, designate it as a landscape.\" With regard to landscape gardening John Aikin, commented in 1794: \"Whatever, therefore, there be of \"novelty\" in the singular scenery of an artificial garden, it is soon exhausted, whereas the infinite diversity of a natural landscape presents an inexhaustible flore of new forms\". Writing in 1844 the prominent American landscape gardener Andrew Jackson Downing comments: \"straight canals, round or oblong pieces of water, and all the regular forms of the geometric mode ... would evidently be in violent opposition to the whole character and expression of natural landscape\".\n\nIn his extensive travels in South America, Alexander von Humboldt became the first to conceptualize a natural landscape separate from the cultural landscape, though he does not actually use these terms. Andrew Jackson Downing was aware of, and sympathetic to, Humboldt's ideas, which therefore influenced American landscape gardening.\n\nSubsequently, the geographer Otto Schlüter, in 1908, argued that by defining geography as a \"Landschaftskunde\" (landscape science) would give geography a logical subject matter shared by no other discipline. He defined two forms of landscape: the \"Urlandschaft\" (original landscape) or landscape that existed before major human induced changes and the \"Kulturlandschaft\" (cultural landscape) a landscape created by human culture. Schlüter argued that the major task of geography was to trace the changes in these two landscapes.\n\nThe term natural landscape is sometimes used as a synonym for wilderness, but for geographers natural landscape is a scientific term which refers to the biological, geological, climatological and other aspects of a landscape, not the cultural values that are implied by the word wilderness.\n\nMatters are complicated by the fact that the words nature and natural have more than one meaning. On the one hand there is the main dictionary meaning for nature: \"The phenomena of the physical world collectively, including plants, animals, the landscape, and other features and products of the earth, as opposed to humans or human creations\". On the other hand, there is the growing awareness, especially since Charles Darwin, of humanities biological affinity with nature.\n\nThe dualism of the first definition has its roots is an \"ancient concept\", because early people viewed \"nature, or the nonhuman world […] as a divine \"Other\", godlike in its separation from humans\". In the West, Christianity's myth of the fall, that is the expulsion of humankind from the Garden of Eden, where all creation lived in harmony, into an imperfect world, has been the major influence. Cartesian dualism, from the seventeenth century on, further reinforced this dualistic thinking about nature. \nWith this dualism goes value judgement as to the superiority of the natural over the artificial. Modern science, however, is moving towards a holistic view of nature.\n\nWhat is meant by natural, within the American conservation movement, has been changing over the last century and a half.\n\nIn the mid-nineteenth century American began to realize that the land was becoming more and more domesticated and wildlife was disappearing. This led to the creation of American National Parks and other conservation sites. Initially it was believed that all that was needed to do was to separate what was seen as natural landscape and \"avoid disturbances such as logging, grazing, fire and insect outbreaks\". This, and subsequent environmental policy, until recently, was influenced by ideas of the wilderness. However, this policy was not consistently applied, and in Yellowstone Park, to take one example, the existing ecology was altered, firstly by the exclusion of Native Americans and later with the virtual extermination of the wolf population.\n\nA century later, in the mid-twentieth century, it began to be believed that the earlier policy of \"protection from disturbance was inadequate to preserve park values\", and that is that direct human intervention was necessary to restore the landscape of National Parks to its ‘’natural’’ condition. In 1963 the Leopold Report argued that \"A national park should represent a vignette of primitive America\". This policy change eventually led to the restoration of wolves in Yellowstone Park in the 1990s.\n\nHowever, recent research in various disciplines indicates that a pristine natural or \"primitive\" landscape is a myth, and it now realised that people have been changing the natural into a cultural landscape for a long while, and that there are few places untouched in some way from human influence. The earlier conservation policies were now seen as cultural interventions. The idea of what is natural and what artificial or cultural, and how to maintain the natural elements in a landscape, has been further complicated by the discovery of global warming and how it is changing natural landscapes.\n\nAlso important is a reaction recently amongst scholars against dualistic thinking about nature and culture. Maria Kaika comments: \"Nowadays, we are beginning to see nature and culture as intertwined once again – not ontologically separated anymore […].What I used to perceive as a compartmentalized world, consisting of neatly and tightly sealed, autonomous ‘space envelopes’ (the home, the city, and nature) was, in fact, a messy socio-spatial continuum”. And William Cronon argues against the idea of wilderness because it \"involves a dualistic vision in which the human is entirely outside the natural\" and affirms that \"wildness (as opposed to wilderness) can be found anywhere\" even \"in the cracks of a Manhattan sidewalk\". According to Cronon we have to \"abandon the dualism that sees the tree in the garden as artificial […] and the tree in the wilderness as natural […] Both in some ultimate sense are wild.\" Here he bends somewhat the regular dictionary meaning of wild, to emphasise that nothing natural, even in a garden, is fully under human control.\n\nThe landscape of Europe has considerably altered by people and even in an area, like the Cairngorm Mountains of Scotland, with a low population density, only \" the high summits of the Cairngorm Mountains, consist entirely of natural elements. These \"high summits\" are of course only part of the Cairngorms, and there are no longer wolves, bears, wild boar or lynx in Scotland's wilderness. The Scots pine in the form of the Caledonian forest also covered much more of the Scottish landscape than today.\n\nThe Swiss National Park, however, represent a more natural landscape. It was founded in 1914, and is one of the earliest national parks in Europe.\nVisitors are not allowed to leave the motor road, or paths through the park, make fire or camp. The only building within the park is Chamanna Cluozza, mountain hut. It is also forbidden to disturb the animals or the plants, or to take home anything found in the park. Dogs are not allowed. Due to these strict rules, the Swiss National Park is the only park in the Alps who has been categorized by the IUCN as a strict nature reserve, which is the highest protection level.\n\nNo place on the Earth is unaffected by people and their culture. People are part of biodiversity, but human activity affects biodiversity, and this alters the natural landscape. Mankind have altered landscape to such an extent that few places on earth remain pristine, but once free of human influences, the landscape can return to a natural or near natural state.\nEven the remote Yukon and Alaskan wilderness, the bi-national Kluane-Wrangell-St. Elias-Glacier Bay-Tatshenshini-Alsek park system comprising Kluane, Wrangell-St Elias, Glacier Bay and Tatshenshini-Alsek parks, a UNESCO World Heritage Site, is not free from human influence, because the Kluane National Park lies within the traditional territories of the Champagne and Aishihik First Nations and Kluane First Nation who have a long history of living in this region. Through their respective Final Agreements with the Canadian Government, they have made into law their rights to harvest in this region.\n\nCultural forces intentionally or unintentionally, have an influence upon the landscape. Cultural landscapes are places or artifacts created and maintained by people. Examples of cultural intrusions into a landscape are: fences, roads, parking lots, sand pits, buildings, hiking trails, management of plants, including the introduction of invasive species, extraction or removal of plants, management of animals, mining, hunting, natural landscaping, farming and forestry, pollution. Areas that might be confused with a natural landscape include public parks, farms, orchards, artificial lakes and reservoirs, managed forests, golf courses, nature center trails, gardens.\n\n"}
{"id": "52634071", "url": "https://en.wikipedia.org/wiki?curid=52634071", "title": "Nature-based solutions", "text": "Nature-based solutions\n\nNature-based solutions (NBS or NbS) refers to the sustainable management and use of nature for tackling socio-environmental challenges. The challenges include issues such as climate change, water security, water pollution, food security, human health, and disaster risk management. \n\nA definition by the European Union states that these solutions are \"inspired and supported by nature, which are cost-effective, simultaneously provide environmental, social and economic benefits and help build resilience. The Nature-based Solutions Initiative meanwhile defines them as \"actions that work with and enhance nature so as to help people adapt to change and disasters\". Such solutions bring more, and more diverse, nature and natural features and processes into cities, landscapes and seascapes, through locally adapted, resource-efficient and systemic interventions\". With NBS, healthy, resilient and diverse ecosystems (whether natural, managed or newly created) can provide solutions for the benefit of societies and overall biodiversity.\n\nFor instance, the restoration or protection of mangroves along coastlines utilizes a nature-based solution to accomplish several things. Mangroves moderate the impact of waves and wind on coastal settlements or cities and sequester CO . They also provide safe nurseries for marine life that can be the basis for sustaining populations of fish that local populations may depend on. Additionally, the mangrove forests can help control coastal erosion resulting from sea level rise. Similarly, in cities green roofs or walls are nature-based solutions that can be used to moderate the impact of high temperatures, capture storm water, abate pollution, and act as carbon sinks, while enhancing biodiversity.\n\nConservation approaches and environment management initiatives have been carried out for decades. What is new is that the benefits of such nature-based solutions to human well-being have been articulated well more recently. Even if the term itself is still being framed, examples of nature-based solutions can be found all over the world, and imitated. Nature-based solutions are on their way to being mainstreamed in national and international policies and programmes (e.g. climate change policy, law, infrastructure investment and financing mechanisms). For example, the theme for World Water Day 2018 was \"Nature for water\" and by UN-Water's accompanying UN World Water Development Report had the title \"Nature-based Solutions for Water\".\n\nSocieties increasingly face challenges such as climate change, urbanization, jeopardized food security and water resource provision, and disaster risk. One approach to answer these challenges is to singularly rely on technological strategies. An alternative approach is to manage the (socio-)ecological systems in a comprehensive way in order to sustain and potentially increase the delivery of ecosystem services to humans. In this context, nature-based solutions (NBS) have recently been put forward by practitioners and quickly thereafter by policymakers. These solutions stress the sustainable use of nature in solving coupled environmental-social-economic challenges. \n\nWhile ecosystem services are often valued in terms of immediate benefits to human well-being and economy, NBS focus on the benefits to people and the environment itself, to allow for sustainable solutions that are able to respond to environmental change and hazards in the long-term. NBS go beyond the traditional biodiversity conservation and management principles by \"re-focusing\" the debate on humans and specifically integrating societal factors such as human well-being and poverty reduction, socio-economic development, and governance principles. \n\nWith respect to water issues, NBS can achieve the following, according to the World Water Development Report 2018 by UN-Water: \n\nIn this sense, NBS are strongly connected to ideas such as natural systems agriculture, natural solutions, ecosystem-based approaches, adaptation services, natural infrastructure, green infrastructure and ecological engineering. For instance, ecosystem-based approaches are increasingly promoted for climate change adaptation and mitigation by organisations like United Nations Environment Programme and non-governmental organisations such as The Nature Conservancy. These organisations refer to \"policies and measures that take into account the role of ecosystem services in reducing the vulnerability of society to climate change, in a multi-sectoral and multi-scale approach\".\n\nLikewise, natural infrastructure is defined as a \"strategically planned and managed network of natural lands, such as forests and wetlands, working landscapes, and other open spaces that conserves or enhances ecosystem values and functions and provides associated benefits to human populations\"; and green infrastructure refers to an \"interconnected network of green spaces that conserves natural systems and provides assorted benefits to human populations\".\n\nSimilarly, the concept of ecological engineering generally refers to \"protecting, restoring (i.e. ecosystem restoration) or modifying ecological systems to increase the quantity, quality and sustainability of particular services they provide, or to build new ecological systems that provide services that would otherwise be provided through more conventional engineering, based on non-renewable resources\".\n\nThe International Union for the Conservation of Nature (IUCN) defines NBS as actions to protect, sustainably manage, and restore natural or modified ecosystems, that address societal challenges effectively and adaptively, simultaneously providing human well-being and biodiversity benefits, with climate change, food security, disaster risks, water security, social and economic development as well as human health being the common societal challenges.\n\nIUCN proposes to consider NBS as an umbrella concept. Categories and examples of NBS approaches according to IUCN include:\n\nThe general objective of NBS is clear, namely the sustainable management and use of nature for tackling societal challenges. However, different stakeholders view NBS from other perspectives. For instance, IUCN defines NBS as \"actions to protect, sustainably manage and restore natural or modified ecosystems, which address societal challenges effectively and adaptively, while simultaneously providing human well-being and biodiversity benefits\". This framing puts the need for well-managed and restored ecosystems at the heart of NBS, with the overarching goal of \"Supporting the achievement of society's development goals and safeguard human well-being in ways that reflect cultural and societal values and enhance the resilience of ecosystems, their capacity for renewal and the provision of services\". \n\nIn the context of the ongoing political debate on jobs and growth (main drivers of the current EU policy agenda), the European Commission underlines that NBS can transform environmental and societal challenges into innovation opportunities, by turning natural capital into a source for green growth and sustainable development. In their view, NBS to societal challenges are \"solutions that are inspired and supported by nature, which are cost-effective, simultaneously provide environmental, social and economic benefits and help build resilience. Such solutions bring more, and more diverse, nature and natural features and processes into cities, landscapes and seascapes, through locally adapted, resource-efficient and systemic interventions.\" \n\nThis framing is somewhat broader, and puts economy and social assets at the heart of NBS as importantly as sustaining environmental conditions. It shares similarities with the definition proposed by Maes and Jacobs (2015) defining NBS as \"any transition to a use of ES with decreased input of non-renewable natural capital and increased investment in renewable natural processes\". In their view, development and evaluation of NBS spans three basic requirements: (1) decrease of fossil fuel input per produced unit; (2) lowering of systemic trade-offs and increasing synergies between ES; and (3) increasing labor input and jobs. Here, nature is seen as a tool to inspire more systemic solutions to societal problems.\n\nWhatever definition used, promoting sustainability and the increased role of natural, self-sustained processes relying on biodiversity, are inherent to NBS. They constitute actions easily seen as positive for a wide range of stakeholders, as they bring about benefits at environmental, economic and social levels. As a consequence, the concept of NBS is gaining acceptance outside the conservation community (e.g. urban planning) and is now on its way to be mainstreamed into policies and programmes (climate change policy, law, infrastructure investment and financing mechanisms).\n\nIn 2015, the European network BiodivERsA mobilized a range of scientists, research donors and stakeholders and proposed a typology characterizing NBS along two gradients. 1. \"how much engineering of biodiversity and ecosystems is involved in NBS\", and 2. \"how many ecosystem services and stakeholder groups are targeted by a given NBS\". The typology highlights that NBS can involve very different actions on ecosystems (from protection to management and even creation of new ecosystems) and is based on the assumption that the higher the number of services and stakeholder groups targeted, the lower the capacity to maximize the delivery of each service and simultaneously fulfil the specific needs of all stakeholder groups. As such, three types of NBS are distinguished (Figure 2):\n\nType 1 NBS consists of no or minimal intervention in ecosystems, with the objectives of maintaining or improving the delivery of a range of ES both inside and outside of these conserved ecosystems. Examples include the protection of mangroves in coastal areas to limit risks associated to extreme weather conditions and provide benefits and opportunities to local populations; and the establishment of marine protected areas to conserve biodiversity within these areas while exporting biomass into fishing grounds. This type of NBS is connected to, for example, the concept of biosphere reserves which incorporates core protected areas for nature conservation and buffer zones and transition areas where people live and work in a sustainable way.\n\nType 2 NBS corresponds to management approaches that develop sustainable and multifunctional ecosystems and landscapes (extensively or intensively managed). These types improve the delivery of selected ES compared to what would be obtained with a more conventional intervention. Examples include innovative planning of agricultural landscapes to increase their multi-functionality; and approaches for enhancing tree species and genetic diversity to increase forest resilience to extreme events. This type of NBS is strongly connected to concepts like natural systems agriculture, agro-ecology, and evolutionary-orientated forestry.\n\nType 3 NBS consists of managing ecosystems in very extensive ways or even creating new ecosystems (e.g., artificial ecosystems with new assemblages of organisms for green roofs and walls to mitigate city warming and clean polluted air). Type 3 is linked to concepts like green and blue infrastructures and objectives like restoration of heavily degraded or polluted areas and greening cities.\n\nType 1 and 2 would typically fall within the IUCN NBS framework, whereas Type 2 and moreover Type 3 are often exemplified by EC for turning natural capital into a source for green growth and sustainable development.\n\nHybrid solutions exist along this gradient both in space and time. For instance, at landscape scale, mixing protected and managed areas could be needed to fulfil multi-functionality and sustainability goals. Similarly, a constructed wetland can be developed as a type 3 but, when well established, may subsequently be preserved and surveyed as a type 1.\n\nDemonstrating the benefits of nature and healthy ecosystems and showcasing the return on investment they can offer is necessary in order to increase awareness, but also to provide support and guidance on how to implement NBS. A large number of initiatives around the world already highlight the effectiveness of NBS approaches to address a wide range of societal challenges.\n\nThe following table shows examples from around the world:\n\nIn 2018, The Hindu reported that the East Kolkata wetlands, the world's largest organic sewage treatment facility had been used to clean the sewage of Kolkata in an organic manner by using algae for several decades. In use since the 1930s, the natural system was discovered by Dhrubajyoti Ghosh, an ecologist and a municipal engineer in the 1970s while working in the region. Ghosh worked for decades to protect the wetlands. It had been a practice in Kolkata, one of the five largest cities in India, for the municipal authorities to pump sewage into shallow ponds (\"bheris\"). Under the heat of the tropical sun, algae proliferated in them, converting the sewage into clean water, which in turn was used by villagers to grow paddy and vegetables. This system has been in use in the region since the 1930s and treats 750 million litres of wastewater per day, giving livelihood to 100,000 people in the vicinity. For his work, Ghosh was included in the UN Global 500 Roll of Honour in 1990 and received the Luc Hoffmann award in 2016.\n\nThere is currently no accepted basis on which a government agency, municipality or private company can systematically assess the efficiency, effectiveness and sustainability of a particular nature-based solution. However, a series of principles are proposed to guide effective and appropriate implementation, and thus to upscale NBS in practice. For example, NBS embrace and are not meant to replace nature conservation norms. Also, NBS are determined by site-specific natural and cultural contexts that include traditional, local and scientific knowledge. NBS are an integral part of the overall design of policies, and measure or actions, to address a specific challenges. Finally, NBS can be implemented alone or in an integrated manner with other solutions to societal challenges (e.g. technological and engineering solutions) and they are applied at the landscape scale.\n\nImplementing NBS requires political, economic, and scientific challenges to be tackled. First and foremost, private sector investment is needed, not to replace but to supplement traditional sources of capital such as public funding or philanthropy. The challenge is therefore to provide a robust evidence base for the contribution of nature to economic growth and jobs, and to demonstrate the economic viability of these solutions – compared to technological ones – on a timescale compatible with that of global change. Furthermore, it requires measures like adaptation of economic subsidy schemes, and the creation of opportunities for conservation finance, to name a few. Indeed, such measures will be needed to scale up NBS interventions, and strengthen their impact in mitigating the world's most pressing challenges.\n\nSince 2016, the EU is supporting a multi-stakeholder dialogue platform (called ThinkNature) to promote the co-design, testing and deployment of improved and innovative NBS in an integrated way. Creation of such science-policy-business-society interfaces could promote the market uptake of NBS. The project is part of the EU’s Horizon 2020 – Research and Innovation programme, and will last for 3 years. There are a total of 17 international partners involved, including the Technical University of Crete (Project Leader), the University of Helsinki and BiodivERsA.\n\nIn 2017, as part of the Presidency of the Estonian Republic of the Council of the European Union, a conference called “Nature-based Solutions: From Innovation to Common-use” was organized by the Ministry of the Environment of Estonia and the University of Tallinn. This conference aimed to strengthen synergies among various recent initiatives and programs related to NBS launched by the European Commission and by the EU Member States, focusing on policy and governance of NBS, and on research and innovation.\n\nIn recognition of the importance of natural ecosystems for mitigation and adaptation, the Paris Agreement calls on all Parties to acknowledge “the importance of the conservation and enhancement, as appropriate, of sinks and reservoirs of the greenhouse gases” and to “note the importance of ensuring the integrity of all ecosystems, including oceans, and the protection of biodiversity, recognized by some cultures as Mother Earth”. It then includes in its Articles several references to nature-based solutions. For example, Article 5.2 encourages Parties to adopt “…policy approaches and positive incentives for activities relating to reducing emissions from deforestation and forest degradation, and the role of conservation and sustainable management of forests and enhancement of forest carbon stocks in developing countries; and alternative policy approaches, such as joint mitigation and adaptation approaches for the integral and sustainable management of forests, while reaffirming the importance of incentivizing, as appropriate, non-carbon benefits associated with such approaches”. Article 7.1 further encourages Parties to build the resilience of socioeconomic and ecological systems, including through economic diversification and sustainable management of natural resources. In total, the Agreement refers to nature (ecosystems, natural resources, forests) in 13 distinct places. An in-depth analysis of all Nationally Determined Contributions submitted to UNFCCC, revealed that around 130 NDCs or 65% of signatories commit to nature-based solutions in their climate pledges, suggesting broad consensus for the role of nature in helping meet climate change goals. However, high-level commitments rarely translate into robust, measurable actions on-the-ground.\n\nThe term NBS was put forward by practitioners in the late 2000s (in particular the International Union for the Conservation of Nature and the World Bank) and thereafter by policymakers in Europe (most notably the European Commission). \n\nThe term \"nature-based solutions\" was first used in the late 2000s. It was used in the context of finding new solutions to mitigate and adapt to climate change effects, whilst simultaneously protecting biodiversity and improving sustainable livelihoods. \n\nThe IUCN referred to NBS in a position paper for the United Nations Framework Convention on Climate Change. The term was also adopted by European policymakers, in particular by the European Commission in a report stressing that NBS can offer innovative means to create jobs and growth as part of a green economy. The term started to make appearances in the mainstream media around the time of the Global Climate Action Summit in California in September 2018 \n\n\n"}
{"id": "208999", "url": "https://en.wikipedia.org/wiki?curid=208999", "title": "Non-renewable resource", "text": "Non-renewable resource\n\nA non-renewable resource (also called a finite resource) is a resource that does not renew itself at a sufficient rate for sustainable economic extraction in meaningful human time-frames. An example is carbon-based, organically-derived fuel. The original organic material, with the aid of heat and pressure, becomes a fuel such as oil or gas. Earth minerals and metal ores, fossil fuels (coal, petroleum, natural gas) and groundwater in certain aquifers are all considered non-renewable resources, though individual elements are almost always conserved.\n\nOn the other hand, resources such as timber (when harvested sustainably) and wind (used to power energy conversion systems) are considered renewable resources, largely because their localized replenishment can occur within time frames meaningful to humans.\n\nEarth minerals and metal ores are examples of non-renewable resources. The metals themselves are present in vast amounts in Earth's crust, and their extraction by humans only occurs where they are concentrated by natural geological processes (such as heat, pressure, organic activity, weathering and other processes) enough to become economically viable to extract. These processes generally take from tens of thousands to millions of years, through plate tectonics, tectonic subsidence and crustal recycling.\n\nThe localized deposits of metal ores near the surface which can be extracted economically by humans are non-renewable in human time-frames. There are certain rare earth minerals and elements that are more scarce and exhaustible than others. These are in high demand in manufacturing, particularly for the electronics industry.\n\nMost metal ores are considered vastly greater in supply to fossil fuels, because metal ores are formed by crustal-scale processes which make up a much larger portion of the Earth's near-surface environment, than those that form fossil fuels which are limited to areas where carbon-based life forms flourish, die, and are quickly buried.\n\nNatural resources such as coal, petroleum (crude oil) and natural gas take thousands of years to form naturally and cannot be replaced as fast as they are being consumed. Eventually it is considered that fossil-based resources will become too costly to harvest and humanity will need to shift its reliance to other sources of energy such as solar or wind power, see renewable energy.\n\nAn alternative hypothesis is that carbon based fuel is virtually inexhaustible in human terms, if one includes all sources of carbon-based energy such as methane hydrates on the sea floor, which are vastly greater than all other carbon based fossil fuel resources combined. These sources of carbon are also considered non-renewable, although their rate of formation/replenishment on the sea floor is not known. However their extraction at economically viable costs and rates has yet to be determined.\n\nAt present, the main energy source used by humans is non-renewable fossil fuels. Since the dawn of internal combustion engine technologies in the 19th century, petroleum and other fossil fuels have remained in continual demand. As a result, conventional infrastructure and transport systems, which are fitted to combustion engines, remain prominent throughout the globe. The continual use of fossil fuels at the current rate is believed to increase global warming and cause more severe climate change.\n\nIn 1987, the World Commission on Environment and Development (WCED) an organization set up by but independent from the United Nations classified fission reactors that produce more fissile nuclear fuel than they consume -i.e. breeder reactors, and when it is developed, fusion power, among conventional renewable energy sources, such as solar and falling water. The American Petroleum Institute likewise does not consider conventional nuclear fission as renewable, but that breeder reactor nuclear power fuel is considered renewable and sustainable, before explaining that radioactive waste from used spent fuel rods remains radioactive, and so has to be very carefully stored for up to a thousand years. With the careful monitoring of radioactive waste products also being required upon the use of other renewable energy sources, such as geothermal energy.\n\nThe use of nuclear technology relying on fission requires Naturally occurring radioactive material as fuel. Uranium, the most common fission fuel, and is present in the ground at relatively low concentrations and mined in 19 countries. This mined uranium is used to fuel energy-generating nuclear reactors with fissionable uranium-235 which generates heat that is ultimately used to power turbines to generate electricity.\n\nAs of 2013 only a few kilograms (picture available) of uranium have been extracted from the ocean in pilot programs and it is also believed that the uranium extracted on an industrial scale from the seawater would constantly be replenished from uranium leached from the ocean floor, maintaining the seawater concentration at a stable level. In 2014, with the advances made in the efficiency of seawater uranium extraction, a paper in the journal of \"Marine Science & Engineering\" suggests that with, light water reactors as its target, the process would be economically competitive if implemented on a large scale.\n\nNuclear power provides about 6% of the world's energy and 13–14% of the world's electricity. Nuclear energy production is associated with potentially dangerous radioactive contamination as it relies upon unstable elements. In particular, nuclear power facilities produce about 200,000 metric tons of low and intermediate level waste (LILW) and 10,000 metric tons of high level waste (HLW) (including spent fuel designated as waste) each year worldwide.\n\nIssues entirely separate from the question of the sustainability of nuclear fuel, relate to the use of nuclear fuel and the high-level radioactive waste the nuclear industry generates that if not properly contained, is highly hazardous to people and wildlife. The United Nations (UNSCEAR) estimated in 2008 that average annual human radiation exposure includes 0.01 millisievert (mSv) from the legacy of past atmospheric nuclear testing plus the Chernobyl disaster and the nuclear fuel cycle, along with 2.0 mSv from natural radioisotopes and 0.4 mSv from cosmic rays; all exposures vary by location. natural uranium in some inefficient reactor nuclear fuel cycles, becomes part of the nuclear waste \"once through\" stream, and in a similar manner to the scenario were this uranium remained naturally in the ground, this uranium emits various forms of radiation in a decay chain that has a half-life of about 4.5 billion years, the storage of this unused uranium and the accompanying fission reaction products have raised public concerns about risks of leaks and containment, however the knowledge gained from studying the Natural nuclear fission reactor in Oklo Gabon, has informed geologists on the proven processes that kept the waste from this 2 billion year old natural nuclear reactor that operated for hundreds of thousands of years, from negatively impacting the surrounding plant and animal life.\n\nNatural resources, known as renewable resources, are replaced by natural processes and forces persistent in the natural environment. There are intermittent and reoccurring renewables, and recyclable materials, which are utilized during a cycle across a certain amount of time, and can be harnessed for any number of cycles.\n\nThe production of goods and services by manufacturing products in economic systems creates many types of waste during production and after the consumer has made use of it. The material is then either incinerated, buried in a landfill or recycled for reuse. Recycling turns materials of value that would otherwise become waste into valuable resources again.\nThe natural environment, with soil, water, forests, plants and animals are all renewable resources, as long as they are adequately monitored, protected and conserved. Sustainable agriculture is the cultivation of plant and animal materials in a manner that preserves plant and animal ecosystems over the long term. The overfishing of the oceans is one example of where an industry practice or method can threaten an ecosystem, endanger species and possibly even determine whether or not a fishery is sustainable for use by humans. An unregulated industry practice or method can lead to a complete resource depletion.\nThe renewable energy from the sun, wind, wave, biomass and geothermal energies are based on renewable resources. Renewable resources such as the movement of water (hydropower, tidal power and wave power), wind and radiant energy from geothermal heat (used for geothermal power) and solar energy (used for solar power) are practically infinite and cannot be depleted, unlike their non-renewable counterparts, which are likely to run out if not used sparingly.\n\nThe potential wave energy on coastlines can provide 1/5 of world demand. Hydroelectric power can supply 1/3 of our total energy global needs. Geothermal energy can provide 1.5 more times the energy we need. There is enough wind to power the planet 30 times over, wind power could power all of humanity's needs alone. Solar currently supplies only 0.1% of our world energy needs, but there is enough out there to power humanity's needs 4,000 times over, the entire global projected energy demand by 2050.\n\nRenewable energy and energy efficiency are no longer niche sectors that are promoted only by governments and environmentalists. The increasing levels of investment and that more of the capital is from conventional financial actors, both suggest that sustainable energy has become mainstream and the future of energy production, as non-renewable resources decline. This is reinforced by climate change concerns, nuclear dangers and accumulating radioactive waste, high oil prices, peak oil and increasing government support for renewable energy. These factors are commercializing renewable energy, enlarging the market and growing demand, the adoption of new products to replace obsolete technology and the conversion of existing infrastructure to a renewable standard.\n\nIn economics, a non-renewable resource is defined as goods, where greater consumption today implies less consumption tomorrow. David Ricardo in his early works analysed the pricing of exhaustible resources, where he argued that the price of a mineral resource should increase over time. He argued that the spot price is always determined by the mine with the highest cost of extraction, and mine owners with lower extraction costs benefit from a differential rent. The first model is defined by Hotelling's rule, which is a 1931 economic model of non-renewable resource management by Harold Hotelling. It shows that efficient exploitation of a nonrenewable and nonaugmentable resource would, under otherwise stable conditions, lead to a depletion of the resource. The rule states that this would lead to a net price or \"Hotelling rent\" for it that rose annually at a rate equal to the rate of interest, reflecting the increasing scarcity of the resources.\nThe Hartwick's rule provides an important result about the sustainability of welfare in an economy that uses non-renewable source.\n\n\n"}
{"id": "177602", "url": "https://en.wikipedia.org/wiki?curid=177602", "title": "Outer space", "text": "Outer space\n\nOuter space, or just space, is the expanse that exists beyond the Earth and between celestial bodies. Outer space is not completely empty—it is a hard vacuum containing a low density of particles, predominantly a plasma of hydrogen and helium as well as electromagnetic radiation, magnetic fields, neutrinos, dust, and cosmic rays. The baseline temperature, as set by the background radiation from the Big Bang, is . The plasma between galaxies accounts for about half of the baryonic (ordinary) matter in the universe; it has a number density of less than one hydrogen atom per cubic metre and a temperature of millions of kelvins; local concentrations of this plasma have condensed into stars and galaxies. Studies indicate that 90% of the mass in most galaxies is in an unknown form, called dark matter, which interacts with other matter through gravitational but not electromagnetic forces. Observations suggest that the majority of the mass-energy in the observable universe is a poorly understood vacuum energy of space, which astronomers label \"dark energy\". Intergalactic space takes up most of the volume of the Universe, but even galaxies and star systems consist almost entirely of empty space.\n\nOuter space does not begin at a definite altitude above the Earth's surface. However, the Kármán line, an altitude of above sea level, is conventionally used as the start of outer space in space treaties and for aerospace records keeping. The framework for international space law was established by the Outer Space Treaty, which entered into force on 10 October 1967. This treaty precludes any claims of national sovereignty and permits all states to freely explore outer space. Despite the drafting of UN resolutions for the peaceful uses of outer space, anti-satellite weapons have been tested in Earth orbit.\n\nHumans began the physical exploration of space during the 20th century with the advent of high-altitude balloon flights, followed by manned rocket launches. Earth orbit was first achieved by Yuri Gagarin of the Soviet Union in 1961, and unmanned spacecraft have since reached all of the known planets in the Solar System. Due to the high cost of getting into space, manned spaceflight has been limited to low Earth orbit and the Moon.\n\nOuter space represents a challenging environment for human exploration because of the hazards of vacuum and radiation. Microgravity also has a negative effect on human physiology that causes both muscle atrophy and bone loss. In addition to these health and environmental issues, the economic cost of putting objects, including humans, into space is very high.\n\nIn 350 BCE, Greek philosopher Aristotle suggested that \"nature abhors a vacuum\", a principle that became known as the \"horror vacui\". This concept built upon a 5th-century BCE ontological argument by the Greek philosopher Parmenides, who denied the possible existence of a void in space. Based on this idea that a vacuum could not exist, in the West it was widely held for many centuries that space could not be empty. As late as the 17th century, the French philosopher René Descartes argued that the entirety of space must be filled.\n\nIn ancient China, the 2nd-century astronomer Zhang Heng became convinced that space must be infinite, extending well beyond the mechanism that supported the Sun and the stars. The surviving books of the Hsüan Yeh school said that the heavens were boundless, \"empty and void of substance\". Likewise, the \"sun, moon, and the company of stars float in the empty space, moving or standing still\".\n\nThe Italian scientist Galileo Galilei knew that air had mass and so was subject to gravity. In 1640, he demonstrated that an established force resisted the formation of a vacuum. However, it would remain for his pupil Evangelista Torricelli to create an apparatus that would produce a partial vacuum in 1643. This experiment resulted in the first mercury barometer and created a scientific sensation in Europe. The French mathematician Blaise Pascal reasoned that if the column of mercury was supported by air, then the column ought to be shorter at higher altitude where the air pressure is lower. In 1648, his brother-in-law, Florin Périer, repeated the experiment on the Puy de Dôme mountain in central France and found that the column was shorter by three inches. This decrease in pressure was further demonstrated by carrying a half-full balloon up a mountain and watching it gradually expand, then contract upon descent.\n\nIn 1650, German scientist Otto von Guericke constructed the first vacuum pump: a device that would further refute the principle of \"horror vacui\". He correctly noted that the atmosphere of the Earth surrounds the planet like a shell, with the density gradually declining with altitude. He concluded that there must be a vacuum between the Earth and the Moon.\n\nBack in the 15th century, German theologian Nicolaus Cusanus speculated that the Universe lacked a center and a circumference. He believed that the Universe, while not infinite, could not be held as finite as it lacked any bounds within which it could be contained. These ideas led to speculations as to the infinite dimension of space by the Italian philosopher Giordano Bruno in the 16th century. He extended the Copernican heliocentric cosmology to the concept of an infinite Universe filled with a substance he called aether, which did not resist the motion of heavenly bodies. English philosopher William Gilbert arrived at a similar conclusion, arguing that the stars are visible to us only because they are surrounded by a thin aether or a void. This concept of an aether originated with ancient Greek philosophers, including Aristotle, who conceived of it as the medium through which the heavenly bodies move.\n\nThe concept of a Universe filled with a luminiferous aether remained in vogue among some scientists until the early 20th century. This form of aether was viewed as the medium through which light could propagate. In 1887, the Michelson–Morley experiment tried to detect the Earth's motion through this medium by looking for changes in the speed of light depending on the direction of the planet's motion. However, the null result indicated something was wrong with the concept. The idea of the luminiferous aether was then abandoned. It was replaced by Albert Einstein's theory of special relativity, which holds that the speed of light in a vacuum is a fixed constant, independent of the observer's motion or frame of reference.\n\nThe first professional astronomer to support the concept of an infinite Universe was the Englishman Thomas Digges in 1576. But the scale of the Universe remained unknown until the first successful measurement of the distance to a nearby star in 1838 by the German astronomer Friedrich Bessel. He showed that the star 61 Cygni had a parallax of just 0.31 arcseconds (compared to the modern value of 0.287″). This corresponds to a distance of over 10 light years. In 1917, Heber Curtis noted that novae in spiral nebulae were, on average, 10 magnitudes fainter than galactic novae, suggesting that the former are 100 times further away. The distance to the Andromeda Galaxy was determined in 1923 by American astronomer Edwin Hubble by measuring the brightness of cepheid variables in that galaxy, a new technique discovered by Henrietta Leavitt. This established that the Andromeda galaxy, and by extension all galaxies, lay well outside the Milky Way.\n\nThe earliest known estimate of the temperature of outer space was by the Swiss physicist Charles É. Guillaume in 1896. Using the estimated radiation of the background stars, he concluded that space must be heated to a temperature of 5–6 K. British physicist Arthur Eddington made a similar calculation to derive a temperature of 3.18 K in 1926. German physicist Erich Regener used the total measured energy of cosmic rays to estimate an intergalactic temperature of 2.8 K in 1933.\n\nThe modern concept of outer space is based on the \"Big Bang\" cosmology, first proposed in 1931 by the Belgian physicist Georges Lemaître. This theory holds that the universe originated from a very dense form that has since undergone continuous expansion. The background energy released during the initial expansion has steadily decreased in density, leading to a 1948 prediction by American physicists Ralph Alpher and Robert Herman of a temperature of 5 K for the temperature of space.\n\nThe term \"outward space\" was used in 1842 by the English poet Lady Emmeline Stuart-Wortley in her poem \"The Maiden of Moscow\". The expression \"outer space\" was used as an astronomical term by Alexander von Humboldt in 1845. It was later popularized in the writings of H. G. Wells in 1901. The shorter term \"space\" is older, first used to mean the region beyond Earth's sky in John Milton's \"Paradise Lost\" in 1667.\n\nAccording to the Big Bang theory, the very early Universe was an extremely hot and dense state about 13.8 billion years ago which rapidly expanded. About 380,000 years later the Universe had cooled sufficiently to allow protons and electrons to combine and form hydrogen—the so-called recombination epoch. When this happened, matter and energy became decoupled, allowing photons to travel freely through the continually expanding space. Matter that remained following the initial expansion has since undergone gravitational collapse to create stars, galaxies and other astronomical objects, leaving behind a deep vacuum that forms what is now called outer space. As light has a finite velocity, this theory also constrains the size of the directly observable universe. This leaves open the question as to whether the Universe is finite or infinite.\n\nThe present day shape of the universe has been determined from measurements of the cosmic microwave background using satellites like the Wilkinson Microwave Anisotropy Probe. These observations indicate that the spatial geometry of the observable universe is \"flat\", meaning that photons on parallel paths at one point remain parallel as they travel through space to the limit of the observable universe, except for local gravity. The flat Universe, combined with the measured mass density of the Universe and the accelerating expansion of the Universe, indicates that space has a non-zero vacuum energy, which is called dark energy.\n\nEstimates put the average energy density of the present day Universe at the equivalent of 5.9 protons per cubic meter, including dark energy, dark matter, and baryonic matter (ordinary matter composed of atoms). The atoms account for only 4.6% of the total energy density, or a density of one proton per four cubic meters. The density of the Universe, however, is clearly not uniform; it ranges from relatively high density in galaxies—including very high density in structures within galaxies, such as planets, stars, and black holes—to conditions in vast voids that have much lower density, at least in terms of visible matter. Unlike matter and dark matter, dark energy seems not to be concentrated in galaxies: although dark energy may account for a majority of the mass-energy in the Universe, dark energy's influence is 5 orders of magnitude smaller than the influence of gravity from matter and dark matter within the Milky Way.\n\nOuter space is the closest known approximation to a perfect vacuum. It has effectively no friction, allowing stars, planets, and moons to move freely along their ideal orbits, following the initial formation stage. However, even the deep vacuum of intergalactic space is not devoid of matter, as it contains a few hydrogen atoms per cubic meter. By comparison, the air humans breathe contains about 10 molecules per cubic meter. The low density of matter in outer space means that electromagnetic radiation can travel great distances without being scattered: the mean free path of a photon in intergalactic space is about 10 km, or 10 billion light years. In spite of this, extinction, which is the absorption and scattering of photons by dust and gas, is an important factor in galactic and intergalactic astronomy.\n\nStars, planets, and moons retain their atmospheres by gravitational attraction. Atmospheres have no clearly delineated upper boundary: the density of atmospheric gas gradually decreases with distance from the object until it becomes indistinguishable from outer space. The Earth's atmospheric pressure drops to about Pa at of altitude, compared to 100,000 Pa for the International Union of Pure and Applied Chemistry (IUPAC) definition of standard pressure. Above this altitude, isotropic gas pressure rapidly becomes insignificant when compared to radiation pressure from the Sun and the dynamic pressure of the solar wind. The thermosphere in this range has large gradients of pressure, temperature and composition, and varies greatly due to space weather.\n\nThe temperature of outer space is measured in terms of the kinetic activity of the gas, as it is on Earth. However, the radiation of outer space has a different temperature than the kinetic temperature of the gas, meaning that the gas and radiation are not in thermodynamic equilibrium. All of the observable universe is filled with photons that were created during the Big Bang, which is known as the cosmic microwave background radiation (CMB). (There is quite likely a correspondingly large number of neutrinos called the cosmic neutrino background.) The current black body temperature of the background radiation is about . The gas temperatures in outer space are always at least the temperature of the CMB but can be much higher. For example, the corona of the Sun reaches temperatures over 1.2–2.6 million K.\n\nMagnetic fields have been detected in the space around just about every class of celestial object. Star formation in spiral galaxies can generate small-scale dynamos, creating turbulent magnetic field strengths of around 5–10 μG. The Davis–Greenstein effect causes elongated dust grains to align themselves with a galaxy's magnetic field, resulting in weak optical polarization. This has been used to show ordered magnetic fields exist in several nearby galaxies. Magneto-hydrodynamic processes in active elliptical galaxies produce their characteristic jets and radio lobes. Non-thermal radio sources have been detected even among the most distant, high-z sources, indicating the presence of magnetic fields.\n\nOutside a protective atmosphere and magnetic field, there are few obstacles to the passage through space of energetic subatomic particles known as cosmic rays. These particles have energies ranging from about 10 eV up to an extreme 10 eV of ultra-high-energy cosmic rays. The peak flux of cosmic rays occurs at energies of about 10 eV, with approximately 87% protons, 12% helium nuclei and 1% heavier nuclei. In the high energy range, the flux of electrons is only about 1% of that of protons. Cosmic rays can damage electronic components and pose a health threat to space travelers. According to astronauts, like Don Pettit, space has a burned/metallic odor that clings to their suits and equipment, similar to the scent of an arc welding torch.\n\nDespite the harsh environment, several life forms have been found that can withstand extreme space conditions for extended periods. Species of lichen carried on the ESA BIOPAN facility survived exposure for ten days in 2007. Seeds of \"Arabidopsis thaliana\" and \"Nicotiana tabacum\" germinated after being exposed to space for 1.5 years. A strain of \"bacillus subtilis\" has survived 559 days when exposed to low-Earth orbit or a simulated martian environment. The lithopanspermia hypothesis suggests that rocks ejected into outer space from life-harboring planets may successfully transport life forms to another habitable world. A conjecture is that just such a scenario occurred early in the history of the Solar System, with potentially microorganism-bearing rocks being exchanged between Venus, Earth, and Mars.\n\nEven at relatively low altitudes in the Earth's atmosphere, conditions are hostile to the human body. The altitude where atmospheric pressure matches the vapor pressure of water at the temperature of the human body is called the Armstrong line, named after American physician Harry G. Armstrong. It is located at an altitude of around . At or above the Armstrong line, fluids in the throat and lungs boil away. More specifically, exposed bodily liquids such as saliva, tears, and liquids in the lungs boil away. Hence, at this altitude, human survival requires a pressure suit, or a pressurized capsule.\n\nOnce in space, sudden exposure of unprotected humans to very low pressure, such as during a rapid decompression, can cause pulmonary barotrauma—a rupture of the lungs, due to the large pressure differential between inside and outside the chest. Even if the subject's airway is fully open, the flow of air through the windpipe may be too slow to prevent the rupture. Rapid decompression can rupture eardrums and sinuses, bruising and blood seep can occur in soft tissues, and shock can cause an increase in oxygen consumption that leads to hypoxia.\n\nAs a consequence of rapid decompression, oxygen dissolved in the blood empties into the lungs to try to equalize the partial pressure gradient. Once the deoxygenated blood arrives at the brain, humans lose consciousness after a few seconds and die of hypoxia within minutes. Blood and other body fluids boil when the pressure drops below 6.3 kPa, and this condition is called ebullism. The steam may bloat the body to twice its normal size and slow circulation, but tissues are elastic and porous enough to prevent rupture. Ebullism is slowed by the pressure containment of blood vessels, so some blood remains liquid. Swelling and ebullism can be reduced by containment in a pressure suit. The Crew Altitude Protection Suit (CAPS), a fitted elastic garment designed in the 1960s for astronauts, prevents ebullism at pressures as low as 2 kPa. Supplemental oxygen is needed at to provide enough oxygen for breathing and to prevent water loss, while above pressure suits are essential to prevent ebullism. Most space suits use around 30–39 kPa of pure oxygen, about the same as on the Earth's surface. This pressure is high enough to prevent ebullism, but evaporation of nitrogen dissolved in the blood could still cause decompression sickness and gas embolisms if not managed.\n\nHumans evolved for life in Earth gravity, and exposure to weightlessness has been shown to have deleterious effects on human health. Initially, more than 50% of astronauts experience space motion sickness. This can cause nausea and vomiting, vertigo, headaches, lethargy, and overall malaise. The duration of space sickness varies, but it typically lasts for 1–3 days, after which the body adjusts to the new environment. Longer-term exposure to weightlessness results in muscle atrophy and deterioration of the skeleton, or spaceflight osteopenia. These effects can be minimized through a regimen of exercise. Other effects include fluid redistribution, slowing of the cardiovascular system, decreased production of red blood cells, balance disorders, and a weakening of the immune system. Lesser symptoms include loss of body mass, nasal congestion, sleep disturbance, and puffiness of the face.\n\nFor long-duration space travel, radiation can pose an acute health hazard.\nExposure to high-energy, ionizing cosmic rays can result in fatigue, nausea, vomiting, as well as damage to the immune system and changes to the white blood cell count. Over longer durations, symptoms include an increased risk of cancer, plus damage to the eyes, nervous system, lungs and the gastrointestinal tract. On a round-trip Mars mission lasting three years, a large fraction of the cells in an astronaut's body would be traversed and potentially damaged by high energy nuclei. The energy of such particles is significantly diminished by the shielding provided by the walls of a spacecraft and can be further diminished by water containers and other barriers. However, the impact of the cosmic rays upon the shielding produces additional radiation that can affect the crew. Further research is needed to assess the radiation hazards and determine suitable countermeasures.\n\nThere is no clear boundary between Earth's atmosphere and space, as the density of the atmosphere gradually decreases as the altitude increases. There are several standard boundary designations, namely:\n\nIn 2009, scientists reported detailed measurements with a Supra-Thermal Ion Imager (an instrument that measures the direction and speed of ions), which allowed them to establish a boundary at above Earth. The boundary represents the midpoint of a gradual transition over tens of kilometers from the relatively gentle winds of the Earth's atmosphere to the more violent flows of charged particles in space, which can reach speeds well over .\n\nThe Outer Space Treaty provides the basic framework for international space law. It covers the legal use of outer space by nation states, and includes in its definition of \"outer space\" the Moon and other celestial bodies. The treaty states that outer space is free for all nation states to explore and is not subject to claims of national sovereignty. It also prohibits the deployment of nuclear weapons in outer space. The treaty was passed by the United Nations General Assembly in 1963 and signed in 1967 by the USSR, the United States of America and the United Kingdom. As of 2017, 105 state parties have either ratified or acceded to the treaty. An additional 25 states signed the treaty, without ratifying it..\n\nSince 1958, outer space has been the subject of multiple United Nations resolutions. Of these, more than 50 have been concerning the international co-operation in the peaceful uses of outer space and preventing an arms race in space. Four additional space law treaties have been negotiated and drafted by the UN's Committee on the Peaceful Uses of Outer Space. Still, there remains no legal prohibition against deploying conventional weapons in space, and anti-satellite weapons have been successfully tested by the US, USSR and China. The 1979 Moon Treaty turned the jurisdiction of all heavenly bodies (including the orbits around such bodies) over to the international community. However, this treaty has not been ratified by any nation that currently practices manned spaceflight.\n\nIn 1976, eight equatorial states (Ecuador, Colombia, Brazil, Congo, Zaire, Uganda, Kenya, and Indonesia) met in Bogotá, Colombia. With their \"Declaration of the First Meeting of Equatorial Countries\", or \"the Bogotá Declaration\", they claimed control of the segment of the geosynchronous orbital path corresponding to each country. These claims are not internationally accepted.\n\nA spacecraft enters orbit when its centripetal acceleration due to gravity is less than or equal to the centrifugal acceleration due to the horizontal component of its velocity. For a low Earth orbit, this velocity is about ; by contrast, the fastest manned airplane speed ever achieved (excluding speeds achieved by deorbiting spacecraft) was in 1967 by the North American X-15.\n\nTo achieve an orbit, a spacecraft must travel faster than a sub-orbital spaceflight. The energy required to reach Earth orbital velocity at an altitude of is about 36 MJ/kg, which is six times the energy needed merely to climb to the corresponding altitude. Spacecraft with a perigee below about are subject to drag from the Earth's atmosphere, which decreases the orbital altitude. The rate of orbital decay depends on the satellite's cross-sectional area and mass, as well as variations in the air density of the upper atmosphere. Below about , decay becomes more rapid with lifetimes measured in days. Once a satellite descends to , it has only hours before it vaporizes in the atmosphere. The escape velocity required to pull free of Earth's gravitational field altogether and move into interplanetary space is about .\n\nSpace is a partial vacuum: its different regions are defined by the various atmospheres and \"winds\" that dominate within them, and extend to the point at which those winds give way to those beyond. Geospace extends from Earth's atmosphere to the outer reaches of Earth's magnetic field, whereupon it gives way to the solar wind of interplanetary space. Interplanetary space extends to the heliopause, whereupon the solar wind gives way to the winds of the interstellar medium. Interstellar space then continues to the edges of the galaxy, where it fades into the intergalactic void.\n\nGeospace is the region of outer space near Earth, including the upper atmosphere and magnetosphere. The Van Allen radiation belts lie within the geospace. The outer boundary of geospace is the magnetopause, which forms an interface between the Earth's magnetosphere and the solar wind. The inner boundary is the ionosphere. The variable space-weather conditions of geospace are affected by the behavior of the Sun and the solar wind; the subject of geospace is interlinked with heliophysics -- the study of the Sun and its impact on the planets of the Solar System.\n\nThe day-side magnetopause is compressed by solar-wind pressure -- the subsolar distance from the center of the Earth is typically 10 Earth radii. On the night side, the solar wind stretches the magnetosphere to form a magnetotail that sometimes extends out to more than 100–200 Earth radii. For roughly four days of each month, the lunar surface is shielded from the solar wind as the Moon passes through the magnetotail.\n\nGeospace is populated by electrically charged particles at very low densities, the motions of which are controlled by the Earth's magnetic field. These plasmas form a medium from which storm-like disturbances powered by the solar wind can drive electrical currents into the Earth's upper atmosphere. Geomagnetic storms can disturb two regions of geospace, the radiation belts and the ionosphere. These storms increase fluxes of energetic electrons that can permanently damage satellite electronics, interfering with shortwave radio communication and GPS location and timing. Magnetic storms can also be a hazard to astronauts, even in low Earth orbit. They also create aurorae seen at high latitudes in an oval surrounding the geomagnetic poles.\n\nAlthough it meets the definition of outer space, the atmospheric density within the first few hundred kilometers above the Kármán line is still sufficient to produce significant drag on satellites. This region contains material left over from previous manned and unmanned launches that are a potential hazard to spacecraft. Some of this debris re-enters Earth's atmosphere periodically.\n\nEarth's gravity keeps the Moon in orbit at an average distance of . The region outside Earth's atmosphere and extending out to just beyond the Moon's orbit, including the Lagrangian points, is sometimes referred to as cislunar space.\n\nThe region of space where Earth's gravity remains dominant against gravitational perturbations from the Sun is called the Hill sphere. This extends well out into translunar space to a distance of roughly 1% of the mean distance from Earth to the Sun, or .\n\nDeep space has different definitions as to where it starts. It has been defined by the United States government and others as any region beyond cislunar space. The International Telecommunication Union responsible for radio communication (including satellites) defines the beginning of deep space at about 5 times that distance ().\n\nInterplanetary space is defined by the solar wind, a continuous stream of charged particles emanating from the Sun that creates a very tenuous atmosphere (the heliosphere) for billions of kilometers into space. This wind has a particle density of 5–10 protons/cm and is moving at a velocity of . Interplanetary space extends out to the heliopause where the influence of the galactic environment starts to dominate over the magnetic field and particle flux from the Sun. The distance and strength of the heliopause varies depending on the activity level of the solar wind.\n\nThe volume of interplanetary space is a nearly total vacuum, with a mean free path of about one astronomical unit at the orbital distance of the Earth. However, this space is not completely empty, and is sparsely filled with cosmic rays, which include ionized atomic nuclei and various subatomic particles. There is also gas, plasma and dust, small meteors, and several dozen types of organic molecules discovered to date by microwave spectroscopy. A cloud of interplanetary dust is visible at night as a faint band called the zodiacal light.\n\nInterplanetary space contains the magnetic field generated by the Sun. There are also magnetospheres generated by planets such as Jupiter, Saturn, Mercury and the Earth that have their own magnetic fields. These are shaped by the influence of the solar wind into the approximation of a teardrop shape, with the long tail extending outward behind the planet. These magnetic fields can trap particles from the solar wind and other sources, creating belts of charged particles such as the Van Allen radiation belts. Planets without magnetic fields, such as Mars, have their atmospheres gradually eroded by the solar wind.\n\nInterstellar space is the physical space within a galaxy beyond the influence each star has upon the encompassed plasma. The contents of interstellar space are called the interstellar medium. Approximately 70% of the mass of the interstellar medium consists of lone hydrogen atoms; most of the remainder consists of helium atoms. This is enriched with trace amounts of heavier atoms formed through stellar nucleosynthesis. These atoms are ejected into the interstellar medium by stellar winds or when evolved stars begin to shed their outer envelopes such as during the formation of a planetary nebula. The cataclysmic explosion of a supernova generates an expanding shock wave consisting of ejected materials that further enrich the medium. The density of matter in the interstellar medium can vary considerably: the average is around 10 particles per m, but cold molecular clouds can hold 10–10 per m.\n\nA number of molecules exist in interstellar space, as can tiny 0.1 μm dust particles. The tally of molecules discovered through radio astronomy is steadily increasing at the rate of about four new species per year. Large regions of higher density matter known as molecular clouds allow chemical reactions to occur, including the formation of organic polyatomic species. Much of this chemistry is driven by collisions. Energetic cosmic rays penetrate the cold, dense clouds and ionize hydrogen and helium, resulting, for example, in the trihydrogen cation. An ionized helium atom can then split relatively abundant carbon monoxide to produce ionized carbon, which in turn can lead to organic chemical reactions.\n\nThe local interstellar medium is a region of space within 100 parsecs (pc) of the Sun, which is of interest both for its proximity and for its interaction with the Solar System. This volume nearly coincides with a region of space known as the Local Bubble, which is characterized by a lack of dense, cold clouds. It forms a cavity in the Orion Arm of the Milky Way galaxy, with dense molecular clouds lying along the borders, such as those in the constellations of Ophiuchus and Taurus. (The actual distance to the border of this cavity varies from 60 to 250 pc or more.) This volume contains about 10–10 stars and the local interstellar gas counterbalances the astrospheres that surround these stars, with the volume of each sphere varying depending on the local density of the interstellar medium. The Local Bubble contains dozens of warm interstellar clouds with temperatures of up to 7,000 K and radii of 0.5–5 pc.\n\nWhen stars are moving at sufficiently high peculiar velocities, their astrospheres can generate bow shocks as they collide with the interstellar medium. For decades it was assumed that the Sun had a bow shock. In 2012, data from Interstellar Boundary Explorer (IBEX) and NASA's Voyager probes showed that the Sun's bow shock does not exist. Instead, these authors argue that a subsonic bow wave defines the transition from the solar wind flow to the interstellar medium. A bow shock is the third boundary of an astrosphere after the termination shock and the astropause (called the heliopause in the Solar System).\n\nIntergalactic space is the physical space between galaxies. Studies of the large scale distribution of galaxies show that the Universe has a foam-like structure, with clusters and groups of galaxies lying along filaments that occupy about a tenth of the total space. The remainder forms huge voids that are mostly empty of galaxies. Typically, a void spans a distance of (10–40) \"h\" Mpc, where \"h\" is the Hubble constant in units of .\n\nSurrounding and stretching between galaxies, there is a rarefied plasma that is organized in a galactic filamentary structure. This material is called the intergalactic medium (IGM). The density of the IGM is 5–200 times the average density of the Universe. It consists mostly of ionized hydrogen; i.e. a plasma consisting of equal numbers of electrons and protons. As gas falls into the intergalactic medium from the voids, it heats up to temperatures of 10 K to 10 K, which is high enough so that collisions between atoms have enough energy to cause the bound electrons to escape from the hydrogen nuclei; this is why the IGM is ionized. At these temperatures, it is called the warm–hot intergalactic medium (WHIM). (Although the plasma is very hot by terrestrial standards, 10 K is often called \"warm\" in astrophysics.) Computer simulations and observations indicate that up to half of the atomic matter in the Universe might exist in this warm–hot, rarefied state. When gas falls from the filamentary structures of the WHIM into the galaxy clusters at the intersections of the cosmic filaments, it can heat up even more, reaching temperatures of 10 K and above in the so-called intracluster medium.\n\nFor the majority of human history, space was explored by observations made from the Earth's surface—initially with the unaided eye and then with the telescope. Prior to the advent of reliable rocket technology, the closest that humans had come to reaching outer space was through the use of balloon flights. In 1935, the U.S. \"Explorer II\" manned balloon flight had reached an altitude of . This was greatly exceeded in 1942 when the third launch of the German A-4 rocket climbed to an altitude of about . In 1957, the unmanned satellite \"Sputnik 1\" was launched by a Russian R-7 rocket, achieving Earth orbit at an altitude of . This was followed by the first human spaceflight in 1961, when Yuri Gagarin was sent into orbit on Vostok 1. The first humans to escape low-Earth orbit were Frank Borman, Jim Lovell and William Anders in 1968 on board the U.S. Apollo 8, which achieved lunar orbit and reached a maximum distance of from the Earth.\n\nThe first spacecraft to reach escape velocity was the Soviet Luna 1, which performed a fly-by of the Moon in 1959. In 1961, Venera 1 became the first planetary probe. It revealed the presence of the solar wind and performed the first fly-by of Venus, although contact was lost before reaching Venus. The first successful planetary mission was the 1962 fly-by of Venus by Mariner 2. The first fly-by of Mars was by Mariner 4 in 1964. Since that time, unmanned spacecraft have successfully examined each of the Solar System's planets, as well their moons and many minor planets and comets. They remain a fundamental tool for the exploration of outer space, as well as observation of the Earth. In August 2012, Voyager 1 became the first man-made object to leave the Solar System and enter interstellar space.\n\nThe absence of air makes outer space an ideal location for astronomy at all wavelengths of the electromagnetic spectrum. This is evidenced by the spectacular pictures sent back by the Hubble Space Telescope, allowing light from more than 13 billion years ago—almost to the time of the Big Bang—to be observed. However, not every location in space is ideal for a telescope. The interplanetary zodiacal dust emits a diffuse near-infrared radiation that can mask the emission of faint sources such as extrasolar planets. Moving an infrared telescope out past the dust increases its effectiveness. Likewise, a site like the Daedalus crater on the far side of the Moon could shield a radio telescope from the radio frequency interference that hampers Earth-based observations.\n\nUnmanned spacecraft in Earth orbit are an essential technology of modern civilization. They allow direct monitoring of weather conditions, relay long-range communications like television, provide a means of precise navigation, and allow remote sensing of the Earth. The latter role serves a wide variety of purposes, including tracking soil moisture for agriculture, prediction of water outflow from seasonal snow packs, detection of diseases in plants and trees, and surveillance of military activities.\n\nThe deep vacuum of space could make it an attractive environment for certain industrial processes, such as those requiring ultraclean surfaces. However, like asteroid mining, space manufacturing requires significant investment with little prospect of immediate return. An important factor in the total expense is the high cost of placing mass into Earth orbit: $– per kg in inflation-adjusted dollars, according to a 2006 estimate. Proposed concepts for addressing this issue include non-rocket spacelaunch, momentum exchange tethers, and space elevators.\n\nInterstellar travel for a human crew remains at present only a theoretical possibility. The distances to the nearest stars will require new technological developments and the ability to safely sustain crews for journeys lasting several decades. For example, the Daedalus Project study, which proposed a spacecraft powered by the fusion of Deuterium and He, would require 36 years to reach the nearby Alpha Centauri system. Other proposed interstellar propulsion systems include light sails, ramjets, and beam-powered propulsion. More advanced propulsion systems could use antimatter as a fuel, potentially reaching relativistic velocities.\n\n\n"}
{"id": "1841288", "url": "https://en.wikipedia.org/wiki?curid=1841288", "title": "Outline of energy", "text": "Outline of energy\n\nThe following outline is provided as an overview of and topical guide to energy:\n\nEnergy – in physics, this is an indirectly observed quantity often understood as the ability of a physical system to do work on other physical systems. Since work is defined as a force acting through a distance (a length of space), energy is always equivalent to the ability to exert force (a pull or a push) against an object that is moving along a definite path of certain length.\n\n\nUnits of energy\n\n\nEnergy industry\n\nSee especially and for a large number of conventional energy related topics.\n\n\nHistory of energy\n\n\n\n\n\n\nEnergy economics\n\n\n\n\n\n\n"}
{"id": "331884", "url": "https://en.wikipedia.org/wiki?curid=331884", "title": "Particle horizon", "text": "Particle horizon\n\nThe particle horizon (also called the cosmological horizon, the comoving horizon (in Dodelson's text), or the cosmic light horizon) is the maximum distance from which particles could have traveled to the observer in the age of the universe. Much like the concept of a terrestrial horizon, it represents the boundary between the observable and the unobservable regions of the universe, so its distance at the present epoch defines the size of the observable universe. Due to the expansion of the universe it is not simply the age of the universe times the speed of light (approximately 13.8 billion light-years), but rather the speed of light times the conformal time. The existence, properties, and significance of a cosmological horizon depend on the particular cosmological model.\n\nIn terms of comoving distance, the particle horizon is equal to the conformal time formula_1 that has passed since the Big Bang, times the speed of light formula_2. In general, the conformal time at a certain time formula_3 is given by\n\nwhere formula_5 is the scale factor of the Friedmann–Lemaître–Robertson–Walker metric, and we have taken the Big Bang to be at formula_6. By convention, a subscript 0 indicates \"today\" so that the conformal time today formula_7. Note that the conformal time is not the age of the universe. Rather, the conformal time is the amount of time it would take a photon to travel from where we are located to the furthest observable distance provided the universe ceased expanding. As such, formula_8 is not a physically meaningful time (this much time has not yet actually passed), though, as we will see, the particle horizon with which it is associated is a conceptually meaningful distance.\n\nThe particle horizon recedes constantly as time passes and the conformal time grows. As such, the observed size of the universe always increases. Since proper distance at a given time is just comoving distance times the scale factor (with comoving distance normally defined to be equal to proper distance at the present time, so formula_9 at present), the proper distance to the particle horizon at time formula_3 is given by\n\nand for today formula_12\n\nIn this section we consider the FLRW cosmological model. In that context, the universe can be approximated as composed by non-interacting constituents, each one being a perfect fluid with density formula_14, partial pressure formula_15 and state equation formula_16, such that they add up to the total density formula_17 and total pressure formula_18. Let us now define the following functions:\n\n\nAny function with a zero subscript denote the function evaluated at the present time formula_25 (or equivalently formula_26). The last term can be taken to be formula_27 including the curvature state equation. It can be proved that the Hubble function is given by\n\nwhere formula_29. Notice that the addition ranges over all possible partial constituents and in particular there can be countably infinitely many. With this notation we have:\n\nwhere formula_31 is the largest formula_32 (possibly infinite). The evolution of the particle horizon for an expanding universe (formula_33) is:\n\nwhere formula_2 is the speed of light and can be taken to be formula_27 (natural units). Notice that the derivative is made with respect to the FLRW-time formula_3, while the functions are evaluated at the redshift formula_23 which are related as stated before. We have an analogous but slightly different result for event horizon.\n\nThe concept of a particle horizon can be used to illustrate the famous horizon problem, which is an unresolved issue associated with the Big Bang model. Extrapolating back to the time of recombination when the cosmic microwave background (CMB) was emitted, we obtain a particle horizon of about\n\nwhich corresponds to a proper size at that time of:\n\nSince we observe the CMB to be emitted essentially from our particle horizon (formula_39), our expectation is that parts of the cosmic microwave background (CMB) that are separated by about a fraction of a great circle across the sky of\n\n(an angular size of formula_40) should be out of causal contact with each other. That the entire CMB is in thermal equilibrium and approximates a blackbody so well is therefore not explained by the standard explanations about the way the expansion of the universe proceeds. The most popular resolution to this problem is cosmic inflation.\n\n"}
{"id": "3759820", "url": "https://en.wikipedia.org/wiki?curid=3759820", "title": "Physis", "text": "Physis\n\nPhysis (Greek: \"phusis\") is a Greek theological, philosophical, and scientific term usually translated into English as \"nature\".\n\nThe term is central to Greek philosophy, and as a consequence to Western philosophy as a whole.\nIn pre-Socratic usage, \"phusis\" contrasted with \"nomos\" \"law, human convention\"\nSince Aristotle, however, the \"physical\" (the subject matter of \"physics\", properly \"natural things\") has more typically been juxtaposed to the \"metaphysical\".\n\nThe word φύσις is a verbal noun based on φύω \"to grow, to appear\" (cognate with English \"to be\"). In Homeric Greek it is used quite literally, of the manner of growth of a particular species of plant. \n\nIn pre-Socratic philosophy, beginning with Heraclitus, \"phusis\" in keeping with its etymology of \"growing, becoming\" is always used in the sense of the \"natural\" \"development\", although the focus might lie either with the origin, or the process, or the end result of the process. There is some evidence that by the 6th century BC, beginning with the Ionian School, the word could also be used \nin the comprehensive sense, as referring to \"\"all\" things\", as it were \"Nature\" in the sense of \"Universe\".\n\nIn the Sophist tradition, the term stood in opposition to \"nomos\" (), \"law\" or \"custom\", in the debate on which parts of human existence are natural, and which are due to convention. \nThe contrast of \"phisis\" vs. \"nomos\" could be applied to any subject, much like the modern contrast of \"nature vs. nurture\".\n\nIn book 10 of \"Laws\", Plato criticizes those who write works \"peri phuseōs\". The criticism is that such authors tend to focus on a purely \"naturalistic\" explanation of the world, ignoring the role of \"intention\" or \"technē\", and thus becoming prone to the error of naive atheism. Plato accuses even Hesiod of this, for the reason that the gods in Hesiod \"grow\" out of primordial entities after the physical universe had been established.\n\n\"Because those who use the term mean to say that nature is the first creative power; but if the soul turns out to be the primeval element, and not fire or air, then in the truest sense and beyond other things the soul may be said to exist \"by\" nature; and this would be true if you proved that the soul is older than the body, but not otherwise.\"\n\nAristotle sought out the definition of \"physis\" to prove that there was more than one definition of \"physis\", and more than one way to interpret nature. \"Though Aristotle retains the ancient sense of \"physis\" as growth, he insists that an adequate definition of \"physis\" requires the different perspectives of the four causes (aitia): material, efficient, formal, and final.\" Aristotle believed that nature itself contained its own source of matter (material), power/motion (efficiency), form, and end (final). A unique feature about Aristotle's definition of \"physis\" was his relationship between art and nature. Aristotle said that \"physis\" (nature) is dependent on techne (art). \"The critical distinction between art and nature concerns their different efficient causes: nature is its own source of motion, whereas techne always requires a source of motion outside itself.\" What Aristotle was trying to bring to light, was that art does not contain within itself its form or source of motion. Consider the process of an acorn becoming an oak tree. This is a natural process that has its own driving force behind it. There is no external force pushing this acorn to its final state, rather it is progressively developing towards one specific end (telos).\nThough φύσις was often used in Hellenistic philosophy, it is used only 14 times in the New Testament (10 of those in the writings of Paul). Its meaning varies throughout Paul's writings. One usage refers to the established or natural order of things, as in \"Romans 2:14\" where Paul writes \"For when Gentiles, who do not have the law, by \"nature\" do what the law requires, they are a law to themselves, even though they do not have the law.\" Another use of φύσις in the sense of \"natural order\" is \"Romans 1:26\" where he writes \"the men likewise gave up \"natural\" relations with women and were consumed with passion for one another\". In \"1 Corinthians 11:14\", Paul asks \"Does not nature itself teach you that if a man wears long hair it is a disgrace for him?\"\n\nThis use of φύσις as referring to a \"natural order\" in \"Romans 1:26\" and \"1 Corinthians 11:14\" may have been influenced by Stoicism. The Greek philosophers, including Aristotle and the Stoics are credited with distinguishing between man-made laws and a natural law of universal validity, but Gerhard Kittel states that the Stoic philosophers were not able to combine the concepts of νόμος (law) and φύσις (nature) to produce the concept of \"natural law\" in the sense that was made possible by Judeo-Christian theology.\n\nAs part of the Pauline theology of salvation by grace, Paul writes in \"Ephesians 2:3\" that \"we all once lived in the passions of our flesh, carrying out the desires of the body and the mind, and were by \"nature\" children of wrath, like the rest of mankind. In the next verse he writes, \"by grace you have been saved.\" \n\nTheologians of the early Christian period differed in the usage of this term. In Antiochene circles, it connoted the humanity or divinity of Christ conceived as a concrete set of characteristics or attributes. In Alexandrine thinking, it meant a concrete individual or independent existent and approximated to hypostasis without being a synonym. While it refers to much the same thing as ousia it is more empirical and descriptive focussing on function while ousia is metaphysical and focuses more on reality. Although found in the context of the Trinitarian debate, it is chiefly important in the Christology of Cyril of Alexandria.\n\nThe Greek adjective \"phusikos\" is represented in various forms in modern English:\nAs \"physics\" \"the study of nature\", as \"physical\" (via Middle Latin \"physicalis\") referring both to physics (the study of nature, the material universe) and to the human body. The term physiology (\"physiologia\") is of 16th-century coinage (Jean Fernel). The term \"physique\", for \"the bodily constitution of a person\", is a 19th-century loan from French. \n\nIn medicine the suffix \"-physis\" occurs in such compounds as \"symphysis\", \"epiphysis\", and a few others, in the sense of \"a growth\". The physis also refers to the \"growth plate\", or site of growth at the end of long bones.\n\n"}
{"id": "169115", "url": "https://en.wikipedia.org/wiki?curid=169115", "title": "Preternatural", "text": "Preternatural\n\nThe preternatural or praeternatural is that which appears outside or beside (Latin \"\") the natural. It is \"suspended between the mundane and the miraculous\".\n\nIn theology, the term is often used to distinguish marvels or deceptive trickery, often attributed to witchcraft or demons, from the purely divine power of the genuinely supernatural to violate the laws of nature. In the early modern period the term was used by scientists to refer to abnormalities and strange phenomena of various kinds that seemed to depart from the norms of nature.\n\nMedieval theologians made a clear distinction between the natural, the preternatural and the supernatural. Thomas Aquinas argued that the supernatural consists in \"God’s unmediated actions\"; the natural is \"what happens always or most of the time\"; and the preternatural is \"what happens rarely, but nonetheless by the agency of created beings...Marvels belong, properly speaking, to the realm of the preternatural.\" Theologians, following Aquinas, argued that only God had the power to disregard the laws of nature that he has created, but that demons could manipulate the laws of nature by a form of trickery, to deceive the unwary into believing they had experienced real miracles. According to historian Lorraine Daston,\n\nAlthough demons, astral intelligences, and other spirits might manipulate natural causes with superhuman dexterity and thereby work marvels, as mere creatures they could never transcend from the preternatural to the supernatural and work genuine miracles.\n\nBy the 16th century, the term \"preternatural\" was increasingly used to refer to demonic activity comparable to the use of magic by human adepts: The Devil, \"being a natural Magician … may perform many acts in ways above our knowledge, though not transcending our natural power.\" According to the philosophy of the time, preternatural phenomena were not contrary to divine law, but used hidden, or occult powers that violated the \"normal\" pattern of natural phenomena.\n\nWith the emergence of early modern science, the concept of the preternatural increasingly came to be used to refer to strange or abnormal phenomena that seemed to violate the normal working of nature, but which were not associated with magic and witchcraft. This was a development of the idea that preternatural phenomena were fake miracles. As Daston puts it, \"To simplify the historical sequence somewhat: first, preternatural phenomena were demonized and thereby incidentally naturalized; then the demons were deleted, leaving only the natural causes.\" The use of the term was especially common in medicine, for example in John Brown's \"A Compleat Treatise of Preternatural Tumours\" (1678), or William Smellie's \"A Collection of Preternatural Cases and Observations in Midwifery\" (1754).\n\nIn the 19th century the term was appropriated in anthropology to refer to folk beliefs about fairies, trolls and other such creatures which were not thought of as demonic, but which were perceived to affect the natural world in unpredictable ways. According to Thorstein Veblen, such preternatural agents were often thought of as forces somewhere between supernatural beings and material processes. \"The preternatural agency is not necessarily conceived to be a personal agent in the full sense, but it is an agency which partakes of the attributes of personality to the extent of somewhat arbitrarily influencing the outcome of any enterprise, and especially of any contest.\"\n\nThe linguistic association between individual agents and unexplained or unfortunate circumstances remains. Many people attribute occurrences that are known to be material processes, such as \"gremlins in the engine\", a \"ghost in the machine\", or attributing motives to objects: \"the clouds are threatening\". The anthropomorphism in our daily life is a combination of the above cultural stems, as well as the manifestation of our pattern-projecting minds.\n\nIn 2011, Penn State Press began publishing a learned journal entitled \"Preternature: Critical and Historical Studies on the Preternatural\". Edited by Kirsten Uszkalo and Richard Raiswell, the journal is dedicated to publishing articles, reviews and short editions of original texts that deal with conceptions and perceptions of the preternatural in any culture and in any historical period. The journal covers \"magics, witchcraft, spiritualism, occultism, prophecy, monstrophy, demonology, and folklore.\"\n\n\n"}
{"id": "10854000", "url": "https://en.wikipedia.org/wiki?curid=10854000", "title": "Statistical study of energy data", "text": "Statistical study of energy data\n\nEnergy statistics refers to collecting, compiling, analyzing and disseminating data on commodities such as coal, crude oil, natural gas, electricity, or renewable energy sources (biomass, geothermal, wind or solar energy), when they are used for the energy they contain. Energy is the capability of some substances, resulting from their physico-chemical properties, to do work or produce heat. Some energy commodities, called fuels, release their energy content as heat when they burn. This heat could be used to run an internal or external combustion engine.\n\nThe need to have statistics on energy commodities became obvious during the 1973 oil crisis that brought tenfold increase in petroleum prices. Before the crisis, to have accurate data on global energy supply and demand was not deemed critical. Another concern of energy statistics today is a huge gap in energy use between developed and developing countries. As the gap narrows (\"see picture\"), the pressure on energy supply increases tremendously. \n\nThe data on energy and electricity come from three principal sources:\nThe flows of and trade in energy commodities are measured both in physical units (e.g., metric tons), and, when energy balances are calculated, in energy units (e.g., terajoules or tons of oil equivalent). What makes energy statistics specific and different from other fields of economic statistics is the fact that energy commodities undergo greater number of transformations (flows) than other commodities. In these transformations energy is conserved, as defined by and within the limitations of the first and second laws of thermodynamics. \n\n\n\n"}
{"id": "13680444", "url": "https://en.wikipedia.org/wiki?curid=13680444", "title": "Streaming vibration current", "text": "Streaming vibration current\n\nThe streaming vibration current (SVI) and the associated streaming vibration potential is an electric signal that arises when an acoustic wave propagates through a porous body in which the pores are filled with fluid.\n\nStreaming vibration current was experimentally observed in 1948 by M. Williams. A theoretical model was developed some 30 years later by Dukhin and coworkers. This effect opens another possibility for characterizing the electric properties of the surfaces in porous bodies.\n\n"}
{"id": "32172794", "url": "https://en.wikipedia.org/wiki?curid=32172794", "title": "Visual space", "text": "Visual space\n\nVisual space is the perceptual space housing the visual world being experienced by an aware observer; it is the subjective counterpart of the space of physical objects before an observer's eyes.\n\nIn object space the location and shape of physical targets can be accurately described with the tools of geometry. For practical purposes it is Euclidean. It is three-dimensional and various co-ordinate systems like the Cartesian x,y,z (with a defined origin in relation to an observer's head or eyes), or bipolar with angles of elevation, azimuth and binocular parallax (based on the separation of the two eyes) are interchangeable. No elaborate mathematics are needed.\n\nPercepts, the counterparts in the aware observer's conscious experience of objects in physical space, constitute an ordered ensemble or, as Ernst Cassirer explained, the perceptual world has a structure and is not an aggregate of scattered sensations. This visual space can be accessed by introspection, by interrogation, or by suitable experimental procedures which allow relative location as well as some structural properties to be assessed, even quantitatively.\n\nAn example illustrates the relationship between the concepts of object and visual space:\nTwo straight lines are presented to an observer who is asked to set them so that they appear parallel. When this has been done, the lines \"are\" parallel in visual space and now a comparison is feasible with the physical lines' setting in object space. Good precision can be achieved using psychophysical procedures in human observers or behavioral ones in trained animals. The reciprocal experiment is easier to perform but does not yield a numerical read-out as readily: show objectively parallel lines and make a determination of their inclination in the observer's perception.\n\nConsidering how it arises (see below), visual space seems, as an immediate, unmediated experience, to provide a remarkably true and unproblematic representation of a real world of objects.\n\nThe distinction is mandatory between what the eye professions call \"visual field\", the area or extent of physical space that is available to the eye or that is being imaged on the retina, and the virtual, perceptual \"visual space\" in which visual percepts are located, the subject of this entry.\nConfusion is caused by the use of \"\" in the German literature for both. There is no doubt that Ewald Hering and his followers meant visual space in their disquisitions.\n\nThe fundamental distinction was made by Rudolf Carnap between three kinds of space which he called \"formal\", \"physical\" and \"perceptual.\" Mathematicians, for example, deal with ordered structures, ensembles of elements for which rules of logico-deductive relationships hold, limited solely by being not self-contradictory. These are the \"formal\" spaces. According to Carnap, studying \"physical\" space means examining the relationship between empirically determined objects. Finally, there is the realm of what students of Kant know as \",\" immediate sensory experiences, often awkwardly translated as \"apperceptions,\" which belong to \"perceptual spaces.\"\n\nGeometry is the discipline devoted to the study of space and the rules relating the elements to each other. For example, in Euclidean space there is the Pythagorean theorem for distances. In a two-dimensional space of constant curvature, like the surface of a sphere, the rule is somewhat more complex but applies everywhere. On the two-dimensional surface of a football, the rule is more complex still and has different values depending on location. In well-behaved spaces such rules used for measurement and called \"Metrics,\" are classically handled by the mathematics invented by Riemann. Object space belongs to that class.\n\nTo the extent that it is reachable by scientifically acceptable probes, visual space as defined is also a candidate for such considerations. The first and remarkably prescient analysis was published by Ernst Mach in 1901. Under the heading \"On Physiological as Distinguished from Geometrical Space\" Mach states that \"Both spaces are threefold manifoldnesses\" but the former is \"...neither constituted everywhere and in all directions alike, nor infinite in extent, nor unbounded.\" A notable attempt at a rigorous formulation was made in 1947 by the highly talented mathematician Rudolf Luneburg, who preceded his essay by a profound analysis of the underlying principles. When features are sufficiently singular and distinct, there is no problem about a correspondence between an individual item \"A\" in object space and its correlate \"A' \" in visual space. Questions can be asked and answered such as \"If visual percepts \"A',B',C' \" are correlates of physical objects \"A,B,C,\" and if \"C\" lies between \"A\" and \"B\", does \"C' \" lie between \"A' \" and \"B' \"?\" In this manner, the possibility of visual space being metrical can be approached. If the exercise is successful, a great deal can be said about the nature of the mapping of the physical space on the visual space.\n\nOn the basis of fragmentary psychophysical data of previous generations, Luneburg concluded that visual space was hyperbolic with constant curvature, meaning that elements can be moved throughout the space without changing shape. One of Luneburg's major arguments is that, in accord with a common observation, the transformation involving hyperbolic space renders infinity into a dome (the sky). The Luneburg proposition gave rise to discussions and attempts at corroborating experiments, which on the whole did not favor it.\n\nBasic to the problem, and underestimated by Luneburg the mathematician, is the likely success of a mathematically viable formulation of the relationship between objects in physical space and percepts in visual space. Any scientific investigation of visual space is colored by the kind of access we have to it, and the precision, repeatability and generality of measurements. Insightful questions can be asked about the mapping of visual space to object space but answers are mostly limited in the range of their validity. If the physical setting that satisfies the criterion of, say, apparent parallelism varies from observer to observer, or from day to day, or from context to context, so does the geometrical nature of, and hence mathematical formulation for, visual space.\n\nAll these arguments notwithstanding, there is a major concordance between the locations of items in object space and their correlates in visual space. It is adequately veridical for us to navigate very effectively in the world, deviations from such a situation are sufficiently notable to warrant special consideration. visual space agnosia is a recognized neurological condition, and the many common distortions, called geometrical-optical illusions, are widely demonstrated but of minor consequence.\n\nIts founder, Gustav Theodor Fechner defined the mission of the discipline of psychophysics as the functional relationship between the mental and material worlds—in this particular case, the visual and object spaces—but he acknowledged an intermediate step, which has since blossomed into the major enterprise of modern neuroscience. In distinguishing between \"inner\" and \"outer\" psychophysics, Fechner recognized that a physical stimulus generates a percept by way of an effect on the organism's sensory and nervous systems. Hence, without denying that its essence is the arc between object and percept, the inquiry can concern itself with the neural substrate of visual space.\n\nTwo major concepts dating back to the middle of the 19th century set the parameters of the discussion here. Johannes Müller emphasized that what matters in a neural path is the connection it makes, and Hermann Lotze, from psychological considerations, enunciated the principle of local sign. Put together in modern neuroanatomical terms they mean that a nerve fiber from a fixed retinal location instructs its target neurons in the brain about the presence of a stimulus in the location in the eye's visual field that is imaged there. The orderly array of retinal locations is preserved in the passage from the retina to the brain, and provides what is aptly called a \"retinotopic\" mapping in the primary visual cortex. Thus in the first instance brain activity retains the relative spatial ordering of the objects and lays the foundations for a neural substrate of visual space. Unfortunately, as is so common in brain studies, simplicity and transparency ends here. Right at the outset, visual signals are analyzed not only for their position, but also, separately in parallel channels, for many other attributes such as brightness, color, orientation, depth. No single neuron or even neuronal center or circuit represents both the nature of a target feature and its accurate location. The unitary mapping of object space into the coherent visual space without internal contradictions or inconsistencies that we as observer automatically experience, demands concepts of conjoint activity in several parts of the nervous system that is at present beyond the reach of neurophysiological research.\n\nThough the details of the process by which the experience of visual space emerges remain opaque, a startling finding gives hope for future insights. Neural units have been demonstrated in the brain structure called hippocampus that show activity only when the animal is in a specific place in its environment.\n\nOnly on an astronomical scale are physical space and its contents interdependent, This major proposition of the general theory of relativity is of no concern in vision. For us, distances in object space are independent of the nature of the objects.\n\nBut this is not so simple in visual space. At a minim an observer judges the relative location of a few light points in an otherwise dark visual field, a simplistic extension from object space that enabled Luneburg to make some statements about the geometry of visual space. In a more richly textured visual world, the various visual percepts carry with them prior perceptual associations which often affect their relative spatial disposition. Identical separations in physical space can look quite different (\"are quite different\" in visual space) depending on the features that demarcate them. This is particularly so in the depth dimension because the apparatus by which values in the third visual dimension are assigned is fundamentally different from that for the height and width of objects.\n\nEven in monocular vision, which physiologically has only two dimensions, cues of size, perspective, relative motion etc. are used to assign depth differences to percepts. Looked at as a mathematical/geometrical problem, expanding a 2-dimensional object manifold into a 3-dimensional visual world is \"ill-posed,\" i.e., not capable of a rational solution, but is accomplished quite effectively by the human observer.\n\nThe problem becomes less ill-posed when binocular vision allows actual determination of relative depth by stereoscopy, but its linkage to the evaluation of distance in the other two dimensions is uncertain (see: stereoscopic depth rendition). Hence, the uncomplicated three-dimensional visual space of every-day experience is the product of many perceptual and cognitive layers superimposed on the physiological representation of the physical world of objects.\n"}
{"id": "382142", "url": "https://en.wikipedia.org/wiki?curid=382142", "title": "Ylem", "text": "Ylem\n\nYlem is a term that was used by George Gamow, his student Ralph Alpher, and their associates in the late 1940s for a hypothetical original substance or condensed state of matter, which became subatomic particles and elements as we understand them today. The term \"ylem\" was actually resuscitated (it appears in Webster's Second \"the first substance from which the elements were supposed to have been formed\") by Ralph Alpher.\n\nIn modern understanding, the \"ylem\" described as by Gamow was the primordial plasma, formed in baryogenesis, which underwent Big Bang nucleosynthesis and was opaque to radiation. Recombination of the charged plasma into neutral atoms made the Universe transparent at the age of 380,000 years, and the radiation released is still observable as cosmic microwave background radiation.\n\nIt comes from an obsolete Middle English philosophical word that Alpher said he found in Webster's dictionary. The word means something along the lines of \"primordial substance from which all matter is formed\" (that in ancient mythology of many different cultures was called the cosmic egg) and ultimately derives from the Greek \"ὕλη\" (\"hūlē, hȳlē\"), \"matter\", probably through an accusative singular form in Latin \"hylen, hylem\". In an oral history interview in 1968 Gamow talked about ylem as an old Hebrew word.\n\nThe ylem is what Gamow and colleagues presumed to exist immediately after the Big Bang. Within the ylem, there were assumed to be a large number of high-energy photons present. Alpher and Robert Herman made a scientific prediction in 1948 that we should still be able to observe these red-shifted photons today as an ambient cosmic microwave background radiation (CMBR) pervading all space with a temperature of about 5 kelvins (when the CMBR was actually first detected in 1965, its temperature was found to be 3 kelvins). It is now recognized that the CMBR originated at the transition from predominantly ionized hydrogen to non-ionized hydrogen at around 400,000 years after the Big Bang.\n\nAfter the term \"ylem\" was resuscitated by Alpher, it was used in the 1952 science fiction novel \"Jack of Eagles\" by James Blish. It was also used by John Brunner in his 1959 short story \"Round Trip\", reprinted in the collection \"Not Before Time\". Keith Laumer in the novel \"Dinosaur Beach\" introduces the ylem field 1969. The term is also used by British author Richard Calder in the 1990s to describe the quantum mechanical state of the \"quantum magic\" in the girls/robots in his \"Dead\" trilogy (\"Dead Girls\", \"Dead Boys\", \"Dead Things\"). John C. Wright used the term in his debut novel \"\" to describe a \"pseudo-matter\" that forms \"temporary virtual particles\". A German black metal band \"Dark Fortress\" also released an album titled \"Ylem\". A the trance classic by The Thrillseekers - Synaesthesia has a \"Ylem\" remix. The video game series Ultima uses \"Ylem\" as a Word of Power in its incantation and runic based spell casting system, its meaning being \"matter\".\n\nIn 1981, Trudy Myrrh Reagan formed an organization, \"Ylem: Artists Using Science and Technology\" (later written YLEM), in the San Francisco Bay Area.\n\nIt is also a usable word in the Official Scrabble Players Dictionary, Fifth edition.\n\n"}
