{"id": "39136", "url": "https://en.wikipedia.org/wiki?curid=39136", "title": "Accelerating expansion of the universe", "text": "Accelerating expansion of the universe\n\nThe accelerating expansion of the universe is the observation that the expansion of the universe is such that the velocity at which a distant galaxy is receding from the observer is continuously increasing with time.\n\nThe accelerated expansion was discovered in 1998, by two independent projects, the Supernova Cosmology Project and the High-Z Supernova Search Team, which both used distant type Ia supernovae to measure the acceleration. The idea was that as type 1a supernovae have almost the same intrinsic brightness (a standard candle), and since objects that are further away appear dimmer, we can use the observed brightness of these supernovae to measure the distance to them. The distance can then be compared to the supernovae's cosmological redshift, which measures how much the universe has expanded since the supernova occurred. The unexpected result was that objects in the universe are moving away from another at an accelerated rate. Cosmologists at the time expected that recession velocity would always be decelerating to the gravitational attraction of the matter in the universe. Three members of these two groups have subsequently been awarded Nobel Prizes for their discovery. Confirmatory evidence has been found in baryon acoustic oscillations, and in analyses of the clustering of galaxies.\n\nThe accelerated expansion of the universe is thought to have begun since the universe entered its dark-energy-dominated era roughly 5 billion years ago.\nWithin the framework of general relativity, an accelerated expansion can be accounted for by a positive value of the cosmological constant , equivalent to the presence of a positive vacuum energy, dubbed \"dark energy\". While there are alternative possible explanations, the description assuming dark energy (positive ) is used in the current standard model of cosmology, which also includes cold dark matter (CDM) and is known as the Lambda-CDM model.\n\nIn the decades since the detection of cosmic microwave background (CMB) in 1965, the Big Bang model has become the most accepted model explaining the evolution of our universe. The Friedmann equation defines how the energy in the universe drives its expansion.\n\nwhere the pressure is defined by the cosmological model chosen. (see explanatory models below)\n\nPhysicists at one time were so assured of the deceleration of the universe's expansion that they introduced a so-called deceleration parameter . Current observations point towards this deceleration parameter being negative.\n\nAccording to the theory of cosmic inflation, the very early universe underwent a period of very rapid, quasi-exponential expansion. While the time-scale for this period of expansion was far shorter than that of the current expansion, this was a period of accelerated expansion with some similarities to the current epoch.\n\nThe definition of \"accelerating expansion\" is that the second time derivative of the cosmic scale factor, formula_2, is positive, which implies that the deceleration parameter is negative. However, note this does not imply that the Hubble parameter is increasing with time. Since the Hubble parameter is defined as formula_3, it follows from the definitions that the derivative of the Hubble parameter is given by \n\nso the Hubble parameter is decreasing with time unless formula_5. Observations prefer formula_6, which implies that formula_2 is positive but formula_8 is negative. Essentially, this implies that the cosmic recession velocity of any one particular galaxy is increasing with time, but its velocity/distance ratio is still decreasing; thus different galaxies expanding across a sphere of fixed radius cross the sphere more slowly at later times.\n\nIt is seen from above that the case of \"zero acceleration/deceleration\" corresponds to formula_9 is a linear function of formula_10, formula_11, formula_12, and formula_13.\n\nTo learn about the rate of expansion of the universe we look at the magnitude-redshift relationship of astronomical objects using standard candles, or their distance-redshift relationship using standard rulers. We can also look at the growth of large-scale structure, and find that the observed values of the cosmological parameters are best described by models which include an accelerating expansion.\n\nThe first evidence for acceleration came from the observation of Type Ia supernovae, which are exploding white dwarfs that have exceeded their stability limit. Because they all have similar masses, their intrinsic luminosity is standardizable. Repeated imaging of selected areas of the sky is used to discover the supernovae, then follow-up observations give their peak brightness, which is converted into a quantity known as luminosity distance (see distance measures in cosmology for details). Spectral lines of their light can be used to determine their redshift.\n\nFor supernovae at redshift less than around 0.1, or light travel time less than 10 percent of the age of the universe, this gives a nearly linear distance–redshift relation due to Hubble's law. At larger distances, since the expansion rate of the universe has changed over time, the distance-redshift relation deviates from linearity, and this deviation depends on how the expansion rate has changed over time. The full calculation requires computer integration of the Friedmann equation, but a simple derivation can be given as follows: the redshift directly gives the cosmic scale factor at the time the supernova exploded.\n\nSo a supernova with a measured redshift implies the universe was  =  of its present size when the supernova exploded. In the standard accelerated expansion scenario the rate of expansion still decreases, but does so more slowly than the non-accelerated case. This means that in the accelerated case, the past rate of expansion is slower than it would be in the non-accelerated case. Thus in a universe with accelerated expansion, it takes a longer time to expand from two thirds its present size, compared to a non-accelerating universe with the same present-day value of the Hubble constant. This results in a larger light-travel time, larger distance and fainter supernovae, which corresponds to the actual observations. Adam Riess \"et al.\" found that \"the distances of the high-redshift SNe Ia were, on average, 10% to 15% farther than expected in a low mass density universe without a cosmological constant\". This means that the measured high-redshift distances were too large, compared to nearby ones, for a decelerating universe.\n\nIn the early universe before recombination and decoupling took place, photons and matter existed in a primordial plasma. Points of higher density in the photon-baryon plasma would contract, being compressed by gravity until the pressure became too large and they expanded again. This contraction and expansion created vibrations in the plasma analogous to sound waves. Since dark matter only interacts gravitationally it stayed at the centre of the sound wave, the origin of the original overdensity. When decoupling occurred, approximately 380,000 years after the Big Bang, photons separated from matter and were able to stream freely through the universe, creating the cosmic microwave background as we know it. This left shells of baryonic matter at a fixed radius from the overdensities of dark matter, a distance known as the sound horizon. As time passed and the universe expanded, it was at these anisotropies of matter density where galaxies started to form. So by looking at the distances at which galaxies at different redshifts tend to cluster, it is possible to determine a standard angular diameter distance and use that to compare to the distances predicted by different cosmological models.\n\nPeaks have been found in the correlation function (the probability that two galaxies will be a certain distance apart) at , indicating that this is the size of the sound horizon today, and by comparing this to the sound horizon at the time of decoupling (using the CMB), we can confirm the accelerated expansion of the universe.\n\nMeasuring the mass functions of galaxy clusters, which describe the number density of the clusters above a threshold mass, also provides evidence for dark energy . By comparing these mass functions at high and low redshifts to those predicted by different cosmological models, values for and are obtained which confirm a low matter density and a non zero amount of dark energy.\n\nGiven a cosmological model with certain values of the cosmological density parameters, it is possible to integrate the Friedmann equations and derive the age of the universe.\n\nBy comparing this to actual measured values of the cosmological parameters, we can confirm the validity of a model which is accelerating now, and had a slower expansion in the past.\n\nRecent discoveries of gravitational waves through LIGO and VIRGO not only confirmed Einstein's predictions but also opened a new window into the universe. These gravitational waves can work as sort of standard sirens to measure the expansion rate of the universe. Abbot et al. 2017 measured the Hubble constant value to be approximately 70 kilometres per second per megaparsec. The amplitudes of the strain 'h' is dependent on the masses of the objects causing waves, distances from observation point and gravitational waves detection frequencies. The associated distance measures are dependent on the cosmological parameters like the Hubble Constant for nearby objects and will be dependent on other cosmological parameters like the dark energy density, matter density, etc. for distant sources.\n\nThe most important property of dark energy is that it has negative pressure (repulsive action) which is distributed relatively homogeneously in space.\n\nwhere is the speed of light and is the energy density. Different theories of dark energy suggest different values of , with for cosmic acceleration (this leads to a positive value of in the acceleration equation above).\n\nThe simplest explanation for dark energy is that it is a cosmological constant or vacuum energy; in this case . This leads to the Lambda-CDM model, which has generally been known as the Standard Model of Cosmology from 2003 through the present, since it is the simplest model in good agreement with a variety of recent observations. Riess \"et al.\" found that their results from supernovae observations favoured expanding models with positive cosmological constant () and a current accelerated expansion ().\n\nCurrent observations allow the possibility of a cosmological model containing a dark energy component with equation of state . This phantom energy density would become infinite in finite time, causing such a huge gravitational repulsion that the universe would lose all structure and end in a Big Rip. For example, for and  =70 km·s·Mpc, the time remaining before the universe ends in this Big Rip is 22 billion years.\n\nThere are many alternative explanations for the accelerating universe. Some examples are quintessence, a proposed form of dark energy with a non-constant state equation, whose density decreases with time. Dark fluid is an alternative explanation for accelerating expansion which attempts to unite dark matter and dark energy into a single framework. Alternatively, some authors have argued that the accelerated expansion of the universe could be due to a repulsive gravitational interaction of antimatter or a deviation of the gravitational laws from general relativity. The measurement of the speed of gravity with the gravitational wave event GW170817 ruled out many modified gravity theories as alternative explanation to dark energy.\n\nAnother type of model, the backreaction conjecture, was proposed by cosmologist Syksy Räsänen: the rate of expansion is not homogenous, but we are in a region where expansion is faster than the background. Inhomogeneities in the early universe cause the formation of walls and bubbles, where the inside of a bubble has less matter than on average. According to general relativity, space is less curved than on the walls, and thus appears to have more volume and a higher expansion rate. In the denser regions, the expansion is slowed by a higher gravitational attraction. Therefore, the inward collapse of the denser regions looks the same as an accelerating expansion of the bubbles, leading us to conclude that the universe is undergoing an accelerated expansion. The benefit is that it does not require any new physics such as dark energy. Räsänen does not consider the model likely, but without any falsification, it must remain a possibility. It would require rather large density fluctuations (20%) to work.\n\nA final possibility is that dark energy is an illusion caused by some bias in measurements. For example, if we are located in an emptier-than-average region of space, the observed cosmic expansion rate could be mistaken for a variation in time, or acceleration. A different approach uses a cosmological extension of the equivalence principle to show how space might appear to be expanding more rapidly in the voids surrounding our local cluster. While weak, such effects considered cumulatively over billions of years could become significant, creating the illusion of cosmic acceleration, and making it appear as if we live in a Hubble bubble. Yet other possibilities are that the accelerated expansion of the universe is an illusion caused by the relative motion of us to the rest of the universe, or that the supernovae sample size used wasn't large enough.\n\nAs the universe expands, the density of radiation and ordinary dark matter declines more quickly than the density of dark energy (see equation of state) and, eventually, dark energy dominates. Specifically, when the scale of the universe doubles, the density of matter is reduced by a factor of 8, but the density of dark energy is nearly unchanged (it is exactly constant if the dark energy is a cosmological constant).\n\nIn models where dark energy is a cosmological constant, the universe will expand exponentially with time in the far future, coming closer and closer to a de Sitter spacetime. This will eventually lead to all evidence for the Big Bang disappearing, as the cosmic microwave background is redshifted to lower intensities and longer wavelengths. Eventually its frequency will be low enough that it will be absorbed by the interstellar medium, and so be screened from any observer within the galaxy. This will occur when the universe is less than 50 times its current age, leading to the end of cosmology as we know it as the distant universe turns dark.\n\nA constantly expanding universe with non-zero cosmological constant has mass density decreasing over time, to an undetermined point when zero matter density is reached. All matter (electrons, protons and neutrons) would ionize and disintegrate, with objects dissipating away.\n\nAlternatives for the ultimate fate of the universe include the Big Rip mentioned above, a Big Bounce, Big Freeze or Big Crunch.\n"}
{"id": "37205291", "url": "https://en.wikipedia.org/wiki?curid=37205291", "title": "Aesthetics of nature", "text": "Aesthetics of nature\n\nAesthetics of nature is a sub-field of philosophical ethics, and refers to the study of natural objects from their aesthetical perspective.\n\nAesthetics of nature developed as a sub-field of philosophical ethics. In the 18th and 19th century, the aesthetics of nature advanced the concepts of disinterestedness, the pictures, and the introduction of the idea of positive aesthetics. The first major developments of nature occurred in the 18th century. The concept of disinterestedness had been explained by many thinkers. Anthony Ashley-Cooper introduced the concept as a way of characterizing the notion of the aesthetic, later magnified by Francis Hutcheson, who expanded it to exclude personal and utilitarianism interests and associations of a more general nature from aesthetic experience. This concept was further developed by Archibald Alison who referred it to a particular state of mind.\n\nThe theory of disinterestedness opened doors for a better understanding of the aesthetics dimensions of nature in terms of three conceptualizations: \n\nObjects experienced as beautiful tend to be small, smooth, and fair in color. In contrast, objects viewed as sublime tend to be powerful, intense and terrifying. Picturesque items are a mixture of both, which can be seen as varied and irregular, rich and forceful, and even vibrant.\n\nCognitive and non-cognitive approaches of nature have directed their focus from natural environments to the consideration of human and human influenced environments and developed aesthetic investigations of everyday life.(Carlson and Lintott, 2007; Parsons 2008a; Carlson 2010)\n\nPeople may be mistaken by the art object analogy. For instance, a sandhill crane is not an art object; an art object is not a sandhill crane. In fact, an art object should be called an \"artifact\". The crane is wildlife on its own and is not an art object. This can be related to Satio's definition of the cognitive view. In elaboration, the crane lives through various ecosystems such as Yellowstone. Nature is a living system which includes animals, plants, and Eco-systems. In contrast, an art object has no regeneration, evolutionary history, or metabolism. An individual may be in the forest and perceive it as beautiful because of the plethora of colors such as red, green, and yellow. This is a result of the chemicals interacting with chlorophyll. An individual's aesthetic experience may increase; however, none of the things mentioned have anything to do with what is really going on in the forest. The chlorophyll is capturing solar energy and the residual chemicals protect the trees from insect grazing.\n\nAny color perceived by human visitors for a few hours is entirely different from what is really happening. According to Leopold, the three features of ecosystems that generate land ethic are integrity, stability and beauty. None of the mentioned features are real in nature. Ecosystems are not stable: they are dramatically changing and they have little integration; ergo, beauty is in the eye of the beholder.\n\nIn a Post-Modern approach, when an individual engages in aesthetically appreciating a natural thing, we give meaning to the thing we appreciate and in that meaning, we express and develop our own attitudes, values and beliefs. Our interest in natural things are not only a passive reflection of our inclinations, as Croce describes as the appreciation of nature as looking in a mirror, or what we might call our inward life; but may instead be the things we come across in nature that engage and stimulate our imagination. As a result, we are challenged to think differently and apply thoughts and associations to in new situations and ways.\nAs a characterization of the appreciation of art, nature aestheticists argue that post modernism is a mistaken view because we do not have a case of anything goes.The aesthetics appreciation of art is governed by some normative standards. In the world of art, criticism may take place when people come together and discuss books and films or critics write appraisals for publications. On the contrary, there are not obvious instances of debate and appraisals where different judgments about the aesthetics of character of nature are evaluated.\n"}
{"id": "4297956", "url": "https://en.wikipedia.org/wiki?curid=4297956", "title": "Clockwork universe", "text": "Clockwork universe\n\nIn the history of science, the clockwork universe compares the universe to a mechanical clock. It continues ticking along, as a perfect machine, with its gears governed by the laws of physics, making every aspect of the machine predictable.\n\nThis idea was very popular among deists during the Enlightenment, when Isaac Newton derived his laws of motion, and showed that alongside the law of universal gravitation, they could explain the behaviour of both terrestrial objects and the solar system.\n\nA similar concept goes back, to John of Sacrobosco's early 13th-century introduction to astronomy: \"On the Sphere of the World\". In this widely popular medieval text, Sacrobosco spoke of the universe as the \"machina mundi\", the machine of the world, suggesting that the reported eclipse of the Sun at the crucifixion of Jesus was a disturbance of the order of that machine.\n\nResponding to Gottfried Leibniz, a prominent supporter of the theory, in the Leibniz–Clarke correspondence, Samuel Clarke wrote:\n\nIn 2009 artist Tim Wetherell created a large wall piece for Questacon (The National Science and Technology centre in Canberra, Australia) representing the concept of the clockwork universe. This steel artwork contains moving gears, a working clock, and a movie of the lunar terminator.\n\n\n\n"}
{"id": "13662027", "url": "https://en.wikipedia.org/wiki?curid=13662027", "title": "Colloid vibration current", "text": "Colloid vibration current\n\nColloid vibration current is an electroacoustic phenomenon that arises when ultrasound propagates through a fluid that contains ions and either solid particles or emulsion droplets. \n\nThe pressure gradient in an ultrasonic wave moves particles relative to the fluid. This motion disturbs the double layer that exists at the particle-fluid interface. The picture illustrates the mechanism of this distortion. Practically all particles in fluids carry a surface charge. This surface charge is screened with an equally charged diffuse layer; this structure is called the double layer. Ions of the diffuse layer are located in the fluid and can move with the fluid. Fluid motion relative to the particle drags these diffuse ions in the direction of one or the other of the particle's poles. The picture shows ions dragged towards the left hand pole. As a result of this drag, there is an excess of negative ions in the vicinity of the left hand pole and an excess of positive surface charge at the right hand pole. As a result of this charge excess, particles gain a dipole moment. These dipole moments generate an electric field that in turn generates measurable electric current. This phenomenon is widely used for measuring zeta potential in concentrated colloids.\n\n"}
{"id": "12527335", "url": "https://en.wikipedia.org/wiki?curid=12527335", "title": "Cosmic time", "text": "Cosmic time\n\nCosmic time (also known as time since the big bang) is the time coordinate commonly used in the Big Bang models of physical cosmology. It is defined for homogeneous, expanding universes as follows: Choose a time coordinate so that the universe has the same density everywhere at each moment in time (the fact that this is possible means that the universe is, by definition, homogeneous). Measure the passage of time using clocks moving with the Hubble flow. Choose the big bang singularity as the origin of the time coordinate. \n\nCosmic time formula_1 is a measure of time by a physical clock with zero peculiar velocity in the absence of matter over-/under-densities (to prevent time dilation due to relativistic effects or confusions caused by expansion of the universe). Unlike other measures of time such as temperature, redshift, particle horizon, or Hubble horizon, the cosmic time (similar and complementary to the comoving coordinates) is blind to the expansion of the universe. \n\nThere are two main ways for establishing a reference point for the cosmic time. The most trivial way is to take the present time as the cosmic reference point (sometimes referred to as the lookback time) or alternatively, take the Big Bang as formula_2 (also referred to as age of the universe). The big bang doesn't necessarily have to correspond to a physical event but rather it refers to the point at which the scale factor would vanish for a standard cosmological model such as ΛCDM. For instance, in the case of inflation, i.e. a non-standard cosmology, the hypothetical moment of big bang is still determined using the benchmark cosmological models which may coincide with the end of the inflationary epoch. For inflationary models, it is not possible to establish a well defined origin of time before the big bang since the universe does not require a beginning event in such models. For technical purposes, concepts such as the average temperature of the universe (in units of eV) or the particle horizon are used when the early universe is the objective of a study since understanding the interaction among particles is more relevant than their time coordinate or age. \n\nCosmic time is the standard time coordinate for specifying the Friedmann–Lemaître–Robertson–Walker solutions of Einstein's equations.\n\n"}
{"id": "42882", "url": "https://en.wikipedia.org/wiki?curid=42882", "title": "Cosmogony", "text": "Cosmogony\n\nCosmogony is any model concerning the origin of either the cosmos or universe. Developing a complete theoretical model has implications in both the philosophy of science and epistemology.\n\nThe word comes from the Koine Greek κοσμογονία (from κόσμος \"cosmos, the world\") and the root of γί(γ)νομαι / γέγονα (\"come into a new state of being\"). In astronomy, cosmogony refers to the study of the origin of particular astrophysical objects or systems, and is most commonly used in reference to the origin of the Universe, the Solar System, or the Earth–Moon system.\n\nThe Big Bang theory is the prevailing cosmological model of the early development of the universe. \nThe most commonly held view is that the universe originates in a gravitational singularity, which expanded extremely rapidly from its hot and dense state. \nCosmologist and science communicator Sean M. Carroll explains two competing types of explanations for the origins of the singularity which is the main disagreement between the scientists who study cosmogony and centers on the question of whether time existed \"before\" the emergence of our universe or not. One cosmogonical view sees time as fundamental and even eternal: The universe could have contained the singularity because the universe evolved or changed from a prior state (the prior state was \"empty space\", or maybe a state that could not be called \"space\" at all). The other view, held by proponents like Stephen Hawking, says that there was no change through time because \"time\" itself emerged along with this universe (in other words, there can be no \"prior\" to the universe). Thus, it remains unclear what combination of \"stuff\", space, or time emerged with the singularity and this universe.\n\nOne problem in cosmogony is that there is currently no theoretical model that explains the earliest moments of the universe's existence (during the Planck time) because of a lack of a testable theory of quantum gravity. Researchers in string theory and its extensions (for example, M theory), and of loop quantum cosmology, have nevertheless proposed solutions of the type just discussed.\n\nCosmogony can be distinguished from cosmology, which studies the universe at large and throughout its existence, and which technically does not inquire directly into the source of its origins. There is some ambiguity between the two terms. For example, the cosmological argument from theology regarding the existence of God is technically an appeal to cosmogonical rather than cosmological ideas. In practice, there is a scientific distinction between cosmological and cosmogonical ideas. Physical cosmology is the science that attempts to explain all observations relevant to the development and characteristics of the universe as a whole. Questions regarding why the universe behaves in such a way have been described by physicists and cosmologists as being extra-scientific (i.e., metaphysical), though speculations are made from a variety of perspectives that include extrapolation of scientific theories to untested regimes (i.e., at Planck scales), and philosophical or religious ideas.\n\nCosmogonists have only tentative theories for the early stages of the universe and its beginning. , no accelerator experiments probe energies of sufficient magnitude to provide any experimental insight into the behavior of matter at the energy levels that prevailed shortly after the Big Bang. \n\nProposed theoretical scenarios differ radically, and include string theory and M-theory, the Hartle–Hawking initial state, string landscape, brane inflation, the Big Bang, and the ekpyrotic universe. Some of these models are mutually compatible, whereas others are not.\n"}
{"id": "2848730", "url": "https://en.wikipedia.org/wiki?curid=2848730", "title": "Cosmological horizon", "text": "Cosmological horizon\n\nA cosmological horizon is a measure of the distance from which one could possibly retrieve information. This observable constraint is due to various properties of general relativity, the expanding universe, and the physics of Big Bang cosmology. Cosmological horizons set the size and scale of the observable universe. This article explains a number of these horizons.\n\nThe particle horizon (also called the cosmological horizon, the comoving horizon, or the cosmic light horizon) is the maximum distance from which particles could have traveled to the observer in the age of the universe. It represents the boundary between the observable and the unobservable regions of the universe, so its distance at the present epoch defines the size of the observable universe. Due to the expansion of the universe it is not simply the age of the universe times the speed of light, as in the Hubble horizon, but rather the speed of light multiplied by the conformal time. The existence, properties, and significance of a cosmological horizon depend on the particular cosmological model.\n\nIn terms of comoving distance, the particle horizon is equal to the conformal time that has passed since the Big Bang, times the speed of light. In general, the conformal time at a certain time is given in terms of the scale factor formula_1 by,\n\nor\n\nThe particle horizon is the boundary between two regions at a point at a given time: one region defined by events that have already been observed by an observer, and the other by events which cannot be observed \"at that time\". It represents the furthest distance from which we can retrieve information from the past, and so defines the observable universe.\n\nHubble radius, Hubble sphere, Hubble volume, or Hubble horizon is a conceptual horizon defining the boundary between particles that are moving slower and faster than the speed of light relative to an observer at one given time. Note that this does not mean the particle is unobservable, the light from the past is reaching and will continue to reach the observer for a while. Also, more importantly, in the current expansion model e.g., light emitted from the Hubble radius will reach us in a finite amount of time. It is a common misconception that light from the Hubble radius can never reach us. It is true that particles on the Hubble radius recede from us with the speed of light, but the Hubble radius gets larger over time (because the Hubble parameter H gets smaller over time), so light emitted towards us from a particle on the Hubble radius will be inside the Hubble radius some time later. Only light emitted from the cosmic event horizon or further will never reach us in a finite amount of time.\n\nThe Hubble velocity of an object is given by Hubble's law,\n\nReplacing formula_5 with speed of light formula_6 and solving for proper distance formula_7 we obtain the radius of Hubble sphere as\n\nIn an ever-accelerating universe, if two particles are separated by a distance greater than the Hubble radius, they cannot talk to each other from now on (as they are now, not as they have been in the past), However, if they are outside of each other's particle horizon, they could have never communicated. Depending on the form of expansion of the universe, they may be able to exchange information in the future. Today,\n\nyielding a Hubble horizon of some 4.1 Gpc. This horizon is not really a physical size, but it is often used as useful length scale as most physical sizes in cosmology can be written in terms of those factors.\n\nOne can also define comoving Hubble horizon by simply dividing Hubble radius by the scale factor\n\nThe particle horizon differs from the cosmic event horizon, in that the particle horizon represents the largest comoving distance from which light could have reached the observer by a specific time, while the event horizon is the largest comoving distance from which light emitted now can \"ever\" reach the observer in the future. The current distance to our cosmic event horizon is about 5 Gpc (16 billion light years), well within our observable range given by the particle horizon.\n\nIn general, the proper distance to the event horizon at time formula_11 is given by\n\nwhere formula_13 is the time-coordinate of the end of the universe, which would be infinite in the case of a universe that expands forever.\n\nFor our case, assuming that dark energy is due to a cosmological constant, formula_14.\n\nIn an accelerating universe, there are events which will be unobservable as formula_15 as signals from future events become redshifted to arbitrarily long wavelengths in the exponentially expanding de Sitter space. This sets a limit on the farthest distance that we can possibly see as measured in units of proper distance today. Or, more precisely, there are events that are spatially separated for a certain frame of reference happening simultaneously with the event occurring right now for which no signal will ever reach us, even though we can observe events that occurred at the same location in space that happened in the distant past. While we will continue to receive signals from this location in space, even if we wait an infinite amount of time, a signal that left from that location today will never reach us. Additionally, the signals coming from that location will have less and less energy and be less and less frequent until the location, for all practical purposes, becomes unobservable. In a universe that is dominated by dark energy which is undergoing an exponential expansion of the scale factor, all objects that are gravitationally unbound with respect to the Milky Way will become unobservable, in a futuristic version of Kapteyn's universe.\n\nWhile not technically \"horizons\" in the sense of an impossibility for observations due to relativity or cosmological solutions, there are practical horizons which include the optical horizon, set at the surface of last scattering. This is the farthest distance that any photon can freely stream. Similarly, there is a \"neutrino horizon\" set for the farthest distance a neutrino can freely stream and a gravitational wave horizon at the farthest distance that gravitational waves can freely stream. The latter is predicted to be a direct probe of the end of cosmic inflation.\n\nFor a simplified summary and overview of different horizons in cosmology, see Different Horizons in Cosmology\n"}
{"id": "38737", "url": "https://en.wikipedia.org/wiki?curid=38737", "title": "Cosmos", "text": "Cosmos\n\nThe cosmos (, ) is the universe. Using the word \"cosmos\" rather than the word \"universe\" implies viewing the universe as a complex and orderly system or entity; the opposite of chaos.\nThe cosmos, and our understanding of the reasons for its existence and significance, are studied in cosmology - a very broad discipline covering any scientific, religious, or philosophical contemplation of the cosmos and its nature, or reasons for existing. Religious and philosophical approaches may include in their concepts of the cosmos various spiritual entities or other matters deemed to exist outside our physical universe.\n\nThe philosopher Pythagoras first used the term \"cosmos\" () for the order of the universe. The term became part of modern language in the 19th century when geographer–polymath Alexander von Humboldt resurrected the use of the word from the ancient Greek, assigned it to his five-volume treatise, \"Kosmos\", which influenced modern and somewhat holistic perception of the universe as one interacting entity.\n\nCosmology is the study of the cosmos, and in its broadest sense covers a variety of very different approaches: scientific, religious and philosophical. All cosmologies have in common an attempt to understand the implicit order within the whole of being. In this way, most religions and philosophical systems have a cosmology.\n\nWhen \"cosmology\" is used without a qualifier, it often signifies physical cosmology, unless the context makes clear that a different meaning is intended.\n\nPhysical cosmology (often simply described as 'cosmology') is the scientific study of the universe, from the beginning of its physical existence. It includes speculative concepts such as a multiverse, when these are being discussed. In physical cosmology, the term \"cosmos\" is often used in a technical way, referring to a particular spacetime continuum within a (postulated) multiverse. Our particular cosmos, the observable universe, is generally capitalized as \"the Cosmos\". \n\nIn physical cosmology, the uncapitalized term cosmic signifies a subject with a relationship to the universe, such as 'cosmic time' (time since the Big Bang), 'cosmic rays' (high energy particles or radiation detected from space), and 'cosmic microwave background' (microwave radiation detectable from all directions in space).\n\nAccording to in Sir William Smith \"Dictionary of Greek and Roman Biography and Mythology\" (1870, see book screenshot for full quote), Pythagoreans described the universe.\n\nCosmology is a branch of metaphysics that deals with the nature of the universe, a theory or doctrine describing the natural order of the universe. The basic definition of Cosmology is the science of the origin and development of the universe. In modern astronomy the Big Bang theory is the dominant postulation.\n\nIn theology, the cosmos is the created heavenly bodies (sun, moon, planets, and fixed stars). In Christian theology, the word is also used synonymously with \"aion\" to refer to \"worldly life\" or \"this world\" or \"this age\" as opposed to the afterlife or world to come.\n\nThe 1870 book \"Dictionary of Greek and Roman Biography and Mythology\" noted\n\nThe book \"The Works of Aristotle\" (1908, p. 80 \"Fragments\") mentioned\n\nBertrand Russell (1947) noted\n\n"}
{"id": "1045142", "url": "https://en.wikipedia.org/wiki?curid=1045142", "title": "Debris", "text": "Debris\n\nDebris or débris (, ) is rubble, wreckage, ruins, litter and discarded garbage/refuse/trash, scattered remains of something destroyed, discarded, or as in geology, large rock fragments left by a melting glacier etc. Depending on context, \"debris\" can refer to a number of different things. The first apparent use of the French word in English is in a 1701 description of the army of Prince Rupert upon its retreat from a battle with the army of Oliver Cromwell, in England.\n\nIn disaster scenarios, tornados leave behind large pieces of houses and mass destruction overall. This debris also flies around the tornado itself when it is in progress. The tornado's winds capture debris it kicks up in its wind orbit, and spins it inside its vortex. The tornado's wind radius is larger than the funnel itself. tsunamis and hurricanes also bring large amounts of debris, such as Hurricane Katrina in 2005 and Hurricane Sandy in 2012. Earthquakes rock cities to rubble debris.\n\nIn geology, debris usually applies to the remains of geological activity including landslides, volcanic explosions, avalanches, mudflows or Glacial lake outburst floods (Jökulhlaups) and moraine, lahars, and lava eruptions. Geological debris sometimes moves in a stream called a debris flow. When it accumulates at the base of hillsides, it can be called \"talus\" or \"scree\".\n\nIn mining, debris called \"attle\" usually consists of rock fragments which contain little or no ore.\n\n\"Marine debris\" applies to floating garbage such as bottles, cans, styrofoam, cruise ship waste, offshore oil and gas exploration and production facilities pollution, and fishing paraphernalia from professional and recreational boaters. Marine debris is also called litter or flotsam and jetsam. Objects that can constitute marine debris include used automobile tires, detergent bottles, medical wastes, discarded fishing line and nets, soda cans, and bilge waste solids.\n\nIn addition to being unsightly, it can pose a serious threat to marine life, boats, swimmers, divers, and others. For example, each year millions of seabirds, sea turtles, fish, and marine mammals become entangled in marine debris, or ingest plastics which they have mistaken for food. As many as 30,000 northern fur seals per year get caught in abandoned fishing nets and either drown or suffocate. Whales mistake plastic bags for squid, and birds may mistake plastic pellets for fish eggs. At other times, animals accidentally eat the plastic while feeding on natural food.\n\nThe largest concentration of marine debris is the Great Pacific Garbage Patch.\n\nMarine debris most commonly originates from land-based sources. Various international agencies are currently working to reduce marine debris levels around the world.\n\nIn meteorology, debris usually applies to the remains of human habitation and natural flora after storm related destruction. This debris is also commonly referred to as storm debris. Storm debris commonly consists of roofing material, downed tree limbs, downed signs, downed power lines and poles, and wind-blown garbage. Storm debris can become a serious problem immediately after a storm, in that it often blocks access to individuals and communities that may require emergency services. This material frequently exists in such large quantities that disposing of it becomes a serious issue for a community. In addition, storm debris is often hazardous by its very nature, since, for example, downed power lines annually account for storm-related deaths.\n\n\"Space debris\" usually refers to the remains of spacecraft that have either fallen to Earth or are still orbiting Earth. Space debris may also consist of natural components such as chunks of rock and ice. The problem of space debris has grown as various space programs have left legacies of launches, explosions, repairs, and discards in both low Earth orbit and more remote orbits. These orbiting fragments have reached a great enough proportion to constitute a hazard to future space launches of both satellite and manned vehicles. Various government agencies and international organizations are beginning to track space debris and also research possible solutions to the problem. While many of these items, ranging in size from nuts and bolts to entire satellites and spacecraft, may fall to Earth, other items located in more remote orbits may stay aloft for centuries. The velocity of some of these pieces of space junk have been clocked in excess of 17,000 miles per hour (27,000 km/h). A piece of space debris falling to Earth leaves a fiery trail, just like a meteor.\n\nA debris disk is a circumstellar disk of dust and debris in orbit around a star.\n\nIn medicine, debris usually refers to biological matter that has accumulated or lodged in surgical instruments and is referred to as surgical debris. The presence of surgical debris can result in cross-infections or nosocomial infections if not removed and the affected surgical instruments or equipment properly disinfected.\n\nIn the aftermath of a war, large areas of the region of conflict are often strewn with \"war debris\" in the form of abandoned or destroyed hardware and vehicles, mines, unexploded ordnance, bullet casings and other fragments of metal.\n\nMuch war debris has the potential to be lethal and continues to kill and maim civilian populations for years after the end of a conflict. The risks from war debris may be sufficiently high to prevent or delay the return of refugees. In addition war debris may contain hazardous chemicals or radioactive components that can contaminate the land or poison civilians who come into contact with it. Many Mine clearance agencies are also involved in the clearance of war debris.\n\nLand mines in particular are very dangerous as they can remain active for decades after a conflict, which is why they have been banned by international war regulations.\n\nIn November 2006 the Protocol on Explosive Remnants of War\ncame into effect with 92 countries subscribing to the treaty that requires the parties involved in a conflict to assist with the removal of unexploded ordnance following the end of hostilities.\n\nSome of the countries most affected by war debris are Afghanistan, Angola, Cambodia, Iraq and Laos.\n\nSimilarly \"military debris\" may be found in and around firing ranges and military training areas.\n\nDebris can also be used as cover for military purposes, depending on the situation.\n\nIn South Louisiana's Creole and Cajun cultures, debris (pronounced \"DAY-bree\") refers to chopped organs such as liver, heart, kidneys, tripe, spleen, brain, lungs and pancreas.\n\n\n"}
{"id": "13566984", "url": "https://en.wikipedia.org/wiki?curid=13566984", "title": "Double layer (surface science)", "text": "Double layer (surface science)\n\nA double layer (DL, also called an electrical double layer, EDL) is a structure that appears on the surface of an object when it is exposed to a fluid. The object might be a solid particle, a gas bubble, a liquid droplet, or a porous body. The DL refers to two parallel layers of charge surrounding the object. The first layer, the surface charge (either positive or negative), consists of ions adsorbed onto the object due to chemical interactions. The second layer is composed of ions attracted to the surface charge via the Coulomb force, electrically screening the first layer. This second layer is loosely associated with the object. It is made of free ions that move in the fluid under the influence of electric attraction and thermal motion rather than being firmly anchored. It is thus called the \"diffuse layer\".\n\nInterfacial DLs are most apparent in systems with a large surface area to volume ratio, such as a colloid or porous bodies with particles or pores (respectively) on the scale of micrometres to nanometres. However, DLs are important to other phenomena, such as the electrochemical behaviour of electrodes.\n\nDLs play a fundamental role in many everyday substances. For instance, homogenized milk exists only because fat droplets are covered with a DL that prevents their coagulation into butter. DLs exist in practically all heterogeneous fluid-based systems, such as blood, paint, ink and ceramic and cement slurry.\n\nThe DL is closely related to electrokinetic phenomena and electroacoustic phenomena.\n\nWhen an \"electronic\" conductor is brought in contact with a solid or liquid \"ionic\" conductor (electrolyte), a common boundary (interface) among the two phases appears. Hermann von Helmholtz was the first to realize that charged electrodes immersed in electrolyte solutions repel the co-ions of the charge while attracting counterions to their surfaces. Two layers of opposite polarity form at the interface between electrode and electrolyte.\nIn 1853 he showed that an electrical double layer (DL) is essentially a molecular dielectric and stores charge electrostatically. Below the electrolyte's decomposition voltage, the stored charge is linearly dependent on the voltage applied.\n\nThis early model predicted a constant differential capacitance independent from the charge density depending on the dielectric constant of the electrolyte solvent and the thickness of the double-layer.\n\nThis model, with a good foundation for the description of the interface, does not consider important factors including diffusion/mixing of ions in solution, the possibility of adsorption onto the surface and the interaction between solvent dipole moments and the electrode.\n\nLouis Georges Gouy in 1910 and David Leonard Chapman in 1913 both observed that capacitance was not a constant and that it depended on the applied potential and the ionic concentration. The \"Gouy-Chapman model\" made significant improvements by introducing a diffuse model of the DL. In this model the charge distribution of ions as a function of distance from the metal surface allows Maxwell–Boltzmann statistics to be applied. Thus the electric potential decreases exponentially away from the surface of the fluid bulk.\n\nThe Gouy-Chapman model fails for highly charged DLs. In 1924 Otto Stern suggested combining the Helmholtz model with the Gouy-Chapman model: In Stern's model, some ions adhere to the electrode as suggested by Helmholtz, giving an internal Stern layer, while some form a Gouy-Chapman diffuse layer.\n\nThe Stern layer accounts for ions' finite size and consequently an ion's closest approach to the electrode is on the order of the ionic radius. The Stern model has its own limitations, namely that it effectively treats ions as point charges, assumes all significant interactions in the diffuse layer are Coulombic, and assumes dielectric permittivity to be constant throughout the double layer and that fluid viscosity is constant plane.\n\nD. C. Grahame modified the Stern model in 1947. He proposed that some ionic or uncharged species can penetrate the Stern layer, although the closest approach to the electrode is normally occupied by solvent molecules. This could occur if ions lose their solvation shell as they approach the electrode. He called ions in direct contact with the electrode \"specifically adsorbed ions\". This model proposed the existence of three regions. The inner Helmholtz plane (IHP) passes through the centres of the specifically adsorbed ions. The outer Helmholtz plane (OHP) passes through the centres of solvated ions at the distance of their closest approach to the electrode. Finally the diffuse layer is the region beyond the OHP.\n\nIn 1963 J. O'M. Bockris, M. A. V. Devanathan and Klaus Müller proposed the BDM model of the double-layer that included the action of the solvent in the interface. They suggested that the attached molecules of the solvent, such as water, would have a fixed alignment to the electrode surface. This first layer of solvent molecules displays a strong orientation to the electric field depending on the charge. This orientation has great influence on the permittivity of the solvent that varies with field strength. The IHP passes through the centers of these molecules. Specifically adsorbed, partially solvated ions appear in this layer. The solvated ions of the electrolyte are outside the IHP. Through the centers of these ions pass the OHP. The diffuse layer is the region beyond the OHP.\n\nFurther research with double layers on ruthenium dioxide films in 1971 by Sergio Trasatti and Giovanni Buzzanca demonstrated that the electrochemical behavior of these electrodes at low voltages with specific adsorbed ions was like that of capacitors. The specific adsorption of the ions in this region of potential could also involve a partial charge transfer between the ion and the electrode. It was the first step towards understanding pseudocapacitance.\n\nBetween 1975 and 1980 Brian Evans Conway conducted extensive fundamental and development work on ruthenium oxide electrochemical capacitors. In 1991 he described the difference between 'Supercapacitor' and 'Battery' behavior in electrochemical energy storage. In 1999 he coined the term supercapacitor to explain the increased capacitance by surface redox reactions with faradaic charge transfer between electrodes and ions.\n\nHis \"supercapacitor\" stored electrical charge partially in the Helmholtz double-layer and partially as the result of faradaic reactions with \"pseudocapacitance\" charge transfer of electrons and protons between electrode and electrolyte. The working mechanisms of pseudocapacitors are redox reactions, intercalation and electrosorption.\n\nThe physical and mathematical basics of electron charge transfer absent chemical bonds leading to pseudocapacitance was developed by Rudolph A. Marcus. Marcus Theory explains the rates of electron transfer reactions—the rate at which an electron can move from one chemical species to another. It was originally formulated to address outer sphere electron transfer reactions, in which two chemical species change only in their charge, with an electron jumping. For redox reactions without making or breaking bonds, Marcus theory takes the place of Henry Eyring's transition state theory which was derived for reactions with structural changes. Marcus received the Nobel Prize in Chemistry in 1992 for this theory.\n\nThere are detailed descriptions of the interfacial DL in many books on colloid and interface science and microscale fluid transport. There is also a recent IUPAC technical report on the subject of interfacial double layer and related electrokinetic phenomena.\n\nAs stated by Lyklema, \"...the reason for the formation of a \"relaxed\" (\"equilibrium\") double layer is the non-electric affinity of charge-determining ions for a surface...\" This process leads to the buildup of an electric surface charge, expressed usually in C/m. This surface charge creates an electrostatic field that then affects the ions in the bulk of the liquid. This electrostatic field, in combination with the thermal motion of the ions, creates a counter charge, and thus screens the electric surface charge. The net electric charge in this screening diffuse layer is equal in magnitude to the net surface charge, but has the opposite polarity. As a result, the complete structure is electrically neutral.\n\nThe diffuse layer, or at least part of it, can move under the influence of tangential stress. There is a conventionally introduced slipping plane that separates mobile fluid from fluid that remains attached to the surface. Electric potential at this plane is called electrokinetic potential or zeta potential (also denoted as ζ-potential).\n\nThe electric potential on the external boundary of the Stern layer versus the bulk electrolyte is referred to as Stern potential. Electric potential difference between the fluid bulk and the surface is called the electric surface potential.\n\nUsually zeta potential is used for estimating the degree of DL charge. A characteristic value of this electric potential in the DL is 25 mV with a maximum value around 100 mV (up to several volts on electrodes). The chemical composition of the sample at which the ζ-potential is 0 is called the point of zero charge or the iso-electric point. It is usually determined by the solution pH value, since protons and hydroxyl ions are the charge-determining ions for most surfaces.\n\nZeta potential can be measured using electrophoresis, electroacoustic phenomena, streaming potential, and electroosmotic flow.\n\nThe characteristic thickness of the DL is the Debye length, κ. It is reciprocally proportional to the square root of the ion concentration \"C\". In aqueous solutions it is typically on the scale of a few nanometers and the thickness decreases with increasing concentration of the electrolyte.\n\nThe electric field strength inside the DL can be anywhere from zero to over 10 V/m. These steep electric potential gradients are the reason for the importance of the DLs.\n\nThe theory for a flat surface and a symmetrical electrolyte is usually referred to as the Gouy-Chapman theory. It yields a simple relationship between electric charge in the diffuse layer σ and the Stern potential Ψ:\n"}
{"id": "36238152", "url": "https://en.wikipedia.org/wiki?curid=36238152", "title": "Driving factors", "text": "Driving factors\n\nIn energy monitoring and targeting, a driving factor is something recurrent and measurable whose variation explains variation in energy consumption. The term \"independent variable\" is sometimes used as a synonym.\n\nOne of the most common driving factors is the weather, expressed usually as heating or cooling degree days. In energy-intensive processes, production throughputs would usually be used. For electrical circuits feeding outdoor lighting, the number of hours of darkness can be employed. For a borehole pump, the quantity of water delivered would be used; and so on. What these examples all have in common is that on a weekly basis (say) numerical values can be recorded for each factor and one would expect particular streams of energy consumption to correlate with them either singly or in a multi-variate model.\n\nCorrelation is arguably more important than causality. Variation in the driving factor merely has to \"explain\" variation in consumption; it does not necessarily have to \"cause\" it, although that will in most scenarios be the case.\n\nDriving factors differ from \"static\" factors, such as building floor areas, which determine energy consumption but change only rarely (if at all).\n"}
{"id": "944638", "url": "https://en.wikipedia.org/wiki?curid=944638", "title": "Earth's energy budget", "text": "Earth's energy budget\n\nEarth's energy budget accounts for the balance between the energy Earth receives from the Sun, the energy Earth radiates back into outer space after having been distributed throughout the five components of Earth's climate system and having thus powered the so-called Earth’s heat engine. This system is made up of earth's water, ice, atmosphere, rocky crust, and all living things.\n\nQuantifying changes in these amounts is required to accurately model the Earth's climate. \n\nReceived radiation is unevenly distributed over the planet, because the Sun heats equatorial regions more than polar regions. The atmosphere and ocean work non-stop to even out solar heating imbalances through evaporation of surface water, convection, rainfall, winds, and ocean circulation. Earth is very close to being in radiative equilibrium, the situation where the incoming solar energy is balanced by an equal flow of heat to space; under that condition, global temperatures will be \"relatively\" stable. Globally, over the course of the year, the Earth system—land surfaces, oceans, and atmosphere—absorbs and then radiates back to space an average of about 240 watts of solar power per square meter. Anything that increases or decreases the amount of incoming or outgoing energy will change global temperatures in response.\n\nHowever, Earth's energy balance and heat fluxes depend on many factors, such as atmospheric composition (mainly aerosols and greenhouse gases), the albedo (reflectivity) of surface properties, cloud cover and vegetation and land use patterns.\n\nChanges in surface temperature due to Earth's energy budget do not occur instantaneously, due to the inertia of the oceans and the cryosphere. The net heat flux is buffered primarily by becoming part of the ocean's heat content, until a new equilibrium state is established between radiative forcings and the climate response.\n\nIn spite of the enormous transfers of energy into and from the Earth, it maintains a relatively constant temperature because, as a whole, there is little net gain or loss: Earth emits via atmospheric and terrestrial radiation (shifted to longer electromagnetic wavelengths) to space about the same amount of energy as it receives via insolation (all forms of electromagnetic radiation).\n\nTo quantify Earth's \"heat budget\" or \"heat balance\", let the insolation received at the top of the atmosphere be 100 units (100 units = about 1,360 watts per square meter facing the sun), as shown in the accompanying illustration. Called the albedo of Earth, around 35 units are reflected back to space: 27 from the top of clouds, 2 from snow and ice-covered areas, and 6 by other parts of the atmosphere. The 65 remaining units are absorbed: 14 within the atmosphere and 51 by the Earth’s surface. These 51 units are radiated to space in the form of terrestrial radiation: 17 directly radiated to space and 34 absorbed by the atmosphere (19 through latent heat of condensation, 9 via convection and turbulence, and 6 directly absorbed). The 48 units absorbed by the atmosphere (34 units from terrestrial radiation and 14 from insolation) are finally radiated back to space. These 65 units (17 from the ground and 48 from the atmosphere) balance the 65 units absorbed from the sun in order to maintain zero net gain of energy by the Earth.\n\nThe total amount of energy received per second at the top of Earth's atmosphere (TOA) is measured in watts and is given by the solar constant times the cross-sectional area of the Earth. Because the surface area of a sphere is four times the cross-sectional surface area of a sphere (i.e. the area of a circle), the average TOA flux is one quarter of the solar constant and so is approximately 340 W/m². Since the absorption varies with location as well as with diurnal, seasonal and annual variations, the numbers quoted are long-term averages, typically averaged from multiple satellite measurements.\n\nOf the ~340 W/m² of solar radiation received by the Earth, an average of ~77 W/m² is reflected back to space by clouds and the atmosphere and ~23 W/m² is reflected by the surface albedo, leaving ~240 W/m² of solar energy input to the Earth's energy budget. This gives the earth a mean net albedo of 0.29.\n\nThe geothermal heat flux from the Earth's interior is estimated to be 47 terawatts. This comes to 0.087 watt/square metre, which represents only 0.027% of Earth's total energy budget at the surface, which is dominated by 173,000 terawatts of incoming solar radiation.\n\nHuman production of energy is even lower, at an estimated 18 TW.\n\nPhotosynthesis has a larger effect: photosynthetic efficiency turns up to 2% of incoming sunlight into biomass, for a total photosynthetic productivity of earth between ~1500–2250 TW (~1%+/-0.26% solar energy hitting the Earth's surface).\n\nOther minor sources of energy are usually ignored in these calculations, including accretion of interplanetary dust and solar wind, light from stars other than the Sun and the thermal radiation from space. Earlier, Joseph Fourier had claimed that deep space radiation was significant in a paper often cited as the first on the greenhouse effect.\n\nLongwave radiation is usually defined as outgoing infrared energy leaving the planet. However, the atmosphere absorbs parts initially, or cloud cover can reflect radiation. Generally, heat energy is transported between the planet's surface layers (land and ocean) to the atmosphere, transported via evapotranspiration and latent heat fluxes or conduction/convection processes. Ultimately, energy is radiated in the form of longwave infrared radiation back into space.\n\nRecent satellite observations indicate additional precipitation, which is sustained by increased energy leaving the surface through evaporation (the latent heat flux), offsetting increases in longwave flux to the surface.\n\nIf the incoming energy flux is not equal to the outgoing energy flux, net heat is added to or lost by the planet (if the incoming flux is larger or smaller than the outgoing respectively).\n\nAn imbalance must show in something on Earth warming or cooling (depending on the direction of the imbalance), and the ocean being the larger thermal reservoir on Earth, is a prime candidate for measurements.\n\nEarth's energy imbalance measurements provided by Argo floats have detected an accumulation of ocean heat content (OHC). The estimated imbalance was measured during a deep solar minimum of 2005–2010 to be 0.58 ± 0.15 W/m². This level of detail cannot be inferred directly from measurements of surface energy fluxes, which have combined uncertainties of the order of ± 17 W/m².\n\nSeveral satellites indirectly measure the energy absorbed and radiated by Earth and by inference the energy imbalance. The NASA Earth Radiation Budget Experiment (ERBE) project involves three such satellites: the Earth Radiation Budget Satellite (ERBS), launched October 1984; NOAA-9, launched December 1984; and NOAA-10, launched September 1986.\n\nToday NASA's satellite instruments, provided by CERES, part of the NASA's Earth Observing System (EOS), are designed to measure both solar-reflected and Earth-emitted radiation.\n\nThe major atmospheric gases (oxygen and nitrogen) are transparent to incoming sunlight but are also transparent to outgoing thermal (infrared) radiation. However, water vapor, carbon dioxide, methane and other trace gases are opaque to many wavelengths of thermal radiation. The Earth's surface radiates the net equivalent of 17 percent of the incoming solar energy in the form of thermal infrared. However, the amount that directly escapes to space is only about 12 percent of incoming solar energy. The remaining fraction, 5 to 6 percent, is absorbed by the atmosphere by greenhouse gas molecules.\nWhen greenhouse gas molecules absorb thermal infrared energy, their temperature rises. Those gases then radiate an increased amount of thermal infrared energy in all directions. Heat radiated upward continues to encounter greenhouse gas molecules; those molecules also absorb the heat, and their temperature rises and the amount of heat they radiate increases. The atmosphere thins with altitude, and at roughly 5–6 kilometres, the concentration of greenhouse gases in the overlying atmosphere is so thin that heat can escape to space.\n\nBecause greenhouse gas molecules radiate infrared energy in all directions, some of it spreads downward and ultimately returns to the Earth's surface, where it is absorbed. The Earth's surface temperature is thus higher than it would be if it were heated only by direct solar heating. This supplemental heating is the natural greenhouse effect. It is as if the Earth is covered by a blanket that allows high frequency radiation (sunlight) to enter, but slows the rate at which the low frequency infrared radiant energy emitted by the Earth leaves.\n\nA change in the incident radiated portion of the energy budget is referred to as a radiative forcing.\n\nClimate sensitivity is the steady state change in the equilibrium temperature as a result of changes in the energy budget.\n\nClimate forcings are changes that cause temperatures to rise or fall, disrupting the energy balance. Natural climate forcings include changes in the Sun's brightness, Milankovitch cycles (small variations in the shape of Earth's orbit and its axis of rotation that occur over thousands of years) and volcanic eruptions that inject light-reflecting particles as high as the stratosphere. Man-made forcings include particle pollution (aerosols) that absorb and reflect incoming sunlight; deforestation, which changes how the surface reflects and absorbs sunlight; and the rising concentration of atmospheric carbon dioxide and other greenhouse gases, which decreases the rate at which heat is radiated to space.\n\nA forcing can trigger feedbacks that intensify (positive feedback) or weaken (negative feedback) the original forcing. For example, loss of ice at the poles, which makes them less reflective, causes greater absorption of energy and so increases the rate at which the ice melts, is an example of a positive feedback.\n\nThe observed planetary energy imbalance during the recent solar minimum shows that solar forcing of climate, although natural and significant, is overwhelmed by anthropogenic climate forcing.\n\nIn 2012, NASA scientists reported that to stop global warming atmospheric CO content would have to be reduced to 350 ppm or less, assuming all other climate forcings were fixed. The impact of anthropogenic aerosols has not been quantified, but individual aerosol types are thought to have substantial heating and cooling effects.\n\n\n"}
{"id": "40159918", "url": "https://en.wikipedia.org/wiki?curid=40159918", "title": "Ecosystem health", "text": "Ecosystem health\n\nEcosystem health is a metaphor used to describe the condition of an ecosystem. Ecosystem condition can vary as a result of fire, flooding, drought, extinctions, invasive species, climate change, mining, overexploitation in fishing, farming or logging, chemical spills, and a host of other reasons. There is no universally accepted benchmark for a healthy ecosystem, rather the apparent health status of an ecosystem can vary depending upon which health metrics are employed in judging it and which societal aspirations are driving the assessment. Advocates of the health metaphor argue for its simplicity as a communication tool. \"Policy-makers and the public need simple, understandable concepts like health.\" Critics worry that ecosystem health, a \"value-laden construct\", is often \"passed off as science to unsuspecting policy makers and the public.\"\n\nThe health metaphor applied to the environment has been in use at least since the early 1800s and the great American conservationist Aldo Leopold (1887–1948) spoke metaphorically of land health, land sickness, mutilation, and violence when describing land use practices. The term \"ecosystem management\" has been in use at least since the 1950s. The term \"ecosystem health\" has become widespread in the ecological literature, as a general metaphor meaning something good, and as an environmental quality goal in field assessments of rivers, lakes, seas, and forests.\n\nRecently however this metaphor has been subject of quantitative formulation using complex systems concepts such as criticality, meaning that a healthy ecosystem is in some sort of balance between adaptability (randomness) and robustness (order) . Nevertheless the universality of criticality is still under examination and is known as the Criticality Hypothesis, which states that systems in a dynamic regime shifting between order and disorder, attain the highest level of computational capabilities and achieve an optimal trade-off between robustness and flexibility. Recent results in cell and evolutionary biology, neuroscience and computer science have great interest in the criticality hypothesis, emphasizing its role as a viable candidate general law in the realm of adaptive complex systems (see and references therein).\n\nThe term ecosystem health has been employed to embrace some suite of environmental goals deemed desirable. Edward Grumbine's highly cited paper \"What is ecosystem management?\" surveyed ecosystem management and ecosystem health literature and summarized frequently encountered goal statements:\n\nGrumbine describes each of these goals as a \"value statement\" and stresses the role of human values in setting ecosystem management goals.\n\nIt is the last goal mentioned in the survey, accommodating humans, that is most contentious. \"We have observed that when groups of stakeholders work to define … visions, this leads to debate over whether to emphasize ecosystem health or human well-being … Whether the priority is ecosystems or people greatly influences stakeholders' assessment of desirable ecological and social states.\" and, for example, \"For some, wolves are critical to ecosystem health and an essential part of nature, for others they are a symbol of government overreach threatening their livelihoods and cultural values.\"\n\nMeasuring ecosystem health requires extensive goal-driven environmental sampling. For example, a vision for ecosystem health of Lake Superior was developed by a public forum and a series of objectives were prepared for protection of habitat and maintenance of populations of some 70 indigenous fish species. A suite of 80 lake health indicators was developed for the Great Lakes Basin including monitoring native fish species, exotic species, water levels, phosphorus levels, toxic chemicals, phytoplankton, zooplankton, fish tissue contaminants, etc.\n\nSome authors have attempted broad definitions of ecosystem health, such as benchmarking as healthy the historical ecosystem state \"prior to the onset of anthropogenic stress.\" A difficulty is that the historical composition of many human-altered ecosystems is unknown or unknowable. Also, fossil and pollen records indicate that the species that occupy an ecosystem reshuffle through time, so it is difficult to identify one snapshot in time as optimum or \"healthy.\".\n\nA commonly cited broad definition states that a healthy ecosystem has three attributes:\n\nWhile this captures significant ecosystem properties, a generalization is elusive as those properties do not necessarily co-vary in nature. For example, there is not necessarily a clear or consistent relationship between productivity and species richness. Similarly, the relationship between resilience and diversity is complex, and ecosystem stability may depend upon one or a few species rather than overall diversity. And some undesirable ecosystems are highly productive.\n\n\"Resilience is not desirable per se. There can be highly resilient states of ecosystems which are very undesirable from some human perspectives , such as algal-dominated coral reefs.\" Ecological resilience is a \"capacity\" that varies depending upon which properties of the ecosystem are to be studied and depending upon what kinds of disturbances are considered and how they are to be quantified. Approaches to assessing it \"face high uncertainties and still require a considerable amount of empirical and theoretical research.\"\n\nOther authors have sought a numerical index of ecosystem health that would permit quantitative comparisons among ecosystems and within ecosystems over time. One such system employs ratings of the three properties mentioned above: Health = system vigor x system organization x system resilience. Ecologist Glenn Suter argues that such indices employ \"nonsense units,\" the indices have \"no meaning; they cannot be predicted, so they are not applicable to most regulatory problems; they have no diagnostic power; effects of one component are eclipsed by responses of other components, and the reason for a high or low index value is unknown.\"\n\nHealth metrics are determined by stakeholder goals, which drive ecosystem definition. An ecosystem is an abstraction. \"Ecosystems cannot be identified or found in nature. Instead, they must be delimited by an observer. This can be done in many different ways for the same chunk of nature, depending on the specific perspectives of interest.\"\n\nEcosystem definition determines the acceptable range of variability (reference conditions) and determines measurement variables. The latter are used as indicators of ecosystem structure and function, and can be used as indicators of \"health\".\n\nAn indicator is a variable, such as a chemical or biological property, that when measured, is used to infer trends in another (unmeasured) environmental variable or cluster of unmeasured variables (the indicandum). For example, rising mortality rate of canaries in a coal mine is an indicator of rising carbon monoxide levels. Rising chlorophyll-a levels in a lake may signal eutrophication.\n\nEcosystem assessments employ two kinds of indicators, descriptive indicators and normative indicators. \"Indicators can be used descriptively for a scientific purpose or normatively for a political purpose.\"\n\nUsed descriptively, high chlorophyll-a is an indicator of eutrophication, but it may also be used as an ecosystem health indicator. When used as a normative (health) indicator, it indicates a rank on a health scale, a rank that can vary widely depending on societal preferences as to what is desirable. A high chlorophyll-a level in a natural successional wetland might be viewed as healthy whereas a human-impacted wetland with the \"same\" indicator value may be judged unhealthy.\n\nEstimation of ecosystem health has been criticized for intermingling the two types of environmental indicators. A health indicator is a normative indicator, and if conflated with descriptive indicators \"implies that normative values can be measured objectively, which is certainly not true. Thus, implicit values are insinuated to the reader, a situation which has to be avoided.\"\n\nIt can be argued that the very act of selecting indicators of any kind is biased by the observer's perspective but separation of goals from descriptions has been advocated as a step toward transparency: \"A separation of descriptive and normative indicators is essential from the perspective of the philosophy of science … Goals and values cannot be deduced directly from descriptions … a fact that is emphasized repeatedly in the literature of environmental ethics … Hence, we advise always specifying the definition of indicators and propose clearly distinguishing ecological indicators in science from policy indicators used for decision-making processes.\"\n\nAnd integration of multiple, possibly conflicting, normative indicators into a single measure of \"ecosystem health\" is problematic. Using 56 indicators, \"determining environmental status and assessing marine ecosystems health in an integrative way is still one of the grand challenges in marine ecosystems ecology, research and management\"\n\nAnother issue with indicators is validity. Good indicators must have an independently validated high predictive value, that is high sensitivity (high probability of indicating a significant change in the indicandum) and high specificity (low probability of wrongly indicating a change). The reliability of various health metrics has been questioned and \"what combination of measurements should be used to evaluate ecosystems is a matter of current scientific debate.\" Most attempts to identify ecological indicators have been correlative rather than derived from prospective testing of their predictive value and the selection process for many indicators has been based upon weak evidence or has been lacking in evidence.\n\nIn some cases no reliable indicators are known: \"We found no examples of invertebrates successfully used in [forest] monitoring programs. Their richness and abundance ensure that they play significant roles in ecosystem function but thwart focus on a few key species.\" And, \"Reviews of species-based monitoring approaches reveal that no single species, nor even a group of species, accurately reflects entire communities. Understanding the response of a single species may not provide reliable predictions about a group of species even when the group is a few very similar species.\"\n\nA trade-off between human health and the \"health\" of nature has been termed the \"health paradox\" and it illuminates how human values drive perceptions of ecosystem health.\n\nHuman health has benefited by sacrificing the \"health\" of wild ecosystems, such as dismantling and damming of wild valleys, destruction of mosquito-bearing wetlands, diversion of water for irrigation, conversion of wilderness to farmland, timber removal, and extirpation of tigers, whales, ferrets, and wolves.\n\nThere has been an acrimonious schism among conservationists and resource managers over the question of whether to \"ratchet back human domination of the biosphere\" or whether to embrace it. These two perspectives have been characterized as utilitarian vs protectionist.\n\nThe utilitarian view treats human health and well-being as criteria of ecosystem health. For example, destruction of wetlands to control malaria mosquitoes \"resulted in an improvement in ecosystem health.\"\nThe protectionist view treats humans as an invasive species: \"If there was ever a species that qualified as an invasive pest, it is \"Homo sapiens\",\"\n\nProponents of the utilitarian view argue that \"healthy ecosystems are characterized by their capability to sustain healthy human populations,\" and \"healthy ecosystems must be economically viable,\" as it is \"unhealthy\" ecosystems that are likely to result in increases in contamination, infectious diseases, fires, floods, crop failures and fishery collapse.\n\nProtectionists argue that privileging of human health is a conflict of interest as humans have demolished massive numbers of ecosystems to maintain their welfare, also disease and parasitism are historically normal in pre-industrial nature. Diseases and parasites promote ecosystem functioning, driving biodiversity and productivity, and parasites may constitute a significant fraction of ecosystem biomass.\n\nThe very choice of the word \"health\" applied to ecology has been questioned as lacking in neutrality in a BioScience article on responsible use of scientific language: \"Some conservationists fear that these terms could endorse human domination of the planet … and could exacerbate the shifting cognitive baseline whereby humans tend to become accustomed to new and often degraded ecosystems and thus forget the nature of the past.\"\n\nCriticism of ecosystem health largely targets the failure of proponents to explicitly distinguish the normative dimension from the descriptive dimension, and has included the following:\n\nAlternatives have been proposed for the term ecosystem health, including more neutral language such as ecosystem status, ecosystem prognosis, and ecosystem sustainability. Another alternative to the use of a health metaphor is to \"express exactly and clearly the public policy and the management objective\", to employ habitat descriptors and real properties of ecosystems. An example of a policy statement is \"The maintenance of viable natural populations of wildlife and ecological functions always takes precedence over any human use of wildlife.\" An example of a goal is \"Maintain viable populations of all native species in situ.\" An example of a management objective is \"Maintain self-sustaining populations of lake whitefish within the range of abundance observed during 1990-99.\"\n\nKurt Jax presented an ecosystem assessment format that avoids imposing a preconceived notion of normality, that avoids the muddling of normative and descriptive, and that gives serious attention to ecosystem definition. (1) Societal purposes for the ecosystem are negotiated by stakeholders, (2) a functioning ecosystem is defined with emphasis on phenomena relevant to stakeholder goals, (3) benchmark reference conditions and permissible variation of the system are established, (4) measurement variables are chosen for use as indicators, and (5) the time scale and spatial scale of assessment are decided.\n\nEcological health has been used as a medical term in reference to human allergy and multiple chemical sensitivity and as a public health term for programs to modify health risks (diabetes, obesity, smoking, etc.). Human health itself, when viewed in its broadest sense, is viewed as having ecological foundations. It is also an urban planning term in reference to \"green\" cities (composting, recycling), and has been used loosely with regard to various environmental issues, and as the condition of human-disturbed environmental sites. Ecosystem integrity implies a condition of an ecosystem exposed to a minimum of human influence. Ecohealth is the relationship of human health to the environment, including the effect of climate change, wars, food production, urbanization, and ecosystem structure and function. Ecosystem management and ecosystem-based management refer to the sustainable management of ecosystems and in some cases may employ the terms ecosystem health or ecosystem integrity as a goal.\n"}
{"id": "7900498", "url": "https://en.wikipedia.org/wiki?curid=7900498", "title": "Energy being", "text": "Energy being\n\nAn energy being or astral being is a theoretical life form that is composed of energy rather than matter. They appear in myths/legends, paranormal/UFO accounts, and in various works of speculative fiction.\n\nEnergy beings are typically rendered as a translucent glowing fluid or as a collection of flames or electrical sparks or bolts; somewhat in common with the representations of ghosts.\n\nEnergy beings have a variety of capacities. The Taelons (from \"\") are barely more powerful than mortals, while others such as \"Star Trek\"s Q, \"Stargate SG-1\"s Ascended Ancients/Ori, \"\"s Anodites, or the Meekrob from \"Invader Zim\" possess god-like powers.\n\n\n"}
{"id": "38328166", "url": "https://en.wikipedia.org/wiki?curid=38328166", "title": "Energy broker", "text": "Energy broker\n\nEnergy brokers assist clients in procuring electric or natural gas from energy wholesalers/suppliers. Since electricity and natural gas are commodities, prices change daily with the market. It is challenging for most businesses without energy managers to obtain price comparisons from a variety of suppliers since prices must be compared on exactly the same day. In addition, the terms of the particular contract offered by the supplier influences the price that is quoted. An energy broker can provide a valuable service if they work with a large number of suppliers and can actually compile the sundry prices from suppliers. An important aspect of this consulting role is to assure that the client understands the differences between the contract offers. Under some State Laws they use the term \"Suppliers\" to refer to energy suppliers, brokers, and aggregators, however there are very important differences between them all.\n\nEnergy brokers do not own or distribute energy, nor are allowed to sell energy directly to you. They simply present the rates of a wholesaler, or supplier.\n\nEnergy consultants offer a lot more than procuring energy contracts from a supplier. In the UK and Europe where there is a lot of legislation and increasing pressure for businesses and countries to do more to reduce their energy consumption a lot of services from brokers now help ensure businesses meet a lot of compliance and accreditation requirements such as the ESOS (energy saving opportunity scheme), ISO 50001, ISO 14001, Energy Performance Certificates and Display Energy Certificates.\nOther services include helping companies reduce energy consumption with the aim of meeting national and international carbon emission standards. Services include, energy health checks, energy audits, carbon zero, carbon offsetting and energy saving consulting.\n\nAdditional services such as arranging a power purchase agreement, energy export contracts can be procured as well as energy monitoring and reporting technology and solutions are also offered by energy consultants.\n\nIn the USA, energy brokers can serve residential, commercial and government entities that reside in energy deregulated states. In the UK, and some countries in Europe, the entire market is deregulated.\n\nEnergy brokers typically do not charge up front fees to provide rates. If an entity purchases energy through a broker, the broker's fee is usually included in the rate the customer pays. Some brokers will charge a fixed fee for their consulting services.\n\nNot all energy brokers are consultants; However, the energy brokers who are also consultants will perform a more detailed analysis of a consumers' usage pattern in order to provide a custom rate, which typically results in more cost savings for the consumer. Typically, they do not need any more information than that of an energy broker, because they can pull usage information from the local utility company. There are some national energy brokers that use auditing teams to verify their client's invoices.\n"}
{"id": "321382", "url": "https://en.wikipedia.org/wiki?curid=321382", "title": "Energy flow (ecology)", "text": "Energy flow (ecology)\n\nIn ecology, energy flow, also called the calorific flow, refers to the flow of energy through a food chain, and is the focus of study in ecological energetics. In an ecosystem, ecologists seek to quantify the relative importance of different component species and feeding relationships.\n\nA general energy flow scenario follows:\n\nThe energy is passed on from trophic level to trophic level and each time about 90% of the energy is lost, with some being lost as heat into the environment (an effect of respiration) and some being lost as incompletely digested food (egesta). Therefore, primary consumers get about 10% of the energy produced by autotrophs, while secondary consumers get 1% and tertiary consumers get 0.1%. This means the top consumer of a food chain receives the least energy, as a lot of the food chain's energy has been lost between trophic levels. This loss of energy at each level limits typical food chains to only four to six links.\n\nEcological energetics appears to have grown out of the Age of Enlightenment and the concerns of the Physiocrats. It began in the works of Sergei Podolinsky in the late 1800s, and subsequently was developed by the Soviet ecologist Vladmir Stanchinsky, the Austro-American Alfred J. Lotka, and American limnologists, Raymond Lindeman and G. Evelyn Hutchinson. It underwent substantial development by Howard T. Odum and was applied by systems ecologists, and radiation ecologists.\n\n\n"}
{"id": "43699304", "url": "https://en.wikipedia.org/wiki?curid=43699304", "title": "Energy rate density", "text": "Energy rate density\n\nEnergy rate density is the amount of free energy per unit time per unit mass (in CGS metric units erg/s/g; in MKS units joule/s/kg). It is terminologically (but not always numerically) equivalent to power density when measured in SI units of W/kg. Regardless of the units used, energy rate density describes the flow of energy through any system of given mass, and has been proposed as a measure of system complexity. The more complex the system, the more energy flows per second through each gram. \n\nEnergy rate density is actually a general term that is equivalent to more specialized terms used by many different disciplinary scientists. For example, in astronomy it is called the luminosity-to-mass ratio (the inverse of the mass-luminosity ratio), in physics the power density, in geology the specific radiant flux (where “specific” denotes per unit mass), in biology the specific metabolic rate, and in engineering the power-to-weight ratio. Interdisciplinary researchers prefer to use the general term, energy rate density, not only to stress the intuitive notion of energy flow (in contrast to more colloquial connotations of the word \"power\"), but also to unify its potential application among all the natural sciences, as in the cosmology of cosmic evolution. When the energy rate density for systems including our galaxy, sun, earth, plants, animals, society are plotted according to when, in historical time, they first emerged, a clear increase in energy rate density over time is observed. \n\nThis term has in recent years gained many diverse applications in various disciplines, including history, cosmology, economics, philosophy, and behavioral biology. \n\n\n"}
{"id": "52058583", "url": "https://en.wikipedia.org/wiki?curid=52058583", "title": "Energy system", "text": "Energy system\n\nAn energy system is a system primarily designed to supply energy-services to end-users. Taking a structural viewpoint, the IPCC Fifth Assessment Report defines an energy system as \"all components related to the production, conversion, delivery, and use of energy\". The field of energy economics includes energy markets and treats an energy system as the technical and economic systems that satisfy consumer demand for energy in the forms of heat, fuels, and electricity.\n\nThe first two definitions allow for demand-side measures, including daylighting, retrofitted building insulation, and passive solar building design, as well as socio-economic factors, such as aspects of energy demand management and even telecommuting, while the third does not. Neither does the third account for the informal economy in traditional biomass that is significant in many developing countries.\n\nThe analysis of energy systems thus spans the disciplines of engineering and economics. Merging ideas from both areas to form a coherent description, particularly where macroeconomic dynamics are involved, is challenging.\n\nThe concept of an energy system is evolving as new regulations, technologies, and practices enter into service – for example, emissions trading, the development of smart grids, and the greater use of energy demand management, respectively.\n\nFrom a structural perspective, an energy system is like any general system and is made up of a set of interacting component parts, located within an environment. These components derive from ideas found in engineering and economics. Taking a process view, an energy system \"consists of an integrated set of technical and economic activities operating within a complex societal framework\". The identification of the components and behaviors of an energy system depends on the circumstances, the purpose of the analysis, and the questions under investigation. The concept of an energy system is therefore an abstraction which usually precedes some form of computer-based investigation, such as the construction and use of a suitable energy model.\n\nViewed in engineering terms, an energy system lends itself to representation as a flow network: the vertices map to engineering components like power stations and pipelines and the edges map to the interfaces between these components. This approach allows collections of similar or adjacent components to be aggregated and treated as one to simplify the model. Once described thus, flow network algorithms, such as minimum cost flow, may be applied. The components themselves can be treated as simple dynamical systems in their own right.\n\nConversely, relatively pure economic modeling may adopt a sectorial approach with only limited engineering detail present. The sector and sub-sector categories published by the International Energy Agency are often used as a basis for this analysis. A 2009 study of the UK residential energy sector contrasts the use of the technology-rich Markal model with several UK sectoral housing stock models.\n\nInternational energy statistics are typically broken down by carrier, sector and sub-sector, and country. Energy carriers ( energy products) are further classified as primary energy and secondary (or intermediate) energy and sometimes final (or end-use) energy. Published energy datasets are normally adjusted so that they are internally consistent, meaning that all energy stocks and flows must balance. The IEA regularly publishes energy statistics and energy balances with varying levels of detail and cost and also offers mid-term projections based on this data. The notion of an energy carrier, as used in energy economics, is distinct and different from the definition of energy used in physics.\n\nEnergy systems can range in scope, from local, municipal, national, and regional, to global, depending on issues under investigation. Researchers may or may not include demand side measures within their definition of an energy system. The IPCC does so, for instance, but covers these measures in separate chapters on transport, buildings, industry, and agriculture.\n\nHousehold consumption and investment decisions may also be included within the ambit of an energy system. Such considerations are not common because consumer behavior is difficult to characterize, but the trend is to include human factors in models. Household decision-taking may be represented using techniques from bounded rationality and agent-based behavior. The American Association for the Advancement of Science (AAAS) specifically advocates that \"more attention should be paid to incorporating behavioral considerations other than price- and income-driven behavior into economic models [of the energy system]\".\n\nThe concept of an energy-service is central, particularly when defining the purpose of an energy system:\n\nEnergy-services can be defined as amenities that are either furnished through energy consumption or could have been thus supplied. More explicitly:\n\nA consideration of energy-services per capita and how such services contribute to human welfare and individual quality of life is paramount to the debate on sustainable energy. People living in poor regions with low levels of energy-services consumption would clearly benefit from greater consumption, but the same is not generally true for those with high levels of consumption.\n\nThe notion of energy-services has given rise to energy-service companies (ESCo) who contract to provide energy-services to a client for an extended period. The ESCo is then free to choose the best means to do so, including investments in the thermal performance and HVAC equipment of the buildings in question.\n\nISO13600, ISO13601, and ISO13602 form a set of international standards covering technical energy systems (TES). Although withdrawn prior to 2016, these documents provide useful definitions and a framework for formalizing such systems. The standards depict an energy system broken down into supply and demand sectors, linked by the flow of tradable energy commodities (or energywares). Each sector has a set of inputs and outputs, some intentional and some harmful byproducts. Sectors may be further divided into subsectors, each fulfilling a dedicated purpose. The demand sector is ultimately present to supply energyware-based services to consumers (see energy-services).\n\n"}
{"id": "32974036", "url": "https://en.wikipedia.org/wiki?curid=32974036", "title": "Fowkes hypothesis", "text": "Fowkes hypothesis\n\nThe Fowkes hypothesis (after F. M. Fowkes) is a first order approximation for surface energy. It states the surface energy is the sum of each component's forces:\nγ=γ+γ+γ+...\nwhere γ is the dispersion component, γ is the polar, γ is the dipole and so on.\n\nThe Fowkes hypothesis goes further making the approximation that the interface between an apolar liquid and apolar solid where there are only dispersive interactions acting across the interface can be estimated using the geometric mean of the contributions from each surface i.e.\n\nγ=γ+γ-2(γ x γ)\n\n\n"}
{"id": "7371308", "url": "https://en.wikipedia.org/wiki?curid=7371308", "title": "Graphical timeline from Big Bang to Heat Death", "text": "Graphical timeline from Big Bang to Heat Death\n\nThis is the timeline of the Universe from Big Bang to Heat Death scenario. The different eras of the universe are shown. The heat death will occur in 10</sup> years, if protons decay.\n\nUsually the logarithmic scale is used for such timelines but it compresses the most interesting Stelliferous Era too much as this example shows. Therefore, a double-logarithmic scale \"s\" (\"s*100\" in the graphics) is used instead. The minimum of it is only 1, not 0 as needed, and the negative outputs for inputs smaller than 10 are useless. Therefore, the time from 0.1 to 10 years is collapsed to a single point 0, but that does not matter in this case because nothing special happens in the history of the universe during that time.\n\nThe seconds in the timescale have been converted to years by formula_2 using the Julian year.\n"}
{"id": "1471037", "url": "https://en.wikipedia.org/wiki?curid=1471037", "title": "Graphical timeline of the Big Bang", "text": "Graphical timeline of the Big Bang\n\nThis timeline of the Big Bang shows a sequence of events as currently theorized by scientists. \n\nIt is a logarithmic scale that shows formula_1 \"second\" instead of \"second\". For example, one microsecond is formula_2. To convert −30 read on the scale to second calculate formula_3 second = one millisecond. On a logarithmic time scale a step lasts ten times longer than the previous step.\n\n"}
{"id": "1471036", "url": "https://en.wikipedia.org/wiki?curid=1471036", "title": "Graphical timeline of the universe", "text": "Graphical timeline of the universe\n\nThis more than 20-billion-year timeline of our universe shows the best estimates of major events from the universe's beginning to anticipated future events. Zero on the scale is the present day. A large step on the scale is one billion years; a small step, one hundred million years. The past is denoted by a minus sign: e.g., the oldest rock on Earth was formed about four billion years ago and this is marked at -4e+09 years, where 4e+09 represents 4 times 10 to the power of 9. The \"Big Bang\" event most likely happened 13.8 billion years ago; see age of the universe.\n\n"}
{"id": "180236", "url": "https://en.wikipedia.org/wiki?curid=180236", "title": "Greisen–Zatsepin–Kuzmin limit", "text": "Greisen–Zatsepin–Kuzmin limit\n\nThe Greisen–Zatsepin–Kuzmin limit (GZK limit) is a theoretical upper limit on the energy of cosmic ray protons traveling from other galaxies through the intergalactic medium to our galaxy. The limit is , or about 8 joules. The limit is set by slowing-interactions of the protons with the microwave background radiation over long distances (~160 million light-years). The limit is at the same order of magnitude as the upper limit for energy at which cosmic rays have experimentally been detected. For example, one extreme-energy cosmic ray has been detected which appeared to possess a record (50 joules) of energy (about the same as the kinetic energy of a 35 mph baseball).\n\nThe GZK limit is derived under the assumption that ultra-high energy cosmic rays are protons. Measurements by the largest cosmic-ray observatory, the Pierre Auger Observatory, suggest that most ultra-high energy cosmic rays are heavier elements. In this case, the argument behind the GZK limit does not apply in the originally simple form and there is no fundamental contradiction in observing cosmic rays with energies that violate the limit.\n\nIn the past, the apparent violation of the GZK limit has inspired cosmologists and theoretical physicists to suggest other ways that circumvent the limit. These theories propose that ultra-high energy cosmic rays are produced nearby our galaxy or that Lorentz covariance is violated in such a way that protons do not lose energy on their way to our galaxy.\n\nThe limit was independently computed in 1966 by Kenneth Greisen, Vadim Kuzmin, and Georgiy Zatsepin, based on interactions between cosmic rays and the photons of the cosmic microwave background radiation (CMB). They predicted that cosmic rays with energies over the threshold energy of would interact with cosmic microwave background photons formula_1, relatively blueshifted by the speed of the cosmic rays, to produce pions via the formula_2 resonance,\n\nor\n\nPions produced in this manner proceed to decay in the standard pion channels—ultimately to photons for neutral pions, and photons, positrons, and various neutrinos for positive pions. Neutrons decay also to similar products, so that ultimately the energy of any cosmic ray proton is drained off by production of high energy photons plus (in some cases) high energy electron/positron pairs and neutrino pairs.\n\nThe pion production process begins at a higher energy than ordinary electron-positron pair production (lepton production) from protons impacting the CMB, which starts at cosmic ray proton energies of only about . However, pion production events drain 20% of the energy of a cosmic ray proton as compared with only 0.1% of its energy for electron positron pair production. This factor of 200 is from two sources: the pion has only about ~130 times the mass of the leptons, but the extra energy appears as different kinetic energies of the pion or leptons, and results in relatively more kinetic energy transferred to a heavier product pion, in order to conserve momentum. The much larger total energy losses from pion production result in the pion production process becoming the limiting one to high energy cosmic ray travel, rather than the lower-energy light-lepton production process.\n\nThe pion production process continues until the cosmic ray energy falls below the pion production threshold. Due to the mean path associated with this interaction, extragalactic cosmic rays traveling over distances larger than () and with energies greater than this threshold should never be observed on Earth. This distance is also known as GZK horizon.\n\nA number of observations have been made by the largest cosmic ray experiments Akeno Giant Air Shower Array, High Resolution Fly's Eye Cosmic Ray Detector, the Pierre Auger Observatory and Telescope Array Project that appeared to show cosmic rays with energies above this limit (called extreme-energy cosmic rays, or EECRs). The observation of these particles was the so-called GZK paradox or cosmic ray paradox.\n\nThese observations appear to contradict the predictions of special relativity and particle physics as they are presently understood. However, there are a number of possible explanations for these observations that may resolve this inconsistency.\n\nAnother suggestion involves ultra-high energy weakly interacting particles (for instance, neutrinos) which might be created at great distances and later react locally to give rise to the particles observed. In the proposed Z-burst model, an ultra-high energy cosmic neutrino collides with a relic anti-neutrino in our galaxy and annihilates to hadrons. This process proceeds via a (virtual) Z-boson:\n\nformula_8\n\nThe cross section for this process becomes large if the center of mass energy of the neutrino antineutrino pair is equal to the Z-boson mass (such a peak in the cross section is called \"resonance\"). Assuming that the relic anti-neutrino is at rest, the energy of the incident cosmic neutrino has to be:\n\nformula_9\n\nwhere formula_10 is the mass of the Z-boson and formula_11 the mass of the neutrino.\n\nA number of exotic theories have been advanced to explain the AGASA observations, including doubly special relativity. However, it is now established that standard doubly special relativity does not predict any GZK suppression (or GZK cutoff), contrary to models of Lorentz symmetry violation involving an absolute rest frame. Other possible theories involve a relation with dark matter, decays of exotic super-heavy particles beyond those known in the Standard Model.\n\nA suppression of the cosmic ray flux which can be explained with the GZK limit has been confirmed by the latest generation of cosmic ray observatories. A former claim by the AGASA experiment that there is no suppression was overruled. It remains controversial, whether the suppression is due to the GZK effect. The GZK limit only applies if ultra-high energy cosmic rays are mostly protons.\n\nIn July 2007, during the 30th International Cosmic Ray Conference in Mérida, Yucatán, México, the High Resolution Fly's Eye Experiment (HiRes) and the Pierre Auger Observatory (Auger) presented their results on ultra-high-energy cosmic rays. HiRes observed a suppression in the UHECR spectrum at just the right energy, observing only 13 events with an energy above the threshold, while expecting 43 with no suppression. This was interpreted as the first observation of the GZK limit. Auger confirmed the flux suppression, but did not claim it to be the GZK limit: instead of the 30 events necessary to confirm the AGASA results, Auger saw only two, which are believed to be heavy nuclei events. The flux suppression was previously brought into question when the AGASA experiment found no suppression in their spectrum. According to Alan Watson, spokesperson for the Auger Collaboration, AGASA results have been shown to be incorrect, possibly due to the systematic shift in energy assignment.\n\nIn 2010 and the following years, both the Pierre Auger Observatory and HiRes confirmed again a flux suppression, in case of the Pierre Auger Observatory the effect is statistically significant at the level of 20 standard deviations.\n\nAfter the flux suppression was established, a heated debate ensued whether cosmic rays that violate the GZK limit are protons. The Pierre Auger Observatory, the world's largest observatory, found with high statistical significance that ultra-high energy cosmic rays are not purely protons, but a mixture of elements which is getting heavier with increasing energy.\nThe Telescope Array Project, a joint effort from members of the HiRes and AGASA collaborations, agrees with the former HiRes result that these cosmic rays look like protons. The claim is based on data with lower statistical significance, however. The area covered by Telescope Array is about one third of the area covered by the Pierre Auger Observatory, and the latter has been running for a longer time.\n\nThe controversy was partially resolved in 2017, when a joint working group formed by members of both experiments presented a report at the 35th International Cosmic Ray Conference. According to the report, the raw experimental results are not in contradiction with each other. The different interpretations are mainly based on the use of different theoretical models (Telescope Array uses an outdated model for its interpretation), and the fact that Telescope Array has not collected enough events yet to distinguish the pure proton hypothesis from the mixed-nuclei hypothesis.\n\nEUSO, which was scheduled to fly on the International Space Station (ISS) in 2009, was designed to use the atmospheric-fluorescence technique to monitor a huge area and boost the statistics of UHECRs considerably. EUSO is to make a deep survey of UHECR-induced extensive air showers (EASs) from space, extending the measured energy spectrum well beyond the GZK-cutoff. It is to search for the origin of UHECRs, determine the nature of the origin of UHECRs, make an all-sky survey of the arrival direction of UHECRs, and seek to open the astronomical window on the extreme-energy universe with neutrinos. The fate of the EUSO Observatory is still unclear since NASA is considering early retirement of the ISS.\n\nLaunched in June 2008, the Fermi Gamma-ray Space Telescope (formerly GLAST) will also provide data that will help resolve these inconsistencies.\n\nIn November 2007, researchers at the Pierre Auger Observatory announced that they had evidence that UHECRs appear to come from the active galactic nuclei (AGNs) of energetic galaxies powered by matter swirling onto a supermassive black hole. The cosmic rays were detected and traced back to the AGNs using the Véron-Cetty-Véron catalog. These results are reported in the journal \"Science\". Nevertheless, the strength of the correlation with AGNs from this particular catalog for the Auger data recorded after 2007 has been slowly diminishing.\n\n"}
{"id": "23721650", "url": "https://en.wikipedia.org/wiki?curid=23721650", "title": "Index of energy articles", "text": "Index of energy articles\n\nThis is an index of energy articles.\n\nActivation energy\n- Alternative energy\n- Alternative energy indexes\n- American Museum of Science and Energy (AMSE)\n- Anisotropy energy\n- Atomic energy\n\nBinding energy\n- Black hole\n- Breeder reactor\n- Brown energy\n\nCharacteristic energy\n- Conservation of energy\n- Consol Energy\n\nDark energy\n- Decay energy\n- Direct Energy\n- Dirichlet's energy\n- Dyson's sphere\n\nEcological energetics\n- Electric potential energy\n- Electrochemical energy conversion\n- Embodied energy\n- Encircled energy\n- Energy\n- Energy accidents\n- Energy accounting\n- Energy amplifier\n- Energy analyser\n- Energy applications of nanotechnology\n- Energy balance (biology)\n- Energy bar\n- Energy barrier\n- Energy being\n- Energy carrier\n- Energy Catalyzer\n- Energy cell\n- Energy charge\n- Energy conservation\n- Energy conversion efficiency\n- Energy crop\n- Energy current\n- Energy density\n- Energy-depth relationship in a rectangular channel\n- Energy development\n- Energy-dispersive X-ray spectroscopy\n- Energy distance\n- Energy drift\n- Energy drink\n- Energy efficiency gap\n- Energy-Efficient Ethernet\n- Energy-efficient landscaping\n- Energy elasticity\n- Energy engineering\n- Energy (esotericism)\n- Energy expenditure\n- Energy factor\n- Energy field disturbance\n- Energy filtered transmission electron microscopy\n- Energy transfer\n- Energy flow (ecology)\n- Energy flux\n- Energy forestry\n- Energy functional\n- Energy gel\n- Energy harvesting\n- Energy input labeling\n- Energy landscape\n- Energy level\n- Energy level splitting\n- Energy management software\n- Energy management system\n- Energy–maneuverability theory\n- Energy Manufacturing Co. Inc\n- Energy medicine\n- Energy–momentum relation\n- Energy monitoring and targeting\n- Energy Probe\n- Energy profile (chemistry)\n- Energy quality\n- Energy recovery ventilation\n- Energy security\n- Energy (signal processing)\n- Energy Slave\n- Energy Star\n- Energy statistics\n- Energy Storage Challenge\n- Energy storage\n- Energy system\n- Energy technology\n- Energy tower (downdraft)\n- Energy transfer\n- Energy transfer upconversion\n- Energy transformation\n- Energy value of coal\n- Energy vortex (stargate)\n- Enthalpy\n- Entropy\n- Equipartition theorem\n- E-statistic\n- Exertion\n\nFermi energy\n- Forms of energy\n- Fuel\n- Fusion power\n\nGeothermal energy\n- Gravitational energy\n- Gravitational potential\n\nHistory of energy\n- Hydroelectricity\n\nInteraction energy\n- Intermittent energy source\n- Internal energy\n- Invariant mass\n\nJosephson energy\n\nKinetic energy\n\nLatent heat\n\nMagnetic confinement fusion \n- Marine energy\n- Mass–energy equivalence\n- Mechanical energy\n- Möbius energy\n\nNegative energy\n- Nuclear fusion\n- Nuclear power\n- Nuclear reactor\n\nOrders of magnitude (energy)\n- Osmotic power\n\nPhotosynthesis\n- Potential energy\n- Power (physics)\n- Primary energy\n\nQi\n- Quasar\n\nRelativistic jet\n- Renewable energy - Rotational energy\n\nSeismic scale\n- Solar energy\n- Solar thermal energy\n- Sound energy\n- Specific energy\n- Specific kinetic energy\n- Specific orbital energy\n- Surface energy\n\nThermodynamic free energy\n- Threshold energy\n- Tidal power\n- Turbulence kinetic energy\n\nUnits of energy\n- Universe of Energy\n\nVacuum energy\n\nWork (physics)\n- World energy resources and consumption\n- World Forum on Energy Regulation\n\nZero-energy building\n- Zero-energy universe\n- Zero-point energy\n\n"}
{"id": "3591456", "url": "https://en.wikipedia.org/wiki?curid=3591456", "title": "Interface (matter)", "text": "Interface (matter)\n\nIn the physical sciences, an interface is the boundary between two spatial regions occupied by different matter, or by matter in different physical states. The interface between matter and air, or matter and vacuum, is called a surface, and studied in surface science. In thermal equilibrium, the regions in contact are called phases, and the interface is called a phase boundary. An example for an interface out of equilibrium is the grain boundary in polycrystalline matter.\n\nThe importance of the interface depends on the type of system: the bigger the quotient area/volume, the greater the effect the interface will have. Consequently, interfaces are very important in systems with large interface area-to-volume ratios, such as colloids.\n\nInterfaces can be flat or curved. For example, oil droplets in a salad dressing are spherical but the interface between water and air in a glass of water is mostly flat.\n\nSurface tension is the physical property which rules interface processes involving liquids. For a liquid film on flat surfaces, the liquid-vapor interface keeps flat to minimize interfacial area and system free energy. For a liquid film on rough surfaces, the surface tension tends to keep the meniscus flat, while the disjoining pressure makes the film conformal to the substrate. The equilibrium meniscus shape is a result of the competition between the capillary pressure and disjoining pressure.\n\nInterfaces may cause various optical phenomena, such as refraction. Optical lenses serve as an example of a practical application of the interface between glass and air.\n\nOne topical interface system is the gas-liquid interface between aerosols and other atmospheric molecules.\n\n"}
{"id": "985963", "url": "https://en.wikipedia.org/wiki?curid=985963", "title": "Lambda-CDM model", "text": "Lambda-CDM model\n\nThe ΛCDM (Lambda cold dark matter) or Lambda-CDM model is a parametrization of the Big Bang cosmological model in which the universe contains a cosmological constant, denoted by Lambda (Greek Λ), associated with dark energy, and cold dark matter (abbreviated CDM). It is frequently referred to as the standard model of Big Bang cosmology because it is the simplest model that provides a reasonably good account of the following properties of the cosmos:\n\nThe model assumes that general relativity is the correct theory of gravity on cosmological scales.\nIt emerged in the late 1990s as a concordance cosmology, after a period of time when disparate observed properties of the universe appeared mutually inconsistent, and there was no consensus on the makeup of the energy density of the universe.\n\nThe ΛCDM model can be extended by adding cosmological inflation, quintessence and other elements that are current areas of speculation and research in cosmology.\n\nSome alternative models challenge the assumptions of the ΛCDM model. Examples of these are modified Newtonian dynamics, modified gravity, theories of large-scale variations in the matter density of the universe, and scale invariance of empty space.\n\nMost modern cosmological models are based on the cosmological principle, which states that our observational location in the universe is not unusual or special; on a large-enough scale, the universe looks the same in all directions (isotropy) and from every location (homogeneity).\n\nThe model includes an expansion of metric space that is well documented both as the red shift of prominent spectral absorption or emission lines in the light from distant galaxies and as the time dilation in the light decay of supernova luminosity curves. Both effects are attributed to a Doppler shift in electromagnetic radiation as it travels across expanding space. Although this expansion increases the distance between objects that are not under shared gravitational influence, it does not increase the size of the objects (e.g. galaxies) in space. It also allows for distant galaxies to recede from each other at speeds greater than the speed of light; local expansion is less than the speed of light, but expansion summed across great distances can collectively exceed the speed of light.\n\nThe letter formula_1 (lambda) represents the cosmological constant, which is currently associated with a vacuum energy or dark energy in empty space that is used to explain the contemporary accelerating expansion of space against the attractive effects of gravity. A cosmological constant has negative pressure, formula_2, which contributes to the stress-energy tensor that, according to the general theory of relativity, causes accelerating expansion. The fraction of the total energy density of our (flat or almost flat) universe that is dark energy, formula_3, is currently (2015) estimated to be 0.692 ± 0.012, or even 0.6911 ± 0.0062 based on Planck satellite data.\n\nDark matter is postulated in order to account for gravitational effects observed in very large-scale structures (the \"flat\" rotation curves of galaxies; the gravitational lensing of light by galaxy clusters; and enhanced clustering of galaxies) that cannot be accounted for by the quantity of observed matter. Cold dark matter is \"non-baryonic\", i.e. it consists of matter other than protons and neutrons (and electrons, by convention, although electrons are not baryons); \"cold\", i.e. its velocity is far less than the speed of light at the epoch of radiation-matter equality (thus neutrinos are excluded, being non-baryonic but not cold); \"dissipationless\", i.e. it cannot cool by radiating photons; and \"collisionless\", i.e. the dark matter particles interact with each other and other particles only through gravity and possibly the weak force. The dark matter component is currently (2013) estimated to constitute about 26.8% of the mass-energy density of the universe.\n\nThe remaining 4.9% (2013) comprises all ordinary matter observed as atoms, chemical elements, gas and plasma, the stuff of which visible planets, stars and galaxies are made. The great majority of ordinary matter in the universe is unseen, since visible stars and gas inside galaxies and clusters account for less than 10% of the ordinary matter contribution to the mass-energy density of the universe.\n\nAlso, the energy density includes a very small fraction (~ 0.01%) in cosmic microwave background radiation, and not more than 0.5% in relic neutrinos. Although very small today, these were much more important in the distant past, dominating the matter at redshift > 3200.\n\nThe model includes a single originating event, the \"Big Bang\", which was not an explosion but the abrupt appearance of expanding space-time containing radiation at temperatures of around 10 K. This was immediately (within 10 seconds) followed by an exponential expansion of space by a scale multiplier of 10 or more, known as cosmic inflation. The early universe remained hot (above 10,000 K) for several hundred thousand years, a state that is detectable as a residual cosmic microwave background, or CMB, a very low energy radiation emanating from all parts of the sky. The \"Big Bang\" scenario, with cosmic inflation and standard particle physics, is the only current cosmological model consistent with the observed continuing expansion of space, the observed distribution of lighter elements in the universe (hydrogen, helium, and lithium), and the spatial texture of minute irregularities (anisotropies) in the CMB radiation. Cosmic inflation also addresses the \"horizon problem\" in the CMB; indeed, it seems likely that the universe is larger than the observable particle horizon.\n\nThe model uses the Friedmann–Lemaître–Robertson–Walker metric, the Friedmann equations and the cosmological equations of state to describe the observable universe from right after the inflationary epoch to present and future.\n\nThe expansion of the universe is parametrized by a dimensionless scale factor formula_4 (with time formula_5 counted from the birth of the universe), defined relative to the present day, so formula_6; the usual convention in cosmology is that subscript 0 denotes present-day values, so formula_7 is the current age of the universe. The scale factor is related to the observed redshift formula_8 of the light emitted at time formula_9 by\n\nThe expansion rate is described by the time-dependent Hubble parameter, formula_11, defined as\nwhere formula_13 is the time-derivative of the scale factor. The first Friedmann equation gives the expansion rate in terms of the matter+radiation density the curvature and the cosmological constant \n\nwhere as usual is the speed of light and is the gravitational constant. \nA critical density formula_15 is the present-day density, which gives zero curvature formula_16, assuming the cosmological constant formula_1 is zero, regardless of its actual value. Substituting these conditions to the Friedmann equation gives\n\nwhere formula_19 is the reduced Hubble constant.\nIf the cosmological constant were actually zero, the critical density would also mark the dividing line between eventual recollapse of the universe to a Big Crunch, or unlimited expansion. For the Lambda-CDM model with a positive cosmological constant (as observed), the universe is predicted to expand forever regardless of whether the total density is slightly above or below the critical density; though other outcomes are possible in extended models where the dark energy is not constant but actually time-dependent.\n\nIt is standard to define the present-day density parameter formula_20 for various species as the dimensionless ratio\nwhere the subscript formula_22 is one of formula_23 for baryons, formula_24 for cold dark matter, formula_25 for radiation (photons plus relativistic neutrinos), and formula_26 or formula_1 for dark energy.\n\nSince the densities of various species scale as different powers of formula_28, e.g. formula_29 for matter etc.,\nthe Friedmann equation can be conveniently rewritten in terms of the various density parameters as\nwhere w is the equation of state of dark energy, and assuming negligible neutrino mass (significant neutrino mass requires a more complex equation). The various formula_31 parameters add up to formula_32 by construction.\nIn the general case this is integrated by computer to give\nthe expansion history formula_33 and also observable distance-redshift relations for any chosen values of the cosmological parameters, which can then be compared with observations such as supernovae and baryon acoustic oscillations.\n\nIn the minimal 6-parameter Lambda-CDM model, it is assumed that curvature formula_34 is zero and formula_35, so this simplifies to\n\nObservations show that the radiation density is very small today, formula_37; if this term is neglected\nthe above has an analytic solution\nwhere formula_39\nthis is fairly accurate for formula_40 or formula_41million years.\nSolving for formula_42 gives the present age of the universe formula_43 in terms of the other parameters.\n\nIt follows that the transition from decelerating to accelerating expansion (the second derivative formula_44 crossing zero) occurred when\n\nwhich evaluates to formula_46 or formula_47 for the best-fit parameters estimated from the Planck spacecraft.\n\nThe discovery of the Cosmic Microwave Background (CMB) in 1964 confirmed a key prediction of the Big Bang cosmology. From that point on, it was generally accepted that the universe started in a hot, dense state and has been expanding over time. The rate of expansion depends on the types of matter and energy present in the universe, and in particular, whether the total density is above or below the so-called critical density. During the 1970s, most attention focused on pure-baryonic models, but there were serious challenges explaining the formation of galaxies, given the small anisotropies in the CMB (upper limits at that time). In the early 1980s, it was realized that this could be resolved if cold dark matter dominated over the baryons, and the theory of cosmic inflation motivated models with critical density. During the 1980s, most research focused on cold dark matter with critical density in matter, around 95% CDM and 5% baryons: these showed success at forming galaxies and clusters of galaxies, but problems remained; notably, the model required a Hubble constant lower than preferred by observations, and observations around 1988-1990 showed more large-scale galaxy clustering than predicted. These difficulties sharpened with the discovery of CMB anisotropy by COBE in 1992, and several modified CDM models, including ΛCDM and mixed cold and hot dark matter, came under active consideration through the mid-1990s. The ΛCDM model then became the leading model following the observations of accelerating expansion in 1998, and was quickly supported by other observations: in 2000, the BOOMERanG microwave background experiment measured the total (matter–energy) density to be close to 100% of critical, whereas in 2001 the 2dFGRS galaxy redshift survey measured the matter density to be near 25%; the large difference between these values supports a positive Λ or dark energy. Much more precise spacecraft measurements of the microwave background from WMAP in 2003 – 2010 and Planck in 2013 - 2015 have continued to support the model and pin down the parameter values, most of which are now constrained below 1 percent uncertainty.\n\nThere is currently active research into many aspects of the ΛCDM model, both to refine the parameters and possibly detect deviations. In addition, ΛCDM has no explicit physical theory for the origin or physical nature of dark matter or dark energy; the nearly scale-invariant spectrum of the CMB perturbations, and their image across the celestial sphere, are believed to result from very small thermal and acoustic irregularities at the point of recombination. A large majority of astronomers and astrophysicists support the ΛCDM model or close relatives of it, but Milgrom, McGaugh, and Kroupa are leading critics, attacking the dark matter portions of the theory from the perspective of galaxy formation models and supporting the alternative MOND theory, which requires a modification of the Einstein field equations and the Friedmann equations as seen in proposals such as MOG theory or TeVeS theory. Other proposals by theoretical astrophysicists of cosmological alternatives to Einstein's general relativity that attempt to account for dark energy or dark matter include f(R) gravity, scalar–tensor theories such as galileon theories, brane cosmologies, the DGP model, and massive gravity and its extensions such as bimetric gravity.\n\nIn addition to explaining pre-2000 observations,\nthe model has made a number of successful predictions: notably the existence of the\nbaryon acoustic oscillation feature, discovered in 2005 in the predicted location; and the statistics of weak gravitational lensing, first observed in 2000 by several teams. The polarization of the CMB, discovered in 2002 by DASI is now a dramatic success: in the 2015 Planck data release, there are seven observed peaks in the temperature (TT) power spectrum, six peaks in the temperature-polarization (TE) cross spectrum, and five peaks in the polarization (EE) spectrum. The six free parameters can be well constrained by the TT spectrum alone, and then the TE and EE spectra can be predicted theoretically to few-percent precision with no further adjustments allowed: comparison of theory and observations shows an excellent match.\n\nExtensive searches for dark matter particles have so far shown no well-agreed detection;\nthe dark energy may be almost impossible to detect in a laboratory, and its value is unnaturally small compared to naive theoretical predictions.\n\nComparison of the model with observations is very successful on large scales (larger than galaxies, up to the observable horizon), but may have some problems on sub-galaxy scales, possibly predicting too many dwarf galaxies and too much dark matter in the innermost regions of galaxies. This problem is called the \"small scale crisis\". These small scales are harder to resolve in computer simulations, so it is not yet clear whether the problem is the simulations, non-standard properties of dark matter, or a more radical error in the model.\n\nIt has been argued that the ΛCDM model is built upon a foundation of conventionalist stratagems, rendering it unfalsifiable in the sense defined by Karl Popper.\n\nThe simple ΛCDM model is based on six parameters: physical baryon density parameter; physical dark matter density parameter; the age of the universe; scalar spectral index; curvature fluctuation amplitude; and reionization optical depth. In accordance with Occam's razor, six is the smallest number of parameters needed to give an acceptable fit to current observations; other possible parameters are fixed at \"natural\" values, e.g. total density parameter = 1.00, dark energy equation of state = −1. (See below for extended models that allow these to vary.)\n\nThe values of these six parameters are mostly not predicted by current theory (though, ideally, they may be related by a future \"Theory of Everything\"), except that most versions of cosmic inflation predict the scalar spectral index should be slightly smaller than 1, consistent with the estimated value 0.96. The parameter values, and uncertainties, are estimated using large computer searches to locate the region of parameter space providing an acceptable match to cosmological observations. From these six parameters, the other model values, such as the Hubble constant and the dark energy density, can be readily calculated.\n\nCommonly, the set of observations fitted includes the cosmic microwave background anisotropy, the brightness/redshift relation for supernovae, and large-scale galaxy clustering including the baryon acoustic oscillation feature. Other observations, such as the Hubble constant, the abundance of galaxy clusters, weak gravitational lensing and globular cluster ages, are generally consistent with these, providing a check of the model, but are less precisely measured at present.\n\nParameter values listed below are from the Planck Collaboration Cosmological parameters 68% confidence limits for the base ΛCDM model from Planck CMB power spectra, in combination with lensing reconstruction and external data (BAO + JLA + H). See also Planck (spacecraft).\n\nMassimo Persic and Paolo Salucci firstly estimated the baryonic density today present in ellipticals, spirals, groups and clusters of galaxies.\nThey performed an integration of the baryonic mass-to-light ratio over luminosity (in the following formula_48), weighted with the luminosity function formula_49 over the previously mentioned classes of astrophysical objects: \n\nThe result was:\n\nwhere formula_52.\n\nNote that this value is much lower than the prediction of standard cosmic nucleosynthesis formula_53, so that stars and gas in galaxies and in galaxy groups and clusters account for less than 10% of the primordially synthesized baryons. This issue is known as the problem of the \"missing baryons\".\n\nExtended models allow one or more of the \"fixed\" parameters above to vary, in addition to the basic six; so these models join smoothly to the basic six-parameter model in the limit that the additional parameter(s) approach the default values. For example, possible extensions of the simplest ΛCDM model allow for spatial curvature (formula_54 may be different from 1); or quintessence rather than a cosmological constant where the equation of state of dark energy is allowed to differ from −1. Cosmic inflation predicts tensor fluctuations (gravitational waves). Their amplitude is parameterized by the tensor-to-scalar ratio (denoted formula_55), which is determined by the unknown energy scale of inflation. Other modifications allow hot dark matter in the form of neutrinos more massive than the minimal value, or a running spectral index; the latter is generally not favoured by simple cosmic inflation models.\n\nAllowing additional variable parameter(s) will generally \"increase\" the uncertainties in the standard six parameters quoted above, and may also shift the central values slightly. The Table below shows results for each of the possible \"6+1\" scenarios with one additional variable parameter; this indicates that, as of 2015, there is no convincing evidence that any additional parameter is different from its default value.\n\nSome researchers have suggested that there is a running spectral index, but no statistically significant study has revealed one. Theoretical expectations suggest that the tensor-to-scalar ratio formula_55 should be between 0 and 0.3, and the latest results are now within those limits.\n\n\n"}
{"id": "14997569", "url": "https://en.wikipedia.org/wiki?curid=14997569", "title": "Location of Earth", "text": "Location of Earth\n\nKnowledge of the location of Earth has been shaped by 400 years of telescopic observations, and has expanded radically in the last century. Initially, Earth was believed to be the center of the Universe, \nwhich consisted only of those planets visible with the naked eye and an outlying sphere of fixed stars. After the acceptance of the heliocentric model in the 17th century, observations by William Herschel and others showed that the Sun lay within a vast, disc-shaped galaxy of stars. By the 20th century, observations of spiral nebulae revealed that our galaxy was one of billions in an expanding universe, grouped into clusters and superclusters. By the end of the 20th century, the overall structure of the visible universe was becoming clearer, with superclusters forming into a vast web of filaments and voids. Superclusters, filaments and voids are the largest coherent structures in the Universe that we can observe. At still larger scales (over 1000 megaparsecs) the Universe becomes homogeneous meaning that all its parts have on average the same density, composition and structure.\n\nSince there is believed to be no \"center\" or \"edge\" of the Universe, there is no particular reference point with which to plot the overall location of the Earth in the universe. Because the observable universe is defined as that region of the Universe visible to terrestrial observers, Earth is, by definition, the center of Earth's observable universe. Reference can be made to the Earth's position with respect to specific structures, which exist at various scales. It is still undetermined whether the Universe is infinite. There have been numerous hypotheses that our universe may be only one such example within a higher multiverse; however, no direct evidence of any sort of multiverse has ever been observed, and some have argued that the hypothesis is not falsifiable.\n\n"}
{"id": "36688650", "url": "https://en.wikipedia.org/wiki?curid=36688650", "title": "Moisture expansion", "text": "Moisture expansion\n\nMoisture expansion is the tendency of matter to change in volume in response to a change in moisture content. The macroscopic effect is similar to that of thermal expansion but the microscopic causes are very different. Moisture expansion is caused by hygroscopy.\n"}
{"id": "19555", "url": "https://en.wikipedia.org/wiki?curid=19555", "title": "Molecule", "text": "Molecule\n\nA molecule is an electrically neutral group of two or more atoms held together by chemical bonds. Molecules are distinguished from ions by their lack of electrical charge. However, in quantum physics, organic chemistry, and biochemistry, the term \"molecule\" is often used less strictly, also being applied to polyatomic ions.\n\nIn the kinetic theory of gases, the term \"molecule\" is often used for any gaseous particle regardless of its composition. According to this definition, noble gas atoms are considered molecules as they are monatomic molecules.\n\nA molecule may be homonuclear, that is, it consists of atoms of one chemical element, as with oxygen (O); or it may be heteronuclear, a chemical compound composed of more than one element, as with water (HO). Atoms and complexes connected by non-covalent interactions, such as hydrogen bonds or ionic bonds, are generally not considered single molecules.\n\nMolecules as components of matter are common in organic substances (and therefore biochemistry). They also make up most of the oceans and atmosphere. However, the majority of familiar solid substances on Earth, including most of the minerals that make up the crust, mantle, and core of the Earth, contain many chemical bonds, but are \"not\" made of identifiable molecules. Also, no typical molecule can be defined for ionic crystals (salts) and covalent crystals (network solids), although these are often composed of repeating unit cells that extend either in a plane (such as in graphene) or three-dimensionally (such as in diamond, quartz, or sodium chloride). The theme of repeated unit-cellular-structure also holds for most condensed phases with metallic bonding, which means that solid metals are also not made of molecules. In glasses (solids that exist in a vitreous disordered state), atoms may also be held together by chemical bonds with no presence of any definable molecule, nor any of the regularity of repeating units that characterizes crystals.\n\nThe science of molecules is called \"molecular chemistry\" or \"molecular physics\", depending on whether the focus is on chemistry or physics. Molecular chemistry deals with the laws governing the interaction between molecules that results in the formation and breakage of chemical bonds, while molecular physics deals with the laws governing their structure and properties. In practice, however, this distinction is vague. In molecular sciences, a molecule consists of a stable system (bound state) composed of two or more atoms. Polyatomic ions may sometimes be usefully thought of as electrically charged molecules. The term \"unstable molecule\" is used for very reactive species, i.e., short-lived assemblies (resonances) of electrons and nuclei, such as radicals, molecular ions, Rydberg molecules, transition states, van der Waals complexes, or systems of colliding atoms as in Bose–Einstein condensate.\n\nAccording to Merriam-Webster and the Online Etymology Dictionary, the word \"molecule\" derives from the Latin \"moles\" or small unit of mass.\n\nThe definition of the molecule has evolved as knowledge of the structure of molecules has increased. Earlier definitions were less precise, defining molecules as the smallest particles of pure chemical substances that still retain their composition and chemical properties. This definition often breaks down since many substances in ordinary experience, such as rocks, salts, and metals, are composed of large crystalline networks of chemically bonded atoms or ions, but are not made of discrete molecules.\n\nMolecules are held together by either covalent bonding or ionic bonding. Several types of non-metal elements exist only as molecules in the environment. For example, hydrogen only exists as hydrogen molecule. A molecule of a compound is made out of two or more elements.\n\nA covalent bond is a chemical bond that involves the sharing of electron pairs between atoms. These electron pairs are termed \"shared pairs\" or \"bonding pairs\", and the stable balance of attractive and repulsive forces between atoms, when they share electrons, is termed \"covalent bonding\".\n\nIonic bonding is a type of chemical bond that involves the electrostatic attraction between oppositely charged ions, and is the primary interaction occurring in ionic compounds. The ions are atoms that have lost one or more electrons (termed cations) and atoms that have gained one or more electrons (termed anions). This transfer of electrons is termed \"electrovalence\" in contrast to covalence. In the simplest case, the cation is a metal atom and the anion is a nonmetal atom, but these ions can be of a more complicated nature, e.g. molecular ions like NH or SO. Basically, an ionic bond is the transfer of electrons from a metal to a non-metal for both atoms to obtain a full valence shell.\n\nMost molecules are far too small to be seen with the naked eye, but there are exceptions. DNA, a macromolecule, can reach macroscopic sizes, as can molecules of many polymers. Molecules commonly used as building blocks for organic synthesis have a dimension of a few angstroms (Å) to several dozen Å, or around one billionth of a meter. Single molecules cannot usually be observed by light (as noted above), but small molecules and even the outlines of individual atoms may be traced in some circumstances by use of an atomic force microscope. Some of the largest molecules are macromolecules or supermolecules.\n\nThe smallest molecule is the diatomic hydrogen (H), with a bond length of 0.74 Å.\n\nEffective molecular radius is the size a molecule displays in solution.\nThe table of permselectivity for different substances contains examples.\n\nThe chemical formula for a molecule uses one line of chemical element symbols, numbers, and sometimes also other symbols, such as parentheses, dashes, brackets, and \"plus\" (+) and \"minus\" (−) signs. These are limited to one typographic line of symbols, which may include subscripts and superscripts.\n\nA compound's empirical formula is a very simple type of chemical formula. It is the simplest integer ratio of the chemical elements that constitute it. For example, water is always composed of a 2:1 ratio of hydrogen to oxygen atoms, and ethyl alcohol or ethanol is always composed of carbon, hydrogen, and oxygen in a 2:6:1 ratio. However, this does not determine the kind of molecule uniquely – dimethyl ether has the same ratios as ethanol, for instance. Molecules with the same atoms in different arrangements are called isomers. Also carbohydrates, for example, have the same ratio (carbon:hydrogen:oxygen= 1:2:1) (and thus the same empirical formula) but different total numbers of atoms in the molecule.\n\nThe molecular formula reflects the exact number of atoms that compose the molecule and so characterizes different molecules. However different isomers can have the same atomic composition while being different molecules.\n\nThe empirical formula is often the same as the molecular formula but not always. For example, the molecule acetylene has molecular formula CH, but the simplest integer ratio of elements is CH.\n\nThe molecular mass can be calculated from the chemical formula and is expressed in conventional atomic mass units equal to 1/12 of the mass of a neutral carbon-12 (C isotope) atom. For network solids, the term formula unit is used in stoichiometric calculations.\n\nFor molecules with a complicated 3-dimensional structure, especially involving atoms bonded to four different substituents, a simple molecular formula or even semi-structural chemical formula may not be enough to completely specify the molecule. In this case, a graphical type of formula called a structural formula may be needed. Structural formulas may in turn be represented with a one-dimensional chemical name, but such chemical nomenclature requires many words and terms which are not part of chemical formulas.\n\nMolecules have fixed equilibrium geometries—bond lengths and angles— about which they continuously oscillate through vibrational and rotational motions. A pure substance is composed of molecules with the same average geometrical structure. The chemical formula and the structure of a molecule are the two important factors that determine its properties, particularly its reactivity. Isomers share a chemical formula but normally have very different properties because of their different structures. Stereoisomers, a particular type of isomer, may have very similar physico-chemical properties and at the same time different biochemical activities.\n\nMolecular spectroscopy deals with the response (spectrum) of molecules interacting with probing signals of known energy (or frequency, according to Planck's formula). Molecules have quantized energy levels that can be analyzed by detecting the molecule's energy exchange through absorbance or emission.\nSpectroscopy does not generally refer to diffraction studies where particles such as neutrons, electrons, or high energy X-rays interact with a regular arrangement of molecules (as in a crystal).\n\nMicrowave spectroscopy commonly measures changes in the rotation of molecules, and can be used to identify molecules in outer space. Infrared spectroscopy measures changes in vibration of molecules, including stretching, bending or twisting motions. It is commonly used to identify the kinds of bonds or functional groups in molecules. Changes in the arrangements of electrons yield absorption or emission lines in ultraviolet, visible or near infrared light, and result in colour. Nuclear resonance spectroscopy actually measures the environment of particular nuclei in the molecule, and can be used to characterise the numbers of atoms in different positions in a molecule.\n\nThe study of molecules by molecular physics and theoretical chemistry is largely based on quantum mechanics and is essential for the understanding of the chemical bond. The simplest of molecules is the hydrogen molecule-ion, H, and the simplest of all the chemical bonds is the one-electron bond. H is composed of two positively charged protons and one negatively charged electron, which means that the Schrödinger equation for the system can be solved more easily due to the lack of electron–electron repulsion. With the development of fast digital computers, approximate solutions for more complicated molecules became possible and are one of the main aspects of computational chemistry.\n\nWhen trying to define rigorously whether an arrangement of atoms is \"sufficiently stable\" to be considered a molecule, IUPAC suggests that it \"must correspond to a depression on the potential energy surface that is deep enough to confine at least one vibrational state\". This definition does not depend on the nature of the interaction between the atoms, but only on the strength of the interaction. In fact, it includes weakly bound species that would not traditionally be considered molecules, such as the helium dimer, He, which has one vibrational bound state and is so loosely bound that it is only likely to be observed at very low temperatures.\n\nWhether or not an arrangement of atoms is \"sufficiently stable\" to be considered a molecule is inherently an operational definition. Philosophically, therefore, a molecule is not a fundamental entity (in contrast, for instance, to an elementary particle); rather, the concept of a molecule is the chemist's way of making a useful statement about the strengths of atomic-scale interactions in the world that we observe.\n\n\n"}
{"id": "47958635", "url": "https://en.wikipedia.org/wiki?curid=47958635", "title": "National Hydrogen and Fuel Cell Day", "text": "National Hydrogen and Fuel Cell Day\n\nNational Hydrogen and Fuel Cell Day was created by the Fuel Cell and Hydrogen Energy Association to help raise awareness of fuel cell and hydrogen technologies and to celebrate how far the industry has come as well as the vast potential the technologies have today and in future. \n\nOctober 8th (10.08) was chosen in reference to the atomic weight of hydrogen (1.008).\n\nNational Hydrogen and Fuel Cell Day was officially launched on October 8, 2015. \n\nThe Fuel Cell and Hydrogen Energy Association (FCHEA), its members, industry organizations, allied groups, state and federal governments and individuals are commemorating National Hydrogen and Fuel Cell Day with a variety of activities and events across the country.\n\n\n"}
{"id": "208999", "url": "https://en.wikipedia.org/wiki?curid=208999", "title": "Non-renewable resource", "text": "Non-renewable resource\n\nA non-renewable resource (also called a finite resource) is a resource that does not renew itself at a sufficient rate for sustainable economic extraction in meaningful human time-frames. An example is carbon-based, organically-derived fuel. The original organic material, with the aid of heat and pressure, becomes a fuel such as oil or gas. Earth minerals and metal ores, fossil fuels (coal, petroleum, natural gas) and groundwater in certain aquifers are all considered non-renewable resources, though individual elements are almost always conserved.\n\nOn the other hand, resources such as timber (when harvested sustainably) and wind (used to power energy conversion systems) are considered renewable resources, largely because their localized replenishment can occur within time frames meaningful to humans.\n\nEarth minerals and metal ores are examples of non-renewable resources. The metals themselves are present in vast amounts in Earth's crust, and their extraction by humans only occurs where they are concentrated by natural geological processes (such as heat, pressure, organic activity, weathering and other processes) enough to become economically viable to extract. These processes generally take from tens of thousands to millions of years, through plate tectonics, tectonic subsidence and crustal recycling.\n\nThe localized deposits of metal ores near the surface which can be extracted economically by humans are non-renewable in human time-frames. There are certain rare earth minerals and elements that are more scarce and exhaustible than others. These are in high demand in manufacturing, particularly for the electronics industry.\n\nMost metal ores are considered vastly greater in supply to fossil fuels, because metal ores are formed by crustal-scale processes which make up a much larger portion of the Earth's near-surface environment, than those that form fossil fuels which are limited to areas where carbon-based life forms flourish, die, and are quickly buried.\n\nNatural resources such as coal, petroleum (crude oil) and natural gas take thousands of years to form naturally and cannot be replaced as fast as they are being consumed. Eventually it is considered that fossil-based resources will become too costly to harvest and humanity will need to shift its reliance to other sources of energy such as solar or wind power, see renewable energy.\n\nAn alternative hypothesis is that carbon based fuel is virtually inexhaustible in human terms, if one includes all sources of carbon-based energy such as methane hydrates on the sea floor, which are vastly greater than all other carbon based fossil fuel resources combined. These sources of carbon are also considered non-renewable, although their rate of formation/replenishment on the sea floor is not known. However their extraction at economically viable costs and rates has yet to be determined.\n\nAt present, the main energy source used by humans is non-renewable fossil fuels. Since the dawn of internal combustion engine technologies in the 19th century, petroleum and other fossil fuels have remained in continual demand. As a result, conventional infrastructure and transport systems, which are fitted to combustion engines, remain prominent throughout the globe. The continual use of fossil fuels at the current rate is believed to increase global warming and cause more severe climate change.\n\nIn 1987, the World Commission on Environment and Development (WCED) an organization set up by but independent from the United Nations classified fission reactors that produce more fissile nuclear fuel than they consume -i.e. breeder reactors, and when it is developed, fusion power, among conventional renewable energy sources, such as solar and falling water. The American Petroleum Institute likewise does not consider conventional nuclear fission as renewable, but that breeder reactor nuclear power fuel is considered renewable and sustainable, before explaining that radioactive waste from used spent fuel rods remains radioactive, and so has to be very carefully stored for up to a thousand years. With the careful monitoring of radioactive waste products also being required upon the use of other renewable energy sources, such as geothermal energy.\n\nThe use of nuclear technology relying on fission requires Naturally occurring radioactive material as fuel. Uranium, the most common fission fuel, and is present in the ground at relatively low concentrations and mined in 19 countries. This mined uranium is used to fuel energy-generating nuclear reactors with fissionable uranium-235 which generates heat that is ultimately used to power turbines to generate electricity.\n\nAs of 2013 only a few kilograms (picture available) of uranium have been extracted from the ocean in pilot programs and it is also believed that the uranium extracted on an industrial scale from the seawater would constantly be replenished from uranium leached from the ocean floor, maintaining the seawater concentration at a stable level. In 2014, with the advances made in the efficiency of seawater uranium extraction, a paper in the journal of \"Marine Science & Engineering\" suggests that with, light water reactors as its target, the process would be economically competitive if implemented on a large scale.\n\nNuclear power provides about 6% of the world's energy and 13–14% of the world's electricity. Nuclear energy production is associated with potentially dangerous radioactive contamination as it relies upon unstable elements. In particular, nuclear power facilities produce about 200,000 metric tons of low and intermediate level waste (LILW) and 10,000 metric tons of high level waste (HLW) (including spent fuel designated as waste) each year worldwide.\n\nIssues entirely separate from the question of the sustainability of nuclear fuel, relate to the use of nuclear fuel and the high-level radioactive waste the nuclear industry generates that if not properly contained, is highly hazardous to people and wildlife. The United Nations (UNSCEAR) estimated in 2008 that average annual human radiation exposure includes 0.01 millisievert (mSv) from the legacy of past atmospheric nuclear testing plus the Chernobyl disaster and the nuclear fuel cycle, along with 2.0 mSv from natural radioisotopes and 0.4 mSv from cosmic rays; all exposures vary by location. natural uranium in some inefficient reactor nuclear fuel cycles, becomes part of the nuclear waste \"once through\" stream, and in a similar manner to the scenario were this uranium remained naturally in the ground, this uranium emits various forms of radiation in a decay chain that has a half-life of about 4.5 billion years, the storage of this unused uranium and the accompanying fission reaction products have raised public concerns about risks of leaks and containment, however the knowledge gained from studying the Natural nuclear fission reactor in Oklo Gabon, has informed geologists on the proven processes that kept the waste from this 2 billion year old natural nuclear reactor that operated for hundreds of thousands of years, from negatively impacting the surrounding plant and animal life.\n\nNatural resources, known as renewable resources, are replaced by natural processes and forces persistent in the natural environment. There are intermittent and reoccurring renewables, and recyclable materials, which are utilized during a cycle across a certain amount of time, and can be harnessed for any number of cycles.\n\nThe production of goods and services by manufacturing products in economic systems creates many types of waste during production and after the consumer has made use of it. The material is then either incinerated, buried in a landfill or recycled for reuse. Recycling turns materials of value that would otherwise become waste into valuable resources again.\nThe natural environment, with soil, water, forests, plants and animals are all renewable resources, as long as they are adequately monitored, protected and conserved. Sustainable agriculture is the cultivation of plant and animal materials in a manner that preserves plant and animal ecosystems over the long term. The overfishing of the oceans is one example of where an industry practice or method can threaten an ecosystem, endanger species and possibly even determine whether or not a fishery is sustainable for use by humans. An unregulated industry practice or method can lead to a complete resource depletion.\nThe renewable energy from the sun, wind, wave, biomass and geothermal energies are based on renewable resources. Renewable resources such as the movement of water (hydropower, tidal power and wave power), wind and radiant energy from geothermal heat (used for geothermal power) and solar energy (used for solar power) are practically infinite and cannot be depleted, unlike their non-renewable counterparts, which are likely to run out if not used sparingly.\n\nThe potential wave energy on coastlines can provide 1/5 of world demand. Hydroelectric power can supply 1/3 of our total energy global needs. Geothermal energy can provide 1.5 more times the energy we need. There is enough wind to power the planet 30 times over, wind power could power all of humanity's needs alone. Solar currently supplies only 0.1% of our world energy needs, but there is enough out there to power humanity's needs 4,000 times over, the entire global projected energy demand by 2050.\n\nRenewable energy and energy efficiency are no longer niche sectors that are promoted only by governments and environmentalists. The increasing levels of investment and that more of the capital is from conventional financial actors, both suggest that sustainable energy has become mainstream and the future of energy production, as non-renewable resources decline. This is reinforced by climate change concerns, nuclear dangers and accumulating radioactive waste, high oil prices, peak oil and increasing government support for renewable energy. These factors are commercializing renewable energy, enlarging the market and growing demand, the adoption of new products to replace obsolete technology and the conversion of existing infrastructure to a renewable standard.\n\nIn economics, a non-renewable resource is defined as goods, where greater consumption today implies less consumption tomorrow. David Ricardo in his early works analysed the pricing of exhaustible resources, where he argued that the price of a mineral resource should increase over time. He argued that the spot price is always determined by the mine with the highest cost of extraction, and mine owners with lower extraction costs benefit from a differential rent. The first model is defined by Hotelling's rule, which is a 1931 economic model of non-renewable resource management by Harold Hotelling. It shows that efficient exploitation of a nonrenewable and nonaugmentable resource would, under otherwise stable conditions, lead to a depletion of the resource. The rule states that this would lead to a net price or \"Hotelling rent\" for it that rose annually at a rate equal to the rate of interest, reflecting the increasing scarcity of the resources.\nThe Hartwick's rule provides an important result about the sustainability of welfare in an economy that uses non-renewable source.\n\n\n"}
{"id": "52660479", "url": "https://en.wikipedia.org/wiki?curid=52660479", "title": "Open energy system databases", "text": "Open energy system databases\n\nOpen energy system database projects employ open data methods to collect, clean, and republish energy-related datasets for open use. The resulting information is then available, given a suitable open license, for statistical analysis and for building numerical energy system models, including open energy system models. Permissive licenses like Creative Commons CC0 and are preferred, but some projects will house data made public under market transparency regulations and carrying unqualified copyright.\n\nThe databases themselves may furnish information on national power plant fleets, renewable generation assets, transmission networks, time series for electricity loads, dispatch, spot prices, and cross-border trades, weather information, and similar. They may also offer other energy statistics including fossil fuel imports and exports, gas, oil, and coal prices, emissions certificate prices, and information on energy efficiency costs and benefits.\n\nMuch of the data is sourced from official or semi-official agencies, including national statistics offices, transmission system operators, and electricity market operators. Data is also crowdsourced using public wikis and public upload facilities. Projects usually also maintain a strict record of the provenance and version histories of the datasets they hold. Some projects, as part of their mandate, also try to persuade primary data providers to release their data under more liberal licensing conditions.\n\nTwo drivers favor the establishment of such databases. The first is a wish to reduce the duplication of effort that accompanies each new analytical project as it assembles and processes the data that it needs from primary sources. And the second is an increasing desire to make public policy energy models more transparent to improve their acceptance by policymakers and the public. Better transparency dictates the use of open information, able to be accessed and scrutinized by third-parties, in addition to releasing the source code for the models in question.\n\nIn the mid-1990s, energy models used structured text files for data interchange but efforts were being made to migrate to relational database management systems for data processing. These early efforts however remained local to a project and did not involve online publishing or open data principles.\n\nThe first energy information portal to go live was OpenEI in late 2009, followed by reegle in 2011.\n\nA 2012 paper marks the first scientific publication to advocate the crowdsourcing of energy data. The 2012 PhD thesis by Chris Davis also discusses the crowdsourcing of energy data in some depth. A 2016 thesis surveyed the spatial (GIS) information requirements for energy planning and finds that most types of data, with the exception of energy expenditure data, are available but nonetheless remain scattered and poorly coordinated.\n\nIn terms of open data, a 2017 paper concludes that energy research has lagged behind other fields, most notably physics, biotechnology, and medicine. The paper also lists the benefits of open data and open models and discusses the reasons that many projects nonetheless remain closed. A one-page opinion piece from 2017 advances the case for using open energy data and modeling to build public trust in policy analysis. The article also argues that scientific journals have a responsibility to require that data and code be submitted alongside text for peer review.\n\nData models are central to the design and organization of databases. Open energy database projects generally try to develop and adhere to well resolved data models, using defacto and published standards where applicable. Some projects attempt to coordinate their data models in order to harmonize their data and improve its utility. Defining and maintaining suitable metadata is also a key issue. The life-cycle management of data includes, but is not limited to, the use of version control to track the provenance of incoming and cleansed data. Some sites allow users to comment on and rate individual datasets.\n\nIssues surrounding copyright remain at the forefront with regard to open energy data. As noted, most energy datasets are collated and published by official or semi-official sources. But many of the publicly available energy datasets carry no license, limiting their reuse in numerical and statistical models, open or otherwise. Copyright protected material cannot lawfully be circulated, nor can it be modified and republished.\n\nMeasures to enforce market transparency have not helped much because the associated information is again not licensed to enable modification and republication. Transparency measures include the 2013 European energy market transparency regulation 543/2013. Indeed, 543/2013 \"is only an obligation to publish, not an obligation to license\". Notwithstanding, 543/2013 does enable downloaded data to be computer processed with legal certainty.\n\nEnergy databases with hardware located with the European Union are protected under a general database law, irrespective of the legal status of the information they hold.\nDatabase rights not waived by public sector providers significantly restrict the amount of data a user can lawfully access.\n\nA December 2017 submission by energy researchers in Germany and elsewhere highlighted a number of concerns over the re-use of public sector information within the Europe Union.\nThe submission drew heavily on a recent legal opinion covering electricity data.\n\nNational and international energy statistics are published regularly by governments and international agencies, such as the IEA. In 2016 the United Nations issued guidelines for energy statistics. While the definitions and sectoral breakdowns are useful when defining models, the information provided is rarely in sufficient detail to enable its use in high-resolution energy system models.\n\nThere are few published standards covering the collection and structuring of high-resolution energy system data. The IEC Common Information Model (CIM) defines data exchange protocols for low and high voltage electricity networks.\n\nEnergy system models are data intensive and normally require detailed information from a number of sources. Dedicated projects to collect, collate, document, and republish energy system datasets have arisen to service this need. Most database projects prefer open data, issued under free licenses, but some will accept datasets with proprietary licenses in the absence of other options.\n\nThe OpenStreetMap project, which uses the Open Database License (ODbL), contains geographic information about energy system components, including transmission lines. Wikipedia itself has a growing set of information related to national energy systems, including descriptions of individual power stations.\n\nThe following table summarizes projects that specifically publish open energy system data. Some are general repositories while others (for instance, oedb) are designed to interact with open energy system models in real-time.\n\nThe Energy Research Data Portal for South Africa is being developed by the Energy Research Centre, University of Cape Town, Cape Town, South Africa. Coverage includes South Africa and certain other African countries where the Centre undertakes projects. The website uses the CKAN open source data portal software. A number of data formats are supported, including CSV and XLSX. The site also offers an API for automated downloads. , the portal contained 65datasets.\n\nThe energydata.info project from the World Bank Group, Washington, DC, USA is an energy database portal designed to support national development by improving public access to energy information. As well as sharing data, the platform also offers tools to visualize and analyze energy data. Although the World Bank Group has made available a number of dataset and apps, external users and organizations are encouraged to contribute. The concepts of open data and open source development are central to the project. energydata.info uses its own fork of the CKAN open source data portal as its web-based platform. The Creative Commons CC BY 4.0 license is preferred for data but other open licenses can be deployed. Users are also bound by the terms of use for the site.\n\n, the database held 131datasets, the great majority related to developing countries. The datasets are tagged and can be easily filtered. A number of download formats, including GIS files, are supported: CSV, XLS, XLSX, ArcGIS, Esri, GeoJSON, KML, and SHP. Some datasets are also offered as HTML. Again, , four apps are available. Some are web-based and run from a browser.\n\nThe semantic wiki-site and database Enipedia lists energy systems data worldwide. Enipedia is maintained by the Energy and Industry Group, Faculty of Technology, Policy and Management, Delft University of Technology, Delft, the Netherlands. A key tenet of Enipedia is that data displayed on the wiki is not trapped within the wiki, but can be extracted via SPARQL queries and used to populate new tools. Any programming environment that can download content from a URL can be used to obtain data. Enipedia went live in March 2011, judging by traffic figures quoted by Davis.\n\nA 2010 study describes how community driven data collection, processing, curation, and sharing is revolutionizing the data needs of industrial ecology and energy system analysis. A 2012 chapter introduces a system of systems engineering (SoSE) perspective and outlines how agent-based models and crowdsourced data can contribute to the solving of global issues.\n\nThe OpenEnergy Platform (OEP) is a collaborative versioned dataset repository for storing open energy system model datasets. A dataset is presumed to be in the form of a database table, together with metadata. Registered users can upload and download datasets manually using a web-interface or programmatically via an API using HTTP POST calls. Uploaded datasets are screened for integrity using deterministic rules and then subject to confirmation by a moderator. The use of versioning means that any prior state of the database can be accessed (as recommended in this 2012 paper). Hence, the repository is specifically designed to interoperate with energy system models. The backend is a PostgreSQL object-relational database under subversion version control. Open source licenses are specific to each dataset. Unlike other database projects, users can download the current version (the public tables) of the entire PostgreSQL database or any previous version. Initial development is being lead by the Reiner Lemoine Institute, Berlin, Germany.\n\nThe Open Data Energy Networks ( or ) portal is run by eight partners, led by the French national transmission system operator (TSO) Réseau de Transport d'Électricité (RTE). The portal was previously known as Open Data RTE. The site offers electricity system datasets under a Creative Commons compatible license, with metadata, an RSS feed for notifying updates, and an interface for submitting questions. of information obtained from the site can also register third-party URLs (be they publications or webpages) against specific datasets.\n\nThe portal uses the French Government Licence Ouverte license and this is explicitly compatible with the United Kingdom Open Government Licence (OGL), the Creative Commons license (and thereby later versions), and the Open Data Commons license.\n\nThe site hosts electricity, gas, and weather information related to France.\n\nThe Open Power System Data (OPSD) project seeks to characterize the German and western European power plant fleets, their associated transmission network, and related information and to make that data available to energy modelers and analysts. The platform was originally implemented by the University of Flensburg, DIW Berlin, the Technical University of Berlin, and the energy economics consultancy Neon Neue Energieökonomik, all from Germany. The first phase of the project, from August 2015 to July 2017, was funded by the Federal Ministry for Economic Affairs and Energy (BMWi) for . The project later received funding for a second phase, from January 2018 to December 2020, with ETH Zurich replacing Flensburg University as a partner.\n\nDevelopers collate and harmonize data from a range of government, regulatory, and industry sources throughout Europe. The website and the metadata utilize English, whereas the original material can be in any one of 24languages. Datasets follow the emerging frictionless data package standard being developed by Open Knowledge International (OKI). The website was launched on 28October 2016. , the project offers the following primary packages, for Germany and other European countries:\n\n\nIn addition, the project hosts selected contributed packages:\n\n\nTo facilitate analysis, the data is aggregated into large structured files (in CSV format) and loaded into data packages with standardized machine-readable metadata (in JSON format). The same data is usually also provided as XLSX (Excel) and SQLite files. The datasets can be accessed in real-time using stable URLs. The Python scripts deployed for data processing are available on GitHub and carry an MIT license. The licensing conditions for the data itself depends on the source and varies in terms of openness. Previous versions of the datasets and scripts can be recovered in order to track changes or replicate earlier studies. The project also engages with energy data providers, such as transmission system operators (TSO) and ENTSO-E, to encourage them to make their data available under open licenses (for instance, Creative Commons and ODbL licenses).\n\nA number of published electricity market modeling analyses are based on OPSD data.\n\nIn 2017, the Open Power System Data project won the Schleswig-Holstein Open Science Award and the Germany Land of Ideas award.\n\nOpen Energy Information (OpenEI) is a collaborative website, run by the US government, providing open energy data to software developers, analysts, users, consumers, and policymakers. The platform is sponsored by the United States Department of Energy (DOE) and is being developed by the National Renewable Energy Laboratory (NREL). OpenEI launched on 9December 2009. While much of its data is from US government sources, the platform is intended to be open and global in scope.\n\nOpenEI provides two mechanisms for contributing structured information: a semantic wiki (using MediaWiki and the Semantic MediaWiki extension) for collaboratively-managed resources and a dataset upload facility for contributor-controlled resources. US government data is distributed under a CC0 public domain dedication, whereas other contributors are free to select an open data license of their choice. Users can rate data using a five-star system, based on accessibility, adaptability, usefulness, and general quality. Individual datasets can be manually downloaded in an appropriate format, often as CSV files. Scripts for processing data can also be shared through the site. In order to build a community around the platform, a number of forums are offered covering energy system data and related topics.\n\nMost of the data on OpenEI is exposed as linked open data (LOD) (described elsewhere on this page). OpenEI also uses LOD methods to populate its definitions throughout the wiki with real-time connections to DBPedia, reegle, and Wikipedia.\n\nOpenEI has been used to classify geothermal resources in the United States. And to publicize municipal utility rates, again within the US.\n\nOpenGridMap employs crowdsourcing techniques to gather detailed data on electricity network components and then infer a realistic network structure using methods from statistics and graph theory. The scope of the project is worldwide and both distribution and transmission networks can be reverse engineered. The project is managed by the Chair of Business Information Systems, TUM Department of Informatics, Technical University of Munich, Munich, Germany. The project maintains a website and a Facebook page and provides an Android mobile app to help the public document electrical devices, such as transformers and substations. The bulk of the data is being made available under a Creative Commons license. The processing software is written primarily in Python and MATLAB and is hosted on GitHub.\n\nOpenGridMap provides a tailored GIS web application, layered on OpenStreetMap, which contributors can use to upload and edit information directly. The same database automatically stores field recordings submitted by the mobile app. Subsequent classification by experts allows normal citizens to document and photograph electrical components and have them correctly identified. The project is experimenting with the use of hobby drones to obtain better information on associated facilities, such as photovoltaic installations. Transmission line data is also sourced from and shared with OpenStreetMap. Each component record is verified by a moderator.\n\nOnce sufficient data is available, the transnet software is run to produce a likely network, using statistical correlation, Voronoi partitioning, and minimum spanning tree (MST) algorithms. The resulting network can be exported in CSV (separate files for nodes and lines), XML, and CIM formats. CIM models are well suited for translation into software-specific data formats for further analysis, including power grid simulation. Transnet also displays descriptive statistics about the resulting network for visual confirmation.\n\nThe project is motivated by the need to provide datasets for high-resolution energy system models, so that energy system transitions (like the German \"Energiewende\") can be better managed, both technically and policy-wise. The rapid expansion of renewable generation and the anticipated uptake of electric vehicles means that electricity system models must increasingly represent distribution and transmission networks in some detail.\n\n, OpenGridMap techniques have been used to estimate the low voltage network in the German city of Garching and to estimate the high voltage grids in several other countries.\n\nreegle is a clean energy information portal covering renewable energy, energy efficiency, and climate compatible development topics. reegle was launched in 2006 by REEEP and REN21 with funding from the Dutch (VROM), German (BMU), and UK (Defra) environment ministries. Originally released as a specialized internet search engine, reegle was relaunched in 2011 as an information portal.\n\nreegle offers and utilizes linked open data (LOD) (described elsewhere on this page). Sources of data include UN and World Bank databases, as well as dedicated partners around the world. reegle maintains a comprehensive structured glossary (driven by an LOD-compliant thesaurus) of energy and climate compatible development terms to assist with the tagging of datasets. The glossary also facilitates intelligent web searches.\n\nreegle offers country profiles which collate and display energy data on a per-country basis for most of the world. These profiles are kept current automatically using LOD techniques.\n\nRenewables.ninja is a website that can calculate the hourly power output from solar photovoltaic installations and wind farms located anywhere in the world. The website is a joint project between the Department of Environmental Systems Science, ETH Zurich, Zürich, Switzerland and the Centre for Environmental Policy, Imperial College London, London, United Kingdom. The website went live during September 2016. The resulting time series are provided under a Creative Commons license and the underlying power plant models are published using a BSD-new license. , only the solar model, written in Python, has been released.\n\nThe project relies on weather data derived from meteorological reanalysis models and weather satellite images. More specifically, it uses the 2016 MERRA-2 reanalysis dataset from NASA and satellite images from CM-SAF SARAH. For locations in Europe, this weather data is further \"corrected\" by country so that it better fits with the output from known PV installations and windfarms. Two 2016 papers describe the methods used in detail in relation to Europe. The first covers the calculation of PV power. And the second covers the calculation of wind power.\n\nThe website displays an interactive world map to aid the selection of a site. Users can then choose a plant type and enter some technical characteristics. , only year 2014 data can be served, due to technical restrictions. The results are automatically plotted and are available for download in hourly CSV format with or without the associated weather information. The site offers an API for programmatic dataset recovery using token-based authorization. Examples deploying cURL and Python are provided.\n\nA number of studies have been undertaking using the power production datasets underpinning the website (these studies predate the launch of the website), with the bulk focusing on energy options for Great Britain.\n\nThe SMARD site (pronounced \"smart\") serves electricity market data from Germany, Austria, and Luxembourg and also provides visual information. The electricity market plots and their underlying time series are released under a permissive CC BY 4.0 license. The site itself was launched on 3July 2017 in German and an English translation followed shortly. The data portal is mandated under the German Energy Industry Act (\"\" or \"EnWG\") section §111d, introduced as an amendment on 13October 2016. Four table formats are offered: CSV, XLS, XML, and PDF. The maximum sampling resolution is . Market data visuals or plots can be downloaded in PDF, SVG, PNG, and JPG formats. Representative output is shown in the thumbnail (on the left), in this case mid-winter dispatch over two days for the whole of Germany. The horizontal ordering by generation type is first split into renewable and conventional generation and then based on merit.\n\n\n\n"}
{"id": "2399976", "url": "https://en.wikipedia.org/wiki?curid=2399976", "title": "Physical universe", "text": "Physical universe\n\nIn religion and esotericism, the term \"physical universe\" or \"material universe\" is used to distinguish the physical matter of the universe from a proposed spiritual or supernatural essence. \n\nIn the Book of Veles, and perhaps in traditional Slavic mythology, the physical universe is referred to as Yav. Gnosticism holds that the physical universe was created by a Demiurge. In Dharmic religions Maya is believed to be the illusion of a physical universe.\n\nPhysicalism, a type of monism, holds that only physical things exist. This is also known as metaphysical naturalism.\n\n"}
{"id": "169115", "url": "https://en.wikipedia.org/wiki?curid=169115", "title": "Preternatural", "text": "Preternatural\n\nThe preternatural or praeternatural is that which appears outside or beside (Latin \"\") the natural. It is \"suspended between the mundane and the miraculous\".\n\nIn theology, the term is often used to distinguish marvels or deceptive trickery, often attributed to witchcraft or demons, from the purely divine power of the genuinely supernatural to violate the laws of nature. In the early modern period the term was used by scientists to refer to abnormalities and strange phenomena of various kinds that seemed to depart from the norms of nature.\n\nMedieval theologians made a clear distinction between the natural, the preternatural and the supernatural. Thomas Aquinas argued that the supernatural consists in \"God’s unmediated actions\"; the natural is \"what happens always or most of the time\"; and the preternatural is \"what happens rarely, but nonetheless by the agency of created beings...Marvels belong, properly speaking, to the realm of the preternatural.\" Theologians, following Aquinas, argued that only God had the power to disregard the laws of nature that he has created, but that demons could manipulate the laws of nature by a form of trickery, to deceive the unwary into believing they had experienced real miracles. According to historian Lorraine Daston,\n\nAlthough demons, astral intelligences, and other spirits might manipulate natural causes with superhuman dexterity and thereby work marvels, as mere creatures they could never transcend from the preternatural to the supernatural and work genuine miracles.\n\nBy the 16th century, the term \"preternatural\" was increasingly used to refer to demonic activity comparable to the use of magic by human adepts: The Devil, \"being a natural Magician … may perform many acts in ways above our knowledge, though not transcending our natural power.\" According to the philosophy of the time, preternatural phenomena were not contrary to divine law, but used hidden, or occult powers that violated the \"normal\" pattern of natural phenomena.\n\nWith the emergence of early modern science, the concept of the preternatural increasingly came to be used to refer to strange or abnormal phenomena that seemed to violate the normal working of nature, but which were not associated with magic and witchcraft. This was a development of the idea that preternatural phenomena were fake miracles. As Daston puts it, \"To simplify the historical sequence somewhat: first, preternatural phenomena were demonized and thereby incidentally naturalized; then the demons were deleted, leaving only the natural causes.\" The use of the term was especially common in medicine, for example in John Brown's \"A Compleat Treatise of Preternatural Tumours\" (1678), or William Smellie's \"A Collection of Preternatural Cases and Observations in Midwifery\" (1754).\n\nIn the 19th century the term was appropriated in anthropology to refer to folk beliefs about fairies, trolls and other such creatures which were not thought of as demonic, but which were perceived to affect the natural world in unpredictable ways. According to Thorstein Veblen, such preternatural agents were often thought of as forces somewhere between supernatural beings and material processes. \"The preternatural agency is not necessarily conceived to be a personal agent in the full sense, but it is an agency which partakes of the attributes of personality to the extent of somewhat arbitrarily influencing the outcome of any enterprise, and especially of any contest.\"\n\nThe linguistic association between individual agents and unexplained or unfortunate circumstances remains. Many people attribute occurrences that are known to be material processes, such as \"gremlins in the engine\", a \"ghost in the machine\", or attributing motives to objects: \"the clouds are threatening\". The anthropomorphism in our daily life is a combination of the above cultural stems, as well as the manifestation of our pattern-projecting minds.\n\nIn 2011, Penn State Press began publishing a learned journal entitled \"Preternature: Critical and Historical Studies on the Preternatural\". Edited by Kirsten Uszkalo and Richard Raiswell, the journal is dedicated to publishing articles, reviews and short editions of original texts that deal with conceptions and perceptions of the preternatural in any culture and in any historical period. The journal covers \"magics, witchcraft, spiritualism, occultism, prophecy, monstrophy, demonology, and folklore.\"\n\n\n"}
{"id": "1413688", "url": "https://en.wikipedia.org/wiki?curid=1413688", "title": "Primary energy", "text": "Primary energy\n\nPrimary energy (PE) is an energy form found in nature that has not been subjected to any human engineered conversion process. It is energy contained in raw fuels, and other forms of energy received as input to a system. Primary energy can be non-renewable or renewable.\n\nWhere primary energy is used to describe fossil fuels, the embodied energy of the fuel is available as thermal energy and around 70% is typically lost in conversion to electrical or mechanical energy. There is a similar 60-80% conversion loss when solar and wind energy is converted to electricity, but today's UN conventions on energy statistics counts the electricity made from wind and solar as the primary energy itself for these sources. One consequence of this counting method is that the contribution of wind and solar energy is under reported compared to fossil energy sources, and there is hence an international debate on how to count primary energy from wind and solar. \n\nTotal primary energy supply (TPES) is the sum of production and imports subtracting exports and storage changes.\n\nThe concept of primary energy is used in energy statistics in the compilation of energy balances, as well as in the field of energetics. In energetics, a primary energy source (PES) refers to the energy forms required by the energy sector to generate the supply of energy carriers used by human society.\n\nSecondary energy is a carrier of energy, such as electricity. These are produced by conversion from a primary energy source.\n\nThe use of primary energy as a measure ignores conversion efficiency. Thus forms of energy with poor conversion efficiency, particularly the thermal sources, coal, gas and nuclear are overstated, whereas energy sources such as hydroelectricity which are converted efficiently, while a small fraction of primary energy are significantly more important than their total raw energy supply may seem to imply.\n\nPE and TPES are better defined in the context of worldwide energy supply.\n\nPrimary energy sources should not be confused with the energy system components (or conversion processes) through which they are converted into energy carriers.\n\nPrimary energy sources are transformed in energy conversion processes to more convenient forms of energy that can directly be used by society, such as electrical energy, refined fuels, or synthetic fuels such as hydrogen fuel. In the field of energetics, these forms are called energy carriers and correspond to the concept of \"secondary energy\" in energy statistics.\n\nEnergy carriers are energy forms which have been transformed from primary energy sources. Electricity is one of the most common energy carriers, being transformed from various primary energy sources such as coal, oil, natural gas, and wind. Electricity is particularly useful since it has low entropy (is highly ordered) and so can be converted into other forms of energy very efficiently. District heating is another example of secondary energy.\n\nAccording to the laws of thermodynamics, primary energy sources cannot be produced. They must be available to society to enable the production of energy carriers.\n\nConversion efficiency varies. For thermal energy, electricity and mechanical energy production is limited by Carnot's theorem, and generates a lot of waste heat. Other non-thermal conversions can be more efficient. For example, while wind turbines do not capture all of the wind's energy, they have a high conversion efficiency and generate very little waste heat since wind energy is low entropy. In principle solar photovoltaic conversions could be very efficient, but current conversion can only be done well for narrow ranges of wavelength, whereas solar thermal is also subject to Carnot efficiency limits. Hydroelectric power is also very ordered, and converted very efficiently. The amount of usable energy is the exergy of a system.\n\nSite energy is the term used in North America for the amount of end-use energy of all forms consumed at a specified location. This can be a mix of primary energy (such as natural gas burned at the site) and secondary energy (such as electricity). Site energy is measured at the campus, building, or sub-building level and is the basis for energy charges on utility bills.\n\nSource energy, in contrast, is the term used in North America for the amount of primary energy consumed in order to provide a facility’s site energy. It is always greater than the site energy, as it includes all site energy and adds to it the energy lost during transmission, delivery, and conversion. While source or primary energy provides a more complete picture of energy consumption, it cannot be measured directly and must be calculated using conversion factors from site energy measurements. For electricity, a typical value is three units of source energy for one unit of site energy. However, this can vary considerably depending on factors such as the primary energy source or fuel type, the type of power plant, and the transmission infrastructure. One full set of conversion factors is available as technical reference from Energy STAR.\n\nEither site or source energy can be an appropriate metric when comparing or analyzing energy use of different facilities. The U.S Energy Information Administration, for example, uses primary (source) energy for its energy overviews but site energy for its Commercial Building Energy Consumption Survey and Residential Building Energy Consumption Survey. The US Environmental Protection Agency's Energy STAR program recommends using source energy, and the US Department of Energy uses site energy in its definition of a zero net energy building.\n\nEnergy accidents are accidents that occur in systems that provide energy or power. These can result in fatalities, as can the normal running of many systems, for example those deaths due to pollution.\n\nGlobally, coal is responsible for 100,000 deaths per trillion kWh.\n\n\n\n"}
{"id": "8994982", "url": "https://en.wikipedia.org/wiki?curid=8994982", "title": "Relational space", "text": "Relational space\n\nThe relational theory of space is a metaphysical theory according to which space is composed of relations between objects, with the implication that it cannot exist in the absence of matter. Its opposite is the container theory. A relativistic physical theory implies a relational metaphysics, but not the other way round: even if space is composed of nothing but relations between observers and events, it would be conceptually possible for all observers to agree on their measurements, whereas relativity implies they will disagree. Newtonian physics can be cast in relational terms, but Newton insisted, for philosophical reasons, on absolute (container) space. The subject was famously debated by Gottfried Wilhelm Leibniz and a supporter of Newton's in the Leibniz–Clarke correspondence.\n\nAn absolute approach can also be applied to time, with, for instance, the implication that there might have been vast epochs of time before the first event.\n\n"}
{"id": "19991258", "url": "https://en.wikipedia.org/wiki?curid=19991258", "title": "Sociology of space", "text": "Sociology of space\n\nThe sociology of space is a sub-discipline of sociology that mostly borrows from theories developed within the discipline of geography, including the sub fields of human geography, economic geography, and feminist geography. The \"sociology\" of space examines the social and material constitution of spaces. It is concerned with understanding the social practices, institutional forces, and material complexity of how humans and spaces interact. The sociology of space is an inter-disciplinary area of study, drawing on various theoretical traditions including Marxism, postcolonialism, and Science and Technology Studies, and overlaps and encompasses theorists with various academic disciplines such as geography and architecture. Edward T. Hall developed the study of Proxemics which concentrates on the empirical analysis of space in psychology. \n\nSpace is one of the most important concepts within the disciplines of social science as it is fundamental to our understanding of geography. The term \"space\" has been defined variously by scholars:\n\nIn general terms, the Oxford English Dictionary defines space in two ways;\n\n1. A continuous extension viewed with or without reference to the existence of objects within it. \n2. The interval between points or objects viewed as having one, two or three dimensions.\n\nHowever, the human geographers’ interest is in the objects within the space and their relative position, which involves the description, explanation and prediction of the distribution of phenomena. Thus, the relationships between objects in space is the central of the study.\n\nMichel Foucault defines space as;\n“The space in which we live, which draws us out of ourselves, in which the erosion of our lives, our time and our history occurs, the space that claws and gnaws at us, is also, in itself, a heterogeneous space…..we live inside a set of relations.\n\nNigel Thrift also defines space as;\n\"The outcome of a series of highly problematic temporary settlements that divide and connect things up into different kinds of collectives which are slowly provided with the meaning which render them durable and sustainable.\" \n\nIn short, \"space\" is the social space in which we live and create relationships with other people, societies and surroundings. Space is an outcome of the hard and continuous work of building up and maintaining collectives by bringing different things into alignments. All kinds of different spaces can and therefore do exist which may or may not relate to each other. Thus, through space, we can understand more about social action.\n\nGeorg Simmel has been seen as the classical sociologist who was most important to this field. Simmel wrote on \"the sociology of space\" in his 1908 book \"Sociology: Investigations on the Forms of Sociation\". His concerns included the process of metropolitanisation and the separation of leisure spaces in modern economic societies.\n\nThe category of space long played a subordinate role in sociological theory formation. Only in the late 1980s did it come to be realised that certain changes in society cannot be adequately explained without taking greater account of the spatial components of life. This shift in perspective is referred to as the topological turn. The space concept directs attention to organisational forms of juxtaposition. The focus is on differences between places and their mutual influence. This applies equally for the micro-spaces of everyday life and the macro-spaces at the nation-state or global levels.\n\nThe theoretical basis for the growing interest of the social sciences in space was set primarily by English and French-speaking sociologists, philosophers, and human geographers. Of particular importance is Michel Foucault’s essay on “Of Other Spaces”, in which the author proclaims the “age of space”, and Henri Lefebvre’s seminal work “La production de l’espace”. The latter provided the grounding for Marxist spatial theory on which David Harvey, Manuel Castells, Edward Soja, and others have built. Marxist theories of space, which are predicated on a structural, i.e., capitalist or global determinants of spaces and the growing homogenization of space, are confronted by action theoretical conceptions, which stress the importance of the corporeal placing and the perception of spaces as albeit habitually predetermined but subjective constructions. One example is the theory of space of the German sociologist Martina Löw. Approaches deriving from the post-colonialism discourse have attracted greater attention in recent years. Also in contrast to (neo)Marxist concepts of space, British geographer Doreen Massey and German sociologist Helmuth Berking, for instance, emphasise the heterogeneity of local contexts and the place-relatedness of our knowledge about the world.\n\nMartina Löw developed the idea of a \"relational\" model of space, which focuses on the “orderings” of living entities and social goods, and examines how space is constituted in processes of perception, recall, or ideation to manifest itself as societal structure. From a social theory point of view, it follows on from the theory of structuration proposed by Anthony Giddens, whose concept of the “duality of structure” Löw extends sociological terms into a “duality of space.” The basic idea is that individuals act as social agents (and constitute spaces in the process), but that their action depends on economic, legal, social, cultural, and, finally, spatial structures. Spaces are hence the outcome of action. At the same time, spaces structure action, that is to say spaces can both constrain and enable action.\n\nWith respect to the constitution of space, Löw distinguishes analytically between two, generally mutually determining factors: “spacing” and “synthesis.” Spacing refers to the act of placing or the state of being placed of social goods and people in places. According to Löw, however, an ordering created through placings is only effectively constituted as space where the elements that compose it are actively interlinked by people – in processes of perception, ideation, or recall. Löw calls this synthesis. This concept has been empirically tested in studies such as those by Lars Meier (who examined the constitution of space in the everyday life of financial managers in London and Singapore), Cedric Janowicz (who carried out an ethnographical-space sociological study of food supply in the Ghanaian city of Accra), and Silke Streets (who looked at processes of space constitution in the creative industries in Leipzig).\n\nThe most important proponent of Marxist spatial theory was Henri Lefebvre. He proposed \"social space\" to be where the relations of production are reproduced and that dialectical contradictions were spatial rather than temporal. Lefèbvre sees the societal production of space as a dialectical interaction between three factors. Space is constituted:\n\n\nIn Lefebvre’s view of the 1970s, this spatial production resulted in a space of non-reflexive everydayness marked by alienation, dominating through mathematical-abstract concepts of space, and reproduced in spatial practice. Lefebvre sees a line of flight from alienated spatiality in the spaces of representation – in notions of non-alienated, mythical, pre-modern, or artistic visions of space.\n\nMarxist spatial theory was given decisive new impetus by David Harvey, in particular, who was interested in the effects of the transition from Fordism to “flexible accumulation” on the experience of space and time. He shows how various innovations at the economic and technological levels have breached the crisis-prone inflexibility of the Fordist system, thus increasing the turnover rate of capital. This causes a general acceleration of economic cycles. According to Harvey, the result is “time-space compression.” While the feeling for the long term, for the future, for continuity is lost, the relationship between proximity and distance becomes more and more difficult to determine.\n\nTheories of space that are inspired by the post-colonialism discourse focus on the heterogeneity of spaces. According to Doreen Massey, calling a country in Africa a “developing country” is not appropriate, since this expression implies that spatial difference is temporal difference (Massey 1999b). This logic treats such a country not as different but merely as an early version of countries in the “developed” world, a view she condemns as \"Eurocentrism.\" In this vein, Helmuth Berking criticises theories that postulate the increasing homogenisation of the world through globalisation as “globocentrism.” He confronts this with the distinctiveness and importance of local knowledge resources for the production of (different and specific) places. He claims that local contexts form a sort of framework or filter through which global processes and globally circulating images and symbols are appropriated, thus attaining meaning. For instance, the film character Conan the Barbarian is a different figure in radical rightwing circles in Germany than in the black ghettoes of the Chicago Southside, just as McDonald’s means something different in Moscow than in Paris.\n\nHenri Lefebvre (see also Edward Soja) says that (social) space is a (social) product, or a complex social construction (based on values, and the social production of meanings) which affects spatial practices and perceptions. He explains space embraces a multitude of intersection in his great book, “Production of Space”. That means that we need to consider how the various modes of spatial production relate to each other.\n\nHe argues that there are three aspects to our spatial existence, which exist in a kind of triad:\n\n1. First Space\n\"The spatial practice of a society secretes that society's space; it propounds and presupposes it, in a dialectical interaction; it produces it slowly and surely as it masters and appropriates it.\"\n\n2. Second Space\n\"Conceptualized space, the space of scientists, planners, urbanists, technocratic subdividers and social engineers, as of a certain type of artist with a scientific bent -- all of whom identify what is lived and what is perceived with what is conceived.\"\n\n3. Third Space\n\"Space as directly lived through its associated images and symbols.\"\n\nEven though there are many disciplines in the study of Human Geography, the most well-known approach is “The third space” formulated by Edward Soja. In unitary theory, there are three approaches; first space, second space and third space. First space is physical space, and spaces are measurable and mappable. The second space is a mental or conceived space which comes from our thinking and ideas. However, the third space is a social space/lived space which is a social product that is a space created by society under oppression or marginalization that want to reclaim the space of inequality and make it into something else. Soja argues that our old ways to thinking about space (first and second space theories) can no longer accommodate the way the world works because he believed that spaces may not be contained within one social category, they may include different aspects of many categories or developed within the boundaries of a number of category. For instance, two different cultures combine together and emerge as a third culture. This third hybrid space displaces the original values that constitute it and set up new values and perspectives that is different from the first two spaces. Thus, the third space theory can explain some of the complexity of poverty, social exclusion and social inclusion, gender and race issues.\n\nIn the work of geographer and critical theorist Nigel Thrift, he wrote a rational view of space in which, rather than seeing space being viewed as a container within which the world proceeds, space should be seen as a co-product of these proceedings. He explained about four constructed space in modern human geography. \nThere are four different kinds of space according to how modern geography thinks about space. They are 1. Empirical Construction of Space, 2. Unblocking space, 3. Image space and 4. Place Space.\n\nFirst Space is the empirical construction of space. Empirical space refers to the process whereby the mundane fabric of daily life is constructed. These simple things like, cars, houses, mobiles, computers and roads are very simple but they are great achievements of our daily life and they play very important role in making up who we are today. For example, today’s technology such as GPS did not suddenly come into existence; in fact, it is laid down in the 18th century and developed throughout time. The first space is real and tangible, and it is also known as physical space.\nSecond space is the unblocking space. This type of space refers to the process whereby routine pathways of interaction as set up around which boundaries are often drawn. The routine may include the movement of office workers, the interaction of drunk teenagers, and the flow of goods, money, people, and information. Unlike the old time in geography when people accepted a space as blocked boundary (Example: A capitalist space, neoliberal space or city space), we began to realize that there is no such thing like boundaries in space. The space of the world is flowing and transforming continuously that it is very difficult to describe in a fixed way. The second space is ideology/conceptual and it is also known as mental space. For example, the second space will explain the behaviors of people from different social class and the social segregation among rich and poor people. \nThird space is the image space that refers to the process whereby the images has produced new kind of space. The images may be in different form and shape; ranging from painting to photograph, from portrait to post card, and from religious theme to entertainment. Nowadays, we are highly influenced by images in many ways and these certain images can tell us new social and cultures values, or something new about how we see the world. Images, symbols and sign do have some kind of spatial expression. \nFourth space is the place that refers to the process whereby spaces are ordered in ways that open up affective and other embodied potentials. Place space has more meaning than a place, and it can represent as different type of space. This fourth type of space tries to understand that place is a vital actor in bringing up people's lives in certain ways and place will let us to understand all kind of things which are hidden form us..\n\nAndrew Herod mentioned that scale, within human geography, is typically seen in one of the two ways: either as a real material thing which actually exists and is the result of political struggle and/or social process, or as a way of framing our understanding of the world. People’s lives across the globe have been re-scaling by contemporary economic, political, cultural and social processes, such as globalization, in complex ways. As a result, we have seen the creation of supra-national political bodies such as the European Union, the devolution of political power from the nation-state to regional political bodies. We have also experienced the increasing homogenization and ‘Americanization’ through the process of globalization while the locals’ tendencies (or counter force)among people who defend traditional ways of life increase around the world .The process of re-scaling people‘s lives and the relationship between the two extremes of our scaled lives- the ‘global’ and the ‘local’ were brought into question.\n\nUntil the 1980s, theorizing the concept of ‘scale’ itself was taken for granted although physical and human geographers looked at issues from ‘regional scale’ or‘national scale’. The questions such as whether scale is simply a mental device categorizing and ordering the world or whether scales really exists as material social products, particularly, were debated among materialists and idealists. Some geographers draw upon Immanuel Kant’s idealist philosophy that scales were handy conceptual mechanism for ordering the world while others, by drawing upon Marxist ideas of materialism, argue that scales really exist in the world and they were the real social products. For those idealists based on Kantian‘s inspiration, the ‘global’ is defined by the geologically given limits of the earth and the ‘local’ is defined as a spatial resolution useful for comprehending the process and practices. For materialists, the ‘national’ scale is a scale that had to be actively created through economic and political processes but not a scale existed in a logical hierarchy between global and the regional.\n\nThe notion of ‘becoming’ and the focus on the politics of producing scales have been central to materialist arguments concerning the global scale. It is important to recognize that social actors may have to work just as hard to become ‘local’as they have to work to become ‘global’. People paid attention to how transnational corporations have ‘gone global’, how institutions of governance have‘become’ supra-national and how labour unions have sought to ‘globalize’ their operations to match those of an increasingly ‘globalized’ city.\n\nFor the scale ‘global’ and ‘local’, Kevin Cox mentioned that moving from the local to the global scale ‘is not a movement from one discrete arena to another’ but a process of developing networks of associations that allow actors to shift between various spaces of engagement. According to his view, ‘scale’ is seen as a process rather than as a fixed entity and, in other words, the global and the local are not static ’arenas’within which social life plays out but are constantly made by social actions.For example, a political organization might attempt to go ‘global’ to engage with actors or opportunities outside of its own space; likewise, a transnational corporation may attempt to ‘go local’ through tailoring its products and operations in different places.\n\nGibson-Graham (2002) has identified at least six ways in which the relationship between the local and the global is often viewed.\n\n1. The global and the local are seen as interpretive frames for analyzing situations\n\n2. Drawing on Dirlik, Gibson-Graham suggests that in such a representation, the global is ‘something more than the national or regional ..anything other than the local’. Meaning that, the global and the local each derive meaning from what they are not.\n\n3. According to French social theorist Bruno Latour, the local and the global ‘offer different points of view on networks that are by nature neither local nor global, but are more or less long and more or less connected. Also, in Latour’s view, it is impossible to distinguish where the local ends and the global begins.\n\n4. The concept ‘The global is local’ was proposed by Gibson-Graham. For instance, multinational firms are actually ‘multi local‘ rather than ‘global’.\n\n5. The local is global. In this view, the local is an entry point to the world of global flows which encircle the planet.\n\n6. The global and the local are actually the processes rather than the locations. All spaces are the hybrids of global and local; so they are ‘glocal.’\n\nThere are some western thoughts that greater size and extensiveness imply domination and superior power, such that the local is often represented as ‘small and relatively powerless, defined and confined by the global’. So, the global is a force and the local is its field of play. However, the local can serve as a powerful scale of political organization; the global is not a scale just controlled by capital – those who challenge capital can also organize globally( Herod, A). There has been the concept ‘Think globally and act locally’ viewed by neoliberals.\n\nFor representing how the world is scaled, there are five different and popular metaphors: they are the ladder, concentric circles, Matryoshka nesting dolls, earthworm burrows and tree roots. First, in using such a metaphor of hierarchical ladder, the global as the highest rung on the ladder is seen to be above the local and all other scales. Second, the use of concentric metaphor leaves us with a particular way of conceptualizing the scalar relationship between places. In this second metaphor, the local is seen as a relatively small circle, with the regional as a larger circle encompassing it, while the national and the global scales are still larger circles encompassing the local and the regional. For the hierarchy of Russian Matryoshka nesting dolls, the global can contain other scales but this does not work the other way round; for instance, the local cannot contain the global. For the fourth metaphor concerning with thinking on scale, what French social theorist Bruno Latour argued is that a world of places is ‘networked’ together. Such the metaphor leaves us with an image of scale in which the global and the local are connected together and not totally separated from each other. For the tree roots metaphor similar with the earthworm burrow metaphor, as the earthworm burrows or tree roots penetrating different strata of the soil, it is difficult to determine exactly where one scale ends and another begins. When thinking about the use of metaphor, it should be aware that the choice of metaphor over another is not made on the basis of which is empirically a ‘more accurate’representation of something but, on the basis of how someone is attempting to understand a particular phenomenon.\n\nSuch an appreciation of metaphors is important because it suggests that how we talk about scale impacts upon the ways in which we engage socially and politically with our scaled world and that may impact on how we conduct our social, economic and political praxis and so make landscapes ( Herod,A )\n\n\n"}
{"id": "13581828", "url": "https://en.wikipedia.org/wiki?curid=13581828", "title": "Surface conductivity", "text": "Surface conductivity\n\nSurface conductivity is an additional conductivity of an electrolyte in the vicinity of charged surfaces. Close to charged surfaces a layer of counter ions of opposite polarity exists which is attracted by the surface charges. This layer of higher ionic concentration is a part of the interfacial double layer. The concentration of the ions in this layer is higher as compared to the volume conductivity far from the charged surface and leads to a higher conductivity of this layer.\n\nSmoluchowski was the first to recognize the importance of surface conductivity at the beginning of the 20th century.\n\nThere is a detailed description of surface conductivity by Lyklema in \"Fundamentals of Interface and Colloid Science\" \n\nThe Double Layer (DL) has two regions, according to the well established Gouy-Chapman-Stern model, Ref.2. The upper level, which is in contact with the bulk fluid is the diffuse layer. The inner layer that is in contact with interface is the Stern layer.\n\nIt is possible that the lateral motion of ions in both parts of the DL contributes to the surface conductivity.\n\nThe contribution of the Stern layer is less well described. It is often called \"additional surface conductivity\".\n\nThe theory of the surface conductivity of the diffuse part of the DL was developed by Bikerman. He derived a simple equation that links surface conductivity κ with the behaviour of ions at the interface. For symmetrical electrolyte and assuming identical ions diffusion coefficients D=D=D it is given in Ref.2:\n\nwhere\n\nThe parameter \"m\" characterizes the contribution of electro-osmosis to the motion of ions within the DL:\n\nThe Dukhin number is a dimensionless parameter that characterizes the contribution of the surface conductivity to a variety of electrokinetic phenomena, such as, electrophoresis and electroacoustic phenomena.\n\n\nSurface conductivity may refer to the electrical conduction across a solid surface measured by surface probes. Experiments may be done to test this material property as in the n-type surface conductivity of p-type . Additionally, surface conductivity is measured in coupled phenomena such as photoconductivity, for example, for the metal oxide semiconductor ZnO. Surface conductivity differs from bulk conductivity for analogous reasons to the electrolyte solution case, where the charge carriers of holes (+1) and electrons (-1) play the role of ions in solution.\n"}
{"id": "31880", "url": "https://en.wikipedia.org/wiki?curid=31880", "title": "Universe", "text": "Universe\n\nThe Universe is all of space and time and their contents, including planets, stars, galaxies, and all other forms of matter and energy. While the spatial size of the entire Universe is still unknown, it is possible to measure the observable universe.\n\nThe earliest scientific models of the Universe were developed by ancient Greek and Indian philosophers and were geocentric, placing Earth at the centre of the Universe. Over the centuries, more precise astronomical observations led Nicolaus Copernicus to develop the heliocentric model with the Sun at the centre of the Solar System. In developing the law of universal gravitation, Sir Isaac Newton built upon Copernicus' work as well as observations by Tycho Brahe and Johannes Kepler's laws of planetary motion.\n\nFurther observational improvements led to the realization that our Sun is one of hundreds of billions of stars in a galaxy we call the Milky Way, which is one of at least hundreds of billions of galaxies in the Universe. Many of the stars in our galaxy have planets. At the largest scale galaxies are distributed uniformly and the same in all directions, meaning that the Universe has neither an edge nor a center. At smaller scales, galaxies are distributed in clusters and superclusters which form immense filaments and voids in space, creating a vast foam-like structure. Discoveries in the early 20th century have suggested that the Universe had a beginning and that space has been expanding since then, and is currently still expanding at an increasing rate.\n\nThe Big Bang theory is the prevailing cosmological description of the development of the Universe. Under this theory, space and time emerged together ago with a fixed amount of energy and matter that has become less dense as the Universe has expanded. After an initial accelerated expansion at around 10 seconds, and the separation of the four known fundamental forces, the Universe gradually cooled and continued to expand, allowing the first subatomic particles and simple atoms to form. Dark matter gradually gathered forming a foam-like structure of filaments and voids under the influence of gravity. Giant clouds of hydrogen and helium were gradually drawn to the places where dark matter was most dense, forming the first galaxies, stars, and everything else seen today. It is possible to see objects that are now further away than 13.799 billion light-years because space itself has expanded, and it is still expanding today. This means that objects which are now up to 46 billion light years away can still be seen in their distant past, because in the past when their light was emitted, they were much closer to the Earth.\n\nFrom studying the movement of galaxies, it has been discovered that the universe contains much more matter than is accounted for by visible objects; stars, galaxies, nebulas and interstellar gas. This unseen matter is known as dark matter  (\"dark\" means that there is a wide range of strong indirect evidence that it exists, but we have not yet detected it directly). The Lambda-CDM model is the most widely accepted model of our universe. It suggests that about [2015] of the mass and energy in the universe is a scalar field known as dark energy which is responsible for the current expansion of space, and about 25.8% [2015] is dark matter. Ordinary (\"baryonic\") matter is therefore only 4.9% [2015] of the physical universe. Stars, planets, and visible gas clouds only form about 6% of ordinary matter, or about 0.3% of the entire universe.\n\nThere are many competing hypotheses about the ultimate fate of the universe and about what, if anything, preceded the Big Bang, while other physicists and philosophers refuse to speculate, doubting that information about prior states will ever be accessible. Some physicists have suggested various multiverse hypotheses, in which the Universe might be one among many universes that likewise exist.\n\nThe physical Universe is defined as all of space and time (collectively referred to as spacetime) and their contents. Such contents comprise all of energy in its various forms, including electromagnetic radiation and matter, and therefore planets, moons, stars, galaxies, and the contents of intergalactic space. The Universe also includes the physical laws that influence energy and matter, such as conservation laws, classical mechanics, and relativity.\n\nThe Universe is often defined as \"the totality of existence\", or everything that exists, everything that has existed, and everything that will exist. In fact, some philosophers and scientists support the inclusion of ideas and abstract concepts – such as mathematics and logic – in the definition of the Universe. The word \"universe\" may also refer to concepts such as \"the cosmos\", \"the world\", and \"nature\".\n\nThe word \"universe\" derives from the Old French word \"univers\", which in turn derives from the Latin word \"universum\". The Latin word was used by Cicero and later Latin authors in many of the same senses as the modern English word is used.\n\nA term for \"universe\" among the ancient Greek philosophers from Pythagoras onwards was , \"tò pân\" (\"the all\"), defined as all matter and all space, and , \"tò hólon\" (\"all things\"), which did not necessarily include the void. Another synonym was , \"ho kósmos\" (meaning the world, the cosmos). Synonyms are also found in Latin authors (\"totum\", \"mundus\", \"natura\") and survive in modern languages, e.g., the German words \"Das All\", \"Weltall\", and \"Natur\" for \"Universe\". The same synonyms are found in English, such as everything (as in the theory of everything), the cosmos (as in cosmology), the world (as in the many-worlds interpretation), and nature (as in natural laws or natural philosophy).\n\nThe prevailing model for the evolution of the Universe is the Big Bang theory. The Big Bang model states that the earliest state of the Universe was an extremely hot and dense one, and that the Universe subsequently expanded and cooled. The model is based on general relativity and on simplifying assumptions such as homogeneity and isotropy of space. A version of the model with a cosmological constant (Lambda) and cold dark matter, known as the Lambda-CDM model, is the simplest model that provides a reasonably good account of various observations about the Universe. The Big Bang model accounts for observations such as the correlation of distance and redshift of galaxies, the ratio of the number of hydrogen to helium atoms, and the microwave radiation background.\n\nThe initial hot, dense state is called the Planck epoch, a brief period extending from time zero to one Planck time unit of approximately 10 seconds. During the Planck epoch, all types of matter and all types of energy were concentrated into a dense state, and gravity - currently the weakest by far of the four known forces - is believed to have been as strong as the other fundamental forces, and all the forces may have been unified. Since the Planck epoch, space has been expanding to its present scale, with a very short but intense period of cosmic inflation believed to have occurred within the first 10 seconds. This was a kind of expansion different from those we can see around us today. Objects in space did not physically move; instead the metric that defines space itself changed. Although objects in spacetime cannot move faster than the speed of light, this limitation does not apply to the metric governing spacetime itself. This initial period of inflation is believed to explain why space appears to be very flat, and much larger than light could travel since the start of the universe.\n\nWithin the first fraction of a second of the universe's existence, the four fundamental forces had separated. As the universe continued to cool down from its inconceivably hot state, various types of subatomic particles were able to form in short periods of time known as the quark epoch, the hadron epoch, and the lepton epoch. Together, these epochs encompassed less than 10 seconds of time following the Big Bang. These elementary particles associated stably into ever larger combinations, including stable protons and neutrons, which then formed more complex atomic nuclei through nuclear fusion. This process, known as Big Bang nucleosynthesis, only lasted for about 17 minutes and ended about 20 minutes after the Big Bang, so only the fastest and simplest reactions occurred. About 25% of the protons and all the neutrons in the universe, by mass, were converted to helium, with small amounts of deuterium (a form of hydrogen) and traces of lithium. Any other element was only formed in very tiny quantities. The other 75% of the protons remained unaffected, as hydrogen nuclei.\n\nAfter nucleosynthesis ended, the universe entered a period known as the photon epoch. During this period, the Universe was still far too hot for matter to form neutral atoms, so it contained a hot, dense, foggy plasma of negatively charged electrons, neutral neutrinos and positive nuclei. After about 377,000 years, the universe had cooled enough that electrons and nuclei could form the first stable atoms. This is known as recombination for historical reasons; in fact electrons and nuclei were combining for the first time. Unlike plasma, neutral atoms are transparent to many wavelengths of light, so for the first time the universe also became transparent. The photons released (\"decoupled\") when these atoms formed can still be seen today; they form the cosmic microwave background (CMB).\n\nAs the Universe expands, the energy density of electromagnetic radiation decreases more quickly than does that of matter because the energy of a photon decreases with its wavelength. At around 47,000 years, the energy density of matter became larger than that of photons and neutrinos, and began to dominate the large scale behavior of the universe. This marked the end of the radiation-dominated era and the start of the matter-dominated era.\n\nIn the earliest stages of the universe, tiny fluctuations within the universe's density led to concentrations of dark matter gradually forming. Ordinary matter, attracted to these by gravity, formed large gas clouds and eventually, stars and galaxies, where the dark matter was most dense, and voids where it was least dense. After around 100 - 300 million years, the first stars formed, known as Population III stars. These were probably very massive, luminous, non metallic and short-lived. They were responsible for the gradual reionization of the Universe between about 200-500 million years and 1 billion years, and also for seeding the universe with elements heavier than helium, through stellar nucleosynthesis. The Universe also contains a mysterious energy - possibly a scalar field - called dark energy, the density of which does not change over time. After about 9.8 billion years, the Universe had expanded sufficiently so that the density of matter was less than the density of dark energy, marking the beginning of the present dark-energy-dominated era. In this era, the expansion of the Universe is accelerating due to dark energy.\n\nOf the four fundamental interactions, gravitation is the dominant at astronomical length scales. Gravity's effects are cumulative; by contrast, the effects of positive and negative charges tend to cancel one another, making electromagnetism relatively insignificant on astronomical length scales. The remaining two interactions, the weak and strong nuclear forces, decline very rapidly with distance; their effects are confined mainly to sub-atomic length scales.\n\nThe Universe appears to have much more matter than antimatter, an asymmetry possibly related to the CP violation. This imbalance between matter and antimatter is partially responsible for the existence of all matter existing today, since matter and antimatter, if equally produced at the Big Bang, would have completely annihilated each other and left only photons as a result of their interaction. The Universe also appears to have neither net momentum nor angular momentum, which follows accepted physical laws if the Universe is finite. These laws are the Gauss's law and the non-divergence of the stress-energy-momentum pseudotensor.\n\nThe size of the Universe is somewhat difficult to define. According to the general theory of relativity, far regions of space may never interact with ours even in the lifetime of the Universe due to the finite speed of light and the ongoing expansion of space. For example, radio messages sent from Earth may never reach some regions of space, even if the Universe were to exist forever: space may expand faster than light can traverse it.\n\nDistant regions of space are assumed to exist and to be part of reality as much as we are, even though we can never interact with them. The spatial region that we can affect and be affected by is the observable universe. The observable universe depends on the location of the observer. By traveling, an observer can come into contact with a greater region of spacetime than an observer who remains still. Nevertheless, even the most rapid traveler will not be able to interact with all of space. Typically, the observable universe is taken to mean the portion of the Universe that is observable from our vantage point in the Milky Way.\n\nThe proper distance—the distance as would be measured at a specific time, including the present—between Earth and the edge of the observable universe is 46 billion light-years (14 billion parsecs), making the diameter of the observable universe about 93 billion light-years (28 billion parsecs). The distance the light from the edge of the observable universe has travelled is very close to the age of the Universe times the speed of light, , but this does not represent the distance at any given time because the edge of the observable universe and the Earth have since moved further apart. For comparison, the diameter of a typical galaxy is 30,000 light-years (9,198 parsecs), and the typical distance between two neighboring galaxies is 3 million light-years (919.8 kiloparsecs). As an example, the Milky Way is roughly 100,000–180,000 light years in diameter, and the nearest sister galaxy to the Milky Way, the Andromeda Galaxy, is located roughly 2.5 million light years away.\n\nBecause we cannot observe space beyond the edge of the observable universe, it is unknown whether the size of the Universe in its totality is finite or infinite. Estimates for the total size of the universe, if finite, reach as high as formula_1 megaparsecs, implied by one resolution of the No-Boundary Proposal.\n\nAstronomers calculate the age of the Universe by assuming that the Lambda-CDM model accurately describes the evolution of the Universe from a very uniform, hot, dense primordial state to its present state and measuring the cosmological parameters which constitute the model. This model is well understood theoretically and supported by recent high-precision astronomical observations such as WMAP and Planck. Commonly, the set of observations fitted includes the cosmic microwave background anisotropy, the brightness/redshift relation for Type Ia supernovae, and large-scale galaxy clustering including the baryon acoustic oscillation feature. Other observations, such as the Hubble constant, the abundance of galaxy clusters, weak gravitational lensing and globular cluster ages, are generally consistent with these, providing a check of the model, but are less accurately measured at present. Assuming that the Lambda-CDM model is correct, the measurements of the parameters using a variety of techniques by numerous experiments yield a best value of the age of the Universe as of 2015 of 13.799 ± 0.021 billion years.\nOver time, the Universe and its contents have evolved; for example, the relative population of quasars and galaxies has changed and space itself has expanded. Due to this expansion, scientists on Earth can observe the light from a galaxy 30 billion light years away even though that light has traveled for only 13 billion years; the very space between them has expanded. This expansion is consistent with the observation that the light from distant galaxies has been redshifted; the photons emitted have been stretched to longer wavelengths and lower frequency during their journey. Analyses of Type Ia supernovae indicate that the spatial expansion is accelerating.\n\nThe more matter there is in the Universe, the stronger the mutual gravitational pull of the matter. If the Universe were \"too\" dense then it would re-collapse into a gravitational singularity. However, if the Universe contained too \"little\" matter then the self-gravity would be too weak for astronomical structures, like galaxies or planets, to form. Since the Big Bang, the universe has expanded monotonically. Perhaps unsurprisingly, our universe has just the right mass-energy density, equivalent to about 5 protons per cubic meter, which has allowed it to expand for the last 13.8 billion years, giving time to form the universe as observed today.\n\nThere are dynamical forces acting on the particles in the Universe which affect the expansion rate. Before 1998, it was expected that the expansion rate would be decreasing as time went on due to the influence of gravitational interactions in the Universe; and thus there is an additional observable quantity in the Universe called the deceleration parameter, which most cosmologists expected to be positive and related to the matter density of the Universe. In 1998, the deceleration parameter was measured by two different groups to be negative, approximately -0.55, which technically implies that the second derivative of the cosmic scale factor formula_2 has been positive in the last 5-6 billion years. This acceleration does not, however, imply that the Hubble parameter is currently increasing; see deceleration parameter for details.\n\nSpacetimes are the arenas in which all physical events take place. The basic elements of spacetimes are events. In any given spacetime, an event is defined as a unique position at a unique time. A spacetime is the union of all events (in the same way that a line is the union of all of its points), formally organized into a manifold.\n\nThe Universe appears to be a smooth spacetime continuum consisting of three spatial dimensions and one temporal (time) dimension (an event in the spacetime of the physical Universe can therefore be identified by a set of four coordinates: (\"x\", \"y\", \"z\", \"t\") ). On the average, space is observed to be very nearly flat (with a curvature close to zero), meaning that Euclidean geometry is empirically true with high accuracy throughout most of the Universe. Spacetime also appears to have a simply connected topology, in analogy with a sphere, at least on the length-scale of the observable Universe. However, present observations cannot exclude the possibilities that the Universe has more dimensions (which is postulated by theories such as the String theory) and that its spacetime may have a multiply connected global topology, in analogy with the cylindrical or toroidal topologies of two-dimensional spaces.\nThe spacetime of the Universe is usually interpreted from a Euclidean perspective, with space as consisting of three dimensions, and time as consisting of one dimension, the \"fourth dimension\". By combining space and time into a single manifold called Minkowski space, physicists have simplified a large number of physical theories, as well as described in a more uniform way the workings of the Universe at both the supergalactic and subatomic levels.\n\nSpacetime events are not absolutely defined spatially and temporally but rather are known to be relative to the motion of an observer. Minkowski space approximates the Universe without gravity; the pseudo-Riemannian manifolds of general relativity describe spacetime with matter and gravity.\n\nGeneral relativity describes how spacetime is curved and bent by mass and energy (gravity). The topology or geometry of the Universe includes both local geometry in the observable universe and global geometry.\nCosmologists often work with a given space-like slice of spacetime called the comoving coordinates. The section of spacetime which can be observed is the backward light cone, which delimits the cosmological horizon.\nThe cosmological horizon (also called the particle horizon or the light horizon) is the maximum distance from which particles can have traveled to the observer in the age of the Universe. This horizon represents the boundary between the observable and the unobservable regions of the Universe. The existence, properties, and significance of a cosmological horizon depend on the particular cosmological model.\n\nAn important parameter determining the future evolution of the Universe theory is the density parameter, Omega (Ω), defined as the average matter density of the universe divided by a critical value of that density. This selects one of three possible geometries depending on whether Ω is equal to, less than, or greater than 1. These are called, respectively, the flat, open and closed universes.\n\nObservations, including the Cosmic Background Explorer (COBE), Wilkinson Microwave Anisotropy Probe (WMAP), and Planck maps of the CMB, suggest that the Universe is infinite in extent with a finite age, as described by the Friedmann–Lemaître–Robertson–Walker (FLRW) models. These FLRW models thus support inflationary models and the standard model of cosmology, describing a flat, homogeneous universe presently dominated by dark matter and dark energy.\n\nThe Universe may be \"fine-tuned\"; the Fine-tuned Universe hypothesis is the proposition that the conditions that allow the existence of observable life in the Universe can only occur when certain universal fundamental physical constants lie within a very narrow range of values, so that if any of several fundamental constants were only slightly different, the Universe would have been unlikely to be conducive to the establishment and development of matter, astronomical structures, elemental diversity, or life as it is understood. The proposition is discussed among philosophers, scientists, theologians, and proponents of creationism.\n\nThe Universe is composed almost completely of dark energy, dark matter, and ordinary matter. Other contents are electromagnetic radiation (estimated to constitute from 0.005% to close to 0.01% of the total mass of the Universe) and antimatter.\n\nThe proportions of all types of matter and energy have changed over the history of the Universe. The total amount of electromagnetic radiation generated within the universe has decreased by 1/2 in the past 2 billion years. Today, ordinary matter, which includes atoms, stars, galaxies, and life, accounts for only 4.9% of the contents of the Universe. The present overall density of this type of matter is very low, roughly 4.5 × 10 grams per cubic centimetre, corresponding to a density of the order of only one proton for every four cubic meters of volume. The nature of both dark energy and dark matter is unknown. Dark matter, a mysterious form of matter that has not yet been identified, accounts for 26.8% of the cosmic contents. Dark energy, which is the energy of empty space and is causing the expansion of the Universe to accelerate, accounts for the remaining 68.3% of the contents.\nMatter, dark matter, and dark energy are distributed homogeneously throughout the Universe over length scales longer than 300 million light-years or so. However, over shorter length-scales, matter tends to clump hierarchically; many atoms are condensed into stars, most stars into galaxies, most galaxies into clusters, superclusters and, finally, large-scale galactic filaments. The observable Universe contains approximately 300 sextillion (3) stars and more than 100 billion (10) galaxies. Typical galaxies range from dwarfs with as few as ten million (10) stars up to giants with one trillion (10) stars. Between the larger structures are voids, which are typically 10–150 Mpc (33 million–490 million ly) in diameter. The Milky Way is in the Local Group of galaxies, which in turn is in the Laniakea Supercluster. This supercluster spans over 500 million light years, while the Local Group spans over 10 million light years. The Universe also has vast regions of relative emptiness; the largest known void measures 1.8 billion ly (550 Mpc) across.\n\nThe observable Universe is isotropic on scales significantly larger than superclusters, meaning that the statistical properties of the Universe are the same in all directions as observed from Earth. The Universe is bathed in highly isotropic microwave radiation that corresponds to a thermal equilibrium blackbody spectrum of roughly 2.72548 kelvins. The hypothesis that the large-scale Universe is homogeneous and isotropic is known as the cosmological principle. A Universe that is both homogeneous and isotropic looks the same from all vantage points and has no center.\n\nAn explanation for why the expansion of the Universe is accelerating remains elusive. It is often attributed to \"dark energy\", an unknown form of energy that is hypothesized to permeate space. On a mass–energy equivalence basis, the density of dark energy (~ 7 × 10 g/cm) is much less than the density of ordinary matter or dark matter within galaxies. However, in the present dark-energy era, it dominates the mass–energy of the universe because it is uniform across space.\n\nTwo proposed forms for dark energy are the cosmological constant, a \"constant\" energy density filling space homogeneously, and scalar fields such as quintessence or moduli, \"dynamic\" quantities whose energy density can vary in time and space. Contributions from scalar fields that are constant in space are usually also included in the cosmological constant. The cosmological constant can be formulated to be equivalent to vacuum energy. Scalar fields having only a slight amount of spatial inhomogeneity would be difficult to distinguish from a cosmological constant.\n\nDark matter is a hypothetical kind of matter that is invisible to the entire electromagnetic spectrum, but which accounts for most of the matter in the Universe. The existence and properties of dark matter are inferred from its gravitational effects on visible matter, radiation, and the large-scale structure of the Universe. Other than neutrinos, a form of hot dark matter, dark matter has not been detected directly, making it one of the greatest mysteries in modern astrophysics. Dark matter neither emits nor absorbs light or any other electromagnetic radiation at any significant level. Dark matter is estimated to constitute 26.8% of the total mass–energy and 84.5% of the total matter in the Universe.\n\nThe remaining 4.9% of the mass–energy of the Universe is ordinary matter, that is, atoms, ions, electrons and the objects they form. This matter includes stars, which produce nearly all of the light we see from galaxies, as well as interstellar gas in the interstellar and intergalactic media, planets, and all the objects from everyday life that we can bump into, touch or squeeze. As a matter of fact, the great majority of ordinary matter in the universe is unseen, since visible stars and gas inside galaxies and clusters account for less than 10 per cent of the ordinary matter contribution to the mass-energy density of the universe.\n\nOrdinary matter commonly exists in four states (or phases): solid, liquid, gas, and plasma. However, advances in experimental techniques have revealed other previously theoretical phases, such as Bose–Einstein condensates and fermionic condensates.\n\nOrdinary matter is composed of two types of elementary particles: quarks and leptons. For example, the proton is formed of two up quarks and one down quark; the neutron is formed of two down quarks and one up quark; and the electron is a kind of lepton. An atom consists of an atomic nucleus, made up of protons and neutrons, and electrons that orbit the nucleus. Because most of the mass of an atom is concentrated in its nucleus, which is made up of baryons, astronomers often use the term \"baryonic matter\" to describe ordinary matter, although a small fraction of this \"baryonic matter\" is electrons.\n\nSoon after the Big Bang, primordial protons and neutrons formed from the quark–gluon plasma of the early Universe as it cooled below two trillion degrees. A few minutes later, in a process known as Big Bang nucleosynthesis, nuclei formed from the primordial protons and neutrons. This nucleosynthesis formed lighter elements, those with small atomic numbers up to lithium and beryllium, but the abundance of heavier elements dropped off sharply with increasing atomic number. Some boron may have been formed at this time, but the next heavier element, carbon, was not formed in significant amounts. Big Bang nucleosynthesis shut down after about 20 minutes due to the rapid drop in temperature and density of the expanding Universe. Subsequent formation of heavier elements resulted from stellar nucleosynthesis and supernova nucleosynthesis.\n\nOrdinary matter and the forces that act on matter can be described in terms of elementary particles. These particles are sometimes described as being fundamental, since they have an unknown substructure, and it is unknown whether or not they are composed of smaller and even more fundamental particles. Of central importance is the Standard Model, a theory that is concerned with electromagnetic interactions and the weak and strong nuclear interactions. The Standard Model is supported by the experimental confirmation of the existence of particles that compose matter: quarks and leptons, and their corresponding \"antimatter\" duals, as well as the force particles that mediate interactions: the photon, the W and Z bosons, and the gluon. The Standard Model predicted the existence of the recently discovered Higgs boson, a particle that is a manifestation of a field within the Universe that can endow particles with mass. Because of its success in explaining a wide variety of experimental results, the Standard Model is sometimes regarded as a \"theory of almost everything\". The Standard Model does not, however, accommodate gravity. A true force-particle \"theory of everything\" has not been attained.\n\nA hadron is a composite particle made of quarks held together by the strong force. Hadrons are categorized into two families: baryons (such as protons and neutrons) made of three quarks, and mesons (such as pions) made of one quark and one antiquark. Of the hadrons, protons are stable, and neutrons bound within atomic nuclei are stable. Other hadrons are unstable under ordinary conditions and are thus insignificant constituents of the modern Universe.\nFrom approximately 10 seconds after the Big Bang, during a period is known as the hadron epoch, the temperature of the universe had fallen sufficiently to allow quarks to bind together into hadrons, and the mass of the Universe was dominated by hadrons. Initially the temperature was high enough to allow the formation of hadron/anti-hadron pairs, which kept matter and antimatter in thermal equilibrium. However, as the temperature of the Universe continued to fall, hadron/anti-hadron pairs were no longer produced. Most of the hadrons and anti-hadrons were then eliminated in particle-antiparticle annihilation reactions, leaving a small residual of hadrons by the time the Universe was about one second old.\n\nA lepton is an elementary, half-integer spin particle that does not undergo strong interactions but is subject to the Pauli exclusion principle; no two leptons of the same species can be in exactly the same state at the same time. Two main classes of leptons exist: charged leptons (also known as the \"electron-like\" leptons), and neutral leptons (better known as neutrinos). Electrons are stable and the most common charged lepton in the Universe, whereas muons and taus are unstable particle that quickly decay after being produced in high energy collisions, such as those involving cosmic rays or carried out in particle accelerators.\nCharged leptons can combine with other particles to form various composite particles such as atoms and positronium. The electron governs nearly all of chemistry, as it is found in atoms and is directly tied to all chemical properties. Neutrinos rarely interact with anything, and are consequently rarely observed. Neutrinos stream throughout the Universe but rarely interact with normal matter.\n\nThe lepton epoch was the period in the evolution of the early Universe in which the leptons dominated the mass of the Universe. It started roughly 1 second after the Big Bang, after the majority of hadrons and anti-hadrons annihilated each other at the end of the hadron epoch. During the lepton epoch the temperature of the Universe was still high enough to create lepton/anti-lepton pairs, so leptons and anti-leptons were in thermal equilibrium. Approximately 10 seconds after the Big Bang, the temperature of the Universe had fallen to the point where lepton/anti-lepton pairs were no longer created. Most leptons and anti-leptons were then eliminated in annihilation reactions, leaving a small residue of leptons. The mass of the Universe was then dominated by photons as it entered the following photon epoch.\n\nA photon is the quantum of light and all other forms of electromagnetic radiation. It is the force carrier for the electromagnetic force, even when static via virtual photons. The effects of this force are easily observable at the microscopic and at the macroscopic level because the photon has zero rest mass; this allows long distance interactions. Like all elementary particles, photons are currently best explained by quantum mechanics and exhibit wave–particle duality, exhibiting properties of waves and of particles.\n\nThe photon epoch started after most leptons and anti-leptons were annihilated at the end of the lepton epoch, about 10 seconds after the Big Bang. Atomic nuclei were created in the process of nucleosynthesis which occurred during the first few minutes of the photon epoch. For the remainder of the photon epoch the Universe contained a hot dense plasma of nuclei, electrons and photons. About 380,000 years after the Big Bang, the temperature of the Universe fell to the point where nuclei could combine with electrons to create neutral atoms. As a result, photons no longer interacted frequently with matter and the Universe became transparent. The highly redshifted photons from this period form the cosmic microwave background. Tiny variations in temperature and density detectable in the CMB were the early \"seeds\" from which all subsequent structure formation took place.\nGeneral relativity is the geometric theory of gravitation published by Albert Einstein in 1915 and the current description of gravitation in modern physics. It is the basis of current cosmological models of the Universe. General relativity generalizes special relativity and Newton's law of universal gravitation, providing a unified description of gravity as a geometric property of space and time, or spacetime. In particular, the curvature of spacetime is directly related to the energy and momentum of whatever matter and radiation are present. The relation is specified by the Einstein field equations, a system of partial differential equations. In general relativity, the distribution of matter and energy determines the geometry of spacetime, which in turn describes the acceleration of matter. Therefore, solutions of the Einstein field equations describe the evolution of the Universe. Combined with measurements of the amount, type, and distribution of matter in the Universe, the equations of general relativity describe the evolution of the Universe over time.\n\nWith the assumption of the cosmological principle that the Universe is homogeneous and isotropic everywhere, a specific solution of the field equations that describes the Universe is the metric tensor called the Friedmann–Lemaître–Robertson–Walker metric,\nwhere (\"r\", θ, φ) correspond to a spherical coordinate system. This metric has only two undetermined parameters. An overall dimensionless length scale factor \"R\" describes the size scale of the Universe as a function of time; an increase in \"R\" is the expansion of the Universe. A curvature index \"k\" describes the geometry. The index \"k\" is defined so that it can take only one of three values: 0, corresponding to flat Euclidean geometry; 1, corresponding to a space of positive curvature; or −1, corresponding to a space of positive or negative curvature. The value of \"R\" as a function of time \"t\" depends upon \"k\" and the cosmological constant \"Λ\". The cosmological constant represents the energy density of the vacuum of space and could be related to dark energy. The equation describing how \"R\" varies with time is known as the Friedmann equation after its inventor, Alexander Friedmann.\n\nThe solutions for \"R(t)\" depend on \"k\" and \"Λ\", but some qualitative features of such solutions are general. First and most importantly, the length scale \"R\" of the Universe can remain constant \"only\" if the Universe is perfectly isotropic with positive curvature (\"k\"=1) and has one precise value of density everywhere, as first noted by Albert Einstein. However, this equilibrium is unstable: because the Universe is known to be inhomogeneous on smaller scales, \"R\" must change over time. When \"R\" changes, all the spatial distances in the Universe change in tandem; there is an overall expansion or contraction of space itself. This accounts for the observation that galaxies appear to be flying apart; the space between them is stretching. The stretching of space also accounts for the apparent paradox that two galaxies can be 40 billion light years apart, although they started from the same point 13.8 billion years ago and never moved faster than the speed of light.\n\nSecond, all solutions suggest that there was a gravitational singularity in the past, when \"R\" went to zero and matter and energy were infinitely dense. It may seem that this conclusion is uncertain because it is based on the questionable assumptions of perfect homogeneity and isotropy (the cosmological principle) and that only the gravitational interaction is significant. However, the Penrose–Hawking singularity theorems show that a singularity should exist for very general conditions. Hence, according to Einstein's field equations, \"R\" grew rapidly from an unimaginably hot, dense state that existed immediately following this singularity (when \"R\" had a small, finite value); this is the essence of the Big Bang model of the Universe. Understanding the singularity of the Big Bang likely requires a quantum theory of gravity, which has not yet been formulated.\n\nThird, the curvature index \"k\" determines the sign of the mean spatial curvature of spacetime averaged over sufficiently large length scales (greater than about a billion light years). If \"k\"=1, the curvature is positive and the Universe has a finite volume. A Universe with positive curvature is often visualized as a three-dimensional sphere embedded in a four-dimensional space. Conversely, if \"k\" is zero or negative, the Universe has an infinite volume. It may seem counter-intuitive that an infinite and yet infinitely dense Universe could be created in a single instant at the Big Bang when \"R\"=0, but exactly that is predicted mathematically when \"k\" does not equal 1. By analogy, an infinite plane has zero curvature but infinite area, whereas an infinite cylinder is finite in one direction and a torus is finite in both. A toroidal Universe could behave like a normal Universe with periodic boundary conditions.\n\nThe ultimate fate of the Universe is still unknown, because it depends critically on the curvature index \"k\" and the cosmological constant \"Λ\". If the Universe were sufficiently dense, \"k\" would equal +1, meaning that its average curvature throughout is positive and the Universe will eventually recollapse in a Big Crunch, possibly starting a new Universe in a Big Bounce. Conversely, if the Universe were insufficiently dense, \"k\" would equal 0 or −1 and the Universe would expand forever, cooling off and eventually reaching the Big Freeze and the heat death of the Universe. Modern data suggests that the rate of expansion of the Universe is not decreasing, as originally expected, but increasing; if this continues indefinitely, the Universe may eventually reach a Big Rip. Observationally, the Universe appears to be flat (\"k\" = 0), with an overall density that is very close to the critical value between recollapse and eternal expansion.\n\nSome speculative theories have proposed that our Universe is but one of a set of disconnected universes, collectively denoted as the multiverse, challenging or enhancing more limited definitions of the Universe. Scientific multiverse models are distinct from concepts such as alternate planes of consciousness and simulated reality.\n\nMax Tegmark developed a four-part classification scheme for the different types of multiverses that scientists have suggested in response to various Physics problems. An example of such multiverses is the one resulting from the chaotic inflation model of the early universe. Another is the multiverse resulting from the many-worlds interpretation of quantum mechanics. In this interpretation, parallel worlds are generated in a manner similar to quantum superposition and decoherence, with all states of the wave functions being realized in separate worlds. Effectively, in the many-worlds interpretation the multiverse evolves as a universal wavefunction. If the Big Bang that created our multiverse created an ensemble of multiverses, the wave function of the ensemble would be entangled in this sense.\n\nThe least controversial category of multiverse in Tegmark's scheme is . The multiverses of this level are composed by distant spacetime events \"in our own universe\". If space is infinite, or sufficiently large and uniform, identical instances of the history of Earth's entire Hubble volume occur every so often, simply by chance. Tegmark calculated that our nearest so-called doppelgänger, is 10 meters away from us (a double exponential function larger than a googolplex). In principle, it would be impossible to scientifically verify the existence of an identical Hubble volume. However, this existence does follow as a fairly straightforward consequence \n\nIt is possible to conceive of disconnected spacetimes, each existing but unable to interact with one another. An easily visualized metaphor of this concept is a group of separate soap bubbles, in which observers living on one soap bubble cannot interact with those on other soap bubbles, even in principle. According to one common terminology, each \"soap bubble\" of spacetime is denoted as a \"universe\", whereas our particular spacetime is denoted as \"the Universe\", just as we call our moon \"the Moon\". The entire collection of these separate spacetimes is denoted as the multiverse. With this terminology, different \"Universes\" are not causally connected to each other. In principle, the other unconnected \"Universes\" may have different dimensionalities and topologies of spacetime, different forms of matter and energy, and different physical laws and physical constants, although such possibilities are purely speculative. Others consider each of several bubbles created as part of chaotic inflation to be separate \"Universes\", though in this model these universes all share a causal origin.\n\nHistorically, there have been many ideas of the cosmos (cosmologies) and its origin (cosmogonies). Theories of an impersonal Universe governed by physical laws were first proposed by the Greeks and Indians. Ancient Chinese philosophy encompassed the notion of the Universe including both all of space and all of time. Over the centuries, improvements in astronomical observations and theories of motion and gravitation led to ever more accurate descriptions of the Universe. The modern era of cosmology began with Albert Einstein's 1915 general theory of relativity, which made it possible to quantitatively predict the origin, evolution, and conclusion of the Universe as a whole. Most modern, accepted theories of cosmology are based on general relativity and, more specifically, the predicted Big Bang.\n\nMany cultures have stories describing the origin of the world and universe. Cultures generally regard these stories as having some truth. There are however many differing beliefs in how these stories apply amongst those believing in a supernatural origin, ranging from a god directly creating the Universe as it is now to a god just setting the \"wheels in motion\" (for example via mechanisms such as the big bang and evolution).\n\nEthnologists and anthropologists who study myths have developed various classification schemes for the various themes that appear in creation stories. For example, in one type of story, the world is born from a world egg; such stories include the Finnish epic poem \"Kalevala\", the Chinese story of Pangu or the Indian Brahmanda Purana. In related stories, the Universe is created by a single entity emanating or producing something by him- or herself, as in the Tibetan Buddhism concept of Adi-Buddha, the ancient Greek story of Gaia (Mother Earth), the Aztec goddess Coatlicue myth, the ancient Egyptian god Atum story, and the Judeo-Christian Genesis creation narrative in which the Abrahamic God created the Universe. In another type of story, the Universe is created from the union of male and female deities, as in the Maori story of Rangi and Papa. In other stories, the Universe is created by crafting it from pre-existing materials, such as the corpse of a dead god — as from Tiamat in the Babylonian epic \"Enuma Elish\" or from the giant Ymir in Norse mythology – or from chaotic materials, as in Izanagi and Izanami in Japanese mythology. In other stories, the Universe emanates from fundamental principles, such as Brahman and Prakrti, the creation myth of the Serers, or the yin and yang of the Tao.\n\nThe pre-Socratic Greek philosophers and Indian philosophers developed some of the earliest philosophical concepts of the Universe. The earliest Greek philosophers noted that appearances can be deceiving, and sought to understand the underlying reality behind the appearances. In particular, they noted the ability of matter to change forms (e.g., ice to water to steam) and several philosophers proposed that all the physical materials in the world are different forms of a single primordial material, or \"arche\". The first to do so was Thales, who proposed this material to be water. Thales' student, Anaximander, proposed that everything came from the limitless \"apeiron\". Anaximenes proposed the primordial material to be air on account of its perceived attractive and repulsive qualities that cause the \"arche\" to condense or dissociate into different forms. Anaxagoras proposed the principle of \"Nous\" (Mind), while Heraclitus proposed fire (and spoke of \"logos\"). Empedocles proposed the elements to be earth, water, air and fire. His four-element model became very popular. Like Pythagoras, Plato believed that all things were composed of number, with Empedocles' elements taking the form of the Platonic solids. Democritus, and later philosophers—most notably Leucippus—proposed that the Universe is composed of indivisible atoms moving through a void (vacuum), although Aristotle did not believe that to be feasible because air, like water, offers resistance to motion. Air will immediately rush in to fill a void, and moreover, without resistance, it would do so indefinitely fast.\n\nAlthough Heraclitus argued for eternal change, his contemporary Parmenides made the radical suggestion that all change is an illusion, that the true underlying reality is eternally unchanging and of a single nature. Parmenides denoted this reality as (The One). Parmenides' idea seemed implausible to many Greeks, but his student Zeno of Elea challenged them with several famous paradoxes. Aristotle responded to these paradoxes by developing the notion of a potential countable infinity, as well as the infinitely divisible continuum. Unlike the eternal and unchanging cycles of time, he believed that the world is bounded by the celestial spheres and that cumulative stellar magnitude is only finitely multiplicative.\n\nThe Indian philosopher Kanada, founder of the Vaisheshika school, developed a notion of atomism and proposed that light and heat were varieties of the same substance. In the 5th century AD, the Buddhist atomist philosopher Dignāga proposed atoms to be point-sized, durationless, and made of energy. They denied the existence of substantial matter and proposed that movement consisted of momentary flashes of a stream of energy.\n\nThe notion of temporal finitism was inspired by the doctrine of creation shared by the three Abrahamic religions: Judaism, Christianity and Islam. The Christian philosopher, John Philoponus, presented the philosophical arguments against the ancient Greek notion of an infinite past and future. Philoponus' arguments against an infinite past were used by the early Muslim philosopher, Al-Kindi (Alkindus); the Jewish philosopher, Saadia Gaon (Saadia ben Joseph); and the Muslim theologian, Al-Ghazali (Algazel).\n\nAstronomical models of the Universe were proposed soon after astronomy began with the Babylonian astronomers, who viewed the Universe as a flat disk floating in the ocean, and this forms the premise for early Greek maps like those of Anaximander and Hecataeus of Miletus.\n\nLater Greek philosophers, observing the motions of the heavenly bodies, were concerned with developing models of the Universe-based more profoundly on empirical evidence. The first coherent model was proposed by Eudoxus of Cnidos. According to Aristotle's physical interpretation of the model, celestial spheres eternally rotate with uniform motion around a stationary Earth. Normal matter is entirely contained within the terrestrial sphere.\n\n\"De Mundo\" (composed before 250 BC or between 350 and 200 BC), stated, \"Five elements, situated in spheres in five regions, the less being in each case surrounded by the greater—namely, earth surrounded by water, water by air, air by fire, and fire by ether—make up the whole Universe\".\n\nThis model was also refined by Callippus and after concentric spheres were abandoned, it was brought into nearly perfect agreement with astronomical observations by Ptolemy. The success of such a model is largely due to the mathematical fact that any function (such as the position of a planet) can be decomposed into a set of circular functions (the Fourier modes). Other Greek scientists, such as the Pythagorean philosopher Philolaus, postulated (according to Stobaeus account) that at the center of the Universe was a \"central fire\" around which the Earth, Sun, Moon and Planets revolved in uniform circular motion.\n\nThe Greek astronomer Aristarchus of Samos was the first known individual to propose a heliocentric model of the Universe. Though the original text has been lost, a reference in Archimedes' book \"The Sand Reckoner\" describes Aristarchus's heliocentric model. Archimedes wrote:\n\nYou, King Gelon, are aware the Universe is the name given by most astronomers to the sphere the center of which is the center of the Earth, while its radius is equal to the straight line between the center of the Sun and the center of the Earth. This is the common account as you have heard from astronomers. But Aristarchus has brought out a book consisting of certain hypotheses, wherein it appears, as a consequence of the assumptions made, that the Universe is many times greater than the Universe just mentioned. His hypotheses are that the fixed stars and the Sun remain unmoved, that the Earth revolves about the Sun on the circumference of a circle, the Sun lying in the middle of the orbit, and that the sphere of fixed stars, situated about the same center as the Sun, is so great that the circle in which he supposes the Earth to revolve bears such a proportion to the distance of the fixed stars as the center of the sphere bears to its surface\n\nAristarchus thus believed the stars to be very far away, and saw this as the reason why stellar parallax had not been observed, that is, the stars had not been observed to move relative each other as the Earth moved around the Sun. The stars are in fact much farther away than the distance that was generally assumed in ancient times, which is why stellar parallax is only detectable with precision instruments. The geocentric model, consistent with planetary parallax, was assumed to be an explanation for the unobservability of the parallel phenomenon, stellar parallax. The rejection of the heliocentric view was apparently quite strong, as the following passage from Plutarch suggests (\"On the Apparent Face in the Orb of the Moon\"):\n\nCleanthes [a contemporary of Aristarchus and head of the Stoics] thought it was the duty of the Greeks to indict Aristarchus of Samos on the charge of impiety for putting in motion the Hearth of the Universe [i.e. the Earth], ... supposing the heaven to remain at rest and the Earth to revolve in an oblique circle, while it rotates, at the same time, about its own axis\n\nThe only other astronomer from antiquity known by name who supported Aristarchus's heliocentric model was Seleucus of Seleucia, a Hellenistic astronomer who lived a century after Aristarchus. According to Plutarch, Seleucus was the first to prove the heliocentric system through reasoning, but it is not known what arguments he used. Seleucus' arguments for a heliocentric cosmology were probably related to the phenomenon of tides. According to Strabo (1.1.9), Seleucus was the first to state that the tides are due to the attraction of the Moon, and that the height of the tides depends on the Moon's position relative to the Sun. Alternatively, he may have proved heliocentricity by determining the constants of a geometric model for it, and by developing methods to compute planetary positions using this model, like what Nicolaus Copernicus later did in the 16th century. During the Middle Ages, heliocentric models were also proposed by the Indian astronomer Aryabhata, and by the Persian astronomers Albumasar and Al-Sijzi.\n\nThe Aristotelian model was accepted in the Western world for roughly two millennia, until Copernicus revived Aristarchus's perspective that the astronomical data could be explained more plausibly if the Earth rotated on its axis and if the Sun were placed at the center of the Universe.\n\nAs noted by Copernicus himself, the notion that the Earth rotates is very old, dating at least to Philolaus (c. 450 BC), Heraclides Ponticus (c. 350 BC) and Ecphantus the Pythagorean. Roughly a century before Copernicus, the Christian scholar Nicholas of Cusa also proposed that the Earth rotates on its axis in his book, \"On Learned Ignorance\" (1440). Al-Sijzi also proposed that the Earth rotates on its axis. Empirical evidence for the Earth's rotation on its axis, using the phenomenon of comets, was given by Tusi (1201–1274) and Ali Qushji (1403–1474).\n\nThis cosmology was accepted by Isaac Newton, Christiaan Huygens and later scientists. Edmund Halley (1720) and Jean-Philippe de Chéseaux (1744) noted independently that the assumption of an infinite space filled uniformly with stars would lead to the prediction that the nighttime sky would be as bright as the Sun itself; this became known as Olbers' paradox in the 19th century. Newton believed that an infinite space uniformly filled with matter would cause infinite forces and instabilities causing the matter to be crushed inwards under its own gravity. This instability was clarified in 1902 by the Jeans instability criterion. One solution to these paradoxes is the Charlier Universe, in which the matter is arranged hierarchically (systems of orbiting bodies that are themselves orbiting in a larger system, \"ad infinitum\") in a fractal way such that the Universe has a negligibly small overall density; such a cosmological model had also been proposed earlier in 1761 by Johann Heinrich Lambert. A significant astronomical advance of the 18th century was the realization by Thomas Wright, Immanuel Kant and others of nebulae.\n\nIn 1919, when Hooker Telescope was completed, the prevailing view still was that the Universe consisted entirely of the Milky Way Galaxy. Using the Hooker Telescope, Edwin Hubble identified Cepheid variables in several spiral nebulae and in 1922–1923 proved conclusively that Andromeda Nebula and Triangulum among others, were entire galaxies outside our own, thus proving that Universe consists of multitude of galaxies.\n\nThe modern era of physical cosmology began in 1917, when Albert Einstein first applied his general theory of relativity to model the structure and dynamics of the Universe.\n\n\n"}
