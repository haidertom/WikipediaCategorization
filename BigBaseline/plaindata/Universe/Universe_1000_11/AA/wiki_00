{"id": "37205291", "url": "https://en.wikipedia.org/wiki?curid=37205291", "title": "Aesthetics of nature", "text": "Aesthetics of nature\n\nAesthetics of nature is a sub-field of philosophical ethics, and refers to the study of natural objects from their aesthetical perspective.\n\nAesthetics of nature developed as a sub-field of philosophical ethics. In the 18th and 19th century, the aesthetics of nature advanced the concepts of disinterestedness, the pictures, and the introduction of the idea of positive aesthetics. The first major developments of nature occurred in the 18th century. The concept of disinterestedness had been explained by many thinkers. Anthony Ashley-Cooper introduced the concept as a way of characterizing the notion of the aesthetic, later magnified by Francis Hutcheson, who expanded it to exclude personal and utilitarianism interests and associations of a more general nature from aesthetic experience. This concept was further developed by Archibald Alison who referred it to a particular state of mind.\n\nThe theory of disinterestedness opened doors for a better understanding of the aesthetics dimensions of nature in terms of three conceptualizations: \n\nObjects experienced as beautiful tend to be small, smooth, and fair in color. In contrast, objects viewed as sublime tend to be powerful, intense and terrifying. Picturesque items are a mixture of both, which can be seen as varied and irregular, rich and forceful, and even vibrant.\n\nCognitive and non-cognitive approaches of nature have directed their focus from natural environments to the consideration of human and human influenced environments and developed aesthetic investigations of everyday life.(Carlson and Lintott, 2007; Parsons 2008a; Carlson 2010)\n\nPeople may be mistaken by the art object analogy. For instance, a sandhill crane is not an art object; an art object is not a sandhill crane. In fact, an art object should be called an \"artifact\". The crane is wildlife on its own and is not an art object. This can be related to Satio's definition of the cognitive view. In elaboration, the crane lives through various ecosystems such as Yellowstone. Nature is a living system which includes animals, plants, and Eco-systems. In contrast, an art object has no regeneration, evolutionary history, or metabolism. An individual may be in the forest and perceive it as beautiful because of the plethora of colors such as red, green, and yellow. This is a result of the chemicals interacting with chlorophyll. An individual's aesthetic experience may increase; however, none of the things mentioned have anything to do with what is really going on in the forest. The chlorophyll is capturing solar energy and the residual chemicals protect the trees from insect grazing.\n\nAny color perceived by human visitors for a few hours is entirely different from what is really happening. According to Leopold, the three features of ecosystems that generate land ethic are integrity, stability and beauty. None of the mentioned features are real in nature. Ecosystems are not stable: they are dramatically changing and they have little integration; ergo, beauty is in the eye of the beholder.\n\nIn a Post-Modern approach, when an individual engages in aesthetically appreciating a natural thing, we give meaning to the thing we appreciate and in that meaning, we express and develop our own attitudes, values and beliefs. Our interest in natural things are not only a passive reflection of our inclinations, as Croce describes as the appreciation of nature as looking in a mirror, or what we might call our inward life; but may instead be the things we come across in nature that engage and stimulate our imagination. As a result, we are challenged to think differently and apply thoughts and associations to in new situations and ways.\nAs a characterization of the appreciation of art, nature aestheticists argue that post modernism is a mistaken view because we do not have a case of anything goes.The aesthetics appreciation of art is governed by some normative standards. In the world of art, criticism may take place when people come together and discuss books and films or critics write appraisals for publications. On the contrary, there are not obvious instances of debate and appraisals where different judgments about the aesthetics of character of nature are evaluated.\n"}
{"id": "4116", "url": "https://en.wikipedia.org/wiki?curid=4116", "title": "Big Bang", "text": "Big Bang\n\nThe Big Bang theory is the prevailing cosmological model for the observable universe from the earliest known periods through its subsequent large-scale evolution. The model describes how the universe expanded from a very high-density and high-temperature state, and offers a comprehensive explanation for a broad range of phenomena, including the abundance of light elements, the cosmic microwave background (CMB), large scale structure and Hubble's law (the farther away galaxies are, the faster they are moving away from Earth). If the observed conditions are extrapolated backwards in time using the known laws of physics, the prediction is that just before a period of very high density there was a singularity which is typically associated with the Big Bang. Physicists are undecided whether this means the universe began from a singularity, or that current knowledge is insufficient to describe the universe at that time. Detailed measurements of the expansion rate of the universe place the Big Bang at around 13.8 billion years ago, which is thus considered the age of the universe. After its initial expansion, the universe cooled sufficiently to allow the formation of subatomic particles, and later simple atoms. Giant clouds of these primordial elements (mostly hydrogen, with some helium and lithium) later coalesced through gravity, eventually forming early stars and galaxies, the descendants of which are visible today. Astronomers also observe the gravitational effects of dark matter surrounding galaxies. Though most of the mass in the universe seems to be in the form of dark matter, Big Bang theory and various observations seem to indicate that it is not made out of conventional baryonic matter (protons, neutrons, and electrons) but it is unclear exactly what it \"is\" made out of.\n\nSince Georges Lemaître first noted in 1927 that an expanding universe could be traced back in time to an originating single point, scientists have built on his idea of cosmic expansion. The scientific community was once divided between supporters of two different theories, the Big Bang and the Steady State theory, but a wide range of empirical evidence has strongly favored the Big Bang which is now universally accepted. In 1929, from analysis of galactic redshifts, Edwin Hubble concluded that galaxies are drifting apart; this is important observational evidence consistent with the hypothesis of an expanding universe. In 1964, the cosmic microwave background radiation was discovered, which was crucial evidence in favor of the Big Bang model, since that theory predicted the existence of background radiation throughout the universe before it was discovered. More recently, measurements of the redshifts of supernovae indicate that the expansion of the universe is accelerating, an observation attributed to dark energy's existence. The known physical laws of nature can be used to calculate the characteristics of the universe in detail back in time to an initial state of extreme density and temperature.\n\nThe Belgian astronomer and Catholic priest Georges Lemaître proposed on theoretical grounds that the universe is expanding, which was observationally confirmed soon afterwards by Edwin Hubble. In 1927 in the \"Annales de la Société Scientifique de Bruxelles\" (\"Annals of the Scientific Society of Brussels\") under the title \"Un Univers homogène de masse constante et de rayon croissant rendant compte de la vitesse radiale des nébuleuses extragalactiques\" (\"A homogeneous Universe of constant mass and growing radius accounting for the radial velocity of extragalactic nebulae\"), he presented his new idea that the universe is expanding and provided the first observational estimation of what is known as the Hubble constant. What later will be known as the \"Big Bang theory\" of the origin of the universe, he called his \"hypothesis of the primeval atom\" or the \"Cosmic Egg\".\n\nAmerican astronomer Edwin Hubble observed that the distances to faraway galaxies were strongly correlated with their redshifts. This was interpreted to mean that all distant galaxies and clusters are receding away from our vantage point with an apparent velocity proportional to their distance: that is, the farther they are, the faster they move away from us, regardless of direction. Assuming the Copernican principle (that the Earth is not the center of the universe), the only remaining interpretation is that all observable regions of the universe are receding from all others. Since we know that the distance between galaxies increases today, it must mean that in the past galaxies were closer together. The continuous expansion of the universe implies that the universe was denser and hotter in the past.\n\nLarge particle accelerators can replicate the conditions that prevailed after the early moments of the universe, resulting in confirmation and refinement of the details of the Big Bang model. However, these accelerators can only probe so far into high energy regimes. Consequently, the state of the universe in the earliest instants of the Big Bang expansion is still poorly understood and an area of open investigation and speculation.\n\nThe first subatomic particles to be formed included protons, neutrons, and electrons. Though simple atomic nuclei formed within the first three minutes after the Big Bang, thousands of years passed before the first electrically neutral atoms formed. The majority of atoms produced by the Big Bang were hydrogen, along with helium and traces of lithium. Giant clouds of these primordial elements later coalesced through gravity to form stars and galaxies, and the heavier elements were synthesized either within stars or during supernovae.\n\nThe Big Bang theory offers a comprehensive explanation for a broad range of observed phenomena, including the abundance of light elements, the CMB, large scale structure, and Hubble's Law. The framework for the Big Bang model relies on Albert Einstein's theory of general relativity and on simplifying assumptions such as homogeneity and isotropy of space. The governing equations were formulated by Alexander Friedmann, and similar solutions were worked on by Willem de Sitter. Since then, astrophysicists have incorporated observational and theoretical additions into the Big Bang model, and its parametrization as the Lambda-CDM model serves as the framework for current investigations of theoretical cosmology. The Lambda-CDM model is the current \"standard model\" of Big Bang cosmology, consensus is that it is the simplest model that can account for the various measurements and observations relevant to cosmology.\n\nExtrapolation of the expansion of the universe backwards in time using general relativity yields an infinite density and temperature at a finite time in the past. This singularity indicates that general relativity is not an adequate description of the laws of physics in this regime. Models based on general relativity alone can not extrapolate toward the singularity beyond the end of the Planck epoch.\n\nThis primordial singularity is itself sometimes called \"the Big Bang\", but the term can also refer to a more generic early hot, dense phase of the universe. In either case, \"the Big Bang\" as an event is also colloquially referred to as the \"birth\" of our universe since it represents the point in history where the universe can be verified to have entered into a regime where the laws of physics as we understand them (specifically general relativity and the standard model of particle physics) work. Based on measurements of the expansion using Type Ia supernovae and measurements of temperature fluctuations in the cosmic microwave background, the time that has passed since that event — otherwise known as the \"age of the universe\" — is 13.799 ± 0.021 billion years. The agreement of independent measurements of this age supports the ΛCDM model that describes in detail the characteristics of the universe.\n\nDespite being extremely dense at this time—far denser than is usually required to form a black hole—the universe did not re-collapse into a black hole. This may be explained by considering that commonly-used calculations and limits for gravitational collapse are usually based upon objects of relatively constant size, such as stars, and do not apply to rapidly expanding space such as the Big Bang.\n\nThe earliest phases of the Big Bang are subject to much speculation. In the most common models the universe was filled homogeneously and isotropically with a very high energy density and huge temperatures and pressures and was very rapidly expanding and cooling. Approximately 10 seconds into the expansion, a phase transition caused a cosmic inflation, during which the universe grew exponentially during which time density fluctuations that occurred because of the uncertainty principle were amplified into the seeds that would later form the large-scale structure of the universe. After inflation stopped, reheating occurred until the universe obtained the temperatures required for the production of a quark–gluon plasma as well as all other elementary particles. Temperatures were so high that the random motions of particles were at relativistic speeds, and particle–antiparticle pairs of all kinds were being continuously created and destroyed in collisions. At some point, an unknown reaction called baryogenesis violated the conservation of baryon number, leading to a very small excess of quarks and leptons over antiquarks and antileptons—of the order of one part in 30 million. This resulted in the predominance of matter over antimatter in the present universe.\n\nThe universe continued to decrease in density and fall in temperature, hence the typical energy of each particle was decreasing. Symmetry breaking phase transitions put the fundamental forces of physics and the parameters of elementary particles into their present form. After about 10 seconds, the picture becomes less speculative, since particle energies drop to values that can be attained in particle accelerators. At about 10 seconds, quarks and gluons combined to form baryons such as protons and neutrons. The small excess of quarks over antiquarks led to a small excess of baryons over antibaryons. The temperature was now no longer high enough to create new proton–antiproton pairs (similarly for neutrons–antineutrons), so a mass annihilation immediately followed, leaving just one in 10 of the original protons and neutrons, and none of their antiparticles. A similar process happened at about 1 second for electrons and positrons. After these annihilations, the remaining protons, neutrons and electrons were no longer moving relativistically and the energy density of the universe was dominated by photons (with a minor contribution from neutrinos).\n\nA few minutes into the expansion, when the temperature was about a billion (one thousand million) kelvin and the density was about that of air, neutrons combined with protons to form the universe's deuterium and helium nuclei in a process called Big Bang nucleosynthesis. Most protons remained uncombined as hydrogen nuclei.\n\nAs the universe cooled, the rest mass energy density of matter came to gravitationally dominate that of the photon radiation. After about 379,000 years, the electrons and nuclei combined into atoms (mostly hydrogen); hence the radiation decoupled from matter and continued through space largely unimpeded. This relic radiation is known as the cosmic microwave background radiation. The chemistry of life may have begun shortly after the Big Bang, 13.8 billion years ago, during a habitable epoch when the universe was only 10–17 million years old.\n\nOver a long period of time, the slightly denser regions of the nearly uniformly distributed matter gravitationally attracted nearby matter and thus grew even denser, forming gas clouds, stars, galaxies, and the other astronomical structures observable today. The details of this process depend on the amount and type of matter in the universe. The four possible types of matter are known as cold dark matter, warm dark matter, hot dark matter, and baryonic matter. The best measurements available, from Wilkinson Microwave Anisotropy Probe (WMAP), show that the data is well-fit by a Lambda-CDM model in which dark matter is assumed to be cold (warm dark matter is ruled out by early reionization), and is estimated to make up about 23% of the matter/energy of the universe, while baryonic matter makes up about 4.6%. In an \"extended model\" which includes hot dark matter in the form of neutrinos, then if the \"physical baryon density\" formula_1 is estimated at about 0.023 (this is different from the 'baryon density' formula_2 expressed as a fraction of the total matter/energy density, which as noted above is about 0.046), and the corresponding cold dark matter density formula_3 is about 0.11, the corresponding neutrino density formula_4 is estimated to be less than 0.0062.\n\nIndependent lines of evidence from Type Ia supernovae and the CMB imply that the universe today is dominated by a mysterious form of energy known as dark energy, which apparently permeates all of space. The observations suggest 73% of the total energy density of today's universe is in this form. When the universe was very young, it was likely infused with dark energy, but with less space and everything closer together, gravity predominated, and it was slowly braking the expansion. But eventually, after numerous billion years of expansion, the growing abundance of dark energy caused the expansion of the universe to slowly begin to accelerate.\n\nDark energy in its simplest formulation takes the form of the cosmological constant term in Einstein's field equations of general relativity, but its composition and mechanism are unknown and, more generally, the details of its equation of state and relationship with the Standard Model of particle physics continue to be investigated both through observation and theoretically.\n\nAll of this cosmic evolution after the inflationary epoch can be rigorously described and modeled by the ΛCDM model of cosmology, which uses the independent frameworks of quantum mechanics and Einstein's General Relativity. There is no well-supported model describing the action prior to 10 seconds or so. Apparently a new unified theory of quantum gravitation is needed to break this barrier. Understanding this earliest of eras in the history of the universe is currently one of the greatest unsolved problems in physics.\n\nThe Big Bang theory depends on two major assumptions: the universality of physical laws and the cosmological principle. The cosmological principle states that on large scales the universe is homogeneous and isotropic.\n\nThese ideas were initially taken as postulates, but today there are efforts to test each of them. For example, the first assumption has been tested by observations showing that largest possible deviation of the fine structure constant over much of the age of the universe is of order 10. Also, general relativity has passed stringent tests on the scale of the Solar System and binary stars.\n\nIf the large-scale universe appears isotropic as viewed from Earth, the cosmological principle can be derived from the simpler Copernican principle, which states that there is no preferred (or special) observer or vantage point. To this end, the cosmological principle has been confirmed to a level of 10 via observations of the CMB. The universe has been measured to be homogeneous on the largest scales at the 10% level.\n\nGeneral relativity describes spacetime by a metric, which determines the distances that separate nearby points. The points, which can be galaxies, stars, or other objects, are themselves specified using a coordinate chart or \"grid\" that is laid down over all spacetime. The cosmological principle implies that the metric should be homogeneous and isotropic on large scales, which uniquely singles out the Friedmann–Lemaître–Robertson–Walker metric (FLRW metric). \nThis metric contains a scale factor, which describes how the size of the universe changes with time. This enables a convenient choice of a coordinate system to be made, called comoving coordinates. In this coordinate system, the grid expands along with the universe, and objects that are moving only because of the expansion of the universe, remain at fixed points on the grid. While their \"coordinate\" distance (comoving distance) remains constant, the \"physical\" distance between two such co-moving points expands proportionally with the scale factor of the universe.\n\nThe Big Bang is not an explosion of matter moving outward to fill an empty universe. Instead, space itself expands with time everywhere and increases the physical distance between two comoving points. In other words, the Big Bang is not an explosion \"in space\", but rather an expansion \"of space\". Because the FLRW metric assumes a uniform distribution of mass and energy, it applies to our universe only on large scales—local concentrations of matter such as our galaxy are gravitationally bound and as such do not experience the large-scale expansion of space.\n\nAn important feature of the Big Bang spacetime is the presence of particle horizons. Since the universe has a finite age, and light travels at a finite speed, there may be events in the past whose light has not had time to reach us. This places a limit or a \"past horizon\" on the most distant objects that can be observed. Conversely, because space is expanding, and more distant objects are receding ever more quickly, light emitted by us today may never \"catch up\" to very distant objects. This defines a \"future horizon\", which limits the events in the future that we will be able to influence. The presence of either type of horizon depends on the details of the FLRW model that describes our universe.\n\nOur understanding of the universe back to very early times suggests that there is a past horizon, though in practice our view is also limited by the opacity of the universe at early times. So our view cannot extend further backward in time, though the horizon recedes in space. If the expansion of the universe continues to accelerate, there is a future horizon as well.\n\nEnglish astronomer Fred Hoyle is credited with coining the term \"Big Bang\" during a 1949 BBC radio broadcast, saying: \"These theories were based on the hypothesis that all the matter in the universe was created in one big bang at a particular time in the remote past.\"\n\nIt is popularly reported that Hoyle, who favored an alternative \"steady state\" cosmological model, intended this to be pejorative, but Hoyle explicitly denied this and said it was just a striking image meant to highlight the difference between the two models.\n\nThe Big Bang theory developed from observations of the structure of the universe and from theoretical considerations. In 1912 Vesto Slipher measured the first Doppler shift of a \"spiral nebula\" (spiral nebula is the obsolete term for spiral galaxies), and soon discovered that almost all such nebulae were receding from Earth. He did not grasp the cosmological implications of this fact, and indeed at the time it was highly controversial whether or not these nebulae were \"island universes\" outside our Milky Way. Ten years later, Alexander Friedmann, a Russian cosmologist and mathematician, derived the Friedmann equations from Albert Einstein's equations of general relativity, showing that the universe might be expanding in contrast to the static universe model advocated by Einstein at that time. In 1924 Edwin Hubble's measurement of the great distance to the nearest spiral nebulae showed that these systems were indeed other galaxies. Independently deriving Friedmann's equations in 1927, Georges Lemaître, a Belgian physicist, proposed that the inferred recession of the nebulae was due to the expansion of the universe.\n\nIn 1931 Lemaître went further and suggested that the evident expansion of the universe, if projected back in time, meant that the further in the past the smaller the universe was, until at some finite time in the past all the mass of the universe was concentrated into a single point, a \"primeval atom\" where and when the fabric of time and space came into existence.\n\nStarting in 1924, Hubble painstakingly developed a series of distance indicators, the forerunner of the cosmic distance ladder, using the Hooker telescope at Mount Wilson Observatory. This allowed him to estimate distances to galaxies whose redshifts had already been measured, mostly by Slipher. In 1929 Hubble discovered a correlation between distance and recession velocity—now known as Hubble's law. Lemaître had already shown that this was expected, given the cosmological principle.\n\nIn the 1920s and 1930s almost every major cosmologist preferred an eternal steady state universe, and several complained that the beginning of time implied by the Big Bang imported religious concepts into physics; this objection was later repeated by supporters of the steady state theory. This perception was enhanced by the fact that the originator of the Big Bang theory, Georges Lemaître, was a Roman Catholic priest. Arthur Eddington agreed with Aristotle that the universe did not have a beginning in time, \"viz\"., that matter is eternal. A beginning in time was \"repugnant\" to him. Lemaître, however, thought thatIf the world has begun with a single quantum, the notions of space and time would altogether fail to have any meaning at the beginning; they would only begin to have a sensible meaning when the original quantum had been divided into a sufficient number of quanta. If this suggestion is correct, the beginning of the world happened a little before the beginning of space and time.\n\nDuring the 1930s other ideas were proposed as non-standard cosmologies to explain Hubble's observations, including the Milne model, the oscillatory universe (originally suggested by Friedmann, but advocated by Albert Einstein and Richard Tolman) and Fritz Zwicky's tired light hypothesis.\n\nAfter World War II, two distinct possibilities emerged. One was Fred Hoyle's steady state model, whereby new matter would be created as the universe seemed to expand. In this model the universe is roughly the same at any point in time. The other was Lemaître's Big Bang theory, advocated and developed by George Gamow, who introduced big bang nucleosynthesis (BBN) and whose associates, Ralph Alpher and Robert Herman, predicted the CMB. Ironically, it was Hoyle who coined the phrase that came to be applied to Lemaître's theory, referring to it as \"this \"big bang\" idea\" during a BBC Radio broadcast in March 1949. For a while, support was split between these two theories. Eventually, the observational evidence, most notably from radio source counts, began to favor Big Bang over Steady State. The discovery and confirmation of the CMB in 1964 secured the Big Bang as the best theory of the origin and evolution of the universe. Much of the current work in cosmology includes understanding how galaxies form in the context of the Big Bang, understanding the physics of the universe at earlier and earlier times, and reconciling observations with the basic theory.\n\nIn 1968 and 1970 Roger Penrose, Stephen Hawking, and George F. R. Ellis published papers where they showed that mathematical singularities were an inevitable initial condition of general relativistic models of the Big Bang. Then, from the 1970s to the 1990s, cosmologists worked on characterizing the features of the Big Bang universe and resolving outstanding problems. In 1981, Alan Guth made a breakthrough in theoretical work on resolving certain outstanding theoretical problems in the Big Bang theory with the introduction of an epoch of rapid expansion in the early universe he called \"inflation\". Meanwhile, during these decades, two questions in observational cosmology that generated much discussion and disagreement were over the precise values of the Hubble Constant and the matter-density of the universe (before the discovery of dark energy, thought to be the key predictor for the eventual fate of the universe).\n\nIn the mid-1990s, observations of certain globular clusters appeared to indicate that they were about 15 billion years old, which conflicted with most then-current estimates of the age of the universe (and indeed with the age measured today). This issue was later resolved when new computer simulations, which included the effects of mass loss due to stellar winds, indicated a much younger age for globular clusters. While there still remain some questions as to how accurately the ages of the clusters are measured, globular clusters are of interest to cosmology as some of the oldest objects in the universe.\n\nSignificant progress in Big Bang cosmology has been made since the late 1990s as a result of advances in telescope technology as well as the analysis of data from satellites such as COBE, the Hubble Space Telescope and WMAP. Cosmologists now have fairly precise and accurate measurements of many of the parameters of the Big Bang model, and have made the unexpected discovery that the expansion of the universe appears to be accelerating.\n\nThe earliest and most direct observational evidence of the validity of the theory are the expansion of the universe according to Hubble's law (as indicated by the redshifts of galaxies), discovery and measurement of the cosmic microwave background and the relative abundances of light elements produced by Big Bang nucleosynthesis. More recent evidence includes observations of galaxy formation and evolution, and the distribution of large-scale cosmic structures, These are sometimes called the \"four pillars\" of the Big Bang theory.\n\nPrecise modern models of the Big Bang appeal to various exotic physical phenomena that have not been observed in terrestrial laboratory experiments or incorporated into the Standard Model of particle physics. Of these features, dark matter is currently subjected to the most active laboratory investigations. Remaining issues include the cuspy halo problem and the dwarf galaxy problem of cold dark matter. Dark energy is also an area of intense interest for scientists, but it is not clear whether direct detection of dark energy will be possible. Inflation and baryogenesis remain more speculative features of current Big Bang models. Viable, quantitative explanations for such phenomena are still being sought. These are currently unsolved problems in physics.\n\nObservations of distant galaxies and quasars show that these objects are redshifted—the light emitted from them has been shifted to longer wavelengths. This can be seen by taking a frequency spectrum of an object and matching the spectroscopic pattern of emission lines or absorption lines corresponding to atoms of the chemical elements interacting with the light. These redshifts are uniformly isotropic, distributed evenly among the observed objects in all directions. If the redshift is interpreted as a Doppler shift, the recessional velocity of the object can be calculated. For some galaxies, it is possible to estimate distances via the cosmic distance ladder. When the recessional velocities are plotted against these distances, a linear relationship known as Hubble's law is observed:\nformula_5\nwhere\n\nHubble's law has two possible explanations. Either we are at the center of an explosion of galaxies—which is untenable given the Copernican principle—or the universe is uniformly expanding everywhere. This universal expansion was predicted from general relativity by Alexander Friedmann in 1922 and Georges Lemaître in 1927, well before Hubble made his 1929 analysis and observations, and it remains the cornerstone of the Big Bang theory as developed by Friedmann, Lemaître, Robertson, and Walker.\n\nThe theory requires the relation formula_9 to hold at all times, where formula_7 is the comoving distance, \"v\" is the recessional velocity, and formula_6, formula_12, and formula_7 vary as the universe expands (hence we write formula_8 to denote the present-day Hubble \"constant\"). For distances much smaller than the size of the observable universe, the Hubble redshift can be thought of as the Doppler shift corresponding to the recession velocity formula_6. However, the redshift is not a true Doppler shift, but rather the result of the expansion of the universe between the time the light was emitted and the time that it was detected.\n\nThat space is undergoing metric expansion is shown by direct observational evidence of the Cosmological principle and the Copernican principle, which together with Hubble's law have no other explanation. Astronomical redshifts are extremely isotropic and homogeneous, supporting the Cosmological principle that the universe looks the same in all directions, along with much other evidence. If the redshifts were the result of an explosion from a center distant from us, they would not be so similar in different directions.\n\nMeasurements of the effects of the cosmic microwave background radiation on the dynamics of distant astrophysical systems in 2000 proved the Copernican principle, that, on a cosmological scale, the Earth is not in a central position. Radiation from the Big Bang was demonstrably warmer at earlier times throughout the universe. Uniform cooling of the CMB over billions of years is explainable only if the universe is experiencing a metric expansion, and excludes the possibility that we are near the unique center of an explosion.\n\nIn 1964 Arno Penzias and Robert Wilson serendipitously discovered the cosmic background radiation, an omnidirectional signal in the microwave band. Their discovery provided substantial confirmation of the big-bang predictions by Alpher, Herman and Gamow around 1950. Through the 1970s the radiation was found to be approximately consistent with a black body spectrum in all directions; this spectrum has been redshifted by the expansion of the universe, and today corresponds to approximately 2.725 K. This tipped the balance of evidence in favor of the Big Bang model, and Penzias and Wilson were awarded a Nobel Prize in 1978.\n\nThe \"surface of last scattering\" corresponding to emission of the CMB occurs shortly after \"recombination\", the epoch when neutral hydrogen becomes stable. Prior to this, the universe comprised a hot dense photon-baryon plasma sea where photons were quickly scattered from free charged particles. Peaking at around , the mean free path for a photon becomes long enough to reach the present day and the universe becomes transparent.\nIn 1989, NASA launched the Cosmic Background Explorer satellite (COBE), which made two major advances: in 1990, high-precision spectrum measurements showed that the CMB frequency spectrum is an almost perfect blackbody with no deviations at a level of 1 part in 10, and measured a residual temperature of 2.726 K (more recent measurements have revised this figure down slightly to 2.7255 K); then in 1992, further COBE measurements discovered tiny fluctuations (anisotropies) in the CMB temperature across the sky, at a level of about one part in 10. John C. Mather and George Smoot were awarded the 2006 Nobel Prize in Physics for their leadership in these results.\n\nDuring the following decade, CMB anisotropies were further investigated by a large number of ground-based and balloon experiments. In 2000–2001 several experiments, most notably BOOMERanG, found the shape of the universe to be spatially almost flat by measuring the typical angular size (the size on the sky) of the anisotropies.\n\nIn early 2003, the first results of the Wilkinson Microwave Anisotropy Probe (WMAP) were released, yielding what were at the time the most accurate values for some of the cosmological parameters. The results disproved several specific cosmic inflation models, but are consistent with the inflation theory in general. The Planck space probe was launched in May 2009. Other ground and balloon based cosmic microwave background experiments are ongoing.\n\nUsing the Big Bang model it is possible to calculate the concentration of helium-4, helium-3, deuterium, and lithium-7 in the universe as ratios to the amount of ordinary hydrogen. The relative abundances depend on a single parameter, the ratio of photons to baryons. This value can be calculated independently from the detailed structure of CMB fluctuations. The ratios predicted (by mass, not by number) are about 0.25 for <chem>^4He/H</chem>, about 10 for <chem>^2H/H</chem>, about 10 for <chem>^3He/H</chem> and about 10 for <chem>^7Li/H</chem>.\n\nThe measured abundances all agree at least roughly with those predicted from a single value of the baryon-to-photon ratio. The agreement is excellent for deuterium, close but formally discrepant for <chem>^4He</chem>, and off by a factor of two for <chem>^7Li</chem>; in the latter two cases there are substantial systematic uncertainties. Nonetheless, the general consistency with abundances predicted by Big Bang nucleosynthesis is strong evidence for the Big Bang, as the theory is the only known explanation for the relative abundances of light elements, and it is virtually impossible to \"tune\" the Big Bang to produce much more or less than 20–30% helium. Indeed, there is no obvious reason outside of the Big Bang that, for example, the young universe (i.e., before star formation, as determined by studying matter supposedly free of stellar nucleosynthesis products) should have more helium than deuterium or more deuterium than <chem>^3He</chem>, and in constant ratios, too.\n\nDetailed observations of the morphology and distribution of galaxies and quasars are in agreement with the current state of the Big Bang theory. A combination of observations and theory suggest that the first quasars and galaxies formed about a billion years after the Big Bang, and since then, larger structures have been forming, such as galaxy clusters and superclusters.\n\nPopulations of stars have been aging and evolving, so that distant galaxies (which are observed as they were in the early universe) appear very different from nearby galaxies (observed in a more recent state). Moreover, galaxies that formed relatively recently, appear markedly different from galaxies formed at similar distances but shortly after the Big Bang. These observations are strong arguments against the steady-state model. Observations of star formation, galaxy and quasar distributions and larger structures, agree well with Big Bang simulations of the formation of structure in the universe, and are helping to complete details of the theory.\n\nIn 2011, astronomers found what they believe to be pristine clouds of primordial gas by analyzing absorption lines in the spectra of distant quasars. Before this discovery, all other astronomical objects have been observed to contain heavy elements that are formed in stars. These two clouds of gas contain no elements heavier than hydrogen and deuterium. Since the clouds of gas have no heavy elements, they likely formed in the first few minutes after the Big Bang, during Big Bang nucleosynthesis.\n\nThe age of the universe as estimated from the Hubble expansion and the CMB is now in good agreement with other estimates using the ages of the oldest stars, both as measured by applying the theory of stellar evolution to globular clusters and through radiometric dating of individual Population II stars.\n\nThe prediction that the CMB temperature was higher in the past has been experimentally supported by observations of very low temperature absorption lines in gas clouds at high redshift. This prediction also implies that the amplitude of the Sunyaev–Zel'dovich effect in clusters of galaxies does not depend directly on redshift. Observations have found this to be roughly true, but this effect depends on cluster properties that do change with cosmic time, making precise measurements difficult.\n\nFuture gravitational waves observatories might be able to detect primordial gravitational waves, relics of the early universe, up to less than a second after the Big Bang.\n\nAs with any theory, a number of mysteries and problems have arisen as a result of the development of the Big Bang theory. Some of these mysteries and problems have been resolved while others are still outstanding. Proposed solutions to some of the problems in the Big Bang model have revealed new mysteries of their own. For example, the horizon problem, the magnetic monopole problem, and the flatness problem are most commonly resolved with inflationary theory, but the details of the inflationary universe are still left unresolved and many, including some founders of the theory, say it has been disproven. What follows are a list of the mysterious aspects of the Big Bang theory still under intense investigation by cosmologists and astrophysicists.\n\nIt is not yet understood why the universe has more matter than antimatter. It is generally assumed that when the universe was young and very hot it was in statistical equilibrium and contained equal numbers of baryons and antibaryons. However, observations suggest that the universe, including its most distant parts, is made almost entirely of matter. A process called baryogenesis was hypothesized to account for the asymmetry. For baryogenesis to occur, the Sakharov conditions must be satisfied. These require that baryon number is not conserved, that C-symmetry and CP-symmetry are violated and that the universe depart from thermodynamic equilibrium. All these conditions occur in the Standard Model, but the effects are not strong enough to explain the present baryon asymmetry.\n\nMeasurements of the redshift–magnitude relation for type Ia supernovae indicate that the expansion of the universe has been accelerating since the universe was about half its present age. To explain this acceleration, general relativity requires that much of the energy in the universe consists of a component with large negative pressure, dubbed \"dark energy\".\n\nDark energy, though speculative, solves numerous problems. Measurements of the cosmic microwave background indicate that the universe is very nearly spatially flat, and therefore according to general relativity the universe must have almost exactly the critical density of mass/energy. But the mass density of the universe can be measured from its gravitational clustering, and is found to have only about 30% of the critical density. Since theory suggests that dark energy does not cluster in the usual way it is the best explanation for the \"missing\" energy density. Dark energy also helps to explain two geometrical measures of the overall curvature of the universe, one using the frequency of gravitational lenses, and the other using the characteristic pattern of the large-scale structure as a cosmic ruler.\n\nNegative pressure is believed to be a property of vacuum energy, but the exact nature and existence of dark energy remains one of the great mysteries of the Big Bang. Results from the WMAP team in 2008 are in accordance with a universe that consists of 73% dark energy, 23% dark matter, 4.6% regular matter and less than 1% neutrinos. According to theory, the energy density in matter decreases with the expansion of the universe, but the dark energy density remains constant (or nearly so) as the universe expands. Therefore, matter made up a larger fraction of the total energy of the universe in the past than it does today, but its fractional contribution will fall in the far future as dark energy becomes even more dominant.\n\nThe dark energy component of the universe has been explained by theorists using a variety of competing theories including Einstein's cosmological constant but also extending to more exotic forms of quintessence or other modified gravity schemes. A cosmological constant problem, sometimes called the \"most embarrassing problem in physics\", results from the apparent discrepancy between the measured energy density of dark energy, and the one naively predicted from Planck units.\n\nDuring the 1970s and the 1980s, various observations showed that there is not sufficient visible matter in the universe to account for the apparent strength of gravitational forces within and between galaxies. This led to the idea that up to 90% of the matter in the universe is dark matter that does not emit light or interact with normal baryonic matter. In addition, the assumption that the universe is mostly normal matter led to predictions that were strongly inconsistent with observations. In particular, the universe today is far more lumpy and contains far less deuterium than can be accounted for without dark matter. While dark matter has always been controversial, it is inferred by various observations: the anisotropies in the CMB, galaxy cluster velocity dispersions, large-scale structure distributions, gravitational lensing studies, and X-ray measurements of galaxy clusters.\n\nIndirect evidence for dark matter comes from its gravitational influence on other matter, as no dark matter particles have been observed in laboratories. Many particle physics candidates for dark matter have been proposed, and several projects to detect them directly are underway.\n\nAdditionally, there are outstanding problems associated with the currently favored cold dark matter model which include the dwarf galaxy problem and the cuspy halo problem. Alternative theories have been proposed that do not require a large amount of undetected matter, but instead modify the laws of gravity established by Newton and Einstein; yet no alternative theory has been as successful as the cold dark matter proposal in explaining all extant observations.\n\nThe horizon problem results from the premise that information cannot travel faster than light. In a universe of finite age this sets a limit—the particle horizon—on the separation of any two regions of space that are in causal contact. The observed isotropy of the CMB is problematic in this regard: if the universe had been dominated by radiation or matter at all times up to the epoch of last scattering, the particle horizon at that time would correspond to about 2 degrees on the sky. There would then be no mechanism to cause wider regions to have the same temperature.\n\nA resolution to this apparent inconsistency is offered by inflationary theory in which a homogeneous and isotropic scalar energy field dominates the universe at some very early period (before baryogenesis). During inflation, the universe undergoes exponential expansion, and the particle horizon expands much more rapidly than previously assumed, so that regions presently on opposite sides of the observable universe are well inside each other's particle horizon. The observed isotropy of the CMB then follows from the fact that this larger region was in causal contact before the beginning of inflation.\n\nHeisenberg's uncertainty principle predicts that during the inflationary phase there would be quantum thermal fluctuations, which would be magnified to cosmic scale. These fluctuations serve as the seeds of all current structure in the universe. Inflation predicts that the primordial fluctuations are nearly scale invariant and Gaussian, which has been accurately confirmed by measurements of the CMB.\n\nIf inflation occurred, exponential expansion would push large regions of space well beyond our observable horizon.\n\nA related issue to the classic horizon problem arises because in most standard cosmological inflation models, inflation ceases well before electroweak symmetry breaking occurs, so inflation should not be able to prevent large-scale discontinuities in the electroweak vacuum since distant parts of the observable universe were causally separate when the electroweak epoch ended.\n\nThe magnetic monopole objection was raised in the late 1970s. Grand unified theories predicted topological defects in space that would manifest as magnetic monopoles. These objects would be produced efficiently in the hot early universe, resulting in a density much higher than is consistent with observations, given that no monopoles have been found. This problem is also resolved by cosmic inflation, which removes all point defects from the observable universe, in the same way that it drives the geometry to flatness.\n\nThe flatness problem (also known as the oldness problem) is an observational problem associated with a Friedmann–Lemaître–Robertson–Walker metric (FLRW). The universe may have positive, negative, or zero spatial curvature depending on its total energy density. Curvature is negative if its density is less than the critical density; positive if greater; and zero at the critical density, in which case space is said to be \"flat\".\n\nThe problem is that any small departure from the critical density grows with time, and yet the universe today remains very close to flat. Given that a natural timescale for departure from flatness might be the Planck time, 10 seconds, the fact that the universe has reached neither a heat death nor a Big Crunch after billions of years requires an explanation. For instance, even at the relatively late age of a few minutes (the time of nucleosynthesis), the density of the universe must have been within one part in 10 of its critical value, or it would not exist as it does today.\n\nPhysics may conclude that time did not exist before 'Big Bang', but 'started' with the Big Bang and hence there might be no 'beginning', 'before' or potentially 'cause' and instead always existed. Quantum fluctuations, or other laws of physics that may have existed at the start of the Big Bang could then create the conditions for matter to occur.\n\nBefore observations of dark energy, cosmologists considered two scenarios for the future of the universe. If the mass density of the universe were greater than the critical density, then the universe would reach a maximum size and then begin to collapse. It would become denser and hotter again, ending with a state similar to that in which it started—a Big Crunch.\n\nAlternatively, if the density in the universe were equal to or below the critical density, the expansion would slow down but never stop. Star formation would cease with the consumption of interstellar gas in each galaxy; stars would burn out, leaving white dwarfs, neutron stars, and black holes. Very gradually, collisions between these would result in mass accumulating into larger and larger black holes. The average temperature of the universe would asymptotically approach absolute zero—a Big Freeze. Moreover, if the proton were unstable, then baryonic matter would disappear, leaving only radiation and black holes. Eventually, black holes would evaporate by emitting Hawking radiation. The entropy of the universe would increase to the point where no organized form of energy could be extracted from it, a scenario known as heat death.\n\nModern observations of accelerating expansion imply that more and more of the currently visible universe will pass beyond our event horizon and out of contact with us. The eventual result is not known. The ΛCDM model of the universe contains dark energy in the form of a cosmological constant. This theory suggests that only gravitationally bound systems, such as galaxies, will remain together, and they too will be subject to heat death as the universe expands and cools. Other explanations of dark energy, called phantom energy theories, suggest that ultimately galaxy clusters, stars, planets, atoms, nuclei, and matter itself will be torn apart by the ever-increasing expansion in a so-called Big Rip.\n\nThe following is a partial list of misconceptions about the Big Bang model:\n\n\"The Big Bang as the origin of the universe:\" One of the common misconceptions about the Big Bang model is the belief that it was the origin of the universe. However, the Big Bang model does not comment about how the universe came into being. Current conception of the Big Bang model assumes the existence of energy, time, and space, and does not comment about their origin or the cause of the dense and high temperature initial state of the universe.\n\n\"The Big Bang was \"small\"\": It is misleading to visualize the Big Bang by comparing its size to everyday objects. When the size of the universe at Big Bang is described, it refers to the size of the observable universe, and not the entire universe.\n\n\"Hubble's law violates the special theory of relativity\": Hubble's law predicts that galaxies that are beyond Hubble Distance recede faster than the speed of light. However, special relativity does not apply beyond motion through space. Hubble's law describes velocity that results from expansion \"of\" space, rather than \"through\" space.\n\n\"Doppler redshift vs cosmological red-shift\": Astronomers often refer to the cosmological red-shift as a normal Doppler shift, which is a misconception. Although similar, the cosmological red-shift is not identical to the Doppler redshift. The Doppler redshift is based on special relativity, which does not consider the expansion of space. On the contrary, the cosmological red-shift is based on general relativity, in which the expansion of space is considered. Although they may appear identical for nearby galaxies, it may cause confusion if the behavior of distant galaxies is understood through the Doppler redshift.\n\nWhile the Big Bang model is well established in cosmology, it is likely to be refined. The Big Bang theory, built upon the equations of classical general relativity, indicates a singularity at the origin of cosmic time; this infinite energy density is regarded as impossible in physics. Still, it is known that the equations are not applicable before the time when the universe cooled down to the Planck temperature, and this conclusion depends on various assumptions, of which some could never be experimentally verified. \"(Also see Planck epoch.)\"\n\nOne proposed refinement to avoid this would-be singularity is to develop a correct treatment of quantum gravity.\n\nIt is not known what could have preceded the hot dense state of the early universe or how and why it originated, though speculation abounds in the field of cosmogony.\n\nSome proposals, each of which entails untested hypotheses, are:\n\nProposals in the last two categories see the Big Bang as an event in either a much larger and older universe or in a multiverse.\n\nAs a description of the origin of the universe, the Big Bang has significant bearing on religion and philosophy. As a result, it has become one of the liveliest areas in the discourse between science and religion. Some believe the Big Bang implies a creator, and some see its mention in their holy books, while others argue that Big Bang cosmology makes the notion of a creator superfluous.\n\n\n\n\n"}
{"id": "585226", "url": "https://en.wikipedia.org/wiki?curid=585226", "title": "Cosmography", "text": "Cosmography\n\nCosmography is the science that maps the general features of the cosmos or universe, describing both heaven and Earth (but without encroaching on geography or astronomy). The 14th-century work \"'Aja'ib al-makhluqat wa-ghara'ib al-mawjudat\" by Persian physician Zakariya al-Qazwini is considered to be an early work of cosmography.\n\nTraditional Hindu, Buddhist and Jain cosmography schematize a universe centered on Mount Meru surrounded by rivers, continents and seas. These cosmographies posit a universe being repeatedly created and destroyed over time cycles of immense lengths.\n\nIn 1551, Martín Cortés de Albacar, from Zaragoza, Spain, published \"Breve compendio de la esfera y del arte de navegar\". Translated into English and reprinted several times, the work was of great influence in Britain for many years. He proposed spherical charts and mentioned magnetic deviation and the existence of magnetic poles.\n\nPeter Heylin's 1652 book \"Cosmographie\" (enlarged from his \"Microcosmos\" of 1621) was one of the earliest attempts to describe the entire world in English, and being the first known description of Australia and among the first of California. The book has 4 sections, examining the geography, politics, and cultures of Europe, Asia, Africa, and America, with an addendum on \"Terra Incognita\", including Australia, and extending to Utopia, Fairyland, and the \"Land of Chivalrie\".\n\nIn 1659, Thomas Porter published a smaller, but extensive \"Compendious Description of the Whole World\", which also included a chronology of world events from Creation forward. These were all part of a major trend in the European Renaissance to explore (and perhaps comprehend) the known world.\n\nThe word was also commonly used by Buckminster Fuller in his lectures.\n\nIn astrophysics, the term \"cosmography\" is beginning to be used to describe attempts to determine the large-scale matter distribution and kinematics of the observable universe, dependent on the Friedmann–Lemaître–Robertson–Walker metric but independent of the temporal dependence of the scale factor on the matter/energy composition of the Universe.\n"}
{"id": "2848730", "url": "https://en.wikipedia.org/wiki?curid=2848730", "title": "Cosmological horizon", "text": "Cosmological horizon\n\nA cosmological horizon is a measure of the distance from which one could possibly retrieve information. This observable constraint is due to various properties of general relativity, the expanding universe, and the physics of Big Bang cosmology. Cosmological horizons set the size and scale of the observable universe. This article explains a number of these horizons.\n\nThe particle horizon (also called the cosmological horizon, the comoving horizon, or the cosmic light horizon) is the maximum distance from which particles could have traveled to the observer in the age of the universe. It represents the boundary between the observable and the unobservable regions of the universe, so its distance at the present epoch defines the size of the observable universe. Due to the expansion of the universe it is not simply the age of the universe times the speed of light, as in the Hubble horizon, but rather the speed of light multiplied by the conformal time. The existence, properties, and significance of a cosmological horizon depend on the particular cosmological model.\n\nIn terms of comoving distance, the particle horizon is equal to the conformal time that has passed since the Big Bang, times the speed of light. In general, the conformal time at a certain time is given in terms of the scale factor formula_1 by,\n\nor\n\nThe particle horizon is the boundary between two regions at a point at a given time: one region defined by events that have already been observed by an observer, and the other by events which cannot be observed \"at that time\". It represents the furthest distance from which we can retrieve information from the past, and so defines the observable universe.\n\nHubble radius, Hubble sphere, Hubble volume, or Hubble horizon is a conceptual horizon defining the boundary between particles that are moving slower and faster than the speed of light relative to an observer at one given time. Note that this does not mean the particle is unobservable, the light from the past is reaching and will continue to reach the observer for a while. Also, more importantly, in the current expansion model e.g., light emitted from the Hubble radius will reach us in a finite amount of time. It is a common misconception that light from the Hubble radius can never reach us. It is true that particles on the Hubble radius recede from us with the speed of light, but the Hubble radius gets larger over time (because the Hubble parameter H gets smaller over time), so light emitted towards us from a particle on the Hubble radius will be inside the Hubble radius some time later. Only light emitted from the cosmic event horizon or further will never reach us in a finite amount of time.\n\nThe Hubble velocity of an object is given by Hubble's law,\n\nReplacing formula_5 with speed of light formula_6 and solving for proper distance formula_7 we obtain the radius of Hubble sphere as\n\nIn an ever-accelerating universe, if two particles are separated by a distance greater than the Hubble radius, they cannot talk to each other from now on (as they are now, not as they have been in the past), However, if they are outside of each other's particle horizon, they could have never communicated. Depending on the form of expansion of the universe, they may be able to exchange information in the future. Today,\n\nyielding a Hubble horizon of some 4.1 Gpc. This horizon is not really a physical size, but it is often used as useful length scale as most physical sizes in cosmology can be written in terms of those factors.\n\nOne can also define comoving Hubble horizon by simply dividing Hubble radius by the scale factor\n\nThe particle horizon differs from the cosmic event horizon, in that the particle horizon represents the largest comoving distance from which light could have reached the observer by a specific time, while the event horizon is the largest comoving distance from which light emitted now can \"ever\" reach the observer in the future. The current distance to our cosmic event horizon is about 5 Gpc (16 billion light years), well within our observable range given by the particle horizon.\n\nIn general, the proper distance to the event horizon at time formula_11 is given by\n\nwhere formula_13 is the time-coordinate of the end of the universe, which would be infinite in the case of a universe that expands forever.\n\nFor our case, assuming that dark energy is due to a cosmological constant, formula_14.\n\nIn an accelerating universe, there are events which will be unobservable as formula_15 as signals from future events become redshifted to arbitrarily long wavelengths in the exponentially expanding de Sitter space. This sets a limit on the farthest distance that we can possibly see as measured in units of proper distance today. Or, more precisely, there are events that are spatially separated for a certain frame of reference happening simultaneously with the event occurring right now for which no signal will ever reach us, even though we can observe events that occurred at the same location in space that happened in the distant past. While we will continue to receive signals from this location in space, even if we wait an infinite amount of time, a signal that left from that location today will never reach us. Additionally, the signals coming from that location will have less and less energy and be less and less frequent until the location, for all practical purposes, becomes unobservable. In a universe that is dominated by dark energy which is undergoing an exponential expansion of the scale factor, all objects that are gravitationally unbound with respect to the Milky Way will become unobservable, in a futuristic version of Kapteyn's universe.\n\nWhile not technically \"horizons\" in the sense of an impossibility for observations due to relativity or cosmological solutions, there are practical horizons which include the optical horizon, set at the surface of last scattering. This is the farthest distance that any photon can freely stream. Similarly, there is a \"neutrino horizon\" set for the farthest distance a neutrino can freely stream and a gravitational wave horizon at the farthest distance that gravitational waves can freely stream. The latter is predicted to be a direct probe of the end of cosmic inflation.\n\nFor a simplified summary and overview of different horizons in cosmology, see Different Horizons in Cosmology\n"}
{"id": "38737", "url": "https://en.wikipedia.org/wiki?curid=38737", "title": "Cosmos", "text": "Cosmos\n\nThe cosmos (, ) is the universe. Using the word \"cosmos\" rather than the word \"universe\" implies viewing the universe as a complex and orderly system or entity; the opposite of chaos.\nThe cosmos, and our understanding of the reasons for its existence and significance, are studied in cosmology - a very broad discipline covering any scientific, religious, or philosophical contemplation of the cosmos and its nature, or reasons for existing. Religious and philosophical approaches may include in their concepts of the cosmos various spiritual entities or other matters deemed to exist outside our physical universe.\n\nThe philosopher Pythagoras first used the term \"cosmos\" () for the order of the universe. The term became part of modern language in the 19th century when geographer–polymath Alexander von Humboldt resurrected the use of the word from the ancient Greek, assigned it to his five-volume treatise, \"Kosmos\", which influenced modern and somewhat holistic perception of the universe as one interacting entity.\n\nCosmology is the study of the cosmos, and in its broadest sense covers a variety of very different approaches: scientific, religious and philosophical. All cosmologies have in common an attempt to understand the implicit order within the whole of being. In this way, most religions and philosophical systems have a cosmology.\n\nWhen \"cosmology\" is used without a qualifier, it often signifies physical cosmology, unless the context makes clear that a different meaning is intended.\n\nPhysical cosmology (often simply described as 'cosmology') is the scientific study of the universe, from the beginning of its physical existence. It includes speculative concepts such as a multiverse, when these are being discussed. In physical cosmology, the term \"cosmos\" is often used in a technical way, referring to a particular spacetime continuum within a (postulated) multiverse. Our particular cosmos, the observable universe, is generally capitalized as \"the Cosmos\". \n\nIn physical cosmology, the uncapitalized term cosmic signifies a subject with a relationship to the universe, such as 'cosmic time' (time since the Big Bang), 'cosmic rays' (high energy particles or radiation detected from space), and 'cosmic microwave background' (microwave radiation detectable from all directions in space).\n\nAccording to in Sir William Smith \"Dictionary of Greek and Roman Biography and Mythology\" (1870, see book screenshot for full quote), Pythagoreans described the universe.\n\nCosmology is a branch of metaphysics that deals with the nature of the universe, a theory or doctrine describing the natural order of the universe. The basic definition of Cosmology is the science of the origin and development of the universe. In modern astronomy the Big Bang theory is the dominant postulation.\n\nIn theology, the cosmos is the created heavenly bodies (sun, moon, planets, and fixed stars). In Christian theology, the word is also used synonymously with \"aion\" to refer to \"worldly life\" or \"this world\" or \"this age\" as opposed to the afterlife or world to come.\n\nThe 1870 book \"Dictionary of Greek and Roman Biography and Mythology\" noted\n\nThe book \"The Works of Aristotle\" (1908, p. 80 \"Fragments\") mentioned\n\nBertrand Russell (1947) noted\n\n"}
{"id": "13566984", "url": "https://en.wikipedia.org/wiki?curid=13566984", "title": "Double layer (surface science)", "text": "Double layer (surface science)\n\nA double layer (DL, also called an electrical double layer, EDL) is a structure that appears on the surface of an object when it is exposed to a fluid. The object might be a solid particle, a gas bubble, a liquid droplet, or a porous body. The DL refers to two parallel layers of charge surrounding the object. The first layer, the surface charge (either positive or negative), consists of ions adsorbed onto the object due to chemical interactions. The second layer is composed of ions attracted to the surface charge via the Coulomb force, electrically screening the first layer. This second layer is loosely associated with the object. It is made of free ions that move in the fluid under the influence of electric attraction and thermal motion rather than being firmly anchored. It is thus called the \"diffuse layer\".\n\nInterfacial DLs are most apparent in systems with a large surface area to volume ratio, such as a colloid or porous bodies with particles or pores (respectively) on the scale of micrometres to nanometres. However, DLs are important to other phenomena, such as the electrochemical behaviour of electrodes.\n\nDLs play a fundamental role in many everyday substances. For instance, homogenized milk exists only because fat droplets are covered with a DL that prevents their coagulation into butter. DLs exist in practically all heterogeneous fluid-based systems, such as blood, paint, ink and ceramic and cement slurry.\n\nThe DL is closely related to electrokinetic phenomena and electroacoustic phenomena.\n\nWhen an \"electronic\" conductor is brought in contact with a solid or liquid \"ionic\" conductor (electrolyte), a common boundary (interface) among the two phases appears. Hermann von Helmholtz was the first to realize that charged electrodes immersed in electrolyte solutions repel the co-ions of the charge while attracting counterions to their surfaces. Two layers of opposite polarity form at the interface between electrode and electrolyte.\nIn 1853 he showed that an electrical double layer (DL) is essentially a molecular dielectric and stores charge electrostatically. Below the electrolyte's decomposition voltage, the stored charge is linearly dependent on the voltage applied.\n\nThis early model predicted a constant differential capacitance independent from the charge density depending on the dielectric constant of the electrolyte solvent and the thickness of the double-layer.\n\nThis model, with a good foundation for the description of the interface, does not consider important factors including diffusion/mixing of ions in solution, the possibility of adsorption onto the surface and the interaction between solvent dipole moments and the electrode.\n\nLouis Georges Gouy in 1910 and David Leonard Chapman in 1913 both observed that capacitance was not a constant and that it depended on the applied potential and the ionic concentration. The \"Gouy-Chapman model\" made significant improvements by introducing a diffuse model of the DL. In this model the charge distribution of ions as a function of distance from the metal surface allows Maxwell–Boltzmann statistics to be applied. Thus the electric potential decreases exponentially away from the surface of the fluid bulk.\n\nThe Gouy-Chapman model fails for highly charged DLs. In 1924 Otto Stern suggested combining the Helmholtz model with the Gouy-Chapman model: In Stern's model, some ions adhere to the electrode as suggested by Helmholtz, giving an internal Stern layer, while some form a Gouy-Chapman diffuse layer.\n\nThe Stern layer accounts for ions' finite size and consequently an ion's closest approach to the electrode is on the order of the ionic radius. The Stern model has its own limitations, namely that it effectively treats ions as point charges, assumes all significant interactions in the diffuse layer are Coulombic, and assumes dielectric permittivity to be constant throughout the double layer and that fluid viscosity is constant plane.\n\nD. C. Grahame modified the Stern model in 1947. He proposed that some ionic or uncharged species can penetrate the Stern layer, although the closest approach to the electrode is normally occupied by solvent molecules. This could occur if ions lose their solvation shell as they approach the electrode. He called ions in direct contact with the electrode \"specifically adsorbed ions\". This model proposed the existence of three regions. The inner Helmholtz plane (IHP) passes through the centres of the specifically adsorbed ions. The outer Helmholtz plane (OHP) passes through the centres of solvated ions at the distance of their closest approach to the electrode. Finally the diffuse layer is the region beyond the OHP.\n\nIn 1963 J. O'M. Bockris, M. A. V. Devanathan and Klaus Müller proposed the BDM model of the double-layer that included the action of the solvent in the interface. They suggested that the attached molecules of the solvent, such as water, would have a fixed alignment to the electrode surface. This first layer of solvent molecules displays a strong orientation to the electric field depending on the charge. This orientation has great influence on the permittivity of the solvent that varies with field strength. The IHP passes through the centers of these molecules. Specifically adsorbed, partially solvated ions appear in this layer. The solvated ions of the electrolyte are outside the IHP. Through the centers of these ions pass the OHP. The diffuse layer is the region beyond the OHP.\n\nFurther research with double layers on ruthenium dioxide films in 1971 by Sergio Trasatti and Giovanni Buzzanca demonstrated that the electrochemical behavior of these electrodes at low voltages with specific adsorbed ions was like that of capacitors. The specific adsorption of the ions in this region of potential could also involve a partial charge transfer between the ion and the electrode. It was the first step towards understanding pseudocapacitance.\n\nBetween 1975 and 1980 Brian Evans Conway conducted extensive fundamental and development work on ruthenium oxide electrochemical capacitors. In 1991 he described the difference between 'Supercapacitor' and 'Battery' behavior in electrochemical energy storage. In 1999 he coined the term supercapacitor to explain the increased capacitance by surface redox reactions with faradaic charge transfer between electrodes and ions.\n\nHis \"supercapacitor\" stored electrical charge partially in the Helmholtz double-layer and partially as the result of faradaic reactions with \"pseudocapacitance\" charge transfer of electrons and protons between electrode and electrolyte. The working mechanisms of pseudocapacitors are redox reactions, intercalation and electrosorption.\n\nThe physical and mathematical basics of electron charge transfer absent chemical bonds leading to pseudocapacitance was developed by Rudolph A. Marcus. Marcus Theory explains the rates of electron transfer reactions—the rate at which an electron can move from one chemical species to another. It was originally formulated to address outer sphere electron transfer reactions, in which two chemical species change only in their charge, with an electron jumping. For redox reactions without making or breaking bonds, Marcus theory takes the place of Henry Eyring's transition state theory which was derived for reactions with structural changes. Marcus received the Nobel Prize in Chemistry in 1992 for this theory.\n\nThere are detailed descriptions of the interfacial DL in many books on colloid and interface science and microscale fluid transport. There is also a recent IUPAC technical report on the subject of interfacial double layer and related electrokinetic phenomena.\n\nAs stated by Lyklema, \"...the reason for the formation of a \"relaxed\" (\"equilibrium\") double layer is the non-electric affinity of charge-determining ions for a surface...\" This process leads to the buildup of an electric surface charge, expressed usually in C/m. This surface charge creates an electrostatic field that then affects the ions in the bulk of the liquid. This electrostatic field, in combination with the thermal motion of the ions, creates a counter charge, and thus screens the electric surface charge. The net electric charge in this screening diffuse layer is equal in magnitude to the net surface charge, but has the opposite polarity. As a result, the complete structure is electrically neutral.\n\nThe diffuse layer, or at least part of it, can move under the influence of tangential stress. There is a conventionally introduced slipping plane that separates mobile fluid from fluid that remains attached to the surface. Electric potential at this plane is called electrokinetic potential or zeta potential (also denoted as ζ-potential).\n\nThe electric potential on the external boundary of the Stern layer versus the bulk electrolyte is referred to as Stern potential. Electric potential difference between the fluid bulk and the surface is called the electric surface potential.\n\nUsually zeta potential is used for estimating the degree of DL charge. A characteristic value of this electric potential in the DL is 25 mV with a maximum value around 100 mV (up to several volts on electrodes). The chemical composition of the sample at which the ζ-potential is 0 is called the point of zero charge or the iso-electric point. It is usually determined by the solution pH value, since protons and hydroxyl ions are the charge-determining ions for most surfaces.\n\nZeta potential can be measured using electrophoresis, electroacoustic phenomena, streaming potential, and electroosmotic flow.\n\nThe characteristic thickness of the DL is the Debye length, κ. It is reciprocally proportional to the square root of the ion concentration \"C\". In aqueous solutions it is typically on the scale of a few nanometers and the thickness decreases with increasing concentration of the electrolyte.\n\nThe electric field strength inside the DL can be anywhere from zero to over 10 V/m. These steep electric potential gradients are the reason for the importance of the DLs.\n\nThe theory for a flat surface and a symmetrical electrolyte is usually referred to as the Gouy-Chapman theory. It yields a simple relationship between electric charge in the diffuse layer σ and the Stern potential Ψ:\n"}
{"id": "13551670", "url": "https://en.wikipedia.org/wiki?curid=13551670", "title": "Electroacoustic phenomena", "text": "Electroacoustic phenomena\n\nElectroacoustic phenomena arise when ultrasound propagates through a fluid containing ions. The associated particle motion generates electric signals because ions have electric charge. This coupling between ultrasound and electric field is called electroacoustic phenomena. The fluid might be a simple Newtonian liquid, or complex heterogeneous dispersion, emulsion or even a porous body. There are several different electroacoustic effects depending on the nature of the fluid.\n\n\nHistorically, the IVI was the first known electroacoustic effect. It was predicted by Debye in 1933.\n\nThe streaming vibration current was experimentally observed in 1948 by Williams. A theoretical model was developed some 30 years later by Dukhin and others. This effect opens another possibility for characterizing the electric properties of the surfaces in porous bodies. A similar effect can be observed at a non-porous surface, when sound is bounced off at an oblique angle. The incident and reflected waves superimpose to cause oscillatory fluid motion in the plane of the interface, thereby generating an AC streaming current at the frequency of the sound waves.\n\nThe electrical double layer can be regarded as behaving like a parallel plate capacitor with a compressible dielectric filling. When sound waves induce a local pressure variation, the spacing of the plates varies at the frequency of the excitation, generating an AC displacement current normal to the interface. For practical reasons this is most readily observed at a conducting surface. It is therefore possible to use an electrode immersed in a conducting electrolyte as a microphone, or indeed as a loudspeaker when the effect is applied in reverse.\n\nColloid vibration potential measures the AC potential difference generated between two identical relaxed electrodes, placed in the dispersion, if the latter is subjected to an ultrasonic field. When a sound wave travels through a colloidal suspension of particles whose density differs from that of the surrounding medium, inertial forces induced by the vibration of the suspension give rise to a motion of the charged particles relative to the liquid, causing an alternating electromotive force. The manifestations of this electromotive force may be measured, depending on the relation between the impedance of the suspension and that of the measuring instrument, either as colloid vibration potential or as \"colloid vibration current\".\n\nColloid vibration potential and current was first reported by Hermans and then independently by Rutgers in 1938. It is widely used for characterizing the ζ-potential of various dispersions and emulsions. The effect, theory, experimental verification and multiple applications are discussed in the book by Dukhin and Goetz.\n\nElectric sonic amplitude was experimentally discovered by Cannon with co-authors in early 1980s. It is also widely used for characterizing ζ-potential in dispersions and emulsions. There is review of this effect theory, experimental verification and multiple applications published by Hunter.\n\nWith regard to the theory of CVI and ESA, there was an important observation made by O'Brien, who linked these measured parameters with dynamic electrophoretic mobility μ.\n\nwhere\n\nDynamic electrophoretic mobility is similar to electrophoretic mobility that appears in electrophoresis theory. They are identical at low frequencies and/or for sufficiently small particles.\n\nThere are several theories of the dynamic electrophoretic mobility. Their overview is given in the Ref.5. Two of them are the most important.\n\nThe first one corresponds to the Smoluchowski limit. It yields following simple expression for CVI for sufficiently small particles with negligible CVI frequency dependence:\n\nwhere:\n\nThis remarkably simple equation has same wide range of applicability as Smoluchowski equation for electrophoresis. It is independent on shape of the particles, their concentration.\n\nValidity of this equation is restricted with the following two requirements.\nFirst, it is valid only for a thin double layer, when the Debye length is much smaller than particle's radius a:\n\nSecondly, it neglects the contribution of the surface conductivity. This assumes a small Dukhin number:\n\nRestriction of the thin double layer limits applicability of this Smoluchowski type theory only to aqueous systems with sufficiently large particles and not very low ionic strength. This theory does not work well for nano-colloids, including proteins and polymers at low ionic strength. It is not valid for low- or non-polar fluids.\n\nThere is another theory that is applicable for the other extreme case of a thick double layer, when \n\nThis theory takes into consideration the double layer overlap that inevitably occurs for concentrated systems with thick double layer. This allows introduction of so-called \"quasi-homogeneous\" approach, when overlapped diffuse layers of particles cover the complete interparticle space. The theory becomes much simplified in this extreme case, as shown by Shilov and others. Their derivation predicts that surface charge density σ is a better parameter than ζ-potential for characterizing electroacoustic phenomena in such systems. An expression for CVI simplified for small particles follows:\n\n"}
{"id": "38328166", "url": "https://en.wikipedia.org/wiki?curid=38328166", "title": "Energy broker", "text": "Energy broker\n\nEnergy brokers assist clients in procuring electric or natural gas from energy wholesalers/suppliers. Since electricity and natural gas are commodities, prices change daily with the market. It is challenging for most businesses without energy managers to obtain price comparisons from a variety of suppliers since prices must be compared on exactly the same day. In addition, the terms of the particular contract offered by the supplier influences the price that is quoted. An energy broker can provide a valuable service if they work with a large number of suppliers and can actually compile the sundry prices from suppliers. An important aspect of this consulting role is to assure that the client understands the differences between the contract offers. Under some State Laws they use the term \"Suppliers\" to refer to energy suppliers, brokers, and aggregators, however there are very important differences between them all.\n\nEnergy brokers do not own or distribute energy, nor are allowed to sell energy directly to you. They simply present the rates of a wholesaler, or supplier.\n\nEnergy consultants offer a lot more than procuring energy contracts from a supplier. In the UK and Europe where there is a lot of legislation and increasing pressure for businesses and countries to do more to reduce their energy consumption a lot of services from brokers now help ensure businesses meet a lot of compliance and accreditation requirements such as the ESOS (energy saving opportunity scheme), ISO 50001, ISO 14001, Energy Performance Certificates and Display Energy Certificates.\nOther services include helping companies reduce energy consumption with the aim of meeting national and international carbon emission standards. Services include, energy health checks, energy audits, carbon zero, carbon offsetting and energy saving consulting.\n\nAdditional services such as arranging a power purchase agreement, energy export contracts can be procured as well as energy monitoring and reporting technology and solutions are also offered by energy consultants.\n\nIn the USA, energy brokers can serve residential, commercial and government entities that reside in energy deregulated states. In the UK, and some countries in Europe, the entire market is deregulated.\n\nEnergy brokers typically do not charge up front fees to provide rates. If an entity purchases energy through a broker, the broker's fee is usually included in the rate the customer pays. Some brokers will charge a fixed fee for their consulting services.\n\nNot all energy brokers are consultants; However, the energy brokers who are also consultants will perform a more detailed analysis of a consumers' usage pattern in order to provide a custom rate, which typically results in more cost savings for the consumer. Typically, they do not need any more information than that of an energy broker, because they can pull usage information from the local utility company. There are some national energy brokers that use auditing teams to verify their client's invoices.\n"}
{"id": "1610231", "url": "https://en.wikipedia.org/wiki?curid=1610231", "title": "Energy density", "text": "Energy density\n\nEnergy density is the amount of energy stored in a given system or region of space per unit volume. Colloquially it may also be used for energy per unit mass, though the accurate term for this is specific energy. Often only the \"useful\" or extractable energy is measured, which is to say that inaccessible energy (such as rest mass energy) is ignored. In cosmological and other general relativistic contexts, however, the energy densities considered are those that correspond to the elements of the stress–energy tensor and therefore do include mass energy as well as energy densities associated with the pressures described in the next paragraph.\n\nEnergy per unit volume has the same physical units as pressure, and in many circumstances is a synonym: for example, the energy density of a magnetic field may be expressed as (and behaves as) a physical pressure, and the energy required to compress a compressed gas a little more may be determined by multiplying the difference between the gas pressure and the external pressure by the change in volume. In short, pressure is a measure of the enthalpy per unit volume of a system. A pressure gradient has the potential to perform work on the surroundings by converting enthalpy to work until equilibrium is reached.\n\nThere are many different types of energy stored in materials, and it takes a particular type of reaction to release each type of energy. In order of the typical magnitude of the energy released, these types of reactions are: nuclear, chemical, electrochemical, and electrical.\n\nNuclear reactions are used by stars and nuclear power plants, both of which derive energy from the binding energy of nuclei. Chemical reactions are used by animals to derive energy from food, and by automobiles to derive energy from gasoline. Liquid hydrocarbons (fuels such as gasoline, diesel and kerozene) are today the most dense way known to economically store and transport chemical energy at a very large scale (1 kg of diesel fuel burns with the oxygen contained in ~15 kg of air). Electrochemical reactions are used by most mobile devices such as laptop computers and mobile phones to release the energy from batteries.\n\nThe following is a list of the thermal energy densities (that is to say: the amount of heat energy that can be extracted) of commonly used or well-known energy storage materials; it doesn't include uncommon or experimental materials. Note that this list does not consider the mass of reactants commonly available such as the oxygen required for combustion or the energy efficiency in use. An extended version of this table is found at Energy density#Extended Reference Table. Major reference = .\n\nThe following unit conversions may be helpful when considering the data in the table: 3.6 MJ = 1 kWh ≈ 1.34 HPh.\n\nIn energy storage applications the energy density relates the mass of an energy store to the volume of the storage facility, e.g. the fuel tank. The higher the energy density of the fuel, the more energy may be stored or transported for the same amount of volume. The energy density of a fuel per unit mass is called the specific energy of that fuel. In general an engine using that fuel will generate less kinetic energy due to inefficiencies and thermodynamic considerations—hence the specific fuel consumption of an engine will always be greater than its rate of production of the kinetic energy of motion.\n\nThe greatest energy source by far is mass itself. This energy, \"E = mc\", where \"m = ρV\", \"ρ\" is the mass per unit volume, \"V\" is the volume of the mass itself and \"c\" is the speed of light. This energy, however, can be released only by the processes of nuclear fission (0.1%), nuclear fusion (1%), or the annihilation of some or all of the matter in the volume \"V\" by matter-antimatter collisions (100%). Nuclear reactions cannot be realized by chemical reactions such as combustion. Although greater matter densities can be achieved, the density of a neutron star would approximate the most dense system capable of matter-antimatter annihilation possible. A black hole, although denser than a neutron star, does not have an equivalent anti-particle form, but would offer the same 100% conversion rate of mass to energy in the form of Hawking radiation. In the case of relatively small black holes (smaller than astronomical objects) the power output would be tremendous.\n\nThe highest density sources of energy aside from antimatter are fusion and fission. Fusion includes energy from the sun which will be available for billions of years (in the form of sunlight) but so far (2018), sustained fusion power production continues to be elusive. \n\nPower from fission of uranium and thorium in nuclear power plants will be available for many decades or even centuries because of the plentiful supply of the elements on earth, though the full potential of this source can only be realised through breeder reactors, which are, apart from the BN-600 reactor, not yet used commercially. Coal, gas, and petroleum are the current primary energy sources in the U.S. but have a much lower energy density. Burning local biomass fuels supplies household energy needs (cooking fires, oil lamps, etc.) worldwide. \n\nThe density of thermal energy contained in the core of a light water reactor (PWR or BWR) of typically 1 GWe (1 000 MW electrical corresponding to ~3 000 MW thermal) is in the range of 10 to 100 MW of thermal energy per cubic meter of cooling water depending on the location considered in the system (the core itself (~30 m), the reactor pressure vessel (~50 m), or the whole primary circuit (~300 m)). This represents a considerable density of energy which requires under all circumstances a continuous water flow at high velocity in order to be able to remove the heat from the core, even after an emergency shutdown of the reactor. The incapacity to cool the cores of three boiling water reactors (BWR) at Fukushima in 2011 after the tsunami and the resulting loss of the external electrical power and of the cold source was the cause of the meltdown of the three cores in only a few hours. Meanwhile, the three reactors were correctly shut down just after the Tōhoku earthquake. This extremely high power density distinguishes nuclear power plants (NPP's) from any thermal power plants (burning coal, fuel or gas) or any chemical plants and explains the large redundancy required to permanently control the neutron reactivity and to remove the residual heat from the core of NPP's.\n\nEnergy density differs from energy conversion efficiency (net output per input) or embodied energy (the energy output costs to provide, as harvesting, refining, distributing, and dealing with pollution all use energy). Large scale, intensive energy use impacts and is impacted by climate, waste storage, and environmental consequences.\n\nNo single energy storage method boasts the best in specific power, specific energy, and energy density. Peukert's Law describes how the amount of useful energy that can be obtained (for a lead-acid cell) depends on how quickly we pull it out. To maximize both specific energy and energy density, one can compute the specific energy density of a substance by multiplying the two values together, where the higher the number, the better the substance is at storing energy efficiently.\n\nAlternative options are discussed for energy storage to increase energy density and decrease charging time.\n\nGravimetric and volumetric energy density of some fuels and storage technologies (modified from the Gasoline article):\n\nThis table lists energy densities of systems that require external components, such as oxidisers or a heat sink or source. These figures do not take into account the mass and volume of the required components as they are assumed to be freely available and present in the atmosphere. Such systems cannot be compared with self-contained systems. These values may not be computed at the same reference conditions.\n\nDivide joule/m by 10 to get MJ/L. Divide MJ/L by 3.6 to get kWh/L.\n\nElectric and magnetic fields store energy. In a vacuum, the (volumetric) energy density is given by\n\nwhere E is the electric field and B is the magnetic field. The solution will be (in SI units) in Joules per cubic metre. In the context of magnetohydrodynamics, the physics of conductive fluids, the magnetic energy density behaves like an additional pressure that adds to the gas pressure of a plasma.\n\nIn normal (linear and nondispersive) substances, the energy density (in SI units) is\n\nwhere D is the electric displacement field and H is the magnetizing field.\n\nIn the case of absence of magnetic fields, by exploting Fröhlich's relationships it is also possible to extend these equations to anisotropy and nonlinearity dielectrics, as well as to calculate the correlated Helmholtz free energy and entropy densities.\n\n\n\n"}
{"id": "28672294", "url": "https://en.wikipedia.org/wiki?curid=28672294", "title": "Energy management", "text": "Energy management\n\nEnergy management includes planning and operation of energy production and energy consumption units. Objectives are resource conservation, climate protection and cost savings, while the users have permanent access to the energy they need. It is connected closely to environmental management, production management, logistics and other established business functions. The VDI-Guideline 4602 released a definition which includes the economic dimension: “Energy management is the proactive, organized and systematic coordination of procurement, conversion, distribution and use of energy to meet the requirements, taking into account environmental and economic objectives”.\n\nOne of initial steps for an effective energy cost control program is the base line energy assessment, which examines the pattern of existing energy usage by the government or any sub-entity of the government or private organization. This program will set the reference point for improvements in energy efficiency. Energy efficiency can improve the existing energy usage and benchmarking of every individual section such as area, sub-area and the industry etc .\n\nIt is important to integrate the energy management in the organizational structure, so that the energy management can be implemented. Responsibilities and the interaction of the decision makers should be regularized. The delegation of functions and competencies extend from the top management to the executive worker. Furthermore, a comprehensive coordination can ensure the fulfillment of the tasks.\n\nIt is advisable to establish a separate organizational unit “energy management” in large or energy-intensive companies. This unit supports the senior management and keeps track. It depends on the basic form of the organizational structure, where this unit is connected. In case of a functional organization the unit is located directly between the first (CEO) and the second hierarchical level (corporate functions such as production, procurement, marketing). In a divisional organization, there should be a central and several sector-specific energy management units. So the diverse needs of the individual sectors and the coordination between the branches and the head office can be fulfilled. In a matrix organization the energy management can be included as a matrix function and thus approach most functions directly.\n\nFacility management is an important part of energy management, because a huge proportion (average 25 per cent) of complete operating costs are energy costs. According to the International Facility Management Association (IFMA), facility management is \"a profession that encompasses multiple disciplines to ensure functionality of the built environment by integrating people, place, processes and technology.\"\n\nThe central task of energy management is to reduce costs for the provision of energy in buildings and facilities without compromising work processes. Especially the availability and service life of the equipment and the ease of use should remain the same. The German Facility Management Association (GEFMA e.V.) has published guidelines (e.g. GEFMA 124-1 and 124-2), which contain methods and ways of dealing with the integration of energy management in the context of a successful facility management. In this topic the facility manager has to deal with economic, ecological, risk-based and quality-based targets. He tries to minimize the total cost of the energy-related processes (supply, distribution and use).\n\nThe most important key figure in this context is kilowatt-hours per square meter per year (kWh/m²a). Based on this key figure properties can be classified according to their energy consumption.\n\n\nIn comparison, the Passive house (Passivhaus in German) ultra-low-energy standard, currently undergoing adoption in some other European countries, has a maximum space heating requirement of 15 kWh/m²a. A Passive House is a very well-insulated and virtually air-tight building. It does not require a conventional heating system. It is heated by solar gain and internal gains from people. Energy losses are minimized.\n\nThere are also buildings that produce more energy (for example by solar water heating or photovoltaic systems) over the course of a year than it imports from external sources. These buildings are called energy-plus-houses.\n\nIn addition, the work regulations manage competencies, roles and responsibilities. Because the systems also include risk factors (e.g., oil tanks, gas lines), you must ensure that all tasks are clearly described and distributed. A clear regulation can help to avoid liability risks.\n\nLogistics is the management of the flow of resources between the point of origin and the point of destination in order to meet some requirements, for example of customers or corporations. Especially the core logistics task, transportation of the goods, can save costs and protect the environment through efficient energy management. The relevant factors are the choice of means of transportation, duration and length of transportation and cooperation with logistics service providers.\n\nThe logistics causes more than 14% percent of CO2 emissions worldwide. For this reason the term Green Logistics is becoming increasingly important.\n\nPossible courses of action in terms of green logistics are:\n\n\nBesides transportation of goods, the transport of persons should be an important part of the logistic strategy of organizations. In case of business trips it is important to attract attention to the choice and the proportionality of the means of transport. It should be balanced whether a physical presence is mandatory or a telephone or video conference is just as useful. Home Office is another possibility in which the company can protect the environment indirectly.\n\nProcurement is the acquisition of goods or services. Energy prices fluctuate constantly, which can significantly affect the energy bill of organizations. Therefore poor energy procurement decisions can be expensive. Organizations can control and reduce energy costs by taking a proactive and efficient approach to buying energy. Even a change of the energy source can be a profitable and eco-friendly alternative.\n\nProduction is the act of creating output, a good or service which has value and contributes to the utility of individuals. This central process may differ depending on the industry. Industrial companies have facilities that require a lot of energy. Service companies, in turn, do not need many materials, their energy-related focus is mainly facility management or Green IT. Therefore the energy-related focus has to be identified first, then evaluated and optimize.\n\nUsually, production is the area with the largest energy consumption within an organization. Therefore also the production planning and control becomes very important. It deals with the operational, temporal, quantitative and spatial planning, control and management of all processes that are necessary in the production of goods and commodities. The \"production planner\" should plan the production processes so that they operate in an energy efficient way. For example, strong power consumer can be moved into the night time. Peaks should be avoided for the benefit of a unified load profile.\n\nThe impending changes in the structure of energy production require an increasing demand for storage capacity. The Production planning and control has to deal with the problem of limited storability of energy. In principle there is the possibility to store energy electrically, mechanically or chemically. Another trend-setting technology is lithium-based electrochemical storage, which can be used in electric vehicles or as an option to control the power grid. The German Federal Ministry of Economics and Technology realized the significance of this topic and established an initiative with the aim to promote technological breakthroughs and support the rapid introduction of new energy storage.\n\nMaintenance is the combination of all technical and administrative actions, including supervision actions, intended to retain an item in, or restore it to, a state in which it can perform a required function. Detailed maintenance is essential to support the energy management. Hereby power losses and cost increases can be avoided.\n\nThrough the energy efficiency it management is remain the key for the any industrial user across globe , to achieve the energy management goal for the federal government or industry the efficiency of water and energy resources play a vital role\n\nExamples of how it is possible to save energy and costs with the help of maintenance:\n\n\nA long-term energy strategy should be part of the overall strategy of a company. This strategy may include the objective of increasing the use of renewable energies. Furthermore, criteria for decisions on energy investments, such as yield expectations, are determined. By formulating an energy strategy companies have the opportunity to avoid risks and to assure a competitive advance against their business rivals.\n\nAccording to Kals there are the following energy strategies:\n\n\nIn reality, you usually find hybrid forms of different strategies.\n\nMany companies are trying to promote its image and time protect the climate through a proactive and public energy strategy. General Motors (GM) strategy is based on continuous improvement. Furthermore they have six principles: e.g. restoring and preserving the environment, reducing waste and pollutants, educating the public about environmental conservation, collaboration for the development of environmental laws and regulations.\n\nNokia created its first climate strategy in 2006. The strategy tries to evaluate the energy consumption and greenhouse gas emissions of products and operations and sets reduction targets accordingly. Furthermore, their environmental efforts is based on four key issues: substance management, energy efficiency, recycling, promoting environmental sustainability.\n\nThe energy strategy of Volkswagen (VW) is based on environmentally friendly products and a resource-efficient production according to the \"Group Strategy 2018\". Almost all locations of the Group are certified to the international standard ISO 14001 for environmental management systems.\n\nWhen looking at the energy strategies of companies it is important to you have the topic greenwashing in mind. This is a form of propaganda in which green strategies are used to promote the opinion that an organization's aims are environmentally friendly.\n\nEven many countries formulate energy strategies. The Swiss Federal Council decided in May 2011 to resign nuclear energy medium-dated. The nuclear power plants will be shut down at the end of life and will not be replaced. In Compensation they put the focus on energy efficiency, renewable energies, fossil energy sources and the development of water power.\n\nThe European Union has clear instructions for its members. The \"20-20-20-targets\" include, that the Member States have to reduce greenhouse gas emissions by 20% below 1990 levels, increase energy efficiency by 20% and achieve a 20% share of renewable energy in total energy consumption by 2020.\n\nThe basis of every energy strategy is the corporate culture and the related ethical standards applying in the company. Ethics, in the sense of business ethics, examines ethical principles and moral or ethical issues that arise in a business environment. Ethical standards can appear in company guidelines, energy and environmental policies or other documents.\n\nThe most relevant ethical ideas for the energy management are:\n\n\n\nManagement of energy in a particular context:\n"}
{"id": "26356935", "url": "https://en.wikipedia.org/wiki?curid=26356935", "title": "Energy operator", "text": "Energy operator\n\nIn quantum mechanics, energy is defined in terms of the energy operator, acting on the wave function of the system as a consequence of time translation symmetry.\n\nIt is given by:\n\nIt acts on the wave function (the probability amplitude for different configurations of the system)\n\nThe energy operator corresponds to the full energy of a system. The Schrödinger equation describes the space- and time-dependence of the slow changing (non-relativistic) wave function of a quantum system. The solution of this equation for a bound system is discrete (a set of permitted states, each characterized by an energy level) which results in the concept of quanta.\n\nUsing the energy operator to the Schrödinger equation:\n\ncan be obtained:\n\nwhere \"i\" is the imaginary unit, \"ħ\" is the reduced Planck constant, and formula_5 is the Hamiltonian operator.\n\nIn a stationary state additionally occurs the time-independent Schrödinger equation:\nwhere \"E\" is an eigenvalue of energy.\n\nThe relativistic mass-energy relation:\n\nwhere again \"E\" = total energy, \"p\" = total 3-momentum of the particle, \"m\" = invariant mass, and \"c\" = speed of light, can similarly yield the Klein–Gordon equation:\n\nthat is:\n\nThe energy operator is easily derived from using the free particle wave function (plane wave solution to Schrödinger's equation). Starting in one dimension the wave function is\n\nThe time derivative of \"Ψ\" is\n\nBy the De Broglie relation:\n\nwe have\n\nRe-arranging the equation leads to\n\nwhere the energy factor \"E\" is a scalar value, the energy the particle has and the value that is measured. The partial derivative is a linear operator so this expression \"is\" the operator for energy:\n\nIt can be concluded that the scalar \"E\" is the eigenvalue of the operator, while formula_16 is the operator. Summarizing these results:\n\nFor a 3-d plane wave\n\nthe derivation is exactly identical, as no change is made to the term including time and therefore the time derivative. Since the operator is linear, they are valid for any linear combination of plane waves, and so they can act on any wave function without affecting the properties of the wave function or operators. Hence this must be true for any wave function. It turns out to work even in relativistic quantum mechanics, such as the Klein–Gordon equation above.\n\n"}
{"id": "43699304", "url": "https://en.wikipedia.org/wiki?curid=43699304", "title": "Energy rate density", "text": "Energy rate density\n\nEnergy rate density is the amount of free energy per unit time per unit mass (in CGS metric units erg/s/g; in MKS units joule/s/kg). It is terminologically (but not always numerically) equivalent to power density when measured in SI units of W/kg. Regardless of the units used, energy rate density describes the flow of energy through any system of given mass, and has been proposed as a measure of system complexity. The more complex the system, the more energy flows per second through each gram. \n\nEnergy rate density is actually a general term that is equivalent to more specialized terms used by many different disciplinary scientists. For example, in astronomy it is called the luminosity-to-mass ratio (the inverse of the mass-luminosity ratio), in physics the power density, in geology the specific radiant flux (where “specific” denotes per unit mass), in biology the specific metabolic rate, and in engineering the power-to-weight ratio. Interdisciplinary researchers prefer to use the general term, energy rate density, not only to stress the intuitive notion of energy flow (in contrast to more colloquial connotations of the word \"power\"), but also to unify its potential application among all the natural sciences, as in the cosmology of cosmic evolution. When the energy rate density for systems including our galaxy, sun, earth, plants, animals, society are plotted according to when, in historical time, they first emerged, a clear increase in energy rate density over time is observed. \n\nThis term has in recent years gained many diverse applications in various disciplines, including history, cosmology, economics, philosophy, and behavioral biology. \n\n\n"}
{"id": "32974036", "url": "https://en.wikipedia.org/wiki?curid=32974036", "title": "Fowkes hypothesis", "text": "Fowkes hypothesis\n\nThe Fowkes hypothesis (after F. M. Fowkes) is a first order approximation for surface energy. It states the surface energy is the sum of each component's forces:\nγ=γ+γ+γ+...\nwhere γ is the dispersion component, γ is the polar, γ is the dipole and so on.\n\nThe Fowkes hypothesis goes further making the approximation that the interface between an apolar liquid and apolar solid where there are only dispersive interactions acting across the interface can be estimated using the geometric mean of the contributions from each surface i.e.\n\nγ=γ+γ-2(γ x γ)\n\n\n"}
{"id": "6732384", "url": "https://en.wikipedia.org/wiki?curid=6732384", "title": "Graphical timeline of the Stelliferous Era", "text": "Graphical timeline of the Stelliferous Era\n\nThis is the timeline of the stelliferous era but also partly charts the primordial era, and charts more of the degenerate era of the heat death scenario.\n\nThe scale is formula_1. Example one million years is formula_2.\n\n"}
{"id": "37493759", "url": "https://en.wikipedia.org/wiki?curid=37493759", "title": "Infrasonic passive differential spectroscopy", "text": "Infrasonic passive differential spectroscopy\n\nInfrasonic Passive Seismic Spectroscopy (IPSS) is a Passive Seismic Low Frequency technique used for mapping potential oil and gas hydrocarbon accumulations.\n\nIt is part of the geophysical techniques also known under the generic naming passive seismic which includes also the Passive Seismic Tomography and Micro Seismic Monitoring for petroleum, gas and geothermal applications. In a larger scale, Passive Seismic includes the Global Seismic Network earthquakes monitoring (GSN).\n\nRegarding petroleum and geothermal exploration (within a small scale), effect of fluid distribution on P- wave propagation in partially saturated rocks is the main responsible for the low frequency reservoir-related wavefield absorption.\nThe high level of attenuation, within the infrasonic bandwidth (below 10 Hz) of the seismic field observed in natural oil-saturated porous media during the last years (successfully explained by mesoscopic homogeneous models) is the main responsible of the passive seismic wave field shifting within a low frequency range.\nPressure differences between regions with different fluid/solid properties induce frequency-dependency of the attenuation (Qp and Qs reservoir factors) and velocity dispersion (Vp, Vs) of the low frequency wave field.\n\nInfrasonic Passive Seismic Spectroscopy techniques quantifies then the absorption and the wave field dispersion within the low frequency bandwidth giving the most predominant areas linked with possible oil-saturated and porous media.\n\nThe low frequency seismic field is not usually reachable by the active seismic surveys being either the explosive waves mainly in the high frequency and the vibroseis currently built not to reach such a low frequencies.\n\nSummary of the theoretical background of the passive seismic.\n\nQuintal B.. Journal of Applied Geophysics 82, pp. 119–128, 2012.\nLambert M.-A., Saenger E.H., Quintal B., Schmalholz S.M.. Geophysics 78, pp. T41-T52, 2013.\n\nArtman, B., I. Podladtchikov, and B. Witten, 2010, Source location using time-reverse imaging. Geophysical Prospecting, 58, 861–873.\n\nBiot M. A. 1956a. Journal of the Acoustical Society of America, 28, 168–178.\n\nBiot M.A. 1956b. Journal of the Acoustical Society ofAmerica, 28, 179–191.\n\nBiot M.A. 1962. Journal of Applied Physics 33, 1482–1498.\n\nCarcione, J. M., H. B. Helle, and N. H. Pham (2003),: Comparison with poroelastic numerical experiments. Geophysics, 68, 1389– 1398.\n\nDutta, N. C., and H. Ode, 1979a,: Geophysics, 44, 1777–1788.\n\nPride S.R. and Berryman J.G. 2003. Physical Review E 68, 036604.\n\nRubino, J. G., C. L. Ravazzoli, and J. E. Santos, 2009,: Geophysics, 74, no. 1, N1–N13.\n\nRiahi, N., B. Birkelo, and E. H. Saenger, 2011,: 73rd Annual Conference and Exhibition, EAGE, Extended Abstracts, P198.\n\nAkrawi, K., Campagna, F., Russo, L., Yousif, M. E., Abdelhafeez, M. H.,: Abstract: 10th Middle East Geosciences Conference and Exhibition, EAGE, Article: #90141©2012 GEO-2012,\n\nArtman, B., M. Duclos, B. Birkelo, F. Huguet, J. F. Dutzer, and R. Habiger, 2011, r: 73rd Annual Conference and Exhibition, EAGE, Extended Abstracts, P331.\n\nLambert, M.-A., S. M. Schmalholz, E. H. Saenger, and B. Steiner, 2009,: Geophysical Prospecting, 57, 393–411.\n\nSteiner, B., E. H. Saenger, and S. M. Schmalholz, 2008,: Application to hydrocarbon reservoir localization: Geophysical Research Letters, 35, L03307.\n\nToms, J., 2008. Effect of Fluid Distribution on Compressional Wave Propagation in Partially Saturated Rocks. PhD Thesis.\n\nWhite J.E., Mikhaylova N.G. and Lyakhovitskiy F.M. 1976. Izvestija Academy of Sciences USSR, Physics Solid Earth 11, 654–659.\n"}
{"id": "13680406", "url": "https://en.wikipedia.org/wiki?curid=13680406", "title": "Ion vibration current", "text": "Ion vibration current\n\nThe ion vibration current (IVI) and the associated ion vibration potential is an electric signal that arises when an acoustic wave propagates through a homogeneous fluid.\n\nHistorically, the IVI was the first known electroacoustic phenomenon. It was predicted by Peter Debye in 1933.\n\nWhen a longitudinal sound wave travels through a solvent, the associated pressure gradients push the fluid particles back and forth, and it is easy in practice to create such accelerations that measure thousands or millions of g's. If a solute molecule is more dense or less dense than the surrounding liquid, then in this accelerating environment, the molecule will move relative to the surrounding liquid. This relative motion is essentially the same phenomenon that occurs in a centrifuge, or more simply, it is essentially the same phenomenon that occurs when low-density objects float to the top of a glass of water, and high-density particles sink to the bottom (see the equivalence principle, which states that gravity is just like any other acceleration). The amount of relative motion depends on the balance between the molecule's effective mass (which includes both the mass of the molecule itself and any solvent molecules that are so tightly bound to the molecule that they follow along with the molecule's motion), its effective volume (related to buoyant force), and the viscous drag (friction) between the molecule and the surrounding fluid.\n\nIVI concerns the case where the particles in question are anions and cations. In general, they will have different amounts of motion relative to the fluid during the sound wave oscillations, and that discrepancy creates an alternating electric potential between various points in a sound wave.\n\nThis effect was extensively used in the 1950s and 1960s for characterizing ion solvation. These works are mostly associated with the names of Zana and Yaeger, who published a review of their studies in 1982.\n"}
{"id": "985963", "url": "https://en.wikipedia.org/wiki?curid=985963", "title": "Lambda-CDM model", "text": "Lambda-CDM model\n\nThe ΛCDM (Lambda cold dark matter) or Lambda-CDM model is a parametrization of the Big Bang cosmological model in which the universe contains a cosmological constant, denoted by Lambda (Greek Λ), associated with dark energy, and cold dark matter (abbreviated CDM). It is frequently referred to as the standard model of Big Bang cosmology because it is the simplest model that provides a reasonably good account of the following properties of the cosmos:\n\nThe model assumes that general relativity is the correct theory of gravity on cosmological scales.\nIt emerged in the late 1990s as a concordance cosmology, after a period of time when disparate observed properties of the universe appeared mutually inconsistent, and there was no consensus on the makeup of the energy density of the universe.\n\nThe ΛCDM model can be extended by adding cosmological inflation, quintessence and other elements that are current areas of speculation and research in cosmology.\n\nSome alternative models challenge the assumptions of the ΛCDM model. Examples of these are modified Newtonian dynamics, modified gravity, theories of large-scale variations in the matter density of the universe, and scale invariance of empty space.\n\nMost modern cosmological models are based on the cosmological principle, which states that our observational location in the universe is not unusual or special; on a large-enough scale, the universe looks the same in all directions (isotropy) and from every location (homogeneity).\n\nThe model includes an expansion of metric space that is well documented both as the red shift of prominent spectral absorption or emission lines in the light from distant galaxies and as the time dilation in the light decay of supernova luminosity curves. Both effects are attributed to a Doppler shift in electromagnetic radiation as it travels across expanding space. Although this expansion increases the distance between objects that are not under shared gravitational influence, it does not increase the size of the objects (e.g. galaxies) in space. It also allows for distant galaxies to recede from each other at speeds greater than the speed of light; local expansion is less than the speed of light, but expansion summed across great distances can collectively exceed the speed of light.\n\nThe letter formula_1 (lambda) represents the cosmological constant, which is currently associated with a vacuum energy or dark energy in empty space that is used to explain the contemporary accelerating expansion of space against the attractive effects of gravity. A cosmological constant has negative pressure, formula_2, which contributes to the stress-energy tensor that, according to the general theory of relativity, causes accelerating expansion. The fraction of the total energy density of our (flat or almost flat) universe that is dark energy, formula_3, is currently (2015) estimated to be 0.692 ± 0.012, or even 0.6911 ± 0.0062 based on Planck satellite data.\n\nDark matter is postulated in order to account for gravitational effects observed in very large-scale structures (the \"flat\" rotation curves of galaxies; the gravitational lensing of light by galaxy clusters; and enhanced clustering of galaxies) that cannot be accounted for by the quantity of observed matter. Cold dark matter is \"non-baryonic\", i.e. it consists of matter other than protons and neutrons (and electrons, by convention, although electrons are not baryons); \"cold\", i.e. its velocity is far less than the speed of light at the epoch of radiation-matter equality (thus neutrinos are excluded, being non-baryonic but not cold); \"dissipationless\", i.e. it cannot cool by radiating photons; and \"collisionless\", i.e. the dark matter particles interact with each other and other particles only through gravity and possibly the weak force. The dark matter component is currently (2013) estimated to constitute about 26.8% of the mass-energy density of the universe.\n\nThe remaining 4.9% (2013) comprises all ordinary matter observed as atoms, chemical elements, gas and plasma, the stuff of which visible planets, stars and galaxies are made. The great majority of ordinary matter in the universe is unseen, since visible stars and gas inside galaxies and clusters account for less than 10% of the ordinary matter contribution to the mass-energy density of the universe.\n\nAlso, the energy density includes a very small fraction (~ 0.01%) in cosmic microwave background radiation, and not more than 0.5% in relic neutrinos. Although very small today, these were much more important in the distant past, dominating the matter at redshift > 3200.\n\nThe model includes a single originating event, the \"Big Bang\", which was not an explosion but the abrupt appearance of expanding space-time containing radiation at temperatures of around 10 K. This was immediately (within 10 seconds) followed by an exponential expansion of space by a scale multiplier of 10 or more, known as cosmic inflation. The early universe remained hot (above 10,000 K) for several hundred thousand years, a state that is detectable as a residual cosmic microwave background, or CMB, a very low energy radiation emanating from all parts of the sky. The \"Big Bang\" scenario, with cosmic inflation and standard particle physics, is the only current cosmological model consistent with the observed continuing expansion of space, the observed distribution of lighter elements in the universe (hydrogen, helium, and lithium), and the spatial texture of minute irregularities (anisotropies) in the CMB radiation. Cosmic inflation also addresses the \"horizon problem\" in the CMB; indeed, it seems likely that the universe is larger than the observable particle horizon.\n\nThe model uses the Friedmann–Lemaître–Robertson–Walker metric, the Friedmann equations and the cosmological equations of state to describe the observable universe from right after the inflationary epoch to present and future.\n\nThe expansion of the universe is parametrized by a dimensionless scale factor formula_4 (with time formula_5 counted from the birth of the universe), defined relative to the present day, so formula_6; the usual convention in cosmology is that subscript 0 denotes present-day values, so formula_7 is the current age of the universe. The scale factor is related to the observed redshift formula_8 of the light emitted at time formula_9 by\n\nThe expansion rate is described by the time-dependent Hubble parameter, formula_11, defined as\nwhere formula_13 is the time-derivative of the scale factor. The first Friedmann equation gives the expansion rate in terms of the matter+radiation density the curvature and the cosmological constant \n\nwhere as usual is the speed of light and is the gravitational constant. \nA critical density formula_15 is the present-day density, which gives zero curvature formula_16, assuming the cosmological constant formula_1 is zero, regardless of its actual value. Substituting these conditions to the Friedmann equation gives\n\nwhere formula_19 is the reduced Hubble constant.\nIf the cosmological constant were actually zero, the critical density would also mark the dividing line between eventual recollapse of the universe to a Big Crunch, or unlimited expansion. For the Lambda-CDM model with a positive cosmological constant (as observed), the universe is predicted to expand forever regardless of whether the total density is slightly above or below the critical density; though other outcomes are possible in extended models where the dark energy is not constant but actually time-dependent.\n\nIt is standard to define the present-day density parameter formula_20 for various species as the dimensionless ratio\nwhere the subscript formula_22 is one of formula_23 for baryons, formula_24 for cold dark matter, formula_25 for radiation (photons plus relativistic neutrinos), and formula_26 or formula_1 for dark energy.\n\nSince the densities of various species scale as different powers of formula_28, e.g. formula_29 for matter etc.,\nthe Friedmann equation can be conveniently rewritten in terms of the various density parameters as\nwhere w is the equation of state of dark energy, and assuming negligible neutrino mass (significant neutrino mass requires a more complex equation). The various formula_31 parameters add up to formula_32 by construction.\nIn the general case this is integrated by computer to give\nthe expansion history formula_33 and also observable distance-redshift relations for any chosen values of the cosmological parameters, which can then be compared with observations such as supernovae and baryon acoustic oscillations.\n\nIn the minimal 6-parameter Lambda-CDM model, it is assumed that curvature formula_34 is zero and formula_35, so this simplifies to\n\nObservations show that the radiation density is very small today, formula_37; if this term is neglected\nthe above has an analytic solution\nwhere formula_39\nthis is fairly accurate for formula_40 or formula_41million years.\nSolving for formula_42 gives the present age of the universe formula_43 in terms of the other parameters.\n\nIt follows that the transition from decelerating to accelerating expansion (the second derivative formula_44 crossing zero) occurred when\n\nwhich evaluates to formula_46 or formula_47 for the best-fit parameters estimated from the Planck spacecraft.\n\nThe discovery of the Cosmic Microwave Background (CMB) in 1964 confirmed a key prediction of the Big Bang cosmology. From that point on, it was generally accepted that the universe started in a hot, dense state and has been expanding over time. The rate of expansion depends on the types of matter and energy present in the universe, and in particular, whether the total density is above or below the so-called critical density. During the 1970s, most attention focused on pure-baryonic models, but there were serious challenges explaining the formation of galaxies, given the small anisotropies in the CMB (upper limits at that time). In the early 1980s, it was realized that this could be resolved if cold dark matter dominated over the baryons, and the theory of cosmic inflation motivated models with critical density. During the 1980s, most research focused on cold dark matter with critical density in matter, around 95% CDM and 5% baryons: these showed success at forming galaxies and clusters of galaxies, but problems remained; notably, the model required a Hubble constant lower than preferred by observations, and observations around 1988-1990 showed more large-scale galaxy clustering than predicted. These difficulties sharpened with the discovery of CMB anisotropy by COBE in 1992, and several modified CDM models, including ΛCDM and mixed cold and hot dark matter, came under active consideration through the mid-1990s. The ΛCDM model then became the leading model following the observations of accelerating expansion in 1998, and was quickly supported by other observations: in 2000, the BOOMERanG microwave background experiment measured the total (matter–energy) density to be close to 100% of critical, whereas in 2001 the 2dFGRS galaxy redshift survey measured the matter density to be near 25%; the large difference between these values supports a positive Λ or dark energy. Much more precise spacecraft measurements of the microwave background from WMAP in 2003 – 2010 and Planck in 2013 - 2015 have continued to support the model and pin down the parameter values, most of which are now constrained below 1 percent uncertainty.\n\nThere is currently active research into many aspects of the ΛCDM model, both to refine the parameters and possibly detect deviations. In addition, ΛCDM has no explicit physical theory for the origin or physical nature of dark matter or dark energy; the nearly scale-invariant spectrum of the CMB perturbations, and their image across the celestial sphere, are believed to result from very small thermal and acoustic irregularities at the point of recombination. A large majority of astronomers and astrophysicists support the ΛCDM model or close relatives of it, but Milgrom, McGaugh, and Kroupa are leading critics, attacking the dark matter portions of the theory from the perspective of galaxy formation models and supporting the alternative MOND theory, which requires a modification of the Einstein field equations and the Friedmann equations as seen in proposals such as MOG theory or TeVeS theory. Other proposals by theoretical astrophysicists of cosmological alternatives to Einstein's general relativity that attempt to account for dark energy or dark matter include f(R) gravity, scalar–tensor theories such as galileon theories, brane cosmologies, the DGP model, and massive gravity and its extensions such as bimetric gravity.\n\nIn addition to explaining pre-2000 observations,\nthe model has made a number of successful predictions: notably the existence of the\nbaryon acoustic oscillation feature, discovered in 2005 in the predicted location; and the statistics of weak gravitational lensing, first observed in 2000 by several teams. The polarization of the CMB, discovered in 2002 by DASI is now a dramatic success: in the 2015 Planck data release, there are seven observed peaks in the temperature (TT) power spectrum, six peaks in the temperature-polarization (TE) cross spectrum, and five peaks in the polarization (EE) spectrum. The six free parameters can be well constrained by the TT spectrum alone, and then the TE and EE spectra can be predicted theoretically to few-percent precision with no further adjustments allowed: comparison of theory and observations shows an excellent match.\n\nExtensive searches for dark matter particles have so far shown no well-agreed detection;\nthe dark energy may be almost impossible to detect in a laboratory, and its value is unnaturally small compared to naive theoretical predictions.\n\nComparison of the model with observations is very successful on large scales (larger than galaxies, up to the observable horizon), but may have some problems on sub-galaxy scales, possibly predicting too many dwarf galaxies and too much dark matter in the innermost regions of galaxies. This problem is called the \"small scale crisis\". These small scales are harder to resolve in computer simulations, so it is not yet clear whether the problem is the simulations, non-standard properties of dark matter, or a more radical error in the model.\n\nIt has been argued that the ΛCDM model is built upon a foundation of conventionalist stratagems, rendering it unfalsifiable in the sense defined by Karl Popper.\n\nThe simple ΛCDM model is based on six parameters: physical baryon density parameter; physical dark matter density parameter; the age of the universe; scalar spectral index; curvature fluctuation amplitude; and reionization optical depth. In accordance with Occam's razor, six is the smallest number of parameters needed to give an acceptable fit to current observations; other possible parameters are fixed at \"natural\" values, e.g. total density parameter = 1.00, dark energy equation of state = −1. (See below for extended models that allow these to vary.)\n\nThe values of these six parameters are mostly not predicted by current theory (though, ideally, they may be related by a future \"Theory of Everything\"), except that most versions of cosmic inflation predict the scalar spectral index should be slightly smaller than 1, consistent with the estimated value 0.96. The parameter values, and uncertainties, are estimated using large computer searches to locate the region of parameter space providing an acceptable match to cosmological observations. From these six parameters, the other model values, such as the Hubble constant and the dark energy density, can be readily calculated.\n\nCommonly, the set of observations fitted includes the cosmic microwave background anisotropy, the brightness/redshift relation for supernovae, and large-scale galaxy clustering including the baryon acoustic oscillation feature. Other observations, such as the Hubble constant, the abundance of galaxy clusters, weak gravitational lensing and globular cluster ages, are generally consistent with these, providing a check of the model, but are less precisely measured at present.\n\nParameter values listed below are from the Planck Collaboration Cosmological parameters 68% confidence limits for the base ΛCDM model from Planck CMB power spectra, in combination with lensing reconstruction and external data (BAO + JLA + H). See also Planck (spacecraft).\n\nMassimo Persic and Paolo Salucci firstly estimated the baryonic density today present in ellipticals, spirals, groups and clusters of galaxies.\nThey performed an integration of the baryonic mass-to-light ratio over luminosity (in the following formula_48), weighted with the luminosity function formula_49 over the previously mentioned classes of astrophysical objects: \n\nThe result was:\n\nwhere formula_52.\n\nNote that this value is much lower than the prediction of standard cosmic nucleosynthesis formula_53, so that stars and gas in galaxies and in galaxy groups and clusters account for less than 10% of the primordially synthesized baryons. This issue is known as the problem of the \"missing baryons\".\n\nExtended models allow one or more of the \"fixed\" parameters above to vary, in addition to the basic six; so these models join smoothly to the basic six-parameter model in the limit that the additional parameter(s) approach the default values. For example, possible extensions of the simplest ΛCDM model allow for spatial curvature (formula_54 may be different from 1); or quintessence rather than a cosmological constant where the equation of state of dark energy is allowed to differ from −1. Cosmic inflation predicts tensor fluctuations (gravitational waves). Their amplitude is parameterized by the tensor-to-scalar ratio (denoted formula_55), which is determined by the unknown energy scale of inflation. Other modifications allow hot dark matter in the form of neutrinos more massive than the minimal value, or a running spectral index; the latter is generally not favoured by simple cosmic inflation models.\n\nAllowing additional variable parameter(s) will generally \"increase\" the uncertainties in the standard six parameters quoted above, and may also shift the central values slightly. The Table below shows results for each of the possible \"6+1\" scenarios with one additional variable parameter; this indicates that, as of 2015, there is no convincing evidence that any additional parameter is different from its default value.\n\nSome researchers have suggested that there is a running spectral index, but no statistically significant study has revealed one. Theoretical expectations suggest that the tensor-to-scalar ratio formula_55 should be between 0 and 0.3, and the latest results are now within those limits.\n\n\n"}
{"id": "58439642", "url": "https://en.wikipedia.org/wiki?curid=58439642", "title": "Margham", "text": "Margham\n\nMargham is an oil and gas field in Dubai, United Arab Emirates (UAE) and the largest onshore gas field in the emirate. The field is managed by Dusup - the Dubai Supply Authority. Condensate production ran at some 25,000 barrels per day in 2010. Margham also has an oil production capability.\n\nProduction at Margham commenced in 1984, with three major gas-bearing formations located up to 10,000 feet below sea level. The field is connected by pipeline to Jebel Ali, where the gas condensate is loaded onto tankers for export. Dry gas is now also sent by pipeline to supply the Dubai grid, with consumption increasing since 2015.\n\nMargham was initially developed as a liquids stripping/gas recycling project (dry gas was pumped back into the reservoir), but now operates as a gas storage facility for Dubai since 2008, allowing Dubai to depend on gas produced from Margham for its elecricity generation and desalination needs. This usage, together with sustainables such as DEWA's Mohammed bin Rashid Al Maktoum Solar Park, means that Dubai has eliminated the use of oil as a domestic energy fuel.\n\nAlthough it is a major producer with ambitions to develop its trading activities to become a major global LNG hub, the UAE is actually a net importer of LNG.\n"}
{"id": "6748280", "url": "https://en.wikipedia.org/wiki?curid=6748280", "title": "Material", "text": "Material\n\nA material is a chemical substance or mixture of substances that constitute an object. Materials can be pure or impure, a singular composite or a complex mix, living or non-living matter, whether natural or man-made, either concrete or abstract. Materials can be classified based on different properties such as physical and chemical properties (see List of materials properties), geological, biological, choreographical, or philosophical properties. In the physical sense, materials are studied in the field of materials science.\n\nIn industry, materials are inputs to production or manufacturing processes. They may either be raw material, that is, unprocessed, or processed before being used in more advanced production processes, either by distillation or synthesis (synthetic materials).\n\nTypes of materials include:\n\nMaterials are classified according to many different criteria including their physical and chemical characteristics as well as their intended applications whether it is thermal, optical, electrical, magnetic, or combined. As their methods of usage dictate their physical appearance, they can be designed, tailored, and/or prepared in many forms such as powders, thin or thick films, and plates and could be introduced/studied in a single or multi layers. End products could be pure materials or doped ones with most useful compounds are those with controlled added impurities.The dopants could be added chemically or mixed and implanted physically. In case the impurities were added chemically, the dopants/co-dopants on substitutional/interstitial sites should be optimized and investigated thoroughly as well as any stresses instigated by their presence within the structure; whereas in the case of the physical mixing, the influence of the degree of heterogeneity of the prepared hybrid composites ought to be studied.The different physical and chemical preparation techniques can be used solely or combined including solid state synthesis, hydrothermal, sol-gel, precipitations and coprecipitations, spin coating, physical vapor deposition, and spray pyrolysis. Types of impurities along with their amounts are usually dictated by types of matrices to be added to, and their ability to maximize the desired products’ usefulness. Among the most commonly used characterization techniques are X-ray diffraction (XRD) either single crystal or powder, scanning electron microscopy (SEM), energy dispersive X-ray spectroscopy (EDS), X-ray fluorescence (XRF), differential scanning calorimetry (DSC), UV-Vis absorption Spectroscopy, Fourier transform infra-red (FTIR), and Photoluminescence spectrometry. In addition, it is usually considered of extreme importance to find theoretical models that can confirm and/or predict the experimental findings and assist in discussion, assignment, and the explanation of results and outcomes. Also, vision and room for future modification and development should always be pinpointed. Hence, one can classify the material as a smart one if its presence can serve multi purposes within the final product.\n\n"}
{"id": "19673093", "url": "https://en.wikipedia.org/wiki?curid=19673093", "title": "Matter", "text": "Matter\n\nIn the classical physics and in the chemistry of things of everyday life, matter is any substance that has mass and takes up space by having volume. All everyday objects that we can touch are ultimately composed of atoms, which are made up of interacting subatomic particles, and in everyday as well as scientific usage, \"matter\" generally includes atoms and anything made up of them, and any particles (or combination of particles) that act as if they have both rest mass and volume. However it does not include massless particles such as photons, or other energy phenomena or waves such as light or sound. Matter exists in various \"states\" (also known as \"phases\"). These include classical everyday phases such as solid, liquid, and gas – for example water exists as ice, liquid water, and gaseous steam – but other states are possible, including plasma, Bose–Einstein condensates, fermionic condensates, and quark–gluon plasma.\n\nUsually atoms can be imagined as a nucleus of protons and neutrons, and a surrounding \"cloud\" of orbiting electrons which \"take up space\". However this is only somewhat correct, because subatomic particles and their properties are governed by their quantum nature, which means they do not act as everyday objects appear to act – they can act like waves as well as particles and they do not have well-defined sizes or positions. In the Standard Model of particle physics, matter is not a fundamental concept because the elementary constituents of atoms are quantum entities which do not have an inherent \"size\" or \"volume\" in any everyday sense of the word. Due to the exclusion principle and other fundamental interactions, some \"point particles\" known as fermions (quarks, leptons), and many composites and atoms, are effectively forced to keep a distance from other particles under everyday conditions; this creates the property of matter which appears to us as matter taking up space.\n\nFor much of the history of the natural sciences people have contemplated the exact nature of matter. The idea that matter was built of discrete building blocks, the so-called \"particulate theory of matter\", was first put forward by the Greek philosophers Leucippus (~490 BC) and Democritus (~470–380 BC).\n\nMatter should not be confused with mass, as the two are not the same in modern physics. Matter is a general term describing any 'physical substance'. By contrast, mass is not a substance but rather a quantitative \"property\" of matter and other substances or systems; various types of mass are defined within physics – including but not limited to rest mass, inertial mass, relativistic mass, mass–energy. \n\nWhile there are different views on what should be considered matter, the mass of a substance has exact scientific definitions. Another difference is that matter has an \"opposite\" called antimatter, but mass has no opposite—there is no such thing as \"anti-mass\" or negative mass, so far as is known, although scientists do discuss the concept. Antimatter has the same (i.e. positive) mass property as its normal matter counterpart.\n\nDifferent fields of science use the term matter in different, and sometimes incompatible, ways. Some of these ways are based on loose historical meanings, from a time when there was no reason to distinguish mass from simply a quantity of matter. As such, there is no single universally agreed scientific meaning of the word \"matter\". Scientifically, the term \"mass\" is well-defined, but \"matter\" can be defined in several ways. Sometimes in the field of physics \"matter\" is simply equated with particles that exhibit rest mass (i.e., that cannot travel at the speed of light), such as quarks and leptons. However, in both physics and chemistry, matter exhibits both wave-like and particle-like properties, the so-called wave–particle duality.\n\nA definition of \"matter\" based on its physical and chemical structure is: \"matter is made up of atoms\". Such \"atomic matter\" is also sometimes termed \"ordinary matter\". As an example, deoxyribonucleic acid molecules (DNA) are matter under this definition because they are made of atoms. This definition can be extended to include charged atoms and molecules, so as to include plasmas (gases of ions) and electrolytes (ionic solutions), which are not obviously included in the atoms definition. Alternatively, one can adopt the \"protons, neutrons, and electrons\" definition.\n\nA definition of \"matter\" more fine-scale than the atoms and molecules definition is: \"matter is made up of what atoms and molecules are made of\", meaning anything made of positively charged protons, neutral neutrons, and negatively charged electrons. This definition goes beyond atoms and molecules, however, to include substances made from these building blocks that are \"not\" simply atoms or molecules, for example electron beams in an old cathode ray tube television, or white dwarf matter—typically, carbon and oxygen nuclei in a sea of degenerate electrons. At a microscopic level, the constituent \"particles\" of matter such as protons, neutrons, and electrons obey the laws of quantum mechanics and exhibit wave–particle duality. At an even deeper level, protons and neutrons are made up of quarks and the force fields (gluons) that bind them together, leading to the next definition.\n\nAs seen in the above discussion, many early definitions of what can be called \"ordinary matter\" were based upon its structure or \"building blocks\". On the scale of elementary particles, a definition that follows this tradition can be stated as: \n\"ordinary matter is everything that is composed of quarks and leptons\", or \"ordinary matter is everything that is composed of any elementary fermions except antiquarks and antileptons\". The connection between these formulations follows.\n\nLeptons (the most famous being the electron), and quarks (of which baryons, such as protons and neutrons, are made) combine to form atoms, which in turn form molecules. Because atoms and molecules are said to be matter, it is natural to phrase the definition as: \"ordinary matter is anything that is made of the same things that atoms and molecules are made of\". (However, notice that one also can make from these building blocks matter that is \"not\" atoms or molecules.) Then, because electrons are leptons, and protons, and neutrons are made of quarks, this definition in turn leads to the definition of matter as being \"quarks and leptons\", which are two of the four types of elementary fermions (the other two being antiquarks and antileptons, which can be considered antimatter as described later). Carithers and Grannis state: \"Ordinary matter is composed entirely of first-generation particles, namely the [up] and [down] quarks, plus the electron and its neutrino.\" (Higher generations particles quickly decay into first-generation particles, and thus are not commonly encountered.)\n\nThis definition of ordinary matter is more subtle than it first appears. All the particles that make up ordinary matter (leptons and quarks) are elementary fermions, while all the force carriers are elementary bosons. The W and Z bosons that mediate the weak force are not made of quarks or leptons, and so are not ordinary matter, even if they have mass. In other words, mass is not something that is exclusive to ordinary matter.\n\nThe quark–lepton definition of ordinary matter, however, identifies not only the elementary building blocks of matter, but also includes composites made from the constituents (atoms and molecules, for example). Such composites contain an interaction energy that holds the constituents together, and may constitute the bulk of the mass of the composite. As an example, to a great extent, the mass of an atom is simply the sum of the masses of its constituent protons, neutrons and electrons. However, digging deeper, the protons and neutrons are made up of quarks bound together by gluon fields (see dynamics of quantum chromodynamics) and these gluons fields contribute significantly to the mass of hadrons. In other words, most of what composes the \"mass\" of ordinary matter is due to the binding energy of quarks within protons and neutrons. For example, the sum of the mass of the three quarks in a nucleon is approximately , which is low compared to the mass of a nucleon (approximately ). The bottom line is that most of the mass of everyday objects comes from the interaction energy of its elementary components.\n\nThe Standard Model groups matter particles into three generations, where each generation consists of two quarks and two leptons. The first generation is the \"up\" and \"down\" quarks, the \"electron\" and the \"electron neutrino\"; the second includes the \"charm\" and \"strange\" quarks, the \"muon\" and the \"muon neutrino\"; the third generation consists of the \"top\" and \"bottom\" quarks and the \"tau\" and \"tau neutrino\". The most natural explanation for this would be that quarks and leptons of higher generations are excited states of the first generations. If this turns out to be the case, it would imply that quarks and leptons are composite particles, rather than elementary particles.\n\nThis quark–lepton definition of matter also leads to what can be described as \"conservation of (net) matter\" laws—discussed later below. Alternatively, one could return to the mass–volume–space concept of matter, leading to the next definition, in which antimatter becomes included as a subclass of matter.\n\nA common or traditional definition of matter is \"anything that has mass and volume (occupies space)\". For example, a car would be said to be made of matter, as it has mass and volume (occupies space).\n\nThe observation that matter occupies space goes back to antiquity. However, an explanation for why matter occupies space is recent, and is argued to be a result of the phenomenon described in the Pauli exclusion principle, which applies to fermions. Two particular examples where the exclusion principle clearly relates matter to the occupation of space are white dwarf stars and neutron stars, discussed further below.\n\nThus, matter can be defined as everything composed of elementary fermions. Although we don't encounter them in everyday life, antiquarks (such as the antiproton) and antileptons (such as the positron) are the antiparticles of the quark and the lepton, are elementary fermions as well, and have essentially the same properties as quarks and leptons, including the applicability of the Pauli exclusion principle which can be said to prevent two particles from being in the same place at the same time (in the same state), i.e. makes each particle \"take up space\". This particular definition leads to matter being defined to include anything made of these antimatter particles as well as the ordinary quark and lepton, and thus also anything made of mesons, which are unstable particles made up of a quark and an antiquark.\n\nIn the context of relativity, mass is not an additive quantity, in the sense that one can not add the rest masses of particles in a system to get the total rest mass of the system. Thus, in relativity usually a more general view is that it is not the sum of rest masses, but the energy–momentum tensor that quantifies the amount of matter. This tensor gives the rest mass for the entire system. \"Matter\" therefore is sometimes considered as anything that contributes to the energy–momentum of a system, that is, anything that is not purely gravity. This view is commonly held in fields that deal with general relativity such as cosmology. In this view, light and other massless particles and fields are all part of \"matter\".\n\nIn particle physics, fermions are particles that obey Fermi–Dirac statistics. Fermions can be elementary, like the electron—or composite, like the proton and neutron. In the Standard Model, there are two types of elementary fermions: quarks and leptons, which are discussed next.\n\nQuarks are particles of spin-, implying that they are fermions. They carry an electric charge of − e (down-type quarks) or + e (up-type quarks). For comparison, an electron has a charge of −1 e. They also carry colour charge, which is the equivalent of the electric charge for the strong interaction. Quarks also undergo radioactive decay, meaning that they are subject to the weak interaction. Quarks are massive particles, and therefore are also subject to gravity.\n\nBaryons are strongly interacting fermions, and so are subject to Fermi–Dirac statistics. Amongst the baryons are the protons and neutrons, which occur in atomic nuclei, but many other unstable baryons exist as well. The term baryon usually refers to triquarks—particles made of three quarks. Also, \"exotic\" baryons made of four quarks and one antiquark are known as pentaquarks, but their existence is not generally accepted.\n\nBaryonic matter is the part of the universe that is made of baryons (including all atoms). This part of the universe does not include dark energy, dark matter, black holes or various forms of degenerate matter, such as compose white dwarf stars and neutron stars. Microwave light seen by Wilkinson Microwave Anisotropy Probe (WMAP), suggests that only about 4.6% of that part of the universe within range of the best telescopes (that is, matter that may be visible because light could reach us from it), is made of baryonic matter. About 26.8% is dark matter, and about 68.3% is dark energy.\n\nAs a matter of fact, the great majority of ordinary matter in the universe is unseen, since visible stars and gas inside galaxies and clusters account for less than 10 per cent of the ordinary matter contribution to the mass–energy density of the universe.\n\nHadronic matter can refer to 'ordinary' baryonic matter, made from hadrons (Baryons and mesons), or quark matter (a generalisation of atomic nuclei), ie. the 'low' temperature QCD matter. It includes degenerate matter and the result of high energy heavy nuclei collisions. Distinct from dark matter.\n\nIn physics, \"degenerate matter\" refers to the ground state of a gas of fermions at a temperature near absolute zero. The Pauli exclusion principle requires that only two fermions can occupy a quantum state, one spin-up and the other spin-down. Hence, at zero temperature, the fermions fill up sufficient levels to accommodate all the available fermions—and in the case of many fermions, the maximum kinetic energy (called the \"Fermi energy\") and the pressure of the gas becomes very large, and depends on the number of fermions rather than the temperature, unlike normal states of matter.\n\nDegenerate matter is thought to occur during the evolution of heavy stars. The demonstration by Subrahmanyan Chandrasekhar that white dwarf stars have a maximum allowed mass because of the exclusion principle caused a revolution in the theory of star evolution.\n\nDegenerate matter includes the part of the universe that is made up of neutron stars and white dwarfs.\n\n\"Strange matter\" is a particular form of quark matter, usually thought of as a \"liquid\" of up, down, and strange quarks. It is contrasted with nuclear matter, which is a liquid of neutrons and protons (which themselves are built out of up and down quarks), and with non-strange quark matter, which is a quark liquid that contains only up and down quarks. At high enough density, strange matter is expected to be color superconducting. Strange matter is hypothesized to occur in the core of neutron stars, or, more speculatively, as isolated droplets that may vary in size from femtometers (strangelets) to kilometers (quark stars).\n\nIn particle physics and astrophysics, the term is used in two ways, one broader and the other more specific.\n\nLeptons are particles of spin-, meaning that they are fermions. They carry an electric charge of −1 e (charged leptons) or 0 e (neutrinos). Unlike quarks, leptons do not carry colour charge, meaning that they do not experience the strong interaction. Leptons also undergo radioactive decay, meaning that they are subject to the weak interaction. Leptons are massive particles, therefore are subject to gravity.\n\nIn bulk, matter can exist in several different forms, or states of aggregation, known as \"phases\", depending on ambient pressure, temperature and volume. A phase is a form of matter that has a relatively uniform chemical composition and physical properties (such as density, specific heat, refractive index, and so forth). These phases include the three familiar ones (solids, liquids, and gases), as well as more exotic states of matter (such as plasmas, superfluids, supersolids, Bose–Einstein condensates, ...). A \"fluid\" may be a liquid, gas or plasma. There are also paramagnetic and ferromagnetic phases of magnetic materials. As conditions change, matter may change from one phase into another. These phenomena are called phase transitions, and are studied in the field of thermodynamics. In nanomaterials, the vastly increased ratio of surface area to volume results in matter that can exhibit properties entirely different from those of bulk material, and not well described by any bulk phase (see nanomaterials for more details).\n\nPhases are sometimes called \"states of matter\", but this term can lead to confusion with thermodynamic states. For example, two gases maintained at different pressures are in different \"thermodynamic states\" (different pressures), but in the same \"phase\" (both are gases).\n\nIn particle physics and quantum chemistry, \"antimatter\" is matter that is composed of the antiparticles of those that constitute ordinary matter. If a particle and its antiparticle come into contact with each other, the two annihilate; that is, they may both be converted into other particles with equal energy in accordance with Einstein's equation . These new particles may be high-energy photons (gamma rays) or other particle–antiparticle pairs. The resulting particles are endowed with an amount of kinetic energy equal to the difference between the rest mass of the products of the annihilation and the rest mass of the original particle–antiparticle pair, which is often quite large. Depending on which definition of \"matter\" is adopted, antimatter can be said to be a particular subclass of matter, or the opposite of matter.\n\nAntimatter is not found naturally on Earth, except very briefly and in vanishingly small quantities (as the result of radioactive decay, lightning or cosmic rays). This is because antimatter that came to exist on Earth outside the confines of a suitable physics laboratory would almost instantly meet the ordinary matter that Earth is made of, and be annihilated. Antiparticles and some stable antimatter (such as antihydrogen) can be made in tiny amounts, but not in enough quantity to do more than test a few of its theoretical properties.\n\nThere is considerable speculation both in science and science fiction as to why the observable universe is apparently almost entirely matter (in the sense of quarks and leptons but not antiquarks or antileptons), and whether other places are almost entirely antimatter (antiquarks and antileptons) instead. In the early universe, it is thought that matter and antimatter were equally represented, and the disappearance of antimatter requires an asymmetry in physical laws called CP (charge-parity) symmetry violation, which can be obtained from the Standard Model, but at this time the apparent asymmetry of matter and antimatter in the visible universe is one of the great unsolved problems in physics. Possible processes by which it came about are explored in more detail under baryogenesis.\n\nFormally, antimatter particles can be defined by their negative baryon number or lepton number, while \"normal\" (non-antimatter) matter particles have positive baryon or lepton number. These two classes of particles are the antiparticle partners of one another.\n\nIn October 2017, scientists reported further evidence that matter and antimatter, equally produced at the Big Bang, are identical, should completely annihilate each other and, as a result, the universe should not exist. This implies that there must be something, as yet unknown to scientists, that either stopped the complete mutual destruction of matter and antimatter in the early forming universe, or that gave rise to an imbalance between the two forms.\n\nTwo quantities that can define an amount of matter in the quark–lepton sense (and antimatter in an antiquark–antilepton sense), baryon number and lepton number, are conserved in the Standard Model. A baryon such as the proton or neutron has a baryon number of one, and a quark, because there are three in a baryon, is given a baryon number of 1/3. So the net amount of matter, as measured by the number of quarks (minus the number of antiquarks, which each have a baryon number of −1/3), which is proportional to baryon number, and number of leptons (minus antileptons), which is called the lepton number, is practically impossible to change in any process. Even in a nuclear bomb, none of the baryons (protons and neutrons of which the atomic nuclei are composed) are destroyed—there are as many baryons after as before the reaction, so none of these matter particles are actually destroyed and none are even converted to non-matter particles (like photons of light or radiation). Instead, nuclear (and perhaps chromodynamic) binding energy is released, as these baryons become bound into mid-size nuclei having less energy (and, equivalently, less mass) per nucleon compared to the original small (hydrogen) and large (plutonium etc.) nuclei. Even in electron–positron annihilation, there is no net matter being destroyed, because there was zero net matter (zero total lepton number and baryon number) to begin with before the annihilation—one lepton minus one antilepton equals zero net lepton number—and this net amount matter does not change as it simply remains zero after the annihilation. So the only way to really \"destroy\" or \"convert\" ordinary matter is to pair it with the same amount of antimatter so that their \"matterness\" cancels out—but in practice there is almost no antimatter generally available in the universe (see baryon asymmetry and leptogenesis) with which to do so.\n\nOrdinary matter, in the quarks and leptons definition, constitutes about 4% of the energy of the observable universe. The remaining energy is theorized to be due to exotic forms, of which 23% is dark matter and 73% is dark energy.\n\nIn astrophysics and cosmology, \"dark matter\" is matter of unknown composition that does not emit or reflect enough electromagnetic radiation to be observed directly, but whose presence can be inferred from gravitational effects on visible matter. Observational evidence of the early universe and the Big Bang theory require that this matter have energy and mass, but is not composed ordinary baryons (protons and neutrons). The commonly accepted view is that most of the dark matter is non-baryonic in nature. As such, it is composed of particles as yet unobserved in the laboratory. Perhaps they are supersymmetric particles, which are not Standard Model particles, but relics formed at very high energies in the early phase of the universe and still floating about.\n\nIn cosmology, \"dark energy\" is the name given to source of the repelling influence that is accelerating the rate of expansion of the universe. Its precise nature is currently a mystery, although its effects can reasonably be modeled by assigning matter-like properties such as energy density and pressure to the vacuum itself.\nExotic matter is a concept of particle physics, which may include dark matter and dark energy but goes further to include any hypothetical material that violates one or more of the properties of known forms of matter. Some such materials might possess hypothetical properties like negative mass.\n\nThe pre-Socratics were among the first recorded speculators about the underlying nature of the visible world. Thales (c. 624 BC–c. 546 BC) regarded water as the fundamental material of the world. Anaximander (c. 610 BC–c. 546 BC) posited that the basic material was wholly characterless or limitless: the Infinite (\"apeiron\"). Anaximenes (flourished 585 BC, d. 528 BC) posited that the basic stuff was \"pneuma\" or air. Heraclitus (c. 535–c. 475 BC) seems to say the basic element is fire, though perhaps he means that all is change. Empedocles (c. 490–430 BC) spoke of four elements of which everything was made: earth, water, air, and fire. Meanwhile, Parmenides argued that change does not exist, and Democritus argued that everything is composed of minuscule, inert bodies of all shapes called atoms, a philosophy called atomism. All of these notions had deep philosophical problems.\n\nAristotle (384 BC – 322 BC) was the first to put the conception on a sound philosophical basis, which he did in his natural philosophy, especially in \"Physics\" book I. He adopted as reasonable suppositions the four Empedoclean elements, but added a fifth, aether. Nevertheless, these elements are not basic in Aristotle's mind. Rather they, like everything else in the visible world, are composed of the basic \"principles\" matter and form.\n\nThe word Aristotle uses for matter, ὕλη (\"hyle\" or \"hule\"), can be literally translated as wood or timber, that is, \"raw material\" for building. Indeed, Aristotle's conception of matter is intrinsically linked to something being made or composed. In other words, in contrast to the early modern conception of matter as simply occupying space, matter for Aristotle is definitionally linked to process or change: matter is what underlies a change of substance. For example, a horse eats grass: the horse changes the grass into itself; the grass as such does not persist in the horse, but some aspect of it—its matter—does. The matter is not specifically described (e.g., as atoms), but consists of whatever persists in the change of substance from grass to horse. Matter in this understanding does not exist independently (i.e., as a substance), but exists interdependently (i.e., as a \"principle\") with form and only insofar as it underlies change. It can be helpful to conceive of the relationship of matter and form as very similar to that between parts and whole. For Aristotle, matter as such can only \"receive\" actuality from form; it has no activity or actuality in itself, similar to the way that parts as such only have their existence \"in\" a whole (otherwise they would be independent wholes).\n\nRené Descartes (1596–1650) originated the modern conception of matter. He was primarily a geometer. Instead of, like Aristotle, deducing the existence of matter from the physical reality of change, Descartes arbitrarily postulated matter to be an abstract, mathematical substance that occupies space:\nFor Descartes, matter has only the property of extension, so its only activity aside from locomotion is to exclude other bodies: this is the mechanical philosophy. Descartes makes an absolute distinction between mind, which he defines as unextended, thinking substance, and matter, which he defines as unthinking, extended substance. They are independent things. In contrast, Aristotle defines matter and the formal/forming principle as complementary \"principles\" that together compose one independent thing (substance). In short, Aristotle defines matter (roughly speaking) as what things are actually made of (with a \"potential\" independent existence), but Descartes elevates matter to an actual independent thing in itself.\n\nThe continuity and difference between Descartes' and Aristotle's conceptions is noteworthy. In both conceptions, matter is passive or inert. In the respective conceptions matter has different relationships to intelligence. For Aristotle, matter and intelligence (form) exist together in an interdependent relationship, whereas for Descartes, matter and intelligence (mind) are definitionally opposed, independent substances.\n\nDescartes' justification for restricting the inherent qualities of matter to extension is its permanence, but his real criterion is not permanence (which equally applied to color and resistance), but his desire to use geometry to explain all material properties. Like Descartes, Hobbes, Boyle, and Locke argued that the inherent properties of bodies were limited to extension, and that so-called secondary qualities, like color, were only products of human perception.\n\nIsaac Newton (1643–1727) inherited Descartes' mechanical conception of matter. In the third of his \"Rules of Reasoning in Philosophy\", Newton lists the universal qualities of matter as \"extension, hardness, impenetrability, mobility, and inertia\". Similarly in \"Optics\" he conjectures that God created matter as \"solid, massy, hard, impenetrable, movable particles\", which were \"...even so very hard as never to wear or break in pieces\". The \"primary\" properties of matter were amenable to mathematical description, unlike \"secondary\" qualities such as color or taste. Like Descartes, Newton rejected the essential nature of secondary qualities.\n\nNewton developed Descartes' notion of matter by restoring to matter intrinsic properties in addition to extension (at least on a limited basis), such as mass. Newton's use of gravitational force, which worked \"at a distance\", effectively repudiated Descartes' mechanics, in which interactions happened exclusively by contact.\n\nThough Newton's gravity would seem to be a \"power\" of bodies, Newton himself did not admit it to be an \"essential\" property of matter. Carrying the logic forward more consistently, Joseph Priestley (1733–1804) argued that corporeal properties transcend contact mechanics: chemical properties require the \"capacity\" for attraction. He argued matter has other inherent powers besides the so-called primary qualities of Descartes, et al.\n\nSince Priestley's time, there has been a massive expansion in knowledge of the constituents of the material world (viz., molecules, atoms, subatomic particles), but there has been no further development in the \"definition\" of matter. Rather the question has been set aside. Noam Chomsky (born 1928) summarizes the situation that has prevailed since that time:\nSo matter is whatever physics studies and the object of study of physics is matter: there is no independent general definition of matter, apart from its fitting into the methodology of measurement and controlled experimentation. In sum, the boundaries between what constitutes matter and everything else remains as vague as the demarcation problem of delimiting science from everything else.\n\nIn the 19th century, following the development of the periodic table, and of atomic theory, atoms were seen as being the fundamental constituents of matter; atoms formed molecules and compounds.\n\nThe common definition in terms of occupying space and having mass is in contrast with most physical and chemical definitions of matter, which rely instead upon its structure and upon attributes not necessarily related to volume and mass. At the turn of the nineteenth century, the knowledge of matter began a rapid evolution.\n\nAspects of the Newtonian view still held sway. James Clerk Maxwell discussed matter in his work \"Matter and Motion\". He carefully separates \"matter\" from space and time, and defines it in terms of the object referred to in Newton's first law of motion.\n\nHowever, the Newtonian picture was not the whole story. In the 19th century, the term \"matter\" was actively discussed by a host of scientists and philosophers, and a brief outline can be found in Levere. A textbook discussion from 1870 suggests matter is what is made up of atoms:Three divisions of matter are recognized in science: masses, molecules and atoms. A Mass of matter is any portion of matter appreciable by the senses. A Molecule is the smallest particle of matter into which a body can be divided without losing its identity. An Atom is a still smaller particle produced by division of a molecule. \n\nRather than simply having the attributes of mass and occupying space, matter was held to have chemical and electrical properties. In 1909 the famous physicist J. J. Thomson (1856–1940) wrote about the \"constitution of matter\" and was concerned with the possible connection between matter and electrical charge.\n\nThere is an entire literature concerning the \"structure of matter\", ranging from the \"electrical structure\" in the early 20th century, to the more recent \"quark structure of matter\", introduced today with the remark: \"Understanding the quark structure of matter has been one of the most important advances in contemporary physics.\" In this connection, physicists speak of \"matter fields\", and speak of particles as \"quantum excitations of a mode of the matter field\". And here is a quote from de Sabbata and Gasperini: \"With the word \"matter\" we denote, in this context, the sources of the interactions, that is spinor fields (like quarks and leptons), which are believed to be the fundamental components of matter, or scalar fields, like the Higgs particles, which are used to introduced mass in a gauge theory (and that, however, could be composed of more fundamental fermion fields).\"\n\nIn the late 19th century with the discovery of the electron, and in the early 20th century, with the discovery of the atomic nucleus, and the birth of particle physics, matter was seen as made up of electrons, protons and neutrons interacting to form atoms. Today, we know that even protons and neutrons are not indivisible, they can be divided into quarks, while electrons are part of a particle family called leptons. Both quarks and leptons are elementary particles, and are currently seen as being the fundamental constituents of matter.\n\nThese quarks and leptons interact through four fundamental forces: gravity, electromagnetism, weak interactions, and strong interactions. The Standard Model of particle physics is currently the best explanation for all of physics, but despite decades of efforts, gravity cannot yet be accounted for at the quantum level; it is only described by classical physics (see quantum gravity and graviton). Interactions between quarks and leptons are the result of an exchange of force-carrying particles (such as photons) between quarks and leptons. The force-carrying particles are not themselves building blocks. As one consequence, mass and energy (which cannot be created or destroyed) cannot always be related to matter (which can be created out of non-matter particles such as photons, or even out of pure energy, such as kinetic energy). Force carriers are usually not considered matter: the carriers of the electric force (photons) possess energy (see Planck relation) and the carriers of the weak force (W and Z bosons) are massive, but neither are considered matter either. However, while these particles are not considered matter, they do contribute to the total mass of atoms, subatomic particles, and all systems that contain them.\n\nThe modern conception of matter has been refined many times in history, in light of the improvement in knowledge of just \"what\" the basic building blocks are, and in how they interact.\nThe term \"matter\" is used throughout physics in a bewildering variety of contexts: for example, one refers to \"condensed matter physics\", \"elementary matter\", \"partonic\" matter, \"dark\" matter, \"anti\"-matter, \"strange\" matter, and \"nuclear\" matter. In discussions of matter and antimatter, normal matter has been referred to by Alfvén as \"koinomatter\" (Gk. \"common matter\"). It is fair to say that in physics, there is no broad consensus as to a general definition of matter, and the term \"matter\" usually is used in conjunction with a specifying modifier.\n\nThe history of the concept of matter is a history of the fundamental \"length scales\" used to define matter. Different building blocks apply depending upon whether one defines matter on an atomic or elementary particle level. One may use a definition that matter is atoms, or that matter is hadrons, or that matter is leptons and quarks depending upon the scale at which one wishes to define matter.\n\nThese quarks and leptons interact through four fundamental forces: gravity, electromagnetism, weak interactions, and strong interactions. The Standard Model of particle physics is currently the best explanation for all of physics, but despite decades of efforts, gravity cannot yet be accounted for at the quantum level; it is only described by classical physics (see quantum gravity and graviton).\n\nAntimatter\n\nCosmology\nDark matter\n\nPhilosophy\nOther\n\nMy Pals Are Here! Science P3&4 Cycless-Chapter 3(Matter)\n\n"}
{"id": "3595285", "url": "https://en.wikipedia.org/wiki?curid=3595285", "title": "Maximum power principle", "text": "Maximum power principle\n\nThe maximum power principle or Lotka's principle has been proposed as the fourth principle of energetics in open system thermodynamics, where an example of an open system is a biological cell. According to Howard T. Odum, \"The maximum power principle can be stated: During self-organization, system designs develop and prevail that maximize power intake, energy transformation, and those uses that reinforce production and efficiency.\"\n\nChen (2006) has located the origin of the statement of maximum power as a formal principle in a tentative proposal by Alfred J. Lotka (1922a, b). Lotka's statement sought to explain the Darwinian notion of evolution with reference to a physical principle. Lotka's work was subsequently developed by the systems ecologist Howard T. Odum in collaboration with the Chemical Engineer Richard C. Pinkerton, and later advanced by the Engineer Myron Tribus.\n\nWhile Lotka's work may have been a first attempt to formalise evolutionary thought in mathematical terms, it followed similar observations made by Leibniz and Volterra and Ludwig Boltzmann, for example, throughout the sometimes controversial history of natural philosophy. In contemporary literature it is most commonly associated with the work of Howard T. Odum.\n\nThe significance of Odum's approach was given greater support during the 1970s, amid times of oil crisis, where, as Gilliland (1978, pp. 100) observed, there was an emerging need for a new method of analysing the importance and value of energy resources to economic and environmental production. A field known as energy analysis, itself associated with net energy and EROEI, arose to fulfill this analytic need. However, in energy analysis intractable theoretical and practical difficulties arose when using the energy unit to understand, a) the conversion among concentrated fuel types (or energy types), b) the contribution of labour, and c) the contribution of the environment.\n\nLotka said (1922b: 151): \nGilliland noted that these difficulties in analysis in turn required some new theory to adequately explain the interactions and transactions of these different energies (different concentrations of fuels, labour and environmental forces). Gilliland (Gilliland 1978, p. 101) suggested that Odum's statement of the maximum power principle (H.T.Odum 1978, pp. 54–87) was, perhaps, an adequate expression of the requisite theory:\nThis theory Odum called maximum power theory. In order to formulate maximum power theory Gilliland observed that Odum had added another law (the maximum power principle) to the already well established laws of thermodynamics. In 1978 Gilliland wrote that Odum's new law had not yet been validated (Gilliland 1978, p. 101). Gilliland stated that in maximum power theory the second law efficiency of thermodynamics required an additional physical concept: \"the concept of second law efficiency under maximum power\" (Gilliland 1978, p. 101):\nIn this way the concept of maximum power was being used as a principle to quantitatively describe the selective law of biological evolution. Perhaps H.T.Odum's most concise statement of this view was (1970, p. 62):\n\nThe Odum–Pinkerton approach to Lotka's proposal was to apply Ohm's law – and the associated maximum power theorem (a result in electrical power systems) – to ecological systems. Odum and Pinkerton defined \"power\" in electronic terms as the rate of work, where Work is understood as a \"useful energy transformation\". The concept of maximum power can therefore be defined as the \"maximum rate of useful energy transformation\". Hence the underlying philosophy aims to unify the theories and associated laws of electronic and thermodynamic systems with biological systems. This approach presupposed an analogical view which sees the world as an ecological-electronic-economic engine.\n\nOdum et al. viewed the maximum power theorem as a principle of power-efficiency reciprocity selection with wider application than just electronics. For example, Odum saw it in open systems operating on solar energy, like both photovoltaics and photosynthesis (1963, p. 438). Like the maximum power theorem, Odum's statement of the maximum power principle relies on the notion of 'matching', such that high-quality energy maximizes power by matching and amplifying energy (1994, pp. 262, 541): \"in surviving designs a matching of high-quality energy with larger amounts of low-quality energy is likely to occur\" (1994, p. 260). As with electronic circuits, the resultant rate of energy transformation will be at a maximum at an intermediate power efficiency. In 2006, T.T. Cai, C.L. Montague and J.S. Davis said that, \"The maximum power principle is a potential guide to understanding the patterns and processes of ecosystem development and sustainability. The principle predicts the selective persistence of ecosystem designs that capture a previously untapped energy source.\" (2006, p. 317). In several texts H.T. Odum gave the Atwood machine as a practical example of the 'principle' of maximum power.\n\nThe mathematical definition given by H.T. Odum is formally analogous to the definition provided on the maximum power theorem article. (For a brief explanation of Odum's approach to the relationship between ecology and electronics see Ecological Analog of Ohm's Law)\n\nWhether or not the principle of maximum power efficiency can be considered the fourth law of thermodynamics and the fourth principle of energetics is moot. Nevertheless, H.T. Odum also proposed a corollary of maximum power as the organisational principle of evolution, describing the evolution of microbiological systems, economic systems, planetary systems, and astrophysical systems. He called this corollary the maximum empower principle. This was suggested because, as S.E. Jorgensen, M.T. Brown, H.T. Odum (2004) note,\n\nC. Giannantoni may have confused matters when he wrote \"The \"Maximum Em-Power Principle\" (Lotka–Odum) is generally considered the \"Fourth Thermodynamic Principle\" (mainly) because of its practical validity for a very wide class of physical and biological systems\" (C. Giannantoni 2002, § 13, p. 155). Nevertheless, Giannantoni has proposed the Maximum Em-Power Principle as the fourth principle of thermodynamics (Giannantoni 2006).\n\nThe preceding discussion is incomplete. The \"maximum power\" was discovered several times independently, in physics and engineering, see: Novikov (1957), El-Wakil (1962), and Curzon and Ahlborn (1975). The incorrectness of this analysis and design evolution conclusions was demonstrated by Gyftopoulos (2002).\n\n\n"}
{"id": "14389994", "url": "https://en.wikipedia.org/wiki?curid=14389994", "title": "Natural landscape", "text": "Natural landscape\n\nA natural landscape is the original landscape that exists before it is acted upon by human culture. The natural landscape and the cultural landscape are separate parts of the landscape. However, in the twenty-first century landscapes that are totally untouched by human activity no longer exist, so that reference is sometimes now made to degrees of naturalness within a landscape.\n\nIn \"Silent Spring\" (1962) Rachel Carson describes a roadside verge as it used to look: \"Along the roads, laurel, viburnum and alder, great ferns and wildflowers delighted the traveler’s eye through much of the year\" and then how it looks now following the use of herbicides: \"The roadsides, once so attractive, were now lined with browned and withered vegetation as though swept by fire\". Even though the landscape before it is sprayed is biologically degraded, and may well contains alien species, the concept of what might constitute a natural landscape can still be deduced from the context.\n\nThe phrase \"natural landscape\" was first used in connection with landscape painting, and landscape gardening, to contrast a formal style with a more natural one, closer to nature. Alexander von Humboldt (1769 – 1859) was to further conceptualize this into the idea of a natural landscape \"separate\" from the cultural landscape. Then in 1908 geographer Otto Schlüter developed the terms original landscape (\"Urlandschaft\") and its opposite cultural landscape (\"Kulturlandschaft\") in an attempt to give the science of geography a subject matter that was different from the other sciences. An early use of the actual phrase \"natural landscape\" by a geographer can be found in Carl O. Sauer's paper \"The Morphology of Landscape\" (1925).\n\nThe concept of a natural landscape was first developed in connection with landscape painting, though the actual term itself was first used in relation to landscape gardening. In both cases it was used to contrast a formal style with a more natural one, that is closer to nature. Chunglin Kwa suggests, \"that a seventeenth-century or early-eighteenth-century person could experience natural scenery ‘just like on a painting,’ and so, with or without the use of the word itself, designate it as a landscape.\" With regard to landscape gardening John Aikin, commented in 1794: \"Whatever, therefore, there be of \"novelty\" in the singular scenery of an artificial garden, it is soon exhausted, whereas the infinite diversity of a natural landscape presents an inexhaustible flore of new forms\". Writing in 1844 the prominent American landscape gardener Andrew Jackson Downing comments: \"straight canals, round or oblong pieces of water, and all the regular forms of the geometric mode ... would evidently be in violent opposition to the whole character and expression of natural landscape\".\n\nIn his extensive travels in South America, Alexander von Humboldt became the first to conceptualize a natural landscape separate from the cultural landscape, though he does not actually use these terms. Andrew Jackson Downing was aware of, and sympathetic to, Humboldt's ideas, which therefore influenced American landscape gardening.\n\nSubsequently, the geographer Otto Schlüter, in 1908, argued that by defining geography as a \"Landschaftskunde\" (landscape science) would give geography a logical subject matter shared by no other discipline. He defined two forms of landscape: the \"Urlandschaft\" (original landscape) or landscape that existed before major human induced changes and the \"Kulturlandschaft\" (cultural landscape) a landscape created by human culture. Schlüter argued that the major task of geography was to trace the changes in these two landscapes.\n\nThe term natural landscape is sometimes used as a synonym for wilderness, but for geographers natural landscape is a scientific term which refers to the biological, geological, climatological and other aspects of a landscape, not the cultural values that are implied by the word wilderness.\n\nMatters are complicated by the fact that the words nature and natural have more than one meaning. On the one hand there is the main dictionary meaning for nature: \"The phenomena of the physical world collectively, including plants, animals, the landscape, and other features and products of the earth, as opposed to humans or human creations\". On the other hand, there is the growing awareness, especially since Charles Darwin, of humanities biological affinity with nature.\n\nThe dualism of the first definition has its roots is an \"ancient concept\", because early people viewed \"nature, or the nonhuman world […] as a divine \"Other\", godlike in its separation from humans\". In the West, Christianity's myth of the fall, that is the expulsion of humankind from the Garden of Eden, where all creation lived in harmony, into an imperfect world, has been the major influence. Cartesian dualism, from the seventeenth century on, further reinforced this dualistic thinking about nature. \nWith this dualism goes value judgement as to the superiority of the natural over the artificial. Modern science, however, is moving towards a holistic view of nature.\n\nWhat is meant by natural, within the American conservation movement, has been changing over the last century and a half.\n\nIn the mid-nineteenth century American began to realize that the land was becoming more and more domesticated and wildlife was disappearing. This led to the creation of American National Parks and other conservation sites. Initially it was believed that all that was needed to do was to separate what was seen as natural landscape and \"avoid disturbances such as logging, grazing, fire and insect outbreaks\". This, and subsequent environmental policy, until recently, was influenced by ideas of the wilderness. However, this policy was not consistently applied, and in Yellowstone Park, to take one example, the existing ecology was altered, firstly by the exclusion of Native Americans and later with the virtual extermination of the wolf population.\n\nA century later, in the mid-twentieth century, it began to be believed that the earlier policy of \"protection from disturbance was inadequate to preserve park values\", and that is that direct human intervention was necessary to restore the landscape of National Parks to its ‘’natural’’ condition. In 1963 the Leopold Report argued that \"A national park should represent a vignette of primitive America\". This policy change eventually led to the restoration of wolves in Yellowstone Park in the 1990s.\n\nHowever, recent research in various disciplines indicates that a pristine natural or \"primitive\" landscape is a myth, and it now realised that people have been changing the natural into a cultural landscape for a long while, and that there are few places untouched in some way from human influence. The earlier conservation policies were now seen as cultural interventions. The idea of what is natural and what artificial or cultural, and how to maintain the natural elements in a landscape, has been further complicated by the discovery of global warming and how it is changing natural landscapes.\n\nAlso important is a reaction recently amongst scholars against dualistic thinking about nature and culture. Maria Kaika comments: \"Nowadays, we are beginning to see nature and culture as intertwined once again – not ontologically separated anymore […].What I used to perceive as a compartmentalized world, consisting of neatly and tightly sealed, autonomous ‘space envelopes’ (the home, the city, and nature) was, in fact, a messy socio-spatial continuum”. And William Cronon argues against the idea of wilderness because it \"involves a dualistic vision in which the human is entirely outside the natural\" and affirms that \"wildness (as opposed to wilderness) can be found anywhere\" even \"in the cracks of a Manhattan sidewalk\". According to Cronon we have to \"abandon the dualism that sees the tree in the garden as artificial […] and the tree in the wilderness as natural […] Both in some ultimate sense are wild.\" Here he bends somewhat the regular dictionary meaning of wild, to emphasise that nothing natural, even in a garden, is fully under human control.\n\nThe landscape of Europe has considerably altered by people and even in an area, like the Cairngorm Mountains of Scotland, with a low population density, only \" the high summits of the Cairngorm Mountains, consist entirely of natural elements. These \"high summits\" are of course only part of the Cairngorms, and there are no longer wolves, bears, wild boar or lynx in Scotland's wilderness. The Scots pine in the form of the Caledonian forest also covered much more of the Scottish landscape than today.\n\nThe Swiss National Park, however, represent a more natural landscape. It was founded in 1914, and is one of the earliest national parks in Europe.\nVisitors are not allowed to leave the motor road, or paths through the park, make fire or camp. The only building within the park is Chamanna Cluozza, mountain hut. It is also forbidden to disturb the animals or the plants, or to take home anything found in the park. Dogs are not allowed. Due to these strict rules, the Swiss National Park is the only park in the Alps who has been categorized by the IUCN as a strict nature reserve, which is the highest protection level.\n\nNo place on the Earth is unaffected by people and their culture. People are part of biodiversity, but human activity affects biodiversity, and this alters the natural landscape. Mankind have altered landscape to such an extent that few places on earth remain pristine, but once free of human influences, the landscape can return to a natural or near natural state.\nEven the remote Yukon and Alaskan wilderness, the bi-national Kluane-Wrangell-St. Elias-Glacier Bay-Tatshenshini-Alsek park system comprising Kluane, Wrangell-St Elias, Glacier Bay and Tatshenshini-Alsek parks, a UNESCO World Heritage Site, is not free from human influence, because the Kluane National Park lies within the traditional territories of the Champagne and Aishihik First Nations and Kluane First Nation who have a long history of living in this region. Through their respective Final Agreements with the Canadian Government, they have made into law their rights to harvest in this region.\n\nCultural forces intentionally or unintentionally, have an influence upon the landscape. Cultural landscapes are places or artifacts created and maintained by people. Examples of cultural intrusions into a landscape are: fences, roads, parking lots, sand pits, buildings, hiking trails, management of plants, including the introduction of invasive species, extraction or removal of plants, management of animals, mining, hunting, natural landscaping, farming and forestry, pollution. Areas that might be confused with a natural landscape include public parks, farms, orchards, artificial lakes and reservoirs, managed forests, golf courses, nature center trails, gardens.\n\n"}
{"id": "14272151", "url": "https://en.wikipedia.org/wiki?curid=14272151", "title": "Nature religion", "text": "Nature religion\n\nA nature religion is a religious movement that believes nature and the natural world is an embodiment of divinity, sacredness or spiritual power. Nature religions include indigenous religions practiced in various parts of the world by cultures who consider the environment to be imbued with spirits and other sacred entities. It also includes contemporary Pagan faiths which are primarily concentrated in Europe and North America.\n\nThe term \"nature religion\" was first coined by the American religious studies scholar Catherine Albanese, who used it in her work \"Nature Religion in America: From the Algonkian Indians to the New Age\" (1991) and later went on to use it in other studies. Following on from Albanese's development of the term it has since been used by other academics working in the discipline.\n\nCatherine Albanese described nature religion as \"a symbolic center and the cluster of beliefs, behaviours, and values that encircles it\", deeming it to be useful for shining a light on aspects of history that are rarely viewed as religious.\nIn a paper of his on the subject, the Canadian religious studies scholar Peter Beyer described \"nature religion\" as a \"useful analytical abstraction\" to refer to \"any religious belief or practice in which devotees consider nature to be the embodiment of divinity, sacredness, transcendence, spiritual power, or whatever cognate term one wishes to use\". He went on to note that in this way nature religion was not an \"identifiable religious tradition\" such as Buddhism or Christianity are, but that it instead covers \"a range of religious and quasi-religious movements, groups and social networks whose participants may or may not identify with one of the many constructed religions of global society which referred to many other nature religion.\"\n\nPeter Beyer noted the existence of a series of common characteristics which he believed were shared by different nature religions. He remarked that although \"one must be careful not to overgeneralise\", he suspected that there were a series of features which \"occur sufficiently often\" in those nature religions known to recorded scholarship to constitute a pattern.\n\nThe first of these common characteristics was nature religion's \"comparative resistance to institutionalisation and legitimisation in terms of identifiable socio-religious authorities and organisations\", meaning that nature religionists rarely formed their religious beliefs into large, visible socio-political structures such as churches. Furthermore, Beyer noted, nature religionists often held a \"concomitant distrust of and even eschewing of politically orientated power\". Instead of this, he felt that among nature religious communities, there was \"a valuing of community as non-hierarchical\" and a \"conditional optimism with regard to human capacity and the future.\"\n\nIn the sphere of the environment, Beyer noted that nature religionists held to a \"holistic conception of reality\" and \"a valorisation of physical place as vital aspects of their spiritualities\". Similarly, Beyer noted the individualism which was favoured by nature religionists. He remarked that those adhering to such beliefs typically had respect for \"charismatic and hence purely individual authority\" and place a \"strong emphasis on individual paths\" which led them to believe in \"the equal value of individuals and groups\". Along similar lines, he also commented on the \"strong experiential basis\" to nature religionist beliefs \"where personal experience is a final arbiter of truth or validity\".\n\nIn April 1996, the University of Lancaster in North West England held a conference on contemporary Paganism entitled \"Nature Religion Today: Western Paganism, Shamanism and Esotericism in the 1990s\", and ultimately led to the publication of an academic anthology of the same name two years later. This book, \"Nature Religion Today: Paganism in the Modern World\", was edited by members of the University's Department of Religious Studies, a postgraduate named Joanne Pearson and two professors, Richard H. Roberts and Geoffrey Samuel.\n\nIn his study of Wicca, the Pagan studies scholar Ethan Doyle White expressed the view that the category of \"nature religion\" was problematic from a \"historical perspective\" because it solely emphasises the \"commonalities of belief and attitude to the natural world\" that are found between different religions and in doing so divorces these different belief systems from their distinctive socio-cultural and historical backgrounds.\n\n\n"}
{"id": "58590304", "url": "https://en.wikipedia.org/wiki?curid=58590304", "title": "Orapa Power Station", "text": "Orapa Power Station\n\nThe Orapa Power Station is a Peak load power generation plant located in the mining town of Orapa in northeastern Botswana in the Central District. It is built within the Debswana Diamond Company Ltd Orapa diamond mine fenced leased area and is operated by the Botswana Power Corporation.\nThe plant construction was initiated when Botswana started experiencing electricity supply challenges from year 2007 when demand for electricity in the country started exceeding the county's electricity generation capacity leading to forced periodic nation wide load shedding excises by the Botswana Power Corporation. The plant was a short term response to the mining industry electricity requirements prior to the development of Mmamabula and Morupule B power stations.\n\nThe plant site is located next to an electrical substation through which it connects to the national grid and was designed to use either natural gas or diesel as fuel. It was commissioned using diesel with a planned conversion to natural gas once construction of a gas pipeline from nearby gas fields was complete and commissioned.\n\nThe plant consists of two 45MW GE LM6000 Sprint Simple Cycle gas turbines fueled using diesel. The energy produced is transferred to the national grid via the interconnected Orapa Substation by use of two short 132kV overhead power lines.\n\n"}
{"id": "939466", "url": "https://en.wikipedia.org/wiki?curid=939466", "title": "Orders of magnitude (energy)", "text": "Orders of magnitude (energy)\n\nThis list compares various energies in joules (J), organized by order of magnitude.\n\n"}
{"id": "35659147", "url": "https://en.wikipedia.org/wiki?curid=35659147", "title": "Patterns in nature", "text": "Patterns in nature\n\nPatterns in nature are visible regularities of form found in the natural world. These patterns recur in different contexts and can sometimes be modelled mathematically. Natural patterns include symmetries, trees, spirals, meanders, waves, foams, tessellations, cracks and stripes. Early Greek philosophers studied pattern, with Plato, Pythagoras and Empedocles attempting to explain order in nature. The modern understanding of visible patterns developed gradually over time.\n\nIn the 19th century, Belgian physicist Joseph Plateau examined soap films, leading him to formulate the concept of a minimal surface. German biologist and artist Ernst Haeckel painted hundreds of marine organisms to emphasise their symmetry. Scottish biologist D'Arcy Thompson pioneered the study of growth patterns in both plants and animals, showing that simple equations could explain spiral growth. In the 20th century, British mathematician Alan Turing predicted mechanisms of morphogenesis which give rise to patterns of spots and stripes. Hungarian biologist Aristid Lindenmayer and French American mathematician Benoît Mandelbrot showed how the mathematics of fractals could create plant growth patterns.\n\nMathematics, physics and chemistry can explain patterns in nature at different levels. Patterns in living things are explained by the biological processes of natural selection and sexual selection. Studies of pattern formation make use of computer models to simulate a wide range of patterns.\n\nEarly Greek philosophers attempted to explain order in nature, anticipating modern concepts. Plato (c. 427 – c. 347 BC) — looking only at his work on natural patterns — argued for the existence of universals. He considered these to consist of ideal forms ( \"eidos\": \"form\") of which physical objects are never more than imperfect copies. Thus, a flower may be roughly circular, but it is never a perfect mathematical circle. Pythagoras explained patterns in nature like the harmonies of music as arising from number, which he took to be the basic constituent of existence. Empedocles to an extent anticipated Darwin's evolutionary explanation for the structures of organisms.\n\nIn 1202, Leonardo Fibonacci (c. 1170 – c. 1250) introduced the Fibonacci number sequence to the western world with his book \"Liber Abaci\". Fibonacci gave an (unrealistic) biological example, on the growth in numbers of a theoretical rabbit population.\n\nIn 1658, the English physician and philosopher Sir Thomas Browne discussed \"how Nature Geometrizeth\" in \"The Garden of Cyrus\", citing Pythagorean numerology involving the number 5, and the Platonic form of the quincunx pattern. The discourse's central chapter features examples and observations of the quincunx in botany.\n\nIn 1917, D'Arcy Wentworth Thompson (1860–1948) published his book \"On Growth and Form\". His description of phyllotaxis and the Fibonacci sequence, the mathematical relationships in the spiral growth patterns of plants, is classic. He showed that simple equations could describe all the apparently complex spiral growth patterns of animal horns and mollusc shells.\n\nThe Belgian physicist Joseph Plateau (1801–1883) formulated the mathematical problem of the existence of a minimal surface with a given boundary, which is now named after him. He studied soap films intensively, formulating Plateau's laws which describe the structures formed by films in foams.\n\nThe German psychologist Adolf Zeising (1810–1876) claimed that the golden ratio was expressed in the arrangement of plant parts, in the skeletons of animals and the branching patterns of their veins and nerves, as well as in the geometry of crystals.\n\nErnst Haeckel (1834–1919) painted beautiful illustrations of marine organisms, in particular Radiolaria, emphasising their symmetry to support his faux-Darwinian theories of evolution.\n\nThe American photographer Wilson Bentley (1865–1931) took the first micrograph of a snowflake in 1885.\nIn 1952, Alan Turing (1912–1954), better known for his work on computing and codebreaking, wrote \"The Chemical Basis of Morphogenesis\", an analysis of the mechanisms that would be needed to create patterns in living organisms, in the process called morphogenesis. He predicted oscillating chemical reactions, in particular the Belousov–Zhabotinsky reaction. These activator-inhibitor mechanisms can, Turing suggested, generate patterns (dubbed \"Turing patterns\") of stripes and spots in animals, and contribute to the spiral patterns seen in plant phyllotaxis.\n\nIn 1968, the Hungarian theoretical biologist Aristid Lindenmayer (1925–1989) developed the L-system, a formal grammar which can be used to model plant growth patterns in the style of fractals. L-systems have an alphabet of symbols that can be combined using production rules to build larger strings of symbols, and a mechanism for translating the generated strings into geometric structures. In 1975, after centuries of slow development of the mathematics of patterns by Gottfried Leibniz, Georg Cantor, Helge von Koch, Wacław Sierpiński and others, Benoît Mandelbrot wrote a famous paper, \"How Long Is the Coast of Britain? Statistical Self-Similarity and Fractional Dimension\", crystallising mathematical thought into the concept of the fractal.\n\nLiving things like orchids, hummingbirds, and the peacock's tail have abstract designs with a beauty of form, pattern and colour that artists struggle to match. The beauty that people perceive in nature has causes at different levels, notably in the mathematics that governs what patterns can physically form, and among living things in the effects of natural selection, that govern how patterns evolve.\n\nMathematics seeks to discover and explain abstract patterns or regularities of all kinds.\nVisual patterns in nature find explanations in chaos theory, fractals, logarithmic spirals, topology and other mathematical patterns. For example, L-systems form convincing models of different patterns of tree growth.\nThe laws of physics apply the abstractions of mathematics to the real world, often as if it were perfect. For example, a crystal is perfect when it has no structural defects such as dislocations and is fully symmetric. Exact mathematical perfection can only approximate real objects. Visible patterns in nature are governed by physical laws; for example, meanders can be explained using fluid dynamics.\n\nIn biology, natural selection can cause the development of patterns in living things for several reasons, including camouflage, sexual selection, and different kinds of signalling, including mimicry and cleaning symbiosis. In plants, the shapes, colours, and patterns of insect-pollinated flowers like the lily have evolved to attract insects such as bees. Radial patterns of colours and stripes, some visible only in ultraviolet light serve as nectar guides that can be seen at a distance.\n\nSymmetry is pervasive in living things. Animals mainly have bilateral or mirror symmetry, as do the leaves of plants and some flowers such as orchids. Plants often have radial or rotational symmetry, as do many flowers and some groups of animals such as sea anemones. Fivefold symmetry is found in the echinoderms, the group that includes starfish, sea urchins, and sea lilies.\n\nAmong non-living things, snowflakes have striking sixfold symmetry; each flake's structure forms a record of the varying conditions during its crystallization, with nearly the same pattern of growth on each of its six arms. Crystals in general have a variety of symmetries and crystal habits; they can be cubic or octahedral, but true crystals cannot have fivefold symmetry (unlike quasicrystals). Rotational symmetry is found at different scales among non-living things, including the crown-shaped splash pattern formed when a drop falls into a pond, and both the spheroidal shape and rings of a planet like Saturn.\n\nSymmetry has a variety of causes. Radial symmetry suits organisms like sea anemones whose adults do not move: food and threats may arrive from any direction. But animals that move in one direction necessarily have upper and lower sides, head and tail ends, and therefore a left and a right. The head becomes specialised with a mouth and sense organs (cephalisation), and the body becomes bilaterally symmetric (though internal organs need not be). More puzzling is the reason for the fivefold (pentaradiate) symmetry of the echinoderms. Early echinoderms were bilaterally symmetrical, as their larvae still are. Sumrall and Wray argue that the loss of the old symmetry had both developmental and ecological causes.\n\nFractals are infinitely self-similar, iterated mathematical constructs having fractal dimension. Infinite iteration is not possible in nature so all 'fractal' patterns are only approximate. For example, the leaves of ferns and umbellifers (Apiaceae) are only self-similar (pinnate) to 2, 3 or 4 levels. Fern-like growth patterns occur in plants and in animals including bryozoa, corals, hydrozoa like the air fern, \"Sertularia argentea\", and in non-living things, notably electrical discharges. Lindenmayer system fractals can model different patterns of tree growth by varying a small number of parameters including branching angle, distance between nodes or branch points (internode length), and number of branches per branch point.\n\nFractal-like patterns occur widely in nature, in phenomena as diverse as clouds, river networks, geologic fault lines, mountains, coastlines, animal coloration, snow flakes, crystals, blood vessel branching, actin cytoskeleton, and ocean waves.\n\nSpirals are common in plants and in some animals, notably molluscs. For example, in the nautilus, a cephalopod mollusc, each chamber of its shell is an approximate copy of the next one, scaled by a constant factor and arranged in a logarithmic spiral. Given a modern understanding of fractals, a growth spiral can be seen as a special case of self-similarity.\n\nPlant spirals can be seen in phyllotaxis, the arrangement of leaves on a stem, and in the arrangement (parastichy) of other parts as in composite flower heads and seed heads like the sunflower or fruit structures like the pineapple and snake fruit, as well as in the pattern of scales in pine cones, where multiple spirals run both clockwise and anticlockwise. These arrangements have explanations at different levels – mathematics, physics, chemistry, biology – each individually correct, but all necessary together. Phyllotaxis spirals can be generated mathematically from Fibonacci ratios: the Fibonacci sequence runs 1, 1, 2, 3, 5, 8, 13... (each subsequent number being the sum of the two preceding ones). For example, when leaves alternate up a stem, one rotation of the spiral touches two leaves, so the pattern or ratio is 1/2. In hazel the ratio is 1/3; in apricot it is 2/5; in pear it is 3/8; in almond it is 5/13. In disc phyllotaxis as in the sunflower and daisy, the florets are arranged in Fermat's spiral with Fibonacci numbering, at least when the flowerhead is mature so all the elements are the same size. Fibonacci ratios approximate the golden angle, 137.508°, which governs the curvature of Fermat's spiral.\n\nFrom the point of view of physics, spirals are lowest-energy configurations which emerge spontaneously through self-organizing processes in dynamic systems. From the point of view of chemistry, a spiral can be generated by a reaction-diffusion process, involving both activation and inhibition. Phyllotaxis is controlled by proteins that manipulate the concentration of the plant hormone auxin, which activates meristem growth, alongside other mechanisms to control the relative angle of buds around the stem. From a biological perspective, arranging leaves as far apart as possible in any given space is favoured by natural selection as it maximises access to resources, especially sunlight for photosynthesis.\n\nIn mathematics, a dynamical system is chaotic if it is (highly) sensitive to initial conditions (the so-called \"butterfly effect\"), which requires the mathematical properties of topological mixing and dense periodic orbits.\n\nAlongside fractals, chaos theory ranks as an essentially universal influence on patterns in nature. There is a relationship between chaos and fractals—the \"strange attractors\" in chaotic systems have a fractal dimension. Some cellular automata, simple sets of mathematical rules that generate patterns, have chaotic behaviour, notably Stephen Wolfram's Rule 30.\n\nVortex streets are zigzagging patterns of whirling vortices created by the unsteady separation of flow of a fluid, most often air or water, over obstructing objects. Smooth (laminar) flow starts to break up when the size of the obstruction or the velocity of the flow become large enough compared to the viscosity of the fluid.\n\nMeanders are sinuous bends in rivers or other channels, which form as a fluid, most often water, flows around bends. As soon as the path is slightly curved, the size and curvature of each loop increases as helical flow drags material like sand and gravel across the river to the inside of the bend. The outside of the loop is left clean and unprotected, so erosion accelerates, further increasing the meandering in a powerful positive feedback loop.\n\nWaves are disturbances that carry energy as they move. Mechanical waves propagate through a medium – air or water, making it oscillate as they pass by. Wind waves are sea surface waves that create the characteristic chaotic pattern of any large body of water, though their statistical behaviour can be predicted with wind wave models. As waves in water or wind pass over sand, they create patterns of ripples. When winds blow over large bodies of sand, they create dunes, sometimes in extensive dune fields as in the Taklamakan desert. Dunes may form a range of patterns including crescents, very long straight lines, stars, domes, parabolas, and longitudinal or seif ('sword') shapes.\n\nBarchans or crescent dunes are produced by wind acting on desert sand; the two horns of the crescent and the slip face point downwind. Sand blows over the upwind face, which stands at about 15 degrees from the horizontal, and falls onto the slip face, where it accumulates up to the angle of repose of the sand, which is about 35 degrees. When the slip face exceeds the angle of repose, the sand avalanches, which is a nonlinear behaviour: the addition of many small amounts of sand causes nothing much to happen, but then the addition of a further small amount suddenly causes a large amount to avalanche. Apart from this nonlinearity, barchans behave rather like solitary waves.\n\nA soap bubble forms a sphere, a surface with minimal area — the smallest possible surface area for the volume enclosed. Two bubbles together form a more complex shape: the outer surfaces of both bubbles are spherical; these surfaces are joined by a third spherical surface as the smaller bubble bulges slightly into the larger one.\n\nA foam is a mass of bubbles; foams of different materials occur in nature. Foams composed of soap films obey Plateau's laws, which require three soap films to meet at each edge at 120° and four soap edges to meet at each vertex at the tetrahedral angle of about 109.5°. Plateau's laws further require films to be smooth and continuous, and to have a constant average curvature at every point. For example, a film may remain nearly flat on average by being curved up in one direction (say, left to right) while being curved downwards in another direction (say, front to back). Structures with minimal surfaces can be used as tents. Lord Kelvin identified the problem of the most efficient way to pack cells of equal volume as a foam in 1887; his solution uses just one solid, the bitruncated cubic honeycomb with very slightly curved faces to meet Plateau's laws. No better solution was found until 1993 when Denis Weaire and Robert Phelan proposed the Weaire–Phelan structure; the Beijing National Aquatics Center adapted the structure for their outer wall in the 2008 Summer Olympics.\n\nAt the scale of living cells, foam patterns are common; radiolarians, sponge spicules, silicoflagellate exoskeletons and the calcite skeleton of a sea urchin, \"Cidaris rugosa\", all resemble mineral casts of Plateau foam boundaries. The skeleton of the Radiolarian, \"Aulonia hexagona\", a beautiful marine form drawn by Ernst Haeckel, looks as if it is a sphere composed wholly of hexagons, but this is mathematically impossible. The Euler characteristic states that for any convex polyhedron, the number of faces plus the number of vertices (corners) equals the number of edges plus two. A result of this formula is that any closed polyhedron of hexagons has to include exactly 12 pentagons, like a soccer ball, Buckminster Fuller geodesic dome, or fullerene molecule. This can be visualised by noting that a mesh of hexagons is flat like a sheet of chicken wire, but each pentagon that is added forces the mesh to bend (there are fewer corners, so the mesh is pulled in).\n\nTessellations are patterns formed by repeating tiles all over a flat surface. There are 17 wallpaper groups of tilings. While common in art and design, exactly repeating tilings are less easy to find in living things. The cells in the paper nests of social wasps, and the wax cells in honeycomb built by honey bees are well-known examples. Among animals, bony fish, reptiles or the pangolin, or fruits like the salak are protected by overlapping scales or osteoderms, these form more-or-less exactly repeating units, though often the scales in fact vary continuously in size. Among flowers, the snake's head fritillary, \"Fritillaria meleagris\", have a tessellated chequerboard pattern on their petals. The structures of minerals provide good examples of regularly repeating three-dimensional arrays. Despite the hundreds of thousands of known minerals, there are rather few possible types of arrangement of atoms in a crystal, defined by crystal structure, crystal system, and point group; for example, there are exactly 14 Bravais lattices for the 7 lattice systems in three-dimensional space.\n\nCracks are linear openings that form in materials to relieve stress. When an elastic material stretches or shrinks uniformly, it eventually reaches its breaking strength and then fails suddenly in all directions, creating cracks with 120 degree joints, so three cracks meet at a node. Conversely, when an inelastic material fails, straight cracks form to relieve the stress. Further stress in the same direction would then simply open the existing cracks; stress at right angles can create new cracks, at 90 degrees to the old ones. Thus the pattern of cracks indicates whether the material is elastic or not. In a tough fibrous material like oak tree bark, cracks form to relieve stress as usual, but they do not grow long as their growth is interrupted by bundles of strong elastic fibres. Since each species of tree has its own structure at the levels of cell and of molecules, each has its own pattern of splitting in its bark.\n\nLeopards and ladybirds are spotted; angelfish and zebras are striped. These patterns have an evolutionary explanation: they have functions which increase the chances that the offspring of the patterned animal will survive to reproduce. One function of animal patterns is camouflage; for instance, a leopard that is harder to see catches more prey. Another function is signalling — for instance, a ladybird is less likely to be attacked by predatory birds that hunt by sight, if it has bold warning colours, and is also distastefully bitter or poisonous, or mimics other distasteful insects. A young bird may see a warning patterned insect like a ladybird and try to eat it, but it will only do this once; very soon it will spit out the bitter insect; the other ladybirds in the area will remain undisturbed. The young leopards and ladybirds, inheriting genes that somehow create spottedness, survive. But while these evolutionary and functional arguments explain why these animals need their patterns, they do not explain how the patterns are formed.\n\nAlan Turing, and later the mathematical biologist James Murray, described a mechanism that spontaneously creates spotted or striped patterns: a reaction-diffusion system. The cells of a young organism have genes that can be switched on by a chemical signal, a morphogen, resulting in the growth of a certain type of structure, say a darkly pigmented patch of skin. If the morphogen is present everywhere, the result is an even pigmentation, as in a black leopard. But if it is unevenly distributed, spots or stripes can result. Turing suggested that there could be feedback control of the production of the morphogen itself. This could cause continuous fluctuations in the amount of morphogen as it diffused around the body. A second mechanism is needed to create standing wave patterns (to result in spots or stripes): an inhibitor chemical that switches off production of the morphogen, and that itself diffuses through the body more quickly than the morphogen, resulting in an activator-inhibitor scheme. The Belousov–Zhabotinsky reaction is a non-biological example of this kind of scheme, a chemical oscillator.\n\nLater research has managed to create convincing models of patterns as diverse as zebra stripes, giraffe blotches, jaguar spots (medium-dark patches surrounded by dark broken rings) and ladybird shell patterns (different geometrical layouts of spots and stripes, see illustrations). Richard Prum's activation-inhibition models, developed from Turing's work, use six variables to account for the observed range of nine basic within-feather pigmentation patterns, from the simplest, a central pigment patch, via concentric patches, bars, chevrons, eye spot, pair of central spots, rows of paired spots and an array of dots. More elaborate models simulate complex feather patterns in the guineafowl \"Numida meleagris\" in which the individual feathers feature transitions from bars at the base to an array of dots at the far (distal) end. These require an oscillation created by two inhibiting signals, with interactions in both space and time.\n\nPatterns can form for other reasons in the vegetated landscape of tiger bush and fir waves. Tiger bush stripes occur on arid slopes where plant growth is limited by rainfall. Each roughly horizontal stripe of vegetation effectively collects the rainwater from the bare zone immediately above it. Fir waves occur in forests on mountain slopes after wind disturbance, during regeneration. When trees fall, the trees that they had sheltered become exposed and are in turn more likely to be damaged, so gaps tend to expand downwind. Meanwhile, on the windward side, young trees grow, protected by the wind shadow of the remaining tall trees. Natural patterns are sometimes formed by animals, as in the Mima mounds of the Northwestern United States and some other areas, which appear to be created over many years by the burrowing activities of pocket gophers, while the so-called fairy circles of Namibia appear to be created by the interaction of competing groups of sand termites, along with competition for water among the desert plants.\n\nIn permafrost soils with an active upper layer subject to annual freeze and thaw, patterned ground can form, creating circles, nets, ice wedge polygons, steps, and stripes. Thermal contraction causes shrinkage cracks to form; in a thaw, water fills the cracks, expanding to form ice when next frozen, and widening the cracks into wedges. These cracks may join up to form polygons and other shapes.\n\nThe fissured pattern that develops on vertebrate brains are caused by a physical process of constrained expansion dependent on two geometric parameters: relative tangential cortical expansion and relative thickness of the cortex. Similar patterns of gyri (peaks) and sulci (troughs) have been demonstrated in models of the brain starting from smooth, layered gels, with the patterns caused by compressive mechanical forces resulting from the expansion of the outer layer (representing the cortex) after the addition of a solvent. Numerical models in computer simulations support natural and experimental observations that the surface folding patterns increase in larger brains.\n\n\n\n\n\n"}
{"id": "58664232", "url": "https://en.wikipedia.org/wiki?curid=58664232", "title": "Phakalane power station", "text": "Phakalane power station\n\nPhakalane Power Station is a photovoltaic pilot power plant located in Phakalane, Botswana. The power station was funded through a Japanese grant which was part of Prime Minister Hatoyama's initiative strategy called Cool Earth Partnership aimed at supporting developing countries in their efforts to combat global warming. The Cool Earth Partnership is part of the initiatives which saw Hatoyama win the Sustainable Development Leadership Award in 2010.\n\n"}
{"id": "3759820", "url": "https://en.wikipedia.org/wiki?curid=3759820", "title": "Physis", "text": "Physis\n\nPhysis (Greek: \"phusis\") is a Greek theological, philosophical, and scientific term usually translated into English as \"nature\".\n\nThe term is central to Greek philosophy, and as a consequence to Western philosophy as a whole.\nIn pre-Socratic usage, \"phusis\" contrasted with \"nomos\" \"law, human convention\"\nSince Aristotle, however, the \"physical\" (the subject matter of \"physics\", properly \"natural things\") has more typically been juxtaposed to the \"metaphysical\".\n\nThe word φύσις is a verbal noun based on φύω \"to grow, to appear\" (cognate with English \"to be\"). In Homeric Greek it is used quite literally, of the manner of growth of a particular species of plant. \n\nIn pre-Socratic philosophy, beginning with Heraclitus, \"phusis\" in keeping with its etymology of \"growing, becoming\" is always used in the sense of the \"natural\" \"development\", although the focus might lie either with the origin, or the process, or the end result of the process. There is some evidence that by the 6th century BC, beginning with the Ionian School, the word could also be used \nin the comprehensive sense, as referring to \"\"all\" things\", as it were \"Nature\" in the sense of \"Universe\".\n\nIn the Sophist tradition, the term stood in opposition to \"nomos\" (), \"law\" or \"custom\", in the debate on which parts of human existence are natural, and which are due to convention. \nThe contrast of \"phisis\" vs. \"nomos\" could be applied to any subject, much like the modern contrast of \"nature vs. nurture\".\n\nIn book 10 of \"Laws\", Plato criticizes those who write works \"peri phuseōs\". The criticism is that such authors tend to focus on a purely \"naturalistic\" explanation of the world, ignoring the role of \"intention\" or \"technē\", and thus becoming prone to the error of naive atheism. Plato accuses even Hesiod of this, for the reason that the gods in Hesiod \"grow\" out of primordial entities after the physical universe had been established.\n\n\"Because those who use the term mean to say that nature is the first creative power; but if the soul turns out to be the primeval element, and not fire or air, then in the truest sense and beyond other things the soul may be said to exist \"by\" nature; and this would be true if you proved that the soul is older than the body, but not otherwise.\"\n\nAristotle sought out the definition of \"physis\" to prove that there was more than one definition of \"physis\", and more than one way to interpret nature. \"Though Aristotle retains the ancient sense of \"physis\" as growth, he insists that an adequate definition of \"physis\" requires the different perspectives of the four causes (aitia): material, efficient, formal, and final.\" Aristotle believed that nature itself contained its own source of matter (material), power/motion (efficiency), form, and end (final). A unique feature about Aristotle's definition of \"physis\" was his relationship between art and nature. Aristotle said that \"physis\" (nature) is dependent on techne (art). \"The critical distinction between art and nature concerns their different efficient causes: nature is its own source of motion, whereas techne always requires a source of motion outside itself.\" What Aristotle was trying to bring to light, was that art does not contain within itself its form or source of motion. Consider the process of an acorn becoming an oak tree. This is a natural process that has its own driving force behind it. There is no external force pushing this acorn to its final state, rather it is progressively developing towards one specific end (telos).\nThough φύσις was often used in Hellenistic philosophy, it is used only 14 times in the New Testament (10 of those in the writings of Paul). Its meaning varies throughout Paul's writings. One usage refers to the established or natural order of things, as in \"Romans 2:14\" where Paul writes \"For when Gentiles, who do not have the law, by \"nature\" do what the law requires, they are a law to themselves, even though they do not have the law.\" Another use of φύσις in the sense of \"natural order\" is \"Romans 1:26\" where he writes \"the men likewise gave up \"natural\" relations with women and were consumed with passion for one another\". In \"1 Corinthians 11:14\", Paul asks \"Does not nature itself teach you that if a man wears long hair it is a disgrace for him?\"\n\nThis use of φύσις as referring to a \"natural order\" in \"Romans 1:26\" and \"1 Corinthians 11:14\" may have been influenced by Stoicism. The Greek philosophers, including Aristotle and the Stoics are credited with distinguishing between man-made laws and a natural law of universal validity, but Gerhard Kittel states that the Stoic philosophers were not able to combine the concepts of νόμος (law) and φύσις (nature) to produce the concept of \"natural law\" in the sense that was made possible by Judeo-Christian theology.\n\nAs part of the Pauline theology of salvation by grace, Paul writes in \"Ephesians 2:3\" that \"we all once lived in the passions of our flesh, carrying out the desires of the body and the mind, and were by \"nature\" children of wrath, like the rest of mankind. In the next verse he writes, \"by grace you have been saved.\" \n\nTheologians of the early Christian period differed in the usage of this term. In Antiochene circles, it connoted the humanity or divinity of Christ conceived as a concrete set of characteristics or attributes. In Alexandrine thinking, it meant a concrete individual or independent existent and approximated to hypostasis without being a synonym. While it refers to much the same thing as ousia it is more empirical and descriptive focussing on function while ousia is metaphysical and focuses more on reality. Although found in the context of the Trinitarian debate, it is chiefly important in the Christology of Cyril of Alexandria.\n\nThe Greek adjective \"phusikos\" is represented in various forms in modern English:\nAs \"physics\" \"the study of nature\", as \"physical\" (via Middle Latin \"physicalis\") referring both to physics (the study of nature, the material universe) and to the human body. The term physiology (\"physiologia\") is of 16th-century coinage (Jean Fernel). The term \"physique\", for \"the bodily constitution of a person\", is a 19th-century loan from French. \n\nIn medicine the suffix \"-physis\" occurs in such compounds as \"symphysis\", \"epiphysis\", and a few others, in the sense of \"a growth\". The physis also refers to the \"growth plate\", or site of growth at the end of long bones.\n\n"}
{"id": "8994982", "url": "https://en.wikipedia.org/wiki?curid=8994982", "title": "Relational space", "text": "Relational space\n\nThe relational theory of space is a metaphysical theory according to which space is composed of relations between objects, with the implication that it cannot exist in the absence of matter. Its opposite is the container theory. A relativistic physical theory implies a relational metaphysics, but not the other way round: even if space is composed of nothing but relations between observers and events, it would be conceptually possible for all observers to agree on their measurements, whereas relativity implies they will disagree. Newtonian physics can be cast in relational terms, but Newton insisted, for philosophical reasons, on absolute (container) space. The subject was famously debated by Gottfried Wilhelm Leibniz and a supporter of Newton's in the Leibniz–Clarke correspondence.\n\nAn absolute approach can also be applied to time, with, for instance, the implication that there might have been vast epochs of time before the first event.\n\n"}
{"id": "27667", "url": "https://en.wikipedia.org/wiki?curid=27667", "title": "Space", "text": "Space\n\nSpace is the boundless three-dimensional extent in which objects and events have relative position and direction. Physical space is often conceived in three linear dimensions, although modern physicists usually consider it, with time, to be part of a boundless four-dimensional continuum known as spacetime. The concept of space is considered to be of fundamental importance to an understanding of the physical universe. However, disagreement continues between philosophers over whether it is itself an entity, a relationship between entities, or part of a conceptual framework.\n\nDebates concerning the nature, essence and the mode of existence of space date back to antiquity; namely, to treatises like the \"Timaeus\" of Plato, or Socrates in his reflections on what the Greeks called \"khôra\" (i.e. \"space\"), or in the \"Physics\" of Aristotle (Book IV, Delta) in the definition of \"topos\" (i.e. place), or in the later \"geometrical conception of place\" as \"space \"qua\" extension\" in the \"Discourse on Place\" (\"Qawl fi al-Makan\") of the 11th-century Arab polymath Alhazen. Many of these classical philosophical questions were discussed in the Renaissance and then reformulated in the 17th century, particularly during the early development of classical mechanics. In Isaac Newton's view, space was absolute—in the sense that it existed permanently and independently of whether there was any matter in the space. Other natural philosophers, notably Gottfried Leibniz, thought instead that space was in fact a collection of relations between objects, given by their distance and direction from one another. In the 18th century, the philosopher and theologian George Berkeley attempted to refute the \"visibility of spatial depth\" in his \"Essay Towards a New Theory of Vision\". Later, the metaphysician Immanuel Kant said that the concepts of space and time are not empirical ones derived from experiences of the outside world—they are elements of an already given systematic framework that humans possess and use to structure all experiences. Kant referred to the experience of \"space\" in his \"Critique of Pure Reason\" as being a subjective \"pure \"a priori\" form of intuition\".\n\nIn the 19th and 20th centuries mathematicians began to examine geometries that are non-Euclidean, in which space is conceived as \"curved\", rather than \"flat\". According to Albert Einstein's theory of general relativity, space around gravitational fields deviates from Euclidean space. Experimental tests of general relativity have confirmed that non-Euclidean geometries provide a better model for the shape of space.\n\nGalilean and Cartesian theories about space, matter and motion are at the foundation of the Scientific Revolution, which is understood to have culminated with the publication of Newton's \"Principia\" in 1687. Newton's theories about space and time helped him explain the movement of objects. While his theory of space is considered the most influential in Physics, it emerged from his predecessors' ideas about the same.\n\nAs one of the pioneers of modern science, Galilei revised the established Aristotelian and Ptolemaic ideas about a geocentric cosmos. He backed the Copernican theory that the universe was heliocentric, with a stationary sun at the center and the planets—including the Earth—revolving around the sun. If the Earth moved, the Aristotelian belief that its natural tendency was to remain at rest was in question. Galilei wanted to prove instead that the sun moved around its axis, that motion was as natural to an object as the state of rest. In other words, for Galilei, celestial bodies, including the Earth, were naturally inclined to move in circles. This view displaced another Aristotelian idea—that all objects gravitated towards their designated natural place-of-belonging.\n\nDescartes set out to replace the Aristotelian worldview with a theory about space and motion as determined by natural laws. In other words, he sought a metaphysical foundation or a mechanical explanation for his theories about matter and motion. Cartesian space was Euclidean in structure—infinite, uniform and flat. It was defined as that which contained matter; conversely, matter by definition had a spatial extension so that there was no such thing as empty space.\n\nThe Cartesian notion of space is closely linked to his theories about the nature of the body, mind and matter. He is famously known for his \"cogito ergo sum\" (I think therefore I am), or the idea that we can only be certain of the fact that we can doubt, and therefore think and therefore exist. His theories belong to the rationalist tradition, which attributes knowledge about the world to our ability to think rather than to our experiences, as the empiricists believe. He posited a clear distinction between the body and mind, which is referred to as the Cartesian dualism.\n\nFollowing Galilei and Descartes, during the seventeenth century the philosophy of space and time revolved around the ideas of Gottfried Leibniz, a German philosopher-mathematician, and Isaac Newton, who set out two opposing theories of what space is. Rather than being an entity that independently exists over and above other matter, Leibniz held that space is no more than the collection of spatial relations between objects in the world: \"space is that which results from places taken together\". Unoccupied regions are those that \"could\" have objects in them, and thus spatial relations with other places. For Leibniz, then, space was an idealised abstraction from the relations between individual entities or their possible locations and therefore could not be continuous but must be discrete.\nSpace could be thought of in a similar way to the relations between family members. Although people in the family are related to one another, the relations do not exist independently of the people.\nLeibniz argued that space could not exist independently of objects in the world because that implies a difference between two universes exactly alike except for the location of the material world in each universe. But since there would be no observational way of telling these universes apart then, according to the identity of indiscernibles, there would be no real difference between them. According to the principle of sufficient reason, any theory of space that implied that there could be these two possible universes must therefore be wrong.\nNewton took space to be more than relations between material objects and based his position on observation and experimentation. For a relationist there can be no real difference between inertial motion, in which the object travels with constant velocity, and non-inertial motion, in which the velocity changes with time, since all spatial measurements are relative to other objects and their motions. But Newton argued that since non-inertial motion generates forces, it must be absolute. He used the example of water in a spinning bucket to demonstrate his argument. Water in a bucket is hung from a rope and set to spin, starts with a flat surface. After a while, as the bucket continues to spin, the surface of the water becomes concave. If the bucket's spinning is stopped then the surface of the water remains concave as it continues to spin. The concave surface is therefore apparently not the result of relative motion between the bucket and the water. Instead, Newton argued, it must be a result of non-inertial motion relative to space itself. For several centuries the bucket argument was considered decisive in showing that space must exist independently of matter.\n\nIn the eighteenth century the German philosopher Immanuel Kant developed a theory of knowledge in which knowledge about space can be both \"a priori\" and \"synthetic\". According to Kant, knowledge about space is \"synthetic\", in that statements about space are not simply true by virtue of the meaning of the words in the statement. In his work, Kant rejected the view that space must be either a substance or relation. Instead he came to the conclusion that space and time are not discovered by humans to be objective features of the world, but imposed by us as part of a framework for organizing experience.\n\nEuclid's \"Elements\" contained five postulates that form the basis for Euclidean geometry. One of these, the parallel postulate, has been the subject of debate among mathematicians for many centuries. It states that on any plane on which there is a straight line \"L\" and a point \"P\" not on \"L\", there is exactly one straight line \"L\" on the plane that passes through the point \"P\" and is parallel to the straight line \"L\". Until the 19th century, few doubted the truth of the postulate; instead debate centered over whether it was necessary as an axiom, or whether it was a theory that could be derived from the other axioms. Around 1830 though, the Hungarian János Bolyai and the Russian Nikolai Ivanovich Lobachevsky separately published treatises on a type of geometry that does not include the parallel postulate, called hyperbolic geometry. In this geometry, an infinite number of parallel lines pass through the point \"P\". Consequently, the sum of angles in a triangle is less than 180° and the ratio of a circle's circumference to its diameter is greater than pi. In the 1850s, Bernhard Riemann developed an equivalent theory of elliptical geometry, in which no parallel lines pass through \"P\". In this geometry, triangles have more than 180° and circles have a ratio of circumference-to-diameter that is less than pi.\n\nAlthough there was a prevailing Kantian consensus at the time, once non-Euclidean geometries had been formalised, some began to wonder whether or not physical space is curved. Carl Friedrich Gauss, a German mathematician, was the first to consider an empirical investigation of the geometrical structure of space. He thought of making a test of the sum of the angles of an enormous stellar triangle, and there are reports that he actually carried out a test, on a small scale, by triangulating mountain tops in Germany.\n\nHenri Poincaré, a French mathematician and physicist of the late 19th century, introduced an important insight in which he attempted to demonstrate the futility of any attempt to discover which geometry applies to space by experiment. He considered the predicament that would face scientists if they were confined to the surface of an imaginary large sphere with particular properties, known as a sphere-world. In this world, the temperature is taken to vary in such a way that all objects expand and contract in similar proportions in different places on the sphere. With a suitable falloff in temperature, if the scientists try to use measuring rods to determine the sum of the angles in a triangle, they can be deceived into thinking that they inhabit a plane, rather than a spherical surface. In fact, the scientists cannot in principle determine whether they inhabit a plane or sphere and, Poincaré argued, the same is true for the debate over whether real space is Euclidean or not. For him, which geometry was used to describe space was a matter of convention. Since Euclidean geometry is simpler than non-Euclidean geometry, he assumed the former would always be used to describe the 'true' geometry of the world.\n\nIn 1905, Albert Einstein published his special theory of relativity, which led to the concept that space and time can be viewed as a single construct known as \"spacetime\". In this theory, the speed of light in a vacuum is the same for all observers—which has the result that two events that appear simultaneous to one particular observer will not be simultaneous to another observer if the observers are moving with respect to one another. Moreover, an observer will measure a moving clock to tick more slowly than one that is stationary with respect to them; and objects are measured to be shortened in the direction that they are moving with respect to the observer.\n\nSubsequently, Einstein worked on a general theory of relativity, which is a theory of how gravity interacts with spacetime. Instead of viewing gravity as a force field acting in spacetime, Einstein suggested that it modifies the geometric structure of spacetime itself. According to the general theory, time goes more slowly at places with lower gravitational potentials and rays of light bend in the presence of a gravitational field. Scientists have studied the behaviour of binary pulsars, confirming the predictions of Einstein's theories, and non-Euclidean geometry is usually used to describe spacetime.\n\nIn modern mathematics spaces are defined as sets with some added structure. They are frequently described as different types of manifolds, which are spaces that locally approximate to Euclidean space, and where the properties are defined largely on local connectedness of points that lie on the manifold. There are however, many diverse mathematical objects that are called spaces. For example, vector spaces such as function spaces may have infinite numbers of independent dimensions and a notion of distance very different from Euclidean space, and topological spaces replace the concept of distance with a more abstract idea of nearness.\n\nSpace is one of the few fundamental quantities in physics, meaning that it cannot be defined via other quantities because nothing more fundamental is known at the present. On the other hand, it can be related to other fundamental quantities. Thus, similar to other fundamental quantities (like time and mass), space can be explored via measurement and experiment.\n\nToday, our three-dimensional space is viewed as embedded in a four-dimensional spacetime, called Minkowski space (see special relativity). The idea behind space-time is that time is hyperbolic-orthogonal to each of the three spatial dimensions.\n\nBefore Einstein's work on relativistic physics, time and space were viewed as independent dimensions. Einstein's discoveries showed that due to relativity of motion our space and time can be mathematically combined into one object–spacetime. It turns out that distances in space or in time separately are not invariant with respect to Lorentz coordinate transformations, but distances in Minkowski space-time along space-time intervals are—which justifies the name.\n\nIn addition, time and space dimensions should not be viewed as exactly equivalent in Minkowski space-time. One can freely move in space but not in time. Thus, time and space coordinates are treated differently both in special relativity (where time is sometimes considered an imaginary coordinate) and in general relativity (where different signs are assigned to time and space components of spacetime metric).\n\nFurthermore, in Einstein's general theory of relativity, it is postulated that space-time is geometrically distorted- \"curved\" -near to gravitationally significant masses.\n\nOne consequence of this postulate, which follows from the equations of general relativity, is the prediction of moving ripples of space-time, called gravitational waves. While indirect evidence for these waves has been found (in the motions of the Hulse–Taylor binary system, for example) experiments attempting to directly measure these waves are ongoing at the LIGO and Virgo collaborations. LIGO scientists reported the first such direct observation of gravitational waves on 14 September 2015.\n\nRelativity theory leads to the cosmological question of what shape the universe is, and where space came from. It appears that space was created in the Big Bang, 13.8 billion years ago and has been expanding ever since. The overall shape of space is not known, but space is known to be expanding very rapidly due to the cosmic inflation.\n\nThe measurement of \"physical space\" has long been important. Although earlier societies had developed measuring systems, the International System of Units, (SI), is now the most common system of units used in the measuring of space, and is almost universally used.\n\nCurrently, the standard space interval, called a standard meter or simply meter, is defined as the distance traveled by light in a vacuum during a time interval of exactly 1/299,792,458 of a second. This definition coupled with present definition of the second is based on the special theory of relativity in which the speed of light plays the role of a fundamental constant of nature.\n\nGeography is the branch of science concerned with identifying and describing places on Earth, utilizing spatial awareness to try to understand why things exist in specific locations. Cartography is the mapping of spaces to allow better navigation, for visualization purposes and to act as a locational device. Geostatistics apply statistical concepts to collected spatial data of Earth to create an estimate for unobserved phenomena.\n\nGeographical space is often considered as land, and can have a relation to ownership usage (in which space is seen as property or territory). While some cultures assert the rights of the individual in terms of ownership, other cultures will identify with a communal approach to land ownership, while still other cultures such as Australian Aboriginals, rather than asserting ownership rights to land, invert the relationship and consider that they are in fact owned by the land. Spatial planning is a method of regulating the use of space at land-level, with decisions made at regional, national and international levels. Space can also impact on human and cultural behavior, being an important factor in architecture, where it will impact on the design of buildings and structures, and on farming.\n\nOwnership of space is not restricted to land. Ownership of airspace and of waters is decided internationally. Other forms of ownership have been recently asserted to other spaces—for example to the radio bands of the electromagnetic spectrum or to cyberspace.\n\nPublic space is a term used to define areas of land as collectively owned by the community, and managed in their name by delegated bodies; such spaces are open to all, while private property is the land culturally owned by an individual or company, for their own use and pleasure.\n\nAbstract space is a term used in geography to refer to a hypothetical space characterized by complete homogeneity. When modeling activity or behavior, it is a conceptual tool used to limit extraneous variables such as terrain.\n\nPsychologists first began to study the way space is perceived in the middle of the 19th century. Those now concerned with such studies regard it as a distinct branch of psychology. Psychologists analyzing the perception of space are concerned with how recognition of an object's physical appearance or its interactions are perceived, see, for example, visual space.\n\nOther, more specialized topics studied include amodal perception and object permanence. The perception of surroundings is important due to its necessary relevance to survival, especially with regards to hunting and self preservation as well as simply one's idea of personal space.\n\nSeveral space-related phobias have been identified, including agoraphobia (the fear of open spaces), astrophobia (the fear of celestial space) and claustrophobia (the fear of enclosed spaces).\n\nThe understanding of three-dimensional space in humans is thought to be learned during infancy using unconscious inference, and is closely related to hand-eye coordination. The visual ability to perceive the world in three dimensions is called depth perception.\n\nSpace has been studied in the social sciences from the perspectives of Marxism, feminism, postmodernism, postcolonialism, urban theory and critical geography. These theories account for the effect of the history of colonialism, transatlantic slavery and globalization on our understanding and experience of space and place. The topic has garnered attention since the 1980s, after the publication of Henri Lefebvre's \"The Production of Space .\" In this book, Lefebvre applies Marxist ideas about the production of commodities and accumulation of capital to discuss space as a social product. His focus is on the multiple and overlapping social processes that produce space.\n\nIn his book \"The Condition of Postmodernity,\" David Harvey describes what he terms the \"time-space compression.\" This is the effect of technological advances and capitalism on our perception of time, space and distance. Changes in the modes of production and consumption of capital affect and are affected by developments in transportation and technology. These advances create relationships across time and space, new markets and groups of wealthy elites in urban centers, all of which annihilate distances and affect our perception of linearity and distance.\n\nIn his book \"Thirdspace,\" Edward Soja describes space and spatiality as an integral and neglected aspect of what he calls the \"trialectics of being,\" the three modes that determine how we inhabit, experience and understand the world. He argues that critical theories in the Humanities and Social Sciences study the historical and social dimensions of our lived experience, neglecting the spatial dimension. He builds on Henri Lefebvre's work to address the dualistic way in which humans understand space—as either material/physical or as represented/imagined. Lefebvre's \"lived space\" and Soja's \"thridspace\" are terms that account for the complex ways in which humans understand and navigate place, which \"firstspace\" and \"Secondspace\" (Soja's terms for material and imagined spaces respectively) do not fully encompass.\n\nPostcolonial theorist Homi Bhabha's concept of Third Space is different from Soja's Thirdspace, even though both terms offer a way to think outside the terms of a binary logic. Bhabha's Third Space is the space in which hybrid cultural forms and identities exist. In his theories, the term hybrid describes new cultural forms that emerge through the interaction between colonizer and colonized.\n\n"}
{"id": "54716184", "url": "https://en.wikipedia.org/wiki?curid=54716184", "title": "United States Energy Association", "text": "United States Energy Association\n\nThe United States Energy Association (USEA) is the U.S. Member Committee of the World Energy Council. Headquartered in Washington, D.C., USEA is an association of public and private energy-related organizations, corporations, and government agencies. \nThe association hosts annual events such as the Carbon Sequestration Leadership Forum, Energy Supply Forum, and State of the Energy Industry Forum.\n\nBarry Worthington has served as USEA’s executive director since 1988. Worthington chairs the Clean Electricity Production working group within the UNECE Committee on Sustainable Energy. He also sits on numerous energy boards, including the National Energy Foundation (chairman) and Energy Law Foundation.\n\nWorthington meets with domestic and international energy leaders to discuss energy infrastructure partnerships. He often advocates for energy cultivation in developing countries, claiming there are \"few priorities greater for the world than getting people linked to the grid.\" Worthington is a firm supporter of energy \"sovereignty.\"\n\nVicky Bailey currently chairs USEA’s board of directors. She succeeded Jack Futcher, the President and COO of Bechtel.\n\nFor 25 years, USEA has been a partner with USAID, expanding energy infrastructure, improving energy access, and reducing energy poverty in developing economies through international energy partnerships. A major function of USEA’s is to help USAID expand energy infrastructure and programs in developing countries. In 2012, the association launched the U.S.-East Africa Geothermal Partnership (EAGP), a public-private partnership “offering assistance at early stages of project development in East Africa.” Through the Djibouti Geothermal Partnership, Ethiopia Geothermal Partnership, and Kenya Electricity Generating Company (KenGen), USEA partners with the Department of Energy and local governments to promote U.S. companies’ involvement in developing additional geothermal generation capacity. According to USEA, the number of U.S. companies conducting geothermal work in East Africa has more than tripled since EAGP’s inception.\n\nUSEA represents the interests of the U.S. energy sector through public education and advocacy. The association supports an “all-of-the-above energy strategy,” from renewable energy to fossil fuels. USEA advocates for the exploration and production of oil and natural gas.\n\n"}
{"id": "32172794", "url": "https://en.wikipedia.org/wiki?curid=32172794", "title": "Visual space", "text": "Visual space\n\nVisual space is the perceptual space housing the visual world being experienced by an aware observer; it is the subjective counterpart of the space of physical objects before an observer's eyes.\n\nIn object space the location and shape of physical targets can be accurately described with the tools of geometry. For practical purposes it is Euclidean. It is three-dimensional and various co-ordinate systems like the Cartesian x,y,z (with a defined origin in relation to an observer's head or eyes), or bipolar with angles of elevation, azimuth and binocular parallax (based on the separation of the two eyes) are interchangeable. No elaborate mathematics are needed.\n\nPercepts, the counterparts in the aware observer's conscious experience of objects in physical space, constitute an ordered ensemble or, as Ernst Cassirer explained, the perceptual world has a structure and is not an aggregate of scattered sensations. This visual space can be accessed by introspection, by interrogation, or by suitable experimental procedures which allow relative location as well as some structural properties to be assessed, even quantitatively.\n\nAn example illustrates the relationship between the concepts of object and visual space:\nTwo straight lines are presented to an observer who is asked to set them so that they appear parallel. When this has been done, the lines \"are\" parallel in visual space and now a comparison is feasible with the physical lines' setting in object space. Good precision can be achieved using psychophysical procedures in human observers or behavioral ones in trained animals. The reciprocal experiment is easier to perform but does not yield a numerical read-out as readily: show objectively parallel lines and make a determination of their inclination in the observer's perception.\n\nConsidering how it arises (see below), visual space seems, as an immediate, unmediated experience, to provide a remarkably true and unproblematic representation of a real world of objects.\n\nThe distinction is mandatory between what the eye professions call \"visual field\", the area or extent of physical space that is available to the eye or that is being imaged on the retina, and the virtual, perceptual \"visual space\" in which visual percepts are located, the subject of this entry.\nConfusion is caused by the use of \"\" in the German literature for both. There is no doubt that Ewald Hering and his followers meant visual space in their disquisitions.\n\nThe fundamental distinction was made by Rudolf Carnap between three kinds of space which he called \"formal\", \"physical\" and \"perceptual.\" Mathematicians, for example, deal with ordered structures, ensembles of elements for which rules of logico-deductive relationships hold, limited solely by being not self-contradictory. These are the \"formal\" spaces. According to Carnap, studying \"physical\" space means examining the relationship between empirically determined objects. Finally, there is the realm of what students of Kant know as \",\" immediate sensory experiences, often awkwardly translated as \"apperceptions,\" which belong to \"perceptual spaces.\"\n\nGeometry is the discipline devoted to the study of space and the rules relating the elements to each other. For example, in Euclidean space there is the Pythagorean theorem for distances. In a two-dimensional space of constant curvature, like the surface of a sphere, the rule is somewhat more complex but applies everywhere. On the two-dimensional surface of a football, the rule is more complex still and has different values depending on location. In well-behaved spaces such rules used for measurement and called \"Metrics,\" are classically handled by the mathematics invented by Riemann. Object space belongs to that class.\n\nTo the extent that it is reachable by scientifically acceptable probes, visual space as defined is also a candidate for such considerations. The first and remarkably prescient analysis was published by Ernst Mach in 1901. Under the heading \"On Physiological as Distinguished from Geometrical Space\" Mach states that \"Both spaces are threefold manifoldnesses\" but the former is \"...neither constituted everywhere and in all directions alike, nor infinite in extent, nor unbounded.\" A notable attempt at a rigorous formulation was made in 1947 by the highly talented mathematician Rudolf Luneburg, who preceded his essay by a profound analysis of the underlying principles. When features are sufficiently singular and distinct, there is no problem about a correspondence between an individual item \"A\" in object space and its correlate \"A' \" in visual space. Questions can be asked and answered such as \"If visual percepts \"A',B',C' \" are correlates of physical objects \"A,B,C,\" and if \"C\" lies between \"A\" and \"B\", does \"C' \" lie between \"A' \" and \"B' \"?\" In this manner, the possibility of visual space being metrical can be approached. If the exercise is successful, a great deal can be said about the nature of the mapping of the physical space on the visual space.\n\nOn the basis of fragmentary psychophysical data of previous generations, Luneburg concluded that visual space was hyperbolic with constant curvature, meaning that elements can be moved throughout the space without changing shape. One of Luneburg's major arguments is that, in accord with a common observation, the transformation involving hyperbolic space renders infinity into a dome (the sky). The Luneburg proposition gave rise to discussions and attempts at corroborating experiments, which on the whole did not favor it.\n\nBasic to the problem, and underestimated by Luneburg the mathematician, is the likely success of a mathematically viable formulation of the relationship between objects in physical space and percepts in visual space. Any scientific investigation of visual space is colored by the kind of access we have to it, and the precision, repeatability and generality of measurements. Insightful questions can be asked about the mapping of visual space to object space but answers are mostly limited in the range of their validity. If the physical setting that satisfies the criterion of, say, apparent parallelism varies from observer to observer, or from day to day, or from context to context, so does the geometrical nature of, and hence mathematical formulation for, visual space.\n\nAll these arguments notwithstanding, there is a major concordance between the locations of items in object space and their correlates in visual space. It is adequately veridical for us to navigate very effectively in the world, deviations from such a situation are sufficiently notable to warrant special consideration. visual space agnosia is a recognized neurological condition, and the many common distortions, called geometrical-optical illusions, are widely demonstrated but of minor consequence.\n\nIts founder, Gustav Theodor Fechner defined the mission of the discipline of psychophysics as the functional relationship between the mental and material worlds—in this particular case, the visual and object spaces—but he acknowledged an intermediate step, which has since blossomed into the major enterprise of modern neuroscience. In distinguishing between \"inner\" and \"outer\" psychophysics, Fechner recognized that a physical stimulus generates a percept by way of an effect on the organism's sensory and nervous systems. Hence, without denying that its essence is the arc between object and percept, the inquiry can concern itself with the neural substrate of visual space.\n\nTwo major concepts dating back to the middle of the 19th century set the parameters of the discussion here. Johannes Müller emphasized that what matters in a neural path is the connection it makes, and Hermann Lotze, from psychological considerations, enunciated the principle of local sign. Put together in modern neuroanatomical terms they mean that a nerve fiber from a fixed retinal location instructs its target neurons in the brain about the presence of a stimulus in the location in the eye's visual field that is imaged there. The orderly array of retinal locations is preserved in the passage from the retina to the brain, and provides what is aptly called a \"retinotopic\" mapping in the primary visual cortex. Thus in the first instance brain activity retains the relative spatial ordering of the objects and lays the foundations for a neural substrate of visual space. Unfortunately, as is so common in brain studies, simplicity and transparency ends here. Right at the outset, visual signals are analyzed not only for their position, but also, separately in parallel channels, for many other attributes such as brightness, color, orientation, depth. No single neuron or even neuronal center or circuit represents both the nature of a target feature and its accurate location. The unitary mapping of object space into the coherent visual space without internal contradictions or inconsistencies that we as observer automatically experience, demands concepts of conjoint activity in several parts of the nervous system that is at present beyond the reach of neurophysiological research.\n\nThough the details of the process by which the experience of visual space emerges remain opaque, a startling finding gives hope for future insights. Neural units have been demonstrated in the brain structure called hippocampus that show activity only when the animal is in a specific place in its environment.\n\nOnly on an astronomical scale are physical space and its contents interdependent, This major proposition of the general theory of relativity is of no concern in vision. For us, distances in object space are independent of the nature of the objects.\n\nBut this is not so simple in visual space. At a minim an observer judges the relative location of a few light points in an otherwise dark visual field, a simplistic extension from object space that enabled Luneburg to make some statements about the geometry of visual space. In a more richly textured visual world, the various visual percepts carry with them prior perceptual associations which often affect their relative spatial disposition. Identical separations in physical space can look quite different (\"are quite different\" in visual space) depending on the features that demarcate them. This is particularly so in the depth dimension because the apparatus by which values in the third visual dimension are assigned is fundamentally different from that for the height and width of objects.\n\nEven in monocular vision, which physiologically has only two dimensions, cues of size, perspective, relative motion etc. are used to assign depth differences to percepts. Looked at as a mathematical/geometrical problem, expanding a 2-dimensional object manifold into a 3-dimensional visual world is \"ill-posed,\" i.e., not capable of a rational solution, but is accomplished quite effectively by the human observer.\n\nThe problem becomes less ill-posed when binocular vision allows actual determination of relative depth by stereoscopy, but its linkage to the evaluation of distance in the other two dimensions is uncertain (see: stereoscopic depth rendition). Hence, the uncomplicated three-dimensional visual space of every-day experience is the product of many perceptual and cognitive layers superimposed on the physiological representation of the physical world of objects.\n"}
{"id": "34043", "url": "https://en.wikipedia.org/wiki?curid=34043", "title": "Wormhole", "text": "Wormhole\n\nA wormhole (or Einstein–Rosen bridge) is a speculative structure linking separate points in spacetime, and is based on a solution of the Einstein field equations. A wormhole can be visualized as a tunnel with two ends, each at separate points in spacetime (i.e., different locations or different points of time). More precisely it is a transcendental bijection of the spacetime continuum, an asymptotic projection of the Calabi–Yau manifold manifesting itself in Anti-de Sitter space. \n\nWormholes are consistent with the general theory of relativity, but whether wormholes actually exist remains to be seen.\n\nA wormhole could connect extremely long distances such as a billion light years or more, short distances such as a few meters, different universes, or different points in time.\n\nFor a simplified notion of a wormhole, space can be visualized as a two-dimensional (2D) surface. In this case, a wormhole would appear as a hole in that surface, lead into a 3D tube (the inside surface of a cylinder), then re-emerge at another location on the 2D surface with a hole similar to the entrance. An actual wormhole would be analogous to this, but with the spatial dimensions raised by one. For example, instead of circular holes on a 2D plane, the entry and exit points could be visualized as spheres in 3D space.\n\nAnother way to imagine wormholes is to take a sheet of paper and draw two somewhat distant points on one side of the paper. The sheet of paper represents a plane in the spacetime continuum, and the two points represent a distance to be traveled, however theoretically a wormhole could connect these two points by folding that plane so the points are touching. In this way it would be much easier to traverse the distance since the two points are now touching.\nIn 1928, Hermann Weyl proposed a wormhole hypothesis of matter in connection with mass analysis of electromagnetic field energy; however, he did not use the term \"wormhole\" (he spoke of \"one-dimensional tubes\" instead).\n\nAmerican theoretical physicist John Archibald Wheeler (inspired by Weyl's work) coined the term \"wormhole\" in a 1957 paper co-authored by Charles Misner:\n\nWormholes have been defined both \"geometrically\" and \"topologically\". From a topological point of view, an intra-universe wormhole (a wormhole between two points in the same universe) is a compact region of spacetime whose boundary is topologically trivial, but whose interior is not simply connected. Formalizing this idea leads to definitions such as the following, taken from Matt Visser's \"Lorentzian Wormholes\" (1996).\n\nGeometrically, wormholes can be described as regions of spacetime that constrain the incremental deformation of closed surfaces. For example, in Enrico Rodrigo's \"The Physics of Stargates, \"a wormhole is defined informally as: \n\nThe equations of the theory of general relativity have valid solutions that contain wormholes. The first type of wormhole solution discovered was the \"Schwarzschild wormhole\", which would be present in the Schwarzschild metric describing an \"eternal black hole\", but it was found that it would collapse too quickly for anything to cross from one end to the other. Wormholes that could be crossed in both directions, known as traversable wormholes, would only be possible if exotic matter with negative energy density could be used to stabilize them.\n\nSchwarzschild wormholes, also known as \"Einstein–Rosen bridges\" (named after Albert Einstein and Nathan Rosen), are connections between areas of space that can be modeled as vacuum solutions to the Einstein field equations, and that are now understood to be intrinsic parts of the maximally extended version of the Schwarzschild metric describing an eternal black hole with no charge and no rotation. Here, \"maximally extended\" refers to the idea that the spacetime should not have any \"edges\": it should be possible to continue this path arbitrarily far into the particle's future or past for any possible trajectory of a free-falling particle (following a geodesic in the spacetime).\n\nIn order to satisfy this requirement, it turns out that in addition to the black hole interior region that particles enter when they fall through the event horizon from the outside, there must be a separate white hole interior region that allows us to extrapolate the trajectories of particles that an outside observer sees rising up \"away\" from the event horizon. And just as there are two separate interior regions of the maximally extended spacetime, there are also two separate exterior regions, sometimes called two different \"universes\", with the second universe allowing us to extrapolate some possible particle trajectories in the two interior regions. This means that the interior black hole region can contain a mix of particles that fell in from either universe (and thus an observer who fell in from one universe might be able to see light that fell in from the other one), and likewise particles from the interior white hole region can escape into either universe. All four regions can be seen in a spacetime diagram that uses Kruskal–Szekeres coordinates.\n\nIn this spacetime, it is possible to come up with coordinate systems such that if a hypersurface of constant time (a set of points that all have the same time coordinate, such that every point on the surface has a space-like separation, giving what is called a 'space-like surface') is picked and an \"embedding diagram\" drawn depicting the curvature of space at that time, the embedding diagram will look like a tube connecting the two exterior regions, known as an \"Einstein–Rosen bridge\". Note that the Schwarzschild metric describes an idealized black hole that exists eternally from the perspective of external observers; a more realistic black hole that forms at some particular time from a collapsing star would require a different metric. When the infalling stellar matter is added to a diagram of a black hole's history, it removes the part of the diagram corresponding to the white hole interior region, along with the part of the diagram corresponding to the other universe.\n\nThe Einstein–Rosen bridge was discovered by Ludwig Flamm in 1916, a few months after Schwarzschild published his solution, and was rediscovered by Albert Einstein and his colleague Nathan Rosen, who published their result in 1935. However, in 1962, John Archibald Wheeler and Robert W. Fuller published a paper showing that this type of wormhole is unstable if it connects two parts of the same universe, and that it will pinch off too quickly for light (or any particle moving slower than light) that falls in from one exterior region to make it to the other exterior region.\n\nAccording to general relativity, the gravitational collapse of a sufficiently compact mass forms a singular Schwarzschild black hole. In the Einstein–Cartan–Sciama–Kibble theory of gravity, however, it forms a regular Einstein–Rosen bridge. This theory extends general relativity by removing a constraint of the symmetry of the affine connection and regarding its antisymmetric part, the torsion tensor, as a dynamical variable. Torsion naturally accounts for the quantum-mechanical, intrinsic angular momentum (spin) of matter. The minimal coupling between torsion and Dirac spinors generates a repulsive spin–spin interaction that is significant in fermionic matter at extremely high densities. Such an interaction prevents the formation of a gravitational singularity. Instead, the collapsing matter reaches an enormous but finite density and rebounds, forming the other side of the bridge.\n\nAlthough Schwarzschild wormholes are not traversable in both directions, their existence inspired Kip Thorne to imagine traversable wormholes created by holding the \"throat\" of a Schwarzschild wormhole open with exotic matter (material that has negative mass/energy).\n\nOther non-traversable wormholes include \"Lorentzian wormholes\" (first proposed by John Archibald Wheeler in 1957), wormholes creating a spacetime foam in a general relativistic spacetime manifold depicted by a Lorentzian manifold, and \"Euclidean wormholes\" (named after Euclidean manifold, a structure of Riemannian manifold).\n\nThis Casimir effect shows that quantum field theory allows the energy density in certain regions of space to be negative relative to the ordinary matter vacuum energy, and it has been shown theoretically that quantum field theory allows states where energy can be \"arbitrarily\" negative at a given point. Many physicists, such as Stephen Hawking, Kip Thorne, and others, therefore argue that such effects might make it possible to stabilize a traversable wormhole. Physicists have not found any natural process that would be predicted to form a wormhole naturally in the context of general relativity, although the quantum foam hypothesis is sometimes used to suggest that tiny wormholes might appear and disappear spontaneously at the Planck scale, and stable versions of such wormholes have been suggested as dark matter candidates. It has also been proposed that, if a tiny wormhole held open by a negative mass cosmic string had appeared around the time of the Big Bang, it could have been inflated to macroscopic size by cosmic inflation.\n\nLorentzian traversable wormholes would allow travel in both directions from one part of the universe to another part of that same universe very quickly or would allow travel from one universe to another. The possibility of traversable wormholes in general relativity was first demonstrated in a 1973 paper by Homer Ellis\nand independently in a 1973 paper by K. A. Bronnikov.\nEllis thoroughly analyzed the topology and the geodesics of the Ellis drainhole, showing it to be geodesically complete, horizonless, singularity-free, and fully traversable in both directions. The drainhole is a solution manifold of Einstein's field equations for a vacuum space-time, modified by inclusion of a scalar field minimally coupled to the Ricci tensor with antiorthodox polarity (negative instead of positive). (Ellis specifically rejected referring to the scalar field as 'exotic' because of the antiorthodox coupling, finding arguments for doing so unpersuasive.) The solution depends on two parameters: formula_1, which fixes the strength of its gravitational field, and formula_2, which determines the curvature of its spatial cross sections. When formula_1 is set equal to 0, the drainhole's gravitational field vanishes. What is left is the Ellis wormhole, a nongravitating, purely geometric, traversable wormhole.\nKip Thorne and his graduate student Mike Morris, unaware of the 1973 papers by Ellis and Bronnikov, manufactured, and in 1988 published, a duplicate of the Ellis wormhole for use as a tool for teaching general relativity. For this reason, the type of traversable wormhole they proposed, held open by a spherical shell of exotic matter, was from 1988 to 2015 exclusively referred to in the literature as a \"Morris–Thorne wormhole\". Later, other types of traversable wormholes were discovered as allowable solutions to the equations of general relativity, including a variety analyzed in a 1989 paper by Matt Visser, in which a path through the wormhole can be made where the traversing path does not pass through a region of exotic matter. However, in the pure Gauss–Bonnet gravity (a modification to general relativity involving extra spatial dimensions which is sometimes studied in the context of brane cosmology) exotic matter is not needed in order for wormholes to exist—they can exist even with no matter. A type held open by negative mass cosmic strings was put forth by Visser in collaboration with Cramer \"et al.\", in which it was proposed that such wormholes could have been naturally created in the early universe.\n\nWormholes connect two points in spacetime, which means that they would in principle allow travel in time, as well as in space. In 1988, Morris, Thorne and Yurtsever worked out explicitly how to convert a wormhole traversing space into one traversing time by accelerating one of its two mouths. However, according to general relativity, it would not be possible to use a wormhole to travel back to a time earlier than when the wormhole was first converted into a time 'machine'. Until this time it could not have been noticed or have been used.\n\nTo see why exotic matter is required, consider an incoming light front traveling along geodesics, which then crosses the wormhole and re-expands on the other side. The expansion goes from negative to positive. As the wormhole neck is of finite size, we would not expect caustics to develop, at least within the vicinity of the neck. According to the optical Raychaudhuri's theorem, this requires a violation of the averaged null energy condition. Quantum effects such as the Casimir effect cannot violate the averaged null energy condition in any neighborhood of space with zero curvature, but calculations in semiclassical gravity suggest that quantum effects may be able to violate this condition in curved spacetime. Although it was hoped recently that quantum effects could not violate an achronal version of the averaged null energy condition, violations have nevertheless been found, so it remains an open possibility that quantum effects might be used to support a wormhole.\n\nIn some hypotheses where general relativity is modified, it is possible to have a wormhole that does not collapse without having to resort to exotic matter. For example, this is possible with R^2 gravity, a form of f(R) gravity.\n\nThe impossibility of faster-than-light relative speed only applies locally. Wormholes might allow effective superluminal (faster-than-light) travel by ensuring that the speed of light is not exceeded locally at any time. While traveling through a wormhole, subluminal (slower-than-light) speeds are used. If two points are connected by a wormhole whose length is shorter than the distance between them \"outside\" the wormhole, the time taken to traverse it could be less than the time it would take a light beam to make the journey if it took a path through the space \"outside\" the wormhole. However, a light beam traveling through the same wormhole would of course beat the traveler.\n\nIf traversable wormholes exist, they could allow time travel. A proposed time-travel machine using a traversable wormhole would hypothetically work in the following way: One end of the wormhole is accelerated to some significant fraction of the speed of light, perhaps with some advanced propulsion system, and then brought back to the point of origin. Alternatively, another way is to take one entrance of the wormhole and move it to within the gravitational field of an object that has higher gravity than the other entrance, and then return it to a position near the other entrance. For both of these methods, time dilation causes the end of the wormhole that has been moved to have aged less, or become \"younger\", than the stationary end as seen by an external observer; however, time connects differently \"through\" the wormhole than \"outside\" it, so that synchronized clocks at either end of the wormhole will always remain synchronized as seen by an observer passing through the wormhole, no matter how the two ends move around. This means that an observer entering the \"younger\" end would exit the \"older\" end at a time when it was the same age as the \"younger\" end, effectively going back in time as seen by an observer from the outside. One significant limitation of such a time machine is that it is only possible to go as far back in time as the initial creation of the machine; It is more of a path through time rather than it is a device that itself moves through time, and it would not allow the technology itself to be moved backward in time.\n\nAccording to current theories on the nature of wormholes, construction of a traversable wormhole would require the existence of a substance with negative energy, often referred to as \"exotic matter\". More technically, the wormhole spacetime requires a distribution of energy that violates various energy conditions, such as the null energy condition along with the weak, strong, and dominant energy conditions. However, it is known that quantum effects can lead to small measurable violations of the null energy condition, and many physicists believe that the required negative energy may actually be possible due to the Casimir effect in quantum physics. Although early calculations suggested a very large amount of negative energy would be required, later calculations showed that the amount of negative energy can be made arbitrarily small.\n\nIn 1993, Matt Visser argued that the two mouths of a wormhole with such an induced clock difference could not be brought together without inducing quantum field and gravitational effects that would either make the wormhole collapse or the two mouths repel each other, or otherwise prevent information from passing through the wormhole. Because of this, the two mouths could not be brought close enough for causality violation to take place. However, in a 1997 paper, Visser hypothesized that a complex \"Roman ring\" (named after Tom Roman) configuration of an N number of wormholes arranged in a symmetric polygon could still act as a time machine, although he concludes that this is more likely a flaw in classical quantum gravity theory rather than proof that causality violation is possible.\n\nA possible resolution to the paradoxes resulting from wormhole-enabled time travel rests on the many-worlds interpretation of quantum mechanics.\n\nIn 1991 David Deutsch showed that quantum theory is fully consistent (in the sense that the so-called density matrix can be made free of discontinuities) in spacetimes with closed timelike curves. However, later it was shown that such model of closed timelike curve can have internal inconsistencies as it will lead to strange phenomena like distinguishing non-orthogonal quantum states and distinguishing proper and improper mixture. Accordingly, the destructive positive feedback loop of virtual particles circulating through a wormhole time machine, a result indicated by semi-classical calculations, is averted. A particle returning from the future does not return to its universe of origination but to a parallel universe. This suggests that a wormhole time machine with an exceedingly short time jump is a theoretical bridge between contemporaneous parallel universes.\n\nBecause a wormhole time-machine introduces a type of nonlinearity into quantum theory, this sort of communication between parallel universes is consistent with Joseph Polchinski's proposal of an Everett phone (named after Hugh Everett) in Steven Weinberg's formulation of nonlinear quantum mechanics.\n\nThe possibility of communication between parallel universes has been dubbed interuniversal travel.\n\nTheories of \"wormhole metrics\" describe the spacetime geometry of a wormhole and serve as theoretical models for time travel. An example of a (traversable) wormhole metric is the following:\n\nfirst presented by Ellis (see Ellis wormhole) as a special case of the Ellis drainhole.\n\nOne type of non-traversable wormhole metric is the Schwarzschild solution (see the first diagram):\n\nThe original Einstein–Rosen bridge was described in an article published in July 1935.\n\nFor the Schwarzschild spherically symmetric static solution \n\nIf one replaces formula_9 with formula_10 according to\nformula_11\n\nFor the combined field, gravity and electricity, Einstein and Rosen derived the following Schwarzschild static spherically symmetric solution\n\nThe field equations without denominators in the case when formula_1 = 0 can be written\n\nIn order to eliminate singularities, if one replaces formula_9 by formula_10 according to the equation:\n\nand with formula_1 = 0 one obtains\n\nWormholes are a common element in science fiction because they allow interstellar, intergalactic, and sometimes even interuniversal travel within human lifetime scales. In fiction, wormholes have also served as a method for time travel.\n\n\n"}
{"id": "13873874", "url": "https://en.wikipedia.org/wiki?curid=13873874", "title": "Zeta potential titration", "text": "Zeta potential titration\n\nZeta potential titration is a titration of heterogeneous systems, for example colloids and emulsions. Solids in such systems have very high surface area. This type of titration is used to study the zeta potential of these surfaces under different conditions.\n\nThe iso-electric point is one such property. The iso-electric point is the pH value at which the zeta potential is approximately zero. At a pH near the iso-electric point (± 2 pH units), colloids are usually unstable; the particles tend to coagulate or flocculate. Such titrations use acids or bases as titration reagents. Tables of iso-electric points for different materials are available. The attached figure illustrates results of such titrations for concentrated dispersions of alumina (4% v/v) and rutile (7% v/v). It is seen that iso-electric point of alumina is around pH 9.3, whereas for rutile it is around pH 4. Alumina is unstable in the pH range from 7 to 11. Rutile is unstable in the pH range from 2 to 6.\n\nAnother purpose of this titration is determination of the optimum dose of surfactant for achieving stabilization or flocculation of a heterogeneous system. Examples can be found in the book by Dukhin and Goetz.\n\nIn a zeta-potential titration, the Zeta potential is the indicator. Measurement of the zeta potential can be performed using microelectrophoresis, or electrophoretic light scattering, or electroacoustic phenomena. The last method makes possible to perform titrations in concentrated systems, with no dilution. The book by Dukhin and Goetz provides a detailed description of such titrations.\n\n"}
