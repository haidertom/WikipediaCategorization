{"id": "57724143", "url": "https://en.wikipedia.org/wiki?curid=57724143", "title": "AMADEE-18", "text": "AMADEE-18\n\nAMADEE-18 is a Mars simulation project. It was launched in February 2018 in a desert in Oman. It is mission of the Austrian Space Forum (OeWF). \n\n"}
{"id": "39136", "url": "https://en.wikipedia.org/wiki?curid=39136", "title": "Accelerating expansion of the universe", "text": "Accelerating expansion of the universe\n\nThe accelerating expansion of the universe is the observation that the expansion of the universe is such that the velocity at which a distant galaxy is receding from the observer is continuously increasing with time.\n\nThe accelerated expansion was discovered in 1998, by two independent projects, the Supernova Cosmology Project and the High-Z Supernova Search Team, which both used distant type Ia supernovae to measure the acceleration. The idea was that as type 1a supernovae have almost the same intrinsic brightness (a standard candle), and since objects that are further away appear dimmer, we can use the observed brightness of these supernovae to measure the distance to them. The distance can then be compared to the supernovae's cosmological redshift, which measures how much the universe has expanded since the supernova occurred. The unexpected result was that objects in the universe are moving away from another at an accelerated rate. Cosmologists at the time expected that recession velocity would always be decelerating to the gravitational attraction of the matter in the universe. Three members of these two groups have subsequently been awarded Nobel Prizes for their discovery. Confirmatory evidence has been found in baryon acoustic oscillations, and in analyses of the clustering of galaxies.\n\nThe accelerated expansion of the universe is thought to have begun since the universe entered its dark-energy-dominated era roughly 5 billion years ago.\nWithin the framework of general relativity, an accelerated expansion can be accounted for by a positive value of the cosmological constant , equivalent to the presence of a positive vacuum energy, dubbed \"dark energy\". While there are alternative possible explanations, the description assuming dark energy (positive ) is used in the current standard model of cosmology, which also includes cold dark matter (CDM) and is known as the Lambda-CDM model.\n\nIn the decades since the detection of cosmic microwave background (CMB) in 1965, the Big Bang model has become the most accepted model explaining the evolution of our universe. The Friedmann equation defines how the energy in the universe drives its expansion.\n\nwhere the pressure is defined by the cosmological model chosen. (see explanatory models below)\n\nPhysicists at one time were so assured of the deceleration of the universe's expansion that they introduced a so-called deceleration parameter . Current observations point towards this deceleration parameter being negative.\n\nAccording to the theory of cosmic inflation, the very early universe underwent a period of very rapid, quasi-exponential expansion. While the time-scale for this period of expansion was far shorter than that of the current expansion, this was a period of accelerated expansion with some similarities to the current epoch.\n\nThe definition of \"accelerating expansion\" is that the second time derivative of the cosmic scale factor, formula_2, is positive, which implies that the deceleration parameter is negative. However, note this does not imply that the Hubble parameter is increasing with time. Since the Hubble parameter is defined as formula_3, it follows from the definitions that the derivative of the Hubble parameter is given by \n\nso the Hubble parameter is decreasing with time unless formula_5. Observations prefer formula_6, which implies that formula_2 is positive but formula_8 is negative. Essentially, this implies that the cosmic recession velocity of any one particular galaxy is increasing with time, but its velocity/distance ratio is still decreasing; thus different galaxies expanding across a sphere of fixed radius cross the sphere more slowly at later times.\n\nIt is seen from above that the case of \"zero acceleration/deceleration\" corresponds to formula_9 is a linear function of formula_10, formula_11, formula_12, and formula_13.\n\nTo learn about the rate of expansion of the universe we look at the magnitude-redshift relationship of astronomical objects using standard candles, or their distance-redshift relationship using standard rulers. We can also look at the growth of large-scale structure, and find that the observed values of the cosmological parameters are best described by models which include an accelerating expansion.\n\nThe first evidence for acceleration came from the observation of Type Ia supernovae, which are exploding white dwarfs that have exceeded their stability limit. Because they all have similar masses, their intrinsic luminosity is standardizable. Repeated imaging of selected areas of the sky is used to discover the supernovae, then follow-up observations give their peak brightness, which is converted into a quantity known as luminosity distance (see distance measures in cosmology for details). Spectral lines of their light can be used to determine their redshift.\n\nFor supernovae at redshift less than around 0.1, or light travel time less than 10 percent of the age of the universe, this gives a nearly linear distance–redshift relation due to Hubble's law. At larger distances, since the expansion rate of the universe has changed over time, the distance-redshift relation deviates from linearity, and this deviation depends on how the expansion rate has changed over time. The full calculation requires computer integration of the Friedmann equation, but a simple derivation can be given as follows: the redshift directly gives the cosmic scale factor at the time the supernova exploded.\n\nSo a supernova with a measured redshift implies the universe was  =  of its present size when the supernova exploded. In the standard accelerated expansion scenario the rate of expansion still decreases, but does so more slowly than the non-accelerated case. This means that in the accelerated case, the past rate of expansion is slower than it would be in the non-accelerated case. Thus in a universe with accelerated expansion, it takes a longer time to expand from two thirds its present size, compared to a non-accelerating universe with the same present-day value of the Hubble constant. This results in a larger light-travel time, larger distance and fainter supernovae, which corresponds to the actual observations. Adam Riess \"et al.\" found that \"the distances of the high-redshift SNe Ia were, on average, 10% to 15% farther than expected in a low mass density universe without a cosmological constant\". This means that the measured high-redshift distances were too large, compared to nearby ones, for a decelerating universe.\n\nIn the early universe before recombination and decoupling took place, photons and matter existed in a primordial plasma. Points of higher density in the photon-baryon plasma would contract, being compressed by gravity until the pressure became too large and they expanded again. This contraction and expansion created vibrations in the plasma analogous to sound waves. Since dark matter only interacts gravitationally it stayed at the centre of the sound wave, the origin of the original overdensity. When decoupling occurred, approximately 380,000 years after the Big Bang, photons separated from matter and were able to stream freely through the universe, creating the cosmic microwave background as we know it. This left shells of baryonic matter at a fixed radius from the overdensities of dark matter, a distance known as the sound horizon. As time passed and the universe expanded, it was at these anisotropies of matter density where galaxies started to form. So by looking at the distances at which galaxies at different redshifts tend to cluster, it is possible to determine a standard angular diameter distance and use that to compare to the distances predicted by different cosmological models.\n\nPeaks have been found in the correlation function (the probability that two galaxies will be a certain distance apart) at , indicating that this is the size of the sound horizon today, and by comparing this to the sound horizon at the time of decoupling (using the CMB), we can confirm the accelerated expansion of the universe.\n\nMeasuring the mass functions of galaxy clusters, which describe the number density of the clusters above a threshold mass, also provides evidence for dark energy . By comparing these mass functions at high and low redshifts to those predicted by different cosmological models, values for and are obtained which confirm a low matter density and a non zero amount of dark energy.\n\nGiven a cosmological model with certain values of the cosmological density parameters, it is possible to integrate the Friedmann equations and derive the age of the universe.\n\nBy comparing this to actual measured values of the cosmological parameters, we can confirm the validity of a model which is accelerating now, and had a slower expansion in the past.\n\nRecent discoveries of gravitational waves through LIGO and VIRGO not only confirmed Einstein's predictions but also opened a new window into the universe. These gravitational waves can work as sort of standard sirens to measure the expansion rate of the universe. Abbot et al. 2017 measured the Hubble constant value to be approximately 70 kilometres per second per megaparsec. The amplitudes of the strain 'h' is dependent on the masses of the objects causing waves, distances from observation point and gravitational waves detection frequencies. The associated distance measures are dependent on the cosmological parameters like the Hubble Constant for nearby objects and will be dependent on other cosmological parameters like the dark energy density, matter density, etc. for distant sources.\n\nThe most important property of dark energy is that it has negative pressure (repulsive action) which is distributed relatively homogeneously in space.\n\nwhere is the speed of light and is the energy density. Different theories of dark energy suggest different values of , with for cosmic acceleration (this leads to a positive value of in the acceleration equation above).\n\nThe simplest explanation for dark energy is that it is a cosmological constant or vacuum energy; in this case . This leads to the Lambda-CDM model, which has generally been known as the Standard Model of Cosmology from 2003 through the present, since it is the simplest model in good agreement with a variety of recent observations. Riess \"et al.\" found that their results from supernovae observations favoured expanding models with positive cosmological constant () and a current accelerated expansion ().\n\nCurrent observations allow the possibility of a cosmological model containing a dark energy component with equation of state . This phantom energy density would become infinite in finite time, causing such a huge gravitational repulsion that the universe would lose all structure and end in a Big Rip. For example, for and  =70 km·s·Mpc, the time remaining before the universe ends in this Big Rip is 22 billion years.\n\nThere are many alternative explanations for the accelerating universe. Some examples are quintessence, a proposed form of dark energy with a non-constant state equation, whose density decreases with time. Dark fluid is an alternative explanation for accelerating expansion which attempts to unite dark matter and dark energy into a single framework. Alternatively, some authors have argued that the accelerated expansion of the universe could be due to a repulsive gravitational interaction of antimatter or a deviation of the gravitational laws from general relativity. The measurement of the speed of gravity with the gravitational wave event GW170817 ruled out many modified gravity theories as alternative explanation to dark energy.\n\nAnother type of model, the backreaction conjecture, was proposed by cosmologist Syksy Räsänen: the rate of expansion is not homogenous, but we are in a region where expansion is faster than the background. Inhomogeneities in the early universe cause the formation of walls and bubbles, where the inside of a bubble has less matter than on average. According to general relativity, space is less curved than on the walls, and thus appears to have more volume and a higher expansion rate. In the denser regions, the expansion is slowed by a higher gravitational attraction. Therefore, the inward collapse of the denser regions looks the same as an accelerating expansion of the bubbles, leading us to conclude that the universe is undergoing an accelerated expansion. The benefit is that it does not require any new physics such as dark energy. Räsänen does not consider the model likely, but without any falsification, it must remain a possibility. It would require rather large density fluctuations (20%) to work.\n\nA final possibility is that dark energy is an illusion caused by some bias in measurements. For example, if we are located in an emptier-than-average region of space, the observed cosmic expansion rate could be mistaken for a variation in time, or acceleration. A different approach uses a cosmological extension of the equivalence principle to show how space might appear to be expanding more rapidly in the voids surrounding our local cluster. While weak, such effects considered cumulatively over billions of years could become significant, creating the illusion of cosmic acceleration, and making it appear as if we live in a Hubble bubble. Yet other possibilities are that the accelerated expansion of the universe is an illusion caused by the relative motion of us to the rest of the universe, or that the supernovae sample size used wasn't large enough.\n\nAs the universe expands, the density of radiation and ordinary dark matter declines more quickly than the density of dark energy (see equation of state) and, eventually, dark energy dominates. Specifically, when the scale of the universe doubles, the density of matter is reduced by a factor of 8, but the density of dark energy is nearly unchanged (it is exactly constant if the dark energy is a cosmological constant).\n\nIn models where dark energy is a cosmological constant, the universe will expand exponentially with time in the far future, coming closer and closer to a de Sitter spacetime. This will eventually lead to all evidence for the Big Bang disappearing, as the cosmic microwave background is redshifted to lower intensities and longer wavelengths. Eventually its frequency will be low enough that it will be absorbed by the interstellar medium, and so be screened from any observer within the galaxy. This will occur when the universe is less than 50 times its current age, leading to the end of cosmology as we know it as the distant universe turns dark.\n\nA constantly expanding universe with non-zero cosmological constant has mass density decreasing over time, to an undetermined point when zero matter density is reached. All matter (electrons, protons and neutrons) would ionize and disintegrate, with objects dissipating away.\n\nAlternatives for the ultimate fate of the universe include the Big Rip mentioned above, a Big Bounce, Big Freeze or Big Crunch.\n"}
{"id": "19468941", "url": "https://en.wikipedia.org/wiki?curid=19468941", "title": "Balance of nature", "text": "Balance of nature\n\nThe balance of nature is a theory that proposes that ecological systems are usually in a stable equilibrium or homeostasis, which is to say that a small change in some particular parameter (the size of a particular population, for example) will be corrected by some negative feedback that will bring the parameter back to its original \"point of balance\" with the rest of the system. It may apply where populations depend on each other, for example in predator/prey systems, or relationships between herbivores and their food source. It is also sometimes applied to the relationship between the Earth's ecosystem, the composition of the atmosphere, and the world's weather.\n\nThe Gaia hypothesis is a balance of nature-based theory that suggests that the Earth and its ecology may act as co-ordinated systems in order to maintain the balance of nature.\n\nThe theory that nature is permanently in balance has been largely discredited by scientists working in ecology, as it has been found that chaotic changes in population levels are common, but nevertheless the idea continues to be popular in the general public. During the later half of the twentieth century the theory was superseded by catastrophe theory and chaos theory.\n\nThe concept that nature maintains its condition is of ancient provenance; Herodotus commented on the wonderful relationship between predator and prey species, which remained in a steady proportion to one another, with predators never excessively consuming their prey populations. The \"balance of nature\" concept once ruled ecological research, as well as once governing the management of natural resources. This led to a doctrine popular among some conservationists that nature was best left to its own devices, and that human intervention into it was by definition unacceptable. The validity of a \"balance of nature\" was already questioned in the early 1900s, but the general abandonment of the theory by scientists working in ecology only happened in the last quarter of that century when studies showed that it did not match what could be observed among plant and animal populations.\n\nPredator-prey populations tend to show chaotic behavior within limits, where the sizes of populations change in a way that may appear random, but is in fact obeying deterministic laws based only on the relationship between a population and its food source illustrated by the Lotka–Volterra equation. An experimental example of this was shown in an eight-year study on small Baltic Sea creatures such as plankton, which were isolated from the rest of the ocean. Each member of the food web was shown to take turns multiplying and declining, even though the scientists kept the outside conditions constant. An article in the journal \"Nature\" stated; \"Advanced mathematical techniques proved the indisputable presence of chaos in this food web ... short-term prediction is possible, but long-term prediction is not.\"\n\nAlthough some conservationist organizations argue that human activity is incompatible with a balanced ecosystem, there are numerous examples in history showing that several modern day habitats originate from human activity: some of Latin America's rain forests owe their existence to humans planting and transplanting them, while the abundance of grazing animals in the Serengeti plain of Africa is thought by some ecologists to be partly due to human-set fires that created savanna habitats.\n\nPossibly one of the best examples of an ecosystem fundamentally modified by human activity can be observed as a consequence of the Australian Aboriginal practice of \"Fire-stick farming\". The legacy of this practice over long periods has resulted in forests being converted to grasslands capable of sustaining larger populations of faunal prey, particularly in the northern and western regions of the continent. So thorough has been the effect of these deliberate regular burnings that many plant and tree species from affected regions have now completely adapted to the annual fire regime in that they require the passage of a fire before their seeds will even germinate. One school in Los Angeles states, \" “We have let our kids go to the forest area of the playground. However, five years later, we found that none of the flowers were growing, the natural damp soil had been hardened, and all of the beautiful grass had been plucked,”.\n\nDespite being discredited among ecologists, the theory is widely held to be true by the general public, with one authority calling it an \"enduring myth\". At least in Midwestern America, the \"balance of nature\" idea was shown to be widely held by both science majors and the general student population. In a study at the University of Patras, educational sciences students were asked to reason about the future of ecosystems which suffered human-driven disturbances. Subjects agreed that it was very likely for the ecosystems to fully recover their initial state, referring to either a 'recovery process' which restores the initial 'balance', or specific 'recovery mechanisms' as an ecosystem's inherent characteristic. In a 2017 study, Ampatzidis and Ergazaki discuss the learning objectives and design criteria that a learning environment for non-biology major students should meet to support them challenge the \"balance of nature\" idea.\n\n"}
{"id": "11003420", "url": "https://en.wikipedia.org/wiki?curid=11003420", "title": "Brown energy", "text": "Brown energy\n\nBrown energy may refer to:\n\n"}
{"id": "13662027", "url": "https://en.wikipedia.org/wiki?curid=13662027", "title": "Colloid vibration current", "text": "Colloid vibration current\n\nColloid vibration current is an electroacoustic phenomenon that arises when ultrasound propagates through a fluid that contains ions and either solid particles or emulsion droplets. \n\nThe pressure gradient in an ultrasonic wave moves particles relative to the fluid. This motion disturbs the double layer that exists at the particle-fluid interface. The picture illustrates the mechanism of this distortion. Practically all particles in fluids carry a surface charge. This surface charge is screened with an equally charged diffuse layer; this structure is called the double layer. Ions of the diffuse layer are located in the fluid and can move with the fluid. Fluid motion relative to the particle drags these diffuse ions in the direction of one or the other of the particle's poles. The picture shows ions dragged towards the left hand pole. As a result of this drag, there is an excess of negative ions in the vicinity of the left hand pole and an excess of positive surface charge at the right hand pole. As a result of this charge excess, particles gain a dipole moment. These dipole moments generate an electric field that in turn generates measurable electric current. This phenomenon is widely used for measuring zeta potential in concentrated colloids.\n\n"}
{"id": "1643492", "url": "https://en.wikipedia.org/wiki?curid=1643492", "title": "Cosmic latte", "text": "Cosmic latte\n\nCosmic latte is a name assigned to the average color of the universe, found by a team of astronomers from Johns Hopkins University. In 2001, Karl Glazebrook and Ivan Baldry determined that the average color of the universe was a greenish white, but they soon corrected their analysis in a 2002 paper in which they reported that their survey of the light from over 200,000 galaxies averaged to a slightly beigeish white. The hex triplet value for cosmic latte is #FFF8E7.\n\nFinding the average color of the universe was not the focus of the study. Rather, the study examined spectral analysis of different galaxies to study star formation. Like Fraunhofer lines, the dark lines displayed in the study's spectral ranges display older and younger stars and allow Glazebrook and Baldry to determine the age of different galaxies and star systems. What the study revealed is that the overwhelming majority of stars formed about 5 billion years ago. Because these stars would have been \"brighter\" in the past, the color of the universe changes over time shifting from blue to red as more blue stars change to yellow and eventually red giants.\n\nAs light from distant galaxies reaches the Earth, the average \"color of the universe\" (as seen from Earth) tends towards pure white, due to the light coming from the stars when they were much younger and bluer.\n\nThe corrected color was initially published on the Johns Hopkins News website and updated on the team's initial announcement. Multiple news outlets, including NPR and BBC, displayed the color in stories and some relayed the request by Glazebrook on the announcement asking for suggestions for names, jokingly adding all were welcome as long as they were not \"beige\".\n\nThese were the results of a vote of the scientists involved based on the new color:\nThough Drum's suggestion of \"cappuccino cosmico\" received the most votes, the researchers favored Drum's other suggestion, \"cosmic latte\". This is because the similar \"Latteo\" means \"Milky\" in Italian, Galileo's native language. It also leads to the similarity to the Italian term for the Milky Way, \"Via Lattea\", and they enjoyed the fact that the color would be similar to the Milky Way's average color as well, as it is part of the sum of the universe. They also claimed to be \"caffeine biased\".\n\nDrum came up with the name while sitting in a Starbucks drinking a latte and reading the \"Washington Post\". Drum noticed that the color of the universe as displayed in the newspaper was the same color as his latte.\n\n"}
{"id": "12527335", "url": "https://en.wikipedia.org/wiki?curid=12527335", "title": "Cosmic time", "text": "Cosmic time\n\nCosmic time (also known as time since the big bang) is the time coordinate commonly used in the Big Bang models of physical cosmology. It is defined for homogeneous, expanding universes as follows: Choose a time coordinate so that the universe has the same density everywhere at each moment in time (the fact that this is possible means that the universe is, by definition, homogeneous). Measure the passage of time using clocks moving with the Hubble flow. Choose the big bang singularity as the origin of the time coordinate. \n\nCosmic time formula_1 is a measure of time by a physical clock with zero peculiar velocity in the absence of matter over-/under-densities (to prevent time dilation due to relativistic effects or confusions caused by expansion of the universe). Unlike other measures of time such as temperature, redshift, particle horizon, or Hubble horizon, the cosmic time (similar and complementary to the comoving coordinates) is blind to the expansion of the universe. \n\nThere are two main ways for establishing a reference point for the cosmic time. The most trivial way is to take the present time as the cosmic reference point (sometimes referred to as the lookback time) or alternatively, take the Big Bang as formula_2 (also referred to as age of the universe). The big bang doesn't necessarily have to correspond to a physical event but rather it refers to the point at which the scale factor would vanish for a standard cosmological model such as ΛCDM. For instance, in the case of inflation, i.e. a non-standard cosmology, the hypothetical moment of big bang is still determined using the benchmark cosmological models which may coincide with the end of the inflationary epoch. For inflationary models, it is not possible to establish a well defined origin of time before the big bang since the universe does not require a beginning event in such models. For technical purposes, concepts such as the average temperature of the universe (in units of eV) or the particle horizon are used when the early universe is the objective of a study since understanding the interaction among particles is more relevant than their time coordinate or age. \n\nCosmic time is the standard time coordinate for specifying the Friedmann–Lemaître–Robertson–Walker solutions of Einstein's equations.\n\n"}
{"id": "2848730", "url": "https://en.wikipedia.org/wiki?curid=2848730", "title": "Cosmological horizon", "text": "Cosmological horizon\n\nA cosmological horizon is a measure of the distance from which one could possibly retrieve information. This observable constraint is due to various properties of general relativity, the expanding universe, and the physics of Big Bang cosmology. Cosmological horizons set the size and scale of the observable universe. This article explains a number of these horizons.\n\nThe particle horizon (also called the cosmological horizon, the comoving horizon, or the cosmic light horizon) is the maximum distance from which particles could have traveled to the observer in the age of the universe. It represents the boundary between the observable and the unobservable regions of the universe, so its distance at the present epoch defines the size of the observable universe. Due to the expansion of the universe it is not simply the age of the universe times the speed of light, as in the Hubble horizon, but rather the speed of light multiplied by the conformal time. The existence, properties, and significance of a cosmological horizon depend on the particular cosmological model.\n\nIn terms of comoving distance, the particle horizon is equal to the conformal time that has passed since the Big Bang, times the speed of light. In general, the conformal time at a certain time is given in terms of the scale factor formula_1 by,\n\nor\n\nThe particle horizon is the boundary between two regions at a point at a given time: one region defined by events that have already been observed by an observer, and the other by events which cannot be observed \"at that time\". It represents the furthest distance from which we can retrieve information from the past, and so defines the observable universe.\n\nHubble radius, Hubble sphere, Hubble volume, or Hubble horizon is a conceptual horizon defining the boundary between particles that are moving slower and faster than the speed of light relative to an observer at one given time. Note that this does not mean the particle is unobservable, the light from the past is reaching and will continue to reach the observer for a while. Also, more importantly, in the current expansion model e.g., light emitted from the Hubble radius will reach us in a finite amount of time. It is a common misconception that light from the Hubble radius can never reach us. It is true that particles on the Hubble radius recede from us with the speed of light, but the Hubble radius gets larger over time (because the Hubble parameter H gets smaller over time), so light emitted towards us from a particle on the Hubble radius will be inside the Hubble radius some time later. Only light emitted from the cosmic event horizon or further will never reach us in a finite amount of time.\n\nThe Hubble velocity of an object is given by Hubble's law,\n\nReplacing formula_5 with speed of light formula_6 and solving for proper distance formula_7 we obtain the radius of Hubble sphere as\n\nIn an ever-accelerating universe, if two particles are separated by a distance greater than the Hubble radius, they cannot talk to each other from now on (as they are now, not as they have been in the past), However, if they are outside of each other's particle horizon, they could have never communicated. Depending on the form of expansion of the universe, they may be able to exchange information in the future. Today,\n\nyielding a Hubble horizon of some 4.1 Gpc. This horizon is not really a physical size, but it is often used as useful length scale as most physical sizes in cosmology can be written in terms of those factors.\n\nOne can also define comoving Hubble horizon by simply dividing Hubble radius by the scale factor\n\nThe particle horizon differs from the cosmic event horizon, in that the particle horizon represents the largest comoving distance from which light could have reached the observer by a specific time, while the event horizon is the largest comoving distance from which light emitted now can \"ever\" reach the observer in the future. The current distance to our cosmic event horizon is about 5 Gpc (16 billion light years), well within our observable range given by the particle horizon.\n\nIn general, the proper distance to the event horizon at time formula_11 is given by\n\nwhere formula_13 is the time-coordinate of the end of the universe, which would be infinite in the case of a universe that expands forever.\n\nFor our case, assuming that dark energy is due to a cosmological constant, formula_14.\n\nIn an accelerating universe, there are events which will be unobservable as formula_15 as signals from future events become redshifted to arbitrarily long wavelengths in the exponentially expanding de Sitter space. This sets a limit on the farthest distance that we can possibly see as measured in units of proper distance today. Or, more precisely, there are events that are spatially separated for a certain frame of reference happening simultaneously with the event occurring right now for which no signal will ever reach us, even though we can observe events that occurred at the same location in space that happened in the distant past. While we will continue to receive signals from this location in space, even if we wait an infinite amount of time, a signal that left from that location today will never reach us. Additionally, the signals coming from that location will have less and less energy and be less and less frequent until the location, for all practical purposes, becomes unobservable. In a universe that is dominated by dark energy which is undergoing an exponential expansion of the scale factor, all objects that are gravitationally unbound with respect to the Milky Way will become unobservable, in a futuristic version of Kapteyn's universe.\n\nWhile not technically \"horizons\" in the sense of an impossibility for observations due to relativity or cosmological solutions, there are practical horizons which include the optical horizon, set at the surface of last scattering. This is the farthest distance that any photon can freely stream. Similarly, there is a \"neutrino horizon\" set for the farthest distance a neutrino can freely stream and a gravitational wave horizon at the farthest distance that gravitational waves can freely stream. The latter is predicted to be a direct probe of the end of cosmic inflation.\n\nFor a simplified summary and overview of different horizons in cosmology, see Different Horizons in Cosmology\n"}
{"id": "944638", "url": "https://en.wikipedia.org/wiki?curid=944638", "title": "Earth's energy budget", "text": "Earth's energy budget\n\nEarth's energy budget accounts for the balance between the energy Earth receives from the Sun, the energy Earth radiates back into outer space after having been distributed throughout the five components of Earth's climate system and having thus powered the so-called Earth’s heat engine. This system is made up of earth's water, ice, atmosphere, rocky crust, and all living things.\n\nQuantifying changes in these amounts is required to accurately model the Earth's climate. \n\nReceived radiation is unevenly distributed over the planet, because the Sun heats equatorial regions more than polar regions. The atmosphere and ocean work non-stop to even out solar heating imbalances through evaporation of surface water, convection, rainfall, winds, and ocean circulation. Earth is very close to being in radiative equilibrium, the situation where the incoming solar energy is balanced by an equal flow of heat to space; under that condition, global temperatures will be \"relatively\" stable. Globally, over the course of the year, the Earth system—land surfaces, oceans, and atmosphere—absorbs and then radiates back to space an average of about 240 watts of solar power per square meter. Anything that increases or decreases the amount of incoming or outgoing energy will change global temperatures in response.\n\nHowever, Earth's energy balance and heat fluxes depend on many factors, such as atmospheric composition (mainly aerosols and greenhouse gases), the albedo (reflectivity) of surface properties, cloud cover and vegetation and land use patterns.\n\nChanges in surface temperature due to Earth's energy budget do not occur instantaneously, due to the inertia of the oceans and the cryosphere. The net heat flux is buffered primarily by becoming part of the ocean's heat content, until a new equilibrium state is established between radiative forcings and the climate response.\n\nIn spite of the enormous transfers of energy into and from the Earth, it maintains a relatively constant temperature because, as a whole, there is little net gain or loss: Earth emits via atmospheric and terrestrial radiation (shifted to longer electromagnetic wavelengths) to space about the same amount of energy as it receives via insolation (all forms of electromagnetic radiation).\n\nTo quantify Earth's \"heat budget\" or \"heat balance\", let the insolation received at the top of the atmosphere be 100 units (100 units = about 1,360 watts per square meter facing the sun), as shown in the accompanying illustration. Called the albedo of Earth, around 35 units are reflected back to space: 27 from the top of clouds, 2 from snow and ice-covered areas, and 6 by other parts of the atmosphere. The 65 remaining units are absorbed: 14 within the atmosphere and 51 by the Earth’s surface. These 51 units are radiated to space in the form of terrestrial radiation: 17 directly radiated to space and 34 absorbed by the atmosphere (19 through latent heat of condensation, 9 via convection and turbulence, and 6 directly absorbed). The 48 units absorbed by the atmosphere (34 units from terrestrial radiation and 14 from insolation) are finally radiated back to space. These 65 units (17 from the ground and 48 from the atmosphere) balance the 65 units absorbed from the sun in order to maintain zero net gain of energy by the Earth.\n\nThe total amount of energy received per second at the top of Earth's atmosphere (TOA) is measured in watts and is given by the solar constant times the cross-sectional area of the Earth. Because the surface area of a sphere is four times the cross-sectional surface area of a sphere (i.e. the area of a circle), the average TOA flux is one quarter of the solar constant and so is approximately 340 W/m². Since the absorption varies with location as well as with diurnal, seasonal and annual variations, the numbers quoted are long-term averages, typically averaged from multiple satellite measurements.\n\nOf the ~340 W/m² of solar radiation received by the Earth, an average of ~77 W/m² is reflected back to space by clouds and the atmosphere and ~23 W/m² is reflected by the surface albedo, leaving ~240 W/m² of solar energy input to the Earth's energy budget. This gives the earth a mean net albedo of 0.29.\n\nThe geothermal heat flux from the Earth's interior is estimated to be 47 terawatts. This comes to 0.087 watt/square metre, which represents only 0.027% of Earth's total energy budget at the surface, which is dominated by 173,000 terawatts of incoming solar radiation.\n\nHuman production of energy is even lower, at an estimated 18 TW.\n\nPhotosynthesis has a larger effect: photosynthetic efficiency turns up to 2% of incoming sunlight into biomass, for a total photosynthetic productivity of earth between ~1500–2250 TW (~1%+/-0.26% solar energy hitting the Earth's surface).\n\nOther minor sources of energy are usually ignored in these calculations, including accretion of interplanetary dust and solar wind, light from stars other than the Sun and the thermal radiation from space. Earlier, Joseph Fourier had claimed that deep space radiation was significant in a paper often cited as the first on the greenhouse effect.\n\nLongwave radiation is usually defined as outgoing infrared energy leaving the planet. However, the atmosphere absorbs parts initially, or cloud cover can reflect radiation. Generally, heat energy is transported between the planet's surface layers (land and ocean) to the atmosphere, transported via evapotranspiration and latent heat fluxes or conduction/convection processes. Ultimately, energy is radiated in the form of longwave infrared radiation back into space.\n\nRecent satellite observations indicate additional precipitation, which is sustained by increased energy leaving the surface through evaporation (the latent heat flux), offsetting increases in longwave flux to the surface.\n\nIf the incoming energy flux is not equal to the outgoing energy flux, net heat is added to or lost by the planet (if the incoming flux is larger or smaller than the outgoing respectively).\n\nAn imbalance must show in something on Earth warming or cooling (depending on the direction of the imbalance), and the ocean being the larger thermal reservoir on Earth, is a prime candidate for measurements.\n\nEarth's energy imbalance measurements provided by Argo floats have detected an accumulation of ocean heat content (OHC). The estimated imbalance was measured during a deep solar minimum of 2005–2010 to be 0.58 ± 0.15 W/m². This level of detail cannot be inferred directly from measurements of surface energy fluxes, which have combined uncertainties of the order of ± 17 W/m².\n\nSeveral satellites indirectly measure the energy absorbed and radiated by Earth and by inference the energy imbalance. The NASA Earth Radiation Budget Experiment (ERBE) project involves three such satellites: the Earth Radiation Budget Satellite (ERBS), launched October 1984; NOAA-9, launched December 1984; and NOAA-10, launched September 1986.\n\nToday NASA's satellite instruments, provided by CERES, part of the NASA's Earth Observing System (EOS), are designed to measure both solar-reflected and Earth-emitted radiation.\n\nThe major atmospheric gases (oxygen and nitrogen) are transparent to incoming sunlight but are also transparent to outgoing thermal (infrared) radiation. However, water vapor, carbon dioxide, methane and other trace gases are opaque to many wavelengths of thermal radiation. The Earth's surface radiates the net equivalent of 17 percent of the incoming solar energy in the form of thermal infrared. However, the amount that directly escapes to space is only about 12 percent of incoming solar energy. The remaining fraction, 5 to 6 percent, is absorbed by the atmosphere by greenhouse gas molecules.\nWhen greenhouse gas molecules absorb thermal infrared energy, their temperature rises. Those gases then radiate an increased amount of thermal infrared energy in all directions. Heat radiated upward continues to encounter greenhouse gas molecules; those molecules also absorb the heat, and their temperature rises and the amount of heat they radiate increases. The atmosphere thins with altitude, and at roughly 5–6 kilometres, the concentration of greenhouse gases in the overlying atmosphere is so thin that heat can escape to space.\n\nBecause greenhouse gas molecules radiate infrared energy in all directions, some of it spreads downward and ultimately returns to the Earth's surface, where it is absorbed. The Earth's surface temperature is thus higher than it would be if it were heated only by direct solar heating. This supplemental heating is the natural greenhouse effect. It is as if the Earth is covered by a blanket that allows high frequency radiation (sunlight) to enter, but slows the rate at which the low frequency infrared radiant energy emitted by the Earth leaves.\n\nA change in the incident radiated portion of the energy budget is referred to as a radiative forcing.\n\nClimate sensitivity is the steady state change in the equilibrium temperature as a result of changes in the energy budget.\n\nClimate forcings are changes that cause temperatures to rise or fall, disrupting the energy balance. Natural climate forcings include changes in the Sun's brightness, Milankovitch cycles (small variations in the shape of Earth's orbit and its axis of rotation that occur over thousands of years) and volcanic eruptions that inject light-reflecting particles as high as the stratosphere. Man-made forcings include particle pollution (aerosols) that absorb and reflect incoming sunlight; deforestation, which changes how the surface reflects and absorbs sunlight; and the rising concentration of atmospheric carbon dioxide and other greenhouse gases, which decreases the rate at which heat is radiated to space.\n\nA forcing can trigger feedbacks that intensify (positive feedback) or weaken (negative feedback) the original forcing. For example, loss of ice at the poles, which makes them less reflective, causes greater absorption of energy and so increases the rate at which the ice melts, is an example of a positive feedback.\n\nThe observed planetary energy imbalance during the recent solar minimum shows that solar forcing of climate, although natural and significant, is overwhelmed by anthropogenic climate forcing.\n\nIn 2012, NASA scientists reported that to stop global warming atmospheric CO content would have to be reduced to 350 ppm or less, assuming all other climate forcings were fixed. The impact of anthropogenic aerosols has not been quantified, but individual aerosol types are thought to have substantial heating and cooling effects.\n\n\n"}
{"id": "1800265", "url": "https://en.wikipedia.org/wiki?curid=1800265", "title": "Emergy", "text": "Emergy\n\nEmergy is the amount of energy that was consumed in direct and indirect transformations to make a product or service. Emergy is a measure of quality differences between different forms of energy. Emergy is an expression of all the energy used in the work processes that generate a product or service in units of one type of energy. Emergy is measured in units of \"emjoule\"s, a unit referring to the available energy consumed in transformations. Emergy accounts for different forms of energy and resources (e.g. sunlight, water, fossil fuels, minerals, etc.) Each form is generated by transformation processes in nature and each has a different ability to support work in natural and in human systems. The recognition of these quality differences is a key concept.\n\nThe theoretical and conceptual basis for the emergy methodology is grounded in thermodynamics, general system theory and systems ecology. Evolution of the theory by Howard T. Odum over the first thirty years is reviewed in \"Environmental Accounting\" and in the volume edited by C. A. S. Hall titled \"Maximum Power\".\n\nBeginning in the 1950s, Odum analyzed energy flow in ecosystems (\"e.g.\" Silver Springs, Florida; Enewetak atoll in the south Pacific; Galveston Bay, Texas and Puerto Rican rainforests, amongst others) where energies in various forms at various scales were observed. His analysis of energy flow in ecosystems, and the differences in the potential energy of sunlight, fresh water currents, wind and ocean currents led him to make the suggestion that when two or more different energy sources drive a system, they cannot be added without first converting them to a common measure that accounts for their differences in energy quality. This led him to introduce the concept of \"energy of one kind\" as a common denominator with the name \"energy cost\". He then expanded the analysis to model food production in the 1960s, and in the 1970s to fossil fuels.\n\nOdum's first formal statement of what would later be termed emergy was in 1973:\nEnergy is measured by calories, btu's, kilowatthours, and other intraconvertable units, but energy has a scale of quality which is not indicated by these measures. The ability to do work for man depends on the energy quality and quantity and this is measurable by the amount of energy of a lower quality grade required to develop the higher grade. The scale of energy goes from dilute sunlight up to plant matter, to coal, from coal to oil, to electricity and up to the high quality efforts of computer and human information processing.\n\nIn 1975, he introduced a table of \"Energy Quality Factors\", kilocalories of sunlight energy required to make a kilocalorie of a higher quality energy, the first mention of the energy hierarchy principle which states that \"energy quality is measured by the energy used in the transformations\" from one type of energy to the next.\n\nThese energy quality factors, were placed on a fossil-fuel basis and called \"Fossil Fuel Work Equivalents\" (FFWE), and the quality of energies were measured based on a fossil fuel standard with rough equivalents of 1 kilocalorie of fossil fuel equal to 2000 kilocalories of sunlight. \"Energy quality ratios\" were computed by evaluating the quantity of energy in a transformation process to make a new form and were then used to convert different forms of energy to a common form, in this case fossil fuel equivalents. FFWE's were replaced with coal equivalents (CE) and by 1977, the system of evaluating quality was placed on a solar basis and termed solar equivalents (SE).\n\nThe term \"embodied energy\" was used for a time in the early 1980s to refer to energy quality differences in terms of their costs of generation, and a ratio called a \"quality factor\" for the calories (or joules) of one kind of energy required to make those of another. However, since the term embodied energy was used by other groups who were evaluating the fossil fuel energy required to generate products and were not including all energies or using the concept to imply quality, embodied energy was dropped in favor of \"embodied solar calories\", and the quality factors became known as \"transformation ratios\".\n\nUse of the term \"embodied energy\" for this concept was modified in 1986 when David Scienceman, a visiting scholar at the University of Florida from Australia, suggested the term \"emergy\" and \"emjoule\" or \"emcalorie\" as the unit of measure to distinguish emergy units from units of available energy. The term transformation ratio was shortened to transformity in about the same time. It is important to note that throughout this twenty years the baseline or the basis for evaluating forms of energy and resources shifted from organic matter, to fossil fuels and finally to solar energy.\n\nAfter 1986, the emergy methodology continued to develop as the community of scientists expanded and as new applied research into combined systems of humans and nature presented new conceptual and theoretical questions. The maturing of the emergy methodology resulted in more rigorous definitions of terms and nomenclature and refinement of the methods of calculating transformities. The International Society for the Advancement of Emergy Research and a biennial International Conference at the University of Florida support this research.\n\nEmergy— amount of energy of one form that is used in transformations directly and indirectly to make a product or service. The unit of emergy is the emjoule or emergy joule. Using emergy, sunlight, fuel, electricity, and human service can be put on a common basis by expressing each of them in the emjoules of solar energy that is required to produce them. If solar emergy is the baseline, then the results are solar emjoules (abbreviated seJ). Although other baselines have been used, such as coal emjoules or electrical emjoules, in most cases emergy data are given in solar emjoules.\n\nUnit Emergy Values (UEVs) — the emergy required to generate one unit of output. Types of UEVs:\n\nEmergy accounting converts the thermodynamic basis of all forms of energy, resources and human services into equivalents of a single form of energy, usually solar. To evaluate a system, a system diagram organizes the evaluation and account for energy inputs and outflows. A table of the flows of resources, labor and energy is constructed from the diagram and all flows are evaluated. The final step involves interpreting the results.\n\nIn some cases, an evaluation is done to determine the fit of a development proposal within its environment. It also allows comparison of alternatives. Another purpose is to seek the best use of resources to maximize economic vitality.\n\nSystem diagrams show the inputs that are evaluated and summed to obtain the emergy of a flow. A diagram of a city and its regional support area is shown in Figure 1.\n\nA table (see example below) of resource flows, labor and energy is constructed from the diagram. Raw data on inflows that cross the boundary are converted into emergy units, and then summed to obtain total emergy supporting the system. Energy flows per unit time (usually per year) are presented in the table as separate line items.\n\nAll tables are followed by footnotes that show citations for data and calculations.\n\nThe table allows a unit emergy value to be calculated. The final, output row (row “O” in the example table above) is evaluated first in units of energy or mass. Then the input emergy is summed and the unit emergy value is calculated by dividing the emergy by the units of the output.\n\nFigure 2 shows non-renewable environmental contributions (N) as an emergy storage of materials, renewable environmental inputs (R), and inputs from the economy as purchased (F) goods and services. Purchased inputs are needed for the process to take place and include human service and purchased non-renewable energy and material brought in from elsewhere (fuels, minerals, electricity, machinery, fertilizer, etc.). Several ratios, or indices are given in Figure 2 that assess the global performance of a process.\n\nOther ratios are useful depending on the type and scale of the system under evaluation.\n\nThe recognition of the relevance of energy to the growth and dynamics of complex systems has resulted in increased emphasis on environmental evaluation methods that can account for and interpret the effects of matter and energy flows at all scales in systems of humanity and nature. The following table lists some general areas in which the emergy methodology has been employed.\n\nThe concept of emergy has been controversial within academe including ecology, thermodynamics and economy. Emergy theory has been criticized for allegedly offering an energy theory of value to replace other theories of value. The stated goal of emergy evaluations is to provide an \"ecocentric\" valuation of systems, processes. Thus it does not purport to replace economic values but to provide additional information, from a different point of view.\n\nThe idea that a calorie of sunlight is not equivalent to a calorie of fossil fuel or electricity strikes many as absurd, based on the 1st Law definition of energy units as measures of heat (i.e. Joule's mechanical equivalent of heat). Others have rejected the concept as impractical since from their perspective it is impossible to objectively quantify the amount of sunlight that is required to produce a quantity of oil. In combining systems of humanity and nature and evaluating environmental input to economies, mainstream economists criticize the emergy methodology for disregarding market values.\n\n"}
{"id": "46488727", "url": "https://en.wikipedia.org/wiki?curid=46488727", "title": "Energoland", "text": "Energoland\n\nEnergoland is an information centre for energy and electricity generation which was opened by Slovenské elektrárne on 14 October 2014 at Mochovce, Slovakia, at the site of the nuclear power plant. It is situated between Levice and Nitra. The centre mainly serves the schools and public. The entry is free of charge. The exposition was awarded as an excellent communication project at the PIME conference and evaluated as a five-star training centre by Laura Elizabeth Kennedy, governor of the United States of America in the Board of Governors of the International Atomic Energy Agency in Vienna and chargé d'affaires of permanent representation of the USA in international organisation in Vienna.\n\nIn Energoland, there are more than thirty objects, applications, and interactive expositions. Edutainment (blending education and entertainment) at Mochovce offers information on energy, electricity generation, global warming, and carbon burden. In addition, visitors learn about the energy mix, electricity grid dispatching, or about the radiation around us. The exposition also focuses on power industry, not only with respect to safety and radioactive waste but also dealing with the fuel cycle, nanowolrd of atoms, and Cherenkov radiation. Some of the specialities include the 3D cinema with its own movie Energy Odyssey, a model of emission-free motorcycle, interactive LED floor or thermal mirror, and mobile application using augmented reality, \"Energoland\", downloadable from Google Play or AppStore. Through the Energy Map, various facts and statistics can be searched which are directly shown in the map of the world with the highlighted countries.\nEnergoland also wants its visitors to make their own opinion of the electricity generation and its sources. Therefore, it does not provide information only about nuclear energy but it also brings knowledge about other sources: water, sun, wind, geothermal energy, or fossil fuels.\n\nThe design of the building is the work of Ing. arch. Viktor Šabík (BARAK Architekti) who, together with the QEX Company and Italian agency Piter Design, designed the interior of Energoland, as well.[2] The total area of the building, including the training rooms and offices is 2,000 square metres; the info centre itself takes up the area of 600 m2.\n\nEnergoland is not only a visitor and information centre; it also serves as a training centre for the employees of Slovenské elektrárne and provides office areas. There are many events for the employees of Slovenské elektrárne or public taking place here, such as the 90 reactor-years anniversary, programme within the national round of Olympics in Physics, Science Talks during the Science and Technology Week, Sustainable Development Conference in May 2015, etc.\n\n"}
{"id": "31450053", "url": "https://en.wikipedia.org/wiki?curid=31450053", "title": "Energy accidents", "text": "Energy accidents\n\nEnergy resources bring with them great social and economic promise, providing financial growth for communities and energy services for local economies. However, the infrastructure which delivers energy services can break down in an energy accident, sometimes causing much damage, and energy fatalities can occur, and with many systems often deaths will happen even when the systems are working as intended.\n\nHistorically, coal mining has been the most dangerous energy activity and the list of historical coal mining disasters is a long one. Underground mining hazards include suffocation, gas poisoning, roof collapse and gas explosions. Open cut mining hazards are principally mine wall failures and vehicle collisions. In the US alone, more than 100,000 coal miners have been killed in accidents over the past century, with more than 3,200 dying in 1907 alone.\n\nAccording to Benjamin K. Sovacool, 279 \"major\" energy accidents occurred from 1907 to 2007 and they caused 182,156 deaths with $41 billion in property damages, with these figures not including deaths from smaller accidents.\n\nHowever, by far the greatest energy fatalities that result from energy generation by humanity, is the creation of air pollution. The most lethal of which, particulate matter, which is primarily generated from the burning of fossil fuels and biomass is (counting outdoor air pollution effects only) estimated to cause 2.1 million deaths annually.\n\nAccording to Benjamin K. Sovacool, while responsible for less than 1 percent of the total number of energy accidents, hydroelectric facilities claimed 94 percent of reported immediate fatalities. Results on immediate fatalities are dominated by one disaster in which Typhoon Nina in 1975 washed out the Shimantan Dam (Henan Province, China) and 171,000 people perished. While the other major accident that involved greater than 1000 immediate deaths followed the rupture of the NNPC petroleum pipeline in 1998 and the resulting explosion. The other singular accident described by Sovacool is the \"predicted\" latent death toll of greater than 1000, as a result of the 1986 steam explosion at the Chernobyl nuclear reactor in the Ukraine. With approximately 4000 deaths in total, to eventually result in the decades ahead due to the radio-isotope pollution released.\n\nIn the oil and gas industry, the need for improved safety culture and training within companies is evidenced by the finding that workers new to a company are more likely to be involved in fatalities.\n\nCoal mining accidents resulted in 5,938 immediate deaths in 2005, and 4746 immediate deaths in 2006 in China alone according to the World Wildlife Fund. Coal mining is the most dangerous occupation in China, the death rate for every 100 tons of coal mined is 100 times that of the death rate in the US and 30 times that achieved in South Africa. Moreover, 600,000 Chinese coal miners, as of 2004, were suffering from Coalworker's pneumoconiosis (known as \"black lung\") a disease of the lungs caused by long-continued inhalation of coal dust. And the figure increases by 70,000 miners every year in China.\n\nHistorically, coal mining has been a very dangerous activity and the list of historical coal mining disasters is a long one. In the US alone, more than 100,000 coal miners were killed in accidents over the past century, with more than 3,200 dying in 1907 alone. In the decades following this peak, an annual death toll of 1,500 miner fatalities occurred every year in the US until approximately the 1970s. Coal mining fatalities in the US between 1990 and 2012 have continued to decline, with fewer than 100 each year. (See more Coal mining disasters in the United States)\n\nIn the United States, in the 2000s, after three decades of regulation on the Environmental impact of the coal industry, including regulations in the 1970s and 1990s from the Clean Air Act, an act created to cut down on pollution related deaths from fossil fuel usage, US coal fired power plants were estimated, in the 2000s, to continue to cause between 10,000 and 30,000 latent, or air pollution related deaths per year, due to the emissions of sulfur dioxide, nitrogen oxides and directly emitted particulate matter that result when coal is burnt.\n\nAccording to the World Health Organization in 2012, urban outdoor air pollution, from the burning of fossil fuels and biomass is estimated to cause 3 million deaths worldwide per year and indoor air pollution from biomass and fossil fuel burning is estimated to cause approximately 4.3 million premature deaths. In 2013 a team of researchers estimated the number of premature deaths caused by particulate matter in outdoor air pollution as 2.1 million, occurring annually.\n\nBenjamin Sovacool says that while hydroelectric plants were responsible for the most fatalities, nuclear power plants rank first in terms of their economic cost, accounting for 41 percent of all property damage. Oil and hydroelectric follow at around 25 percent each, followed by natural gas at 9 percent and coal at 2 percent. Excluding Chernobyl and the Shimantan Dam, the three other most expensive accidents involved the Exxon Valdez oil spill (Alaska), The Prestige oil spill (Spain), and the Three Mile Island nuclear accident (Pennsylvania). However analysis presented in the international Journal, \"Human and Ecological Risk Assessment\" found that coal, oil, Liquid petroleum gas and hydro accidents have cost more than nuclear power accidents.\n\nModern-day U.S. regulatory agencies frequently implement regulations on conventional pollution if one life or more is predicted saved per $6 million to $8 million of economic costs incurred.\n\n\n\n"}
{"id": "41731857", "url": "https://en.wikipedia.org/wiki?curid=41731857", "title": "Energy customer switching", "text": "Energy customer switching\n\nEnergy customer switching is a concept stemming from the global energy markets. The concept refers to the action of one energy customer switching energy supplier, a switch is essentially seen as the free (by choice) movement of a customer. In addition to that a switch can include:\n\n\nIf a customer moves, there is often a switch, however this will only be counted if the customer is not dealing with the incumbent in the new area of residence.\n\nThe above is the official definition of switching and is being used by public energy institutions such as CEER & ERGEG (forerunner to ACER). The definition was originally developed by Dr Philip E. Lewis, international switching expert.\n\nSwitching is a key concept to understanding competition-related issues on the global energy markets as the switching level of a concrete market reveals the state of the competition; High switching rates equals high level of competition and low switching rates equals limited competition. Thus measuring and assessing switching rates is necessary in order to have a correct impression of the energy markets. The action of switching is often done via a price comparison website or by the traditional door-to-door sales method, where a salesperson assists the customer in switching.\n\n"}
{"id": "321382", "url": "https://en.wikipedia.org/wiki?curid=321382", "title": "Energy flow (ecology)", "text": "Energy flow (ecology)\n\nIn ecology, energy flow, also called the calorific flow, refers to the flow of energy through a food chain, and is the focus of study in ecological energetics. In an ecosystem, ecologists seek to quantify the relative importance of different component species and feeding relationships.\n\nA general energy flow scenario follows:\n\nThe energy is passed on from trophic level to trophic level and each time about 90% of the energy is lost, with some being lost as heat into the environment (an effect of respiration) and some being lost as incompletely digested food (egesta). Therefore, primary consumers get about 10% of the energy produced by autotrophs, while secondary consumers get 1% and tertiary consumers get 0.1%. This means the top consumer of a food chain receives the least energy, as a lot of the food chain's energy has been lost between trophic levels. This loss of energy at each level limits typical food chains to only four to six links.\n\nEcological energetics appears to have grown out of the Age of Enlightenment and the concerns of the Physiocrats. It began in the works of Sergei Podolinsky in the late 1800s, and subsequently was developed by the Soviet ecologist Vladmir Stanchinsky, the Austro-American Alfred J. Lotka, and American limnologists, Raymond Lindeman and G. Evelyn Hutchinson. It underwent substantial development by Howard T. Odum and was applied by systems ecologists, and radiation ecologists.\n\n\n"}
{"id": "4772620", "url": "https://en.wikipedia.org/wiki?curid=4772620", "title": "Energy monitoring and targeting", "text": "Energy monitoring and targeting\n\nEnergy monitoring and targeting (M&T) is an energy efficiency technique based on the standard management axiom stating that “you cannot manage what you cannot measure”. M&T techniques provide energy managers with feedback on operating practices, results of energy management projects, and guidance on the level of energy use that is expected in a certain period. Importantly, they also give early warning of unexpected excess consumption caused by equipment malfunctions, operator error, unwanted user behaviours, lack of effective maintenance and the like.\n\nThe foundation of M&T lies in determining the normal relationships of energy consumptions to relevant driving factors (HVAC equipment, production though puts, weather, occupancy available daylight, etc.) and the goal is to help business managers:\n\n\nThe ultimate goal is to reduce energy costs through improved energy efficiency and energy management control. Other benefits generally include increased resource efficiency, improved production budgeting and reduction of greenhouse gas (GHG) emissions.\n\nM&T is an established technique that was first launched as a national program in the UK in 1980, and has since then spread throughout Europe. These techniques are now also rapidly growing in America.\n\nThroughout the numerous M&T projects implemented since the 1980s, a certain number of benefits have proved to be recurrent:\n\nMonitoring and Targeting techniques rely on three main principles, which form a constant feedback cycle, therefore improving control of energy use.\n\nMonitoring information of energy use, in order to establish a basis for energy management and explain deviations from an established pattern. Its primary goal is to maintain said pattern, by providing all the necessary data on energy consumption, as well as certain driving factors, as identified during preliminary investigation (production, weather, etc.)\n\nThe final principle is the one which enables ongoing control of energy use, achievement of targets and verification of savings: reports must be issued to the appropriate managers. This in turn allows decision-making and actions to be taken in order to achieve the targets, as well as confirmation or denial that the targets have been reached.\n\nBefore the M&T measures themselves are implemented, a few preparatory steps are necessary. First of all, key energy consumers on the site must be identified. Generally, most of the energy consumption is concentrated in a small number of processes, like heating, or certain machinery. This normally requires a certain survey of the building and the equipment to estimate their energy consumption level.\n\nIt is also necessary to assess what other measurements will be required to analyze the consumption appropriately. This data will be used to chart against the energy consumption: these are underlying factors which influence the consumption, often production (for industry processes) or exterior temperature (for heating processes), but may include many other variables.\n\nOnce all variables to be measured have been established, and the necessary meters installed, it is possible to initiate the M&T procedures.\n\nThe first step is to compile the data from the different meters. Low-cost energy feedback displays have become available. The frequency at which the data is compiled varies according to the desired reporting interval, but can go once every 30 seconds to once every 15 minutes. Some measurements can be taken directly from the meters, others must be calculated. These different measurements are often called streams or channels.\n\nDriving factors such as production or degree days also constitute streams and must be collected at intervals to match.\n\nThe data compiled must then be plotted on a graph in order to define the general consumption base-line. Consumption rates are plotted in a scatter plot against production or any other variable previously identified, and the best fit line is identified. This graph is the image of the business’ average energy performance, and conveys a lot of information:\n\nThe slope is not used quite as often for M&T purposes. However, a high y-intercept can mean that there is a fault in the process, causing it to use too much energy with no performance, unless there are specific distinctive features which lead to high base loads. Very scattered points, on the other hand, may reflect other significant factors playing in the variation of the energy consumption, other than the one plotted in the first place, but it can also be the illustration of a lack of control over the process.\n\nThe next step is to monitor the difference between the expected consumption and the actual measured consumption. One of the tools most commonly used for this is the CUSUM, which is the CUmulative SUM of differences. This consists in first calculating the difference between the expected and actual performances (the best fit line previously identified and the points themselves).\n\nThe CUSUM can then be plotted against time on a new graph, which then yields more information for the energy efficiency specialist. Variances scattered around zero usually mean that the process is operating normally. Marked variations, increasing or decreasing steadily usually reflect a modification in the conditions of the process.\n\nIn the case of the CUSUM graph, the slope becomes very important, as it is the main indicator of the savings achieved. A slope going steadily down indicates steady savings. Any variation in the slope indicates a change in the process. For example, in the graph on the right, the first section indicated no savings. However, in September (beginning of the yellow line), an energy efficiency measure must have been implemented, as savings start to occur. The green line indicates an increase in the savings (as the slope is becoming steeper), whereas the red line must reflect a modification in the process having occurred in November, as savings have decreased slightly.\n\nEnergy efficiency specialists, in collaboration with building managers, will decipher the CUSUM graph and identify the causes leading to variations in the consumption. This can be a change in behaviour, a modification to the process, different exterior conditions, etc. These changes must be monitored and the causes identified in order to promote and enhance good behaviour, and discourage bad ones.\n\nOnce the base line has been established, and causes for variations in energy consumption have been identified, it is time to set targets for the future. Now with all this information in hand, the targets are more realistic, as they are based on the building’s actual consumption.\nTargeting consists in two main parts: the measure to which the consumption can be reduced, and the timeframe during which the compression will be achieved.\n\nA good initial target is the best fit line identified during step 2. This line represents the average historical performance. Therefore, keeping all consumption below or equal to the historical average is an achievable target, yet remains a challenge as it involves eliminating high consumption peaks.\n\nSome companies, as they improve their energy consumption, might even decide to bring their average performance down to their historical best. This is considered a much more challenging target.\n\nThis brings us back to step 1: measure consumption. One of the specificities of M&T is that it is an ongoing process, requiring constant feedback in order to consistently improve performance. Once the targets are set and the desired measures are implemented, repeating the procedure from the start ensures that the managers are aware of the success or failure of the measures, and can then decide on further action.\n\nAn example with some features of an M&T application is the ASU Campus Metabolism, which provides real-time and historic energy use and generation data for facilities of Arizona State University on a public web site. Many utilities also offer customers electric interval data monitoring services. Xcel Energy is an example of an investor owned utility that offers its customer electric and natural gas monitoring services under the product name InfoWise from Xcel Energy which is administered by Power TakeOff, a third party partner.\n\n"}
{"id": "26356935", "url": "https://en.wikipedia.org/wiki?curid=26356935", "title": "Energy operator", "text": "Energy operator\n\nIn quantum mechanics, energy is defined in terms of the energy operator, acting on the wave function of the system as a consequence of time translation symmetry.\n\nIt is given by:\n\nIt acts on the wave function (the probability amplitude for different configurations of the system)\n\nThe energy operator corresponds to the full energy of a system. The Schrödinger equation describes the space- and time-dependence of the slow changing (non-relativistic) wave function of a quantum system. The solution of this equation for a bound system is discrete (a set of permitted states, each characterized by an energy level) which results in the concept of quanta.\n\nUsing the energy operator to the Schrödinger equation:\n\ncan be obtained:\n\nwhere \"i\" is the imaginary unit, \"ħ\" is the reduced Planck constant, and formula_5 is the Hamiltonian operator.\n\nIn a stationary state additionally occurs the time-independent Schrödinger equation:\nwhere \"E\" is an eigenvalue of energy.\n\nThe relativistic mass-energy relation:\n\nwhere again \"E\" = total energy, \"p\" = total 3-momentum of the particle, \"m\" = invariant mass, and \"c\" = speed of light, can similarly yield the Klein–Gordon equation:\n\nthat is:\n\nThe energy operator is easily derived from using the free particle wave function (plane wave solution to Schrödinger's equation). Starting in one dimension the wave function is\n\nThe time derivative of \"Ψ\" is\n\nBy the De Broglie relation:\n\nwe have\n\nRe-arranging the equation leads to\n\nwhere the energy factor \"E\" is a scalar value, the energy the particle has and the value that is measured. The partial derivative is a linear operator so this expression \"is\" the operator for energy:\n\nIt can be concluded that the scalar \"E\" is the eigenvalue of the operator, while formula_16 is the operator. Summarizing these results:\n\nFor a 3-d plane wave\n\nthe derivation is exactly identical, as no change is made to the term including time and therefore the time derivative. Since the operator is linear, they are valid for any linear combination of plane waves, and so they can act on any wave function without affecting the properties of the wave function or operators. Hence this must be true for any wave function. It turns out to work even in relativistic quantum mechanics, such as the Klein–Gordon equation above.\n\n"}
{"id": "53806811", "url": "https://en.wikipedia.org/wiki?curid=53806811", "title": "Final Straw: Food, Earth, Happiness", "text": "Final Straw: Food, Earth, Happiness\n\nFinal Straw: Food, Earth, Happiness is a documentary/art film released in June 2015 that takes audiences through farms and urban landscapes in Japan, South Korea, and the United States, interviewing leading practitioners in the Natural Farming movement. The film began when an environmental artist (Patrick M. Lydon) and an environmental book editor (Suhee Kang), had a chance meeting in Seoul, South Korea, and began conducting short interviews together with leaders in the ecology and social justice movements. Upon meeting Korean farmer Seong Hyun Choi however, the two were so impressed by his ecological mindset and way of working, that they set out to produce a feature film about the movement. Lydon and Kang ended up quitting their jobs, giving away most of their possessions, and becoming voluntarily homeless for four years in order to afford producing the film.\n\nThe film is split into three sections 1) Modern Life, 2) Foundations and Mindset of Natural Farming, and 3) Natural Farming in Practice and Life. According to the filmmakers, as they began to understand more about how natural farming itself was not rooted in methods, but in a way of thinking, they chose to explore the life philosophies and ways of thinking of natural farming practitioners in a more free-flowing and artistic way, rather than an instructive one; the result is an unconventional documentary that features slow paced musical interludes alongside interviews. Reviewers have called both \"meditative, and mindful,\" and \"an inspiring call to action.\" Author and musician Alicia Bay Laurel called the film \"both art and documentary\".\n\nLydon and Kang spent what they call a \"meager\" life savings to make the film, along with the volunteer efforts of farmers, translators, writers, musicians they had met during their journey. Although the film was filmed, written, and edited entirely by the two directors, they readily admit that the process of making the film was co-operative effort, with more than 200 volunteers directly involved in the process in some way. The soundtrack was recorded with professional musicians from each of the three countries where filming took place, all of whom donated their time to contribute to the film project. With the continued help of international volunteers, the film is available in four languages (English, Korean, Japanese, Vietnamese), and three more (Chinese, Portuguese, French) are in progress.\n\nFrustrated by the lack of distribution and film festival options for low- and no-budget films, the filmmakers made the decision to manage distribution and touring in the same way they went about filming, through co-operative effort. With the help of volunteers, independent theater owners, and community organizers, they launched an extensive tour throughout Japan and South Korea from 2015-2016, eventually screening the film at over 130 venues.\n\nRather than simply screening the film, the filmmakers decided to transition their existing media production organization \"SocieCity,\" into a vehicle for art and community engagement. They made a point of hosting interactive events along with their screenings and in several cases, stayed in communities for up to three months at a time to build natural gardens and host a project they call REALtimeFOOD, a grown-to-order restaurant which connects the ideas from the film with real-world practices in farming, food, and crafts. In most cases, these efforts were funded by grants from local philanthropic organizations and/or supported by the communities themselves.\n\nInterested in the unconventional way the film was being made and toured, multiple magazines and newspapers in Japan and Korea followed the directors during several parts of their journey, notably ESSEN, Bar and Dining, and Road magazines, and Shikoku Shinbun and Huffington Post newspapers.\n\nDuring the tour, the film was eventually picked up by festivals including Tassie Eco Film Festival and Belleville Doc Fest. \n\n"}
{"id": "6732384", "url": "https://en.wikipedia.org/wiki?curid=6732384", "title": "Graphical timeline of the Stelliferous Era", "text": "Graphical timeline of the Stelliferous Era\n\nThis is the timeline of the stelliferous era but also partly charts the primordial era, and charts more of the degenerate era of the heat death scenario.\n\nThe scale is formula_1. Example one million years is formula_2.\n\n"}
{"id": "1471036", "url": "https://en.wikipedia.org/wiki?curid=1471036", "title": "Graphical timeline of the universe", "text": "Graphical timeline of the universe\n\nThis more than 20-billion-year timeline of our universe shows the best estimates of major events from the universe's beginning to anticipated future events. Zero on the scale is the present day. A large step on the scale is one billion years; a small step, one hundred million years. The past is denoted by a minus sign: e.g., the oldest rock on Earth was formed about four billion years ago and this is marked at -4e+09 years, where 4e+09 represents 4 times 10 to the power of 9. The \"Big Bang\" event most likely happened 13.8 billion years ago; see age of the universe.\n\n"}
{"id": "40111102", "url": "https://en.wikipedia.org/wiki?curid=40111102", "title": "Intelligent Energy", "text": "Intelligent Energy\n\nIntelligent Energy is a fuel cell engineering company focused on the development and commercialisation of its PEM fuel cell technologies for a range of markets including automotive, stationary power and UAVs. We are headquartered in the UK, with offices and representation in the US, Japan, India, and China.\n\nThe origins of Intelligent Energy began at Loughborough University in the UK during the late 1980s, when the University became one of Europe’s first research and development centres for proton exchange membrane (PEM) fuel cell technology. In 1995, the UK’s first kW-level PEM fuel cell stack was produced by the R&D team. In June of that year, Advanced Power Sources (APS) Ltd was founded as a spin-out from Loughborough University by Paul Adcock, Phil Mitchell, Jon Moore and Anthony Newbold, and was the first company in the UK formed specifically to address the development and commercialisation of PEM fuel cells.\n\nFounded by Harry Bradbury, Intelligent Energy was established in 2001, acquiring Advanced Power Sources Ltd, together with its personnel and fuel cell related intellectual property that originated from research conducted by both APS and Loughborough University into PEM fuel cell technology. This triggered investment and enabled the company to grow its business activities.\nIn March 2005, it launched the ENV, the world’s first purpose-built fuel cell motorbike which gained the company recognition as a Technology Pioneer by the World Economic Forum in 2006. The ENV incorporated the company’s air-cooled fuel cell technology hybridised with a battery pack to provide 6 kW peak load to the motor to improve performance during spikes in power demand i.e. acceleration.\n\nIn 2007, a partnership was announced with Suzuki Motor Corporation to develop hydrogen fuel cells for a range of vehicles. In 2008, Intelligent Energy established the company, IE-CHP in a joint venture with SSE plc, to develop fuel cells and other technologies for CHP (Combined Heat and Power) applications. In the same year, Intelligent Energy also produced the power system for the first fuel cell powered manned flight in conjunction with Boeing.\nIn 2010, its fuel-cell taxi received The Engineer Technology and Innovation Award.\n\nIn March 2011, the Suzuki Burgman fuel cell scooter, equipped with Intelligent Energy’s fuel cell system, became the first fuel cell vehicle to achieve European Whole Vehicle Type Approval.\n\nIn 2012, SMILE FC System Corporation, a joint venture between Intelligent Energy and Suzuki Motor Corporation, was established to develop and manufacture air-cooled fuel cell systems for the automotive and a range of industry sectors.\nDuring the same year, a fleet of fuel cell taxis incorporating Intelligent Energy’s technology was used during the 2012 London Olympics. Part of the European Union-funded HyTEC (Hydrogen Technologies in European Cities) project launched in 2011, the taxis were used to transport VIP guests of the Mayor of London around the city.\nIn 2013, SMILE FC Corporation announced that it had established a ready-to-scale production line for its fuel cell systems, utilising Intelligent Energy’s semi-automated production technology. IE-CHP also received CE certification for its first-generation product, a 10 kWe/12 kWth combined heat and power (CHP) fuel cell. The certification allows the product to be sold in the European Economic Area, confirming that the product satisfies all the EU regulatory and conformity assessment procedures covering the design, manufacture, and testing of the system.\n\nIntelligent Energy was acquired by Meditor Energy, part of the Meditor Group, in October 2017.\n\nIntelligent Energy's fuel-cell technology is divided into two platforms: air-cooled (AC) and evaporatively-cooled (EC). The air-cooled fuel cell systems use low-power fans to provide cooling and the oxidant supply for operation. Heat from the fuel cell stack is conducted to cooling plates and removed through airflow channels, a simplified and cost-effective system for the power range from a few watts to several kilowatts. They are used in a wide range of UAV, stationary power and automotive applications for two-wheel and small car range extender applications.\n\nEvaporatively-cooled (EC) fuel cell systems provide power generation from a few kilowatts up to 200 kW. Efficient thermal management of the EC fuel cell stack reduces system complexity, mass and cost. These systems are designed for high-volume, low-cost manufacturing, and use modular architecture that can be quickly modified to suit the application.\n\nThe firm's fuel cell stacks have been developed for small and large cars, scooters and motorbikes. \nIn 2010, the company was involved in the development of the report entitled “A portfolio of power-trains for Europe: a fact-based analysis. The role of Battery Electric Vehicles, Plug-In Hybrids and Fuel Cell Electric Vehicles”, produced by McKinsey & Company with input from car manufacturers, oil and gas suppliers, utilities and industrial gas companies, wind turbine and electrolyser companies as well as governmental and non-governmental organisations. The report concluded, amongst other findings, that fuel cell vehicles are technology ready, and cost competitive, and that decarbonisation targets for Europe are unlikely to be met without the introduction of fuel cell powertrains.\n\nThe firm provides fuel cells to power UAVs and aerial drones. Its UAV Fuel Cell Modules run on hydrogen and ambient air to produce DC power in a lightweight package providing extended flight times when compared to battery systems.\n\nThe company’s fuel cell systems are used to provide diesel replacement and backup power initially for telecom towers but also for other sectors. The company has field proven its fuel cell products in the Indian telecommunications market with a tower uptime of close to 100%.\n\nThe company is a founding member of UKH Mobility, a government and industry group aiming to accelerate the commercial roll out of hydrogen vehicles in 2014/15;\nIt is also a member of the Fuel Cell and Hydrogen Energy Association (FCHEA), the US-based trade association for the fuel cell and hydrogen energy industry, dedicated to the commercialisation of fuel cells and hydrogen energy technologies.\n\n"}
{"id": "3591456", "url": "https://en.wikipedia.org/wiki?curid=3591456", "title": "Interface (matter)", "text": "Interface (matter)\n\nIn the physical sciences, an interface is the boundary between two spatial regions occupied by different matter, or by matter in different physical states. The interface between matter and air, or matter and vacuum, is called a surface, and studied in surface science. In thermal equilibrium, the regions in contact are called phases, and the interface is called a phase boundary. An example for an interface out of equilibrium is the grain boundary in polycrystalline matter.\n\nThe importance of the interface depends on the type of system: the bigger the quotient area/volume, the greater the effect the interface will have. Consequently, interfaces are very important in systems with large interface area-to-volume ratios, such as colloids.\n\nInterfaces can be flat or curved. For example, oil droplets in a salad dressing are spherical but the interface between water and air in a glass of water is mostly flat.\n\nSurface tension is the physical property which rules interface processes involving liquids. For a liquid film on flat surfaces, the liquid-vapor interface keeps flat to minimize interfacial area and system free energy. For a liquid film on rough surfaces, the surface tension tends to keep the meniscus flat, while the disjoining pressure makes the film conformal to the substrate. The equilibrium meniscus shape is a result of the competition between the capillary pressure and disjoining pressure.\n\nInterfaces may cause various optical phenomena, such as refraction. Optical lenses serve as an example of a practical application of the interface between glass and air.\n\nOne topical interface system is the gas-liquid interface between aerosols and other atmospheric molecules.\n\n"}
{"id": "9332507", "url": "https://en.wikipedia.org/wiki?curid=9332507", "title": "Leibniz–Clarke correspondence", "text": "Leibniz–Clarke correspondence\n\nThe Leibniz–Clarke correspondence was a scientific, theological and philosophical debate conducted in an exchange of letters between the German thinker Gottfried Wilhelm Leibniz and Samuel Clarke, an English supporter of Isaac Newton during the years 1715 and 1716. The exchange began because of a letter Leibniz wrote to Caroline of Ansbach, in which he remarked that Newtonian physics was detrimental to natural theology. Eager to defend the Newtonian view, Clarke responded, and the correspondence continued until the death of Leibniz in 1716.\n\nAlthough a variety of subjects is touched on in the letters, the main interest for modern readers is in the dispute between the absolute theory of space favoured by Newton and Clarke, and Leibniz's relational approach. Also important is the conflict between Clarke's and Leibniz's opinions on free will and whether God must create the best of all possible worlds.\n\nLeibniz had published only a book on moral matters, the \"Theodicée\" (1710), and his more metaphysical views had never been exposed to a sufficient extent, so the collected letters were met with interest by their contemporaries. The priority dispute between Leibniz and Newton about the calculus was still fresh in the public's mind and it was taken as a matter of course that it was Newton himself who stood behind Clarke's replies.\n\nThe Leibniz-Clarke letters were first published under Clarke's name in the year following Leibniz' death. He wrote a preface, took care of the translation from French, added notes and some of his own writing. In 1720 Pierre Desmaizeaux published a similar volume in a French translation, including quotes from Newton's work. It is quite certain that for both editions the opinion of Newton himself has been sought and Leibniz left at a disadvantage. However the German translation of the correspondence published by Kohler, also in 1720, contained a reply to Clarke's last letter which Leibniz had not been able to answer. The letters have been reprinted in most collections of Leibniz' works and regularly published in stand alone editions.\n\n\n\n"}
{"id": "3595285", "url": "https://en.wikipedia.org/wiki?curid=3595285", "title": "Maximum power principle", "text": "Maximum power principle\n\nThe maximum power principle or Lotka's principle has been proposed as the fourth principle of energetics in open system thermodynamics, where an example of an open system is a biological cell. According to Howard T. Odum, \"The maximum power principle can be stated: During self-organization, system designs develop and prevail that maximize power intake, energy transformation, and those uses that reinforce production and efficiency.\"\n\nChen (2006) has located the origin of the statement of maximum power as a formal principle in a tentative proposal by Alfred J. Lotka (1922a, b). Lotka's statement sought to explain the Darwinian notion of evolution with reference to a physical principle. Lotka's work was subsequently developed by the systems ecologist Howard T. Odum in collaboration with the Chemical Engineer Richard C. Pinkerton, and later advanced by the Engineer Myron Tribus.\n\nWhile Lotka's work may have been a first attempt to formalise evolutionary thought in mathematical terms, it followed similar observations made by Leibniz and Volterra and Ludwig Boltzmann, for example, throughout the sometimes controversial history of natural philosophy. In contemporary literature it is most commonly associated with the work of Howard T. Odum.\n\nThe significance of Odum's approach was given greater support during the 1970s, amid times of oil crisis, where, as Gilliland (1978, pp. 100) observed, there was an emerging need for a new method of analysing the importance and value of energy resources to economic and environmental production. A field known as energy analysis, itself associated with net energy and EROEI, arose to fulfill this analytic need. However, in energy analysis intractable theoretical and practical difficulties arose when using the energy unit to understand, a) the conversion among concentrated fuel types (or energy types), b) the contribution of labour, and c) the contribution of the environment.\n\nLotka said (1922b: 151): \nGilliland noted that these difficulties in analysis in turn required some new theory to adequately explain the interactions and transactions of these different energies (different concentrations of fuels, labour and environmental forces). Gilliland (Gilliland 1978, p. 101) suggested that Odum's statement of the maximum power principle (H.T.Odum 1978, pp. 54–87) was, perhaps, an adequate expression of the requisite theory:\nThis theory Odum called maximum power theory. In order to formulate maximum power theory Gilliland observed that Odum had added another law (the maximum power principle) to the already well established laws of thermodynamics. In 1978 Gilliland wrote that Odum's new law had not yet been validated (Gilliland 1978, p. 101). Gilliland stated that in maximum power theory the second law efficiency of thermodynamics required an additional physical concept: \"the concept of second law efficiency under maximum power\" (Gilliland 1978, p. 101):\nIn this way the concept of maximum power was being used as a principle to quantitatively describe the selective law of biological evolution. Perhaps H.T.Odum's most concise statement of this view was (1970, p. 62):\n\nThe Odum–Pinkerton approach to Lotka's proposal was to apply Ohm's law – and the associated maximum power theorem (a result in electrical power systems) – to ecological systems. Odum and Pinkerton defined \"power\" in electronic terms as the rate of work, where Work is understood as a \"useful energy transformation\". The concept of maximum power can therefore be defined as the \"maximum rate of useful energy transformation\". Hence the underlying philosophy aims to unify the theories and associated laws of electronic and thermodynamic systems with biological systems. This approach presupposed an analogical view which sees the world as an ecological-electronic-economic engine.\n\nOdum et al. viewed the maximum power theorem as a principle of power-efficiency reciprocity selection with wider application than just electronics. For example, Odum saw it in open systems operating on solar energy, like both photovoltaics and photosynthesis (1963, p. 438). Like the maximum power theorem, Odum's statement of the maximum power principle relies on the notion of 'matching', such that high-quality energy maximizes power by matching and amplifying energy (1994, pp. 262, 541): \"in surviving designs a matching of high-quality energy with larger amounts of low-quality energy is likely to occur\" (1994, p. 260). As with electronic circuits, the resultant rate of energy transformation will be at a maximum at an intermediate power efficiency. In 2006, T.T. Cai, C.L. Montague and J.S. Davis said that, \"The maximum power principle is a potential guide to understanding the patterns and processes of ecosystem development and sustainability. The principle predicts the selective persistence of ecosystem designs that capture a previously untapped energy source.\" (2006, p. 317). In several texts H.T. Odum gave the Atwood machine as a practical example of the 'principle' of maximum power.\n\nThe mathematical definition given by H.T. Odum is formally analogous to the definition provided on the maximum power theorem article. (For a brief explanation of Odum's approach to the relationship between ecology and electronics see Ecological Analog of Ohm's Law)\n\nWhether or not the principle of maximum power efficiency can be considered the fourth law of thermodynamics and the fourth principle of energetics is moot. Nevertheless, H.T. Odum also proposed a corollary of maximum power as the organisational principle of evolution, describing the evolution of microbiological systems, economic systems, planetary systems, and astrophysical systems. He called this corollary the maximum empower principle. This was suggested because, as S.E. Jorgensen, M.T. Brown, H.T. Odum (2004) note,\n\nC. Giannantoni may have confused matters when he wrote \"The \"Maximum Em-Power Principle\" (Lotka–Odum) is generally considered the \"Fourth Thermodynamic Principle\" (mainly) because of its practical validity for a very wide class of physical and biological systems\" (C. Giannantoni 2002, § 13, p. 155). Nevertheless, Giannantoni has proposed the Maximum Em-Power Principle as the fourth principle of thermodynamics (Giannantoni 2006).\n\nThe preceding discussion is incomplete. The \"maximum power\" was discovered several times independently, in physics and engineering, see: Novikov (1957), El-Wakil (1962), and Curzon and Ahlborn (1975). The incorrectness of this analysis and design evolution conclusions was demonstrated by Gyftopoulos (2002).\n\n\n"}
{"id": "52634071", "url": "https://en.wikipedia.org/wiki?curid=52634071", "title": "Nature-based solutions", "text": "Nature-based solutions\n\nNature-based solutions (NBS or NbS) refers to the sustainable management and use of nature for tackling socio-environmental challenges. The challenges include issues such as climate change, water security, water pollution, food security, human health, and disaster risk management. \n\nA definition by the European Union states that these solutions are \"inspired and supported by nature, which are cost-effective, simultaneously provide environmental, social and economic benefits and help build resilience. The Nature-based Solutions Initiative meanwhile defines them as \"actions that work with and enhance nature so as to help people adapt to change and disasters\". Such solutions bring more, and more diverse, nature and natural features and processes into cities, landscapes and seascapes, through locally adapted, resource-efficient and systemic interventions\". With NBS, healthy, resilient and diverse ecosystems (whether natural, managed or newly created) can provide solutions for the benefit of societies and overall biodiversity.\n\nFor instance, the restoration or protection of mangroves along coastlines utilizes a nature-based solution to accomplish several things. Mangroves moderate the impact of waves and wind on coastal settlements or cities and sequester CO . They also provide safe nurseries for marine life that can be the basis for sustaining populations of fish that local populations may depend on. Additionally, the mangrove forests can help control coastal erosion resulting from sea level rise. Similarly, in cities green roofs or walls are nature-based solutions that can be used to moderate the impact of high temperatures, capture storm water, abate pollution, and act as carbon sinks, while enhancing biodiversity.\n\nConservation approaches and environment management initiatives have been carried out for decades. What is new is that the benefits of such nature-based solutions to human well-being have been articulated well more recently. Even if the term itself is still being framed, examples of nature-based solutions can be found all over the world, and imitated. Nature-based solutions are on their way to being mainstreamed in national and international policies and programmes (e.g. climate change policy, law, infrastructure investment and financing mechanisms). For example, the theme for World Water Day 2018 was \"Nature for water\" and by UN-Water's accompanying UN World Water Development Report had the title \"Nature-based Solutions for Water\".\n\nSocieties increasingly face challenges such as climate change, urbanization, jeopardized food security and water resource provision, and disaster risk. One approach to answer these challenges is to singularly rely on technological strategies. An alternative approach is to manage the (socio-)ecological systems in a comprehensive way in order to sustain and potentially increase the delivery of ecosystem services to humans. In this context, nature-based solutions (NBS) have recently been put forward by practitioners and quickly thereafter by policymakers. These solutions stress the sustainable use of nature in solving coupled environmental-social-economic challenges. \n\nWhile ecosystem services are often valued in terms of immediate benefits to human well-being and economy, NBS focus on the benefits to people and the environment itself, to allow for sustainable solutions that are able to respond to environmental change and hazards in the long-term. NBS go beyond the traditional biodiversity conservation and management principles by \"re-focusing\" the debate on humans and specifically integrating societal factors such as human well-being and poverty reduction, socio-economic development, and governance principles. \n\nWith respect to water issues, NBS can achieve the following, according to the World Water Development Report 2018 by UN-Water: \n\nIn this sense, NBS are strongly connected to ideas such as natural systems agriculture, natural solutions, ecosystem-based approaches, adaptation services, natural infrastructure, green infrastructure and ecological engineering. For instance, ecosystem-based approaches are increasingly promoted for climate change adaptation and mitigation by organisations like United Nations Environment Programme and non-governmental organisations such as The Nature Conservancy. These organisations refer to \"policies and measures that take into account the role of ecosystem services in reducing the vulnerability of society to climate change, in a multi-sectoral and multi-scale approach\".\n\nLikewise, natural infrastructure is defined as a \"strategically planned and managed network of natural lands, such as forests and wetlands, working landscapes, and other open spaces that conserves or enhances ecosystem values and functions and provides associated benefits to human populations\"; and green infrastructure refers to an \"interconnected network of green spaces that conserves natural systems and provides assorted benefits to human populations\".\n\nSimilarly, the concept of ecological engineering generally refers to \"protecting, restoring (i.e. ecosystem restoration) or modifying ecological systems to increase the quantity, quality and sustainability of particular services they provide, or to build new ecological systems that provide services that would otherwise be provided through more conventional engineering, based on non-renewable resources\".\n\nThe International Union for the Conservation of Nature (IUCN) defines NBS as actions to protect, sustainably manage, and restore natural or modified ecosystems, that address societal challenges effectively and adaptively, simultaneously providing human well-being and biodiversity benefits, with climate change, food security, disaster risks, water security, social and economic development as well as human health being the common societal challenges.\n\nIUCN proposes to consider NBS as an umbrella concept. Categories and examples of NBS approaches according to IUCN include:\n\nThe general objective of NBS is clear, namely the sustainable management and use of nature for tackling societal challenges. However, different stakeholders view NBS from other perspectives. For instance, IUCN defines NBS as \"actions to protect, sustainably manage and restore natural or modified ecosystems, which address societal challenges effectively and adaptively, while simultaneously providing human well-being and biodiversity benefits\". This framing puts the need for well-managed and restored ecosystems at the heart of NBS, with the overarching goal of \"Supporting the achievement of society's development goals and safeguard human well-being in ways that reflect cultural and societal values and enhance the resilience of ecosystems, their capacity for renewal and the provision of services\". \n\nIn the context of the ongoing political debate on jobs and growth (main drivers of the current EU policy agenda), the European Commission underlines that NBS can transform environmental and societal challenges into innovation opportunities, by turning natural capital into a source for green growth and sustainable development. In their view, NBS to societal challenges are \"solutions that are inspired and supported by nature, which are cost-effective, simultaneously provide environmental, social and economic benefits and help build resilience. Such solutions bring more, and more diverse, nature and natural features and processes into cities, landscapes and seascapes, through locally adapted, resource-efficient and systemic interventions.\" \n\nThis framing is somewhat broader, and puts economy and social assets at the heart of NBS as importantly as sustaining environmental conditions. It shares similarities with the definition proposed by Maes and Jacobs (2015) defining NBS as \"any transition to a use of ES with decreased input of non-renewable natural capital and increased investment in renewable natural processes\". In their view, development and evaluation of NBS spans three basic requirements: (1) decrease of fossil fuel input per produced unit; (2) lowering of systemic trade-offs and increasing synergies between ES; and (3) increasing labor input and jobs. Here, nature is seen as a tool to inspire more systemic solutions to societal problems.\n\nWhatever definition used, promoting sustainability and the increased role of natural, self-sustained processes relying on biodiversity, are inherent to NBS. They constitute actions easily seen as positive for a wide range of stakeholders, as they bring about benefits at environmental, economic and social levels. As a consequence, the concept of NBS is gaining acceptance outside the conservation community (e.g. urban planning) and is now on its way to be mainstreamed into policies and programmes (climate change policy, law, infrastructure investment and financing mechanisms).\n\nIn 2015, the European network BiodivERsA mobilized a range of scientists, research donors and stakeholders and proposed a typology characterizing NBS along two gradients. 1. \"how much engineering of biodiversity and ecosystems is involved in NBS\", and 2. \"how many ecosystem services and stakeholder groups are targeted by a given NBS\". The typology highlights that NBS can involve very different actions on ecosystems (from protection to management and even creation of new ecosystems) and is based on the assumption that the higher the number of services and stakeholder groups targeted, the lower the capacity to maximize the delivery of each service and simultaneously fulfil the specific needs of all stakeholder groups. As such, three types of NBS are distinguished (Figure 2):\n\nType 1 NBS consists of no or minimal intervention in ecosystems, with the objectives of maintaining or improving the delivery of a range of ES both inside and outside of these conserved ecosystems. Examples include the protection of mangroves in coastal areas to limit risks associated to extreme weather conditions and provide benefits and opportunities to local populations; and the establishment of marine protected areas to conserve biodiversity within these areas while exporting biomass into fishing grounds. This type of NBS is connected to, for example, the concept of biosphere reserves which incorporates core protected areas for nature conservation and buffer zones and transition areas where people live and work in a sustainable way.\n\nType 2 NBS corresponds to management approaches that develop sustainable and multifunctional ecosystems and landscapes (extensively or intensively managed). These types improve the delivery of selected ES compared to what would be obtained with a more conventional intervention. Examples include innovative planning of agricultural landscapes to increase their multi-functionality; and approaches for enhancing tree species and genetic diversity to increase forest resilience to extreme events. This type of NBS is strongly connected to concepts like natural systems agriculture, agro-ecology, and evolutionary-orientated forestry.\n\nType 3 NBS consists of managing ecosystems in very extensive ways or even creating new ecosystems (e.g., artificial ecosystems with new assemblages of organisms for green roofs and walls to mitigate city warming and clean polluted air). Type 3 is linked to concepts like green and blue infrastructures and objectives like restoration of heavily degraded or polluted areas and greening cities.\n\nType 1 and 2 would typically fall within the IUCN NBS framework, whereas Type 2 and moreover Type 3 are often exemplified by EC for turning natural capital into a source for green growth and sustainable development.\n\nHybrid solutions exist along this gradient both in space and time. For instance, at landscape scale, mixing protected and managed areas could be needed to fulfil multi-functionality and sustainability goals. Similarly, a constructed wetland can be developed as a type 3 but, when well established, may subsequently be preserved and surveyed as a type 1.\n\nDemonstrating the benefits of nature and healthy ecosystems and showcasing the return on investment they can offer is necessary in order to increase awareness, but also to provide support and guidance on how to implement NBS. A large number of initiatives around the world already highlight the effectiveness of NBS approaches to address a wide range of societal challenges.\n\nThe following table shows examples from around the world:\n\nIn 2018, The Hindu reported that the East Kolkata wetlands, the world's largest organic sewage treatment facility had been used to clean the sewage of Kolkata in an organic manner by using algae for several decades. In use since the 1930s, the natural system was discovered by Dhrubajyoti Ghosh, an ecologist and a municipal engineer in the 1970s while working in the region. Ghosh worked for decades to protect the wetlands. It had been a practice in Kolkata, one of the five largest cities in India, for the municipal authorities to pump sewage into shallow ponds (\"bheris\"). Under the heat of the tropical sun, algae proliferated in them, converting the sewage into clean water, which in turn was used by villagers to grow paddy and vegetables. This system has been in use in the region since the 1930s and treats 750 million litres of wastewater per day, giving livelihood to 100,000 people in the vicinity. For his work, Ghosh was included in the UN Global 500 Roll of Honour in 1990 and received the Luc Hoffmann award in 2016.\n\nThere is currently no accepted basis on which a government agency, municipality or private company can systematically assess the efficiency, effectiveness and sustainability of a particular nature-based solution. However, a series of principles are proposed to guide effective and appropriate implementation, and thus to upscale NBS in practice. For example, NBS embrace and are not meant to replace nature conservation norms. Also, NBS are determined by site-specific natural and cultural contexts that include traditional, local and scientific knowledge. NBS are an integral part of the overall design of policies, and measure or actions, to address a specific challenges. Finally, NBS can be implemented alone or in an integrated manner with other solutions to societal challenges (e.g. technological and engineering solutions) and they are applied at the landscape scale.\n\nImplementing NBS requires political, economic, and scientific challenges to be tackled. First and foremost, private sector investment is needed, not to replace but to supplement traditional sources of capital such as public funding or philanthropy. The challenge is therefore to provide a robust evidence base for the contribution of nature to economic growth and jobs, and to demonstrate the economic viability of these solutions – compared to technological ones – on a timescale compatible with that of global change. Furthermore, it requires measures like adaptation of economic subsidy schemes, and the creation of opportunities for conservation finance, to name a few. Indeed, such measures will be needed to scale up NBS interventions, and strengthen their impact in mitigating the world's most pressing challenges.\n\nSince 2016, the EU is supporting a multi-stakeholder dialogue platform (called ThinkNature) to promote the co-design, testing and deployment of improved and innovative NBS in an integrated way. Creation of such science-policy-business-society interfaces could promote the market uptake of NBS. The project is part of the EU’s Horizon 2020 – Research and Innovation programme, and will last for 3 years. There are a total of 17 international partners involved, including the Technical University of Crete (Project Leader), the University of Helsinki and BiodivERsA.\n\nIn 2017, as part of the Presidency of the Estonian Republic of the Council of the European Union, a conference called “Nature-based Solutions: From Innovation to Common-use” was organized by the Ministry of the Environment of Estonia and the University of Tallinn. This conference aimed to strengthen synergies among various recent initiatives and programs related to NBS launched by the European Commission and by the EU Member States, focusing on policy and governance of NBS, and on research and innovation.\n\nIn recognition of the importance of natural ecosystems for mitigation and adaptation, the Paris Agreement calls on all Parties to acknowledge “the importance of the conservation and enhancement, as appropriate, of sinks and reservoirs of the greenhouse gases” and to “note the importance of ensuring the integrity of all ecosystems, including oceans, and the protection of biodiversity, recognized by some cultures as Mother Earth”. It then includes in its Articles several references to nature-based solutions. For example, Article 5.2 encourages Parties to adopt “…policy approaches and positive incentives for activities relating to reducing emissions from deforestation and forest degradation, and the role of conservation and sustainable management of forests and enhancement of forest carbon stocks in developing countries; and alternative policy approaches, such as joint mitigation and adaptation approaches for the integral and sustainable management of forests, while reaffirming the importance of incentivizing, as appropriate, non-carbon benefits associated with such approaches”. Article 7.1 further encourages Parties to build the resilience of socioeconomic and ecological systems, including through economic diversification and sustainable management of natural resources. In total, the Agreement refers to nature (ecosystems, natural resources, forests) in 13 distinct places. An in-depth analysis of all Nationally Determined Contributions submitted to UNFCCC, revealed that around 130 NDCs or 65% of signatories commit to nature-based solutions in their climate pledges, suggesting broad consensus for the role of nature in helping meet climate change goals. However, high-level commitments rarely translate into robust, measurable actions on-the-ground.\n\nThe term NBS was put forward by practitioners in the late 2000s (in particular the International Union for the Conservation of Nature and the World Bank) and thereafter by policymakers in Europe (most notably the European Commission). \n\nThe term \"nature-based solutions\" was first used in the late 2000s. It was used in the context of finding new solutions to mitigate and adapt to climate change effects, whilst simultaneously protecting biodiversity and improving sustainable livelihoods. \n\nThe IUCN referred to NBS in a position paper for the United Nations Framework Convention on Climate Change. The term was also adopted by European policymakers, in particular by the European Commission in a report stressing that NBS can offer innovative means to create jobs and growth as part of a green economy. The term started to make appearances in the mainstream media around the time of the Global Climate Action Summit in California in September 2018 \n\n\n"}
{"id": "251399", "url": "https://en.wikipedia.org/wiki?curid=251399", "title": "Observable universe", "text": "Observable universe\n\nThe observable universe is a spherical region of the Universe comprising all matter that can be observed from Earth at the present time, because electromagnetic radiation from these objects has had time to reach Earth since the beginning of the cosmological expansion. There are at least 2 trillion galaxies in the observable universe. Assuming the Universe is isotropic, the distance to the edge of the observable universe is roughly the same in every direction. That is, the observable universe has a spherical volume (a ball) centered on the observer. Every location in the Universe has its own observable universe, which may or may not overlap with the one centered on Earth.\n\nThe word \"observable\" in this sense does not refer to the capability of modern technology to detect light or other information from an object, or whether there is anything to be detected. It refers to the physical limit created by the speed of light itself. Because no signals can travel faster than light, any object farther away from us than light could travel in the age of the Universe (estimated around years) simply cannot be detected, as they have not reached us yet. Sometimes astrophysicists distinguish between the \"visible\" universe, which includes only signals emitted since recombination—and the \"observable\" universe, which includes signals since the beginning of the cosmological expansion (the Big Bang in traditional physical cosmology, the end of the inflationary epoch in modern cosmology).\n\nAccording to calculations, the current \"comoving distance\"—proper distance, which takes into account that the universe has expanded since the light was emitted—to particles from which the cosmic microwave background radiation (CMBR) was emitted, which represent the radius of the visible universe, is about 14.0 billion parsecs (about 45.7 billion light-years), while the comoving distance to the edge of the observable universe is about 14.3 billion parsecs (about 46.6 billion light-years), about 2% larger. The radius of the observable universe is therefore estimated to be about 46.5 billion light-years and its diameter about 28.5 gigaparsecs (93 billion light-years, ). The total mass of ordinary matter in the universe can be calculated using the critical density and the diameter of the observable universe to be about 1.5×10 kg.\n\nSince the expansion of the universe is known to accelerate and will become exponential in the future, the light emitted from all distant objects, past some time dependent on their current redshift, will never reach the Earth. In the future all currently observable objects will slowly freeze in time while emitting progressively redder and fainter light. For instance, objects with the current redshift \"z\" from 5 to 10 will remain observable for no more than 4–6 billion years. In addition, light emitted by objects currently situated beyond a certain comoving distance (currently about 19 billion parsecs) will never reach Earth.\n\nSome parts of the universe are too far away for the light emitted since the Big Bang to have had enough time to reach Earth, and so lie outside the observable universe. In the future, light from distant galaxies will have had more time to travel, so additional regions will become observable. However, due to Hubble's law, regions sufficiently distant from the Earth are expanding away from it faster than the speed of light (special relativity prevents nearby objects in the same local region from moving faster than the speed of light with respect to each other, but there is no such constraint for distant objects when the space between them is expanding; see uses of the proper distance for a discussion) and furthermore the expansion rate appears to be accelerating due to dark energy. Assuming dark energy remains constant (an unchanging cosmological constant), so that the expansion rate of the universe continues to accelerate, there is a \"future visibility limit\" beyond which objects will \"never\" enter our observable universe at any time in the infinite future, because light emitted by objects outside that limit would never reach the Earth. (A subtlety is that, because the Hubble parameter is decreasing with time, there can be cases where a galaxy that is receding from the Earth just a bit faster than light does emit a signal that reaches the Earth eventually.) This future visibility limit is calculated at a comoving distance of 19 billion parsecs (62 billion light-years), assuming the universe will keep expanding forever, which implies the number of galaxies that we can ever theoretically observe in the infinite future (leaving aside the issue that some may be impossible to observe in practice due to redshift, as discussed in the following paragraph) is only larger than the number currently observable by a factor of 2.36.\n\nThough in principle more galaxies will become observable in the future, in practice an increasing number of galaxies will become extremely redshifted due to ongoing expansion, so much so that they will seem to disappear from view and become invisible. An additional subtlety is that a galaxy at a given comoving distance is defined to lie within the \"observable universe\" if we can receive signals emitted by the galaxy at any age in its past history (say, a signal sent from the galaxy only 500 million years after the Big Bang), but because of the universe's expansion, there may be some later age at which a signal sent from the same galaxy can \"never\" reach the Earth at any point in the infinite future (so, for example, we might never see what the galaxy looked like 10 billion years after the Big Bang), even though it remains at the same comoving distance (comoving distance is defined to be constant with time—unlike proper distance, which is used to define recession velocity due to the expansion of space), which is less than the comoving radius of the observable universe. This fact can be used to define a type of cosmic event horizon whose distance from the Earth changes over time. For example, the current distance to this horizon is about 16 billion light-years, meaning that a signal from an event happening \"at present\" can eventually reach the Earth in the future if the event is less than 16 billion light-years away, but the signal will never reach the Earth if the event is more than 16 billion light-years away.\n\nBoth popular and professional research articles in cosmology often use the term \"universe\" to mean \"observable universe\". This can be justified on the grounds that we can never know anything by direct experimentation about any part of the universe that is causally disconnected from the Earth, although many credible theories require a total universe much larger than the observable universe. No evidence exists to suggest that the boundary of the observable universe constitutes a boundary on the universe as a whole, nor do any of the mainstream cosmological models propose that the universe has any physical boundary in the first place, though some models propose it could be finite but unbounded, like a higher-dimensional analogue of the 2D surface of a sphere that is finite in area but has no edge. It is plausible that the galaxies within our observable universe represent only a minuscule fraction of the galaxies in the universe. According to the theory of cosmic inflation initially introduced by its founder, Alan Guth (and by D. Kazanas ), if it is assumed that inflation began about 10 seconds after the Big Bang, then with the plausible assumption that the size of the universe before the inflation occurred was approximately equal to the speed of light times its age, that would suggest that at present the entire universe's size is at least 3×10 times the radius of the observable universe. There are also lower estimates claiming that the entire universe is in excess of 250 times larger than the observable universe and also higher estimates implying that the universe is at least 10 times larger than the observable universe.\n\nIf the universe is finite but unbounded, it is also possible that the universe is \"smaller\" than the observable universe. In this case, what we take to be very distant galaxies may actually be duplicate images of nearby galaxies, formed by light that has circumnavigated the universe. It is difficult to test this hypothesis experimentally because different images of a galaxy would show different eras in its history, and consequently might appear quite different. Bielewicz et al. claims to establish a lower bound of 27.9 gigaparsecs (91 billion light-years) on the diameter of the last scattering surface (since this is only a lower bound, the paper leaves open the possibility that the whole universe is much larger, even infinite). This value is based on matching-circle analysis of the WMAP 7 year data. This approach has been disputed.\n\nThe comoving distance from Earth to the edge of the observable universe is about 14.26 gigaparsecs (46.5 billion light-years or ) in any direction. The observable universe is thus a sphere with a diameter of about 28.5 gigaparsecs (93 billion light-years or ). Assuming that space is roughly flat (in the sense of being a Euclidean space), this size corresponds to a comoving volume of about ( or ).\n\nThe figures quoted above are distances \"now\" (in cosmological time), not distances \"at the time the light was emitted\". For example, the cosmic microwave background radiation that we see right now was emitted at the time of photon decoupling, estimated to have occurred about years after the Big Bang, which occurred around 13.8 billion years ago. This radiation was emitted by matter that has, in the intervening time, mostly condensed into galaxies, and those galaxies are now calculated to be about 46 billion light-years from us. To estimate the distance to that matter at the time the light was emitted, we may first note that according to the Friedmann–Lemaître–Robertson–Walker metric, which is used to model the expanding universe, if at the present time we receive light with a redshift of \"z\", then the scale factor at the time the light was originally emitted is given by\n\nformula_1.\n\nWMAP nine-year results combined with other measurements give the redshift of photon decoupling as \"z\" = , which implies that the scale factor at the time of photon decoupling would be . So if the matter that originally emitted the oldest CMBR photons has a \"present\" distance of 46 billion light-years, then at the time of decoupling when the photons were originally emitted, the distance would have been only about 42 \"million\" light-years.\n\nMany secondary sources have reported a wide variety of incorrect figures for the size of the visible universe. Some of these figures are listed below, with brief descriptions of possible reasons for misconceptions about them.\n\n\n\n\n\n\n\nSky surveys and mappings of the various wavelength bands of electromagnetic radiation (in particular 21-cm emission) have yielded much information on the content and character of the universe's structure. The organization of structure appears to follow as a hierarchical model with organization up to the scale of superclusters and filaments. Larger than this (at scales between 30 and 200 megaparsecs), there seems to be no continued structure, a phenomenon that has been referred to as the \"End of Greatness\".\n\nThe organization of structure arguably begins at the stellar level, though most cosmologists rarely address astrophysics on that scale. Stars are organized into galaxies, which in turn form galaxy groups, galaxy clusters, superclusters, sheets, walls and filaments, which are separated by immense voids, creating a vast foam-like structure sometimes called the \"cosmic web\". Prior to 1989, it was commonly assumed that virialized galaxy clusters were the largest structures in existence, and that they were distributed more or less uniformly throughout the universe in every direction. However, since the early 1980s, more and more structures have been discovered. In 1983, Adrian Webster identified the Webster LQG, a large quasar group consisting of 5 quasars. The discovery was the first identification of a large-scale structure, and has expanded the information about the known grouping of matter in the universe. In 1987, Robert Brent Tully identified the Pisces–Cetus Supercluster Complex, the galaxy filament in which the Milky Way resides. It is about 1 billion light-years across. That same year, an unusually large region with a much lower than average distribution of galaxies was discovered, the Giant Void, which measures 1.3 billion light-years across. Based on redshift survey data, in 1989 Margaret Geller and John Huchra discovered the \"Great Wall\", a sheet of galaxies more than 500 million light-years long and 200 million light-years wide, but only 15 million light-years thick. The existence of this structure escaped notice for so long because it requires locating the position of galaxies in three dimensions, which involves combining location information about the galaxies with distance information from redshifts.\nTwo years later, astronomers Roger G. Clowes and Luis E. Campusano discovered the Clowes–Campusano LQG, a large quasar group measuring two billion light-years at its widest point which was the largest known structure in the universe at the time of its announcement. In April 2003, another large-scale structure was discovered, the Sloan Great Wall. In August 2007, a possible supervoid was detected in the constellation Eridanus. It coincides with the 'CMB cold spot', a cold region in the microwave sky that is highly improbable under the currently favored cosmological model. This supervoid could cause the cold spot, but to do so it would have to be improbably big, possibly a billion light-years across, almost as big as the Giant Void mentioned above.\n\nAnother large-scale structure is the SSA22 Protocluster, a collection of galaxies and enormous gas bubbles that measures about 200 million light-years across.\n\nIn 2011, a large quasar group was discovered, U1.11, measuring about 2.5 billion light-years across. On January 11, 2013, another large quasar group, the Huge-LQG, was discovered, which was measured to be four billion light-years across, the largest known structure in the universe at that time. In November 2013, astronomers discovered the Hercules–Corona Borealis Great Wall, an even bigger structure twice as large as the former. It was defined by the mapping of gamma-ray bursts.\n\nThe \"End of Greatness\" is an observational scale discovered at roughly 100 Mpc (roughly 300 million light-years) where the lumpiness seen in the large-scale structure of the universe is homogenized and isotropized in accordance with the Cosmological Principle. At this scale, no pseudo-random fractalness is apparent.\nThe superclusters and filaments seen in smaller surveys are randomized to the extent that the smooth distribution of the universe is visually apparent. It was not until the redshift surveys of the 1990s were completed that this scale could accurately be observed.\n\nAnother indicator of large-scale structure is the 'Lyman-alpha forest'. This is a collection of absorption lines that appear in the spectra of light from quasars, which are interpreted as indicating the existence of huge thin sheets of intergalactic (mostly hydrogen) gas. These sheets appear to be associated with the formation of new galaxies.\n\nCaution is required in describing structures on a cosmic scale because things are often different from how they appear. Gravitational lensing (bending of light by gravitation) can make an image appear to originate in a different direction from its real source. This is caused when foreground objects (such as galaxies) curve surrounding spacetime (as predicted by general relativity), and deflect passing light rays. Rather usefully, strong gravitational lensing can sometimes magnify distant galaxies, making them easier to detect. Weak lensing (gravitational shear) by the intervening universe in general also subtly changes the observed large-scale structure. \n\nThe large-scale structure of the universe also looks different if one only uses redshift to measure distances to galaxies. For example, galaxies behind a galaxy cluster are attracted to it, and so fall towards it, and so are slightly blueshifted (compared to how they would be if there were no cluster) On the near side, things are slightly redshifted. Thus, the environment of the cluster looks a bit squashed if using redshifts to measure distance. An opposite effect works on the galaxies already within a cluster: the galaxies have some random motion around the cluster center, and when these random motions are converted to redshifts, the cluster appears elongated. This creates a \"finger of God\"—the illusion of a long chain of galaxies pointed at the Earth.\n\nAt the centre of the Hydra-Centaurus Supercluster, a gravitational anomaly called the Great Attractor affects the motion of galaxies over a region hundreds of millions of light-years across. These galaxies are all redshifted, in accordance with Hubble's law. This indicates that they are receding from us and from each other, but the variations in their redshift are sufficient to reveal the existence of a concentration of mass equivalent to tens of thousands of galaxies.\n\nThe Great Attractor, discovered in 1986, lies at a distance of between 150 million and 250 million light-years (250 million is the most recent estimate), in the direction of the Hydra and Centaurus constellations. In its vicinity there is a preponderance of large old galaxies, many of which are colliding with their neighbours, or radiating large amounts of radio waves.\n\nIn 1987, astronomer R. Brent Tully of the University of Hawaii's Institute of Astronomy identified what he called the Pisces–Cetus Supercluster Complex, a structure one billion light-years long and 150 million light-years across in which, he claimed, the Local Supercluster was embedded.\n\nThe mass of the observable universe is often quoted as 10 tonnes or 10 kg. In this context, mass refers to ordinary matter and includes the interstellar medium (ISM) and the intergalactic medium (IGM). However, it excludes dark matter and dark energy. This quoted value for the mass of ordinary matter in the universe can be estimated based on critical density. The calculations are for the observable universe only as the volume of the whole is unknown and may be infinite.\nCritical density is the energy density for which the universe is flat. If there is no dark energy, it is also the density for which the expansion of the universe is poised between continued expansion and collapse. From the Friedmann equations, the value for formula_2 critical density, is:\n\nwhere \"G\" is the gravitational constant and H = \"H\" is the present value of the Hubble constant. The current value for \"H\", due to the European Space Agency's Planck Telescope, is \"H\" = 67.15 kilometers per second per mega parsec. This gives a critical density of (commonly quoted as about 5 hydrogen atoms per cubic meter). This density includes four significant types of energy/mass: ordinary matter (4.8%), neutrinos (0.1%), cold dark matter (26.8%), and dark energy (68.3%). Note that although neutrinos are Standard Model particles, they are listed separately because they are difficult to detect and so different from ordinary matter. The density of ordinary matter, as measured by Planck, is 4.8% of the total critical density or . To convert this density to mass we must multiply by volume, a value based on the radius of the \"observable universe\". Since the universe has been expanding for 13.8 billion years, the comoving distance (radius) is now about 46.6 billion light-years. Thus, volume (\"πr\") equals and the mass of ordinary matter equals density () times volume () or .\n\nAssuming the mass of ordinary matter is about (refer to previous section) and assuming all atoms are hydrogen atoms (which in reality make up about 74% of all atoms in our galaxy by mass, see Abundance of the chemical elements), calculating the estimated total number of atoms in the observable universe is straightforward. Divide the mass of ordinary matter by the mass of a hydrogen atom ( divided by ). The result is approximately 10 hydrogen atoms.\n\nThe most distant astronomical object yet announced as of 2016 is a galaxy classified GN-z11. In 2009, a gamma ray burst, GRB 090423, was found to have a redshift of 8.2, which indicates that the collapsing star that caused it exploded when the universe was only 630 million years old. The burst happened approximately 13 billion years ago, so a distance of about 13 billion light-years was widely quoted in the media (or sometimes a more precise figure of 13.035 billion light-years), though this would be the \"light travel distance\" (\"see\" Distance measures (cosmology)) rather than the \"proper distance\" used in both Hubble's law and in defining the size of the observable universe (cosmologist Ned Wright argues against the common use of light travel distance in astronomical press releases on this page, and at the bottom of the page offers online calculators that can be used to calculate the current proper distance to a distant object in a flat universe based on either the redshift \"z\" or the light travel time). The proper distance for a redshift of 8.2 would be about 9.2 Gpc, or about 30 billion light-years. Another record-holder for most distant object is a galaxy observed through and located beyond Abell 2218, also with a light travel distance of approximately 13 billion light-years from Earth, with observations from the Hubble telescope indicating a redshift between 6.6 and 7.1, and observations from Keck telescopes indicating a redshift towards the upper end of this range, around 7. The galaxy's light now observable on Earth would have begun to emanate from its source about 750 million years after the Big Bang.\n\nThe limit of observability in our universe is set by a set of cosmological horizons which limit—based on various physical constraints—the extent to which we can obtain information about various events in the universe. The most famous horizon is the particle horizon which sets a limit on the precise distance that can be seen due to the finite age of the universe. Additional horizons are associated with the possible future extent of observations (larger than the particle horizon owing to the expansion of space), an \"optical horizon\" at the surface of last scattering, and associated horizons with the surface of last scattering for neutrinos and gravitational waves.\n\n\n"}
{"id": "939466", "url": "https://en.wikipedia.org/wiki?curid=939466", "title": "Orders of magnitude (energy)", "text": "Orders of magnitude (energy)\n\nThis list compares various energies in joules (J), organized by order of magnitude.\n\n"}
{"id": "1841288", "url": "https://en.wikipedia.org/wiki?curid=1841288", "title": "Outline of energy", "text": "Outline of energy\n\nThe following outline is provided as an overview of and topical guide to energy:\n\nEnergy – in physics, this is an indirectly observed quantity often understood as the ability of a physical system to do work on other physical systems. Since work is defined as a force acting through a distance (a length of space), energy is always equivalent to the ability to exert force (a pull or a push) against an object that is moving along a definite path of certain length.\n\n\nUnits of energy\n\n\nEnergy industry\n\nSee especially and for a large number of conventional energy related topics.\n\n\nHistory of energy\n\n\n\n\n\n\nEnergy economics\n\n\n\n\n\n\n"}
{"id": "35659147", "url": "https://en.wikipedia.org/wiki?curid=35659147", "title": "Patterns in nature", "text": "Patterns in nature\n\nPatterns in nature are visible regularities of form found in the natural world. These patterns recur in different contexts and can sometimes be modelled mathematically. Natural patterns include symmetries, trees, spirals, meanders, waves, foams, tessellations, cracks and stripes. Early Greek philosophers studied pattern, with Plato, Pythagoras and Empedocles attempting to explain order in nature. The modern understanding of visible patterns developed gradually over time.\n\nIn the 19th century, Belgian physicist Joseph Plateau examined soap films, leading him to formulate the concept of a minimal surface. German biologist and artist Ernst Haeckel painted hundreds of marine organisms to emphasise their symmetry. Scottish biologist D'Arcy Thompson pioneered the study of growth patterns in both plants and animals, showing that simple equations could explain spiral growth. In the 20th century, British mathematician Alan Turing predicted mechanisms of morphogenesis which give rise to patterns of spots and stripes. Hungarian biologist Aristid Lindenmayer and French American mathematician Benoît Mandelbrot showed how the mathematics of fractals could create plant growth patterns.\n\nMathematics, physics and chemistry can explain patterns in nature at different levels. Patterns in living things are explained by the biological processes of natural selection and sexual selection. Studies of pattern formation make use of computer models to simulate a wide range of patterns.\n\nEarly Greek philosophers attempted to explain order in nature, anticipating modern concepts. Plato (c. 427 – c. 347 BC) — looking only at his work on natural patterns — argued for the existence of universals. He considered these to consist of ideal forms ( \"eidos\": \"form\") of which physical objects are never more than imperfect copies. Thus, a flower may be roughly circular, but it is never a perfect mathematical circle. Pythagoras explained patterns in nature like the harmonies of music as arising from number, which he took to be the basic constituent of existence. Empedocles to an extent anticipated Darwin's evolutionary explanation for the structures of organisms.\n\nIn 1202, Leonardo Fibonacci (c. 1170 – c. 1250) introduced the Fibonacci number sequence to the western world with his book \"Liber Abaci\". Fibonacci gave an (unrealistic) biological example, on the growth in numbers of a theoretical rabbit population.\n\nIn 1658, the English physician and philosopher Sir Thomas Browne discussed \"how Nature Geometrizeth\" in \"The Garden of Cyrus\", citing Pythagorean numerology involving the number 5, and the Platonic form of the quincunx pattern. The discourse's central chapter features examples and observations of the quincunx in botany.\n\nIn 1917, D'Arcy Wentworth Thompson (1860–1948) published his book \"On Growth and Form\". His description of phyllotaxis and the Fibonacci sequence, the mathematical relationships in the spiral growth patterns of plants, is classic. He showed that simple equations could describe all the apparently complex spiral growth patterns of animal horns and mollusc shells.\n\nThe Belgian physicist Joseph Plateau (1801–1883) formulated the mathematical problem of the existence of a minimal surface with a given boundary, which is now named after him. He studied soap films intensively, formulating Plateau's laws which describe the structures formed by films in foams.\n\nThe German psychologist Adolf Zeising (1810–1876) claimed that the golden ratio was expressed in the arrangement of plant parts, in the skeletons of animals and the branching patterns of their veins and nerves, as well as in the geometry of crystals.\n\nErnst Haeckel (1834–1919) painted beautiful illustrations of marine organisms, in particular Radiolaria, emphasising their symmetry to support his faux-Darwinian theories of evolution.\n\nThe American photographer Wilson Bentley (1865–1931) took the first micrograph of a snowflake in 1885.\nIn 1952, Alan Turing (1912–1954), better known for his work on computing and codebreaking, wrote \"The Chemical Basis of Morphogenesis\", an analysis of the mechanisms that would be needed to create patterns in living organisms, in the process called morphogenesis. He predicted oscillating chemical reactions, in particular the Belousov–Zhabotinsky reaction. These activator-inhibitor mechanisms can, Turing suggested, generate patterns (dubbed \"Turing patterns\") of stripes and spots in animals, and contribute to the spiral patterns seen in plant phyllotaxis.\n\nIn 1968, the Hungarian theoretical biologist Aristid Lindenmayer (1925–1989) developed the L-system, a formal grammar which can be used to model plant growth patterns in the style of fractals. L-systems have an alphabet of symbols that can be combined using production rules to build larger strings of symbols, and a mechanism for translating the generated strings into geometric structures. In 1975, after centuries of slow development of the mathematics of patterns by Gottfried Leibniz, Georg Cantor, Helge von Koch, Wacław Sierpiński and others, Benoît Mandelbrot wrote a famous paper, \"How Long Is the Coast of Britain? Statistical Self-Similarity and Fractional Dimension\", crystallising mathematical thought into the concept of the fractal.\n\nLiving things like orchids, hummingbirds, and the peacock's tail have abstract designs with a beauty of form, pattern and colour that artists struggle to match. The beauty that people perceive in nature has causes at different levels, notably in the mathematics that governs what patterns can physically form, and among living things in the effects of natural selection, that govern how patterns evolve.\n\nMathematics seeks to discover and explain abstract patterns or regularities of all kinds.\nVisual patterns in nature find explanations in chaos theory, fractals, logarithmic spirals, topology and other mathematical patterns. For example, L-systems form convincing models of different patterns of tree growth.\nThe laws of physics apply the abstractions of mathematics to the real world, often as if it were perfect. For example, a crystal is perfect when it has no structural defects such as dislocations and is fully symmetric. Exact mathematical perfection can only approximate real objects. Visible patterns in nature are governed by physical laws; for example, meanders can be explained using fluid dynamics.\n\nIn biology, natural selection can cause the development of patterns in living things for several reasons, including camouflage, sexual selection, and different kinds of signalling, including mimicry and cleaning symbiosis. In plants, the shapes, colours, and patterns of insect-pollinated flowers like the lily have evolved to attract insects such as bees. Radial patterns of colours and stripes, some visible only in ultraviolet light serve as nectar guides that can be seen at a distance.\n\nSymmetry is pervasive in living things. Animals mainly have bilateral or mirror symmetry, as do the leaves of plants and some flowers such as orchids. Plants often have radial or rotational symmetry, as do many flowers and some groups of animals such as sea anemones. Fivefold symmetry is found in the echinoderms, the group that includes starfish, sea urchins, and sea lilies.\n\nAmong non-living things, snowflakes have striking sixfold symmetry; each flake's structure forms a record of the varying conditions during its crystallization, with nearly the same pattern of growth on each of its six arms. Crystals in general have a variety of symmetries and crystal habits; they can be cubic or octahedral, but true crystals cannot have fivefold symmetry (unlike quasicrystals). Rotational symmetry is found at different scales among non-living things, including the crown-shaped splash pattern formed when a drop falls into a pond, and both the spheroidal shape and rings of a planet like Saturn.\n\nSymmetry has a variety of causes. Radial symmetry suits organisms like sea anemones whose adults do not move: food and threats may arrive from any direction. But animals that move in one direction necessarily have upper and lower sides, head and tail ends, and therefore a left and a right. The head becomes specialised with a mouth and sense organs (cephalisation), and the body becomes bilaterally symmetric (though internal organs need not be). More puzzling is the reason for the fivefold (pentaradiate) symmetry of the echinoderms. Early echinoderms were bilaterally symmetrical, as their larvae still are. Sumrall and Wray argue that the loss of the old symmetry had both developmental and ecological causes.\n\nFractals are infinitely self-similar, iterated mathematical constructs having fractal dimension. Infinite iteration is not possible in nature so all 'fractal' patterns are only approximate. For example, the leaves of ferns and umbellifers (Apiaceae) are only self-similar (pinnate) to 2, 3 or 4 levels. Fern-like growth patterns occur in plants and in animals including bryozoa, corals, hydrozoa like the air fern, \"Sertularia argentea\", and in non-living things, notably electrical discharges. Lindenmayer system fractals can model different patterns of tree growth by varying a small number of parameters including branching angle, distance between nodes or branch points (internode length), and number of branches per branch point.\n\nFractal-like patterns occur widely in nature, in phenomena as diverse as clouds, river networks, geologic fault lines, mountains, coastlines, animal coloration, snow flakes, crystals, blood vessel branching, actin cytoskeleton, and ocean waves.\n\nSpirals are common in plants and in some animals, notably molluscs. For example, in the nautilus, a cephalopod mollusc, each chamber of its shell is an approximate copy of the next one, scaled by a constant factor and arranged in a logarithmic spiral. Given a modern understanding of fractals, a growth spiral can be seen as a special case of self-similarity.\n\nPlant spirals can be seen in phyllotaxis, the arrangement of leaves on a stem, and in the arrangement (parastichy) of other parts as in composite flower heads and seed heads like the sunflower or fruit structures like the pineapple and snake fruit, as well as in the pattern of scales in pine cones, where multiple spirals run both clockwise and anticlockwise. These arrangements have explanations at different levels – mathematics, physics, chemistry, biology – each individually correct, but all necessary together. Phyllotaxis spirals can be generated mathematically from Fibonacci ratios: the Fibonacci sequence runs 1, 1, 2, 3, 5, 8, 13... (each subsequent number being the sum of the two preceding ones). For example, when leaves alternate up a stem, one rotation of the spiral touches two leaves, so the pattern or ratio is 1/2. In hazel the ratio is 1/3; in apricot it is 2/5; in pear it is 3/8; in almond it is 5/13. In disc phyllotaxis as in the sunflower and daisy, the florets are arranged in Fermat's spiral with Fibonacci numbering, at least when the flowerhead is mature so all the elements are the same size. Fibonacci ratios approximate the golden angle, 137.508°, which governs the curvature of Fermat's spiral.\n\nFrom the point of view of physics, spirals are lowest-energy configurations which emerge spontaneously through self-organizing processes in dynamic systems. From the point of view of chemistry, a spiral can be generated by a reaction-diffusion process, involving both activation and inhibition. Phyllotaxis is controlled by proteins that manipulate the concentration of the plant hormone auxin, which activates meristem growth, alongside other mechanisms to control the relative angle of buds around the stem. From a biological perspective, arranging leaves as far apart as possible in any given space is favoured by natural selection as it maximises access to resources, especially sunlight for photosynthesis.\n\nIn mathematics, a dynamical system is chaotic if it is (highly) sensitive to initial conditions (the so-called \"butterfly effect\"), which requires the mathematical properties of topological mixing and dense periodic orbits.\n\nAlongside fractals, chaos theory ranks as an essentially universal influence on patterns in nature. There is a relationship between chaos and fractals—the \"strange attractors\" in chaotic systems have a fractal dimension. Some cellular automata, simple sets of mathematical rules that generate patterns, have chaotic behaviour, notably Stephen Wolfram's Rule 30.\n\nVortex streets are zigzagging patterns of whirling vortices created by the unsteady separation of flow of a fluid, most often air or water, over obstructing objects. Smooth (laminar) flow starts to break up when the size of the obstruction or the velocity of the flow become large enough compared to the viscosity of the fluid.\n\nMeanders are sinuous bends in rivers or other channels, which form as a fluid, most often water, flows around bends. As soon as the path is slightly curved, the size and curvature of each loop increases as helical flow drags material like sand and gravel across the river to the inside of the bend. The outside of the loop is left clean and unprotected, so erosion accelerates, further increasing the meandering in a powerful positive feedback loop.\n\nWaves are disturbances that carry energy as they move. Mechanical waves propagate through a medium – air or water, making it oscillate as they pass by. Wind waves are sea surface waves that create the characteristic chaotic pattern of any large body of water, though their statistical behaviour can be predicted with wind wave models. As waves in water or wind pass over sand, they create patterns of ripples. When winds blow over large bodies of sand, they create dunes, sometimes in extensive dune fields as in the Taklamakan desert. Dunes may form a range of patterns including crescents, very long straight lines, stars, domes, parabolas, and longitudinal or seif ('sword') shapes.\n\nBarchans or crescent dunes are produced by wind acting on desert sand; the two horns of the crescent and the slip face point downwind. Sand blows over the upwind face, which stands at about 15 degrees from the horizontal, and falls onto the slip face, where it accumulates up to the angle of repose of the sand, which is about 35 degrees. When the slip face exceeds the angle of repose, the sand avalanches, which is a nonlinear behaviour: the addition of many small amounts of sand causes nothing much to happen, but then the addition of a further small amount suddenly causes a large amount to avalanche. Apart from this nonlinearity, barchans behave rather like solitary waves.\n\nA soap bubble forms a sphere, a surface with minimal area — the smallest possible surface area for the volume enclosed. Two bubbles together form a more complex shape: the outer surfaces of both bubbles are spherical; these surfaces are joined by a third spherical surface as the smaller bubble bulges slightly into the larger one.\n\nA foam is a mass of bubbles; foams of different materials occur in nature. Foams composed of soap films obey Plateau's laws, which require three soap films to meet at each edge at 120° and four soap edges to meet at each vertex at the tetrahedral angle of about 109.5°. Plateau's laws further require films to be smooth and continuous, and to have a constant average curvature at every point. For example, a film may remain nearly flat on average by being curved up in one direction (say, left to right) while being curved downwards in another direction (say, front to back). Structures with minimal surfaces can be used as tents. Lord Kelvin identified the problem of the most efficient way to pack cells of equal volume as a foam in 1887; his solution uses just one solid, the bitruncated cubic honeycomb with very slightly curved faces to meet Plateau's laws. No better solution was found until 1993 when Denis Weaire and Robert Phelan proposed the Weaire–Phelan structure; the Beijing National Aquatics Center adapted the structure for their outer wall in the 2008 Summer Olympics.\n\nAt the scale of living cells, foam patterns are common; radiolarians, sponge spicules, silicoflagellate exoskeletons and the calcite skeleton of a sea urchin, \"Cidaris rugosa\", all resemble mineral casts of Plateau foam boundaries. The skeleton of the Radiolarian, \"Aulonia hexagona\", a beautiful marine form drawn by Ernst Haeckel, looks as if it is a sphere composed wholly of hexagons, but this is mathematically impossible. The Euler characteristic states that for any convex polyhedron, the number of faces plus the number of vertices (corners) equals the number of edges plus two. A result of this formula is that any closed polyhedron of hexagons has to include exactly 12 pentagons, like a soccer ball, Buckminster Fuller geodesic dome, or fullerene molecule. This can be visualised by noting that a mesh of hexagons is flat like a sheet of chicken wire, but each pentagon that is added forces the mesh to bend (there are fewer corners, so the mesh is pulled in).\n\nTessellations are patterns formed by repeating tiles all over a flat surface. There are 17 wallpaper groups of tilings. While common in art and design, exactly repeating tilings are less easy to find in living things. The cells in the paper nests of social wasps, and the wax cells in honeycomb built by honey bees are well-known examples. Among animals, bony fish, reptiles or the pangolin, or fruits like the salak are protected by overlapping scales or osteoderms, these form more-or-less exactly repeating units, though often the scales in fact vary continuously in size. Among flowers, the snake's head fritillary, \"Fritillaria meleagris\", have a tessellated chequerboard pattern on their petals. The structures of minerals provide good examples of regularly repeating three-dimensional arrays. Despite the hundreds of thousands of known minerals, there are rather few possible types of arrangement of atoms in a crystal, defined by crystal structure, crystal system, and point group; for example, there are exactly 14 Bravais lattices for the 7 lattice systems in three-dimensional space.\n\nCracks are linear openings that form in materials to relieve stress. When an elastic material stretches or shrinks uniformly, it eventually reaches its breaking strength and then fails suddenly in all directions, creating cracks with 120 degree joints, so three cracks meet at a node. Conversely, when an inelastic material fails, straight cracks form to relieve the stress. Further stress in the same direction would then simply open the existing cracks; stress at right angles can create new cracks, at 90 degrees to the old ones. Thus the pattern of cracks indicates whether the material is elastic or not. In a tough fibrous material like oak tree bark, cracks form to relieve stress as usual, but they do not grow long as their growth is interrupted by bundles of strong elastic fibres. Since each species of tree has its own structure at the levels of cell and of molecules, each has its own pattern of splitting in its bark.\n\nLeopards and ladybirds are spotted; angelfish and zebras are striped. These patterns have an evolutionary explanation: they have functions which increase the chances that the offspring of the patterned animal will survive to reproduce. One function of animal patterns is camouflage; for instance, a leopard that is harder to see catches more prey. Another function is signalling — for instance, a ladybird is less likely to be attacked by predatory birds that hunt by sight, if it has bold warning colours, and is also distastefully bitter or poisonous, or mimics other distasteful insects. A young bird may see a warning patterned insect like a ladybird and try to eat it, but it will only do this once; very soon it will spit out the bitter insect; the other ladybirds in the area will remain undisturbed. The young leopards and ladybirds, inheriting genes that somehow create spottedness, survive. But while these evolutionary and functional arguments explain why these animals need their patterns, they do not explain how the patterns are formed.\n\nAlan Turing, and later the mathematical biologist James Murray, described a mechanism that spontaneously creates spotted or striped patterns: a reaction-diffusion system. The cells of a young organism have genes that can be switched on by a chemical signal, a morphogen, resulting in the growth of a certain type of structure, say a darkly pigmented patch of skin. If the morphogen is present everywhere, the result is an even pigmentation, as in a black leopard. But if it is unevenly distributed, spots or stripes can result. Turing suggested that there could be feedback control of the production of the morphogen itself. This could cause continuous fluctuations in the amount of morphogen as it diffused around the body. A second mechanism is needed to create standing wave patterns (to result in spots or stripes): an inhibitor chemical that switches off production of the morphogen, and that itself diffuses through the body more quickly than the morphogen, resulting in an activator-inhibitor scheme. The Belousov–Zhabotinsky reaction is a non-biological example of this kind of scheme, a chemical oscillator.\n\nLater research has managed to create convincing models of patterns as diverse as zebra stripes, giraffe blotches, jaguar spots (medium-dark patches surrounded by dark broken rings) and ladybird shell patterns (different geometrical layouts of spots and stripes, see illustrations). Richard Prum's activation-inhibition models, developed from Turing's work, use six variables to account for the observed range of nine basic within-feather pigmentation patterns, from the simplest, a central pigment patch, via concentric patches, bars, chevrons, eye spot, pair of central spots, rows of paired spots and an array of dots. More elaborate models simulate complex feather patterns in the guineafowl \"Numida meleagris\" in which the individual feathers feature transitions from bars at the base to an array of dots at the far (distal) end. These require an oscillation created by two inhibiting signals, with interactions in both space and time.\n\nPatterns can form for other reasons in the vegetated landscape of tiger bush and fir waves. Tiger bush stripes occur on arid slopes where plant growth is limited by rainfall. Each roughly horizontal stripe of vegetation effectively collects the rainwater from the bare zone immediately above it. Fir waves occur in forests on mountain slopes after wind disturbance, during regeneration. When trees fall, the trees that they had sheltered become exposed and are in turn more likely to be damaged, so gaps tend to expand downwind. Meanwhile, on the windward side, young trees grow, protected by the wind shadow of the remaining tall trees. Natural patterns are sometimes formed by animals, as in the Mima mounds of the Northwestern United States and some other areas, which appear to be created over many years by the burrowing activities of pocket gophers, while the so-called fairy circles of Namibia appear to be created by the interaction of competing groups of sand termites, along with competition for water among the desert plants.\n\nIn permafrost soils with an active upper layer subject to annual freeze and thaw, patterned ground can form, creating circles, nets, ice wedge polygons, steps, and stripes. Thermal contraction causes shrinkage cracks to form; in a thaw, water fills the cracks, expanding to form ice when next frozen, and widening the cracks into wedges. These cracks may join up to form polygons and other shapes.\n\nThe fissured pattern that develops on vertebrate brains are caused by a physical process of constrained expansion dependent on two geometric parameters: relative tangential cortical expansion and relative thickness of the cortex. Similar patterns of gyri (peaks) and sulci (troughs) have been demonstrated in models of the brain starting from smooth, layered gels, with the patterns caused by compressive mechanical forces resulting from the expansion of the outer layer (representing the cortex) after the addition of a solvent. Numerical models in computer simulations support natural and experimental observations that the surface folding patterns increase in larger brains.\n\n\n\n\n\n"}
{"id": "58664232", "url": "https://en.wikipedia.org/wiki?curid=58664232", "title": "Phakalane power station", "text": "Phakalane power station\n\nPhakalane Power Station is a photovoltaic pilot power plant located in Phakalane, Botswana. The power station was funded through a Japanese grant which was part of Prime Minister Hatoyama's initiative strategy called Cool Earth Partnership aimed at supporting developing countries in their efforts to combat global warming. The Cool Earth Partnership is part of the initiatives which saw Hatoyama win the Sustainable Development Leadership Award in 2010.\n\n"}
{"id": "2399976", "url": "https://en.wikipedia.org/wiki?curid=2399976", "title": "Physical universe", "text": "Physical universe\n\nIn religion and esotericism, the term \"physical universe\" or \"material universe\" is used to distinguish the physical matter of the universe from a proposed spiritual or supernatural essence. \n\nIn the Book of Veles, and perhaps in traditional Slavic mythology, the physical universe is referred to as Yav. Gnosticism holds that the physical universe was created by a Demiurge. In Dharmic religions Maya is believed to be the illusion of a physical universe.\n\nPhysicalism, a type of monism, holds that only physical things exist. This is also known as metaphysical naturalism.\n\n"}
{"id": "3759820", "url": "https://en.wikipedia.org/wiki?curid=3759820", "title": "Physis", "text": "Physis\n\nPhysis (Greek: \"phusis\") is a Greek theological, philosophical, and scientific term usually translated into English as \"nature\".\n\nThe term is central to Greek philosophy, and as a consequence to Western philosophy as a whole.\nIn pre-Socratic usage, \"phusis\" contrasted with \"nomos\" \"law, human convention\"\nSince Aristotle, however, the \"physical\" (the subject matter of \"physics\", properly \"natural things\") has more typically been juxtaposed to the \"metaphysical\".\n\nThe word φύσις is a verbal noun based on φύω \"to grow, to appear\" (cognate with English \"to be\"). In Homeric Greek it is used quite literally, of the manner of growth of a particular species of plant. \n\nIn pre-Socratic philosophy, beginning with Heraclitus, \"phusis\" in keeping with its etymology of \"growing, becoming\" is always used in the sense of the \"natural\" \"development\", although the focus might lie either with the origin, or the process, or the end result of the process. There is some evidence that by the 6th century BC, beginning with the Ionian School, the word could also be used \nin the comprehensive sense, as referring to \"\"all\" things\", as it were \"Nature\" in the sense of \"Universe\".\n\nIn the Sophist tradition, the term stood in opposition to \"nomos\" (), \"law\" or \"custom\", in the debate on which parts of human existence are natural, and which are due to convention. \nThe contrast of \"phisis\" vs. \"nomos\" could be applied to any subject, much like the modern contrast of \"nature vs. nurture\".\n\nIn book 10 of \"Laws\", Plato criticizes those who write works \"peri phuseōs\". The criticism is that such authors tend to focus on a purely \"naturalistic\" explanation of the world, ignoring the role of \"intention\" or \"technē\", and thus becoming prone to the error of naive atheism. Plato accuses even Hesiod of this, for the reason that the gods in Hesiod \"grow\" out of primordial entities after the physical universe had been established.\n\n\"Because those who use the term mean to say that nature is the first creative power; but if the soul turns out to be the primeval element, and not fire or air, then in the truest sense and beyond other things the soul may be said to exist \"by\" nature; and this would be true if you proved that the soul is older than the body, but not otherwise.\"\n\nAristotle sought out the definition of \"physis\" to prove that there was more than one definition of \"physis\", and more than one way to interpret nature. \"Though Aristotle retains the ancient sense of \"physis\" as growth, he insists that an adequate definition of \"physis\" requires the different perspectives of the four causes (aitia): material, efficient, formal, and final.\" Aristotle believed that nature itself contained its own source of matter (material), power/motion (efficiency), form, and end (final). A unique feature about Aristotle's definition of \"physis\" was his relationship between art and nature. Aristotle said that \"physis\" (nature) is dependent on techne (art). \"The critical distinction between art and nature concerns their different efficient causes: nature is its own source of motion, whereas techne always requires a source of motion outside itself.\" What Aristotle was trying to bring to light, was that art does not contain within itself its form or source of motion. Consider the process of an acorn becoming an oak tree. This is a natural process that has its own driving force behind it. There is no external force pushing this acorn to its final state, rather it is progressively developing towards one specific end (telos).\nThough φύσις was often used in Hellenistic philosophy, it is used only 14 times in the New Testament (10 of those in the writings of Paul). Its meaning varies throughout Paul's writings. One usage refers to the established or natural order of things, as in \"Romans 2:14\" where Paul writes \"For when Gentiles, who do not have the law, by \"nature\" do what the law requires, they are a law to themselves, even though they do not have the law.\" Another use of φύσις in the sense of \"natural order\" is \"Romans 1:26\" where he writes \"the men likewise gave up \"natural\" relations with women and were consumed with passion for one another\". In \"1 Corinthians 11:14\", Paul asks \"Does not nature itself teach you that if a man wears long hair it is a disgrace for him?\"\n\nThis use of φύσις as referring to a \"natural order\" in \"Romans 1:26\" and \"1 Corinthians 11:14\" may have been influenced by Stoicism. The Greek philosophers, including Aristotle and the Stoics are credited with distinguishing between man-made laws and a natural law of universal validity, but Gerhard Kittel states that the Stoic philosophers were not able to combine the concepts of νόμος (law) and φύσις (nature) to produce the concept of \"natural law\" in the sense that was made possible by Judeo-Christian theology.\n\nAs part of the Pauline theology of salvation by grace, Paul writes in \"Ephesians 2:3\" that \"we all once lived in the passions of our flesh, carrying out the desires of the body and the mind, and were by \"nature\" children of wrath, like the rest of mankind. In the next verse he writes, \"by grace you have been saved.\" \n\nTheologians of the early Christian period differed in the usage of this term. In Antiochene circles, it connoted the humanity or divinity of Christ conceived as a concrete set of characteristics or attributes. In Alexandrine thinking, it meant a concrete individual or independent existent and approximated to hypostasis without being a synonym. While it refers to much the same thing as ousia it is more empirical and descriptive focussing on function while ousia is metaphysical and focuses more on reality. Although found in the context of the Trinitarian debate, it is chiefly important in the Christology of Cyril of Alexandria.\n\nThe Greek adjective \"phusikos\" is represented in various forms in modern English:\nAs \"physics\" \"the study of nature\", as \"physical\" (via Middle Latin \"physicalis\") referring both to physics (the study of nature, the material universe) and to the human body. The term physiology (\"physiologia\") is of 16th-century coinage (Jean Fernel). The term \"physique\", for \"the bodily constitution of a person\", is a 19th-century loan from French. \n\nIn medicine the suffix \"-physis\" occurs in such compounds as \"symphysis\", \"epiphysis\", and a few others, in the sense of \"a growth\". The physis also refers to the \"growth plate\", or site of growth at the end of long bones.\n\n"}
{"id": "169115", "url": "https://en.wikipedia.org/wiki?curid=169115", "title": "Preternatural", "text": "Preternatural\n\nThe preternatural or praeternatural is that which appears outside or beside (Latin \"\") the natural. It is \"suspended between the mundane and the miraculous\".\n\nIn theology, the term is often used to distinguish marvels or deceptive trickery, often attributed to witchcraft or demons, from the purely divine power of the genuinely supernatural to violate the laws of nature. In the early modern period the term was used by scientists to refer to abnormalities and strange phenomena of various kinds that seemed to depart from the norms of nature.\n\nMedieval theologians made a clear distinction between the natural, the preternatural and the supernatural. Thomas Aquinas argued that the supernatural consists in \"God’s unmediated actions\"; the natural is \"what happens always or most of the time\"; and the preternatural is \"what happens rarely, but nonetheless by the agency of created beings...Marvels belong, properly speaking, to the realm of the preternatural.\" Theologians, following Aquinas, argued that only God had the power to disregard the laws of nature that he has created, but that demons could manipulate the laws of nature by a form of trickery, to deceive the unwary into believing they had experienced real miracles. According to historian Lorraine Daston,\n\nAlthough demons, astral intelligences, and other spirits might manipulate natural causes with superhuman dexterity and thereby work marvels, as mere creatures they could never transcend from the preternatural to the supernatural and work genuine miracles.\n\nBy the 16th century, the term \"preternatural\" was increasingly used to refer to demonic activity comparable to the use of magic by human adepts: The Devil, \"being a natural Magician … may perform many acts in ways above our knowledge, though not transcending our natural power.\" According to the philosophy of the time, preternatural phenomena were not contrary to divine law, but used hidden, or occult powers that violated the \"normal\" pattern of natural phenomena.\n\nWith the emergence of early modern science, the concept of the preternatural increasingly came to be used to refer to strange or abnormal phenomena that seemed to violate the normal working of nature, but which were not associated with magic and witchcraft. This was a development of the idea that preternatural phenomena were fake miracles. As Daston puts it, \"To simplify the historical sequence somewhat: first, preternatural phenomena were demonized and thereby incidentally naturalized; then the demons were deleted, leaving only the natural causes.\" The use of the term was especially common in medicine, for example in John Brown's \"A Compleat Treatise of Preternatural Tumours\" (1678), or William Smellie's \"A Collection of Preternatural Cases and Observations in Midwifery\" (1754).\n\nIn the 19th century the term was appropriated in anthropology to refer to folk beliefs about fairies, trolls and other such creatures which were not thought of as demonic, but which were perceived to affect the natural world in unpredictable ways. According to Thorstein Veblen, such preternatural agents were often thought of as forces somewhere between supernatural beings and material processes. \"The preternatural agency is not necessarily conceived to be a personal agent in the full sense, but it is an agency which partakes of the attributes of personality to the extent of somewhat arbitrarily influencing the outcome of any enterprise, and especially of any contest.\"\n\nThe linguistic association between individual agents and unexplained or unfortunate circumstances remains. Many people attribute occurrences that are known to be material processes, such as \"gremlins in the engine\", a \"ghost in the machine\", or attributing motives to objects: \"the clouds are threatening\". The anthropomorphism in our daily life is a combination of the above cultural stems, as well as the manifestation of our pattern-projecting minds.\n\nIn 2011, Penn State Press began publishing a learned journal entitled \"Preternature: Critical and Historical Studies on the Preternatural\". Edited by Kirsten Uszkalo and Richard Raiswell, the journal is dedicated to publishing articles, reviews and short editions of original texts that deal with conceptions and perceptions of the preternatural in any culture and in any historical period. The journal covers \"magics, witchcraft, spiritualism, occultism, prophecy, monstrophy, demonology, and folklore.\"\n\n\n"}
{"id": "1413688", "url": "https://en.wikipedia.org/wiki?curid=1413688", "title": "Primary energy", "text": "Primary energy\n\nPrimary energy (PE) is an energy form found in nature that has not been subjected to any human engineered conversion process. It is energy contained in raw fuels, and other forms of energy received as input to a system. Primary energy can be non-renewable or renewable.\n\nWhere primary energy is used to describe fossil fuels, the embodied energy of the fuel is available as thermal energy and around 70% is typically lost in conversion to electrical or mechanical energy. There is a similar 60-80% conversion loss when solar and wind energy is converted to electricity, but today's UN conventions on energy statistics counts the electricity made from wind and solar as the primary energy itself for these sources. One consequence of this counting method is that the contribution of wind and solar energy is under reported compared to fossil energy sources, and there is hence an international debate on how to count primary energy from wind and solar. \n\nTotal primary energy supply (TPES) is the sum of production and imports subtracting exports and storage changes.\n\nThe concept of primary energy is used in energy statistics in the compilation of energy balances, as well as in the field of energetics. In energetics, a primary energy source (PES) refers to the energy forms required by the energy sector to generate the supply of energy carriers used by human society.\n\nSecondary energy is a carrier of energy, such as electricity. These are produced by conversion from a primary energy source.\n\nThe use of primary energy as a measure ignores conversion efficiency. Thus forms of energy with poor conversion efficiency, particularly the thermal sources, coal, gas and nuclear are overstated, whereas energy sources such as hydroelectricity which are converted efficiently, while a small fraction of primary energy are significantly more important than their total raw energy supply may seem to imply.\n\nPE and TPES are better defined in the context of worldwide energy supply.\n\nPrimary energy sources should not be confused with the energy system components (or conversion processes) through which they are converted into energy carriers.\n\nPrimary energy sources are transformed in energy conversion processes to more convenient forms of energy that can directly be used by society, such as electrical energy, refined fuels, or synthetic fuels such as hydrogen fuel. In the field of energetics, these forms are called energy carriers and correspond to the concept of \"secondary energy\" in energy statistics.\n\nEnergy carriers are energy forms which have been transformed from primary energy sources. Electricity is one of the most common energy carriers, being transformed from various primary energy sources such as coal, oil, natural gas, and wind. Electricity is particularly useful since it has low entropy (is highly ordered) and so can be converted into other forms of energy very efficiently. District heating is another example of secondary energy.\n\nAccording to the laws of thermodynamics, primary energy sources cannot be produced. They must be available to society to enable the production of energy carriers.\n\nConversion efficiency varies. For thermal energy, electricity and mechanical energy production is limited by Carnot's theorem, and generates a lot of waste heat. Other non-thermal conversions can be more efficient. For example, while wind turbines do not capture all of the wind's energy, they have a high conversion efficiency and generate very little waste heat since wind energy is low entropy. In principle solar photovoltaic conversions could be very efficient, but current conversion can only be done well for narrow ranges of wavelength, whereas solar thermal is also subject to Carnot efficiency limits. Hydroelectric power is also very ordered, and converted very efficiently. The amount of usable energy is the exergy of a system.\n\nSite energy is the term used in North America for the amount of end-use energy of all forms consumed at a specified location. This can be a mix of primary energy (such as natural gas burned at the site) and secondary energy (such as electricity). Site energy is measured at the campus, building, or sub-building level and is the basis for energy charges on utility bills.\n\nSource energy, in contrast, is the term used in North America for the amount of primary energy consumed in order to provide a facility’s site energy. It is always greater than the site energy, as it includes all site energy and adds to it the energy lost during transmission, delivery, and conversion. While source or primary energy provides a more complete picture of energy consumption, it cannot be measured directly and must be calculated using conversion factors from site energy measurements. For electricity, a typical value is three units of source energy for one unit of site energy. However, this can vary considerably depending on factors such as the primary energy source or fuel type, the type of power plant, and the transmission infrastructure. One full set of conversion factors is available as technical reference from Energy STAR.\n\nEither site or source energy can be an appropriate metric when comparing or analyzing energy use of different facilities. The U.S Energy Information Administration, for example, uses primary (source) energy for its energy overviews but site energy for its Commercial Building Energy Consumption Survey and Residential Building Energy Consumption Survey. The US Environmental Protection Agency's Energy STAR program recommends using source energy, and the US Department of Energy uses site energy in its definition of a zero net energy building.\n\nEnergy accidents are accidents that occur in systems that provide energy or power. These can result in fatalities, as can the normal running of many systems, for example those deaths due to pollution.\n\nGlobally, coal is responsible for 100,000 deaths per trillion kWh.\n\n\n\n"}
{"id": "8994982", "url": "https://en.wikipedia.org/wiki?curid=8994982", "title": "Relational space", "text": "Relational space\n\nThe relational theory of space is a metaphysical theory according to which space is composed of relations between objects, with the implication that it cannot exist in the absence of matter. Its opposite is the container theory. A relativistic physical theory implies a relational metaphysics, but not the other way round: even if space is composed of nothing but relations between observers and events, it would be conceptually possible for all observers to agree on their measurements, whereas relativity implies they will disagree. Newtonian physics can be cast in relational terms, but Newton insisted, for philosophical reasons, on absolute (container) space. The subject was famously debated by Gottfried Wilhelm Leibniz and a supporter of Newton's in the Leibniz–Clarke correspondence.\n\nAn absolute approach can also be applied to time, with, for instance, the implication that there might have been vast epochs of time before the first event.\n\n"}
{"id": "19991258", "url": "https://en.wikipedia.org/wiki?curid=19991258", "title": "Sociology of space", "text": "Sociology of space\n\nThe sociology of space is a sub-discipline of sociology that mostly borrows from theories developed within the discipline of geography, including the sub fields of human geography, economic geography, and feminist geography. The \"sociology\" of space examines the social and material constitution of spaces. It is concerned with understanding the social practices, institutional forces, and material complexity of how humans and spaces interact. The sociology of space is an inter-disciplinary area of study, drawing on various theoretical traditions including Marxism, postcolonialism, and Science and Technology Studies, and overlaps and encompasses theorists with various academic disciplines such as geography and architecture. Edward T. Hall developed the study of Proxemics which concentrates on the empirical analysis of space in psychology. \n\nSpace is one of the most important concepts within the disciplines of social science as it is fundamental to our understanding of geography. The term \"space\" has been defined variously by scholars:\n\nIn general terms, the Oxford English Dictionary defines space in two ways;\n\n1. A continuous extension viewed with or without reference to the existence of objects within it. \n2. The interval between points or objects viewed as having one, two or three dimensions.\n\nHowever, the human geographers’ interest is in the objects within the space and their relative position, which involves the description, explanation and prediction of the distribution of phenomena. Thus, the relationships between objects in space is the central of the study.\n\nMichel Foucault defines space as;\n“The space in which we live, which draws us out of ourselves, in which the erosion of our lives, our time and our history occurs, the space that claws and gnaws at us, is also, in itself, a heterogeneous space…..we live inside a set of relations.\n\nNigel Thrift also defines space as;\n\"The outcome of a series of highly problematic temporary settlements that divide and connect things up into different kinds of collectives which are slowly provided with the meaning which render them durable and sustainable.\" \n\nIn short, \"space\" is the social space in which we live and create relationships with other people, societies and surroundings. Space is an outcome of the hard and continuous work of building up and maintaining collectives by bringing different things into alignments. All kinds of different spaces can and therefore do exist which may or may not relate to each other. Thus, through space, we can understand more about social action.\n\nGeorg Simmel has been seen as the classical sociologist who was most important to this field. Simmel wrote on \"the sociology of space\" in his 1908 book \"Sociology: Investigations on the Forms of Sociation\". His concerns included the process of metropolitanisation and the separation of leisure spaces in modern economic societies.\n\nThe category of space long played a subordinate role in sociological theory formation. Only in the late 1980s did it come to be realised that certain changes in society cannot be adequately explained without taking greater account of the spatial components of life. This shift in perspective is referred to as the topological turn. The space concept directs attention to organisational forms of juxtaposition. The focus is on differences between places and their mutual influence. This applies equally for the micro-spaces of everyday life and the macro-spaces at the nation-state or global levels.\n\nThe theoretical basis for the growing interest of the social sciences in space was set primarily by English and French-speaking sociologists, philosophers, and human geographers. Of particular importance is Michel Foucault’s essay on “Of Other Spaces”, in which the author proclaims the “age of space”, and Henri Lefebvre’s seminal work “La production de l’espace”. The latter provided the grounding for Marxist spatial theory on which David Harvey, Manuel Castells, Edward Soja, and others have built. Marxist theories of space, which are predicated on a structural, i.e., capitalist or global determinants of spaces and the growing homogenization of space, are confronted by action theoretical conceptions, which stress the importance of the corporeal placing and the perception of spaces as albeit habitually predetermined but subjective constructions. One example is the theory of space of the German sociologist Martina Löw. Approaches deriving from the post-colonialism discourse have attracted greater attention in recent years. Also in contrast to (neo)Marxist concepts of space, British geographer Doreen Massey and German sociologist Helmuth Berking, for instance, emphasise the heterogeneity of local contexts and the place-relatedness of our knowledge about the world.\n\nMartina Löw developed the idea of a \"relational\" model of space, which focuses on the “orderings” of living entities and social goods, and examines how space is constituted in processes of perception, recall, or ideation to manifest itself as societal structure. From a social theory point of view, it follows on from the theory of structuration proposed by Anthony Giddens, whose concept of the “duality of structure” Löw extends sociological terms into a “duality of space.” The basic idea is that individuals act as social agents (and constitute spaces in the process), but that their action depends on economic, legal, social, cultural, and, finally, spatial structures. Spaces are hence the outcome of action. At the same time, spaces structure action, that is to say spaces can both constrain and enable action.\n\nWith respect to the constitution of space, Löw distinguishes analytically between two, generally mutually determining factors: “spacing” and “synthesis.” Spacing refers to the act of placing or the state of being placed of social goods and people in places. According to Löw, however, an ordering created through placings is only effectively constituted as space where the elements that compose it are actively interlinked by people – in processes of perception, ideation, or recall. Löw calls this synthesis. This concept has been empirically tested in studies such as those by Lars Meier (who examined the constitution of space in the everyday life of financial managers in London and Singapore), Cedric Janowicz (who carried out an ethnographical-space sociological study of food supply in the Ghanaian city of Accra), and Silke Streets (who looked at processes of space constitution in the creative industries in Leipzig).\n\nThe most important proponent of Marxist spatial theory was Henri Lefebvre. He proposed \"social space\" to be where the relations of production are reproduced and that dialectical contradictions were spatial rather than temporal. Lefèbvre sees the societal production of space as a dialectical interaction between three factors. Space is constituted:\n\n\nIn Lefebvre’s view of the 1970s, this spatial production resulted in a space of non-reflexive everydayness marked by alienation, dominating through mathematical-abstract concepts of space, and reproduced in spatial practice. Lefebvre sees a line of flight from alienated spatiality in the spaces of representation – in notions of non-alienated, mythical, pre-modern, or artistic visions of space.\n\nMarxist spatial theory was given decisive new impetus by David Harvey, in particular, who was interested in the effects of the transition from Fordism to “flexible accumulation” on the experience of space and time. He shows how various innovations at the economic and technological levels have breached the crisis-prone inflexibility of the Fordist system, thus increasing the turnover rate of capital. This causes a general acceleration of economic cycles. According to Harvey, the result is “time-space compression.” While the feeling for the long term, for the future, for continuity is lost, the relationship between proximity and distance becomes more and more difficult to determine.\n\nTheories of space that are inspired by the post-colonialism discourse focus on the heterogeneity of spaces. According to Doreen Massey, calling a country in Africa a “developing country” is not appropriate, since this expression implies that spatial difference is temporal difference (Massey 1999b). This logic treats such a country not as different but merely as an early version of countries in the “developed” world, a view she condemns as \"Eurocentrism.\" In this vein, Helmuth Berking criticises theories that postulate the increasing homogenisation of the world through globalisation as “globocentrism.” He confronts this with the distinctiveness and importance of local knowledge resources for the production of (different and specific) places. He claims that local contexts form a sort of framework or filter through which global processes and globally circulating images and symbols are appropriated, thus attaining meaning. For instance, the film character Conan the Barbarian is a different figure in radical rightwing circles in Germany than in the black ghettoes of the Chicago Southside, just as McDonald’s means something different in Moscow than in Paris.\n\nHenri Lefebvre (see also Edward Soja) says that (social) space is a (social) product, or a complex social construction (based on values, and the social production of meanings) which affects spatial practices and perceptions. He explains space embraces a multitude of intersection in his great book, “Production of Space”. That means that we need to consider how the various modes of spatial production relate to each other.\n\nHe argues that there are three aspects to our spatial existence, which exist in a kind of triad:\n\n1. First Space\n\"The spatial practice of a society secretes that society's space; it propounds and presupposes it, in a dialectical interaction; it produces it slowly and surely as it masters and appropriates it.\"\n\n2. Second Space\n\"Conceptualized space, the space of scientists, planners, urbanists, technocratic subdividers and social engineers, as of a certain type of artist with a scientific bent -- all of whom identify what is lived and what is perceived with what is conceived.\"\n\n3. Third Space\n\"Space as directly lived through its associated images and symbols.\"\n\nEven though there are many disciplines in the study of Human Geography, the most well-known approach is “The third space” formulated by Edward Soja. In unitary theory, there are three approaches; first space, second space and third space. First space is physical space, and spaces are measurable and mappable. The second space is a mental or conceived space which comes from our thinking and ideas. However, the third space is a social space/lived space which is a social product that is a space created by society under oppression or marginalization that want to reclaim the space of inequality and make it into something else. Soja argues that our old ways to thinking about space (first and second space theories) can no longer accommodate the way the world works because he believed that spaces may not be contained within one social category, they may include different aspects of many categories or developed within the boundaries of a number of category. For instance, two different cultures combine together and emerge as a third culture. This third hybrid space displaces the original values that constitute it and set up new values and perspectives that is different from the first two spaces. Thus, the third space theory can explain some of the complexity of poverty, social exclusion and social inclusion, gender and race issues.\n\nIn the work of geographer and critical theorist Nigel Thrift, he wrote a rational view of space in which, rather than seeing space being viewed as a container within which the world proceeds, space should be seen as a co-product of these proceedings. He explained about four constructed space in modern human geography. \nThere are four different kinds of space according to how modern geography thinks about space. They are 1. Empirical Construction of Space, 2. Unblocking space, 3. Image space and 4. Place Space.\n\nFirst Space is the empirical construction of space. Empirical space refers to the process whereby the mundane fabric of daily life is constructed. These simple things like, cars, houses, mobiles, computers and roads are very simple but they are great achievements of our daily life and they play very important role in making up who we are today. For example, today’s technology such as GPS did not suddenly come into existence; in fact, it is laid down in the 18th century and developed throughout time. The first space is real and tangible, and it is also known as physical space.\nSecond space is the unblocking space. This type of space refers to the process whereby routine pathways of interaction as set up around which boundaries are often drawn. The routine may include the movement of office workers, the interaction of drunk teenagers, and the flow of goods, money, people, and information. Unlike the old time in geography when people accepted a space as blocked boundary (Example: A capitalist space, neoliberal space or city space), we began to realize that there is no such thing like boundaries in space. The space of the world is flowing and transforming continuously that it is very difficult to describe in a fixed way. The second space is ideology/conceptual and it is also known as mental space. For example, the second space will explain the behaviors of people from different social class and the social segregation among rich and poor people. \nThird space is the image space that refers to the process whereby the images has produced new kind of space. The images may be in different form and shape; ranging from painting to photograph, from portrait to post card, and from religious theme to entertainment. Nowadays, we are highly influenced by images in many ways and these certain images can tell us new social and cultures values, or something new about how we see the world. Images, symbols and sign do have some kind of spatial expression. \nFourth space is the place that refers to the process whereby spaces are ordered in ways that open up affective and other embodied potentials. Place space has more meaning than a place, and it can represent as different type of space. This fourth type of space tries to understand that place is a vital actor in bringing up people's lives in certain ways and place will let us to understand all kind of things which are hidden form us..\n\nAndrew Herod mentioned that scale, within human geography, is typically seen in one of the two ways: either as a real material thing which actually exists and is the result of political struggle and/or social process, or as a way of framing our understanding of the world. People’s lives across the globe have been re-scaling by contemporary economic, political, cultural and social processes, such as globalization, in complex ways. As a result, we have seen the creation of supra-national political bodies such as the European Union, the devolution of political power from the nation-state to regional political bodies. We have also experienced the increasing homogenization and ‘Americanization’ through the process of globalization while the locals’ tendencies (or counter force)among people who defend traditional ways of life increase around the world .The process of re-scaling people‘s lives and the relationship between the two extremes of our scaled lives- the ‘global’ and the ‘local’ were brought into question.\n\nUntil the 1980s, theorizing the concept of ‘scale’ itself was taken for granted although physical and human geographers looked at issues from ‘regional scale’ or‘national scale’. The questions such as whether scale is simply a mental device categorizing and ordering the world or whether scales really exists as material social products, particularly, were debated among materialists and idealists. Some geographers draw upon Immanuel Kant’s idealist philosophy that scales were handy conceptual mechanism for ordering the world while others, by drawing upon Marxist ideas of materialism, argue that scales really exist in the world and they were the real social products. For those idealists based on Kantian‘s inspiration, the ‘global’ is defined by the geologically given limits of the earth and the ‘local’ is defined as a spatial resolution useful for comprehending the process and practices. For materialists, the ‘national’ scale is a scale that had to be actively created through economic and political processes but not a scale existed in a logical hierarchy between global and the regional.\n\nThe notion of ‘becoming’ and the focus on the politics of producing scales have been central to materialist arguments concerning the global scale. It is important to recognize that social actors may have to work just as hard to become ‘local’as they have to work to become ‘global’. People paid attention to how transnational corporations have ‘gone global’, how institutions of governance have‘become’ supra-national and how labour unions have sought to ‘globalize’ their operations to match those of an increasingly ‘globalized’ city.\n\nFor the scale ‘global’ and ‘local’, Kevin Cox mentioned that moving from the local to the global scale ‘is not a movement from one discrete arena to another’ but a process of developing networks of associations that allow actors to shift between various spaces of engagement. According to his view, ‘scale’ is seen as a process rather than as a fixed entity and, in other words, the global and the local are not static ’arenas’within which social life plays out but are constantly made by social actions.For example, a political organization might attempt to go ‘global’ to engage with actors or opportunities outside of its own space; likewise, a transnational corporation may attempt to ‘go local’ through tailoring its products and operations in different places.\n\nGibson-Graham (2002) has identified at least six ways in which the relationship between the local and the global is often viewed.\n\n1. The global and the local are seen as interpretive frames for analyzing situations\n\n2. Drawing on Dirlik, Gibson-Graham suggests that in such a representation, the global is ‘something more than the national or regional ..anything other than the local’. Meaning that, the global and the local each derive meaning from what they are not.\n\n3. According to French social theorist Bruno Latour, the local and the global ‘offer different points of view on networks that are by nature neither local nor global, but are more or less long and more or less connected. Also, in Latour’s view, it is impossible to distinguish where the local ends and the global begins.\n\n4. The concept ‘The global is local’ was proposed by Gibson-Graham. For instance, multinational firms are actually ‘multi local‘ rather than ‘global’.\n\n5. The local is global. In this view, the local is an entry point to the world of global flows which encircle the planet.\n\n6. The global and the local are actually the processes rather than the locations. All spaces are the hybrids of global and local; so they are ‘glocal.’\n\nThere are some western thoughts that greater size and extensiveness imply domination and superior power, such that the local is often represented as ‘small and relatively powerless, defined and confined by the global’. So, the global is a force and the local is its field of play. However, the local can serve as a powerful scale of political organization; the global is not a scale just controlled by capital – those who challenge capital can also organize globally( Herod, A). There has been the concept ‘Think globally and act locally’ viewed by neoliberals.\n\nFor representing how the world is scaled, there are five different and popular metaphors: they are the ladder, concentric circles, Matryoshka nesting dolls, earthworm burrows and tree roots. First, in using such a metaphor of hierarchical ladder, the global as the highest rung on the ladder is seen to be above the local and all other scales. Second, the use of concentric metaphor leaves us with a particular way of conceptualizing the scalar relationship between places. In this second metaphor, the local is seen as a relatively small circle, with the regional as a larger circle encompassing it, while the national and the global scales are still larger circles encompassing the local and the regional. For the hierarchy of Russian Matryoshka nesting dolls, the global can contain other scales but this does not work the other way round; for instance, the local cannot contain the global. For the fourth metaphor concerning with thinking on scale, what French social theorist Bruno Latour argued is that a world of places is ‘networked’ together. Such the metaphor leaves us with an image of scale in which the global and the local are connected together and not totally separated from each other. For the tree roots metaphor similar with the earthworm burrow metaphor, as the earthworm burrows or tree roots penetrating different strata of the soil, it is difficult to determine exactly where one scale ends and another begins. When thinking about the use of metaphor, it should be aware that the choice of metaphor over another is not made on the basis of which is empirically a ‘more accurate’representation of something but, on the basis of how someone is attempting to understand a particular phenomenon.\n\nSuch an appreciation of metaphors is important because it suggests that how we talk about scale impacts upon the ways in which we engage socially and politically with our scaled world and that may impact on how we conduct our social, economic and political praxis and so make landscapes ( Herod,A )\n\n\n"}
{"id": "30762208", "url": "https://en.wikipedia.org/wiki?curid=30762208", "title": "Three-torus model of the universe", "text": "Three-torus model of the universe\n\nThe three-torus model is a cosmological model proposed in 1984 by Alexei Starobinsky and Yakov Borisovich Zel'dovich at the Landau Institute in Moscow. The theory describes the shape of the universe (topology) as a three-dimensional torus. It is also informally known as the doughnut theory.\n\nThe cosmic microwave background (CMB) was discovered by Bell Labs in 1964. Greater understanding of the universe's CMB provided greater understanding of the universe's topology. In order to understand these CMB results, NASA supported development of two exploratory satellites, the Cosmic Background Explorer (COBE) in 1989 and the Wilkinson Microwave Anisotropy Probe (WMAP) in 2001.\n\n"}
{"id": "54716184", "url": "https://en.wikipedia.org/wiki?curid=54716184", "title": "United States Energy Association", "text": "United States Energy Association\n\nThe United States Energy Association (USEA) is the U.S. Member Committee of the World Energy Council. Headquartered in Washington, D.C., USEA is an association of public and private energy-related organizations, corporations, and government agencies. \nThe association hosts annual events such as the Carbon Sequestration Leadership Forum, Energy Supply Forum, and State of the Energy Industry Forum.\n\nBarry Worthington has served as USEA’s executive director since 1988. Worthington chairs the Clean Electricity Production working group within the UNECE Committee on Sustainable Energy. He also sits on numerous energy boards, including the National Energy Foundation (chairman) and Energy Law Foundation.\n\nWorthington meets with domestic and international energy leaders to discuss energy infrastructure partnerships. He often advocates for energy cultivation in developing countries, claiming there are \"few priorities greater for the world than getting people linked to the grid.\" Worthington is a firm supporter of energy \"sovereignty.\"\n\nVicky Bailey currently chairs USEA’s board of directors. She succeeded Jack Futcher, the President and COO of Bechtel.\n\nFor 25 years, USEA has been a partner with USAID, expanding energy infrastructure, improving energy access, and reducing energy poverty in developing economies through international energy partnerships. A major function of USEA’s is to help USAID expand energy infrastructure and programs in developing countries. In 2012, the association launched the U.S.-East Africa Geothermal Partnership (EAGP), a public-private partnership “offering assistance at early stages of project development in East Africa.” Through the Djibouti Geothermal Partnership, Ethiopia Geothermal Partnership, and Kenya Electricity Generating Company (KenGen), USEA partners with the Department of Energy and local governments to promote U.S. companies’ involvement in developing additional geothermal generation capacity. According to USEA, the number of U.S. companies conducting geothermal work in East Africa has more than tripled since EAGP’s inception.\n\nUSEA represents the interests of the U.S. energy sector through public education and advocacy. The association supports an “all-of-the-above energy strategy,” from renewable energy to fossil fuels. USEA advocates for the exploration and production of oil and natural gas.\n\n"}
{"id": "31880", "url": "https://en.wikipedia.org/wiki?curid=31880", "title": "Universe", "text": "Universe\n\nThe Universe is all of space and time and their contents, including planets, stars, galaxies, and all other forms of matter and energy. While the spatial size of the entire Universe is still unknown, it is possible to measure the observable universe.\n\nThe earliest scientific models of the Universe were developed by ancient Greek and Indian philosophers and were geocentric, placing Earth at the centre of the Universe. Over the centuries, more precise astronomical observations led Nicolaus Copernicus to develop the heliocentric model with the Sun at the centre of the Solar System. In developing the law of universal gravitation, Sir Isaac Newton built upon Copernicus' work as well as observations by Tycho Brahe and Johannes Kepler's laws of planetary motion.\n\nFurther observational improvements led to the realization that our Sun is one of hundreds of billions of stars in a galaxy we call the Milky Way, which is one of at least hundreds of billions of galaxies in the Universe. Many of the stars in our galaxy have planets. At the largest scale galaxies are distributed uniformly and the same in all directions, meaning that the Universe has neither an edge nor a center. At smaller scales, galaxies are distributed in clusters and superclusters which form immense filaments and voids in space, creating a vast foam-like structure. Discoveries in the early 20th century have suggested that the Universe had a beginning and that space has been expanding since then, and is currently still expanding at an increasing rate.\n\nThe Big Bang theory is the prevailing cosmological description of the development of the Universe. Under this theory, space and time emerged together ago with a fixed amount of energy and matter that has become less dense as the Universe has expanded. After an initial accelerated expansion at around 10 seconds, and the separation of the four known fundamental forces, the Universe gradually cooled and continued to expand, allowing the first subatomic particles and simple atoms to form. Dark matter gradually gathered forming a foam-like structure of filaments and voids under the influence of gravity. Giant clouds of hydrogen and helium were gradually drawn to the places where dark matter was most dense, forming the first galaxies, stars, and everything else seen today. It is possible to see objects that are now further away than 13.799 billion light-years because space itself has expanded, and it is still expanding today. This means that objects which are now up to 46 billion light years away can still be seen in their distant past, because in the past when their light was emitted, they were much closer to the Earth.\n\nFrom studying the movement of galaxies, it has been discovered that the universe contains much more matter than is accounted for by visible objects; stars, galaxies, nebulas and interstellar gas. This unseen matter is known as dark matter  (\"dark\" means that there is a wide range of strong indirect evidence that it exists, but we have not yet detected it directly). The Lambda-CDM model is the most widely accepted model of our universe. It suggests that about [2015] of the mass and energy in the universe is a scalar field known as dark energy which is responsible for the current expansion of space, and about 25.8% [2015] is dark matter. Ordinary (\"baryonic\") matter is therefore only 4.9% [2015] of the physical universe. Stars, planets, and visible gas clouds only form about 6% of ordinary matter, or about 0.3% of the entire universe.\n\nThere are many competing hypotheses about the ultimate fate of the universe and about what, if anything, preceded the Big Bang, while other physicists and philosophers refuse to speculate, doubting that information about prior states will ever be accessible. Some physicists have suggested various multiverse hypotheses, in which the Universe might be one among many universes that likewise exist.\n\nThe physical Universe is defined as all of space and time (collectively referred to as spacetime) and their contents. Such contents comprise all of energy in its various forms, including electromagnetic radiation and matter, and therefore planets, moons, stars, galaxies, and the contents of intergalactic space. The Universe also includes the physical laws that influence energy and matter, such as conservation laws, classical mechanics, and relativity.\n\nThe Universe is often defined as \"the totality of existence\", or everything that exists, everything that has existed, and everything that will exist. In fact, some philosophers and scientists support the inclusion of ideas and abstract concepts – such as mathematics and logic – in the definition of the Universe. The word \"universe\" may also refer to concepts such as \"the cosmos\", \"the world\", and \"nature\".\n\nThe word \"universe\" derives from the Old French word \"univers\", which in turn derives from the Latin word \"universum\". The Latin word was used by Cicero and later Latin authors in many of the same senses as the modern English word is used.\n\nA term for \"universe\" among the ancient Greek philosophers from Pythagoras onwards was , \"tò pân\" (\"the all\"), defined as all matter and all space, and , \"tò hólon\" (\"all things\"), which did not necessarily include the void. Another synonym was , \"ho kósmos\" (meaning the world, the cosmos). Synonyms are also found in Latin authors (\"totum\", \"mundus\", \"natura\") and survive in modern languages, e.g., the German words \"Das All\", \"Weltall\", and \"Natur\" for \"Universe\". The same synonyms are found in English, such as everything (as in the theory of everything), the cosmos (as in cosmology), the world (as in the many-worlds interpretation), and nature (as in natural laws or natural philosophy).\n\nThe prevailing model for the evolution of the Universe is the Big Bang theory. The Big Bang model states that the earliest state of the Universe was an extremely hot and dense one, and that the Universe subsequently expanded and cooled. The model is based on general relativity and on simplifying assumptions such as homogeneity and isotropy of space. A version of the model with a cosmological constant (Lambda) and cold dark matter, known as the Lambda-CDM model, is the simplest model that provides a reasonably good account of various observations about the Universe. The Big Bang model accounts for observations such as the correlation of distance and redshift of galaxies, the ratio of the number of hydrogen to helium atoms, and the microwave radiation background.\n\nThe initial hot, dense state is called the Planck epoch, a brief period extending from time zero to one Planck time unit of approximately 10 seconds. During the Planck epoch, all types of matter and all types of energy were concentrated into a dense state, and gravity - currently the weakest by far of the four known forces - is believed to have been as strong as the other fundamental forces, and all the forces may have been unified. Since the Planck epoch, space has been expanding to its present scale, with a very short but intense period of cosmic inflation believed to have occurred within the first 10 seconds. This was a kind of expansion different from those we can see around us today. Objects in space did not physically move; instead the metric that defines space itself changed. Although objects in spacetime cannot move faster than the speed of light, this limitation does not apply to the metric governing spacetime itself. This initial period of inflation is believed to explain why space appears to be very flat, and much larger than light could travel since the start of the universe.\n\nWithin the first fraction of a second of the universe's existence, the four fundamental forces had separated. As the universe continued to cool down from its inconceivably hot state, various types of subatomic particles were able to form in short periods of time known as the quark epoch, the hadron epoch, and the lepton epoch. Together, these epochs encompassed less than 10 seconds of time following the Big Bang. These elementary particles associated stably into ever larger combinations, including stable protons and neutrons, which then formed more complex atomic nuclei through nuclear fusion. This process, known as Big Bang nucleosynthesis, only lasted for about 17 minutes and ended about 20 minutes after the Big Bang, so only the fastest and simplest reactions occurred. About 25% of the protons and all the neutrons in the universe, by mass, were converted to helium, with small amounts of deuterium (a form of hydrogen) and traces of lithium. Any other element was only formed in very tiny quantities. The other 75% of the protons remained unaffected, as hydrogen nuclei.\n\nAfter nucleosynthesis ended, the universe entered a period known as the photon epoch. During this period, the Universe was still far too hot for matter to form neutral atoms, so it contained a hot, dense, foggy plasma of negatively charged electrons, neutral neutrinos and positive nuclei. After about 377,000 years, the universe had cooled enough that electrons and nuclei could form the first stable atoms. This is known as recombination for historical reasons; in fact electrons and nuclei were combining for the first time. Unlike plasma, neutral atoms are transparent to many wavelengths of light, so for the first time the universe also became transparent. The photons released (\"decoupled\") when these atoms formed can still be seen today; they form the cosmic microwave background (CMB).\n\nAs the Universe expands, the energy density of electromagnetic radiation decreases more quickly than does that of matter because the energy of a photon decreases with its wavelength. At around 47,000 years, the energy density of matter became larger than that of photons and neutrinos, and began to dominate the large scale behavior of the universe. This marked the end of the radiation-dominated era and the start of the matter-dominated era.\n\nIn the earliest stages of the universe, tiny fluctuations within the universe's density led to concentrations of dark matter gradually forming. Ordinary matter, attracted to these by gravity, formed large gas clouds and eventually, stars and galaxies, where the dark matter was most dense, and voids where it was least dense. After around 100 - 300 million years, the first stars formed, known as Population III stars. These were probably very massive, luminous, non metallic and short-lived. They were responsible for the gradual reionization of the Universe between about 200-500 million years and 1 billion years, and also for seeding the universe with elements heavier than helium, through stellar nucleosynthesis. The Universe also contains a mysterious energy - possibly a scalar field - called dark energy, the density of which does not change over time. After about 9.8 billion years, the Universe had expanded sufficiently so that the density of matter was less than the density of dark energy, marking the beginning of the present dark-energy-dominated era. In this era, the expansion of the Universe is accelerating due to dark energy.\n\nOf the four fundamental interactions, gravitation is the dominant at astronomical length scales. Gravity's effects are cumulative; by contrast, the effects of positive and negative charges tend to cancel one another, making electromagnetism relatively insignificant on astronomical length scales. The remaining two interactions, the weak and strong nuclear forces, decline very rapidly with distance; their effects are confined mainly to sub-atomic length scales.\n\nThe Universe appears to have much more matter than antimatter, an asymmetry possibly related to the CP violation. This imbalance between matter and antimatter is partially responsible for the existence of all matter existing today, since matter and antimatter, if equally produced at the Big Bang, would have completely annihilated each other and left only photons as a result of their interaction. The Universe also appears to have neither net momentum nor angular momentum, which follows accepted physical laws if the Universe is finite. These laws are the Gauss's law and the non-divergence of the stress-energy-momentum pseudotensor.\n\nThe size of the Universe is somewhat difficult to define. According to the general theory of relativity, far regions of space may never interact with ours even in the lifetime of the Universe due to the finite speed of light and the ongoing expansion of space. For example, radio messages sent from Earth may never reach some regions of space, even if the Universe were to exist forever: space may expand faster than light can traverse it.\n\nDistant regions of space are assumed to exist and to be part of reality as much as we are, even though we can never interact with them. The spatial region that we can affect and be affected by is the observable universe. The observable universe depends on the location of the observer. By traveling, an observer can come into contact with a greater region of spacetime than an observer who remains still. Nevertheless, even the most rapid traveler will not be able to interact with all of space. Typically, the observable universe is taken to mean the portion of the Universe that is observable from our vantage point in the Milky Way.\n\nThe proper distance—the distance as would be measured at a specific time, including the present—between Earth and the edge of the observable universe is 46 billion light-years (14 billion parsecs), making the diameter of the observable universe about 93 billion light-years (28 billion parsecs). The distance the light from the edge of the observable universe has travelled is very close to the age of the Universe times the speed of light, , but this does not represent the distance at any given time because the edge of the observable universe and the Earth have since moved further apart. For comparison, the diameter of a typical galaxy is 30,000 light-years (9,198 parsecs), and the typical distance between two neighboring galaxies is 3 million light-years (919.8 kiloparsecs). As an example, the Milky Way is roughly 100,000–180,000 light years in diameter, and the nearest sister galaxy to the Milky Way, the Andromeda Galaxy, is located roughly 2.5 million light years away.\n\nBecause we cannot observe space beyond the edge of the observable universe, it is unknown whether the size of the Universe in its totality is finite or infinite. Estimates for the total size of the universe, if finite, reach as high as formula_1 megaparsecs, implied by one resolution of the No-Boundary Proposal.\n\nAstronomers calculate the age of the Universe by assuming that the Lambda-CDM model accurately describes the evolution of the Universe from a very uniform, hot, dense primordial state to its present state and measuring the cosmological parameters which constitute the model. This model is well understood theoretically and supported by recent high-precision astronomical observations such as WMAP and Planck. Commonly, the set of observations fitted includes the cosmic microwave background anisotropy, the brightness/redshift relation for Type Ia supernovae, and large-scale galaxy clustering including the baryon acoustic oscillation feature. Other observations, such as the Hubble constant, the abundance of galaxy clusters, weak gravitational lensing and globular cluster ages, are generally consistent with these, providing a check of the model, but are less accurately measured at present. Assuming that the Lambda-CDM model is correct, the measurements of the parameters using a variety of techniques by numerous experiments yield a best value of the age of the Universe as of 2015 of 13.799 ± 0.021 billion years.\nOver time, the Universe and its contents have evolved; for example, the relative population of quasars and galaxies has changed and space itself has expanded. Due to this expansion, scientists on Earth can observe the light from a galaxy 30 billion light years away even though that light has traveled for only 13 billion years; the very space between them has expanded. This expansion is consistent with the observation that the light from distant galaxies has been redshifted; the photons emitted have been stretched to longer wavelengths and lower frequency during their journey. Analyses of Type Ia supernovae indicate that the spatial expansion is accelerating.\n\nThe more matter there is in the Universe, the stronger the mutual gravitational pull of the matter. If the Universe were \"too\" dense then it would re-collapse into a gravitational singularity. However, if the Universe contained too \"little\" matter then the self-gravity would be too weak for astronomical structures, like galaxies or planets, to form. Since the Big Bang, the universe has expanded monotonically. Perhaps unsurprisingly, our universe has just the right mass-energy density, equivalent to about 5 protons per cubic meter, which has allowed it to expand for the last 13.8 billion years, giving time to form the universe as observed today.\n\nThere are dynamical forces acting on the particles in the Universe which affect the expansion rate. Before 1998, it was expected that the expansion rate would be decreasing as time went on due to the influence of gravitational interactions in the Universe; and thus there is an additional observable quantity in the Universe called the deceleration parameter, which most cosmologists expected to be positive and related to the matter density of the Universe. In 1998, the deceleration parameter was measured by two different groups to be negative, approximately -0.55, which technically implies that the second derivative of the cosmic scale factor formula_2 has been positive in the last 5-6 billion years. This acceleration does not, however, imply that the Hubble parameter is currently increasing; see deceleration parameter for details.\n\nSpacetimes are the arenas in which all physical events take place. The basic elements of spacetimes are events. In any given spacetime, an event is defined as a unique position at a unique time. A spacetime is the union of all events (in the same way that a line is the union of all of its points), formally organized into a manifold.\n\nThe Universe appears to be a smooth spacetime continuum consisting of three spatial dimensions and one temporal (time) dimension (an event in the spacetime of the physical Universe can therefore be identified by a set of four coordinates: (\"x\", \"y\", \"z\", \"t\") ). On the average, space is observed to be very nearly flat (with a curvature close to zero), meaning that Euclidean geometry is empirically true with high accuracy throughout most of the Universe. Spacetime also appears to have a simply connected topology, in analogy with a sphere, at least on the length-scale of the observable Universe. However, present observations cannot exclude the possibilities that the Universe has more dimensions (which is postulated by theories such as the String theory) and that its spacetime may have a multiply connected global topology, in analogy with the cylindrical or toroidal topologies of two-dimensional spaces.\nThe spacetime of the Universe is usually interpreted from a Euclidean perspective, with space as consisting of three dimensions, and time as consisting of one dimension, the \"fourth dimension\". By combining space and time into a single manifold called Minkowski space, physicists have simplified a large number of physical theories, as well as described in a more uniform way the workings of the Universe at both the supergalactic and subatomic levels.\n\nSpacetime events are not absolutely defined spatially and temporally but rather are known to be relative to the motion of an observer. Minkowski space approximates the Universe without gravity; the pseudo-Riemannian manifolds of general relativity describe spacetime with matter and gravity.\n\nGeneral relativity describes how spacetime is curved and bent by mass and energy (gravity). The topology or geometry of the Universe includes both local geometry in the observable universe and global geometry.\nCosmologists often work with a given space-like slice of spacetime called the comoving coordinates. The section of spacetime which can be observed is the backward light cone, which delimits the cosmological horizon.\nThe cosmological horizon (also called the particle horizon or the light horizon) is the maximum distance from which particles can have traveled to the observer in the age of the Universe. This horizon represents the boundary between the observable and the unobservable regions of the Universe. The existence, properties, and significance of a cosmological horizon depend on the particular cosmological model.\n\nAn important parameter determining the future evolution of the Universe theory is the density parameter, Omega (Ω), defined as the average matter density of the universe divided by a critical value of that density. This selects one of three possible geometries depending on whether Ω is equal to, less than, or greater than 1. These are called, respectively, the flat, open and closed universes.\n\nObservations, including the Cosmic Background Explorer (COBE), Wilkinson Microwave Anisotropy Probe (WMAP), and Planck maps of the CMB, suggest that the Universe is infinite in extent with a finite age, as described by the Friedmann–Lemaître–Robertson–Walker (FLRW) models. These FLRW models thus support inflationary models and the standard model of cosmology, describing a flat, homogeneous universe presently dominated by dark matter and dark energy.\n\nThe Universe may be \"fine-tuned\"; the Fine-tuned Universe hypothesis is the proposition that the conditions that allow the existence of observable life in the Universe can only occur when certain universal fundamental physical constants lie within a very narrow range of values, so that if any of several fundamental constants were only slightly different, the Universe would have been unlikely to be conducive to the establishment and development of matter, astronomical structures, elemental diversity, or life as it is understood. The proposition is discussed among philosophers, scientists, theologians, and proponents of creationism.\n\nThe Universe is composed almost completely of dark energy, dark matter, and ordinary matter. Other contents are electromagnetic radiation (estimated to constitute from 0.005% to close to 0.01% of the total mass of the Universe) and antimatter.\n\nThe proportions of all types of matter and energy have changed over the history of the Universe. The total amount of electromagnetic radiation generated within the universe has decreased by 1/2 in the past 2 billion years. Today, ordinary matter, which includes atoms, stars, galaxies, and life, accounts for only 4.9% of the contents of the Universe. The present overall density of this type of matter is very low, roughly 4.5 × 10 grams per cubic centimetre, corresponding to a density of the order of only one proton for every four cubic meters of volume. The nature of both dark energy and dark matter is unknown. Dark matter, a mysterious form of matter that has not yet been identified, accounts for 26.8% of the cosmic contents. Dark energy, which is the energy of empty space and is causing the expansion of the Universe to accelerate, accounts for the remaining 68.3% of the contents.\nMatter, dark matter, and dark energy are distributed homogeneously throughout the Universe over length scales longer than 300 million light-years or so. However, over shorter length-scales, matter tends to clump hierarchically; many atoms are condensed into stars, most stars into galaxies, most galaxies into clusters, superclusters and, finally, large-scale galactic filaments. The observable Universe contains approximately 300 sextillion (3) stars and more than 100 billion (10) galaxies. Typical galaxies range from dwarfs with as few as ten million (10) stars up to giants with one trillion (10) stars. Between the larger structures are voids, which are typically 10–150 Mpc (33 million–490 million ly) in diameter. The Milky Way is in the Local Group of galaxies, which in turn is in the Laniakea Supercluster. This supercluster spans over 500 million light years, while the Local Group spans over 10 million light years. The Universe also has vast regions of relative emptiness; the largest known void measures 1.8 billion ly (550 Mpc) across.\n\nThe observable Universe is isotropic on scales significantly larger than superclusters, meaning that the statistical properties of the Universe are the same in all directions as observed from Earth. The Universe is bathed in highly isotropic microwave radiation that corresponds to a thermal equilibrium blackbody spectrum of roughly 2.72548 kelvins. The hypothesis that the large-scale Universe is homogeneous and isotropic is known as the cosmological principle. A Universe that is both homogeneous and isotropic looks the same from all vantage points and has no center.\n\nAn explanation for why the expansion of the Universe is accelerating remains elusive. It is often attributed to \"dark energy\", an unknown form of energy that is hypothesized to permeate space. On a mass–energy equivalence basis, the density of dark energy (~ 7 × 10 g/cm) is much less than the density of ordinary matter or dark matter within galaxies. However, in the present dark-energy era, it dominates the mass–energy of the universe because it is uniform across space.\n\nTwo proposed forms for dark energy are the cosmological constant, a \"constant\" energy density filling space homogeneously, and scalar fields such as quintessence or moduli, \"dynamic\" quantities whose energy density can vary in time and space. Contributions from scalar fields that are constant in space are usually also included in the cosmological constant. The cosmological constant can be formulated to be equivalent to vacuum energy. Scalar fields having only a slight amount of spatial inhomogeneity would be difficult to distinguish from a cosmological constant.\n\nDark matter is a hypothetical kind of matter that is invisible to the entire electromagnetic spectrum, but which accounts for most of the matter in the Universe. The existence and properties of dark matter are inferred from its gravitational effects on visible matter, radiation, and the large-scale structure of the Universe. Other than neutrinos, a form of hot dark matter, dark matter has not been detected directly, making it one of the greatest mysteries in modern astrophysics. Dark matter neither emits nor absorbs light or any other electromagnetic radiation at any significant level. Dark matter is estimated to constitute 26.8% of the total mass–energy and 84.5% of the total matter in the Universe.\n\nThe remaining 4.9% of the mass–energy of the Universe is ordinary matter, that is, atoms, ions, electrons and the objects they form. This matter includes stars, which produce nearly all of the light we see from galaxies, as well as interstellar gas in the interstellar and intergalactic media, planets, and all the objects from everyday life that we can bump into, touch or squeeze. As a matter of fact, the great majority of ordinary matter in the universe is unseen, since visible stars and gas inside galaxies and clusters account for less than 10 per cent of the ordinary matter contribution to the mass-energy density of the universe.\n\nOrdinary matter commonly exists in four states (or phases): solid, liquid, gas, and plasma. However, advances in experimental techniques have revealed other previously theoretical phases, such as Bose–Einstein condensates and fermionic condensates.\n\nOrdinary matter is composed of two types of elementary particles: quarks and leptons. For example, the proton is formed of two up quarks and one down quark; the neutron is formed of two down quarks and one up quark; and the electron is a kind of lepton. An atom consists of an atomic nucleus, made up of protons and neutrons, and electrons that orbit the nucleus. Because most of the mass of an atom is concentrated in its nucleus, which is made up of baryons, astronomers often use the term \"baryonic matter\" to describe ordinary matter, although a small fraction of this \"baryonic matter\" is electrons.\n\nSoon after the Big Bang, primordial protons and neutrons formed from the quark–gluon plasma of the early Universe as it cooled below two trillion degrees. A few minutes later, in a process known as Big Bang nucleosynthesis, nuclei formed from the primordial protons and neutrons. This nucleosynthesis formed lighter elements, those with small atomic numbers up to lithium and beryllium, but the abundance of heavier elements dropped off sharply with increasing atomic number. Some boron may have been formed at this time, but the next heavier element, carbon, was not formed in significant amounts. Big Bang nucleosynthesis shut down after about 20 minutes due to the rapid drop in temperature and density of the expanding Universe. Subsequent formation of heavier elements resulted from stellar nucleosynthesis and supernova nucleosynthesis.\n\nOrdinary matter and the forces that act on matter can be described in terms of elementary particles. These particles are sometimes described as being fundamental, since they have an unknown substructure, and it is unknown whether or not they are composed of smaller and even more fundamental particles. Of central importance is the Standard Model, a theory that is concerned with electromagnetic interactions and the weak and strong nuclear interactions. The Standard Model is supported by the experimental confirmation of the existence of particles that compose matter: quarks and leptons, and their corresponding \"antimatter\" duals, as well as the force particles that mediate interactions: the photon, the W and Z bosons, and the gluon. The Standard Model predicted the existence of the recently discovered Higgs boson, a particle that is a manifestation of a field within the Universe that can endow particles with mass. Because of its success in explaining a wide variety of experimental results, the Standard Model is sometimes regarded as a \"theory of almost everything\". The Standard Model does not, however, accommodate gravity. A true force-particle \"theory of everything\" has not been attained.\n\nA hadron is a composite particle made of quarks held together by the strong force. Hadrons are categorized into two families: baryons (such as protons and neutrons) made of three quarks, and mesons (such as pions) made of one quark and one antiquark. Of the hadrons, protons are stable, and neutrons bound within atomic nuclei are stable. Other hadrons are unstable under ordinary conditions and are thus insignificant constituents of the modern Universe.\nFrom approximately 10 seconds after the Big Bang, during a period is known as the hadron epoch, the temperature of the universe had fallen sufficiently to allow quarks to bind together into hadrons, and the mass of the Universe was dominated by hadrons. Initially the temperature was high enough to allow the formation of hadron/anti-hadron pairs, which kept matter and antimatter in thermal equilibrium. However, as the temperature of the Universe continued to fall, hadron/anti-hadron pairs were no longer produced. Most of the hadrons and anti-hadrons were then eliminated in particle-antiparticle annihilation reactions, leaving a small residual of hadrons by the time the Universe was about one second old.\n\nA lepton is an elementary, half-integer spin particle that does not undergo strong interactions but is subject to the Pauli exclusion principle; no two leptons of the same species can be in exactly the same state at the same time. Two main classes of leptons exist: charged leptons (also known as the \"electron-like\" leptons), and neutral leptons (better known as neutrinos). Electrons are stable and the most common charged lepton in the Universe, whereas muons and taus are unstable particle that quickly decay after being produced in high energy collisions, such as those involving cosmic rays or carried out in particle accelerators.\nCharged leptons can combine with other particles to form various composite particles such as atoms and positronium. The electron governs nearly all of chemistry, as it is found in atoms and is directly tied to all chemical properties. Neutrinos rarely interact with anything, and are consequently rarely observed. Neutrinos stream throughout the Universe but rarely interact with normal matter.\n\nThe lepton epoch was the period in the evolution of the early Universe in which the leptons dominated the mass of the Universe. It started roughly 1 second after the Big Bang, after the majority of hadrons and anti-hadrons annihilated each other at the end of the hadron epoch. During the lepton epoch the temperature of the Universe was still high enough to create lepton/anti-lepton pairs, so leptons and anti-leptons were in thermal equilibrium. Approximately 10 seconds after the Big Bang, the temperature of the Universe had fallen to the point where lepton/anti-lepton pairs were no longer created. Most leptons and anti-leptons were then eliminated in annihilation reactions, leaving a small residue of leptons. The mass of the Universe was then dominated by photons as it entered the following photon epoch.\n\nA photon is the quantum of light and all other forms of electromagnetic radiation. It is the force carrier for the electromagnetic force, even when static via virtual photons. The effects of this force are easily observable at the microscopic and at the macroscopic level because the photon has zero rest mass; this allows long distance interactions. Like all elementary particles, photons are currently best explained by quantum mechanics and exhibit wave–particle duality, exhibiting properties of waves and of particles.\n\nThe photon epoch started after most leptons and anti-leptons were annihilated at the end of the lepton epoch, about 10 seconds after the Big Bang. Atomic nuclei were created in the process of nucleosynthesis which occurred during the first few minutes of the photon epoch. For the remainder of the photon epoch the Universe contained a hot dense plasma of nuclei, electrons and photons. About 380,000 years after the Big Bang, the temperature of the Universe fell to the point where nuclei could combine with electrons to create neutral atoms. As a result, photons no longer interacted frequently with matter and the Universe became transparent. The highly redshifted photons from this period form the cosmic microwave background. Tiny variations in temperature and density detectable in the CMB were the early \"seeds\" from which all subsequent structure formation took place.\nGeneral relativity is the geometric theory of gravitation published by Albert Einstein in 1915 and the current description of gravitation in modern physics. It is the basis of current cosmological models of the Universe. General relativity generalizes special relativity and Newton's law of universal gravitation, providing a unified description of gravity as a geometric property of space and time, or spacetime. In particular, the curvature of spacetime is directly related to the energy and momentum of whatever matter and radiation are present. The relation is specified by the Einstein field equations, a system of partial differential equations. In general relativity, the distribution of matter and energy determines the geometry of spacetime, which in turn describes the acceleration of matter. Therefore, solutions of the Einstein field equations describe the evolution of the Universe. Combined with measurements of the amount, type, and distribution of matter in the Universe, the equations of general relativity describe the evolution of the Universe over time.\n\nWith the assumption of the cosmological principle that the Universe is homogeneous and isotropic everywhere, a specific solution of the field equations that describes the Universe is the metric tensor called the Friedmann–Lemaître–Robertson–Walker metric,\nwhere (\"r\", θ, φ) correspond to a spherical coordinate system. This metric has only two undetermined parameters. An overall dimensionless length scale factor \"R\" describes the size scale of the Universe as a function of time; an increase in \"R\" is the expansion of the Universe. A curvature index \"k\" describes the geometry. The index \"k\" is defined so that it can take only one of three values: 0, corresponding to flat Euclidean geometry; 1, corresponding to a space of positive curvature; or −1, corresponding to a space of positive or negative curvature. The value of \"R\" as a function of time \"t\" depends upon \"k\" and the cosmological constant \"Λ\". The cosmological constant represents the energy density of the vacuum of space and could be related to dark energy. The equation describing how \"R\" varies with time is known as the Friedmann equation after its inventor, Alexander Friedmann.\n\nThe solutions for \"R(t)\" depend on \"k\" and \"Λ\", but some qualitative features of such solutions are general. First and most importantly, the length scale \"R\" of the Universe can remain constant \"only\" if the Universe is perfectly isotropic with positive curvature (\"k\"=1) and has one precise value of density everywhere, as first noted by Albert Einstein. However, this equilibrium is unstable: because the Universe is known to be inhomogeneous on smaller scales, \"R\" must change over time. When \"R\" changes, all the spatial distances in the Universe change in tandem; there is an overall expansion or contraction of space itself. This accounts for the observation that galaxies appear to be flying apart; the space between them is stretching. The stretching of space also accounts for the apparent paradox that two galaxies can be 40 billion light years apart, although they started from the same point 13.8 billion years ago and never moved faster than the speed of light.\n\nSecond, all solutions suggest that there was a gravitational singularity in the past, when \"R\" went to zero and matter and energy were infinitely dense. It may seem that this conclusion is uncertain because it is based on the questionable assumptions of perfect homogeneity and isotropy (the cosmological principle) and that only the gravitational interaction is significant. However, the Penrose–Hawking singularity theorems show that a singularity should exist for very general conditions. Hence, according to Einstein's field equations, \"R\" grew rapidly from an unimaginably hot, dense state that existed immediately following this singularity (when \"R\" had a small, finite value); this is the essence of the Big Bang model of the Universe. Understanding the singularity of the Big Bang likely requires a quantum theory of gravity, which has not yet been formulated.\n\nThird, the curvature index \"k\" determines the sign of the mean spatial curvature of spacetime averaged over sufficiently large length scales (greater than about a billion light years). If \"k\"=1, the curvature is positive and the Universe has a finite volume. A Universe with positive curvature is often visualized as a three-dimensional sphere embedded in a four-dimensional space. Conversely, if \"k\" is zero or negative, the Universe has an infinite volume. It may seem counter-intuitive that an infinite and yet infinitely dense Universe could be created in a single instant at the Big Bang when \"R\"=0, but exactly that is predicted mathematically when \"k\" does not equal 1. By analogy, an infinite plane has zero curvature but infinite area, whereas an infinite cylinder is finite in one direction and a torus is finite in both. A toroidal Universe could behave like a normal Universe with periodic boundary conditions.\n\nThe ultimate fate of the Universe is still unknown, because it depends critically on the curvature index \"k\" and the cosmological constant \"Λ\". If the Universe were sufficiently dense, \"k\" would equal +1, meaning that its average curvature throughout is positive and the Universe will eventually recollapse in a Big Crunch, possibly starting a new Universe in a Big Bounce. Conversely, if the Universe were insufficiently dense, \"k\" would equal 0 or −1 and the Universe would expand forever, cooling off and eventually reaching the Big Freeze and the heat death of the Universe. Modern data suggests that the rate of expansion of the Universe is not decreasing, as originally expected, but increasing; if this continues indefinitely, the Universe may eventually reach a Big Rip. Observationally, the Universe appears to be flat (\"k\" = 0), with an overall density that is very close to the critical value between recollapse and eternal expansion.\n\nSome speculative theories have proposed that our Universe is but one of a set of disconnected universes, collectively denoted as the multiverse, challenging or enhancing more limited definitions of the Universe. Scientific multiverse models are distinct from concepts such as alternate planes of consciousness and simulated reality.\n\nMax Tegmark developed a four-part classification scheme for the different types of multiverses that scientists have suggested in response to various Physics problems. An example of such multiverses is the one resulting from the chaotic inflation model of the early universe. Another is the multiverse resulting from the many-worlds interpretation of quantum mechanics. In this interpretation, parallel worlds are generated in a manner similar to quantum superposition and decoherence, with all states of the wave functions being realized in separate worlds. Effectively, in the many-worlds interpretation the multiverse evolves as a universal wavefunction. If the Big Bang that created our multiverse created an ensemble of multiverses, the wave function of the ensemble would be entangled in this sense.\n\nThe least controversial category of multiverse in Tegmark's scheme is . The multiverses of this level are composed by distant spacetime events \"in our own universe\". If space is infinite, or sufficiently large and uniform, identical instances of the history of Earth's entire Hubble volume occur every so often, simply by chance. Tegmark calculated that our nearest so-called doppelgänger, is 10 meters away from us (a double exponential function larger than a googolplex). In principle, it would be impossible to scientifically verify the existence of an identical Hubble volume. However, this existence does follow as a fairly straightforward consequence \n\nIt is possible to conceive of disconnected spacetimes, each existing but unable to interact with one another. An easily visualized metaphor of this concept is a group of separate soap bubbles, in which observers living on one soap bubble cannot interact with those on other soap bubbles, even in principle. According to one common terminology, each \"soap bubble\" of spacetime is denoted as a \"universe\", whereas our particular spacetime is denoted as \"the Universe\", just as we call our moon \"the Moon\". The entire collection of these separate spacetimes is denoted as the multiverse. With this terminology, different \"Universes\" are not causally connected to each other. In principle, the other unconnected \"Universes\" may have different dimensionalities and topologies of spacetime, different forms of matter and energy, and different physical laws and physical constants, although such possibilities are purely speculative. Others consider each of several bubbles created as part of chaotic inflation to be separate \"Universes\", though in this model these universes all share a causal origin.\n\nHistorically, there have been many ideas of the cosmos (cosmologies) and its origin (cosmogonies). Theories of an impersonal Universe governed by physical laws were first proposed by the Greeks and Indians. Ancient Chinese philosophy encompassed the notion of the Universe including both all of space and all of time. Over the centuries, improvements in astronomical observations and theories of motion and gravitation led to ever more accurate descriptions of the Universe. The modern era of cosmology began with Albert Einstein's 1915 general theory of relativity, which made it possible to quantitatively predict the origin, evolution, and conclusion of the Universe as a whole. Most modern, accepted theories of cosmology are based on general relativity and, more specifically, the predicted Big Bang.\n\nMany cultures have stories describing the origin of the world and universe. Cultures generally regard these stories as having some truth. There are however many differing beliefs in how these stories apply amongst those believing in a supernatural origin, ranging from a god directly creating the Universe as it is now to a god just setting the \"wheels in motion\" (for example via mechanisms such as the big bang and evolution).\n\nEthnologists and anthropologists who study myths have developed various classification schemes for the various themes that appear in creation stories. For example, in one type of story, the world is born from a world egg; such stories include the Finnish epic poem \"Kalevala\", the Chinese story of Pangu or the Indian Brahmanda Purana. In related stories, the Universe is created by a single entity emanating or producing something by him- or herself, as in the Tibetan Buddhism concept of Adi-Buddha, the ancient Greek story of Gaia (Mother Earth), the Aztec goddess Coatlicue myth, the ancient Egyptian god Atum story, and the Judeo-Christian Genesis creation narrative in which the Abrahamic God created the Universe. In another type of story, the Universe is created from the union of male and female deities, as in the Maori story of Rangi and Papa. In other stories, the Universe is created by crafting it from pre-existing materials, such as the corpse of a dead god — as from Tiamat in the Babylonian epic \"Enuma Elish\" or from the giant Ymir in Norse mythology – or from chaotic materials, as in Izanagi and Izanami in Japanese mythology. In other stories, the Universe emanates from fundamental principles, such as Brahman and Prakrti, the creation myth of the Serers, or the yin and yang of the Tao.\n\nThe pre-Socratic Greek philosophers and Indian philosophers developed some of the earliest philosophical concepts of the Universe. The earliest Greek philosophers noted that appearances can be deceiving, and sought to understand the underlying reality behind the appearances. In particular, they noted the ability of matter to change forms (e.g., ice to water to steam) and several philosophers proposed that all the physical materials in the world are different forms of a single primordial material, or \"arche\". The first to do so was Thales, who proposed this material to be water. Thales' student, Anaximander, proposed that everything came from the limitless \"apeiron\". Anaximenes proposed the primordial material to be air on account of its perceived attractive and repulsive qualities that cause the \"arche\" to condense or dissociate into different forms. Anaxagoras proposed the principle of \"Nous\" (Mind), while Heraclitus proposed fire (and spoke of \"logos\"). Empedocles proposed the elements to be earth, water, air and fire. His four-element model became very popular. Like Pythagoras, Plato believed that all things were composed of number, with Empedocles' elements taking the form of the Platonic solids. Democritus, and later philosophers—most notably Leucippus—proposed that the Universe is composed of indivisible atoms moving through a void (vacuum), although Aristotle did not believe that to be feasible because air, like water, offers resistance to motion. Air will immediately rush in to fill a void, and moreover, without resistance, it would do so indefinitely fast.\n\nAlthough Heraclitus argued for eternal change, his contemporary Parmenides made the radical suggestion that all change is an illusion, that the true underlying reality is eternally unchanging and of a single nature. Parmenides denoted this reality as (The One). Parmenides' idea seemed implausible to many Greeks, but his student Zeno of Elea challenged them with several famous paradoxes. Aristotle responded to these paradoxes by developing the notion of a potential countable infinity, as well as the infinitely divisible continuum. Unlike the eternal and unchanging cycles of time, he believed that the world is bounded by the celestial spheres and that cumulative stellar magnitude is only finitely multiplicative.\n\nThe Indian philosopher Kanada, founder of the Vaisheshika school, developed a notion of atomism and proposed that light and heat were varieties of the same substance. In the 5th century AD, the Buddhist atomist philosopher Dignāga proposed atoms to be point-sized, durationless, and made of energy. They denied the existence of substantial matter and proposed that movement consisted of momentary flashes of a stream of energy.\n\nThe notion of temporal finitism was inspired by the doctrine of creation shared by the three Abrahamic religions: Judaism, Christianity and Islam. The Christian philosopher, John Philoponus, presented the philosophical arguments against the ancient Greek notion of an infinite past and future. Philoponus' arguments against an infinite past were used by the early Muslim philosopher, Al-Kindi (Alkindus); the Jewish philosopher, Saadia Gaon (Saadia ben Joseph); and the Muslim theologian, Al-Ghazali (Algazel).\n\nAstronomical models of the Universe were proposed soon after astronomy began with the Babylonian astronomers, who viewed the Universe as a flat disk floating in the ocean, and this forms the premise for early Greek maps like those of Anaximander and Hecataeus of Miletus.\n\nLater Greek philosophers, observing the motions of the heavenly bodies, were concerned with developing models of the Universe-based more profoundly on empirical evidence. The first coherent model was proposed by Eudoxus of Cnidos. According to Aristotle's physical interpretation of the model, celestial spheres eternally rotate with uniform motion around a stationary Earth. Normal matter is entirely contained within the terrestrial sphere.\n\n\"De Mundo\" (composed before 250 BC or between 350 and 200 BC), stated, \"Five elements, situated in spheres in five regions, the less being in each case surrounded by the greater—namely, earth surrounded by water, water by air, air by fire, and fire by ether—make up the whole Universe\".\n\nThis model was also refined by Callippus and after concentric spheres were abandoned, it was brought into nearly perfect agreement with astronomical observations by Ptolemy. The success of such a model is largely due to the mathematical fact that any function (such as the position of a planet) can be decomposed into a set of circular functions (the Fourier modes). Other Greek scientists, such as the Pythagorean philosopher Philolaus, postulated (according to Stobaeus account) that at the center of the Universe was a \"central fire\" around which the Earth, Sun, Moon and Planets revolved in uniform circular motion.\n\nThe Greek astronomer Aristarchus of Samos was the first known individual to propose a heliocentric model of the Universe. Though the original text has been lost, a reference in Archimedes' book \"The Sand Reckoner\" describes Aristarchus's heliocentric model. Archimedes wrote:\n\nYou, King Gelon, are aware the Universe is the name given by most astronomers to the sphere the center of which is the center of the Earth, while its radius is equal to the straight line between the center of the Sun and the center of the Earth. This is the common account as you have heard from astronomers. But Aristarchus has brought out a book consisting of certain hypotheses, wherein it appears, as a consequence of the assumptions made, that the Universe is many times greater than the Universe just mentioned. His hypotheses are that the fixed stars and the Sun remain unmoved, that the Earth revolves about the Sun on the circumference of a circle, the Sun lying in the middle of the orbit, and that the sphere of fixed stars, situated about the same center as the Sun, is so great that the circle in which he supposes the Earth to revolve bears such a proportion to the distance of the fixed stars as the center of the sphere bears to its surface\n\nAristarchus thus believed the stars to be very far away, and saw this as the reason why stellar parallax had not been observed, that is, the stars had not been observed to move relative each other as the Earth moved around the Sun. The stars are in fact much farther away than the distance that was generally assumed in ancient times, which is why stellar parallax is only detectable with precision instruments. The geocentric model, consistent with planetary parallax, was assumed to be an explanation for the unobservability of the parallel phenomenon, stellar parallax. The rejection of the heliocentric view was apparently quite strong, as the following passage from Plutarch suggests (\"On the Apparent Face in the Orb of the Moon\"):\n\nCleanthes [a contemporary of Aristarchus and head of the Stoics] thought it was the duty of the Greeks to indict Aristarchus of Samos on the charge of impiety for putting in motion the Hearth of the Universe [i.e. the Earth], ... supposing the heaven to remain at rest and the Earth to revolve in an oblique circle, while it rotates, at the same time, about its own axis\n\nThe only other astronomer from antiquity known by name who supported Aristarchus's heliocentric model was Seleucus of Seleucia, a Hellenistic astronomer who lived a century after Aristarchus. According to Plutarch, Seleucus was the first to prove the heliocentric system through reasoning, but it is not known what arguments he used. Seleucus' arguments for a heliocentric cosmology were probably related to the phenomenon of tides. According to Strabo (1.1.9), Seleucus was the first to state that the tides are due to the attraction of the Moon, and that the height of the tides depends on the Moon's position relative to the Sun. Alternatively, he may have proved heliocentricity by determining the constants of a geometric model for it, and by developing methods to compute planetary positions using this model, like what Nicolaus Copernicus later did in the 16th century. During the Middle Ages, heliocentric models were also proposed by the Indian astronomer Aryabhata, and by the Persian astronomers Albumasar and Al-Sijzi.\n\nThe Aristotelian model was accepted in the Western world for roughly two millennia, until Copernicus revived Aristarchus's perspective that the astronomical data could be explained more plausibly if the Earth rotated on its axis and if the Sun were placed at the center of the Universe.\n\nAs noted by Copernicus himself, the notion that the Earth rotates is very old, dating at least to Philolaus (c. 450 BC), Heraclides Ponticus (c. 350 BC) and Ecphantus the Pythagorean. Roughly a century before Copernicus, the Christian scholar Nicholas of Cusa also proposed that the Earth rotates on its axis in his book, \"On Learned Ignorance\" (1440). Al-Sijzi also proposed that the Earth rotates on its axis. Empirical evidence for the Earth's rotation on its axis, using the phenomenon of comets, was given by Tusi (1201–1274) and Ali Qushji (1403–1474).\n\nThis cosmology was accepted by Isaac Newton, Christiaan Huygens and later scientists. Edmund Halley (1720) and Jean-Philippe de Chéseaux (1744) noted independently that the assumption of an infinite space filled uniformly with stars would lead to the prediction that the nighttime sky would be as bright as the Sun itself; this became known as Olbers' paradox in the 19th century. Newton believed that an infinite space uniformly filled with matter would cause infinite forces and instabilities causing the matter to be crushed inwards under its own gravity. This instability was clarified in 1902 by the Jeans instability criterion. One solution to these paradoxes is the Charlier Universe, in which the matter is arranged hierarchically (systems of orbiting bodies that are themselves orbiting in a larger system, \"ad infinitum\") in a fractal way such that the Universe has a negligibly small overall density; such a cosmological model had also been proposed earlier in 1761 by Johann Heinrich Lambert. A significant astronomical advance of the 18th century was the realization by Thomas Wright, Immanuel Kant and others of nebulae.\n\nIn 1919, when Hooker Telescope was completed, the prevailing view still was that the Universe consisted entirely of the Milky Way Galaxy. Using the Hooker Telescope, Edwin Hubble identified Cepheid variables in several spiral nebulae and in 1922–1923 proved conclusively that Andromeda Nebula and Triangulum among others, were entire galaxies outside our own, thus proving that Universe consists of multitude of galaxies.\n\nThe modern era of physical cosmology began in 1917, when Albert Einstein first applied his general theory of relativity to model the structure and dynamics of the Universe.\n\n\n"}
{"id": "26830333", "url": "https://en.wikipedia.org/wiki?curid=26830333", "title": "Water-energy nexus", "text": "Water-energy nexus\n\nThere is no formal definition for the water-energy nexus - the concept refers to the relationship between the water used for energy production, including both electricity and sources of fuel such as oil and natural gas, and the energy consumed to extract, purify, deliver, heat/cool, treat and dispose of water (and wastewater) sometimes referred to as the energy intensity (EI). The relationship is not truly a closed loop as the water used for energy production need not be the same water that is processed using that energy, but all forms of energy production require some input of water making the relationship inextricable.\n\nAmong the first studies to evaluate the water and energy relationship was a life-cycle analysis conducted by Peter Gleick in 1994 that highlighted the interdependence and initiated the joint study of water and energy. In 2014 the US Department of Energy (DOE) released their report on the water-energy nexus citing the need for joint water-energy policies and better understanding of the nexus and its susceptibility to climate change as a matter of national security. The hybrid Sankey diagram in the DOE's 2014 water-energy nexus report summarizes water and energy flows in the US by sector, demonstrating interdependence as well as singling out thermoelectric power as the single largest user of water, used mainly for cooling. \n\nAll types of energy generation consume water either to process the raw materials used in the facility, constructing and maintaining the plant, or to just generate the electricity itself. Renewable power sources such as photovoltaic solar and wind power, which require little water to produce energy, require water in processing the raw materials to build. Water can either be \"used\" or \"consumed,\" and can be categorised as fresh, ground, surface, blue, grey or green among others. Water is considered used if it does not reduce the supply of water to downstream users, i.e. water that is taken and returned to the same source (instream use), such as in thermoelectric plants that use water for cooling and are by far the largest users of water. While used water is returned to the system for downstream uses, it has usually been degraded in some way, mainly due to thermal or chemical pollution, and the natural flow has been altered which does not factor into an assessment if only the quantity of water is considered. Water is consumed when it is removed completely from the system, such as by evaporation or consumption by crops or humans. When assessing water use all these factors must be considered as well as spatiotemporal considerations making precise determination of water use very difficult.\n\nSpang et al. (2014) conducted a study looking at the water consumption for energy production (WCEP) internationally that both showed the variation in energy types produced across countries as well as the vast differences in efficiency of energy production per unit of water use (figure 1). Operations of water distribution systems and power distribution systems under emergency conditions of limited power and water availability is an important consideration for improving the overall resilience of the water - energy nexus. Khatavkar and Mays (2017) present a methodology for control of water distribution and power distribution systems under emergency conditions of drought and limited power availability to ascertain at least minimal supply of cooling water to the power plants. Khatavkar and Mays (2017 b) applied an optimization model for water - energy nexus system for a hypothetical regional level system which showed an improved resilience for several contingency scenarios.\n\nIn 2001 operating water systems in the US consumed approximately 3% of the total annual electricity (~75 TWh). The California's State Water Project (SWP) and Central Valley Project (CVP) are together the largest water system in the world with the highest water lift, over 2000 ft. across the Tehachapi mountains, delivering water from the wetter and relatively rural north of the state, to the agriculturally intensive central valley, and finally to the arid and heavily populated south. Consequently, the SWP and CVP are the single largest consumers of electricity in California consuming approximately 5 TWh of electricity each per year. In 2001, 19% of the state’s total electricity use (~48 TWh/year) was used in processing water including end uses, with the urban sector accounting for 65% of this. In addition to electricity, 30% of California’s natural gas consumption was due to water-related processes, mainly residential water heating, and 88 million gallons of diesel was consumed by groundwater pumps for agriculture. The residential sector alone accounted for 48% of the total combined electricity and natural gas consumed for water-related processes in the state.\n\nAccording to the California Public Utilities Commission (CPUC) Energy Division’s Embedded Energy in Water Studies report:\"“'Energy Intensity' refers to the average amount of energy needed to transport or treat water or wastewater on a per unit basis\"\"Energy Intensity is sometimes used synonymously with embedded or embodied energy. In 2005, water deliveries to Southern California were assessed to have an average EI of 12.7 MWh/MG, nearly two-thirds of which was due to transportation. Following the findings that a fifth of California’s electricity is consumed in water-related processes including end-use, the CPUC responded by authorising a statewide study into the relationship between energy and water that was conducted by the California Institute for Energy and Environment (CIEE), and developed programs to save energy through water conservation.\n\nHydroelectricity is a special case of water used for energy production mainly because hydroelectric power generation is regarded as being clean and renewable, and dams (the main source of hydroelectric production) serve multiple purposes besides energy generation, including flood prevention, storage, control and recreation which make justifiable allocation analyses difficult. Furthermore, the impacts of hydroelectric power generation can be hard to quantify both in terms of evaporative consumptive losses and altered quality of water, since damming results in flows that are much colder than for flowing streams. In some cases the moderation of flows can be seen as a rivalry of water use in time may also need to accounted for in impact analysis.\n\n\n"}
{"id": "34043", "url": "https://en.wikipedia.org/wiki?curid=34043", "title": "Wormhole", "text": "Wormhole\n\nA wormhole (or Einstein–Rosen bridge) is a speculative structure linking separate points in spacetime, and is based on a solution of the Einstein field equations. A wormhole can be visualized as a tunnel with two ends, each at separate points in spacetime (i.e., different locations or different points of time). More precisely it is a transcendental bijection of the spacetime continuum, an asymptotic projection of the Calabi–Yau manifold manifesting itself in Anti-de Sitter space. \n\nWormholes are consistent with the general theory of relativity, but whether wormholes actually exist remains to be seen.\n\nA wormhole could connect extremely long distances such as a billion light years or more, short distances such as a few meters, different universes, or different points in time.\n\nFor a simplified notion of a wormhole, space can be visualized as a two-dimensional (2D) surface. In this case, a wormhole would appear as a hole in that surface, lead into a 3D tube (the inside surface of a cylinder), then re-emerge at another location on the 2D surface with a hole similar to the entrance. An actual wormhole would be analogous to this, but with the spatial dimensions raised by one. For example, instead of circular holes on a 2D plane, the entry and exit points could be visualized as spheres in 3D space.\n\nAnother way to imagine wormholes is to take a sheet of paper and draw two somewhat distant points on one side of the paper. The sheet of paper represents a plane in the spacetime continuum, and the two points represent a distance to be traveled, however theoretically a wormhole could connect these two points by folding that plane so the points are touching. In this way it would be much easier to traverse the distance since the two points are now touching.\nIn 1928, Hermann Weyl proposed a wormhole hypothesis of matter in connection with mass analysis of electromagnetic field energy; however, he did not use the term \"wormhole\" (he spoke of \"one-dimensional tubes\" instead).\n\nAmerican theoretical physicist John Archibald Wheeler (inspired by Weyl's work) coined the term \"wormhole\" in a 1957 paper co-authored by Charles Misner:\n\nWormholes have been defined both \"geometrically\" and \"topologically\". From a topological point of view, an intra-universe wormhole (a wormhole between two points in the same universe) is a compact region of spacetime whose boundary is topologically trivial, but whose interior is not simply connected. Formalizing this idea leads to definitions such as the following, taken from Matt Visser's \"Lorentzian Wormholes\" (1996).\n\nGeometrically, wormholes can be described as regions of spacetime that constrain the incremental deformation of closed surfaces. For example, in Enrico Rodrigo's \"The Physics of Stargates, \"a wormhole is defined informally as: \n\nThe equations of the theory of general relativity have valid solutions that contain wormholes. The first type of wormhole solution discovered was the \"Schwarzschild wormhole\", which would be present in the Schwarzschild metric describing an \"eternal black hole\", but it was found that it would collapse too quickly for anything to cross from one end to the other. Wormholes that could be crossed in both directions, known as traversable wormholes, would only be possible if exotic matter with negative energy density could be used to stabilize them.\n\nSchwarzschild wormholes, also known as \"Einstein–Rosen bridges\" (named after Albert Einstein and Nathan Rosen), are connections between areas of space that can be modeled as vacuum solutions to the Einstein field equations, and that are now understood to be intrinsic parts of the maximally extended version of the Schwarzschild metric describing an eternal black hole with no charge and no rotation. Here, \"maximally extended\" refers to the idea that the spacetime should not have any \"edges\": it should be possible to continue this path arbitrarily far into the particle's future or past for any possible trajectory of a free-falling particle (following a geodesic in the spacetime).\n\nIn order to satisfy this requirement, it turns out that in addition to the black hole interior region that particles enter when they fall through the event horizon from the outside, there must be a separate white hole interior region that allows us to extrapolate the trajectories of particles that an outside observer sees rising up \"away\" from the event horizon. And just as there are two separate interior regions of the maximally extended spacetime, there are also two separate exterior regions, sometimes called two different \"universes\", with the second universe allowing us to extrapolate some possible particle trajectories in the two interior regions. This means that the interior black hole region can contain a mix of particles that fell in from either universe (and thus an observer who fell in from one universe might be able to see light that fell in from the other one), and likewise particles from the interior white hole region can escape into either universe. All four regions can be seen in a spacetime diagram that uses Kruskal–Szekeres coordinates.\n\nIn this spacetime, it is possible to come up with coordinate systems such that if a hypersurface of constant time (a set of points that all have the same time coordinate, such that every point on the surface has a space-like separation, giving what is called a 'space-like surface') is picked and an \"embedding diagram\" drawn depicting the curvature of space at that time, the embedding diagram will look like a tube connecting the two exterior regions, known as an \"Einstein–Rosen bridge\". Note that the Schwarzschild metric describes an idealized black hole that exists eternally from the perspective of external observers; a more realistic black hole that forms at some particular time from a collapsing star would require a different metric. When the infalling stellar matter is added to a diagram of a black hole's history, it removes the part of the diagram corresponding to the white hole interior region, along with the part of the diagram corresponding to the other universe.\n\nThe Einstein–Rosen bridge was discovered by Ludwig Flamm in 1916, a few months after Schwarzschild published his solution, and was rediscovered by Albert Einstein and his colleague Nathan Rosen, who published their result in 1935. However, in 1962, John Archibald Wheeler and Robert W. Fuller published a paper showing that this type of wormhole is unstable if it connects two parts of the same universe, and that it will pinch off too quickly for light (or any particle moving slower than light) that falls in from one exterior region to make it to the other exterior region.\n\nAccording to general relativity, the gravitational collapse of a sufficiently compact mass forms a singular Schwarzschild black hole. In the Einstein–Cartan–Sciama–Kibble theory of gravity, however, it forms a regular Einstein–Rosen bridge. This theory extends general relativity by removing a constraint of the symmetry of the affine connection and regarding its antisymmetric part, the torsion tensor, as a dynamical variable. Torsion naturally accounts for the quantum-mechanical, intrinsic angular momentum (spin) of matter. The minimal coupling between torsion and Dirac spinors generates a repulsive spin–spin interaction that is significant in fermionic matter at extremely high densities. Such an interaction prevents the formation of a gravitational singularity. Instead, the collapsing matter reaches an enormous but finite density and rebounds, forming the other side of the bridge.\n\nAlthough Schwarzschild wormholes are not traversable in both directions, their existence inspired Kip Thorne to imagine traversable wormholes created by holding the \"throat\" of a Schwarzschild wormhole open with exotic matter (material that has negative mass/energy).\n\nOther non-traversable wormholes include \"Lorentzian wormholes\" (first proposed by John Archibald Wheeler in 1957), wormholes creating a spacetime foam in a general relativistic spacetime manifold depicted by a Lorentzian manifold, and \"Euclidean wormholes\" (named after Euclidean manifold, a structure of Riemannian manifold).\n\nThis Casimir effect shows that quantum field theory allows the energy density in certain regions of space to be negative relative to the ordinary matter vacuum energy, and it has been shown theoretically that quantum field theory allows states where energy can be \"arbitrarily\" negative at a given point. Many physicists, such as Stephen Hawking, Kip Thorne, and others, therefore argue that such effects might make it possible to stabilize a traversable wormhole. Physicists have not found any natural process that would be predicted to form a wormhole naturally in the context of general relativity, although the quantum foam hypothesis is sometimes used to suggest that tiny wormholes might appear and disappear spontaneously at the Planck scale, and stable versions of such wormholes have been suggested as dark matter candidates. It has also been proposed that, if a tiny wormhole held open by a negative mass cosmic string had appeared around the time of the Big Bang, it could have been inflated to macroscopic size by cosmic inflation.\n\nLorentzian traversable wormholes would allow travel in both directions from one part of the universe to another part of that same universe very quickly or would allow travel from one universe to another. The possibility of traversable wormholes in general relativity was first demonstrated in a 1973 paper by Homer Ellis\nand independently in a 1973 paper by K. A. Bronnikov.\nEllis thoroughly analyzed the topology and the geodesics of the Ellis drainhole, showing it to be geodesically complete, horizonless, singularity-free, and fully traversable in both directions. The drainhole is a solution manifold of Einstein's field equations for a vacuum space-time, modified by inclusion of a scalar field minimally coupled to the Ricci tensor with antiorthodox polarity (negative instead of positive). (Ellis specifically rejected referring to the scalar field as 'exotic' because of the antiorthodox coupling, finding arguments for doing so unpersuasive.) The solution depends on two parameters: formula_1, which fixes the strength of its gravitational field, and formula_2, which determines the curvature of its spatial cross sections. When formula_1 is set equal to 0, the drainhole's gravitational field vanishes. What is left is the Ellis wormhole, a nongravitating, purely geometric, traversable wormhole.\nKip Thorne and his graduate student Mike Morris, unaware of the 1973 papers by Ellis and Bronnikov, manufactured, and in 1988 published, a duplicate of the Ellis wormhole for use as a tool for teaching general relativity. For this reason, the type of traversable wormhole they proposed, held open by a spherical shell of exotic matter, was from 1988 to 2015 exclusively referred to in the literature as a \"Morris–Thorne wormhole\". Later, other types of traversable wormholes were discovered as allowable solutions to the equations of general relativity, including a variety analyzed in a 1989 paper by Matt Visser, in which a path through the wormhole can be made where the traversing path does not pass through a region of exotic matter. However, in the pure Gauss–Bonnet gravity (a modification to general relativity involving extra spatial dimensions which is sometimes studied in the context of brane cosmology) exotic matter is not needed in order for wormholes to exist—they can exist even with no matter. A type held open by negative mass cosmic strings was put forth by Visser in collaboration with Cramer \"et al.\", in which it was proposed that such wormholes could have been naturally created in the early universe.\n\nWormholes connect two points in spacetime, which means that they would in principle allow travel in time, as well as in space. In 1988, Morris, Thorne and Yurtsever worked out explicitly how to convert a wormhole traversing space into one traversing time by accelerating one of its two mouths. However, according to general relativity, it would not be possible to use a wormhole to travel back to a time earlier than when the wormhole was first converted into a time 'machine'. Until this time it could not have been noticed or have been used.\n\nTo see why exotic matter is required, consider an incoming light front traveling along geodesics, which then crosses the wormhole and re-expands on the other side. The expansion goes from negative to positive. As the wormhole neck is of finite size, we would not expect caustics to develop, at least within the vicinity of the neck. According to the optical Raychaudhuri's theorem, this requires a violation of the averaged null energy condition. Quantum effects such as the Casimir effect cannot violate the averaged null energy condition in any neighborhood of space with zero curvature, but calculations in semiclassical gravity suggest that quantum effects may be able to violate this condition in curved spacetime. Although it was hoped recently that quantum effects could not violate an achronal version of the averaged null energy condition, violations have nevertheless been found, so it remains an open possibility that quantum effects might be used to support a wormhole.\n\nIn some hypotheses where general relativity is modified, it is possible to have a wormhole that does not collapse without having to resort to exotic matter. For example, this is possible with R^2 gravity, a form of f(R) gravity.\n\nThe impossibility of faster-than-light relative speed only applies locally. Wormholes might allow effective superluminal (faster-than-light) travel by ensuring that the speed of light is not exceeded locally at any time. While traveling through a wormhole, subluminal (slower-than-light) speeds are used. If two points are connected by a wormhole whose length is shorter than the distance between them \"outside\" the wormhole, the time taken to traverse it could be less than the time it would take a light beam to make the journey if it took a path through the space \"outside\" the wormhole. However, a light beam traveling through the same wormhole would of course beat the traveler.\n\nIf traversable wormholes exist, they could allow time travel. A proposed time-travel machine using a traversable wormhole would hypothetically work in the following way: One end of the wormhole is accelerated to some significant fraction of the speed of light, perhaps with some advanced propulsion system, and then brought back to the point of origin. Alternatively, another way is to take one entrance of the wormhole and move it to within the gravitational field of an object that has higher gravity than the other entrance, and then return it to a position near the other entrance. For both of these methods, time dilation causes the end of the wormhole that has been moved to have aged less, or become \"younger\", than the stationary end as seen by an external observer; however, time connects differently \"through\" the wormhole than \"outside\" it, so that synchronized clocks at either end of the wormhole will always remain synchronized as seen by an observer passing through the wormhole, no matter how the two ends move around. This means that an observer entering the \"younger\" end would exit the \"older\" end at a time when it was the same age as the \"younger\" end, effectively going back in time as seen by an observer from the outside. One significant limitation of such a time machine is that it is only possible to go as far back in time as the initial creation of the machine; It is more of a path through time rather than it is a device that itself moves through time, and it would not allow the technology itself to be moved backward in time.\n\nAccording to current theories on the nature of wormholes, construction of a traversable wormhole would require the existence of a substance with negative energy, often referred to as \"exotic matter\". More technically, the wormhole spacetime requires a distribution of energy that violates various energy conditions, such as the null energy condition along with the weak, strong, and dominant energy conditions. However, it is known that quantum effects can lead to small measurable violations of the null energy condition, and many physicists believe that the required negative energy may actually be possible due to the Casimir effect in quantum physics. Although early calculations suggested a very large amount of negative energy would be required, later calculations showed that the amount of negative energy can be made arbitrarily small.\n\nIn 1993, Matt Visser argued that the two mouths of a wormhole with such an induced clock difference could not be brought together without inducing quantum field and gravitational effects that would either make the wormhole collapse or the two mouths repel each other, or otherwise prevent information from passing through the wormhole. Because of this, the two mouths could not be brought close enough for causality violation to take place. However, in a 1997 paper, Visser hypothesized that a complex \"Roman ring\" (named after Tom Roman) configuration of an N number of wormholes arranged in a symmetric polygon could still act as a time machine, although he concludes that this is more likely a flaw in classical quantum gravity theory rather than proof that causality violation is possible.\n\nA possible resolution to the paradoxes resulting from wormhole-enabled time travel rests on the many-worlds interpretation of quantum mechanics.\n\nIn 1991 David Deutsch showed that quantum theory is fully consistent (in the sense that the so-called density matrix can be made free of discontinuities) in spacetimes with closed timelike curves. However, later it was shown that such model of closed timelike curve can have internal inconsistencies as it will lead to strange phenomena like distinguishing non-orthogonal quantum states and distinguishing proper and improper mixture. Accordingly, the destructive positive feedback loop of virtual particles circulating through a wormhole time machine, a result indicated by semi-classical calculations, is averted. A particle returning from the future does not return to its universe of origination but to a parallel universe. This suggests that a wormhole time machine with an exceedingly short time jump is a theoretical bridge between contemporaneous parallel universes.\n\nBecause a wormhole time-machine introduces a type of nonlinearity into quantum theory, this sort of communication between parallel universes is consistent with Joseph Polchinski's proposal of an Everett phone (named after Hugh Everett) in Steven Weinberg's formulation of nonlinear quantum mechanics.\n\nThe possibility of communication between parallel universes has been dubbed interuniversal travel.\n\nTheories of \"wormhole metrics\" describe the spacetime geometry of a wormhole and serve as theoretical models for time travel. An example of a (traversable) wormhole metric is the following:\n\nfirst presented by Ellis (see Ellis wormhole) as a special case of the Ellis drainhole.\n\nOne type of non-traversable wormhole metric is the Schwarzschild solution (see the first diagram):\n\nThe original Einstein–Rosen bridge was described in an article published in July 1935.\n\nFor the Schwarzschild spherically symmetric static solution \n\nIf one replaces formula_9 with formula_10 according to\nformula_11\n\nFor the combined field, gravity and electricity, Einstein and Rosen derived the following Schwarzschild static spherically symmetric solution\n\nThe field equations without denominators in the case when formula_1 = 0 can be written\n\nIn order to eliminate singularities, if one replaces formula_9 by formula_10 according to the equation:\n\nand with formula_1 = 0 one obtains\n\nWormholes are a common element in science fiction because they allow interstellar, intergalactic, and sometimes even interuniversal travel within human lifetime scales. In fiction, wormholes have also served as a method for time travel.\n\n\n"}
{"id": "13873874", "url": "https://en.wikipedia.org/wiki?curid=13873874", "title": "Zeta potential titration", "text": "Zeta potential titration\n\nZeta potential titration is a titration of heterogeneous systems, for example colloids and emulsions. Solids in such systems have very high surface area. This type of titration is used to study the zeta potential of these surfaces under different conditions.\n\nThe iso-electric point is one such property. The iso-electric point is the pH value at which the zeta potential is approximately zero. At a pH near the iso-electric point (± 2 pH units), colloids are usually unstable; the particles tend to coagulate or flocculate. Such titrations use acids or bases as titration reagents. Tables of iso-electric points for different materials are available. The attached figure illustrates results of such titrations for concentrated dispersions of alumina (4% v/v) and rutile (7% v/v). It is seen that iso-electric point of alumina is around pH 9.3, whereas for rutile it is around pH 4. Alumina is unstable in the pH range from 7 to 11. Rutile is unstable in the pH range from 2 to 6.\n\nAnother purpose of this titration is determination of the optimum dose of surfactant for achieving stabilization or flocculation of a heterogeneous system. Examples can be found in the book by Dukhin and Goetz.\n\nIn a zeta-potential titration, the Zeta potential is the indicator. Measurement of the zeta potential can be performed using microelectrophoresis, or electrophoretic light scattering, or electroacoustic phenomena. The last method makes possible to perform titrations in concentrated systems, with no dilution. The book by Dukhin and Goetz provides a detailed description of such titrations.\n\n"}
