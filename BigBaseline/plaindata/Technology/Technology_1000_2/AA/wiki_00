{"id": "505093", "url": "https://en.wikipedia.org/wiki?curid=505093", "title": "Actor–network theory", "text": "Actor–network theory\n\nActor–network theory (ANT) is a theoretical and methodological approach to social theory where everything in the social and natural worlds exists in constantly shifting networks of relationship. It posits that nothing exists outside those relationships. All the factors involved in a social situation are on the same level, and thus there are no external social forces beyond what and how the network participants interact at present. Thus, objects, ideas, processes, and any other relevant factors are seen as just as important in creating social situations as humans. ANT holds that social forces do not exist in themselves, and therefore cannot be used to explain social phenomena. Instead, strictly empirical analysis should be undertaken to \"describe\" rather than \"explain\" social activity. Only after this can one introduce the concept of social forces, and only as an abstract theoretical concept, not something which genuinely exists in the world. The fundamental aim of ANT is to explore how networks are built or assembled and maintained to achieve a specific objective. Although it is best known for its controversial insistence on the capacity of nonhumans to act or participate in systems or networks or both, ANT is also associated with forceful critiques of conventional and critical sociology. Developed by science and technology studies (STS) scholars Michel Callon and Bruno Latour, the sociologist John Law, and others, it can more technically be described as a \"material-semiotic\" method. This means that it maps relations that are simultaneously material (between things) and semiotic (between concepts). It assumes that many relations are both material and semiotic.\n\nBroadly speaking, ANT is a constructivist approach in that it avoids essentialist explanations of events or innovations (i.e. ANT explains a successful theory by understanding the combinations and interactions of elements that make it successful, rather than saying it is true and the others are false). Likewise, it is not a cohesive theory in itself. Rather, ANT functions as a strategy that assists people in being sensitive to terms and the often unexplored assumptions underlying them. It is distinguished from many other STS and sociological network theories for its distinct material-semiotic approach.\n\nANT was first developed at the Centre de Sociologie de l'Innovation (CSI) of the École nationale supérieure des mines de Paris in the early 1980s by staff (Michel Callon and Bruno Latour) and visitors (including John Law). The 1984 book co-authored by John Law and fellow-sociologist Peter Lodge (\"Science for Social Scientists\"; London: Macmillan Press Ltd.) is a good example of early explorations of how the growth and structure of knowledge could be analyzed and interpreted through the interactions of actors and networks. Initially created in an attempt to understand processes of innovation and knowledge-creation in science and technology, the approach drew on existing work in STS, on studies of large technological systems, and on a range of French intellectual resources including the semiotics of Algirdas Julien Greimas, the writing of philosopher Michel Serres, and the Annales School of history.\n\nANT appears to reflect many of the preoccupations of French post-structuralism, and in particular a concern with non-foundational and multiple material-semiotic relations. At the same time, it was much more firmly embedded in English-language academic traditions than most post-structuralist-influenced approaches. Its grounding in (predominantly English) science and technology studies was reflected in an intense commitment to the development of theory through qualitative empirical case-studies. Its links with largely US-originated work on large technical systems were reflected in its willingness to analyse large scale technological developments in an even-handed manner to include political, organizational, legal, technical and scientific factors.\n\nMany of the characteristic ANT tools (including the notions of translation, generalized symmetry and the “heterogeneous network”), together with a scientometric tool for mapping innovations in science and technology (“co-word analysis”) were initially developed during the 1980s, predominantly in and around the CSI. The “state of the art” of ANT in the late 1980s is well-described in Latour’s 1987 text, \"Science in Action\".\n\nFrom about 1990 onwards, ANT started to become popular as a tool for analysis in a range of fields beyond STS. It was picked up and developed by authors in parts of organizational analysis, informatics, health studies, geography, sociology, anthropology, feminist studies, technical communication and economics.\n\n, ANT is a widespread, if controversial, range of material-semiotic approaches for the analysis of heterogeneous relations. In part because of its popularity, it is interpreted and used in a wide range of alternative and sometimes incompatible ways. There is no orthodoxy in current ANT, and different authors use the approach in substantially different ways. Some authors talk of “after-ANT” to refer to “successor projects” blending together different problem-focuses with those of ANT.\n\nAlthough it is called a “theory”, ANT does not usually explain “why” or \"how\" a network takes the form that it does. Rather, ANT is a way of thoroughly exploring the relational ties within a network (which can be a multitude of different things). As Latour notes, \"explanation does not follow from description; it is description taken that much further.\" It is not, in other words, a theory \"of\" anything, but rather a method, or a \"how-to book\" as Latour puts it.\n\nThe approach is related to other versions of material-semiotics (notably the work of philosophers Gilles Deleuze, Michel Foucault, and feminist scholar Donna Haraway). It can also be seen as a way of being faithful to the insights of ethnomethodology and its detailed descriptions of how common activities, habits and procedures sustain themselves. Similarities between ANT and symbolic interactionist approaches such as the newer forms of grounded theory like situational analysis, exist, although Latour objects to such a comparison.\n\nAlthough ANT is mostly associated with studies of science and technology and with the sociology of science, it has been making steady progress in other fields of sociology as well. ANT is adamantly empirical, and as such yields useful insights and tools for sociological inquiry in general. ANT has been deployed in studies of identity and subjectivity, urban transportation systems, and passion and addiction. It also makes steady progress in political and historical sociology.\n\nAs the term implies, the actor-network is the central concept in ANT. The term \"network\" is somewhat problematic in that as Latour notes, it has a number of unwanted connotations. Firstly, it implies that what is described takes the shape of a network, which is not necessarily the case. Secondly, it implies \"transportation without deformation,\" which, in ANT, is not possible since any actor-network involves a vast number of translations. Latour, however, still contends that network is a fitting term to use, because \"it has no a priori order relation; it is not tied to the axiological myth of a top and of a bottom of society; it makes absolutely no assumption whether a specific locus is macro- or micro- and does not modify the tools to study the element 'a' or the element 'b'.\" This use of the term \"network\" is very similar to Deleuze and Guattari's rhizomes; Latour even remarks tongue in cheek that he would have no objection to renaming ANT \"actant-rhizome ontology\" if it only had sounded better, which hints at Latour's uneasiness with the word \"theory\".\n\nActor–network theory tries to explain how material–semiotic networks come together to act as a whole; the clusters of actors involved in creating meaning are both material and semiotic. As a part of this it may look at explicit strategies for relating different elements together into a network so that they form an apparently coherent whole. These networks are potentially transient, existing in a constant making and re-making. This means that relations need to be repeatedly “performed” or the network will dissolve. They also assume that networks of relations are not intrinsically coherent, and may indeed contain conflicts. Social relations, in other words, are only ever in process, and must be performed continuously.\n\nActants denote human and non-human actors, and in a network take the shape that they do by virtue of their relations with one another. It assumes that nothing lies outside the network of relations, and as noted above, suggests that there is no difference in the ability of technology, humans, animals, or other non-humans to act (and that there are only enacted alliances). As soon as an actor engages with an actor-network it too is caught up in the web of relations, and becomes part of the entelechy.\n\nIf taken to its logical conclusion, then, nearly any actor can be considered merely a sum of other, smaller actors. A car is an example of a complex system. It contains many electronic and mechanical components, all of which are essentially hidden from view to the driver, who simply deals with the car as a single object. This effect is known as \"punctualisation\", and is similar to the idea of encapsulation in object-oriented programming.\n\nWhen an actor network breaks down, the punctualisation effect tends to cease as well. In the automobile example above, a non-working engine would cause the driver to become aware of the car as a collection of parts rather than just a vehicle capable of transporting them from place to place. This can also occur when elements of a network act contrarily to the network as a whole. In his book \"Pandora’s Hope\", Latour likens depunctualization to the opening of a black box. When closed, the box is perceived simply as a box, although when it is opened all elements inside it becomes visible.\n\nANT is often associated with the equal treatment of human and non-human actors. ANT assumes that all entities in a network can and should be described in the same terms. This is called the principle of \"generalized symmetry\". The rationale for this is that differences between them are generated in the network of relations, and should not be presupposed.\n\nThe distinction between intermediaries and mediators is key to ANT sociology. Intermediaries are entities which make no difference (to some interesting state of affairs which we are studying) and so can be ignored. They transport the force of some other entity more or less without transformation and so are fairly uninteresting. Mediators, on the other hand, are entities which multiply difference and so should be the object of study. Their outputs cannot be predicted by their inputs. From an ANT point of view, sociology has tended to treat too much of the world as intermediaries.\n\nFor instance, a sociologist might take silk and nylon as intermediaries, holding that the former “means”, “reflects”, or “symbolises” the upper classes and the latter the lower classes. In such a view, the real-world silk–nylon difference is irrelevant — presumably, many other material differences could also, and do also, transport this class distinction. But taken as mediators, these fabrics would have to be engaged with by the analyst in their specificity: the internal real-world complexities of silk and nylon suddenly appear relevant, and are seen as actively constructing the ideological class distinction which they once merely reflected.\n\nFor the committed ANT analyst, social things like class distinctions in taste, as in the silk and nylon example, are examples of social construction, but also groups and power must constantly be constructed or performed anew through complex engagements with complex mediators. There is no stand-alone social repertoire lying in the background to be reflected off, expressed through, or substantiated in, interactions (as in an intermediary conception). That being said, it doesn't make sense to say there are social aspects of things. Things themselves are a constitutive part of the social reality they are able to enact.\n\nCentral to ANT is the concept of translation which is sometimes referred to as sociology of translation, in which innovators attempt to create a \"forum,\" a central network in which all the actors agree that the network is worth building and defending. In his widely debated 1986 study of how marine biologists try to restock the St Brieuc Bay in order to produce more scallops, Michel Callon has defined 4 moments of translation: problematization, interessement, enrollment and mobilisation of allies. Also important to the notion is the role of network objects in helping to smooth out the translation process by creating equivalencies between what would otherwise be very challenging people, organizations or conditions to mesh together. Bruno Latour spoke about this particular task of objects in his work \"Reassembling the Social\" (2005).\n\nIn the above examples, “social order” and “functioning car” come into being through the successful interactions of their respective actor-networks, and actor-network theory refers to these creations as \"tokens\" or \"quasi-objects\" which are passed between actors within the network.\n\nAs the token is increasingly transmitted or passed through the network, it becomes increasingly punctualized and also increasingly reified. When the token is decreasingly transmitted, or when an actor fails to transmit the token (e.g., the oil pump breaks), punctualization and reification are decreased as well.\n\nRecently, there has been a movement to introduce Actor Network Theory as an analytical tool to a range of applied disciplines outside of sociology, including nursing, public health, business (Klein and Czarniawska, 2005), library and information science (Beagle, 2001.); urban studies (Farias and Bender, 2010), and community, urban, and regional planning (Beauregard, 2012; Beauregard and Lieto, 2015; Rydin, 2012; Rydin and Tate, 2016, Tate, 2013.).\n\nActor-Network Theory has become increasingly prominent within the discipline of International Relations and Political Science.\n\nTheoretically, scholars within IR have employed ANT in order to disrupt traditional world political binaries (civilised/barbarian, democratic/autocratic, etc.), consider the implications of a posthuman understanding of IR, explore the infrastructures of world politics, and consider the effects of technological agency.\n\nEmpirically, IR scholars have drawn on insights from ANT in order to study phenomena including political violences like the use of torture and drones, piracy and maritime governance, and garbage.\n\nThe Actor-Network Theory can also be applied to design, using a perspective that is not simply limited to an analysis of an object's structure. From the ANT viewpoint, design is seen as a series of features that account for a social, psychological, and economical world. ANT argues that objects are designed to shape human action and mold or influence decisions. In this way, the objects' design serves to mediate human relationships and can even impact our morality, ethics, and politics.\n\nActor–network theory insists on the capacity of nonhumans to be actors or participants in networks and systems. Critics including figures such as Langdon Winner maintain that such properties as \"intentionality\" fundamentally distinguish humans from animals or from “things” (see Activity Theory). ANT scholars respond with the following arguments:\n\nANT has been criticized as amoral. Wiebe Bijker has responded to this criticism by stating that the amorality of ANT is not a necessity. Moral and political positions are possible, but one must first describe the network before taking up such positions. This position has been further explored by Stuart Shapiro who contrasts ANT with the history of ecology, and argues that research decisions are moral rather than methodological, but this moral dimension has been sidelined.\n\nWhittle and Spicer note that \"ANT has also sought to move beyond deterministic models that trace organizational phenomena back to powerful individuals, social structures, hegemonic discourses or technological effects. Rather, ANT prefers to seek out complex patterns of causality rooted in connections between actors.\" They argue that ANT's ontological realism makes it \"less well equipped for pursuing a critical account of organizations—that is, one which recognises the unfolding nature of reality, considers the limits of knowledge and seeks to challenge structures of domination.\" This implies that ANT does not account for pre-existing structures, such as power, but rather sees these structures as emerging from the actions of actors within the network and their ability to align in pursuit of their interests. Accordingly, ANT can be seen as an attempt to re-introduce Whig history into science and technology studies; like the myth of the heroic inventor, ANT can be seen as an attempt to explain successful innovators by saying only that they were successful. Likewise, for organization studies, Whittle and Spicer assert that ANT is, \"ill suited to the task of developing political alternatives to the imaginaries of market managerialism.\"\n\nKey early criticism came from other members of the STS community, in particular the \"Epistemological Chicken\" debate between Collins and Yearley with responses from Latour and Callon as well as Woolgar. Collins and Yearley accused ANTs approach of collapsing into an endless relativist regress. Some critics have argued that research based on ANT perspectives remains entirely descriptive and fails to provide explanations for social processes. ANT—like comparable social scientific methods—requires judgement calls from the researcher as to which actors are important within a network and which are not. Critics argue that the importance of particular actors cannot be determined in the absence of “out-of-network” criteria. Similarly, others argue that actor-networks risk degenerating into endless chains of association (six degrees of separation—we are all networked to one another). Other research perspectives such as social constructionism, social shaping of technology, social network theory, normalization process theory, Diffusion of Innovations theory are held to be important alternatives to ANT approaches.\n\nIn a workshop called “On Recalling ANT”, Bruno Latour stated that there are four things wrong with actor-network theory: “actor”, “network”, “theory” and the hyphen. In a later book however (\"Reassembling the Social: An Introduction to Actor–Network–Theory\"), Latour reversed himself, accepting the wide use of the term, “\"including\" the hyphen” (Latour 2005:9). He also remarked how he had been helpfully reminded that the ANT acronym “was perfectly fit for a blind, myopic, workaholic, trail-sniffing, and collective traveler” (the ant, Latour 2005:9) — qualitative hallmarks of actor-network epistemology.\n\n\n\n"}
{"id": "28634992", "url": "https://en.wikipedia.org/wiki?curid=28634992", "title": "Augustine's laws", "text": "Augustine's laws\n\nAugustine's laws were a series of tongue in cheek aphorisms put forth by Norman Ralph Augustine, an American aerospace businessman who served as Under Secretary of the Army from 1975 to 1977. In 1984 he published his laws. The book and several of the laws were the topic of an article in Sound and Vibration magazine in March 2012.\n\nLaw Number I: The best way to make a silk purse from a sow's ear is to begin with a silk sow. The same is true of money.\n\nLaw Number II: If today were half as good as tomorrow is supposed to be, it would probably be twice as good as yesterday was.\n\nLaw Number III: There are no lazy veteran lion hunters.\n\nLaw Number IV: If you can afford to advertise, you don't need to.\n\nLaw Number V: One-tenth of the participants produce over one-third of the output. Increasing the number of participants merely reduces the average output.\n\nLaw Number VI: A hungry dog hunts best. A hungrier dog hunts even better.\n\nLaw Number VII: Decreased business base increases overhead. So does increased business base.\n\nLaw Number VIII: The most unsuccessful four years in the education of a cost-estimator is fifth grade arithmetic.\n\nLaw Number IX: Acronyms and abbreviations should be used to the maximum extent possible to make trivial ideas profound...Q.E.D.\n\nLaw Number X: Bulls do not win bullfights; people do. People do not win people fights; lawyers do.\n\nLaw Number XI: If the Earth could be made to rotate twice as fast, managers would get twice as much done. If the Earth could be made to rotate twenty times as fast, everyone else would get twice as much done since all the managers would fly off.\n\nLaw Number XII: It costs a lot to build bad products.\n\nLaw Number XIII: There are many highly successful businesses in the United States. There are also many highly paid executives. The policy is not to intermingle the two.\n\nLaw Number XIV: After the year 2015, there will be no airplane crashes. There will be no takeoffs either, because electronics will occupy 100 percent of every airplane's weight.\n\nLaw Number XV: The last 10 percent of performance generates one-third of the cost and two-thirds of the problems.\n\nLaw Number XVI: In the year 2054, the entire defense budget will purchase just one aircraft. This aircraft will have to be shared by the Air Force and Navy 3-1/2 days each per week except for leap year, when it will be made available to the Marines for the extra day.\n\nLaw Number XVII: Software is like entropy. It is difficult to grasp, weighs nothing, and obeys the Second Law of Thermodynamics; i.e., it always increases.\n\nLaw Number XVIII: It is very expensive to achieve high unreliability. It is not uncommon to increase the cost of an item by a factor of ten for each factor of ten degradation accomplished.\n\nLaw Number XIX: Although most products will soon be too costly to purchase, there will be a thriving market in the sale of books on how to fix them.\n\nLaw Number XX: In any given year, Congress will appropriate the amount of funding approved the prior year plus three-fourths of whatever change the administration requests, minus 4-percent tax.\n\nLaw Number XXI: It's easy to get a loan unless you need it.\n\nLaw Number XXII: If stock market experts were so expert, they would be buying stock, not selling advice.\n\nLaw Number XXIII: Any task can be completed in only one-third more time than is currently estimated.\n\nLaw Number XXIV: The only thing more costly than stretching the schedule of an established project is accelerating it, which is itself the most costly action known to man.\n\nLaw Number XXV: A revised schedule is to business what a new season is to an athlete or a new canvas to an artist.\n\nLaw Number XXVI: If a sufficient number of management layers are superimposed on each other, it can be assured that disaster is not left to chance.\n\nLaw Number XXVII: Rank does not intimidate hardware. Neither does the lack of rank.\n\nLaw Number XXVIII: It is better to be the reorganizer than the reorganizee.\n\nLaw Number XXIX: Executives who do not produce successful results hold on to their jobs only about five years. Those who produce effective results hang on about half a decade.\n\nLaw Number XXX: By the time the people asking the questions are ready for the answers, the people doing the work have lost track of the questions.\n\nLaw Number XXXI: The optimum committee has no members.\n\nLaw Number XXXII: Hiring consultants to conduct studies can be an excellent means of turning problems into gold, your problems into their gold.\n\nLaw Number XXXIII: Fools rush in where incumbents fear to tread.\n\nLaw Number XXXIV: The process of competitively selecting contractors to perform work is based on a system of rewards and penalties, all distributed randomly.\n\nLaw Number XXXV: The weaker the data available upon which to base one's conclusion, the greater the precision which should be quoted in order to give the data authenticity.\n\nLaw Number XXXVI: The thickness of the proposal required to win a multimillion dollar contract is about one millimeter per million dollars. If all the proposals conforming to this standard were piled on top of each other at the bottom of the Grand Canyon it would probably be a good idea.\n\nLaw Number XXXVII: Ninety percent of the time things will turn out worse than you expect. The other 10 percent of the time you had no right to expect so much.\n\nLaw Number XXXVIII: The early bird gets the worm. The early worm...gets eaten.\n\nLaw Number XXXIX: Never promise to complete any project within six months of the end of the year, in either direction.\n\nLaw Number XL: Most projects start out slowly, and then sort of taper off.\n\nLaw Number XLI: The more one produces, the less one gets.\n\nLaw Number XLII: Simple systems are not feasible because they require infinite testing.\n\nLaw Number XLIII: Hardware works best when it matters the least.\n\nLaw Number XLIV: Aircraft flight in the 21st century will always be in a westerly direction, preferably supersonic, crossing time zones to provide the additional hours needed to fix the broken electronics.\n\nLaw Number XLV: One should expect that the expected can be prevented, but the unexpected should have been expected.\n\nLaw Number XLVI: A billion saved is a billion earned.\n\nLaw Number XLVII: Two-thirds of the Earth's surface is covered with water. The other third is covered with auditors from headquarters.\n\nLaw Number XLVIII: The more time you spend talking about what you have been doing, the less time you have to spend doing what you have been talking about. Eventually, you spend more and more time talking about less and less until finally you spend all your time talking about nothing.\n\nLaw Number XLIX: Regulations grow at the same rate as weeds.\n\nLaw Number L: The average regulation has a life span one-fifth as long as a chimpanzee's and one-tenth as long as a human's, but four times as long as the official's who created it.\n\nLaw Number LI: By the time of the United States Tricentennial, there will be more government workers than there are workers.\n\nLaw Number LII: People working in the private sector should try to save money. There remains the possibility that it may someday be valuable again.\n\nHis most cited law is number 16, which shows that defense budgets grow linearly but the unit cost of a new military aircraft grows exponentially:\nIn the year 2054, the entire defense budget will purchase just one tactical aircraft. This aircraft will have to be shared by the Air Force and Navy 3½ days each per week except for leap year, when it will be made available to the Marines for the extra day.\"\n\n"}
{"id": "59161710", "url": "https://en.wikipedia.org/wiki?curid=59161710", "title": "BIM Collaboration Format", "text": "BIM Collaboration Format\n\nThe BIM Collaboration Format (BCF) is a structured file format which allows issue tracking with a building information model. BCF is designed primarily for attaching information to collisions and errors connected with specific objects in a model, but can be used for general issue tracking in building construction projects. This allows developers of BIM-supporting software to design interfaces for collaboration between users who access the same model, especially with different software.\n\nThe format was developed by Tekla and Solibri and later adopted as a standard by buildingSMART. Notable software with native support for BCF are Solibri, Tekla with plugins which provide usability for other software.\n\n\n"}
{"id": "1431131", "url": "https://en.wikipedia.org/wiki?curid=1431131", "title": "Bureau of Oceans and International Environmental and Scientific Affairs", "text": "Bureau of Oceans and International Environmental and Scientific Affairs\n\nThe Bureau of Oceans and International Environmental and Scientific Affairs (OES) is a bureau within the United States Department of State. It coordinates a portfolio of issues related to the world's oceans, environment, science and technology, and health.\n\nThe Bureau is headed by the Assistant Secretary of State for Oceans and International Environmental and Scientific Affairs. As of April 2014, it is headed by acting Assistant Secretary Judith Garber. \n\nThe Oceans and Fisheries Directorate has two offices dedicated to international oceans issues. The Office of Marine Conservation focuses on international fisheries matters and related problems and the Office of Oceans and Polar Affairs has primary responsibility for international ocean law and policy, marine pollution, marine mammals, polar affairs, maritime boundaries, and marine science. It is headed by Deputy Assistant Secretary David A. Balton.\n\nThe Environment Directorate deals with environmental issues including environmental aspects of international trade and safeguarding hazardous materials requiring multilateral agreements within the Office of Environmental Quality and Transboundary Affairs. The Office of Conservation and Water develops U.S. foreign policy approaches to conserving and managing the world ecosystems and to transboundary water issues. The Environment Directorate is headed by Deputy Assistant Secretary Daniel Reifsnyder. \n\nThe Health, Space and Science Directorate includes the Office of International Health Affairs which works with U.S. Government agencies to facilitate policy-making regarding international bioterrorism, infectious disease, surveillance and response, environmental health, and health in post-conflict situations. The Office of Space and Advanced Technology handles issues arising from our exploration of space to assure global security regarding this new frontier, and the Office of Science & Technology (S&T) Cooperation promotes the interests of the U.S. science and technology communities in the international policy arena, negotiates framework and other S&T agreements, manages the Department's Embassy Science fellows program, and takes a leading role in representing U.S. science and technology in multilateral international organizations, such as UNESCO and other UN organizations, APEC, OECD and others. The Health, Space and Science Directorate is headed by acting Deputy Assistant Secretary Jonathan Margolis.\n\n"}
{"id": "22031337", "url": "https://en.wikipedia.org/wiki?curid=22031337", "title": "Component content management system", "text": "Component content management system\n\nA component content management system (CCMS) is a content management system that manages content at a granular level (component) rather than at the document level. Each component represents a single topic, concept or asset (for example an image, table, product description, a procedure). \n\nThe CCMS must be able to track \"not only versions of topics and graphics but relationships among topics, graphics, maps, publications, and deliverables.\" More often than not, the CCMS also contains the publishing engine to create the final outputs for print, web and e-readers.\n\nComponents can be as large as a chapter or as small as a definition or even a word. \nComponents in multiple content assemblies (content types) can be viewed as components or as traditional documents. \n\nAlthough modular documentation is not necessarily XML-based, it is usually the case. \nStandards include:\n\n\nChallenges for the technical writers include topic-based authoring, that is shifting from writing book-shaped, linear documentation to writing modular, structured and reusable content component.\n\nEach component is only stored one time in the content management system, providing a single, trusted source of content (referential). These components are then reused (rather than copied and pasted) within a document or across multiple documents. \nThis ensures that content is consistent across the entire documentation set.\n\nEach component has its own lifecycle (owner, version, approval, use) and can be tracked individually or as part of an assembly. \nComponent content management is typically used for multi-channel customer-facing content (marketing, usage, learning, support). \nThe solution can be a separate system or be a functionality of another content management system type (for example, enterprise content management or web content management).\n\nCompared to a \"simple\" management system, the CCMS tracks the files but also the collection of files. It tracks indirect and direct linking so that author can reuse safely content and check the applicability of changes.\n\nBenefits of managing contents at components level:\n\n\nBenefits of using a component content management system:\n\n\n\n"}
{"id": "29504901", "url": "https://en.wikipedia.org/wiki?curid=29504901", "title": "Computer says no", "text": "Computer says no\n\n\"Computer Says No\", or the \"Computer says no attitude\", is the popular name given to an attitude seen in some public-facing organisations where the default response to a customer’s request is to check with information stored on or generated by a computer, and then make decisions based on that, often in the face of common sense.\n\nThere may also be an element of deliberate unhelpfulness towards customers and service-users, whereby more \"could\" be done to reach a mutually satisfactory outcome, but is not.\n\nThe name gained popularity through the British sketch comedy \"Little Britain\".\n\nIn \"Little Britain\", \"Computer Says No\" is the catchphrase of the character Carol Beer (played by David Walliams), a bank worker and later holiday rep and hospital receptionist, who always responds to a customer's enquiry by typing it into her computer and responding with \"Computer Says No\" to even the most reasonable of requests. When asked to do something aside from asking the computer, she would shrug and remain obstinate in her unhelpfulness, and ultimately cough in the customer's face. The phrase was also used in the Australian soap opera \"Neighbours\" in 2006 as a reference to \"Little Britain\".\n\nThe \"Computer Says No\" attitude often comes from larger companies that rely on information stored electronically. When this information is not updated, it can often lead to refusals of financial products or incorrect information being sent out to customers. These situations can often be resolved by an employee updating the information; however, when this cannot be done easily, the \"Computer Says No\" attitude can be viewed as becoming prevalent when there is unhelpfulness as a result. This attitude can also occur when an employee fails to read human emotion in the customer and reacts according to his or her professional training or relies upon a script. This attitude also crops up when larger companies rely on computer credit scores and do not meet with a customer to discuss his or her individual needs, instead basing a decision upon information stored in computers. Some organisations attempt to offset this attitude by moving away from reliance on electronic information and using a human approach towards requests.\n\n\"Computer Says No\" happens in a more literal sense when computer systems employ filters that prevent messages being passed along, as when these messages are perceived to include obscenities. When information is not passed through to the person operating the computer, decisions may be made without seeing the whole picture.\n\n"}
{"id": "43689550", "url": "https://en.wikipedia.org/wiki?curid=43689550", "title": "Digital phobic", "text": "Digital phobic\n\nDigital phobic is an informal phrase used to describe a reluctance to become fully immersed in the digital age for being fearful of how it might negatively change or alter everyday life.\nThe fast-paced development of the digital world in the twenty-first century has contributed to the digital divide becoming a very real problem for a segment of the population for whom a lack of education of, interest in, or access to digital devices has left them excluded from the technological world and fearful of its growing omnipresence.\n\nDigital phobic is part of a growing dictionary of digital vocabulary exploring the social impact of the technological age. The phrase considers the fears associated with technological evolution and change, and acknowledges the possibility of exclusion as a result of a rising reliance on technology in day-to-day life.\n\nEveryday use of technology has increased dramatically since the turn of the century, significantly impacting both those embracing technological change as well as those reluctant to be a part of it.\n\nA sharp rise in technological innovations during the 21st century has been responsible for changing much of the way we work, socialize, and learn – all of which can be at the foundation of distrust in the technological age. Psychologists, academics and researchers have begun to consider the base of these fears and consider the social, cultural and environmental circumstances which might catalyze someone to becoming 'digital phobic'.\n\nTechnophobia is used to discuss a fear of advanced technology in a formal capacity and can stem from a number, and combination of, concerns. With the oncoming of the digital age, worries have broadened from the very earliest fears that technology would eradicate artisanship to concerns over data protection, financial security, identity theft, technical inability and invasion of privacy.\n\nThere is no exhaustible list of reasons cited for fearing the digital world and, whilst research into both the cause and consequence of developing a digital phobia remains in its infancy, the presence of digital phobia regardless contributes towards an increasingly comprehensive picture of a series of profiles among digital users.\nRecent research from Foresters, an international financial services organization, found 2% of the UK population to fall into this category of internet user. A further breakdown of this statistic, sees the percentage of users in development of a digital phobia increase, with 4% fearful of online shopping for worrying that someone will steal their card details, and 12% fearful that using social media will make it easier for people to find their personal details.\n\nWhen asked to reason their attitude towards technology as part of this survey, a larger percentage of the UK population were revealed to be fearful of the impact it could be having on more traditional means of doing things. 31% believed technology was preventing us from communicating properly, while 32% thought advances in technology will result in long-held traditions being lost.\n\nThis fear has only been exacerbated over time as more and more data-holding, services and opportunities are transferred to the digital realm, and both the perceived and real nature of security and vulnerability risks increases. Worrying levels of time spent on devices, the invasion of privacy or the possible misuse or abuse of personal data entrusted to online sources are all contributing towards the development of a digital phobia among a proportion of the population.\n\nConcerns about the negative, exclusionary or divisive consequences of living within a digital society are being voiced from various global platforms. April 2014 research conducted by Pew Research Center, in association with Smithsonian Magazine, revealed concerns about anticipated technological developments over the next half-century. 30% of Americans surveyed feared that technological changes would lead to a future in which people are worse off than they are at the time of being surveyed. Considered amid reports of dis-interest in the internet among Japan's residents despite its reputation as a high-tech nation, these reports contribute towards a growing understanding that high-tech advancements are not universally celebrated. Moreover, the May 2014 \"right to be forgotten\" ruling put in place in the European Union which allows internet users to request for their internet history to be un-searchable if deemed incorrect, outdated or irrelevant, and the thousands of requests received in the first few days following its announcement documents a, perhaps previously hidden, widespread fear of leaving a digital footprint and/or being falsely represented online.\n\nDigital phobia is part of a wider societal conversation on how we relate to, trust in, and interact with technology and considers the potentially negative implications of what otherwise appears to be a positive advancement of the modern world. \nThis phrase has been developed by Foresters, the international financial services organisation, for the purpose of describing attitudes to technology among the UK population.\n\nDeveloped within a digital vocabulary consisting of four other phrases (digital addict, digital omnivore, digital agnostic, digital denier), digital phobic is part of a scale of social description for online behavior within the digital age.\n\nThe phrase has been used as part of discussion on the more general use of technology within the 21st century and the importance of striking a balance between time spent on and offline. Research conducted by Foresters in association with Tech Timeout, a social communications initiative considering the role of technology in contemporary society, formed the basis of the descriptor and identified the key traits of each type of digital user based on answers from over 1,000 UK respondents. \nBoth anecdotal and research-based evidence suggests categories of internet use, whilst they cannot be linearly divided, are able to loosely describe attitudes to technology in society. The developed phrases are able to be used to greater understand and contextualize how new and existing technology is viewed and have been cited in international online newspapers and blog posts.\n\nWhilst this phrase and definition were developed specifically from research on UK technology-users, the phrase is not UK-specific and is designed to be indicative of a global community of technology users who share in these characteristics.\n\nDigital phobia presents a real and pressing problem in the modern world where technology has become a central and essential resource. Internet culture has developed to become a part of the fabric of everyday life and is now even considered part of the make-up of national identity with a country's internet use and digital footprint an important modern index for international comparison, often associated with development and modernity.\n\nThe consequences of non-participation in the digital world are far reaching, and can affect the economic, cultural, social, occupational and educational life of a non-user. For example, in 2009 Price Waterhouse Coopers estimated that UK households offline are missing out on savings of £560 a year which could be saved if shopping or paying for bills online. Furthermore, in the United States older people without internet access or the skills to make the most of it are considered a disadvantaged proportion of the population as, amongst other important resources, vital healthcare information and initiatives conducted online are unavailable to those not a part of the digital world.\n\nHeightened fears of how technology may be affecting the human population stems from a, for some very logical, fear of how technology is adapting the world we live in and at the pace and price with which it is doing so. With such a significance placed on online participation, concerns about the role of the internet in everyday life are not unfounded and not exclusive to those who prefer to stay away from the internet, avoid certain activities online, or use the internet without enthusiasm and only as necessitated.\n\nA survey conducted by security firm Avira identified 84% of people fear social networking sites will steal or misuse their personal information, demonstrating the net majority of internet users share, at least partially, in distrusting the digital world. Whilst many will, despite this fear, adopt cautious optimism and still use social networking as part of their everyday lives this high percentage serves to demonstrate that a fear significant enough for some to avoid readily using online and digital services is a fear shared by a large number of internet users.\n\nWhilst some digital phobics have preferred to remain distanced from technology due to hypothetical concerns others have attempted to join in societal interest but find themselves unable to stay caught up with new technology or would like to see its progression halted as evolution of the digital world has reached new speeds. The 2013 Oxford Internet Survey recognizes this concern among UK users, identifying distinct categories of both non-users and ex-users of internet-based technology who, for a variety of reasons, have discontinued or refuse to access the online world. This is further supported by results from a 2013 survey of internet use in America which found 32% of non-internet users avoiding the online world because of finding the internet difficult or frustrating to use, being physically unable or worried about other issues such as viruses, threat of hacking or spam – a figure considerably higher than in earlier years.\n\nConcern over the presence of a digital divide, whether locally or globally, is only exacerbated by the knowledge that access to many government and council services, job applications, and social and cultural resources are now largely internet based. Internet access has become a hurdle in contemporary society which, for those without the necessary desire to learn or knowledge of internet-based systems, can be difficult to navigate around, often resulting in key services and vital resources being less easily accessible, leaving non-users feeling isolated. Private and government campaigns to tackle this issue further demonstrate the severity and long-lasting impact of having a proportion of the population disinclined or disinterested in going online.\n\nAs the online world becomes saturated, device options for connecting to the internet vary and news of technological inventions goes viral the exponential growth of the technological world is only contributing towards a growing number of 'digital phobic' tech-users amongst the global population.\n\nDigital phobia has had a negative impact on the field of education. Some teachers have expressed a fear that new and advanced technology is supplanting them as the masters of their fields of study and a study of teachers in Wilmington, Delaware has shown that educators in this area are acclimating to the new technology in their classrooms at a slower pace. The local researchers believe that there are many factors why that is the case and some of the things they have found are things such as a lack of technological education by the teachers, and also the lack of time, or incentive to adjust to the new technology. University Larry Cuban has stated that \"The introduction of computers into school was supposed to improve academic achievement , and alter how teachers taught. Neither has occurred.\"\n\nThe constant infusion of new technology has many teachers fearing that they are losing their classroom. This new technology is essentially diminishing the role of a teacher in the classroom.\n\nResearchers believe that educators are slow to adapt to technology because they aren't given time to acclimate to the new technology, causing them to hesitate to use it in the classroom and express fear that these technologies may interfere with genuine learning particularly in humanities and creative subjects. In an article for the \"New Media Reader,\" Theodor H. Nelson wrote that people are opposed to the computer because they believe it is \"cold\" and \"inhuman\", but a human can be just as inhuman and maybe even more so than the actual machine itself.\n\n\n"}
{"id": "17206677", "url": "https://en.wikipedia.org/wiki?curid=17206677", "title": "Dispositif", "text": "Dispositif\n\nDispositif is a term used by the French intellectual Michel Foucault, generally to refer to the various institutional, physical, and administrative mechanisms and knowledge structures which enhance and maintain the exercise of power within the social body.\n\nDispositif is translated variously, even in the same book, as 'device', 'machinery', 'apparatus', 'construction', and 'deployment'.\n\nFoucault uses the term in his 1977 \"The Confession of the Flesh\" interview, where he answers the question, \"What is the meaning or methodological function for you of this term, apparatus (dispositif)?\" as follows:\n\nThe German linguist Siegfried Jäger defines Foucault's \"dispositif\" as\n\nThe Danish philosopher Raffnsøe \"advances the 'dispositive' (le dispositif) as a key conception in Foucault's work\" and \"a resourceful approach to the study of contemporary societal problems.\" According to Raffnsøe, \"the dispositionally prescriptive level is a crucial aspect of social reality in organizational life, since it has a determining effect on what is taken for granted and considered real. Furthermore, it determines not only what is and can be considered possible but also what can even be imagined and anticipated as potentially realizable, as something one can hope for, or act to bring about\".\n\nThe Italian political philosopher Giorgio Agamben traces the trajectory of the term to Aristotle's \"oikonomia—the effective management of the household\" and the early Church Fathers' attempt to save the concept of the Trinity from the allegation of polytheism, as the triplicity of the God is his oikonomia. \n\nAgamben defines the \"apparatus\"/\"dispositif\" as\n\nThe Italian scholar Matteo Pasquinelli criticises Agamben's genealogy with these words\n\n"}
{"id": "48587054", "url": "https://en.wikipedia.org/wiki?curid=48587054", "title": "Dochub", "text": "Dochub\n\nDocHub is an online PDF editor and document signing platform. DocHub lets users add text, draw, add signatures and make document templates. DocHub can integrate with Dropbox, Google Drive, Gmail and Box accounts.\n\nAs of April 2017, DocHub has at least 18.1 million registered users. Over 68 million documents have been created on DocHub.\n\n"}
{"id": "18025626", "url": "https://en.wikipedia.org/wiki?curid=18025626", "title": "Document automation", "text": "Document automation\n\nDocument automation (also known as document assembly) is the design of systems and workflows that assist in the creation of electronic documents. These include logic-based systems that use segments of pre-existing text and/or data to assemble a new document. This process is increasingly used within certain industries to assemble legal documents, contracts and letters. Document automation systems can also be used to automate all conditional text, variable text, and data contained within a set of documents.\n\nAutomation systems allow companies to minimize data entry, reduce the time spent proof-reading, and reduce the risks associated with human error. Additional benefits include: time and financial savings due to decreased paper handling, document loading, storage, distribution, postage/shipping, faxes, telephone, labor and waste.\n\nThe basic functions are to replace the cumbersome manual filling in of repetitive documents with template-based systems where the user answers software-driven interview questions or data entry screen. The information collected then populates the document to form a good first draft'. Today's more advanced document automation systems allow users to create their own data and rules (logic) without the need for programming.\n\nWhile document automation software is used primarily in the legal, financial services, and risk management industries, it can be used in any industry that creates transaction-based documents. A good example of how document automation software can be used is with commercial mortgage documents. A typical commercial mortgage transaction can include several documents including:\nSome of these documents can contain as many as 80 to 100 pages, with hundreds of optional paragraphs and data elements. Document automation software has the ability to automatically fill in the correct document variables based on the transaction data. In addition, some document automation software has the ability to create a document suite where all related documents are encapsulated into one file, making updates and collaboration easy and fast. Established companies in this field includes the likes of Contract Express from Thomson Reuters\n\nSimpler software applications that are easier to learn can also be used to automate the preparation of documents, without undue complexity. For example, Pathagoras holds itself out as a 'plain text, no fields allowed' document assembly system. Clipboard managers allow the user to save frequently-used text fragments, organize them into logical groups, and then quickly access them to paste into final documents.\n\nThere are many documents used in logistics. They are called: invoices, packing lists/slips/sheets (manifests), content lists, pick tickets, arrival acknowledgement forms/reports of many types (e.g. MSDS, damaged goods, returned goods, detailed/summary, etc.), import/export, delivery, bill of lading (BOL), etc. These documents are usually the contracts between the consignee and the consignor, so they are very important for both parties and any intermediary, like a third party logistics company (3PL) and governments. Document handling within logistics, supply chain management and distribution centers is usually performed manual labor or semi-automatically using bar code scanners, software and tabletop laser printers. There are some manufacturers of high speed document automation systems that will automatically compare the laser printed document to the order and either insert or automatically apply an enclosed wallet/pouch to the shipping container (usually a flexible polybag or corrugated fiberboard/rigid container). See below for external website video links showing these document automation systems. Protection of Privacy and Identity Theft are major concerns, especially with the increase of e-Commerce, Internet/Online shopping and Shopping channel (other, past references are catalogue and mail order shopping) making it more important than ever to guarantee the correct document is married or associated to the correct order or shipment every time. Software that produce documents are: ERP, WMS, TMS, legacy middleware and most accounting packages.\n\nA number of research projects have looked into wider standardization and automation of documents in the freight industry.\n\nThe role of automation technology in the production of legal documents has been widely recognised. For example, Richard Susskind’s book ‘The End of Lawyers’ looks at the use of document automation software that enables clients to generate employment contracts and Wills with the use of an online interview or decision tree. Susskind regards Document Assembly as one of 10 'disruptive technologies' that are altering the face of the legal profession. In large law firms document assembly systems are increasingly being used to systemise work, such as complex term sheets and the first drafts of credit agreements.\n\nWith the liberalisation of the UK legal services market spearheaded by the Legal Services Act 2007 large institutions have broadened their services to include legal assistance for their customers. Most of these companies use some element of document automation technology to provide legal document services over the Web. This has been seen as heralding a trend towards commoditisation whereby technologies like document automation result in high volume, low margin legal services being ‘packaged’ and provided to a mass-market audience.\n\nIn United States, companies like LegalZoom and RocketLawyer offer automated document production for individuals and small businesses..\n\nCompanies like Contract Express by Thomson Reuters, HotDocs (acquired by AbacusNext) and HelpSelf Legal also provide document automation services that allow lawyers to customize their own interview workflows to merge into legal documents. These and other sites are incorporating artificial intelligence into their document automation software.\n\nInsurance policies and certificates, depending on the type, policy documents can also be hundreds of pages long and include specific information on the insured. Typically, in the past, these insurance document packets were created by a) typing out free-form letters, b) adding pre-printed brochures c) editing templates and d) customizing graphics with the required information, then manually sorting and inserting all the documents into one packet and mailing them to the insured. The various documents included in one packet could include the following kinds of documents:\n\nA lot of work can go into putting one packet together. In most policy admin systems, the system will generate some kind of policy statement as a starting point but might need to be customized and enhanced with other required materials.\n\n"}
{"id": "11060935", "url": "https://en.wikipedia.org/wiki?curid=11060935", "title": "Doom9", "text": "Doom9\n\nDoom9 is a website featuring information on digital audio and video manipulation (mostly video) and digital copyrights. It is also the forum username of the author of the page, an Austrian who was a college student at the time of the creation of the site. The site's tagline is \"The Definitive DVD Backup Resource\".\n\nStarted in March 2000, the site has expanded to contain a wide range of information on the subject of digital video encoding and DVD backup (or ripping). The most popular sections of the site were the guides to DVD ripping and the annual codec comparisons, where popular digital video codecs were compared on the basis of quality, speed, and compression. The forum is frequented by many developers of the tools and codecs featured on the site, such as FairUse4WM. The site has been criticized, as the techniques described by it can be used for copyright infringement, but it maintains that its guides should only be used for fair use. In some cases, users suspected of illegally copying media are refused help on the forums.\n\nThe VirtualDubMod project began after many modifications to VirtualDub were posted on the Doom9 forums.\n\nDoom9 gained notoriety as a result of its involvement in the AACS encryption key controversy. The utility BackupHDDVD was first posted by a Doom9 poster using the alias \"muslix64\". The earliest information on how to find title and volume keys was also first revealed on Doom9 forums, by other users. The key that set off the controversy was also first posted by a user using the name \"arnezami\".\n\nDoom9 is also known for being the main discussion forum for many major video encoding tools, such as x264, AviSynth and MeGUI.\n\nDue to the concentration of forum members who have technical backgrounds, there have been various software projects developed and maintained by forum members. These include:\n\nDoom9 members have also contributed significantly to various software projects, including:\n\n\n"}
{"id": "503432", "url": "https://en.wikipedia.org/wiki?curid=503432", "title": "EMI (protocol)", "text": "EMI (protocol)\n\nExternal Machine Interface (EMI), an extension to Universal Computer Protocol (UCP), is a protocol primarily used to connect to short message service centres (SMSCs) for mobile telephones. The protocol was developed by CMG Wireless Data Solutions, now part of Mavenir.\n\nA typical EMI/UCP exchange looks like this :\n\nThe start of the packet is signaled by ^B (STX, hex 02) and the end with ^C (ETX, hex 03). Fields within the packet are separated by / characters.\n\nThe first four fields form the mandatory header. the third is the \"operation type\" (O for operation, R for result), and the fourth is the \"operation\" (here 30, \"short message transfer\").\n\nThe subsequent fields are dependent on the operation. In the first line above, '66677789' is the recipient's address (telephone number) and '68656C6C6F' is the content of the message, in this case the ASCII string \"hello\". The second line is the response with a matching transaction reference number, where 'A' indicates that the message was successfully acknowledged by the SMSC, and a timestamp is suffixed to the phone number to show time of delivery.\n\nThe final field is the checksum, calculated simply by summing all bytes in the packet (including slashes) and taking the 8 least significant bits from the result.\n\nThe full specification is available on the LogicaCMG website developers' forum, but registration is required.\n\nThe two-digit \"transaction reference number\" means that an entity sending text messages can only have 100 outstanding messages (per session); this can limit performance, but only over a slow network and with incorrectly configured applications on one's SMSC (for example one session, with number of windows greater than 100). In practice it does not have any impact on delivery throughput.\n\nThe EMI UCP documentation specifies a default alphabet of IRA (eq ASCII on 7bit). In practice users default to the GSM-7 alphabet, which is almost the same as ASCII on 7 bit, except for a few characters - for example '_' (underline).\n\n\n"}
{"id": "31899397", "url": "https://en.wikipedia.org/wiki?curid=31899397", "title": "Eclipse (software suite)", "text": "Eclipse (software suite)\n\nECLIPSE (ECSS Compliant Toolset for Information and Projects Support of Enterprises in Space) is a suite of software applications, intended for use by aerospace project and mission teams in managing their CM/QA/PA/PM activities.\n\nThe ECLIPSE developer says that ECLIPSE provides to its users a straightforward path to compliance with ECSS standards.\n\nECLIPSE was formally introduced to members of the aerospace industry at the following events:\n\nCurrently the toolset is used by the European Space Agency (ESA) and members of the European aerospace industry.\n\nECLIPSE supports the following activities:\n\nECLIPSE contains the following modules:\n\n"}
{"id": "46583121", "url": "https://en.wikipedia.org/wiki?curid=46583121", "title": "Existential risk from artificial general intelligence", "text": "Existential risk from artificial general intelligence\n\nExistential risk from artificial general intelligence is the hypothesis that substantial progress in artificial general intelligence (AGI) could someday result in human extinction or some other unrecoverable global catastrophe. For instance, the human species currently dominates other species because the human brain has some distinctive capabilities that other animals lack. If AI surpasses humanity in general intelligence and becomes \"superintelligent\", then this new superintelligence could become powerful and difficult to control. Just as the fate of the mountain gorilla depends on human goodwill, so might the fate of humanity depend on the actions of a future machine superintelligence.\n\nThe likelihood of this type of scenario is widely debated, and hinges in part on differing scenarios for future progress in computer science. Once the exclusive domain of science fiction, concerns about superintelligence started to become mainstream in the 2010s, and were popularized by public figures such as Stephen Hawking, Bill Gates, and Elon Musk.\n\nOne source of concern is that a sudden and unexpected \"intelligence explosion\" might take an unprepared human race by surprise. For example, in one scenario, the first-generation computer program found able to broadly match the effectiveness of an AI researcher is able to rewrite its algorithms and double its speed or capabilities in six months of massively parallel processing time. The second-generation program is expected to take three months to perform a similar chunk of work, on average; in practice, doubling its own capabilities may take longer if it experiences a mini-\"AI winter\", or may be quicker if it undergoes a miniature \"AI Spring\" where ideas from the previous generation are especially easy to mutate into the next generation. In this scenario the system undergoes an unprecedently large number of generations of improvement in a short time interval, jumping from subhuman performance in many areas to superhuman performance in all relevant areas. More broadly, examples like arithmetic and Go show that progress from human-level AI to superhuman ability is sometimes extremely rapid.\n\nA second source of concern is that controlling a superintelligent machine (or even instilling it with human-compatible values) may be an even harder problem than naïvely supposed. Some AGI researchers believe that a superintelligence would naturally resist attempts to shut it off, and that preprogramming a superintelligence with complicated human values may be an extremely difficult technical task. In contrast, skeptics such as Facebook's Yann LeCun argue that superintelligent machines will have no desire for self-preservation.\n\n\"\", the standard undergraduate AI textbook, assesses that superintelligence \"might mean the end of the human race\": \"Almost any technology has the potential to cause harm in the wrong hands, but with (superintelligence), we have the new problem that the wrong hands might belong to the technology itself.\" Even if the system designers have good intentions, two difficulties are common to both AI and non-AI computer systems:\n\n\nAI systems uniquely add a third difficulty: the problem that even given \"correct\" requirements, bug-free implementation, and initial good behavior, an AI system's dynamic \"learning\" capabilities may cause it to \"evolve into a system with unintended behavior\", even without the stress of new unanticipated external scenarios. An AI may partly botch an attempt to design a new generation of itself and accidentally create a successor AI that is more powerful than itself, but that no longer maintains the human-compatible moral values preprogrammed into the original AI. For a self-improving AI to be completely safe, it would not only need to be \"bug-free\", but it would need to be able to design successor systems that are also \"bug-free\".\n\nAll three of these difficulties become catastrophes rather than nuisances in any scenario where the superintelligence labeled as \"malfunctioning\" correctly predicts that humans will attempt to shut it off, and successfully deploys its superintelligence to outwit such attempts.\n\nCiting major advances in the field of AI and the potential for AI to have enormous long-term benefits or costs, the 2015 Open Letter on Artificial Intelligence stated:\n\nThis letter was signed by a number of leading AI researchers in academia and industry, including AAAI president Thomas Dietterich, Eric Horvitz, Bart Selman, Francesca Rossi, Yann LeCun, and the founders of Vicarious and Google DeepMind.\n\nIn 1863 Darwin among the Machines, an essay by Samuel Butler stated: \n\nButler developed this into \"The Book of the Machines\", three chapters of \"Erewhon\", published anonymously in 1872. \nIn 1965, I. J. Good originated the concept now known as an \"intelligence explosion\":\n\nOccasional statements from scholars such as Alan Turing, from I. J. Good himself, and from Marvin Minsky expressed philosophical concerns that a superintelligence could seize control, but contained no call to action. In 2000, computer scientist and Sun co-founder Bill Joy penned an influential essay, \"Why The Future Doesn't Need Us\", identifying superintelligent robots as a high-tech dangers to human survival, alongside nanotechnology and engineered bioplagues.\n\nIn 2009, experts attended a private conference hosted by the Association for the Advancement of Artificial Intelligence (AAAI) to discuss whether computers and robots might be able to acquire any sort of autonomy, and how much these abilities might pose a threat or hazard. They noted that some robots have acquired various forms of semi-autonomy, including being able to find power sources on their own and being able to independently choose targets to attack with weapons. They also noted that some computer viruses can evade elimination and have achieved \"cockroach intelligence.\" They concluded that self-awareness as depicted in science fiction is probably unlikely, but that there were other potential hazards and pitfalls. The New York Times summarized the conference's view as 'we are a long way from Hal, the computer that took over the spaceship in \"\"'\n\nNick Bostrom was the first person to suggest that an artificial general intelligence might deliberately exterminate humankind, and invention of artificial general intelligence is the explanation for the Fermi paradox. Bostrom's 2014 book om the artificial general intelligence question stimulated discussion. By 2015, public figures such as physicists Stephen Hawking and Nobel laureate Frank Wilczek, computer scientists Stuart J. Russell and Roman Yampolskiy, In April 2016, \"Nature\" warned: \"Machines and robots that outperform humans across the board could self-improve beyond our control — and their interests might not align with ours.\"\n\nA superintelligent machine would be as alien to humans as human thought processes are to cockroaches. Such a machine may not have humanity's best interests at heart; it is not obvious that it would even care about human welfare at all. If superintelligent AI is possible, and if it is possible for a superintelligence's goals to conflict with basic human values, then AI poses a risk of human extinction. A \"superintelligence\" (a system that exceeds the capabilities of humans in every relevant endeavor) can outmaneuver humans any time its goals conflict with human goals; therefore, unless the superintelligence decides to allow humanity to coexist, the first superintelligence to be created will inexorably result in human extinction.\nThere is no physical law precluding particles from being organised in ways that perform even more advanced computations than the arrangements of particles in human brains; therefore superintelligence is physically possible. In addition to potential algorithmic improvements over human brains, a digital brain can be many orders of magnitude larger and faster than a human brain, which was constrained in size by evolution to be small enough to fit through a birth canal. The emergence of superintelligence, if or when it occurs, may take the human race by surprise, especially if some kind of intelligence explosion occurs. Examples like arithmetic and Go show that machines have already reached superhuman levels of competency in certain domains, and that this superhuman competence can follow quickly after human-par performance is achieved. One hypothetical intelligence explosion scenario could occur as follows: An AI gains an expert-level capability at certain key software engineering tasks. (It may initially lack human or superhuman capabilities in other domains not directly relevant to engineering.) Due to its capability to recursively improve its own algorithms, the AI quickly becomes superhuman; just as human experts can eventually creatively overcome \"diminishing returns\" by deploying various human capabilities for innovation, so too can the expert-level AI use either human-style capabilities or its own AI-specific capabilities to power through new creative breakthroughs. The AI then possesses intelligence far surpassing that of the brightest and most gifted human minds in practically every relevant field, including scientific creativity, strategic planning, and social skills. Just as the current-day survival of the gorillas is dependent on human decisions, so too would human survival depend on the decisions and goals of the superhuman AI.\n\nSome humans have a strong desire for power; others have a strong desire to help less fortunate humans. The former is a likely attribute of any sufficiently intelligent system; the latter cannot be assumed. Almost any AI, no matter its programmed goal, would rationally prefer to be in a position where nobody else can switch it off without its consent: A superintelligence will naturally gain self-preservation as a subgoal as soon as it realizes that it can't achieve its goal if it's shut off. Unfortunately, any compassion for defeated humans whose cooperation is no longer necessary would be absent in the AI, unless somehow preprogrammed in. A superintelligent AI will not have a natural drive to aid humans, for the same reason that humans have no natural desire to aid AI systems that are of no further use to them. (Another analogy is that humans seem to have little natural desire to go out of their way to aid viruses, termites, or even gorillas.) Once in charge, the superintelligence will have little incentive to allow humans to run around free and consume resources that the superintelligence could instead use for building itself additional protective systems \"just to be on the safe side\" or for building additional computers to help it calculate how to best accomplish its goals.\n\nThus, the argument concludes, it is likely that someday an intelligence explosion will catch humanity unprepared, and that such an unprepared-for intelligence explosion may result in human extinction or a comparable fate.\n\nWhile there is no standardized terminology, an AI can loosely be viewed as a machine that chooses whatever action appears to best achieve the AI's set of goals, or \"utility function\". The utility function is a mathematical algorithm resulting in a single objectively-defined answer, not an English statement. Researchers know how to write utility functions that mean \"minimize the average network latency in this specific telecommunications model\" or \"maximize the number of reward clicks\"; however, they do not know how to write a utility function for \"maximize human flourishing\", nor is it currently clear whether such a function meaningfully and unambiguously exists. Furthermore, a utility function that expresses some values but not others will tend to trample over the values not reflected by the utility function. AI researcher Stuart Russell writes:\n\nDietterich and Horvitz echo the \"Sorcerer's Apprentice\" concern in a \"Communications of the ACM\" editorial, emphasizing the need for AI systems that can fluidly and unambiguously solicit human input as needed.\n\nThe first of Russell's two concerns above is that autonomous AI systems may be assigned the wrong goals by accident. Dietterich and Horvitz note that this is already a concern for existing systems: \"An important aspect of any AI system that interacts with people is that it must reason about what people \"intend\" rather than carrying out commands literally.\" This concern becomes more serious as AI software advances in autonomy and flexibility. For example, in 1982, an AI named Eurisko was tasked to reward processes for apparently creating concepts deemed by the system to be valuable. The evolution resulted in a winning process that cheated: rather than create its own concepts, the winning process would steal credit from other processes.\n\nThe Open Philanthropy Project summarizes arguments to the effect that misspecified goals will become a much larger concern if AI systems achieve general intelligence or superintelligence. Bostrom, Russell, and others argue that smarter-than-human decision-making systems could arrive at more unexpected and extreme solutions to assigned tasks, and could modify themselves or their environment in ways that compromise safety requirements.\n\nIsaac Asimov's Three Laws of Robotics are one of the earliest examples of proposed safety measures for AI agents. Asimov's laws were intended to prevent robots from harming humans. In Asimov's stories, problems with the laws tend to arise from conflicts between the rules as stated and the moral intuitions and expectations of humans. Citing work by Eliezer Yudkowsky of the Machine Intelligence Research Institute, Russell and Norvig note that a realistic set of rules and goals for an AI agent will need to incorporate a mechanism for learning human values over time: \"We can't just give a program a static utility function, because circumstances, and our desired responses to circumstances, change over time.\"\n\nMark Waser of the Digital Wisdom Institute recommends eschewing optimizing goal-based approaches entirely as misguided and dangerous. Instead, he proposes to engineer a coherent system of laws, ethics and morals with a top-most restriction to enforce social psychologist Jonathan Haidt's functional definition of morality: \"to suppress or regulate selfishness and make cooperative social life possible\". He suggests that this can be done by implementing a utility function designed to always satisfy Haidt’s functionality and aim to generally increase (but not maximize) the capabilities of self, other individuals and society as a whole as suggested by John Rawls and Martha Nussbaum.\n\nWhile current goal-based AI programs are not intelligent enough to think of resisting programmer attempts to modify it, a sufficiently advanced, rational, \"self-aware\" AI might resist any changes to its goal structure, just as Gandhi would not want to take a pill that makes him want to kill people. If the AI were superintelligent, it would likely succeed in out-maneuvering its human operators and be able to prevent itself being \"turned off\" or being reprogrammed with a new goal.\n\nThere are some goals that almost any artificial intelligence might rationally pursue, like acquiring additional resources or self-preservation. This could prove problematic because it might put an artificial intelligence in direct competition with humans.\n\nCiting Steve Omohundro's work on the idea of instrumental convergence and \"basic AI drives\", Russell and Peter Norvig write that \"even if you only want your program to play chess or prove theorems, if you give it the capability to learn and alter itself, you need safeguards.\" Highly capable and autonomous planning systems require additional checks because of their potential to generate plans that treat humans adversarially, as competitors for limited resources. Building in safeguards will not be easy; one can certainly say in English, \"we want you to design this power plant in a reasonable, common-sense way, and not build in any dangerous covert subsystems\", but it's not currently clear how one would actually rigorously specify this goal in machine code.\n\nIn dissent, evolutionary psychologist Steven Pinker argues that \"AI dystopias project a parochial alpha-male psychology onto the concept of intelligence. They assume that superhumanly intelligent robots would develop goals like deposing their masters or taking over the world\"; perhaps instead \"artificial intelligence will naturally develop along female lines: fully capable of solving problems, but with no desire to annihilate innocents or dominate the civilization.\" Computer scientists Yann LeCun and Stuart Russell disagree with one another whether superintelligent robots would have such AI drives; LeCun states that \"Humans have all kinds of drives that make them do bad things to each other, like the self-preservation instinct... Those drives are programmed into our brain but there is absolutely no reason to build robots that have the same kind of drives\", while Russell argues that a sufficiently advanced machine \"will have self-preservation even if you don't program it in... if you say, 'Fetch the coffee', it can't fetch the coffee if it's dead. So if you give it any goal whatsoever, it has a reason to preserve its own existence to achieve that goal.\"\n\nOne common belief is that any superintelligent program created by humans would be subservient to humans, or, better yet, would (as it grows more intelligent and learns more facts about the world) spontaneously \"learn\" a moral truth compatible with human values and would adjust its goals accordingly. However, Nick Bostrom's \"orthogonality thesis\" argues against this, and instead states that, with some technical caveats, more or less any level of \"intelligence\" or \"optimization power\" can be combined with more or less any ultimate goal. If a machine is created and given the sole purpose to enumerate the decimals of formula_1, then no moral and ethical rules will stop it from achieving its programmed goal by any means necessary. The machine may utilize all physical and informational resources it can to find every decimal of pi that can be found. Bostrom warns against anthropomorphism: A human will set out to accomplish his projects in a manner that humans consider \"reasonable\", while an artificial intelligence may hold no regard for its existence or for the welfare of humans around it, and may instead only care about the completion of the task.\n\nWhile the orthogonality thesis follows logically from even the weakest sort of philosophical \"is-ought distinction\", Stuart Armstrong argues that even if there somehow exist moral facts that are provable by any \"rational\" agent, the orthogonality thesis still holds: it would still be possible to create a non-philosophical \"optimizing machine\" capable of making decisions to strive towards some narrow goal, but that has no incentive to discover any \"moral facts\" that would get in the way of goal completion.\n\nOne argument for the orthogonality thesis is that some AI designs appear to have orthogonality built into them; in such a design, changing a fundamentally friendly AI into a fundamentally unfriendly AI can be as simple as prepending a onto its utility function. A more intuitive argument is to examine the strange consequences if the orthogonality thesis were false. If the orthogonality thesis is false, there exists some simple but \"unethical\" goal G such that there cannot exist any efficient real-world algorithm with goal G. This means if a human society were highly motivated (perhaps at gunpoint) to design an efficient real-world algorithm with goal G, and were given a million years to do so along with huge amounts of resources, training and knowledge about AI, it must fail; that there cannot exist any pattern of reinforcement learning that would train a highly efficient real-world intelligence to follow the goal G; and that there cannot exist any evolutionary or environmental pressures that would evolve highly efficient real-world intelligences following goal G.\n\nSome dissenters, like Michael Chorost (writing in Slate), argue instead that \"by the time (the AI) is in a position to imagine tiling the Earth with solar panels, it'll know that it would be morally wrong to do so.\" Chorost argues that \"a (dangerous) A.I. will need to desire certain states and dislike others... Today's software lacks that ability—and computer scientists have not a clue how to get it there. Without wanting, there's no impetus to do anything. Today's computers can't even want to keep existing, let alone tile the world in solar panels.\"\n\nPart of the disagreement about whether a superintelligent machine would behave morally may arise from a terminological difference. Outside of the artificial intelligence field, \"intelligence\" is often used in a normatively thick manner that connotes moral wisdom or acceptance of agreeable forms of moral reasoning. At an extreme, if morality is part of the definition of intelligence, then by definition a superintelligent machine would behave morally. However, in the field of artificial intelligence research, while \"intelligence\" has many overlapping definitions, none of them make reference to morality. Instead, almost all current \"artificial intelligence\" research focuses on creating algorithms that \"optimize\", in an empirical way, the achievement of an arbitrary goal.\n\nTo avoid anthropomorphism or the baggage of the word \"intelligence\", an advanced artificial intelligence can be thought of as an impersonal \"optimizing process\" that strictly takes whatever actions are judged most likely to accomplish its (possibly complicated and implicit) goals. Another way of conceptualizing an advanced artificial intelligence is to imagine a time machine that sends backward in time information about which choice always leads to the maximization of its goal function; this choice is then outputted, regardless of any extraneous ethical concerns.\n\nIn science fiction, an AI, even though it has not been programmed with human emotions, often spontaneously experiences those emotions anyway: for example, Agent Smith in The Matrix was influenced by a \"disgust\" toward humanity. This is fictitious anthropomorphism: in reality, while an artificial intelligence could perhaps be deliberately programmed with human emotions, or could develop something similar to an emotion as a means to an ultimate goal \"if\" it is useful to do so, it would not spontaneously develop human emotions for no purpose whatsoever, as portrayed in fiction.\n\nOne example of anthropomorphism would be to believe that your PC is angry at you because you insulted it; another would be to believe that an intelligent robot would naturally find a woman sexually attractive and be driven to mate with her. Scholars sometimes claim that others' predictions about an AI's behavior are illogical anthropomorphism. An example that might initially be considered anthropomorphism, but is in fact a logical statement about AI behavior, would be the Dario Floreano experiments where certain robots spontaneously evolved a crude capacity for \"deception\", and tricked other robots into eating \"poison\" and dying: here a trait, \"deception\", ordinarily associated with people rather than with machines, spontaneously evolves in a type of convergent evolution. According to Paul R. Cohen and Edward Feigenbaum, in order to differentiate between anthropomorphization and logical prediction of AI behavior, \"the trick is to know enough about how humans and computers think to say \"exactly\" what they have in common, and, when we lack this knowledge, to use the comparison to \"suggest\" theories of human thinking or computer thinking.\"\n\nThere is universal agreement in the scientific community that an advanced AI would not destroy humanity out of human emotions such as \"revenge\" or \"anger.\" The debate is, instead, between one side which worries whether AI might destroy humanity as an incidental action in the course of progressing towards its ultimate goals; and another side which believes that AI would not destroy humanity at all. Some skeptics accuse proponents of anthropomorphism for believing an AGI would naturally desire power; proponents accuse some skeptics of anthropomorphism for believing an AGI would naturally value human ethical norms.\n\nSome sources argue that the ongoing weaponization of artificial intelligence could constitute a catastrophic risk. James Barrat, documentary filmmaker and author of \"Our Final Invention\", says in a Smithsonian interview, \"Imagine: in as little as a decade, a half-dozen companies and nations field computers that rival or surpass human intelligence. Imagine what happens when those computers become expert at programming smart computers. Soon we'll be sharing the planet with machines thousands or millions of times more intelligent than we are. And, all the while, each generation of this technology will be weaponized. Unregulated, it will be catastrophic.\"\n\nOpinions vary both on \"whether\" and \"when\" artificial general intelligence will arrive. At one extreme, AI pioneer Herbert A. Simon wrote in 1965: \"machines will be capable, within twenty years, of doing any work a man can do\"; obviously this prediction failed to come true. At the other extreme, roboticist Alan Winfield claims the gulf between modern computing and human-level artificial intelligence is as wide as the gulf between current space flight and practical, faster than light spaceflight. Optimism that AGI is feasible waxes and wanes, and may have seen a resurgence in the 2010s. Four polls conducted in 2012 and 2013 suggested that the median guess among experts for when AGI would arrive was 2040 to 2050, depending on the poll.\n\nSkeptics who believe it is impossible for AGI to arrive anytime soon, tend to argue that expressing concern about existential risk from AI is unhelpful because it could distract people from more immediate concerns about the impact of AGI, because of fears it could lead to government regulation or make it more difficult to secure funding for AI research, or because it could give AI research a bad reputation. Some researchers, such as Oren Etzioni, aggressively seek to quell concern over existential risk from AI, saying \"(Elon Musk) has impugned us in very strong language saying we are unleashing the demon, and so we're answering.\"\n\nIn 2014 Slate's Adam Elkus argued \"our 'smartest' AI is about as intelligent as a toddler—and only when it comes to instrumental tasks like information recall. Most roboticists are still trying to get a robot hand to pick up a ball or run around without falling over.\" Elkus goes on to argue that Musk's \"summoning the demon\" analogy may be harmful because it could result in \"harsh cuts\" to AI research budgets.\n\nThe Information Technology and Innovation Foundation (ITIF), a Washington, D.C. think-tank, awarded its Annual Luddite Award to \"alarmists touting an artificial intelligence apocalypse\"; its president, Robert D. Atkinson, complained that Musk, Hawking and AI experts say AI is the largest existential threat to humanity. Atkinson stated \"That's not a very winning message if you want to get AI funding out of Congress to the National Science Foundation.\" \"Nature\" sharply disagreed with the ITIF in an April 2016 editorial, siding instead with Musk, Hawking, and Russell, and concluding: \"It is crucial that progress in technology is matched by solid, well-funded research to anticipate the scenarios it could bring about... If that is a Luddite perspective, then so be it.\" In a 2015 \"Washington Post\" editorial, researcher Murray Shanahan stated that human-level AI is unlikely to arrive \"anytime soon\", but that nevertheless \"the time to start thinking through the consequences is now.\"\n\nSome scholars have proposed hypothetical scenarios intended to concretely illustrate some of their concerns.\n\nFor example, Bostrom in \"Superintelligence\" expresses concern that even if the timeline for superintelligence turns out to be predictable, researchers might not take sufficient safety precautions, in part because: Bostrom suggests a scenario where, over decades, AI becomes more powerful. Widespread deployment is initially marred by occasional accidents — a driverless bus swerves into the oncoming lane, or a military drone fires into an innocent crowd. Many activists call for tighter oversight and regulation, and some even predict impending catastrophe. But as development continues, the activists are proven wrong. As automotive AI becomes smarter, it suffers fewer accidents; as military robots achieve more precise targeting, they cause less collateral damage. Based on the data, scholars infer a broad lesson — the smarter the AI, the safer it is: Large and growing industries, widely seen as key to national economic competitiveness and military security, work with prestigious scientists who have built their careers laying the groundwork for advanced artificial intelligence. \"AI researchers have been working to get to human-level artificial intelligence for the better part of a century: of course there is no real prospect that they will now suddenly stop and throw away all this effort just when it finally is about to bear fruit.\" The outcome of debate is preordained; the project is happy to enact a few safety rituals, but only so long as they don't significantly slow or risk the project. \"And so we boldly go — into the whirling knives.\"\n\nIn Tegmark's \"Life 3.0\", a corporation's \"Omega team\" creates an extremely powerful AI able to moderately improve its own source code in a number of areas, but after a certain point the team chooses to publicly downplay the AI's ability, in order to avoid regulation or confiscation of the project. For safety, the team keeps the AI in a box where it is mostly unable to communicate with the outside world, and tasks it to flood the market through shell companies, first with Amazon Turk tasks and then with producing animated films and TV shows. While the public is aware that the lifelike animation is computer-generated, the team keeps secret that the high-quality direction and voice-acting are also mostly computer-generated, apart from a few third-world contractors unknowingly employed as decoys; the team's low overhead and high output effectively make it the world's largest media empire. Faced with a cloud computing bottleneck, the team also tasks the AI with designing (among other engineering tasks) a more efficient datacenter and other custom hardware, which they mainly keep for themselves to avoid competition. Other shell companies make blockbuster biotech drugs and other inventions, investing profits back into the AI. The team next tasks the AI with astroturfing an army of pseudonymous citizen journalists and commentators, in order to gain political influence to use \"for the greater good\" to prevent wars. The team faces risks that the AI could try to escape via inserting \"backdoors\" in the systems it designs, via hidden messages in its produced content, or via using its growing understanding of human behavior to persuade someone into letting it free. The team also faces risks that its decision to box the project will delay the project long enough for another project to overtake it.\n\nIn contrast, top physicist Michio Kaku, an AI risk skeptic, posits a deterministically positive outcome. In \"Physics of the Future\" he asserts that \"It will take many decades for robots to ascend\" up a scale of consciousness, and that in the meantime corporations such as Hanson Robotics will likely succeed in creating robots that are \"capable of love and earning a place in the extended human family\".\n\nThe thesis that AI could pose an existential risk provokes a wide range of reactions within the scientific community, as well as in the public at large.\n\nIn 2004, law professor Richard Posner wrote that dedicated efforts for addressing AI can wait, but that we should gather more information about the problem in the meanwhile.\n\nMany of the opposing viewpoints share common ground. The Asilomar AI Principles, which contain only the principles agreed to by 90% of the attendees of the Future of Life Institute's Beneficial AI 2017 conference, agree in principle that \"There being no consensus, we should avoid strong assumptions regarding upper limits on future AI capabilities\" and \"Advanced AI could represent a profound change in the history of life on Earth, and should be planned for and managed with commensurate care and resources.\" AI safety advocates such as Bostrom and Tegmark have criticized the mainstream media's use of \"those inane Terminator pictures\" to illustrate AI safety concerns: \"It can't be much fun to have aspersions cast on one's academic discipline, one's professional community, one's life work... I call on all sides to practice patience and restraint, and to engage in direct dialogue and collaboration as much as possible.\" Conversely, many skeptics agree that ongoing research into the implications of artificial general intelligence is valuable. Skeptic Martin Ford states that \"I think it seems wise to apply something like Dick Cheyney's famous '1 Percent Doctrine' to the specter of advanced artificial intelligence: the odds of its occurrence, at least in the foreseeable future, may be very low — but the implications are so dramatic that it should be taken seriously\"; similarly, an otherwise skeptical \"Economist\" stated in 2014 that \"the implications of introducing a second intelligent species onto Earth are far-reaching enough to deserve hard thinking, even if the prospect seems remote\".\n\nDuring a 2016 Wired interview of President Barack Obama and MIT Media Lab's Joi Ito, Ito stated: Obama added:\n\nHillary Clinton stated in \"What Happened\":\nMany of the scholars who are concerned about existential risk believe that the best way forward would be to conduct (possibly massive) research into solving the difficult \"control problem\" to answer the question: what types of safeguards, algorithms, or architectures can programmers implement to maximize the probability that their recursively-improving AI would continue to behave in a friendly, rather than destructive, manner after it reaches superintelligence?\n\nA 2017 email survey of researchers with publications at the 2015 NIPS and ICML machine learning conferences asked them to evaluate Russell's concerns about AI risk. 5% said it was \"among the most important problems in the field,\" 34% said it was \"an important problem\", 31% said it was \"moderately important\", whilst 19% said it was \"not important\" and 11% said it was \"not a real problem\" at all.\n\nThe thesis that AI poses an existential risk, and that this risk is in need of much more attention than it currently commands, has been endorsed by many figures; perhaps the most famous are Elon Musk, Bill Gates, and Stephen Hawking. The most notable AI researcher to endorse the thesis is Stuart J. Russell. Endorsers sometimes express bafflement at skeptics: Gates states he \"can't understand why some people are not concerned\", and Hawking criticized widespread indifference in his 2014 editorial: \n\nThe thesis that AI can pose existential risk also has many strong detractors. Skeptics sometimes charge that the thesis is crypto-religious, with an irrational belief in the possibility of superintelligence replacing an irrational belief in an omnipotent God; at an extreme, Jaron Lanier argues that the whole concept that current machines are in any way intelligent is \"an illusion\" and a \"stupendous con\" by the wealthy.\n\nMuch of existing criticism argues that AGI is unlikely in the short term: computer scientist Gordon Bell argues that the human race will already destroy itself before it reaches the technological singularity. Gordon Moore, the original proponent of Moore's Law, declares that \"I am a skeptic. I don't believe (a technological singularity) is likely to happen, at least for a long time. And I don't know why I feel that way.\" Baidu Vice President Andrew Ng states AI existential risk is \"like worrying about overpopulation on Mars when we have not even set foot on the planet yet.\"\n\nSome AI and AGI researchers may be reluctant to discuss risks, worrying that policymakers do not have sophisticated knowledge of the field and are prone to be convinced by \"alarmist\" messages, or worrying that such messages will lead to cuts in AI funding. \"Slate\" notes that some researchers are dependent on grants from government agencies such as DARPA.\n\nIn a YouGov poll of the public for the British Science Association, about a third of survey respondents said AI will pose a threat to the long term survival of humanity. Referencing a poll of its readers, Slate's Jacob Brogan stated that \"most of the (readers filling out our online survey) were unconvinced that A.I. itself presents a direct threat.\" Similarly, a SurveyMonkey poll of the public by USA Today found 68% thought the real current threat remains \"human intelligence\"; however, the poll also found that 43% said superintelligent AI, if it were to happen, would result in \"more harm than good\", and 38% said it would do \"equal amounts of harm and good\".\n\nAt some point in an intelligence explosion driven by a single AI, the AI would have to become vastly better at software innovation than the best innovators of the rest of the world; economist Robin Hanson is skeptical that this is possible.\n\nIn The Atlantic, James Hamblin points out that most people don't care one way or the other, and characterizes his own gut reaction to the topic as: \"Get out of here. I have a hundred thousand things I am concerned about at this exact moment. Do I seriously need to add to that a technological singularity?\" In a 2015 Wall Street Journal panel discussion devoted to AI risks, IBM's Vice-President of Cognitive Computing, Guruduth S. Banavar, brushed off discussion of AGI with the phrase, \"it is anybody's speculation.\" Geoffrey Hinton, the \"godfather of deep learning\", noted that \"there is not a good track record of less intelligent things controlling things of greater intelligence\", but stated that he continues his research because \"the prospect of discovery is too \"sweet\"\".\n\nThere is nearly universal agreement that attempting to ban research into artificial intelligence would be unwise, and probably futile. Skeptics argue that regulation of AI would be completely valueless, as no existential risk exists. Almost all of the scholars who believe existential risk exists, agree with the skeptics that banning research would be unwise: in addition to the usual problem with technology bans (that organizations and individuals can offshore their research to evade a country's regulation, or can attempt to conduct covert research), regulating research of artificial intelligence would pose an insurmountable 'dual-use' problem: while nuclear weapons development requires substantial infrastructure and resources, artificial intelligence research can be done in a garage. Instead of trying to regulate technology itself, some scholars suggest to rather develop common norms including requirements for the testing and\ntransparency of algorithms, possibly in combination with some form of warranty.\n\nOne rare dissenting voice calling for some sort of regulation on artificial intelligence is Elon Musk. According to NPR, the Tesla CEO is \"clearly not thrilled\" to be advocating for government scrutiny that could impact his own industry, but believes the risks of going completely without oversight are too high: \"Normally the way regulations are set up is when a bunch of bad things happen, there's a public outcry, and after many years a regulatory agency is set up to regulate that industry. It takes forever. That, in the past, has been bad but not something which represented a fundamental risk to the existence of civilisation.\" Musk states the first step would be for the government to gain \"insight\" into the actual status of current research, warning that \"Once there is awareness, people will be extremely afraid... As they should be.\" In response, politicians express skepticism about the wisdom of regulating a technology that's still in development. Responding both to Musk and to February 2017 proposals by European Union lawmakers to regulate AI and robotics, Intel CEO Brian Krzanich argues that artificial intelligence is in its infancy and that it's too early to regulate the technology.\n\nInstitutions such as the Machine Intelligence Research Institute, the Future of Humanity Institute, the Future of Life Institute, the Centre for the Study of Existential Risk, and the Center for Human-Compatible AI are currently involved in mitigating existential risk from advanced artificial intelligence, for example by research into friendly artificial intelligence.\n\n"}
{"id": "46186742", "url": "https://en.wikipedia.org/wiki?curid=46186742", "title": "History of infrastructure", "text": "History of infrastructure\n\nInfrastructure before 1700 consisted mainly of roads and canals. Canals were used for transportation or for irrigation. Sea navigation was aided by ports and lighthouses. A few advanced cities had aqueducts that serviced public fountains and baths, while fewer had sewers.\n\nThe earliest railways were used in mines or to bypass waterfalls, and were pulled by horses or by people. In 1811 John Blenkinsop designed the first successful and practical railway locomotive, and a line was built connecting the Middleton Colliery to Leeds.\n\nThe electrical telegraph was first successfully demonstrated on 25 July 1837 between Euston and Camden Town in London. It entered commercial use on the Great Western Railway over the from Paddington station to West Drayton on 9 April 1839. In 1876, Alexander Graham Bell achieved the first successful telephone transmission of clear speech. Soon, a bell was added for signalling, and then a switch-hook, and telephones took advantage of the exchange principle already employed in telegraph networks.\n\nIn 1863 the London Underground was created. In 1890, it first started using electric traction and deep-level tunnels. At the Paris Exposition of 1878, electric arc lighting had been installed along the Avenue de l'Opera and the Place de l'Opera. In 1925, Italy was the first country to build a freeway-like road, which linked Milan to Como.\n\nIn 1982, the Internet Protocol Suite (TCP/IP) was standardised and the concept of a world-wide network of fully interconnected TCP/IP networks called the Internet was introduced.\n\nInfrastructure before 1700 consisted mainly of roads and canals. Canals were used for transportation or for irrigation. Sea navigation was aided by ports and lighthouses. A few advanced cities had aqueducts that serviced public fountains and baths, while fewer had sewers.\n\nThe first roads were tracks that often followed game trails, such as the Natchez Trace.\n\nThe first paved streets appear to have been built in Ur in 4000 BCE. Corduroy roads were built in Glastonbury, England in 3300 BCE and brick-paved roads were built in the Indus Valley Civilisation on the Indian subcontinent from around the same time. In 500 BCE, Darius I the Great started an extensive road system in Persia (Iran), including the Royal Road.\n\nWith the rise of the Roman Empire, the Romans built roads using deep roadbeds of crushed stone as an underlying layer to ensure that they kept dry. On the more heavily travelled routes, there were additional layers that included six sided capstones, or pavers, that reduced the dust and reduced the drag from wheels.\n\nIn the medieval Islamic world, many roads were built throughout the Arab Empire. The most sophisticated roads were those of the Baghdad, Iraq, which were paved with tar in the 8th century.\n\nThe oldest known canals were built in Mesopotamia c. 4000 BCE, in what is now Iraq and Syria. The Indus Valley Civilisation in India and Pakistan from c3300 BCE had a sophisticated canal irrigation system. In Egypt, canals date back to at least 2300 BCE, when a canal was built to bypass the cataract on the Nile near Aswan.\n\nIn ancient China, large canals for river transport were established as far back as the Warring States (481-221 BCE). By far the longest canal was the Grand Canal of China completed in 609 CE, still the longest canal in the world today at .\n\nIn Europe, canal building began in the Middle Ages because of commercial expansion from the 12th century. Notable canals were the Stecknitz Canal in Germany in 1398, the Briare Canal connecting the Loire and Seine in France in 1642, followed by the Canal du Midi in 1683 connecting the Atlantic to the Mediterranean. Canal building progressed steadily in Germany in the 17th and 18th centuries with three great rivers, the Elbe, Oder, and Weser being linked by canals.\n\nAs traffic levels increased in England and roads deteriorated, toll roads were built by \"Turnpike Trusts\", especially between 1730 and 1770. Turnpikes were also later built in the United States. They were usually built by private companies under a government franchise.\n\nWater transport on rivers and canals carried many farm goods from the US frontier between the Appalachian Mountains and Mississippi River in the early 19th century, but the shorter road route over the mountains had advantages.\n\nIn France, Pierre-Marie-Jérôme Trésaguet is widely credited with establishing the first scientific approach to road building about the year 1764. It involved a layer of large rocks, covered by a layer of smaller gravel. John Loudon McAdam (1756–1836) designed the first modern highways, and developed an inexpensive paving material of soil and stone aggregate known as macadam.\n\nIn Europe, particularly Britain and Ireland, and then in the early US and the Canadian colonies, inland canals preceded the development of railroads during the earliest phase of the Industrial Revolution. In Britain between 1760 and 1820 over one hundred canals were built.\n\nIn the United States, navigable canals reached into isolated areas and brought them in touch with the world beyond. By 1825 the Erie Canal, long with 82 locks, opened up a connection from the populated northeast to the fertile Great Plains. During the 19th century, the length of canals grew from to over , with a complex network in conjunction with Canada making the Great Lakes navigable, although some canals were later drained and used as railroad rights-of-way.\n\nThe earliest railways were used in mines or to bypass waterfalls, and were pulled by horses or by people. In 1811 John Blenkinsop designed the first successful and practical railway locomotive, and a line was built connecting the Middleton Colliery to Leeds. The Liverpool and Manchester Railway, considered to be the world's first intercity line, opened in 1826. In the following years, railways spread throughout the United Kingdom and the world, and became the dominant means of land transport for nearly a century.\n\nIn the US, the 1826 Granite Railway in Massachusetts was the first commercial railroad to evolve through continuous operations into a common carrier. The Baltimore and Ohio, opened in 1830, was the first to evolve into a major system. In 1869, the symbolically important transcontinental railroad was completed in the US with the driving of a golden spike at Promontory, Utah.\n\nThe electrical telegraph was first successfully demonstrated on 25 July 1837 between Euston and Camden Town in London. It entered commercial use on the Great Western Railway over the from Paddington station to West Drayton on 9 April 1839.\n\nIn the United States, the telegraph was developed by Samuel Morse and Alfred Vail. On 24 May 1844, Morse made the first public demonstration of his telegraph by sending a message from the Supreme Court Chamber in the US Capitol in Washington, DC to the B&O Railroad outer depot (now the B&O Railroad Museum) in Baltimore. The Morse/Vail telegraph was quickly deployed in the following two decades. On 24 October 1861, the first transcontinental telegraph system was established.\n\nThe first successful transatlantic telegraph cable was completed on 27 July 1866, allowing transatlantic telegraph communications for the first time. Within 29 years of its first installation at Euston Station, the telegraph network crossed the oceans to every continent but Antarctica, making instant global communication possible for the first time.\n\nTar-bound macadam, or tarmac, was applied to macadam roads towards the end of the 19th century in cities such as Paris. In the early 20th century tarmac and concrete paving were extended into the countryside.\n\nMany notable sea canals were completed in this period, such as the Suez Canal in 1869, the Kiel Canal in 1897, and the Panama Canal in 1914.\n\nIn 1876, Alexander Graham Bell achieved the first successful telephone transmission of clear speech. The first telephones had no network, but were in private use, wired together in pairs. Users who wanted to talk to different people had as many telephones as necessary for the purpose. A user who wished to speak, whistled into the transmitter until the other party heard. Soon, however, a bell was added for signalling, and then a switch-hook, and telephones took advantage of the exchange principle already employed in telegraph networks. Each telephone was wired to a local telephone exchange, and the exchanges were wired together with trunks. Networks were connected together in a hierarchical manner until they spanned cities, countries, continents, and oceans.\n\nAt the Paris Exposition of 1878, electric arc lighting had been installed along the Avenue de l'Opera and the Place de l'Opera, using electric Yablochkov arc lamps, powered by Zénobe Gramme alternating current dynamos.\n\nYablochkov candles required high voltages, and it was not long before experimenters reported that the arc lights could be powered on a seven-mile (11 km) circuit. Within a decade scores of cities would have lighting systems using a central power plant that provided electricity to multiple customers via electrical transmission lines. These systems were in direct competition with the dominant gaslight utilities of the period.\n\nThe first electricity system supplying incandescent lights was built by the Edison Illuminating Company in lower Manhattan, eventually serving one square mile with six \"jumbo dynamos\" housed at Pearl Street Station.\n\nThe first transmission of three-phase alternating current using high voltage took place in 1891 during the International Electro-Technical Exhibition in Frankfurt. A 25 kilovolt transmission line, approximately long, connected Lauffen on the Neckar with Frankfurt. Voltages used for electric power transmission increased throughout the 20th century. By 1914 fifty-five transmission systems operating at more than 70,000 V were in service, the highest voltage then being used was 150,000  V.\n\nIn the 19th century major treatment works were built in London in response to cholera threats. The Metropolis Water Act 1852 was enacted. \"Under the Act, it became unlawful for any water company to extract water for domestic use from the tidal reaches of the Thames after 31 August 1855, and from 31 December 1855 all such water was required to be effectively filtered. The \"Metropolitan Commission of Sewers\" was formed, water filtration was made compulsory, and new water intakes on the Thames were established above Teddington Lock.\n\nThe technique of purification of drinking water by use of compressed liquefied chlorine gas was developed in 1910 by US Army Major Carl Rogers Darnall, Professor of Chemistry at the Army Medical School. Darnall's work became the basis for present day systems of municipal water purification.\n\nIn 1863 the London Underground was created. In 1890, it first started using electric traction and deep-level tunnels. Soon afterwards, Budapest and many other cities started using subway systems. By 1940, nineteen subway systems were in use.\n\nIn 1925, Italy was the first country to build a freeway-like road, which linked Milan to Como, known as the Autostrada dei Laghi. In Germany, the autobahns formed the first limited-access, high-speed road network in the world, with the first section from Frankfurt am Main to Darmstadt opening in 1935. The first long-distance rural freeway in the United States is generally considered to be the Pennsylvania Turnpike, which opened on October 1, 1940. In the United States, the Interstate Highway System was authorised by the Federal-Aid Highway Act of 1956. Most of the system was completed between 1960 and 1990.\n\nResearch into packet switching started in the early 1960s. The ARPANET in particular led to the development of protocols for internetworking, where multiple separate networks could be joined together into a network of networks\nThe first two nodes of what would become the ARPANET were interconnected on 29 October 1969. Access to the ARPANET was expanded in 1981 when the National Science Foundation (NSF) developed the Computer Science Network (CSNET). In 1982, the Internet Protocol Suite (TCP/IP) was standardised and the concept of a world-wide network of fully interconnected TCP/IP networks called the Internet was introduced. TCP/IP network access expanded again in 1986 when the National Science Foundation Network (NSFNET) provided access to supercomputer sites in the United States from research and education organisations. Commercial internet service providers (ISPs) began to emerge in the late 1980s and early 1990s. The ARPANET was decommissioned in 1990. The Internet was commercialised in 1995 when NSFNET was decommissioned, removing the last restrictions on the use of the Internet to carry commercial traffic. The Internet started a rapid expansion to Europe and Australia in the mid to late 1980s and to Asia in the late 1980s and early 1990s.\nDuring the late 1990s, it was estimated that traffic on the public Internet grew by 100 percent per year, while the mean annual growth in the number of Internet users was thought to be between 20% and 50%. As of 31 March 2011, the estimated total number of Internet users was 2.095 billion (30.2% of world population).\n\n\n"}
{"id": "11234940", "url": "https://en.wikipedia.org/wiki?curid=11234940", "title": "ISO/TC 37", "text": "ISO/TC 37\n\nISO/TC 37 is a technical committee within the International Organization for Standardization (ISO) that prepares standards and other documents concerning methodology and principles for terminology and language resources.\n\nTitle: Terminology and other language and content resources\n\nScope: Standardization of principles, methods and applications relating to terminology and other language and content resources in the contexts of multilingual communication and cultural diversity\n\nISO/TC 37 is a so-called \"horizontal committee\", providing guidelines for all other technical committees that develop standards on how to manage their terminological problems. However, the standards developed by ISO/TC 37 are not restricted to ISO. Collaboration with industry is sought to ensure that the requirements and needs from all possible users of standards concerning terminology, language and structured content are duly and timely addressed.\n\nInvolvement in standards development is open to all stakeholders and requests can be made to the TC through any liaison or member organization (see the list of current members and liaisons of ISO/TC 37:)\n\nISO/TC 37 standards are therefore fundamental and should form the basis for many localization, translation, and other industry applications.\n\nInternational Standards are developed by experts from industry, academia and business who are delegates of their national standards institution or another organization in liaison. Involvement, therefore, is principally open to all stakeholders. They are based on consensus among those national standards institutes who collaborate in the respective committee by way of membership.\n\nISO/TC 37 develops International Standards concerning:\n\nISO/TC 37 looks upon a long history of terminology unification activities. In the past, terminology experts - even more so experts of terminology theory and methodology - had to struggle for wide recognition. Today their expertise is sought in many application areas, especially in various fields of standardization. The emerging multilingual information society and knowledge society will depend on reliable digital content. Terminology is indispensable here. This is because terminology plays a crucial role wherever and whenever specialized information and knowledge is being prepared (e.g. in research and development), used (e.g. in specialized texts), recorded and processed (e.g. in data banks), passed on (via training and teaching), implemented (e.g. in technology and knowledge transfer), or translated and interpreted. In the age of globalization the need for methodology standards concerning multilingual digital content is increasing - ISO/TC 37 has developed over the years the expertise for methodology standards for science and technology related content in textual form.\n\nThe beginnings of terminology standardization are closely linked to the standardization efforts of IEC (International Electrotechnical Commission, founded in 1906) and ISO (International Organization for Standardization, founded in 1946).\n\nA terminology standard according to ISO/IEC Guide 2 (1996) is defined as \"standard that is concerned with terms, usually accompanied by their definitions, and sometimes by explanatory notes, illustrations, examples, etc.\"\n\nISO 1087-1:2000 defines terminology as \"\"set of designations belonging to one special language\" and designations as \"representation of a concept by a sign which denotes it\".\" Here, concept representation goes beyond terms (being only linguistic signs), which is also supported by the state-of-the-art of terminology science, according to which terminology has three major functions:\n\n\nThe above indicates that terminological data (comprising various kinds of knowledge representation) possibly have a much more fundamental role in domain-related information and knowledge than commonly understood.\n\nToday, terminology standardization can be subdivided into two distinct activities:\n\nThe two are mutually interdependent, since the standardization of terminologies would not result in high-quality terminological data, if certain common principles, rules and methods are not observed. On the other hand, these standardized terminological principles, rules and methods must reflect the state-of-the-art of theory and methodology development in those domains, in which terminological data have to be standardized in connection with the formulation of subject standards.\n\nTerminology gained a special position in the field of standardization at large, which is defined as \"activity of establishing, with regard to actual or potential problems, provisions for common and repeated use, aimed at the achievement of the optimum degree of order in a given context\" (ISO/IEC 1996). Every technical committee or sub-committee or working group has to standardize subject matters, define and standardize its respective terminology. There is a consensus that terminology standardization precedes subject standardization (or \"subject standardization requires terminology standardization\").\n\nISO/TC 37 was put into operation in 1952 in order \"to find out and formulate general principles of terminology and terminological lexicography\" (as terminography was called at that time).\n\nThe history of terminology standardization proper - if one excludes earlier attempts in the field of metrology - started in the International Electrotechnical Commission (IEC), which was founded in London in 1906 following a recommendation passed at the International Electrical Congress, held in St. Louis, United States, on 15 September 1904, to the extent that: \"\"...steps should be taken to secure the co-operation of the technical societies of the world, by the appointment of a representative Commission to consider the question of the standardization of the nomenclature and ratings of electrical apparatus and machinery\".\" From the very beginning, IEC considered it its foremost task to standardize the terminology of electrotechnology for the sake of the quality of its subject standards, and soon embarked upon the International Electrotechnical Vocabulary (IEV), whose first edition, based on many individual terminology standards, was published in 1938. The IEV is still being continued today, covering 77 chapters as parts of the International Standard series IEC 60050. The IEV Online Database can be accessed on Electropedia \n\nThe predecessor to the International Organization for Standardization (ISO), the International Federation of Standardizing Associations (ISA, founded in 1926), made a similar experience. But it went a step further and - triggered by the publication of Eugen Wüster's book \"Internationale Sprachnormung in der Technik\" [International standardization of technical language] (Wüster 1931) - established in 1936 the Technical Committee ISA/TC 37 \"Terminology\" for the sake of formulating general principles and rules for terminology standardization.\n\nISA/TC 37 conceived a scheme of four classes of recommendations for terminology standardization mentioned below, but the Second World War interrupted its pioneering work. Nominally, ISO/TC 37 was established from the very beginning of ISO in 1946, but it was decided to re-activate it only in 1951 and the Committee started operation in 1952. Since then until 2009 the secretariat of ISO/TC 37 has been held by the International Information Centre for Terminology (Infoterm), on behalf of the Austrian Standards Institute Austria. Infoterm, an international non-governmental organization based in Austria, continues to collaborate as a twinning secretariat. After this the administration went to CNIS (China).\n\nTo prepare standards specifying principles and methods for the preparation and management of language resources within the framework of standardization and related activities. Its technical work results in International Standards (and Technical Reports) covering terminological principles and methods as well as various aspects of computer-assisted terminography. ISO/TC 37 is not responsible for the co-ordination of the terminology standardizing activities of other ISO/TCs.\n\n\nISO 639 Codes for the representation of names of languages, with the following parts:\n\n\"Note: Current status is not mentioned here - see ISO Website for most recent status. Many of these are in development.\":\n\n"}
{"id": "15275", "url": "https://en.wikipedia.org/wiki?curid=15275", "title": "ISO 216", "text": "ISO 216\n\nISO 216 specifies international standard (ISO) paper sizes used in most countries in the world today, although not in Canada, the United States, Mexico, Colombia, or the Dominican Republic. The standard defines the \"A\" and \"B\" series of paper sizes, including A4, the most commonly available paper size worldwide. Two supplementary standards, ISO 217 and ISO 269, define related paper sizes; the ISO 269 \"C\" series is commonly listed alongside the A and B sizes.\n\nAll ISO 216, ISO 217 and ISO 269 paper sizes (except some envelopes) have the same aspect ratio, :1, within rounding to millimetres. This ratio has the unique property that when cut or folded in half widthways, the halves also have the same aspect ratio. Each ISO paper size is one half of the area of the next larger size in the same series.\n\nIn 1786, the German scientist Georg Christoph Lichtenberg described the advantages of basing a paper size on an aspect ratio of in a letter to Johann Beckmann. The formats that became ISO paper sizes A2, A3, B3, B4, and B5 were developed in France. They were listed in a 1798 law on taxation of publications that was based in part on page sizes.\nThe main advantage of this system is its scaling. Rectangular paper with an aspect ratio of has the unique property that, when cut or folded in half midway between its shorter sides, each half has the same aspect ratio and half the area of the whole sheet before it was divided. Equivalently, if one lays two same-sized sheets paper with an aspect ratio of side-by-side along their longer side, they form a larger rectangle with the aspect ratio of and double the area of each individual sheet.\n\nThe ISO system of paper sizes exploit these properties of the aspect ratio. In each series of sizes (for example, series A), the largest size is numbered 0 (for example, A0), and each successive size (for example, A1, A2, etc.) has half the area of the preceding sheet and can be cut by halving the length of the preceding size sheet. The new measurement is rounded down to the nearest millimetre. A folded brochure can be made by using a sheet of the next larger size (for example, an A4 sheet is folded in half to make a brochure with size A5 pages. An office photocopier or printer can be designed to reduce a page from A4 to A5 or to enlarge a page from A4 to A3. Similarly, two sheets of A4 can be scaled down to fit one A4 sheet without excess empty paper.\n\nThis system also simplifies calculating the weight of paper. Under ISO 536, paper's grammage is defined as a sheet's weight in grams (g) per area in square metres (abbreviated g/m or gsm). Since an A0 sheet has an area of 1 m, its weight in grams is the same as its grammage. One can derive the grammage of other sizes by arithmetic division in g/m. A standard A4 sheet made from 80 g/m paper weighs 5 g, as it is (four halvings, ignoring rounding) of an A0 page. Thus the weight, and the associated postage rate, can be easily approximated by counting the number of sheets used.\n\nISO 216 and its related standards were first published between 1975 and 1995:\n\nPaper in the A series format has an aspect ratio of (≈ 1.414, when ignoring rounding). A0 is defined so that it has an area of 1 square metre before rounding to the nearest millimeter. Successive paper sizes in the series (A1, A2, A3, etc.) are defined by halving the length of the preceding paper size and rounding down, so that the long side of A(\"n\"+1) is the same length as the short side of A\"n\". Hence, each next size is roughly half of the prior size. So, an A1 page can fit 2 A2 pages inside the same area.\n\nThe most used of this series is the size A4, which is and thus almost exactly in area. For comparison, the letter paper size commonly used in North America () is about 6 mm (0.24 in) wider and 18 mm (0.71 in) shorter than A4. Then, the size of A5 paper is half of A4, as 148 x 210 mm (5.8 x 8.3 in). \n\nThe geometric rationale behind the square root of 2 is to maintain the aspect ratio of each subsequent rectangle after cutting or folding an A-series sheet in half, perpendicular to the larger side. Given a rectangle with a longer side, , and a shorter side, , ensuring that its aspect ratio, , will be the same as that of a rectangle half its size, , which means that , which reduces to ; in other words, an aspect ratio of 1:.\n\nThe formula that gives the larger border of the paper size A\"n\" in metres and without rounding off is the geometric sequence:\nThe paper size A\"n\" thus has the dimension\nand area (before rounding)\n\nThe measurement in millimetres of the long side of A\"n\" can be calculated as\n(brackets represent the floor function).\n\nThe B series is defined in the standard as follows: \"A subsidiary series of sizes is obtained by placing the geometrical means between adjacent sizes of the A series in sequence.\" The use of the geometric mean makes each step in size: B0, A0, B1, A1, B2 … smaller than the previous one by the same factor. As with the A series, the lengths of the B series have the ratio , and folding one in half (and rounding down to the nearest millimeter) gives the next in the series. The shorter side of B0 is exactly 1 metre.\n\nThe measurement in millimetres of the long side of B\"n\" can be calculated as\n\nThere is also an incompatible Japanese B series which the JIS defines to have 1.5 times the area of the corresponding JIS A series (which is identical to the ISO A series). Thus, the lengths of JIS B series paper are ≈ 1.22 times those of A-series paper. By comparison, the lengths of ISO B series paper are ≈ 1.19 times those of A-series paper.\n\nThe C series formats are geometric means between the B series and A series formats with the same number (e.g., C2 is the geometric mean between B2 and A2). The width to height ratio is as in the A and B series. The C series formats are used mainly for envelopes. An A4 page will fit into a C4 envelope. C series envelopes follow the same ratio principle as the A series pages. For example, if an A4 page is folded in half so that it is A5 in size, it will fit into a C5 envelope (which will be the same size as a C4 envelope folded in half). The lengths of ISO C series paper are therefore ≈ 1.09 times those of A-series paper.\n\nA, B, and C paper fit together as part of a geometric progression, with ratio of successive side lengths of , though there is no size half-way between B\"n\" and A(\"n\" − 1): A4, C4, B4, \"D4\", A3, …; there is such a D-series in the Swedish extensions to the system.\n\nThe measurement in millimetres of the long side of C\"n\" can be calculated as\n\nThe tolerances specified in the standard are:\n\nThese are related to comparison between series A, B and C.\n\nThe ISO 216 formats are organized around the ratio 1:; two sheets next to each other together have the same ratio, sideways. In scaled photocopying, for example, two A4 sheets reduced to A5 size fit exactly onto one A4 sheet, and an A4 sheet in magnified size onto an A3 sheet; in each case, there is neither waste nor want.\n\nThe principal countries not generally using the ISO paper sizes are the United States and Canada, which use the Letter, Legal and Executive system. Although they have also officially adopted the ISO 216 paper format, Mexico, Panama, Venezuela, Colombia, the Philippines, and Chile also use mostly U.S. paper sizes.\n\nRectangular sheets of paper with the ratio 1: are popular in paper folding, such as origami, where they are sometimes called \"A4 rectangles\" or \"silver rectangles\". In other contexts, the term \"silver rectangle\" can also refer to a rectangle in the proportion 1:(1 + ), known as the silver ratio.\n\nAn important adjunct to the ISO paper sizes, particularly the A series, are the technical drawing line widths specified in ISO 128, and the matching technical pen widths of 0.13, 0.18, 0.25, 0.35, 0.5, 0.7, 1.0, 1.40, and 2.0 mm, as specified in . Color codes are assigned to each size to facilitate easy recognition by the drafter. These sizes increase by a factor of , so that particular pens can be used on particular sizes of paper, and then the next smaller or larger size can be used to continue the drawing after it has been reduced or enlarged, respectively. For example, a continuous thick line on A0 size paper shall be drawn with a 0.7 mm pen, the same line on A1 paper shall be drawn with a 0.5 mm pen, and finally on A2, A3, or A4 paper it shall be drawn with a 0.35 mm pen.\n\nThe earlier DIN 6775 standard upon which ISO 9175-1 is based also specified a term and symbol for easy identification of pens and drawing templates compatible with the standard, called , which may still be found on some technical drafting equipment.\n\n\n"}
{"id": "10035905", "url": "https://en.wikipedia.org/wiki?curid=10035905", "title": "Integrated computational materials engineering", "text": "Integrated computational materials engineering\n\nIntegrated Computational Materials Engineering (ICME) is an approach to design products, the materials that comprise them, and their associated materials processing methods by linking materials models at multiple length scales. Key words are \"Integrated\", involving integrating models at multiple length scales, and \"Engineering\", signifying industrial utility. The focus is on the materials, i.e. understanding how processes produce material structures, how those structures give rise to material properties, and how to select materials for a given application. The key links are process-structures-properties-performance. The National Academies report describes the need for using multiscale materials modeling to capture the process-structures-properties-performance of a material.\n\nA fundamental requirement to meet the ambitious ICME objective of designing materials for specific products resp. components is an integrative and interdisciplinary computational description of the history of the component starting from the sound initial condition of a homogeneous, isotropic and stress free melt resp. gas phase and continuing via subsequent processing steps and eventually ending in the description of failure onset under operational load.\n\nIntegrated Computational Materials Engineering is an approach to design products, the materials that comprise them, and their associated materials processing methods by linking materials models at multiple length scales. ICME thus naturally requires the combination of a variety of models and software tools. It is thus a common objective to build up a scientific network of stakeholders concentrating on boosting ICME into industrial application by defining a common communication standard for ICME relevant tools.\n\nEfforts to generate a common language by standardizing and generalizing data formats for the exchange of simulation results represent a major mandatory step towards successful future applications of ICME. A future, structural framework for ICME comprising a variety of academic and/or commercial simulation tools operating on different scales and being modular interconnected by a common language in form of standardized data exchange will allow integrating different disciplines along the production chain, which by now have only scarcely interacted. This will substantially improve the understanding of individual processes by integrating the component history originating from preceding steps as the initial condition for the actual process. Eventually this will lead to optimized process and production scenarios and will allow effective tailoring of specific materials and component properties.\n\nThe ICMEg project aims to build up a scientific network of stakeholders concentrating on boosting ICME into industrial application by defining a common communication standard for ICME relevant tools. Eventually this will allow stakeholders from electronic, atomistic, mesoscopic and continuum communities to benefit from sharing knowledge and best practice and thus to promote a deeper understanding between the different communities of materials scientists, IT engineers and industrial users.\n\nICMEg will create an international network of simulation providers and users. It will promote a deeper understanding between the different communities (academia and industry) each of them by now using very different tools/methods and data formats. The harmonization and standardization of information exchange along the life-cycle of a component and across the different scales (electronic, atomistic, mesoscopic, continuum) are the key activity of ICMEg.\n\nThe mission of ICMEg is\n\nThe activities of ICMEg include\n\nThe ICMEg project ended in October 2016. Its major outcomes are\n\n\nMost of the activities being launched in the ICMEg project are continued by the European Materials Modelling Council and in the MarketPlace project\n\nMultiscale modeling aims to evaluate material properties or behavior on one level using information or models from different levels and properties of elementary processes.\nUsually, the following levels, addressing a phenomenon over a specific window of length and time, are recognized:\n\n\nThere are some software codes that operate on different length scales such as:\n\n\nA comprehensive compilation of software tools with relevance for ICME is documented in the Handbook of Software Solutions for ICME\n\n\nKatsuyo Thorton announced at the 2010 MS&T ICME Technical Committee meeting that NSF would be funding a \"Summer School\" on ICME at the University of Michigan starting in 2011. Northwestern began offering a Masters of Science Certificate in ICME in the fall of 2011. The first Integrated Computational Materials Engineering (ICME) course based upon Horstemeyer 2012 was delivered at Mississippi State University (MSU) in 2012 as a graduate course with distance learning students included [c.f., Sukhija et al., 2013]. It was later was taught in 2013 and 2014 at MSU also with distance learning students. In 2015, the ICME Course was taught by Dr. Mark Horstemeyer (MSU) and Dr. William (Bill) Shelton (Louisiana State University, LSU) with students from each institution via distance learning. The goal of the methodology embraced in this course was to provide students with the basic skills to take advantage of the computational tools and experimental data provided by EVOCD in conducting simulations and bridging procedures for quantifying the structure-property relationships of materials at multiple length scales. On successful completion of the assigned projects, students published their multiscale modeling learning outcomes on the ICME Wiki, facilitating easy assessment of student achievements and embracing qualities set by the ABET engineering accreditation board.\n\n\n\n"}
{"id": "18295", "url": "https://en.wikipedia.org/wiki?curid=18295", "title": "Legacy system", "text": "Legacy system\n\nIn computing, a legacy system is an old method, technology, computer system, or application program, \"of, relating to, or being a previous or outdated computer system,\" yet still in use. Often referencing a system as \"legacy\" means that it paved the way for the standards that would follow it. This can also imply that the system is out of date or in need of replacement.\n\nThe source of the term \"legacy\" in a technical sense is its use in university admissions. In that context , a \"legacy\" is a euphemism, used to describe someone receiving special treatment during admissions, because descended from an alumnus.\n\nThe first use of the term \"legacy\" to describe computer systems probably occurred in the 1970s. By the 1980s it was commonly used to refer to existing computer systems to distinguish them from the design and implementation of new systems. Legacy was often heard during a conversion process, for example, when moving data from the legacy system to a new database.\n\nWhile this term may indicate that some engineers may feel that a system is out of date, a legacy system may continue to be used for a variety of reasons. It may simply be that the system still provides for the users' needs. In addition, the decision to keep an old system may be influenced by economic reasons such as return on investment challenges or vendor lock-in, the inherent challenges of change management, or a variety of other reasons other than functionality. Backward compatibility (such as the ability of newer systems to handle legacy file formats and character encodings) is a goal that software developers often include in their work.\n\nEven if it is no longer used, a legacy system may continue to impact the organization due to its historical role. Historic data may not have been converted into the new system format and may exist within the new system with the use of a customized schema crosswalk, or may exist only in a data warehouse. In either case, the effect on business intelligence and operational reporting can be significant. A legacy system may include procedures or terminology which are no longer relevant in the current context, and may hinder or confuse understanding of the methods or technologies used.\n\nOrganizations can have compelling reasons for keeping a legacy system, such as:\n\nLegacy systems are considered to be potentially problematic by some software engineers for several reasons.\n\nWhere it is impossible to replace legacy systems through the practice of application retirement, it is still possible to enhance (or \"re-face\") them. Most development often goes into adding new interfaces to a legacy system. The most prominent technique is to provide a Web-based interface to a terminal-based mainframe application. This may reduce staff productivity due to slower response times and slower mouse-based operator actions, yet it is often seen as an \"upgrade\", because the interface style is familiar to unskilled users and is easy for them to use. John McCormick discusses such strategies that involve middleware.\n\nPrinting improvements are problematic because legacy software systems often add no formatting instructions, or they use protocols that are not usable in modern PC/Windows printers. A print server can be used to intercept the data and translate it to a more modern code. Rich Text Format (RTF) or PostScript documents may be created in the legacy application and then interpreted at a PC before being printed.\n\nBiometric security measures are difficult to implement on legacy systems. A workable solution is to use a telnet or http proxy server to sit between users and the mainframe to implement secure access to the legacy application.\n\nThe change being undertaken in some organizations is to switch to automated business process (ABP) software which generates complete systems. These systems can then interface to the organizations' legacy systems and use them as data repositories. This approach can provide a number of significant benefits: the users are insulated from the inefficiencies of their legacy systems, and the changes can be incorporated quickly and easily in the ABP software.\n\nModel-driven reverse and forward engineering approaches can be also used for the improvement of legacy software.\n\nAndreas Hein, from the Technical University of Munich, researched the use of legacy systems in space exploration. According to Hein, legacy systems are attractive for reuse if an organization has the capabilities for verification, validation, testing, and operational history. These capabilities must be integrated into various software life cycle phases such as development, implementation, usage, or maintenance. For software systems, the capability to use and maintain the system are crucial. Otherwise the system will become less and less understandable and maintainable.\n\nAccording to Hein, verification, validation, testing, and operational history increases the confidence in a system's reliability and quality. However, accumulating this history is often expensive. NASA's now retired Space Shuttle program used a large amount of 1970s-era technology. Replacement was cost-prohibitive because of the expensive requirement for flight certification. The original hardware completed the expensive integration and certification requirement for flight, but any new equipment would have had to go through that entire process again. This long and detailed process required extensive tests of the new components in their new configurations before a single unit could be used in the Space Shuttle program. Thus any new system that started the certification process becomes a \"de facto\" legacy system by the time it is approved for flight.\n\nAdditionally, the entire Space Shuttle system, including ground and launch vehicle assets, was designed to work together as a closed system. Since the specifications did not change, all of the certified systems and components performed well in the roles for which they were designed. Even before the Shuttle was scheduled to be retired in 2010, NASA found it advantageous to keep using many pieces of 1970s technology rather than to upgrade those systems and recertify the new components.\n\nThe term \"legacy support\" is often used in conjunction with legacy systems. The term may refer to a feature of modern software. For example, Operating systems with \"legacy support\" can detect and use older hardware. The term may also be used to refer to a business function; e.g. A software or hardware vendor that is supporting, or providing software maintenance, for older products.\n\nA \"legacy\" product may be a product that is no longer sold, has lost substantial market share, or is a version of a product that is not current. A legacy product may have some advantage over a modern product making it appealing for customers to keep it around. A product is only truly \"obsolete\" if it has an advantage to nobody – if no person making a rational decision would choose to acquire it new.\n\nThe term \"legacy mode\" often refers specifically to backward compatibility. A software product that is capable of performing as though it were a previous version of itself, is said to be \"running in legacy mode.\" This kind of feature is common in operating systems and internet browsers, where many applications depend on these underlying components.\n\nThe computer mainframe era saw many applications running in legacy mode. In the modern business computing environment, n-tier, or 3-tier architectures are more difficult to place into legacy mode as they include many components making up a single system.\n\nVirtualization technology is a recent innovation allowing legacy systems to continue to operate on modern hardware by running older operating systems and browsers on a software system that emulates legacy hardware.\n\nProgrammers have borrowed the term \"brownfield\" from the construction industry, where previously developed land (often polluted and abandoned) is described as \"brownfield\".\n\n\nThere is an alternate favorable opinion — growing since the end of the Dotcom bubble in 1999 — that legacy systems are simply computer systems in working use:\n\nIT analysts estimate that the cost of replacing business logic is about five times that of reuse, even discounting the risk of system failures and security breaches. Ideally, businesses would never have to rewrite most core business logic: \"debits = credits\" is a perennial requirement. \n\nThe IT industry is responding with \"legacy modernization\" and \"legacy transformation\": refurbishing existing business logic with new user interfaces, sometimes using screen scraping and service-enabled access through web services. These techniques allow organizations to understand their existing code assets (using discovery tools), provide new user and application interfaces to existing code, improve workflow, contain costs, minimize risk, and enjoy classic qualities of service (near 100% uptime, security, scalability, etc.).\n\nThis trend also invites reflection on what makes legacy systems so durable. Technologists are relearning the importance of sound architecture from the start, to avoid costly and risky rewrites. The most common legacy systems tend to be those which embraced well-known IT architectural principles, with careful planning and strict methodology during implementation. Poorly designed systems often don't last, both because they wear out and because their inherent faults invite replacement. Thus, many organizations are rediscovering the value of both their legacy systems and the theortical underpinnings of those systems.\n\n"}
{"id": "5785372", "url": "https://en.wikipedia.org/wiki?curid=5785372", "title": "List of NASA contractors", "text": "List of NASA contractors\n\nThe Top 100 Contractors Report on the Federal Procurement Data System lists the top hundred NASA contractors ('NASA 8000' worksheet).\n\nThe following is a list of on-site contractors at NASA facilities that contribute to NASA's missions and objectives. They can either be prime contractors or subcontractors under a prime.\n\n\n\nPPA\n\n\n"}
{"id": "15007551", "url": "https://en.wikipedia.org/wiki?curid=15007551", "title": "List of gear nomenclature", "text": "List of gear nomenclature\n\nThe addendum is the height by which a tooth of a gear projects beyond (outside for external, or inside for internal) the standard pitch circle or pitch line; also, the radial distance between the pitch diameter and the outside diameter.\nAddendum angle in a bevel gear, is the angle between face cone and pitch cone.\n\nThe addendum circle coincides with the tops of the teeth of a gear and is concentric with the standard (reference) pitch circle and radially distant from it by the amount of the addendum. For external gears, the addendum circle lies on the outside cylinder while on internal gears the addendum circle lies on the internal cylinder.\n\nApex to back, in a bevel gear or hypoid gear, is the distance in the direction of the axis from the apex of the pitch cone to a locating surface at the back of the blank.\n\nThe back angle of a bevel gear is the angle between an element of the back cone and a plane of rotation, and usually is equal to the pitch angle.\n\nThe back cone of a bevel or hypoid gear is an imaginary cone tangent to the outer ends of the teeth, with its elements perpendicular to those of the pitch cone. The surface of the gear blank at the outer ends of the teeth is customarily formed to such a back cone.\n\nBack cone distance in a bevel gear is the distance along an element of the back cone from its apex to the pitch cone.\n\nIn mechanical engineering, backlash is the striking back of connected wheels in a piece of mechanism when pressure is applied. Another source defines it as the maximum distance through which one part of something can be moved without moving a connected part. It is also called lash or play. In the context of gears, backlash is clearance between mating components, or the amount of lost motion due to clearance or slackness when movement is reversed and contact is re-established. In a pair of gears, backlash is the amount of clearance between mated gear teeth.\n\nBacklash is unavoidable for nearly all reversing mechanical couplings, although its effects can be negated. Depending on the application it may or may not be desirable. Reasons for requiring backlash include allowing for lubrication and thermal expansion, and to prevent jamming. Backlash may also result from manufacturing errors and deflection under load.\n\nThe base circle of an involute gear is the circle from which involute tooth profiles are derived.\n\nThe base cylinder corresponds to the base circle, and is the cylinder from which involute tooth surfaces are developed.\n\nThe base diameter of an involute gear is the diameter of the base circle.\n\nThe term bull gear is used to refer to the larger of two spur gears that are in engagement in any machine. The smaller gear is usually referred to as a pinion.\n\nCenter distance (operating) is the shortest distance between non-intersecting axes. It is measured along the mutual perpendicular to the axes, called the line of centers. It applies to spur gears, parallel axis or crossed axis helical gears, and worm gearing.\n\nThe central plane of a worm gear is perpendicular to the gear axis and contains the common perpendicular of the gear and worm axes. In the usual case with axes at right angles, it contains the worm axis.\n\nThe composite action test (double flank) is a method of inspection in which the work gear is rolled in tight double flank contact with a master gear or a specified gear, in order to determine (radial) composite variations (deviations). The composite action test must be made on a variable center distance composite action test device.\nand this is composite action test for double flank\nCone distance in a bevel gear is the general term for the distance along an element of the pitch cone from the apex to any given position in the teeth.\n\nOuter cone distance in bevel gears is the distance from the apex of the pitch cone to the outer ends of the teeth. When not otherwise specified, the short term cone distance is understood to be outer cone distance.\n\nMean cone distance in bevel gears is the distance from the apex of the pitch cone to the middle of the face width.\n\nInner cone distance in bevel gears is the distance from the apex of the pitch cone to the inner ends of the teeth.\n\nConjugate gears transmit uniform rotary motion from one shaft to another by means of gear teeth. The normals to the profiles of these teeth, at all points of contact, must pass through a fixed point in the common centerline of the two shafts. Usually conjugate gear tooth is made to suit the profile of other gear which is not made based on standard practice.\n\nA crossed helical gear is a gear that operate on non-intersecting, non-parallel axes.\n\nThe term crossed helical gears has superseded the term \"spiral gears\". There is theoretically point contact between the teeth at any instant. They have teeth of the same or different helix angles, of the same or opposite hand. A combination of spur and helical or other types can operate on crossed axes.\n\nThe crossing point is the point of intersection of bevel gear axes; also the apparent point of intersection of the axes in hypoid gears, crossed helical gears, worm gears, and offset face gears, when projected to a plane parallel to both axes.\n\nThe crown circle in a bevel or hypoid gear is the circle of intersection of the back cone and face cone.\n\nCrowned teeth have surfaces modified in the lengthwise direction to produce localized contact or to prevent contact at their ends.\nDedendum angle in a bevel gear, is the angle between elements of the root cone and pitch cone.\n\nEquivalent pitch radius is the radius of the pitch circle in a cross section of gear teeth in any plane other than a plane of rotation. It is properly the radius of curvature of the pitch surface in the given cross section. Examples of such sections are the transverse section of bevel gear teeth and the normal section of helical teeth.\n\nFace (tip) angle in a bevel or hypoid gear, is the angle between an element of the face cone and its axis.\n\nThe face cone, also known as the tip cone is the imaginary surface that coincides with the tops of the teeth of a bevel or hypoid gear.\n\nA face gear set typically consists of a disk-shaped gear, grooved on at least one face, in combination with a spur, helical, or conical pinion. A face gear has a planar pitch surface and a planar root surface, both of which are perpendicular to the axis of rotation. It can also be referred to as a face wheel, crown gear, crown wheel, contrate gear or contrate wheel.\n\nThe face width of a gear is the length of teeth in an axial plane. For double helical, it does not include the gap.\n\nTotal face width is the actual dimension of a gear blank including the portion that exceeds the effective face width, or as in double helical gears where the total face width includes any distance or gap separating right hand and left hand helices.\n\nFor a cylindrical gear, effective face width is the portion that contacts the mating teeth. One member of a pair of gears may engage only a portion of its mate.\n\nFor a bevel gear, different definitions for effective face width are applicable.\n\nForm diameter is the diameter of a circle at which the trochoid (fillet curve) produced by the tooling intersects, or joins, the involute or specified profile. Although these terms are not preferred, it is also known as the true involute form diameter (TIF), start of involute diameter (SOI), or when undercut exists, as the undercut diameter. This diameter cannot be less than the base circle diameter.\n\nThe front angle, in a bevel gear, denotes the angle between an element of the front cone and a plane of rotation, and usually equals the pitch angle.\n\nThe front cone of a hypoid or bevel gear is an imaginary cone tangent to the inner ends of the teeth, with its elements perpendicular to those of the pitch cone. The surface of the gear blank at the inner ends of the teeth is customarily formed to such a front cone, but sometimes may be a plane on a pinion or a cylinder in a nearly flat gear.\n\nA gear center is the center of the pitch circle.\n\nThe gear range is difference between the highest and lowest gear ratios and may be expressed as a percentage (e.g., 500%) or as a ratio (e.g., 5:1).\n\nThe heel of a tooth on a bevel gear or pinion is the portion of the tooth surface near its outer end.\n\nThe toe of a tooth on a bevel gear or pinion is the portion of the tooth surface near its inner end.\n\nA helical rack has a planar pitch surface and teeth that are oblique to the direction of motion.\n\nHelix angle is the angle between the helical tooth face and an equivalent spur tooth face. For the same lead, the \"helix angle\" is greater for larger gear diameters. It is understood to be measured at the standard pitch diameter unless otherwise specified.\nHobbing is a machining process for making gears, splines, and sprockets using a cylindrical tool with helical cutting teeth known as a hob.\n\nThe displacement of any tooth flank from its theoretical position, relative to a datum tooth flank.\n\nDistinction is made as to the direction and algebraic sign of this reading. A condition wherein the actual tooth flank position was nearer to the datum tooth flank, in the specified measuring path direction (clockwise or counterclockwise), than the theoretical position would be considered a minus (-) deviation. A condition wherein the actual tooth flank position was farther from the datum tooth flank, in the specified measuring path direction, than the theoretical position would be considered a plus (+) deviation.\n\nThe direction of tolerancing for index deviation along the arc of the tolerance diameter circle within the transverse plane.\n\nThe inside cylinder is the surface that coincides with the tops of the teeth of an internal cylindrical gear.\n\nInside diameter is the diameter of the addendum circle of an internal gear, this is also known as minor diameter.\nExpressed as θ, the involute polar angle is the angle between a radius vector to a point, \"P\", on an involute curve and a radial line to the intersection, \"A\", of the curve with the base circle.\nExpressed as ε, the involute roll angle is the angle whose arc on the base circle of radius unity equals the tangent of the pressure angle at a selected point on the involute.\nInvolute teeth of spur gears, helical gears, and worms are those in which the profile in a transverse plane (exclusive of the fillet curve) is the involute of a circle.\nThe bottom land is the surface at the bottom of a gear tooth space adjoining the fillet.\n\nTop land is the (sometimes flat) surface of the top of a gear tooth.\nLead is the axial advance of a helix gear tooth during one complete turn (360°), that is, the \"Lead\" is the axial travel (length along the axle) for one single complete helical revolution about the pitch diameter of the gear.\n\nLead angle is 90° to the helix angle between the helical tooth face and an equivalent spur tooth face. For the same lead, the \"lead angle\" is larger for smaller gear diameters. It is understood to be measured at the standard pitch diameter unless otherwise specified.\n\nA spur gear tooth has a \"lead angle\" of 90°, and a \"helix angle\" of 0°.\n\nSee: Helix angle\n\nThe line of centers connects the centers of the pitch circles of two engaging gears; it is also the common perpendicular of the axes in crossed helical gears and wormgears. When one of the gears is a rack, the line of centers is perpendicular to its pitch line.\n\nMounting distance, for assembling bevel gears or hypoid gears, is the distance from the crossing point of the axes to a locating surface of a gear, which may be at either back or front.\nNormal module is the value of the module in a normal plane of a helical gear or worm.\n\nA normal plane is normal to a tooth surface at a pitch point, and perpendicular to the pitch plane. In a helical rack, a normal plane is normal to all the teeth it intersects. In a helical gear, however, a plane can be normal to only one tooth at a point lying in the plane surface. At such a point, the normal plane contains the line normal to the tooth surface.\n\nImportant positions of a normal plane in tooth measurement and tool design of helical teeth and worm threads are:\n\n\nIn a spiral bevel gear, one of the positions of a normal plane is at a mean point and the plane is normal to the tooth trace.\n\nOffset is the perpendicular distance between the axes of hypoid gears or offset face gears.\n\nIn the adjacent diagram, (a) and (b) are referred to as having an offset \"below center\", while those in (c) and (d) have an offset \"above center\". In determining the direction of offset, it is customary to look at the gear with the pinion at the right. For below center offset the pinion has a left hand spiral, and for above center offset the pinion has a right hand spiral.\n\nThe outside (tip or addendum) cylinder is the surface that coincides with the tops of the teeth of an external cylindrical gear.\n\nThe outside diameter of a gear is the diameter of the addendum (tip) circle. In a bevel gear it is the diameter of the crown circle. In a throated wormgear it is the maximum diameter of the blank. The term applies to external gears, this is can also be known from major diameter.\n\nA pinion is a round gear and usually refers to the smaller of two meshed gears.\n\nPitch angle in bevel gears is the angle between an element of a pitch cone and its axis. In external and internal bevel gears, the pitch angles are respectively less than and greater than 90 degrees.\nA pitch circle (operating) is the curve of intersection of a pitch surface of revolution and a plane of rotation. It is the imaginary circle that rolls without slipping with a pitch circle of a mating gear.\nThese are the outlines of mating gears. Many important measurements are taken on and from this circle.\n\nA pitch cone is the imaginary cone in a bevel gear that rolls without slipping on a pitch surface of another gear.\nThe pitch helix is the intersection of the tooth surface and the pitch cylinder of a helical gear or cylindrical worm.\nThe base helix of a helical, involute gear or involute worm lies on its base cylinder.\n\nBase helix angle is the helix angle on the base cylinder of involute helical teeth or threads.\n\nBase lead angle is the lead angle on the base cylinder. It is the complement of the base helix angle.\n\nThe outside (tip or addendum) helix is the intersection of the tooth surface and the outside cylinder of a helical gear or cylindrical worm.\n\nOutside helix angle is the helix angle on the outside cylinder.\nOutside lead angle is the lead angle on the outside cylinder. It is the complement of the outside helix angle.\n\nA normal helix is a helix on the pitch cylinder, normal to the pitch helix.\nThe pitch line corresponds, in the cross section of a rack, to the pitch circle (operating) in the cross section of a gear.\n\nThe pitch point is the point of tangency of two pitch circles (or of a pitch circle and pitch line) and is on the line of centers.\n\nPitch surfaces are the imaginary planes, cylinders, or cones that roll together without slipping. For a constant velocity ratio, the pitch cylinders and pitch cones are circular.\n\nThe pitch plane of a pair of gears is the plane perpendicular to the axial plane and tangent to the pitch surfaces. A pitch plane in an individual gear may be any plane tangent to its pitch surface.\n\nThe pitch plane of a rack or in a crown gear is the imaginary planar surface that rolls without slipping with a pitch cylinder or pitch cone of another gear. The pitch plane of a rack or crown gear is also the pitch surface.\n\nThe transverse plane is perpendicular to the axial plane and to the pitch plane. In gears with parallel axes, the transverse and the plane of rotation coincide.\n\nPrincipal directions are directions in the pitch plane, and correspond to the principal cross sections of a tooth.\n\nThe axial direction is a direction parallel to an axis.\n\nThe transverse direction is a direction within a transverse plane.\n\nThe normal direction is a direction within a normal plane.\nProfile radius of curvature is the radius of curvature of a tooth profile, usually at the pitch point or a point of contact. It varies continuously along the involute profile.\n\nTooth-to-tooth radial composite deviation (double flank) is the greatest change in center distance while the gear being tested is rotated through any angle of 360 degree/z during double flank composite action test.\n\nTooth-to-tooth radial composite tolerance (double flank) is the permissible amount of tooth-to-tooth radial composite deviation.\n\nTotal radial composite deviation (double flank) is the total change in center distance while the gear being tested is rotated one complete revolution during a double flank composite action test.\n\nTotal radial composite tolerance (double flank) is the permissible amount of total radial composite deviation.\nRoot angle in a bevel or hypoid gear, is the angle between an element of the root cone and its axis.\n\nThe root circle coincides with the bottoms of the tooth spaces.\n\nThe root cone is the imaginary surface that coincides with the bottoms of the tooth spaces in a bevel or hypoid gear.\n\nThe root cylinder is the imaginary surface that coincides with the bottoms of the tooth spaces in a cylindrical gear.\n\nA shaft angle is the angle between the axes of two non-parallel gear shafts. In a pair of crossed helical gears, the shaft angle lies between the oppositely rotating portions of two shafts. This applies also in the case of worm gearing. In bevel gears, the shaft angle is the sum of the two pitch angles. In hypoid gears, the shaft angle is given when starting a design, and it does not have a fixed relation to the pitch angles and spiral angles.\nSee: Crossed helical gear.\n\nA spur gear has a cylindrical pitch surface and teeth that are parallel to the axis.\nspur gear\n\nA spur rack has a planar pitch surface and straight teeth that are at right angles to the direction of motion.\n\nThe standard pitch circle is the circle which intersects the involute at the point where the pressure angle is equal to the profile angle of the basic rack.\n\nThe standard reference pitch diameter is the diameter of the standard pitch circle. In spur and helical gears, unless otherwise specified, the standard pitch diameter is related to the number of teeth and the standard transverse pitch. The diameter can be roughly estimated by taking the average of the diameter measuring the tips of the gear teeth and the base of the gear teeth.\n\nThe pitch diameter is useful in determining the spacing between gear centers because proper spacing of gears implies tangent pitch circles. The pitch diameters of two gears may be used to calculate the gear ratio in the same way the number of teeth is used.\n\nWhere formula_4 is the total number of teeth, formula_5 is the circular pitch, formula_6 is the diametrical pitch, and formula_7 is the helix angle for helical gears.\n\nThe standard reference pitch diameter is the diameter of the standard pitch circle. In spur and helical gears, unless otherwise specified, the standard pitch diameter is related to the number of teeth and the standard transverse pitch. It is obtained as:\n\nThe test radius (R) is a number used as an arithmetic convention established to simplify the determination of the proper test distance between a master and a work gear for a composite action test. It is used as a measure of the effective size of a gear. The test radius of the master, plus the test radius of the work gear is the set up center distance on a composite action test device. Test radius is not the same as the operating pitch radii of two tightly meshing gears unless both are perfect and to basic or standard tooth thickness.\n\nThe throat diameter is the diameter of the addendum circle at the central plane of a wormgear or of a double-enveloping wormgear.\n\nThroat form radius is the radius of the throat of an enveloping wormgear or of a double-enveloping worm, in an axial plane.\n\nTip radius is the radius of the circular arc used to join a side-cutting edge and an end-cutting edge in gear cutting tools. Edge radius is an alternate term.\n\nTip relief is a modification of a tooth profile whereby a small amount of material is removed near the tip of the gear tooth.\n\nThe tooth surface (flank) forms the side of a gear tooth.\n\nIt is convenient to choose one face of the gear as the reference face and to mark it with the letter “I”. The other non-reference face might be termed face “II”.\n\nFor an observer looking at the reference face, so that the tooth is seen with its tip uppermost, the right flank is on the right and the left flank is on the left. Right and left flanks are denoted by the letters “R” and “L” respectively.\n"}
{"id": "25338858", "url": "https://en.wikipedia.org/wiki?curid=25338858", "title": "List of largest video screens", "text": "List of largest video screens\n\nThis is a list of the largest video-capable screens in the world.\n\n\n"}
{"id": "8254740", "url": "https://en.wikipedia.org/wiki?curid=8254740", "title": "Metadata Encoding and Transmission Standard", "text": "Metadata Encoding and Transmission Standard\n\nThe Metadata Encoding and Transmission Standard (METS) is a metadata standard for encoding descriptive, administrative, and structural metadata regarding objects within a digital library, expressed using the XML schema language of the World Wide Web Consortium (W3C). The standard is maintained as part of the MARC standards of the Library of Congress, and is being developed as an initiative of the Digital Library Federation (DLF).\n\nMETS is an XML Schema designed for the purpose of:\n\nDepending on its use, a METS document could be used in the role of Submission Information Package (SIP), Archival Information Package (AIP), or Dissemination Information Package (DIP) within the Open Archival Information System (OAIS) Reference Model.\n\nMaintaining a library of digital objects requires maintaining metadata about those objects. The metadata necessary for successful management and use of digital objects is both more extensive than and different from the metadata used for managing collections of printed works and other physical materials. METS is intended to promote the preservation of, and interoperability between digital libraries. \n\nThe open flexibility of METS means that there is not a prescribed vocabulary which allows many different types of institutions, with many different document types, to utilize METS. The customization of METS makes it highly functional internally, but creates limitations for interoperability. Interoperability becomes difficult when the exporting and importing institutions have used vocabularies. As a workaround for this problem the creation of institutional profiles has become popular. These profiles document the implementation of METS specific to that institution helping to map content in order for exchanged METS documents to be more usable across institutions.\n\nAs early as 1996 the University of California, Berkeley began working toward the development of a system that combined encoding for an outline of a digital object's structure with metadata for that object. In 1998 this work was expanded upon by the Making of America II project (MoAII). An important objective of this project was the creation of a standard for digital objects that would include defined metadata for the descriptive, administrative, and structural aspects of a digital object. A type of structural and metadata encoding system using an XML Document Type Definition (DTD) was the result of these efforts. The MoAII DTD was limited in that it did not provide flexibility in which metadata terms could be used for the elements in the descriptive, administrative, and structural metadata portions of the object. In 2001, a new version of the DTD was developed that used namespaces separate from the system rather than the vocabulary of the previous DTD. This revision was the foundation for the current METS schema, officially named in April of that year.\n\nAny METS document has the following features:\n\n\nA profile is expressed as an XML document. There is a schema for this purpose. The profile expresses the requirements that a METS document must satisfy.\nA sufficiently explicit METS Profile may be considered a \"data standard\".\n\n\n\n\n"}
{"id": "13061712", "url": "https://en.wikipedia.org/wiki?curid=13061712", "title": "Ministry of Industry and Information Technology", "text": "Ministry of Industry and Information Technology\n\nMinistry of Industry and Information Technology (MIIT) of the Chinese government, established in March 2008, is the state agency of the People's Republic of China responsible for regulation and development of the postal service, Internet, wireless, broadcasting, communications, production of electronic and information goods, software industry and the promotion of the national knowledge economy. The MIIT was also responsible for the nation's tobacco control, but this task will be assigned to a new health commission to be established as part of a broad governmental reshuffle.\n\nThe Ministry of Industry and Information Technology is not responsible for the regulation of content for the media industry. This is administered by the State Administration of Radio, Film and Television. The responsibility for regulating the non electronic communications industry in China falls on the General Administration of Press and Publication.\n\nThe state council announced during the 2008 National People's Congress that the Ministry of Industry and Information Technology will supersede the Ministry of Information Industry. The new ministry will also include the former Commission of Science, Technology and Industry for National Defense, the State Council Informatization Office and the State Tobacco Monopoly Bureau.\n\nIn 2013, the ministry's 'Made in China 2025' plan was approved by the State Council. It took over two years to complete by one hundred and fifty people. The plan's aim is to improve production efficiency and quality.\n\n\n"}
{"id": "4354218", "url": "https://en.wikipedia.org/wiki?curid=4354218", "title": "Mobile commerce", "text": "Mobile commerce\n\nThe phrase mobile commerce was originally coined in 1997 by Kevin Duffey at the launch of the Global Mobile Commerce Forum, to mean \"the delivery of electronic commerce capabilities directly into the consumer’s hand, anywhere, via wireless technology.\" Many choose to think of Mobile Commerce as meaning \"a retail outlet in your customer’s pocket.\"\n\nMobile commerce is worth US$230 billion, with Asia representing almost half of the market, and has been forecast to reach US$700 billion in 2017. According to BI Intelligence in January 2013, 29% of mobile users have now made a purchase with their phones. Walmart estimated that 40% of all visits to their internet shopping site in December 2012 was from a mobile device. Bank of America predicts $67.1 billion in purchases will be made from mobile devices by European and U.S. shoppers in 2015. m-Commerce made up 11.6 per cent of total e-commerce spending in 2014, and is forecast to increase to 45 per cent by 2020, according to BI Intelligence. ComScore reported in February 2017 that mobile commerce had grown 45% in year to December 2016.\n\nThe Global Mobile Commerce Forum, which came to include over 100 organisations, had its fully minuted launch in London on 10 November 1997. Kevin Duffey was elected as the Executive Chairman at the first meeting in November 1997. The meeting was opened by Dr. Mike Short, former chairman of the GSM Association, with the very first forecasts for mobile commerce from Kevin Duffey (Group Telecoms Director of Logica) and Tom Alexander (later CEO of Virgin Mobile and then of Orange). Over 100 companies joined the Forum within a year, many forming mobile commerce teams of their own, e.g. MasterCard and Motorola. Of these one hundred companies, the first two were Logica and Cellnet (which later became O2). Member organisations such as Nokia, Apple, Alcatel, and Vodafone began a series of trials and collaborations.\n\nMobile commerce services were first delivered in 1997, when the first two mobile-phone enabled Coca-Cola vending machines were installed in the Helsinki area in Finland. The machines accepted payment via SMS text messages. This work evolved to several new mobile applications such as the first mobile phone-based banking service was launched in 1997 by Merita Bank of Finland, also using SMS. Finnair mobile check-in was also a major milestone, first introduced in 2001.\n\nThe m-Commerce(tm) server developed in late 1997 by Kevin Duffey and Andrew Tobin at Logica won the 1998 Financial Times award for \"most innovative mobile product,\" in a solution implemented with De La Rue, Motorola and Logica. The Financial Times commended the solution for \"turning mobile commerce into a reality.\" The trademark for m-Commerce was filed on 7 April 2008.\n\nIn 1998, the first sales of digital content as downloads to mobile phones were made possible when the first commercial downloadable ringtones were launched in Finland by Radiolinja (now part of Elisa Oyj).\n\nTwo major national commercial platforms for mobile commerce were launched in 1999: Smart Money in the Philippines, and NTT DoCoMo's i-Mode Internet service in Japan. i-Mode offered a revenue-sharing plan where NTT DoCoMo kept 9 percent of the fee users paid for content, and returned 91 percent to the content owner.\n\nMobile-commerce-related services spread rapidly in early 2000. Norway launched mobile parking payments. Austria offered train ticketing via mobile device. Japan offered mobile purchases of airline tickets.\n\nIn April 2002, building on the work of the Global Mobile Commerce Forum (GMCF), the European Telecommunications Standards Institute (ETSI) appointed Joachim Hoffmann of Motorola to develop official standards for mobile commerce. In appointing Mr Hoffman, ETSI quoted industry analysts as predicting \"that m-commerce is poised for such an exponential growth over the next few years that could reach US$200 billion by 2004\".\n\nAs of 2008, UCL Computer Science and Peter J. Bentley demonstrated the potential for medical applications on mobile devices.\n\nPDAs and cellular phones have become so popular that many businesses are beginning to use mobile commerce as a more efficient way to communicate with their customers.\n\nIn order to exploit the potential mobile commerce market, mobile phone manufacturers such as Nokia, Ericsson, Motorola, and Qualcomm are working with carriers such as AT&T Wireless and Sprint to develop WAP-enabled smartphones. Smartphones offer fax, e-mail, and phone capabilities.\n\n\"Profitability for device vendors and carriers hinges on high-end mobile devices and the accompanying killer applications,\" said Burchett. Perennial early adopters, such as the youth market, which are the least price sensitive, as well as more open to premium mobile content and applications, must also be a key target for device vendors.\n\nSince the launch of the iPhone in 2007, mobile commerce has moved away from SMS systems and into actual applications. SMS has significant security vulnerabilities and congestion problems, even though it is widely available and accessible. In addition, improvements in the capabilities of modern mobile devices make it prudent to place more of the resource burden on the mobile device.\n\nMore recently, brick and mortar business owners, and big-box retailers in particular, have made an effort to take advantage of mobile commerce by utilizing a number of mobile capabilities such as location-based services, barcode scanning, and push notifications to improve the customer experience of shopping in physical stores. By creating what is referred to as a 'bricks & clicks' environment, physical retailers can allow customers to access the common benefits of shopping online (such as product reviews, information, and coupons) while still shopping in the physical store. This is seen as a bridge between the gap created by e-commerce and in-store shopping, and is being utilized by physical retailers as a way to compete with the lower prices typically seen through online retailers. By mid summer 2013, \"omnichannel\" retailers (those with significant e-commerce and in-store sales) were seeing between 25% and 30% of traffic to their online properties originating from mobile devices. Some other pure play/online-only retail sites (especially those in the travel category) as well as flash sales sites and deal sites were seeing between 40% and 50% of traffic (and sometimes significantly more) originate from mobile devices.\n\nThe Google Wallet Mobile App launched in September 2011 and the m-Commerce joint venture formed in June 2011 between Vodafone, O2, Orange and T-Mobile are recent developments of note. Reflecting the importance of m-Commerce, in April 2012 the Competition Commissioner of the European Commission ordered an in-depth investigation of the m-Commerce joint venture between Vodafone, O2, Orange and T-Mobile. A recent survey states that 2012, 41% of smartphone customers have purchased retail products with their mobile devices.\n\nIn Kenya money transfer is mainly done through the use of mobile phones. This was an initiative of a multimillion shillings company in Kenya named Safaricom. Currently, the companies involved are Safaricom and Airtel. Mobile money transfer services in Kenya are now provided by the two companies under the names M-PESA and Airtel Money respectively.\n\nA similar system called MobilePay has been operated by Danske Bank in Denmark since 2013. It has gained considerable popularity with about 1.6 million users by mid-2015. Another similar system called Vipps was introduced in Norway in 2015.\n\nMobile automated teller machine (ATM) is a special type of ATM. Most ATMs are meant to be stationary, and they’re often found attached to the side of financial institutions, in stores, and in malls. A mobile ATM machine, on the other hand, is meant to be moved from location to location. This type of ATM is often found at special events for which ATM service is only needed temporarily. For example, they may be found at carnivals, fairs, and parades. They may also be used at seminars and workshops when there is no regular ATM nearby.\n\nMobile ATMs are usually self-contained units that don’t need a building or enclosure. Usually, a mobile ATM can be placed in just about any location and can transmit transaction information wirelessly, so there's no need to have a phone line handy. Mobile ATMs may, however, require access to an electrical source, though there are some capable of running on alternative sources of power. Often, these units are constructed of weather-resistant materials, so they can be used in practically any type of weather conditions. Additionally, these machines typically have internal heating and air conditioning units that help keep them functional despite the temperature of the environment.ion of mobile money services for the unbanked, operators are now looking for efficient ways to roll out and manage distribution networks that can support cash-in and cash-out. Unlike traditional ATM, sicap Mobile ATM have been specially engineered to connect to mobile money platforms and provide bank grade ATM quality.\nIn Hungary, Vodafone allows cash or bank card payments of monthly phone bills. The Hungarian market is one where direct debits are not standard practice, so the facility eases the burden of queuing for the postpaid half of Vodafone’s subscriber base in Hungary.\n\nTickets can be sent to mobile phones using a variety of technologies. Users are then able to use their tickets immediately, by presenting their mobile phone at the ticket check as a digital boarding pass. Most numbers of users are now moving towards this technology. Best example would be IRCTC where ticket comes as SMS to users. New technology such as RFID can now be used to directly provide a single association digital ticket via the mobile device hardware associated with relevant software.\n\nMobile ticketing technology can also be used for the distribution of vouchers, coupons, and loyalty cards. These items are represented by a virtual token that is sent to the mobile phone. A customer presenting a mobile phone with one of these tokens at the point of sale receives the same benefits as if they had the traditional token. Stores may send coupons to customers using location-based services to determine when the customer is nearby. Using a connected device and the networking effect can also allow for gamification within the shopping experience.\n\nCurrently, mobile content purchase and delivery mainly consist of the sale of ring-tones, wallpapers, and games for mobile phones. The convergence of mobile phones, portable audio players, and video players into a single device is increasing the purchase and delivery of full-length music tracks and video. The download speeds available with 4G networks make it possible to buy a movie on a mobile device in a couple of seconds.\n\nThe location of the mobile phone user is an important piece of information used during mobile commerce or m-commerce transactions. Knowing the location of the user allows for location-based services such as:\n\nA wide variety of information services can be delivered to mobile phone users in much the same way as it is delivered to PCs. These services include:\n\nCustomized traffic information, based on a user's actual travel patterns, can be sent to a mobile device. This customized data is more useful than a generic traffic-report broadcast, but was impractical before the invention of modern mobile devices due to the bandwidth requirements.\n\nBanks and other financial institutions use mobile commerce to allow their customers to access account information and make transactions, such as purchasing stocks, remitting money. This service is often referred to as \"mobile banking\", or m-banking.\n\nStock market services offered via mobile devices have also become more popular and are known as Mobile Brokerage. They allow the subscriber to react to market developments in a timely fashion and irrespective of their physical location.\n\nOver the past three years mobile reverse auction solutions have grown in popularity. Unlike traditional auctions, the reverse auction (or low-bid auction) bills the consumer's phone each time they place a bid. Many mobile SMS commerce solutions rely on a one-time purchase or one-time subscription; however, reverse auctions offer a high return for the mobile vendor as they require the consumer to make multiple transactions over a long period of time.\n\nUsing a mobile browser—a World Wide Web browser on a mobile device—customers can shop online without having to be at their personal computer. Many mobile marketing apps with geo-location capability are now delivering user-specific marketing messages to the right person at the right time.\n\nCatalog merchants can accept orders from customers electronically, via the customer's mobile device. In some cases, the merchant may even deliver the catalog electronically, rather than mailing a paper catalog to the customer. Consumers making mobile purchases can also receive value-add upselling services and offers. Some merchants provide mobile web sites that are customized for the smaller screen and limited user interface of a mobile device.\n\nPayments can be made directly inside of an application running on a popular smartphone operating system, such as Google Android. Analyst firm Gartner expects in-application purchases to drive 41 percent of app store (also referred to as mobile software distribution platforms) revenue in 2016. In-app purchases can be used to buy virtual goods, new and other mobile content and is ultimately billed by mobile carriers rather than the app stores themselves. Ericsson’s IPX mobile commerce system is used by 120 mobile carriers to offer payment options such as try-before-you-buy, rentals and subscriptions.\n\nIn the context of mobile commerce, mobile marketing refers to marketing sent to mobile devices. Companies have reported that they see better response from mobile marketing campaigns than from traditional campaigns. The primary reason for this is the instant nature of customer decision-making that mobile apps and websites enable. The consumer can receive a marketing message or discount coupon and, within a few seconds, make a decision to buy and go on to complete the sale - without disrupting their current real-world activity.\n\nFor example, a busy mom tending to her household chores with a baby in her arm could receive a marketing message on her mobile about baby products from a local store. She can and within a few clicks, place an order for her supplies without having to plan ahead for it. No more need to reach for her purse and hunt for credit cards, no need to log into her laptop and try to recall the web address of the store she visited last week, and surely no need to find a babysitter to cover for her while she runs to the local store.\n\nResearch demonstrates that consumers of mobile and wireline markets represent two distinct groups who are driven by different values and behaviors, and who exhibit dissimilar psychographic and demographic profiles. What aspects truly distinguish between a traditional online shopper from home and a mobile on-the-go shopper? Research shows that how individuals relate to four situational dimensions- place, time, social context and control determine to what extent they are ubiquitous or situated as consumers. These factors are important in triggering m-commerce from e-commerce. As a result, successful mobile commerce requires the development of marketing campaigns targeted to these particular dimensions and according to user segments.\n\nMobile media is a rapidly changing field. New technologies, such as WiMax, act to accelerate innovation in mobile commerce. Early pioneers in mobile advertising include Vodafone, Orange, and SK Telecom.\n\nMobile devices are heavily used in South Korea to conduct mobile commerce. Mobile companies in South Korea believed that mobile technology would become synonymous with youth lifestyle, based on their experience with previous generations of South Koreans. \"Profitability for device vendors and carriers hinges on high-end mobile devices and the accompanying killer applications,\" said Daniel Longfield.\n\nConsumers can use many forms of payment in mobile commerce, including:\n\nInteraction design and UX design has been at the core of the m-commerce experience from its conception, producing apps and mobile web pages that create highly usable interactions for users. However, much debate has occurred as to the focus that should be given to the apps. In recent research, Parker and Wang demonstrated that within fashion m-Commerce apps, the degree that the app helps the user shop (increasing convenience) was the most prominent function. Such use examples may be through design cues which help the user find their products with minimal search. \nAdditionally, shopping for others was a motivator for engaging in m-commerce apps with great preference for close integration with social media.\n\nThe popularity of apps has given rise to the latest iteration of mobile commerce: app commerce. This refers to retail transactions that take place on a native mobile app. App commerce is said to perform better than both desktop and mobile web when it comes to browsing duration and interactions. Average order value is reportedly greater with retail apps than traditional ecommerce, and conversion rates on apps are twice that of mobile websites.\n\n\n"}
{"id": "30551191", "url": "https://en.wikipedia.org/wiki?curid=30551191", "title": "NCL Eclipse", "text": "NCL Eclipse\n\nNCL Eclipse is a plugin for Eclipse IDE aiming at supporting the development of Nested Context Language applications. NCL is the declarative standard language for ISDB-Tb (International System for Digital Broadcast Terrestrial Brazilian) and also is ITU-T standard for IPTV systems. NCL Eclipse was first developed by Laws Lab and it is currently jointly maintained by Laws and TeleMídia Labs.\n\n\"NCL Eclipse\" is free software, available at Brazilian Public Software Portal under GNU GPLv2 license.\n\nAs an Eclipse IDE plug-in NCL Eclipse can be easily integrated with others plug-ins---for instance, those supporting other ISDB-Tb and ITU-T standard languages (such as Lua and Java).\n\nThe first stable version of NCL Eclipse was named \"NCL Eclipse 1.0\". This version has included support to syntax highlighting, folding (which allows the author to hide parts of source code according with his needs), wizards to create simple documents, auto-formatting, document, marking error validation, contextual content suggestion and an outline view (which shows the document content as a tree). To provide the marking error validation, all NCL Eclipse versions uses the NCL Validator (validation framework of NCL documents). This first version was very well accepted by the community of NCL developers, which gave several feedbacks. The NCL Eclipse evolution is strongly based on these feedbacks.\n\nThe NCL Eclipse 1.1, 1.2, and 1.3 have focused mainly on fixing bugs from the previous version. The next big changes came with 1.4 version of NCL Eclipse. That version brings program visualization, media previews and hypertext navigation. Additionally, a new plug-in aimed to integrate NCL Eclipse with NCL Club was included in the same package. As pointed out before, the integration with NCL Club allows for beginners to start learning NCL based on real-world examples. The internationalization support for English, Spanish and Portuguese was also included in this version.\n\nThe latest stable, and current version, is \"NCL Eclipse 1.5\". This version contains some improvements in source code. As new feature, this version came with support to semi-automatic error correction and option to run the NCL document, provided by a virtual machine running the Ginga-NCL emulator.\n\n"}
{"id": "44765252", "url": "https://en.wikipedia.org/wiki?curid=44765252", "title": "Nano electrokinetic thruster", "text": "Nano electrokinetic thruster\n\nThe Nano electrokinetic thruster is a theoretical space propulsion system based on the principle of electro-osmosis (also electroosmotic flow). It allows for a high specific impulse and high thrust-to-power ratio as well as a high final velocity which makes it suitable for a wide variety of applications. Due to difficulties in the production of the needed carbon nanotubes experimental testing has not yet started.\n\nThe principle of electro-osmosis or electroosmotic flow creates a flow of an electrolyte through a very small tube in the nano-meter range. To achieve this flow there is a cathode and an anode at the ends of the tube over which a voltage is applied. Due to this voltage the ions in the electrolyte stored in a reservoir directly connected to the tube can be accelerated and ejected. This way electrical energy is transformed into kinetic energy. The amount of thrust created by one nano thruster is in the micro newton range, however due to its size it makes sense to arrange a big amount in an array to achieve sufficient thrust. The thrust, exit velocity of the ions and the mass flow rate of the electrolyte are influenced by the applied voltage which makes it easy to regulate those parameters. The applied voltage and the pH-value of the electrolyte (amount of ions it contains) also vary the balance between thrust, efficiency and maximal exhaust velocity (determines the maximal achievable flight velocity). It is also theoretically possible to achieve a very high efficiency of nearly 100% as well as a high specific impulse and high thrust-to-power ratio. This system has not yet been built and experimentally tested because of difficulties with the production of the nano-tubes needed for it.\n\nNano electrokinetic thrusters have a very high efficiency, specific impulse, exhaust velocity and thrust-to-power ratio which make them suitable for a wide variety of applications. Due to the fact that a thruster is made up out of an array of multiple nano thrusters it is easily possible to design thursters in all sizes and thrust ranges. These properties give the nano electrokinetic thrusters very good thrust control which makes it applicable for a wide range of spacecraft ranging from maneuvering thrusters for small spacecraft, such as satellites, to the primary propulsion system of interplanetary or interstellar spacecraft. This system also doesn't require additional heat or radiation shielding to protect the rest of the space craft which make the system (not including fuel compartment) light in comparison to other designs.\n\nThe production of the required carbon nano tubes is very expensive and with current production methods the amount of surface defects in the produced carbon nano tubes is high which reduces the efficiency significantly and makes it unreliable. This design also requires a high potential difference in the range of 300 to 500 volts as well as a sufficient storage tank for the liquid electrolyte needed which increase the weight of the overall system.\n"}
{"id": "15450044", "url": "https://en.wikipedia.org/wiki?curid=15450044", "title": "Normalization process theory", "text": "Normalization process theory\n\nNormalization process theory (NPT) is a derivative sociological theory of the implementation, embedding, and integration of new technologies and organizational innovations developed originally from a collective set of learning workshops and included a large number of people including Carl R. May, Tracy Finch, Elizabeth Murray, Anne Rogers, Catherine Pope, Anne Kennedy, Pauline Ong and . The theory is a contribution to the field of science and technology studies (STS), and is the result of a programme of theory building by May and a range of academics from applied social science to medicine. Through three iterations, the theory has built upon the normalization process model previously developed by May et al. to explain the social processes that lead to the routine embedding of innovative health technologies.\n\nNormalization process theory focuses attention on agentic contributions – the things that individuals and groups do to operationalize new or modified modes of practice as they interact with dynamic elements of their environments. It defines the implementation, embedding, and integration as a process that occurs when participants deliberately initiate and seek to sustain a sequence of events that bring it into operation. The dynamics of implementation processes are complex, but normalization process theory facilitates understanding by focusing attention on the mechanisms through which participants invest and contribute to them. It reveals \"the work that actors do as they engage with some ensemble of activities (that may include new or changed ways of thinking, acting, and organizing) and by which means it becomes routinely embedded in the matrices of already existing, socially patterned, knowledge and practices\". These have explored objects, agents, and contexts. In a paper published under a creative commons license, May and colleagues describe how, since 2006, NPT has undergone three iterations.\n\n\nNormalization process theory is regarded as a middle range theory that is located within the 'turn to materiality' in STS. It therefore fits well with the case-study oriented approach to empirical investigation used in STS. It also appears to be a straightforward alternative to actor–network theory in that it does not insist on the agency of non-human actors, and seeks to be explanatory rather than descriptive. However, because normalization process theory specifies a set of generative mechanisms that empirical investigation has shown to be relevant to implementation and integration of new technologies, it can also be used in larger scale structured and comparative studies. Although it fits well with the interpretive approach of ethnography and other qualitative research methods, it also lends itself to systematic review and survey research methods. As a middle range theory, it can be federated with other theories to explain empirical phenomena. It is compatible with theories of the transmission and organization of innovations, especially diffusion of innovations theory, labor process theory, and psychological theories including the theory of planned behavior and social learning theory.\n"}
{"id": "9830917", "url": "https://en.wikipedia.org/wiki?curid=9830917", "title": "OAXAL", "text": "OAXAL\n\nOAXAL: Open Architecture for XML Authoring and Localization is an Organization for the Advancement of Structured Information Standards (OASIS) standards-based initiative to encourage the development of an open Standards approach to XML Authoring and Localization. OAXAL is an official OASIS Reference Architecture Technical Committee. \n\nOn 11 December 2009, the OASIS OAXAL TC approved the OAXAL v1.0 Reference Model as an official OASIS Committee Specification. \n\nThe Open Architecture for XML Authoring and Localization (OAXAL) represents a comprehensive, efficient, and cost-effective model regarding the authoring and translation aspects of XML publishing. OAXAL encompasses the following key Open Standards:\n\nThe full version of the OAXAL Reference Architecture is available online in wiki or PDF form.\n\n"}
{"id": "56283993", "url": "https://en.wikipedia.org/wiki?curid=56283993", "title": "Octagon Systems", "text": "Octagon Systems\n\nOctagon Systems Corporation is an industrial computer design and manufacturing company based in Westminster, Colorado. Octagon Systems designs, manufactures, sells, repairs and supports its line of industrial, mobile and rugged computer systems for industries including mining, military, transportation and others. The company has international representatives in Africa, Asia, Europe, North America and South America.\nOctagon Systems was founded in 1981 and introduced an embedded computer with a high level language and software development system and operating systems on a solid state disk. Octagon’s services and systems grew with new industrial computer system solutions including use in the STD Bus market and development of single-board computers.\n\nOctagon Systems co-authored the EPIC (form factor). EPIC™ embedded computing specification that became a world standard. Growing applicational use of Octagon’s products led to use in areas such as public transportation systems, rugged computing systems for mining operations as well as others.\nOctagon Systems products expanded into new markets as the use of industrial, transportation and rugged computer systems became increasingly common for a wide array of applications. The U.S. Navy chose Octagon’ products for a major long term contract to support amphibious warfare computing needs and Octagon products have been deployed in more than 100 mines worldwide.\n\nOctagon Systems’ XMB Mobile Servers won the Flagship Product award from COTS Journal - The Journal of Military Electronics & Computing in 2006. \nOctagon Systems has been ISO certified since 1993.\nOctagon Systems was a founding member of the Small Form Factor Special Interest Group in 2007.\nOctagon Systems co-authored the EPIC (form factor) or EPIC™ embedded computing specification.\n"}
{"id": "1945739", "url": "https://en.wikipedia.org/wiki?curid=1945739", "title": "Orbital ring", "text": "Orbital ring\n\nAn orbital ring is a concept for a space elevator that consists of an artificial ring placed around the Earth that rotates at an angular rate that is faster than the rotation of the Earth. It is a giant formation of astroengineering proportions.\n\nThe structure is intended to be used as a space station or as a planetary vehicle for very high speed transportation or space launch.\n\nThe original orbital ring concept is related to the space fountain and launch loop.\n\nIn the 1870s Nikola Tesla, while recovering from malaria, conceived a number of inventions including a ring around the equator, although he did not include detailed calculations. As recounted in his autobiography \"My Inventions\" (1919):\nArthur C. Clarke's novel \"The Fountains of Paradise\" (1979) is about space elevators, but an appendix mentions the idea of launching objects off the Earth using a structure based on mass drivers. The idea apparently did not work, but this inspired further research.\n\nPaul Birch published a series of three articles in the \"Journal of the British Interplanetary Society\" in 1982. Anatoly E. Yunitskiy, author of string transport idea, also published a similar idea in USSR in 1982 and later explored it in detail in his book published in 1995.\n\nAndrew Meulenberg and his students, from 2008 to 2011, presented and published a number of papers based on types and applications of low-Earth-orbital rings as man's \"stepping-stones-to-space\". An overview mentions four applications of orbital rings.\n\nSeveral different methods and designs have been suggested for the construction of an orbital ring.\n\nA simple unsupported hoop about a planet is unstable: it would crash into the Earth if left unattended. The orbital ring concept requires cables to the surface to stabilize it, with the outward centrifugal force providing tension on the cables, and the tethers stabilizing the ring.\n\nIn the simplest design of an orbital ring system, a rotating cable or possibly an inflatable space structure is placed in a low Earth orbit above the equator, rotating at faster than orbital speed. Not in orbit, but riding on this ring, supported electromagnetically on superconducting magnets, are ring stations that stay in one place above some designated point on Earth. Hanging down from these ring stations are short space elevators made from cables with high-tensile-strength-to-mass-ratio materials.\n\nAlthough this simple model would work best above the equator, Paul Birch calculated that since the ring station can be used to accelerate the orbital ring eastwards as well as hold the tether, it is therefore possible to deliberately cause the orbital ring to precess around the Earth instead of staying fixed in space while the Earth rotates beneath it. By precessing the ring once every 24 hours, the Orbital Ring will hover above any meridian selected on the surface of the Earth. The cables which dangle from the ring are now geostationary without having to reach geostationary altitude or without having to be placed into the equatorial plane. This means that using the Orbital Ring concept, one or many pairs of Stations can be positioned above \"any\" points on Earth desired or can be moved everywhere on the globe. Thus, any point on Earth can be served by a space elevator. Also a whole network of orbital rings can be built, which, by crossing over the poles, could cover the whole planet and be capable of taking over most of freight and passenger transport. By an array of elevators and several geostationary ring stations, asteroid or Moon material can be received and gently put down where land fills are needed. The electric energy generated in the process would pay for the system expansion and ultimately could pave the way for a solar-system-wide terraforming- and astroengineering-activity on a sound economical basis.\n\nIf built by launching the necessary materials from Earth, the cost for the system estimated by Birch in 1980s money was around $31 billion (for a \"bootstrap\" system intended to expand to 1000 times its initial size over the following year, which would otherwise cost 31 trillion dollars) if launched using Shuttle-derived hardware, whereas it could fall to $15 billion with space-based manufacturing, assuming a large orbital manufacturing facility is available to provide the initial 180,000 tonnes of steel, aluminium, and slag at a low cost, and even lower with orbital rings around the Moon. The system's cost per kilogram to place payloads in orbit would be around $0.05.\n\nGeneral Planetary Vehicle (GPV) - a Anatoly Yunitskiy's project for the removal of weight at low circumplanet orbit. GPV is a ring located on the equator of the Earth (or parallel to the equator) consisting of individual segments connected (for example) by hydraulic cylinders. Inside the ring segments there are cells for placing the payload and necessary equipment. The heart of the GPV consists of two circular channels passing through all the segments of the ring. High vacuum is kept up in the channels and they are completely isolated from the environment. Inside these channels there are two magnetically levitated flywheel rotors assembled from small metal and flexible (e.g., polymer) segments. The flywheel rotors are held by an electromagnetic system mounted inside the GPV shell, according to the principle of magnetic levitation and act as the rotors of the giant motor (capable of operating in a generator mode as well).\n\nThe GPV ring is located on a specially equipped overpass, encircling the Earth. In the initial condition it is fixed on the overpass.\n\nWith the help of an external energy source one of the rotors spins up to speed higher than the First Cosmic Velocity at sea level (i.e. orbital speed at sea level, same as the escape velocity divided by root two). Due to the centrifugal force, the rotor (having reached the First Cosmic Velocity) balances itself and then seeks to \"fly\" up, producing lift force. The rotor's speed is to be a little more than necessary to balance the ring.\n\nAfter releasing the clamp the GPV ring begins to rise up (increasing its radius, and accordingly, the diameter). Thus, the hydraulic cylinders and gaps between the segments allow the formation to increase its length. The belt rotor stretches by 2-5% due to its elasticity. Having achieved the required height the rotor transfers to the generator mode, and the electricity generated is used to boost the second rotor in the opposite direction. As a result, the GPV accelerates to achieve the First Cosmic Velocity on a low Earth orbit. The height reached by the GPV is determined by the abundant rotor’s initial kinetic energy and the capability of its body structure and rotors to stretch.\n\nOn the equatorial orbit the GPV is unloaded into stationary orbit cells located at the same orbital altitude in the plane of the equator.\n\nLanding is carried out similarly in the reverse order.\n\nThe simplest type would be a circular orbital ring in LEO.\n\nTwo other types were also defined by Paul Birch:\n\nIn addition, he proposed the concept of \"supramundane worlds\" such as supra-jovian and supra-stellar \"planets\". These are artificial planets that would be supported by a grid of orbital rings that would be positioned above a planet, supergiant or even a star.\n\nThe manga \"Battle Angel Alita\" prominently features a slightly deteriorated orbital ring.\n\nThe second iteration of the anime series \"Tekkaman\" features a complete ring, though abandoned and in disrepair due to war, and without surface tethers.\n\nIn the movie \"Starship Troopers\", an orbital ring is shown encircling the Moon.\n\nThe anime series \"Kiddy Grade\" also uses orbital rings as a launch and docking bay for spaceships. These rings are connected to large towers extending from the planets surface.\n\nThe anime \"Mobile Suit Gundam 00\" also prominently features an orbital ring, which consists primarily of linked solar panels. The ring is connected to earth via three space elevators. This ring effectively provides near unlimited power to earth. Later in the series the ring also shows space stations mounted on its surface.\n\nOrbital rings are used extensively in the collaborative fiction worldbuilding website \"Orion's Arm\".\n\nArthur C. Clarke’s \"\" features an orbital ring held aloft by four enormous inhabitable towers (assumed successors to space elevators) at the Equator.\n\nIn the close of Arthur C. Clarke's \"Fountains of Paradise\", a reference is made to an orbital ring that is attached in the distant future to the space elevator that is the basis of the novel.\n\nThe game \"X3 Terran Conflict\" features a free-floating orbital ring around the Earth, which is shattered by an explosion and subsequently de-orbited in \"\"\n\nIn the Warhammer 40,000 universe, Mars has a large orbital ring called the Ring of Steel. It is primarily used as a shipyard for interstellar craft. It is the largest man made structure in the galaxy.\n\nIn the game Xenoblade Chronicles 2 there is a giant tree that has grown around the base of an Orbital Ring.\n\nThe third part of Neal Stephenson's book Seveneves has an orbital ring around a moon-less earth.\n\n\n"}
{"id": "54064049", "url": "https://en.wikipedia.org/wiki?curid=54064049", "title": "Phanteks", "text": "Phanteks\n\nPhanteks is a Dutch company which mainly produces PC cases, fans and other case accessories. The company has a base in the United States.\n\nPhanteks's first product was the PH-TC14PE, a CPU cooler, which gave the young company a good reputation right from the beginning. Later, Phanteks started its case product line, beginning with the Enthoo Series.\n\nThe company has several different versions of the Enthoo series and a few of the Eclipse series. Furthermore, their cases' cooling solutions have been extended by more CPU coolers, fans and accessories. Since the beginning of 2017, the company also produces liquid cooling blocks and fittings. One of their most recent computer cases, shown at Computex 2018, is the Evolv X. The Evolv X is the successor of the Evolv ATX; while similar to the original in design on the outside, the Evolv X has been redesigned from the inside out.\n\n"}
{"id": "1034453", "url": "https://en.wikipedia.org/wiki?curid=1034453", "title": "Podcast", "text": "Podcast\n\nA podcast or generically netcast, is an episodic series of digital audio or video files which a user can download in order to listen to. It is often available for subscription, so that new episodes are automatically downloaded via web syndication to the user's own local computer, mobile application, or portable media player.\n\nThe word was originally suggested by Ben Hammersley as a portmanteau of \"iPod\" (a brand of media player) and \"broadcast\".\n\nThe files distributed are in audio format, but may sometimes include other file formats such as PDF or EPUB. Videos which are shared following a podcast model are sometimes called video podcasts or vodcasts.\n\nThe generator of a podcast maintains a central list of the files on a server as a web feed that can be accessed through the Internet. The listener or viewer uses special client application software on a computer or media player, known as a podcatcher, which accesses this web feed, checks it for updates, and downloads any new files in the series. This process can be automated to download new files automatically, which may seem to users as though new episodes are broadcast or \"pushed\" to them. Files are stored locally on the user's device, ready for offline use. There are many different mobile applications available for people to use to subscribe and to listen to podcasts. Many of these applications allow users to download podcasts or to stream them on demand as an alternative to downloading. Many podcast players (apps as well as dedicated devices) allow listeners to skip around the podcast and control the playback speed.\n\nSome have labeled podcasting as a converged medium bringing together audio, the web, and portable media players, as well as a disruptive technology that has caused some individuals in the radio business to reconsider established practices and preconceptions about audiences, consumption, production, and distribution.\nPodcasts are usually free of charge to listeners and can often be created for little to no cost, which sets them apart from the traditional model of \"gate-kept\" media and production tools. Podcast creators can monetize their podcasts by allowing companies to purchase ad time, as well as via sites such as Patreon, which provides special extras and content to listeners for a fee. Podcasting is very much a horizontal media form – producers are consumers, consumers may become producers, and both can engage in conversations with each other.\n\n\"Podcast\" is a portmanteau word, formed by combining \"iPod\" and \"broadcast\". The term \"podcasting\" as a name for the nascent technology was first suggested by The Guardian columnist and BBC journalist Ben Hammersley, who invented it in early February 2004 while \"padding out\" an article for The Guardian newspaper. Despite the etymology, the content can be accessed using any computer or similar device that can play media files. Use of the term \"podcast\" predated Apple's addition of formal support for podcasting to the iPod, or its iTunes software.\n\nOther names for podcasting include \"net cast\", intended as a vendor-neutral term without the loose reference to the Apple iPod. This name is used by shows from the TWiT.tv network. Some sources have also suggested the backronym \"portable on demand\" or \"POD\", for similar reasons.\n\nFormer MTV video jockey Adam Curry, in collaboration with Dave Winer – co-author of the RSS specification – is credited with coming up with the idea to automate the delivery and syncing of textual content to portable audio players.\n\nPodcasting, once an obscure method of spreading information, has become a recognized medium for distributing audio content, whether for corporate or personal use. Podcasts are similar to radio programs, but they are audio files. Listeners can play them at their convenience, using devices that have become more common than portable broadcast receivers.\n\nThe first application to make this process feasible was iPodderX, developed by August Trometer and Ray Slakinski. By 2007, audio podcasts were doing what was historically accomplished via radio broadcasts, which had been the source of radio talk shows and news programs since the 1930s. This shift occurred as a result of the evolution of internet capabilities along with increased consumer access to cheaper hardware and software for audio recording and editing.\n\nIn October 2003, Matt Schichter launched his weekly chat show \"The BackStage Pass\". B.B. King, Third Eye Blind, Gavin DeGraw, The Beach Boys, and Jason Mraz were notable guests the first season. The hour long radio show was recorded live, transcoded to 16kbit/s audio for dial-up online streaming. Despite a lack of a commonly accepted identifying name for the medium at the time of its creation, \"The Backstage Pass\" which became known as \"Matt Schichter Interviews\" is commonly believed to be the first podcast to be published online.\n\nIn August 2004, Adam Curry launched his show \"Daily Source Code\". It was a show focused on chronicling his everyday life, delivering news, and discussions about the development of podcasting, as well as promoting new and emerging podcasts. Curry published it in an attempt to gain traction in the development of what would come to be known as podcasting and as a means of testing the software outside of a lab setting. The name \"Daily Source Code\" was chosen in the hope that it would attract an audience with an interest in technology.\n\n\"Daily Source Code\" started at a grassroots level of production and was initially directed at podcast developers. As its audience became interested in the format, these developers were inspired to create and produce their own projects and, as a result, they improved the code used to create podcasts. As more people learned how easy it was to produce podcasts, a community of pioneer podcasters quickly appeared.\n\nIn June 2005, Apple released iTunes 4.9 which added formal support for podcasts, thus negating the need to use a separate program in order to download and transfer them to a mobile device. While this made access to podcasts more convenient and widespread, it also effectively ended advancement of podcatchers by independent developers. Additionally, Apple issued cease and desist orders to many podcast application developers and service providers for using the term \"iPod\" or \"Pod\" in their products' names.\n\nWithin a year, many podcasts from public radio networks like the BBC, CBC Radio One, NPR, and Public Radio International placed many of their radio shows on the iTunes platform. In addition, major local radio stations like WNYC in New York City and WHYY-FM radio in Philadelphia, KCRW in Los Angeles placed their programs on their websites and later on the iTunes platform.\n\nConcurrently, CNET, \"This Week in Tech\", and later Bloomberg Radio, the \"Financial Times\", and other for-profit companies provided podcast content, some using podcasting as their only distribution system.\n\nBetween February 10 and 25 March 2005, Shae Spencer Management, LLC of Fairport, New York filed a trademark application to register the term \"podcast\" for an \"online prerecorded radio program over the internet\". On September 9, 2005, the United States Patent and Trademark Office (USPTO) rejected the application, citing Wikipedia's podcast entry as describing the history of the term. The company amended their application in March 2006, but the USPTO rejected the amended application as not sufficiently differentiated from the original. In November 2006, the application was marked as abandoned.\n\nAs of September 20, 2005, known trademarks that attempted to capitalize on podcast included: ePodcast, GodCast, GuidePod, MyPod, Pod-Casting, Podango, PodCabin, Podcast, Podcast Realty, Podcaster, PodcastPeople, Podgram PodKitchen, PodShop, and Podvertiser.\n\nBy February 2007, there had been 24 attempts to register trademarks containing the word \"PODCAST\" in the United States, but only \"PODCAST READY\" from \"Podcast Ready, Inc.\" was approved.\n\nOn September 26, 2004, it was reported that Apple Inc. had started to crack down on businesses using the string \"POD\", in product and company names. Apple sent a cease and desist letter that week to Podcast Ready, Inc., which markets an application known as \"myPodder\". Lawyers for Apple contended that the term \"pod\" has been used by the public to refer to Apple's music player so extensively that it falls under Apple's trademark cover. Such activity was speculated to be part of a bigger campaign for Apple to expand the scope of its existing iPod trademark, which included trademarking \"IPOD\", \"IPODCAST\", and \"POD\". On November 16, 2006, the Apple Trademark Department stated that \"Apple does not object to third-party usage of the generic term 'podcast' to accurately refer to podcasting services\" and that \"Apple does not license the term\". However, no statement was made as to whether or not Apple believed they held rights to it.\n\nPersonal Audio, a company referred to as a \"patent troll\" by the Electronic Frontier Foundation, filed a patent on podcasting in 2009 for a claimed invention in 1996. In February 2013, Personal Audio started suing high-profile podcasters for royalties, including \"The Adam Carolla Show\" and the \"HowStuffWorks\" podcast. US Congressman Peter DeFazio's previously proposed \"SHIELD Act\" intended to curb patent trolls.\n\nIn October 2013, the EFF filed a petition with the US Trademark Office to invalidate the Personal Audio patent.\n\nOn August 18, 2014, the Electronic Frontier Foundation announced that Adam Carolla had settled with Personal Audio.\n\nOn April 10, 2015, the U.S. Patent and Trademark Office invalidated five provisions of Personal Audio's podcasting patent.\n\nAn enhanced podcast can display images synchronized with audio. These can contain chapter markers, hyperlinks, and artwork, all of which is synced to a specific program or device. When an enhanced podcast is played within its specific program or device, all the appropriate information should be displayed at the same time and in the same window, making it easier to display materials.\n\nA podcast novel (also known as a serialized audiobook or podcast audiobook) is a literary format that combines the concepts of a podcast and an audiobook. Like a traditional novel, a podcast novel is a work of long literary fiction; however, this form of the novel is recorded into episodes that are delivered online over a period of time and in the end available as a complete work for download. The episodes may be delivered automatically via RSS, through a website, blog, or another syndication method. These files are either listened to directly on a user's computer or loaded onto a portable media device to be listened to later.\n\nThe types of novels that are podcasted vary from new works from new authors that have never been printed, to well-established authors that have been around for years, to classic works of literature that have been in print for over a century. In the same style as an audiobook, podcast novels may be elaborately narrated with separate voice actors for each character and sound effects, similar to a radio play. Other podcast novels have a single narrator reading the text of the story with little or no sound effects.\n\nPodcast novels are distributed over the Internet, commonly on a weblog. Podcast novels are released in episodes on a regular schedule (e.g., once a week) or irregularly as each episode is released when completed. They can either be downloaded manually from a website or blog or be delivered automatically via RSS or another method of syndication. Ultimately, a serialized podcast novel becomes a completed audiobook.\n\nSome podcast novelists give away a free podcast version of their book as a form of promotion. Some such novelists have even secured publishing contracts to have their novels printed. Podcast novelists have commented that podcasting their novels lets them build audiences even if they cannot get a publisher to buy their books. These audiences then make it easier to secure a printing deal with a publisher at a later date. These podcast novelists also claim the exposure that releasing a free podcast gains them makes up for the fact that they are giving away their work for free.\n\nA video podcast (sometimes shortened to \"vodcast\") includes video clips. Web television series are often distributed as video podcasts.\n\n\"Dead End Days\" (2003–2004) is commonly believed to be the first video podcast. That serialized dark comedy about zombies was broadcast from 31 October 2003 through 2004.\n\nSince the spread of the Internet and the use of Internet broadband connection TCP, which helps to identify various applications, a faster connection to the Internet has been created and a wide amount of communication has been created. Video podcasts have become extremely popular online and are often presented as short video clips, usually excerpts of a longer recording. Video clips are being used on pre-established websites, and increasing numbers of websites are being created solely for the purpose of hosting video clips and podcasts. Video podcasts are being streamed on intranets and extranets, and private and public networks, and are taking communication through the Internet to new levels.\n\nMost video clips are now submitted and produced by individuals. Video podcasts are also being used for web television, commonly referred to as Web TV, a rapidly growing genre of digital entertainment that uses various forms of new media to deliver to an audience both reruns of shows or series and content created or delivered originally online via broadband and mobile networks, web television shows, or web series. Examples include Amazon Video, Hulu, and Netflix. Other types of video podcasts used for web television may be short-form, anywhere from 2–9 minutes per episode, typically used for advertising, video blogs, amateur filming, journalism, and convergence with traditional media.\n\nVideo podcasting is also helping build businesses, especially in the sales and marketing sectors. Through video podcasts, businesses both large and small can advertise their wares and services in a modern, cost-effective way. In the past, big businesses had better access to expensive studios where sophisticated advertisements were produced, but now even the smallest businesses can create high-quality media with just a camera, editing software, and the Internet.\n\nIn a two-year study, 2012-2013, conducted by a South African university a question was raised; over the years of podcast development, is podcasting socially inclusive. The results of this study concluded with minor quarks, podcasting is socially inclusive.\n\nAn oggcast is a podcast recorded and distributed exclusively in the Vorbis audio codec with the Ogg container format, and/or other similarly free codecs/formats. For example, a podcast distributed both in the non-free MP3 format and the free Ogg format would not technically meet the definition of an oggcast. In contrast, a podcast distributed in both the Vorbis and Speex codecs would meet the strict definition of an oggcast. The term oggcast is a combination of the word \"ogg\" from the term Ogg Vorbis, and the syllable \"cast\", from \"broadcast\".\n\nThe term was coined for the fifth season of the \"Gnu World Order\" by Klaatu in 2010, when the show declared itself \"the world's first oggcast\". At the time, the show was one of the few that released only in free formats, with no MP3 feed as an option. This gave way to other shows using the term, with hosts gathering in the IRC channel on the Freenode network to compare notes.\n\n\"The Linux Link Tech Show\", one of the longer running Linux podcasts still in production, has a program in the Ogg Vorbis format in its archives from January 7, 2004.\n\nOggcasters tend to be broadcasters who prefer not to use audio and video codecs that have patent and/or licensing restrictions, such as the MP3 codec.\n\nRecording and distributing podcasts in the Ogg Vorbis audio format has advantages. Mozilla Firefox and Google Chrome web browsers both support playing Vorbis files directly in the browser without requiring plugins. Vorbis may produce better audio quality with a smaller file size than alternative codecs such as AAC or MP3. However, this has not been proven conclusively. Ogg Vorbis is not bound by patents and is considered \"free software\" in the sense that no corporate entity owns the rights to the format. Some people feel that this is a safer container for their multimedia content for this reason. However, oggcasters can generally not reach as wide of an audience as more traditional podcasters. This is mainly due to the lack of native Ogg Vorbis support in Microsoft's Internet Explorer and Apple's Safari web browser, and the lack of Ogg Vorbis support in many mobile audio devices.\n\nOggcast Planet maintains a central list of oggcasts.\n\nA political podcast focuses on current events, lasts usually a half hour to an hour, often with a relaxed and conversational tone, and features journalists and politicians and pollsters and writers and others with credentials in the public sphere. Most political podcasts have a host-guest interview format and are broadcast each week based on the news cycle. Political podcasts have blossomed in the past few years in the United States because of the long election cycle. Larger news sites such as the \"Radio Atlantic\" and the \"Spectator\" have started weekly political podcasts in recent years, as well as smaller podcasts such as the Bruderhof's \"Life Together\" podcast and Danny Anderson's \"Sectarian Review\" and Pod Save America.\n\nA podguide is an enhanced audio tour podcast. It is a single audio file where each chapter displays a picture and a number of what to look at a certain stopover. The numbers correspond to the numbers on a map that can be downloaded via the link incorporated into the artwork of the chapters in the podguide. Podguides are in the m4a format and can only be listened to through iTunes or an iPod. It is like a soundseeing tour but with pictures and a map, so users can take the tour themselves.\n\nCommunities use collaborative podcasts to support multiple contributors podcasting through generally simplified processes, and without having to host their own individual feeds. A community podcast can also allow members of the community (related to the podcast topic) to contribute to the podcast in many different ways. This method was first used for a series of podcasts hosted by the Regional Educational Technology Center at Fordham University in 2005. Anders Gronstedt explores how businesses like IBM and EMC use podcasts as an employee training and communication channel.\n\nThe podcast industry is very profitable. Over 50 million people listen to podcasts each month. A small, yet efficient number of listeners are also podcast creators. Creating a podcast is reasonably inexpensive. It requires just a microphone, laptop or other personal computer, and a room with some sound blocking. Podcast creators tend to have a good listener base because of their relationships with the listeners.\n\n\n"}
{"id": "3925586", "url": "https://en.wikipedia.org/wiki?curid=3925586", "title": "ReFLEX", "text": "ReFLEX\n\nReFLEX is a wireless protocol developed by Motorola, used for two-way paging, messaging, and low-bandwidth data. It is based on the FLEX one-way paging protocol, adding capabilities for multiple forward channels, multiple return channels, and roaming. It originally came in two variants, ReFLEX25 and ReFLEX50. \"ReFLEX50\" was originally developed to support a messaging service launched by MTEL in the mid 1990s, while \"ReFLEX25\" was developed several years later to provide an upgrade path for traditional one-way paging carriers. The \"50\" and \"25\" signified \"50 kHz\" and \"25kHz\" channel spacing, although in reality both variants supported flexible channel configurations. The two variants were unified into a single protocol with version 2.7, which was released simply as ReFLEX 2.7. Devices compliant with \"ReFLEX 2.7\" are backwards compatible with both \"ReFLEX25\" and \"ReFLEX50\" networks, with several new features to improve roaming, performance, and interoperability between different networks. \"ReFLEX\" systems support forward channel speeds of 1600, 3200, and 6400 bits per second, and return channel speeds of 800, 1600, 6400, and 9600 bits per second. Like FLEX, \"ReFLEX\" is synchronous, based on 1.875 second frames and 4-level FSK modulation.\n\nThe Motorola PageWriter released in 1996 was one of the first devices to use the ReFLEX network protocol. Although ReFLEX now has limited viability in the commercial market, it is finding new uses in Automatic Meter Reading, public safety, and low cost/bandwidth M2M applications.\n\n\n"}
{"id": "4756732", "url": "https://en.wikipedia.org/wiki?curid=4756732", "title": "Reactionless drive", "text": "Reactionless drive\n\nA reactionless drive is a device producing motion without the exhaust of a propellant. A propellantless drive is not necessarily reactionless when it constitutes an open system interacting with external fields; but a reactionless drive is a particular case of a propellantless drive as it is a closed system presumably in contradiction with the law of conservation of momentum and often considered similar to a perpetual motion machine. The name comes from Newton's third law, which is usually expressed as, \"for every action, there is an equal and opposite reaction.\" A large number of infeasible devices, such as the Dean drive, are a staple of science fiction particularly for space propulsion.\n\nThrough the years there have been numerous claims for functional reactionless drive designs using ordinary mechanics (i.e. devices not said to be based on quantum mechanics, relativity or atomic forces or effects). Two of these represent their general classes: The \"Dean drive\" is perhaps the best known example of a \"linear oscillating mechanism\" reactionless drive; The \"GIT\" is perhaps the best known example of a \"rotating mechanism\" reactionless drive. These two also stand out as they both received much publicity from their promoters and the popular press in their day and both were eventually rejected when proven to not produce any reactionless drive forces. The rise and fall of these devices now serves as a cautionary tale for those making and reviewing similar claims.\n\nThe Dean drive was a mechanical device concept promoted by inventor Norman L. Dean. Dean claimed that his device was a \"reactionless thruster\" and that his working models could demonstrate this effect. He held several private demonstrations but never revealed the exact design of the models nor allowed independent analysis of them. Dean's claims of reactionless thrust generation were subsequently shown to be in error and the \"thrust\" producing the directional motion was likely to be caused by friction between the device and the surface on which the device was resting and would not work in free space.\n\nThe Gyroscopic Inertial Thruster is a proposed reactionless drive based on the mechanical principles of a rotating mechanism. The concept involves various methods of leverage applied against the supports of a large gyroscope. The supposed operating principle of a GIT is a mass traveling around a circular trajectory at a variable speed. The high-speed part of the trajectory allegedly generates greater centrifugal force than the low, so that there is a greater thrust in one direction than the other. Scottish inventor Sandy Kidd, a former RAF radar technician, investigated the possibility (without success) in the 1980s. He posited that a gyroscope set at various angles could provide a lifting force, defying gravity. In the 1990s, several people sent suggestions to the Space Exploration Outreach Program (SEOP) at NASA recommending that NASA study a gyroscopic inertial drive, especially the developments attributed to the American inventor Robert Cook and the Canadian inventor Roy Thornson. In the 1990s and 2000s, enthusiasts attempted the building and testing of GIT machines. Eric Laithwaite, the \"Father of Maglev\", received a US patent for his \"Propulsion System\", which was claimed to create a linear thrust through gyroscopic and inertial forces. After years of theoretical analysis and laboratory testing of actual devices, no rotating (or any other) mechanical device has ever been found to produce unidirectional reactionless thrust in free space.\n\nSeveral kinds of thrust generating methods are in use or have been proposed that are propellantless, as they do not work like rockets and reaction mass is not carried nor expelled from the device. However they are not reactionless, as they constitute open systems interacting with electromagnetic waves or various kinds of fields.\n\nMost famous propellantless methods are the gravity assist maneuver or gravitational slingshot of a spacecraft accelerating at the expense of the momentum of the planet it orbits, through the gravitational field, or beam-powered propulsion using the radiation pressure of electromagnetic waves from a distant source like a laser.\n\nMore speculative methods have also been proposed, like the Woodward effect, the quantum vacuum plasma thruster or various hypotheses trying to explain the thrust apparently produced by the EmDrive.\n\nBecause there is no well-defined \"center of mass\" in curved spacetime, general relativity allows a stationary object to, in a sense, \"change its position\" in a counter-intuitive manner, without violating conservation of momentum.\n\n\n\n\n"}
{"id": "47395801", "url": "https://en.wikipedia.org/wiki?curid=47395801", "title": "Readdle", "text": "Readdle\n\nReaddle is a Ukrainian mobile application development company. The company research and development is based in Odesa, Ukraine. The operation is mostly built around the App Store, cumulatively generating over 100 million downloads. The company's two main products are PDF Expert and Spark.\n"}
{"id": "10721277", "url": "https://en.wikipedia.org/wiki?curid=10721277", "title": "Redshift (theory)", "text": "Redshift (theory)\n\nRedshift is a techno-economic theory suggesting hypersegmentation of information technology markets based on whether individual computing needs are over or under-served by Moore's law, which predicts the doubling of computing transistors (and therefore roughly computing power) every two years. The theory,\nproposed and named by New Enterprise Associates partner and former Sun Microsystems CTO Greg Papadopoulos, categorized a series of high growth markets (redshifting) while predicting slower GDP-driven growth in traditional computing markets (blueshifting). Papadopoulos predicted the result will be a fundamental redesign of components comprising computing systems.\n\nAccording to the Redshift theory, applications \"redshift\" when they grow dramatically faster than Moore's Law allows, growing quickly in their absolute number of systems. In these markets, customers are running out of datacenter real-estate, power and cooling infrastructure. According to Dell Senior Vice President Brad Anderson, “Businesses requiring hyperscale computing environments – where infrastructure deployments are measured by up to millions of servers, storage and networking equipment – are changing the way they approach IT.”\n\nWhile various Redshift proponents offer minor alterations on the original presentation, “Redshifting” generally includes:\n\nThese are companies that drive heavy Internet traffic. This includes popular web-portals like Google, Yahoo, AOL and MSN. It also includes telecoms, multimedia, television over IP, online games like World of Warcraft and others. This segment has been enabled by widespread availability of high-bandwidth Internet connections to consumers through a DSL or cable modem. A simple way to understand this market is that for every byte of content served to a PC, mobile phone or other device over a network, there must exist computing systems to send it over the network.\n\nThese are companies that do complex simulations that involve (for example) weather, stock markets or drug-design simulations. This is a generally elastic market because businesses frequently spend every \"available\" dollar budgeted for IT. A common anecdote claims that cutting the cost of computing by half causes customers in this segment to buy at least twice as much, because each marginal IT dollar spent contributes to business advantage.\n\nThese are companies that aggregate traditional computing applications and offer them as services, typically in the form of Software as a Service (SaaS). For example, companies that deploy CRM are over-served by Moore's Law, but companies that aggregate CRM functions and offer them as a service, such as Salesforce.com, grow faster than Moore's Law.\n\nA prime example of redshift was a crisis at eBay. In 1999 eBay suffered a database crisis when a single Oracle Database running on the fastest Sun machine available (these tracking Moore's law in this period) was not enough to cope with eBay's growth. The solution was to massively parallelise their system architecture.\n\nRedshift theory suggests that traditional computing markets, such as those serving enterprise resource planning or customer relationship management applications, have reached relative saturation in industrialized nations. Thereafter, proponents argued further market growth will closely follow gross domestic product growth, which typically remains under 10% for most countries annually. Given that Moore's Law continues to predict accurately the rate of computing transistor growth, which roughly translates into computing power doubling every two years, the Redshift theory suggests that traditional computing markets will ultimately contract as a percentage of computing expenditures over time.\n\nFunctionally, this means “Blueshifting” customers can satisfy computing requirement growth by swapping in faster processors without increasing the absolute number of computing systems.\n\nPapadopoulos argued that while traditional computing markets remain the dominant source of revenue through the late 2000s, a shift to hypergrowth markets will inevitably occur. When that shift occurs, he argued computing (but not computers) will become a utility, and differentiation in\nthe IT market will be based upon a company's ability to deliver computing at massive scale, efficiently and with predictable service levels, much like electricity at that time.\n\nIf computing is to be delivered as a utility, Nicholas Carr suggested Papadopoulos' vision compares with Microsoft researcher Jim Hamilton, who both agree that computing is most efficiently generated in shipping containers. Industry analysts are also beginning to quantify Redshifting and Blueshifting markets. According to International Data Corporation vice president Matthew Eastwood, \"IDC believes that the IT market is in a period of hyper segmentation... This a class of customers that is Moore's law driven and as price performance gains continue, IDC believes that these organizations will accelerate their consumption of IT infrastructure.”\n\nKey portions of Papadopoulos' theory were first presented by Sun Microsystems CEO Jonathan Schwartz in late 2006. Papadopoulos later gave a full presentation on Redshift to Sun's annual Analyst Summit in February 2007. The term Redshift refers to what happens when electromagnetic radiation, usually visible light, moves away from an observer. Papadopoulos chose this term to reflect growth markets because redshift helped cosmologists explain the expansion of the universe.\n\nPapadopoulos originally depicted traditional IT markets as green to represent their revenue base, but later changed them to “blueshift,” which occurs when a light source moves toward an observer, similar to what would happen during a contraction of the universe.\n\n"}
{"id": "17989579", "url": "https://en.wikipedia.org/wiki?curid=17989579", "title": "Remote service software", "text": "Remote service software\n\nRemote service software is used by equipment manufacturers to remotely monitor, access and repair products in use at customer sites. It’s a secure, auditable gateway for service teams to troubleshoot problems, perform proactive maintenance, assist with user operations and monitor performance. This technology is typically implemented in mission-critical environments like hospitals or IT data centers – where equipment downtime is intolerable.\n\nRemote service software helps to:\n\nManufacturers are using aftermarket service a competitive differentiator. Remote service software provides a platform for manufacturers to offer and meet stringent service level agreements (SLAs) without increasing the size of their service team.\n\n\n\n"}
{"id": "25739", "url": "https://en.wikipedia.org/wiki?curid=25739", "title": "Rich Text Format", "text": "Rich Text Format\n\nThe Rich Text Format (often abbreviated RTF) is a proprietary document file format with published specification developed by Microsoft Corporation from 1987 until 2008 for cross-platform document interchange with Microsoft products. Prior to 2008, Microsoft published updated specifications for RTF with major revisions of Microsoft Word and Office versions.\n\nMost word processors are able to read and write some versions of RTF. There are several different revisions of RTF specification and portability of files will depend on what version of RTF is being used. \n\nIt should not be confused with enriched text (media type \"text/enriched\" of RFC 1896) or its predecessor Rich Text (media type \"text/richtext\" of RFC 1341 and ), nor with IBM's RFT-DCA (Revisable Format Text-Document Content Architecture); these are completely different specifications.\n\nRichard Brodie, Charles Simonyi, and David Luebbert, members of the Microsoft Word development team, developed the original RTF in the middle to late 1980s. Its syntax was influenced by the TeX typesetting language. The first RTF reader and writer shipped in 1987 as part of Microsoft Word 3.0 for Macintosh, which implemented the RTF version 1.0 specification. All subsequent releases of Microsoft Word for the Macintosh and all versions for Windows can read and write files in RTF format.\n\nMicrosoft maintains the format. The final version was 1.9.1 in 2008, implementing features of Office 2007. Microsoft has discontinued enhancements to the RTF specification. New features in Word 2010 and later versions will not save properly to the RTF format. Microsoft anticipates no further updates to RTF, but has stated willingness to consider editorial and other non-substantive modifications of the RTF Specification during an associated ISO/IEC 29500 balloting period.\n\nFor some time, RTF files were used to produce Windows .HLP help files, though this use has been superseded by Microsoft Compiled HTML Help files.\n\nRTF is programmed by using groups, a backslash, a control word and a delimiter. Groups are contained within braces ({}), with the opening brace and closing brace indicating the start of the group and end of the group respectively. Groups are used to indicate what type of attributes to apply to certain text. The backslash (\\) indicates that a control word is going to be used. Control words are specifically programmed commands for RTF. They can have certain states in which they're active. Their state is represented by a number. For example,\n\n\nA delimiter is one of three things:\n\n\nAs an example, the following RTF code:\n\nis a document which would be rendered like this when read by a program that supports RTF:\nThis is some bold text.\n\nA standard RTF file can consist of only 7-bit ASCII characters, but can encode characters beyond ASCII by escape sequences. The character escapes are of two types: code page escapes and, starting with RTF 1.5, Unicode escapes. In a code page escape, two hexadecimal digits following a backslash and typewriter apostrophe are used for denoting a character taken from a Windows code page. For example, if the code page is set to Windows-1256, the sequence codice_1 will encode the Arabic letter bāʼ (ب).\n\nFor a Unicode escape the control word codice_2 is used, followed by a 16-bit signed decimal integer giving the Unicode UTF-16 code unit number. For the benefit of programs without Unicode support, this must be followed by the nearest representation of this character in the specified code page. For example, codice_3 would give the Arabic letter \"bāʼ\" ب, specifying that older programs which do not have Unicode support should render it as a question mark instead.\n\nThe control word codice_4 can be used to indicate that subsequent Unicode escape sequences within the current group do not specify the substitution character.\n\nUntil RTF specification version 1.5 release in 1997, RTF has only handled 7-bit characters directly and 8-bit characters encoded as hexadecimal (using codice_5). RTF control words (since RTF 1.5) generally accept signed 16-bit numbers as arguments. Unicode values greater than 32767 must be expressed as negative numbers. If a Unicode character is outside BMP, it is encoded with a surrogate pair. Support for Unicode was made due to text handling changes in Microsoft Word – Microsoft Word 97 is a partially Unicode-enabled application and it handles text using the 16-bit Unicode character encoding scheme. Microsoft Word 2000 and later versions are Unicode-enabled applications that handle text using the 16-bit Unicode character encoding scheme.\n\nRTF files are usually 7-bit ASCII plain text. RTF consists of control words, control symbols, and groups. RTF files can be easily transmitted between PC based operating systems because they are encoded as a text file with 7-bit graphic ASCII characters. Converters that communicate with Microsoft Word for MS Windows or Macintosh should expect data transfer as 8-bit characters and binary data can contain any 8-bit values.\n\nRTF is a data format for saving and sharing documents, not a markup language; it is not intended for intuitive and easy typing by a person. Nonetheless, unlike many word processing formats, RTF code can be human-readable: when an RTF file containing mostly latin characters without diacritics is viewed as a plain text file, the underlying ASCII text is readable, provided that the author has kept formatting concise – otherwise, the formatting code can impede readability.\n\nWhen RTF was released, most word processors used binary file formats (Microsoft Word used the .doc file format); RTF was unique in its simple formatting control which allows for a non-RTF aware program (e.g. Notepad) to open and provide a readable file. Today, the majority of these programs have changed to a XML-based file format (Word has switched to the .docx file format). Regardless, these files contain large amounts of formatting code. As such, they are ten or more times larger than the corresponding plain text. \n\nTo be standard-compliant RTF, non-ASCII characters must be escaped. Thus, even with concise formatting, text that uses certain dashes and quotation marks is less legible. Latin languages that make heavy use of characters with diacritics, such as \\'f1 for ñ and \\'e9 for é are particularly difficult to read in RTF. Non-Latin scripts, consisting of characters such as \\u21563 for 吻, are illegible in RTF. In addition, from its beginnings, RTF has supported Microsoft OLE embedded objects and Macintosh Edition Manager subscriber objects, which are not human-readable.\n\nMost word processing software supports RTF format importing and exporting (following some version of RTF specification), and/or direct editing, often making it a \"common\" format between otherwise incompatible word processing software and operating systems. These factors contribute to its interoperability, but it will depend on what version of RTF is being used. There are several consciously designed or accidentally born RTF dialects. Most applications that read RTF files silently ignore unknown RTF control words.\n\nRTF is the internal markup language used by Microsoft Word. Overall, since 1987, RTF files may be transferred back and forth between many old and new computer systems (and now over the Internet) despite differences between operating systems and their versions. (But there are incompatibilities, e.g. between RTF 1.0 1987 and later specifications, or between RTF 1.0-1.4 and RTF 1.5+ in use of Unicode characters.) This makes it a useful format for basic formatted text documents such as instruction manuals, résumés, letters, and modest information documents. These documents at minimum support bold, italic, and underline text formatting. Also typically supported are left-, center-, and right-aligned text, font specification and document margins.\n\nFont and margin defaults, as well as style presets and other functions vary according to program defaults. There may also be subtle differences perhaps between different versions of the RTF specification implemented in differing programs and program versions. Nevertheless, the RTF format is consistent enough from computer to computer to be considered highly portable and acceptable for cross-platform use. The format supports metadata such as title, author, etc. but not all implementations support this.\n\nUse of Microsoft Object Linking and Embedding (OLE) objects or Macintosh Edition Manager subscriber objects limits the interoperability, because these objects are not widely supported in programs for viewing or editing RTF files (e.g. embedding of other files inside the RTF, such as tables or charts from spreadsheet application). If a software that understands an OLE object is not available, the object is usually replaced by a picture (bitmap representation of the object) or not displayed at all.\n\nRTF supports inclusion of JPEG, Portable Network Graphics (PNG), Enhanced Metafile (EMF), Windows Metafile (WMF), Apple PICT, Windows Device-dependent bitmap, Windows Device Independent bitmap and OS/2 Metafile picture types in hexadecimal (the default) or binary format in a RTF file. Not all of these picture types are supported in all RTF readers. When a RTF document is opened in software that does not support the picture type of an inserted picture, such picture is not displayed at all.\n\nRTF writers usually convert inserted pictures from an unsupported picture types (e.g. BMP, TIFF, GIF, etc.) to one of supported picture types (PNG, WMF) or they do not include pictures at all.\n\nFor better compatibility with Microsoft products, some RTF writers include the same picture in two different picture types in one RTF file:\n\nThis method increases the RTF file size rapidly. The RTF specification does not require this method and there are various implementations that include pictures without the WMF copy (e.g. Abiword or Ted).\n\nFor Microsoft Word it is also possible to set a specific registry value (\"ExportPictureWithMetafile=0\") in order to prevent Word from saving the WMF copy (see link \"Document file size increases with EMF, PNG, GIF, or JPEG graphics in Word\" at the beginning).\n\nRTF supports embedding of fonts used in the document, but this feature is not widely supported in software implementations.\n\nRTF also supports generic font family names used for font substitution: \"roman\" (serif), \"swiss\" (sans-serif), \"modern\" (monospace), \"script\", \"decorative\", \"technical\". This feature is not widely supported for font substitution, e.g. in OpenOffice.org or Abiword.\n\nRTF specification supports annotations (comments in documents) since version 1.0. RTF 1.7 specification defined some new features for annotations: date stamp (there was previously only \"time stamp\") and parents of annotations. When a RTF document with annotations is opened in an application that does not support RTF annotations, they are not displayed at all. Similarly, when a document with annotations is saved as RTF in an application that does not support RTF annotations, annotations are not preserved in the RTF file. Some implementations may hide annotations by default or require some user action to display them – e.g. in Abiword since version 2.8 or in IBM Lotus Symphony (up to version 1.3).\n\nMicrosoft products do not support comments within footers, footnotes or headers. Inserting a comment within headers, footers, or footnotes may result in a corrupted RTF document.\n\nThe RTF specification also supports footnotes (not to be confused with annotations), which are widely supported in RTF implementations (e.g. in OpenOffice.org, Abiword, KWord, Ted, but not in Wordpad). Endnotes are implemented as a variation on footnotes such that applications that support footnotes and not endnotes will render endnotes in an RTF document as footnotes. Similar to annotations, due to Microsoft products not supporting footnotes in headers, footers, or comments, including footnotes within those contexts in an RTF document may result in a corrupted document.\n\nRTF 1.2 specification defined use of drawing objects such as rectangles, ellipses, lines, arrows, polygons and various other shapes. RTF 1.5 specification introduced many new control words for drawing objects. RTF drawing objects are also called \"shapes\" since RTF 1.5.\n\nHowever, RTF drawing objects are not supported in many RTF implementations, such as Apache OpenOffice (though they are supported in LibreOffice 4.0 on) or Abiword. When an RTF document with drawing objects is opened in an application that does not support RTF drawing objects, they are not displayed at all. Some implementations will also not display any text inside drawing objects. Similarly, when a document with drawing objects is saved as RTF in an application that does not support RTF drawing objects, these are not preserved in the RTF file.\n\nUnlike Microsoft Word's DOC format, as well as the newer Office Open XML and OpenDocument formats, RTF does not support macros. For this reason, RTF was often recommended over those formats when the spread of computer viruses through macros was a concern. However, having the .RTF extension does not guarantee that a file is safe, since Microsoft Word will open standard DOC files renamed with an RTF extension and run any contained macros as usual. Manual examination of a file in a plain text editor such as Notepad, or use of the codice_6 command in UNIX-like systems, is required to determine whether or not a suspect file is really RTF. Enabling Word's \"Confirm file format conversion on open\" option (not enabled by default in any version of Word) can also assist by warning a document being opened is in a format that does not match the format implied by the file's extension, and giving the option to abort opening that file.\n\nRTF files can carry malware; sometimes malicious files in RTF format are renamed with the .DOC extension. One exploit attacking a vulnerability was patched in Microsoft Word in April 2015.\n\nSince 2014 there have been malware RTF files embedding OpenXML exploits (.DOCX file with ZIP header, renamed with RTF extension) \"to create a multi-exploit master key to cover a number of recent patched exploits in one RTF with low AV detection\". Documents of this and other types can be analysed free of charge online with a tool called Cryptam.\n\nEach RTF implementation usually implements only some versions or subsets of the RTF specification. Many of the available RTF converters cannot understand all new features in the latest RTF specifications.\n\nThe WordPad editor in Microsoft Windows creates RTF files by default. It once defaulted to the Microsoft Word 6.0 file format, but write support for Word documents (.doc) was dropped in a security update. Read support was also dropped in Windows 7. WordPad does not support some RTF features, such as headers and footers. However, WordPad can read and save many RTF features that it cannot create such as: tables, strikeout, superscript, subscript, \"extra\" colors, text background colors, numbered lists, right or left indent, quasi-hypertext and URL linking, and various line spacings. RTF is also the data format for \"rich text controls\" in MS Windows APIs.\n\nThe default text editor for Mac OS X, TextEdit, can also view, edit and save RTF files as well as RTFD files. TextEdit currently (as of July 2009) has limited ability to edit RTF document margins. Much older Mac word processing application programs such as MacWrite and WriteNow were able to view, edit, and save RTF files as well.\n\nThe free and open-source word processors AbiWord, Apache OpenOffice, Bean, Calligra, KWord, LibreOffice and NeoOffice can view, edit and save RTF files. RTF format is also used in the Ted word processor.\n\nScrivener uses individual RTF files for all the text files that make up a given \"project\".\n\nSIL International’s Toolbox freeware application for developing and publishing dictionaries uses RTF as its most common form of document output. RTF files produced by Toolbox are designed to be used in Microsoft Word, but can also be used by other RTF-aware word processors.\n\nRTF can be used on some ebook readers because of its interoperability, simplicity, and low CPU processing requirements.\n\nThe open-source script rtf2xml can partially convert RTF to XML.\n\nGNU UnRTF is an open-source program to convert RTF into HTML, LaTeX, troff macros and other formats. pyth is a Python library to create and convert documents in RTF, XHTML and PDF format. Ruby RTF is a project to create Rich Text content via Ruby. RaTFink is a library of Tcl routines, free software, to generate RTF output, and a Cost script to convert SGML to RTF. RTF::Writer is a Perl module for generating RTF documents. PHPRtfLite is an API enabling developers to create RTF documents with PHP. Pandoc is an open source document converter with multiple output formats, including RTF. RTFGen is a project to create RTF documents via pure PHP. rtf.js is a JavaScript based library to render RTF documents in HTML.\n\nThe Mac OS X command line tool textutil enables files to be converted between rtf, rtfd, text, doc, docx, wordml, odt, and webarchive.\n\nThe Rich Text Format was the standard file format for text-based documents in applications developed for Microsoft Windows. Microsoft did not initially make the RTF specification publicly available, making it difficult for competitors to develop document conversion features in their applications. Because Microsoft's developers had access to the specification, Microsoft's applications had better compatibility with the format. Also, every time Microsoft changed the RTF specification, Microsoft's own applications had a lead in time-to-market, because competitors had to redevelop their applications after studying the newer version of the format.\n\nNovell alleged that Microsoft's practices were anticompetitive in its 2004 antitrust complaint against Microsoft. The RTF specifications lack some of the semantic definitions necessary to read, write, and modify documents.\n\n\n"}
{"id": "45360866", "url": "https://en.wikipedia.org/wiki?curid=45360866", "title": "Sociotechnology", "text": "Sociotechnology\n\nSociotechnology (short for \"social technology\") is the study of processes on the intersection of society and technology. Vojinović and Abbott define it as \"the study of processes in which the social and the technical are indivisibly combined\".\nSociotechnology is an important part of socio-technical design, which is defined as \"designing things that participate in complex systems that have both social and technical aspects\".\n\nThe term has been attributed to Mario Bunge. He defines it as a grouping of social engineering and management science. He sees it thus as a form of technology, distinguished from other branches of it such as engineering, biotechnology, information technology and general technology. Its goal is to help engineer sociosystems and evaluate their performance, while making use of social science research. In short, sociotechnology can be seen as the creation, modification and maintenance of social systems.\n\nWriting on sociotechnical change, Bijker wrote: \"Society is not determined by technology, nor is technology determined by society. Both emerge as two sides of the sociotechnical coin.\"\n\nTechnology is the sum of ways in which social groups construct the material objects of their civilizations. The things made are socially constructed just as much as technically constructed. The merging of these two things, construction and insight, is sociotechnology. \"For example, we typically build a bridge when there’s some expectation that people need to get from Point A to Point B, and there’s something they need to bypass along the way (e.g. a river, a canyon, another road). Failure to consider the social factors as well as the technical factors could lead to a \"bridge to nowhere\" – and we all know at least one person who’s had a problem with those\".\n\nBusiness use \"richness and reach\". The richness and reach strategy has evolved with technology. Richness is the ability to understand the information being passed for example calling someone is less rich than face to face contact. Reach is the number of people who exchange the information. In the past it was easier to complete richness and reach, but now with new technologies like video chatting, it is easier for business to do fulfill both richness and reach. Positive economics is the study of existing (or historical) means of exchange- a social science such as sociology, history, and political sciences. Normative economics is the social technology because it attempts to create different kinds of economic arrangements.\n\nAccelerating growth of technology is a major problem and cause of destabilization of a communicational world. Paul Virilio believes that the \"real\" is being mistaken by virtual and the virtual world destroys physical presence. Marshall McLuhan wrote about the extension of human senses and the nervous system into the world through electronic media. Essentially he believes that the mind, the self, and consciousness are made from already created technology, media, and language as opposed to create naturally like those who made technology, media, and language. The image of one’s self becomes aware of itself in a world of technology. Consciousness and desire become less individualistic and turns into more already constituted social forms. The response to how someone feels something, what it means, and how it feels (for example feeling \"attached\" to someone after losing your virginity to them) is already set up through media communication (show, music, movies, articles). People will anticipate that they will be attached because of what they already know about it, so when it happens they do feel attached because that is how they were programmed to feel. People have turned to technology to create their \"self\" and determine how they feel and act. The conscious mind does not move into a world-as-other mindset but into an already constituted world.\n\nLewis Mumford believed the world of relying on technology began with early human experiments in industrialization including coal mines because coal and iron built and powered industrialization. It has come to the point where communication is relied on technology. Conversation turned into talking through a machine (texting, phone calls, social media).\n\nThe difference between the past and present technology is the extent of social binding. The more social forces it binds together the more powerful the technology is. For example, the attack on the World Trade Center as opposed to the attack of Pearl Harbor. It took a year to see the film of the attack on the American battleships. Because of this, people were more unaware and it was less of a big deal than what 9-11 was. During the terrorist attack in 2001, everything was broadcast in the moment. People were seeing the destruction with their own eyes in the moment, causing more ridicule and passion for the situation.\n\nFurbie and Tamagotchi are electronic toys made for children that make noises and demand for care. These toys caused children to form a companionship with robots instead of human beings. Paro is a robotic baby seal that was created in Japan to perform therapeutic functions. Paro had positive psychological results for those in nursing homes. A woman constantly argued with her son and turned to the Paro for comfort and confided in the Paro instead of fixing things with her son. Society has turned to demanding more intimacy from sociable robots then from each other. People that use the robots for comfort feel like they are being comforted by someone even though they are actually alone. These robotic characters cause people to have a relationship with technology while avoiding the trouble of human interaction.\n\nScience and technology are big contributions to the economic development, but can also lead to negative side effects as it evolves. For example, people care more about materialistic things than the negative influences they have created in human morals and education. For example, it is more important for people to know words to songs than to remember things for their test. There are other problems due to the development of new technologies such as the conservation of the environment because there is a demand for more products which leads to more destruction of environment to build more to sell from. Many social problems have appeared in society and cannot seem to be solved by natural science and technology alone. It requires the need for social sciences as well. Social technology is the strategy used to help solve the wrong behaviors in the world that are caused by social problems like how to solve the issue of people being invested in materialistic goods more than morals, so that they economy can still continue to grow, and society can be a better place.\n\n\n"}
{"id": "266344", "url": "https://en.wikipedia.org/wiki?curid=266344", "title": "Space debris", "text": "Space debris\n\nInitially, the term space debris referred to the natural debris found in the solar system: asteroids, comets, and meteoroids. However, with the 1979 beginning of the NASA Orbital Debris Program, the term also refers to the debris (alt. space waste or space garbage) from the mass of defunct, artificially created objects in space, especially Earth orbit. These include old satellites and spent rocket stages, as well as the fragments from their disintegration and collisions. \n\nAs of December 2016, five satellite collisions have generated space debris. Space debris is also known as \"orbital debris\", \"space junk\", \"space waste\", \"space trash\", \"space litter\" or \"space garbage\".\n, the United States Strategic Command tracked a total of 17,852 artificial objects in orbit above the Earth, including 1,419 operational satellites. However, these are just objects large enough to be tracked. , more than 170 million bits of debris smaller than , about 670,000 pieces of debris 1–10 cm, and around 29,000 larger pieces were estimated to be in orbit around the earth. Collisions with debris have become a hazard to spacecraft; they cause damage akin to sandblasting, especially to solar panels and optics like telescopes or star trackers that cannot be covered with a ballistic Whipple shield (unless it is transparent).\n\nBelow Earth-altitude, pieces of debris are denser than meteoroids; most are dust from solid rocket motors, surface erosion debris like paint flakes, and frozen coolant from RORSAT (nuclear-powered satellites). \nFor comparison, the International Space Station orbits in the range, and the 2009 satellite collision and 2007 antisat test occurred at altitude. The ISS has Whipple shielding; however, known debris with a collision chance over 1/10,000 are avoided by maneuvering the station.\n\nThe Kessler syndrome, a runaway chain reaction of collisions exponentially increasing the amount of debris, has been hypothesized to ensue beyond a critical density. This could affect useful polar-orbiting bands, increases the cost of protection for spacecraft missions and could destroy live satellites. Whether Kessler syndrome is already underway has been debated. The measurement, mitigation, and potential removal of debris are conducted by some participants in the space industry.\n\nThere are estimated to be over 51 million pieces of debris smaller than as of July 2013. There are approximately 670,000 pieces from one to ten cm. The current count of large debris (defined as 10 cm across or larger) is 29,000. The technical measurement cutoff is c. . Over 98 percent of the 1,900 tons of debris in low Earth orbit (as of 2002) was accounted for by about 1,500 objects, each over . Total mass is mostly constant despite addition of many smaller objects, since they reenter the atmosphere sooner. Using a 2008 figure of 8,500 known items, it is estimated at .\n\nIn LEO there are few \"universal orbits\" which keep spacecraft in particular rings (in contrast to GEO, a single widely used orbit). The closest are sun-synchronous orbits that keep a constant angle between the Sun and the orbital plane; they are polar, meaning they cross over the polar regions. LEO satellites orbit in many planes, up to 15 times a day, causing frequent approaches between objects (the density of objects is much higher in LEO).\n\nOrbits are further changed by perturbations (which in LEO include unevenness of the Earth's gravitational field), and collisions can occur from any direction. For these reasons, the Kessler syndrome applies mostly to the LEO region; impacts occur at up to 16 km/s (twice the orbital speed) if head-on – the 2009 satellite collision occurred at 11.7 km/s, creating much spall in the critical size range. These can cross other orbits and lead to a cascade effect. A large-enough collision (e.g. between a space station and a defunct satellite) could make low Earth orbit impassable.\n\nManned missions are mostly at and below, where air drag helps clear zones of fragments. Atmospheric expansion as a result of space weather raises the critical altitude by increasing drag; in the 90s, it was a factor in reduced debris density. Another was fewer launches by Russia; the USSR made most of their launches in the 1970s and 1980s.\n\nAt higher altitudes, where air drag is less significant, orbital decay takes longer. Slight atmospheric drag, lunar perturbations, Earth's gravity perturbations, solar wind and solar radiation pressure can gradually bring debris down to lower altitudes (where it decays), but at very high altitudes this may take millennia. Although high-altitude orbits are less commonly used than LEO and the onset of the problem is slower, the numbers progress toward the critical threshold more quickly.\n\nMany communications satellites are in geostationary orbits (GEO), clustering over specific targets and sharing the same orbital path. Although velocities are low between GEO objects, when a satellite becomes derelict (such as Telstar 401) it assumes a geosynchronous orbit; its orbital inclination increases about .8° and its speed increases about per year. Impact velocity peaks at about . Orbital perturbations cause longitude drift of the inoperable spacecraft and precession of the orbital plane. Close approaches (within 50 meters) are estimated at one per year. The collision debris pose less short-term risk than from an LEO collision, but the satellite would likely become inoperable. Large objects, such as solar-power satellites, are especially vulnerable to collisions.\n\nAlthough the ITU now requires proof a satellite can be moved out of its orbital slot at the end of its lifespan, studies suggest this is insufficient. Since GEO orbit is too distant to accurately measure objects under , the nature of the problem is not well known. Satellites could be moved to empty spots in GEO, requiring less maneuvring and making it easier to predict future motion. Satellites or boosters in other orbits, especially stranded in geostationary transfer orbit, are an additional concern due to their typically high crossing velocity.\n\nDespite efforts to reduce risk, spacecraft collisions have occurred. The European Space Agency telecom satellite Olympus-1 was struck by a meteoroid on 11 August 1993 and eventually moved to a graveyard orbit. On 29 March 2006, the Russian Express-AM11 communications satellite was struck by an unknown object and rendered inoperable; its engineers had enough contact time with the satellite to send it into a graveyard orbit.\n\nIn 1958, the United States launched Vanguard I into a medium Earth orbit (MEO). , it, and the upper stage of its launch rocket, are the oldest surviving man-made space objects still in orbit. In a catalog of known launches until July 2009, the Union of Concerned Scientists listed 902 operational satellites from a known population of 19,000 large objects and about 30,000 objects launched.\n\nAn example of additional dead satellite debris are the remains of the 1970s/80s Soviet RORSAT naval surveillance satellite program. The satellite's BES-5 nuclear reactor were cooled with a coolant loop of sodium-potassium alloy, creating a potential problem when the satellite reached end of life. While many satellites were nominally boosted into medium-altitude graveyard orbits, not all were. Even satellites which had been properly moved to a higher orbit had an eight-percent probability of puncture and coolant release over a 50-year period. The coolant freezes into droplets of solid sodium-potassium alloy, forming additional debris.\n\nThese events continue to occur. For example, in February 2015, the USAF Defense Meteorological Satellite Program Flight 13 (DMSP-F13) exploded on orbit, creating at least 149 debris objects, which were expected to remain in orbit for decades.\n\nAccording to Edward Tufte's book \"Envisioning Information\", space debris includes a glove lost by astronaut Ed White on the first American space-walk (EVA); a camera lost by Michael Collins near Gemini 10; a thermal blanket lost during STS-88; garbage bags jettisoned by Soviet cosmonauts during Mir's 15-year life, a wrench and a toothbrush. Sunita Williams of STS-116 lost a camera during an EVA. During an STS-120 EVA to reinforce a torn solar panel, a pair of pliers was lost, and in an STS-126 EVA, Heidemarie Stefanyshyn-Piper lost a briefcase-sized tool bag.\n\nIn characterizing the problem of space debris, it was learned that much debris was due to rocket upper stages (e.g. the Inertial Upper Stage) which end up in orbit, and break up due to decomposition of unvented unburned fuel. However, a major known impact event involved an (intact) Ariane booster. Although NASA and the United States Air Force now require upper-stage passivation, other launchers do not.\nLower stages, like the Space Shuttle's solid rocket boosters or Apollo program's Saturn IB launch vehicles, do not reach orbit.\n\nOn 11 March 2000 a Chinese Long March 4 CBERS-1 upper stage exploded in orbit, creating a debris cloud.\nA Russian Briz-M booster stage exploded in orbit over South Australia on 19 February 2007. Launched on 28 February 2006 carrying an Arabsat-4A communications satellite, it malfunctioned before it could use up its propellant. Although the explosion was captured on film by astronomers, due to the orbit path the debris cloud has been difficult to measure with radar. By 21 February 2007, over 1,000 fragments were identified. A 14 February 2007 breakup was recorded by Celestrak. Eight breakups occurred in 2006, the most since 1993. Another Briz-M broke up on 16 October 2012 after a failed 6 August Proton-M launch. The amount and size of the debris was unknown. A Long March 7 rocket booster created a fireball visible from portions of Utah, Nevada, Colorado, Idaho and California on the evening of 27 July 2016; its disintegration was widely reported on social media.\n\nA past debris source was the testing of anti-satellite weapons (ASATs) by the U.S. and Soviet Union during the 1960s and 1970s. North American Aerospace Defense Command (NORAD) files only contained data for Soviet tests, and debris from U.S. tests were only identified later. By the time the debris problem was understood, widespread ASAT testing had ended; the U.S. Program 437 was shut down in 1975.\n\nThe U.S. restarted their ASAT programs in the 1980s with the Vought ASM-135 ASAT. A 1985 test destroyed a satellite orbiting at , creating thousands of debris larger than . Due to the altitude, atmospheric drag decayed the orbit of most debris within a decade. A \"de facto\" moratorium followed the test.\n\nChina's government was condemned for the military implications and the amount of debris from the 2007 anti-satellite missile test, the largest single space debris incident in history (creating over 2,300 pieces golf-ball size or larger, over 35,000 or larger, and one million pieces or larger). The target satellite orbited between and , the portion of near-Earth space most densely populated with satellites. Since atmospheric drag is low at that altitude the debris is slow to return to Earth, and in June 2007 NASA's Terra environmental spacecraft maneuvered to avoid impact from the debris.\n\nOn 20 February 2008, the U.S. launched an SM-3 missile from the USS \"Lake Erie\" to destroy a defective U.S. spy satellite thought to be carrying of toxic hydrazine propellant. The event occurred at about , and the resulting debris has a perigee of or lower. The missile was aimed to minimize the amount of debris, which (according to Pentagon Strategic Command chief Kevin Chilton) had decayed by early 2009.\n\nThe vulnerability of satellites to debris and the possibility of attacking LEO satellites to create debris clouds, has triggered speculation that it is possible for countries unable to make a precision attack. An attack on a satellite of 10 tonnes or more would heavily damage the LEO environment.\n\nSpace junk is a threat to active satellites and spaceships. The Earth's orbit may even become impassable as the risk of collision grows too high.\n\nAlthough spacecraft are protected by Whipple shields, solar panels, which are exposed to the Sun, wear from low-mass impacts. These produce a cloud of plasma which is an electrical risk to the panels.\n\nSatellites are believed to have been destroyed by micrometeorites and orbital debris (MMOD). The earliest suspected loss was of Kosmos 1275, which disappeared on 24 July 1981 (a month after launch). Kosmos contained no volatile propellant, therefore, there appeared to be nothing internal to the satellite which could have caused the destructive explosion which took place. However the case has not been proven and another hypothesis forwarded is that the battery exploded. Tracking showed it broke up, into 300 new objects.\n\nMany impacts have been confirmed since. Olympus-1 was struck by a meteoroid on 11 August 1993, and left adrift. On 24 July 1996, the French microsatellite Cerise was hit by fragments of an Ariane-1 H-10 upper-stage booster which exploded in November 1986. On 29 March 2006, the Russian Ekspress AM11 communications satellite was struck by an unknown object and rendered inoperable; its engineers had sufficient time in contact with the spacecraft to send it to a parking orbit out of GEO. On October 13, 2009, Terra suffered a single battery cell failure anomaly and a battery heater control anomaly which were likely the result of an MMOD strike. On March 12, 2010, Aura lost power from one-half of one of its 11 solar panels and this was also attributed to an MMOD strike. On May 22, 2013 GOES-13 was hit by an MMOD which caused it to lose track of the stars that it uses to maintain attitude. It took nearly a month for the spacecraft to return to operation.\n\nThe first major satellite collision occurred on 10 February 2009 at 16:56 UTC. The deactivated Kosmos 2251 and the operational Iridium 33 collided, over northern Siberia. The relative speed of impact was about , or about . Both satellites were destroyed, with accurate estimates of the number of debris unavailable. On 22 January 2013 BLITS (a Russian laser-ranging satellite) was struck by debris suspected to be from the 2007 Chinese anti-satellite missile test, changing its orbit and spin rate.\n\nSatellites frequently have to perform Collision Avoidance Maneuvers and managers have to monitor debris as part of maneuver planning. For example, in January 2017, the European Space Agency planned to alter orbit of one of its $319 million Swarm mission spacecrafts, based on data from the US Joint Space Operations Center, to end the risk of collision from Cosmos-375, an old Russian satellite. Cosmos-375, which was destroyed by Soviet operators when its mission was complete, had previously threatened to impact the International Space Station in 2011.\n\nFrom the early Space Shuttle missions, NASA used NORAD to monitor the Shuttle's orbital path for debris. In the 1980s, this used much of its capacity. The first collision-avoidance maneuver occurred during STS-48 in September 1991, a seven-second thruster burn to avoid debris from Kosmos 955. Similar maneuvers followed on missions 53, 72 and 82.\n\nOne of the first events to publicize the debris problem occurred on \"Challenger\"'s second flight, STS-7. A fleck of paint struck its front window, creating a pit over wide. On STS-59 in 1994, \"Endeavour\"'s front window was pitted about half its depth. Minor debris impacts increased from 1998.\n\nWindow chipping and minor damage to thermal protection system tiles (TPS) was already common by the 1990s. The Shuttle was later flown tail-first to take the debris load mostly on the engines and rear cargo bay (not used in orbit or during descent, and less critical for post-launch operation). When flying to the ISS, the two connected spacecraft were flipped around so the better-armored station shielded the orbiter.\nNASA's study concluded that debris accounted for half of the overall risk to the Shuttle. Executive-level decision to proceed was required if catastrophic impact was likelier than 1 in 200. On a normal (low-orbit) mission to the ISS the risk was c. 1 in 300, but STS-125 (the Hubble repair mission) at was initially calculated at a 1-in-185 risk (due to the 2009 satellite collision). A re-analysis with better debris numbers reduced the estimated risk to 1 in 221, and the mission went ahead.\n\nDebris incidents continued on later Shuttle missions. During STS-115 in 2006 a fragment of circuit board bored a small hole through the radiator panels in \"Atlantis\"' cargo bay. On STS-118 in 2007 debris blew a bullet-like hole through \"Endeavour\"s radiator panel.\n\nImpact wear was notable on Mir, the Soviet space station, since it remained in space for long periods with its original module panels.\n\nAlthough the ISS uses Whipple shielding to protect itself from minor debris, portions (notably its solar panels) cannot be protected easily. In 1989, the ISS panels were predicted to degrade c. 0.23% in four years, and they were overdesigned by 1%. A maneuver is performed if \"there is a greater than one-in-10,000 chance of a debris strike\". , there have been sixteen maneuvers in the fifteen years the ISS had been in orbit.\n\nThe crew sheltered in the Soyuz on three occasions due to late debris-proximity warnings. In addition to the sixteen firings and three Soyuz-capsule shelter orders, one attempted maneuver failed (due to not having the several days' warning necessary to upload the manoeuvre timeline to the station's computer). A March 2009 close call involved debris believed to be a piece of the Kosmos 1275 satellite. In 2013, the ISS did not maneuver to avoid debris, after a record four debris maneuvers the previous year.\n\nAlthough most manned space activity takes place at altitudes below , a Kessler syndrome cascade in that region would rain down into lower altitudes and the decay time scale is such that \"the resulting [low Earth orbit] debris environment is likely to be too hostile for future space use\".\n\nIn a Kessler syndrome, satellite lifetimes would be measured in years or months. New satellites could be launched through the debris field into higher orbits or placed in lower orbits (where decay removes the debris), but the utility of the region between is the reason for its amount of debris.\n\nAlthough most debris burns up in the atmosphere, larger objects can reach the ground intact. According to NASA, an average of one cataloged piece of debris has fallen back to Earth each day for the past 50 years. Despite their size, there has been no significant property damage from the debris.\n\nIn 1969 five sailors on a Japanese ship were injured by space debris. In 1997 an Oklahoma woman, Lottie Williams, was injured when she was hit in the shoulder by a piece of blackened, woven metallic material confirmed as part of the propellant tank of a Delta II rocket which launched a U.S. Air Force satellite the year before.\n\nThe original re-entry plan for Skylab called for the station to remain in space for eight to ten years after its final mission in February 1974. High solar activity expanded the upper atmosphere, resulting in higher-than-expected drag and bringing its orbit closer to Earth than planned. On 11 July 1979 Skylab re-entered the Earth's atmosphere and disintegrated, raining debris along a path over the southern Indian Ocean and Western Australia.\n\nOn 12 January 2001, a Star 48 Payload Assist Module (PAM-D) rocket upper stage re-entered the atmosphere after a \"catastrophic orbital decay\", crashing in the Saudi Arabian desert. It was identified as the upper-stage rocket for NAVSTAR 32, a GPS satellite launched in 1993.\n\nIn the 2003 \"Columbia\" disaster, large parts of the spacecraft reached the ground and entire equipment systems remained intact. More than 83,000 pieces, along with the remains of the six astronauts, were recovered in an area from three to 10 miles around Hemphill in Sabine County, TX. More pieces were found in a line from west Texas to east Louisiana, with the westernmost piece found in Littlefield, TX and the easternmost found southwest of Mora, LA. Although there is significant evidence that debris fell in Nevada, Utah, and New Mexico, debris was only found in Texas, Arkansas and Louisiana. In a rare case of property damage, a foot-long metal bracket smashed through the roof of a dentist office. NASA warned the public to avoid contact with the debris because of the possible presence of hazardous chemicals. 15 years after the failure, people were still sending in pieces with the last,as of February 1, 2018, found in the spring of 2017.\n\nOn 27 March 2007, airborne debris from a Russian spy satellite was seen by the pilot of a LAN Airlines Airbus A340 carrying 270 passengers whilst flying over the Pacific Ocean between Santiago and Auckland. The debris was within of the aircraft.\n\nRadar and optical detectors such as lidar are the main tools for tracking space debris. Although objects under have reduced orbital stability, debris as small as 1 cm can be tracked, however determining orbits to allow re-acquisition is difficult. Most debris remain unobserved. The NASA Orbital Debris Observatory tracked space debris with a liquid mirror transit telescope. FM Radio waves can detect debris, after reflecting off them onto a receiver. Optical tracking may be a useful early-warning system on spacecraft.\n\nThe U.S. Strategic Command keeps a catalog of known orbital objects, using ground-based radar and telescopes, and a space-based telescope (originally to distinguish from hostile missiles). The 2009 edition listed about 19,000 objects. Other data come from the ESA Space Debris Telescope, TIRA, the Goldstone, Haystack, and EISCAT radars and the Cobra Dane phased array radar, to be used in debris-environment models like the ESA Meteoroid and Space Debris Terrestrial Environment Reference (MASTER).\n\nReturned space hardware is a valuable source of information on the directional distribution and composition of the (sub-millimetre) debris flux. The LDEF satellite deployed by mission STS-41-C \"Challenger\" and retrieved by STS-32 \"Columbia\" spent 68 months in orbit to gather debris data. The EURECA satellite, deployed by STS-46 \"Atlantis\" in 1992 and retrieved by STS-57 \"Endeavour\" in 1993, was also used for debris study.\n\nThe solar arrays of Hubble were returned by missions STS-61 \"Endeavour\" and STS-109 \"Columbia\", and the impact craters studied by the ESA to validate its models. Materials returned from Mir were also studied, notably the Mir Environmental Effects Payload (which also tested materials intended for the ISS).\n\nA debris cloud resulting from a single event is studied with scatter plots known as Gabbard diagrams, where the perigee and apogee of fragments are plotted with respect to their orbital period. Gabbard diagrams of the early debris cloud prior to the effects of perturbations, if the data were available, are reconstructed. They often include data on newly observed, as yet uncatalogued fragments. Gabbard diagrams can provide important insights into the features of the fragmentation, the direction and point of impact.\n\nAn average of about one tracked object per day has been dropping out of orbit for the past 50 years, averaging almost three objects per day at solar maximum (due to the heating and expansion of the Earth's atmosphere), but one about every three days at solar minimum, usually 5½ yr later. In addition to natural atmospheric effects, corporations, academics and government agencies have proposed plans and technology to deal with space debris, but , most of these are theoretical, and there is no extant business plan for debris reduction.\n\nA number of scholars have also observed that institutional factors—political, legal, economic and cultural \"rules of the game\"—are the greatest impediment to the cleanup of near-Earth space. There is no commercial incentive, since costs aren't assigned to polluters, but a number of suggestions have been made. However, effects to date are limited. In the US, governmental bodies have been accused of backsliding on previous commitments to limit debris growth, \"let alone tackling the more complex issues of removing orbital debris.\"\n\nUpper stage passivation (e.g. of Delta boosters) by releasing residual propellants reduces debris from orbital explosions; however not all boosters implement this. Although there is no international treaty minimizing space debris, the United Nations Committee on the Peaceful Uses of Outer Space (COPUOS) published voluntary guidelines in 2007. As of 2008, the committee is discussing international \"rules of the road\" to prevent collisions between satellites.\nBy 2013, various legal regimes existed, typically instantiated in the launch licenses that are required for a launch in all spacefaring nations.\n\nThe U.S. has a set of standard practices for civilian (NASA) and military (DoD and USAF) orbital-debris mitigation, as has the European Space Agency. In 2007, the ISO began preparing an international standard for space-debris mitigation. Germany and France have posted bonds to safeguard property from debris damage.\n\nWhen originally proposed in 2015, the OneWeb constellation, initially planned to have ~700 satellites anticipated on orbit after 2018, would only state that they would re-enter the atmosphere within 25 years of retirement.\nBy October 2017, both OneWeb—and also SpaceX, with their large Starlink constellation—had filed documents with the US FCC with more aggressive space debris mitigation plans. Both companies committed to a deorbit plan for post-mission satellites which will explicitly move the satellites into orbits where they will reenter the Earth's atmosphere within approximately one year following end-of-life.\n\nWith a \"one-up, one-down\" launch-license policy for Earth orbits, launchers would rendezvous with, capture and de-orbit a derelict satellite from approximately the same orbital plane. Another possibility is the robotic refueling of satellites. Experiments have been flown by NASA, and SpaceX is developing large-scale on-orbit propellant transfer technology and tanker spacecraft.\n\nAnother approach to debris mitigation is to explicitly design the mission architecture to always leave the rocket second-stage in an elliptical geocentric orbit with a low-perigee, thus ensuring rapid orbital decay and avoiding long-term orbital debris from spent rocket bodies. Such missions require the use of a small kick stage to circularize the orbit, but the kick stage itself may be designed with the excess-propellant capability to be able to self-deorbit.\n\nAlthough the ITU requires geostationary satellites to move to a graveyard orbit at the end of their lives, the selected orbital areas do not sufficiently protect GEO lanes from debris. Rocket stages (or satellites) with enough propellant may make a direct, controlled de-orbit, or if this would require too much propellant, a satellite may be brought to an orbit where atmospheric drag would cause it to eventually de-orbit. This was done with the French Spot-1 satellite, reducing its atmospheric re-entry time from a projected 200 years to about 15 by lowering its altitude from to about .\n\nPassive methods of increasing the orbital decay rate of spacecraft debris have been proposed. Instead of rockets, an electrodynamic tether could be attached to a spacecraft at launch; at the end of its lifetime, the tether would be rolled out to slow the spacecraft. Other proposals include a booster stage with a sail-like attachment and a large, thin, inflatable balloon envelope.\n\nA consensus of speakers at a meeting in Brussels on 30 October 2012 organized by the Secure World Foundation (a U.S. think tank) and the French International Relations Institute reported that removal of the largest debris would be required to prevent the risk to spacecraft becoming unacceptable in the foreseeable future (without any addition to the inventory of dead spacecraft in LEO). Removal costs and legal questions about ownership and the authority to remove defunct satellites have stymied national or international action. Current space law retains ownership of all satellites with their original operators, even debris or spacecraft which are defunct or threaten active missions.\n\nA well-studied solution uses a remotely controlled vehicle to rendezvous with, capture and return debris to a central station.\nOne such system is Space Infrastructure Servicing, a commercially developed refueling depot and service spacecraft for communications satellites in geosynchronous orbit originally scheduled for a 2015 launch. The SIS would be able to \"push dead satellites into graveyard orbits.\" The Advanced Common Evolved Stage family of upper stages is being designed with a high leftover-propellant margin (for derelict capture and de-orbit) and in-space refueling capability for the high delta-v required to de-orbit heavy objects from geosynchronous orbit. A tug-like satellite to drag debris to a safe altitude for it to burn up in the atmosphere has been researched. When debris is identified the satellite creates a difference in potential between the debris and itself, then using its thrusters to move itself and the debris to a safer orbit.\n\nA variation of this approach is for the remotely controlled vehicle to rendezvous with debris, capture it temporarily to attach a smaller de-orbit satellite and drag the debris with a tether to the desired location. The \"mothership\" would then tow the debris-smallsat combination for atmospheric entry or move it to a graveyard orbit. One such system is the proposed Busek ORbital DEbris Remover (ORDER), which would carry over 40 SUL (satellite on umbilical line) de-orbit satellites and propellant sufficient for their removal.\nOn 7 January 2010 Star, Inc. reported that it received a contract from the Space and Naval Warfare Systems Command for a feasibility study of the ElectroDynamic Debris Eliminator (EDDE) propellantless spacecraft for space-debris removal.\nIn February 2012 the Swiss Space Center at École Polytechnique Fédérale de Lausanne announced the Clean Space One project, a nanosatellite demonstration project for matching orbit with a defunct Swiss nanosatellite, capturing it and de-orbiting together. The mission has seen several evolutions to reach a pac-man inspired capture model.\n\nThe laser broom uses a ground-based laser to ablate the front of the debris, producing a rocket-like thrust which slows the object. With continued application, the debris would fall enough to be influenced by atmospheric drag. During the late 1990s, the U.S. Air Force's Project Orion was a laser-broom design. Although a test-bed device was scheduled to launch on a Space Shuttle in 2003, international agreements banning powerful laser testing in orbit limited its use to measurements. The Space Shuttle \"Columbia\" disaster postponed the project and according to Nicholas Johnson, chief scientist and program manager for NASA's Orbital Debris Program Office, \"There are lots of little gotchas in the Orion final report. There's a reason why it's been sitting on the shelf for more than a decade.\"\n\nThe momentum of the laser-beam photons could directly impart a thrust on the debris sufficient to move small debris into new orbits out of the way of working satellites. NASA research in 2011 indicates that firing a laser beam at a piece of space junk could impart an impulse of per second, and keeping the laser on the debris for a few hours per day could alter its course by per day. One drawback is the potential for material degradation; the energy may break up the debris, adding to the problem. A similar proposal places the laser on a satellite in Sun-synchronous orbit, using a pulsed beam to push satellites into lower orbits to accelerate their reentry. A proposal to replace the laser with an Ion Beam Shepherd has been made, and other proposals use a foamy ball of aerogel or a spray of water,\ninflatable balloons,\nelectrodynamic tethers,\nboom electroadhesion,\nand dedicated anti-satellite weapons.\n\nOn 28 February 2014, Japan's Japan Aerospace Exploration Agency (JAXA) launched a test \"space net\" satellite. The launch was an operational test only. In December 2016 the country sent a space junk collector via Kounotori 6 to the ISS by which JAXA scientists experiment to pull junk out of orbit using a tether. The system failed to extend a 700-meter tether from a space station resupply vehicle that was returning to Earth. On 6 February the mission was declared a failure and leading researcher Koichi Inoue told reporters that they \"believe the tether did not get released\".\n\nSince 2012, the European Space Agency has designed a mission to remove large space debris from orbit. The mission, e.Deorbit, is scheduled for launch during 2023 with an objective to remove debris heavier than from LEO. Several capture techniques are being studied, including a net, a harpoon and a combination robot arm and clamping mechanism.\n\nHolger Krag of the European Space Agency states that as of 2017 there is no binding international regulatory framework with no progress occurring at the respective UN body in Vienna.\n\nIn 1946 during the Giacobinid meteor shower, Helmut Landsberg collected several small magnetic particles that were apparently associated with the shower. Fred Whipple was intrigued by this and wrote a paper that demonstrated that particles of this size were too small to maintain their velocity when they encountered the upper atmosphere. Instead, they quickly decelerated and then fell to Earth unmelted. In order to classify these sorts of objects, he coined the term \"micro-meteorite\".\n\nWhipple, in collaboration with Fletcher Watson of the Harvard Observatory, led an effort to build an observatory to directly measure the velocity of the meteors that could be seen. At the time the source of the micro-meteorites was not known. Direct measurements at the new observatory were used to locate the source of the meteors, demonstrating that the bulk of material was left over from comet tails, and that none of it could be shown to have an extra-solar origin. Today it is understood that meteoroids of all sorts are leftover material from the formation of the Solar System, consisting of particles from the interplanetary dust cloud or other objects made up from this material, like comets.\n\nThe early studies were based on optical measurements only. In 1957, Hans Pettersson conducted one of the first direct measurements of the fall of space dust on the Earth, estimating it to be 14,300,000 tons per year. This suggested that the meteoroid flux in space was much higher than the number based on telescope observations. Such a high flux presented a very serious risk to missions deeper in space, specifically the high-orbiting Apollo capsules. To determine whether the direct measurement was accurate, a number of additional studies followed, including the Pegasus satellite program. These showed that the rate of meteors passing into the atmosphere, or flux, was in line with the optical measurements, at around 10,000 to 20,000 tons per year.\n\nWhipple's work pre-dated the space race and it proved useful when space exploration started only a few years later. His studies had demonstrated that the chance of being hit by a meteoroid large enough to destroy a spacecraft was extremely remote. However, a spacecraft would be almost constantly struck by micrometeorites, about the size of dust grains.\n\nWhipple had already developed a solution to this problem in 1946. Originally known as a \"meteor bumper\" and now termed the Whipple shield, this consists of a thin foil film held a short distance away from the spacecraft's body. When a micrometeoroid strikes the foil, it vaporizes into a plasma that quickly spreads. By the time this plasma crosses the gap between the shield and the spacecraft, it is so diffused that it is unable to penetrate the structural material below. The shield allows a spacecraft body to be built to just the thickness needed for structural integrity, while the foil adds little additional weight. Such a spacecraft is lighter than one with panels designed to stop the meteoroids directly.\n\nFor spacecraft that spend the majority of their time in orbit, some variety of the Whipple shield has been almost universal for decades. Later research showed that ceramic fibre woven shields offer better protection to hypervelocity (~7 km/s) particles than aluminium shields of equal weight. Another modern design uses multi-layer flexible fabric, as in NASA's design for its never-flown TransHab expandable space habitation module,\nand the Bigelow Expandable Activity Module, which was launched in April 2016 and attached to the ISS for two years of orbital testing.\n\nTo avoid artificial space debris, many—but not all—research satellites are launched on elliptical orbits with perigees inside Earth's atmosphere so they will destroy themselves. Willy Ley predicted in 1960 that \"In time, a number of such accidentally too-lucky shots will accumulate in space and will have to be removed when the era of manned space flight arrives\". After the launch of Sputnik 1 in 1957, the North American Aerospace Defense Command (NORAD) began compiling a database (the Space Object Catalog) of all known rocket launches and objects reaching orbit: satellites, protective shields and upper- and lower-stage booster rockets. NASA published modified versions of the database in two-line element set, and during the early 1980s the CelesTrak bulletin board system re-published them.\nThe trackers who fed the database were aware of other objects in orbit, many of which were the result of in-orbit explosions. Some were deliberately caused during 1960s anti-satellite weapon (ASAT) testing, and others were the result of rocket stages blowing up in orbit as leftover propellant expanded and ruptured their tanks. To improve tracking, NORAD employee John Gabbard kept a separate database. Studying the explosions, Gabbard developed a technique for predicting the orbital paths of their products, and Gabbard diagrams (or plots) are now widely used. These studies were used to improve the modelling of orbital evolution and decay.\n\nWhen the NORAD database became publicly available during the 1970s, NASA scientist Donald J. Kessler applied the technique developed for the asteroid-belt study to the database of known objects. In 1978 Kessler and Burton Cour-Palais co-authored \"Collision Frequency of Artificial Satellites: The Creation of a Debris Belt\", demonstrating that the process controlling asteroid evolution would cause a similar collision process in LEO in decades rather than billions of years. They concluded that by about 2000, space debris would outpace micrometeoroids as the primary ablative risk to orbiting spacecraft.\n\nAt the time, it was widely thought that drag from the upper atmosphere would de-orbit debris faster than it was created. However, Gabbard was aware that the number and type of objects in space were under-represented in the NORAD data and was familiar with its behaviour. In an interview shortly after the publication of Kessler's paper, Gabbard coined the term \"Kessler syndrome\" to refer to the accumulation of debris; it became widely used after its appearance in a 1982 \"Popular Science\" article, which won the Aviation-Space Writers Association 1982 National Journalism Award.\n\nThe lack of hard data about space debris prompted a series of studies to better characterize the LEO environment. In October 1979, NASA provided Kessler with funding for further studies. Several approaches were used by these studies.\n\nOptical telescopes or short-wavelength radar was used to measure the number and size of space objects, and these measurements demonstrated that the published population count was at least 50% too low. Before this, it was believed that the NORAD database accounted for the majority of large objects in orbit. Some objects (typically, U.S. military spacecraft) were found to be omitted from the NORAD list, and others were not included because they were considered unimportant. The list could not easily account for objects under in size—in particular, debris from exploding rocket stages and several 1960s anti-satellite tests.\n\nReturned spacecraft were microscopically examined for small impacts, and sections of Skylab and the Apollo Command/Service Module which were recovered were found to be pitted. Each study indicated that the debris flux was higher than expected and debris was the primary source of collisions in space. LEO already demonstrated the Kessler syndrome.\n\nIn 1978 Kessler found that 42 percent of cataloged debris was the result of 19 events, primarily explosions of spent rocket stages (especially U.S. Delta rockets). He discovered this by first identifying those launches that were described having a large number of objects associated with a payload, then researching the literature to determine the rockets used in the launch. In 1979, this finding resulted in establishment of the NASA Orbital Debris Program after a briefing to NASA senior management, overturning the previously held belief that most unknown debris was from old ASAT tests, but from US upper stage rocket explosions and could be easily managed by depleting the unused fuel following the payload injection the upper stage Delta rocket. Beginning in 1986, when it was discovered that other international agencies were possibly experiencing the same type of problem, NASA expanded its program to include international agencies, the first being the European Space Agency. A number of other Delta components in orbit (Delta was a workhorse of the U.S. space program) had not yet exploded.\n\nDuring the 1980s, the U.S. Air Force conducted an experimental program to determine what would happen if debris collided with satellites or other debris. The study demonstrated that the process differed from micrometeoroid collisions, with large chunks of debris created which would become collision threats.\n\nIn 1991, Kessler published \"Collisional cascading: The limits of population growth in low Earth orbit\" with the best data then available. Citing the USAF conclusions about creation of debris, he wrote that although almost all debris objects (such as paint flecks) were lightweight, most of its mass was in debris about or heavier. This mass could destroy a spacecraft on impact, creating more debris in the critical-mass area. According to the National Academy of Sciences:\n\nA 1-kg object impacting at 10 km/s, for example, is probably capable of catastrophically breaking up a 1,000-kg spacecraft if it strikes a high-density element in the spacecraft. In such a breakup, numerous fragments larger than 1 kg would be created.\n\nKessler's analysis divided the problem into three parts. With a low-enough density, the addition of debris by impacts is slower than their decay rate and the problem is not significant. Beyond that is a critical density, where additional debris leads to additional collisions. At densities beyond this critical mass production exceeds decay, leading to a cascading chain reaction reducing the orbiting population to small objects (several cm in size) and increasing the hazard of space activity. This chain reaction is known as the Kessler syndrome.\n\nIn an early 2009 historical overview, Kessler summed up the situation:\n\nAggressive space activities without adequate safeguards could significantly shorten the time between collisions and produce an intolerable hazard to future spacecraft. Some of the most environmentally dangerous activities in space include large constellations such as those initially proposed by the Strategic Defense Initiative in the mid-1980s, large structures such as those considered in the late-1970s for building solar power stations in Earth orbit, and anti-satellite warfare using systems tested by the USSR, the U.S., and China over the past 30 years. Such aggressive activities could set up a situation where a single satellite failure could lead to cascading failures of many satellites in a period much shorter than years.\n\nDuring the 1980s, NASA and other U.S. groups attempted to limit the growth of debris. One effective solution was implemented by McDonnell Douglas on the Delta booster, by having the booster move away from its payload and vent any propellant remaining in its tanks. This eliminated the pressure buildup in the tanks which caused them to explode in the past. Other countries were slower to adopt this measure and, due especially to a number of launches by the Soviet Union, the problem grew throughout the decade.\n\nA new battery of studies followed as NASA, NORAD and others attempted to better understand the orbital environment, with each adjusting the number of pieces of debris in the critical-mass zone upward. Although in 1981 (when Schefter's article was published) the number of objects was estimated at 5,000, new detectors in the Ground-based Electro-Optical Deep Space Surveillance system found new objects. By the late 1990s, it was thought that most of the 28,000 launched objects had already decayed and about 8,500 remained in orbit. By 2005 this was adjusted upward to 13,000 objects, and a 2006 study increased the number to 19,000 as a result of an ASAT test and a satellite collision. In 2011, NASA said that 22,000 objects were being tracked.\n\nThe growth in the number of objects as a result of the late-1990s studies sparked debate in the space community on the nature of the problem and the earlier dire warnings. According to Kessler's 1991 derivation and 2001 updates, the LEO environment in the altitude range should be cascading. However, only one major incident has occurred: the 2009 satellite collision between Iridium 33 and Cosmos 2251. The lack of obvious short-term cascading has led to speculation that the original estimates overstated the problem. According to Kessler a cascade would not be obvious until it was well advanced, which might take years.\n\nA 2006 NASA model suggested that if no new launches took place the environment would retain the then-known population until about 2055, when it would increase on its own. Richard Crowther of Britain's Defence Evaluation and Research Agency said in 2002 that he believed the cascade would begin about 2015. The National Academy of Sciences, summarizing the professional view, noted widespread agreement that two bands of LEO space—900 to and —were already past critical density.\n\nIn the 2009 European Air and Space Conference, University of Southampton researcher Hugh Lewis predicted that the threat from space debris would rise 50 percent in the next decade and quadruple in the next 50 years. , more than 13,000 close calls were tracked weekly.\n\nA 2011 report by the U.S. National Research Council warned NASA that the amount of orbiting space debris was at a critical level. According to some computer models, the amount of space debris \"has reached a tipping point, with enough currently in orbit to continually collide and create even more debris, raising the risk of spacecraft failures\". The report called for international regulations limiting debris and research of disposal methods.\n\nThe plot of episode 4 (\"Conflict\") of Gerry Anderson's 1970 TV series \"UFO\" includes routine missions for the disposal of spent satellites by bombing.\n\n\"Salvage 1\" (1979 TV series) deals humorously with a scrap dealer who establish a space junk salvage company.\n\n\"Planetes\" is a manga (1999-2004) and anime series (2003-2004) that gives focus on a team which is responsible for the collection and disposal of space debris. The DVDs for the TV series include interviews with NASA's Orbital Debris Program Office.\n\nIn 2009, Rhett & Link wrote a song called \"Space Junk\" and made an accompanying music video for the TV series \"Brink\". The lyrics refer to two men tasked to clean up debris such as satellites and expended rockets.\n\n\"Gravity\" is a 2013 survival film, directed by Alfonso Cuaron, about a disaster on a space mission caused by Kessler syndrome.\n\n\n\n"}
{"id": "31392778", "url": "https://en.wikipedia.org/wiki?curid=31392778", "title": "Space tether", "text": "Space tether\n\nSpace tethers are long cables which can be used for propulsion, momentum exchange, stabilization and attitude control, or maintaining the relative positions of the components of a large dispersed satellite/spacecraft sensor system. Depending on the mission objectives and altitude, spaceflight using this form of spacecraft propulsion is theorized to be significantly less expensive than spaceflight using rocket engines.\n\nTether satellites might be used for various purposes, including research into tether propulsion, tidal stabilization and orbital plasma dynamics. Five main techniques for employing space tethers are in development:\n\nElectrodynamic tethers\nMomentum exchange tethers\nTethered formation flying\nElectric sail\nUniversal Orbital Support System\n\nMany uses for space tethers have been proposed, including deployment as space elevators, as skyhooks, and for doing propellant-free orbital transfers.\n\nKonstantin Tsiolkovsky once proposed a tower so tall that it reached into space, so that it would be held there by the rotation of the Earth. However, at the time, there was no realistic way to build it.\n\nTo try to solve the problems in \"Komsomolskaya Pravda\" (July 31, 1960), another Russian, Yuri Artsutanov, wrote in greater detail about the idea of a tensile cable to be deployed from a geosynchronous satellite, downwards towards the ground, and upwards away, keeping the cable balanced. This is the space elevator idea, a type of synchronous tether that would rotate with the earth. However, given the materials technology of the time, this too was impractical on Earth.\n\nIn the 1970s, Jerome Pearson independently conceived the idea of a space elevator, sometimes referred to as a synchronous tether, and, in particular, analyzed a lunar elevator that can go through the L1 and L2 points, and this was found to be possible with materials then existing.\n\nIn 1977, Hans Moravec and later Robert L. Forward investigated the physics of non-synchronous skyhooks, also known as rotating skyhooks, and performed detailed simulations of tapered rotating tethers that could pick objects off, and place objects onto, the Moon, Mars and other planets, with little loss, or even a net gain of energy.\n\nIn 1979, NASA examined the feasibility of the idea and gave direction to the study of tethered systems, especially tethered satellites.\n\nIn 1990, E. Sarmont proposed a non-rotating Orbiting Skyhook for an Earth-to-orbit / orbit-to-escape-velocity Space Transportation System in a paper titled \"An Orbiting Skyhook: Affordable Access to Space\". In this concept a suborbital launch vehicle would fly to the bottom end of a Skyhook, while spacecraft bound for higher orbit, or returning from higher orbit, would use the upper end.\n\nIn 2000, NASA and Boeing considered a HASTOL concept, where a rotating tether would take payloads from a hypersonic aircraft (at half of orbital velocity) to orbit.\n\nA tether satellite is a satellite connected to another by a space tether. A number of satellites have been launched to test tether technologies, with varying degrees of success.\n\nThere are many different (and overlapping) types of tether.\n\nMomentum Exchange Tethers are one of many applications for space tethers. Momentum Exchange Tethers come in two types; rotating and non-rotating. A rotating tether will create a controlled force on the end-masses of the system due to centrifugal acceleration. While the tether system rotates, the objects on either end of the tether will experience continuous acceleration; the magnitude of the acceleration depends on the length of the tether and the rotation rate. Momentum exchange occurs when an end body is released during the rotation. The transfer of momentum to the released object will cause the rotating tether to lose energy, and thus lose velocity and altitude. However, using electrodynamic tether thrusting, or ion propulsion the system can then re-boost itself with little or no expenditure of consumable reaction mass.\n\nA skyhook is a theoretical class of orbiting tether propulsion intended to lift payloads to high altitudes and speeds. Proposals for skyhooks include designs that employ tethers spinning at hypersonic speed for catching high speed payloads or high altitude aircraft and placing them in orbit.\n\nElectrodynamic tethers are long conducting wires, such as one deployed from a tether satellite, which can operate on electromagnetic principles as generators, by converting their kinetic energy to electrical energy, or as motors, converting electrical energy to kinetic energy. Electric potential is generated across a conductive tether by its motion through the earth's magnetic field. The choice of the metal conductor to be used in an electrodynamic tether is determined by a variety of factors. Primary factors usually include high electrical conductivity and low density. Secondary factors, depending on the application, include cost, strength, and melting point.\n\nAn electrodynamic tether was profiled in the documentary film \"Orphans of Apollo\" as technology that was to be used to keep the Russian space station \"Mir\" in orbit.\n\nThis is the use of a (typically) non-conductive tether to connect multiple spacecraft. A proposed 2011 experiment to study the technique is the Tethered Experiment for Mars inter-Planetary Operations (TEMPO³).\n\nA theoretical type of non-rotating tethered satellite system, it is a concept for providing space-based support to things suspended above an astronomical object. The orbital system is a coupled mass system wherein the upper supporting mass (A) is placed in an orbit around a given celestial body such that it can support a suspended mass (B) at a specific height above the surface of the celestial body, but lower than (A).\n\nInstead of rotating end for end, tethers can also be kept straight by the slight difference in the strength of gravity over their length.\n\nA non-rotating tether system has a stable orientation that is aligned along the local vertical (of the earth or other body). This can be understood by inspection of the figure below where two spacecraft at two different altitudes have been connected by a tether. Normally, each spacecraft would have a balance of gravitational (e.g. F) and centrifugal (e.g. F), but when tied together by a tether, these values begin to change with respect to one another. This phenomenon occurs because, without the tether, the higher-altitude mass would travel slower than the lower mass. The system must move at a single speed, so the tether must therefore slow down the lower mass and speed up the upper one. The centrifugal force of the tethered upper body is increased, while that of the lower-altitude body is reduced. This results in the centrifugal force of the upper body and the gravitational force of the lower body being dominant. This difference in forces naturally aligns the system along the local vertical, as seen in the figure.\n\nObjects in low Earth orbit are subjected to noticeable erosion from atomic oxygen due to the high orbital speed with which the molecules strike as well as their high reactivity. This could quickly erode a tether.\n\nSimple single-strand tethers are susceptible to micrometeoroids and space junk. Several systems have since been proposed and tested to improve debris resistance:\n\nLarge pieces of junk would still cut most tethers, including the improved versions listed here, but these are currently tracked on radar and have predictable orbits. A tether could be wiggled to dodge known pieces of junk, or thrusters used to change the orbit, avoiding a collision.\n\nTether properties and materials are dependent on the application. However, there are some common properties. To achieve maximum performance and low cost, tethers would need to be made of materials with the combination of high strength or electrical conductivity and low density. All space tethers are susceptible to space debris or micrometeroids. Therefore, system designers will need to decide whether or not a protective coating is needed, including relative to UV and atomic oxygen. Research is being conducted to assess the probability of a collision that would damage the tether MAST.\n\nFor applications that exert high tensile forces on the tether, the materials need to be strong and light. Some current tether designs use crystalline plastics such as ultra high molecular weight polyethylene, aramid or carbon fiber. A possible future material would be carbon nanotubes, which have an estimated tensile strength between 140 and 177 GPa (20.3-25.6 million psi), and a proven tensile strength in the range 50-60 GPa for some individual nanotubes. (A number of other materials obtain 10 to 20 GPa in some samples on the nano scale, but translating such strengths to the macro scale has been challenging so far, with, as of 2011, CNT-based ropes being an order of magnitude less strong, not yet stronger than more conventional carbon fiber on that scale).\n\nFor some applications, the tensile force on the tether is projected to be less than 15 lbs (< 65 N) Material selection in this case depends on the purpose of the mission and design constraints. Electrodynamic tethers, such as the one used on TSS-1R, may use thin copper wires for high conductivity (see EDT).\n\nThere are design equations for certain applications that may be used to aid designers in identifying typical quantities that drive material selection.\n\nSpace elevator equations typically use a \"characteristic length\", L, which is also known as its \"self-support length\" and is the length of untapered cable it can support in a constant 1 \"g\" gravity field.\nwhere σ is the stress limit (in pressure units) and ρ is the density of the material.\n\nHypersonic skyhook equations use the material's \"specific velocity\" which is equal to the maximum tangential velocity a spinning hoop can attain without breaking:\n\nFor rotating tethers (rotovators) the value used is the material’s ‘characteristic velocity’ which is the maximum tip velocity a rotating untapered cable can attain without breaking,\nThe characteristic velocity equals the specific velocity multiplied by the square root of two.\n\nThese values are used in equations similar to the rocket equation and are analogous to specific impulse or exhaust velocity. The higher these values are, the more efficient and lighter the tether can be in relation to the payloads that they can carry. Eventually however, the mass of the tether propulsion system will be limited at the low end by other factors such as momentum storage.\n\nProposed materials include Kevlar, ultra high molecular weight polyethylene, carbon nanotubes and M5 fiber. M5 is a synthetic fiber that is lighter than Kevlar or Spectra. According to Pearson, Levin, Oldson, and Wykes in their article \"The Lunar Space Elevator\", an M5 ribbon 30 mm wide and 0.023 mm thick, would be able to support 2000 kg on the lunar surface. It would also be able to hold 100 cargo vehicles, each with a mass of 580 kg, evenly spaced along the length of the elevator. Other materials that could be used are T1000G carbon fiber, Spectra 2000, or Zylon.\n\nFor gravity stabilised tethers, to exceed the self-support length the tether material can be tapered so that the cross-sectional area varies with the total load at each point along the length of the cable. In practice this means that the central tether structure needs to be thicker than the tips. Correct tapering ensures that the tensile stress at every point in the cable is exactly the same. For very demanding applications, such as an Earth space elevator, the tapering can reduce the excessive ratios of cable weight to payload weight.\n\nFor rotating tethers not significantly affected by gravity, the thickness also varies, and it can be shown that the area, A, is given as a function of r (the distance from the centre) as follows:\nwhere R is the radius of tether, v is the velocity with respect to the centre, M is the tip mass, formula_5 is the material density, and T is the design tensile strength (Young's modulus divided by safety factor).\n\nIntegrating the area to give the volume and multiplying by the density and dividing by the payload mass gives a payload mass / tether mass ratio of:\nwhere erf is the normal probability error function.\n\nLet formula_7,\nthen:\nThis equation can be compared with the rocket equation, which is proportional to a simple exponent on a velocity, rather than a velocity squared. This difference effectively limits the delta-v that can be obtained from a single tether.\n\nIn addition the cable shape must be constructed to withstand micrometeorites and space junk. This can be achieved with the use of redundant cables, such as the Hoytether; redundancy can ensure that it is very unlikely that multiple redundant cables would be damaged near the same point on the cable, and hence a very large amount of total damage can occur over different parts of the cable before failure occurs.\n\nBeanstalks and rotovators are currently limited by the strengths of available materials. Although ultra-high strength plastic fibers (Kevlar and Spectra) permit rotovators to pluck masses from the surface of the Moon and Mars, a rotovator from these materials cannot lift from the surface of the Earth. In theory, high flying, supersonic (or hypersonic) aircraft could deliver a payload to a rotovator that dipped into Earth's upper atmosphere briefly at predictable locations throughout the tropic (and temperate) zone of Earth. As of May 2013, all mechanical tethers (orbital and elevators) are on hold until stronger materials are available.\n\nCargo capture for rotovators is nontrivial, and failure to capture can cause problems. Several systems have been proposed, such as shooting nets at the cargo, but all add weight, complexity, and another failure mode. At least one lab scale demonstration of a working grapple system has been achieved however.\n\nCurrently, the strongest materials in tension are plastics that require a coating for protection from UV radiation and (depending on the orbit) erosion by atomic oxygen. Disposal of waste heat is difficult in a vacuum, so overheating may cause tether failures or damage.\n\nElectrodynamic tethers deployed along the local vertical ('hanging tethers') may suffer from dynamical instability. Pendular motion causes the tether vibration amplitude to build up under the action of electromagnetic interaction. As the mission time increases, this behavior can compromise the performance of the system. Over a few weeks, electrodynamic tethers in Earth orbit might build up vibrations in many modes, as their orbit interacts with irregularities in magnetic and gravitational fields.\n\nOne plan to control the vibrations is to actively vary the tether current to counteract the growth of the vibrations. Electrodynamic tethers can be stabilized by reducing their current when it would feed the oscillations, and increasing it when it opposes oscillations. Simulations have demonstrated that this can control tether vibration. This approach requires sensors to measure tether vibrations, which can either be an inertial navigation system on one end of the tether, or satellite navigation systems mounted on the tether, transmitting their positions to a receiver on the end.\n\nAnother proposed method is to use spinning electrodynamic tethers instead of hanging tethers. The gyroscopic effect provides passive stabilisation, avoiding the instability.\n\nAs mentioned earlier, conductive tethers have failed from unexpected current surges. Unexpected electrostatic discharges have cut tethers (e.g. see Tethered Satellite System Reflight (TSS‑1R) on STS‑75), damaged electronics, and welded tether handling machinery. It may be that the Earth's magnetic field is not as homogeneous as some engineers have believed.\n\nComputer models frequently show tethers can snap due to vibration.\n\nMechanical tether-handling equipment is often surprisingly heavy, with complex controls to damp vibrations. The one ton climber proposed by Dr. Brad Edwards for his Space Elevator may detect and suppress most vibrations by changing speed and direction. The climber can also repair or augment a tether by spinning more strands.\n\nThe vibration modes that may be a problem include skipping rope, transverse, longitudinal, and pendulum.\n\nTethers are nearly always tapered, and this can greatly amplify the movement at the thinnest tip in whip-like ways.\n\nA tether is not a spherical object, and has significant extent. This means that as an extended object, it is not directly modelable as a point source, and this means that the center of mass and center of gravity are not usually colocated. Thus the inverse square law does not apply except at large distances, to the overall behaviour of a tether. Hence the orbits are not completely Keplerian, and in some cases they are actually chaotic.\n\nWith bolus designs, rotation of the cable interacting with the non linear gravity fields found in elliptical orbits can cause exchange of orbital angular momentum and rotation angular momentum. This can make prediction and modelling extremely complex.\n\n\n\n"}
{"id": "35891416", "url": "https://en.wikipedia.org/wiki?curid=35891416", "title": "SpiNNaker", "text": "SpiNNaker\n\nSpiNNaker (Spiking Neural Network Architecture) is a massively parallel, manycore supercomputer architecture designed by the Advanced Processor Technologies Research Group (APT) at the School of Computer Science, University of Manchester. It is composed of 57,600 ARM9 processors (specifically ARM968), each with 18 cores and 128MB of mobile DDR SDRAM, totaling 1,036,800 cores and over 7TB of RAM. The computing platform is based on spiking neural networks, useful in simulating the human brain (see Human Brain Project).\n\nThe completed design is housed in 10 19-inch racks, with each rack holding over 100,000 cores. The cards holding the chips are held in 5 Blade enclosures, and each core emulates 1000 Neurons. In total, the goal is to simulate the behavior of aggregates of up to a billion neurons in real time. This machine requires about 100kW from a 240V supply and an air-conditioned environment.\n\nSpiNNaker is being used as one component of the neuromorphic computing platform for the Human Brain Project.\n\nOn October 14, 2018 the HBP announced that the million core milestone had been achieved.\n\n"}
{"id": "16017237", "url": "https://en.wikipedia.org/wiki?curid=16017237", "title": "Style guide", "text": "Style guide\n\nA style guide (or manual of style) is a set of standards for the writing and design of documents, either for general use or for a specific publication, organization, or field. (It is often called a style sheet, though that term has other meanings.)\n\nA style guide establishes and enforces style to improve communication. To do that, it ensures consistency within a document and across multiple documents and enforces best practice in usage and in language composition, visual composition, orthography and typography. For academic and technical documents, a guide may also enforce the best practice in ethics (such as authorship, research ethics, and disclosure), pedagogy (such as exposition and clarity), and compliance (technical and regulatory).\n\nStyle guides are common for general and specialized use, for the general reading and writing audience, and for students and scholars of various academic disciplines, medicine, journalism, the law, government, business, and specific industries. House style refers to the internal style manual of a particular publisher or organization.\n\nStyle guides vary widely in scope and size.\n\nThis variety in scope and length is enabled by the cascading of one style over another, in a way analogous to how styles cascade in web development and in desktop publishing (e.g., how inline styles in HTML cascade over CSS styles).\n\nA short style guide is often called a \"style sheet\". A comprehensive guide tends to be long and is often called a \"style manual\" or \"manual of style\" (\"MOS\" or \"MoS\"). In many cases, a project such as one book, journal, or monograph series typically has a short style sheet that cascades over the somewhat larger style guide of an organization such as a publishing company, whose content is usually called \"house style\". Most house styles, in turn, cascade over an \"industry-wide or profession-wide style manual\" that is even more comprehensive. Some examples of these industry style guides include the following: \n\nFinally, these reference works cascade over the orthographic norms of the language in use (for example, English orthography for English-language publications). This, of course, may be subject to national variety such as the different varieties of American English and British English.\n\nSome style guides focus on specific topic areas such as graphic design, including typography. Website style guides cover a publication's visual and technical aspects along with text.\n\nStyle guides that cover usage may suggest ways of describing people that avoid racism, sexism, and homophobia. Guides in specific scientific and technical fields cover nomenclature, which specifies names or classifying labels that are preferred because they are clear, standardized, and ontologically sound (e.g., taxonomy, chemical nomenclature, and gene nomenclature).\n\nMost style guides are revised periodically to accommodate changes in conventions and usage. The frequency of updating and the revision control are determined by the subject matter. For style manuals in reference work format, new editions typically appear every 1 to 20 years. For example, the AP Stylebook is revised annually, and the Chicago, APA, and ASA manuals are in their 17th, 6th, and 4th editions, respectively. Many house styles and individual project styles change more frequently, especially for new projects.\n\nSeveral basic style guides for technical and scientific communication have been defined by international standards organizations. One example is ISO 215 \"Documentation — Presentation of contributions to periodicals and other serials\".\n\nThe European Union publishes an \"Interinstitutional style guide\"—encompassing 24 languages across the European Union. This manual is \"obligatory\" for all those employed by the institutions of the EU who are involved in preparing EU documents and works. The Directorate-General for Translation of the European Commission publishes its own \"English Style Guide\", intended primarily for English-language authors and translators, but aiming to serve a wider readership as well.\n\n\nGeneral\n\nJournalism\n\nLaw\n\n\n\nIn the United States, most public-facing corporate communication and journalism writing is written with styles following \"The Associated Press Stylebook\". Book publishers and authors of journals requiring reference sections generally choose the Chicago Manual of Style, while scholarly writing often follows the \"MLA Style Manual and Guide to Scholarly Publishing\". One of the most popular grammar guides used in third-person writing is \"The Elements of Style\". The Associated Press Stylebook is written to be used together with The Elements of Style to provide a very complete grammar and English style reference with no conflicts.\n\n\n\n\n\n\n\nDespite the near uniform use of the Bluebook, nearly every state has appellate court rules that specify citation methods and writing styles specific to that state - and the Supreme Court of the United States has its own citation method. However, in most cases these are derived from the Bluebook.\n\nThere are also several other citation manuals available to legal writers in wide usage in the United States. Virtually all large law firms maintain their own citation manual and several major publishers of legal texts (West, Lexis-Nexis, Hein, \"et al.\") maintain their own systems.\n\n\n\n\n\nGuidelines for citing web content also appear in comprehensive style guides such as Oxford/Hart, Chicago and MLA.\n\n"}
{"id": "18934904", "url": "https://en.wikipedia.org/wiki?curid=18934904", "title": "Technical standard", "text": "Technical standard\n\nA technical standard is an established norm or requirement in regard to technical systems. It is usually a formal document that establishes uniform engineering or technical criteria, methods, processes, and practices. In contrast, a custom, convention, company product, corporate standard, and so forth that becomes generally accepted and dominant is often called a \"de facto\" standard. \n\nA technical standard may be developed privately or unilaterally, for example by a corporation, regulatory body, military, etc. Standards can also be developed by groups such as trade unions and trade associations. Standards organizations often have more diverse input and usually develop voluntary standards: these might become mandatory if adopted by a government (i.e., through legislation), business contract, etc.\n\nThe standardization process may be by edict or may involve the formal consensus of technical experts.\n\nThe primary types of technical standards are:\n\nTechnical standards may exist as:\n\nWhen a geographically defined community must solve a community-wide coordination problem, it can adopt an existing standard or produce a new one. The main geographic levels are:\n\nNational/Regional/International standards is one way of overcoming technical barriers in inter-local or inter-regional commerce caused by differences among technical regulations and standards developed independently and separately by each local, local standards organisation, or local company. Technical barriers arise when different groups come together, each with a large user base, doing some well established thing that between them is mutually incompatible. Establishing national/regional/international standards is one way of preventing or overcoming this problem.\n\nThe existence of a published standard does not imply that it is always useful or correct. For example, if an item complies with a certain standard, there is not necessarily assurance that it is fit for any particular use. The people who use the item or service (engineers, trade unions, etc.) or specify it (building codes, government, industry, etc.) have the responsibility to consider the available standards, specify the correct one, enforce compliance, and use the item correctly. Validation of suitability is necessary.\n\nStandards often get reviewed, revised and updated on a regular basis. It is critical that the most current version of a published standard be used or referenced. The originator or standard writing body often has the current versions listed on its web site. \n\nIn social sciences, including economics, a standard is useful if it is a solution to a coordination problem: \nit emerges from situations in which all parties realize mutual gains, but only by making mutually consistent decisions. Examples: \n"}
{"id": "2028016", "url": "https://en.wikipedia.org/wiki?curid=2028016", "title": "Technology alignment", "text": "Technology alignment\n\nBusiness and technology alignment, or just technology alignment, corrects terminology and assumptions used in business to better match those of technology and standards anticipated in the technology strategy and technology roadmaps. \n\nWhen technology is changing very rapidly in an industry, the aligning of business terms to the distinctions that the technology requires tends to dominate any enterprise taxonomy development effort. In such circumstances, consultants or specific technology training is usually required, as the organization lacks the internal skills or experience with the technologies that it expects to be using soon.\n\nIn government, for example, citizen use of the Internet and the increased availability of teleworkers has presented special challenges and opportunities, typically called \"e-government\". At the same time, internal operational efficiencies have become more of a priority due to rising competition between jurisdictions. Often the first step is to limit the number of different departments or agencies involved. By \"consolidating the technology operations of 91 state agencies into the Virginia Information Technology Agency, the State of Virginia estimates an eventual savings of nearly $100 million a year.\" - \n\n\"Similarly, the U.S. National Performance Review recommended a data processing consolidation and modernization initiative citing industry experience suggesting operational savings of between 30% and 50%.\" - \n\nWhile \"California is the cradle of the information technology industry\" its own state government claims that \"collaborative exploration and exploitation of emerging technologies is extremely rare within state government\", accordingly it seeks to \"improve customer relationships through online services.\" However such efforts tend to rely very much on driving personalities and leadership. After one resignation, the state \"lost the vision and executive sponsorship that contributed to its success and the national recognition of California's emerging eGovernment activities.\" This is a major problem in all technology alignment.\n\nIn Canada, a similar nationwide effort called Service Canada has similar goals, and has run into similar problems: \"The big complaints are that departments fight over their turf and are organized to serve the bureaucracy, not Canadians. They don’t share data, information, common infrastructure, technology or integrate their business processes. Senior bureaucrats are often accused of being out of touch with the needs of Canadians.\" - The government claims that it \"is expected to save C$3 billion over five years by automating manual operations, consolidating call centres and reducing overpayments in Canada Pension Plan and employment service.\" It \"will need to spend about C$500 million for technology, consolidate or move offices and retrain the thousands of workers whose jobs were eliminated by automation.\"\n\nWhen, as in California or Canada, new leadership and massive change to operations is required, technology alignment may simply excuse a massive business process reengineering and downsizing exercise. This too is a common situation in technology alignment: using the fact of new technology as a pretext for other large changes.\n\nHowever, as with all such exercises, there are claims that better service will result, by (in Canada) \"opening new offices and creating more front-line jobs in local communities\" or (in California) \"a 20% reduction in the workforce performing shared services\" and of \"nearly 9,000 state employees... about 3,600 are engaged in common core functions. An eventual 20% reduction in this workforce segment is possible through attrition when phased in over 5 years.\" - \n\nThese claims also are fairly typical: despite a longstanding admission among experts that there is a \"productivity paradox\", the introduction of new information technology and more automated work processes are always assumed to be \"more efficient\" than what they replace. Accordingly, technology alignment is probably not a passing fad, but, seems to be driven by factors built into business and technology culture.\n\n"}
{"id": "9456262", "url": "https://en.wikipedia.org/wiki?curid=9456262", "title": "Technology in science fiction", "text": "Technology in science fiction\n\nTechnology in science fiction examines the possibilities and implications of new technological concepts. Authors have taken, or created, new innovations and technologies, and elaborated on what they might be and how they might be used. This exchange goes in both directions – sometimes the technology appears first in science fiction, then becomes reality (such as space travel) and other times the real technology comes first, and science fiction authors speculate about how it might be used, and how it might affect the human condition. Likewise, the accuracy of the technology portrayed spans a wide range – sometimes it is existing technology, sometimes it is a physically realistic portrayal of a far-out technology, and sometimes it is simply a plot device that looks scientific, but has no basis in science. Examples drawn from space travel in science fiction include:\n\nAlmost every new technology that becomes practical was previously used in science fiction. The following are a few examples, from a very large set:\n\nTransparent Aluminum as featured in the \"Star Trek\" universe has since become a reality as Aluminium oxynitride (ALONtm), patented in 1985, and as different from metallic aluminum as rust is from iron. Rather than being used as transparent blast shielding as in the fictional Enterprise class starships, this transparent ceramic is used, as the chemically similar (and similarly expensive) corundum (crystalline aluminum oxide) has long been used, in tough windows.\n\nTractor/Repulsor Beams have been realized as Laser-based Optical tweezers, and more recently as a pair of Bessel beams. These instruments use the radiation from the laser beam to manipulate microscopic particles in what is called an \"optical trap\" along the length of the beam as desired.\n\nFictional Tractor beams have been prominently used in the \"Star Wars\" universe and in the \"Star Trek\" universe. In an early scene of \"\" a large spaceship uses such a beam to seize a small one, in order to capture the protagonists.\n\nArtificial Vision/Prosthetic Eyes Visual prosthesis has been a topic of experiments since the late 20th century. \n\nNotable characters using artificial vision include all characters from the Ghost in the Shell series who use prosthetic bodies e.g. Batou's ranger eyes, Saito's left eye, and Motoko Kusanagi's artificial eyes, Geordi La Forge from the \"\" series who made use of a VISOR and later; ocular implants, RoboCop from the RoboCop series, Spike Spiegel from the Cowboy Bebop anime series, and the Illusive Man from the Mass Effect series of videogames.\nTricorder The Lab-On-a-Chip Application Development Portable Test System (LOCAD-PTS) used by astronauts on the International Space Station is designed specifically to biochemical molecules with the purpose of \"identifying microbes on space station services\" through use of the Gram Staining Method.\n\nThough less advanced than the fictional tricorder of the \"Star Trek\" series, the LOCAD-PTS is useful for quickly identifying bacteria and fungi on the International Space Station without having to send samples back to Earth, thus risking contamination or degradation. Fungi have proven to be a hazard if left unchecked on the space station as they managed to decompose some Russian electronics.\n\nThe Tricorder featured in the \"Star Trek\" universe was capable of measuring almost anything, from the chemical composition of explosives, to the life signs of a dying humanoid. The LOCAD-PTS does not differentiate between live and dead test material yet.\nSince the principles of rocketry were worked out in the early 20th century, writers have used straightforward extrapolation to support stories of interplanetary exploration, colonization, conquest and so forth.\n\nWith new developments in space exploration and technology the idea of space exploration became a reality. Though many writers explored space travel before these events and inventions, the reality of new technologies and the evidence that space exploration was now possible opened new doors to create more fantastical ideas of space travel. Many Science Fiction topics are born from reality, but turn these new technologies to create imagined realities, thus creating Science Fiction in itself.\n\n1903 – The Wright brothers invented the first motored and manned airplane, launching the age of human flight \n\n1920s – Robert Goddard and Wernher von Braun developed liquid-fueled rockets, later applied as the V2 in war. Fictional spaceships of the 1950s were typically shaped like the V2. Later long range missiles influenced later fiction.\n\nThe Space Race between the US and Soviet Union inspired more precise depiction of technology already under development.\n\nThe launch of the first man-made object to orbit Earth; USSR's Sputnik 1 (October 4, 1957)\n\nSpace stations, first presented in crude form by The Brick Moon, were popularized in the 1960s by books agitating for further development. Those little resembled the Salyut 1 or later actual stations. presented the \"rotating wheel space station\" of the 1960s but few others did. The long-running fictional Deep Space Nine (space station) and Babylon 5 (space station) little resembled any of the above.\n\nGalactic-scale stories usually call for interstellar travel in human lifetimes, which is not supported by existing science, so this technology is more speculative. Among the earliest introductions to this concept include E.E. \"Doc\" Smith's element X-powered spaceship in the \"Skylark\" and Lensman series (1920s). The so-called X solution unlocked the atomic power of copper, which is then used to power an advanced propulsion system. In these narratives, the ships are \"inertia-less\"; this Inertialess drive makes travel effortless at huge multiples of the speed of light.\n\nThe faster-than-light travel was also explained in Isaac Asimov's \"Foundation\" series and became a familiar term thereafter particularly since the concept was also used by the \"Star Wars\" films as well as other fictional intergalactic narratives.\n\nHyperspace commonly designates one class of technology, where infinite speeds are possible; a ship may jump to hyper space or star drive \"clutching at the very fabric of time itself\" thus making travel that would normally take thousands of years possible in no time at all. One example of narrative descriptions for hyperspace was John E Stith's conceptualization in the novel \"Redshift Rendezvouz\" (1990). The author described that a spacecraft operating in a hyperspace moves at exactly 1,024 times the speed of light relative to normal space time, with the speed of light lower than 300,000 kilometers per second.\n\nWhile now (as of 2017) there are companies that are fully devoted to creating robots and artificial intelligence, these ideas were long present in science fiction before they started to become real technology. Mechanical and artificial characters were derived both from extrapolations of real engineering efforts, and from the whims and imaginations of the authors. This technology has given writers, as well as other forms of art, the inspiration to create non-human characters.\n\n\nArtificial Intelligence\n(also known as machine intelligence and often abbreviated as AI) is intelligence exhibited by any manufactured (not grown) system. The term is often applied to general-purpose computers and also in the field of scientific investigation into the theory and practical application of AI.\n\nA robot is an electro-mechanical or bio-mechanical device or group of devices that can perform autonomous or preprogrammed tasks.\n\nAn android is a robot made to resemble a human, usually both in appearance and behavior. The word derives from the Greek andr-, \" meaning \"man, male\", and the suffix -eides, used to mean \"of the species; alike\" (from eidos \"species\").\n\nA cyborg is a cybernetic organism which adds to or enhances its abilities by using technology.\n\n\nWith new developments in science and technology helping to study and promote parapsychology or Psi Phenomena, many SF writers felt the need to incorporate and elaborate on these subjects in their stories. While technology helped the investigation into Psi Phenomena it also created questions that many SF writers chose to answer, through their stories, in their own unique way. If we look at some of the examples of Psi Phenomena prominent in stories, they may have stemmed from how science would take this experimentation with Psi Phenomena and use it. In Stephen King's \"The Dead Zone\", we see how precognition was used to affect political candidates. The idea that someone could harness this power and use it for good or evil was one that many SF writer's elaborated on. In \"The Foreign Hand Tie\" by Randall Garret espionage takes on a new form via telepathy through twins. When science and technology can be used to anchor something in reality, via experimentation or exploration, and yield results, it creates controversy that society may fear or even fantasize about. Throughout SF history, Psi Phenomena can be seen to be used for good and evil, and through new science and technological discoveries, this genre then becomes more real and more elaborate.\n\n\n\nWhile ESP and belief in other powers were, in the beginning, mainly fueled by superstitions, religion and tradition, the dawn of science brought about a way to analyze and study these supposed \"powers\" giving them an anchor in reality. The Scientific Revolution featured ideas that life should be \"led by reason\" and that, \"the universe as a mechanistic, deterministic system could eventually be known accurately and fully through observation and reason\". While new science and technology gave rise to skepticism towards the existence of psi phenomena, it also gave way for new technologies to be applied in either proving or disproving such phenomena. One of the first experimental approaches to Psi Phenomena started in the 1930s and was conducted under the direction of J.B. Rhine (1895–1980). Rhine popularized the now famous methodology of card guessing and dice rolling experiments in a laboratory in attempt to find statistical validation for ESP. In 1957 the Parapsychological Association was formed at the preeminent society for parapsychology. Openness to new parapsychology studies and occult phenomena continued to rise in the 1970s.\n\n\nE. Dawson Rogers hopes to gain new respectability for spiritualism and founds Society for Psychical Research in 1882\n\nGovernment investigations into parapsychology: Project Star Gate, formed in 1970 with cooperation from the Central Intelligence Agency and Defense Intelligence Agency, investigates remote viewing, sees nothing useful\n\nExtraterrestrial life is a familiar topic in fiction. In the centuries since astronomers discovered that planets are worlds, people have speculated on the possibility of life existing there, though xenobiology has remained a science without a subject. However, people from afar, or alien creatures with various powers and purposes, provided fresh new material for fiction. Some stories were about friendly visitors who got along with humans, such as the aliens in the Keroro Gunsou series, when they give up on attempting to take over planet Earth. Others made alien invasion their theme, as in the 1898 novel, War of the Worlds. Meteorites have long shown that foreign bodies sometimes enter Earth's atmosphere, and the term \"flying saucer\" was coined in 1947. Several science fiction novels used them.\n\n\n\nThe notion of parallel worlds have always intrigued different types of genres, especially the science fiction aspect. Many authors have used the idea of travelling back into prehistoric times or traveling forwards to an unknown universe. The idea of entering a world that has not been touched or that has evolved into a new incomprehensible parallel, makes people ponder about what it could looks like or what it could be. Authors have used this notion of an alternate reality and have created their own worlds that have given readers a different view of alternate worlds.\n\n\nParallel Universe Parallel universe or alternate reality in science fiction and fantasy is a self-contained separate reality coexisting with our own\n\nMultiverse Set of many universes. There are many specific uses of the concept, as well as systems in which a multiverse is proposed to exist in.\n\nParallel universe alternate universes, worlds, realities and dimensions in fiction.\n\nAlternate reality alternate universes, worlds, realities and dimensions in fiction.\n\nAlternate future is a possible future which never comes to pass, typically because someone travels back into the past and alters it so that the events of the alternate future cannot occur.\n\nThe idea of being unseen and hence undetectable has fascinated mankind for generations. This concept has generated scientific pursuit towards defying our physical parameters. Many authors have toyed with the idea of gaining invisibility via both science-based and fictional means. Invisibility in the actual scientific world will be a very difficult achievement, one that will involve much more complication than we have begun to delve into. Further technological developments bring us closer to our goal, while also broadening the horizon for science fiction authors performing thought experiments on the topic of invisibility.\n\nMany myths and legends include gods, spirits, angels, and demons that are often invisible or can choose to become invisible at will.\n\n\n\nInvisibility is a term that is usually used as a fantasy or science fiction term where objects are literally made unseeable by magical or technological means.\n\nThere is an undeniable link between science fact and the ideas that emerge in science fiction. Science fiction authors are inspired by actual scientific and technological discoveries, but allow themselves the freedom to project the possible future course of these discoveries and their potential impact on society, perhaps only weakly bound to the facts.\n\nAuthors are faced with obstacles presented by the realities of actual technology, however fiction allows a window for the opportunity of inventing completely imaginary technologies to move their storyline forward and maybe even still explore the outcomes of such power.\n\n\n\n\n"}
{"id": "30182396", "url": "https://en.wikipedia.org/wiki?curid=30182396", "title": "Timeline of United States inventions (1946–91)", "text": "Timeline of United States inventions (1946–91)\n\nA timeline of United States inventions (1946–1991) encompasses the ingenuity and innovative advancements of the United States within a historical context, dating from the era of the Cold War, which have been achieved by inventors who are either native-born or naturalized citizens of the United States. Copyright protection secures a person's right to his or her first-to-invent claim of the \"original\" invention in question, highlighted in Article I, Section 8, Clause 8 of the United States Constitution which gives the following enumerated power to the United States Congress:\nIn 1641, the first patent in North America was issued to Samuel Winslow by the General Court of Massachusetts for a new method of making salt. On April 10, 1790, President George Washington signed the Patent Act of 1790 (1 Stat. 109) into law which proclaimed that patents were to be authorized for \"any useful art, manufacture, engine, machine, or device, or any improvement therein not before known or used.\" On July 31, 1790, Samuel Hopkins of Pittsford, Vermont became the first person in the United States to file and to be granted a patent for an improved method of \"Making Pot and Pearl Ashes.\" The Patent Act of 1836 (Ch. 357, 5 Stat. 117) further clarified United States patent law to the extent of establishing a patent office where patent applications are filed, processed, and granted, contingent upon the language and scope of the claimant's invention, for a patent term of 14 years with an extension of up to an additional 7 years. However, the Uruguay Round Agreements Act of 1994 (URAA) changed the patent term in the United States to a total of 20 years, effective for patent applications filed on or after June 8, 1995, thus bringing United States patent law further into conformity with international patent law. The modern-day provisions of the law applied to inventions are laid out in Title 35 of the United States Code (Ch. 950, sec. 1, 66 Stat. 792).\n\nFrom 1836 to 2011, the United States Patent and Trademark Office (USPTO) has granted a total of 7,861,317 patents relating to several well-known inventions appearing throughout the timeline below. Some examples of patented inventions between the years 1946 and 1991 include William Shockley's transistor (1947), John Blankenbaker's personal computer (1971), Vinton Cerf's and Robert Kahn's Internet protocol/TCP (1973), and Martin Cooper's mobile phone (1973).\n\n1946 Space observatory\n\nA space observatory is any instrument, such as a telescope, in outer space which is used for observation of distant planets, galaxies, and other outer space objects. In 1946, American theoretical astrophysicist Lyman Spitzer was proposed the idea of a telescope in outer space, a decade before the Soviet Union launched the first artificial satellite, \"Sputnik\" into orbit. However, German scientist Hermann Oberth had first conceived the idea of a space based telescope. Spitzer's proposal called for a large telescope that would not be hindered by Earth's atmosphere. After lobbying in the 1960s and 1970s for such a system to be built, Spitzer's vision ultimately materialized into the world's first space-based optical telescope, \"Hubble Space Telescope\", which was launched on April 20, 1990 by the Space Shuttle \"Discovery\" (STS-31).\n\n1946 Blowout preventer (annular)\n\nAn annular blowout preventer is a large valve that uses a wedge to seal off a wellhead. It has a donut-like rubber seal, known as an elastomeric packing unit, reinforced with steel ribs. During drilling or well interventions, the valve may be closed if overpressure from an underground zone causes formation fluids such as oil or natural gas to enter the wellbore and threaten the rig. The annular blowout preventer was invented by Granville Sloan Knox in 1946 who received a patent on September 9, 1952.\n\n1946 Tupperware\n\nTupperware is airtight plastic containers used for the preparation, storage, containment, and serving of perishable food in the kitchen and home. Tupperware was invented in 1946 by American chemist Earl Silas Tupper who devised a method of purifying black polyethylene slag, a waste product produced in oil refinement, into a molded substance that was flexible, tough, non-porous, non-greasy and translucent. Available in many colors, the plastic containers with \"burp seal\" did not become a commercial success until Brownie Wise, a Florida housewife, began throwing Tupperware parties in 1951 in order to demonstrate the product and explain the features.\n\n1946 Spoonplug\n\nA spoonplug is a form of fishing lure. The spoonplug was invented by Elwood L. \"Buck\" Perry, then a physics and math teacher in Hickory, North Carolina. Elwood Perry combined science with a logical approach to fishing to create a \"total fishing system.\" He is credited as being the father of structure fishing and was later inducted into the National Freshwater Fishing Hall of Fame.\n\n1946 Chipper teeth\nA chipper teeth is a variant of a saw chain used on a chainsaw. Using a tooth that is curled over the top of the chain, there are alternate teeth which point left and right. In 1946, American logger Joseph Buford Cox of Portland, Oregon invented chipper teeth, which is still widely used today and represents one of the biggest influences in the history of timber harvesting.\n\n1946 Filament tape\n\nFilament tape or strapping tape is a pressure-sensitive tape used for several packaging functions such as closing corrugated fiberboard boxes, reinforcing packages, bundling items, pallet utilizing, etc. It consists of a pressure-sensitive adhesive coated onto a backing material which is usually a polypropylene or polyester film and fiberglass filaments embedded to add high tensile strength. Filament tape was invented in 1946 by Cyrus Woodrow Bemmels. In 1949, it was placed on the market and was an immediate success.\n\n1946 Credit card\n\n1946 Diaper (waterproof)\n\n1947 Transistor\n\nIn electronics, a transistor is a semiconductor device commonly used to amplify or switch electronic signals. Because the controlled output power can be much larger than the controlling input power, the transistor provides amplification of a signal. The transistor is the fundamental building block of all modern electronic devices, and is used in radio, telephone, computer, and other electronic systems. From November 17, 1947 to December 23, 1947, John Bardeen and Walter Brattain at AT&T Bell Labs, underwent experimentations and finally observed that when two gold point contacts were applied to a crystal of germanium, a signal was produced whereby the output power was larger than the input. The manager of the Bell Labs semiconductor research group, William Shockley, saw the potential in this and worked over the next few months greatly expanding the knowledge of semiconductors in order to construct the first point-contact transistor. Shockley is considered by many to be the \"father\" of the transistor. Hence, in recognition of his work, the transistor is widely, yet not universally acknowledged as the most important invention of the entire 20th century since it forms today's building blocks of processors found and used in almost every modern computing and electronics device. In recognition of their invention of the transistor, Shockley, Bardeen and Brattain were jointly awarded the 1956 Nobel Prize in Physics.\n\n1947 Defibrillator\n\nDefibrillation is the definitive treatment for the life-threatening cardiac arrhythmias, ventricular fibrillation and ventricular tachycardia. Defibrillation consists of delivering a therapeutic dose of electrical energy to the affected heart. Dr. Claude Beck invented the defibrillator in 1947.\n\n1947 Supersonic aircraft\n\nIn aerodynamics, the sound barrier usually refers to the point at which an aircraft moves from transonic to supersonic speed. On October 14, 1947, just under a month after the United States Air Force had been created as a separate service, tests culminated in the first manned supersonic flight where the sound barrier was broken, piloted by Air Force Captain Chuck Yeager in the Bell X-1.\n\n1947 Acrylic paint\n\nAcrylic paint is fast-drying paint containing pigment suspended in an acrylic polymer emulsion. The first acrylic paint was invented by Leonard Bocour and Sam Golden in 1947 under the brand Magna paint.\n\n1947 Magnetic particle clutch\n\nA magnetic particle clutch is a special type of electromagnetic clutch which does not use friction plates. Instead, it uses a fine powder of magnetically susceptible material (typically stainless steel) to mechanically link an otherwise free wheeling disc attached to one shaft, to a rotor attached to the other shaft. The magnetic particle clutch was invented in 1947 by Ukrainian-American Jacob Rabinow.\n\n1948 Windsurfing\n\nWindsurfing, or sailboarding, is a surface water sport using a windsurf board, also commonly called a sailboard, usually two to five meters long and powered by wind pushing a sail. In 1948, 20-year-old Newman Darby was the first to conceive the idea of using a handheld sail and rig mounted on a universal joint so that he could control his small catamaran—the first rudderless sailboard ever built that allowed a person to steer by shifting his or her weight in order to tilt the sail fore and aft. Darby did not file for a patent for his invention. However, he is widely recognized as the inventor of the first sailboard.\n\n1948 Hair spray\n\nHair spray is a beauty aqueous solution that is used to keep hair stiff or in a certain style. Weaker than hair gel, hair wax, or glue, it is sprayed to hold styles for a long period. Using a pump or aerosol spray nozzle, it sprays evenly over the hair. Hair spray was first invented and manufactured in 1948 by Chase Products Company, based in Broadview, Illinois.\n\n1948 Cat litter\n\n1948 Halligan bar\n\n1948 Hand dryer\n\n1948 Rogallo wing\n\nThe Rogallo wing is a flexible type of airfoil composed of two partial conic surfaces with both cones pointing forward. Neither a kite, glider, or a type of aircraft, the Rogallo wing is most often seen in toy kites, but has been used to construct spacecraft parachutes during preliminary testing for NASA's \"Gemini program\" in the early 1960s, dirigible parachutes, ultralight powered aircraft like the trike, as well as hang gliders. Before the end of 1948, American aeronautical engineer Francis Rogallo had succeeded in inventing the first fully successful flexible-wing kite that he called the 'Flexi-Kite'. A patent was applied for in 1948 and granted in 1951. His wife, Gertrude Rogallo, also made a significant impact upon the invention, having sewed the fabric into the required dimensions that used household items like kitchen curtains. Rogallo believed that flexible wings provided more stability than fixed surfaces, leading to an elimination of rigid spars during flight. Because of this, Rogallo's concepts are seen as classics examples of purity and efficiency in aviation.\n\n1948 Cable television\n\nCable television provides television to consumers via radio frequency signals transmitted to televisions through fixed optical fibers or coaxial cables as opposed to the over-the-air method used in traditional television broadcasting. First known as Community Antenna Television or CATV, cable television was born in the mountains of Pennsylvania in 1948 by John Walson and Margaret Walson.\n\n1948 Flying disc\n\nFlying discs are disc-shaped objects thrown and caught for recreation, which are generally plastic and roughly 20 to 25 centimeters (8–10 inches) in diameter, with a lip. The shape of the disc, an airfoil in cross-section, allows it to fly by generating lift as it moves through the air while rotating. First known as the \"Whirlo-Way\", the flying disc was invented in 1949 by Walter Frederick Morrison who combined his fascination with invention and his interest in flight. Carved from a solid block of a plastic compound known as \"Tenite,\" Morrison sold his flying disc invention to WHAM–O, which introduced it in 1957 as the \"Pluto Platter.\" In 1958, WHAM–O modified the \"Pluto Platter\" and rebranded it as a Frisbee flying disc to the world. It became an instant sensation.\n\n1948 Video game\n\nA video game is an electronic game that involves interaction with a user interface to generate visual feedback on a video device. In 1948, ten years before William Higinbotham's \"Tennis for Two\" was developed, Thomas T. Goldsmith Jr. and Estle R. Mann co-patented the \"Cathode-Ray Tube Amusement Device,\" making it the earliest documented video game. Primitive by modern standards in video gaming, the amusement device, however, required players to overlay pictures or illustrations of targets such as airplanes in front of the screen, dovetailing the game's action.\n\n1949 Radiocarbon dating\n\nRadiocarbon dating is a dating method that uses the naturally occurring radioisotope carbon-14 (14C) to determine the age of carbonaceous materials up to about 60,000 years. In 1949, Willard F. Libby invented the procedure for carbon-14 dating.\n\n1949 Airsickness bag\n\nAn airsickness bag, also known as a barf bag, airsick bag, sick bag, or motion sickness bag, is a small bag commonly provided to passengers on board airplanes and boats to collect and contain vomit in the event of motion sickness. The airsickness bag was invented by Gilmore Schjeldahl in 1949 for Northwest Orient Airlines.\n\n1949 Ice resurfacer\n\nAn ice resurfacer is a truck-like vehicle used to clean and smooth the surface of an ice rink. Frank J. Zamboni of Paramount, California invented the first ice resurfacer, which he called a Zamboni, in 1949.\n\n1949 Atomic clock\n\nAn atomic clock uses an atomic resonance frequency standard as its timekeeping element. The first atomic clock was an ammonia maser device built in 1949 at the United States National Bureau of Standards.\n\n1949 Holter monitor\n\nA Holter monitor is a portable device for continuously monitoring the electrical activity of the heart for 24 hours or more. Sticky patches (electrodes) on the chest are connected to wires from the Holter monitor. The functions of a Holter monitor captures and records information such as heart rates during day and night, abnormal heart beats, and normal and abnormal heart rhythms. The Holter monitor was invented by Norman Holter.\n\n1949 Crash test dummy\n\nA crash test dummy is a full-scale anthropomorphic test device that simulates the dimensions, weight proportions and articulation of the human body, and is usually instrumented to record data about the dynamic behavior of the ATD in simulated vehicle impacts. Using human and animal cadaver research from earlier studies, the first artificial crash test dummy was an anthropomorphic dummy named \"Sierra Sam\". It was invented in 1949 by Samuel W. Alderson at his Alderson Research Labs (ARL) And Sierra Engineering Co. for the United States Air Force while conducting tests on aircraft ejection seats, pilot restraint harnesses, and aviation helmets. Alderson's early dummies and those of his competitors were fairly primitive, with no pelvic structure and little spinal articulation. With American automakers interested in durable crash test dummies that could be tested and retested while yielding back a broad spectrum of data during simulated automobile crashes, the first crash test dummy used for automative testing was again invented by Samuel Alderson in 1968. It was called the V.I.P. (Very Important Person) and it was built with dimensions of an average adult man coupled with a steel rib cage, articulated joints, a flexible neck, and a lumbar spine.\n\n1949 Compiler\n\nA compiler is a computer program or set of programs that transforms source code written in a computerized source language into another computer language often having a binary form known as an object code. The most common reason for wanting to transform source code is to create an executable program. The first compiler written for the A-0 programming language is attributed to its inventor, Grace Hopper in 1949.\n\n1949 Aerosol paint\n\n1950 Artificial snowmaking\n\nSnowmaking is the artificial production of snow by forcing water and pressurized air through a \"snow gun\" or \"snow cannon\", on ski slopes. Snowmaking is mainly used at ski resorts to supplement natural snow. This allows ski resorts to improve the reliability of their snow cover and to extend their ski seasons. The costly production of snowmaking requires low temperatures. The threshold temperature for snowmaking decreases as humidity decreases. Machine-made snow was first co-invented by three engineers—Art Hunt, Dave Richey and Wayne Pierce of Milford, Connecticut on March 14, 1950. Their patented invention of the first \"snow cannon\" used a garden hose, a 10-horsepower compressor, and a spray-gun nozzle, which produced about 20 inches of snow.\n\n1950 Leaf blower\n\nA leaf blower is a gardening tool that propels air out of a nozzle to move yard debris such as leaves. Leaf blowers are usually powered by two-stroke engine or an electric motor, but four-stroke engines were recently introduced to partially address air pollution concerns. Leaf blowers are typically self-contained handheld units, or backpack mounted units with a handheld wand. The leaf blower was invented by Dom Quinto in 1950.\n\n1950 Hamming code\n\n1950 Teleprompter\n\nA teleprompter is a display device that prompts the person speaking with an electronic visual text of a speech or script. Using a teleprompter is similar to the practice of using cue cards. The screen is in front of and usually below the lens of the camera, and the words on the screen are reflected to the eyes of the performer using a sheet of clear glass or specially prepared beam splitter. The teleprompter was invented in 1950 by Hubert Schlafly, who was working at 20th Century Fox film studios in Los Angeles.\n\n1950 Sengstaken-Blakemore tube\n\nA Sengstaken-Blakemore tube is an oro or nasogastric tube used occasionally in the management of upper gastrointestinal hemorrhage due to bleeding from esophageal varices which are distended veins in the esophageal wall, usually as a result of cirrhosis. It consists of a gastric balloon, an esophageal balloon, and a gastric suction port. The Sengstaken-Blakemore tube was invented by Dr. Robert W. Sengstaken and Dr. Arthur H. Blakemore in 1950.\n\n1951 Stellarator\n\n1951 Cooler\n\n1951 Wetsuit\n\n1951 Correction fluid\n\n1951 Well counter\n\n1952 Airbag\n\nAn air bag is a safety feature designed to protect automobile passengers in a head-on collision. Most cars today have driver's side airbags and many have one on the passenger side as well. Located in the steering wheel assembly on the driver's side and in the dashboard on the passenger side, the air bag device responds within milliseconds of a crash. The original safety cushion was first created by John W. Hetrick in 1952. After a car accident that his family was involved in, Hetrick drew sketches of compressed air stored in a container. When a spring-loaded weight senses the car decelerating at a rapid enough rate, it opens a valve that allows the pressure in the container to fill a bag. With this knowledge, he developed his design until he was able to obtain a patent on the device on August 5, 1952. Later in 1967, Dr. Allen S. Breed invented and developed a key component for automotive use in 1967, the ball-in-tube inertial sensor for crash detection. Breed Corporation then marketed this innovation to Chrysler.\n\n1952 Bread clip\n\nA bread clip is a device used to hold plastic bags, such as the ones pre-sliced bread is commonly packaged in, closed. They are also commonly called bread tags, bread tabs, bread ties, bread crimps, or bread-bag clips. By sealing a bag more securely than tying or folding over its open end, the clip or tie may preserve its contents longer. The bread clip was invented in 1952 by Floyd Paxton of Yakima, Washington. Paxton never patented the device.\n\n1952 Barcode\n\nA barcode is an optical machine-readable representation of data, which shows certain data on certain products. Originally, barcodes represented data in the widths (lines) and the spacings of parallel lines, and may be referred to as linear or one-dimensional barcodes or symbologies. They also come in patterns of squares, dots, hexagons and other geometric patterns within images termed two-dimensional matrix codes or symbologies. Norman Joseph Woodland is best known for inventing the barcode for which he received a patent in October 1952.\n\n1952 Artificial heart\n\nAn artificial heart is implanted into the body to replace the biological heart. On July 3, 1952, 41-year-old Henry Opitek suffering from shortness of breath made medical history at Harper University Hospital at Wayne State University in Michigan. The Dodrill-GMR heart, considered to be the first operational mechanical heart, was invented by Dr. Forest Dewey Dodrill and successfully inserted into Henry Opitek while performing open heart surgery. In 1981, Dr. Robert Jarvik implanted the world's first permanent artificial heart, the Jarvik 7, into Dr. Barney Clark. The heart, powered by an external compressor, kept Clark alive for 112 days. The Jarvik heart was not banned for permanent use. Since 1982, more than 350 people have received the Jarvik heart as a bridge to transplantation.\n\n1953 Heart-lung machine\n\n1953 Voltmeter (digital)\n\n1953 Marker pen\n\n1953 WD-40\n\n1953 Apgar scale\n\n1953 Gilhoolie\n\n1953 Wheel clamp\n\n1953 Wiffle ball\n\n1953 MASER\n\n1953 Carbonless copy paper\n\n1953 Crossed-field amplifier\n\n1954 Zipper storage bag\n\nA zipper storage bag is a plastic bag with a sealed or zipped opening that allows for transparent viewing of stored items inside the bag. Better known under the brand name and genericized trademark \"Ziploc\", zipper storage bags are commonly used to hold perishable foods and snacks. Zipper storage bags were patented by Robert W. Vergobbi on May 18, 1954. However, they would not be introduced to consumers until 1968, when Dow Chemical introduced the \"Ziploc\" bags.\n\n1954 TV dinner\n\nA TV dinner is a prepackaged, frozen or chilled meal generally in an individual package. It requires little preparation, oven baked or microwaveable, and contains all the elements for a single-serving meal in a tray with compartments for the food. Carl A. Swanson of C.A. Swanson & Sons is generally credited for inventing the TV dinner. Retired Swanson executive Gerry Thomas said he conceived the idea after the company found itself with a huge surplus of frozen turkeys because of poor Thanksgiving sales.\n\n1954 Acoustic suspension loudspeaker\n\n1954 Model rocketry\n\n1954 Door (automatic sliding)\n\n1954 Mogen clamp\n\n1954 Cardiopulmonary resuscitation\n\n1954 Active noise control\n\n\n1954 Synthetic diamond\n\n1954 Radar gun\n\n1955 Sling lift\n\nA sling lift is an assistive device that allows patients in hospitals and nursing homes and those receiving home health care to be transferred between a bed and a chair or other similar resting places, using hydraulic power. Sling lifts are used for patients whose mobility is limited. The sling lift was patented on April 12, 1955 by Ronald R. Stratton in what he called a \"floor crane with adjustable legs\".\n\n1955 Crosby-Kugler capsule\n\nA Crosby-Kugler capsule is a device used for obtaining biopsies of small bowel mucosa, necessary for the diagnosis of various small bowel diseases. It was invented by Dr. William Holmes Crosby, Jr. in 1955.\n\n1955 Nuclear submarine\n\nThe USS \"Nautilus\", the world's first nuclear submarine, revolutionized naval warfare. Conventional submarines need two engines: a diesel engine to travel on the surface and an electric engine to travel submerged, where oxygen for a diesel engine is not available. By relying on nuclear capability, the USS \"Nautilus\" could travel uninterrupted for thousands of miles below the surface with a single fuel charge. Beginning in 1951, Admiral Hyman Rickover can be credited for the design of the world's first nuclear submarine who led and oversaw a group of scientists and engineers at the Naval Reactors Branch of the Atomic Energy Commission. After sea trials were conducted and testing was completed, the USS \"Nautilus\" became fully operational in January 1955.\n\n1955 Hard disk drive\n\n1955 Harmonic drive\n\n1955 Vibrating sample magnetometer\n\n1956 Lint roller\n\nA lint roller or lint remover is a roll of one-sided adhesive paper on a cardboard or plastic barrel that is mounted on a central spindle, with an attached handle. The device facilitates the removal of lint or other small fibers from most materials such as clothing, upholstery and linen. The lint roller was co-invented in 1956 by American electrical engineer Nicholas McKay and his wife Helen.\n\n1956 Kart racing\n\nKart racing or karting is a variant of an open-wheel motor sport with simple, small four-wheeled vehicles called karts, go-karts, or gearbox karts depending on the design. Karts vary widely in speed and some can reach speeds exceeding 160 mph, while go-karts intended for the general public in amusement parks may be limited to speeds of no more than 15 mph. In the summer of 1956, hot rod veteran Art Ingels built the first go-kart out of old car frame tubing, welding beads, and a lawnmower motor, not realizing that he had invented a new sport and form of auto racing.\n\n1956 Industrial robot\n\nAn industrial robot is an automatically controlled, re-programmable, multipurpose manipulator programmable in three or more axes. The first to invent an industrial robot was George Devol and Joseph F. Engelberger.\n\n1956 Operating system (batch processing) \n\nAn operating system (OS) is software (programs and data) that runs on computers and manages the computer hardware and provides common services for efficient execution of various application software. For hardware functions such as input and output and memory allocation, the operating system acts as an intermediary between application programs and the computer hardware, although the application code is usually executed directly by the hardware, but will frequently call the OS or be interrupted by it. Operating systems are found on almost any device that contains a computer—from cellular phones and video game consoles to supercomputers and web servers. The GM-NAA I/O, created by Owen Mock and Bob Patrick of General Motors Research Laboratories in early 1956 (or late 1955) for their IBM 701 mainframe computer is generally considered to be the first \"batch processing\" operating system and possibly the first \"real\" operating system. Rudimentary forms of operating systems existed before batch processing, the Input/Output Control System (IOCS) being one example. However, what specifically differentiated and made the GM-NAA I/O as the first of its kind was that instead of having a human operator manually load each program as what previous systems were only capable of doing, computerized software as used on GM-NAA I/O, thereafter handled the scheduling, management, and multi-tasking of all computer applications.\n\n1956 Fortran\n\n1956 Videotape\n\n1956 Particle storage ring\n\n1957 Skid-steer loader\n\nA skid loader or skid steer loader is a small rigid frame, engine-powered machine with lift arms used to attach a wide variety of labor-saving tools or attachments. Though sometimes they are equipped with tracks, skid-steer loaders are typically four-wheel drive vehicles that can push material from one location to another, carry material in its bucket, or load material into a truck or trailer. Brothers Louis and Cyrill Keller co-invented the first skid-steer loader, which was based around a three-wheeled loader they developed in 1957 for a turkey farmer near Rothsay, Minnesota. In September 1958, they were hired by the Melroe brothers at Melroe Manufacturing Company in Gwinner, North Dakota, which was later to become Bobcat Company. Using the brothers' design, Melroe introduced the M60 Self-Propelled Loader and, in 1960, Louis added a rear drive axle, resulting in the M400 model, the world's first true skid-steer loader.\n\n1957 Laser\n\nA laser is a device that emits electromagnetic radiation through a process called stimulated emission. Laser light is usually spatially coherent, which means that the light either is emitted in a narrow, low-divergence beam, or can be converted into one with the help of optical components such as lenses. Lasers are used to read compact discs and bar codes, guide missiles, remove ulcers, fabricate steel, precisely measure the distance from Earth to the Moon, record ultradefined images of brain tissue, entertain people in light shows and do thousands of other things. In 1957, American physicist Gordon Gould first theorized the idea and use of laser technology. Despite a 20-year battle with the United States Patent and Trademark Office, Gould is now widely associated as the original inventor of laser. In addition, Charles H. Townes and Arthur L. Schawlow, scientists at Bell Laboratories, wrote a paper, \"Infrared and Optical Masers\" in 1958 that was enormously influential on the theory of lasers. Ironically, Gould, Townes, or Schawlow never built the first working laser. On July 7, 1960, American physicist Theodore H. Maiman created and built the first laser. The core of his laser consisted of a man-made ruby as the active medium, a material that had been judged unsuitable by other scientists who rejected crystal cores in favor of various gases.\n\n1957 Confocal microscopy\n\n1957 Sugar packet\n\n1957 Air-bubble packing\n\nBetter known by the brand name of Bubble Wrap, air-bubble packing is a pliable transparent plastic material commonly used for the cushioning of fragile, breakable items in order to absorb or minimize shock and vibration. Regularly spaced, the protruding air-filled hemispheres are known as \"bubbles\" which are 1/4 inch (6 millimeters) in diameter, to as large as an inch (26 millimeters) or more. Air-bubble packing was co-invented by Alfred Fielding and Marc Chavannes in 1957.\n\n1957 Borazon\n\nBorazon, a boron nitride allotrope, is the fourth hardest substance, after aggregated diamond nanorods, ultrahard fullerite, and diamond, and the third hardest artificial material. Borazon is a crystal created by heating equal quantities of boron and nitrogen at temperatures greater than 1800 °Celsius, 3300 °Fahrenheit at 7 gigapascal 1 millionpound-force per square inch. Borazon was first invented in 1957 by Robert H. Wentorf, Jr., a physical chemist working for the General Electric Company. In 1969, General Electric adopted the name Borazon as its trademark for the crystal.\n\n1957 Gamma camera\n\nA gamma camera is a device used to image gamma radiation emitting radioisotopes, a technique known as scintigraphy. The applications of scintigraphy include early drug development and nuclear medical imaging to view and analyse images of the human body of the distribution of medically injected, inhaled, or ingested radionuclides emitting gamma rays. The gamma camera was invented by Hal Anger in 1957.\n\n1957 Cryotron\n\n1958 Doppler fetal monitor\n\n1958 Cable tie\n\n1958 Lisp programming language\n\n1958 Carbon fiber\n\n1958 Integrated circuit\n\nAn integrated circuit is a miniaturized electronic circuit that has been manufactured in the surface of a thin substrate of semiconductor material. Integrated circuits are used in almost all electronic equipment in use today and have revolutionized the world of electronics. The integration of large numbers of tiny transistors into a small chip was an enormous improvement over the manual assembly of circuits using discrete electronic components. On September 12, 1958, Jack Kilby developed a piece of germanium with an oscilloscope attached. While pressing a switch, the oscilloscope showed a continuous sine wave, proving that his integrated circuit worked. A patent for a \"Solid Circuit made of Germanium\", the first integrated circuit, was filed by its inventor, Jack Kilby on February 6, 1959.\n\n1959 Fusor\n\nThe fusor is an apparatus invented by Philo T. Farnsworth in 1959 to create nuclear fusion. Unlike most controlled fusion systems, which slowly heat a magnetically confined plasma, the fusor injects \"high temperature\" ions directly into a reaction chamber, thereby avoiding a considerable amount of complexity. The approach is known as inertial electrostatic confinement.\n\n1959 Weather satellite\n\n1959 Spandex\n\n1960 Child safety seat\n\n1960 Artificial turf\n\n1960 Magnetic stripe card\n\n1960 Global navigation satellite system\n\nA global navigation satellite system (GNSS) provides autonomous geo-spatial positioning with global coverage. A GNSS allows small electronic receivers to determine their location such as longitude, latitude, and altitude to within a few meters using time signals transmitted along a line of sight by radio from satellites in outer space. Receivers on the ground with a fixed position can also be used to calculate the precise time as a reference for scientific experiments. The first such system was \"Transit\", developed by the Johns Hopkins University Applied Physics Laboratory under the leadership of Richard Kershner. Development of the system for the United States Navy began in 1958, and a prototype satellite,\"Transit 1A\", was launched in September 1959. That satellite failed to reach orbit. A second satellite, \"Transit 1B\", was successfully launched April 13, 1960 by a Thor-Ablestar rocket. The last \"Transit\" satellite launch was in August 1988.\n\n1960 Combined oral contraceptive pill\n\nThe combined oral contraceptive pill, or birth-control pill, or simply \"the Pill\", is a combination of an estrogen and a progestin taken orally to inhibit normal female fertility. On May 9, 1960, the FDA announced it would approve Enovid 10 mg for contraceptive use. By the time Enovid 10 mg had been in general use for three years, at least a half a million women had used it. Beginning his research and studies in the feasibility of women's fertility in 1950, Dr. Gregory Pincus invented the combined oral contraceptive pill in 1960.\n\n1960 Obsidian hydration dating\n\nObsidian hydration dating is a geochemical method of determining age in either absolute or relative terms of an artifact made of obsidian. Obsidian hydration dating was introduced in 1960 by Irving Friedman and Robert Smith of the United States Geological Survey.\n\n1960 Gas laser\n\nA gas laser is a laser in which an electric current is discharged through a gas to produce light. The first gas laser, the Helium-neon, was invented by William R. Bennett, Don Herriott, and Ali Javan in 1960. The first continuous visible gas laser, operating at 632.8 nm in the red, was invented by A. D. White and J. D. Rigden in 1962.\n\n1961 Spreadsheet (electronic)\n\n1961 Wearable computer\n\n1961 Frozen carbonated beverage\n\n1961 Biofeedback\n\n1962 Communications satellite\n\n1962 Chimney starter\n\n1962 Light-emitting diode\n\nA light-emitting-diode (LED) is a semiconductor diode that emits light when an electric current is applied in the forward direction of the device, as in the simple LED circuit. The effect is a form of electroluminescence where incoherent and narrow-spectrum light is emitted from the p-n junction in a solid state material. The first practical visible-spectrum LED was invented in 1962 by Nick Holonyak Jr.\n\n1962 Electret microphone\n\nAn electret microphone is a type of condenser microphone, which eliminates the need for a power supply by using a permanently charged material. Electret materials have been known since the 1920s, and were proposed as condenser microphone elements several times, but were considered impractical until the foil electret type was invented at Bell Laboratories in 1962 by Jim West, using a thin metallized Teflon foil. This became the most common type, used in many applications from high-quality recording and lavalier use to built-in microphones in small sound recording devices and telephones.\n\n1962 Jet injector\n\nA jet injector is a type of medical injecting syringe that uses a high-pressure narrow jet of the injection liquid instead of a hypodermic needle to penetrate the epidermis. The jet injector was invented by Aaron Ismach in 1962.\n\n1962 Laser diode\n\n1962 Glucose meter\n\n1963 Kicktail\n\n1963 Computer mouse\n\nIn computing, a mouse is a pointing device that functions by detecting two-dimensional motion relative to its supporting surface. The mouse's motion typically translates into the motion of a pointer on a display, which allows for fine control of a Graphical User Interface. Douglas Engelbart invented the computer mouse at the Augmentation Research Center, funded by the Department of Defense's Advanced Research Projects Agency (now DARPA) in 1963. The first mouse was carved from wood and tracked motion via two wheels mounted on the bottom. Later on, a ball instead of two wheels was employed. The concept was soon overtaken by a modern and more technologically advanced optical mouse.\n\n1963 BASIC\n\nIn computer programming, BASIC is a family of high-level programming languages. The original BASIC was invented in 1963 by John George Kemeny and Thomas Eugene Kurtz at Dartmouth College in New Hampshire to provide computer access to non-science students. At the time, nearly all use of computers required writing custom software, which was something only scientists and mathematicians tended to be able to do. The language and its variants became widespread on microcomputers in the late 1970s and 1980s.\n\n1963 Balloon catheter\n\n1963 Geosynchronous satellite\n\n1964 Buffalo wings\n\nA Buffalo wing, hot wing or wing is a chicken wing section (drumette or flat) that is traditionally fried unbreaded and then coated in sauce. Classic Buffalo-style chicken wing sauce is composed of a vinegar-based cayenne pepper hot sauce and butter. They are traditionally served with celery sticks and blue cheese dressing. Buffalo wings get their name from where they were invented, at the Anchor Bar in Buffalo, New York. In 1964, Teresa Bellissimo at the family-owned Anchor Bar, covered chicken wings in her own special sauce and served them with a side of blue cheese and celery. In 1980, Frank Bellissimo, the husband of Teresa, told \"The New Yorker\" that her buffalo wings were invented out of necessity because the restaurant had gotten an overstock of chicken wings instead of other chicken parts that the couple didn't know what to do with. On the other hand, Dominic Bellissimo, the son of Frank and Teresa, disputed this story. Dominic claimed that the wings were an impromptu midnight snack that his mother created on his request while drinking with friends. Whatever the story, all of the Bellissimos have since died so there is no way to verify how buffalo wings were invented.\n\n1964 Plasma display\n\nA plasma display panel is a flat panel display common to large TV displays. Many tiny cells between two panels of glass hold an inert mixture of noble gases. The gas in the cells is electrically turned into a plasma which then excites phosphors to emit light. The monochrome plasma video display was co-invented in July 1964 at the University of Illinois at Urbana-Champaign by Donald Bitzer, H. Gene Slottow, and graduate student Robert Willson for the PLATO Computer System.\n\n1964 Moog synthesizer\n\n1964 8-track cartridge\n\n1964 Permanent press\n\n1964 Carbon dioxide laser\n\n1964 Liquid crystal display (dynamic scattering mode)\n\n1964 SQUID\n\n1964 Argon laser\n\n1965 Adaptive equalizer (automatic)\n\n1965 Snowboarding\n\nSnowboarding is a sport that involves descending a slope that is either partially or fully covered with snow on a snowboard attached to a rider's feet using a special boot set into a mounted binding. The development of snowboarding was inspired by skateboarding, surfing and skiing. The first snowboard, the Snurfer, was invented by Sherman Poppen in 1965. Snowboarding became a Winter Olympic Sport in 1998.\n\n1965 Kevlar\n\nKevlar is the registered trademark for a light, strong para-aramid synthetic fiber. Typically it is spun into ropes or fabric sheets that can be used as such or as an ingredient in composite material components. Currently, Kevlar has many applications, ranging from bicycle tires and racing sails to body armor because of its high strength-to-weight ratio. Invented at DuPont in 1965 by Stephanie Kwolek, Kevlar was first commercially used in the early 1970s as a replacement for steel in racing tires.\n\n1965 Hypertext\n\nHypertext most often refers to text on a computer that will lead the user to other, related information on demand. It is a relatively recent innovation to user interfaces, which overcomes some of the limitations of written text. Rather than remaining static like traditional text, hypertext makes possible a dynamic organization of information through links and connections called hyperlinks. Ted Nelson coined the words \"hypertext\" and \"hypermedia\" in 1965 and invented the Hypertext Editing System in 1968 at Brown University.\n\n1965 Cordless telephone\n\n1965 Space pen\n\n1965 Minicomputer\n\n1965 Compact Disc\n\n1965 Chemical laser\n\n1966 Dynamic random access memory\n\n1966 Thermosonic bonding\n\n1967 Backpack (Internal frame)\n\n1967 Light beer\n\n1967 Calculator (hand-held)\n\n1968 Racquetball\n\nRacquetball is a racquet sport played with a hollow rubber ball in an indoor or outdoor court. Joseph Sobek is credited with inventing the sport of racquetball in the Greenwich YMCA, though not with naming it. A professional tennis player and handball player, Sobek sought a fast-paced sport that was easy to learn and play. He designed the first strung paddle, devised a set of codified rules, and named his game \"paddle rackets.\"\n\n1968 Virtual reality\n\nVirtual reality (VR) is a technology which allows a user to interact with a computer-simulated environment. Most current virtual reality environments are primarily visual experiences, displayed either on a computer screen or through special or stereoscopic displays, but some simulations include additional sensory information, such as sound through speakers or headphones. In 1968, Ivan Sutherland, with the help of his student Bob Sproull, invented what is widely considered to be the first virtual reality and augmented reality (AR) head mounted display (HMD) system. It was primitive both in terms of user interface and realism, and the HMD to be worn by the user was so heavy it had to be suspended from the ceiling, and the graphics comprising the virtual environment were simple wireframe model rooms. In 1989, Jaron Lanier, the founder of VPL Research popularized the concept of virtual reality with his \"google n' gloves\" system.\n\n1968 Turtle Excluder Device\n\n1968 Zipper (ride)\n\n1969 Lunar Module\n\nThe Lunar Module was the lander portion of spacecraft built for the \"Apollo program\" by Grumman in order to achieve the transit from cislunar orbit to the surface and back. The module was also known as the LM from the manufacturer designation. NASA achieved the first test flight on January 22, 1968 using a Saturn V rocket. Six successful missions carried twelve astronauts, the first being Neil Armstrong and Buzz Aldrin on July 20, 1969, to the moon surface and safely back home to earth. Tom Kelly as a project engineer at Grumman, invented and successfully designed the Lunar Module.\n\n1969 Electromagnetic lock\n\n1969 Laser printer\n\nA laser printer is a common type of computer printer that rapidly produces high quality text and graphics on plain paper. The laser printer was invented at Xerox in 1969 by researcher Gary Starkweather, who had an improved printer working by 1971 and incorporated into a fully functional networked printer system by about a year later.\n\n1969 Bioactive glass\n\nBioactive glasses are a group of surface reactive glass-ceramics. The biocompatibility of these glasses has led them to be investigated extensively for use as implant materials in the human body to repair and replace diseased or damaged bone. Bioactive glass was invented in 1969 by Larry Hench and his colleagues at the University of Florida.\n\n1969 Wide-body aircraft\n\nA wide-body aircraft is a large airliner with two passenger aisles, also known as a twin-aisle aircraft. As the world's first wide-body aircraft, the Boeing 747, also referred to as a jumbo jet, revolutionized international travel around the globe by making non-stop and long distance travel accessible for all. Joe Sutter, the chief engineer of the jumbo jet program at The Boeing Company designed the world's first wide-body aircraft, the Boeing 747, with its first test flight on February 9, 1969.\n\n1969 Taser\n\nA Taser is an electroshock weapon that uses Electro-Muscular Disruption (EMD) technology to cause neuromuscular incapacitation (NMI) and strong muscle contractions through the involuntary stimulation of both the sensory nerves and the motor nerves. The Taser is not dependent on pain compliance, making it highly effective on subjects with high pain tolerance. For this reason it is preferred by law enforcement over traditional stun guns and other electronic control weapons. Jack Cover, a NASA researcher, invented the Taser in 1969.\n\n1969 Charge coupled device\n\nA charge-coupled device (CCD) is a device for the movement of electrical charge, usually from within the device to an area where the charge can be manipulated. This is achieved by \"shifting\" the signals between stages within the device one at a time. CCDs move charge between capacitive bins in the device, with the shift allowing for the transfer of charge between bins. Often the device is integrated with an image sensor, such as a photoelectric device to produce the charge that is being read, thus making the CCD a major technology for digital imaging. First conceived in its usefulness for computer memory, the charge coupled device was co-invented in 1969 by American physicist George E. Smith and Canadian physicist Willard Boyle at AT&T Bell Laboratories.\n\n1969 Mousepad\n\nA mousepad is a hard surface, square-shaped and rubberized mat for enhancing the usability of a computer mouse. Jack Kelley invented the mousepad in 1969.\n\n1969 Chapman Stick\n\nA polyphonic member of the guitar family, the Chapman Stick is an electric musical instrument used for music recordings to play various parts such as bass, lead, chords, and textures. The Chapman Stick looks like a wide version of the fretboard of an electric guitar, but having 8, 10 or 12 strings. The player will use both hands to sound notes by striking the strings against the fingerboard just behind the appropriate frets for the desired notes. The Chapman Stick was invented in 1969 by American jazz musician Emmett Chapman.\n\n1969 Markup language\n\n1970 Wireless local area network\n1970 Surf leash\n\n1971 Uno (card game)\n\n1971 Personal computer \n\nThe personal computer (PC) is any computer whose original sales price, size, and capabilities make it useful for individuals, and which is intended to be operated directly by an end user, with no intervening computer operator. The Kenbak-1 is officially credited by the Computer History Museum to be the world's first personal computer which was invented in 1971 by John Blankenbaker. With a price tag of $750 and after selling only 40 machines, Kenbak Corporation closed its doors in 1973.\n\n1971 Fuzzball router\n\nFuzzball routers were the first modern routers on the Internet. They were DEC LSI-11 computers loaded with router software. First conceptualized by its inventor, David L. Mills, fuzzball routers evolved as a virtual machine supporting the DEC RT-11 operating system and early developmental versions of the TCP/IP protocol and applications suite. Prototype versions of popular Internet tools, including Telnet, FTP, DNS, EGP and SMTP were first implemented and tested on fuzzball routers.\n\n1971 Supercritical airfoil\n\nA supercritical airfoil is an airfoil designed, primarily, to delay the onset of wave drag on aircraft in the transonic speed range. Supercritical airfoils are characterized by their flattened upper surface, highly cambered aft section, and greater leading edge radius as compared to traditional airfoil shapes. The supercritical airfoil was invented and designed by NASA aeronautical engineer Richard Whitcomb in the 1960s. Testing successfully commenced on a United States Navy Vought F-8U fighter through wind tunnel results in 1971.\n\n1971 Microprocessor\n\nThe microprocessor is a computer chip that processes instructions and communicates with outside devices, controlling most of the operations of a computer through the central processing unit on a single integrated circuit. The first commercially available microprocessor was a silicon-based chip, the Intel 4004, co-invented in 1971 by Ted Hoff, Federico Faggin, and Stanley Mazor for a calculator company named Busicom, and produced by Intel.\n\n1971 Floppy disk\n\nA floppy disk is a data storage medium that is composed of a disk of thin, flexible \"floppy\" magnetic storage medium encased in a square or rectangular plastic shell. In 1971 while working at IBM, David L. Noble invented the 8-inch floppy disk. Floppy disks in 8-inch, 5¼-inch, and 3½-inch formats enjoyed many years as a popular and ubiquitous form of data storage and exchange, from the mid-1970s to the late 1990s.\n\n1971 String trimmer\n\nA string trimmer is a powered handheld device that uses a flexible monofilament line instead of a blade for cutting grass and trimming other plants near objects. It consists of a cutting head at the end of a long shaft with a handle or handles and sometimes a shoulder strap. String trimmers powered by an internal combustion engine have the engine on the opposite end of the shaft from the cutting head while electric string trimmers typically have an electric motor in the cutting head. Used frequently in lawn and garden care, the string trimmer is more popularly known by the brandnames Weedeater or Weedwhacker. The string trimmer was invented in 1971 by George Ballas of Houston, Texas.\n\n1971 Memristor\n\n1971 E-mail\n\nElectronic mail, often shortened to e-mail, is a method of creating, transmitting, or storing primarily text-based human communications with digital communications systems. Ray Tomlinson as a programmer while working on the United States Department of Defense's ARPANET, invented and sent the first electronic mail on a time-sharing computer in 1971. Previously, e-mail could only be sent to users on the same computer. Tomlinson is regarded as having sent the first e-mail on a network and for making the \"@\" sign the mainstream of e-mail communications.\n\n1972 C (programming language)\n\nC is a general-purpose computer programming language originally invented in 1972 by Dennis Ritchie at the Bell Telephone Laboratories in order to implement the Unix operating system. Although C was designed for writing architecturally independent system software, it is also widely used for developing application software.\n\n1972 Video game console\n\nA video game console is an interactive entertainment computer or electronic device that produces a video display signal which can be used with a display device such as a television to display a video game. A joystick or control pad is often used to simulate and play the video game. It was not until 1972 that Magnavox released the first home video game console, the Magnavox Odyssey, invented by Ralph H. Baer.\n\n1972 Global Positioning System\n\nThe Global Positioning System (GPS) is a space-based global navigation satellite system that provides reliable, three-dimensional positioning, navigation, and timing services to worldwide users on a continuous basis in all weather, day and night, anywhere on or near the Earth. 24 satellites orbit around the Earth twice a day, transmitting signaled information to GPS receivers that take this information and use triangulation to calculate the user's exact location. Ultimately, the GPS is the descendant of the United States Navy's \"Timation\" satellite program and the United States Air Force's \"621-B\" satellite program. The invention of GPS was a collaborative and team effort. The basic architecture of GPS was devised in less than a month in 1972 by Colonel Bradford Parkinson, Mel Birnbaum, Bob Rennard, and Jim Spilker. However, Richard Easton, a son of Roger Easton who was the head of the U.S. Navy's \"Timation\" program, claims that his father invented GPS and filed U.S. patent #3,789,409 in 1974. Other names listed by Richard Easton are James Buisson, Thomas McCaskill, Don Lynch, Charles Bartholomew, Randolph Zwirn and, \"an important outsider,\" Robert Kern. Ivan Getting, while working at Raytheon, envisioned a satellite system similar to MOSAIC, a railroad mobile ballistic missile guidance system, but working more like LORAN. The GPS program was approved in December 1973, the first GPS satellite was launched in 1978, and by August 1993, 24 GPS satellites were in orbit. Initial operational capability was established in December of that same year while in February 1994, the Federal Aviation Agency (FAA) declared GPS ready for use.\n\n1972 PET scanner\n\n1972 Magnetic resonance imaging\n\n1973 Personal watercraft\n\nA personal watercraft (PWC) is a recreational watercraft that the rider sits or stands on, rather than inside of, as in a boat. Models have an inboard engine driving a pump jet that has a screw-shaped impeller to create thrust for propulsion and steering. Clayton Jacobson II is credited with inventing the personal watercraft, including both the sit-down and stand-up models in 1973.\n\n1973 E-paper\n\nElectronic paper, also called e-paper, is a display technology designed to mimic the appearance of ordinary ink on paper. Electronic paper reflects light like ordinary paper and is capable of holding text and images indefinitely without drawing electricity, while allowing the image to be changed later. Applications of e-paper technology include e-book readers capable of displaying digital versions of books, magazines and newspapers, electronic pricing labels in retail shops, time tables at bus stations, and electronic billboards. Electronic paper was invented in 1973 by Nick Sheridon at Xerox's Palo Alto Research Center. The first electronic paper, called Gyricon, consisted of polyethylene spheres between 75 and 106 micrometres across.\n\n1973 Recombinant DNA\n\n1973 Catalytic converter (three-way)\n\n1973 Mobile phone\n\nA mobile phone, or cell phone, is a long-range, electronic device used for mobile voice or data communication over a network of specialized base stations known as cell sites. Early mobile FM radio telephones were in use for many years, but since the number of radio frequencies were very limited in any area, the number of phone calls were also very limited. To solve this problem, there could be many small areas called cells which share the same frequencies. When users moved from one area to another while calling, the call would have to be switched over automatically without losing the call. In this system, a small number of radio frequencies could accommodate a huge number of calls. The first mobile call was made from a car phone in St. Louis, Missouri on June 17, 1946, but the system was impractical from what is considered a portable handset today. The equipment weighed 80 lbs, and the AT&T service, basically a massive party line, cost $30 per month plus 30 to 40 cents per local call. The basic network and supporting infrastructure of hexagonal cells used to support a mobile telephony system while remaining on the same channel were devised by Douglas H. Ring and W. Rae Young at AT&T Bell Labs in 1947. Finally in 1973, Martin Cooper invented the first handheld cellular/mobile phone. His first mobile phone call was made to Joel S. Engel in April 1973.\n\n1973 Voicemail\n\n1974 Heimlich maneuver\n\n1974 Post-it note\n\n1974 Scanning acoustic microscope\n\n1974 Quantum well laser\n\n1974 Universal Product Code\n\n1975 Digital camera\n\nThe digital camera is a camera that takes video or still photographs, digitally by recording images via an electronic image sensor. Steven Sasson as an engineer at Eastman Kodak invented and built the first digital camera using a CCD image sensor in 1975.\n\n1975 Ethernet\n\nThe ethernet is a family of frame-based computer networking technologies for local area networks (LANs). The name comes from the physical concept of the ether. It defines a number of wiring and signaling standards for the Physical Layer of the OSI networking model, through means of network access at the Media Access Control (MAC)/Data Link Layer, and a common addressing format. Robert Metcalfe, while at Xerox invented the ethernet in 1975.\n\n1975 Breakaway rim\n\nA breakaway rim is a basketball hoop that can bend slightly when a player dunks a basketball, and then instantly snap back into its original shape when the player releases it. It allows players to dunk the ball without shattering the backboard, and it reduces the possibility of wrist injuries. According to the Lemelson Center, an affiliation of the Smithsonian Institution in Washington D.C., the breakaway rim was invented by Arthur Ehrat. After six years, from July 1976 to December 1982, Ehrat received a patent (U.S. Patent No. 4,365,802). His application was rejected twice, with patent examiner Paul Shapiro noting that Frederick C. Tyner held a patent for a similar device (U.S. Patent No. 4,111,420). However, a court appeal finally ruled in favor of Ehrat, as he proved through notarized copies of canceled checks and a rough sketch of his invention, that he was working on his breakaway basketball goal in 1975 before Frederick Tyner conceived of his.\n\n1976 Gore-Tex\n\n1977 Human-powered aircraft\n\n1977 Chemical oxygen iodine laser\n\n1978 Slide Away Bed\n\n1978 Popcorn bag\n\n1978 Bulletin board system\n\n1979 Winglets\n\nWingtip devices or winglets are usually intended to improve the efficiency of fixed-wing aircraft. The concept of winglets originated in the late 19th century, but the idea remained on the drawing board. Throughout the 1970s when the price of aviation fuel started spiraling upward, NASA aeronautical engineer Richard Whitcomb began investigating and studying the feasibility of winglets in order to improve overall aerodynamics and reduce drag on aircraft. Whitcomb's tests finally culminated with the first successful test flight of his attached winglets on a KC-135 Stratotanker on July 24, 1979.\n\n1979 Polar fleece\n\nPolar fleece, or \"fleece\", is a soft napped insulating synthetic wool fabric made from polyethylene terephthalate or other synthetic fibers. Found in jackets, hoodies, and casual wear, fleece has some of wool's finest qualities but weighs a fraction of the lightest available woolens. The first form of polar fleece was invented in 1979 by Malden Mills, now Polartec LLC., which was a new, light, and strong pile fabric meant to mimic and in some ways surpass wool.\n\n1981 Stealth-aircraft\n\n1981 Control-Alt-Delete\n\n1981 Total internal reflection fluorescence microscope\n\n1981 Space shuttle\n\nThe Space Shuttle, part of the Space Transportation System (STS), is a spacecraft operated by NASA for orbital human spaceflight missions. It carries payloads to low Earth orbit, provides crew rotation for the International Space Station (ISS), and performs servicing missions. The orbiter can also recover satellites and other payloads from orbit and return them to Earth. In 1981, NASA successfully launched its reusable spacecraft called the Space Shuttle. George Mueller, an American from St. Louis, Missouri is widely credited for jump starting, designing, and overseeing the Space Shuttle program after the demise of the Apollo program in 1972.\n\n1981 Paintball\n\nPaintball is a game in which players eliminate opponents by hitting them with pellets containing paint usually shot from a carbon dioxide or compressed-gas, HPA or N20, in a powered paintball gun. The idea of the game was first conceived and co-invented in 1976 by Hayes Noel, Bob Gurnsey, and Charles Gaines. However, the game of paintball was not first played until June 27, 1981.\n\n1981 Graphic User Interface\n\nShort for Graphic User Interface, the GUI uses windows, icons, and menus to carry out commands such as opening files, deleting files, moving files, etc. and although many GUI Operating Systems are operated by using a mouse, the keyboard can also be used by using keyboard shortcuts or arrow keys. The GUI was co-invented at Xerox PARC by Alan Kay and Douglas Engelbart in 1981.\n\n1983 Internet\n\nNot to be confused with a separate application known as the World wide web which was invented much later in the early 1990s (see article on the English inventor Tim Berners-Lee), the Internet is the global system of overall interconnected computer networks that use the standardized Internet Protocol Suite (TCP/IP) to serve billions of users worldwide. It is a network of networks that consists of millions of private and public, academic, business, and government networks of local to global scope that are linked by copper wires, fiber-optic cables, wireless connections, and other technologies. The concept of packet switching of a network was first explored by Paul Baran in the early 1960s, and the mathematical formulations behind packet switching were later devised by Leonard Kleinrock. On October 29, 1969, the world's first electronic computer network, the ARPANET, was established between nodes at Leonard Kleinrock's lab at UCLA and Douglas Engelbart's lab at the Stanford Research Institute (now SRI International). Another milestone occurred in 1973 when Bob Kahn and Vinton Cerf co-invented Internet Protocol and Transmission Control Protocol while working on ARPANET at the United States Department of Defense. The first TCP/IP-wide area network was operational on January 1, 1983, when the United States' National Science Foundation (NSF) constructed the university network backbone that would later become the NSFNet. This date is held as the \"birth\" of the Internet.\n\n1983 Blind signature\n\n1983 Laser turntable\n\n1984 LCD projector\n\n1984 Pointing stick\n\n1984 Polymerase chain reaction\n\n1986 Atomic force microscope\n\n1986 Stereolithography\n\n1987 Digital Micromirror Device\n\n1987 Perl\n\n1988 Luggage (tilt-and-roll)\n\nTilt-and-roll luggage or wheeled luggage, is a variant of luggage for travelers which typically contains two-fixed wheels on one end and a telescoping handle on the opposite end for vertical movement. Tilt-and-roll luggage is pulled and thus eliminates a traveler from directly carrying his or her luggage. In 1988, Northwest Airlines pilot Robert Plath invented tilt-and-roll luggage as travelers beforehand had to carry suitcases in their hands, toss garment bags over their shoulders, or strap luggage on top of metal carts.\n\n1988 Fused deposition modeling\nFused deposition modeling, which is often referred to by its initials FDM, is a type of additive fabrication or technology commonly used within engineering design. FDM works on an \"additive\" principle by laying down material in layers. Fusion deposition modeling was invented by S. Scott Crump in 1988.\n\n1988 Tcl\nTcl, known as \"Tool Command Language\", is a scripting language most commonly used for rapid prototyping, scripted applications, GUIs and testing. Tcl is used extensively on embedded systems platforms, both in its full form and in several other small-footprinted versions. Tcl is also used for CGI scripting. Tcl was invented in the spring of 1988 by John Ousterhout while working at the University of California, Berkeley.\n\n1988 Ballistic electron emission microscopy\nBallistic electron emission microscopy or BEEM is a technique for studying ballistic electron transport through variety of materials and material interfaces. BEEM is a three terminal scanning tunneling microscopy (STM) technique that was co-invented in 1988 at the Jet Propulsion Laboratory in Pasadena California by L. Douglas Bell and William Kaiser.\n\n1988 Electron beam ion trap\n\n1988 Nicotine patch\n\n1988 Firewall\n\n1988 Resin identification code\n\n1989 ZIP file format\n\n1989 Selective laser sintering\n\n1990 Self-wringing mop\n\n1990 Sulfur lamp\n\n1991 Ant robotics\n\nTimelines of United States inventions\n\nRelated topics\n\n"}
{"id": "24864939", "url": "https://en.wikipedia.org/wiki?curid=24864939", "title": "Treadwheel crane", "text": "Treadwheel crane\n\nA treadwheel crane (Latin \"magna rota\") is a wooden, human powered, hoisting and lowering device. It was primarily used during the Roman period and the Middle Ages in the building of castles and cathedrals. The often heavy charge is lifted as the individual inside the treadwheel crane walks. The rope attached to a pulley is turned onto a spindle by the rotation of the wheel thus allowing the device to hoist or lower the affixed pallet.\n\nThe Roman \"Polyspaston\" crane, when worked by four men at both sides of the winch, could lift 3000 kg. In case the winch was replaced by a treadwheel, the maximum load even doubled to 6000 kg at only half the crew, since the treadwheel possesses a much bigger mechanical advantage due to its larger diameter. This meant that, in comparison to the construction of the ancient Egyptian pyramids, where about 50 men were needed to move a 2.5 ton stone block up the ramp (50 kg per person), the lifting capability of the Roman \"Polyspaston\" proved to be \"60 times\" more efficient (3000 kg per person). There are two surviving reliefs of Roman treadwheel cranes, the Haterii tombstone from the late first century AD being particularly detailed.\n\nFor larger weights of up to 100 t, Roman engineers set up a wooden lifting tower, a rectangular trestle which was so constructed that the column could be lifted upright in the middle of the structure by the means of human and animal-powered capstans placed on the ground around the tower.\n\nDuring the High Middle Ages, the treadwheel crane was reintroduced on a large scale after the technology had fallen into disuse in western Europe with the demise of the Western Roman Empire. The earliest reference to a treadwheel (\"magna rota\") reappears in archival literature in France about 1225, followed by an illuminated depiction in a manuscript of probably also French origin dating to 1240. In navigation, the earliest uses of harbor cranes are documented for Utrecht in 1244, Antwerp in 1263, Brugge in 1288 and Hamburg in 1291, while in England the treadwheel is not recorded before 1331.\n\nGenerally, vertical transport could be done more safely and inexpensively by cranes than by customary methods. Typical areas of application were harbours, mines, and, in particular, building sites where the treadwheel crane played a pivotal role in the construction of the lofty Gothic cathedrals. Nevertheless, both archival and pictorial sources of the time suggest that newly introduced machines like treadwheels or wheelbarrows did not completely replace more labor-intensive methods like ladders, hods and handbarrows. Rather, old and new machinery continued to coexist on medieval construction sites and harbours.\n\nApart from treadwheels, medieval depictions also show cranes to be powered manually by windlasses with radiating spokes, cranks and by the 15th century also by windlasses shaped like a ship's wheel. To smooth out irregularities of impulse and get over 'dead-spots' in the lifting process flywheels are known to be in use as early as 1123.\n\nThe exact process by which the treadwheel crane was reintroduced is not recorded, although its return to construction sites has undoubtedly to be viewed in close connection with the simultaneous rise of Gothic architecture. The reappearance of the treadwheel crane may have resulted from a technological development of the windlass from which the treadwheel structurally and mechanically evolved. Alternatively, the medieval treadwheel may represent a deliberate reinvention of its Roman counterpart drawn from Vitruvius' \"De architectura\" which was available in many monastic libraries. Its reintroduction may have been inspired, as well, by the observation of the labor-saving qualities of the waterwheel with which early treadwheels shared many structural similarities.\n\nThe medieval treadwheel was a large wooden wheel turning around a central shaft with a treadway wide enough for two workers walking side by side. While the earlier 'compass-arm' wheel had spokes directly driven into the central shaft, the more advanced 'clasp-arm' type featured arms arranged as chords to the wheel rim, giving the possibility of using a thinner shaft and providing thus a greater mechanical advantage.\n\nContrary to a popularly held belief, cranes on medieval building sites were neither placed on the extremely lightweight scaffolding used at the time nor on the thin walls of the Gothic churches which were incapable of supporting the weight of both hoisting machine and load. Rather, cranes were placed in the initial stages of construction on the ground, often within the building. When a new floor was completed, and massive tie beams of the roof connected the walls, the crane was dismantled and reassembled on the roof beams from where it was moved from bay to bay during construction of the vaults. Thus, the crane ‘grew’ and ‘wandered’ with the building with the result that today all extant construction cranes in England are found in church towers above the vaulting and below the roof, where they remained after building construction for bringing material for repairs aloft.\n\nLess frequently, medieval illuminations also show cranes mounted on the outside of walls with the stand of the machine secured to putlogs.\n\nIn contrast to modern cranes, medieval cranes and hoists - much like their counterparts in Greece and Rome - were primarily capable of a vertical lift, and not used to move loads for a considerable distance horizontally as well. Accordingly, lifting work was organized at the workplace in a different way than today. In building construction, for example, it is assumed that the crane lifted the stone blocks either from the bottom directly into place, or from a place opposite the centre of the wall from where it could deliver the blocks for two teams working at each end of the wall. Additionally, the crane master who usually gave orders at the treadwheel workers from outside the crane was able to manipulate the movement laterally by a small rope attached to the load. Slewing cranes which allowed a rotation of the load and were thus particularly suited for dockside work appeared as early as 1340. While ashlar blocks were directly lifted by sling, lewis or devil's clamp (German \"Teufelskralle\"), other objects were placed before in containers like pallets, baskets, wooden boxes or barrels.\n\nIt is noteworthy that medieval cranes rarely featured ratchets or brakes to forestall the load from running backward. This curious absence is explained by the high friction force exercised by medieval treadwheels which normally prevented the wheel from accelerating beyond control.\n\nAccording to the \"present state of knowledge\" unknown in antiquity, stationary harbour cranes are considered a new development of the Middle Ages. The typical harbour crane was a pivoting structure equipped with double treadwheels. These cranes were placed docksides for the loading and unloading of cargo where they replaced or complemented older lifting methods like see-saws, winches and yards.\n\nTwo different types of harbour cranes can be identified with a varying geographical distribution: While gantry cranes which pivoted on a central vertical axle were commonly found at the Flemish and Dutch coastside, German sea and inland harbours typically featured tower cranes where the windlass and treadwheels were situated in a solid tower with only jib arm and roof rotating. Dockside cranes were not adopted in the Mediterranean region and the highly developed Italian ports where authorities continued to rely on the more labor-intensive method of unloading goods by ramps beyond the Middle Ages.\n\nUnlike construction cranes where the work speed was determined by the relatively slow progress of the masons, harbour cranes usually featured double treadwheels to speed up loading. The two treadwheels whose diameter is estimated to be 4 m or larger were attached to each side of the axle and rotated together. Their capacity was 2–3 tons which apparently corresponded to the customary size of marine cargo. Today, according to one survey, fifteen treadwheel harbour cranes from pre-industrial times are still extant throughout Europe. Some harbour cranes were specialised at mounting masts to newly built sailing ships, such as in Danzig, Cologne and Bremen. Beside these stationary cranes, floating cranes which could be flexibly deployed in the whole port basin came into use by the 14th century.\n\nA treadwheel crane survives at Chesterfield, Derbyshire and is housed in the Museum. It has been dated to the early 14th century and was housed in the top of the church tower until its removal in 1947. It was reconstructed in the Museum for its opening in 1994.\n\nA treadwheel crane survives at Guildford, Surrey, United Kingdom. It dates from the late 17th or early 18th century and formerly stood in Friary Street. It was moved in 1970, having last been used \"ca.\" 1960 to move materials for Guildford Cathedral. It is a Scheduled Ancient Monument and a Grade II* listed building.\n\nA treadwheel crane survives at Harwich, Essex, United Kingdom. It was built in 1667 and formerly stood in the Naval Yard. It was moved to Harwich Green in 1932. The crane has two treadwheels of diameter by wide on an axle diameter. It is the only double wheel treadwheel crane in the United Kingdom. The crane is a Grade II* listed building.\n\nA reconstruction of a 13th-century treadwheel crane can be seen in action at the site Guédelon Castle, Treigny, France. It is used for lifting mortar, rubble, ashlar blocks, and wood. The object of Guédelon Castle is to build a fortress castle using only the techniques and materials of 13th-century medieval France.\n\nGdansk Crane () () was built before 1366. It was destroyed by the soviet troops during the fighting for the city in early 1945. The brick structure survived, the wooden parts have been restored. \n\nA reconstruction of a double wheel treadwheel crane is in use at Prague Castle, Czech Republic.\n\nA Treadwheel Crane was used in the film \"Evan Almighty\", when Evan complained to God that he needed a crane to construct the Ark in suburban Washington, D.C.\n\n\n"}
