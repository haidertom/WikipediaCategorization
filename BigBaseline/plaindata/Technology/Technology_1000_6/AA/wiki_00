{"id": "37856", "url": "https://en.wikipedia.org/wiki?curid=37856", "title": "Alcubierre drive", "text": "Alcubierre drive\n\nThe Alcubierre drive or Alcubierre warp drive (or Alcubierre metric, referring to metric tensor) is a speculative idea based on a solution of Einstein's field equations in general relativity as proposed by Mexican theoretical physicist Miguel Alcubierre, by which a spacecraft could achieve apparent faster-than-light travel if a configurable energy-density field lower than that of vacuum (that is, negative mass) could be created.\n\nRather than exceeding the speed of light within a local reference frame, a spacecraft would traverse distances by contracting space in front of it and expanding space behind it, resulting in effective faster-than-light travel. Objects cannot accelerate to the speed of light within normal spacetime; instead, the Alcubierre drive shifts space around an object so that the object would arrive at its destination faster than light would in normal space without breaking any physical laws.\n\nAlthough the metric proposed by Alcubierre is consistent with the Einstein field equations, it may not be physically meaningful, in which case a drive will not be possible. Even if it is physically meaningful, its possibility would not necessarily mean that a drive can be constructed. The proposed mechanism of the Alcubierre drive implies a negative energy density and therefore requires exotic matter. So if exotic matter with the correct properties can not exist, then the drive could not be constructed. However, at the close of his original article Alcubierre argued (following an argument developed by physicists analyzing traversable wormholes) that the Casimir vacuum between parallel plates could fulfill the negative-energy requirement for the Alcubierre drive.\n\nAnother possible issue is that, although the Alcubierre metric is consistent with Einstein's equations, general relativity does not incorporate quantum mechanics. Some physicists have presented arguments to suggest that a theory of quantum gravity (which would incorporate both theories) would eliminate those solutions in general relativity that allow for backwards time travel (\"see\" the chronology protection conjecture) and thus make the Alcubierre drive invalid.\n\nIn 1994, Alcubierre proposed a theory of physics for a method for changing the geometry of space by creating a wave that would cause the fabric of space ahead of a spacecraft to contract and the space behind it to expand. The ship would then ride this wave inside a region of flat space, known as a \"warp bubble\", and would not move within this bubble but instead be carried along as the region itself moves due to the actions of the drive. It was thought to use too much negative energy until Harold Sonny White said that the amount of energy required could be reduced if the warp bubble were changed into a warp ring.\n\nThe Alcubierre metric defines the warp-drive spacetime. It is a Lorentzian manifold that, if interpreted in the context of general relativity, allows a warp bubble to appear in previously flat spacetime and move away effectively faster than lightspeed. The interior of the bubble is an inertial reference frame and inhabitants suffer no proper acceleration. This method of transport does not involve objects in motion at speeds faster than light with respect to the contents of the warp bubble; that is, a light beam within the warp bubble would still always move faster than the ship. Because objects within the bubble are not moving (locally) faster than light, the mathematical formulation of the Alcubierre metric is consistent with the conventional claims of the laws of relativity (namely, that an object with mass cannot attain or exceed the speed of light) and conventional relativistic effects such as time dilation would not apply as they would with conventional motion at near-light speeds.\n\nThe Alcubierre drive, however, remains a hypothetical concept with seemingly difficult problems, though the amount of energy required is no longer thought to be unobtainably large.\n\nUsing the ADM formalism of general relativity, the spacetime is described by a foliation of space-like hypersurfaces of constant coordinate time , with the metric taking the following general form:\n\nwhere\n\nThe particular form that Alcubierre studied is defined by:\n\nwhere\n\nwith arbitrary parameters and . Alcubierre's specific form of the metric can thus be written\n\nWith this particular form of the metric, it can be shown that the energy density measured by observers whose 4-velocity is normal to the hypersurfaces is given by\n\nwhere is the determinant of the metric tensor.\n\nThus, because the energy density is negative, one needs exotic matter to travel faster than the speed of light. The existence of exotic matter is not theoretically ruled out; however, generating and sustaining enough exotic matter to perform feats such as faster-than-light travel (and also to keep open the \"throat\" of a wormhole) is thought to be impractical. Low has argued that within the context of general relativity, it is impossible to construct a warp drive in the absence of exotic matter.\n\nFor those familiar with the effects of special relativity, such as Lorentz contraction and time dilation, the Alcubierre metric has some apparently peculiar aspects. In particular, Alcubierre has shown that a ship using an Alcubierre drive travels on a free-fall geodesic even while the warp bubble is accelerating: its crew would be in free fall while accelerating without experiencing accelerational g-forces. Enormous tidal forces, however, would be present near the edges of the flat-space volume because of the large space curvature there, but suitable specification of the metric would keep them very small within the volume occupied by the ship.\n\nThe original warp-drive metric and simple variants of it happen to have the ADM form, which is often used in discussing the initial-value formulation of general relativity. This might explain the widespread misconception that this spacetime is a \"solution\" of the field equation of general relativity. Metrics in ADM form are \"adapted\" to a certain family of inertial observers, but these observers are not really physically distinguished from other such families. Alcubierre interpreted his \"warp bubble\" in terms of a contraction of space ahead of the bubble and an expansion behind, but this interpretation could be misleading, since the contraction and expansion actually refer to the relative motion of nearby members of the family of ADM observers.\n\nIn general relativity, one often first specifies a plausible distribution of matter and energy, and then finds the geometry of the spacetime associated with it; but it is also possible to run the Einstein field equations in the other direction, first specifying a metric and then finding the energy–momentum tensor associated with it, and this is what Alcubierre did in building his metric. This practice means that the solution can violate various energy conditions and require exotic matter. The need for exotic matter raises questions about whether one can distribute the matter in an initial spacetime that lacks a warp bubble in such a way that the bubble is created at a later time, although some physicists have proposed models of dynamical warp-drive spacetimes in which a warp bubble is formed in a previously flat space. Moreover, according to Serguei Krasnikov, generating a bubble in a previously flat space for a \"one-way\" FTL trip requires forcing the exotic matter to move at local faster-than-light speeds, something that would require the existence of tachyons, although Krasnikov also notes that when the spacetime is not flat from the outset, a similar result could be achieved without tachyons by placing in advance some devices along the travel path and programming them to come into operation at preassigned moments and to operate in a preassigned manner. Some suggested methods avoid the problem of tachyonic motion, but would probably generate a naked singularity at the front of the bubble. Allen Everett and Thomas Roman comment on Krasnikov's finding: \"[The finding] does not mean that Alcubierre bubbles, if it were possible to create them, could not be used as a means of superluminal travel. It only means that the actions required to change the metric and create the bubble must be taken beforehand by some observer whose forward light cone contains the entire trajectory of the bubble.\"\n\nFor example, if one wanted to travel to Deneb (2,600 light years away) and arrive less than 2,600 years in the future according to external clocks, it would be required that someone had already begun work on warping the space from Earth to Deneb at least 2,600 years ago: \"A spaceship appropriately located with respect to the bubble trajectory could then choose to enter the bubble, rather like a passenger catching a passing trolley car, and thus make the superluminal journey...as Krasnikov points out, causality considerations do not prevent the crew of a spaceship from arranging, by their own actions, to complete a \"round trip\" from Earth to a distant star and back in an arbitrarily short time, as measured by clocks on Earth, by altering the metric along the path of their outbound trip.\"\n\nWithin the framework of conformal gravity, an extension of general relativity in which the angles of spacetime are locally preserved, the Alcubierre metric does not violate the weak energy condition for particular spacetime shapes, and hence faster-than-light travel would not require exotic matter.\n\nThe metric of this form has significant difficulties because all known warp-drive spacetime theories violate various energy conditions. Nevertheless, an Alcubierre-type warp drive might be realized by exploiting certain experimentally verified quantum phenomena, such as the Casimir effect, that lead to stress–energy tensors that also violate the energy conditions, such as negative mass–energy, when described in the context of the quantum field theories.\n\nIf certain quantum inequalities conjectured by Ford and Roman hold, the energy requirements for some warp drives may be unfeasibly large as well as negative. For example, the energy equivalent of −10 kg might be required to transport a small spaceship across the Milky Way—an amount orders of magnitude greater than the estimated mass of the observable universe. Counterarguments to these apparent problems have also been offered.\n\nChris Van den Broeck of the Katholieke Universiteit Leuven in Belgium, in 1999, tried to address the potential issues. By contracting the 3+1-dimensional surface area of the bubble being transported by the drive, while at the same time expanding the three-dimensional volume contained inside, Van den Broeck was able to reduce the total energy needed to transport small atoms to less than three solar masses. Later, by slightly modifying the Van den Broeck metric, Serguei Krasnikov reduced the necessary total amount of negative mass to a few milligrams. Van den Broeck detailed this by saying that the total energy can be reduced dramatically by keeping the surface area of the warp bubble itself microscopically small, while at the same time expanding the spatial volume inside the bubble. However, Van den Broeck concludes that the energy densities required are still unachievable, as are the small size (a few orders of magnitude above the Planck scale) of the spacetime structures needed.\n\nIn 2012, physicist Harold White and collaborators announced that modifying the geometry of exotic matter could reduce the mass–energy requirements for a macroscopic space ship from the equivalent of the planet Jupiter to that of the Voyager 1 spacecraft (~700 kg) or less, and stated their intent to perform small-scale experiments in constructing warp fields. White proposed changing the shape of the warp bubble from a sphere to a torus. Furthermore, if the intensity of the space warp can be oscillated over time, the energy required is reduced even more. According to White, a modified Michelson–Morley interferometer could test the idea: one of the legs of the interferometer would appear to have a slightly different length when the test devices were energised.\n\nKrasnikov proposed that if tachyonic matter cannot be found or used, then a solution might be to arrange for masses along the path of the vessel to be set in motion in such a way that the required field was produced. But in this case, the Alcubierre drive vessel can only travel routes that, like a railroad, have first been equipped with the necessary infrastructure. The pilot inside the bubble is causally disconnected with its walls and cannot carry out any action outside the bubble: the bubble cannot be used for the first trip to a distant star because the pilot cannot place infrastructure ahead of the bubble while \"in transit\". For example, travelling to Vega (which is 25 light-years from Earth) requires arranging everything so that the bubble moving toward Vega with a superluminal velocity would appear; such arrangements will always take more than 25 years.\n\nCoule has argued that schemes, such as the one proposed by Alcubierre, are infeasible because matter placed \"en route\" of the intended path of a craft must be placed at superluminal speed—that constructing an Alcubierre drive requires an Alcubierre drive even if the metric that allows it is physically meaningful. Coule further argues that an analogous objection will apply to \"any\" proposed method of constructing an Alcubierre drive.\n\nAn article by José Natário (2002) argues that crew members could not control, steer or stop the ship because the ship could not send signals to the front of the bubble.\n\nA more recent article by Carlos Barceló, Stefano Finazzi, and Stefano Liberati uses quantum theory to argue that the Alcubierre drive at faster-than-light velocities is impossible mostly because extremely high temperatures caused by Hawking radiation would destroy anything inside the bubble at superluminal velocities and destabilize the bubble itself; the article also argues that these problems are absent if the bubble velocity is subluminal, although the drive still requires exotic matter.\n\nBrendan McMonigal, Geraint F. Lewis, and Philip O'Byrne have argued that when an Alcubierre-driven ship decelerates from superluminal speed, the particles that its bubble has gathered in transit would be released in energetic outbursts akin to a sonic boom shockwave; in the case of forward-facing particles, energetic enough to destroy anything at the destination directly in front of the ship.\n\nThe amount of negative energy required for such a propulsion is not yet known. Pfenning and Allen Everett of Tufts hold that a warp bubble traveling at 10 times light-speed must have a wall thickness of no more than 10 meters—close to the limiting Planck length, 1.6 × 10 meters. In Miguel Alcubierre's original calculations, a bubble macroscopically large enough to enclose a ship of 200 meters would require a total amount of exotic matter greater than the mass of the observable universe, and straining the exotic matter to an extremely thin band of 10 meters is considered impractical. Similar constraints apply to Krasnikov's superluminal subway. Chris Van den Broeck recently constructed a modification of Alcubierre's model that requires much less exotic matter but places the ship in a curved space-time \"bottle\" whose neck is about 10 meters.\n\nCalculations by physicist Allen Everett show that warp bubbles could be used to create closed timelike curves in general relativity, meaning that the theory predicts that they could be used for backwards time travel. While it is possible that the fundamental laws of physics might allow closed timelike curves, the chronology protection conjecture hypothesizes that in all cases where the classical theory of general relativity allows them, quantum effects would intervene to eliminate the possibility, making these spacetimes impossible to realize (a possible type of effect that would accomplish this is a buildup of vacuum fluctuations on the border of the region of spacetime where time travel would first become possible, causing the energy density to become high enough to destroy the system that would otherwise become a time machine). Some results in semiclassical gravity appear to support the conjecture, including a calculation dealing specifically with quantum effects in warp-drive spacetimes that suggested that warp bubbles would be semiclassically unstable, but ultimately the conjecture can only be decided by a full theory of quantum gravity.\n\nMiguel Alcubierre briefly discusses some of these issues in a series of lecture slides posted online, where he writes: \"beware: in relativity, any method to travel faster than light can in principle be used to travel back in time (a time machine)\". In the next slide he brings up the chronology protection conjecture and writes: \"The conjecture has not been proven (it wouldn’t be a conjecture if it had), but there are good arguments in its favor based on quantum field theory. The conjecture does not prohibit faster-than-light travel. It just states that if a method to travel faster than light exists, and one tries to use it to build a time machine, something will go wrong: the energy accumulated will explode, or it will create a black hole.\"\n\nIn 2012, a NASA laboratory announced that they had constructed an interferometer that they claim will detect the spatial distortions produced by the expanding and contracting spacetime of the Alcubierre metric. The work has been described in \"Warp Field Mechanics 101\", a NASA paper by Harold Sonny White. Alcubierre has expressed skepticism about the experiment, saying: \"from my understanding there is no way it can be done, probably not for centuries if at all\".\n\nThe \"Star Trek\" television series used the term \"warp drive\" to describe their method of faster-than-light travel. Neither the Alcubierre theory, nor anything similar, existed when the series was conceived, but Alcubierre stated in an email to William Shatner that his theory was directly inspired by the term used in the show and references the \"'warp drive' of science fiction\" in his 1994 article. The 1975 \"Star Trek Star Fleet Technical Manual\" suggests that the essential concept is the same as that later proposed by Alcubierre.\n\n\n\n\n"}
{"id": "18498287", "url": "https://en.wikipedia.org/wiki?curid=18498287", "title": "An Introduction to Sustainable Development", "text": "An Introduction to Sustainable Development\n\nAn Introduction to Sustainable Development is a 2007 Earthscan book which presents sustainable development as a process that \"meets the needs of the present generation without compromising the ability of future generations to meet their own needs\". This textbook examines the environmental, economic, and social dimensions of sustainable development by exploring changing patterns of consumption, production, and distribution of resources. Case studies include coastal wetlands; community-based water supply and sanitation systems; and sustainable energy, forest, and industrial development.\n\nAuthor Peter P. Rogers is a Professor of Environmental Engineering at Harvard University, USA. Co-authors Kazi F. Jalal and John A. Boyd are lecturers at Harvard’s Extension School.\n"}
{"id": "20657020", "url": "https://en.wikipedia.org/wiki?curid=20657020", "title": "Anti-nuclear protests", "text": "Anti-nuclear protests\n\nAnti-nuclear protests began on a small scale in the U.S. as early as 1946 in response to Operation Crossroads. Large scale anti-nuclear protests first emerged in the mid-1950s in Japan in the wake of the March 1954 Lucky Dragon Incident. August 1955 saw the first meeting of the World Conference against Atomic and Hydrogen Bombs, which had around 3,000 participants from Japan and other nations. Protests began in Britain in the late 1950s and early 1960s. In the United Kingdom, the first Aldermaston March, organised by the Campaign for Nuclear Disarmament, took place in 1958. In 1961, at the height of the Cold War, about 50,000 women brought together by Women Strike for Peace marched in 60 cities in the United States to demonstrate against nuclear weapons. In 1964, Peace Marches in several Australian capital cities featured \"Ban the Bomb\" placards.\n\nNuclear power became an issue of major public protest in the 1970s and demonstrations in France and West Germany began in 1971. In France, between 1975 and 1977, some 175,000 people protested against nuclear power in ten demonstrations. In West Germany, between February 1975 and April 1979, some 280,000 people were involved in seven demonstrations at nuclear sites. Many mass demonstrations took place in the aftermath of the 1979 Three Mile Island accident and a New York City protest in September 1979 involved two hundred thousand people. Some 120,000 people demonstrated against nuclear power in Bonn, in October 1979. In May 1986, following the Chernobyl disaster, an estimated 150,000 to 200,000 people marched in Rome to protest against the Italian nuclear program, and clashes between anti-nuclear protesters and police became common in West Germany.\n\nIn the early 1980s, the revival of the nuclear arms race triggered large protests about nuclear weapons. In October 1981 half a million people took to the streets in several cities in Italy, more than 250,000 people protested in Bonn, 250,000 demonstrated in London, and 100,000 marched in Brussels. The largest anti-nuclear protest was held on June 12, 1982, when one million people demonstrated in New York City against nuclear weapons. In October 1983, nearly 3 million people across western Europe protested nuclear missile deployments and demanded an end to the arms race; the largest crowd of almost one million people assembled in the Hague in the Netherlands. In Britain, 400,000 people participated in what was probably the largest demonstration in British history.\n\nOn May 1, 2005, 40,000 anti-nuclear/anti-war protesters marched past the United Nations in New York, 60 years after the atomic bombings of Hiroshima and Nagasaki. This was the largest anti-nuclear rally in the U.S. for several decades. In 2005 in Britain, there were many protests about the government's proposal to replace the aging Trident weapons system with a newer model. The largest protest had 100,000 participants. In May 2010, some 25,000 people, including members of peace organizations and 1945 atomic bomb survivors, marched from downtown New York to the United Nations headquarters, calling for the elimination of nuclear weapons.\n\nThe 2011 Japanese nuclear accidents undermined the nuclear power industry's proposed renaissance and revived anti-nuclear passions worldwide, putting governments on the defensive. There were large protests in Germany, India, Japan, Switzerland, and Taiwan.\n\nIn 1964, Peace Marches which featured \"Ban the bomb\" placards, were held in several Australian capital cities.\n\nIn 1972, the anti-nuclear weapons movement maintained a presence in the Pacific, largely in response to French nuclear testing there. Activists, including David McTaggart from Greenpeace, defied the French government by sailing small vessels into the test zone and interrupting the testing program. In Australia, thousands joined protest marches in Adelaide, Melbourne, Brisbane, and Sydney. Scientists issued statements demanding an end to the tests; unions refused to load French ships, service French planes, or carry French mail; and consumers boycotted French products. In Fiji, activists formed an Against Testing on Mururoa organization.\n\nIn November and December 1976, 7,000 people marched through the streets of Australian cities, protesting against uranium mining. The Uranium Moratorium group was formed and it called for a five-year moritorium on uranium mining. In April 1977 the first national demonstration co-ordinated by the Uranium Moratorium brought around 15,000 demonstrators into the streets of Melbourne, 5,000 in Sydney, and smaller numbers elsewhere. A National signature campaign attracted over 250,000 signatures calling for a five-year moratorium. In August, another demonstration brought 50,000 people out nationally and the opposition to uranium mining looked like a potential political force.\n\nOn Palm Sunday 1982, an estimated 100,000 Australians participated in anti-nuclear rallies in the nation's largest cities. Growing year by year, the rallies drew 350,000 participants in 1985. The movement focused on halting Australia's uranium mining and exports, abolishing nuclear weapons, removing foreign military bases from Australia's soil, and creating a nuclear-free Pacific.\n\nOn Dec 17th 2001, 46 Greenpeace activists occupied the Lucas Heights facility to protest the construction of a second research reactor. Protestors gained access to the grounds, the HIFAR reactor, the high-level radioactive waste store and the radio tower. Their protest highlighted the security and environmental risks of the production of nuclear materials and the shipment of radioactive waste from the facility.\n\nIn March 2012, hundreds of anti-nuclear demonstrators converged on the Australian headquarters of global mining giants BHP Billiton and Rio Tinto to mark one year since the Fukushima nuclear disaster. The 500-strong march through southern Melbourne called for an end to uranium mining in Australia. There were also events in Sydney, and in Melbourne the protest included speeches and performances by representatives of the expatriate Japanese community as well as Australia's Indigenous communities, who are worried about the effects of uranium mining near tribal lands.\n\nAs early as 1993 there were local and international protests against the Temelin Nuclear Power Plant's construction. Large grassroots civil disobedience actions took place in 1996 and 1997. These were organized by the so-called Clean Energy Brigades. In September and October 2000, Austrian anti-nuclear protesters demonstrated against the Temelin Nuclear Power Plant and at one stage temporarily blocked all 26 border crossings between Austria and the Czech Republic. The first reactor was finally commissioned in 2000 and the second in 2002.\n\nIn 1971, 15,000 people demonstrated against French plans to locate the first light-water reactor power plant in Bugey. This was the first of a series of mass protests organized at nearly every planned nuclear site in France until the massive demonstration at the Superphénix breeder reactor in Creys-Malvillein in 1977 culminated in violence.\n\nIn France, between 1975 and 1977, some 175,000 people protested against nuclear power in ten demonstrations.\n\nIn January 2004, up to 15,000 anti-nuclear protesters marched in Paris against a new generation of nuclear reactors, the European Pressurised Water Reactor (EPWR).\n\nOn March 17, 2007 simultaneous protests, organised by \"Sortir du nucléaire\", were staged in five French towns to protest construction of EPR plants; Rennes, Lyon, Toulouse, Lille, and Strasbourg.\n\nFollowing the 2011 Fukushima I nuclear accidents, around 1,000 people took part in a protest against nuclear power in Paris on March 20. Most of the protests, however, are focused on the closure of the Fessenheim Nuclear Power Plant, where some 3,800 French and Germans demonstrated on April 8 and April 25.\n\nThousands staged anti-nuclear protests around France, on the eve of the 25th anniversary of Chernobyl and after Japan's Fukushima nuclear disaster, demanding reactors be closed. Protesters' demands were focused on getting France to shut its oldest nuclear power station at Fessenheim, which lies in a densely populated part of France, less than two kilometres from Germany and around 40 kilometres (25 miles) from Switzerland.\n\nAround 2,000 people also protested at the Cattenom nuclear plant, France's second most powerful, in the Mosel region to the northwest of Strasbourg. Protesters in southwestern France staged another demonstration in the form of a mass picnic in front of the Blayais nuclear reactor, also in memory of Chernobyl. In France's northwestern region of Brittany, around 800 people staged a good-humoured march in front of the Brennilis experimental heavy-water atomic plant that was built in the 1960s. It was taken offline in 1985 but its dismantling is still not completed after 25 years.\n\nThree months after the Fukushima nuclear disaster, thousands of anti-nuclear campaigners protested in Paris.\n\nOn June 26, 2011, around 5,000 protesters gathered near Fessenheim nuclear power plant, demanding the plant be shut down immediately. Demonstrators from France and Germany came to Fessenheim and formed a human chain along the road. Protesters claim that the plant is vulnerable to flooding and earthquakes. Fessenheim has become a flashpoint in renewed debate over nuclear safety in France after the Fukushima accident. The plant is operated by French power group EDF.\n\nIn November 2011, thousands of anti-nuclear protesters delayed a train carrying radioactive waste from France to Germany. Many clashes and obstructions made the journey the slowest one since the annual shipments of radioactive waste began in 1995. The shipment, the first since Japan's Fukishima nuclear disaster, faced large protests in France where activists damaged the train tracks. Thousands of people in Germany also interrupted the train's journey, forcing it to proceed at a snail's pace, covering 1,200 kilometers (746 miles) in 109 hours. More than 200 people were reported injured in the protests and several arrests were made.\n\nOn December 5, 2011, nine Greenpeace activists cut through a fence at the Nogent Nuclear Power Plant. They scaled the roof of the domed reactor building and unfurled a \"Safe Nuclear Doesn't Exist\" banner before attracting the attention of security guards. Two activists remained at large for four hours. On the same day, two more campaigners breached the perimeter of the Cruas Nuclear Power Plant, escaping detection for more than 14 hours, while posting videos of their sit-in on the internet.\n\nIn Aquitaine, the local group TchernoBlaye continue to protest against the continued operation of the Blayais Nuclear Power Plant.\n\nOn the first anniversary of the Fukushima nuclear disaster, organisers of French anti-nuclear demonstrations claim 60,000 supporters formed a human chain 230 kilometres long, stretching from Lyon to Avignon.\n\nIn March 2014, police arrested 57 Greenpeace protesters who used a truck to break through security barriers and enter the Fessenheim nuclear power plant in eastern France. The activists hung antinuclear banners, but France’s nuclear safety authority said that the plant’s security had not been compromised. President Hollande has promised to close Fessenheim by 2016, but Greenpeace wants immediate closure.\n\nIn 1971, the town of Wyhl, in Germany, was a proposed site for a nuclear power station. In the years that followed, public opposition steadily mounted, and there were large protests. Television coverage of police dragging away farmers and their wives helped to turn nuclear power into a major issue. In 1975, an administrative court withdrew the construction licence for the plant. The Wyhl experience encouraged the formation of citizen action groups near other planned nuclear sites. Many other anti-nuclear groups formed elsewhere, in support of these local struggles, and some existing citizen action groups widened their aims to include the nuclear issue.\n\nIn West Germany, between February 1975 and April 1979, some 280,000 people were involved in seven demonstrations at nuclear sites. Several site occupations were also attempted. In the aftermath of the Three Mile Island accident in 1979, some 120,000 people attended a demonstration against nuclear power in Bonn.\n\nIn 1981, Germany's largest anti-nuclear power demonstration took place to protest against the construction of the Brokdorf Nuclear Power Plant on the North Sea coast west of Hamburg. Some 100,000 people came face to face with 10,000 police officers. Twenty-one policemen were injured by demonstrators armed with gasoline bombs, sticks, stones and high-powered slingshots.\n\nThe largest anti-nuclear protest was most likely a 1983 nuclear weapons protest in West Berlin which had about 600,000 participants.\n\nIn October 1983, nearly 3 million people across western Europe protested nuclear missile deployments and demanded an end to the arms race. The largest turnout of protesters occurred in West Germany when, on a single day, 400,000 people marched in Bonn, 400,000 in Hamburg, 250,000 in Stuttgart, and 100,000 in West Berlin.\n\nIn May 1986, following the Chernobyl disaster, clashes between anti-nuclear protesters and West German police became common. More than 400 people were injured in mid-May at the site of a nuclear-waste reprocessing plant being built near Wackersdorf. Police \"used water cannons and dropped tear-gas grenades from helicopters to subdue protesters armed with slingshots, crowbars and Molotov cocktails\".\n\nDuring a weekend in October 2008, some 15,000 people disrupted the transport of radioactive nuclear waste from France to a dump in Germany. This was one of the largest such protests in many years and, according to \"Der Spiegel\", it signals a revival of the anti-nuclear movement in Germany. In 2009, the coalition of green parties in the European parliament, who are unanimous in their anti-nuclear position, increased their presence in the parliament from 5.5% to 7.1% (52 seats).\n\nA convoy of 350 farm tractors and 50,000 protesters took part in an anti-nuclear rally in Berlin on September 5, 2009. The marchers demanded that Germany close all nuclear plants by 2020 and close the Gorleben radioactive dump. Gorleben is the focus of the anti-nuclear movement in Germany, which has tried to derail train transports of waste and to destroy or block the approach roads to the site. Two above-ground storage units house 3,500 containers of radioactive sludge and thousands of tonnes of spent fuel rods.\n\nFollowing the Fukushima I nuclear accidents, anti-nuclear opposition intensified in Germany. On 12 March 2011, 60,000 Germans formed a 45-km human chain from Stuttgart to the Neckarwestheim power plant. On 14 March, 110,000 people protested in 450 other German towns, with opinion polls indicating 80% of Germans opposed the government's extension of nuclear power. On March 15, 2011, Angela Merkel said that seven nuclear power plants which went online before 1980 would be temporarily closed and the time would be used to study speedier renewable energy commercialization.\n\nIn March 2011, more than 200,000 people took part in anti-nuclear protests in four large German cities, on the eve of state elections. Organisers called it the biggest anti-nuclear demonstration the country has seen. Thousands of Germans demanding an end to the use of nuclear power took part in nationwide demonstrations on 2 April 2011. About 7,000 people took part in anti-nuclear protests in Bremen. About 3,000 people protested outside of RWE's headquarters in Essen.\n\nThousands of Germans demanding an end to the use of nuclear power took part in nationwide demonstrations on 2 April 2011. About 7,000 people took part in anti-nuclear protests in Bremen. About 3,000 people protested outside of RWE's headquarters in Essen. Other smaller rallies were held elsewhere.\n\nChancellor Angela Merkel's coalition announced on May 30, 2011, that Germany’s 17 nuclear power stations will be shut down by 2022, in a policy reversal following Japan's Fukushima I nuclear accidents. Seven of the German power stations were closed temporarily in March, and they will remain off-line and be permanently decommissioned. An eighth was already off line, and will stay so.\n\nIn November 2011, thousands of anti-nuclear protesters delayed a train carrying radioactive waste from France to Germany. Many clashes and obstructions made the journey the slowest one since the annual shipments of radioactive waste began in 1995. The shipment, the first since Japan's Fukishima nuclear disaster, faced large protests in France where activists damaged the train tracks.\n\nFollowing the March 2011 Fukushima disaster, many are questioning the mass roll-out of new plants in India, including the World Bank, the former Indian Environment Minister, Jairam Ramesh, and the former head of the country's nuclear regulatory body, A. Gopalakrishnan. The massive Jaitapur Nuclear Power Project is the focus of concern — \"931 hectares of farmland will be needed to build the reactors, land that is now home to 10,000 people, their mango orchards, cashew trees and rice fields\" — and it has attracted many protests. Fishermen in the region say their livelihoods will be wiped out.\n\nEnvironmentalists, local farmers and fishermen have been protesting for months over the planned six-reactor nuclear power complex on the plains of Jaitapur, 420 km south of Mumbai. If built, it would be one of the world's largest nuclear power complexes. Protests have escalated in the wake of Japan's Fukushima I nuclear accidents. During two days of violent rallies in April 2011, a local man was killed and dozens were injured.\n\nAs of October 2011, thousands of protesters and villagers living around the Russian-built Koodankulam Nuclear Power Plant in the southern Tamil Nadu province, are blocking highways and staging hunger strikes, preventing further construction work, and demanding its closure as they fear of the disasters like the Environmental impact of nuclear power, Radioactive waste, nuclear accident similar to the releases of radioactivity in March at Japan's Fukushima nuclear disaster.\n\nA Public Interest Litigation (PIL) has also been filed against the government’s civil nuclear program at the apex Supreme Court. The PIL specifically asks for the \"staying of all proposed nuclear power plants till satisfactory safety measures and cost-benefit analyses are completed by independent agencies\".\n\nThe People's Movement Against Nuclear Energy is an anti-nuclear power group in Tamil Nadu, India. The aim of the group is to close the Kudankulam Nuclear Power Plant site and to preserve the largely untouched coastal landscape, as well as educate locals about nuclear power. In March 2012, police said they had arrested nearly 200 anti-nuclear activists who were protesting the restart of work at the long-stalled nuclear power plant. Engineers have resumed working on one of two 1,000-megawatt Koodankulam nuclear reactors a day after the local government gave the green light for the resumption of the Russia-backed project.\n\nIn May 1986, an estimated 150,000 to 200,000 people marched in Rome to protest against the Italian nuclear program, and 50,000 marched in Milan.\n\nIn March 1982 some 200,000 people participated in a nuclear disarmament rally in Hiroshima. In May 1982, 400,000 people demonstrated in Tokyo. In mid-April, 17,000 people protested at two demonstrations in Tokyo against nuclear power.\n\nIn 1982, Chugoku Electric Power Company proposed building a nuclear power plant near Iwaishima, but many residents opposed the idea, and the island’s fishing cooperative voted overwhelmingly against the plans. In January 1983, almost 400 islanders staged a protest march, which was the first of more than 1,000 protests the islanders carried out. Since the Fukushima nuclear disaster in March 2011 there has been wider opposition to construction plans for the plant.\n\nResearch results show that some 95 post-war attempts to site and build nuclear power plants resulted in only 54 completions. Many affected communities \"fought back in highly publicized battles\". Co-ordinated opposition groups, such as the Citizens' Nuclear Information Center and the anti-nuclear newspaper \"Hangenpatsu Shinbun\" have operated since the early 1980s. Cancelled plant orders included:\n\n\nIn May 2006, an international awareness campaign about the dangers of the Rokkasho Reprocessing Plant, Stop Rokkasho, was launched by musician Ryuichi Sakamoto. Greenpeace has opposed the Rokkasho Reprocessing Plant under a campaign called \"Wings of Peace – No more Hiroshima Nagasaki\", since 2002 and has launched a cyberaction to stop the project. Consumers Union of Japan together with 596 organisations and groups participated in a parade on 27 January 2008 in central Tokyo against the Rokkasho Reprocessing Plant. Over 810,000 signatures were collected and handed in to the government on 28 January 2008. Representatives of the protesters, which include fishery associations, consumer cooperatives and surfer groups, handed the petition to the Cabinet Office and the Ministry of Economy, Trade and Industry. Seven consumer organisations have joined in this effort: Consumers Union of Japan, Seikatsu Club Consumer's Co-operative Union, Daichi-o-Mamoru Kai, Green Consumer's Co-operative Union, Consumer's Co-operative Union \"Kirari\", Consumer's Co-operative Miyagi and Pal-system Co-operative Union. In June 2008, several scientists stated that the Rokkasho plant is sited directly above an active geological fault line that could produce a magnitude 8 earthquake. But Japan Nuclear Fuel Limited have stated that there was no reason to fear an earthquake of more than magnitude 6.5 at the site, and that the plant could withstand a 6.9 quake.\n\nThree months after the Fukushima nuclear disaster, thousands of anti-nuclear protesters marched in Japan. Company workers, students, and parents with children rallied across Japan, \"venting their anger at the government's handling of the crisis, carrying flags bearing the words 'No Nukes!' and 'No More Fukushima'.\" Problems in stabilizing the Fukushima I plant have hardened attitudes to nuclear power. As of June 2011, \"more than 80 percent of Japanese now say they are anti-nuclear and distrust government information on radiation\". The ongoing Fukushima crisis may spell the end of nuclear power in Japan, as \"citizen opposition grows and local authorities refuse permission to restart reactors that have undergone safety checks\". Local authorities are skeptical that sufficient safety measures have been taken and are reticent to give their permission – now required by law – to bring suspended nuclear reactors back online. More than 60,000 people in Japan marched in demonstrations in Tokyo, Osaka, Hiroshima and Fukushima on June 11, 2011.\n\nIn July 2011, Japanese mothers, many new to political activism, have started \"taking to the streets to urge the government to protect their children from radiation leaking from the crippled Fukushima No. 1 nuclear plant\". Using social networking media, such as Facebook and Twitter, they have \"organized antinuclear energy rallies nationwide attended by thousands of protesters\".\n\nIn September 2011, anti-nuclear protesters, marching to the beat of drums, \"took to the streets of Tokyo and other cities to mark six months since the March earthquake and tsunami and vent their anger at the government's handling of the nuclear crisis set off by meltdowns at the Fukushima power plant\". An estimated 2,500 people marched past TEPCO headquarters, and created a human chain around the building of the Trade Ministry that oversees the power industry. Protesters called for a complete shutdown of Japanese nuclear power plants and demanded a shift in government policy toward alternative sources of energy. Among the protestors were four young men who started a 10-day hunger strike to bring about change in Japan's nuclear policy.\n\nTens of thousands of people marched in central Tokyo in September 2011, chanting \"Sayonara nuclear power\" and waving banners, to call on Japan's government to abandon atomic energy in the wake of the Fukushima nuclear disaster. Author Kenzaburō Ōe, who won the Nobel Prize for literature in 1994, and has campaigned for pacifist and anti-nuclear causes addressed the crowd. Musician Ryuichi Sakamoto, who composed the score to the movie \"The Last Emperor\" was also among the event's supporters.\n\nThousands of demonstrators took to the streets of Yokohama on the weekend of January 14–15, 2012, to show their support for a nuclear power-free world. The demonstration showed that organized opposition to nuclear power has gained momentum in the wake of the Fukushima nuclear disaster. The most immediate demand was for the protection of rights for those affected by the Fukushima accident, including basic human rights such as health care, living standards and safety.\n\nOn the anniversary of the 11 March 2011 earthquake and tsunami all over Japan protesters called for the abolishment of nuclear power, and the scrapping of nuclear reactors.\n\nIn June 2012, tens of thousands of protesters participated in anti-nuclear power rallies in Tokyo and Osaka, over the government's decision to restart the first idled reactors since the Fukushima disaster, at Oi Nuclear Power Plant in Fukui Prefecture.\n\nFrom the early 1960s New Zealand peace groups CND and the Peace Media organised nationwide anti-nuclear campaigns in protest of atmospheric testing in French Polynesia. These included two large national petitions presented to the New Zealand government which led to a joint New Zealand and Australian Government action to take France to the International Court of Justice (1972). In 1972, Greenpeace and an amalgam of New Zealand peace groups managed to delay nuclear tests by several weeks by trespassing with a ship in the testing zone. During the time, the skipper, David McTaggart, was beaten and severely injured by members of the French military.\n\nOn 1 July 1972, the Canadian ketch \"Vega\", flying the Greenpeace III banner, collided with the French naval minesweeper \"La Paimpolaise\" while in international waters to protest French nuclear weapon tests in the South Pacific.\n\nIn 1973 the New Zealand Peace Media organised an international flotilla of protest yachts including the Fri, Spirit of Peace, Boy Roel, Magic Island and the Tanmure to sail into the test exclusion zone. Also in 1973, New Zealand Prime Minister Norman Kirk as a symbolic act of protest sent two navy frigates, HMNZS Canterbury and HMNZS Otago, to Mururoa. They were accompanied by HMAS \"Supply\", a fleet oiler of the Royal Australian Navy.\n\nIn 1985 the Greenpeace ship \"Rainbow Warrior\" was bombed and sunk by the French DGSE in Auckland, New Zealand, as it prepared for another protest of nuclear testing in French military zones. One crew member, Fernando Pereira of Portugal, photographer, drowned on the sinking ship while attempting to recover his photographic equipment. Two members of DGSE were captured and sentenced, but eventually repatriated to France in a controversial affair.\n\nIn the Philippines, a focal point for protests in the late 1970s and 1980s was the proposed Bataan Nuclear Power Plant, which was built but never operated. The project was criticised for being a potential threat to public health, especially since the plant was located in an earthquake zone.\n\nIn March 2012, environmental conservation groups staged a rally in central Seoul to voice opposition to nuclear power on the first anniversary of the Fukushima nuclear disaster. According to organizers, over 5,000 people attended, and the turnout was one of the biggest in recent memory for an antinuclear demonstration. The rally adopted a declaration demanding that President Lee Myung Bak abandon his policy to promote nuclear power.\n\nIn Spain, in response to a surge in nuclear power plant proposals in the 1960s, a strong anti-nuclear movement emerged in 1973, which ultimately impeded the realisation of most of the projects. On July 14, 1977, in Bilbao, Spain, between 150,000 and 200,000 people protested against the Lemoniz Nuclear Power Plant. This has been called the \"biggest ever anti-nuclear demonstration\".\n\nIn June 2010, Greenpeace anti-nuclear activists invaded Forsmark nuclear power plant to protest the then-plan to remove the government prohibition on building new nuclear power plants. In October 2012, 20 Greenpeace activists scaled the outer perimeter fence of the Ringhals nuclear plant, and there was also an incursion of 50 activists at the Forsmark plant. Greenpeace said that its non-violent actions were protests against the continuing operation of these reactors, which it says are unsafe in European stress tests, and to emphasise that stress tests did nothing to prepare against threats from outside the plant. A report by the Swedish nuclear regulator said that \"the current overall level of protection against sabotage is insufficient\". Although Swedish nuclear power plants have security guards, the police are responsible for emergency response. The report criticised the level of cooperation between nuclear site staff and police in the case of sabotage or attack.\n\nIn May 2011, some 20,000 people turned out for Switzerland's largest anti-nuclear power demonstration in 25 years. Demonstrators marched peacefully near the Beznau Nuclear Power Plant, the oldest in Switzerland, which started operating 40 years ago. Days after the anti-nuclear rally, Cabinet decided to ban the building of new nuclear power reactors. The country’s five existing reactors would be allowed to continue operating, but \"would not be replaced at the end of their life span\".\n\nIn March 2011, around 2,000 anti-nuclear protesters demonstrated in Taiwan for an immediate end to the construction of the island's fourth nuclear power plant. The protesters were also opposed to lifespan extensions for three existing nuclear plants.\n\nIn May 2011, 5,000 people joined an anti-nuclear protest in Taipei City, which had a carnival-like atmosphere, with protesters holding yellow banners and waving sunflowers. This was part of a nationwide \"No Nuke Action\" protest, against construction of the fourth nuclear plant and in favor of a more renewable energy policy.\n\nOn World Environment Day in June 2011, environmental groups demonstrated against Taiwan's nuclear power policy. The Taiwan Environmental Protection Union, together with 13 environmental groups and legislators, gathered in Taipei and protested against the nation’s three operating nuclear power plants and the construction of the fourth plant.\n\nIn March 2012, about 2,000 people staged an anti-nuclear protest in Taiwan's capital following the massive earthquake and tsunami that hit Japan one year ago. The protesters rallied in Taipei to renew calls for a nuclear-free island by taking lessons from Japan's disaster on March 11, 2011. They \"want the government to scrap a plan to operate a newly constructed nuclear power plant – the fourth in densely populated Taiwan\". Scores of aboriginal protesters \"demanded the removal of 100,000 barrels of nuclear waste stored on their Orchid Island, off south-eastern Taiwan. Authorities have failed to find a substitute storage site amid increased awareness of nuclear danger over the past decade\".\n\nIn March 2013, 68,000 Taiwanese protested across major cities against the island’s fourth nuclear power plant, which is under construction. Taiwan’s three existing nuclear plants are near the ocean, and prone to geological fractures, under the island.\n\nActive seismic faults run across the island, and some environmentalists argue Taiwan is unsuited for nuclear plants. Construction of the Lungmen Nuclear Power Plant using the ABWR design has encountered public opposition and a host of delays, and in April 2014 the government decided to halt construction.\n\nIn October 1983, nearly one million people assembled in the Hague to protest nuclear missile deployments and demand an end to the arms race.\n\nThe first Aldermaston March organised by the Campaign for Nuclear Disarmament took place at Easter 1958, when several thousand people marched for four days from Trafalgar Square, London, to the Atomic Weapons Research Establishment close to Aldermaston in Berkshire, England, to demonstrate their opposition to nuclear weapons. The Aldermaston marches continued into the late 1960s when tens of thousands of people took part in the four-day marches.\n\nMany significant anti-nuclear mobilizations in the 1980s occurred at the Greenham Common Women's Peace Camp. It began in September 1981 after a Welsh group called \"Women for Life on Earth\" arrived at Greenham to protest against the decision of the Government to allow cruise missiles to be based there. The women's peace camp attracted significant media attention and \"prompted the creation of other peace camps at more than a dozen sites in Britain and elsewhere in Europe\". In December 1982 some 30,000 women from various peace camps and other peace organisations held a major protest against nuclear weapons on Greenham Common.\n\nOn 1 April 1983, about 70,000 people linked arms to form a human chain between three nuclear weapons centres in Berkshire. The anti-nuclear demonstration stretched for 14 miles along the Kennet Valley.\n\nIn London, in October 1983, more than 300,000 people assembled in Hyde Park. This was \"the largest protest against nuclear weapons in British history\", according to \"The New York Times\".\n\nIn 2005 in Britain, there were many protests about the government's proposal to replace the aging Trident weapons system with a newer model. The largest protest had 100,000 participants and, according to polls, 59 percent of the public opposed the move.\n\nIn October 2008 in the United Kingdom, more than 30 people were arrested during one of the largest anti-nuclear protests at the Atomic Weapons Establishment at Aldermaston for 10 years. The demonstration marked the start of the UN World Disarmament Week and involved about 400 people.\n\nIn October 2011, more than 200 protesters blockaded the Hinkley Point C nuclear power station site. Members of several anti-nuclear groups that are part of the Stop New Nuclear alliance barred access to the site in protest at EDF Energy's plans to renew the site with two new reactors.\n\nIn January 2012, three hundred anti-nuclear protestors took to the streets of Llangefnia, against plans to build a new nuclear power station at Wylfa. The march was organised by a number of organisations, including Pobl Atal Wylfa B, Greenpeace and Cymdeithas yr Iaith, which are supporting farmer Richard Jones who is in dispute with Horizon.\n\nOn March 10, 2012, the first anniversary of the Fukushima nuclear disaster, hundreds of anti-nuclear campaigners formed a symbolic chain around Hinkley Point to express their determined opposition to new nuclear power plants, and to call on the coalition government to abandon its plan for seven other new nuclear plants across the UK. The human chain continued for 24 hours, with the activists blocking the main Hinkley Point entrance.\n\nIn April 2013, thousands of Scottish campaigners, MSPs, and union leaders, rallied against nuclear weapons. The Scrap Trident Coalition wants to see an end to nuclear weapons, and says saved monies should be used for health, education and welfare initiatives. There was also a blockade of the Faslane Naval Base, where Trident missiles are stored.\n\nOn November 1, 1961, at the height of the Cold War, about 50,000 women brought together by Women Strike for Peace marched in 60 cities in the United States to demonstrate against nuclear weapons. It was the largest national women's peace protest of the 20th century.\n\nOn May 2, 1977, 1,414 Clamshell Alliance protesters were arrested at Seabrook Station Nuclear Power Plant.\nThe protesters who were arrested were charged with criminal trespass and asked to post bail ranging from $100 to $500. They refused and were then held in five national guard armories for 12 days. The Seabrook conflict, and role of New Hampshire Governor Meldrim Thomson, received much national media coverage.\n\nThe American public were concerned about the release of radioactive gas from the Three Mile Island accident in 1979 and many mass demonstrations took place across the country in the following months. The largest one was held in New York City in September 1979 and involved two hundred thousand people; speeches were given by Jane Fonda and Ralph Nader.\n\nOn June 3, 1981, Thomas launched the longest running peace vigil in US history at Lafayette Square in Washington, D.C.. He was later joined on the White House Peace Vigil by anti-nuclear activists Concepcion Picciotto and Ellen Benjamin.\n\nOn June 12, 1982, one million people demonstrated in New York City's Central Park against nuclear weapons and for an end to the cold war arms race. It was the largest anti-nuclear protest and the largest political demonstration in American history.\n\nBeginning in 1982, an annual series of Christian peace vigils called the \"Lenten Desert Experience\" were held over a period of several weeks at a time, at the entrance to the Nevada Test Site in the USA. This led to a faith-based aspect of the nuclear disarmament movement and the formation of the anti-nuclear Nevada Desert Experience group.\n\nThe Seneca Women's Encampment for a Future of Peace and Justice was located in Seneca County, New York, adjacent to the Seneca Army Depot. It took place mainly during the summer of 1983. Thousands of women came to participate and rally against nuclear weapons and the \"patriarchal society\" that created and used those weapons. The purpose of the Encampment was to stop the scheduled deployment of Cruise and Pershing II missiles before their suspected shipment from the Seneca Army Depot to Europe that fall. The Encampment continued as an active political presence in the Finger Lakes area for at least 5 more years.\n\nHundreds of people walked from Los Angeles to Washington, D.C. in 1986 in what is referred to as the Great Peace March for Global Nuclear Disarmament. The march took nine months to traverse , advancing approximately fifteen miles per day.\n\nOther notable anti-nuclear protests in the United States have included:\n\nAnti-nuclear protests preceded the shutdown of the Shoreham, Yankee Rowe, Millstone I, Rancho Seco, Maine Yankee, and about a dozen other nuclear power plants.\n\nOn May 1, 2005, 40,000 anti-nuclear/anti-war protesters marched past the United Nations in New York, 60 years after the atomic bombings of Hiroshima and Nagasaki. This was the largest anti-nuclear rally in the U.S. for several decades.\n\nIn 2008 and 2009, there have been protests about, and criticism of, several new nuclear reactor proposals in the United States. There have also been some objections to license renewals for existing nuclear plants.\n\nIn May 2010, some 25,000 people, including members of peace organizations and 1945 atomic bomb survivors, marched for about two kilometers from downtown New York to a square in front of United Nations headquarters, calling for the elimination of nuclear weapons. The march occurred ahead of the opening of the review conference on the Non-Proliferation of Nuclear Weapons Treaty (NPT).\n\nThe anti-nuclear organisation \"Nevada Semipalatinsk\" was formed in 1989 and was one of the first major anti-nuclear groups in the former Soviet Union. It attracted thousands of people to its protests and campaigns which eventually led to the closure of the nuclear test site at Semipalatinsk, in north-east Kazakhstan, in 1991. The Soviet Union conducted over 400 nuclear weapons tests at the Semipalatinsk Test Site between 1949 and 1989. The United Nations believes that one million people were exposed to radiation.\n\n"}
{"id": "42076934", "url": "https://en.wikipedia.org/wiki?curid=42076934", "title": "Antique Telescope Society", "text": "Antique Telescope Society\n\nThe Antique Telescope Society (ATS) is a society for people interested in antique telescopes, binoculars, instruments, books, atlases, etc.\n\nThe society has an annual meeting. It also publishes the \"Journal of the Antique Telescope Society\" and has an active email list. The American astronomer Michael D. Reynolds has been President of the Antique Telescope Society.\n\n"}
{"id": "586357", "url": "https://en.wikipedia.org/wiki?curid=586357", "title": "Artificial general intelligence", "text": "Artificial general intelligence\n\nArtificial general intelligence (AGI) is the intelligence of a machine that could successfully perform any intellectual task that a human being can. It is a primary goal of some artificial intelligence research and a common topic in science fiction and future studies. Artificial general intelligence is also referred to as \"strong AI\", \"full AI\" or as the ability of a machine to perform \"general intelligent action\". Academic sources reserve \"strong AI\" to refer to machines capable of experiencing consciousness.\n\nSome references emphasize a distinction between strong AI and \"applied AI\" (also called \"narrow AI\" or \"weak AI\"): the use of software to study or accomplish specific problem solving or reasoning tasks. Weak AI, in contrast to strong AI, does not attempt to perform the full range of human cognitive abilities.\n\nAs of 2017, over forty organizations worldwide are doing active research on AGI.\n\nVarious criteria for intelligence have been proposed (most famously the Turing test) but to date, there is no definition that satisfies everyone. However, there \"is\" wide agreement among artificial intelligence researchers that intelligence is required to do the following:\n\nOther important capabilities include the ability to sense (e.g. see) and the ability to act (e.g. move and manipulate objects) in the world where intelligent behaviour is to be observed. This would include an ability to detect and respond to hazard. Many interdisciplinary approaches to intelligence (e.g. cognitive science, computational intelligence and decision making) tend to emphasise the need to consider additional traits such as imagination (taken as the ability to form mental images and concepts that were not programmed in) and autonomy.\nComputer based systems that exhibit many of these capabilities do exist (e.g. see computational creativity, automated reasoning, decision support system, robot, evolutionary computation, intelligent agent), but not yet at human levels.\n\n\nThe most difficult problems for computers are informally known as \"AI-complete\" or \"AI-hard\", implying that solving them is equivalent to the general aptitude of human intelligence, or strong AI, beyond the capabilities of a purpose-specific algorithm.\n\nAI-complete problems are hypothesised to include general computer vision, natural language understanding, and dealing with unexpected circumstances while solving any real world problem.\n\nAI-complete problems cannot be solved with current computer technology alone, and also require human computation. This property can be useful to test for the presence of humans, as with CAPTCHAs, and for computer security to repel brute-force attacks.\n\nModern AI research began in the mid 1950s. The first generation of AI researchers was convinced that artificial general intelligence was possible and that it would exist in just a few decades. As AI pioneer Herbert A. Simon wrote in 1965: \"machines will be capable, within twenty years, of doing any work a man can do.\" Their predictions were the inspiration for Stanley Kubrick and Arthur C. Clarke's character HAL 9000, who accurately embodied what AI researchers believed they could create by the year 2001. Of note is the fact that AI pioneer Marvin Minsky was a consultant on the project of making HAL 9000 as realistic as possible according to the consensus predictions of the time; Crevier quotes him as having said on the subject in 1967, \"Within a generation ... the problem of creating 'artificial intelligence' will substantially be solved,\" although Minsky states that he was misquoted.\n\nHowever, in the early 1970s, it became obvious that researchers had grossly underestimated the difficulty of the project. Funding agencies became skeptical of AGI and put researchers under increasing pressure to produce useful \"applied AI\". As the 1980s began, Japan's Fifth Generation Computer Project revived interest in AGI, setting out a ten-year timeline that included AGI goals like \"carry on a casual conversation\". In response to this and the success of expert systems, both industry and government pumped money back into the field. However, confidence in AI spectacularly collapsed in the late 1980s, and the goals of the Fifth Generation Computer Project were never fulfilled. For the second time in 20 years, AI researchers who had predicted the imminent achievement of AGI had been shown to be fundamentally mistaken. By the 1990s, AI researchers had gained a reputation for making vain promises. They became reluctant to make predictions at all and to avoid any mention of \"human level\" artificial intelligence for fear of being labeled \"wild-eyed dreamer[s].\"\n\nIn the 1990s and early 21st century, mainstream AI has achieved far greater commercial success and academic respectability by focusing on specific sub-problems where they can produce verifiable results and commercial applications, such as artificial neural networks, computer vision or data mining. These \"applied AI\" systems are now used extensively throughout the technology industry, and research in this vein is very heavily funded in both academia and industry.\n\nMost mainstream AI researchers hope that strong AI can be developed by combining the programs that solve various sub-problems using an integrated agent architecture, cognitive architecture or subsumption architecture. Hans Moravec wrote in 1988: \"I am confident that this bottom-up route to artificial intelligence will one day meet the traditional top-down route more than half way, ready to provide the real world competence and the commonsense knowledge that has been so frustratingly elusive in reasoning programs. Fully intelligent machines will result when the metaphorical golden spike is driven uniting the two efforts.\"\n\nHowever, even this fundamental philosophy has been disputed; for example, Stevan Harnad of Princeton concluded his 1990 paper on the Symbol Grounding Hypothesis by stating: \"The expectation has often been voiced that \"top-down\" (symbolic) approaches to modeling cognition will somehow meet \"bottom-up\" (sensory) approaches somewhere in between. If the grounding considerations in this paper are valid, then this expectation is hopelessly modular and there is really only one viable route from sense to symbols: from the ground up. A free-floating symbolic level like the software level of a computer will never be reached by this route (or vice versa) – nor is it clear why we should even try to reach such a level, since it looks as if getting there would just amount to uprooting our symbols from their intrinsic meanings (thereby merely reducing ourselves to the functional equivalent of a programmable computer).\"\n\nArtificial general intelligence (AGI) describes research that aims to create machines capable of general intelligent action. The term was introduced by Mark Gubrud in 1997 in a discussion of the implications of fully automated military production and operations. The research objective is much older, for example Doug Lenat's Cyc project (that began in 1984), and Allen Newell's Soar project are regarded as within the scope of AGI. AGI research activity in 2006 was described by Pei Wang and Ben Goertzel as \"producing publications and preliminary results\". As yet, most AI researchers have devoted little attention to AGI, with some claiming that intelligence is too complex to be completely replicated in the near term. However, a small number of computer scientists are active in AGI research, and many of this group are contributing to a series of AGI conferences. The research is extremely diverse and often pioneering in nature. In the introduction to his book, Goertzel says that estimates of the time needed before a truly flexible AGI is built vary from 10 years to over a century, but the consensus in the AGI research community seems to be that the timeline discussed by Ray Kurzweil in \"The Singularity is Near\" (i.e. between 2015 and 2045) is plausible. Most mainstream AI researchers doubt that progress will be this rapid. Organizations explicitly pursuing AGI include the Swiss AI lab IDSIA, Nnaisense, Vicarious, Maluuba, the OpenCog Foundation, Adaptive AI, LIDA, and Numenta and the associated Redwood Neuroscience Institute. In addition, organizations such as the Machine Intelligence Research Institute and OpenAI have been founded to influence the development path of AGI. Finally, projects such as the Human Brain Project have the goal of building a functioning simulation of the human brain. A 2017 survey of AGI categorized forty-five known \"active R&D projects\" that explicitly or implicitly (through published research) research AGI, with the largest three being DeepMind, the Human Brain Project, and OpenAI.\n\nA popular approach discussed to achieving general intelligent action is whole brain emulation. A low-level brain model is built by scanning and mapping a biological brain in detail and copying its state into a computer system or another computational device. The computer runs a simulation model so faithful to the original that it will behave in essentially the same way as the original brain, or for all practical purposes, indistinguishably. Whole brain emulation is discussed in computational neuroscience and neuroinformatics, in the context of brain simulation for medical research purposes. It is discussed in artificial intelligence research as an approach to strong AI. Neuroimaging technologies that could deliver the necessary detailed understanding are improving rapidly, and futurist Ray Kurzweil in the book \"The Singularity Is Near\" predicts that a map of sufficient quality will become available on a similar timescale to the required computing power.\n\n For low-level brain simulation, an extremely powerful computer would be required. The human brain has a huge number of synapses. Each of the 10 (one hundred billion) neurons has on average 7,000 synaptic connections to other neurons. It has been estimated that the brain of a three-year-old child has about 10 synapses (1 quadrillion). This number declines with age, stabilizing by adulthood. Estimates vary for an adult, ranging from 10 to 5×10 synapses (100 to 500 trillion). An estimate of the brain's processing power, based on a simple switch model for neuron activity, is around 10 (100 trillion) synaptic updates per second (SUPS). In 1997 Kurzweil looked at various estimates for the hardware required to equal the human brain and adopted a figure of 10 computations per second (cps). (For comparison, if a \"computation\" was equivalent to one \"floating point operation\" – a measure used to rate current supercomputers – then 10 \"computations\" would be equivalent to 10 petaFLOPS, achieved in 2011). He used this figure to predict the necessary hardware would be available sometime between 2015 and 2025, if the exponential growth in computer power at the time of writing continued.\n\nThe artificial neuron model assumed by Kurzweil and used in many current artificial neural network implementations is simple compared with biological neurons. A brain simulation would likely have to capture the detailed cellular behaviour of biological neurons, presently only understood in the broadest of outlines. The overhead introduced by full modeling of the biological, chemical, and physical details of neural behaviour (especially on a molecular scale) would require computational powers several orders of magnitude larger than Kurzweil's estimate. In addition the estimates do not account for glial cells, which are at least as numerous as neurons, and which may outnumber neurons by as much as 10:1, and are now known to play a role in cognitive processes.\n\nThere are some research projects that are investigating brain simulation using more sophisticated neural models, implemented on conventional computing architectures. The Artificial Intelligence System project implemented non-real time simulations of a \"brain\" (with 10 neurons) in 2005. It took 50 days on a cluster of 27 processors to simulate 1 second of a model. The Blue Brain project used one of the fastest supercomputer architectures in the world, IBM's Blue Gene platform, to create a real time simulation of a single rat neocortical column consisting of approximately 10,000 neurons and 10 synapses in 2006. A longer term goal is to build a detailed, functional simulation of the physiological processes in the human brain: \"It is not impossible to build a human brain and we can do it in 10 years,\" Henry Markram, director of the Blue Brain Project said in 2009 at the TED conference in Oxford. There have also been controversial claims to have simulated a cat brain. Neuro-silicon interfaces have been proposed as an alternative implementation strategy that may scale better.\n\nHans Moravec addressed the above arguments (\"brains are more complicated\", \"neurons have to be modeled in more detail\") in his 1997 paper \"When will computer hardware match the human brain?\". He measured the ability of existing software to simulate the functionality of neural tissue, specifically the retina. His results do not depend on the number of glial cells, nor on what kinds of processing neurons perform where.\n\nA fundamental criticism of the simulated brain approach derives from embodied cognition where human embodiment is taken as an essential aspect of human intelligence. Many researchers believe that embodiment is necessary to ground meaning. If this view is correct, any fully functional brain model will need to encompass more than just the neurons (i.e., a robotic body). Goertzel proposes virtual embodiment (like Second Life), but it is not yet known whether this would be sufficient.\n\nDesktop computers using microprocessors capable of more than 10 cps (Kurzweil's non-standard unit \"computations per second\", see above) have been available since 2005. According to the brain power estimates used by Kurzweil (and Moravec), this computer should be capable of supporting a simulation of a bee brain, but despite some interest no such simulation exists . There are at least three reasons for this:\n\nIn addition, the scale of the human brain is not currently well-constrained. One estimate puts the human brain at about 100 billion neurons and 100 trillion synapses. Another estimate is 86 billion neurons of which 16.3 billion are in the cerebral cortex and 69 billion in the cerebellum. Glial cell synapses are currently unquantified but are known to be extremely numerous.\n\nAlthough the role of consciousness in strong AI/AGI is debatable, many AGI researchers regard research that investigates possibilities for implementing consciousness as vital. In an early effort Igor Aleksander argued that the principles for creating a conscious machine already existed but that it would take forty years to train such a machine to understand language.\n\nIn 1980, philosopher John Searle coined the term \"strong AI\" as part of his Chinese room argument. He wanted to distinguish between two different hypotheses about artificial intelligence:\nThe first one is called \"the \"strong\" AI hypothesis\" and the second is \"the \"weak\" AI hypothesis\" because the first one makes the \"stronger\" statement: it assumes something special has happened to the machine that goes beyond all its abilities that we can test. Searle referred to the \"strong AI hypothesis\" as \"strong AI\". This usage is also common in academic AI research and textbooks.\n\nThe weak AI hypothesis is equivalent to the hypothesis that artificial general intelligence is possible. According to Russell and Norvig, \"Most AI researchers take the weak AI hypothesis for granted, and don't care about the strong AI hypothesis.\"\n\nIn contrast to Searle, Kurzweil uses the term \"strong AI\" to describe any artificial intelligence system that acts like it has a mind, regardless of whether a philosopher would be able to determine if it \"actually\" has a mind or not.\n\nSince the launch of AI research in 1956, the growth of this field has slowed down over time and has stalled the aims of creating machines skilled with intelligent action at the human level. A possible explanation for this delay is that computers lack a sufficient scope of memory or processing power. In addition, the level of complexity that connects to the process of AI research may also limit the progress of AI research.\n\nWhile most AI researchers believe that strong AI can be achieved in the future, there are some individuals like Hubert Dreyfus and Roger Penrose that deny the possibility of achieving AI. John McCarthy was one of various computer scientists who believe human-level AI will be accomplished, but a date cannot accurately be predicted.\n\nConceptual limitations are another possible reason for the slowness in AI research. AI researchers may need to modify the conceptual framework of their discipline in order to provide a stronger base and contribution to the quest of achieving strong AI. As William Clocksin wrote in 2003: \"the framework starts from Weizenbaum’s observation that intelligence manifests itself only relative to specific social and cultural contexts\".\n\nFurthermore, AI researchers have been able to create computers that can perform jobs that are complicated for people to do, but conversely they have struggled to develop a computer that is capable of carrying out tasks that are simple for humans to do . A problem that is described by David Gelernter is that some people assume that thinking and reasoning are equivalent. However, the idea of whether thoughts and the creator of those thoughts are isolated individually has intrigued AI researchers.\n\nThe problems that have been encountered in AI research over the past decades have further impeded the progress of AI. The failed predictions that have been promised by AI researchers and the lack of a complete understanding of human behaviors have helped diminish the primary idea of human-level AI. Although the progress of AI research has brought both improvement and disappointment, most investigators have established optimism about potentially achieving the goal of AI in the 21st century.\n\nOther possible reasons have been proposed for the lengthy research in the progress of strong AI. The intricacy of scientific problems and the need to fully understand the human brain through psychology and neurophysiology have limited many researchers from emulating the function of the human brain into a computer hardware. Many researchers tend to underestimate any doubt that is involved with future predictions of AI, but without taking those issues seriously can people then overlook solutions to problematic questions.\n\nClocksin says that a conceptual limitation that may impede the progress of AI research is that people may be using the wrong techniques for computer programs and implementation of equipment. When AI researchers first began to aim for the goal of artificial intelligence, a main interest was human reasoning. Researchers hoped to establish computational models of human knowledge through reasoning and to find out how to design a computer with a specific cognitive task.\n\nThe practice of abstraction, which people tend to redefine when working with a particular context in research, provides researchers with a concentration on just a few concepts. The most productive use of abstraction in AI research comes from planning and problem solving. Although the aim is to increase the speed of a computation, the role of abstraction has posed questions about the involvement of abstraction operators.\n\nA possible reason for the slowness in AI relates to the acknowledgement by many AI researchers that heuristics is a section that contains a significant breach between computer performance and human performance. The specific functions that are programmed to a computer may be able to account for many of the requirements that allow it to match human intelligence. These explanations are not necessarily guaranteed to be the fundamental causes for the delay in achieving strong AI, but they are widely agreed by numerous researchers.\n\nThere have been many AI researchers that debate over the idea whether machines should be created with emotions. There are no emotions in typical models of AI and some researchers say programming emotions into machines allows them to have a mind of their own. Emotion sums up the experiences of humans because it allows them to remember those experiences. David Gelernter writes, \"No computer will be creative unless it can simulate all the nuances of human emotion.\" This concern about emotion has posed problems for AI researchers and it connects to the concept of strong AI as its research progresses into the future.\n\nThere are other aspects of the human mind besides intelligence that are relevant to the concept of strong AI which play a major role in science fiction and the ethics of artificial intelligence:\nThese traits have a moral dimension, because a machine with this form of strong AI may have legal rights, analogous to the rights of non-human animals. Also, Bill Joy, among others, argues a machine with these traits may be a threat to human life or dignity. It remains to be shown whether any of these traits are necessary for strong AI. The role of consciousness is not clear, and currently there is no agreed test for its presence. If a machine is built with a device that simulates the neural correlates of consciousness, would it automatically have self-awareness? It is also possible that some of these properties, such as sentience, naturally emerge from a fully intelligent machine, or that it becomes natural to \"ascribe\" these properties to machines once they begin to act in a way that is clearly intelligent. For example, intelligent action may be sufficient for sentience, rather than the other way around.\n\nIn science fiction, AGI is associated with traits such as consciousness, sentience, sapience, and self-awareness observed in living beings. However, according to philosopher John Searle, it is an open question whether general intelligence is sufficient for consciousness. \"Strong AI\" (as defined above by Ray Kurzweil) should not be confused with Searle's \"'strong AI hypothesis\". The strong AI hypothesis is the claim that a computer which behaves as intelligently as a person must also necessarily have a mind and consciousness. AGI refers only to the amount of intelligence that the machine displays, with or without a mind.\n\nOpinions vary both on \"whether\" and \"when\" artificial general intelligence will arrive. At one extreme, AI pioneer Herbert A. Simon wrote in 1965: \"machines will be capable, within twenty years, of doing any work a man can do\". However, this prediction failed to come true. Microsoft co-founder Paul Allen believed that such intelligence is unlikely in the 21st century because it would require \"unforeseeable and fundamentally unpredictable breakthroughs\" and a \"scientifically deep understanding of cognition\". Writing in The Guardian, roboticist Alan Winfield claimed the gulf between modern computing and human-level artificial intelligence is as wide as the gulf between current space flight and practical faster-than-light spaceflight. AI experts' views on the feasibility of AGI wax and wane, and may have seen a resurgence in the 2010s. Four polls conducted in 2012 and 2013 suggested that the median guess among experts for when they'd be 50% confident AGI would arrive was 2040 to 2050, depending on the poll, with the mean being 2081. It is also interesting to note 16.5% of the experts answered with \"never\" when asked the same question but with a 90% confidence instead.\n\nThe creation of artificial general intelligence may have repercussions so great and so complex that it may not be possible to forecast what will come afterwards. Thus the event in the hypothetical future of achieving strong AI is called the technological singularity, because theoretically one cannot see past it. But this has not stopped philosophers and researchers from guessing what the smart computers or robots of the future may do, including forming a utopia by being our friends or overwhelming us in an AI takeover. The latter potentiality is particularly disturbing as it poses an existential risk for mankind.\n\nSmart computers or robots would be able to design and produce improved versions of themselves. A growing population of intelligent robots could conceivably out-compete inferior humans in job markets, in business, in science, in politics (pursuing robot rights), and technologically, sociologically (by acting as one), and militarily.\n\nIf research into strong AI produced sufficiently intelligent software, it would be able to reprogram and improve itself – a feature called \"recursive self-improvement\". It would then be even better at improving itself, and would probably continue doing so in a rapidly increasing cycle, leading to an intelligence explosion and the emergence of superintelligence. Such an intelligence would not have the limitations of human intellect, and might be able to invent or discover almost anything.\n\nHyper-intelligent software might not necessarily decide to support the continued existence of mankind, and might be extremely difficult to stop. This topic has also recently begun to be discussed in academic publications as a real source of risks to civilization, humans, and planet Earth.\n\nOne proposal to deal with this is to make sure that the first generally intelligent AI is a friendly AI that would then endeavor to ensure that subsequently developed AIs were also nice to us. But friendly AI is harder to create than plain AGI, and therefore it is likely, in a race between the two, that non-friendly AI would be developed first. Also, there is no guarantee that friendly AI would remain friendly, or that its progeny would also all be good.\n\n"}
{"id": "21642043", "url": "https://en.wikipedia.org/wiki?curid=21642043", "title": "Audience (company)", "text": "Audience (company)\n\nAudience was an American mobile voice and audio-processing company based in Mountain View, California, and was one of the 34 founding members of The Open Handset Alliance. They specialized in improving voice clarity and noise suppression for a broad range of consumer products, including cellular phones, mobile devices and PCs. They were bought by Knowles for $85 Million in 3Q15 who changed their name to Knowles Intelligent Audio.\n\nAudience was the first company to have reverse-engineered the human hearing system and model its processes onto a chip, enabling computers and mobile devices to use the same kind of “auditory intelligence” that humans employ. By using this technology in conjunction with two or more microphones, background noise is suppressed, improving the quality of the remaining voice and reducing distraction for the listener. This technology mimics the “cocktail party effect”.\n\nIn 2010, Audience partnered with HTC to integrate their noise suppression technology into the Google Nexus One smartphone. The next year AT&T introduced eight different handsets powered by Audience's earSmart technology including the Samsung Galaxy S III Skyrocket and the HTC Vivid.\n\nIn 2013, Audience unveiled its eS515, a combination voice processor and audio codec. This single slot solution enables device manufacturers to streamline their designs, negating the need for a separate voice processor and codec.\n\nThe company’s core technology reduces background noise and improves voice clarity on smartphones, computers and tablets. The company’s technology has shipped in more than 140 devices including the Nexus One, Samsung Galaxy S, Galaxy S 4, Galaxy Note, the iPhone4 and 4S, and the LG G2. Audience also works with other mobile phone manufacturers such as Huawei, Google, HTC, Sharp, Pantech, ZTE and others. In 2010, the company announced a collaboration with AT&T to deliver its advanced voice capabilities to AT&T devices and is engaged with multiple operators around the world. Most recently the company announced its first design implementation in the PC space with the Dell Vostro 5460.\n\nThey had two lines of processors, Advanced Voice and Smart Sound, both branded as \"earSmart.\" Advanced Voice processors come in a variety of models, the eS110 (supports 1 microphone), eS305 and eS310 (supports 2 microphones) and the company’s latest generation eS325 (supports 3 microphones), which was announced at Mobile World Congress 2013.\n\nSmart Sound processors combined the Advanced Voice processor feature set (HD call quality, wideband noise suppression, bandwidth expansion, acoustic echo cancelation, and improved performance for Automatic Speech Recognition) with a stereo audio codec. The company introduced its Smart Sound processor category at Consumer Electronics Show 2013 with the launch of the eS515.\n\nThe company’s technology was built around the foundation of Computational Auditory Scene Analysis (CASA) -- a field of study that builds on the concept of Auditory Scene Analysis (ASA), a term first coined by psychologist Albert Bregman. ASA enables humans to accurately group sounds—even when composed of multiple frequencies, as in music, or when heard simultaneously –- and avoid blending \"sources.\" As a result, ASA allows the listener to correctly distinguish and identify a sound of interest, like a voice, from other noise sources.\n\nCASA attempts to recreate sound source separation in the same manner as human hearing, but in machines. Using the principles of CASA, Audience’s earSmart processors act like a human cochlea and group different sounds, based on a diverse list of cues such as pitch, onset/offset time, spatial location and harmonicity. These simultaneous sounds are evaluated and grouped by source. In doing this, the microphones of a mobile device, together with Audience's proprietary \"Fast Cochlea Transform\" technology, can identify and group sounds which are classified as noise, remove or at least reduce them, and leave the remaining clear voice signal intact.\n\nBeyond voice and video calls, this technology has been proven to improve the reliability and task completion rate of ASR (automatic speech recognition) in noisy environments as well as improve the quality and usability of media capture functions. Outside of the smartphone, tablet and mobile PC space, Audience is said to be targeting the Smart TV and Automotive markets where voice is becoming more commonplace as a form of user interface.\n\nAudience was the Silver Winner in the 2008 Wall Street Journal Technology Innovation Awards and the Winner of the Semiconductor category. The company was included in Gartner’s “Cool Vendors in Semiconductors, 2008” and selected as the Most Innovative True Mobile Start-Up for the “2008 GSMA Mobile Innovation Global Awards.”\n\n\n"}
{"id": "44455145", "url": "https://en.wikipedia.org/wiki?curid=44455145", "title": "Automation engineering", "text": "Automation engineering\n\nAutomation engineering has two different meanings:\n\nAutomation engineers are experts who have the knowledge and ability to design, create, develop and manage systems, for example, factory automation, process automation and warehouse automation.\n\nAutomation engineering is the integration of standard engineering fields.\nAutomatic control of various control system for operating various systems or machines to reduce human efforts & time to increase accuracy.\n\nGraduates can work for both government and private sector entities such as industrial production, \ncompanies that create and use automation systems, for example paper industry, automotive industry or \nfood and agricultural industry and water treatment.\n\nAutomation engineers can design, program, simulate and test automated machinery and processes. Automation engineers usually are employed in industries such as the energy sector in plants, car manufacturing facilities or food processing plants and robots. Automation engineers are responsible for detailed design specifications and other documents in their creations.\n"}
{"id": "59161710", "url": "https://en.wikipedia.org/wiki?curid=59161710", "title": "BIM Collaboration Format", "text": "BIM Collaboration Format\n\nThe BIM Collaboration Format (BCF) is a structured file format which allows issue tracking with a building information model. BCF is designed primarily for attaching information to collisions and errors connected with specific objects in a model, but can be used for general issue tracking in building construction projects. This allows developers of BIM-supporting software to design interfaces for collaboration between users who access the same model, especially with different software.\n\nThe format was developed by Tekla and Solibri and later adopted as a standard by buildingSMART. Notable software with native support for BCF are Solibri, Tekla with plugins which provide usability for other software.\n\n\n"}
{"id": "6936288", "url": "https://en.wikipedia.org/wiki?curid=6936288", "title": "Baseball telecasts technology", "text": "Baseball telecasts technology\n\nThe following is a chronological list of the technological advancements of Major League Baseball television broadcasts:\n\nOn August 26, the first ever Major League Baseball telecast (the Brooklyn Dodgers vs. Cincinnati Reds from Ebbets Field) aired by W2XBS, an experimental station in New York City which would ultimately become WNBC-TV.\n\nRed Barber called the game without the benefit of a monitor and with only two cameras capturing the game. One camera was on Barber and the other was behind the plate. Barber had to guess from which light was on and where it pointed.\n\nIn 1939, baseball games were usually covered by one camera providing a point-of-view along the third base line.\n\n\nOn August 11, 1951, WCBS-TV in New York City televised the first baseball game (in which the Boston Braves beat the Brooklyn Dodgers by the score of 8-1) in color.\n\nOn October 3 of that year, NBC aired the first coast-to-coast baseball telecast as the Brooklyn Dodgers were beaten by the New York Giants in the final game of a playoff series by the score of 5-4 (off Bobby Thomson's now-legendary home run).\n\n\n1955 marked the first time that the World Series was televised in color.\n\n\n\nOn July 23, 1962, Major League Baseball had its first satellite telecast (via Telstar Communications). The telecast included portion of a contest between the Chicago Cubs vs. the Philadelphia Phillies from Wrigley Field with Jack Brickhouse commentating on WGN-TV.\n\nBy 1969, the usage of chroma key (in which the commentators would open a telecast by standing in front of a greenscreen composite of the stadiums' crowds) became a common practice for baseball telecasts.\n\n\n\nIn the bottom of the 12th inning of Game 6 of the 1975 World Series at Boston's Fenway Park, Red Sox catcher Carlton Fisk was facing Cincinnati Reds pitcher Pat Darcy. Fisk then hit a pitch down the left field line that appeared to be heading to foul territory. The enduring image of Fisk jumping and waving the ball fair as he made his way to first base is arguably one of baseball's greatest moments. The ball struck the foul pole, giving the Red Sox a 7-6 win and forcing a seventh and deciding game of the Fall Classic. During this time, cameramen covering baseball games were instructed to follow the flight of the ball; reportedly, Fisk's reaction was only being recorded because NBC cameraman Lou Gerard, positioned inside Fenway's scoreboard at the base of the left-field Green Monster wall, had become distracted by a large rat. This play was perhaps the most important catalyst in getting camera operators to focus most of their attention on the players themselves.\n\nOn July 6, 1983, NBC televised the All-Star Game out of Chicago's Comiskey Park. During the telecast, special guest analyst, Don Sutton helped introduce NBC's new pitching tracking device dubbed The NBC Tracer. \"The NBC Tracer\" was a stroboscopic comet tail showing the path of a pitch to the catcher's glove. For instance, \"The NBC Tracer\" helped track a Dave Stieb curveball among others.\n\nIn 1985, NBC's telecast of the All-Star Game out of the Metrodome in Minnesota was the first program to be broadcast in stereo by a television network.\n\n\nFor the 1987 World Series between the Minnesota Twins and St. Louis Cardinals, ABC utilized 12 cameras and nine tape machines. This includes cameras positioned down the left field line, on the roof of the Metrodome, and high above third base.\n\nIn 1990, CBS took over from both ABC and NBC as Major League Baseball's national, over-the-air television provider. They in the process brought along their telestration technology that they dubbed CBS Chalkboard. \"CBS Chalkboard\" made its debut eight years earlier during CBS' coverage of Super Bowl XVI.\n\nFor CBS' coverage of the 1992 All-Star Game, they introduced Basecam, a lipstick-size camera, inside first base.\n\nDuring CBS' coverage of the 1993 World Series, umpires were upset with the overhead replays being televised by CBS. Dave Phillips, the crew chief, said just prior to Game 2 that the umpires want \"CBS to be fair with their approach.\"\n\nRick Gentile, the senior vice president for production of CBS Sports, said that Richie Phillips, the lawyer for the Major League Umpires Association, tried to call the broadcast booth during Saturday's game, but the call was not put through. Richie Phillips apparently was upset when Dave Phillips called the Philadelphia Phillies' Ricky Jordan out on strikes in the fourth inning, and a replay showed the pitch to be about 6 inches outside.\n\nNational League President Bill White, while using a CBS headset in the broadcast booth during Game 1, was overheard telling Gentile and the producer Bob Dekas: \n\n\n\nOn July 8, 1997, Fox televised its first ever All-Star Game (out of Jacobs Field in Cleveland). For this particular game, Fox introduced \"Catcher-Cam\" in which a camera was affixed to the catchers' masks in order to provide unique perspectives of the action around home plate. Catcher-Cam soon would become a regular fixture in Fox's baseball broadcasts.\n\nIn addition to Catcher-Cam, other innovations (some of which have received more acclaim than others) that Fox has provided for baseball telecasts have been:\n\nFor a Saturday afternoon telecast of a Los Angeles Dodgers/Chicago Cubs game at Wrigley Field on August 26, 2000, Fox aired a special \"Turn Back the Clock\" broadcast to commemorate the 61st anniversary of the first televised baseball game. The broadcast started with a re-creation of the television technology of 1939, with play-by-play announcer Joe Buck working alone with a single microphone, a single black-and-white camera, and no graphics; then, each subsequent half-inning would see the broadcast \"jump ahead in time\" to a later era, showing the evolving technologies and presentation of network baseball coverage through the years.\n\n\n\nIn October 2002, Fox televised the first ever World Series to be shown in high definition.\n\n\n\nStarting in 2004, some TBS telecasts (mostly Fridays or Saturdays) became more enhanced. The network decided to call it Braves TBS Xtra. Enhancements included catcher cam, \"Xtra Motion\", which featured the type of pitch and movement, also \"leadOff Line\". It would also show features with inside access to players.\n\nIn October 2004, Fox started airing all Major League Baseball postseason broadcasts (including the League Championship Series and World Series) in high definition. Fox also started airing the Major League Baseball All-Star Game in HD the following year. At the same time, the FoxBox and graphics are upgraded.\n\n\n\nFor their 2007 Division Series coverage, TBS debuted various new looks, such as the first live online views from cameras in dugouts and ones focused on pitchers. TBS also introduced a graphic that creates sort of a rainbow to trace the arc of pitches on game replays. The graphic was superimposed in the studio so analysts like Cal Ripken, Jr. for instance, could take virtual cuts at pitches thrown in games.\n\nDuring their 2009 playoff coverage, TBS displays their \"PitchTrax\" graphic full-time during at-bats (with the center field camera only) during the high-definition version of the broadcast in the extreme right-hand corner of the screen.\n\nMeanwhile, for their own 2009 playoff coverage, Fox announced that they would occasionally include this stat on replays: Speed of pitches as they leave pitchers' hands as well as their speed when they cross home plate.\n\n\nWith the start of the 2011 postseason, TBS planned to introduce the following\n\nThe screen on TBS's standard definition feed now airs a letterboxed version of the native HD feed to match Fox's default widescreen SD presentation, allowing the right side pitch tracking graphic to be seen by SD viewers.\n\nFor the 2011 World Series, Fox debuted infrared technology that's designed to pinpoint heat made by a ball making contact — with, say, bats, face masks, players' bodies — and mark the spot for viewers by making it glow. During Game 1, Fox used \"Hot Spot\" to show that a batted ball was fouled off Texas Rangers batter Adrián Beltré's foot.\n\nFox's 2012 World Series coverage would include a camera whose replays could generate as many as 20,000 frames per second, the most ever seen on Fox—and up from about 60 frames per second on regular replays. The camera would allow viewers \"to see the ball compress\" when batted, similar to how cameras now show golf balls getting compressed when struck. The technology for the camera originated with the U.S. military looking at replays of missile impacts.\n\n\n"}
{"id": "1283671", "url": "https://en.wikipedia.org/wiki?curid=1283671", "title": "Blobject", "text": "Blobject\n\nA blobject is a design product, often a household object, distinguished by smooth flowing curves, bright colors, and an absence of sharp edges. The word is generally held to be a portmanteau, a contraction of \"blob\" and \"object.\"\n\nThe origin of the term is disputed, but it is often attributed to either the designer-author Steven Skov Holt or the designer Karim Rashid. Author and design journalist Phil Patton attributed the word to Holt in 1993 in Esquire magazine. Holt has defined a blobject as, most often, a colorful, mass-produced, plastic-based, emotionally engaging consumer product with a curvilinear, flowing shape. This fluid and curvaceous form is the blobject's most distinctive feature. Rashid, the contemporary designer who wrote the book \"I want to Change the World,\" was an early leader in creating blobjects and has become one of the most celebrated designers of his generation.\n\nBlobjects can also be found in most areas of contemporary visual culture. \nCommon materials used in fabricating blobjects are plastic (especially polycarbonate, polypropylene, or polyethylene), metal, and rubber, with the aim being to give a more organic and animate feel. \n\nThe blobject trend has largely been driven by advances in computer-aided design, information visualization, rapid prototyping, materials, and injection molding. These technologies have given designers the chance to use new shapes and to explore transparency and translucency without significant extra production costs.\n\nMore recently, in 2004, author Bruce Sterling used the word in the title of his keynote speech at Siggraph. In his speech titled, \"When Blobjects Rule the Earth\", Sterling speculated on the future of graphic simulation when practical differences no longer exist between computer generated models and physically manufactured objects.\n\n\n\n\n"}
{"id": "1431131", "url": "https://en.wikipedia.org/wiki?curid=1431131", "title": "Bureau of Oceans and International Environmental and Scientific Affairs", "text": "Bureau of Oceans and International Environmental and Scientific Affairs\n\nThe Bureau of Oceans and International Environmental and Scientific Affairs (OES) is a bureau within the United States Department of State. It coordinates a portfolio of issues related to the world's oceans, environment, science and technology, and health.\n\nThe Bureau is headed by the Assistant Secretary of State for Oceans and International Environmental and Scientific Affairs. As of April 2014, it is headed by acting Assistant Secretary Judith Garber. \n\nThe Oceans and Fisheries Directorate has two offices dedicated to international oceans issues. The Office of Marine Conservation focuses on international fisheries matters and related problems and the Office of Oceans and Polar Affairs has primary responsibility for international ocean law and policy, marine pollution, marine mammals, polar affairs, maritime boundaries, and marine science. It is headed by Deputy Assistant Secretary David A. Balton.\n\nThe Environment Directorate deals with environmental issues including environmental aspects of international trade and safeguarding hazardous materials requiring multilateral agreements within the Office of Environmental Quality and Transboundary Affairs. The Office of Conservation and Water develops U.S. foreign policy approaches to conserving and managing the world ecosystems and to transboundary water issues. The Environment Directorate is headed by Deputy Assistant Secretary Daniel Reifsnyder. \n\nThe Health, Space and Science Directorate includes the Office of International Health Affairs which works with U.S. Government agencies to facilitate policy-making regarding international bioterrorism, infectious disease, surveillance and response, environmental health, and health in post-conflict situations. The Office of Space and Advanced Technology handles issues arising from our exploration of space to assure global security regarding this new frontier, and the Office of Science & Technology (S&T) Cooperation promotes the interests of the U.S. science and technology communities in the international policy arena, negotiates framework and other S&T agreements, manages the Department's Embassy Science fellows program, and takes a leading role in representing U.S. science and technology in multilateral international organizations, such as UNESCO and other UN organizations, APEC, OECD and others. The Health, Space and Science Directorate is headed by acting Deputy Assistant Secretary Jonathan Margolis.\n\n"}
{"id": "22256588", "url": "https://en.wikipedia.org/wiki?curid=22256588", "title": "Comparison of Canon EOS digital cameras", "text": "Comparison of Canon EOS digital cameras\n\nThe following tables provide general information as well as a comparison of technical specifications for a number of Canon EOS digital cameras.\n\n"}
{"id": "41174313", "url": "https://en.wikipedia.org/wiki?curid=41174313", "title": "Comparison of Firefox OS devices", "text": "Comparison of Firefox OS devices\n\nFirefox OS is an operating system for mobile devices. This page seeks to list and compare hardware devices that are shipped with Firefox OS operating system.\n\n\n"}
{"id": "5938019", "url": "https://en.wikipedia.org/wiki?curid=5938019", "title": "Complexity theory and organizations", "text": "Complexity theory and organizations\n\nComplexity theory and organizations, also called complexity strategy or complex adaptive organizations, is the use of the study of complexity systems in the field of strategic management and organizational studies.\n\nComplexity theory is an interdisciplinary theory that grew out of systems theory in the 1960s. It draws from research in the natural sciences that examines uncertainty and non-linearity. Complexity theory emphasizes interactions and the accompanying feedback loops that constantly change systems. While it proposes that systems are unpredictable, they are also constrained by order-generating rules.\n\nComplexity theory has been used in the fields of strategic management and organizational studies. Application areas include understanding how organizations or firms adapt to their environments and how they cope with conditions of uncertainty. Organisations have complex structures in that they are dynamic networks of interactions, and their relationships are not aggregations of the individual static entities. They are adaptive; in that the individual and collective behavior mutate and self-organize corresponding to a change-initiating micro-event or collection of events.\n\nOrganizations can be treated as complex adaptive systems (CAS) as they exhibit fundamental CAS principles like self-organization, complexity, emergence, interdependence, space of possibilities, co-evolution, chaos, and self-similarity.\n\nCAS are contrasted with ordered and chaotic systems by the relationship that exists between the system and the agents which act within it. In an ordered system the level of constraint means that all agent behaviour is limited to the rules of the system. In a chaotic system the agents are unconstrained and susceptible to statistical and other analysis. In a CAS, the system and the agents co-evolve; the system lightly constrains agent behaviour, but the agents modify the system by their interaction with it. This self-organizing nature is an important characteristic of CAS; and its ability to learn to adapt, differentiate it from other self organizing systems.\n\nCAS approaches to strategy seek to understand the nature of system constraints and agent interaction and generally takes an evolutionary or naturalistic approach to strategy. Some research integrates computer simulation and organizational studies.\n\nComplexity theory also relates to knowledge management (KM) and organizational learning (OL). \"Complex systems are, by any other definition, learning organizations.\" Complexity Theory, KM, and OL are all complimentary and co-dependent. “KM and OL each lack a theory of how cognition happens in human social systems – complexity theory offers this missing piece”.\n\nComplexity theory is also being used to better understand new ways of doing project management, as traditional models have been found lacking to current challenges. This approaches advocates forming a \"culture of trust\" that \"welcomes outsiders, embraces new ideas, and promotes cooperation.\"\n\nComplexity Theory implies approaches that focus on flatter, more flexible organizations, rather than top-down, command-and-control styles of management.\n\nA typical example for an organization behaving as CAS, is Wikipedia – collaborated and managed by a loosely organized management structure, composed of a complex mix of human–computer interactions. By managing behavior, and not only mere content, Wikipedia uses simple rules to produce a complex, evolving knowledge base which has largely replaced older sources in popular use.\n\nOther examples include the complex global macroeconomic network within a country or group of countries; stock market and complex web of cross-border holding companies; manufacturing businesses; and any human social group-based endeavour in a particular ideology and social system such as political parties, communities, geopolitical organisations, and terrorist networks of both hierarchical and leaderless nature. This new macro level state may create difficulty for an observer in explaining and describing the collective behaviour in terms of its constituent parts; as a result of the complex dynamic networks of interactions, outlined earlier.\n\n\n"}
{"id": "2030477", "url": "https://en.wikipedia.org/wiki?curid=2030477", "title": "Connections (TV series)", "text": "Connections (TV series)\n\nConnections is a 10-episode documentary television series and 1978 book (\"Connections\", based on the series) created, written, and presented by science historian James Burke. The series was produced and directed by Mick Jackson of the BBC Science and Features Department and first aired in 1978 (UK) and 1979 (USA). It took an interdisciplinary approach to the history of science and invention, and demonstrated how various discoveries, scientific achievements, and historical world events were built from one another successively in an interconnected way to bring about particular aspects of modern technology. The series was noted for Burke's crisp and enthusiastic presentation (and dry humour), historical re-enactments, and intricate working models.\n\nThe popular success of the series led to the production of \"The Day the Universe Changed\" (1985), a similar program but showing a more linear history of several important scientific developments. Years later, the success in syndication led to two sequels, \"Connections\" (1994) and \"Connections\" (1997), both for TLC. In 2004, KCSM-TV produced a program called \"Re-Connections\", consisting of an interview of Burke and highlights of the original series, for the 25th anniversary of the first broadcast in the USA on PBS.\n\n\"Connections\" explores an \"Alternative View of Change\" (the subtitle of the series) that rejects the conventional linear and teleological view of historical progress. Burke contends that one cannot consider the development of any particular piece of the modern world in isolation. Rather, the entire gestalt of the modern world is the result of a web of interconnected events, each one consisting of a person or group acting for reasons of their own motivations (e.g., profit, curiosity, religion) with no concept of the final, modern result to which the actions of either them or their contemporaries would lead. The interplay of the results of these isolated events is what drives history and innovation, and is also the main focus of the series and its sequels.\n\nTo demonstrate this view, Burke begins each episode with a particular event or innovation in the past (usually ancient or medieval times) and traces the path from that event through a series of seemingly unrelated connections to a fundamental and essential aspect of the modern world. For example, the episode \"The Long Chain\" traces the invention of plastics from the development of the fluyt, a type of Dutch cargo ship.\n\nBurke also explores three corollaries to his initial thesis. The first is that, if history is driven by individuals who act only on what they know at the time, and not because of any idea as to where their actions will eventually lead, then predicting the future course of technological progress is merely conjecture. Therefore, if we are astonished by the connections Burke is able to weave among past events, then we will be equally surprised to what the events of today eventually will lead, especially events of which we were not even aware at the time.\n\nThe second and third corollaries are explored most in the introductory and concluding episodes, and they represent the downside of an interconnected history. If history progresses because of the synergistic interaction of past events and innovations, then as history does progress, the number of these events and innovations increases. This increase in possible connections causes the process of innovation to not only continue, but also to accelerate. Burke poses the question of what happens when this rate of innovation, or more importantly 'change' itself, becomes too much for the average person to handle, and what this means for individual power, liberty, and privacy.\n\nLastly, if the entire modern world is built from these interconnected innovations, all increasingly maintained and improved by specialists who required years of training to gain their expertise, what chance does the average citizen without this extensive training have in making an informed decision on practical technological issues, such as the building of nuclear power plants or the funding of controversial projects such as stem cell research? Furthermore, if the modern world is increasingly interconnected, what happens when one of those nodes collapses? Does the entire system follow suit?\n\nThe original 1978 Connections 10-episode documentary television series and had a companion book (\"Connections\", based on the series) created, written, and presented by science historian James Burke. The 1978 Connections companion book was published about the time the middle of the series was airing, so likely was written in parallel to the series and had a post-production editing release. The very popular book was re-released as a work in a 1995 edition, 1998, (relations to sections below is unknown.) and again in 2007 as both hardcover or softcover editions. Since the television series varied in content with each corresponding production run and release, it is likely the companion volumes (as is suggest by the plethora of ISBN codes) are also different works. This 1978 work's coverage deviates in some topics and details being both more in depth and a bit broader, from the lighter coverage of the episodes. It can be found in many libraries. \n\n\n\nAll three \"Connections\" documentaries have been released in their entirety as DVD box sets in the US. The ten episodes of series one were released in Europe (Region 2) on 6 February 2017.\n\nBurke also wrote a series of \"Connections\" articles in \"Scientific American\", and published a book of the same name (1995, ), all built on the same theme of exploring the history of science and ideas, going back and forth through time explaining things on the way and, generally, coming back to the starting point.\n\nBurke produced another documentary series called \"The Day the Universe Changed\" in 1985, which explored man's concept of how the universe worked in a manner similar to the original \"Connections\".\n\n\"\" is a video series launched in 2011 on Kickstarter.com that was inspired by James Burke's \"Connections\". However, it follows concepts rather than inventions through time.\n\n\"Richard Hammond's Engineering Connections\", shown on BBC2, follows a similar format.\n\n\"Connections\", a \"Myst\"-style computer game with James Burke and others providing video footage and voice acting, was released in 1995. It was a runner-up for \"Computer Gaming World\"s award for the best \"Classics/Puzzles\" game of 1995, which ultimately went to \"You Don't Know Jack\". The editors wrote of \"Connections\", \"That you enjoy yourself so much you hardly realize that you're learning is a tribute to the design.\"\n\nA clip from the episode \"Yesterday, Tomorrow and You\" appears in the 2016 video game The Witness.\n\n"}
{"id": "21657889", "url": "https://en.wikipedia.org/wiki?curid=21657889", "title": "Cosmic Call", "text": "Cosmic Call\n\nCosmic Call was the name of two sets of interstellar radio messages that were sent from RT-70 in Yevpatoria, Crimea in 1999 (Cosmic Call 1) and 2003 (Cosmic Call 2) to various nearby stars. The messages were designed with noise-resistant format and characters.\n\nThe project was funded by Team Encounter, Charlie Chafer (CEO) a Texas-based startup, which went out of business in 2004.\n\nBoth transmissions were at ~150 kW, 5.01 GHz (FSK +/-24 kHz).\n\nEach Cosmic Call 1 session had the following structure. The Scientific Part (DDM, BM, AM, and ESM) was sent three times (at 100 bit/s), and the Public Part (PP) was sent once (at 2000 bit/s), according to the following arrangement:\n\nwhere DDM is the Dutil-Dumas Message, created by Canadian scientists Yvan Dutil and Stéphane Dumas, BM is the Braastad Message, AM is the Arecibo Message, and ESM is the Encounter 2001 Staff Message.\n\nEach Cosmic Call 2 session in 2003 had the following structure:\n\nwhere DDM2 is modernized DDM (aka Interstellar Rosetta Stone, ISR), BIG is Bilingual Image Glossary. All but the PP were transmitted at 400 bit/s\n\nThe ISR was 263,906 bits; BM, 88,687 bits, AM, 1,679 bits; BIG was 12 binary images 121,301 bits; ESM 24,899 bits. Total = 500,472 bits for 53 minutes. PP was 220 megabytes and sent at a rate of 100,000 bit/s for 11 hours total.\n\nThe messages were sent to the following stars:\n\n\n"}
{"id": "13363237", "url": "https://en.wikipedia.org/wiki?curid=13363237", "title": "Cyberpolitics", "text": "Cyberpolitics\n\nCyberpolitics is a term widely employed across the world, largely by academics interested in analyzing its breadth and scope, of the use of the Internet for political activity. It embraces all forms of social software. Cyberpolitics includes: journalism, fundraising, blogging, volunteer recruitment, and organization building. \n\nThe campaign of Howard Dean, in which a previously little-known former Democratic governor of a small state emerged for a while as the front runner for the 2004 Democratic presidential nomination on the strength of his campaign's skill in cyberpolitics, was a wake-up call to the American political establishments of political parties around the United States as to the importance of cyberpolitics as both a concept and as a series of organizational and communications strategies.\n\n\n\n\nThe physical dimensions of cyberpolitics in Eastern Asia]\n\n"}
{"id": "3990817", "url": "https://en.wikipedia.org/wiki?curid=3990817", "title": "Differential technological development", "text": "Differential technological development\n\nDifferential technological development is a strategy proposed by transhumanist philosopher Nick Bostrom in which societies would seek to influence the sequence in which emerging technologies developed. On this approach, societies would strive to retard the development of harmful technologies and their applications, while accelerating the development of beneficial technologies, especially those that offer protection against the harmful ones.\n\nPaul Christiano believes that while accelerating technological progress appears to be one of the best ways to improve human welfare in the next few decades, a faster rate of growth cannot be equally important for the far future because growth must eventually saturate due to physical limits. Hence, from the perspective of the far future, differential technological development appears more crucial.\n\nInspired by Bostrom's proposal, Luke Muehlhauser and Anna Salamon suggested a more general project of \"differential intellectual progress\", in which society advances its wisdom, philosophical sophistication, and understanding of risks faster than its technological power. Brian Tomasik has expanded on this notion.\n\n"}
{"id": "41920098", "url": "https://en.wikipedia.org/wiki?curid=41920098", "title": "ENIAC Day", "text": "ENIAC Day\n\nENIAC Day or the World’s First Computer Day is celebrated on 15 February.\nOn February 10, 2011, the City of Philadelphia officially declared that February 15, 2011 - the 65th anniversary of the unveiling of the Electronic Numerical Integrator and Computer (ENIAC), the world's first general-purpose electronic computer, developed at the University of Pennsylvania's Moore School of Electrical Engineering - would that year and henceforth be known as ENIAC Day.\n"}
{"id": "6487468", "url": "https://en.wikipedia.org/wiki?curid=6487468", "title": "Edublog", "text": "Edublog\n\nAn edublog is a blog created for educational purposes. Edublogs archive and support student and teacher learning by facilitating reflection, questioning by self and others, collaboration and by providing contexts for engaging in higher-order thinking. Edublogs proliferated when blogging architecture became more simplified and teachers perceived the instructional potential of blogs as an online resource. The use of blogs has become popular in education institutions including public schools and colleges. Blogs can be useful tools for sharing information and tips among co-workers, providing information for students, or keeping in contact with parents. Common examples include blogs written by or for teachers, blogs maintained for the purpose of classroom instruction, or blogs written about educational policy. Educators who blog are sometimes called edubloggers.\n\nWeblogs have existed for close to two decades. However, it wasn't until the second half of the 1990s that weblogs began to grow in popularity. In 1998, there were just a handful of sites of the type that are now identified as weblogs (so named by Jorn Barger in December 1997). In 1999, there were 23 known weblogs, and Pitas http://www.pitas.com/, the first free build your own weblog tool, was launched. Also in 1999 weblogs changed from a mix of links, commentary, and thoughts, to short form journal entries. An early recorded use of the term \"edublog\" can be traced to a webring called the Edublog WebRing, founded on January 30, 2002. The new use of weblogs are largely interest driven and attract readers who have similar interests. In 2004, there were an estimated 3 million blogs and as of July 2011, there are an estimated 164 million blogs.\n\nThe Edublog Awards, the international and community based awards programme for the use of blogs and social media to support education, runs annually online across a range of platforms. The Awards were founded by James N. Farmer in 2004.\n\nThere are several uses of edublogs. Some bloggers use their blogs as a learning journal or a knowledge log to gather relevant information and ideas, and communicate with other people. Some teachers use blogs to keep in contact with students' parents. Some bloggers use blogs to record their own personal life, \nand express emotions or feelings. Some instructors use blogs as an instructional and assessment tool, and blogs can be used as a task management tool. Blogs are used to teach individuals about writing for an audience as they can be made public, and blogging software makes it easier to create content for the Web without knowing much HTML.\n\nEducational blogs have also been used as an engagement and reflective assessment tool for Accounting students and can improve educational outcomes for Accounting students.\n\nThere are many teacher-related blogs on the internet where teachers can share information with one another. Teachers familiarize themselves with edublogs before implementing them with their students. Many teachers share materials and ideas to meet the diverse needs of all learners in their classrooms. Teachers can often rely on these sources to communicate with one another regarding any issues in education that they may be having, including classroom management techniques and policies. In this way, the blog often acts as a support system for teachers where they can access ideas, tools, and gain support and recognition from other professionals in their field. Weblogs can provide a forum for reading, writing and collaborating.\n\nEdublogs can be used as instructional resources, in which teachers can post tips, explanations or samples to help students learn. The use of blogs in the classroom allows both the teacher and student the ability to edit and add content at any time. The ability for both the teacher and student to edit content allows for study to take place outside the classroom environment, since blogs can usually be accessed using the URL of the blog on any computer. Blogs increase exposure to other students from around the country or world, while improving writing and communication skills. Teachers are using blogs as a way to post important information such as homework, important dates, missed lessons, projects, discussion boards, and other useful classroom information that is accessible by all. As noted, students can access this information from home, or from any computer that is connected to the Internet.\n\nTeachers and parents can also use blogs in order to communicate with one another. They can be used to post class announcements for parents or providing schedule reminders. Connecting to a teacher's blog is also a convenient way for parents to find out daily assignments so that they can monitor their children's progress and understand classroom expectations.\n\nStudent blogging describes students in Kindergarten to Grade 12 who are using blogs in some way in a formal classroom context. Blogs are digital platforms that provide students with a medium for sharing knowledge and experiences that go beyond the traditional means of reading and writing in classrooms. Student blogging is a relative newcomer to the digital writing scene, and appears to have gained ground only in the past 7–8 years. In the past 5 years, however, student blogging has become a relatively common phenomenon in classrooms around the world. This may be attributable to the increase in free blog hosting services that have adjustable privacy settings, and the opening up of school internet filters to a greater range of social media.\n\nThe use of blogs in education gives students a global perspective. Teachers and students from different states, countries, and continents are able to collaborate on different projects and ideas. A classroom in China can collaborate with classrooms in Germany, Mexico, Australia, etc. with just a few clicks of a button. Learning through blogs allows students to take control of their own learning and steer it to their own needs. Students are able to see that opinions and even strategies vary based on location and culture. Children are all different, but a common thread of learning can unite them. The use of blogs in the classroom engages children in learning and using technological literacy that will help them in adulthood.\n\nThere has not been a significant amount of research conducted on K-12 students regarding the efficacy of edublogs in enhancing learning. However, anecdotal results discussed by educators have given a glimpse into their utility or promise. There is a general consensus that edublogs create many opportunities for collaborative learning, as well as enhance the ability to locate and reflect upon work.\n\nAccording to extant literature, students use blogging in classrooms for different purposes. Blogs are used to showcase individual student work by enabling them to publish texts, video clips, audio clips, maps, photos and other images, projects and suchlike in a potentially publicly accessible forum. Proponents of student blogging argue that blogging can contribute directly to improved writing abilities and argue that classroom blogging can enable students to engage with audiences beyond their classroom walls by using blogs as personal journals, as diaries, for story writing, and for making editorial responses to news events. \nResearchers have also documented teachers using student blogging to promote creativity and self-expression.\n\nSome researchers claim that student blogs promote learning by providing opportunities for students to take more control of their learning and the content they engage. It is also claimed that student blogging intrinsically motivates students to become better readers and writers.\n\nThere is very little research on student blogging available. That being said, there is a large amount of published anecdotal evidence regarding criticisms of student blogging or limitations in using blogging in classrooms. For example, commentators claim that student blogs often include uncorrected inaccuracies of information, or can be used to instigate online bullying. Commentators also complain that student blogs are difficult to archive or index.\n\n\nAlthough there are many blogs that teachers can use in the classroom with their students, there is also a multitude of blogs that teachers can use for their own professional development. Such blogs include hints on ways to be a better teacher in a certain subject area such as music, mathematics, or ESL, blogs on educational theory, blogs on advice for new teachers, blogs on where to find free technology, and blogs on transforming education, for example. There is much that can be learned from blogs of other teaching professionals and the learning can be done anytime and anywhere.\n\n\n\n\n"}
{"id": "32741783", "url": "https://en.wikipedia.org/wiki?curid=32741783", "title": "Emtech", "text": "Emtech\n\nThe Emtech (short for \"Emerging Technologies\") conference, produced by the Massachusetts Institute of Technology's Technology Review magazine, is an annual conference highlighting invention and new developments in engineering and technology. Started in 1999, the 2011 conference is planned for October 18-19 at MIT.\n\nIn addition to two days of presentations, the conference highlights the winners of the annual TR35 award, recognizing the world's top 35 innovators under the age of 35. Some of the most famous winners of the award include Larry Page and Sergey Brin (creators of Google), Mark Zuckerberg (creator of Facebook), Jack Dorsey (creator of Twitter), and Konstantin Novoselov, who later won the Nobel Prize in Physics.\n"}
{"id": "4403957", "url": "https://en.wikipedia.org/wiki?curid=4403957", "title": "Equiaxed crystals", "text": "Equiaxed crystals\n\nEquiaxed crystals are crystals that have axes of approximately the same length. \n\nEquiaxed grains can in some cases be an indication for recrystallization.\n\nEquiaxed crystals can be achieved by heat treatment, namely annealing and normalizing.\n\n"}
{"id": "1021118", "url": "https://en.wikipedia.org/wiki?curid=1021118", "title": "Fast fracture", "text": "Fast fracture\n\nIn structural engineering and material science, fast fracture is a term given to a phenomenon in which a flaw (such as a crack) in a material expands quickly, and leads to catastrophic failure of the material. Stress acting on a material when fast fracture occurs is less than the material's yield stress. A very representative example of this is what happens when poking a blown up balloon with a needle, that is, fast fracture of the balloon's material.\n\n"}
{"id": "12959588", "url": "https://en.wikipedia.org/wiki?curid=12959588", "title": "First Tuesday (networking forum)", "text": "First Tuesday (networking forum)\n\nFirst Tuesday is a networking forum for technology start-ups, their investors and related service providers.\n\nFirst Tuesday was launched on the first Tuesday of October 1998 at the Alphabet Bar in London by its founders Adam Gold, Julie Meyer, Mark Davies, John Browning and Nick Denton. The early First Tuesdays were held in the loft on Great Titchfield Street which housed Syzygy’s offices. Each month London’s Internet digerati would gather for drinks, and to listen to a speaker and share information on raising money and growing their Internet businesses. Early speakers included Rikki Tahta, Brent Hoberman and Martha Lane Fox, the founders of Lastminute.com; and John Taysom.\n\nIn late Spring 1999 Meyer left her job at NewMedia Investors to build First Tuesday's international network. In September 1999, First Tuesday launched into 17 cities across Europe. At Christmas 1999 and through the Millennium, Julie Meyer went to Israel, meeting Shlomo Kalisch of Jerusalem Global who had launched Yazam, a similar concept to First Tuesday.\n\nFirst Tuesday London was incorporated in early February 2000. Yazam tried to purchase First Tuesday in February 2000, but failed, ultimately succeeding on the 20th of July 2000. First Tuesday's City Leaders and Julie Meyer had led an alternative funding strategy with investments ready from Karl Christian Agerup of NorthZone Ventures and Rod Schwartz of Catalyst Technologies. This funding might have enabled First Tuesday to implement appropriate business models, corporate structures and management teams. The business model emerging through the structured First Tuesday Matchmaking events was to take 2% of funds raised at a First Tuesday Matchmaking event. Despite evidence that this was taking off, the company was sold.\nFirst Tuesday was run as an independent business or not-for-profit organisation in most countries.\n\n"}
{"id": "14889055", "url": "https://en.wikipedia.org/wiki?curid=14889055", "title": "Information grazing", "text": "Information grazing\n\nInformation grazing refers to the ability to quickly obtain knowledge and facts just in time to solve new problems or answer questions. \"Information grazing\" can also be \"information jumping\", jumping from site to site and cherry-picking information seems to \"rewire\" the brain to deleterious effects or focus on something long enough to fully understand all its implications.\n\nUnlike traditional learning, where learning a subject in depth was necessary to draw enough pertinent knowledge to solve new problems or answer questions, information grazing assumes some subjects are so large, fast-changing, interdependent, or esoteric, that traditional methods of learning may be unable to solve new problems or answer questions as efficiently. Information grazing is also one of the most commonly used coping techniques for stress and experts have noticed a trend with people many under the age of 35.\n\nThe change from a traditional in-depth learning and memorizing of facts to a mentality of quickly finding, using, and then forgetting knowledge, stems from the technological singularity concept that information is growing so fast (see information explosion or information overload) that an individual can no longer hope to be a \"renaissance man\" or effectively keep up with some fields of knowledge. Examples of fields of knowledge that are more susceptible to information grazing techniques are science and engineering, where the “newest” knowledge has become so dynamic, that documentation and dissemination has increasingly moved from fixed paper media to digital formats allowing easier updating and searchability. With the advent of the Internet and modern computer-cataloging of libraries, vast sums of knowledge are easily accessible in overwhelming quantities in real time. And, with expected future advances in search engine technology and library services, the trend of information overload is expected to worsen; information grazing techniques will become more prevalent to deal with the overload.\n\nDisadvantages of information grazing come from its advantages. Switching from a \"fixed\" source of information that is constant, verifiable, and worth memorizing to \"fluid\" sources that are always in flux can lead to quick solutions that are unverified or worse, incorrect. Studies have shown that many people don't read past the first sentences of a Web site's content, and many never go beyond the first ten links listed in a search. As information becomes more like an instantaneous consumable item, memorization is less fact-based but more procedural (i.e. how to find it). Similar concepts are found in Japanese education where after intense study of many unconnected facts, most of the information is forgotten or if remembered, not connected to other relevant facts. In the USA, this is similar to cramming an exam. Information grazing may have the same effect but greater, since it is done over the period of years.\n\n\n"}
{"id": "17420716", "url": "https://en.wikipedia.org/wiki?curid=17420716", "title": "List of electric-vehicle-battery manufacturers", "text": "List of electric-vehicle-battery manufacturers\n\nAccording to Shenzhen-Guangdong Industry Research CATL is the largest producer of Lithum-ion batteries for electric mobility with a capacity of 12 GWh. Followed by Panasonic and BYD. According to Shenzhen Gaogong Industry Research, CATL last year churned out 11.8 GWh in battery capacity, a surge of over 74% from 2016. Japan’s Panasonic was second-largest with 10 GWh, and BYD came in third with 7.2 GWh. Another China-based maker, OptimumNano Energy Co. Ltd., and South Korea’s LG Chem were No. 4 and No. 5, producing 5.5 GWh and 4.5 GWh respectively.\n\n"}
{"id": "1577252", "url": "https://en.wikipedia.org/wiki?curid=1577252", "title": "List of radars", "text": "List of radars\n\nThis is a list of radars. A radar is an electronic system used to detect, range (determine the distance of), and map various types of targets.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEarly S-band RADAR Designations\nFrom February 1943 the US used a universal system to identify radar variants, consisting of three letters and a number, respectively designating platform, type of equipment, function, and version. This system was continued after WWII with multiservice designations being prefixed by 'AN/' for Army-Navy.\n\nBuShips 1943 classifications\nMulti-service classifications\n\nMulti-service classification codes according to the Joint Electronics Type Designation System.\n\nSpecific radar systems\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "6726416", "url": "https://en.wikipedia.org/wiki?curid=6726416", "title": "Mobile equipment identifier", "text": "Mobile equipment identifier\n\nA mobile equipment identifier (MEID) is a globally unique number identifying a physical piece of CDMA2000 mobile station equipment. The number format is defined by the 3GPP2 report S.R0048 but in practical terms, it can be seen as an IMEI but with hexadecimal digits.\n\nAn MEID is 56 bits long (14 hex digits). It consists of three fields, including an 8-bit regional code (RR), a 24-bit manufacturer code, and a 24-bit manufacturer-assigned serial number. The check digit (CD) is not considered part of the MEID.\n\nThe MEID was created to replace ESNs, whose virgin form was exhausted in November 2008. As of TIA/EIA/IS-41 Revision D and TIA/EIA/IS-2000 Rev C, the ESN is still a required field in many messages—for compatibility, devices with an MEID can use a pseudo-ESN (pESN), which is a manufacturer code of 0x80 (formerly reserved) followed by the least significant 24 bits of the SHA-1 hash of the MEID. MEIDs are used on CDMA mobile phones. GSM phones do not have ESN or MIN, only an International Mobile Station Equipment Identity (IMEI) number.\n\nOpen your phone's dialler and type *#06# to get its MEID number.\n\nThe separation between international mobile equipment identifiers (IMEIs) used by GSM/UMTS and MEIDs is based on the number ranges. There are two administrators: the global decimal administrator (GDA) for IMEIs and the global hexadecimal administrator (GHA).\n\nAs of August 2006, the TIA acts as the GHA to assign MEID code prefixes (0xA0 and up), and the GSM Association acts as the global decimal administrator. http://www.babt.com/gsm-imei-number-allocation.asp\n\nThe TIA also allocates IMEI codes, specifically destined for dual-technology phones, out of the RR=99 range. Other administrators working under GSMA may also allocate any IMEI for use in dual-technology phones. Every IMEI can also be used as an MEID in CDMA2000 devices (as well as in single-mode devices designed with GSM or other 3GPP protocols) but MEID codes may also contain hexadecimal digits and this class of MEID codes cannot be used as an IMEI.\n\nThere are two standard formats for MEIDs, and both can include an optional check-digit. This is defined by 3GPP2 standard X.S0008.\n\nThe hexadecimal form is specified to be 14 digits grouped together and applies whether all digits are in the decimal range or whether some are in the range 'A'-'F'. In the first case, all digits are in the range '0'-'9', the check-digit is calculated using the normal base 10 Luhn algorithm, but if at least one digit is in the range 'A'-'F' this check digit algorithm uses base 16 arithmetic. The check-digit is never transmitted or stored. It is intended to detect most (but not all) input errors, it is not intended to be a checksum or CRC to detect transmission errors. Consequently, it may be printed on phones or their packaging in case of manual entry of an MEID (e.g. because there is no bar code or the bar code is unreadable).\n\nThe decimal form is specified to be 18 digits grouped in a 5 5 4 4 pattern and is calculated by converting the manufacturer code portion (32 bits) to decimal and padding on the left with '0' digits to 10 digits and separately converting the serial number portion to decimal and padding on the left to 8 digits. A check-digit can be calculated from the 18 digit result using the standard base 10 Luhn algorithm and appended to the end. Note that to produce this form the MEID digits are treated as base 16 numbers even if all of them are in the range '0'-'9'.\n\nBecause the pESN is formed by a hash on the MEID there is the potential for hash collisions. These will cause an extremely rare condition known as a 'collision' on a pure ESN-only network as the ESN is used for the calculation of the Public Long Code Mask (PLCM) used for communication with the base-station. Two mobiles using the same pESN within the same base-station area (operating on the same frequency) can result in call setup and page failures.\n\nThe probability of a collision has been carefully examined. Roughly, it is estimated that even on a heavily loaded network the frequency of this situation is closer to 1 out of 1 million calls than to 1 out of 100 000.\n\n3GPP2 specification C.S0072 provides a solution to this problem by allowing the PLCM to be established by the base station. It is easy for the base station to ensure that all PLCM codes are unique when this is done. This specification also allows the PLCM to be based on the MEID or IMSI.\n\nA different problem occurs when ESN codes are stored in a database (such as for OTASP). In this situation, the risk of at least two phones having the same pseudo-ESN can be calculated using the birthday paradox and works out to about a 50 per cent probability in a database with 4,800 pseudo-ESN entries. 3GPP2 specifications C.S0016 (Revision C or higher) and C.S0066 have been modified to allow the replacement MEID identifier to be transmitted, resolving this problem.\n\nAnother problem is that messages delivered on the forward paging channel using the pESN as an address could be delivered to multiple mobiles seemingly randomly. This problem can be avoided by using mobile identification number (MIN) or IMSI based addressing instead.\n\nThis short Python script will convert an MEID to a pESN.\nThe CDG also provides a javascript calculator with more conversion options.\n\n"}
{"id": "39207065", "url": "https://en.wikipedia.org/wiki?curid=39207065", "title": "Mobile forms", "text": "Mobile forms\n\nA mobile form is an electronic or digital form application that functions on a smartphone or tablet device. Mobile forms enable users to collect data using mobile devices, and then to send the results back to the source. Mobile forms exist to replace paper forms as a more productive means of data collection, eliminating the need to transcribe or scan paper data results into a back office system.\n\nDepending on the mobile form application provider, some mobile form solutions allow offices to dispatch data to mobile form applications. In addition, other mobile form applications can be connected with various cloud services, servers, and social media platforms.\n\nDepending on the business, the motivating factors to deploying mobile forms may vary. Some businesses implement mobile forms to speed up processes, while others institute mobile forms with field users to reduce costs associated with transporting paper forms back and forth. Furthermore, green-minded businesses implement mobile forms in order to be more environmentally friendly, thus reducing their reliance on paper, ink printing, and subsequent waste.\n\nAdvanced mobile form features include signature capture, bar code capture, photo capture, GPS location form info, time form info, and skip logic.\n\nUses for mobile forms include:\n\n"}
{"id": "38325858", "url": "https://en.wikipedia.org/wiki?curid=38325858", "title": "Momo (software)", "text": "Momo (software)\n\nMomo (Chinese: 陌陌; pinyin: mò mò) is a free social search and instant messaging mobile app. The app allows users to chat with nearby friends and strangers. Momo provides users with free instant messaging services through Wifi, 3G and 4G. The client software is available for Android, iOS, and Windows Phone.\n\nMomo officially began operations in July 2011, and one month later launched the first version of the app for iOS. Momo filed for a NASDAQ IPO on November 7, 2014 and was listed in December 2014.\n\nTang Yan, Zhang Sichuan, Lei Xiaoliang, Yong Li, and Li Zhiwei co-founded Beijing Momo Technology Co., Ltd. in July 2011.\nPrior to founding the company, Tang Yan worked as editor and then editor-in-chief at NetEase. In October 2014, Tang was named by Fortune Magazine as one of its \"40 Under 40,\" a list of the most powerful business elites under the age of 40.\nThe other co-founders all have prior experience with major Chinese Internet companies. \nIn order to facilitate foreign investments, Momo’s co-founders incorporated a holding company called Momo Technology Company Limited in the British Virgin Islands in November 2011. In July 2014, Momo Technology Company Limited was renamed to Momo Inc. and re-domiciled to the Cayman Islands.\nIn December 2011, Momo established Momo Technology HK Company Limited (Momo HK) as a wholly owned subsidiary in Hong Kong. In March 2012, Momo HK established Beijing Momo Information Technology Co., Ltd.(Beijing Momo IT), a wholly owned People’s Republic of China subsidiary. In May 2013, Beijing Momo established Chengdu Momo Technology Co., Ltd.(Chengdu Momo), as a wholly owned subsidiary.\n\nIn December 2011, Momo announced reaching half a million users. Three months later, the number of Momo users reached 2 million. Momo reached 10 million users on its first anniversary in August 2012. In October 2012, Momo surpassed 15 million users. \nIn 2014, App Annie reported that Momo was the number 2 non-game app of 2013 in terms of revenue.\nIn February 2014, TechNode reported that Momo had announced reaching 100 million registered users. Momo executives also claimed they had reached 40 million monthly active users (MAU).\nAccording to Momo, in June 2014, total registered users and MAU reached 148 million and 52.4 million respectively.\nChina Internet Watch reported more conservative estimates. In the months of August and September 2014, Momo had 51.279 and 52.101 million MAU. While Momo’s MAU grew, Wechat and QQ both lost MAU within the same time frame. Momo's prospectus reported 60.2 million MAU in September 2014.\n\nMomo reportedly raised USD 2.5 million in Series A financing. Angel investor, PurpleSky Capital (ZiHui ChuangTou), and Matrix Hong Kong led this round of financing. However, Momo's Form F-1 filed with the SEC reports that USD 5 million was raised in this round of financing. \nMomo Inc. completed its Series B financing in October 2012. This round of financing was led by two institution investors and received $100 million valuation. China Renaissance Partners acted as the exclusive financial advisor. There was much speculation as to whether or not Chinese e-commerce giant, Alibaba Group, was involved in this round of financing. Momo’s registration statement verifies this claim. In total, Momo raised approximately USD 40 million.\nIn October 2013, raised USD 45 million in Series C financing. Matrix Hong Kong, Gothic Partners, L.P., PJF Acorn I Trust, Gansett Partners, L.L.C., PH momo investment Ltd., Tenzing Holding 2011 Ltd., Alibaba Investment Limited, and DST Team Fund Limited were all issued and sold Series C preferred shares.\nIn May 2014, Momo raised USD 211.8 million in Series D financing. Momo sold Series D preferred shares to Sequoia Capital China Investment Holdco II, Ltd., Sequoia Capital China GF Holdco III-A, Ltd., SC China Growth III Co-Investment 2014-A, L.P., Rich Moon Limited, and Tiger Global Eight Holdings.\n\nMomo’s mobile application is available on Android, iOS, and Windows platforms. It enables users to establish and expand their social relationships based on similar locations and interests. Some features of the application include subsections like: Nearby Users, Groups, Message Board, Topics, and Nearby Events. Users can send multi-media instant messages as well as play single and multi-player games within the app’s platform. Users also make a Facebook-like profile and are encouraged to include as much information as possible. Momo execs claim that this allows their software to create more accurate matches with nearby strangers. Momo prides itself on sifting through the clutter of mobile Internet users to find personalized matches for its users.\nMomo offers users paid membership subscriptions. A membership will cost around USD 2 a month, or less if a user commits to a longer term of use. Benefits of a paid membership includes: VIP logos, advanced search options, discounts in the emoticon store, higher limits on maximum users in a group, and the ability to see a list of recent visitors to a user’s profile page. As of September 30, 2014, there was 2.3 million paid subscriptions.\nLike many other instant messaging services, Momo has integrated mobile games into their platform to monetize off their large user base. Third parties develop games, and revenues from in-game purchases are shared between Momo and the developers.\n\nIn August 2014, Momo launched Dao Dian Tong, a marketing tool for local merchants. Through Dao Dian Tong, local businesses and merchants can construct profile pages that allow Momo users to find them with the Momo’s LBS. Members can see the businesses just as they would see other Momo users.\nMomo plans to further monetize user traffic by referring users from the Momo platform to e-commerce companies. Alibaba was specifically mentioned in Momo’s Form F-1.\n\nIn December 2012, Momo made an official announcement to accuse Sina Corp of copycatting straight from all the features of Momo Group. However, Sina Corp did not give its formal response.\n\nOn December 10, 2014, Momo's founder and CEO Tang Yan's former employer NetEase released a statement, accusing that Tang Yan has professional ethic issues, business ethics issues, and has been detained due to personal affairs by the local police in 2007.\n\nOn April 27, 2012, Mike Sui, a mixed-race comedian and performer in China, first posted his \"12 Beijingers\" viral video which attracted nearly 5.17 million hits. In this video, one character mentions Momo, for the first time calling it \"a magical tool to get laid\" (Chinese: 约炮神器; pinyin: yuē pào shén qì ). \nMomo has spent millions of dollars to reverse the image of Momo as a one-night stand app. Momo, through its Weibo account, continues to engage the online community through various campaigns. Momo’s latest online campaign focused on supporting the homeless cats and dogs of China.\n\nAlthough Momo is widely considered as adult social application, there are couples claiming that meetings on Momo resulted in marriage. Momo now focuses on building all types of relationships, and no longer propagates itself as exclusively a monogamous relationship building applications.\n\n"}
{"id": "873475", "url": "https://en.wikipedia.org/wiki?curid=873475", "title": "Neo-Victorian", "text": "Neo-Victorian\n\nNeo-Victorianism is an aesthetic movement which amalgamates Victorian and Edwardian aesthetic sensibilities with modern principles and technologies. A large number of magazines and websites are devoted to Neo-Victorian ideas in dress, family life, interior decoration, morals, and other topics.\n\nA large number of neo-Victorian novels have reinterpreted, reproduced and rewritten Victorian culture. Significant texts include \"The French Lieutenant’s Woman\" (John Fowles, 1969), \"Possession\" (A. S. Byatt, 1990), \"Arthur and George\" (Julian Barnes, 2005), \"Dorian, An Imitation\" (Will Self, 2002) \"Jack Maggs\" (Peter Carey, 1997), \"Wide Sargasso Sea\" (Jean Rhys, 1966).\nRecent neo-Victorian novels have often been adapted to the screen, from \"The French Lieutenant’s Woman\" (Karel Reisz, 1981) to the television adaptations of Sarah Waters (\"Tipping the Velvet\", BBC2, 2002, \"Fingersmith \", BBC1, 2005, \"Affinity \" ITV, 2008) and Michel Faber (\"The Crimson Petal and the White\", BBC 1, 2011). These narratives may indicate a 'sexsation' of neo-Victorianism (Kohlke) and have been called 'in-yer-face' neo-Victorianism (Voigts-Virchow).\nRecent productions of neo-Victorianism on screen include Guy Ritchie’s Sherlock Holmes films, the BBC’s \"Sherlock\" (2010-), \"Ripper Street\" (2012-16), ITV’s \"Whitechapel \" (2009–13) or the Showtime series \"Penny Dreadful\" (2014-2016). The neo-Victorian formula can be expanded to include Edwardian consumer culture (\"Downton Abbey\", ITV 2010-2015, \"The Paradise\", BBC 2012-2013) and \"Mr Selfridge\" (ITV 2013-2016).\n\nIn September 2007, Exeter University explored the phenomenon in a major international conference titled \"Neo-Victorianism: The Politics and Aesthetics of Appropriation\". Academic studies include \"Neo-Victorianism: The Victorians in the Twenty-First Century, 1999–2009\".\nOther foundational texts of neo-Victorian criticism are Kucich and Sadoff (2000), Kaplan (2007), Kohlke (2008-), Munford and Young (2009), Mitchell (2010), Davies (2012), Whelehan (2012), Kleinecke-Bates (2014), Böhm-Schnitker and Gruss (2014) and others.\n\nExamples of crafts made in this style would include push-button cordless telephones made to look like antique wall-mounted phones, CD players resembling old time radios, Victorianesque furniture, and Victorian era-style clothing.\n\nIn neo-romantic and fantasy art one can often see the elements of Victorian aesthetic values. There is also a strongly emerging genre of steampunk art. McDermott & McGough are a couple of contemporary artists whose work is all about a recreation of life in the nineteenth century: they only use the ultimate technology available, and since they are supposed to live anachronistically, this means the use of earlier photographic processes, and maintaining the illusion of a life stuck in the ways of a forgotten era.\n\nMany who have adopted Neo-Victorian style have also adopted Victorian behavioural affectations, seeking to imitate standards of Victorian conduct, pronunciation, interpersonal interaction. Some even go so far as to embrace certain Victorian habits such as shaving with straight razors, riding penny farthings, exchanging calling cards, and using fountain pens to write letters in florid prose sealed by wax. Gothic fashion sometimes incorporates Neo-Victorian style.\n\nNeo-Victorianism is embraced in, but also quite distinguished from, the Lolita, Aristocrat and Madam fashions popular in Japan, and which are becoming more noticeable in Europe.\n\nNeo-Victorian aesthetics are also popular in the United States and United Kingdom among cultural conservatives and social conservatives. Books such as \"The Benevolence of Manners: Recapturing the Lost Art of Gracious Victorian Living\" call for a return to Victorian morality. The term Neo-Victorian is also commonly used in a derogatory way towards social conservatives.\n\nThe cultural social attitudes and conventions that many associate with the Victorian era are inaccurate. In fact, many of the things that seem commonplace in modern life began in the Victorian era, such as sponsorship, sensational journalism and popular merchandise.\n\nNeo-Victorianism can also be seen in the growing steampunk genre of speculative fiction and in music performers such as Emilie Autumn. Neo-Victorianism is also popular with, and in many ways prefigured by, those who are interested in Victoriana and historical reenactment.\n\nNeo-Victorian details appear in \"The Diamond Age\" by Neal Stephenson, in which Neo-Victorians are one of the main groups of protagonists.\n\nCarnival Diablo is a Neo-Victorian circus sideshow that has been touring North America for 20 years.\n\nUnhallowed Metropolis is a roleplaying game based in a Neo-Victorian setting.\n\n\n"}
{"id": "666246", "url": "https://en.wikipedia.org/wiki?curid=666246", "title": "Office of Science and Technology", "text": "Office of Science and Technology\n\n\"For the Office of Science and Technology (OST) that existed in the United States see President's Science Advisory Committee\"\n\nThe Office of Science and Technology (OST), later (briefly) named the Office of Science and Innovation, was a non-ministerial government department of the British government between 1992 and 2007.\n\nThe office was responsible for co-ordination of the Government's science and technology related activities and policies, and the distribution of some £2.4 billion among the seven UK Research Councils. It was headed by the Chief Scientific Adviser; initially this was Sir William Stewart, then Sir Robert (later Lord) May, and finally Sir David King. \n\nThe OST was originally formed in 1992 as a merger of the Office of the Chief Scientific Adviser with the Science Branch of the Department of Education and Science (as it then was). Although originally run under the Cabinet Office, it was moved between Departments in 1995 to operate under the Department of Trade and Industry. In early 2006, the office was renamed to the \"Office of Science and Innovation\", and was subsequently absorbed into the Department for Innovation, Universities and Skills in the Summer of 2007 when the Department for Education and Skills was split in two.\n\nThe Government Chief Scientific Advisor now heads the Government Office for Science.\n\n"}
{"id": "21222762", "url": "https://en.wikipedia.org/wiki?curid=21222762", "title": "Online charging system", "text": "Online charging system\n\nOnline charging system (OCS) is a system allowing a communications service provider to charge their customers, in real time, based on service usage.\n\nEvent based charging function (EBCF) is used to charge events based on their occurrence rather than their duration or volume used in the event. Typical events are SMS, MMS, purchase of a content (application, game, music, video on demand, etc.).\n\nEvent based charging function is used when the CC-Request-Type AVP = 4 i.e. for event request ex: diameter-sms or diameter-...\n\nThe session based charging function (SBCF) is responsible for online charging of network / user sessions, e.g. voice calls, IP CAN bearers, IP CAN session or IMS sessions.\n\nThe account balance management function (ABMF) is the location of the subscriber’s account balance within the OCS.\n\n\n\n"}
{"id": "236268", "url": "https://en.wikipedia.org/wiki?curid=236268", "title": "Pioneer plaque", "text": "Pioneer plaque\n\nThe Pioneer plaques are a pair of gold-anodized aluminium plaques which were placed on board the 1972 \"Pioneer 10\" and 1973 \"Pioneer 11\" spacecraft, featuring a pictorial message, in case either \"Pioneer 10\" or \"11\" is intercepted by extraterrestrial life. The plaques show the nude figures of a human male and female along with several symbols that are designed to provide information about the origin of the spacecraft.\n\nThe \"Pioneer 10\" and \"11\" spacecraft were the first human-built objects to achieve escape velocity from the Solar System. The plaques were attached to the spacecraft's antenna support struts in a position that would shield them from erosion by interstellar dust.\n\nThe original idea, that the Pioneer spacecraft should carry a message from mankind, was first mentioned by Eric Burgess when he visited the Jet Propulsion Laboratory in Pasadena, California, during the Mariner 9 mission. He approached Carl Sagan, who had lectured about communication with intelligent extraterrestrials at a conference in Crimea.\n\nSagan was enthusiastic about the idea of sending a message with the Pioneer spacecraft. NASA agreed to the plan and gave him three weeks to prepare a message. Together with Frank Drake he designed the plaque, and the artwork was prepared by Linda Salzman Sagan, who was Sagan's wife at the time.\n\nBoth plaques were manufactured at Precision Engravers, San Carlos, California.\n\nThe first plaque was launched with \"Pioneer 10\" on March 2, 1972, and the second followed with \"Pioneer 11\" on April 5, 1973.\n\nIn May 2017 a limited edition of 200 replicas engraved from the original master design at Precision Engravers was made available in a Kickstarter Campaign, which also offered laser-engraved replicas.\n\n\nAt the top left of the plate is a schematic representation of the hyperfine transition of hydrogen, which is the most abundant element in the universe. The spin-flip transition of a hydrogen atom’s electron has a frequency of about 1420.405 MHz, which corresponds to a period of 0.704 ns. Light at this frequency has a vacuum wavelength of 21.106 cm (which is also the distance the light travels in that time period). Below the symbol, the small vertical line—representing the binary digit 1—specifies a unit of length (21 cm) as well as a unit of time (0.7 ns). Both units are used as measurements in the other symbols.\n\nOn the right side of the plaque, a man and a woman are shown in front of the spacecraft. Between the brackets that indicate the height of the woman, the binary representation of the number 8 can be seen (1000, with a small defect in the first zero). In units of the wavelength of the hyperfine transition of hydrogen this means 8 × 21 cm = 168 cm.\n\nThe right hand of the man is raised as a sign of good will. Although this gesture may not be understood, it offers a way to show the opposable thumb and how the limbs can be moved.\n\nOriginally Sagan intended the humans to be shown holding hands, but soon realized that an extraterrestrial might perceive them as a single creature rather than two organisms.\n\nThe original drawings of the figures were based on drawings by Leonardo da Vinci and Greek sculptures.\n\nThe woman's genitals are not depicted in detail; only the \"Mons pubis\" is shown. It has been claimed that Sagan, having little time to complete the plaque, suspected that NASA would have rejected a more intricate drawing and therefore made a compromise just to be safe. Carl Sagan said that the decision to not include the vertical line on the woman's genitalia (pudendal cleft) which would be caused by the intersection of the labia majora was due to two reasons. First, Greek sculptures of women do not include that line. Second, Carl Sagan believed that a design with such an explicit depiction of a woman's genitalia would be considered too obscene to be approved by NASA. According to the memoirs of Robert S. Kraemer, however, the original design that was presented to NASA headquarters included a line which indicated the woman's vulva, and this line was erased as a condition for approval of the design by John Naugle, former head of NASA's Office of Space Science and the agency's former chief scientist.\n\nThe radial pattern on the left of the plaque shows 15 lines emanating from the same origin. Fourteen of the lines have corresponding long binary numbers, which stand for the periods of pulsars, using the hydrogen spin-flip transition frequency as the unit. Since these periods will change over time, the epoch of the launch can be calculated from these values.\n\nThe lengths of the lines show the relative distances of the pulsars to the Sun. A tick mark at the end of each line gives the Z coordinate perpendicular to the galactic plane.\n\nIf the plaque is found, only some of the pulsars may be visible from the location of its discovery. Showing the location with as many as 14 pulsars provides redundancy so that the location of the origin can be triangulated even if only some of the pulsars are recognized.\n\nThe data for one of the pulsars is misleading. When the plaque was designed, the frequency of pulsar \"1240\" (now known as J1243-6423) was known to only three significant decimal digits: 0.388 second. The map lists the period of this pulsar in binary to much greater precision: 100000110110010110001001111000. Rounding this off at about 10 significant bits (100000110100000000000000000000) would have provided a hint of this uncertainty. This pulsar is represented by the long line pointing down and to the right.\n\nThe fifteenth line on the plaque extends to the far right, behind the human figures. This line indicates the Sun's relative distance to the center of the galaxy.\n\nThe pulsar map and hydrogen atom diagram are shared in common with the Voyager Golden Record.\n\nAt the bottom of the plaque is a schematic diagram of the Solar System. A small picture of the spacecraft is shown, and the trajectory shows its way past Jupiter and out of the Solar System. Both \"Pioneers 10\" and \"11\" have identical plaques; however, after launch, Pioneer 11 was redirected toward Saturn and from there it exited the Solar System. In this regard the \"Pioneer 11\" plaque is somewhat inaccurate.\nThe Saturn flyby of \"Pioneer 11\" would also greatly influence its future direction and destination as compared to \"Pioneer 10\", but this fact is not depicted in the plaques.\n\nSaturn's rings could give a further hint to identifying the Solar System. Rings around the planets Jupiter, Uranus, and Neptune were unknown when the plaque was designed; however, unlike Saturn, the ring systems on these planets are not so easily visible and apparent as Saturn's. Pluto was considered to be a planet when the plaque was designed; in 2006 the IAU reclassified Pluto as a dwarf planet and then in 2008 as a plutoid. Other large bodies classed as dwarf planets, such as Eris, are not depicted, as they were unknown at the time the plaque was made.\n\nThe binary numbers above and below the planets show the relative distance to the Sun. The unit is 1/10 of Mercury's orbit. Rather than the familiar \"1\" and \"0\", \"I\" and \"-\" are used.\n\nBehind the figures of the human beings, the silhouette of the Pioneer spacecraft is shown in the same scale so that the size of the human beings can be deduced by measuring the spacecraft.\n\nOne of the parts of the diagram that is among the easiest for humans to understand may be among the hardest for the extraterrestrial finders to understand: the arrow showing the trajectory of Pioneer. An article in \"Scientific American\" criticized the use of an arrow because arrows are an artifact of hunter-gatherer societies like those on Earth; finders with a different cultural heritage may find the arrow symbol meaningless.\n\nArt critic and activist Craig Owens said that sexual bias is exhibited by the choice to make the man in the diagram perform the wave gesture to greet the extraterrestrials while the woman in the diagram has her hands at her sides. Feminists also took issue with this choice for the same reason.\n\nCarl Sagan regretted that the figures in the finished engraving failed to look panracial. Although this was the intent, the final figures appeared Caucasian. In the original drawing, the man was drawn with an \"Afro\" haircut, so an additional African physical trait would be included in the man to make the figures look more panracial, but that detail was changed to a \"non-African Mediterranean-curly haircut\" in the finished engraving. Furthermore, Carl Sagan said that Linda Sagan intended to portray both the man and woman as having brown hair, but the hair being only outlined rather than being both outlined and shaded made the hair appear blonde instead. Other people had different interpretations of the race of people depicted by the figures. Whites, blacks and Asians each tended to think that the figures resembled their own racial group, so, although some people were proud that their race appeared to have been selected to represent all of humankind, others viewed the figures as \"terribly racist\" for \"the apparently blatant exclusion\" of other races.\n\nLinda Sagan decided to make the figures nude to address the problem of the type of clothes they should wear to represent all of humanity and to make the figures more anatomically educational for extraterrestrials, but their nudity was viewed as pornographic by the American media. According to astronomer Frank Drake, there were many negative reactions to the plaque because the human beings were displayed naked. When images of the final design were published in American newspapers, one newspaper published the image with the man's genitalia removed and another newspaper published the image with both the man's genitalia and the woman's nipples removed. In one letter to a newspaper, a person angrily wrote that they felt that the nudity of the images made the images obscene. In contrast, in another letter to the same newspaper, a person was critical of the prudishness of the people who found depictions of nudity to be obscene.\n\n\n\n\n"}
{"id": "11241476", "url": "https://en.wikipedia.org/wiki?curid=11241476", "title": "Project on Emerging Nanotechnologies", "text": "Project on Emerging Nanotechnologies\n\nThe Project on Emerging Nanotechnologies was established in 2005 as a partnership between the Woodrow Wilson International Center for Scholars and the Pew Charitable Trusts. The Project was intended to address the social, political, and public safety aspects of nanotechnology. It intended in particular to look for research and policy gaps and opportunities in knowledge and regulatory processes, and to develop strategies for closing them. The project worked with multiple U.S. and foreign governments and organizations.\n\nThe project's stated goal was \"to inform the debate and to create an active public and policy dialogue. It was not an advocate either for, or against, particular nanotechnologies. Rather, the Project sought to ensure that as these technologies are developed, potential human health and environmental risks were anticipated, properly understood, and effectively managed.\"\n\nThey have produced many publications on the various aspects of nanotechnology policy. One of the notable reports is on \"Managing the Effects of Nanotechnology\", written by J. Clarence (Terry) Davies in 2006. They also maintain several online databases including the widely cited consumer products inventory, the \"Nanotechnology Health and Environmental Implications: An inventory of current research\" as well as a series of PEN Reports. Their work has also been published in academic journals such as Nature Nanotechnology.\n\nA major activity of the Project was testimony on public forums.\n\nThe Advisory Board included Linda J. Fisher, Vice President and Chief sustainability officer at DuPont, Margaret A. Hamburg M.D., Vice President for Biological Programs, Nuclear Threat Initiative, Donald Kennedy, editor-in-chief of Science magazine and president emeritus and Bing Professor of Environmental Science, Emeritus, at Stanford University, John Ryan is Director of the Bionanotechnology IRC at Oxford University, and Stan Williams, Senior Fellow and Director of Quantum Science Research at Hewlett-Packard.\n\n"}
{"id": "38570790", "url": "https://en.wikipedia.org/wiki?curid=38570790", "title": "Quad chart", "text": "Quad chart\n\nA quad chart is a form of technical documentation used to briefly describe an invention or other innovation through writing, illustration and/or photographs. Such documents are described as \"quad\" charts because they are divided into four quadrants laid out on a landscape perspective. They are typically one-page only; their succinctness facilitates rapid decision-making. Though shorter, quad charts often serve in a similar capacity to white papers and the two documents are often requested alongside one another.\n\nQuad charts as a genre were developed by the United States Department of Commerce's National Oceanic and Atmospheric Administration in an attempt to improve budgeting and planning systems, and became widely used in the Administration's National Weather Service. The genre's development was parallel to that of display boards, also an early tool used by the NWS for staff communication.\n\nIn the early 2000s, software was developed to allow automated creation of quad charts as a means of saving time for technical writers who would otherwise spend long periods of time drafting them.\n\nBoth government agencies and large businesses often require submission of a quad chart on the part of potential contractors as part of the contract bidding process. NASA, for example, uses quad charts to document the process of all Small Business Innovation Research projects. Because decision makers often review a large volume of both solicited and unsolicited proposals, the quad chart may be the only submission from a potential contractor which the decision maker actually reads.\n\nDue to the nature of quad charts as relatively short documents, there are opportunities for misuse. While quad charts are intended for brief overviews of a topic, they can also be misconstrued to influence public policy and budgeting decisions, as was the case with the politicization of the National Defense Strategy's 2005 edition.\n\nWhile there are no industry-wide standards for quad charts, there are a number of common elements. In addition to the title of the invention or idea and the name of the developer, the technical approach and the need which the invention or idea addresses are often included. Decision makers often look to operational needs first, though including the cost and projected schedule are also often required elements.\n\n"}
{"id": "56133555", "url": "https://en.wikipedia.org/wiki?curid=56133555", "title": "RGK Mobile", "text": "RGK Mobile\n\nRGK Mobile is a global m-commerce service provider.\n\nRGK Mobile specializes is a global provider of mobile carrier payment solutions, specializing in payment aggregation. Facilitating direct billing services with over 65 leading mobile carriers telecommunications company worldwide in over 30 countries, RGK Mobile allows service merchants content providers direct access to millions of new, potential users.The company was named as one of the key providers of Direct Carrier Billing Aggregation services in the world by MRS.\n\nIn November 2017, the company signed a deal with telecommunications company BITE, providing the mobile operator with a new content subscription revenue channel, and with direct operator billing service. RGK also provides customer support. BITE is a telecommunications company which serves subscribers in Latvia and Lithuania.\n\nIn December 2017, RGK Mobile partnered with the Lazar Angelov Academy, distributing the company’s online personal training and body building programs to mobile operators worldwide. RGK also provides subscriber support for Angleov’s users. \n\nIn January 2018, the company signed a deal with telecommunications company Bharti Airtel, providing India's largest mobile operator with a new content subscription services and direct operator billing service. \n\nIn March 2018, the company announced a deal with telecommunications company Axiata Digital, providing one of Asia's largest mobile carriers with a new content subscription services and direct operator billing service. \n\nIn May 2018, the company announced its European expansion, signing a Direct Carrier Billing agreement with telecommunications company Sunrise Communications AG, providing one of Switzerland's leading mobile carriers with a new content subscription and billing services. \n\nIn November 2018, RGK Mobile announced its first carrier billing agreement with one of Europe's largest mobile carriers telecommunications company Vodafone. \n\n\nThe company’s in-house automatic fraud detection and prevention system detects up to 70% of fraudulent traffic preventatively, while 25% is detected within a few minutes to a few hours of the breech.\n\n\nRGK Mobile was a Gold Sponsor at World Telemedia 2017, in which Vladimir Yuzak, Business Development Officer of RGK, spoke about “Virtual Content Provider-next step in evolution?”\nRGK Mobile was a Sponsor at TelcoDays Prague 2017, in which Vladimir Yuzak, Business Development Officer of RGK, introduced the RGK Engine”\n\nRGK Mobile was named as one of the key providers of Direct Carrier Billing Aggregation services in the world by MRS.\n\nIn June 2018, Forbes Magazine has named RGK Mobile a thought leader on GDPR privacy policies, in the EU. \n"}
{"id": "4692150", "url": "https://en.wikipedia.org/wiki?curid=4692150", "title": "Report", "text": "Report\n\nA report or account is an informational work, such as writing, speech, television or film, made with the intention of relaying information or recounting events in a presentable form.\n\nA report is made with the specific intention of relaying information or recounting certain events in a way that is concise, factual and relevant to the audience at hand. Reports may be conveyed through a written medium, speech, television, or film. In professional spheres, reports are a common and vital communication tool. Additionally, reports may be official or unofficial, and can be listed publicly or only available privately depending on the specific scenario. The audience for a report can vary dramatically, from an elementary school classroom to a boardroom on Wall Street.\n\nReports fill a vast array of informational needs for a spectrum of audiences. They may be used to keep track of information, evaluate a strategy, or make decisions. Written reports are documents which present focused and salient content, generally to a specific audience. A type of an official report would be a police report, which could have legally binding consequences. Other types of reports, such as \"Consumer Reports\", inform the public about the quality of products available on the market. Reports are used in government, business, education, science, and other fields, often to display the results of an experiment, investigation or inquiry.\n\nOne of the most common formats for presenting reports is IMRAD—introduction, methods, results, and discussion. This structure, standard for the genre, mirrors traditional publication of scientific research and summons the ethos and credibility of that discipline. Reports are not required to follow this pattern and may use alternative methods such as the problem-solution format, wherein the author first lists an issue and then details what must be done to fix the problem. Transparency and a focus on quality are keys to writing a useful report. Accuracy is also important. Faulty numbers in a financial report could lead to disastrous consequences.\n\nReports use features such as tables, graphics, images, voice, or specialized vocabulary in order to persuade a specific audience to undertake an action or inform the reader of the subject at hand. Some common elements of written reports include headings to indicate topics and help the reader locate relevant information quickly, and visual elements such as charts, tables and figures, which are useful for breaking up large sections of text and making complex issues more accessible. Lengthy written reports will almost always contain a table of contents, appendices, footnotes, and references. A bibliography or list of references will appear at the end of any credible report and citations are often included within the text itself. Complex terms are explained within the body of the report or listed as footnotes in order to make the report easier to follow. A short summary of the report's contents, called an abstract, may appear in the beginning so that the audience knows what the report will cover. Online reports often contain hyperlinks to internal or external sources as well.\n\nVerbal reports differ from written reports in the minutiae of their format, but they still educate or advocate for a course of action. Quality reports will be well researched and the speaker will list their sources if at all possible.\n\nSome examples of reports are:\n\n\n"}
{"id": "13065101", "url": "https://en.wikipedia.org/wiki?curid=13065101", "title": "Robert E. Horn", "text": "Robert E. Horn\n\nRobert E. Horn (born 1933) is an American political scientist who taught at Harvard, Columbia, and Sheffield (U.K.) universities, and has been a visiting scholar at Stanford University's Center for the Study of Language and Information. He is known for the development of information mapping.\n\nBob Horn is perhaps best known for his development of information mapping, a method of information development called structured writing suited especially for technical communication.\n\nHis latest contributions to the presentation of information have been in the field of visual language. Horn has extended the use of visual language and visual analytics to develop methods—involving large, detailed infographics and argument map murals—for exploring and resolving wicked problems.\n\n\n"}
{"id": "1227094", "url": "https://en.wikipedia.org/wiki?curid=1227094", "title": "Single-source publishing", "text": "Single-source publishing\n\nSingle-source publishing, also known as single-sourcing publishing, is a content management method which allows the same source content to be used across different forms of media and more than one time. The labor-intensive and expensive work of editing need only be carried out once, on only one document; that source document can then be stored in one place and reused. This reduces the potential for error, as corrections are only made one time in the source document.\n\nThe benefits of single-source publishing primarily relate to the editor rather than the user. The user benefits from the consistency that single-sourcing brings to terminology and information. This assumes the content manager has applied an organized conceptualization to the underlying content (A poor conceptualization can make single-source publishing less useful). Single-source publishing is sometimes used synonymously with multi-channel publishing though whether or not the two terms are synonymous is a matter of discussion.\n\nWhile there is a general definition of single-source publishing, there is no single official delineation between single-source publishing and multi-channel publishing, nor are there any official governing bodies to provide such a delineation. Single-source publishing is most often understood as the creation of one source document in an authoring tool and converting that document into different file formats or human languages (or both) multiple times with minimal effort. Multi-channel publishing can either be seen as synonymous with single-source publishing, or similar in that there is one source document but the process itself results in more than a mere reproduction of that source.\n\nThe origins of single-source publishing lie, indirectly, with the release of Windows 3.0 in 1990. With the eclipsing of MS-DOS by graphical user interfaces, help files went from being unreadable text along the bottom of the screen to hypertext systems such as WinHelp. On-screen help interfaces allowed software companies to cease the printing of large, expensive help manuals with their products, reducing costs for both producer and consumer. This system raised opportunities as well, and many developers fundamentally changed the way they thought about publishing. Writers of software documentation did not simply move from being writers of traditional bound books to writers of electronic publishing, but rather they became authors of central documents which could be reused multiple times across multiple formats.\n\nThe first single-source publishing project was started in 1993 by Cornelia Hofmann at Schneider Electric in Seligenstadt, using software based on Interleaf to automatically create paper documentation in multiple languages based on a single original source file.\n\nXML, developed during the mid- to late-1990s, was also significant to the development of single-source publishing as a method. XML, a markup language, allows developers to separate their documentation into two layers: a shell-like layer based on presentation and a core-like layer based on the actual written content. This method allows developers to write the content only one time while switching it in and out of multiple different formats and delivery methods.\n\nIn the mid-1990s, several firms began creating and using single-source content for technical documentation (Boeing Helicopter, Sikorsky Aviation and Pratt & Whitney Canada) and user manuals (Ford owners manuals) based on tagged SGML and XML content generated using the Arbortext Epic editor with add-on functions developed by a contractor. The concept behind this usage was that complex, hierarchical content that did not lend itself to discrete componentization could be used across a variety of requirements by tagging the differences within a single document using the capabilities built into SGML and XML.\nFord, for example, was able to tag its single owner's manual files so that 12 model years could be generated via a resolution script running on the single completed file. Pratt & Whitney, likewise, was able to tag up to 20 subsets of its jet engine manuals in single-source files, calling out the desired version at publication time. World Book Encyclopedia also used the concept to tag its articles for American and British versions of English.\n\nStarting from the early 2000s, single-source publishing was used with an increasing frequency in the field of technical translation. It is still regarded as the most efficient method of publishing the same material in different languages. Once a printed manual was translated, for example, the online help for the software program which the manual accompanies could be automatically generated using the method. Metadata could be created for an entire manual and individual pages or files could then be translated from that metadata with only one step, removing the need to recreate information or even database structures.\n\nAlthough single-source publishing is now decades old, its importance has increased urgently as of the 2010s. As consumption of information products rises and the number of target audiences expands, so does the work of developers and content creators. Within the industry of software and its documentation, there is a perception that the choice is to embrace single-source publishing or render one's operations obsolete.\n\nEditors using single-source publishing have been criticized for below-standard work quality, leading some critics to describe single-source publishing as the \"conveyor belt assembly\" of content creation. \n\nWhile heavily used in technical translation, there are risks of error in regard to indexing. While two words might be synonyms in English, they may not be synonyms in another language. In a document produced via single-sourcing, the index will be translated automatically and the two words will be rendered as synonyms. This is because they are synonyms in the source language, while in the target language they are not.\n\n\n\n\n"}
{"id": "41771", "url": "https://en.wikipedia.org/wiki?curid=41771", "title": "System integrity", "text": "System integrity\n\nIn telecommunications, the term system integrity has the following meanings: \n\n\n"}
{"id": "1544179", "url": "https://en.wikipedia.org/wiki?curid=1544179", "title": "Technogaianism", "text": "Technogaianism\n\nTechnogaianism (a portmanteau word combining \"techno-\" for technology and \"gaian\" for Gaia philosophy) is a bright green environmentalist stance of active support for the research, development and use of emerging and future technologies to help restore Earth's environment. Technogaians argue that developing safe, clean, alternative technology should be an important goal of environmentalists.\n\nThis point of view is different from the default position of radical environmentalists and a common opinion that all technology necessarily degrades the environment, and that environmental restoration can therefore occur only with reduced reliance on technology. Technogaians argue that technology gets cleaner and more efficient with time. They would also point to such things as hydrogen fuel cells to demonstrate that developments do not have to come at the environment's expense. More directly, they argue that such things as nanotechnology and biotechnology can directly reverse environmental degradation. Molecular nanotechnology, for example, could convert garbage in landfills into useful materials and products, while biotechnology could lead to novel microbes that devour hazardous waste.\n\nWhile many environmentalists still contend that \"most\" technology is detrimental to the environment, technogaians point out that it has been in humanity's best interests to exploit the environment mercilessly until fairly recently. This sort of behaviour follows accurately to current understandings of evolutionary systems, in that when new factors (such as foreign species or mutant subspecies) are introduced into an ecosystem, they tend to maximise their own resource consumption until either, \"a)\" they reach an equilibrium beyond which they cannot continue unmitigated growth, or \"b)\" they become extinct. In these models, it is \"completely impossible\" for such a factor to totally destroy its host environment, though they may precipitate major ecological transformation before their ultimate eradication. Technogaians believe humanity has currently reached just such a threshold, and that the only way for human civilization to continue advancing is to accept the tenets of technogaianism and limit future exploitive exhaustion of natural resources and minimize further unsustainable development or face the widespread, ongoing mass extinction of species. The destructive effects of modern civilization can be mitigated by technological solutions, such as using nuclear power. Furthermore, technogaians argue that only science and technology can help humanity be aware of, and possibly develop counter-measures for, risks to civilization, humans and planet Earth such as a possible impact event. \n\nSociologist James Hughes mentions Walter Truett Anderson, author of \"To Govern Evolution: Further Adventures of the Political Animal\", as an example of a technogaian political philosopher; argues that technogaianism applied to environmental management is found in the reconciliation ecology writings such as Michael Rosenzweig's \"Win-Win Ecology: How The Earth's Species Can Survive In The Midst of Human Enterprise\"; and considers Bruce Sterling's Viridian design movement to be an exemplary technogaian initiative.\n\nThe theories of English writer Fraser Clark may be broadly categorised as technogaian. Clark advocated \"balancing the hippie right brain with the techno left brain\". The idea of combining technology and ecology were extrapolated at length by a South African eco-anarchist project in the 1990s. The Kagenna Magazine project aimed to combine technology, art and ecology in an emerging movement that could restore the balance between human and nature.\n\nGeorge Dvorsky suggests the sentiment of technogaianism is to heal the Earth, use sustainable technology, and create ecologically diverse environments. Dvorsky argues that defensive counter measures could be designed to counter the harmful effects of asteroid impacts, earthquakes, and volcanic eruptions. Dvorksky also suggest that genetic engineering could be used to reduce the environmental impact humans have on the earth.\n\nTechnology facilities the sampling, testing and monitoring of various environments and ecosystems. NASA uses space-based observations to conduct research on solar activity, sea level rise, the temperature of the atmosphere and the oceans, the state of the ozone layer, air pollution, and changes in sea ice and land ice.\n\nClimate engineering is a technogaian method that uses two categories of technologies- carbon dioxide removal and solar radiation management. Carbon dioxide removal addresses a cause of climate change by removing one of the greenhouse gases from the atmosphere. Solar radiation management attempts to offset effects of greenhouse gases by causing the Earth to absorb less solar radiation.\n\nEarthquake engineering is a technogaian method concerned with protecting society and the natural and man-made environment from earthquakes by limiting the seismic risk to acceptable levels.\nAnother example of a technogaian practice is an artificial closed ecological system used to test if and how people could live and work in a closed biosphere, while carrying out scientific experiments. It is in some cases used to explore the possible use of closed biospheres in space colonization, and also allows the study and manipulation of a biosphere without harming Earth's. The most advanced technogaian proposal is the \"terraforming\" of a planet, moon, or other body by deliberately modifying its atmosphere, temperature, or ecology to be similar to those of Earth in order to make it habitable by humans.\n\nS. Matthew Liao, professor of philosophy and bioethics at New York University, claims that the human impact on the environment could be reduced by genetically engineering humans to have, a smaller stature, an intolerance to eating meat, and an increased ability to see in the dark, thereby using less lighting. Liao argues that human engineering is less risky than geoengineering.\n\nGenetically modified foods have reduced the amount of herbicide and insecticide needed for cultivation. The development of glyphosate-resistant (Roundup Ready) plants has changed the herbicide use profile away from more environmentally persistent herbicides with higher toxicity, such as atrazine, metribuzin and alachlor, and reduced the volume and danger of herbicide runoff.\n\nAn environmental benefit of Bt-cotton and maize is reduced use of chemical insecticides. A PG Economics study concluded that global pesticide use was reduced by 286,000 tons in 2006, decreasing the environmental impact of herbicides and pesticides by 15%. A survey of small Indian farms between 2002 and 2008 concluded that Bt cotton adoption had led to higher yields and lower pesticide use. Another study concluded insecticide use on cotton and corn during the years 1996 to 2005 fell by of active ingredient, which is roughly equal to the annual amount applied in the EU. A Bt cotton study in six northern Chinese provinces from 1990 to 2010 concluded that it halved the use of pesticides and doubled the level of ladybirds, lacewings and spiders and extended environmental benefits to neighbouring crops of maize, peanuts and soybeans.\n\n"}
{"id": "30862857", "url": "https://en.wikipedia.org/wiki?curid=30862857", "title": "Technology and society", "text": "Technology and society\n\nTechnology society and life or technology and culture refers to cyclical co-dependence, co-influence, and co-production of technology and society upon the other (technology upon culture, and vice versa). This synergistic relationship occurred from the dawn of humankind, with the invention of simple tools and continues into modern technologies such as the printing press and computers. The academic discipline studying the impacts of science, technology, and society, and vice versa is called science and technology studies.\n\nThe importance of stone tools, circa 2.5 million years ago, is considered fundamental in the human development in the hunting hypothesis.\n\nPrimatologist, Richard Wrangham, theorizes that the control of fire by early humans and the associated development of cooking was the spark that radically changed human evolution. Texts such as \"Guns, Germs, and Steel\" suggest that early advances in plant agriculture and husbandry fundamentally shifted the way that collective groups of individuals, and eventually societies, developed.\n\nTechnology has become a huge part in society and day-to-day life. When societies know more about the development in a technology, they become able to take advantage of it. When an innovation achieves a certain point after it has been presented and promoted, this technology becomes part of the society.The use of technology in education provides students with technology literacy, information literacy, capacity for life-long learning and other skills necessary for the 21st century workplace. Digital technology has entered each process and activity made by the social system. In fact, it constructed another worldwide communication system in addition to its origin.\n\nA 1982 study by \"The New York Times\" described a technology assessment study by the Institute for the Future, \"peering into the future of an electronic world.\" The study focused on the emerging videotex industry, formed by the marriage of two older technologies, communications and computing. It estimated that 40 percent of American households will have two-way videotex service by the end of the century. By comparison, it took television 16 years to penetrate 90 percent of households from the time commercial service was begun.\n\nSince the creation of computers achieved an entire better approach to transmit and store data. Digital technology became commonly used for downloading music and watching movies at home either by DVDs or purchasing it online.\nDigital music records are not quite the same as traditional recording media. Obviously, because digital ones are reproducible, portable and free.\n\nSeveral states started to implement education technology in schools, universities and colleges. According to the statistics, in the early beginnings of 1990s the use of Internet in schools was ,on average, 2-3%. Continuously, by the end of 1990s the evolution of technology increases rapidly and reaches to 60%, and by the year of 2008 nearly 100% of schools use Internet on educational form. According to ISTE researchers, technological improvements can lead to numerous achievements in classrooms. E-learning system, collaboration of students on project based learning, and technological skills for future results in motivation of students. \n\nAlthough these previous examples only show a few of the positive aspects of technology in society, there are negative side effects as well. Within this virtual realm, social media platforms such as Instagram, Facebook, and Snapchat have altered the way Generation Y culture is understanding the world and thus how they view themselves. In recent years, there has been more research on the development of social media depression in users of sites like these. \"Facebook Depression\" is when users are so affected by their friends' posts and lives that their own jealousy depletes their sense of self-worth. They compare themselves to the posts made by their peers and feel unworthy or monotonous because they feel like their lives are not nearly as exciting as the lives of others.\n\nAnother instance of the negative effects of technology in society, is how quickly it is pushing younger generations into maturity. With the world at their fingertips, children can learn anything they wish to. But with the uncensored sources from the internet, without proper supervision, children can be exposed to explicit material at inappropriate ages. This comes in the forms of premature interests in experimenting with makeup or opening an email account or social media page—all of which can become a window for predators and other dangerous entities that threaten a child's innocence. Technology has a serious effect on youth's health. The overuse of technology is said to be associated with sleep deprivation which is linked to obesity and poor academic performance in the lives of adolescents.\n\nIn ancient history, economics began when spontaneous exchange of goods and services was replaced over time by deliberate trade structures. Makers of arrowheads, for example, might have realized they could do better by concentrating on making arrowheads and barter for other needs. Regardless of goods and services bartered, some amount of technology was involved—if no more than in the making of shell and bead jewelry. Even the shaman's potions and sacred objects can be said to have involved some technology. So, from the very beginnings, technology can be said to have spurred the development of more elaborate economies.Technology is seen as primary source in economic development.\n\nTechnology advancement and economic growth are related to each other.The level of technology is important to determine the economic growth.It is the technological process which keeps the economy moving.\n\nIn the modern world, superior technologies, resources, geography, and history give rise to robust economies; and in a well-functioning, robust economy, economic excess naturally flows into greater use of technology. Moreover, because technology is such an inseparable part of human society, especially in its economic aspects, funding sources for (new) technological endeavors are virtually illimitable. However, while in the beginning, technological investment involved little more than the time, efforts, and skills of one or a few men, today, such investment may involve the collective labor and skills of many millions.\n\nConsequently, the sources of funding for large technological efforts have dramatically narrowed, since few have ready access to the collective labor of a whole society, or even a large part. It is conventional to divide up funding sources into governmental (involving whole, or nearly whole, social enterprises) and private (involving more limited, but generally more sharply focused) business or individual enterprises.\n\nThe government is a major contributor to the development of new technology in many ways. In the United States alone, many government agencies specifically invest billions of dollars in new technology.\n\n[In 1980, the UK government invested just over six million pounds in a four-year program, later extended to six years, called the Microelectronics Education Programme (MEP), which was intended to give every school in Britain at least one computer, software, training materials, and extensive teacher training. Similar programs have been instituted by governments around the world.]\n\nTechnology has frequently been driven by the military, with many modern applications developed for the military before they were adapted for civilian use. However, this has always been a two-way flow, with industry often developing and adopting a technology only later adopted by the military.\n\nEntire government agencies are specifically dedicated to research, such as America's National Science Foundation, the United Kingdom's scientific research institutes, America's Small Business Innovative Research effort. Many other government agencies dedicate a major portion of their budget to research and development.\n\nResearch and development is one of the smallest areas of investments made by corporations toward new and innovative technology.\nMany foundations and other nonprofit organizations contribute to the development of technology. In the OECD, about two-thirds of research and development in scientific and technical fields is carried out by industry, and 98 percent and 10 percent, respectively, by universities and government. But in poorer countries such as Portugal and Mexico the industry contribution is significantly less. The U.S. government spends more than other countries on military research and development, although the proportion has fallen from about 30 percent in the 1980s to less than 10 percent.\n\nThe 2009 founding of Kickstarter allows individuals to receive funding via crowdsourcing for many technology related products including both new physical creations as well as documentaries, films, and webseries that focus on technology management. This circumvents the corporate or government oversight most inventors and artists struggle against but leaves the accountability of the project completely with the individual receiving the funds.\n\n\nThe implementation of technology influences the values of a society by changing expectations and realities. The implementation of technology is also influenced by values. There are (at least) three major, interrelated values that inform, and are informed by, technological innovations:\n\nTechnology often enables organizational and bureaucratic group structures that otherwise and heretofore were simply not possible. Examples of this might include:\n\nTechnology enables greater knowledge of international issues, values, and cultures. Due mostly to mass transportation and mass media, the world seems to be a much smaller place, due to the following:\n\nTechnology provides an understanding, and an appreciation for the world around us.\n\nMost modern technological processes produce unwanted by products in addition to the desired products, which is known as industrial waste and pollution. While most material waste is re-used in the industrial process, many forms are released into the environment, with negative environmental side effects, such as pollution and lack of sustainability. Different social and political systems establish different balances between the value they place on additional goods versus the disvalues of waste products and pollution. Some technologies are designed specifically with the environment in mind, but most are designed first for economic or ergonomic effects. Historically, the value of a clean environment and more efficient productive processes has been the result of an increase in the wealth of society, because once people are able to provide for their basic needs, they are able to focus on less tangible goods such as clean air and water.\n\nThe effects of technology on the environment are both obvious and subtle. The more obvious effects include the depletion of nonrenewable natural resources (such as petroleum, coal, ores), and the added pollution of air, water, and land. The more subtle effects include debates over long-term effects (e.g., global warming, deforestation, natural habitat destruction, coastal wetland loss.)\n\nEach wave of technology creates a set of waste previously unknown by humans: toxic waste, radioactive waste, electronic waste.\n\nOne of the main problems is the lack of an effective way to remove these pollutants on a large scale expediently. In nature, organisms \"recycle\" the wastes of other organisms, for example, plants produce oxygen as a by-product of photosynthesis, oxygen-breathing organisms use oxygen to metabolize food, producing carbon dioxide as a by-product, which plants use in a process to make sugar, with oxygen as a waste in the first place. No such mechanism exists for the removal of technological wastes.\n\nSociety also controls technology through the choices it makes. These choices not only include consumer demands; they also include:\n\nAccording to Williams and Edge, the construction and shaping of technology includes the concept of choice (and not necessarily conscious choice). Choice is inherent in both the design of individual artifacts and systems, and in the making of those artifacts and systems.\n\nThe idea here is that a single technology may not emerge from the unfolding of a predetermined logic or a single determinant, technology could be a garden of forking paths, with different paths potentially leading to different technological outcomes. This is a position that has been developed in detail by Judy Wajcman. Therefore, choices could have differing implications for society and for particular social groups.\n\nIn one line of thought, technology develops autonomously, in other words, technology seems to feed on itself, moving forward with a force irresistible by humans. To these individuals, technology is \"inherently dynamic and self-augmenting.\"\n\nJacques Ellul is one proponent of the irresistibleness of technology to humans. He espouses the idea that humanity cannot resist the temptation of expanding our knowledge and our technological abilities. However, he does not believe that this seeming autonomy of technology is inherent. But the perceived autonomy is because humans do not adequately consider the responsibility that is inherent in technological processes.\n\nLangdon Winner critiques the idea that technological evolution is essentially beyond the control of individuals or society in his book Autonomous Technology. He argues instead that the apparent autonomy of technology is a result of \"technological somnambulism,\" the tendency of people to uncritically and unreflectively embrace and utilize new technologies without regard for their broader social and political effects.\n\nIndividuals rely on governmental assistance to control the side effects and negative consequences of technology.\n\nRecently, the social shaping of technology has had new influence in the fields of e-science and e-social science in the United Kingdom, which has made centers focusing on the social shaping of science and technology a central part of their funding programs.\n\n\n"}
{"id": "17163802", "url": "https://en.wikipedia.org/wiki?curid=17163802", "title": "Technology dynamics", "text": "Technology dynamics\n\nTechnology dynamics is broad and relatively new scientific field that has been developed in the framework of the postwar science and technology studies field. It studies the process of technological change. Under the field of Technology Dynamics the process of technological change is explained by taking into account influences from \"internal factors\" as well as from \"external factors\". Internal factors relate technological change to unsolved technical problems and the established modes of solving technological problems and external factors relate it to various (changing) characteristics of the social environment, in which a particular technology is embedded.\n\nFor the last three decades, it has been argued that technology development is neither an autonomous process, determined by the \"inherent progress\" of human history, nor a process completely determined by external conditions like the prices of the resources that are needed to operate (develop) a technology, as it is theorized in neoclassical economic thinking. In mainstream neoclassical economic thinking, technology is seen as an exogenous factor: at the moment a technology is required, the most appropriate version can be taken down from the shelf based on costs of labor, capital and eventually raw materials.\n\nConversely, modern technology dynamics studies generally advocate that technologies are not \"self-evident\" or market-demanded, but are the upshot of a particular path of technology development and are shaped by social, economic and political factors. in this sense, technology dynamics aims at overcoming distinct \"internal\" and \"external\" points of views by presenting co-evolutionary approach regarding technology development.\n\nIn general, technology dynamics studies, besides giving a \"thick description\" of technology development, uses constructivist viewpoints emphasizing that technology is the outcome of particular social context. Accordingly, Technology Dynamics emphasizes the significance and possibility of regaining social control of technology, and also provides mechanisms needed to adapt to and steer the development of certain technologies. In that respect, it uses insights from retrospective studies to formulate hypotheses of a prospective nature on technology development of emerging technologies, besides formulating prescriptive policy recommendations.\n\nAn important feature of relevant theories of technological change therein is that they underline the quasi-evolutionary character of technological change: change based on technological variation and social selection in which technological knowledge, systems and institutions develop in interaction with each other. Processes of 'path dependence' are crucial in explaining technological change.\n\nFollowing these lines, there have been different approaches and concepts used under the field of technology dynamics.\n\n\nBased on the analysis of the various perspectives, one can aim at developing interventions in the dynamics of a technology. Some approaches have been developed targeting on interventions in technological change:\n\n\n\n"}
{"id": "26149061", "url": "https://en.wikipedia.org/wiki?curid=26149061", "title": "Tectonic weapon", "text": "Tectonic weapon\n\nA tectonic weapon is a hypothetical device or system which could create earthquakes, volcanoes, or other seismic events in specified locations by interfering with the Earth's natural geological processes. It was defined in 1992 by Aleksey Vsevolovidich Nikolayev, corresponding member Russian Academy of Sciences: \"A tectonic or seismic weapon would be the use of the accumulated tectonic energy of the Earth's deeper layers to induce a destructive earthquake\". He added \"to set oneself the objective of inducing an earthquake is extremely doubtful\".\n\nTheoretically, the tectonic weapon functions by creating a powerful charge of elastic energy in the form of deformed volume of the Earth's crust in a region of tectonic activity. This then becomes an earthquake once triggered by a nuclear explosion in the epicenter or a vast electric pulse. As to the question of whether a nuclear explosion can trigger an earthquake, there was the analysis of local seismic recordings within a couple of miles of nuclear tests in the 1960s at Nevada that showed nuclear explosions caused some tectonic stress. An account provided by a member of the Nevada Commission on Nuclear Projects, also claimed that a 1968 underground nuclear test called Faultless successfully induced an earthquake. The United States Geological Survey stated that it produced fresh fault rupture some 1200 meters long. There is also a theory that 1998 earthquake in Afghanistan was triggered by thermonuclear tests conducted in Indian and Pakistani test sites 2-20 days prior. \n\nRoger Clark, lecturer in geophysics at Leeds University said in the respected journal Nature in 1996, responding to a newspaper report that there had been two secret Soviet programs, \"Mercury\" and \"Volcano\", aimed at developing a \"tectonic weapon\" that could set off earthquakes from great distance by manipulating electromagnetism, said \"We don't think it is impossible, or wrong, but past experience suggests it is very unlikely\". According to Nature these programs had been \"unofficially known to Western geophysicists for several years\". According to the story the Mercury program began in 1987, three tests were conducted in Kyrgyzstan, and Volcano's last test occurred in 1992.\n\nSuch weapons, whether or not they exist or are feasible, are a source of concern in official circles. For example, US Secretary of Defense William S. Cohen, said on 28 April 1997 at the Conference on Terrorism, Weapons of Mass Destruction, and U.S. Strategy, University of Georgia, while discussing the dangers of false threats, \"Others are engaging even in an eco-type of terrorism whereby they can alter the climate, set off earthquakes, volcanoes remotely through the use of electromagnetic waves.\"\n\nNew Zealand's unsuccessful Project Seal programme during World War II attempted to create tsunami waves as a weapon. It was reported in 1999 that such a weapon might be viable.\n\nNikola Tesla claimed a small steam-powered mechanical oscillator he was experimenting with in 1898 produced earthquake-like effects, but this has never been replicated. The television show \"MythBusters\" in Episode 60 .E2.80.93 made a small machine based on the same principle but powered by electricity rather than steam; it produced vibrations in a large structure detectable 100 feet away, but no significant shaking, and they judged the effect to be a busted myth.\n\nThe 1978 Convention on the Prohibition of Military or Any Other Hostile Use of Environmental Modification Techniques is an international treaty ratified by 75 states, and signed by a further 17, that prohibits use of environmental modification techniques to cause earthquakes and tsunamis, amongst other phenomena.\n\nAfter natural tectonic phenomena such as the 2010 Haiti earthquake, conspiracy theories, usually relating to the armed forces of the United States and formerly the Soviet Union, often arise, though no evidence is advanced. After the Haiti earthquake it was widely reported that president Hugo Chávez of Venezuela made unsupported allegations that it had been caused by testing of a US tectonic weapon. The newspaper Komsomolskaya Pravda of Moscow reported on page 1 on 30 May 1992 that \"a geophysical or tectonic weapon was actually developed in the USSR despite the UN Convention\", but that Chief Seismologist Major-General V Bochrov of the USSR Ministry of Defence categorically rejected any hints on the existence of tectonic weapons.\n\nWhile the British Tallboy and Grand Slam bombs of World War II were called \"earthquake bombs\", the name came from their way of destroying very hardened targets by shaking their foundations as an earthquake would; they were never intended to cause an actual earthquake.\n\n"}
{"id": "50555537", "url": "https://en.wikipedia.org/wiki?curid=50555537", "title": "Thermal power station Regina Margherita", "text": "Thermal power station Regina Margherita\n\nThe thermal power station Regina Margherita was a large power station for the production of electricity, preserved at the Museo nazionale della scienza e della tecnologia Leonardo da Vinci in Milan, Italy. The station opened in 1895 and was originally installed in the Egidio e Pio Gavazzi silk factory in Desio (Milan), where it operated until 1954. It supplied electricity for lighting and for the operation of 1,800 looms, generating alternating electric current at a voltage of 200 V.\n\nDesigned at the Polytechnic University of Milan, it was built by combining a steam engine from the Franco Tosi company of Legnano and a pair of alternators from the Brown Boveri company.\n\nThe power station opened on November 9, 1895; the ceremony was attended by King Umberto I and Margherita of Savoy, to whom the plant was dedicated.\n\nIn 1958 Egidio e Pio Gavazzi proposed to donate the power plant to the Museo nazionale della scienza e della tecnologia Leonardo da Vinci. In order to exhibit the large machine, the floor was demolished, a stronger basement was built to support the item and the technical press consultation room was moved. Then the Desio plant was dismantled using maintenance cranes and it was transported with a Riva lorry to the museum, where it was reassembled by hand and connected to an electric motor, coupled with a reduction gear, and set in motion. The furnace and boiler, with their connected steam distribution pipes and pumps, were not transferred to the museum.\n\nThe station contains two parts: thermal, consisting of a steam engine with two horizontal cylinders, and electric, consisting of two alternators and two exciter dynamos. There is also an electric control panel and a lighting system with 8 lamps. The machine is activated by an electric motor, connected to it by a chain which encircles a pulley, and it no longer produces current.\n\nThis machine is an example of a compound steam engine.\n\nAlthough it relied on the finest nineteenth-century technologies, the \"Regina Margherita\" was not a cutting-edge piece of machinery. Ten years before its making, the Englishman Sir Charles Algernon Parsons had already invented the steam turbine. In the latter device the force of the steam acts directly on the blades of the wheel, producing the rotation necessary to operate the alternators. The steam turbine is more efficient than a cylinder and piston system because it reduces energy waste from the transformation of alternating motion into rotary motion and from the transmission of movement through connecting rods, cranks and belts.\n\n"}
{"id": "27944981", "url": "https://en.wikipedia.org/wiki?curid=27944981", "title": "Timeline of United States inventions (before 1890)", "text": "Timeline of United States inventions (before 1890)\n\nA timeline of United States inventions (before 1890) encompasses the ingenuity and innovative advancements of the United States within a historical context, dating from the Colonial Period to the Gilded Age, which have been achieved by inventors who are either native-born or naturalized citizens of the United States. Copyright protection secures a person's right to his or her first-to-invent claim of the \"original\" invention in question, highlighted in Article I, Section 8, Clause 8 of the United States Constitution, which gives the following enumerated power to the United States Congress:\nIn 1641, the first patent in North America was issued to Samuel Winslow by the General Court of Massachusetts for a new method of making salt. On April 10, 1790, President George Washington signed the Patent Act of 1790 (1 Stat. 109) into law proclaiming that patents were to be authorized for \"any useful art, manufacture, engine, machine, or device, or any improvement therein not before known or used\". On July 31, 1790, Samuel Hopkins of Pittsford, Vermont became the first person in the United States to file and to be granted a patent for an improved method of \"Making Pot and Pearl Ashes\". The Patent Act of 1836 (Ch. 357, 5 Stat. 117) further clarified United States patent law to the extent of establishing a patent office where patent applications are filed, processed, and granted, contingent upon the language and scope of the claimant's invention, for a patent term of 14 years with an extension of up to an additional 7 years. However, the Uruguay Round Agreements Act of 1994 (URAA) changed the patent term in the United States to a total of 20 years, effective for patent applications filed on or after June 8, 1995, thus bringing United States patent law further into conformity with international patent law. The modern-day provisions of the law applied to inventions are laid out in Title 35 of the United States Code (Ch. 950, sec. 1, 66 Stat. 792).\n\nFrom 1836 to 2011, the United States Patent and Trademark Office (USPTO) has granted a total of 7,861,317 patents relating to several well-known inventions appearing throughout the timeline below.\n\n\n\n\n\n\n1749 Lightning rod\n\n1752 Flexible urinary catheter\n\n1761 Armonica\n\n1776 Swivel chair\n\n1782 Flatboat\n\n1784 Bifocals\n\n\n1785 Artificial diffraction grating\n\n\n1787 Automatic flour mill\n\n\n1792 Cracker\n\n1793 Cotton gin\n\n1795 Wheel cypher\n\n\n1796 Rumford fireplace\n\n\n1796 Cupcake\n\n\n1801 Suspension bridge\n\n\n1801 Fire hydrant\n\n1802 Banjo clock\n\n\n1804 Burr Truss\n\n\n1805 Amphibious vehicle\n\n\n1805 Vapor-compression refrigeration\n\n\n1806 Coffee percolator\n\n\n1808 Lobster trap\n\n1812 Columbiad\n\n\n1813 Circular saw\n\n\n1815 Dental floss\n\n1816 Milling machine\n\n1818 Profile lathe\n\n1827 Detachable collar\n\n1829 Graham cracker\n\n\n1830 Platform scale\n\n\n1831 Flanged T rail\n\n1831 Multiple coil magnet\n\n1831 Doorbell (electric)\n\n1833 Sewing machine (lock-stitch)\n\n\n1834 Combine harvester\n\n\n1835 Steam shovel\n\n\n1835 Solar compass\n\n\n1835 Relay\n\n\n1836 Morse code\n\n\n1836 Gridiron (cooking)\n\n1836 Circuit breaker\n\n\n1837 Self-polishing cast steel plow\n\n\n1839 Corn sheller\n\n\n1839 Sleeping car\n\n1839 Vulcanized rubber\n\n1839 Babbitt (metal)\n\n1840 Howe truss\n\n1842 Inhalational anaesthetic\n\n1842 Grain elevator\n\n1843 Ice cream maker (hand-cranked)\n\n\n1843 Multiple-effect evaporator\n\n\n1843 Rotary printing press\n\n\n1844 Pratt truss\n\n\n1845 Pressure-sensitive tape\n\n1845 Maynard tape primer\n\n1845 Baseball\n\n\n1846 Transverse shuttle\n\n1846 Printing telegraph\n\n1847 Gas mask\n\n\n1847 Doughnut (ring-shaped)\n\n\n1848 Pin tumbler lock\n\n1849 Jackhammer\n\n\n1849 Safety pin\n\n\n1850 Dishwasher\n\n\n1850 Feed dogs\n\n1850 Vibrating shuttle\n\n\n1850 Inverted microscope\n\n\n1851 Rotary hook\n\n\n1851 Fire alarm box\n\n1852 Elevator brake\n\n1853 Burglar alarm\n\n1853 Potato chips\n\n1853 spring Clothespin\n\n\n1854 Breast pump\n\n\n1855 Calliope\n\n\n1856 Egg beater\n\n\n1856 Condensed milk\n\n\n1856 Equatorial sextant\n\n\n1857 Toilet paper (mass-produced and rolled)\n\n1857 Pink lemonade\n\n1857 Brown Truss\n\n1858 screw top Pepper shaker\n\n\n1858 Mason jar\n\n\n1858 Pencil eraser\n\n\n1858 Ironing board\n\n\n1858 Twine knotter\n\n\n1858 Dustpan\n\n1859 Electric stove\n\n1859 Escalator\n\n\n1860 Vacuum cleaner\n\n\n1860 Repeating rifle (lever action)\n\n1861 Jelly bean\n\n\n1861 Twist drill\n\n\n1861 Kinematoscope\n\n1861 Postcard\n\n1861 Machine gun (hand-cranked)\n\n1863 Breakfast cereal\n\n\n1863 Ratchet wrench\n\n\n1863 Quad skates\n\n\n1863 Double-barreled cannon\n\n\n1864 Spar torpedo\n\n\n1865 Cowboy hat\n\n\n1865 Rotary printing press (web)\n\n1866 Urinal (restroom version)\n\n1866 Chuckwagon\n\n1867 Motorcycle (steam-powered)\n\n\n1867 Paper clip\n\n\n1867 Barbed wire\n\n1867 Ticker tape\n\n1867 Water-tube boiler\n\n1867 Refrigerator car\n\n1868 Paper bag\n\n\n1868 Tape measure\n\n\n1869 Vibrator\n\n\n1869 American football\n\n\n1869 Pipe wrench\n\n1869 Clothes hanger\n\n1870 Bee smoker\n\n\n1870 Can opener (rotary)\n\n\n1870 Sandblasting\n\n\n1870 Feather duster\n\n1871 Rowing machine\n\n1872 Railway air brake\n\n\n1872 Diner\n\n\n1873 Earmuffs\n\n\n1873 Silo\n\n\n1873 Jeans\n\nJeans are trousers generally made from denim. Jeans became popular among teenagers starting in the 1950s which remains as a distinct icon of American fashion. In 1873, Levi Strauss and Jacob Davis co-invented and co-patented the idea of using copper rivets at the stress points of sturdy work pants. After one of Davis' customers kept purchasing cloth to reinforce torn pants, he had an idea to use copper rivets to reinforce the points of strain, such as on the pocket corners and at the top of the button fly. Davis did not have the required money to purchase a patent, so he wrote to Strauss suggesting that they both go into business together. Early Levis, called \"waist overalls\", came in a brown canvas duck fabric and a heavy blue denim fabric. His business became extremely successful, revolutionizing the apparel industry.\n\n1873 Knuckle coupler\n\nAlso known as a Janney coupler and the buckeye coupler, the knuckle coupler is the derivative of a coupling device that links and connects rolling railway cars such as passenger, refrigerator, freight, and stock cars together on railroad track. The knuckle coupler have a bifurcated drawhead and a revolving hook, which, when brought in contact with another coupler, automatically interlocks with its mate. Knuckle couplers replaced the much more dangerous link-and-pin couplers and became the basis for standard coupler design for the rest of the 19th century. The knuckle coupler was invented and patented by Eli H. Janney in 1873.\n\n1874 Fire sprinkler (automated)\n\nA fire sprinkler is the part of a fire sprinkler system that discharges water when the effects of a fire have been detected, such as when a pre-determined temperature has been reached. Henry S. Parmelee of New Haven, Connecticut invented and installed the first closed-head or automated fire sprinkler in 1874.\n\n1874 Spork\n\nA spork or a foon is a hybrid form of cutlery taking the form of a spoon-like shallow scoop with three or four fork tines. The spork is a portmanteau word combining \"spoon\" and \"fork\". The spork was invented in 1874 by Samuel W. Francis. U.S. patent #147,119 was filed on January 22, 1874, and issued to Francis on February 3, 1874.\n\n1874 Ice cream soda\n\n1874 Quadruplex telegraph\n\n1874 Jockstrap\n\nA jockstrap, also known as a jock, jock strap, strap, supporter, or athletic supporter, is an undergarment designed for supporting the male genitalia during sports or other vigorous physical activity. A jockstrap consists of a waistband (usually elastic) with a support pouch for the genitalia and two elastic straps affixed to the base of the pouch and to the left and right sides of the waistband at the hip. The jockstrap has been part of men's undergarments since 1874 when it was invented by C.F. Bennett of Chicago to protect and support bicycle riders (back then they were known as \"jockeys\") who were navigating the cobblestone streets common to the era.\n\n1874 Forstner bit\n\nForstner bits, also known as Forstner flange bits or webfoot augers, bore precise, flat-bottomed holes in wood, in any orientation with respect to the wood grain. Forstner bits can cut on the edge of a block of wood, and can cut overlapping holes. Because of the flat bottom to the hole, they are useful for drilling through veneer already glued to add an inlay. Forstner bits were invented and patented by Benjamin Forstner in 1874.\n\n1874 QWERTY\n\nQWERTY is the most used modern-day keyboard layout on English-language computer and typewriter keyboards. It takes its name from the first six characters seen in the far left of the keyboard's top row of letters. The QWERTY design was invented and patented by Christopher Sholes in 1874.\n\n1875 Biscuit cutter\n\n1875 Dental drill (electric)\n\n1875 Mimeograph\n\n1876 Synthesizer\n\n1876 Airbrush\n\n1876 Tattoo machine\n\n1877 Phonograph\n\nThe phonograph, record player or gramophone is an instrument for recording, reproducing and playing back sounds. The earliest phonographs used cylinders containing an audio recording engraved on the outside surface which could be reproduced when the cylinder was played. Later, the gramophone record with modulated spiral grooves set atop a rotating turntable. The phonograph was invented in 1877 by Thomas Alva Edison at his laboratory in Menlo Park, New Jersey. On February 8, 1878, Edison was issued the first patent (U.S. patent #200,521) for the phonograph.\n\n1877 District heating\n\nDistrict heating distributes heat generated in a centralized location for residential and commercial heating requirements. The heat is often obtained from a cogeneration plant burning fossil fuels but increasingly biomass, although heat-only boiler stations, geothermal heating and central solar heating are also used, as well as nuclear power. A system was built in France in the 14th Century and the United States Naval Academy in Annapolis, Maryland began steam district heating service in 1853. However, the first commercially successful district heating system was launched in Lockport, New York, in 1877 by American hydraulic engineer Birdsill Holly, considered the founder of modern district heating.\n\n1878 Carbon microphone\n\n1878 Free jet water turbine\n\n1878 Bolometer\n\n1879 mechanical production of Photographic plate\n\n1879 Carton\n\n1879 Cash register\n\nThe cash register is a device for calculating and recording sales transactions. When a transaction was completed, the first cash registers used a bell that rang and the amount was noted on a large dial on the front of the machine. During each sale, a paper tape was punched with holes so that the merchant could keep track of sales. Known as the \"Incorruptible Cashier\", the mechanical cash register was invented and patented in 1879 by James Ritty of Dayton, Ohio. John H. Patterson bought Ritty's patent and his cash register company in 1884.\n\n1880 Oil burner\n\nAn oil burner is a heating device which burns fuel oil. The oil is directed under pressure through a nozzle to produce a fine spray, which is usually ignited by an electric spark with the air being forced through by an electric fan. In 1880, Amanda Jones invented the oil burner in the oil fields of northern Pennsylvania where Jones completed her trial and error efforts of heating furnaces.\n\n1880 Candlepin bowling\n\nCandlepin bowling is a North American variation of bowling that is played primarily in the Canadian Maritime provinces, Quebec, Maine, Massachusetts, and New Hampshire. A candlepin bowling lane somewhat resembles lanes used in tenpin bowling. However, unlike tenpin bowling lanes that are flat, candlepin lanes are slightly depressed ahead of the pindeck. The candlepins themselves take on a cylindrical shape which are tapered at the tops and bottoms, thus giving them a resemblance to wax candles. In 1880, candlepin bowling was invented by Justin White of Worcester, Massachusetts.\n\n1881 Electric chair\n\n1881 Metal detector\n\n1881 Iron (electric)\n\n\n1882 Fan (electric)\n\n1883 Salt water taffy\n\n1883 Solar cell\n\nA solar cell is any device that directly converts the energy in light into electrical energy through the process of photovoltaics. Although French physicist Antoine-César Becquerel discovered the photovoltaic effect much earlier in 1839, the first solar cell, according to Encyclopædia Britannica, was invented by Charles Fritts in 1883, who used junctions formed by coating selenium with an extremely thin layer of gold. In 1941, the silicon solar cell was invented by another American named Russell Ohl. Drawing upon Ohl's work, three American researchers named Gerald Pearson, Calvin Fuller, and Daryl Chapin essentially introduced the first practical use of solar panels through their improvement of the silicone solar cell in 1954, which by placing them in direct sunlight, free electrons are turned into electric current enabling a six percent energy conversion efficiency.\n\n1883 Thermostat\n\nA thermostat is a device for regulating the temperature of a system so that the system's temperature is maintained near a desired setpoint temperature. The thermostat does this by switching heating or cooling devices on or off, or regulating the flow of a heat transfer fluid as needed, to maintain the correct temperature. The thermostat was invented in 1883 by Warren S. Johnson.\n\n1884 Machine gun\n\nThe machine gun is defined as a \"fully automatic\" firearm, usually designed to fire rifle cartridges in quick succession from an ammunition belt or large-capacity magazine. The world's first true machine gun, the Maxim gun, was invented in 1884 by the American inventor Hiram Stevens Maxim, who devised a recoil power of the previously fired bullet to reload rather than the crude method of a manually operated, hand-cranked firearm. With the ability to fire 750 rounds per minute, Maxim's other great innovation was the use of water cooling to reduce overheating. Maxim's gun was widely adopted and derivative designs were used on all sides during World War I.\n\n1884 Dissolvable pill\n\nA dissolvable pill is any pharmaceutical in tablet form that is ingested orally, which are crushable and able to dissolve in the stomach unlike tablets with hard coatings. The dissolvable pill was invented in 1884 by William E. Upjohn.\n\n1884 Skyscraper\n\nA skyscraper is a tall building that uses a steel-frame construction. After the Great Fire of 1871, Chicago had become a magnet for daring experiments in architecture as one of those was the birth of the skyscraper. The edifice known as the world's first skyscraper was the 10-story Home Insurance Company Building built in 1884. It was designed by the Massachusetts-born architect William Le Baron Jenney.\n\n1885 Popcorn machine\n\n1885 Photographic film\n\n1885 Mixer (cooking)\n\n1885 Fuel dispenser\n\nA fuel dispenser is used to pump gasoline, diesel, or other types of fuel into vehicles or containers. As the automobile was not invented yet, the gas pump was used for kerosene lamps and stoves. Sylvanus F. Bowser of Fort Wayne, Indiana invented the gasoline/petrol pump on September 5, 1885. Coincidentally, the term \"bowser\" is still often used in countries such as New Zealand and Australia as a reference to the fuel dispenser.\n\n1886 Filing cabinet (horizontal)\n\nA filing cabinet is a piece of office furniture used to store paper documents in file folders. It is an enclosure for drawers in which items are stored. On November 2, 1886, Henry Brown patented his invention of a \"receptacle for storing and preserving papers\". This was a fire- and accident-safe container made of forged metal, which could be sealed with a lock and key. It was special in that it kept the papers separated.\n\n1886 Telephone directory\n\nA telephone directory is a listing of telephone subscribers in a geographical area or subscribers to services provided by the organization that publishes the directory. R. H. Donnelley created the first official telephone directory which was referred to as the Yellow Pages in 1886.\n\n1887 Screen door\n\nA screen door can refer to a hinged storm door (cold climates) or hinged screen door (warm climates) covering an exterior door; or a screened sliding door used with sliding glass doors. In any case, the screen door incorporates screen mesh to block flying insects from entering and pets and small children from exiting interior spaces, while allowing for air, light, and views. The screen door was invented in 1887 by Hannah Harger.\n\n1887 Gramophone record\nA gramophone record, commonly known as a record, or a vinyl record, is an analog sound storage medium consisting of a flat disc with an inscribed, modulated spiral groove. The groove usually starts near the periphery and ends near the center of the disc. Ever since Thomas Edison invented the phonograph in 1877, it produced distorted sound because of gravity's pressure on the playing stylus. In response, Emile Berliner invented a new medium for recording and listening to sound in 1887 in the form of a horizontal disc, originally known as the \"platter\".\n\n1887 Slot machine\n\nA slot machine is a casino gambling machine. Due to the vast number of possible wins with the original poker card based game, it proved practically impossible to come up with a way to make a machine capable of making an automatic pay-out for all possible winning combinations. The first \"one-armed bandit\" was invented in 1887 by Charles Fey of San Francisco, California who devised a simple automatic mechanism with three spinning reels containing a total of five symbols – horseshoes, diamonds, spades, hearts and a Liberty Bell, which also gave the machine its name.\n\n1887 Softball\n\nAs a bat-and-ball team sport, softball is a variant of baseball. The difference between the two sports is that softball uses larger balls and requires a smaller playing field. Beginning as an indoor game in Chicago, softball was invented in 1887 by George Hancock.\n\n1887 Comptometer\n\n1888 Induction motor\n\nAn induction motor is an AC electric motor in which the electric current in the rotor needed to produce torque is induced by electromagnetic induction from the magnetic field of the stator winding instead of using mechanical commutation (brushes) that caused sparking in earlier electric motors. They are also self-starting. The Serbian-American inventor Nikola Tesla explored the idea of using a rotating magnetic induction field principle, using it in his invention of a poly-phase induction motor using alternating current which he received a patent for on May 1, 1888. The rights to Tesla's invention were licensed by George Westinghouse for the AC power system his company was developing.\n\nThe induction motor Tesla patented in the U.S. is considered to have been an independent invention since the Europe Italian physicist Galileo Ferraris published a paper on a rotating magnetic field based induction motor on 11 March 1888, almost two months before Tesla was granted his patent. A working model of the Ferraris inductionmotor may have been demonstrated at the University of Turin as early as 1885.\n\n1888 Kinetoscope\n\nThe Kinetoscope was an early motion picture exhibition device. It was designed for films to be viewed individually through the window of a cabinet housing its components. The Kinetoscope introduced the basic approach that would become the standard for all cinematic projection before the advent of video, creating the illusion of movement by conveying a strip of perforated film bearing sequential images over a light source with a high-speed shutter. First described in conceptual terms by Thomas Alva Edison in 1888, his invention was largely developed by one of his assistants, William Kennedy Laurie Dickson, between 1889 and 1892.\n\n1888 Trolley pole\n\nA trolley pole is a tapered cylindrical pole of wood or metal placed in contact with an overhead wire to provide electricity to the trolley car. The trolley pole sits atop a sprung base on the roof of the trolley vehicle, the springs maintaining the tension to keep the trolley wheel or shoe in contact with the wire. Occasionally, a Canadian named John Joseph Wright is credited with inventing the trolley pole when an experimental tramway in Toronto, Ontario, was built in 1883. While Wright may have assisted in the installation of railways at the Canadian National Exhibition (CNE), and may even have used a pole system, there is no hard evidence to prove it. Likewise, Wright never filed or was issued a patent. Official credit for the invention of the electric trolley pole has gone to an American, Frank J. Sprague, who devised his working system in Richmond, Virginia, in 1888. Known as the Richmond Union Passenger Railway, this 12-mile system was the first large-scale trolley line in the world, opening to great fanfare on February 12, 1888.\n\n1888 Drinking straw\n\nThe drinking straw is a tube used for transferring a liquid to the mouth, usually a drink from one location to another. The first crude forms of drinking straws were made of dry, hollow, rye grass. Marvin Stone is the inventor of the drinking straw. Stone, who worked in a factory that made paper cigarette holders, did not like this design because it made beverages taste like grass. As an alternative, on January 3, 1888, Stone got a piece of paper from his factory and wrapped it around a pencil. By coating it with wax, his drinking straw became leak-proof so that it would not get waterlogged.\n\n1888 Stepping switch\n\nIn electrical controls, a stepping switch, also known as a stepping relay, is an electromechanical device which allows an input connection to be connected to one of a number of possible output connections, under the control of a series of electrical pulses. The major use for these devices was in early automatic telephone exchanges to route telephone calls. It can step on one axis (called a uniselector), or on two axes (a Strowger switch). As the first automated telephone switch using electromagnets and hat pins, stepping switches were invented by Almon Brown Strowger in 1888. Strowger filed his patent application on March 12, 1889, and it was issued on March 10, 1891.\n\n1888 Revolving door\n\nA revolving door has three or four doors that hang on a center shaft and rotate around a vertical axis within a round enclosure. In high-rise buildings, regular doors are hard to open because of air pressure differentials. In order to address this problem, the revolving door was invented in 1888 by Theophilus Van Kannel of Philadelphia, Pennsylvania. Van Kannel patented the revolving door on August 7, 1888.\n\n1888 Ballpoint pen\n\nA ballpoint pen is a writing instrument with an internal ink reservoir and a sphere for a point. The internal chamber is filled with a viscous ink that is dispensed at its tip during use by the rolling action of a small sphere. The first ballpoint pen is the creation of American leather tanner John Loud of Weymouth, Massachusetts in 1888 which contained a reservoir for ink and a roller ball to mark up his leather hides. Despite Loud being the inventor of the ballpoint pen, it wasn't a practical success since the ink often leaked or clogged up. Loud took out a patent (British patent #15630) in the United Kingdom on October 30, 1888. However, it wasn't until 1935 when Hungarian newspaper editor László Bíró offered an improved version of the ballpoint pen that left paper smudge-free.\n\n1888 Telautograph\n\nThe telautograph, an analog precursor to the modern fax machine, transmits electrical impulses recorded by potentiometers at the sending station to stepping motors attached to a pen at the receiving station, thus reproducing at the receiving station a drawing or signature made by sender. It was the first such device to transmit drawings to a stationary sheet of paper. The telautograph's invention is attributed to Elisha Gray, who patented it in 1888.\n\n1888 Touch typing\n\n1888 Salisbury steak\n\n1889 Flexible flyer\n\nA flexible flyer or steel runner sled is a steerable wooden sled with thin metal runners whereby a rider may sit upright on the sled or lie on their stomach, allowing the possibility to descend a snowy slope feet-first or head-first. To steer the sled, the rider may either push on the wooden cross piece with their hands or feet, or pull on the rope attached to the wooden cross-piece. The flexible flyer was invented in 1889 by Philadelphia resident Samuel Leeds Allen. U.S. patent #408,681 was issued to Allen on August 13, 1889.\n\n1889 Payphone\n\nA payphone or pay phone is a public telephone, usually located in a stand-alone upright container such as a phone booth, with payment done by inserting money (usually coins), a credit or debit card, or a telephone card before the call is made. Pay telephone stations preceded the invention of the pay phone and existed as early as 1878. These stations were supervised by telephone company attendants or agents who collected the money due after people made their calls. In 1889, the first coin-operated telephone was installed by inventor William Gray at a bank in Hartford, Connecticut. However, it was a \"postpay\" machine that only accepted coins deposited after the call was placed.\n\nTimelines of United States inventions\n\nRelated topics\n\n"}
{"id": "31793449", "url": "https://en.wikipedia.org/wiki?curid=31793449", "title": "Transition management (governance)", "text": "Transition management (governance)\n\nTransition management is a governance approach that aims to facilitate and accelerate sustainability transitions through a participatory process of visioning, learning and experimenting. In its application, transition management seeks to bring together multiple viewpoints and multiple approaches in a 'transition arena'. Participants are invited to structure their shared problems with the current system and develop shared visions and goals which are then tested for practicality through the use of experimentation, learning and reflexivity. The model is often discussed in reference to sustainable development and the possible use of the model as a method for change.\n\nKey principles to transition management as a form of governance:\n\n\nThere have been numerous societal transitions in the past, studied examples include the transition from horse-drawn carriage to motorised cars and the change from physical telegraphy to the electric telephone. There are a number of theories that muse over how transition management evolved into being. One school of thought identifies the sociological aspect of transition as deeply rooted within population dynamics and the evolution of society from high birth rate/high death rate to low birth rate/low death rate. Other theorists consider that transition management has its basis within systems theory and the co-evolution of social and technical factors within the system. Most agree that the shift in the political landscape, from a centralised government to a more liberal, market-based structure has allowed new forms of bottom-up governance styles to rise to prominence and a break from dominant approaches. The most notable use of Transition Management can be established through its development into a practical tool by the Dutch Government to manage the radical transformation of their energy systems in the early 2000s. It was introduced into national policy in the Netherlands in the fourth National Environmental Policy Plan based on a report by Jan Rotmans, Rene Kemp, Frank Geels, Geert Verbong and Marjolein van Asselt.\n\nTransition management is an approach for tackling the complex issue of sustainable development. Sustainable development in itself is a dynamic, multi-dimensional, multi-actor and multi-level problem that is in a constant state of flux. Critics consider that the current political system is insufficiently equipped to deal with the complexity of the issue and that incremental changes will not address the fundamental system failures that underpin the issue. As an alternative to traditional politics, Transition Management will seek to steer development in a more sustainable direction by identifying and fundamentally restructuring the unsustainable systems that underpin our society. The goal of transition management is geared towards enabling, facilitating and guiding the social, technical and political transformations required by embedded societal systems to bring about sustainability. The need for such a model of governance has arisen through the persistence of problems which have developed to span multi-actors, multi-levels and multi-domains. The inherent complexity of society (from the difference of perspectives, norms and values) added to the intricacy of modern-day issues requires a new form of governance. Therefore, transition management recognises the need to address this problem on the multiple levels and dimensions in which it manifests. The approach seeks to widen participation by encouraging bottom-up approaches that are supported in a top-down manner. The synergy gained from utilising transition management to provide a novel approach to the complex issue of Sustainable Development could be essential if progress is to be made on the issue.\n\nUnlike traditional forms of regulation that use command and control techniques, transition management does not seek to control the uncertainties of change but steer, indirectly influence and redirect the choices of actors towards sustainability. In the long-term, transition management will seek to completely transform the system through the process of creative destruction, much of the literature considers that only the radical rebuilding of our society's systems will be able to transcend the stable lock-in to unsustainable systems which has been systematically reinforced by aspects of the landscape and regime levels.\nMost literature recognises that there are three separate levels that transition management must work within; Landscape, Regime and Niche:.\n\n\nThese processes occur within the wider political, cultural and economic background termed the socio-technical landscape. The landscape is an external backdrop to the interplay of actors at the regime and niche level. Changes can occur in the landscape but much more slowly than regime level. One such change is the increase in environmental awareness. This socio-cultural process is leading to pressure on numerous regimes (aviation, agriculture etc.) whilst providing openings for new technologies to establish themselves.\n\n\nTechnological regimes are defined as a set of rules embedded in an engineering community's institutions and infrastructure which shape technological innovations. Geels expanded the focus from engineers to include a wider of range of social groups such as policy makers, financiers and suppliers. This web of inter-linking actors, following a set of rules was termed 'socio-technical regime', in effect, the established practices of a given system. Drawing on evolutionary economics; socio-technical regimes act as a selection and retention mechanism, filtering out the unsuccessful while incorporating more worthy innovations into the existing regime. The regime sits at the meso-level, sandwiched between the micro-level of the niche and the macro-level of the landscape. Change occurs at the regime level incrementally and is geared to achieving optimization. Radical change is potentially threatening to the vested interests of the established regime. The inertia of key industries is seen as an explanation of the difficulties in achieving transitions to sustainability.\n\n\nRadical innovations occur in niches, which act as safe environments in which breakthrough developments can grow, sheltered from the selection process that occurs at regime level. A regime may host a range of niches which generate innovations to challenge the status-quo. The military is seen as a primary niche for major technologies of the last century, supporting the development of radio, aircraft, computers and the internet. The framework of support provided can be financial (most early ventures being commercially unviable); establishing learning processes and facilitating the social networks that lead to growth.\n\nMulti-level Perspective\nGeels presents the interplay between regime, niche and landscape concepts as a multi-level perspective depicting technological transitions. The model is heuristic rather than ontological, and is an aid to better understand the process.\n\nFigure 1: A Dynamic multi-level perspective on Technological transition at: http://www.emeraldinsight.com/content_images/fig/0420240602001.png\nGeels and Schot: 2007\n\nOngoing processes at the regime and landscape level present 'windows of opportunity' for new technologies to exploit and become established. These breakthroughs tend to occur gradually through niche-accumulation. As innovations are used in multiple applications they build until achieving a critical mass. The model proposed by Geels shows how the success of a new technology requires developments across all levels to support the processes occurring within the niche (figure 1). Such an alignment is the basis of a regime shift.\n\nEach level has its own set of actors that interact in different ways, broadly they can be defined into the following categories:\n\n\nPower within the transition process is distributed rather than being top-down in nature; every actor plays a part in shaping the transition. This distributed power enables the process of mutual adaptation towards collective goals and the emergence of self-organised socio-technical 'trajectories'. However, power is not necessarily evenly distributed; relationships and the power of actors within any system are always mixed which gives rise to different forms of interaction and transition. Transition management seeks to exploit this opportunity by involving a wide selection of participants within the process. There are also a number of other important reasons for widening the participation of actors within governance. Firstly, most actors will have different preferences, a small group of actors, even if representative, will fail to identify one vision that will be accepted by everybody. By engaging all actors a plurality of visions that share common factors can be established and provide the basis for the next step. Furthermore, the use of widened participation is likely to attract stronger support and therefore less resistance to the transition. In order to fully transform the landscape level, the underpinning socio-political values and beliefs will also need to be radically rewritten, without the full involvement of society, this may be susceptible to failure. Finally it has also been considered that the heterogeneity of society allows for collective learning which spurs the development of innovations through exploration at the niche level.\n\nKemp \"et al.\" put forward a framework for identifying the different type of activities of which transition management attempts to align. These activities are broadly divided into, strategic, tactical and operational and each activity has its own actors, agendas and strategies which co-evolve.\n\nStrategic activities encompass the process of vision development; the collective action of goal and norm setting through discussion and the formulation of long-term goals. Strategic activities will lead to changes in the 'culture' of the societal system at the landscape level. The focus of this activity is long-term in scale (30 years/generations) and directed towards transition at the landscape level and the system as a whole.\n\nTactical activities relate to the interaction between actors that steers development of both institutions in the landscape level and socio-technical structures (practices, regulations, technology) at the regime level. Tactical activities focus on interpreting the visions created by strategic activities into the regime level and into the various networks, organisations and institutions involved. Tactical activities will also seek to identify the barriers that may be encountered (such as regulation, economic conditions) when interpreting these visions into the regime level. At this stage, actors that have the ability to make changes are recruited to translate the transition vision into their own agendas.\n\nFinally the operational activities constitute the process of learning by doing through experimentation and implementation at the niche level. It has a much short time span of 0–5 years and is focused on the radical innovation that will transform \"societal, technological, institutional, and behavioural practices\" that will in turn filter up and transform structures, culture and routines in the regime and landscape levels (Ibid).\n\nA further activity can be considered which establishes the process of assessment and evaluation of societal change. One can observe that reflexive activities can be both embedded within policy and regulation but also as a function of society and the evaluation of policies through the media and internet (ibid).\n\nThere are number of key aspects that differentiate transition management from other forms of governance, these differences are most apparent due to the model's prescriptive nature. With the concept still being quite fresh and only a handful of case studies to draw from the methodology is still under debate. However, most literature (See References - Loorbach 2007, Kemp, Meadowcroft 2009, Kemp and Loorbach 2003, Foxon \"et al.\" 2009) constitutes the following methodology:\n\nA problem becomes apparent to actors throughout the levels and domains; the first step involves defining the key parameters of the problem then characterising the existing regime and landscape pressures. Differences in interpretation, perceived pressures, opinions and preference ensues the construction of a plurality of visions and solutions for consideration. A 'basket of objectives' is created which express the shared visions and goals of the actors. The visions outlined are long term in nature most commonly spanning at least one generation if not more and are used to inform short-term objectives. These visions can be expressed in a number of ways; more common forms include the use of pathways, scenarios and blueprints. As an example, if the problem of the unsustainable nature of our oil-dependent nation is presented, visions that may be put forward that constitute a carbon-neutral future include; the hydrogen economy, the all-electric society and the transition to global energy infrastructure to facilitate the maximisation of renewable energy.\nAlthough each vision will require different socio-technical changes they will all seek to broadly ensure the same goals are met, for example a low or no carbon economy and a secure and reliable supply.\n\nOnce the pathways have been created short-term 'interim' objectives can be formed through Backcasting (as illustrated by the purple lines in the diagram) and actors within the niche level have the opportunity to form innovative solutions to the problem that may contribute to one of more of the pathways. Transition management will seek to identify those niches that will likely destabilise the regime level and contribute the most to its transformation. Once identified these niche opportunities can be supported by regime changes such as new policy implementation that provide new funding opportunities. In addition to deliberative steering of such choices, pressure can also filter down from the landscape level in the form of market forces which may also steer transition. Experiments within the niche level form a series of 'development rounds' which provide information to decision makers regarding the viability of different options. The information provided by the development rounds is evaluated and if the options are considered to be viable the solution is rolled out primarily on a small scale. Evaluative information can also be used to inform the overall vision created (as illustrated in red in the diagram). Eventually the development of options snowball down a particular pathway and a full system transition takes place. Not all development and transition time-scales are consistent within and between the different levels, actors and domains; some change much more quickly than others. In particular, the landscape level is much slower to change than the regime or niche levels; this has led to the concept being labelled as chaotic and non-linear in nature.\n\nAlthough being less apparent than the Dutch energy transformation, it appears that there is an increasing pressure for theorists to establish frameworks to guide a similar transition within the UK. The UK energy sector is an example of a socio-technical subsystem that exhibits strong lock-in, socially, politically and technically. The technical domain exhibits a strongly centralised infrastructure facilitated by a distribution network, socially speaking the UK energy system is heavily relied upon to provide welfare and enable economic activity. A number of landscape and regime pressures have emerged which impact on the system, primarily related to security of supply and the issue of climate change. The system has also come under pressure from the landscape level in the form of volatile market prices and the impact of the credit crunch. Meanwhile, development at the niche level has continued to build significant levels of alternative technologies and system possibilities through innovation such as offshore wind, wave and tidal power.\n\nAt the regime level, institutions such as the European Investment Bank are beginning to provide funding to facilitate the resources required to advance such niche technology. Furthermore, policies such as that of the Renewables Obligation channels funding from the energy industry to support the niche space and development. As per the method, a number of visions are currently being explored, including smart infrastructure, renewable energy alternatives and the viability of hydrogen. The UK Government uses the Renewable Obligation to 'steer' innovators towards particular solutions. More Renewable Obligation Certificates are awarded to those technologies that are pre-demonstration such as tidal and wave power, while developed technologies such as onshore wind receive less.\n\nHowever, although there are a number of indicators that suggest the transition management model of governance could emerge within the sector, (including in-depth literature outlining the methodology and possible pathways) the UK Government's current pathway (embodied in the UK Low Carbon Transition Plan) does not appear to embody the radical transformation required by transition management. Instead of seeking radical socio-technical transformation, the plan seeks to incrementally improve the system by seeking behavioural changes such as energy efficiency and technical changes by enhancing the contribution of renewables. Without radical infrastructure change the UK risks locking-out a number of promising possible alternatives and may significantly increase the cost of transformation in the future when the need for fundamental systems change becomes more apparent as the current system becomes less able to meet demand.\n\n"}
{"id": "6828605", "url": "https://en.wikipedia.org/wiki?curid=6828605", "title": "Woodward effect", "text": "Woodward effect\n\nThe Woodward effect, also referred to as a Mach effect, is part of a hypothesis proposed by James F. Woodward in 1990. The hypothesis states that transient mass fluctuations arise in any object that absorbs internal energy while undergoing a proper acceleration. Harnessing this effect could generate a reactionless thrust, which Woodward and others claim to measure in various experiments.\n\nHypothetically, the Woodward effect would allow for field propulsion spacecraft engines that would not have to expel matter. Such a proposed engine, is sometimes called a Mach effect thruster (MET) or a Mach Effect Gravitational Assist (MEGA) drive. So far, experimental results have not strongly supported this hypothesis, but experimental research on this effect, and its potential applications, continues. The anomalous thrust detected in some RF resonant cavity thruster (EmDrive/Cannae drive) experiments may be explained by the same type of Mach effect proposed by Woodward.\n\nThe Space Studies Institute was selected as part of NASA's Innovative Advanced Concepts program as a Phase I proposal in April 2017 for Mach Effect research. The year after, NASA awarded a NIAC Phase II grant to the SSI to further develop these propellantless thrusters.\n\nThe effect is controversial within mainstream physics because the underlying mathematics appears faulty, and the effect, if found to be real, would violate momentum conservation and energy conservation.\n\nAccording to Woodward, at least three Mach effects are theoretically possible: vectored impulse thrust, open curvature of spacetime, and closed curvature of spacetime.\n\nThe first effect, the Woodward effect, is the minimal energy effect of the hypothesis. The Woodward effect is focused primarily on proving the hypothesis and providing the basis of a Mach effect impulse thruster. In the first of three general Mach effects for propulsion or transport, the Woodward effect is an impulse effect usable for in-orbit satellite station-keeping, spacecraft reaction control systems, or at best, thrust within the solar system. The second and third effects are open and closed space-time effects. Open curved space-time effects can be applied in a field generation system to produce warp fields. Closed-curve space-time effects would be part of a field generation system to generate wormholes.\n\nThe third Mach effect is a closed-curve spacetime effect or closed timelike curve called a benign wormhole. Closed-curve space is generally known as a wormhole or black hole. Prompted by Carl Sagan for the scientific basis of wormhole transport in the movie \"Contact\", Kip Thorne developed the theory of benign wormholes. The generation, stability, and traffic control of transport through a benign wormhole is only theoretical at present. One difficulty is the requirement for energy levels approximating a \"Jupiter size mass\".\n\nKenneth Nordtvedt showed in 1988 that gravitomagnetism, which is an effect predicted by general relativity but hadn't yet been observed at that time and was even challenged by the scientific community, is inevitably a real effect because it is a direct consequence of the gravitational vector potential. He subsequently showed that the gravitomagnetism interaction (not to be confused with the Nordtvedt effect), like inertial frame dragging and the Lense–Thirring precession, is typically a Mach effect.\n\nThe Woodward effect is based on the relativistic effects theoretically derived from Mach's principle on inertia within general relativity, attributed by Albert Einstein to Ernst Mach. Mach's Principle is generally defined as \"the local inertia frame that is completely determined by the dynamic fields in the Universe.\" The conjecture comes from a thought experiment:\n\nA formulation of Mach's principle was first proposed as a vector theory of gravity, modeled on Maxwell's formalism for electrodynamics, by Dennis Sciama in 1953, who then reformulated it in a tensor formalism equivalent to general relativity in 1964.\n\nIn this paper, Sciama stated that instantaneous inertial forces in all accelerating objects are produced by a primordial gravity-based inertial radiative field created by distant cosmic matter and propagating both forward \"and\" backward in time at light speed:\nSciama's inertial-induction idea has been shown to be correct in Einstein's general relativity for any Friedmann–Robertson–Walker cosmology. According to Woodward, the derivation of Mach effects is relativistically invariant, so the conservation laws are satisfied, and no \"new physics\" is involved besides general relativity.\n\nAs previously formulated by Sciama, Woodward suggests that the Wheeler–Feynman absorber theory would be the correct way to understand the action of instantaneous inertial forces in Machian terms.\n\nThe Wheeler-Feynman absorber theory is an interpretation of electrodynamics that starts from the idea that a solution to the electromagnetic field equations has to be symmetric with respect to time-inversion, as are the field equations themselves. Wheeler and Feynman showed that the propagating solutions to classical wave equations can either be \"retarded\" (i.e. propagate forward in time) or \"advanced\" (propagate backward in time). The absorber theory has been used to explain quantum entanglement and led to the transactional interpretation of quantum mechanics, as well as the Hoyle-Narlikar theory of gravity, a Machian version of Einstein's general relativity. Fred Hoyle and Jayant Narlikar originally developed their cosmological model as a quasi steady state model of the universe, adding a \"Creation field\" generating matter out of empty space, an hypothesis contradicted by recent observations. When the C-field is not used, ignoring the parts regarding mass creation, the theory is no longer steady state and becomes a Machian extension of general relativity. This modern development is known as the \"Gravitational Absorber Theory\".\n\nAs the gravitational absorber theory reduces to general relativity in the limit of a smooth fluid model of particle distribution, both theories make the same predictions. Except in the Machian approach, a mass changing effect emerges from the general equation of motion, from which Woodward's transient mass equation can be derived. A resulting force suitable for Mach effect thrusters can then be calculated.\n\nWhile the Hoyle-Narlikar derivation of the Mach effect transient terms is done from a fully nonlinear, covariant formulation, it has been shown Woordward's transient mass equation can also be retrieved from linearized general relativity.\n\nThe following has been detailed by Woodward in various peer-reviewed papers throughout the last twenty years.\n\nAccording to Woodward, a transient mass fluctuation arises in an object when it absorbs \"internal\" energy as it is accelerated. Several devices could be built to store internal energy during accelerations. A measurable effect needs to be driven at a high frequency, so macroscopic mechanical systems are out of question since the rate at which their internal energy could be modified is too limited. The only systems that could run at a high frequency are electromagnetic energy storage devices. For fast transient effects, batteries are ruled out. A magnetic energy storage device like an inductor using a high-permeability core material to transfer the magnetic energy could be specially built. But capacitors are preferable to inductors because compact devices storing energy at a very high energy density without electrical breakdown are readily available. Shielding electrical interferences are easier than shielding magnetic ones. Ferroelectric materials can be used to make high-frequency electro-mechanical actuators, and they are themselves capacitors so they can be used for both energy storage and acceleration. Finally, capacitors are cheap and available in various configurations. So Mach effect experiments have always relied on capacitors so far.\n\nWhen the dielectric of a capacitor is submitted to a varying electric power (charge or discharge), Woodward's hypothesis predicts a transient mass fluctuation arises according to the transient mass equation (TME):\n\nwhere:\n\nThis equation is not the full Woodward equation as seen in the book. There is a third term, formula_8which Woodward discounts because his gauge setsformula_9; the derivatives of this quantity must therefore be negligible.\n\nThe previous equation shows that when the dielectric material of a capacitor is cyclically charged then discharged while being accelerated, its mass density fluctuates, by around plus or minus its rest mass value. Therefore, a device can be made to oscillate either in a linear or orbital path, such that its mass density is higher while the mass is moving forward, and lower while moving backward, thus creating an acceleration of the device in the forward direction, i.e. a thrust. This effect, used repeatedly, does not expel any particle and thus would represent a type of apparent propellantless propulsion, which seems to be in contradiction with Newton's third law of motion. However, Woodward states there is no violation of momentum conservation in Mach effects:\nTwo terms are important for propulsion on the right-hand side of the previous equation:\n\nApplications of propellantless propulsion include straight-line thruster or impulse engine, open curved fields for starship warp drives, and even the possibility of closed curved fields such as traversable benign wormholes.\n\nThe mass of the electron is positive according to the mass–energy equivalence \"E\" = \"mc\" but this invariant mass is made from the bare mass of the electron \"clothed\" by a virtual photon cloud. According to quantum field theory, as those virtual particles have an energy more than twice the bare mass of the electron, mandatory for pair production in renormalization, the nonelectromagnetic bare mass of the \"unclothed\" electron has to be \"negative\".\n\nUsing the ADM formalism, Woodward proposes that the physical interpretation of the \"wormhole term\" in his transient mass equation could be a way to expose the negative bare mass of the electron, in order to produce large quantities of exotic matter that could be used in a warp drive to propel a spacecraft or generate traversable wormholes.\n\nCurrent spacecraft achieve a change in velocity by the expulsion of propellant, the extraction of momentum from stellar radiation pressure or the stellar wind or the utilisation of a gravity assist (\"slingshot\") from a planet or moon. These methods are limiting in that rocket propellants have to be accelerated as well and are eventually depleted, and the stellar wind or the gravitational fields of planets can only be utilized locally in the Solar System. In interstellar space and bereft of the above resources, different forms of propulsion are needed to propel a spacecraft, and they are referred to as advanced or .\n\nIf the Woodward effect is confirmed and if an engine can be designed to use applied Mach effects, then a spacecraft may be possible that could maintain a steady acceleration into and through interstellar space without the need to carry along propellants. Woodward presented a paper about the concept at the NASA Breakthrough Propulsion Physics Program Workshop conference in 1997, and continued to publish on this subject thereafter.\n\nEven ignoring for the moment the impact on interstellar travel, future spacecraft driven by impulse engines based on Mach effects would represent an astounding breakthrough in terms of interplanetary spaceflight alone, enabling the rapid colonization of the entire solar system. Travel times being limited only by the specific power of the available power supplies and the acceleration human physiology can endure, they would allow crews to reach any moon or planet in our solar system in less than three weeks. For example, a typical one-way trip at an acceleration of 1 g from the Earth to the Moon would last only about 4 hours; to Mars, 2 to 5 days; to the asteroid belt, 5 to 6 days; and to Jupiter, 6 to 7 days.\n\nAs shown by the transient mass fluctuation equation above, exotic matter could be theoretically created. A large quantity of negative energy density would be the key element needed to create warp drives as well as traversable wormholes. As such, if proven to be scientifically valid, practically feasible and scaling as predicted by the hypothesis, the Woodward effect could not only be used for interplanetary travel, but also for apparent faster-than-light interstellar travel:\n\nTwo patents have been issued to Woodward and associates based on how the Woodward effect might be used in practical devices for producing thrust:\n\nWoodward and his associates have claimed since the 1990s to have successfully measured forces at levels great enough for practical use and also claim to be working on the development of a practical prototype thruster. No practical working devices have yet been publicly demonstrated.\n\nThe NIAC contract awarded in 2017 by NASA for the development of Mach effect thrusters is a primary three-task effort, two experimental and one analytical:\n\n\nA former type of Mach effect thruster was the Mach-Lorentz thruster (MLT). It used a charging capacitor embedded in a magnetic field created by a magnetic coil. A Lorentz force, the cross product between the electric field and the magnetic field, appears and acts upon the ions inside the capacitor dielectric. In such electromagnetic experiments, the power can be applied at frequencies of several megahertz, unlike PZT stack actuators where frequency is limited to tens of kilohertz. The photograph shows the components of a Woodward effect test article used in a 2006 experiment.\n\nHowever, a problem with some of these devices was discovered in 2007 by physicist Nembo Buldrini, who called it the \"Bulk Acceleration Conjecture\":\n\nTo address this issue, Woodward started to design and build a new kind of device known as a MET (Mach Effect Thruster) and later a MEGA drive (Mach Effect Gravitational Assist drive), using capacitors and a series of thick PZT disks. This ceramic is piezoelectric, so it can be used as an electromechanical actuator to accelerate an object placed against it: its crystalline structure expands when a certain electrical polarity is applied, then contracts when the opposite field is applied, and the stack of discs vibrates.\n\nIn the first tests, Woodward simply used a capacitor between two stacks of PZT disks. The capacitor, while being electrically charged to change its internal energy density, is shuttled back and forth between the PZT actuators. Piezoelectric materials can also generate a measurable voltage potential across their two faces when pressed, so Woodward first used some small portions of PZT material as little accelerometers put on the surface of the stack, to precisely tune the device with the power supply. Then Woodward realized that PZT material and the dielectric of a capacitor were very similar, so he built devices that are made exclusively of PZT disks, without any conventional capacitor, applying different signals to different portions of the cylindrical stack. The available picture taken by his graduate student Tom Mahood in 1999 shows a typical all-PZT stack with different disks:\nDuring forward acceleration and before the transient mass change in the capacitor decays, the resultant increased momentum is transferred forward to a bulk \"reaction mass\" through an elastic collision (the brass end cap on the left in the picture). Conversely, the following decrease in the mass density takes place during its backward movement.\nWhile operating, the PZT stack is isolated in a Faraday cage and put on a sensitive torsion arm for thrust measurements, inside a vacuum chamber. Throughout the years, a wide variety of different types of devices and experimental setups have been tested. The force measuring setups have ranged from various load cell devices to ballistic pendulums to multiple torsion arm pendulums, in which movement is actually observed. Those setups have been improved against spurious effects by isolating and canceling thermal transfers, vibration and electromagnetic interference, while getting better current feeds and bearings. Null tests were also conducted.\n\nIn the future, Woodward plans to scale thrust levels, switching from the current piezoelectric dielectric ceramics (PZT stacks) to new high-κ dielectric nanocomposite polymers, like PMN, PMN-PT or CCTO. Nevertheless, such materials are new, quite difficult to find, and are electrostrictive, not piezoelectric.\n\nIn 2013, the Space Studies Institute announced the Exotic Propulsion Initiative, a new project privately funded that aims to replicate Woodward's experiments and then, if proven successful, fully develop exotic propulsion. Gary Hudson, president and CEO of SSI, presented the program at the 2014 NASA Institute for Advanced Concepts Symposium, and a NIAC phase I grant was awarded in April 2017 to develop a better theoretical model and technical solutions for greater efficiency as a TRL-1 technology: reduction of heating and longer operating time using \"chirped pulses\"; and design of a dedicated electronic circuit with better frequency impedance matching. The concept of an interstellar mission to Proxima Centauri b was also detailed. Consecutively to these achievements, a NIAC Phase II grant was awarded in March 2018 to test an improved design with a higher operational frequency to increase the output thrust.\n\nAnother type of claimed propellantless thruster, called the EmDrive by its inventor British engineer Roger Shawyer, has been proposed to work due to a Mach effect:\n\nThe asymmetric resonant microwave cavity would act as a capacitor where:\nWhen a polymer insert is placed asymmetrically in the cavity, its dielectric properties result in greater asymmetry, while decreasing the cavity \"Q\" factor.\nThe cavity's acceleration is a function of all the above factors, and the model can explain the acceleration of the cavity with and without a dielectric.\n\nFrom his initial paper onward Woodward has claimed that this effect is detectable with modern technology. He and others have performed and continue to perform experiments to detect the small forces that are predicted to be produced by this effect. So far some groups claim to have detected forces at the levels predicted and other groups have detected forces at much greater than predicted levels or nothing at all. To date there has been no announcement conclusively confirming proof for the existence of this effect or ruling it out.\n\n\n\n\n\n\n\n\n\n\n\nAll inertial frames are in a state of constant, rectilinear motion with respect to one another; they are not accelerating in the sense that an accelerometer at rest in one would detect zero acceleration. Despite their ubiquitous nature, inertial frames are still not fully understood. That they exist is certain, but what causes them to exist – and whether these sources could constitute reaction-media – are still unknown. Marc Millis, of the NASA Breakthrough Propulsion Physics Program, stated \" \"For example, the notion of thrusting without propellant evokes objections of violating conservation of momentum. This, in turn, suggests that space drive research must address conservation of momentum. From there it is found that many relevant unknowns still linger regarding the source of the inertial frames against which conservation is referenced. Therefore, research should revisit the unfinished physics of inertial frames, but in the context of propulsive interactions.\" \" Mach's principle is generally defined within general relativity as \"the local inertia frame is completely determined by the dynamic fields in the universe.\" Rovelli evaluated a number of versions of \"Mach's principle\" that exist in the literature. Some are partially correct and some have been dismissed as incorrect.\n\nA challenge to the mathematical foundations of Woodward's hypothesis were raised in a paper published by the Oak Ridge National Laboratory in 2001. In the paper, John Whealton noted that the experimental results of Oak Ridge scientists can be explained in terms of force contributions due to time-varying thermal expansion, and stated that a laboratory demonstration produced 100 times the Woodward effect without resorting to non-Newtonian explanations. In response, Woodward published a criticism of Whealton's math and understanding of the physics involved, and built an experiment attempting to demonstrate the flaw.\n\nA rate of change in momentum represents a force, whereby F \"ma. Whealton et al. use the technical definition, Fd(\"mv)/d\"t\", which can be expanded to F\"m\" dv/d\"t\" + d\"m\"/d\"t\" v. This second term has both delta mass and v, which is measured instantaneously; this term will, in general, cancel out the force from the inertial response terms predicted by Woodward. Woodward argued that the d\"m\"/d\"t\" v term does not represent a physical force on the device, because it vanishes in a frame where the device is momentarily stationary.\n\nIn an appendix to his thesis, Mahood argues that the unexpectedly small magnitude of the results in his experiments are a confirmation of the cancellation predicted by Whealton; the results are instead due to higher-order mass transients which are not exactly cancelled. Mahood would later describe this argument as \"one of the very few things I've done in my life that I actually regret\".\n\nAlthough the momentum and energy exchange with distant matter guarantees global conservation of energy and momentum, this field exchange is supplied at no material cost, unlike the case with conventional fuels. For this reason, when the field exchange is ignored, a propellantless thruster behaves locally like a free energy device. This is immediately apparent from basic Newtonian analysis: if constant power produces constant thrust, then input energy is linear with time and output (kinetic) energy is quadratic with time. Thus there exists a break-even time (or distance or velocity) of operation, above which more energy is output than is input. The longer it is allowed to accelerate, the more pronounced will this effect become, as simple Newtonian physics predicts.\n\nConsidering those conservation issues, a Mach effect thruster relies on Mach's principle, hence it is not an electrical to kinetic transducer, i.e. it does not convert electric energy to kinetic energy. Rather, a Mach effect thruster is a gravinertial transistor that controls the flow of gravinertial flux, in and out of the active mass of the thruster. The primary power into the thruster is contained in the flux of the gravitational field, not the electricity that powers the device. Failing to account for this flux, is much the same as failing to account for the wind on a sail. Mach effects are relativistic by nature, and considering a spaceship accelerating with a Mach effect thruster, the propellant is not accelerating with the ship, so the situation should be treated as an accelerating and therefore non-inertial reference frame, where F does not equal \"m\"a. Keith H. Wanser, professor of physics at California State University, Fullerton, published a paper in 2013 concerning the conservation issues of Mach effect thrusters.\n\nIn 2009, Harold \"Sonny\" White of NASA proposed the Quantum Vacuum Fluctuation (QVF) conjecture, a non-relativistic hypothesis based on quantum mechanics to produce momentum fluxes even in empty outer space. Where Sciama's gravinertial field of Wheeler–Feynman absorber theory is used in the Woodward effect, the White conjecture replaces the Sciama gravinertial field with the quantum electrodynamic vacuum field. The local reactive forces are generated and conveyed by momentum fluxes created in the QED vacuum field by the same process used to create momentum fluxes in the gravinertial field. White uses MHD plasma rules to quantify this local momentum interaction where in comparison Woodward applies condensed matter physics.\n\nBased on the White conjecture, the proposed theoretical device is called a quantum vacuum plasma thruster (QVPT) or Q-thruster. No experiments have been performed to date. Unlike a Mach effect thruster instantaneously exchanging momentum with the distant cosmic matter through the advanced/retarded waves (Wheeler–Feynman absorber theory) of the radiative gravinertial field, Sonny's \"Q-thruster\" would appear to violate momentum conservation, for the thrust would be produced by pushing off virtual \"Q\" particle/antiparticle pairs that would annihilate after they have been pushed on. However, it would not necessarily violate the law of conservation of energy, as it requires an electric current to function, much like any \"standard\" MHD thruster, and cannot produce more kinetic energy than its equivalent net energy input.\n\nWoodward and Fearn showed why the amount of electron-positron virtual pairs of the quantum vacuum, used by White as a virtual plasma propellant, cannot account for thrusts in any isolated, closed electromagnetic system such as the QVPT or the EmDrive.\n\nWoodward's claims in his papers and in space technology conference press releases of a potential breakthrough technology for spaceflight have generated interest in the popular press\nand university news as well as the space news media. Woodward also gave a video interview for the TV show Ancient Aliens, season 6, episode 12. However doubters do exist.\n\n\n"}
