{"id": "59052343", "url": "https://en.wikipedia.org/wiki?curid=59052343", "title": "2018 in technology and computing", "text": "2018 in technology and computing\n\nSignificant events that have occurred in 2018 in all fields of technology, including computing, robotics, electronics, as well as any other areas of technology as well, including any machines, devices, or other technological developments, occurrences, and items. \n\n\n"}
{"id": "57096844", "url": "https://en.wikipedia.org/wiki?curid=57096844", "title": "ACP 125", "text": "ACP 125\n\nACP 125 is the short name for \"Allied Communications Publication 125: Communications Instructions—Radiotelephone Procedures\", developed and published by the Combined Communications Electronics Board, for use by the Five Eyes nations and the rest of NATO. According to the latest version, \"The aim of ACP 125 is to prescribe the voice procedure for use by the armed forces of Allied nations on secure and non-secure tactical voice nets. Its purpose is to provide a standardized way of passing speech and data traffic as securely as possible consistent with accuracy, speed and the needs of command and control.\"The standard defines the procedures for communicating by voice over two-way radio, and has served as the basis for radio communications procedures for many non-military organizations, as well as numerous U.S. government organizations, including the United States Department of State and the Civil Air Patrol. \n\nFirst published as ACP 125(A) in about 1951, the current version is ACP 125(G), published in 2016. The standard itself is ACP 125, with the letter in parenthesis indicating the major revision level. There are at least two supplements, including: \n\n\nAlthough the standard is designed for use by all NATO countries, especially when operating in conjunction with each other, some efforts have been made in the past to translate the procedure words into the native language of member countries. For example, NATO memo SGM-623-56 from 1956, \"French Equivalents to English Expressions Used in ACP 125(B)\", includes the following table of prowords:\n\n\n"}
{"id": "49071369", "url": "https://en.wikipedia.org/wiki?curid=49071369", "title": "Agritech", "text": "Agritech\n\nAgritech is the use of technology in agriculture, horticulture, and aquaculture with the aim of improving yield, efficiency, and profitability. Agritech can be products, services or applications derived from agriculture that improve various input/output processes.\n\nA major turning point for agricultural technology is the Industrial Revolution.\n\nTechnologies and applications in agri-tech include:\n\nDrones\nsatellite photography and sensors\nIoT-based sensor networks\nphase tracking\nweather forecasts\nautomated irrigation\nlight and heat control\nintelligent software analysis for pest and disease prediction, soil management and other involved analytical tasks\nBiotech is another type of agri-tech.\n\nAgriculture has been wrongly perceived in the past as a \"dirty job\" for the old people in rural communities but with the renaissance Technology brought to Agriculture, young people now see it as a potential sector to explore.\n\nThis is the practical use of Technology in Agriculture by a few start-ups in Africa.\n\nThere is a Nigeria’s digital agriculture platform which focused on connecting farm sponsors with real farmers in order to increase food production while promoting youth participation in agriculture.\n\nThis agritech startup is currently disrupting the agriculture ecosystem in the country by connecting small-scale farmers with investors using their platform App which is available on Google and Apple app stores.\n\nFarmers and sponsors all receive a percentage of the profits on harvest. The platform also makes provision for insurance cover for all existing farm projects, so that in the event of unforeseen circumstances, the sponsors’ capital can be refunded. \nDrones can be used on Crop field for scanning with compact multispectral imaging sensors, GPS map creation through onboard cameras, heavy payload transportation, and livestock monitoring with thermal-imaging camera-equipped drones.\n\nHigh-income countries have seen recent improvements in their agricultural management systems through modern remote sensing technology, such as satellites and aircraft and the information they collect. Out of the vast amount of data collected, advice is provided to farmers and fishers to help inform their decisions.\n\nThis has led to better crop yields, higher quality produce and more sustainable agricultural practices in some cases. Big data also informs high-level decision-makers on how to better manage food supply at national and regional levels.\n\nThe use of small Unmanned Aerial Vehicles (UAVs) better known as 'drones' for agricultural purposes is a new emerging technology which could revolutionise the way agricultural entrepreneurs interact with their land, water, crops and infrastructure. UAVs can be made specifically for business use and farming in particular, they can capture geo-referenced, overlapping, high-resolution images (2–5 cm) of 400 hectares in a single flight; can seamlessly upload data and produce agricultural analytics from their data management systems, and fly autonomously from take-off to landing.\n\nA blockchain is a digitized, decentralized, public ledger of all cryptocurrency transactions. Constantly growing as ‘completed’ blocks (the most recent transactions) are recorded and added to it in chronological order, it allows market participants to keep track of digital currency transactions without central recordkeeping. Each node (a computer connected to the network) gets a copy of the blockchain, which is downloaded automatically.\n\nOriginally developed as the accounting method for the virtual currency Bitcoin, blockchains – which use what's known as distributed ledger technology (DLT) – are appearing in a variety of commercial applications today. Currently, the technology is primarily used to verify transactions, within digital currencies though it is possible to digitize, code and insert practically any document into the blockchain. Doing so creates an indelible record that cannot be changed; furthermore, the record’s authenticity can be verified by the entire community using the blockchain instead of a single centralized authority.\n\nToday, Blockchain is revolutionizing the Agriculture sector in many ways including\n\nTrading commodities on the blockchain is helping to reduce middlemen interference by promoting a peer-to-peer model of connecting farmers with end users. It is also helping to promote fairer trades by removing trade barriers, reducing over reliance on United States Dollars and promoting cross-border trading in local currency.\n\nOther methods of leveraging blockchain technology is in settling land disputes through blockchain based land registry, and using QR codes to promote traceability.\n\nInformation Communication Technologies like podcasts, weblogs, social media platforms, e-books are constantly helping to bridge the information gap in the Agriculture sector for farmers and Agripreneurs.\n\nHydroponics is a soilless farming technology that is used to grow vegetables and tomatoes.\n\nIt guarantees an all-year-round production for farmers and insulates these crops from the effects of climate change. \n"}
{"id": "49449678", "url": "https://en.wikipedia.org/wiki?curid=49449678", "title": "Ami Dar", "text": "Ami Dar\n\nAmi Dar (born January 7, 1961) is the Founder and Executive Director of Idealist.org. Idealist serves more than 120,000 organizations around the world and has more than 1.4 million visitors every month between the English site (idealist.org), and the Spanish site (idealistas.org). \n\nDar was born January 7, 1961, in Jerusalem, Israel, the eldest of three children, to a school teacher mother and diplomat father. He grew up in Peru and Mexico, and it was in Mexico City where he first became aware of the contrast of wealth and poverty around him, which started him on a path of dedication to social justice. \n\nIn 1976, Dar and his family returned to Israel and from 1979 - 1982, he completed his mandatory service as a paratrooper. During this time Dar had an insight that fundamentally changed his thinking about borders and humanity and how labels define and separate people, which is often referred to as the \"sock sharers story.\" \n\nIn 1988 Dar joined Aladdin Knowledge Systems, a software company based in Tel Aviv. From 1988 to 1992 he served as the International Marketing Manager. In 1992 he was named President and he relocated to New York City to establish their North American branch. \n\nBy 1995 he had founded an early iteration of Idealist, The Contact Center Network sponsored meeting spaces in several communities where people could connect with neighbors who might share interests and ideas for local action. In 1996 Dar began calling the online network Idealist.org. \n\nIn January of 2018 the Idealist team launched Idealists of the World which has over 23,300 members world wide. The Idealists of the World participate in global Idealist Days which occur when the day and the month are the same number. The first global Idealist Day occurred on March 3rd, 2018 (3/3) with \"75 events mobilizing nearly 500 people around the world, and 3000 people in the Idealists of the World Facebook group.\" The philosophy behind Idealist Day is rooted in Ami Dar's vision of a world progressing towards positive social change. This is accomplished by people making connections online and in person, bridging divides to collaborate together for a better tomorrow. Regarding this vision Dar has stated,I can't help noticing that all over the world, behind every label and stereotype, there are people who share some basic values. And I can’t help thinking that if these people could somehow work together, the world would be a very different place. What are these values? Treating others the way we’d like to be treated is a good start. But we can go beyond that. For example, I believe that in every country and every culture there are many people who would agree with this sentence:\n\n\"“Working with others, in a spirit of generosity and mutual respect, I want to help build a world where all people can lead free and dignified lives.”\"\n\nThe Stern Family Fund awarded Dar a $100,000 Public Interest Pioneer grant (2000). \n\nDar was named an Ashoka Fellow (2004). Fellows are leading social entrepreneurs recognized to have innovative solutions to social problems and the potential to change patterns across society. They demonstrate unrivaled commitment to bold new ideas and prove that compassion, creativity, and collaboration are tremendous forces for change. Ashoka Fellows work in over 60 countries around the globe in every area of human need.\n\nHe is a Board Member Emeritus of the Nonprofit Technology Network (NTEN), the largest community of nonprofit professionals transforming technology into social change.\n\nDar was named \"Time Magazine's\" Philanthropy Innovator (2005).\n\nHe was recognized in the \"Nonprofit Times: Power and Influence\", 50 Most Influential People in 2000, 2002, 2003, 2004, and 2005.\n\nDar received Duke University's Fuqua School of Business, CASE Leadership in Social Entrepreneurship Award (2006).\n\nDar wrote the Forward to \"Nonprofit Management 101: A Complete and Practical Guide for Leaders and Professionals.\" Wiley.com. May 03, 2011.\n\nHe gave the Commencement Address for City University of New York (CUNY) School of Professional Studies in June 23, 2011.\n\nIn 2012, Dar was profiled in Forbes magazine. \n\nDar's career was profiled by Bloomberg in 2014.\n"}
{"id": "23091839", "url": "https://en.wikipedia.org/wiki?curid=23091839", "title": "Axle track", "text": "Axle track\n\nThe axle track in automobiles and other wheeled vehicles which have two or more wheels on an axle, is the distance between the centerline of two roadwheels on the same axle. In a case of the axle with dual wheels, the centerline in the middle of the dual wheel is used for the axle track specification. \n\nIn a vehicle with two axles, this is expressed as \"front track\" and \"rear track\".\n\nHowever the front wheels and/or rear wheels on either side of a vehicle do not necessarily have to be mounted on the same axle for the distance that they are apart to be called the \"track\".\n\nIn the case of a rail wheelset the \"track\" is called \"wheel gauge\" and is measured from wheel flange reference line to wheel flange reference line wheels of any rail car or tram.\n\n\n"}
{"id": "3135176", "url": "https://en.wikipedia.org/wiki?curid=3135176", "title": "B2MML", "text": "B2MML\n\nB2MML or Business To Manufacturing Markup Language is an XML implementation of the ANSI/ISA-95 family of standards (ISA-95), known internationally as IEC/ISO 62264. B2MML consists of a set of XML schemas written using the World Wide Web Consortium's XML Schema language (XSD) that implement the data models in the ISA-95 standard.\n\nB2MML is meant to be a common data definition to link ERP and supply chain management systems with manufacturing systems such as Industrial Control Systems and Manufacturing Execution Systems.\n\nB2MML is published by the Manufacturing Enterprise Solutions Association (MESA).\n\n"}
{"id": "99656", "url": "https://en.wikipedia.org/wiki?curid=99656", "title": "Creative destruction", "text": "Creative destruction\n\nCreative destruction (German: \"schöpferische Zerstörung\"), sometimes known as Schumpeter's gale, is a concept in economics which since the 1950s has become most readily identified with the Austrian economist Joseph Schumpeter who derived it from the work of Karl Marx and popularized it as a theory of economic innovation and the business cycle.\n\nAccording to Schumpeter, the \"gale of creative destruction\" describes the \"process of industrial mutation that incessantly revolutionizes the economic structure from within, incessantly destroying the old one, incessantly creating a new one\". In Marxian economic theory the concept refers more broadly to the linked processes of the accumulation and annihilation of wealth under capitalism.\n\nThe German Marxist sociologist Werner Sombart has been credited with the first use of these terms in his work \"Krieg und Kapitalismus\" (\"War and Capitalism\", 1913). In the earlier work of Marx, however, the idea of creative destruction or annihilation (German: \"Vernichtung\") implies not only that capitalism destroys and reconfigures previous economic orders, but also that it must ceaselessly devalue existing wealth (whether through war, dereliction, or regular and periodic economic crises) in order to clear the ground for the creation of new wealth.\n\nIn \"Capitalism, Socialism and Democracy\" (1942), Joseph Schumpeter developed the concept out of a careful reading of Marx’s thought (to which the whole of Part I of the book is devoted), arguing (in Part II) that the creative-destructive forces unleashed by capitalism would eventually lead to its demise as a system (see below). Despite this, the term subsequently gained popularity within neoliberal or free-market economics as a description of processes such as downsizing in order to increase the efficiency and dynamism of a company. The Marxian usage has, however, been retained and further developed in the work of social scientists such as David Harvey, Marshall Berman, Manuel Castells and Daniele Archibugi.\n\nAlthough the modern term \"creative destruction\" is not used explicitly by Marx, it is largely derived from his analyses, particularly in the work of Werner Sombart (whom Engels described as the only German professor who understood Marx's \"Capital\"), and of Joseph Schumpeter, who discussed at length the origin of the idea in Marx's work (see below).\n\nIn \"The Communist Manifesto\" of 1848, Karl Marx and Friedrich Engels described the crisis tendencies of capitalism in terms of \"the enforced destruction of a mass of productive forces\":\nModern bourgeois society, with its relations of production, of exchange and of property, a society that has conjured up such gigantic means of production and of exchange, is like the sorcerer who is no longer able to control the powers of the nether world whom he has called up by his spells. [...] It is enough to mention the commercial crises that by their periodical return put the existence of the whole of bourgeois society on trial, each time more threateningly. In these crises, \"a great part not only of existing production, but also of previously created productive forces, are periodically destroyed\". In these crises, there breaks out an epidemic that, in all earlier epochs, would have seemed an absurdity – the epidemic of over-production. Society suddenly finds itself put back into a state of momentary barbarism; it appears as if a famine, a universal war of devastation, had cut off the supply of every means of subsistence; industry and commerce seem to be destroyed; and why? Because there is too much civilisation, too much means of subsistence, too much industry, too much commerce. The productive forces at the disposal of society no longer tend to further the development of the conditions of bourgeois property; on the contrary, they have become too powerful for these conditions. […] And how does the bourgeoisie get over these crises? On the one hand by \"enforced destruction of a mass of productive forces\"; on the other, by the conquest of new markets, and by the more thorough exploitation of the old ones. That is to say, by paving the way for more extensive and more destructive crises, and by diminishing the means whereby crises are prevented.\nA few years later, in the \"Grundrisse\", Marx was writing of \"the violent destruction of capital not by relations external to it, but rather as a condition of its self-preservation\". In other words, he establishes a necessary link between the generative or creative forces of production in capitalism and the destruction of capital value as one of the key ways in which capitalism attempts to overcome its internal contradictions:\nThese contradictions lead to explosions, cataclysms, crises, in which [...] momentaneous suspension of labour and annihilation of a great portion of capital [...] violently lead it back to the point where it is enabled [to go on] fully employing its productive powers without committing suicide.\nIn the \"Theories of Surplus Value\" (\"Volume IV\" of \"Das Kapital\", 1863), Marx refines this theory to distinguish between scenarios where the destruction of (commodity) values affects either use values or exchange values or both together. The destruction of exchange value combined with the preservation of use value presents clear opportunities for new capital investment and hence for the repetition of the production-devaluation cycle:\nthe destruction of capital through crises means the depreciation of values which prevents them from later renewing their reproduction process as capital on the same scale. This is the ruinous effect of the fall in the prices of commodities. It does not cause the destruction of any use-values. What one loses, the other gains. Values used as capital are prevented from acting again as capital in the hands of the same person. The old capitalists go bankrupt. [...] A large part of the nominal capital of the society, i.e., of the exchange-value of the existing capital, is once for all destroyed, although this very destruction, since it does not affect the use-value, may very much expedite the new reproduction. This is also the period during which moneyed interest enriches itself at the cost of industrial interest.\nSocial geographer David Harvey sums up the differences between Marx's usage of these concepts and Schumpeter's: \"Both Karl Marx and Joseph Schumpeter wrote at length on the 'creative-destructive' tendencies inherent in capitalism. While Marx clearly admired capitalism's creativity he [...] strongly emphasised its self-destructiveness. The Schumpeterians have all along gloried in capitalism's endless creativity while treating the destructiveness as mostly a matter of the normal costs of doing business\".\n\nIn the Origin of Species, which was published in 1859, Charles Darwin wrote that the \"extinction of old forms is the almost inevitable consequence of the production of new forms.\" One notable exception to this rule is how the extinction of the dinosaurs facilitated the adaptive radiation of mammals. In this case creation was the consequence, rather than the cause, of destruction. \n\nIn philosophical terms, the concept of \"creative destruction\" is close to Hegel´s concept of sublation. In German economic discourse it was taken up from Marx's writings by Werner Sombart, particularly in his 1913 text \"Krieg und Kapitalismus\":\nAgain, however, \"from destruction a new spirit of creation arises;\" the scarcity of wood and the needs of everyday life... forced the discovery or invention of substitutes for wood, forced the use of coal for heating, forced the invention of coke for the production of iron.\nHugo Reinert has argued that Sombart's formulation of the concept was influenced by Eastern mysticism, specifically the image of the Hindu god Shiva, who is presented in the paradoxical aspect of simultaneous destroyer and creator. Conceivably this influence passed from Johann Gottfried Herder, who brought Hindu thought to German philosophy in his \"Philosophy of Human History\" (Ideen zur Philosophie der Geschichte der Menschheit) (Herder 1790–92), specifically volume III, pp. 41–64. via Arthur Schopenhauer and the Orientalist Friedrich Maier through Friedrich Nietzsche´s writings. Nietzsche represented the creative destruction of modernity through the mythical figure of Dionysus, a figure whom he saw as at one and the same time \"destructively creative\" and \"creatively destructive\". In the following passage from \"On the Genealogy of Morality\" (1887), Nietzsche argues for a universal principle of a cycle of creation and destruction, such that every creative act has its destructive consequence:\nBut have you ever asked yourselves sufficiently how much the erection of every ideal on earth has cost? How much reality has had to be misunderstood and slandered, how many lies have had to be sanctified, how many consciences disturbed, how much \"God\" sacrificed every time? If a temple is to be erected a temple must be destroyed: that is the law – let anyone who can show me a case in which it is not fulfilled! – Friedrich Nietzsche, \"On the Genealogy of Morality\"\n\nOther nineteenth-century formulations of this idea include Russian anarchist Mikhail Bakunin, who wrote in 1842, \"The passion for destruction is a creative passion, too!\" Note, however, that this earlier formulation might more accurately be termed \"destructive creation\", and differs sharply from Marx's and Schumpeter's formulations in its focus on the active destruction of the existing social and political order by human agents (as opposed to systemic forces or contradictions in the case of both Marx and Schumpeter).\n\nThe expression \"creative destruction\" was popularized by and is most associated with Joseph Schumpeter, particularly in his book \"Capitalism, Socialism and Democracy\", first published in 1942. Already in his 1939 book \"Business Cycles\", he attempted to refine the innovative ideas of Nikolai Kondratieff and his long-wave cycle which Schumpeter believed was driven by technological innovation. Three years later, in \"Capitalism, Socialism and Democracy\", Schumpeter introduced the term \"creative destruction\", which he explicitly derived from Marxist thought (analysed extensively in Part I of the book) and used it to describe the disruptive process of transformation that accompanies such innovation:\nCapitalism [...] is by nature a form or method of economic change and not only never is but never can be stationary. [...] The fundamental impulse that sets and keeps the capitalist engine in motion comes from the new consumers’ goods, the new methods of production or transportation, the new markets, the new forms of industrial organization that capitalist enterprise creates.[...] The opening up of new markets, foreign or domestic, and the organizational development from the craft shop and factory to such concerns as U.S. Steel illustrate the process of industrial mutation that incessantly revolutionizes the economic structure \"from within\", incessantly destroying the old one, incessantly creating a new one. This process of Creative Destruction is the essential fact about capitalism. It is what capitalism consists in and what every capitalist concern has got to live in.[... Capitalism requires] the perennial gale of Creative Destruction.\nIn Schumpeter's vision of capitalism, innovative entry by entrepreneurs was the disruptive force that sustained economic growth, even as it destroyed the value of established companies and laborers that enjoyed some degree of monopoly power derived from previous technological, organizational, regulatory, and economic paradigms. However, Schumpeter was pessimistic about the sustainability of this process, seeing it as leading eventually to the undermining of capitalism's own institutional frameworks:\nIn breaking down the pre-capitalist framework of society, capitalism thus broke not only barriers that impeded its progress but also flying buttresses that prevented its collapse. That process, impressive in its relentless necessity, was not merely a matter of removing institutional deadwood, but of removing partners of the capitalist stratum, symbiosis with whom was an essential element of the capitalist schema. [... T]he capitalist process in much the same way in which it destroyed the institutional framework of feudal society also undermines its own.\nSchumpeter nevertheless elaborated the concept, making it central to his economic theory, and it was later taken up as a major doctrine of the so-called Austrian School of free-market economic thought.\n\n Schumpeter (1949) in one of his examples used \"the railroadization of the Middle West as it was initiated by the Illinois Central.\" He wrote, \"The Illinois Central not only meant very good business whilst it was built and whilst new cities were built around it and land was cultivated, but it spelled the death sentence for the [old] agriculture of the West.\"\n\nCompanies that once revolutionized and dominated new industries – for example, Xerox in copiers or Polaroid in instant photography – have seen their profits fall and their dominance vanish as rivals launched improved designs or cut manufacturing costs. In technology, the cassette tape replaced the 8-track, only to be replaced in turn by the compact disc, which was undercut by downloads to MP3 players, which is now being usurped by web-based streaming services. Companies which made money out of technology which becomes obsolete do not necessarily adapt well to the business environment created by the new technologies.\n\nOne such example is the way in which online ad-supported news sites such as \"The Huffington Post\" are leading to creative destruction of the traditional newspaper. The \"Christian Science Monitor\" announced in January 2009 that it would no longer continue to publish a daily paper edition, but would be available online daily and provide a weekly print edition. The \"Seattle Post-Intelligencer\" became online-only in March 2009. At a national level in USA, employment in the newspaper business fell from 455,700 in 1990 to 225,100 in 2013. Over that same period, employment in internet publishing and broadcasting grew from 29,400 to 121,200. Traditional French alumni networks, which typically charge their students to network online or through paper directories, are in danger of creative destruction from free social networking sites such as Linkedin and Viadeo.\n\nIn fact, successful innovation is normally a source of temporary market power, eroding the profits and position of old firms, yet ultimately succumbing to the pressure of new inventions commercialised by competing entrants. Creative destruction is a powerful economic concept because it can explain many of the dynamics or kinetics of industrial change: the transition from a competitive to a monopolistic market, and back again. It has been the inspiration of endogenous growth theory and also of evolutionary economics.\n\nDavid Ames Wells (1890), who was a leading authority on the effects of technology on the economy in the late 19th century, gave many examples of creative destruction (without using the term) brought about by improvements in steam engine efficiency, shipping, the international telegraph network, and agricultural mechanization.\n\nGeographer and historian David Harvey in a series of works from the 1970s onwards (\"Social Justice and the City\", 1973; \"The Limits to Capital\", 1982; \"The Urbanization of Capital\", 1985; \"Spaces of Hope\", 2000; \"Spaces of Capital\", 2001; \"Spaces of Neoliberalization\", 2005; \"The Enigma of Capital and the Crises of Capitalism\", 2010), elaborated Marx's thought on the systemic contradictions of capitalism, particularly in relation to the production of the urban environment (and to the production of space more broadly). He developed the notion that capitalism finds a \"spatial fix\" for its periodic crises of overaccumulation through investment in fixed assets of infrastructure, buildings, etc.: \"The built environment that constitutes a vast field of collective means of production and consumption absorbs huge amounts of capital in both its construction and its maintenance. Urbanization is one way to absorb the capital surplus\". While the creation of the built environment can act as a form of crisis displacement, it can also constitute a limit in its own right, as it tends to freeze productive forces into a fixed spatial form. As capital cannot abide a limit to profitability, ever more frantic forms of \"time-space compression\" (increased speed of turnover, innovation of ever faster transport and communications' infrastructure, \"flexible accumulation\") ensue, often impelling technological innovation. Such innovation, however, is a double-edged sword:\n\nGlobalization can be viewed as some ultimate form of time-space compression, allowing capital investment to move almost instantaneously from one corner of the globe to another, devaluing fixed assets and laying off labour in one urban conglommeration while opening up new centres of manufacture in more profitable sites for production operations. Hence, in this continual process of creative destruction, capitalism does not resolve its contradictions and crises, but merely \"moves them around geographically\".\n\nIn his 1987 book \"All That is Solid Melts into Air: The Experience of Modernity\", particularly in the chapter entitled \"Innovative Self-Destruction\" (pp. 98–104), Marshall Berman provides a reading of Marxist \"creative destruction\" to explain key processes at work within modernity. The title of the book is taken from a well-known passage from \"The Communist Manifesto\". Berman elaborates this into something of a \"Zeitgeist\" which has profound social and cultural consequences:\n\nHere Berman emphasizes Marx's perception of the fragility and evanescence of capitalism's immense creative forces, and makes this apparent contradiction into one of the key explanatory figures of modernity.\n\nThe sociologist Manuel Castells, in his trilogy on \"\" (the first volume of which, \"\", appeared in 1996), reinterpreted the processes by which capitalism invests in certain regions of the globe, while divesting from others, using the new paradigm of \"informational networks\". In the era of globalization, capitalism is characterized by near-instantaneous flow, creating a new spatial dimension, \"the space of flows\". While technological innovation has enabled this unprecedented fluidity, this very process makes redundant whole areas and populations who are bypassed by informational networks. Indeed, the new spatial form of the mega-city or megalopolis, is defined by Castells as having the contradictory quality of being \"globally connected and locally disconnected, physically and socially\". Castells explicitly links these arguments to the notion of creative destruction:\n\nDeveloping the Schumpeterian legacy, the school of the Science Policy Research Unit of the University of Sussex has further detailed the importance of creative destruction exploring, in particular, how new technologies are often idiosyncratic with the existing productive regimes and will lead to bankruptcy companies and even industries that do not manage to sustain the rate of change. Chris Freeman and Carlota Perez have developed these insights. More recently, Daniele Archibugi and Andrea Filippetti have associated the 2008 economic crisis to the slow-down of opportunities offered by information and communication technologies (ICTs). Using as a metaphor the film \"Blade Runner\", Archibugi has argued that of the innovations described in the film in 1982, all those associated to ICTs have become part of our everyday life. But, on the contrary, none of those in the field of Biotech have been fully commercialized. A new economic recovery will occur when some key technological opportunities will be identified and sustained.\n\nIn 1992, the idea of creative destruction was put into formal mathematical terms by Philippe Aghion and Peter Howitt, giving an alternative model of endogenous growth compared to Paul Romer's expanding varieties model.\n\nIn 1995, Harvard Business School authors Richard L. Nolan and David C. Croson released \"Creative Destruction: A Six-Stage Process for Transforming the Organization.\" The book advocated downsizing to free up slack resources, which could then be reinvested to create competitive advantage.\n\nMore recently, the idea of \"creative destruction\" was utilized by Max Page in his 1999 book, \"The Creative Destruction of Manhattan, 1900–1940.\" The book traces Manhattan's constant reinvention, often at the expense of preserving a concrete past. Describing this process as \"creative destruction,\" Page describes the complex historical circumstances, economics, social conditions and personalities that have produced crucial changes in Manhattan's cityscape.\n\nIn addition to Max Page, others have used the term “creative destruction” to describe the process of urban renewal and modernization. T.C. Chang and Shirlena Huang referenced “creative destruction” in their paper \"Recreating place, replacing memory: Creative Destruction at the Singapore River.\" The authors explored the efforts to redevelop a waterfront area that reflected a vibrant new culture while paying sufficient homage to the history of the region. Rosemary Wakeman chronicled the evolution of an area in central Paris, France known as Les Halles. Les Halles housed a vibrant marketplace starting in the twelfth century. Ultimately, in 1971, the markets were relocated and the pavilions torn down. In their place, now stand a hub for trains, subways and buses. Les Halles is also the site of the largest shopping mall in France and the controversial Centre Georges Pompidou.\n\nThe term “creative destruction” has been applied to the arts. Alan Ackerman and Martin Puncher (2006) edited a collection of essays under the title \"Against Theater: Creative destruction on the modernist stage.\" They detail the changes and the causal motivations experienced in theater as a result of the modernization of both the production of performances and the underlying economics. They speak of how theater has reinvented itself in the face of anti-theatricality, straining the boundaries of the traditional to include more physical productions, which might be considered avant-garde staging techniques.\n\nIn his 1999 book, \"Still the New World, American Literature in a Culture of Creative Destruction\", Philip Fisher analyzes the themes of creative destruction at play in literary works of the twentieth century, including the works of such authors as Ralph Waldo Emerson, Walt Whitman, Herman Melville, Mark Twain, and Henry James, among others. Fisher argues that creative destruction exists within literary forms just as it does within the changing of technology.\n\nNeoconservative author Michael Ledeen argued in his 2002 book \"The War Against the Terror Masters\" that America is a revolutionary nation, undoing traditional societies: \"Creative destruction is our middle name, both within our own society and abroad. We tear down the old order every day, from business to science, literature, art, architecture, and cinema to politics and the law.\" His characterization of creative destruction as a model for social development has met with fierce opposition from paleoconservatives.\n\nCreative destruction has also been linked to sustainable development. The connection was explicitly mentioned for the first time by Stuart L. Hart and Mark B. Milstein in their 1999 article \"Global Sustainability and the Creative Destruction of Industries\", in which he argues new profit opportunities lie in a round of creative destruction driven by global sustainability. (An argument which they would later on strengthen in their 2003 article \"Creating Sustainable Value\" and, in 2005, with \"Innovation, Creative Destruction and Sustainability\".) Andrea L. Larson agreed with this vision a year later in \"Sustainable Innovation Through an Entrepreneurship Lens\", stating entrepreneurs should be open to the opportunities for disruptive improvement based on sustainability. In 2005, James Hartshorn (et al.) emphasized the opportunities for sustainable, disruptive improvement in the construction industry in his article \"Creative Destruction: Building Toward Sustainability\".\n\nThe following text appears to be the source of the phrase \"Schumpeter's Gale\" to refer to creative destruction:\n\nThe film \"Other People's Money\" (1991) provides contrasting views of creative destruction, presented in two speeches regarding the takeover of a publicly traded wire and cable company in a small New England town. One speech is by a corporate raider, and the other is given by the company CEO, who is principally interested in protecting his employees and the town.\n\n\n"}
{"id": "28244922", "url": "https://en.wikipedia.org/wiki?curid=28244922", "title": "Creative technology", "text": "Creative technology\n\nCreative technology is a broadly interdisciplinary and transdisciplinary field combining computing, design, art and the humanities. The field of creative technology encompasses art, digital product design, digital media or an advertising and media made with a software-based, electronic and/or data-driven engine. Examples of creative technology include multi-sensory experiences made using computer graphics, video production, digital cinematography, virtual reality, augmented reality, video editing, software engineering, 3D printing, the Internet of Things, CAD/CAM and wearable technology. In the art world, new media art and internet art are examples of work being done in the creative technology field. Performances, interactive installations and other immersive experiences take museum-going to the next level and may serve as research processes for humans' artistic and emotional integration with machines. Some believe that \"creativity has the potential to be revolutionised with technology\", or view the field of creative technology as helping to \"disrupt\" the way people today interact with computers, and usher in a more integrated, immersive experience.\n\nCreative technology facilities may be organized as arts, research or job development entities, such as the UK's Foundation for Art and Creative Technology which has presented hundreds of new media and digital artworks from around the world, or a recently established $20.5m project in Hawaii specializing in film industry job training and workforce development programs which plans to offer robotics, computer labs, recording studios and editing bays, pitched as a \"game-changing\" opportunity to bring new skills and jobs to Kauai. Degrees in this field were designed to address needs for cross-disciplinary interaction and aim to develop lateral thinking skills across more rigidly defined academic areas. Some educators have complained that creative technology tools, though \"widely available\", are difficult to use for young populations. \n\nThe first major corporation to have a corporate officer bearing the title creative technology was The Walt Disney Company, which gave it first to the imagineer, Bran Ferren in 1993, who eventually became Disney's president of creative technology in 1998. At about the same time, the first educational research center in the United States was created to bridge these disciplines across industry, academia and the defense communities, designated the University of Southern California's, Institute for Creative Technologies. The ICT was established with funding by the US Army.\n\nMarketers and advertisers are also looking toward the power of creative technology to re-engage customers. The UK's Marketing Agencies Association is promoting creative technology as a way to build a more connected and personalized engagement with prospective customers, which launched a Creative Technology Initiative in early 2015. Industry associations and developers, arts organizations and agency creatives alike call for more investment in technology, which has lagged behind the sea change in the industry that is introducing more technology into creative fields. Many advertising agencies and other businesses have begun to create internal labs for research in creative technology. For example, Unilever created its Foundry Project as a way for the company to \"embrace the mentality of hacking, deploying and scaling\"; they share their discoveries and view the lab as a way to incorporate technology into the company, drive experimentation and engage with strategic partners. The Adobe Creative Technologies lab collaborated with the MIT Media Lab, one of the most notable endeavors in the creative technology field, to give artists the ability to draw geometric designs with a computer without having to master text-based programming or math.\n\nCreative technology is best seen as the intersection of new technology with creative initiatives such as fashion, art, advertising, media and entertainment. As such, it is a way to make connections between countries seeking to update their culture; a winter 2015 \"Forbes\" article tells of 30 creative technology startups from the UK making the rounds in Singapore, Kuala Lampur and New York City in an effort to raise funds and make connections.\n\n\nProfessionals who work in the field of creative technology tend to have a background as developers and may work in digital or entertainment media, with an advertising agency or in a new electronic product development role. In an advertising agency setting, a professional with a job description including creative technology may be a designer who became interested in technology, or a developer who focuses on the bigger picture of experience design. Department heads in creative technology may be charged with integrating new technologies into the agency's departments, leveraging partnerships with cutting edge providers and platforms. For example, the head of creative technology at Grey Global Group in New York \"created an in-house lab... which highlights new tech each month with exhibits, events and workshops.\" Members of the team may have the ability to both write computer code and build electronics for prototypes.\n\nThe creative technologist job title is likely to refer to a developer who understands the creative process and the world of advertising. The person is actually making and coding and may be building web projects, mobile apps and other digital experiences. They are trying out new concepts and ideas, and modifying; this is recognized as similar to the artistic process but applied to media, advertising and other creative industries. Creative technologists have been referred to as technology-focussed individuals who either sit within or work closely with the creative team, recognizing that siloed departments of technology and design have historically led to bad agency work. Responsibilities described in a 2014 job posting for \"Creative Technologist\" at Google included \"collaborating on the ideation and development of 'never been done before' digital experiences in partnership with top brands and agencies\", and \"contributing to the development of cutting edge prototypes in the field of creative technology\".\n\nA Master or Bachelor degree in creative technologies or creative technology is a broadly interdisciplinary and transdisciplinary course of study combining fields of computer technology, design, art and the humanities. Established as a modern degree addressing needs for cross-disciplinary interaction, one of its fundamental objectives is to develop lateral thinking skills across more rigidly defined academic areas recognized as a valuable component in expanding technological horizons. The Creative Technology & Design (CT&D) subject area at Fashion Institute of Technology offers specialized courses and both credit and non-credit programs. According to FIT's web site, the mission of this transdisciplinary subject area is to elevate students’ understanding of advanced design concepts, as well as their command of cutting-edge technologies. The Creative Technology 2-year portfolio program at Miami Ad School description reads, \"You are a techie with creative passion and talent – or – a creative with a knack for tech...It's about how we integrate machine learning and artificial intelligence into a creative environment\". Creative Technology is also seen as an industry and skill set for the emerging economy, as in this quote by a University of Texas at Austin dean at the opening of a new school at the University presumed to become the largest academic unit in the college: \"The School of Design and Creative Technologies moves UT Austin more assertively into emerging creative, commercial disciplines that are driving culture and economies in the 21st century\".\n\n\n"}
{"id": "889576", "url": "https://en.wikipedia.org/wiki?curid=889576", "title": "Curb weight", "text": "Curb weight\n\nCurb weight (American English) or kerb weight (British English) is the total weight of a vehicle with standard equipment and hardpoints, all necessary operating consumables such as motor oil, transmission oil, coolant, air conditioning refrigerant, and sometimes a full tank of fuel, while not loaded with either passengers, cargo, or weaponry.\n\nThis definition may differ from definitions used by governmental regulatory agencies or other organizations. For example, many European Union manufacturers include the weight of a driver to follow European Directive 95/48/EC. Organizations may also define curb weight with fixed levels of fuel and other variables to equalize the value for the comparison of different vehicles.\n\nThe United States Environmental Protection Agency regulations define curb weight as follows: Curb weight means the actual or the manufacturer’s estimated weight of the vehicle in operational status with all standard equipment, and weight of fuel at nominal tank capacity, and the weight of optional equipment computed in accordance with §86.1832–01; incomplete light-duty trucks shall have the curb weight specified by the manufacturer.\n\nUnladen mass depends on the manufacturer and can be the same as curb weight, however, it is often the total mass of the car without a driver, fluid or any additional equipment.\n\n"}
{"id": "451964", "url": "https://en.wikipedia.org/wiki?curid=451964", "title": "Defect tracking", "text": "Defect tracking\n\nIn engineering, defect tracking is the process of tracking the logged defects in a product from beginning to closure (by inspection, testing, or recording feedback from customers), and making new versions of the product that fix the defects. Defect tracking is important in software engineering as complex software systems typically have tens or hundreds or thousands of defects: managing, evaluating and prioritizing these defects is a difficult task. When the numbers of defects gets quite large, and the defects need to be tracked over extended periods of time, use of a defect tracking system can make the management task much easier.\n\n"}
{"id": "10458499", "url": "https://en.wikipedia.org/wiki?curid=10458499", "title": "Democratic rationalization", "text": "Democratic rationalization\n\nDemocratic rationalization is term used by Andrew Feenberg in his article \"Subversive Rationalization: Technology, Power and Democracy with technology.\" Feenberg argues against the idea of \"technological determinism\" citing flaws in its two fundamental theses.\n\nThe first is the \"thesis of unilinear progress\". This is the belief that technological progress follows a direct and predictable path from lower to higher levels of complexity and that each stage along this path is necessary for progress to occur (Feenberg 211).\n\nThe second is the \"thesis of determination by the base\". This is the concept that in a society where a technology had been introduced, that society must organize itself or adapt to the technology (Feenberg 211).\n\nIn his argument against the former thesis Feenberg says that constructivist studies of technology will lead us to realize that there is not a set path by which development of technologies occur but rather an emerging of similar technologies at the same time leading to a multiplicity of choices. These choices are made based upon certain social factors and upon examining them we will see that they are not deterministic in nature (Feenberg 212).\n\nArguing against the latter thesis, Feenberg calls to our attention social reforms that have been mandated by governments mainly in regards to the protection of its citizens and laborers. Most of the time these mandates are widely accepted after being passed through the governing body. At which point technology and industry will reform and re-evolve to meet the new standards in a way that has greater efficiency than it did so previously (Feenberg 214)\n\n\n"}
{"id": "38468351", "url": "https://en.wikipedia.org/wiki?curid=38468351", "title": "Democratization of technology", "text": "Democratization of technology\n\nDemocratization of technology refers to the process by which access to technology rapidly continues to become more accessible to more people. New technologies and improved user experiences have empowered those outside of the technical industry to access and use technological products and services. At an increasing scale, consumers have greater access to use and purchase technologically sophisticated products, as well as to participate meaningfully in the development of these products. Industry innovation and user demand have been associated with more affordable, user-friendly products. This is an ongoing process, beginning with the development of mass production and increasing dramatically as digitization became commonplace.\n\nThomas Friedman argued that the era of globalization has been characterized by the democratization of technology, democratization of finance, and democratization of information. Technology has been critical in the latter two processes, facilitating the rapid expansion of access to specialized knowledge and tools, as well as changing the way that people view and demand such access.\n\nScholars and social critics often cite the invention of the printing press as a major invention that changed the course of history. The force of the printing press rested not in its impact on the printing industry or inventors, but on its ability to transmit information to a broader public by way of mass production. This event is so widely recognized because of its social impact – as a democratizing force.\n\nThe printing press is often seen as the historical counterpart to the Internet.\n\nAfter the development of the Internet in 1969, its use remained limited to communications between scientists and within government, although use of email and boards gained popularity among those with access. It did not become a popular means of communication until the 1990s. In 1993 the US federal government opened the Internet to commerce and the creation of HTML formed the basis for universal accessibility.\n\nThe Internet has played a critical role in modern life as a typical feature of most Western households, and has been key in the democratization of knowledge. It not only constitutes arguably the most critical innovation in this trend thus far; it has also allowed users to gain knowledge of and access to other technologies. Users can learn of new developments more quickly, and purchase high-tech products otherwise only actively marketed to recognized experts. Some have argued that cloud computing is having a major effect by allowing users greater access through mobility and pay-as-you-use capacity.\n\nSocial media has also empowered and emboldened users to become contributors and critics of technological developments.\n\nThe open-source model allows users to participate directly in development of software, rather than indirect participation, through contributing opinions. By being shaped by the user, development is directly responsive to user demand and can be obtained for free or at a low cost. In a comparable trend, arduino and littleBits have made electronics more accessible to users of all backgrounds and ages. The development of 3D printers has the potential to increasingly democratize production.\n\nThis trend is linked to the spread of knowledge of and ability to perform high-tech tasks, challenging previous conceptions of expertise.\n\nWidespread access to technology, including lower costs, was critical to the transition to the new economy. Similarly, democratization of technology was also fuelled by this economic transition, which produced demands for technological innovation and optimism in technology-driven progress.\n\nSince the 1980s, a spreading constructivist conception of technology has emphasized that the social and technical domains are critically intertwined. Scholars have argued that technology is non-neutral, defined contextually and locally by a certain relationship with society.\n\nAndrew Feenberg, a central thinker in the philosophy of technology, argued that democratizing technology means expanding technological design to include alternative interests and values. When successful in doing so, this can be a tool for increasing inclusiveness. This also suggests an important participatory role for consumers if technology is to be truly democratic. Feenberg asserts that this must be achieved by consumer intervention in a liberated design process.\n\nImproved access to specialized knowledge and tools has been associated with an increase in the \"do it yourself\" (DIY) trend. This has also been associated with consumerization, whereby personal or privately owned devices and software are also used for business purposes. Some have argued that this is linked to reduced dependence on traditional information technology departments.\n\nAstra Taylor, the author of the book \"The People's Platform: Taking Back Power and Culture in the Digital Age\", argues, \"The promotion of Internet-enabled amateurism is a lazy substitute for real equality of opportunity.\"\n\nIn some ways, democratization of technology has strengthened this industry. Markets have broadened and diversified. Consumer feedback and input is available at a very low or no cost.\n\nHowever, related industries are experiencing decreased demand for qualified professionals as consumers are able to fill more of their demands themselves. Users of a range of types and status have access to increasingly similar technology. Because of the decreased costs and expertise necessary to use products and software, professionals (e.g. in the audio industry) may experience loss of work.\n\nIn some cases, technology is accessible but sufficiently complex that most users without specialized training are able to operate it without necessarily understanding how it works. Additionally, the process of consumerization has led to an influx in the number of devices in businesses and accessing private networks that IT departments cannot control or access. While this can lead to lowered operating costs and increased innovation, it is also associated with security concerns that most businesses are unable to address at the pace of the spread of technology.\n\nSome scholars have argued that technological change will bring about a third wave of democracy. The Internet has been recognized for its role in promoting increased citizen advocacy and government transparency. Jesse Chen, a leading thinker in democratic engagement technologies, distinguishes the democratizing effects of technology from democracy itself. Chen has argued that, while the Internet may have democratizing effects, the Internet alone cannot deliver democracy at all levels of society unless technologies are purposely designed for the nuances of democracy, specifically the engagement of large groups of people in between elections in and beyond government.\n\nThe spread of the Internet and other forms of technology has led to increased global connectivity. Many scholars believe that it has been associated in the developing world not only with increased Western influence, but also with the spread of democracy through increased communication, efficiency, and access to information. Scholars have drawn associations between the level of technological connectedness and democracy in many nations.\n\nTechnology can enhance democracy in the developed world as well. In addition to increased communication and transparency, some electorates have implemented online voting to accommodate an increased number of citizens.\n\n"}
{"id": "41368073", "url": "https://en.wikipedia.org/wiki?curid=41368073", "title": "Department of Science (1972–75)", "text": "Department of Science (1972–75)\n\nThe Department of Science was an Australian government department that existed between December 1972 and June 1975.\n\nInformation about the department's functions and/or government funding allocation could be found in the Administrative Arrangements Orders, the annual Portfolio Budget Statements and in the Department's annual reports.\n\nAccording to the Administrative Arrangements Order issued 19 December 1972, at its creation, the Department was responsible for:\n\nThe Department was an Australian Public Service department, staffed by officials who were responsible to the Minister for Science.\n\nThe Secretary of the Department was Hugh Ennor.\n"}
{"id": "18672045", "url": "https://en.wikipedia.org/wiki?curid=18672045", "title": "Digital component video", "text": "Digital component video\n\nDigital component video is defined by ITU-R BT.601 (formerly CCIR 601) standard and uses the Y'CbCr colorspace. Like Analog Component Video it gets its name from the fact that the video signal has been split into two or more components, that are then carried on multiple conductors between devices. Digital component video is slowly becoming popular in both computer and home-theatre applications. Component video is capable of carrying signals such as 480i, 480p, 576i, 576p, 720p, 1080i and 1080p, although many TVs do not support 1080p through component video.\n\n"}
{"id": "26672892", "url": "https://en.wikipedia.org/wiki?curid=26672892", "title": "ET3 Global Alliance", "text": "ET3 Global Alliance\n\nET3 Global Alliance is an American open consortium of licensees dedicated to global implementation of Evacuated Tube Transport Technologies (ET3). It was founded by Daryl Oster in 1997 with the goal of establishing a global transportation system utilizing car-sized cargo and passenger capsules traveling in 1.5m diameter tubes via frictionless superconductive maglev.\n\nOster claims that the ET3 system will be able to provide 50 times the amount of transportation per kilowatt-hour compared with electric cars and electric trains, costing only 20 cents' worth of electrical energy to get up to . ET3 claims that initial systems would travel at the speed of for in state trips, and later will be developed to 6,500 km/h (4,000 mph, hypersonic speed) for international travel that will allow passenger or cargo travel from New York to Beijing in 2 hours. The initial proof of concept system could be built in as little 3 years for operational transport.\n\nThe first patent issued in the field of evacuated tube transport was credited to Daryl Oster as inventor while the original assigner was issued to Et3.com Inc on Sept 14, 1999. ET3 has been active since 1999. But it was two years prior, on Oct 10 1997, when Daryl Oster filed the 1999 patent relating to evacuated tube transport. In 2001, Southwest Jiaotong University (SWJTU) invited Daryl to China to advance the field of Evacuated Tube Transport (ETT) and to discuss adopting High Temperature Superconducting Maglev (HTSM) as the levitation system for ETT.\n\nIn 2004, Daryl Oster had published \"A New Industrial Era Coming Initial Dialogue on Evacuated Tube Transport\".\n\nYaoping Zhang of SWJTU and Daryl Oster shared year long communications during the early 2000s via email which can be read in his 2004 published book. SWJTU became the first university institution to become licensees of the ET3 GA consortium. The most ET3 licensees held outside of the USA are held in China. SWJTU and individuals from China have contributed significant IP to the ET3 consortia. By 2007, Yaoping Zhang, a former professor of SWJTU, began promoting ETT as \"evolutionary transportation\". Yaoping Zhang currently operates ET3 GA's subsidiary ET3 China Inc.\n\nET3 has filed a series of new patents in 2014 relating to the field of high-temperature superconductivity (HTS). As of 2016, more than 380 licenses have been sold in 22 different countries, including China, where ET3 claims that more than a dozen licenses have been sold. Daryl Oster and his team met with Tesla Motors/SpaceX CEO Elon Musk in late July, 2013, to discuss the technology, resulting in Musk promising an investment in a prototype of ET3's design.\n\nIn March 2018, a ET3 paper entitled \"closing the infrastructure gap through innovative and sustainable solutions\" was published.\n\n\nPatent application (priority date 2013-03-14): Evacuated tube transport system with interchange capability - A High Temperature Superconductor Maglev (HTSM) for Evacuated Tube Transport (ETT) with a magnetic levitation structure for ETT capsule vehicles traveling in an evacuated tube.\nPatent application (priority date 2013-03-14): Evacuated tube and capsule having interchange capability - The method selectively energizes the force elements to enable the capsule to diverge or converge in an interchange. \n"}
{"id": "13433476", "url": "https://en.wikipedia.org/wiki?curid=13433476", "title": "Edholm's law", "text": "Edholm's law\n\nEdholm's law, named for Phil Edholm, says the wireless, nomadic and wired network capabilities will converge. Soon, even slower communications channels like cellphones and radio modems will eclipse the capacity of early Ethernet, thanks to upcoming standards known as UMTS and MIMO, which will boost bandwidth by maximizing antenna usage.\n\nExtrapolating forward indicates a convergence between the rates of nomadic and wireless technologies around 2030.\n\n"}
{"id": "7363430", "url": "https://en.wikipedia.org/wiki?curid=7363430", "title": "Electronic authentication", "text": "Electronic authentication\n\nElectronic authentication is the process of establishing confidence in user identities electronically presented to an information system. Digital authentication or e-authentication may be used synonymously when referring to the authentication process that confirms or certifies a person's identity and works. \nWhen used in conjunction with an electronic signature, it can provide evidence whether data received has been tampered with after being signed by its original sender. In a time where fraud and identity theft has become rampant, electronic authentication can be a more secure method of verifying that a person is who they say they are when performing transactions online.\n\nThere are various e-authentication methods that can be used to authenticate a user's identify ranging from a password to higher levels of security that utilize multifactor authentication (MFA). Depending on the level of security used, the user might need to prove his or her identity through the use of security tokens, challenge questions or being in possession of a certificate from a third-party certificate authority that attests to their identity.\n\n The American National Institute of Standards and Technology (NIST) has developed a generic electronic authentication model that provides a basic framework on how the authentication process is accomplished regardless of jurisdiction or geographic region. According to this model, the enrollment process begins with an individual applying to a Credential Service Provider (CSP). The CSP will need to prove the applicant's identity before proceeding with the transaction. Once the applicant's identity has been confirmed by the CSP, he or she receives the status of \"subscriber\", is given an authenticator, such as a token and a credential, which may be in the form of a username.\n\nThe CSP is responsible for managing the credential along with the subscriber's enrollment data for the life of the credential. The subscriber will be tasked with maintaining the authenticators. An example of this is when a user normally uses a specific computer to do their online banking. If he or she attempts to access their bank account from another computer, the authenticator will not be present. In order to gain access, the subscriber would need to verify their identity to the CSP, which might be in the form of answering a challenge question successfully before being given access.\n\nThe need for authentication has been prevalent throughout history. In ancient times, people would identify each other through eye contact and physical appearance. The Sumerians in ancient Mesopotamia attested to the authenticity of their writings by using seals embellished with identifying symbols. As time moved on, the most common way to provide authentication would be the handwritten signature.\n\nThere are three generally accepted factors that are used to establish a digital identity for electronic authentication, including:\nOut of the three factors, the biometric factor is the most convenient and convincing to prove an individual's identity. However, having to rely on this sole factor can be expensive to sustain. Although having their own unique weaknesses, by combining two or more factors allows for reliable authentication. It is always recommended to use multifactor authentication for that reason.\n\nAuthentication systems are often categorized by the number of factors that they incorporate. The three factors often considered as the cornerstone of authentication are:\nSomething you know (for example, a password)\nSomething you have (for example, an ID badge or a cryptographic key)\nSomething you are (for example, a voice print, thumb print or other biometric)\n\nMultifactor authentication is generally more secure than single-factor authentication. But, some multi-factor authentication approaches are still vulnerable to cases like man-in-the-middle attacks and Trojan attacks. Common methods used in authentication systems are summarized below.\n\nTokens generically are something the claimant possesses and controls that may be used to authenticate the claimant's identity. In e-authentication, the claimant authenticates to a system or application over a network. Therefore, a token used for e-authentication is a secret and the token must be protected. The token may, for example, be a cryptographic key, that is protected by encrypting it under a password. An impostor must steal the encrypted key and learn the password to use the token.\n\nPasswords and PINs are categorized as \"something you know\" method. A combination of numbers, symbols, and mixed cases are considered to be stronger than all-letter password. Also, the adoption of Transport Layer Security (TLS) or Secure Socket Layer (SSL) features during the information transmission process will as well create an encrypted channel for data exchange and to further protect information delivered. Currently, most security attacks target on password-based authentication systems.\n\nThis type of authentication has two parts. One is a public key, the other is a private key. A public key is issued by a Certification Authority and is available to any user or server. A private key is known by the user only.\n\nThe user shares a unique key with an authentication server. When the user sends a randomly generated message (the challenge) encrypted by the secret key to the authentication server, if the message can be matched by the server using its shared secret key, the user is authenticated.\nWhen implemented together with the password authentication, this method also provides a possible solution for two-factor authentication systems.\n\nThe user receives password by reading the message in the cell phone, and types back the password to complete the authentication. Short Message Service (SMS) is very effective when cell phones are commonly adopted. SMS is also suitable against man-in-the-middle (MITM) attacks, since the use of SMS does not involve the Internet.\n\nBiometric authentication is the use of unique physical attributes and body measurements as the intermediate for better identification and access control. Physical characteristics that are often used for authentication include fingerprints, voice recognition, face, recognition, and iris scans because all of these are unique to every individual separately. Traditionally, Biometric Authentication based on token-based identification systems, such as passport, and nowadays becomes one of the most secure identification systems to user protections. A new technological innovation which provides a wide variety of either behavioral or physical characteristics which are defining the proper concept of Biometric Authentication.\n\nDigital identity authentication refers to the combined use of device, behavior, location and other data, including email address, account and credit card information, to authenticate online users in real time.\n\nPaper credentials are documents that attest to the identity or other attributes of an individual or entity called the subject of the credentials. Some common paper credentials include passports, birth certificates, driver's licenses, and employee identity cards. The credentials themselves are authenticated in a variety of ways: traditionally perhaps by a signature or a seal, special papers and inks, high quality engraving, and today by more complex mechanisms, such as holograms, that make the credentials recognizable and difficult to copy or forge. In some cases, simple possession of the credentials is sufficient to establish that the physical holder of the credentials is indeed the subject of the credentials. More commonly, the credentials contain biometric information such as the subject's description, a picture of the subject or the handwritten signature of the subject that can be used to authenticate that the holder of the credentials is indeed the subject of the credentials. When these paper credentials are presented in-person, authentication biometrics contained in those credentials can be checked to confirm that the physical holder of the credential is the subject.\n\nElectronic identity credentials bind a name and perhaps other attributes to a token. There are a variety of electronic credential types in use today, and new types of credentials are constantly being created (eID, electronic voter ID card, biometric passports, bank cards, etc.) At a minimum, credentials include identifying information that permits recovery of the records of the registration associated with the credentials and a name that is associated with the subscriber.\n\nIn any authenticated on-line transaction, the verifier is the party that verifies that the claimant has possession and control of the token that verifies his or her identity. A claimant authenticates his or her identity to a verifier by the use of a token and an authentication protocol. This is called Proof of Possession (PoP). Many PoP protocols are designed so that a verifier, with no knowledge of the token before the authentication protocol run, learns nothing about the token from the run. The verifier and CSP may be the same entity, the verifier and relying party may be the same entity or they may all three be separate entities. It is undesirable for verifiers to learn shared secrets unless they are a part of the same entity as the CSP that registered the tokens. Where the verifier and the relying party are separate entities, the verifier must convey the result of the authentication protocol to the relying party. The object created by the verifier to convey this result is called an assertion.\n\nThere are four types of authentication schemes: local authentication, centralized authentication, global centralized authentication, global authentication and web application (portal).\n\nWhen using a local authentication scheme, the application retains the data that pertains to the user's credentials. This information is not usually shared with other applications. The onus is on the user to maintain and remember the types and number of credentials that are associated with the service in which they need to access. This is a high risk scheme because of the possibility that the storage area for passwords might become compromised.\n\nUsing the central authentication scheme allows for each user to use the same credentials to access various services. Each application is different and must be designed with interfaces and the ability to interact with a central system to successfully provide authentication for the user. This allows the user to access important information and be able to access private keys that will allow he or she to electronically sign documents.\n\nUsing a third party through a global centralized authentication scheme allows the user direct access to authentication services. This then allows the user to access the particular services they need.\n\nThe most secure scheme is the global centralized authentication and web application (portal). It is ideal for E-Government use because it allows a wide range of services. It uses a single authentication mechanism involving a minimum of two factors to allow access to required services and the ability to sign documents.\n\nOften, authentication and digital signing are applied in conjunction. In advanced electronic signatures, the signatory has authenticated and uniquely linked to a signature. In the case of a qualified electronic signature as defined in the eIDAS-regulation, the signer's identity is even certified by a qualified trust service provider. This linking of signature and authentication firstly supports the probative value of the signature – commonly referred to as non-repudiation of origin. The protection of the message on the network-level is called non-repudiation of emission. The authenticated sender and the message content are linked to each other. If a 3rd party tries to change the message content, the signature loses validity.\n\nBiometric authentication can be defined by many different procedures and sensors which are being used to produce security. Biometric can be separated into physical or behavioral security. Physical protection is based on identification through fingerprint, face, hand, iris, etc. On the other hand, behavioral safety is succeeded by keystroke, signature, and voice. The main point is that all of these different procedures and mechanism that exist, produce the same homogeneous result, namely the security of the system and users. When thinking of the decoupling of hardware, the hardware is not coded in the same form by digitization which directly makes decoupling more difficult. Because of unlinkability and irreversibility of biometric templates, this technology can secure user authentication.\n\nBiometric authentication has a substantial impact on digital traces. For example when the user decides to use his fingerprint to protect his data on his smartphone, then the system memorizes the input so it will be able to be re-used again. During this procedure and many other similar applications proves that the digital trace is vital and exist on biometric authentication.\n\nAnother characteristic of biometric authentication is that it combines different components such as security tokens with computer systems to protect the user. Another example is the connection between devices, such as camera and computer systems to scan the user’s retina and produce new ways of security. So biometric authentication could be defined by connectivity as long it connects different applications or components and through these users are getting connected and can work under the same roof and especially on a safe environment on the cyber world.\n\nAs new kinds of cybercrime are appearing, the ways of authentication must be able to adapt. This adaptation means that it is always ready for evolution and updating, and so it will be able to protect the users at any time. At first biometric authentication started in the sampler form of user’s access and defining user profiles and policies. Over time the need of biometric authentication became more complex, so cybersecurity organizations started reprogramming their products/technology from simple personal user’s access to allow interoperability of identities across multiple solutions. Through this evolution, business value also rises.\n\nWhen developing electronic systems, there are some industry standards requiring United States agencies to ensure the transactions provide an appropriate level of assurance. Generally, servers adopt the US' Office of Management and Budget's (OMB's) E-Authentication Guidance for Federal Agencies (M-04-04) as a guideline, which is published to help federal agencies provide secure electronic services that protect individual privacy. It asks agencies to check whether their transactions require e-authentication, and determine a proper level of assurance.\n\nIt established four levels of assurance:\n\nAssurance Level 1: Little or no confidence in the asserted identity's validity.\nAssurance Level 2: Some confidence in the asserted identity's validity. \nAssurance Level 3: High confidence in the asserted identity's validity. \nAssurance Level 4: Very high confidence in the asserted identity's validity.\n\nThe OMB proposes a five-step process to determine the appropriate assurance level for their applications:\n\nThe required level of authentication assurance are assessed through the factors below:\n\nNational Institute of Standards and Technology (NIST) guidance defines technical requirements for each of the four levels of assurance in the following areas:\n\nTriggered by the growth of new cloud solutions and online transactions, person-to-machine and machine-to-machine identities play a significant role in identifying individuals and accessing information. According to the Office of Management and Budget in the U.S, more than $70 million was spent on identity management solutions in both 2013 and 2014.\n\nGovernments use e-authentication systems to offer services and reduce time people traveling to a government office. Services ranging from applying for visas to renewing driver's licenses can all be achieved in a more efficient and flexible way. Infrastructure to support e-authentication is regarded as an important component in successful e-government. Poor coordination and poor technical design might be major barriers to electronic authentication.\n\nIn several countries there has been established nationwide common e-authentication schemes to ease the reuse of digital identities in different electronic services. Other policy initiatives have included the creation of frameworks for electronic authentication, in order to establish common levels of trust and possibly interoperability between different authentication schemes.\n\nE-authentication is a centerpiece of the United States government's effort to expand electronic government, or e-government, as a way of making government more effective and efficient and easier to access. The e-authentication service enables users to access government services online using log-in IDs (identity credentials) from other web sites that both the user and the government trust.\n\nE-authentication is a government-wide partnership that is supported by the agencies that comprise the Federal CIO Council. The United States General Services Administration (GSA) is the lead agency partner. E-authentication works through an association with a trusted credential issuer, making it necessary for the user to log into the issuer's site to obtain the authentication credentials. Those credentials or e-authentication ID are then transferred the supporting government web site causing authentication.The system was created in response a December 16, 2003 memorandum was issued through the Office of Management and Budget. Memorandum M04-04 Whitehouse. That memorandum updates the guidance issued in the \"Paperwork Elimination Act\" of 1998, 44 U.S.C. § 3504 and implements section 203 of the E-Government Act, 44 U.S.C. ch. 36.\n\nNIST provides guidelines for digital authentication standards and does away with most knowledge-based authentication methods. A stricter standard has been drafted on more complicated passwords that at least 8 characters long or passphrases that are at least 64 characters long.\n\nIn Europe, eIDAS provides guidelines to be used for electronic authentication in regards to electronic signatures and certificate services for website authentication. Once confirmed by the issuing Member State, other participating States are required to accept the user's electronic signature as valid for cross border transactions.\n\nUnder eIDAS, electronic identification refers to a material/immaterial unit that contains personal identification data to be used for authentication for an online service. Authentication is referred to as an electronic process that allows for the electronic identification of a natural or legal person. A trust service is an electronic service that is used to create, verify and validate electronic signatures, in addition to creating, verifying and validating certificates for website authentication.\n\nArticle 8 of eIDAS allows for the authentication mechanism that is used by a natural or legal person to use electronic identification methods in confirming their identity to a relying party. Annex IV provides requirements for qualified certificates for website authentication.\nE-authentication is a centerpiece of the Russia government's effort to expand e-government, as a way of making government more effective and efficient and easier for the Russian people to access. The e-authentication service enables users to access government services online using log-in IDs (identity credentials) they already have from web sites that they and the government trust.\n\nApart from government services, e-authentication is also widely used in other technology and industries. These new applications combine the features of authorizing identities in traditional database and new technology to provide a more secure and diverse use of e-authentication. Some examples are described below.\n\nMobile authentication is the verification of a user's identity through the use a mobile device. It can be treated as an independent field or it can also be applied with other multifactor authentication schemes in the e-authentication field.\n\nFor mobile authentication, there are five levels of application sensitivity from Level 0 to Level 4. Level 0 is for public use over a mobile device and requires no identity authentications, while level 4 has the most multi-procedures to identify users. For either level, mobile authentication is relatively easy to process. Firstly, users send a one-time password (OTP) through offline channels. Then, a server identifies the information and makes adjustment in the database. Since only the user has the access to a PIN code and can send information through their mobile devices, there is a low risk of attacks.\n\nIn the early 1980s, electronic data interchange (EDI) systems was implemented, which was considered as an early representative of E-commerce. But ensuring its security is not a significant issue since the systems are all constructed around closed networks. However, more recently, business-to-consumer transactions have transformed. Remote transacting parties have forced the implementation of E-commerce authentication systems.\n\nGenerally speaking, the approaches adopted in E-commerce authentication are basically the same as e-authentication. The difference is E-commerce authentication is a more narrow field that focuses on the transactions between customers and suppliers. A simple example of E-commerce authentication includes a client communicating with a merchant server via the Internet. The merchant server usually utilizes a web server to accept client requests, a database management system to manage data and a payment gateway to provide online payment services.\n\nTo keep up with the evolution of services in the digital world, there is continued need for security mechanisms. While passwords will continued to be used, it is important to rely on authentication mechanisms, most importantly multifactor authentication. As the usage of e-signatures continues to significantly expand throughout the United States, the EU and throughout the world, there is expectation that regulations such as eIDAS will eventually be amended to reflect changing conditions along with regulations in the United States.\n\n"}
{"id": "53198596", "url": "https://en.wikipedia.org/wiki?curid=53198596", "title": "GameFace Labs", "text": "GameFace Labs\n\nGameFace Labs is an American technology company that develops hardware and software solutions for the consumer virtual reality market, and was founded in 2013 by Edward Mason. The company's headquarters are in San Francisco, with international offices in London, United Kingdom.\n\nThe company’s first product is a standalone headset that features an onboard Nvidia Tegra SoC (system on a chip) and does not require a tethered connection to a computer or console in order to operate. GameFace Labs is a founding member of the Open Source Virtual Reality project (OSVR) and the Immersive Technology Alliance. In December 2015, GameFace Labs joined Valve’s OpenVR ecosystem.\n\nDuring Insomnia Gaming Festival 49 (2013), the company revealed its first prototype; a standalone headset featuring a 720p LCD display panel and an Nvidia Tegra 3 SoC. At CES 2014, the company showcased an updated prototype with a 1080p OLED display and a Tegra 4 SoC. \n\nIn June 2014, the company showcased the first iteration of its user interface, allowing users to launch applications inside a virtual environment, as well as invite friends from other platforms to enjoy and collaborate on content inside the same virtual space.\nIn June 2014, the company also demoed its Mk V prototype at E3 - running on a version of Nvidia’s Tegra K1 Soc. The Mk V headset was the first publicly demoed headset to include a 1440p display, which was supplied by Samsung.. The K1 prototype utilized Nvidia’s mobile Kepler architecture, which means it supported Android content that used DirectX 11 and OpenGL 4.4 and was also compatible with all Google Cardboard content.\n\nAt 2017’s E3 expo, GameFace Labs demoed its seventh standalone prototype: a multi-platform headset supporting Android / DayDream content, as well as full 6DoF positional tracking inside an Android environment. The prototype featured dual OLED display panels, low persistence display technology and support for 6DoF input control via a wireless IR beacon (in January 2018 the company revealed it had been developing an ARM-based implementation of Valve's generation 2 Lighthouse technology, using GameFace's custom firmware\n). The company also showed off an Alpha build of its developer platform, allowing users to share and download content up to 1000x faster than traditional downloading methods.\n\nIn May 2018, GameFace Labs began shipping developer kits powered by Nvidia's TX2 SoC. This makes the GameFace Labs developer kit the first consumer electronics device to use the TX2, with Magic Leap announcing the second device later in July. The GameFace Labs developer kit is the only system to pair the TX2 SoC with an Android Operating system. \n\nIn 2014, GameFace Labs was awarded ‘Hack of Honor’ at the annual Penguicon open source convention\n"}
{"id": "13537211", "url": "https://en.wikipedia.org/wiki?curid=13537211", "title": "General content descriptor", "text": "General content descriptor\n\nA General Content Descriptor (GCD) is a file which describes downloads like ringtones and pictures to wireless devices. GCD's are plain text files. They are required by many wireless carriers to install applications on devices. The name of the file will end with a \".gcd\" extension.\n"}
{"id": "7754370", "url": "https://en.wikipedia.org/wiki?curid=7754370", "title": "History of artificial life", "text": "History of artificial life\n\nThe idea of human artifacts being given life has fascinated humankind for as long as people have been recording their myths and stories. Whether Pygmalion or Frankenstein, humanity has been fascinated with the idea of artificial life.\n\nAutomatons were quite a novelty. In the days before computers and electronics, some were very sophisticated, using pneumatics, mechanics, and hydraulics. The first automata were conceived during the third and second centuries BC and these were demonstrated by the theorems of Hero of Alexandria, which included sophisticated mechanical and hydraulic solutions. Many of his notable works were included in the book \"Pneumatics\", which was also used for constructing machines until early modern times. In 1490, Leonardo Da Vinci also constructed an armored knight, which is considered the first humanoid robot in Western civilization.\n\nOther early famous examples include al-Jazari's humanoid robots. This Arabic inventor once constructed a band of automata, which can be commanded to play different pieces of music. There is also the case of Jacques de Vaucanson's artificial duck exhibited in 1735, which had thousands of moving parts and one of the first to mimic a biological system. The duck could reportedly eat and digest, drink, quack, and splash in a pool. It was exhibited all over Europe until it fell into disrepair.\n\nHowever, it wasn't until the invention of cheap computing power that artificial life as a legitimate science began in earnest, steeped more in the theoretical and computational than the mechanical and mythological.\n\nOne of the earliest thinkers of the modern age to postulate the potentials of artificial life, separate from artificial intelligence, was math and computer prodigy John von Neumann. At the Hixon Symposium, hosted by Linus Pauling in Pasadena, California in the late 1940s, von Neumann delivered a lecture titled \"The General and Logical Theory of Automata.\" He defined an \"automaton\" as any machine whose behavior proceeded logically from step to step by combining information from the environment and its own programming, and said that natural organisms would in the end be found to follow similar simple rules. He also spoke about the idea of self-replicating machines. He postulated a machine – a kinematic automaton – made up of a control computer, a construction arm, and a long series of instructions, floating in a lake of parts. By following the instructions that were part of its own body, it could create an identical machine. He followed this idea by creating (with Stanislaw Ulam) a purely logic-based automaton, not requiring a physical body but based on the changing states of the cells in an infinite grid – the first cellular automaton. It was extraordinarily complicated compared to later CAs, having hundreds of thousands of cells which could each exist in one of twenty-nine states, but von Neumann felt he needed the complexity in order for it to function not just as a self-replicating \"machine\", but also as a universal computer as defined by Alan Turing. This \"universal constructor\" read from a tape of instructions and wrote out a series of cells that could then be made active to leave a fully functional copy of the original machine and its tape. Von Neumann worked on his automata theory intensively right up to his death, and considered it his most important work.\n\nHomer Jacobson illustrated basic self-replication in the 1950s with a model train set – a seed \"organism\" consisting of a \"head\" and \"tail\" boxcar could use the simple rules of the system to consistently create new \"organisms\" identical to itself, so long as there was a random pool of new boxcars to draw from.\nEdward F. Moore proposed \"Artificial Living Plants\", which would be floating factories which could create copies of themselves. They could be programmed to perform some function (extracting fresh water, harvesting minerals from seawater) for an investment that would be relatively small compared to the huge returns from the exponentially growing numbers of factories. Freeman Dyson also studied the idea, envisioning self-replicating machines sent to explore and exploit other planets and moons, and a NASA group called the Self-Replicating Systems Concept Team performed a 1980 study on the feasibility of a self-building lunar factory.\n\nUniversity of Cambridge professor John Horton Conway invented the most famous cellular automaton in the 1960s. He called it the Game of Life, and publicized it through Martin Gardner's column in \"Scientific American\" magazine.\n\nPhilosophy scholar Arthur Burks, who had worked with von Neumann (and indeed, organized his papers after Neumann's death), headed the Logic of Computers Group at the University of Michigan. He brought the overlooked views of 19th century American thinker Charles Sanders Peirce into the modern age. Peirce was a strong believer that all of nature's workings were based on logic (though not always deductive logic). The Michigan group was one of the few groups still interested in alife and CAs in the early 1970s; one of its students, Tommaso Toffoli argued in his PhD thesis that the field was important because its results explain the simple rules that underlay complex effects in nature. Toffoli later provided a key proof that CAs were reversible, just as the true universe is considered to be.\n\nChristopher Langton was an unconventional researcher, with an undistinguished academic career that led him to a job programming DEC mainframes for a hospital. He became enthralled by Conway's Game of Life, and began pursuing the idea that the computer could emulate living creatures. After years of study (and a near-fatal hang-gliding accident), he began attempting to actualize Von Neumann's CA and the work of Edgar F. Codd, who had simplified Von Neumann's original twenty-nine state monster to one with only eight states. He succeeded in creating the first self-replicating computer organism in October 1979, using only an Apple II desktop computer. He entered Burks' graduate program at the Logic of Computers Group in 1982, at the age of 33, and helped to found a new discipline.\n\nLangton's official conference announcement of Artificial Life I was the earliest description of a field which had previously barely existed:\n\n\"Artificial life is the study of artificial systems that exhibit behavior characteristic of natural living systems. It is the quest to explain life in any of its possible manifestations, without restriction to the particular examples that have evolved on earth. This includes biological and chemical experiments, computer simulations, and purely theoretical endeavors. Processes occurring on molecular, social, and evolutionary scales are subject to investigation. The ultimate goal is to extract the logical form of living systems.\"\n\n\"Microelectronic technology and genetic engineering will soon give us the capability to create new life forms \"in silico\" as well as \"in vitro\". This capacity will present humanity with the most far-reaching technical, theoretical and ethical challenges it has ever confronted. The time seems appropriate for a gathering of those involved in attempts to simulate or synthesize aspects of living systems.\"\n\nEd Fredkin founded the Information Mechanics Group at MIT, which united Toffoli, Norman Margolus, Gerard Vichniac, and Charles Bennett. This group created a computer especially designed to execute cellular automata, eventually reducing it to the size of a single circuit board. This \"cellular automata machine\" allowed an explosion of alife research among scientists who could not otherwise afford sophisticated computers.\n\nIn 1982, computer scientist named Stephen Wolfram turned his attention to cellular automata. He explored and categorized the types of complexity displayed by one-dimensional CAs, and showed how they applied to natural phenomena such as the patterns of seashells and the nature of plant growth.\nNorman Packard, who worked with Wolfram at the Institute for Advanced Study, used CAs to simulate the growth of snowflakes, following very basic rules.\n\nComputer animator Craig Reynolds similarly used three simple rules to create recognizable flocking behaviour in a computer program in 1987 to animate groups of boids. With no top-down programming at all, the boids produced lifelike solutions to evading obstacles placed in their path. Computer animation has continued to be a key commercial driver of alife research as the creators of movies attempt to find more realistic and inexpensive ways to animate natural forms such as plant life, animal movement, hair growth, and complicated organic textures.\n\nThe Unit of Theoretical Behavioural Ecology at the Free University of Brussels applied the self-organization theories of Ilya Prigogine and the work of entomologist E.O. Wilson to research the behavior of social insects, particularly allelomimesis, in which an individual's actions are dictated by those of a neighbor. They developed partial differential equations which modeled the shapes created by termites when constructing their nest. They then compared that to the reaction of real termites to identical changes in laboratory colonies, and refined their theories about the rules which underlay the behavior.\n\nJ. Doyne Farmer was a key figure in tying artificial life research to the emerging field of complex adaptive systems, working at the Center for Nonlinear Studies (a basic research section of Los Alamos National Laboratory), just as its star chaos theorist Mitchell Feigenbaum was leaving. Farmer and Norman Packard chaired a conference in May 1985 called \"Evolution, Games, and Learning\", which was to presage many of the topics of later alife conferences.\n\nOn the ecological front, research regarding the evolution of animal cooperative behavior (started by W. D. Hamilton in the 1960s resulting in theories of kin selection, reciprocity, multilevel selection and cultural group selection) was re-introduced via artificial life by Peter Turchin and Mikhail Burtsev in 2006. Previously, game theory has been utilized in similar investigation, however, that approach was deemed to be rather limiting in its amount of possible strategies and debatable set of payoff rules. The alife model designed here, instead, is based upon Conway's Game of Life but with much added complexity (there are over 10 strategies that can potentially emerge). Most significantly, the interacting agents are characterized by external phenotype markers which allows for recognition amongst in-group members. In effect, it is shown that given the capacity to perceive these markers, agents within the system are then able to evolve new group behaviors under minimalistic assumptions. On top of the already known strategies of the bourgeois-hawk-dove game, here two novel modes of cooperative attack and defense arise from the simulation.\n\nFor the setup, this two-dimensional artificial world is divided into cells, each empty or containing a resource bundle. An empty cell can acquire a resource bundle with a certain probability per unit of time and lose it when an agent consumes the resource. Each agent is plainly constructed with a set of receptors, effectors (the components that govern the agents' behavior), and neural net which connect the two. In response to the environment, an agent may rest, eat, reproduce by division, move, turn and attack. All actions expend energy taken from its internal energy storage; once that is depleted, the agent dies. Consumption of resource, as well as other agents after defeating them, yields an increase in the energy storage. Reproduction is modeled as being asexual while the offspring receive half the parental energy. Agents are also equipped with sensory inputs that allow them to detect resources or other members within a parameter in addition to its own level of vitality. As for the phenotype markers, they do not influence behavior but solely function as indicator of 'genetic' similarity. Heredity is achieved by having the relevant information be inherited by the offspring and subjected to a set rate of mutation.\n\nThe objective of the investigation is to study how the presence of phenotype markers affects the model's range of evolving cooperative strategies. In addition, as the resource available in this 2D environment is capped, the simulation also serves to determine the effect of environmental carrying capacity on their emergence.\n\nOne previously unseen strategy is termed the \"raven\". These agents leave cells with in-group members, thus avoiding intra-specific competition, and attack out-group members voluntarily. Another strategy, named the 'starling', involves the agent sharing cells with in-group members. Despite individuals having smaller energy storage due to resource partitioning, this strategy permits highly effective defense against large invaders via the advantage in numbers. Ecologically speaking, this resembles the mobbing behavior that characterizes many species of small birds when they collectively defend against the predator.\n\nIn conclusion, the research claims that the simulated results have important implications for the evolution of territoriality by showing that within the alife framework it is possible to \"model not only how one strategy displaces another, but also the very process by which new strategies emerge from a large quantity of possibilities\".\n\nWork is also underway to create cellular models of artificial life. Initial work on building a complete biochemical model of cellular behavior is underway as part of a number of different research projects, namely Blue Gene which seeks to understand the mechanisms behind protein folding.\n\n\nAguilar, W., Santamaría-Bonfil, G., Froese, T., and Gershenson, C. (2014). The past, present, and future of artificial life. Frontiers in Robotics and AI, 1(8). https://dx.doi.org/10.3389/frobt.2014.00008\n"}
{"id": "14449116", "url": "https://en.wikipedia.org/wiki?curid=14449116", "title": "History of timekeeping devices", "text": "History of timekeeping devices\n\nFor thousands of years, devices have been used to measure and keep track of time. The current sexagesimal system of time measurement dates to approximately 2000  from the Sumerians.\n\nThe Egyptians divided the day into two 12-hour periods, and used large obelisks to track the movement of the sun. They also developed water clocks, which were probably first used in the Precinct of Amun-Re, and later outside Egypt as well; they were employed frequently by the Ancient Greeks, who called them \"clepsydrae\". The Zhou dynasty is believed to have used the outflow water clock around the same time, devices which were introduced from Mesopotamia as early as 2000.\n\nOther ancient timekeeping devices include the candle clock, used in ancient China, ancient Japan, England and Mesopotamia; the timestick, widely used in India and Tibet, as well as some parts of Europe; and the hourglass, which functioned similarly to a water clock. The sundial, another early clock, relies on shadows to provide a good estimate of the hour on a sunny day. It is not so useful in cloudy weather or at night and requires recalibration as the seasons change (if the gnomon was not aligned with the Earth's axis).\n\nThe earliest known clock with a water-powered escapement mechanism, which transferred rotational energy into intermittent motions, dates back to 3rd century in ancient Greece; Chinese engineers later invented clocks incorporating mercury-powered escapement mechanisms in the 10th century, followed by Iranian engineers inventing water clocks driven by gears and weights in the 11th century.\n\nThe first mechanical clocks, employing the verge escapement mechanism with a foliot or balance wheel timekeeper, were invented in Europe at around the start of the 14th century, and became the standard timekeeping device until the pendulum clock was invented in 1656. The invention of the mainspring in the early 15th century allowed portable clocks to be built, evolving into the first pocketwatches by the 17th century, but these were not very accurate until the balance spring was added to the balance wheel in the mid 17th century.\n\nThe pendulum clock remained the most accurate timekeeper until the 1930s, when quartz oscillators were invented, followed by atomic clocks after World War 2. Although initially limited to laboratories, the development of microelectronics in the 1960s made quartz clocks both compact and cheap to produce, and by the 1980s they became the world's dominant timekeeping technology in both clocks and wristwatches.\n\nAtomic clocks are far more accurate than any previous timekeeping device, and are used to calibrate other clocks and to calculate the International Atomic Time; a standardized civil system, Coordinated Universal Time, is based on atomic time.\n\nMany ancient civilizations observed astronomical bodies, often the Sun and Moon, to determine times, dates, and seasons. The first calendars may have been created during the last glacial period, by hunter-gatherers who employed tools such as sticks and bones to track the phases of the moon or the seasons. Stone circles, such as England's Stonehenge, were built in various parts of the world, especially in Prehistoric Europe, and are thought to have been used to time and predict seasonal and annual events such as equinoxes or solstices. As those megalithic civilizations left no recorded history, little is known of their calendars or timekeeping methods. Methods of sexagesimal timekeeping, now common in both Western and Eastern societies, are first attested nearly 4,000 years ago in Mesopotamia and Egypt. Mesoamericans similarly modified their usual vigesimal counting system when dealing with calendars to produce a 360-day year.\n\nThe oldest known sundial is from Egypt; it dates back to around 1500 (19th Dynasty), and was discovered in the Valley of the Kings in 2013. Sundials have their origin in shadow clocks, which were the first devices used for measuring the parts of a day. Ancient Egyptian obelisks, constructed about 3500, are also among the earliest shadow clocks.\nEgyptian shadow clocks divided daytime into 12 parts with each part further divided into more precise parts. One type of shadow clock consisted of a long stem with five variable marks and an elevated crossbar which cast a shadow over those marks. It was positioned eastward in the morning, and was turned west at noon. Obelisks functioned in much the same manner: the shadow cast on the markers around it allowed the Egyptians to calculate the time. The obelisk also indicated whether it was morning or afternoon, as well as the summer and winter solstices. A third shadow clock, developed c. 1500, was similar in shape to a bent T-square. It measured the passage of time by the shadow cast by its crossbar on a non-linear rule. The \"T\" was oriented eastward in the mornings, and turned around at noon, so that it could cast its shadow in the opposite direction.\n\nAlthough accurate, shadow clocks relied on the sun, and so were useless at night and in cloudy weather. The Egyptians therefore developed a number of alternative timekeeping instruments, including water clocks, and a system for tracking star movements. The oldest description of a water clock is from the tomb inscription of the 16th-century Egyptian court official Amenemhet, identifying him as its inventor. There were several types of water clocks, some more elaborate than others. One type consisted of a bowl with small holes in its bottom, which was floated on water and allowed to fill at a near-constant rate; markings on the side of the bowl indicated elapsed time, as the surface of the water reached them. The oldest-known waterclock was found in the tomb of pharaoh Amenhotep I (1525–1504), suggesting that they were first used in ancient Egypt. Another Egyptian method of determining the time during the night was using plumb-lines called merkhets. In use since at least 600, two of these instruments were aligned with Polaris, the north pole star, to create a north–south meridian. The time was accurately measured by observing certain stars as they crossed the line created with the \"merkhets\".\n\nWater clocks, or clepsydrae, were commonly used in Ancient Greece following their introduction by Plato, who also invented a water-based alarm clock. One account of Plato's alarm clock describes it as depending on the nightly overflow of a vessel containing lead balls, which floated in a columnar vat. The vat held a steadily increasing amount of water, supplied by a cistern. By morning, the vessel would have floated high enough to tip over, causing the lead balls to cascade onto a copper platter. The resultant clangor would then awaken Plato's students at the Academy. Another possibility is that it comprised two jars, connected by a siphon. Water emptied until it reached the siphon, which transported the water to the other jar. There, the rising water would force air through a whistle, sounding an alarm. The Greeks and Chaldeans regularly maintained timekeeping records as an essential part of their astronomical observations.\n\nGreek astronomer, Andronicus of Cyrrhus, supervised the construction of the Tower of the Winds in Athens in the 1st century.\n\nIn Greek tradition, clepsydrae were used in court; later, the Romans adopted this practice, as well. There are several mentions of this in historical records and literature of the era; for example, in \"Theaetetus\", Plato says that \"Those men, on the other hand, always speak in haste, for the flowing water urges them on\". Another mention occurs in Lucius Apuleius' \"The Golden Ass\": \"The Clerk of the Court began bawling again, this time summoning the chief witness for the prosecution to appear. Up stepped an old man, whom I did not know. He was invited to speak for as long as there was water in the clock; this was a hollow globe into which water was poured through a funnel in the neck, and from which it gradually escaped through fine perforations at the base\". The clock in Apuleius's account was one of several types of water clock used. Another consisted of a bowl with a hole in its centre, which was floated on water. Time was kept by observing how long the bowl took to fill with water.\n\nAlthough clepsydrae were more useful than sundials—they could be used indoors, during the night, and also when the sky was cloudy—they were not as accurate; the Greeks, therefore, sought a way to improve their water clocks. Although still not as accurate as sundials, Greek water clocks became more accurate around 325, and they were adapted to have a face with an hour hand, making the reading of the clock more precise and convenient. One of the more common problems in most types of clepsydrae was caused by water pressure: when the container holding the water was full, the increased pressure caused the water to flow more rapidly. This problem was addressed by Greek and Roman horologists beginning in 100, and improvements continued to be made in the following centuries. To counteract the increased water flow, the clock's water containers—usually bowls or jugs—were given a conical shape; positioned with the wide end up, a greater amount of water had to flow out in order to drop the same distance as when the water was lower in the cone. Along with this improvement, clocks were constructed more elegantly in this period, with hours marked by gongs, doors opening to miniature figurines, bells, or moving mechanisms. There were some remaining problems, however, which were never solved, such as the effect of temperature. Water flows more slowly when cold, or may even freeze.\n\nBetween 270 and 500, Hellenistic (Ctesibius, Hero of Alexandria, Archimedes) and Roman horologists and astronomers began developing more elaborate mechanized water clocks. The added complexity was aimed at regulating the flow and at providing fancier displays of the passage of time. For example, some water clocks rang bells and gongs, while others opened doors and windows to show figurines of people, or moved pointers, and dials. Some even displayed astrological models of the universe.\n\nAlthough the Greeks and Romans did much to advance water clock technology, they still continued to use shadow clocks. The mathematician and astronomer Theodosius of Bithynia, for example, is said to have invented a universal sundial that was accurate anywhere on Earth, though little is known about it. Others wrote of the sundial in the mathematics and literature of the period. Marcus Vitruvius Pollio, the Roman author of \"De Architectura\", wrote on the mathematics of gnomons, or sundial blades. During the reign of Emperor Augustus, the Romans constructed the largest sundial ever built, the Solarium Augusti. Its gnomon was an obelisk from Heliopolis. Similarly, the obelisk from Campus Martius was used as the gnomon for Augustus's zodiacal sundial. Pliny the Elder records that the first sundial in Rome arrived in 264, looted from Catania, Sicily; according to him, it gave the incorrect time until the markings and angle appropriate for Rome's latitude were used—a century later.\n\nAccording to Callisthenes, the Persians were using water clocks in 328 to ensure a just and exact distribution of water from qanats to their shareholders for agricultural irrigation. The use of water clocks in Iran, especially in Zeebad, dates back to 500. Later they were also used to determine the exact holy days of pre-Islamic religions, such as the \"Nowruz\", \"Chelah\", or \"Yaldā\" – the shortest, longest, and equal-length days and nights of the years. The water clocks used in Iran were one of the most practical ancient tools for timing the yearly calendar.\n\nWater clocks, or \"Fenjaan\", in Persia reached a level of accuracy comparable to today's standards of timekeeping. The fenjaan was the most accurate and commonly used timekeeping device for calculating the amount or the time that a farmer must take water from a qanat or well for irrigation of the farms, until it was replaced by more accurate current clock. Persian water clocks were a practical and useful tool for the qanat's shareholders to calculate the length of time they could divert water to their farm. The qanat was the only water source for agriculture and irrigation so a just and fair water distribution was very important. Therefore, a very fair and clever old person was elected to be the manager of the water clock, and at least two full-time managers were needed to control and observe the number of fenjaans and announce the exact time during the days and nights.\n\nThe fenjaan was a big pot full of water and a bowl with small hole in the center. When the bowl become full of water, it would sink into the pot, and the manager would empty the bowl and again put it on the top of the water in the pot. He would record the number of times the bowl sank by putting small stones into a jar.\n\nThe place where the clock was situated, and its managers, were collectively known as \"khaneh fenjaan\". Usually this would be the top floor of a public-house, with west- and east-facing windows to show the time of sunset and sunrise. There was also another time-keeping tool named a \"staryab\" or astrolabe, but it was mostly used for superstitious beliefs and was not practical for use as a farmers' calendar. The Zeebad Gonabad water clock was in use until 1965 when it was substituted by modern clocks.\n\nJoseph Needham speculated that the introduction of the outflow clepsydra to China, perhaps from Mesopotamia, occurred as far back as the 2nd millennium, during the Shang Dynasty, and at the latest by the 1st millennium. By the beginning of the Han Dynasty, in 202, the outflow clepsydra was gradually replaced by the inflow clepsydra, which featured an indicator rod on a float. To compensate for the falling pressure head in the reservoir, which slowed timekeeping as the vessel filled, Zhang Heng added an extra tank between the reservoir and the inflow vessel. Around 550 AD, Yin Gui was the first in China to write of the overflow or constant-level tank added to the series, which was later described in detail by the inventor Shen Kuo. Around 610, this design was trumped by two Sui Dynasty inventors, Geng Xun and Yuwen Kai, who were the first to create the balance clepsydra, with standard positions for the steelyard balance. Joseph Needham states that:\nThe term 'clock' encompasses a wide spectrum of devices, ranging from wristwatches to the Clock of the Long Now. The English word \"clock\" is said to derive from the Middle English \"clokke\", Old North French \"cloque\", or Middle Dutch \"clocke\", all of which mean \"bell\", and are derived from the Medieval Latin \"clocca\", also meaning bell. Indeed, bells were used to mark the passage of time; they marked the passage of the hours at sea and in abbeys.\n\nThroughout history, clocks have had a variety of power sources, including gravity, springs, and electricity. Mechanical clocks became widespread in the 14th century, when they were used in medieval monasteries to keep the regulated schedule of prayers. The clock continued to be improved, with the first pendulum clock being designed and built in the 17th century.\n\nThe earliest mention of candle clocks comes from a Chinese poem, written in 520 by You Jianfu. According to the poem, the graduated candle was a means of determining time at night. Similar candles were used in Japan until the early 10th century.\n\nThe candle clock most commonly mentioned and written of is attributed to King Alfred the Great. It consisted of six candles made from 72 pennyweights of wax, each high, and of uniform thickness, marked every inch (2.54 cm). As these candles burned for about four hours, each mark represented 20 minutes. Once lit, the candles were placed in wooden framed glass boxes, to prevent the flame from extinguishing.\n\nThe most sophisticated candle clocks of their time were those of Al-Jazari in 1206. One of his candle clocks included a dial to display the time and, for the first time, employed a bayonet fitting, a fastening mechanism still used in modern times. Donald Routledge Hill described Al-Jazari's candle clocks as follows:\n\nA variation on this theme were oil-lamp clocks. These early timekeeping devices consisted of a graduated glass reservoir to hold oil — usually whale oil, which burned cleanly and evenly — supplying the fuel for a built-in lamp. As the level in the reservoir dropped, it provided a rough measure of the passage of time.\n\nIn addition to water, mechanical, and candle clocks, incense clocks were used in the Far East, and were fashioned in several different forms. Incense clocks were first used in China around the 6th century; in Japan, one still exists in the Shōsōin, although its characters are not Chinese, but Devanagari. Due to their frequent use of Devanagari characters, suggestive of their use in Buddhist ceremonies, Edward H. Schafer speculated that incense clocks were invented in India. Although similar to the candle clock, incense clocks burned evenly and without a flame; therefore, they were more accurate and safer for indoor use.\n\nSeveral types of incense clock have been found, the most common forms include the incense stick and incense seal. An incense stick clock was an incense stick with calibrations; most were elaborate, sometimes having threads, with weights attached, at even intervals. The weights would drop onto a platter or gong below, signifying that a certain amount of time had elapsed. Some incense clocks were held in elegant trays; open-bottomed trays were also used, to allow the weights to be used together with the decorative tray. Sticks of incense with different scents were also used, so that the hours were marked by a change in fragrance. The incense sticks could be straight or spiraled; the spiraled ones were longer, and were therefore intended for long periods of use, and often hung from the roofs of homes and temples. In Japan, a geisha was paid for the number of \"senkodokei\" (incense sticks) that had been consumed while she was present, a practice which continued until 1924.\n\nIncense seal clocks were used for similar occasions and events as the stick clock; while religious purposes were of primary importance, these clocks were also popular at social gatherings, and were used by Chinese scholars and intellectuals. The seal was a wooden or stone disk with one or more grooves etched in it into which incense was placed. These clocks were common in China, but were produced in fewer numbers in Japan. To signal the passage of a specific amount of time, small pieces of fragrant woods, resins, or different scented incenses could be placed on the incense powder trails. Different powdered incense clocks used different formulations of incense, depending on how the clock was laid out. The length of the trail of incense, directly related to the size of the seal, was the primary factor in determining how long the clock would last; all burned for long periods of time, ranging between 12 hours and a month.\n\nWhile early incense seals were made of wood or stone, the Chinese gradually introduced disks made of metal, most likely beginning during the Song dynasty. This allowed craftsmen to more easily create both large and small seals, as well as design and decorate them more aesthetically. Another advantage was the ability to vary the paths of the grooves, to allow for the changing length of the days in the year. As smaller seals became more readily available, the clocks grew in popularity among the Chinese, and were often given as gifts. Incense seal clocks are often sought by modern-day clock collectors; however, few remain that have not already been purchased or been placed on display at museums or temples.\n\nSundials had been used for timekeeping since Ancient Egypt. Ancient dials were nodus-based with straight hour-lines that indicated unequal hours—also called temporary hours—that varied with the seasons. Every day was divided into 12 equal segments regardless of the time of year; thus, hours were shorter in winter and longer in summer. The sundial was further developed by Muslim astronomers. The idea of using hours of equal length throughout the year was the innovation of Abu'l-Hasan Ibn al-Shatir in 1371, based on earlier developments in trigonometry by Muhammad ibn Jābir al-Harrānī al-Battānī (Albategni). Ibn al-Shatir was aware that \"using a gnomon that is parallel to the Earth's axis will produce sundials whose hour lines indicate equal hours on any day of the year\". His sundial is the oldest polar-axis sundial still in existence. The concept appeared in Western sundials starting in 1446.\n\nFollowing the acceptance of heliocentrism and equal hours, as well as advances in trigonometry, sundials appeared in their present form during the Renaissance, when they were built in large numbers. In 1524, the French astronomer Oronce Finé constructed an ivory sundial, which still exists; later, in 1570, the Italian astronomer Giovanni Padovani published a treatise including instructions for the manufacture and laying out of mural (vertical) and horizontal sundials. Similarly, Giuseppe Biancani's \"Constructio instrumenti ad horologia solaria\" (c. 1620) discusses how to construct sundials.\n\nSince the hourglass was one of the few reliable methods of measuring time at sea, it is speculated that it was used on board ships as far back as the 11th century, when it would have complemented the magnetic compass as an aid to navigation. However, the earliest unambiguous evidence of their use appears in the painting \"Allegory of Good Government\", by Ambrogio Lorenzetti, from 1338. From the 15th century onwards, hourglasses were used in a wide range of applications at sea, in churches, in industry, and in cooking; they were the first dependable, reusable, reasonably accurate, and easily constructed time-measurement devices. The hourglass also took on symbolic meanings, such as that of death, temperance, opportunity, and Father Time, usually represented as a bearded, old man. Though also used in China, the hourglass's history there is unknown. The Portuguese navigator Ferdinand Magellan used 18 hourglasses on each ship during his circumnavigation of the globe in 1522.\n\nThe earliest instance of a liquid-driven escapement was described by the Greek engineer Philo of Byzantium (fl. 3rd century) in his technical treatise \"Pneumatics\" (chapter 31) where he likens the escapement mechanism of a washstand automaton with those as employed in (water) clocks. Another early clock to use escapements was built during the 7th century in Chang'an, by Tantric monk and mathematician, Yi Xing, and government official Liang Lingzan. An astronomical instrument that served as a clock, it was discussed in a contemporary text as follows:\n[It] was made in the image of the round heavens and on it were shown the lunar mansions in their order, the equator and the degrees of the heavenly circumference. Water, flowing into scoops, turned a wheel automatically, rotating it one complete revolution in one day and night. Besides this, there were two rings fitted around the celestial sphere outside, having the sun and moon threaded on them, and these were made to move in circling orbit ... And they made a wooden casing the surface of which represented the horizon, since the instrument was half sunk in it. It permitted the exact determinations of the time of dawns and dusks, full and new moons, tarrying and hurrying. Moreover, there were two wooden jacks standing on the horizon surface, having one a bell and the other a drum in front of it, the bell being struck automatically to indicate the hours, and the drum being beaten automatically to indicate the quarters. All these motions were brought about by machinery within the casing, each depending on wheels and shafts, hooks, pins and interlocking rods, stopping devices and locks checking mutually.\n\nSince Yi Xing's clock was a water clock, it was affected by temperature variations. That problem was solved in 976 by Zhang Sixun by replacing the water with mercury, which remains liquid down to . Zhang implemented the changes into his clock tower, which was about tall, with escapements to keep the clock turning and bells to signal every quarter-hour. Another noteworthy clock, the elaborate Cosmic Engine, was built by Su Song, in 1088. It was about the size of Zhang's tower, but had an automatically rotating armillary sphere—also called a celestial globe—from which the positions of the stars could be observed. It also featured five panels with mannequins ringing gongs or bells, and tablets showing the time of day, or other special times. Furthermore, it featured the first known endless power-transmitting chain drive in horology. Originally built in the capital of Kaifeng, it was dismantled by the Jin army and sent to the capital of Yanjing (now Beijing), where they were unable to put it back together. As a result, Su Song's son Su Xie was ordered to build a replica.\nThe clock towers built by Zhang Sixun and Su Song, in the 10th and 11th centuries, respectively, also incorporated a striking clock mechanism, the use of clock jacks to sound the hours. A striking clock outside of China was the Jayrun Water Clock, at the Umayyad Mosque in Damascus, Syria, which struck once every hour. It was constructed by Muhammad al-Sa'ati in the 12th century, and later described by his son Ridwan ibn al-Sa'ati, in his \"On the Construction of Clocks and their Use\" (1203), when repairing the clock. In 1235, an early monumental water-powered alarm clock that \"announced the appointed hours of prayer and the time both by day and by night\" was completed in the entrance hall of the Mustansiriya Madrasah in Baghdad.\n\nThe first geared clock was invented in the 11th century by the Arab engineer Ibn Khalaf al-Muradi in Islamic Iberia; it was a water clock that employed a complex gear train mechanism, including both segmental and epicyclic gearing, capable of transmitting high torque. The clock was unrivalled in its use of sophisticated complex gearing, until the mechanical clocks of the mid-14th century. Al-Muradi's clock also employed the use of mercury in its hydraulic linkages, which could function mechanical automata. Al-Muradi's work was known to scholars working under Alfonso X of Castile, hence the mechanism may have played a role in the development of the European mechanical clocks. Other monumental water clocks constructed by medieval Muslim engineers also employed complex gear trains and arrays of automata. Like the earlier Greeks and Chinese, Arab engineers at the time also developed a liquid-driven escapement mechanism which they employed in some of their water clocks. Heavy floats were used as weights and a constant-head system was used as an escapement mechanism, which was present in the hydraulic controls they used to make heavy floats descend at a slow and steady rate.\n\nA mercury clock, described in the \"Libros del saber de Astronomia\", a Spanish work from 1277 consisting of translations and paraphrases of Arabic works, is sometimes quoted as evidence for Muslim knowledge of a mechanical clock. However, the device was actually a compartmented cylindrical water clock, which the Jewish author of the relevant section, Rabbi Isaac, constructed using principles described by a philosopher named \"Iran\", identified with Heron of Alexandria (fl. 1st century AD), on how heavy objects may be lifted.\n\nClock towers in Western Europe in the Middle Ages were also sometimes striking clocks. The most famous original still standing is possibly St Mark's Clock on the top of St Mark's Clocktower in St Mark's Square in Venice, assembled in 1493 by the clockmaker Gian Carlo Rainieri from Reggio Emilia. In 1497, Simone Campanato moulded the great bell on which every definite time-lapse is beaten by two mechanical bronze statues (h. 2,60 m.) called \"Due Mori\" (\"Two Moors\"), handling a hammer. Possibly earlier (1490) is the Prague Astronomical Clock by clockmaster Jan Růže (also called Hanuš) – according to another source this device was assembled as early as 1410 by clockmaker Mikuláš of Kadaň and mathematician Jan Šindel. The allegorical parade of animated sculptures rings on the hour every day.\n\nDuring the 11th century in the Song Dynasty, the Chinese astronomer, horologist and mechanical engineer Su Song created a water-driven astronomical clock for his clock tower of Kaifeng City. It incorporated an escapement mechanism as well as the earliest known endless power-transmitting chain drive, which drove the armillary sphere.\n\nContemporary Muslim astronomers also constructed a variety of highly accurate astronomical clocks for use in their mosques and observatories, such as the water-powered astronomical clock by Al-Jazari in 1206, and the astrolabic clock by Ibn al-Shatir in the early 14th century. The most sophisticated timekeeping astrolabes were the geared astrolabe mechanisms designed by Abū Rayhān Bīrūnī in the 11th century and by Muhammad ibn Abi Bakr in the 13th century. These devices functioned as timekeeping devices and also as calendars.\nA sophisticated water-powered astronomical clock was built by Al-Jazari in 1206. This castle clock was a complex device that was about high, and had multiple functions alongside timekeeping. It included a display of the zodiac and the solar and lunar paths, and a pointer in the shape of the crescent moon which travelled across the top of a gateway, moved by a hidden cart and causing doors to open, each revealing a mannequin, every hour. It was possible to reset the length of day and night in order to account for the changing lengths of day and night throughout the year. This clock also featured a number of automata including falcons and musicians who automatically played music when moved by levers operated by a hidden camshaft attached to a water wheel.\n\nThe earliest medieval European clockmakers were Catholic monks. Medieval religious institutions required clocks because they regulated daily prayer- and work-schedules strictly, using various types of time-telling and recording devices, such as water clocks, sundials and marked candles, probably in combination. When mechanical clocks came into use, they were often wound at least twice a day to ensure accuracy. Monasteries broadcast important times and durations with bells, rung either by hand or by a mechanical device, such as by a falling weight or by rotating beater.\n\nAlthough the mortuary inscription of Pacificus, archdeacon of Verona, records that he constructed a night clock (\"horologium nocturnum\") as early as 850, his clock has been identified as being an observation tube used to locate stars with an accompanying book of astronomical observations, rather than a mechanical or water clock, an interpretation supported by illustrations from medieval manuscripts.\n\nThe religious necessities and technical skill of the medieval monks were crucial factors in the development of clocks, as the historian Thomas Woods writes:\nThe appearance of clocks in writings of the 11th century implies that they were well known in Europe in that period. In the early 14th-century, the Florentine poet Dante Alighieri referred to a clock in his \"Paradiso\"; the first known literary reference to a clock that struck the hours. Giovanni da Dondi, Professor of Astronomy at Padua, presented the earliest detailed description of clockwork in his 1364 treatise \"Il Tractatus Astrarii\". This has inspired several modern replicas, including some in London's Science Museum and the Smithsonian Institution. Other notable examples from this period were built in Milan (1335), Strasbourg (1354), Lund (1380), Rouen (1389), and Prague (1462).\n\nSalisbury cathedral clock, dating from about 1386, is one of the oldest working clocks in the world, and may be the oldest. It still has most of its original parts, although its original verge and foliot timekeeping mechanism is lost, having been converted to a pendulum, which was replaced by a replica verge in 1956. It has no dial, as its purpose was to strike a bell at precise times. The wheels and gears are mounted in an open, box-like iron frame, measuring about square. The framework is held together with metal dowels and pegs. Two large stones, hanging from pulleys, supply the power. As the weights fall, ropes unwind from the wooden barrels. One barrel drives the main wheel, which is regulated by the escapement, and the other drives the striking mechanism and the air brake.\n\nNote also Peter Lightfoot's Wells Cathedral clock, constructed c. 1390. The dial represents a geocentric view of the universe, with the Sun and Moon revolving around a central fixed Earth. It is unique in having its original medieval face, showing a philosophical model of the pre-Copernican universe. Above the clock is a set of figures, which hit the bells, and a set of jousting knights who revolve around a track every 15 minutes. The clock was converted to pendulum-and-anchor escapement in the 17th century, and was installed in London's Science Museum in 1884, where it continues to operate. Similar astronomical clocks, or \"horologes\", survive at Exeter, Ottery St Mary, and Wimborne Minster.\nOne clock that has not survived is that of the Abbey of St Albans, built by the 14th-century abbot Richard of Wallingford. It may have been destroyed during Henry VIII's Dissolution of the Monasteries, but the abbot's notes on its design have allowed a full-scale reconstruction. As well as keeping time, the astronomical clock could accurately predict lunar eclipses, and may have shown the Sun, Moon (age, phase, and node), stars and planets, as well as a wheel of fortune, and an indicator of the state of the tide at London Bridge. According to Thomas Woods, \"a clock that equaled it in technological sophistication did not appear for at least two centuries\". Giovanni de Dondi was another early mechanical clockmaker whose clock did not survive, but his work has been replicated based on the designs. De Dondi's clock was a seven-faced construction with 107 moving parts, showing the positions of the Sun, Moon, and five planets, as well as religious feast days. Around this period, mechanical clocks were introduced into abbeys and monasteries to mark important events and times, gradually replacing water clocks which had served the same purpose.\n\nDuring the Middle Ages, clocks primarily served religious purposes; the first employed for secular timekeeping emerged around the 15th century. In Dublin, the official measurement of time became a local custom, and by 1466 a public clock stood on top of the Tholsel (the city court and council chamber). It was the first of its kind to be clearly recorded in Ireland, and would only have had an hour hand. The increasing lavishness of castles led to the introduction of turret clocks. A 1435 example survives from Leeds castle; its face is decorated with the images of the Crucifixion of Jesus, Mary and St George.\n\nEarly clock dials showed hours: the display of minutes and seconds evolved later. A clock with a minutes dial is mentioned in a 1475 manuscript, and clocks indicating minutes and seconds existed in Germany in the 15th century. Timepieces which indicated minutes and seconds were occasionally made from this time on, but this was not common until the increase in accuracy made possible by the pendulum clock and, in watches, by the spiral balance spring. The 16th-century astronomer Tycho Brahe used clocks with minutes and seconds to observe stellar positions.\n\nThe Ottoman engineer Taqi al-Din described a weight-driven clock with a verge-and-foliot escapement, a striking train of gears, an alarm, and a representation of the moon's phases in his book \"The Brightest Stars for the Construction of Mechanical Clocks\" (\"Al-Kawākib al-durriyya fī wadh' al-bankāmat al-dawriyya\"), written around 1556.\n\nThe concept of the wristwatch goes back to the production of the very earliest watches in the 16th century. Elizabeth I of England received a wristwatch from Robert Dudley in 1571, described as an arm watch. From the beginning, wrist watches were almost exclusively worn by women, while men used pocket-watches up until the early 20th century. This was not just a matter of fashion or prejudice; watches of the time were notoriously prone to fouling from exposure to the elements, and could only reliably be kept safe from harm if carried securely in the pocket. When the waistcoat was introduced as a manly fashion at the court of Charles II in the 17th century, the pocket watch was tucked into its pocket. Prince Albert, the consort to Queen Victoria, introduced the 'Albert chain' accessory, designed to secure the pocket watch to the man's outergarment by way of a clip. By the mid nineteenth century, most watchmakers produced a range of wristwatches, often marketed as bracelets, for women.\n\nWristwatches were first worn by military men towards the end of the nineteenth century, when the importance of synchronizing manoeuvres during war without potentially revealing the plan to the enemy through signalling was increasingly recognized. It was clear that using pocket watches while in the heat of battle or while mounted on a horse was impractical, so officers began to strap the watches to their wrist. The Garstin Company of London patented a 'Watch Wristlet' design in 1893, although they were probably producing similar designs from the 1880s. Clearly, a market for men's wristwatches was coming into being at the time. Officers in the British Army began using wristwatches during colonial military campaigns in the 1880s, such as during the Anglo-Burma War of 1885.\n\nDuring the Boer War, the importance of coordinating troop movements and synchronizing attacks against the highly mobile Boer insurgents was paramount, and the use of wristwatches subsequently became widespread among the officer class. The company Mappin & Webb began production of their successful 'campaign watch' for soldiers during the campaign at the Sudan in 1898 and ramped up production for the Boer War a few years later.\nThese early models were essentially standard pocket-watches fitted to a leather strap, but by the early 20th century, manufacturers began producing purpose-built wristwatches. The Swiss company, Dimier Frères & Cie patented a wristwatch design with the now standard wire lugs in 1903. In 1904, Alberto Santos-Dumont, an early aviator, asked his friend, a French watchmaker called Louis Cartier, to design a watch that could be useful during his flights.\n\nThe impact of the First World War dramatically shifted public perceptions on the propriety of the man's wristwatch, and opened up a mass market in the post-war era. The creeping barrage artillery tactic, developed during the War, required precise synchronization between the artillery gunners and the infantry advancing behind the barrage. Service watches produced during the War were specially designed for the rigours of trench warfare, with luminous dials and unbreakable glass. Wristwatches were also found to be needed in the air as much as on the ground: military pilots found them more convenient than pocket watches for the same reasons as Santos-Dumont had. The British War Department began issuing wristwatches to combatants from 1917.\nThe company H. Williamson Ltd., based in Coventry, was one of the first to capitalize on this opportunity. During the company's 1916 AGM it was noted that \"...the public is buying the practical things of life. Nobody can truthfully contend that the watch is a luxury. It is said that one soldier in every four wears a wristlet watch, and the other three mean to get one as soon as they can.\" By the end of the War, almost all enlisted men wore a wristwatch, and after they were demobilized, the fashion soon caught on – the British \"Horological Journal\" wrote in 1917 that \"...the wristlet watch was little used by the sterner sex before the war, but now is seen on the wrist of nearly every man in uniform and of many men in civilian attire.\" Within a decade, sales of wristwatches had outstripped those of pocket watches.\n\nIn the late 17th and 18th Centuries, equation clocks were made, which allowed the user to see or calculate apparent solar time, as would be shown by a sundial. Before the invention of the pendulum clock, sundials were the only accurate timepieces. When good clocks became available, they appeared inaccurate to people who were used to trusting sundials. The annual variation of the equation of time made a clock up to about 15 minutes fast or slow, relative to a sundial, depending on the time of year. Equation clocks satisfied the demand for clocks that always agreed with sundials. Several types of equation clock mechanism were devised. which can be seen in surviving examples, mostly in museums.\n\nInnovations to the mechanical clock continued, with miniaturization leading to domestic clocks in the 15th century, and personal watches in the 16th. In the 1580s, the Italian polymath Galileo Galilei investigated the regular swing of the pendulum, and discovered that it could be used to regulate a clock. Although Galileo studied the pendulum as early as 1582, he never actually constructed a clock based on that design. The first pendulum clock was designed and built by Dutch scientist Christiaan Huygens, in 1656. Early versions erred by less than one minute per day, and later ones only by 10 seconds, very accurate for their time.\n\nIn England, the manufacturing of pendulum clocks was soon taken up. The longcase clock (also known as the grandfather clock) was first created to house the pendulum and works by the English clockmaker William Clement in 1670 or 1671; this became feasible after Clement invented the anchor escapement mechanism in about 1670. Before then, pendulum clocks used the older verge escapement mechanism, which required very wide pendulum swings of about 100°. To avoid the need for a very large case, most clocks using the verge escapement had a short pendulum. The anchor mechanism, however, reduced the pendulum's necessary swing to between 4° to 6°, allowing clockmakers to use longer pendulums with consequently slower beats. These required less power to move, caused less friction and wear, and were more accurate than their shorter predecessors. Most longcase clocks use a pendulum about a metre (39 inches) long to the center of the bob, with each swing taking one second. This requirement for height, along with the need for a long drop space for the weights that power the clock, gave rise to the tall, narrow case.\n\nClement also introduced the pendulum suspension spring in 1671. The concentric minute hand was added to the clock by Daniel Quare, a London clock-maker, and the Second Hand was introduced.\n\nThe Jesuits were another major contributor to the development of pendulum clocks in the 17th and 18th centuries, having had an \"unusually keen appreciation of the importance of precision\". In measuring an accurate one-second pendulum, for example, the Italian astronomer Father Giovanni Battista Riccioli persuaded nine fellow Jesuits \"to count nearly 87,000 oscillations in a single day\". They served a crucial role in spreading and testing the scientific ideas of the period, and collaborated with contemporary scientists, such as Huygens.\n\nThe invention of the mainspring in the early 15th century allowed portable clocks to be built, evolving into the first pocketwatches by the 17th century, but these were not very accurate until the balance spring was added to the balance wheel in the mid 17th century. Some dispute remains as to whether British scientist Robert Hooke (his was a straight spring) or Dutch scientist Christiaan Huygens was the actual inventor of the balance spring. Huygens was clearly the first to use a spiral balance spring, the form used in virtually all watches to the present day. The addition of the balance spring made the balance wheel a harmonic oscillator like the pendulum in a pendulum clock, which oscillated at a fixed resonant frequency and resisted oscillating at other rates. This innovation increased watches' accuracy enormously, reducing error from perhaps several hours per day to perhaps 10 minutes per day, resulting in the addition of the minute hand to the watch face around 1680 in Britain and 1700 in France.\n\nLike the invention of pendulum clock, Huygens' spiral hairspring (balance spring) system of portable timekeepers, helped lay the foundations for the modern watchmaking industry. The application of the spiral balance spring for watches ushered in a new era of accuracy for portable timekeepers, similar to that which the pendulum had introduced for clocks. From its invention in 1675 by Christiaan Huygens, the spiral hairspring (balance spring) system for portable timekeepers, still used in mechanical watchmaking industry today.\n\nIn 1675, Huygens and Robert Hooke invented the spiral balance, or the hairspring, designed to control the oscillating speed of the balance wheel. This crucial advance finally made accurate pocket watches possible. This resulted in a great advance in accuracy of pocket watches, from perhaps several hours per day to 10 minutes per day, similar to the effect of the pendulum upon mechanical clocks. The great English clockmaker, Thomas Tompion, was one of the first to use this mechanism successfully in his pocket watches, and he adopted the minute hand which, after a variety of designs were trialled, eventually stabilised into the modern-day configuration.\n\nThe Rev. Edward Barlow invented the rack and snail striking mechanism for striking clocks, which was a great improvement over the previous mechanism. The repeating clock, that chimes the number of hours (or even minutes) was invented by either Quare or Barlow in 1676. George Graham invented the deadbeat escapement for clocks in 1720.\n\nMarine chronometers are clocks used at sea as time standards, to determine longitude by celestial navigation.\nA major stimulus to improving the accuracy and reliability of clocks was the importance of precise time-keeping for navigation. The position of a ship at sea could be determined with reasonable accuracy if a navigator could refer to a clock that lost or gained less than about 10 seconds per day. The marine chronometer would have to keep the time of a fixed location—usually Greenwich Mean Time—allowing seafarers to determine longitude by comparing the local high noon to the clock. This clock could not contain a pendulum, which would be virtually useless on a rocking ship.\nAfter the Scilly naval disaster of 1707 where four ships ran aground due to navigational mistakes, the British government offered a large prize of £20,000, equivalent to millions of pounds today, for anyone who could determine longitude accurately. The reward was eventually claimed in 1761 by Yorkshire carpenter John Harrison, who dedicated his life to improving the accuracy of his clocks.\n\nIn 1735 Harrison built his first chronometer, which he steadily improved on over the next thirty years before submitting it for examination. The clock had many innovations, including the use of bearings to reduce friction, weighted balances to compensate for the ship's pitch and roll in the sea and the use of two different metals to reduce the problem of expansion from heat.\n\nThe chronometer was trialled in 1761 by Harrison's son and by the end of 10 weeks the clock was in error by less than 5 seconds.\n\nIn 1815, Sir Francis Ronalds (1788-1873) of London published the forerunner of the electric clock, the electrostatic clock. It was powered with dry piles, a high voltage battery with extremely long life but the disadvantage of its electrical properties varying with the weather. He trialled various means of regulating the electricity and these models proved to be reliable across a range of meteorological conditions.\n\nAlexander Bain, a Scottish clock and instrument maker, was the first to invent and patent the electric clock in 1840. On January 11, 1841, Alexander Bain along with John Barwise, a chronometer maker, took out another important patent describing a clock in which an electromagnetic pendulum and an electric current is employed to keep the clock going instead of springs or weights. Later patents expanded on his original ideas.\n\nThe piezoelectric properties of crystalline quartz were discovered by Jacques and Pierre Curie in 1880. The first quartz crystal oscillator was built by Walter G. Cady in 1921, and in 1927 the first quartz clock was built by Warren Marrison and J. W. Horton at Bell Telephone Laboratories in Canada. The following decades saw the development of quartz clocks as precision time measurement devices in laboratory settings—the bulky and delicate counting electronics, built with vacuum tubes, limited their practical use elsewhere. In 1932, a quartz clock able to measure small weekly variations in the rotation rate of the Earth was developed. The National Bureau of Standards (now NIST) based the time standard of the United States on quartz clocks from late 1929 until the 1960s, when it changed to atomic clocks. In 1969, Seiko produced the world's first quartz wristwatch, the Astron. Their inherent accuracy and low cost of production has resulted in the subsequent proliferation of quartz clocks and watches.\n\nAtomic clocks are the most accurate timekeeping devices in practical use today. Accurate to within a few seconds over many thousands of years, they are used to calibrate other clocks and timekeeping instruments.\n\nThe idea of using atomic transitions to measure time was first suggested by Lord Kelvin in 1879, although it was only in the 1930s with the development of Magnetic resonance that there was a practical method for doing this. A prototype ammonia maser device was built in 1949 at the U.S. National Bureau of Standards (NBS, now NIST). Although it was less accurate than existing quartz clocks, it served to demonstrate the concept.\nThe first accurate atomic clock, a caesium standard based on a certain transition of the caesium-133 atom, was built by Louis Essen in 1955 at the National Physical Laboratory in the UK. Calibration of the caesium standard atomic clock was carried out by the use of the astronomical time scale \"ephemeris time\" (ET).\n\nThe International System of Units standardized its unit of time, the second, on the properties of cesium in 1967. SI defines the second as 9,192,631,770 cycles of the radiation which corresponds to the transition between two electron spin energy levels of the ground state of the Cs atom. The cesium atomic clock, maintained by the National Institute of Standards and Technology, is accurate to 30 billionths of a second per year. Atomic clocks have employed other elements, such as hydrogen and rubidium vapor, offering greater stability—in the case of hydrogen clocks—and smaller size, lower power consumption, and thus lower cost (in the case of rubidium clocks).\n\nThe first professional clockmakers came from the guilds of locksmiths and jewellers. Clockmaking developed from a specialized craft into a mass production industry over many years.\n\nParis and Blois were the early centres of clockmaking in France. French clockmakers such as Julien Le Roy, clockmaker of Versailles, were leaders in case design and ornamental clocks. Le Roy belonged to the fifth generation of a family of clockmakers, and was described by his contemporaries as \"the most skillful clockmaker in France, possibly in Europe\". He invented a special repeating mechanism which improved the precision of clocks and watches, a face that could be opened to view the inside clockwork, and made or supervised over 3,500 watches. The competition and scientific rivalry resulting from his discoveries further encouraged researchers to seek new methods of measuring time more accurately.\n\nBetween 1794 and 1795, in the aftermath of the French Revolution, the French government briefly mandated decimal clocks, with a day divided into 10 hours of 100 minutes each. The astronomer and mathematician Pierre-Simon Laplace, among other individuals, modified the dial of his pocket watch to decimal time. A clock in the Palais des Tuileries kept decimal time as late as 1801, but the cost of replacing all the nation's clocks prevented decimal clocks from becoming widespread. Because decimalized clocks only helped astronomers rather than ordinary citizens, it was one of the most unpopular changes associated with the metric system, and it was abandoned.\n\nIn Germany, Nuremberg and Augsburg were the early clockmaking centers, and the Black Forest came to specialize in wooden cuckoo clocks.\nThe English became the predominant clockmakers of the 17th and 18th centuries. The main centres of the British industry were in the City of London, the West End of London, Soho where many skilled French Huguenots settled and later in Clerkenwell. The Worshipful Company of Clockmakers was established in 1631 as one of the Livery Companies of the City of London.\n\nThomas Tompion was the first English clockmaker with an international reputation and many of his pupils went on to become great horologists in their own right, such as George Graham who invented the deadbeat escapement, orrery and mercury pendulum, and his pupil Thomas Mudge who created the first lever escapement. Famous clockmakers of this period included Joseph Windmills, Simon de Charmes who established the De Charmes clockmaker firm and Christopher Pinchbeck who invented the alloy pinchbeck.\n\nLater famous horologists included John Arnold who made the first practical and accurate modern watch by refining Harrison's chronometer, Thomas Earnshaw who was the first to make these available to the public, Daniel Quare, who invented a repeating watch movement, a portable barometer and introduced the concentric minute hand.\n\nQuality control and standards were imposed on clockmakers by the Worshipful Company of Clockmakers, a guild which licensed clockmakers for doing business. By the rise of consumerism in the late 18th century, clocks, especially pocket watches, became regarded as fashion accessories and were made in increasingly decorative styles. By 1796, the industry reached a high point with almost 200,000 clocks being produced annually in London, however by the mid-19th century the industry had gone into steep decline from Swiss competition.\n\nSwitzerland established itself as a clockmaking center following the influx of Huguenot craftsmen, and in the 19th century, the Swiss industry \"gained worldwide supremacy in high-quality machine-made watches\". The leading firm of the day was Patek Philippe, founded by Antoni Patek of Warsaw and Adrien Philippe of Bern.\n\n\n\n\n"}
{"id": "27761763", "url": "https://en.wikipedia.org/wiki?curid=27761763", "title": "IPhone (1st generation)", "text": "IPhone (1st generation)\n\nThe iPhone is the first smartphone model designed and marketed by Apple Inc. After years of rumors and speculation, it was officially announced on January 9, 2007, and was later released in the United States on June 29, 2007. It featured quad-band GSM cellular connectivity with GPRS and EDGE support for data transfer.\n\nOn June 9, 2008, Apple announced its successor, the iPhone 3G. After this announcement, the first-generation iPhone became referred to by some sources as the iPhone 2G due to the fact that it was the only iPhone to solely support 2G-speed networks. The original iPhone has not received software updates from Apple since iPhone OS (now iOS) 3.1.3.\n\nSince June 2013, the original iPhone is considered \"vintage\" by some service providers in the US, and \"obsolete\" in Apple retail stores and all other regions. Apple does not service vintage or obsolete products, and replacement parts for obsolete products are not available to service providers.\n\nIn 2005, Apple CEO Steve Jobs conceived an idea of using a multi-touch touchscreen to interact with a computer in a way in which he could directly type onto the display. He decided that it needed to have a triple layered touch screen, a very new and high tech technology at the time. This helped out with removing the physical keyboard and mouse, the same as a tablet computer. Jobs recruited a group of Apple engineers to investigate the idea as a side project. When Jobs reviewed the prototype and its user interface, he conceived a second idea of implementing the technology onto a mobile phone. The whole effort was called Project Purple 2 and began in 2005.\n\nApple created the device during a secretive and unprecedented collaboration with AT&T, formerly Cingular Wireless. The development cost of the collaboration was estimated to have been $150 million over a thirty-month period. Apple rejected the \"design by committee\" approach that had yielded the Motorola ROKR E1, a largely unsuccessful collaboration with Motorola. Instead, Cingular Wireless gave Apple the liberty to develop the iPhone's hardware and software in-house.\n\nThe original iPhone was introduced by Steve Jobs on January 9, 2007 in a keynote address at the Macworld Conference & Expo held in Moscone West in San Francisco, California. In his address, Jobs said, \"This is a day, that I have been looking forward to for two and a half years\", and that \"today, Apple is going to reinvent the phone.\" Jobs introduced the iPhone as a combination of three devices: a \"widescreen iPod with touch controls\"; a \"revolutionary mobile phone\"; and a \"breakthrough Internet communicator\".\n\nSix weeks before the iPhone was to be released, the plastic screen was replaced with a glass one, after Jobs was upset that the screen of the prototype he was carrying in his pocket had been scratched by his keys. The quick switch led to a bidding process for a manufacturing contractor that was won by Foxconn, which had just opened up a new wing of its Shenzhen factory complex specifically for this bid.\n\nIn February 2007, LG Electronics accused Apple of \"copying\" their LG Prada phone, which was introduced around the same time as iPhone.\n\nThe iPhone was released in the United States on June 29, 2007 at the price of $499 for the 4 GB model and $599 for the 8 GB model, both requiring a 2-year contract. Thousands of people were reported to have waited outside Apple and AT&T retail stores days before the device's launch; many stores reported stock shortages within an hour of availability. To avoid repeating the problems of the PlayStation 3 launch, which caused burglaries and even a shooting, off-duty police officers were hired to guard stores overnight.\n\nIt was later made available in the United Kingdom, France, Germany, Portugal, the Republic of Ireland, and Austria in November 2007.\n\nSix out of ten Americans surveyed said they knew the iPhone was coming before its release.\n\nThe iPhone's main competitors in both consumer and business markets were considered to be the LG Prada, LG Viewty, Samsung Ultra Smart F700, Nokia N95, Nokia E61i, Palm Treo 750, Palm Centro, HTC Touch, Sony Ericsson W960 and BlackBerry.\n\nThe iPod Touch, a touchscreen device with the media and internet abilities and interface of the iPhone but without the ability to connect to a cellular network for phone functions or internet access, was released on September 5, 2007. At the same time, Apple significantly dropped the price of the 8 GB model (from $599 to $399, still requiring a 2-year contract with AT&T) while discontinuing the 4 GB model. Apple sold the one millionth iPhone five days later, or 74 days after the release. After receiving \"hundreds of emails...upset\" about the price drop, Apple gave store credit to early adopters.\n\nA 16 GB model was released on February 5, 2008 for $499, the original launch price of the 4 GB model. Apple released an SDK on March 6, 2008, allowing developers to create the apps that would be available starting in iPhone OS version 2.0, a free upgrade for iPhone users. On June 9, Apple announced the iPhone 3G, which began shipping July 11. The original iPhone was discontinued 4 days later; total sales volume came to 6,124,000 units.\n\nDuring release, the iPhone was marketed as running \"OS X\". The name of the operating system was revealed as iPhone OS with the release of the iPhone SDK. The original iPhone supported three major versions of the operating system before it was discontinued: iPhone OS 1, 2, and 3. However, the full iPhone OS 3 feature set was not supported, and the last update the original iPhone received was iPhone OS 3.1.3\n\nThe original operating system for the original iPhone was iPhone OS 1, marketed as OS X, and included Visual Voicemail, multi-touch gestures, HTML email, Safari web browser, threaded text messaging, and YouTube. However, many features like MMS, apps, and copy and paste were not supported at release, leading hackers jailbreaking their phones to add these features. Official software updates slowly added these features.\n\niPhone OS 2 was released on July 11, 2008, around the same time as the release of the iPhone 3G, and introduced third-party applications, Microsoft Exchange support, push e-mail, and other enhancements.\n\niPhone OS 3 was released on June 17, 2009, and introduced copy and paste functionality, Spotlight search for the home screen, and new features for the YouTube app. iPhone OS 3 was available for the original iPhone as well as the iPhone 3G. However, not all features of iPhone OS 3 were supported on the original iPhone.\n\niPhone OS 3.1.3 was the last version of iPhone OS (now iOS) to be released for the original iPhone.\n\nOnly four writers were given review models of the original iPhone: David Pogue of \"The New York Times\", Walt Mossberg of \"The Wall Street Journal\", Steven Levy of \"Newsweek\", and Ed Baig of \"USA Today\". \"The New York Times\" and \"The Wall Street Journal\" published positive, but cautious, reviews of the iPhone, their primary criticisms being the relatively slow speed of the AT&T's 2.5G EDGE network and the phone's inability to connect using 3G services. \"The Wall Street Journal\"s technology columnist, Walt Mossberg, concluded that \"despite some flaws and feature omissions, the iPhone is, on balance, a beautiful and breakthrough handheld computer.\" \"Time\" magazine named it the Invention of the Year in 2007.\n\n\"Mobile Gazette\" reported that whilst the iPhone has many impressive points, it equally has many bad ones too, noting the lack of 3G, MMS, third-party applications, and its weak camera without autofocus and flash.\n\n\n"}
{"id": "16723782", "url": "https://en.wikipedia.org/wiki?curid=16723782", "title": "Jackshaft", "text": "Jackshaft\n\nA jackshaft, also called a \"countershaft\", is a common mechanical design component used to transfer or synchronize rotational force in a machine. A jackshaft is often just a short stub with supporting bearings on the ends and two pulleys, gears, or cranks attached to it. In general, a jackshaft is any shaft that is used as an intermediary transmitting power from a driving shaft to a driven shaft.\n\nThe oldest uses of the term \"jackshaft\" appear to involve shafts that were intermediate between water wheels or stationary steam engines and the line shafts of 19th century mills. In these early sources from New England mills in 1872 and 1880, the term \"jack shaft\" always appears in quotes. Another 1872 author wrote \"Gear wheels are used in England to transmit the power of the engine to what is usually called the jack shaft.\"\nBy 1892, the quotes were gone, but the use remained the same.\n\nThe pulleys on the jackshafts of mills or power plants were frequently connected to the shaft with clutches. For example, in the 1890s, the generating room of the Virginia Hotel in Chicago had two Corliss engines and five dynamos, linked through a jackshaft. Clutches on the jackshaft pulleys allowed any or all of the dynamos to be driven by either or both of the engines. With the advent of chain-drive vehicles, the term \"jackshaft\" was generally applied to the final intermediate shaft in the drive train, either a chain driven shaft driving pinions that directly engaged teeth on the inside of the rims of the drive wheels, or the output shaft of the transmission/differential that is linked by chain to the drive wheels.\n\nOne of the first uses of the term \"jackshaft\" in the context of railroad equipment was in an 1890 patent application by Samuel Mower. In his electric-motor driven railroad truck, the motor was geared to a jackshaft mounted between the side frames. A sliding Dog clutch inside the jackshaft was used to select one of several gear ratios on the chain drive to the driven axle. Later railroad jackshafts were generally connected to the driving wheels using side rods; see Jackshaft (locomotive) for details.\nThe term \"countershaft\" is somewhat older. In 1828, the term was used to refer to an intermediate horizontal shaft in a gristmill driven through gearing by the waterwheel and driving the millstones through bevel gears. An 1841 textbook used the term to refer to a short shaft driven by a belt from the line shaft and driving the spindle of a lathe through additional belts. The countershaft and the lathe spindle each carried cones of different-diameter pulleys for speed control. In 1872, this definition was given: \"The term countershaft is applied to all shafts driven from the main line <nowiki>[shaft]</nowiki> when placed at or near the machines to be driven ...\"\nModern jackshafts and countershafts are often hidden inside large machinery as components of the larger overall device.\nIn farm equipment, a spinning output shaft on the rear of the vehicle is commonly referred to as the \"Power Take-Off\" or PTO, and the power-transfer shaft that is connected to it is commonly called a \"PTO shaft\", but is also a jackshaft.\n\n"}
{"id": "49298589", "url": "https://en.wikipedia.org/wiki?curid=49298589", "title": "Just10", "text": "Just10\n\nJust10 Incorporated (Just10), is an advertising-free private social network where users are limited to having just 10 friends, and all content, posts, and messages are strictly private, and are permanently deleted in 10 days. The company was started by privacy advocate and technology entrepreneur Frederick Ghahramani.\n\n"}
{"id": "48050093", "url": "https://en.wikipedia.org/wiki?curid=48050093", "title": "Learning nugget", "text": "Learning nugget\n\nLearning nuggets is a standalone mini learning activity, usually less than 5 minutes in length, that would vary in size and scope that learners undertake in a particular context in order to attain specific learning outcomes A learning nugget task will take a prescribed length of time and may, or may not be assessed. Nuggets should be designed with a particular approach to learning and teaching in mind (Conole & Fill, 2005)\n\nLearning nuggets are the essential elements of the Subscription Learning approach. In this context, the learning happens through a stream of intermittent nuggets which involves a variety of learning-related events which include \"content presentation, diagnostics, scenario-based questions, job aids, reflection questions, assignments, discussions, etc. The nuggets are delivered to the learner in many format like email, text message, smart-phone notifications, or any other form of prompting. They are designed to be delivered on predetermined intervals to support learning. The series of learning nuggets are called learning threads. For utmost effective learning, sending a learning nugget could be dynamically triggered by many factors like learners' leaning need, results of a learning assessment or learners' performance.\n\nFujitsu and MIT described some examples of Learning Nuggets as follows:\n\n\n"}
{"id": "31824565", "url": "https://en.wikipedia.org/wiki?curid=31824565", "title": "List of devices with Gorilla Glass", "text": "List of devices with Gorilla Glass\n\nWith the expanded use of touchscreen mobile phones, mobile makers started looking for scratch-resistant solutions for the bigger mobile displays. Here is a list of mobile phones and tablets that make use of the Gorilla Glass technology developed by Corning Incorporated from various makes: Though this list is sourced from Corning's website, there is a statement on that site indicating that this list is not necessarily comprehensive, as some companies have contractual arrangements with Corning that prohibit listing of said companies' products.\n\n\n"}
{"id": "5715706", "url": "https://en.wikipedia.org/wiki?curid=5715706", "title": "List of silicon producers", "text": "List of silicon producers\n\n\n\nA partial list of major producers of wafers (made of high purity silicon, mono- or polycrystalline) includes:\n\n\n"}
{"id": "7697949", "url": "https://en.wikipedia.org/wiki?curid=7697949", "title": "List of technology centers", "text": "List of technology centers\n\nThis is a list of technology centers throughout the world. Governmental planners and business networks like to use the name \"silicon\" or \"valley\" to describe their own areas as a result of the success of Silicon Valley in California. Nevertheless, there are a few qualitative differences between these places, and metrics may be applied to measure their dominance.\n\nThese metrics include:\n\n\nCameroon\n\nEgypt\n\nKenya\n\nMauritius\n\nMorocco\n\nSouth Africa\n\nZambia\n\nBolivia\n\nBrazil\n\nCanada\nChile\nGuatemala\n\nMexico\n\nUnited States\n\nChina\n\nHong Kong\n\nIndia\n\nIran\n\n\nIsrael\n\nJapan\n\nMalaysia\n\nMyanmar\n\n\nPakistan\n\nPhilippines\n\nQatar\n\nSaudi Arabia\n\nSingapore\n\nSouth Korea\n\nTaiwan\n\n\nThailand\n\nUnited Arab Emirates\n\nVietnam\n\n\nAustria\n\nBelarus\n\nCzech Republic\n\nFinland\n\nFrance\nGermany\n\nHungary\nIreland\n\nItaly\n\nNetherlands\n\nPortugal\n\nRussia\n\nRomania\n\nSlovakia\n\nSpain\n\nSweden\n\nTurkey\n\nUnited Kingdom\n\n\nUkraine\n\n\nThe following list contains places with \"Silicon\" names, that is, places with nicknames inspired by the \"Silicon Valley\" nickname given to part of the San Francisco Bay Area:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUkraine\n\n\n\n\n\n\n"}
{"id": "5932556", "url": "https://en.wikipedia.org/wiki?curid=5932556", "title": "List of wireless community networks by region", "text": "List of wireless community networks by region\n\nSome wireless community network projects are:\n\nMesh Bukavu: A project of News for Peace, which among other things runs Radio Maendeleo, a community radio station. It is the result of a collaboration of Free Press, a Dutch organization, and was bootstrapped with funding and training from the Open Technology Institute (OTI). The training took place in November 2014 and the network was deployed in January 2015. It has a strong emphasis on local hosting of content (Wikipedia, blogs, audio lessons, e-books), and also other local services like a local chat. It is valued also as a standby when official net shutdowns occur, e.g. at election times.\n\nThe network, which is still operational, consists of 10-15 nodes, (Ubiquiti NanoStation and Rockets) running Commotion firmware. Equipment is mounted on rooftops of participant organisations especially if they have backup electricity (solar is being phased in).using-their-own-mesh-network scratch/ \n\nMesh Goma: An experiment of a local organization, the Collective of Community Radio and TV in North Kivu (CORACON), in partnership with Free Press Unlimited and inspired by Mesh Bukavu. It also received Seed funding from the OTI.\n\nThe network was initially deployed in January 2015 with the idea of providing access to information to the areas in the city of Goma which lacked this access. It consisted of 15 nodes (14 Ubiquiti and 1 Tp-Link). It is no longer operational due to the problems with access and costs of reliable electricity in Goma, and the lack of digital stewards keen to work voluntarily in maintaining the network. Additionally, the network was only providing intranet services, which did not make it attractive to the majority of the population in Goma. \n\nPamoja Net (pamoja means 'together' in Swahili) is a local mesh on the island of Idjwi in Lake Kivu, between the DRC and Rwanda. It reports 200 regular WiFi users, as well as institutions such as a radio station, the police, and the electoral commission. It also has a public display screen and tablets for casual users in a kiosk. Its backhaul is via line-of-sight links to Bukavu, where another community network is located (although Pamoja's gateway is via an internet cafe there).\n\nThe network was created in 2015 by Project First Light, a partnership of NGOs and businesses. The partnership still oversees operations, although it has trained local \"guardians\" to conduct day-to-day running. In terms of governance, the local traditional leader (who originally requested internet connectivity) is committed to operating Pamoja as a commons.\n\nAkwapim Community Wireless Network: installed and maintained by a small group of volunteers associated with the Apirede Community Resource center. Both the resource centre and the community network are projects of the Community-Based Libraries and Information Technology (CBLit), a non-government organization based in both Ghana and the United States. It started in 2005 in response to the local community’s requests for connectivity to help them break their isolation, as an extension of a public library initiative, with the support of the US Peace Corps. Its first phase had 10 nodes, and the second was to have another 10. The second phase was also to use a V-Sat link. Most nodes use old PCs as routers, with new WiFi cards (Dell OptiPlex units and D-Link DWLG520 cards). Its website is down and no information after 2009 can be found online.\n\nTunapandanet: This was started and funded in the last 5 years by an educational-development NGO (Tunapanda Institute, which got early funding via Indiegogo, and had high input from American “backpacker” volunteers). The network serves to further its outreach (educational activities center on Edubuntu thin client system) into large, high-density slum. Of particular interest is the organization’s emphasis on cached/recorded content to avoid external data costs. Base station and 3 nodes: Ubiquity PicoStation (short range, omnidirectional), Ubiquiti NanoStation (medium range, directional), Ubiquiti Rocket. Network likely expanding as the organisation is active.\n\nConnecting Eenhana: This was created in 2015 by a partnership between the University of Namibia and the Glowdom Educational Foundation with a grant from the OTI to support learning amongst community members of the small town of Eenhana and surrounding villages. In particular, it aimed at supporting the generation and sharing local content and to increase access of schools to educational content, including for learners and students at a Special school for Deaf learners. Additionally, the local content creation was extended to provide local government information, as well as transparency and accountability of local government It connects 7 sites using Ubiquiti routers, indoor coverage is provided using Mesh Potatoes to provide VoIP services too. All the network runs SECN firmware. It is only partly functional at the moment, due to equipment failure due to overheating and difficult terrain (very flat and with tall trees, which prevent Line of Sight between the nodes).\n\nFantsuam Foundation: Started in 2009 with SEED funding and having only 2 nodes, ZittNet is a department of Fantsuam Foundation (an NGO in Kaduna), and focuses mainly on ICT training; nonetheless it was honored as Nigeria’s first rural ISP. It was also intended to provide rural students with access to (downloaded) offline study materials. It started off having a VSat connection but due to cost is trying to replace this with a fiber connection. It notably uses solar backup to maintain service in the absence of reliable grid power.\n\nIbadan WUG: Started by one of the Mesh Potato project’s earlier contributors in a residential precinct in Ibadan. It is still actively providing connectivity largely to students. It consists of 22 Mesh Potatoes.\n\nAbaarso: Initiated by an American working as ICT instructor in Somalia to serve the Abaarso School of Science and Technology due to poor and at times nonexistent internet. Also involved some local cloud hosting. Used Ubiquity & Commotion. Current status unknown.\n\nSiyakhula Living Labs: It started in 2005 involving the Telkom Centre of Excellence at two universities, Rhodes and Fort Hare. The first network was intended to provide exposure to international markets to the local arts and craft entrepreneurs within the Dwesa community; and started off with 3 nodes at schools, consisting of WiMAX backbone with WIFI hotspots around each node, and a VSAT backhaul and later growing to at least 14 nodes. From the initial offering of e-commerce services this grew to providing information and communication services (including telephony services, emailing, school administration, etc.) both for the schools and the surrounding communities. The network grew from the three nodes to about 14+ (might have actually been 17 schools at the end) and offering a whole plethora of services to the schools and the communities. The network is still operational but barely, largely due to funding challenge. The second network was started by a researcher from the University of Fort Hare (UFH) who deployed a community wireless mesh network consisting of seven nodes using nanostations m2 (3) and picostations m2-hp (4) connected through a VSAT sponsored by Sifunda Kunye Educational Foundation in a location called Ntselamanzi, Its operation is linked to a research project by a student at UFH, which meant that its active management and administration has slowed down since the student completed his work. Still, it has plans to be extended to neighbouring communities.\n\nRural Telehealth: The University of Western Cape led a series of rural wireless projects, lasting several years each, between 2003-2012, connecting remote hospitals and clinics in the Eastern Cape province. There was also an initial try at rural community networks, even mesh. These involved long range (up to 15 km) line-of-sight WiFi links, in both 2.4 and 5 GHz. At times, they were connected to one or two expensive VSAT links. All were powered by deep cycle 12v batteries, charged either by solar panels or trickle charged from unreliable mains. Despite robust technical performance, the networks and apps created for them were not fully utilised mainly due to social reasons, e.g. power relations, suspicion of motives and the single champion problem.\n\nPeebles Valley Mesh Network: Centred on a clinic dealing with AIDS patients, where a VSat connection provided a gateway for a mesh of 6 nodes serving the surrounding area including a school. Supported by First Mile First Inch, a collaboration including Meraka and various academic and development agencies, mainly funded by IDRC. Lapsed due to high cost of VSat and lack of continued support.\n\nBo-Kaap: A now-defunct “testbed” experiment in a historic inner-city precinct involving 75 of the early Mesh Potatoes and an internet gateway, funded by the Shuttleworth Foundation. Unfortunately technology was prioritised over human capacity-building; problems with the equipment combined with lack of community buy-in led to the project lapsing.\n\nOrange Farm: Another pilot project in the development of the Village Telco technology/business model, in a township near Johannesburg. Social enterprise Dabba installed Mesh Potatoes and cheap VoIP handsets. However the network seems to have lapsed after the rapid proliferation of cellular telephony created a more powerful “network effect” so the micro-entrepreneurs Dabba anticipated were not forthcoming.Dabba\n\nKranshoek Mesh: A truly community-driven network in a historic coastal village occupied by an ethnic minority. Using Mesh Potatoes, it promised to bring relief from high communications cost in a context of high unemployment, but current status is unknown.\n\nZenzeleni Networks: Formed with technical assistance from the University of the Western Cape, it is registered as a co-operative enterprise and a telecoms provider, operated and managed by members of Mankosi, a rural community in one of the most disadvantaged areas of South Africa. It has 12 nodes linked via a long distance WiFi backbone to a fibre gateway. Each node consists of a Mesh Potato connected to a solar power supply. It provides access to voices services at a fraction of the cost offered by incumbent operators. Currently under way are the provision of WiFi hotspots and connection of local schools’ computer labs, as well as backhaul improvement.\n\nScarborough Wireless User Group: A middle-class peri-urban DIY community was sharing internet access by mesh with each other and some poorer neighbours near Cape Town. At its peak it had about 200 nodes. Used Linksys WRT54GL routers. Defunct due to arrival of cheaper, faster ADSL & fibre.\n\nCape Town WUG: Although using the term “mesh” this large urban network (registered as a nonprofit organisation) is more correctly described as decentralised. With some hundreds of members, its only official connection to the internet is for POP email; otherwise, the main functions are offline file-sharing and gaming. It has a progressive constitution regarding sharing of skills and resources and some cross-subsidisation is evident between richer and poorer areas.\n\nJohannesburg Area WUG: Similar to above; is a member not only of Wireless Application Providers Association, but Internet Service Providers Association.\n\nSoWUG: Hybrid CN/ISP in Soweto, South Africa. Started in 2010 with corporate sponsorship and technical support from the Johannesburg Area WUG. It provides WiFi hotspots in public spaces in Soweto and nearby peri-urban areas. The organisation’s website maps out several operations it intends to expand into such as educational support.\n\nDurban Wireless Community: Smaller (approx 50 nodes) and more sporadically active; founded 2004 and recently revived; a non-profit promoting wireless technology and computer networks.\n\nOther WUGs: Another five nuclei of smaller, sporadic groups are listed.\n\nPretoria Mesh: This is an experimental project started in 2006 in the residential village of the CSIR, to test hardware and software deployed in other projects throughout the country. It has about 20 nodes and is still active.\n\nB4All: This is an abbreviation of Broadband Community Wireless Mesh Network which was a government program targeting the digital divide in rural areas, but which eventually merged with a commercial organisation providing school connectivity and public hotspots. It launched in 2009 in Limpopo province and public-sector involvement ended in 2014.\n\nICT4RED: The ICT for Rural Education (ICT4RED) project (2012-2016), undertaken by CSIR Meraka, was the largest research, development, innovation and implementation project of its kind in South Africa. It formed the ICT aspect of the larger Cofimvaba Technology for Rural Education Development (TECH4RED) project, a joint initiative between the Department of Science and Technology, the Department of Basic Education and the Eastern Cape Department of Education. TECH4RED is aimed at contributing to the improvement of rural education through technology-led innovation. ICT4RED was particularly successful in implementing technology in schools and in empowering rural teachers to comfortably use tablets in their day-to-day teaching activities, by using gamification principles and an \"earn-as-you-learn\" approach. ICT4RED employed a mesh network connecting 26 schools, with internet connectivity provided via shared satellite infrastructure. In 2016 the project has been handed over to the district and the province authorities to institutionalize the initiative.\n\nHome of Compassion: This N.P.O. based in Delft (Cape Town), has piloted a community network since 2015. It started with funding from the Western Cape Government and the support of an external ISP to roll out the network but once they built enough technical capacity, they decided to become an ISP themselves. By November 2015, it had 20 active access points and 17,150 active devices. They offer 50MB allowance a day per device and once users reach their cap, they sell prepaid top-up vouchers though a network of local resellers. In addition to the provision of Wi-Fi connectivity, Home of Compassion provides IT training and through its network it is able to set up “call centres on-demand”. Finally, Home of Compassion has developed an app, which is zero-rated across its network, to purchase goods and services within the community.\n\nICT for Rural Development (ICT4RD): This nationwide initiative had 2 pilot networks set up in Bunda and Serengeti in 2006. Assistance came from Swedish researchers and the agency SIDA. Both pilots were motivated by existence of fibre optic cables owned by other entities, however use was also made of a VSat connection at Bunda. Local governments were involved to create ownership and sustainability but contributed to the demise of the first. The remaining one connects schools and hospitals.\n\nSengerema Wireless Community Network: A project of the Sengerema multi-purpose Telecenter which provides computer services, printing, office, internet, education, FM Radio Station (reaching 400 000 people) with support from Dutch NGO, IICD. In 2012 it had an internet connection - VSAT 128/64 kbit/s through COSTECH (Tanzania Commission for Science and Technology). The network served a large number of civil society and official organisations including schools. It featured wireless routers: Linksys WRT54GL; firmware OpenWRT/Freifunk; self-built antennas (with some exceptions) and locally built masts. Current status unknown, believed to be lapsed.\n\nMesh SAYADA: A project of Clibre (a local open-web advocacy group) that started in 2012. The networking equipment (12 nodes) was donated by the Open Technology Institute, and the time was volunteer. The network is not really operating currently, largely due to the unstable sociopolitical situation.\n\nBosco Network: Battery Operated System for Community Outreach (BOSCO)-Uganda is a Non-Profit Organization (NGO) under the trusteeship of the Archdiocese of Gulu. Funded and operational since the year 2007 the organization started in installing wireless Internet and VoIP telephony in internally displaced persons (IDP) camps with reliable eco-friendly energy. 9 years later BOSCO-Uganda is managing 32 Information and Communication Technology (ICT) Centers situated in rural communities and former internally displaced persons camps, consist of low-power, solar powered PCs connected to a high-speed, long-range WiFi Internet connection. Each communication station is linked to other BOSCO sites via a free VoIP telephony network and through a high-speed internal network (INTRANET) content management page that are all powered by solar energy and enabling thousands of Ugandans in acquiring ICT and entrepreneurship knowledge, connecting them with other communities (e.g. market platform) and the outside world.\n\nMacha Works: Macha Works' LinkNet internet provisioning in the rural community of Macha, in Choma District, Southern Zambia, is a renowned example of a community networking in Southern Africa. The first internet connection became available for the community, early 2004, by means of a shared satellite internet connection with a medical research center in the community. From its start, so-called ‘local talent’ gained experience in collaboration with international partners but always approached realities from the local perspective first. The networking morphs continuously, utilising a variety of available technologies. These (did) include mesh network technologies and direct WiFi links, the use of second-hand computers, and the deployment of locally refurbished sea containers for the sensitising for the internet in other rural communities in Zambia and Zimbabwe. With the implementation of a WiMax network by a commercial operator and with debilitating donor-led intrusions of the LinkNet developed 'market' in its community, Macha Works focuses more on user training, technical training, and the maintenance of ICT equipment. The Macha network inspired and trained local leadership in both the technical and communal aspects of the provisioning of internet access in rural area. At least 7 other communities in Zambia emulate Macha's example and run a plethora of community networks and ICT services..\n\nMurambinda Works: Established in the early 2000s, Murambinda Works at Buhera in Zimbabwe is focused on bridging the digital divide between rural and urban settings. In this sense, Murambinda works provides ICT training in rural communities where most schools lack relevant computer facilities. It is affiliated with Macha Works in Zambia and has various operations, including internet service provision and an internet cafe.\n\nAfter using dial-up internet until 2015, it now has a fibre link to the national backbone. Training is provided for public officials including teachers, and any surplus revenue is channeled to a local foster home. Plans are afoot to extend to remote access points, perhaps even by satellite although for now costs are prohibitive..\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "56182351", "url": "https://en.wikipedia.org/wiki?curid=56182351", "title": "Managed access (corrections)", "text": "Managed access (corrections)\n\nManaged access is a term for managing cellular network access from contraband phones within a corrections facility. Managed access differs from cellular jamming technologies which are outlawed in the United States. A managed access system functions like a femtocell or low power cell tower which passes calls to cellular carriers, however, only communications from approved devices and emergency calling is allowed. The managed access signal appears as an extension of nearby commercial cellular signals, once a phone connects to the network its identifying information is compared with approved devices and communications are accepted or denied. Managed access networks work with commercial cellular signals including 2G, 3G, 4G/LTE, and WiMAX.\n\nIn 2010, the Mississippi Department of Corrections tested the first managed access system at Parchman Mississippi State Penitentiary, during just one month the system blocked more than 216,000 texts and 600 phone calls. In 2013, the FCC recommended that prisons be allowed to managed their own network access without having to seek approval from the agency, saying that the process of inspecting the systems is \"time-consuming and complex\" and \"discourages their use.\" In a 2016 Op-ed, FCC Chairman Ajit Pai requested that the reforms proposed in 2013 aimed at loosening regulations on managed access and other solutions used to prevent the use of contraband cell phones should be enacted.\n\nAs of 2016, only California, Maryland, Mississippi, South Carolina, and Texas had tested managed access systems.\n\nManaged access systems are unable to stop the use of contraband devices using Wi-Fi to connect to the internet. Deployment of managed access systems requires FCC approval as well as consent from cellular network carriers. The devices can also cause interference outside of the prison if they are not properly implemented.\n\n"}
{"id": "13113721", "url": "https://en.wikipedia.org/wiki?curid=13113721", "title": "Ministry of Education and Science of Georgia", "text": "Ministry of Education and Science of Georgia\n\nThe Ministry of Education and Science of Georgia () is a governmental body responsible for education system and children's services in Georgia. Ministry of Education works under the Minister of Education and Science of Georgia. The ministry is located on Uzandze street in Tbilisi in a historical building built in Mauritanic style. Tamar Sanikidze's resignation was announced on 3 June 2016. Her proposed replacement is Aleksandre Jejelava. \n\n"}
{"id": "44231927", "url": "https://en.wikipedia.org/wiki?curid=44231927", "title": "Ministry of Environment, Science, Technology and Innovation (Ghana)", "text": "Ministry of Environment, Science, Technology and Innovation (Ghana)\n\nMinistry of Environment, Science, Technology and Innovation is the ministry of Ghana responsible for the development of environment & science in the country.\n\nThe Ministry conceives of maintaining development through the use of science, technology and innovation for economical growth and healthy external conditions through advanced and a well structured economy. \n\nThe purpose of this Ministry is to sustain growth by putting up measures to strengthen market aimed research and development (R&D) for appropriate external conditions, science, technology and innovation through massive cognizance conception, partnership and team work. \n\n\n\n\n\n\n"}
{"id": "41520190", "url": "https://en.wikipedia.org/wiki?curid=41520190", "title": "Ministry of Science, ICT and Future Planning", "text": "Ministry of Science, ICT and Future Planning\n\nThe Ministry of Science, ICT and Future Planning (MSIP, ) was a ministry of the Government of South Korea. Its purpose is to set, manage, and evaluate science and technology policy, support scientific research and development, develop human resources, conduct R&D leading to the production and consumption of Atomic power, plan national informatization and information protection strategies, manage radio frequency bands, oversee the information and communications technology (ICT) industry, and operate Korea Post. Its headquarters are in Building #4 of the Gwacheon Government Complex in Gwacheon, Gyeonggi Province.\n\nMinistry of Science and ICT succeeds the ministry from 2017.\n\nThe creation of the ministry was announced in February 2013. The ministry was created under a reorganization plan initiated by South Korean President Park Geun-hye in an effort to generate new sources of economic growth in the areas of science and information technology.\n\nThe creation of the ministry was one of Park's core pledges during the 2013 campaign leading to her election.\n\nThe ministry dissolved in July, 2017. Ministry of Science and ICT (과학기술정보통신부) succeeds the former ministry.\n\nChoi Mun-kee was the inaugural Minister of this Ministry. Later Choi Yanghee became the Minister of Science, ICT and Future Planning. He was nominated by President Park Geun-hye.\n\nPolicies on new media, such as cable TV service operators, satellite channels and digital multimedia broadcasting , have been transferred to this ministry. The ministry is expected to contribute to the creation of about 410,000 jobs in these areas by the year 2017, including about 90,000 jobs in business start-ups.\n\nThe ministry will help drive the so-called national informatization project, which seeks to introduce technology into a variety of areas including traditional markets, agriculture, and small- and medium-sized businesses.\n\nThe ministry is responsible for awarding the Korea Science and Technology Award in conjunction with the Korean Federation of Science and Technology Societies and the Korea Mobile App Award in conjunction with the MoneyToday publication.\n\nBecause the ministry not only took over from the former Ministry of Science and Technology, but also assumed responsibility for ICT from the Ministry of Information and Communication and control of Korea Post, some people worry that it has become bloated. Some others worry about its Korean name, as the Korean name translates directly to English as \"Ministry of Future Creation and Science\". Some scientists worry that it hints at Creationism.\n\nOn May 2013, a mission was created within the ministry, stating their purpose as \"Becoming lead gospel on ministry, and change country for god\". According to their business plan, they planned to evangelize one person every month, identifying Islam as a cult organization. People worry about the religious neutrality of public servants. A person in charge said \"the business plan is just a document made by a member of the mission, and is not an official policy\".\n\n"}
{"id": "32611510", "url": "https://en.wikipedia.org/wiki?curid=32611510", "title": "Mixlink", "text": "Mixlink\n\nMixlink (Mixlink II) is a computer used with Agfa scales. It was developed to facilitate calculation of color mixes.\n\nMixlink contains the Intel 80386 processor with the clock rate which may be set to either 9.2 or 33 MHz. The RAM size is 640 kB. The HDD function is served by the built-in flash memory that has the size of approximately 800 kB. Mixlink has the monochrome display.\n\nMixlink is also staffed with CD drive and floppy drive.\n\nMixlink was intended to be used with the supplied floppy and CD disk, which provided the system environment (\"operating system\") and the application to be used for calculating color mixes. However, it is possible to install and run MS-DOS on Mixlink.\n\nCurrently, Mixlink computers are not used; their functions may solely be performed by personal computers (PCs).\n"}
{"id": "51682311", "url": "https://en.wikipedia.org/wiki?curid=51682311", "title": "Multefire", "text": "Multefire\n\nMulteFire™ is an LTE-based technology that operates standalone in unlicensed and shared spectrum, including the global 5 GHz band. Based on 3GPP Release 13 and 14, MulteFire technology supports Listen-Before-Talk for fair co-existence with Wi-Fi and other technologies operating in the same spectrum. It supports private LTE and neutral host deployment models. Target vertical markets include industrial IoT, enterprise, cable, and various other vertical markets.\n\nThe MulteFire Release 1.0 specification was developed by the MulteFire Alliance, an independent, diverse and international member-driven consortium. Release 1.0 was published to MulteFire Alliance members in January 2017 and was made publicly available in April 2017. The MulteFire Alliance is currently working on Release 1.1 which will add further optimizations for IoT and new spectrum bands.\n\nAccording to Harbor Research in its published white paper, the market opportunity for private LTE networks for industrial and commercial IoT will reach $118.5 billion in 2023. It also reported that the total addressable revenue for Enterprise markets deploying private and neutral host LTE with MulteFire will reach $5.7 billion by 2025.  \n\nThe MulteFire Alliance has grown to more than 40 members. Its board members include Boingo Wireless, CableLabs, Ericsson, Huawei, Intel, Nokia, Qualcomm and SoftBank. The organization is open to any company with an interest in advancing LTE and cellular technology in unlicensed and shared spectrum.  \n\n"}
{"id": "48068261", "url": "https://en.wikipedia.org/wiki?curid=48068261", "title": "Nericell", "text": "Nericell\n\nNericell is a system which uses smartphones for monitoring traffic data. Nericell performs rich sensing by piggybacking on smartphones that users carry. It uses the accelerometer, radio, GPS, and microphone sensors found in these phones to detect potholes, bumps, braking, and honking. Nericell addresses several issues, including virtually reorienting the accelerometer on a phone that is in an arbitrary orientation, and performing honk detection and localization in an energy efficient manner.\n"}
{"id": "3898021", "url": "https://en.wikipedia.org/wiki?curid=3898021", "title": "Nyctography", "text": "Nyctography\n\nNyctography is a form of substitution cipher writing created by Lewis Carroll (Charles Lutwidge Dodgson) in 1891.\n\nNyctography is written with a nyctograph (also invented by Carroll) and uses a system of dots and strokes all based on a dot placed in the upper left corner. Using the Nyctograph, one could quickly jot down ideas or notes without the aid of light.\n\nCarroll invented the Nyctograph and Nyctography because he was often awakened during the night with thoughts that needed to be written down immediately, and didn't want to go through the lengthy process of lighting a lamp just to have to extinguish it shortly thereafter.\n\nThe device consisted of a gridded card with sixteen square holes, each a quarter inch wide, and system of symbols representing an alphabet of Carroll's design, which could then be transcribed the following day. \n\nHe first named it \"typhlograph\", but at the suggestion of one of his brother-students, this was subsequently changed into \"Nyctograph\".\n\nInitially, Carroll used an oblong of card with an oblong cut out of the centre to guide his writing in the dark. This did not appear to be satisfactory as the results were illegible. The new and final version of the nyctograph is recorded in his journal of September 24, 1891, and is the subject of a letter to \"The Lady\" magazine of October 29, 1891: \nFrom the description it appears that Carroll’s nyctograph was a single row of 16 boxes cut from a piece of card. Carroll would enter one of his symbols in each box, then move the card down to the next line (which, in the darkness, probably, he would have to estimate) and then repeat the process.\n\nEach character had a large dot or circle in the upper-left corner. Beside the 26 letters of the alphabet, there were five additional characters for 'and', 'the', the corners of the letter 'f' to indicate that the following characters were digits ('figures'), the corners of the letter 'l' to indicate that they were letters, and the corners of the letter 'd' to indicate that the following six characters were a date in DDMMYY format. There was no capitalization, punctuation or digits \"per se\", though modern font designers have created them (e.g. capitals may be double-scored, punctuation marks may have the large dot at the bottom right corner, digits at the bottom left). \n\nAs in braille, letters were assigned to represent digits. The values were taken from his Memoria Technica, which assigned two consonants to each digit, with vowels unassigned, so that any number could be read off as a word. For nyctography, one of the consonants was used for each digit. Most are the initials of the numerals, as follows. (In brackets are the other values of the Memoria Technica, which apart from leftover \"j\" for 3 have their own motivations.)\n\n\n\n"}
{"id": "3452561", "url": "https://en.wikipedia.org/wiki?curid=3452561", "title": "OpenDocument technical specification", "text": "OpenDocument technical specification\n\nThis article describes the technical specifications of the OpenDocument office document standard, as developed by the OASIS industry consortium. A variety of organizations developed the standard publicly and make it publicly accessible, meaning it can be implemented by anyone without restriction. The OpenDocument format aims to provide an open alternative to proprietary document formats.\n\nThe OpenDocument format supports the following two ways of document representation:\n\n\nThe recommended filename extensions and MIME types are included in the official standard (OASIS, May 1, 2005 and its later revisions or versions). The MIME types and extensions contained in the ODF specification are applicable only to office documents that are contained in a package. Office documents that conform to the OpenDocument specification but are not contained in a package should use the MIME type text/xml.\n\nThe MIME type is also used in the codice_8 attribute. It is very important to use this attribute in flat XML files/single XML documents, where this is the only way the type of the document can be detected (in a package, the MIME type is also present in a separate file \"mimetype\"). Its values are the MIME types that are used for the packaged variant of office documents.\n\nThe most common file extensions used for OpenDocument documents are codice_1 for text documents, codice_3 for spreadsheets, codice_4 for presentation programs, and codice_12 for graphics. These are easily remembered by considering \".od\" as being short for \"OpenDocument\", and then noting that the last letter indicates its more specific type (such as t for text).\nHere is the complete list of document types, showing the type of file, the recommended file extension, and the MIME Type:\n\nOpenDocument also supports a set of template types. Templates represent formatting information (including styles) for documents, without\nthe content themselves. The recommended filename extension begins with \".ot\" (interpretable as short for \"OpenDocument template\"), with the last letter indicating what kind of template (such as \"t\" for text). The supported set includes:\n\nAs noted above, the OpenDocument format can describe text documents (for example, those typically edited by a word processor), spreadsheets, presentations, drawings/graphics, images, charts, mathematical formulas, and \"master documents\" (which can combine them). It can also represent templates for many of them.\n\nThe official OpenDocument standard version 1.0 (OASIS, May 1, 2005) defines OpenDocument's capabilities. The text below provides a brief summary of the format's capabilities.\n\nThe OpenDocument format supports storing metadata (data about the data) by having a set of pre-defined\nmetadata elements, as well as allowing user-defined and custom metadata.\nThe format predefines the following metadata fields:\n\n\nOpenDocument's text content format supports both typical and advanced capabilities. Headings of various levels, lists of various kinds (numbered and not), numbered paragraphs,\nand change tracking are all supported. Page sequences and section attributes can be used to control how the text is displayed. Hyperlinks, ruby text (which provides annotations and is especially critical for some languages), bookmarks, and references are supported as well. Text fields (for autogenerated content), and mechanisms for automatically generating\ntables such as tables of contents, indexes, and bibliographies, are included as well.\n\nThe OpenDocument format implements spreadsheets as sets of tables. Thus it features extensive capabilities for formatting the display of tables and spreadsheets. OpenDocument also supports database ranges, filters, and \"data pilots\" (known in Microsoft Excel contexts as \"pivot tables\"). Change tracking is available for spreadsheets as well.\n\nThe graphics format supports a vector graphic representation, in which a set of layers and the contents of each layer is defined. Available drawing shapes include Rectangle, Line, Polyline, Polygon, Regular Polygon, Path, Circle, Ellipse, and Connector. 3D Shapes are also available; the format includes information about the Scene, Light, Cube, Sphere, Extrude, and Rotate (it is intended for use as for office data exchange, and not sufficient to represent videos or other extensive 3D scenes). Custom shapes can also be defined.\n\nPresentations are supported. Users can include animations in presentations, with control over the sound, showing a shape or text, hiding a shape or text, or dimming something, and these can be grouped. In OpenDocument, much of the format capabilities are reused from the text format, simplifying implementations. However, tables are not supported within OpenDocument as drawing objects, so may only be included in presentations as embedded tables.\n\nCharts define how to create graphical displays from numerical data. They support titles, subtitles, a footer, and a legend to explain the chart. The format defines the series of data that is to be used for the graphical display, and a number of different kinds of graphical displays (such as line charts, pie charts, and so on).\n\nForms are specially supported, building on the existing XForms standard.\n\nA document in OpenDocument format can contain two types of objects, as follows:\n\n\nUse of Microsoft Object Linking and Embedding (OLE) objects limits the interoperability, because these objects are not widely supported in programs for viewing or editing files (e.g. embedding of other files inside the file, such as tables or charts from a spreadsheet application in a text document or presentation file). If a software that understands an OLE object is not available, the object is usually replaced by a picture (bitmap representation of the object) or not displayed at all.\n\nThe style and formatting controls are numerous, providing a number of controls over the display of information.\n\nPage layout is controlled by a variety of attributes. These include page size, number format, paper tray, print orientation, margins, border (and its line width), padding, shadow, background, columns, print page order, first page number, scale, table centering, maximum footnote height and separator, and many layout grid properties.\n\nHeaders and footer can have defined fixed and minimum heights, margins, border line width, padding, background, shadow, and dynamic spacing.\n\nThere are many attributes for specific text, paragraphs, ruby text, sections, tables, columns, lists, and fills. Specific characters can have their fonts, sizes, generic font family names (\"roman\" serif, \"swiss\" sans-serif, \"modern\" monospace, \"decorative\", \"script\" or \"system\"), and other properties set. Paragraphs can have their vertical space controlled through attributes on keep together, widow, and orphan, and have other attributes such as \"drop caps\" to provide special formatting.\nThe list is extremely extensive; see the references (in particular the actual standard) for details.\n\nOpenDocument version 1.2 fully describes mathematical formulas displayable on-screen. It is fully capable of exchanging spreadsheet data, formats, pivot tables, and other information typically included in a spreadsheet. OpenDocument exchanges formulas as values of the attribute table:formula.\n\nThe allowed syntax of table:formula was not defined in sufficient detail in the OpenDocument version 1.0 specification, which defined spreadsheet formulas using a set of simple examples showing, for example, how to specify ranges and the SUM() function. The OASIS OpenDocument Formula sub group therefore standardized the table:formula in the OpenFormula specification. For more information \"(see the OpenFormula article)\".\n\nWhen OpenDocument file is password protected the file structure of bundle remains the same, but contents of XML files in package are encrypted using following algorithm:\n\n\nAn OpenDocument file commonly consists of a standard ZIP archive (JAR archive) containing a number of files and directories; but OpenDocument file can also consist only of a single XML document. OpenDocument file is commonly a collection of several subdocuments within a (ZIP) \"package\". OpenDocument file as a \"single XML\" is not widely used.\nAccording to the OpenDocument 1.0 specification, the ZIP file specification is defined in \"Info-ZIP Application Note 970311, 1997\". \nThe simple compression mechanism used for a package normally makes OpenDocument files significantly smaller than equivalent Microsoft \"codice_13\" or \"codice_14\" files. This smaller size is important for organizations who store a vast number of documents for long periods of time, and to those organizations who must exchange documents over low bandwidth connections. Once uncompressed, most data is contained in simple text-based XML files, so the uncompressed data contents have the typical ease of modification and processing of XML files. The standard also allows for the creation of a single XML document, which uses \"<office:document>\" as the root element, for use in document processing.\n\nThe standard allows the inclusion of directories to store images, non-SMIL animations, and other files that are used by the document but cannot be expressed directly in the XML.\n\nDue to the openly specified compression format used, it is possible for a user to extract the container file to manually edit the contained files. This allows repair of a corrupted file or low-level manipulation of the contents.\n\nThe zipped set of files and directories includes the following:\n\n\nThe OpenDocument format provides a strong separation between content, layout and metadata. The most notable components of the format are described in the subsections below. The files in XML format are further defined using the RELAX NG language for defining XML schemas. RELAX NG is itself defined by an OASIS specification, as well as by part two of the international standard ISO/IEC 19757: Document Schema Definition Languages (DSDL).\n\ncontent.xml, the most important file, carries the actual content of the document (except for binary data, such as images). The base format is inspired by HTML, and though far more complex, it should be reasonably legible to humans:\n\nstyles.xml contains style information. OpenDocument makes heavy use of styles for formatting and layout. Most of the style information is here (though some is in content.xml). Styles types include:\n\n\nThe OpenDocument format is somewhat unusual in that using styles for formatting cannot be avoided. Even \"manual\" formatting is implemented through styles (the application dynamically makes new styles as needed).\n\nmeta.xml contains the file metadata. For example, Author, \"Last modified by\", date of last modification, etc. The contents look somewhat like this:\n\nThe names of the <dc:...> tags come from the Dublin Core XML standard.\n\nsettings.xml includes settings such as the zoom factor or the cursor position. These are properties that are not content or layout.\n\nmimetype is just a one-line file with the mimetype of the document. One implication of this is that the file extension is actually immaterial to the format. The file extension is only there for the benefit of the user.\n\nThumbnails is a separate folder for a document thumbnail. The thumbnail must be saved as “thumbnail.png”. A thumbnail representation of a document should be generated by default when the file is saved. It should be a representation of the first page, first sheet, etc. of the document. The required size for the thumbnails is 128x128 pixel. In order to conform to the Thumbnail Managing Standard (TMS) at www.freedesktop.org, thumbnails must be saved as 8bit, non-interlaced PNG image with full alpha transparency.\n\nMETA-INF is a separate folder. Information about the files contained in the OpenDocument package is stored in an XML file called the manifest file. The manifest file is always stored at the pathname META-INF/manifest.xml. The main pieces of information stored in the manifest are:\n\n\nPictures is a separate folder for images included in the document. This folder is not defined in the OpenDocument specification. Files in this folder can use various image formats, depending on the format of inserted file. While the image data may have an arbitrary format, it is recommended that bitmap graphics are stored in the PNG format and vector graphics in the SVG format.\n\nBy design, OpenDocument reuses existing open XML standards whenever they are available, and it creates new tags only where no existing standard can provide the needed functionality. Thus OpenDocument uses a subset of DublinCore for metadata, MathML for displayed formulas, SMIL for multimedia, XLink for hyperlinks etc.\n\nAlthough not fully reusing SVG for vector graphics, OpenDocument does use SVG-compatible vector graphics within an ODF-format-specific namespace, but also includes non-SVG graphics.\n\n\nTo indicate which version of the OpenDocument specification a file complies with, all root elements take an codice_15 attribute. The version number is in the format \"revision.version\" (e.g. office:version=\"1.1\"). The codice_15 attribute identifies the version of ODF specification that defined the associated element, its schema, its complete content, and its interpretation.\n\nIf the file has a version known to an XML processor, it may validate the document. Otherwise, it is optional to validate the document, but the document must be well formed. It is not mandatory to use codice_15 attribute in ODF 1.0 and ODF 1.1 files.\n\nThe codice_15 attribute shall be present in each and every <office:document>, <office:document-content>, <office:document-styles>, <office:document-meta>, and <office:document-settings> element in the XML documents that comprise an OpenDocument 1.2 document. The value of the office:version attribute shall be \"1.2\".\n\nWhen an element has office:version=\"1.1\" the element and its content are based on the OpenDocument v1.1 specification. For office:version=\"1.0\" the element and its content are based on the OpenDocument v1.0 specification. When an element has office:version omitted, the element is based on a version of the OpenDocument specification earlier than v1.2. In these cases and in the case of values other than \"1.2\", the elements do not comprise an OpenDocument 1.2 document.\n\nThe OpenDocument specification does not specify which elements and attributes conforming applications must, should, or may support. Even typical office applications may only support a subset of the elements and attributes defined in the specification. The specification contains a non-normative table that provides an overview which element and attributes usually are supported by\ntypical office application.\n\nDocuments that conform to the OpenDocument specification may contain elements and attributes not specified within the OpenDocument schema. Such elements and attributes must not be part of a namespace that is defined within the specification and are called foreign elements and attributes.\n\nConforming applications either shall read documents that are valid against the OpenDocument schema if all foreign elements and attributes are removed before validation takes place, or shall write documents that are valid against the OpenDocument schema if all foreign elements and attributes are removed before validation takes place.\nConforming applications that read and write documents may preserve foreign elements and attributes. In addition to this, conforming applications should preserve meta information and the content of styles.\n\nConforming applications shall read documents containing processing instructions and should preserve them.\n\nODF 1.2 defines precisely the conformance needs. The specification defines conformance for documents, consumers, and producers, with two conformance classes called conforming and extended conforming. It further defines conforming text, spreadsheet, drawing, presentation, chart, image, formula and database front end documents. Chapter 2 defines the basic requirements for the individual conformance targets.\n\n"}
{"id": "54184232", "url": "https://en.wikipedia.org/wiki?curid=54184232", "title": "Out in Science, Technology, Engineering, and Mathematics", "text": "Out in Science, Technology, Engineering, and Mathematics\n\nOut in Science, Technology, Engineering, and Mathematics, Inc., abbreviated oSTEM, is a 501(c)(3) non-profit professional society dedicated to LGBTQ+ individuals within the science, technology, engineering, and mathematics (STEM) community.\n\nIn October 2005, IBM sponsored a focus group where students from across the United States convened at the Human Rights Campaign headquarters in Washington D.C. These students discussed topics relevant to LGBTQA communities at their own colleges and universities, and they debated how to structure an organization that serves students in science, technology, engineering, and mathematics.\n\nFounded in 2009 and achieving 501(c)(3) status in 2010, oSTEM, Inc. currently consists of more than 50 chapters across the United States and the United Kingdom. In 2017, oSTEM, Inc. expanded its mission to be inclusive of professionals, serving all LGBTQ people in the STEM community.\n\nAs an organization dedicated to community, oSTEM, Inc. strives to stay engaged to identify, address, and advocate for the needs of LGBTQA+ students and professionals within the STEM fields. oSTEM, Inc. fulfills these needs in many ways, including networking opportunities, mentorship connections, strategic collaborations and professional/leadership development, as well as an annual global conference.\n\noSTEM hosts annual conferences that discuss LGBT+ topics in STEM as well as intelligence fields. Topics discussed include inclusion, outreach, and diversity within the workplace. Workshops, presentations, and networking events for LGBT+ individuals aim to facilitate integration and advancement in their respective fields. The 4 annual conference was hosted jointly with NOGLSTP's Out to Innovate in Atlanta in 2014.\n\nConferences have been held in the following cities:\nOn July 5, 2018 oSTEM along with Pride in STEM, House of STEM, and InterEngineering created international awareness for LGBTQ+ people in Science, Technology, Engineering, and Math.\n\noSTEM presents a variety of awards annually to individuals and organizations that demonstrate a strong dedication to advancing and empowering LGBT+ in STEM fields.\n\nThe oSTEM Global STEM Service Award is given to present and past oSTEM members who show strong dedication to inclusion, diversity, and equality for LGBT+ and other marginalized individuals in STEM fields.\n\nAwardees are: \n\nThe oSTEM Strategic Alliance Award is presented to a current sponsoring organization, community partner, or grant provider of oSTEM who demonstrates strong dedication, engagement, and support to oSTEM and its values.\n\nAwardees are:\n\nThe oSTEM Partner Excellence Award is presented to individuals associated with oSTEM accomplished in their academic or professional lives who regularly advocate for the full inclusion of people of all marginalized identities.\n\nAwardees are: \n\nThe Overall Student Chapter of the Year is given to oSTEM chapters that educate, empower, and engage a diverse community. These chapters contribute greatly to identifying, addressing, and advocating for LGBTQ students in the STEM community\n\nAwardees are: \n\nThe Rookie Student Chapter of the Year celebrates achievements by oSTEM chapters that have been founded within two years of application submission.\n\nAwardees are:\n\noSTEM has over 90 chapters as of September, 2018. Chapters are organized into 6 geographic regions (A-F) and two types (student and professional).\nThe six regions are:\n\nThe first professional chapter is currently being tested in the Boston metropolitan area. Further cities will be announced at a later date.\n\noSTEM is sponsored by industries involved in STEM fields as well as government entities and academic STEM programs.\n\n\n\n\n\n\n"}
{"id": "50056803", "url": "https://en.wikipedia.org/wiki?curid=50056803", "title": "Pakathon", "text": "Pakathon\n\nPakathon is a Boston-based, registered global non-profit organization with eight global chapters in four countries: Pakistan, the United States, Canada, and Australia. Founded in 2013, Pakathon aims to create jobs through innovation and entrepreneurship by mobilizing the Pakistani diaspora. By working to foster a global network of like-minded individuals, investors and entrepreneurs, Pakathon aims to solve problems in sectors such as education, healthcare, gender equality, energy and law.\n\nThe organization's flagship event is a weekend-long hackathon hosted in chapter cities where students and professionals develop and pitch their projects to a panel of mentors. Top-performing teams are then invited to present their ideas to a global audience where they compete for funding, mentorship, and additional support, with the aim of advancing their ideas from conception to reality. Two winning teams are then selected: one from Pakistan and another from the international host cities.\n\nIn 2015, Pakathon partnered with the Higher Education Commission of Pakistan to expand Pakathon's innovation-centred programming in up to 50 Pakistani universities.\n\nPakathon's first winning team conceived the idea for ProCheck, a serialization system for authentic medicines in Pakistan. ProCheck allows customers and patients to verify authentic medicines using their mobile phones via text messaging. In November, 2015 ProCheck partnered with Ferozsons Labs, a leading manufacturer of pharmaceuticals in Pakistan, to serialize 35 million units of medicine using ProCheck's track and trace solution. As a result, more than 50,000 patients across the country will be able to distinguish between genuine and counterfeit medicines.\n"}
{"id": "11946749", "url": "https://en.wikipedia.org/wiki?curid=11946749", "title": "Respirocyte", "text": "Respirocyte\n\nRespirocytes are hypothetical, microscopic, artificial red blood cells that are intended to emulate the function of their organic counterparts, so as to supplement or replace the function of much of the human body's normal respiratory system. Respirocytes were proposed by Robert A. Freitas Jr in his 1998 paper \"A Mechanical Artificial Red Blood Cell: Exploratory Design in Medical Nanotechnology\".\n\nRespirocytes are an example of molecular nanotechnology, a field of technology still in the very earliest, purely hypothetical phase of development. Current technology is not sufficient to build a respirocyte due to considerations of power, atomic-scale manipulation, immune reaction or toxicity, computation and communication.\n\nFreitas proposed a spherical robot made up of 18 billion atoms arranged as a tiny pressure tank, which would be filled up with oxygen and carbon dioxide. \n\nIn Freitas' proposal, each respirocyte could store and transport 236 times more oxygen than a natural red blood cell, and could release it in a more controlled manner.\n\nRay Kurzweil hypothesised that such respirocytes would allow an adult human to sprint at top speed for at least 15 minutes without taking a breath.\n\nFreitas has also proposed \"microbivore\" robots that would attack pathogens in the manner of white blood cells.\n\n"}
{"id": "28429398", "url": "https://en.wikipedia.org/wiki?curid=28429398", "title": "Rxqual", "text": "Rxqual\n\nRxQual is used in GSM and is a part of the Network Measurement Reports (NMR).\n\nThis is an integer value of which can be between 0 and 7 and reflects the quality of voice.\n0 is the best quality, 7 is the worst.\n\nEach RxQual value corresponds to an estimated number of bit errors in a number of bursts.\n\nThere are two types of RxQual values, FULL and SUB.\n\nWe use RxQual SUB when we have DTX DL activated because RxQual FULL values will not be reliable, because they will use Bit error rate (BER) measurements when nothing has been sent, what leads to a very high BER and a poor RxQual.\n\nIf DTX DL is deactivated, is better to use RxQual FULL values, they are more precise, because it uses all frames on the SACCH multiframe, whether they have been transmitted from the base station or not.\n\nThe official definition of RxQual is given in Chapter 8.2.4 of GSM TS 05.08 (ETSI TS 100 911), later superseded by 3GPP TS 45.008.\n\n"}
{"id": "8144819", "url": "https://en.wikipedia.org/wiki?curid=8144819", "title": "SMS banking", "text": "SMS banking\n\nSMS banking is a form of mobile banking. It is a facility used by some banks or other financial institutions to send messages (also called notifications or alerts) to customers' mobile phones using SMS messaging, or a service provided by them which enables customers to perform some financial transactions using SMS.\n\nSMS banking services may use either push and pull messages. Push messages are those that a bank sends out to a customer's mobile phone, without the customer initiating a request for the information. Typically, a push message could be a mobile marketing message or an alert of an event which happens in the customer's bank account, such as a large withdrawal of funds from an ATM or a large payment involving the customer's credit card, etc. It may also be an alert that some payment is due, or that an e-statement is ready to be downloaded.\n\nAnother type of push message is one-time password (OTPs). OTPs are the latest tool used by financial institutions to combat cyber fraud. Instead of relying on traditional memorized passwords, OTPs are sent to a customer's mobile phone via SMS, who are required to repeat the OTP to complete transactions using online or mobile banking. The OTP is valid for a relatively short period and expires once it has been used.\n\nBank customers can select the type of activities for which they wish to receive an alert. The selection can be done either using internet banking or by phone.\n\nPull messages are initiated by the customer, using a mobile phone, for obtaining information or performing a transaction in the bank account. Examples of pull messages include an account balance enquiry, or requests for current information like currency exchange rates and deposit interest rates, as published and updated by the bank.\n\nDepending on the selected extent of SMS banking transactions offered by the bank, a customer can be authorized to carry out either non-financial transactions, or both and financial and non-financial transactions. SMS banking solutions offer customers a range of functionality, classified by push and pull services as outlined below.\n\nTypical push services would include:\n\n\nTypical pull services would include:\n\n\nThere is a very real possibility for fraud when SMS banking is involved, as SMS uses insecure encryption and is easily spoofable (see the SMS page for details). Supporters of SMS banking claim that while SMS banking is not as secure as other conventional banking channels, like the ATM and internet banking, the SMS banking channel is not intended to be used for very high-risk transactions.\n\nDue to the concerns made explicit above, it is extremely important that SMS gateway providers can provide a decent quality of service for banks and financial institutions in regards to SMS services. Therefore, the provision of Service Level Agreement (SLA) is a requirement for this industry; it is necessary to give the bank customer delivery guarantees of all messages, as well as measurements on the speed of delivery, throughput, etc. SLAs give the service parameters in which a messaging solution is guaranteed to perform.\n\nThe convenience of executing simple transactions and sending out information or alerting a customer on the mobile phone is often the overriding factor that dominates over the skeptics who tend to be overly bitten by security concerns.\n\nAs a personalized end-user communication instrument, today mobile phones are perhaps the easiest channel on which customers can be reached on the spot, as they carry the mobile phone all the time no matter where they are. Besides, the operation of SMS banking functionality over phone key instructions makes its use very simple. This is quite different from internet banking which can offer broader functionality, but has the limitation of use only when the customer has access to a computer and the Internet. Also, urgent warning messages, such as SMS alerts, are received by the customer instantaneously; unlike other channels such as the post, email, Internet, telephone banking, etc. on which a bank's notifications to the customer involves the risk of delayed delivery and response.\n\nThe SMS banking channel also acts as the bank’s means of alerting its customers, especially in an emergency situation; e.g. when there is an ATM fraud happening in the region, the bank can push a mass alert (although not subscribed by all customers) or automatically alert on an individual basis when a predefined ‘abnormal’ transaction happens on a customer’s account using the ATM or credit card. This capability mitigates the risk of fraud going unnoticed for a long time and increases customer confidence in the bank’s information systems.\n\nThe lack of encryption on SMS messages is an area of concern that is often discussed. This concern sometimes arises within the group of the bank’s technology personnel, due to their familiarity and past experience with encryption on the ATM and other payment channels. The lack of encryption is inherent to the SMS banking channel and several banks that use it have overcome their fears by introducing compensating controls and limiting the scope of the SMS banking application to where it offers an advantage over other channels.\n\nSuppliers of SMS banking software solutions have found reliable means by which the security concerns can be addressed. Typically the methods employed are by pre-registration and using security tokens where the transaction risk is perceived to be high. Sometimes ATM type PINs are also employed, but the usage of PINs in SMS banking makes the customer's task more cumbersome.\n\nSMS banking usually integrates with a bank’s computer and communications systems. As most banks have multiple backend hosts, the more advanced SMS banking systems are built to be able to work in a multi-host banking environment; and to have open interfaces which allow for messaging between existing banking host systems using industry or de facto standards.\n\nWell developed and mature SMS banking software normally provide a robust control environment and a flexible and scalable operating environment. These solutions are able to connect seamlessly to multiple SMSC operators in the country of operation. Depending on the volume of messages that are required to be pushed, means to connect to the SMSC could be different, such as using simple modems or connecting over leased line using low level communication protocols (like SMPP, UCP etc.) Advanced SMS banking solutions also cater to providing failover mechanisms and least-cost routing options.\n\nMost online banking platforms are owned and developed by the banks using them. There is only one open source online banking platform supporting mobile banking and SMS payments called Cyclos, which is developed to stimulate and empower local banks in development countries.\n\n"}
{"id": "15763075", "url": "https://en.wikipedia.org/wiki?curid=15763075", "title": "Technology intelligence", "text": "Technology intelligence\n\nTechnology Intelligence (TI) is an activity that enables companies to identify the technological opportunities and threats that could affect the future growth and survival of their business. It aims to capture and disseminate the technological information needed for strategic planning and decision making. As technology life cycles shorten and business become more globalized having effective TI capabilities is becoming increasingly important.\n\nIn the United States, Project Socrates identified the exploitation of technology as the most effective foundation for decision making for the complete set of functions within the private and public sectors that determine competitiveness.\n\nThe Centre for Technology Management has defined 'technology intelligence' as \"the capture and delivery of technological information as part of the process whereby an organisation develops an awareness of technological threats and opportunities.\"\n\n\n\n\n"}
{"id": "1177467", "url": "https://en.wikipedia.org/wiki?curid=1177467", "title": "Telehealth", "text": "Telehealth\n\nTelehealth involves the distribution of health-related services and information via electronic information and telecommunication technologies. It allows long distance patient/clinician contact and care, advice, reminders, education, intervention, monitoring and remote admissions. As well as provider distance-learning; meetings, supervision, and presentations between practitioners; online information and health data management and healthcare system integration. Telehealth could include two clinicians discussing a case over video conference; a robotic surgery occurring through remote access; physical therapy done via digital monitoring instruments, live feed and application combinations; tests being forwarded between facilities for interpretation by a higher specialist; home monitoring through continuous sending of patient health data; client to practitioner online conference; or even videophone interpretation during a consult.\n\nAs the population grows and ages, and medical advances are made which prolong life, demands increase on the healthcare system. Healthcare providers are also being asked to do more, with no increase in funding, or are encouraged to move to new models of funding and care such as patient-centered or outcomes based, rather than fee-for-service. Some specific health professions already have a shortage (i.e. Speech-language pathologists). When rural settings, lack of transport, lack of mobility (i.e. In the elderly or disabled), decreased funding or lack of staffing restrict access to care, telehealth can bridge the gap.\n\nTelehealth is sometimes discussed interchangeably with telemedicine. The Health Resources and Services Administration (HRSA) distinguishes telehealth from telemedicine in its scope. According to HRSA, telemedicine only describes remote clinical services; such as diagnosis and monitoring, while telehealth includes preventative, promotive and curative care delivery. This includes the above-mentioned non-clinical applications like administration and provider education which make telehealth the preferred modern terminology.\n\nThe development and history of telehealth or telemedicine (terms used interchangeably in literature) is deeply rooted in the history and development in not only technology but also society itself. Humans have long sought to relay important messages through torches, optical telegraphy, electroscopes, and wireless transmission. In the 21st century, with the advent of the internet, portable devices and other such digital devices are taking a transformative role in healthcare and its delivery.\n\nAlthough, traditional medicine relies on in-person care, the need and want for remote care has existed from the Roman and pre-Hippocratic periods in antiquity. The elderly and infirm who could not visit temples for medical care sent representatives to convey information on symptoms and bring home a diagnosis as well as treatment. In Africa, villagers would use smoke signals to warn neighbouring villages of disease outbreak. The beginnings of telehealth have existed through primitive forms of communication and technology.\n\nAs technology developed and wired communication became increasingly commonplace, the ideas surrounding telehealth began emerging. The earliest telehealth encounter can be traced to Alexander Graham Bell in 1876, when he used his early telephone as a means of getting help from his assistant Mr. Watson after he spilt acid on his trousers. Another instance of early telehealth, specifically telemedicine was reported in \"The Lancet\" in 1879. An anonymous writer described a case where a doctor successfully diagnosed a child over the telephone in the middle of the night. This Lancet issue, also further discussed the potential of Remote Patient Care in order to avoid unnecessary house visits, which were part of routine health care during the 1800s. Other instances of telehealth during this period came from the American Civil War, during which telegraphs were used to deliver mortality lists and medical care to soldiers.\n\nFrom the late 1800s to the early 1900s the early foundations of wireless communication were laid down. Radios provided an easier and near instantaneous form of communication. The use of radio to deliver healthcare became accepted for remote areas. The Royal Flying Doctor Service of Australia is an example of the early adoption of radios in telehealth.\n\nIt was during the mid-1900s well into the 1980s that a lot of the momentum and foundations of telehealth were founded. When the American National Aeronautics and Space Administration (NASA), began plans to send astronauts into space, the need for Telemedicine became all too clear. In order to monitor their astronauts in space, telemedicine capabilities were built into the spacecraft as well as the first spacesuits. Additionally, during this period, telehealth and Telemedicine were promoted in different countries especially the United States. Different projects were funded across North America and Canada in order to realise the exciting potential of this new innovation.\n\nIn 1964, the Nebraska Psychiatric Institute began using television links to form two-way communication with the Norfolk State Hospital which was 112 miles away for the education and consultation purposes between clinicians in the two locations. The Logan International Airport in Boston established in-house medical stations in 1967. These stations were linked to Massachusetts General Hospital. Clinicians at the hospital would provide consultation services to patients who were at the airport. Consultations were achieved through microwave audio as well as video links.\n\nIn 1972, there was a key emphasis on telemedicine so much so that the Department of Health, Education and Welfare in the United States approved funding for seven telemedicine projects across different states. This funding was renewed and two further projects were funded the following year.\n\nAlthough the excitement of telehealth and telemedicine remained, enthusiasm waned in the 1980s. Telehealth projects underway before and during the 1980s would take off but fail to proliferate mainstream healthcare. This put a halt on various projects and reduced opportunities for funding. As a result, this period of telehealth history is called the \"maturation\" stage and made way for sustainable growth.\n\nSustained growth happened most notably in North America. Although State funding was beginning to run low, different hospitals in various states began to launch their own telehealth initiatives. Additionally, NASA started experimenting with their ATS-3 satellite. Eventually, NASA started their SateLife/HealthNet programme which tried to increase the health services connectivity in developing countries.\n\nThe combination of sustained growth, the advent of the internet and the increasing adoption of ICT in traditional methods of care spurred the revival or \"renaissance\" of telehealth into the early 2000s and onwards.\n\nThe early 2000s were characterised by accelerated development in both science and technology. The early adoption of technology in society made way for widespread adoption in society. The diffusion of portable devices like laptops and mobile devices in everyday life made ideas surrounding telehealth more plausible. This continuing trend of better and innovative technology in homes, schools and organisations is contributing to the growing research in telehealth. Telehealth is no longer bound within the realms of telemedicine but has expanded itself to promotion, prevention and education.\n\nTelehealth requires a strong, reliable broadband connection. The broadband signal transmission infrastructure includes wires, cables, microwaves and optic fibre, which must be maintained for the provision of telehealth services. The better the connection (bandwidth quality), the more data can be sent and received. Historically this has priced providers or patients out of the service, but as the infrastructure improves and becomes more accessible, telehealth usage can grow.\n\nWhen a healthcare service decides to provide telehealth to its patients, there are steps to consider, besides just whether the above resources are available. A needs assessment is the best way to start, which includes assessing the access the community currently has to the proposed specialists and care, whether the organisation currently has underutilized equipment which will make them useful to the area they are trying to service, and the hardships they are trying to improve by providing the access to their intended community (i.e. Travel time, costs, time off work). A service then needs to consider potential collaborators. Other services may exist in the area with similar goals who could be joined to provide a more holistic service, and/or they may already have telehealth resources available. The more services involved, the easier to spread the cost of IT, training, workflow changes and improve buy-in from clients. Services need to have the patience to wait for the accrued benefits of providing their telehealth service and cannot necessarily expect community-wide changes reflected straight away.\n\nOnce the need for a Telehealth service is established, delivery can come within four distinct domains. They are live video (synchronous), store-and-forward (asynchronous), remote patient monitoring, and mobile health. Live video involves a real-time two-way interaction, such as patient/caregiver-provider or provider-provider, over a digital (i.e. broadband) connection. This often is used to substitute a face to face meeting such as consults, and saves time and cost in travel. Store-and-forward is when data is collected, recorded, and then sent on to a provider. For example, a patient's' digital health history file including x-rays and notes, being securely transmitted electronically to evaluate the current case. Remote patient monitoring includes patients' medical and health data being collected and transferred to a provider elsewhere who can continue to monitor the data and any changes that may occur. This may best suit cases that require ongoing care such as rehabilitation, chronic care, or elderly clients trying to stay in the community in their own homes as opposed to a care facility. Mobile health includes any health information, such as education, monitoring and care, that is present on and supported by mobile communication devices such as cell phones or tablet computers. This might include an application, or text messaging services like appointment reminders or public health warning systems.\n\nTelehealth is a modern form of health care delivery. Telehealth breaks away from traditional health care delivery by using modern telecommunication systems including wireless communication methods. Traditional health is legislated through policy to ensure the safety of medical practitioners and patients. Consequently, since telehealth is a new form of health care delivery that is now gathering momentum in the health sector, many organizations have started to legislate the use of telehealth into policy. In New Zealand, the Medical Council has a statement about telehealth on their website. This illustrates that the medical council has foreseen the importance that telehealth will have on the health system and have started to introduce telehealth legislation to practitioners along with government.\n\nTraditional use of telehealth services has been for specialist treatment. However, there has been a paradigm shift and telehealth is no longer considered a specialist service. This development has ensured that many access barriers are eliminated, as medical professionals are able to use wireless communication technologies to deliver health care. This is evident in rural communities. For individuals living in rural communities, specialist care can be some distance away, particularly in the next major city. Telehealth eliminates this barrier, as health professionals are able to conduct a medical consultation through the use of wireless communication technologies. However, this process is dependent on both parties having Internet access.\n\nTelehealth allows the patient to be monitored between physician office visits which can improve patient health. Telehealth also allows patients to access expertise which is not available in their local area. This remote patient monitoring ability enables patients to stay at home longer and helps avoid unnecessary hospital time. In the long-term, this could potentially result in less burdening of the healthcare system and consumption of resources.\n\nThe technological advancement of wireless communication devices is a major development in telehealth. This allows patients to self-monitor their health conditions and to not rely as much on health care professionals. Furthermore, patients are more willing to stay on their treatment plans as they are more invested and included in the process, decision-making is shared. Technological advancement also means that health care professionals are able to use better technologies to treat patients for example in surgery. Technological developments in telehealth are essential to improve health care, especially the delivery of healthcare services, as resources are finite along with an ageing population that is living longer.\n\nTelehealth allows multiple, different disciplines to merge and deliver a much more uniform level of care using the efficiency and accessibility of everyday technology. As telehealth proliferates mainstream healthcare and challenges notions of traditional healthcare delivery, different populations are starting to experience better quality, access and personalised care in their lives.\n\n\"See Also: Health Promotion\" Telehealth can also increase health promotion efforts. These efforts can now be more personalised to the target population and professionals can extend their help into homes or private and safe environments in which patients of individuals can practice, ask and gain health information. Health promotion using telehealth has become increasingly popular in underdeveloped countries where there are very poor physical resources available. There has been a particular push toward mHealth applications as many areas, even underdeveloped ones have mobile phone coverage.\n\nIn developed countries, health promotion efforts using telehealth have been met with some success. The Australian hands-free breastfeeding Google Glass application reported promising results in 2014. This application made in collaboration with the Australian Breastfeeding Association and a tech startup called Small World Social, helped new mothers learn how to breastfeed. Breastfeeding is beneficial to infant health and maternal health and is recommended by the World Health Organisation and health organisations all over the world. Widespread breastfeeding can prevent 820,000 infant deaths globally but the practice is often stopped prematurely or intents to do are disrupted due to lack of social support, know-how or other factors. This application gave mother's hands-free information on breastfeeding, instructions on how to breastfeed and also had an option to call a lactation consultant over Google Hangout. When the trial ended, all participants were reported to be confident in breastfeeding.\n\nTheoretically, the whole health system stands to benefit from telehealth. In a UK telehealth trial done in 2011, it was reported that the cost of health could be dramatically reduced with the use of telehealth monitoring. The usual cost of in vitro fertilisation (IVF) per cycle would be around $15,000, with telehealth it was reduced to $800 per patient. In Alaska the Federal Health Care Access Network which connects 3,000 healthcare providers to communities, engaged in 160,000 telehealth consultations from 2001 and saved the state $8.5 million in travel costs for just Medicaid patients. There are indications telehealth consumes fewer resources and requires fewer people to operate it with shorter training periods to implement initiatives.\n\nHowever, whether or not the standard of health care quality is increasing is quite debatable, with literature refuting such claims. Research is increasingly reporting that clinicians find the process difficult and complex to deal with. Furthermore, there are concerns around informed consent, legality issues as well as legislative issues. Although health care may become affordable with the help of technology, whether or not this care will be \"good\" is the issue.\n\nDue to its digital nature it is often assumed that telehealth saves the health system money. However, the evidence to support this is varied. When conducting economic evaluations of telehealth services, the individuals evaulating them need to be aware of potential outcomes and extraclinical benefits of the telehealth service. \n\n\nWhile many branches of medicine have wanted to fully embrace telehealth for a long time, there are certain risks and barriers which bar the full amalgamation of telehealth into best practice. For a start, it is dubious as to whether a practitioner can fully leave the \"hands-on\" experience behind. Although it is predicted that telehealth will replace many consultations and other health interactions, it cannot yet fully replace a physical examination, this is particularly so in diagnostics, rehabilitation or mental health.\n\nThe benefits posed by telehealth challenge the normative means of healthcare delivery set in both legislation and practice. Therefore, the growing prominence of telehealth is starting to underscore the need for updated regulations, guidelines and legislation which reflect the current and future trends of healthcare practices. Telehealth enables timely and flexible care to patients wherever they may be; although this is a benefit, it also poses threats to privacy, safety, medical licensing and reimbursement. When a clinician and patient are in different locations, it is difficult to determine which laws apply to the context. Once healthcare crosses borders different state bodies are involved in order to regulate and maintain the level of care that is warranted to the patient or telehealth consumer. As it stands, telehealth is complex with many grey areas when put into practice especially as it crosses borders. This effectively limits the potential benefits of telehealth.\n\nAn example of these limitations include the current American reimbursement infrastructure, where Medicare will reimburse for telehealth services only when a patient is living in an area where specialists are in shortage, or in particular rural counties. The area is defined by whether it is a medical facility as opposed to a patient's' home. The site that the practitioner is in, however, is unrestricted. Medicare will only reimburse live video (synchronous) type services, not store-and-forward, mhealth or remote patient monitoring (if it does not involve live-video). Some insurers currently will reimburse telehealth, but not all yet. So providers and patients must go to the extra effort of finding the correct insurers before continuing. Again in America, states generally tend to require that clinicians are licensed to practice in the surgery' state, therefore they can only provide their service if licensed in an area that they do not live in themselves.\n\nMore specific and widely reaching laws, legislations and regulations will have to evolve with the technology. They will have to be fully agreed upon, for example, will all clinicians need full licensing in every community they provide telehealth services too, or could there be a limited use telehealth licence? Would the limited use licence cover all potential telehealth interventions, or only some? Who would be responsible if an emergency was occurring and the practitioner could not provide immediate help – would someone else have to be in the room with the patient at all consult times? Which state, city or country would the law apply in when a breach or malpractice occurred? \n\nA major legal action prompt in telehealth thus far has been issues surrounding online prescribing and whether an appropriate clinician-patient relationship can be established online to make prescribing safe, making this an area that requires particular scrutiny. It may be required that the practitioner and patient involved must meet in person at least once before online prescribing can occur, or that at least a live-video conference must occur, not just impersonal questionnaires or surveys to determine need.\n\n\"Informed consent\" is another issue – should the patient give informed consent to receive online care before it starts? Or will it be implied if it is care that can only practically be given over distance? When telehealth includes the possibility for technical problems such as transmission errors or security breaches or storage which impact on ability to communicate, it may be wise to obtain informed consent in person first, as well as having backup options for when technical issues occur. In person, a patient can see who is involved in their care (namely themselves and their clinician in a consult), but online there will be other involved such as the technology providers, therefore consent may need to involve disclosure of anyone involved in the transmission of the information and the security that will keep their information private, and any legal malpractice cases may need to involve all of those involved as opposed to what would usually just be the practitioner.\n\nThe rate of adoption of telehealth services in any jurisdiction is frequently influenced by factors such as the adequacy and cost of existing conventional health services in meeting patient needs; the policies of governments and/or insurers with respect to coverage and payment for telehealth services; and medical licensing requirements that may inhibit or deter the provision of telehealth second opinions or primary consultations by physicians.\n\nProjections for the growth of the telehealth market are optimistic, and much of this optimism is predicated upon the increasing demand for remote medical care. According to a recent survey, nearly three-quarters of U.S. consumers say they would use telehealth. At present, several major companies along with a bevvy of startups are working to develop a leading presence in the field.\n\nIn the UK, the Government's Care Services minister, Paul Burstow, has stated that telehealth and telecare would be extended over the next five years (2012–2017) to reach three million people.\n\n\n\n"}
{"id": "9916529", "url": "https://en.wikipedia.org/wiki?curid=9916529", "title": "Topic-based authoring", "text": "Topic-based authoring\n\nIn technical communication, topic-based authoring is a modular approach to content creation where content is structured around topics that can be mixed and reused in different contexts. It is defined in contrast with \"book-oriented\" or \"narrative\" content, written in the linear structure of written books.\n\nThis authoring approach is popular in the technical publications and documentation arenas, as it is adequate for technical documentation. Tools supporting this approach typically store content in XML document format in a way that facilitates content reuse, content management, and makes the dynamic assembly of personalized information possible.\n\nA topic is a discrete piece of content that is about a specific subject, has an identifiable purpose, and can stand alone (does not need to be presented in context for the end-user to make sense of the content). Topics are also reusable. They can, when constructed properly (without reliance on other content for its meaning), be reused in any context anywhere needed.\n\nThe Darwin Information Typing Architecture (DITA) is a standard designed to help authors create topic-based content. The standard is managed by the Organization for the Advancement of Structured Information Standards (OASIS) DITA Technical Committee.\n\n\n"}
{"id": "1717878", "url": "https://en.wikipedia.org/wiki?curid=1717878", "title": "Zhongguancun", "text": "Zhongguancun\n\nZhongguancun (), or Zhong Guan Cun, is a technology hub in Haidian District, Beijing, China.\n\nIt is geographically situated in the northwestern part of Beijing city, in a band between the northwestern Third Ring Road and the northwestern Fourth Ring Road. Zhongguancun is very well known in China, and is often referred to as \"China's Silicon Valley\".\n\nZhongguancun has only existed since the 1950s and only became a household name in the early 1980s. The first person who envisioned the future for Zhongguancun was Chen Chunxian, a member of the Chinese Academy of Sciences (CAS), who came up with the idea for a Silicon Valley in China after he visited the U.S. as part of a government-sponsored trip. The location of the Chinese Academy of Sciences within Zhongguancun reinforced, and perhaps was in part responsible for the technological growth in this area.\n\nThroughout the 1980s and still today, Zhongguancun was known as \"electronics avenue,\" because of its connections to information technology and the preponderance of stores along a central, crowded street.\n\nZhongguancun was officially recognized by the central government of China in 1988. It was given the wordy name \"Beijing High-Technology Industry Development Experimental Zone.\"\n\nThe current designation Zhongguancun refers commonly to the original site. However, officially (as of 1999) Zhongguancun has become the \"Zhongguancun Science & Technology Zone.\" It is a zone with seven parks, including Haidian Park, Fengtai Park, Changping Park, Electronics City (in Chaoyang), Yizhuang Park, Desheng Park, and Jianxiang Park.\n\nThe original Zhongguancun is now known as the Haidian Park of the Zhongguancun Zone. The area and environs, however, remain the same.\n\nHailong Market, Guigu Market, Taipingyang Market, Dinghao Market and Kemao Market are the five prominent IT and electronics markets. They are technology bazaars, famous for their \"shops with a shop\", where prices are easily but grudgingly bargained. Zhongguancun shops mainly deal in PC-compatible hardware, peripherals and software. AppleCentre and Apple Experience Centre are also close by.\n\nA very particular sight to visit is the Haidian Christian Church, designed by Hamburg-based architects Gerkan, Marg and Partners.\n\nDue to the proximity and participation of China's two most prestigious universities, Peking University and Tsinghua University, along with the Chinese Academy of Sciences, many analysts elsewhere are optimistic about Zhongguancun's future prospects.\n\nNotable high schools in Zhongguancun include Affiliated High School of Peking University and High School Affiliated to Renmin University of China.\n\nThe State Administration of Foreign Experts Affairs (SAFEA) has its headquarters in Zhongguancun.\n\nThe most famous companies that grew up in Zhongguancun are Stone Group, Founder Group, and Lenovo Group. They were all founded in 1984-85. Stone was the first successful technology company to be operated by private individuals outside the government of China. Founder is a technology company that spun off Peking University. Lenovo Group spun off from Chinese Academy of Sciences with Liu Chuanzhi, a hero of Zhongguancun and current Chairmain, eventually taking the helm. Lenovo purchased IBM's PC division with $1.75 billion in 2005, making it the world's third-largest PC maker. Both Founder and Lenovo Group maintain strong connections to their academic backers, who are significant shareholders.\n\nAccording to the 2004 Beijing Statistical Yearbook, there are over 12,000 high-tech enterprises throughout Zhongguancun's seven parks, with 489,000 technicians employed.\n\nEastdawn Corporation is in the Sinosteel building.\n\nMany world-renowned technology companies built their Chinese headquarters and research centers in Zhongguancun Technology Park, such as Google, Intel, AMD, Oracle Corporation, Motorola, MySpace, Sony, Solstice, and Ericsson. Microsoft has built its Chinese research headquarters in the park that costs $280 million and can accommodate 5000 employees, which was completed in April, 2011, and now houses Microsoft Research Asia.\n\nThe development center of Loongson, which is China's first general-purpose microprocessor design, is also in the Zhongguancun area.\n\nIn addition, many conferences are held in this location, including the annual ChinICT conference - which is the largest Information technology Development and Entrepreneurship event in China.\n\nBeijing Subway Line 4 runs through the Zhongguancun area with stops at Zhongguancun Station and Haidianhuangzhuang Station. Haidianhuangzhuang is also a transfer station with Line 10. In addition to the subway, Zhongguancun is served by many of Beijing's public buses- 26, 302, 304, 307, 332, 333, 355, 365, 384, 386, 466, 498, 549, 579, 584, 601, 608, 611, 614, 630, 634, 641, 653, 671, 681, 689, 696, 697, 699, 717, 740, 913, 944, 982, 983, 特4, 特6, 特9, 特15, 特18, 夜8, 夜9, 运通105, 运通106, 运通109, 运通110, and 运通113.\n\n\n"}
