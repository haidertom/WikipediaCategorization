{"id": "17504593", "url": "https://en.wikipedia.org/wiki?curid=17504593", "title": "1:1 pixel mapping", "text": "1:1 pixel mapping\n\n1:1 pixel mapping is a video display technique applicable to devices with native fixed pixels, such as LCD monitors and plasma displays. A monitor that has been set to 1:1 pixel mapping will display an input source without scaling it, such that each pixel received is mapped to a single native pixel on the monitor. This technique avoids loss of sharpness due to scaling artifacts and normally avoids incorrect aspect ratio due to stretching. If the input resolution is less than the monitor's native resolution, this will result in black borders around the image (e.g. letterboxing or windowboxing).\n\n"}
{"id": "18498287", "url": "https://en.wikipedia.org/wiki?curid=18498287", "title": "An Introduction to Sustainable Development", "text": "An Introduction to Sustainable Development\n\nAn Introduction to Sustainable Development is a 2007 Earthscan book which presents sustainable development as a process that \"meets the needs of the present generation without compromising the ability of future generations to meet their own needs\". This textbook examines the environmental, economic, and social dimensions of sustainable development by exploring changing patterns of consumption, production, and distribution of resources. Case studies include coastal wetlands; community-based water supply and sanitation systems; and sustainable energy, forest, and industrial development.\n\nAuthor Peter P. Rogers is a Professor of Environmental Engineering at Harvard University, USA. Co-authors Kazi F. Jalal and John A. Boyd are lecturers at Harvard’s Extension School.\n"}
{"id": "11275526", "url": "https://en.wikipedia.org/wiki?curid=11275526", "title": "Audio router", "text": "Audio router\n\nAn audio router is a device that transports audio signals from inputs to outputs.\n\nThe number of inputs and outputs varies dramatically. The way routers are described is normally \"number of inputs by number of outputs\" e.g. 2x1, 256x256.\n\nThe type of signals transported - switched can be analogue - Analog - audio signals or Digital. Digital audio usually is in the AES/EBU standard for broadcast use. Broadband routers can route more than one signal type e.g. analogue or more than one type of digital.\n\nBecause any of the inputs can be routed to any output, the internal arrangement of the router is arranged as a number of crosspoints which can be activated to pass the corresponding signal to the desired output.\n\n\n"}
{"id": "34855410", "url": "https://en.wikipedia.org/wiki?curid=34855410", "title": "Biopeople", "text": "Biopeople\n\nBiopeople - Denmark's Life Science Cluster is a publicly funded partnership and National Center established, authorised, and funded by the Ministry for Science and Higher Education to improve innovation, collaboration and education within the National Danish Innovation System. Biopeople is established as a Center at the Faculty of Health and Medical Sciences at University of Copenhagen.\n\nBiopeople was the first European cluster organisation within health and life sciences to receive the recognition of Gold Label of the European Cluster Excellence Initiative (ECEI).\n\nBiopeople helps academia and industry to co-create and develop ideas into new projects, products and services to benefit global health and welfare. Biopeople embraces and clusters universities, research organisations, and hospitals, the National Board of Health (Denmark) / Danish Health and Medical Authority, industry associations as well as pharma, medtech, medical device, food and biotech companies. The aim is to stimulate innovation through activities that bring researchers and stakeholders together across disciplines, sectors and public-private boundaries.\n\nBiopeople embeds all relevant Danish stakeholders. Biopeople is a Center at University of Copenhagen. Member Companies affiliate by in-kind means by participating in innovation activities and projects. Companies include major large companies - fx Novo Nordisk, Lundbeck, LEO Pharma, Danisco, Novozymes, and Chr. Hansen – and many small and medium enterprises.\n\nThe founding partners are:\n\n\n\n\n"}
{"id": "349459", "url": "https://en.wikipedia.org/wiki?curid=349459", "title": "Brazilian Silicon Valley", "text": "Brazilian Silicon Valley\n\nBrazilian Silicon Valley is a term commonly applied to the region of Campinas and in southern region this term is applied for Florianópolis city, Brazil because of its similarity to the 'original' Silicon Valley, located in California in the USA.\n\nCampinas has gained this distinction because it has several comparable features, such as:\n\n\nUntil the 1970s, the Campinas region had few industries and had an economy based on agriculture and in the services and commerce sectors. With the foundation of UNICAMP and the ready availability of high-quality researchers, engineers and students focused on physics, electrical engineering, computer sciences, mathematics, mechanical engineering, etc., a number of high-tech companies started to establish their industrial plants and R&D labs nearby, such as IBM. The municipality of Campinas and those surrounding it began to foster actively the growth of this new area, and the CIATEC I and II (Companhia de Desenvolvimento do Pólo de Alta Tecnologia de Campinas) industrial zones were established around the university campus, in the subdistrict of Barão Geraldo. The Center for Research and Development (CPqD) set up by Telebras, a state holding for the telecommunications industry in Brazil, which had grown enormously under the military regime umbrella, was the second boost to Campinas Silicon Valley. A law was passed by the Federal government, protecting Brazilian-made technology against imports, and this resulted in further growth. Together with UNICAMP researchers a number of pioneering developments occurred in the brand-new areas of lasers, fiber optics, digital telephony, computer technology, software development, and so on. In addition, the Petrobras state-owned oil giant was starting to develop a long range oil exploration program with the aim of making Brazil independent of oil imports, a policy also started by the military for strategic and economic reasons (the oil shock had deeply affected the country), and UNICAMP was one of the leading research universities to participate. In this respect, UNICAMP's open philosophy of collaboration with the private sector (unheard of in Brazil until that time), established by his visionary founder and first rector, Dr. Zeferino Vaz, prepared the way for a unique synergy between industry and university.\n\nOther areas in Brazil are also claiming a similar status to Campinas Silicon Valley, although they are much less organized and with smaller companies. They are:\n\n\n\n"}
{"id": "6987714", "url": "https://en.wikipedia.org/wiki?curid=6987714", "title": "Cebu IT Park", "text": "Cebu IT Park\n\nThe Cebu IT Park (formerly known as Asiatown IT Park) is a 27-hectare mixed use business park in Cebu City, Philippines, envisioned to attract locators in the information technology services. It is developed by \"Cebu Property Ventures and Development Corporation\", a subsidiary of Cebu Holdings, Inc.\n\nTenants include Cebu Bombardier, NEC, SPI Tech, 1&1 Internet Philippines, Inc., Aegis (now acquired by Teleperformance), Convergys (who later acquired both eTelecare and Stream), Qualfon, Promotional USB, Accenture, NCR, IBM, Microsoft, Xlibris/Author Solutions, JP Morgan Chase, and Epson. The main infrastructures found at the park are i1, i2, i3, E-BLOC, TGU Tower, PIPC 15, Skyrise 3, Skyrise 2, Skyrise 1, E-Office One, PIPC 11, PIPC 14, Globe IT, Aegis Tower, the CJRS and The Central Bloc.\n\nIn January 2010, IBM inaugurated its 2nd Global Delivery Center at TGU Tower. IBM established its initial presence in the Philippines in 1937. In 2007, IBM partnered with the Philippines Department of Science and Technology on the Philippine Intellectual Property Policy Strategy, Engineering Research & Development for Technology Program, and the National Technology Business Incubators Program. IBM Philippines Country General Manager James Velasquez said the company recognizes Cebu as the gateway both for its domestic clients in the Visayas and Mindanao, as well as overseas clients.\n\nIt was approved by the Philippine Economic Zone Authority (PEZA) board as an economic zone on April 6, 2000. On February 27, 2001, Presidential Proclamation No. 12 made it an Information Technology Special Economic Zone.\n\nConstruction on eOffice One, the first office modules in Cebu IT Park, began in 2001 and opened in 2002.\n\n\n"}
{"id": "1636593", "url": "https://en.wikipedia.org/wiki?curid=1636593", "title": "Collaboratory", "text": "Collaboratory\n\nA collaboratory, as defined by William Wulf in 1989, is a “center without walls, in which the nation’s researchers can perform their research without regard to physical location, interacting with colleagues, accessing instrumentation, sharing data and computational resources, [and] accessing information in digital libraries” (Wulf, 1989).\n\nBly (1998) refines the definition to “a system which combines the interests of the scientific community at large with those of the computer science and engineering community to create integrated, tool-oriented computing and communication systems to support scientific collaboration” (Bly, 1998, p. 31).\n\nRosenberg (1991) considers a collaboratory as being an experimental and empirical research environment in which scientists work and communicate with each other to design systems, participate in collaborative science, and conduct experiments to evaluate and improve systems.\n\nA simplified form of these definitions would describe the collaboratory as being an environment where participants make use of computing and communication technologies to access shared instruments and data, as well as to communicate with others.\n\nHowever, a wide-ranging definition is provided by Cogburn (2003) who states that “a collaboratory is more than an elaborate collection of information and communications technologies; it is a new networked organizational form that also includes social processes; collaboration techniques; formal and informal communication; and agreement on norms, principles, values, and rules” (Cogburn, 2003, p. 86).\n\nThis concept has a lot in common with the notions of Interlock research, Information Routing Group and Interlock diagrams introduced in 1984.\n\nOther meaning\n\nThe word “collaboratory” is also used to describe an open space, creative process where a group of people work together to generate solutions to complex problems.\n\nThis meaning of the word originates from the visioning work of a large group of people – including scholars, artists, consultant, students, activists, and other professionals – who worked together on the 50+20 initiative aiming at transforming management education.\n\nIn this context, by fusing two elements, “collaboration” and “laboratory”, the word “collaboratory” suggests the construction of a space where people explore collaborative innovations. \nIt is, as defined by Dr. Katrin Muff, “an open space for all stakeholders where action learning and action research join forces, and students, educators, and researchers work with members of all facets of society to address current dilemmas.”\n\nThe concept of the collaboratory as a creative group process and its application are further developed in the book “The Collaboratory: A co-creative stakeholder engagement process for solving complex problems”.\n\nExamples of collaboratory events are provided on the website of the Collaboratory community as well as by Business School Lausanne- a Swiss business school that has adopted the collaboratory method to harness collective intelligence.\n\nProblems of geographic separation are especially present in large research projects. The time and cost for traveling, the difficulties in keeping contact with other scientists, the control of experimental apparatus, the distribution of information, and the large number of participants in a research project are just a few of the issues researchers are faced with.\n\nTherefore, collaboratories have been put into operation in response to these concerns and restrictions. However, the development and implementation proves to be not so inexpensive. From 1992 to 2000 financial budgets for scientific research and development of collaboratories ranged from US$447,000 to US$10,890,000 and the total use ranged from 17 to 215 users per collaboratory (Sonnenwald, 2003). Particularly higher costs occurred when software packages were not available for purchase and direct integration into the collaboratory or when requirements and expectations were not met.\n\nChin and Lansing (2004) state that the research and development of scientific collaboratories had, thus far, a tool-centric approach. The main goal was to provide tools for shared access and manipulation of specific software systems or scientific instruments. Such an emphasis on tools was necessary in the early development years of scientific collaboratories due to the lack of basic collaboration tools (e.g. text chat, synchronous audio or videoconferencing) to support rudimentary levels of communication and interaction. Today, however, such tools are available in off-the-shelf software packages such as Microsoft NetMeeting, IBM Lotus Sametime, Mbone Videoconferencing (Chin and Lansing, 2004). Therefore, the design of collaboratories may now move beyond developing general communication mechanisms to evaluating and supporting the very nature of collaboration in the scientific context (Chin & Lansing, 2004).\n\nThe evolution of the collaboratory\n\nAs stated in Chapter 4 of the 50+20 \"Management Education for the World\" book, \"the term collaboratory was first introduced in the late 1980s to address problems of geographic separation in large research projects related to travel time and cost, difficulties in keeping contact with other scientists, control of experimental apparatus, distribution of information, and the large number of participants. In their first decade of use, collaboratories were seen as complex and expensive information and communication technology (ICT) solutions supporting 15 to 200 users per project, with budgets ranging from 0.5 to 10 million USD. At that time, collaboratories were designed from an ICT perspective to serve the interests of the scientific community with tool-oriented computing requirements, creating an environment that enabled systems design and participation in collaborative science and experiments.\n\nThe introduction of a user-centered approach provided a first evolutionary step in the design philosophy of the collaboratory, allowing rapid prototyping and development circles. Over the past decade the concept of the collaboratory expanded beyond that of an elaborate ICT solution, evolving into a “new networked organizational form that also includes social processes, collaboration techniques, formal and informal communication, and agreement on norms, principles, values, and rules”. The collaboratory shifted from being a tool-centric to a data-centric approach, enabling data sharing beyond a common repository for storing and retrieving shared data sets. These developments have led to the evolution of the collaboratory towards a globally distributed knowledge work that produces intangible goods and services capable of being both developed and distributed around the world using traditional ICT networks.\n\nInitially, the collaboratory was used in scientific research projects with variable degrees of success. In recent years, collaboratory models have been applied to areas beyond scientific research and the national context. The wide acceptance of collaborative technologies in many parts of the world opens promising opportunities for international cooperation in critical areas where societal stakeholders are unable to work out solutions in isolation, providing a platform for large multidisciplinary teams to work on complex global challenges.\n\nThe emergence of open-source technology transformed the collaboratory into its next evolution. The term open-source was adopted by a group of people in the free software movement in Palo Alto in 1998 in reaction to the source code release of the Netscape Navigator browser. Beyond providing a pragmatic methodology for free distribution and access to an end product’s design and implementation details, open-source represents a paradigm shift in the philosophy of collaboration. The collaboratory has proven to be a viable solution for the creation of a virtual organization. Increasingly, however, there is a need to expand this virtual space into the real world. We propose another paradigm shift, moving the collaboratory beyond its existing ICT framework to a methodology of collaboration beyond the tool- and data-centric approaches, and towards an issue-centered approach that is transdisciplinary in nature.\"\n\nThe Collaboratory as a creative group process\n\nCopyright 50+20 \"Management Education for the World\" book\n\n\"Translating the concept of the collaboratory from the virtual space into a real environment demands a number of significant adjustments, leading us to yet another evolution. While the virtual collaboratory could count on ICT solutions to create and maintain an environment of collaboration, real-life interactions require facilitation experts to create and hold a space for members of the community, jointly developing transdisciplinary solutions around issues of concern. The ability to hold a space is central to the vision of management education.\n\nThe technology involved with holding a space implies the ability to create and maintain a powerful and safe learning platform. Such a space invites the whole person (mind, heart, soul and hands) into a place where the potential of a situation is fully realized. Holding a space is deeply grounded in our human heritage, and is still considered an important duty of the elders amongst many indigenous peoples. In Western society, good coaches fulfill a similar role, including the ability to be present in the moment, listening with all senses, being attuned to the invisible potential about to be expressed. As a result, what needs to happen, will happen. Facilitation and coaching experts understand the specific challenges involved in setting up an environment in which a great number of people can meet to discuss solutions that none of them could develop individually. Coaching and facilitation solutions already exist to create and hold such spaces, but are nevertheless distinctly different in a felt sense from the ICT-driven virtual collaboratories we have seen over the past two decades.\n\nThe evolution from the virtual collaboratory bears its own challenges and opportunities. In the co-creative process of the 50+20 vision, we learned to appreciate the power of the collaboratory both in real-life retreats as well as interactions between our gatherings. We propose that the next evolutionary step of the collaboratory will include both the broader community of researchers engaged in collaboratories around the world, as well as stakeholders in management education who seek to transform themselves by providing responsible leadership.\n\nIn our new definition, a collaboratory is an inclusive learning environment where action learning and action research meet. It involves the active collaboration of a diversified group of participants that bring in different perspectives on a given issue or topic. In such a space, learning and research is organized around issues rather than disciplines or theory. Such issues include: hunger, energy, water, climate change, migration, democracy, capitalism, terrorism, disease, the financial crisis, new economic models, management education that serves the world and similarly pressing matters. These issues are usually complex, messy and hard to resolve, demanding creative, systemic and divergent approaches. The collaboratory’s primary aim is to foster collective creativity.\nThe collaboratory is a place where people can think, work, learn together, and invent their respective futures. Its facilitators are highly experienced coaches who act as lead learners and guardians of the collaboratory space. They see themselves as transient gatekeepers of a world in need of new solutions. Subject experts are responsible for providing relevant knowledge and contributing it to the discussion in a relevant and pertinent matter. Students will continue to acquire subject knowledge outside the collaboratories – both through traditional and developing channels (such as online or blended learning).\n\nAs such, the faculty (of a business school, note added by the editor) is challenged to develop their capacities as facilitators and coaches in order to effectively guide these collaborative learning and research processes. To do this, they must step back from their role as experts and rather serve as facilitators in an open, participative and creative process. Faculty training and development needs to include not only a broad understanding of global issues, but also the development of facilitation and coaching skills.\n\nThe circular space of the collaboratory can become the preferred meeting place for citizens to jointly question, discuss and construct new ideas and approaches to resolve environmental, societal and economic challenges on both a regional and global level. Collaboratories should always reflect a rich combination of stakeholders: coaches, business and management faculty, citizens, politicians, entrepreneurs, people from different regions and cultures, youth and elders. Together they assemble differences in perspective, expertise and personal backgrounds, thereby adding a vital creative edge to every encounter, negotiation or problem-solving session.\"\n\nA distinctive characteristic of collaboratories is that they focus on data collection and analysis. Hence the interest to apply collaborative technologies to support data sharing as opposed to tool sharing. Chin and Lansing (2004) explore the shift of collaboratory development from traditional tool-centric approaches to more data-centric ones, to effectively support data sharing. This means more than just providing a common repository for storing and retrieving shared data sets. Collaboration, Chin and Lansing (2004) state, is driven both by the need to share data and to share knowledge about data. Shared data is only useful if sufficient context is provided about the data such that collaborators may comprehend and effectively apply it. It is therefore imperative, according to Chin and Lansing (2004), to know and understand how data sets relate to aspects of overall data space, applications, experiments, projects, and the scientific community, identifying the critical features or properties among which we can mention:\n\n\nHenline (1998) argues that communication about experimental data is another important characteristic of a collaboratory. By focusing attention on the dynamics of information exchange, the study of Zebrafish Information Network Project (Henline, 1998) concluded that the key challenges in creating a collaboratory may be social rather than technical. “A successful system must respect existing social conventions while encouraging the development of analogous mechanisms within the new electronic forum” (Henline, 1998, p. 69). Similar observations were made in the Computer-supported collaborative learning (CSCL) case study (Cogburn, 2003). The author (Cogburn, 2003) is investigating a collaboratory established for researchers in education and other related domains from United States of America and southern Africa. The main finding was that there have been important intellectual contributions on both sides, although the context was that of a developed country working together with a developing one and there have been social as well as cultural barriers. He further develops the idea that a successful CSCL would need to draw the best lessons learned on both sides in computer-mediated communication (CMC) and computer-supported cooperative work (CSCW).\n\nSonnenwald (2003) conducted seventeen interviews with scientists and revealed important considerations. Scientists expect a collaboratory to “support their strategic plans; facilitate management of the scientific process; have a positive or neutral impact on scientific outcomes; provide advantages and disadvantages for scientific task execution; and provide personal conveniences when collaborating across distances” (Sonnenwald, 2003, p. 68). Many scientists looked at the collaboratory as means to achieve strategic goals that were organizational and personal in nature. Other scientists anticipated that the scientific process would speed up when they had access to the collaboratory.\n\nFinholt (1995), based on the case studies of the Upper Atmospheric Research Collaboratory (UARC) and the Medical Collaboratory, establishes a design philosophy: a collaboratory project must be dedicated to a user-centered design (UCD) approach. This means a commitment to develop software in programming environments that allow rapid prototyping, rapid development cycles (Finholt, 1995). A consequence of the user-centered design in the collaboratory is that the system developers must be able to distinguish when a particular system or modification has positive impact on users’ work practices. An important part of obtaining this understanding is producing an accurate picture of how work is done prior to the introduction of technology. Finholt (1995) explains that behavioral scientists had the task of understanding the actual work settings for which new information technologies were developed. The goal of a user-centered design effort was to inject those observations back into the design process to provide a baseline for evaluating future changes and to illuminate productive directions for prototype development (Finholt, 1995).\n\nA similar viewpoint is expressed by Cogburn (2003) who relates the collaboratory to a globally distributed knowledge work, stating that human-computer interaction (HCI) and user-centered design (UCD) principles are critical for organizations to take advantage of the opportunities of globalization and the emergence of an Information society. He (Cogburn, 2003) refers to distributed knowledge work as being a set of “economic activities that produce intangible goods and services […], capable of being both developed and distributed around the world using the global information and communication networks” (Cogburn, 2003, p. 81). Through the use of these global information and communications networks, organizations are able to take part in globally disarticulated production, which means they can locate their research and development facilities almost anywhere in the world, and engineers can collaborate across time zones, institutions and national boundaries.\n\nMeeting expectations is a factor that influences adoption of innovations, including scientific collaboratories. Some of the collaboratories implemented thus far have not been entirely successful. The Mathematics and Computer Science Division of Argonne National Laboratory, Waterfall Glen collaboratory (Henline, 1998) is an illustrative example. This collaboratory had its shares of problems. There have been the occasional technical and social disasters, but most importantly it did not meet all of the collaboration and interaction requirements.\n\nThe vast majority of the evaluations performed thus far are concentrating mainly on the usage statistics (e.g. total number of members, hours of use, amount of data communicated) or on the immediate role in the production of traditional scientific outcomes (e.g. publications and patents). Sonnenwald (2003), however, argues that we should rather look for longer-term and intangible measures such as new and continued relationship among scientists, and subsequent, longer-term creation of new knowledge.\n\nRegardless of the criteria used for evaluation, we must focus on understanding the expectations and requirements defined for a collaboratory. Without such understanding a collaboratory runs the risk of not being adopted.\n\nOlson, Teasley, Bietz, and Cogburn (2002) ascertain some of the success factors of a collaboratory. They are: collaboration readiness, collaboration infrastructure readiness, and collaboration technology readiness.\n\nCollaboration readiness is the most basic pre-requisite for an effective collaboratory, according to Olson, Teasley, Bietz, and Cogburn (2002). Often the critical component to collaboration readiness is based on the concept of “working together in order to achieve a science goal” (Olson, Teasley, Bietz, & Cogburn, 2002, p. 46). Incentives to collaborate, shared principles of collaboration, and experience with the elements of collaboration are also crucial. Successful interaction between users requires a certain amount of common ground. Interactions require a high degree of trust or negotiation, especially when they involve areas where there is a cultural difference. “Ethical norms tend to be culturally specific, and negotiations about ethical issues require high levels of trust” (Olson, Teasley, Bietz, & Cogburn, 2002, p. 49).\n\nWhen analyzing the collaboration infrastructure readiness Olson, Teasley, Bietz, and Cogburn (2002) state that modern collaboration tools require adequate infrastructure to operate properly. Many off-the-shelf applications will run effectively only on state-of-the-art workstations. An important piece of the infrastructure is the technical support necessary to ensure version control, to get participants registered, and to recover in case of disaster. Communications cost is another element which can be critical for collaboration infrastructure readiness (Olson, Teasley, Bietz, & Cogburn, 2002). Pricing structures for network connectivity can affect the choices that users will make and therefore have an effect on the collaboratory’s final design and implementation.\n\nCollaboration technology readiness, according to Olson, Teasley, Bietz, and Cogburn (2002), refers to the fact that collaboration does not involve only technology and infrastructure, but also requires a considerable investment in training. Thus, it is essential to assess the state of technology readiness in the community to ensure success. If the level is too primitive more training is required to bring the users’ knowledge up-to-date.\n\nA comprehensively described example of a collaboratory, the Biological Sciences Collaboratory (BSC) at the Pacific Northwest National Laboratory (Chin & Lansing, 2004), enables the sharing and analysis of biological data through metadata capture, electronic laboratory notebooks, data organization views, data provenance tracking, analysis notes, task management, and scientific workflow management. BSC supports various data formats, has data translation capabilities, and can interact and exchange data with other sources (external databases, for example). It offers subscription capabilities (to allow certain individuals to access data) and verification of identities, establishes and manages permissions and privileges, and has data encryption capabilities (to ensure secure data transmission) as part of its security package.\n\nBSC also provides a data provenance tool and a data organization tool. These tools allow a hierarchical tree to display the historical lineage of a data set. From this tree-view the scientist may select a particular node (or an entire branch) to access a specific version of the data set (Chin & Lansing, 2004).\n\nThe task management provided by BSC allows users to define and track tasks related to a specific experiment or project. Tasks can have deadlines assigned, levels of priority, and dependencies. Tasks can also be queried and various reports produced. Related to task management, BSC provides workflow management to capture, manage, and supply standard paths of analyses. The scientific workflow may be viewed as process templates that captures and semi-automate the steps of an analysis process and its encompassing data sets and tools (Chin & Lansing, 2004).\n\nBSC provides project collaboration by allowing scientists to define and manage members of their group. Security and authentication mechanisms are therefore applied to limit access to project data and applications. Monitoring capability allows for members to identify other members that are online working on the project (Chin & Lansing, 2004).\n\nBSC offers community collaboration capabilities: scientists may publish their data sets to a larger community through the data portal. Notifications are in place for scientists interested in a particular set of data - when that data changes, the scientists get notification via email (Chin & Lansing, 2004).\n\nThe Collaboratory for Adaptation to Climate Change is an interdisciplinary project funded by a grant from the National Science Foundation’s Office of CyberInfrastructure and supported by the University of Notre Dame.\n\nThe research mission of the Collaboratory is to improve the dissemination and integration of knowledge that will inform the development of prescient adaptation strategies and policies. Goals of the Collaboratory include:\n\n\nThe Collaboratory is not a place for political debate, but is a place for pitching ideas and information with legitimate scientific backing that inform adaptation decisions.\n\nThe website is a resource for research, education, and collaboration in the area of adaptation and climate change. It incorporates a multitude of tools, which take several forms including biological simulation, searchable clearinghouses of legal information, and dissemination of emerging opinion from experts on the benefits and risks of adaptation. These tools can be used individually and in an integrative way to inform decision-making, research, and awareness. The site was created in April 2011 and is under continual development. Check back often for new additions and improvements.\n\nAdaptation, together with reduction of greenhouse gas emissions, is an essential part of solving the climate change crisis. Adaptation includes all of the steps that humans might take to help reduce the effects of climate change that are projected to occur in the 21st century and beyond. The Intergovernmental Panel on Climate Change (IPCC), and international body of scientists, projects that the global climate is likely to shift 2–6 °C (4–11 °F) warmer within 100 years. That amount of warming will disrupt ecosystems, raise sea levels, and perturb agriculture and human infrastructure. It also will affect human health. We can take steps now to reduce these effects in some instances and places. Adaptation includes, for example, helping species relocate to newly-suitable locations, protecting shores from rising waters and contoling pests that increase or spread under altered conditions. In 2011, Adapt is focused on adaptation for wildlife and biological resources.\n\nWe welcome scientists, natural resource managers and planners, students and the interested public to participate.Take a tour of our web site and see how you can use our infrastructure to further your own research and educational activities. Create your own account. It's free and will give you access to our online simulation tools and other features. Become a contributor by uploading your own presentations and simulation tools for others to share. Ask a question in our community forum, and let the community help you out.\n\nPancerella, Rahn, and Yang (1999) analyzed the Diesel Combustion Collaboratory (DCC) which was a problem-solving environment for combustion research. The main goal of DCC was to make the information exchange for the combustion researchers more efficient. Researchers would collaborate over the Internet using various DCC tools. These tools included “a distributed execution management system for running combustion models on widely distributed computers (distributed computing), including supercomputers; web accessible data archiving capabilities for sharing graphical experimental or modeling data; electronic notebooks and shared workspaces for facilitating collaboration; visualization of combustion data; and videoconferencing and data conferencing among researchers at remote sites” (Pancerella, Rahn, & Yang, 1999, p. 1).\n\nThe collaboratory design team defined the requirements to be (Pancerella, Rahn, & Yang, 1999):\n\n\nEach of these requirements had to be done securely and efficiently across the Internet. Resources availability was a major concern because many of the chemistry simulations could run for hours or even days on high-end workstations and produce Kilobytes to Megabytes of data sets. These data sets had to be visualized using simultaneous 2-D plots of multiple variables (Pancerella, Rahn, & Yang, 1999).\n\nThe deployment of the DCC was done in a phased approach. The first phase was based on iterative development, testing, and deployment of individual collaboratory tools. Once collaboratory team members had adequately tested each new tool, it was deployed to combustion researchers. The deployment of the infrastructure (videoconferencing tools, multicast routing capabilities, and data archives) was done in parallel (Pancerella, Rahn, & Yang, 1999). The next phase was to implement full security in the collaboratory. The primary focus was on two-way synchronous and multi-way asynchronous collaborations (Pancerella, Rahn, & Yang, 1999). The challenge was to balance the increased access to data that was needed with the security requirements. The final phase was the broadening of the target research to multiple projects including a broader range of collaborators.\n\nThe collaboratory team found that the highest impact was perceived by the geographically separated scientists that truly depended on each other to achieve their goals. One of the team’s major challenges was to overcome the technological and social barriers in order to meet all of the objectives (Pancerella, Rahn, & Yang, 1999). User openness and low maintenance security collaboratories are hard to achieve, therefore user feedback and evaluation are constantly required.\n\nOther collaboratories that have been implemented and can be further investigated are:\n\nSpecial consideration should be attributed to TANGO (Henline, 1998) because it is a step forward in implementing collaboratories, as it has distance learning and health care as main domains of operation. Henline (1998) mentions that the collaboratory has been successfully used to implement applications for distance learning, command and control center, telemedical bridge, and a remote consulting tool suite.\n\nTo date, most collaboratories have been applied largely in scientific research projects, with various degrees of success and failure. Recently, however, collaboratory models have been applied to additional areas of scientific research in both national and international contexts. As a result, a substantial knowledge base has emerged helping us in understanding their development and application in science and industry (Cogburn, 2003). Extending the collaboratory concept to include both social and behavioral research as well as more scientists from the developing world could potentially strengthen the concept and provide opportunities of learning more about the social and technical factors that support a distributed knowledge network (Cogburn, 2003).\n\nThe use of collaborative technologies to support geographically distributed scientific research is gaining wide acceptance in many parts of the world. Such collaboratories hold great promise for international cooperation in critical areas of scientific research and not only. As the frontiers of knowledge are pushed back the problems get more and more difficult, often requiring large multidisciplinary teams to make progress. The collaboratory is emerging as a viable solution, using communication and computing technologies to relax the constraints of distance and time, creating an instance of a virtual organization. The collaboratory is both an opportunity with very useful properties, but also a challenge to human organizational practices (Olson, 2002).\n\n\n"}
{"id": "3561034", "url": "https://en.wikipedia.org/wiki?curid=3561034", "title": "Color magazine (lighting)", "text": "Color magazine (lighting)\n\nThe term boomerang is also used to describe a color magazine.\n\nColor magazines are now becoming rarer with the widespread availability of programmable-colour LED lighting.\n"}
{"id": "40925", "url": "https://en.wikipedia.org/wiki?curid=40925", "title": "Communications system", "text": "Communications system\n\nIn telecommunication, a communications system is a collection of individual communications networks, transmission systems, relay stations, tributary stations, and data terminal equipment (DTE) usually capable of interconnection and interoperation to form an integrated whole. The components of a communications system serve a common purpose, are technically compatible, use common procedures, respond to controls, and operate in union. \n\nTelecommunications is a method of communication (e.g., for sports broadcasting, mass media, journalism, etc.). Communication is the act of conveying intended meanings from one entity or group to another through the use of mutually understood signs and semiotic rules\n\nAn optical communication system is any form of telecommunication that uses light as the transmission medium. Equipment consists of a transmitter, which encodes a \"message\" into an optical \"signal\", a \"communication channel\", which carries the signal to its destination, and a receiver, which reproduces the message from the received optical signal. Fiber-optic communication systems transmit information from one place to another by sending light through an optical fiber. The light forms a carrier signal that is modulated to carry information.\n\nA radio communication system is composed of several communications subsystems that give exterior communications capabilities. A radio communication system comprises a transmitting conductor in which electrical oscillations or currents are produced and which is arranged to cause such currents or oscillations to be propagated through the free space medium from one point to another remote therefrom and a receiving conductor at such distant point adapted to be excited by the oscillations or currents propagated from the transmitter.\n\nPower line communication systems operate by impressing a modulated carrier signal on power wires. Different types of powerline communications use different frequency bands, depending on the signal transmission characteristics of the power wiring used. Since the power wiring system was originally intended for transmission of AC power, the power wire circuits have only a limited ability to carry higher frequencies. The propagation problem is a limiting factor for each type of power line communications.\n\nA duplex communication system is a system composed of two connected parties or devices which can communicate with one another in both directions. The term \"duplex\" is used when describing communication between two parties or devices. Duplex systems are employed in nearly all communications networks, either to allow for a communication \"two-way street\" between two connected parties or to provide a \"reverse path\" for the monitoring and remote adjustment of equipment in the field.\nAn Antenna is basically a small length of a qwert conductor that is used to radiate or receive electromagnetic waves.\nIt acts as a conversion device.At the transmitting end it converts high frequency current into electromagnetic waves. At the receiving end it transforms electromagnetic waves into electrical signals that is fed into the input of the receiver. several types of antenna are used in communication.\n\nExamples of communications subsystems include the Defense Communications System (DCS).\n\nA tactical communications system is a communications system that \n(a) is used within, or in direct support of tactical forces\n(b) is designed to meet the requirements of changing tactical situations and varying environmental conditions, \n(c) provides securable communications, such as voice, data, and video, among mobile users to facilitate command and control within, and in support of, tactical forces, and \n(d) usually requires extremely short installation times, usually on the order of hours, in order to meet the requirements of frequent relocation.\n\nAn Emergency communication system is any system (typically computer based) that is organized for the primary purpose of supporting the two way communication of emergency messages between both individuals and groups of individuals. These systems are commonly designed to integrate the cross-communication of messages between are variety of communication technologies.\n\nAn Automatic call distributor (ACD) is a communication system that automatically queues, assigns and connects callers to handlers. This is used often in customer service (such as for product or service complaints), ordering by telephone (such as in a ticket office), or coordination services (such as in air traffic control).\n\nA Voice Communication Control System (VCCS) is essentially an ACD with characteristics that make it more adapted to use in critical situations (no waiting for dialtone, or lengthy recorded announcements, radio and telephone lines equally easily connected to, individual lines immediately accessible etc..)\n\nSources can be classified as electric or non-electric; they are the origins of a message or input signal. Examples of sources include but are not limited to the following:\n\nSensors, like microphones and cameras, capture non-electric sources, like sound and light (respectively), and convert them into electrical signals. These types of sensors are called input transducers in modern analog and digital communication systems. Without input transducers there would not be an effective way to transport non-electric sources or signals over great distances, i.e. humans would have to rely solely on our eyes and ears to see and hear things despite the distances. Not good!\n\nOther examples of input transducers include:\n\nOnce the source signal has been converted into an electric signal, the transmitter will modify this signal for efficient transmission. In order to do this, the signal must pass through an electronic circuit containing the following components:\nAfter the signal has been amplified, it is ready for transmission. At the end of the circuit is an antenna, the point at which the signal is released as electromagnetic waves (or electromagnetic radiation).\n\nA communication channel is simply referring to the medium by which a signal travels. There are two types of media by which electrical signals travel, i.e. guided and unguided. Guided media refers to any medium that can be directed from transmitter to receiver by means of connecting cables. In optical fiber communication, the medium is an optical (glass-like) fiber. Other guided media might include coaxial cables, telephone wire, twisted-pairs, etc... The other type of media, unguided media, refers to any communication channel that creates space between the transmitter and receiver. For radio or RF communication, the medium is air. Air is the only thing between the transmitter and receiver for RF communication while in other cases, like sonar, the medium is usually water because sound waves travel efficiently through certain liquid media. Both types of media are considered unguided because there are no connecting cables between the transmitter and receiver. Communication channels include almost everything from the vacuum of space to solid pieces of metal; however, some mediums are preferred more than others. That is because differing sources travel through subjective mediums with fluctuating efficiencies.\n\nOnce the signal has passed through the communication channel, it must be effectively captured by a receiver. The goal of the receiver is to capture and reconstruct the signal before it passed through the transmitter (i.e. the A/D converter, modulator and encoder). This is done by passing the \"received\" signal through another circuit containing the following components:\nMost likely the signal will have lost some of its energy after having passed through the communication channel or medium. The signal can be boosted by passing it through a signal amplifier.\nWhen the analog signal converted into digital signal.\n\nThe output transducer simply converts the electric signal (created by the input transducer) back into its original form. Examples of output transducers include but are not limited to the following:\n\nSome common pairs of input and output transducers include:\n\nAgain, input transducers convert non-electric signals like voice into electric signals that can be transmitted over great distances very quickly. Output transducers convert the electric signal back into sound or picture, etc... There are many different types of transducers and the combinations are limitless.\n\n\n"}
{"id": "33897723", "url": "https://en.wikipedia.org/wiki?curid=33897723", "title": "Coordinatograph", "text": "Coordinatograph\n\nA coordinatograph is an instrument which mechanically plots X and Y coordinates onto a surface, such as in compiling maps or in plotting control points such as in electronic circuit design.\n\nOne historic application of a coordinatograph was a machine that precisely placed and cut rubylith to create photomasks for early integrated circuits including some of the earliest generations of the modern PC microprocessor. The coordinatograph produced layout would then be photographically reduced 100:1 to create the production photomask.\n\n\n"}
{"id": "24023788", "url": "https://en.wikipedia.org/wiki?curid=24023788", "title": "Crystatech", "text": "Crystatech\n\nCrystaTech Inc. is a supplier of process technology to the energy industry. CrystaTech commercializes the patented Crystasulf process. CrystaSulf is the first commercially available product to provide low cost hydrogen sulfide (HS) removal from gas streams.\n\nThe company was founded in 1999 and is financially backed by the Gas Technology Institute and major energy companies through sponsored clean energy technology development. The corporate office is located in Austin, Texas.\n\nCrystaTech is a member of the Gas Processors Suppliers Association.\n\nRegional offices are in Alberta, Canada and Houston, Texas. All early stage R&D takes place at the Gas Technology Institute in Des Plaines, Illinois. Representative customers include Total, Petrobank Energy and Resources Ltd., Queensland Energy Resources, U.S. Department of Energy, Luminant, and American Electric Power.\n\nKey People\n\n\n\n"}
{"id": "6534081", "url": "https://en.wikipedia.org/wiki?curid=6534081", "title": "DCL Technology Demonstrator programme", "text": "DCL Technology Demonstrator programme\n\nThe US DCL (Detection Classification and Localisation) demonstrator program is aimed at proving that an active torpedo detection system is able to resolve a salvo of torpedoes with sufficient time and accuracy that an anti-torpedo torpedo may be fired back to hit and destroy the threat.\n\nThe DCL systems consist of an active source emitter which sends high-frequency pings into the water. Reflections from in-water objects are received by a towed array tuned to those frequencies. By processing the reflections it is possible to determine whether objects are torpedoes, or non-threat objects.\n\nThe system is also combined with a passive acoustic towed array specifically designed for torpedo detection. The passive acoustic array is able to analyse the structured sound emanating from a torpedo and thereby classify the weapon type and mode of operation.\n\nTwo teams are currently building alternative DCL demonstration systems, the first to test was Ultra Electronics who in 2006 successfully resolved a salvo of torpedoes. The second company APC has yet to undergo tests.\n\nThe aim of the programme is to resolve threats sufficiently well that an anti-torpedo torpedo may be fired at the threat to neutralise it (a hard-kill solution). This differs from the UK S2170 Surface Ship Torpedo Defence solution which utilises soft-kill.\n"}
{"id": "58230778", "url": "https://en.wikipedia.org/wiki?curid=58230778", "title": "Dave's Redistricting", "text": "Dave's Redistricting\n\nDave's Redistricting is an online web app created by Dave Bradlee that allows anyone to redistrict a U.S. state's legislative districts.\n\nAccording to Bradlee, the software was designed to \"put power in people's hands,\" and so that they \"can see how the process works, so it's a little less mysterious than it was 10 years ago.\" Bradlee has noticed that many citizens are taking this process seriously and using his app to create legitimate redistricting maps that could be put in place. Some websites have called Bradlee the pioneer and cause of the rise of do-it-yourself redistricting.\n\nDave's Redistricting has frequently been mentioned as a resource that can be used to combat gerrymandering, given that the public has free access to it.\n\nPolitical science firms such as FiveThirtyEight have used the website to draw examples of gerrymandered districts.\n\nUsers can redraw the congressional districts for all 50 states with a given Cook PVI. With the use of PVI, any state can knowingly be gerrymandered to favor one political party over the other.\n\n"}
{"id": "10458499", "url": "https://en.wikipedia.org/wiki?curid=10458499", "title": "Democratic rationalization", "text": "Democratic rationalization\n\nDemocratic rationalization is term used by Andrew Feenberg in his article \"Subversive Rationalization: Technology, Power and Democracy with technology.\" Feenberg argues against the idea of \"technological determinism\" citing flaws in its two fundamental theses.\n\nThe first is the \"thesis of unilinear progress\". This is the belief that technological progress follows a direct and predictable path from lower to higher levels of complexity and that each stage along this path is necessary for progress to occur (Feenberg 211).\n\nThe second is the \"thesis of determination by the base\". This is the concept that in a society where a technology had been introduced, that society must organize itself or adapt to the technology (Feenberg 211).\n\nIn his argument against the former thesis Feenberg says that constructivist studies of technology will lead us to realize that there is not a set path by which development of technologies occur but rather an emerging of similar technologies at the same time leading to a multiplicity of choices. These choices are made based upon certain social factors and upon examining them we will see that they are not deterministic in nature (Feenberg 212).\n\nArguing against the latter thesis, Feenberg calls to our attention social reforms that have been mandated by governments mainly in regards to the protection of its citizens and laborers. Most of the time these mandates are widely accepted after being passed through the governing body. At which point technology and industry will reform and re-evolve to meet the new standards in a way that has greater efficiency than it did so previously (Feenberg 214)\n\n\n"}
{"id": "43683079", "url": "https://en.wikipedia.org/wiki?curid=43683079", "title": "Distributed Processing Technology", "text": "Distributed Processing Technology\n\nDistributed Processing Technology (DPT) was founded in 1977, in Maitland, Florida. DPT was an early pioneer in computer storage technology, popularizing the use of disk caching in the 1980s and 1990s. DPT was the first company to design, manufacture and sell microprocessor-based intelligent caching disk controllers to the OEM computer market. Prior to DPT, disk caching technology had been implemented in proprietary hardware in mainframe computing to improve the speed of disk access.\n\nDPT's products popularized the use of disk caching in the 1980s. According to Bill Brothers, Unix product manager at the Santa Cruz Operation (SCO), a computer operating system vendor, \"The kind of performance those guys (DPT) produce is phenomenal. It's unlike any other product on the market.\"\n\nDPT was founded by Steve Goldman, who served as the President and Chief Executive Officer until DPT was acquired by Adaptec in November 1999.\n\n"}
{"id": "54953193", "url": "https://en.wikipedia.org/wiki?curid=54953193", "title": "Doppler Labs", "text": "Doppler Labs\n\nDoppler Labs was a San Francisco-based audio technology company. Founded in 2013 in New York City, the company's mission was to make computing more immersive and human. The company designed and manufactured in-ear computing technology, including earplugs and wireless smart earbuds.\n\nDoppler Labs was co-founded by Noah Kraft and Fritz Lanman. Kraft had previously worked in the entertainment industry, while Lanman was an executive at Microsoft and a prominent angel investor.\n\nIn July 2015, Doppler raised $17 million in Series B funding bringing the company's total funding to over $50 million. The round was led by The Chernin Group, Wildcat Capital Management, and Acequia Capital and included luminary investors like Henry Kravis, David Geffen, Blake Krikorian, Dan Gilbert, David Bonderman and Barry Sternlicht.\n\nDoppler Labs first product was DUBS Acoustic Filters, high-tech ear plugs designed that use a proprietary 17-piece physical acoustic filter system to reduce the sound pressure at different frequencies while maintaining acoustical fidelity. In July 2016, Doppler Labs Labs launched Here Active Listening at the Coachella Valley Music and Arts Festival and in 2017 launched its flagship product Here One, a pair of wireless smart earbuds that allow users to selectively filter ambient sound, stream music, and amplify speech. It can also be used to take phone calls and selectively filter certain sounds, such as background noise. Here One has been called the world’s first in-ear computer.\n\nIn March 2017, Doppler Labs sued Bose for trademark infringement of their Here Buds trademark.\n\nThe company supported the Over-the-Counter (OTC) Hearing Aid Act of 2017, which passed both houses of Congress.\n\nOn November 1, 2017, Doppler Labs announced that the company would be winding down operations, and officially closed on December 1. The company cited problems raising additional Series C funding as the reason for the company shutting down. Wired wrote that the company unsuccessfully explored options to stay afloat including partnership, investment, and acquisition from companies such as Microsoft, Apple, Google, Amazon, and Facebook. It was preparing to launch its next product, Here Two, in 2018.\n\nBefore voice assistants or true wireless technology were prevalent, Doppler Labs envisioned that computing would move onto the body and into the ear and that voice would become a more primary interface for how humans interact with technology. With Apple’s removal of the headphone jack, the launch of the AirPods, and the prevalence of Alexa, the smart earbud category that Doppler helped create was expected to become a $40 billion industry by 2020.\n\nThe company develops and distributes the Here One Wireless Smart Earbuds, a pair of wireless 3-in-1 earbuds that combine premium audio streaming, smart noise cancellation, and speech enhancement into a single form factor, as well as DUBS Acoustic Filters high-tech earplug. The company previously developed and released the predecessor to Here One called Here Active Listening, which was originally launched on Kickstarter.\n\nDoppler Labs has announced and demonstrated future product features, including real-time language translation and its “machine hearing” system.\n\nIn addition to its pre-existing partnerships with the Tao Group, Coachella, Bonnaroo and Outside Lands, in November 2016, Doppler Labs announced seven new partnerships with The New York Philharmonic, the Cleveland Cavaliers, the Fine Arts Museums of San Francisco, JetBlue, Gimlet Media, MADE Fashion and The New York Mets to bring Here One technology to sporting events, museums, concert halls, and other live environments.\n\nIn December 2016, they also partnered with the Global Citizen Festival to launch #HereTogether, a movement aimed at bringing greater global awareness around efforts to prevent hearing loss and to promote innovation in hearing accessibility As part of this initiative, Doppler Labs announced its Hearing Bill of Rights in April 2017\n\n"}
{"id": "23048878", "url": "https://en.wikipedia.org/wiki?curid=23048878", "title": "EchoStar Mobile", "text": "EchoStar Mobile\n\nEchoStar Mobile was set up in 2008 as Solaris Mobile, a joint venture company between SES and Eutelsat Communications to develop and commercialize the first geostationary satellite systems in Europe for broadcasting video, radio and data to in-vehicle receivers and to mobile devices, such as mobile phones, portable media players and PDAs. In January 2014 all stock in Solaris Mobile was acquired by EchoStar Corporation and in March 2015 the company was renamed EchoStar Mobile.\n\nThe agreement to set up Solaris Mobile was reached in 2006 with the company formed in 2008. SES and Eutelsat – both successful European satellite operators, providing TV and other services from geostationary satellites to millions of cable and direct-to-home viewers – invested €130m in the venture. The services to be developed included video, radio, multimedia data, interactive services, and voice communications, with the primary aim of delivering mobile television any time, anywhere. Its headquarters is in Dublin, in the Republic of Ireland.\n\nSolaris Mobile's first commercial contract was with Italian media publishing group Class Editori, to launch a digital radio service in Italy. A hybrid satellite/terrestrial network will initially be deployed in Milan, in October 2011 and extended across the country in 2012. Solaris claims that the network will enable Italians to access dozens of new digital radio channels broadcasting music, news, entertainment and sports, in their original format with continuity of reception across the entire country, and that the digital audio signal will be complemented with new visual media services such as programme information and traffic data.\n\nThe EU Telecoms Commissioner, Viviane Reding, has commented, \"Mobile satellite services have huge potential: they can enable Europeans to access new communication services, particularly in rural and less populated regions.\"\n\nSolaris Mobile primarily intends to provide mobile TV and interactive services to handheld and vehicle receivers. For in-vehicle use, the mobile satellite receivers could also double as web browsers providing full Internet access, and deliver interactive services such as online reservations, emergency warnings, or toll payments.\n\nSolaris claims the technology brings a \"fully-fledged TV experience\" to mobile television, unavailable with purely terrestrial systems, delivering high-quality live TV, viable at high vehicle speeds, dozens of TV channels per country, and universal coverage.\n\nThe coverage across Europe will also enable the system to be used for situations when other means of communication are not possible, such as gathering data (traffic, weather, pollution) from moving vehicles, and support for emergency and rescue services in isolated regions, under extreme conditions or when terrestrial networks have been compromised.\n\nTo avoid the requirement for mobile phones with S-band reception for the satellite services, Solaris Mobile has developed a 'Pocket Gateway' in conjunction with Finnish company Elekrobit. The Gateway is a compact S-band receiver which decodes DVB-SH transmissions from the Solaris satellite and relays them over WiFi to any compatible handset with a web browser. The Gateway is also planned to be used in vehicles with a roof-mounted antenna for S-band reception with services accessed on passengers' mobile phones. The technology was demonstrated at the GSMA Mobile World Congress in Barcelona in February 2010.\n\nThe Solaris Mobile services use DVB-SH technology to deliver IP based data and media content to handheld and in-vehicle terminals using a hybrid satellite/terrestrial system with satellite transmission serving the whole of Europe and beyond, and terrestrial repeaters for urban and indoor penetration.\n\nThe S-band frequencies used (2.00 GHz) are reserved for the exclusive use of satellite and terrestrial mobile services, and sit alongside the UMTS frequencies already in use across Europe for 3G terrestrial mobile phone services, allowing the reuse of existing cellular towers and antennas, and the simple incorporation of Solaris services in mobile handsets.\n\nHandsets equipped with the first DVB-SH chipsets were successfully demonstrated live at the \"Mobile World Congress\" in Barcelona in February 2008.\n\nSolaris was intended to first use the Eutelsat W2A satellite at 10° east, which contains an S-band payload, and was scheduled for launch in early 2009. However, following the successful launch on April 3, 2009, the S-band payload was found to show \"an anomaly\" which has put in doubt the payload's capability to provide mobile satellite services for Solaris.\n\nFurther testing of the satellite was undertaken to establish its future in the Solaris programme. Investigation of S-band payload has confirmed significant non-compliance from its original specifications. On 1 July 2009, Solaris Mobile filed an insurance claim. The technical findings indicate that the company should be able to offer some, but not all of the services it was planning to offer.\n\nOn June 30, 2008 the European Parliament and the Council adopted the European’s Decision to establish a single selection and authorisation process to ensure a coordinated introduction of mobile satellite services (MSS) in Europe. The selection process was launched in August 2008 and attracted four applications by prospective operators (ICO, Inmarsat, Solaris Mobile, TerreStar).\n\nIn May 2009, the European Commission selected two operators, Inmarsat Ventures and Solaris Mobile, giving these operators \"the right to use the specific radio frequencies identified in the Commission's decision and the right to operate their respective mobile satellite systems\". EU Member States now have to ensure that the two operators have the right to use the specific radio frequencies identified in the Commission's decision and the right to operate their respective mobile satellite systems for 18 years from the selection decision. The operators are compelled to start operations within 24 months from the selection decision.\n\nAlthough the EU’s decision was announced days after the apparent failure of the payload intended to serve Solaris, the company remains confident of \"its ability to meet the commitments made under the European Commission selection process\".\n\nIn May 2010, Solaris Mobile announced that following the granting of spectrum by the European Commission, the company had been actively pursuing licenses from European member states and it had just been granted 18-year licences to operate Mobile Satellite Services in France, Sweden and Germany, to add to existing licenses for Finland, Luxembourg, Italy, and Slovenia.\n\n\n"}
{"id": "43751582", "url": "https://en.wikipedia.org/wiki?curid=43751582", "title": "Equivalent input", "text": "Equivalent input\n\nEquivalent input (also input-referred or input-related), is a method of referring to the signal or noise level at the output of a system as if it were an input to the same system. This is accomplished by removing all signal changes (e.g. amplifier gain, transducer sensitivity, etc.) to get the units to match the input.\n\nA microphone converts acoustical energy to electrical energy. Microphones have some level of electrical noise at their output. This noise may have contributions from random diaphragm movement, thermal noise, or a dozen other sources, but those can all be thought of as an imaginary acoustic noise source injecting sound into the (now noiseless) microphone. The units on this noise are no longer volts, but units of sound pressure (pascals or dBSPL), which can be directly compared to the desired sound pressure inputs.\n\nA device which uses a microphone may be susceptible to electromagnetic interference which causes sonic artifacts. The problem is not in the microphone, but the interference level can be \"related\" back to the input to compare to the level of typical inputs to see how audible the artifact is.\n"}
{"id": "41896815", "url": "https://en.wikipedia.org/wiki?curid=41896815", "title": "Fast interrupt request", "text": "Fast interrupt request\n\nFast Interrupt Requests (FIQs) are a specialized type of Interrupt Request, a standard technique used in computer CPUs to deal with events which need to be processed as they occur such as receiving data from a network card, or keyboard or mouse actions. FIQs are specific to the ARM CPU architecture, which supports two types of interrupts; FIQs for fast, low latency interrupt handling and Interrupt Requests (IRQs), for more general interrupts.\n\nAn FIQ takes priority over an IRQ in an ARM system. Also, only one FIQ source at a time is supported. This helps reduce interrupt latency as the interrupt service routine can be executed directly without determining the source of the interrupt. A context save is not required for servicing an FIQ since it has its own set of banked registers. This reduces the overhead of context switching.\n"}
{"id": "56776732", "url": "https://en.wikipedia.org/wiki?curid=56776732", "title": "Fiber optic display", "text": "Fiber optic display\n\nA fiber optic display is a light-emitting display that uses fiber optics to display images, text, and notification lights. Fiber-optic displays can either be static or dynamic, with the typical lighting source being halogen light bulbs.\n\nStatic fiber optic displays have been commonly used for some types of traffic signals. One common use for static fiber optic displays are lane control lights, which display either a green downward-pointing arrow or a red X to indicate the open/closed status of road lanes.\n\nDynamic fiber optic displays typically display alphanumeric text, and utilize electromechanical shutters to open or close the ends of the fiber strands to display an alphanumeric pixel. These type of displays were commonly used as variable-message signs on highways. Compared to eggcrate displays, dynamic fiber optic displays offered lower energy consumption due to requiring fewer bulbs, and offered improved nighttime legibility. For daytime legibility, they were sometimes combined with flip-disc displays to be reflective in daylight and emissive at night.\n"}
{"id": "21150165", "url": "https://en.wikipedia.org/wiki?curid=21150165", "title": "Growth of photovoltaics", "text": "Growth of photovoltaics\n\nWorldwide growth of photovoltaics has been an exponential curve between 1992–2017. During this period of time, photovoltaics (PV), also known as solar PV, evolved from a niche market of small scale applications to a mainstream electricity source. When solar PV systems were first recognized as a promising renewable energy technology, programs, such as feed-in tariffs, were implemented by a number of governments in order to provide economic incentives for investments. For several years, growth was mainly driven by Japan and pioneering European countries. As a consequence, cost of solar declined significantly due to experience curve effects like improvements in technology and economies of scale.\n\nExperience curves describe that the price of a thing decreases with the sum-total ever produced. PV growth increased even more rapidly when production of solar cells and modules started to ramp up in the USA with their Million Solar Roofs project, and when renewables were added to China's 2011 five-year-plan for energy production. Since then, deployment of photovoltaics has gained momentum on a worldwide scale, particularly in Asia but also in North America and other regions, where solar PV by 2015–17 was increasingly competing with conventional energy sources as grid parity has already been reached in about 30 countries.\n\nProjections for photovoltaic growth are difficult and burdened with many uncertainties. Official agencies, such as the International Energy Agency consistently increased their estimates over the years, but still fell short of actual deployment.\n\nHistorically, the United States was the leader of installed photovoltaics for many years, and its total capacity amounted to 77 megawatts in 1996—more than any other country in the world at the time. Then, Japan was the world's leader of produced solar electricity until 2005, when Germany took the lead and by 2016 had a capacity of over 40 gigawatts. However, in 2015, China became world's largest producer of photovoltaic power, and in 2017 became the first country to surpass the 100 GW of cumulative installed PV capacity. China is expected to be the leader in installed PV capacity, and along with India and US, it is forecasted to be the largest market for solar PV installations in the coming decade.\n\nBy the end of 2017, cumulative photovoltaic capacity reached about 401 gigawatts (GW), estimated to be sufficient to supply 2.1% of global electricity demand. Solar contributed 8%, 7.4% and 7.1% to the respective annual domestic consumption in Italy, Greece and Germany. The European Photovoltaic Industry Association, a solar industry trade group, claims installed worldwide capacity will more than double or even triple to more than 500 GW between 2016 and 2020; by 2050, it claims solar power will become the world's largest source of electricity. Such an achievement would require PV capacity to grow to 4,600 GW, of which more than half was forecast to be deployed in China and India.\n\nNameplate capacity denotes the peak power output of power stations in unit watt prefixed as convenient, to e.g. kilowatt (kW), megawatt (MW) and gigawatt (GW). Because power output for variable renewable sources is unpredictable, however, using nameplate capacity as a metric significantly overstates a source's average generation. Thus, capacity is typically multiplied by a suitable capacity factor, which takes into account varying conditions - weather, nighttime, latitude, maintenance, etc. to give energy planners an idea of a source's value to the public. In addition, depending on context, the stated peak power may be prior to a subsequent conversion to alternating current, e.g. for a single photovoltaic panel, or include this conversion and its loss for a grid connected photovoltaic power station. Worldwide, the average solar PV capacity factor is 11%.\n\nWind power has different characteristics, e.g. a higher capacity factor and about four times the 2015 electricity production of solar power. Compared with wind power, photovoltaic power production correlates well with power consumption for air-conditioning in warm countries. a handful of utilities have started combining PV installations with battery banks, thus obtaining several hours of dispatchable generation to help mitigate problems associated with the duck curve after sunset.\n\nFor a complete history of deployment over the last two decades, also see section \"History of deployment\".\n\nIn 2017, photovoltaic capacity increased by 95 GW, with a 34% growth year-on-year of new installations. Cumulative installed capacity exceeded 401 GW by the end of the year, sufficient to supply 2.1 percent of the world's total electricity consumption.\n\nAs of 2018, Asia was the fastest growing region, with almost 75% of global installations. China alone accounted for more than half of worldwide deployment in 2017. In terms of cumulative capacity, Asia was the most developed region with more than half of the global total of 401 GW in 2017. Europe continued to decline as a percentage of the global PV market. In 2017, Europe represented 28% of global capacity, the Americas 19% and Middle East 2%.\n\nSolar PV covered 3.5% and 7% of European electricity demand and peak electricity demand, respectively in 2014.\n\nSince the 1950s, when the first solar cells were commercially manufactured, there has been a succession of countries leading the world as the largest producer of electricity from solar photovoltaics. First it was the United States, then Japan, followed by Germany, and currently China.\n\nThe United States, where modern solar PV was invented, led installed capacity for many years. Based on preceding work by Swedish and German engineers, the American engineer Russell Ohl at Bell Labs patented the first modern solar cell in 1946. It was also there at Bell Labs where the first practical c-silicon cell was developed in 1954. Hoffman Electronics, the leading manufacturer of silicon solar cells in the 1950s and 1960s, improved on the cell's efficiency, produced solar radios, and equipped Vanguard I, the first solar powered satellite launched into orbit in 1958.\n\nIn 1977 US-President Jimmy Carter installed solar hot water panels on the White House promoting solar energy and the National Renewable Energy Laboratory, originally named \"Solar Energy Research Institute\" was established at Golden, Colorado. In the 1980s and early 1990s, most photovoltaic modules were used in stand-alone power systems or powered consumer products such as watches, calculators and toys, but from around 1995, industry efforts have focused increasingly on developing grid-connected rooftop PV systems and power stations. By 1996, solar PV capacity in the US amounted to 77 megawatts–more than any other country in the world at the time. Then, Japan moved ahead.\n\nJapan took the lead as the world's largest producer of PV electricity, after the city of Kobe was hit by the Great Hanshin earthquake in 1995. Kobe experienced severe power outages in the aftermath of the earthquake, and PV systems were then considered as a temporary supplier of power during such events, as the disruption of the electric grid paralyzed the entire infrastructure, including gas stations that depended on electricity to pump gasoline. Moreover, in December of that same year, an accident occurred at the multibillion-dollar experimental Monju Nuclear Power Plant. A sodium leak caused a major fire and forced a shutdown (classified as INES 1). There was massive public outrage when it was revealed that the semigovernmental agency in charge of Monju had tried to cover up the extent of the accident and resulting damage. Japan remained world leader in photovoltaics until 2004, when its capacity amounted to 1,132 megawatts. Then, focus on PV deployment shifted to Europe.\n\nIn 2005, Germany took the lead from Japan. With the introduction of the Renewable Energy Act in 2000, feed-in tariffs were adopted as a policy mechanism. This policy established that renewables have priority on the grid, and that a fixed price must be paid for the produced electricity over a 20-year period, providing a guaranteed return on investment irrespective of actual market prices. As a consequence, a high level of investment security lead to a soaring number of new photovoltaic installations that peaked in 2011, while investment costs in renewable technologies were brought down considerably. In 2016 Germany's installed PV capacity was over the 40 GW mark.\n\nChina surpassed Germany's capacity by the end of 2015, becoming the world's largest producer of photovoltaic power. China's rapid PV growth continued in 2016 – with 34.2 GW of solar photovoltaics installed. The quickly lowering feed in tariff rates at the end of 2015 motivated many developers to secure tariff rates before mid-year 2016 – as they were anticipating further cuts (correctly so). During the course of the year, China announced its goal of installing 100 GW during the next Chinese Five Year Economic Plan (2016–2020). China expected to spend ¥1 trillion ($145B) on solar construction during that period. Much of China's PV capacity was built in the relatively less populated west of the country whereas the main centres of power consumption were in the east (such as Shanghai and Beijing). Due to lack of adequate power transmission lines to carry the power from the solar power plants, China had to curtail its PV generated power.\n\nThe average price per watt dropped drastically for solar cells in the decades leading up to 2017. While in 1977 prices for crystalline silicon cells were about $77 per watt, average spot prices in August 2018 were as low as $0.13 per watt or nearly 600 times less than forty years ago. Prices for thin-film solar cells and for c-Si solar panels were around $.60 per watt. Module and cell prices declined even further after 2014 \"(see price quotes in table)\".\n\nThis price trend was seen as evidence supporting Swanson's law (an observation similar to the famous Moore's Law) that states that the per-watt cost of solar cells and panels fall by 20 percent for every doubling of cumulative photovoltaic production. A 2015 study showed price/kWh dropping by 10% per year since 1980, and predicted that solar could contribute 20% of total electricity consumption by 2030.\nIn its 2014 edition of the \"Technology Roadmap: Solar Photovoltaic Energy\" report, the International Energy Agency (IEA) published prices for residential, commercial and utility-scale PV systems for eight major markets as of 2013 \"(see table below)\". However, DOE's SunShot Initiative report states lower prices than the IEA report, although both reports were published at the same time and referred to the same period. After 2014 prices fell further. For 2014, the SunShot Initiative modeled U.S. system prices to be in the range of $1.80 to $3.29 per watt. Other sources identified similar price ranges of $1.70 to $3.50 for the different market segments in the U.S. In the highly penetrated German market, prices for residential and small commercial rooftop systems of up to 100 kW declined to $1.36 per watt (€1.24/W) by the end of 2014. In 2015, Deutsche Bank estimated costs for small residential rooftop systems in the U.S. around $2.90 per watt. Costs for utility-scale systems in China and India were estimated as low as $1.00 per watt. As of May 2017, a residential 5 kW-system in Australia cost on average about AU$1.25, or US$0.93 per watt.\n\nThere were significant advances in conventional crystalline silicon (c-Si) technology in the years leading up to 2017. The falling cost of the polysilicon since 2009, that followed after a period of severe shortage \"(see below)\" of silicon feedstock, pressure increased on manufacturers of commercial thin-film PV technologies, including amorphous thin-film silicon (a-Si), cadmium telluride (CdTe), and copper indium gallium diselenide (CIGS), lead to the bankruptcy of several thin-film companies that had once been highly touted. The sector faced price competition from Chinese crystalline silicon cell and module manufacturers, and some companies together with their patents were sold below cost.\n\nIn 2013 thin-film technologies accounted for about 9 percent of worldwide deployment, while 91 percent was held by crystalline silicon (mono-Si and multi-Si). With 5 percent of the overall market, CdTe held more than half of the thin-film market, leaving 2 percent to each CIGS and amorphous silicon.\n\n\n\n\nIn the early 2000s, prices for polysilicon, the raw material for conventional solar cells, were as low as $30 per kilogram and silicon manufacturers had no incentive to expand production.\n\nHowever, there was a severe silicon shortage in 2005, when governmental programmes caused a 75% increase in the deployment of solar PV in Europe. In addition, the demand for silicon from semiconductor manufacturers was growing. Since the amount of silicon needed for semiconductors makes up a much smaller portion of production costs, semiconductor manufacturers were able to outbid solar companies for the available silicon in the market.\n\nInitially, the incumbent polysilicon producers were slow to respond to rising demand for solar \napplications, because of their painful experience with over-investment in the past. Silicon prices sharply rose to about $80 per kilogram, and reached as much as $400/kg for long-term contracts and spot prices. In 2007, the constraints on silicon became so severe that the solar industry was forced to idle about a quarter of its cell and module manufacturing capacity—an estimated 777 MW of the then available production capacity. The shortage also provided silicon specialists with both the cash and an incentive to develop new technologies and several new producers entered the market. Early responses from the solar industry focused on improvements in the recycling of silicon. When this potential was exhausted, companies have been taking a harder look at alternatives to the conventional Siemens process.\n\nAs it takes about three years to build a new polysilicon plant, the shortage continued until 2008. Prices for conventional solar cells remained constant or even rose slightly during the period of silicon shortage from 2005 to 2008. This is notably seen as a \"shoulder\" that sticks out in the and it was feared that a prolonged shortage could delay solar power becoming competitive with conventional energy prices without subsidies.\n\nIn the meantime the solar industry lowered the number of grams-per-watt by reducing wafer thickness and kerf loss, increasing yields in each manufacturing step, reducing module loss, and raising panel efficiency. Finally, the ramp up of polysilicon production alleviated worldwide markets from the scarcity of silicon in 2009 and subsequently lead to an overcapacity with sharply declining prices in the photovoltaic industry for the following years.\n\nAs the polysilicon industry had started to build additional large production capacities during the shortage period, prices dropped as low as $15 per kilogram forcing some producers to suspend production or exit the sector. Prices for silicon stabilized around $20 per kilogram and the booming solar PV market helped to reduce the enormous global overcapacity from 2009 onwards. However, overcapacity in the PV industry continued to persist. In 2013, global record deployment of 38 GW (updated EPIA figure) was still much lower than China's annual production capacity of approximately 60 GW. Continued overcapacity was further reduced by significantly lowering solar module prices and, as a consequence, many manufacturers could no longer cover costs or remain competitive. As worldwide growth of PV deployment continued, the gap between overcapacity and global demand was expected in 2014 to close in the next few years.\n\nIEA-PVPS published in 2014 historical data for the worldwide utilization of solar PV module production capacity that showed a slow return to normalization in manufacture in the years leading up to 2014. The utilization rate is the ratio of production capacities versus actual production output for a given year. A low of 49% was reached in 2007 and reflected the peak of the silicon shortage that idled a significant share of the module production capacity. As of 2013, the utilization rate had recovered somewhat and increased to 63%.\n\nAfter anti-dumping petition were filed and investigations carried out, the United States imposed tariffs of 31 percent to 250 percent on solar products imported from China in 2012. A year later, the EU also imposed definitive anti-dumping and anti-subsidy measures on imports of solar panels from China at an average of 47.7 percent for a two-year time span.\n\nShortly thereafter, China, in turn, levied duties on U.S. polysilicon imports, the feedstock for the production of solar cells. In January 2014, the Chinese Ministry of Commerce set its anti-dumping tariff on U.S. polysilicon producers, such as Hemlock Semiconductor Corporation to 57%, while other major polysilicon producing companies, such as German Wacker Chemie and Korean OCI were much less affected. All this has caused much controversy between proponents and opponents and was subject of debate.\n\nDeployment figures on a global, regional and nationwide scale are well documented since the early 1990s. While worldwide photovoltaic capacity grew continuously, deployment figures by country were much more dynamic, as they depended strongly on national policies. A number of organizations release comprehensive reports on PV deployment on a yearly basis. They include annual and cumulative deployed PV capacity, typically given in watt-peak, a break-down by markets, as well as in-depth analysis and forecasts about future trends.\n\nDue to the exponential nature of PV deployment, most of the overall capacity has been installed in the years leading up to 2017 \"(see pie-chart)\". Since the 1990s, each year has been a record-breaking year in terms of newly installed PV capacity, except for 2012. Contrary to some earlier predictions, early 2017 forecasts were that 85 gigawatts would be installed in 2017. Near end-of-year figures however raised estimates to 95 GW for 2017-installations.<ref name=\"Est-Global-PV-2016/17\">Global Solar Market Demand Expected To Reach 100 Gigawatts In 2017, Says SolarPower Europe, CleanTechnica, 27 October 2017</ref>\n\nWorldwide growth of solar PV capacity was an exponential curve between 1992 and 2017. Tables below show global cumulative nominal capacity by the end of each year in megawatts, and the year-to-year increase in percent. In 2014, global capacity was expected to grow by 33 percent from 139 to 185 GW. This corresponded to an exponential growth rate of 29 percent or about 2.4 years for current worldwide PV capacity to double. Exponential growth rate: P(t) = Pe, where \"P\" is 139 GW, growth-rate \"r\" 0.29 (results in doubling time \"t\" of 2.4 years).\n\nThe following table contains data from multiple different sources. For 1992–1995: compiled figures of 16 main markets \"(see section All time PV installations by country),\" for 1996–1999: BP-Statistical Review of world energy (Historical Data Workbook) for 2000–2013: EPIA Global Outlook on Photovoltaics Report\n\n\n"}
{"id": "5768303", "url": "https://en.wikipedia.org/wiki?curid=5768303", "title": "Handheld television", "text": "Handheld television\n\nA handheld television is a portable device for watching television that usually uses a TFT LCD or OLED color display. Many of these devices resemble handheld transistor radios.\n\nIn the 1970s and early 1980s, Panasonic and Sinclair Research released the first TVs which were small enough to fit in a large pocket; called the Panasonic IC TV MODEL TR-001 and MTV-1. Since LCD technology was not yet mature at the time, the TV used a minuscule CRT which set the record for being the smallest CRT on a commercially marketed product.\n\nLater in 1982, Sony released the first model of the Watchman, a pun on Walkman. It had grayscale video at first. Several years later, a color model with an active-matrix LCD was released. Some smartphones integrate a television receiver, although Internet broadband video is far more common.\n\nSince the switch-over to digital broadcasting, handheld TVs have reduced in size and improved in quality. The major current manufacturers of DVB-T standard (common throughout Europe) handheld TVs are August International, ODYS and Xoro.\n\nThese devices often have stereo 1⁄8 inch (3.5 mm) phono plugs for composite video-analog mono audio relay to serve them as composite monitors; also, some models have mono 3.5 mm jacks for the broadcast signal that is usually relayed via F connector or Belling-Lee connector on standard television models. \n\nSome include HDMI, USB and SD ports.\n\nScreen sizes vary from . Some handheld televisions also double as portable DVD players and USB personal video recorders.\n\nPortable televisions cannot fit in a pocket, but often run on batteries and include a cigarette lighter receptacle plug.\n\nPocket televisions fit in a pocket.\n\nWearable televisions sometimes are made in the form of a wristwatch.\n\n\n\n"}
{"id": "15632708", "url": "https://en.wikipedia.org/wiki?curid=15632708", "title": "History of the roller coaster", "text": "History of the roller coaster\n\nRoller coaster amusement rides have origins back to ice slides constructed in 18th-century Russia. Early technology featured sleds or wheeled carts that were sent down hills of snow reinforced by wooden supports. The technology evolved in the 19th century to feature railroad track using wheeled cars that were securely locked to the track. Newer innovations emerged in the early 20th century with side friction and underfriction technologies to allow for greater speeds and sharper turns. By the mid-to-late 20th century, these elements intensified with the introduction of steel roller coaster designs and the ability for them to invert riders.\n\nThe world's oldest roller coasters descended from the \"Russian Mountains,\" which were specially constructed hills of snow located in the gardens of palaces around the Russian capital, Saint Petersburg, in the 18th century. This attraction was called a \"Katalnaya Gorka\" or \"sliding mountain\" in Russian. The slides were built to a height of between and , had a 50 degree drop, and were reinforced by wooden supports. Sometimes wheeled carts were used instead of sleds. These slides became popular with the Russian upper class, and with Catherine II of Russia herself, who had such mountains built in the gardens of the Oranienbaum Palace near St. Petersburg, with a pavilion next to it for drinking tea after the sliding. \"Russian mountains\" remains the term for roller coasters in many languages, such as Spanish (\"\"), Italian (\"\"), and French (\"\"). Ironically, the Russian term for roller coaster, \"\" (amerikanskie gorki), translates literally as \"American mountains.\"\nRussian soldiers occupying Paris from 1815 through 1816, after the defeat of Napoleon at Waterloo, may have introduced the Russian amusement of sledding down steep hills. In July 1817, a French banker named Nicolas Beaujon opened the Parc Beaujon, an amusement park on the Champs Elysees. Its most famous feature was the \"Promenades Aériennes\" or \"Aerial Strolls.\" It featured wheeled cars securely locked to the track, guide rails to keep them on course, and higher speeds. The three-wheel carts were towed to the top of a tower, and then released to descend two curving tracks on either side. King Louis XVIII of France came to see the park, but it is not recorded if he tried the ride. Before long there were seven similar rides in Paris: \"Les Montagnes françaises\" (The French Mountains), \"le Delta\", \"les Montagnes de Belleville\" (The Mountains of Belleville), \"les Montagnes américaines\" (the American Mountains), \"Les Montages lilliputiennes\", (The miniature mountains), \"Les Montagnes susses\" (The Swiss mountains) and \"Les Montagnes égyptiennes\" (The Egyptian mountains).\n\nIn the beginning, these attractions were primarily for the upper classes. In 1845 a new amusement park opened in Copenhagen, Tivoli, which was designed for the middle class. These new parks featured roller coasters as permanent attractions. The first permanent loop track was probably also built in Paris from an English design in 1846, with a single-person wheeled sled running through a 13-foot (4 m) diameter vertical loop. These early single loop designs were called Centrifugal Railways. In 1887, a French entrepreneur, Joseph Oller, the owner of the Moulin Rouge music hall, built \"Les Montagnes Russes à Belleville\" (\"The Russian Mountains of Belleville\") a permanent roller coaster with a length of two hundred meters in the form of a double-eight, later enlarged to four figure-eight-shaped loops.\n\nIn the 1850s, a mining company in Summit Hill, Pennsylvania, constructed the Mauch Chunk gravity railroad, a brakeman-controlled, 8.7-mile (14 km) downhill track used to deliver coal to Mauch Chunk (now known as Jim Thorpe), Pennsylvania. By 1872, the \"Gravity Road\" (as it became known) was selling rides to thrill seekers. Railway companies used similar tracks to provide amusement on days when ridership was low.\n\nUsing this idea as a basis, LaMarcus Adna Thompson began work on a gravity Switchback Railway that opened at Coney Island in Brooklyn, New York in 1884. Passengers climbed to the top of a platform and rode a bench-like car down the track up to the top of another tower where the vehicle was switched to a return track and the passengers took the return trip. This track design was soon replaced with an oval complete circuit. In 1885, Phillip Hinkle introduced the first complete-circuit coaster with a lift hill, the \"Gravity Pleasure Road\", which became the most popular attraction at Coney Island. Not to be outdone, in 1886 LaMarcus Adna Thompson patented his design for a roller coaster that included dark tunnels with painted scenery. \"Scenic Railways\" were soon found in amusement parks across the county, with Frederick Ingersoll's construction company building many of them in the first two decades of the 20th century.\n\nAs it grew in popularity, experimentation in coaster dynamics took off. In the 1880s the concept of a vertical loop was again explored by Lina Beecher, and in 1895 the concept came into fruition with the \"Flip Flap Railway\", located at Sea Lion Park in Brooklyn, and shortly afterward with \"Loop the Loop\" at Olentangy Park near Columbus, Ohio as well as similar coasters in Atlantic City and Coney Island. The rides were incredibly dangerous, and many passengers suffered whiplash. Both were soon dismantled, and looping coasters had to wait for over a half century before making a reappearance.\n\nBy 1919, the first underfriction roller coaster had been developed by John Miller. Soon, roller coasters spread to amusement parks all around the world. Perhaps the best known historical roller coaster, \"The Cyclone\", was opened at Coney Island in 1927. Like \"The Cyclone\", all early roller coasters were made of wood. Many old wooden roller coasters are still operational, at parks such as Kennywood near Pittsburgh, Pennsylvania and Pleasure Beach Blackpool, England. The oldest operating roller coaster is \"Leap-The-Dips\" at Lakemont Park in Pennsylvania, a side friction roller coaster built in 1902. The oldest wooden roller coaster in the United Kingdom is the \"Scenic Railway\" at Dreamland Amusement Park in Margate, Kent and features a system where the brakeman rides the car with wheels. It was severely damaged by fire on April 7, 2008, but was subsequently restored and reopened to the public in 2015. \"Scenic Railway\" at Melbourne's Luna Park built in 1912, is the world's oldest continually-operating roller coaster, and it also still features a system where the brakeman rides the car with wheels. One of only 13 remaining examples of John Miller's work worldwide is the wooden roller coaster at Lagoon in Utah. The coaster opened in 1921 and is the 6th oldest coaster in the world.\n\nThe Great Depression marked the end of the golden age of roller coasters, as amusement parks generally went into a decline that resulted in less demand for new coasters. This lasted until 1972, when \"The Racer\" opened at Kings Island amusement park located in what was then a part of Deerfield Township in Warren County, Ohio. Designed by John C. Allen, the instant success of \"The Racer\" helped to ignite a renaissance for roller coasters, reviving worldwide interest throughout the industry.\n\nIn 1959, the Disneyland theme park introduced a new design breakthrough in roller coasters with the \"Matterhorn Bobsleds\". This was the first roller coaster to use a tubular steel track. Unlike conventional wooden rails, which are generally formed using steel strips mounted on laminated wood, tubular steel can be bent in any direction, which allows designers to incorporate loops, corkscrews, and many other maneuvers into their designs. Most modern roller coasters are made of steel, although wooden roller coasters are still being built along with hybrids of steel and wood.\n\nIn 1975 the first modern-day roller coaster to perform an inverting element opened: Corkscrew, located at Knott's Berry Farm in Buena Park, California. In 1976 the vertical loop made a permanent comeback with the Great American Revolution at Magic Mountain in Valencia, California.\n\nThe roller coasters mentioned here are significant for their role in the amusement industry. They were notable for specific reasons, including:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "11234940", "url": "https://en.wikipedia.org/wiki?curid=11234940", "title": "ISO/TC 37", "text": "ISO/TC 37\n\nISO/TC 37 is a technical committee within the International Organization for Standardization (ISO) that prepares standards and other documents concerning methodology and principles for terminology and language resources.\n\nTitle: Terminology and other language and content resources\n\nScope: Standardization of principles, methods and applications relating to terminology and other language and content resources in the contexts of multilingual communication and cultural diversity\n\nISO/TC 37 is a so-called \"horizontal committee\", providing guidelines for all other technical committees that develop standards on how to manage their terminological problems. However, the standards developed by ISO/TC 37 are not restricted to ISO. Collaboration with industry is sought to ensure that the requirements and needs from all possible users of standards concerning terminology, language and structured content are duly and timely addressed.\n\nInvolvement in standards development is open to all stakeholders and requests can be made to the TC through any liaison or member organization (see the list of current members and liaisons of ISO/TC 37:)\n\nISO/TC 37 standards are therefore fundamental and should form the basis for many localization, translation, and other industry applications.\n\nInternational Standards are developed by experts from industry, academia and business who are delegates of their national standards institution or another organization in liaison. Involvement, therefore, is principally open to all stakeholders. They are based on consensus among those national standards institutes who collaborate in the respective committee by way of membership.\n\nISO/TC 37 develops International Standards concerning:\n\nISO/TC 37 looks upon a long history of terminology unification activities. In the past, terminology experts - even more so experts of terminology theory and methodology - had to struggle for wide recognition. Today their expertise is sought in many application areas, especially in various fields of standardization. The emerging multilingual information society and knowledge society will depend on reliable digital content. Terminology is indispensable here. This is because terminology plays a crucial role wherever and whenever specialized information and knowledge is being prepared (e.g. in research and development), used (e.g. in specialized texts), recorded and processed (e.g. in data banks), passed on (via training and teaching), implemented (e.g. in technology and knowledge transfer), or translated and interpreted. In the age of globalization the need for methodology standards concerning multilingual digital content is increasing - ISO/TC 37 has developed over the years the expertise for methodology standards for science and technology related content in textual form.\n\nThe beginnings of terminology standardization are closely linked to the standardization efforts of IEC (International Electrotechnical Commission, founded in 1906) and ISO (International Organization for Standardization, founded in 1946).\n\nA terminology standard according to ISO/IEC Guide 2 (1996) is defined as \"standard that is concerned with terms, usually accompanied by their definitions, and sometimes by explanatory notes, illustrations, examples, etc.\"\n\nISO 1087-1:2000 defines terminology as \"\"set of designations belonging to one special language\" and designations as \"representation of a concept by a sign which denotes it\".\" Here, concept representation goes beyond terms (being only linguistic signs), which is also supported by the state-of-the-art of terminology science, according to which terminology has three major functions:\n\n\nThe above indicates that terminological data (comprising various kinds of knowledge representation) possibly have a much more fundamental role in domain-related information and knowledge than commonly understood.\n\nToday, terminology standardization can be subdivided into two distinct activities:\n\nThe two are mutually interdependent, since the standardization of terminologies would not result in high-quality terminological data, if certain common principles, rules and methods are not observed. On the other hand, these standardized terminological principles, rules and methods must reflect the state-of-the-art of theory and methodology development in those domains, in which terminological data have to be standardized in connection with the formulation of subject standards.\n\nTerminology gained a special position in the field of standardization at large, which is defined as \"activity of establishing, with regard to actual or potential problems, provisions for common and repeated use, aimed at the achievement of the optimum degree of order in a given context\" (ISO/IEC 1996). Every technical committee or sub-committee or working group has to standardize subject matters, define and standardize its respective terminology. There is a consensus that terminology standardization precedes subject standardization (or \"subject standardization requires terminology standardization\").\n\nISO/TC 37 was put into operation in 1952 in order \"to find out and formulate general principles of terminology and terminological lexicography\" (as terminography was called at that time).\n\nThe history of terminology standardization proper - if one excludes earlier attempts in the field of metrology - started in the International Electrotechnical Commission (IEC), which was founded in London in 1906 following a recommendation passed at the International Electrical Congress, held in St. Louis, United States, on 15 September 1904, to the extent that: \"\"...steps should be taken to secure the co-operation of the technical societies of the world, by the appointment of a representative Commission to consider the question of the standardization of the nomenclature and ratings of electrical apparatus and machinery\".\" From the very beginning, IEC considered it its foremost task to standardize the terminology of electrotechnology for the sake of the quality of its subject standards, and soon embarked upon the International Electrotechnical Vocabulary (IEV), whose first edition, based on many individual terminology standards, was published in 1938. The IEV is still being continued today, covering 77 chapters as parts of the International Standard series IEC 60050. The IEV Online Database can be accessed on Electropedia \n\nThe predecessor to the International Organization for Standardization (ISO), the International Federation of Standardizing Associations (ISA, founded in 1926), made a similar experience. But it went a step further and - triggered by the publication of Eugen Wüster's book \"Internationale Sprachnormung in der Technik\" [International standardization of technical language] (Wüster 1931) - established in 1936 the Technical Committee ISA/TC 37 \"Terminology\" for the sake of formulating general principles and rules for terminology standardization.\n\nISA/TC 37 conceived a scheme of four classes of recommendations for terminology standardization mentioned below, but the Second World War interrupted its pioneering work. Nominally, ISO/TC 37 was established from the very beginning of ISO in 1946, but it was decided to re-activate it only in 1951 and the Committee started operation in 1952. Since then until 2009 the secretariat of ISO/TC 37 has been held by the International Information Centre for Terminology (Infoterm), on behalf of the Austrian Standards Institute Austria. Infoterm, an international non-governmental organization based in Austria, continues to collaborate as a twinning secretariat. After this the administration went to CNIS (China).\n\nTo prepare standards specifying principles and methods for the preparation and management of language resources within the framework of standardization and related activities. Its technical work results in International Standards (and Technical Reports) covering terminological principles and methods as well as various aspects of computer-assisted terminography. ISO/TC 37 is not responsible for the co-ordination of the terminology standardizing activities of other ISO/TCs.\n\n\nISO 639 Codes for the representation of names of languages, with the following parts:\n\n\"Note: Current status is not mentioned here - see ISO Website for most recent status. Many of these are in development.\":\n\n"}
{"id": "203614", "url": "https://en.wikipedia.org/wiki?curid=203614", "title": "Information mapping", "text": "Information mapping\n\nInformation mapping is a research-based method for writing clear and user focused information, based on the audience's needs and the purpose of the information. The method is applied primarily to designing and developing business and technical communications. It is used as a content standard within organizations throughout the world.\n\nThe information mapping method is a research-based methodology used to analyze, organize and present information based on an audience's needs and the purpose of the information. The method applies to all subject matter and media technology. Information mapping has close ties to information visualization, information architecture, graphic design, information design, data analysis, experience design, graphic user interface design, and knowledge management systems.\n\nInformation mapping provides a number of tools for analyzing, organizing and presenting information.\n\nSome of Robert E. Horn's best-known work was his development of the theory of information types. Horn identified six types of information that account for nearly all the content of business and technical communications. The types categorize elements according to their purpose for the audience:\nThe information mapping method proposes six principles for organizing information so that it is easy to access, understand, and remember:\nDocuments written according to information mapping have a modular structure. They consist of clearly outlined information units (\"maps\" and \"blocks\") that take into account how much information a reader is able to assimilate.\n\nThere is an essential difference between an information unit and the traditional text paragraph. A \"block\" is limited to a single topic and consists of a single type of information. Blocks are grouped into \"maps\", and each map consists only of relevant blocks. The hierarchical approach to structuring information greatly facilitates electronic control of content via content management systems and knowledge management systems.\n\nThe information mapping method offers advantages to writers and readers, as well as to an entire organization.\n\nInformation mapping offers these advantages for writers:\n\nInformation mapping offers these advantages for readers:\n\nAlso an entire organization can benefit from using a content standard like information mapping if the method is used with the following objectives in mind:\n\nInformation mapping was developed in the late 20th century by Robert E. Horn, a researcher in the cognitive and behavioral sciences. Horn was interested in visual presentation of information to improve accessibility, comprehension and performance. Horn's development of the information mapping method has won him recognition from the International Society for Performance Improvement and the Association for Computing Machinery.\n\nMany independent studies have confirmed that applying the information mapping method to business and technical communications results in quicker, easier access to information, improved comprehension and enhanced performance. It also facilitates repurposing for publication in different formats.\n\nDoubts have been raised over the strength of the research Horn uses to justify some of his principles. For instance, his chunking principle requires lists, paragraphs, sub-sections and sections in a document to contain no more than 7±2 chunks of information. Horn does not state where he got this principle, but an Information Mapping website stated that the principle is \"based on George A. Miller's 1956 research\". Miller did write a paper in 1956 called \"The Magical Number Seven, Plus or Minus Two: Some Limits on our Capacity for Processing Information\", but its relevance to writing is tenuous. Miller himself said that his research had nothing to do with writing. Insisting that lists, paragraphs, sub-sections and sections throughout a document contain no more than 7±2 chunks of information paradoxically assumes that the size of what is not read in a document can influence a reader's ability to comprehend what they do read.\n\n\n"}
{"id": "40869050", "url": "https://en.wikipedia.org/wiki?curid=40869050", "title": "Kronos effect", "text": "Kronos effect\n\nThe Kronos effect is a term coined by Columbia Law School professor Tim Wu in his 2010 book \"The Master Switch: The Rise and Fall of Information Empires\". It describes how companies that establish early dominance in a period of disruptive innovation will do everything in their power to maintain their first-mover advantage.\n\nThe name derives from Greek mythology, in which the Titan Kronos ate his own children in order to preempt the prophecy that one would dethrone him.\n\nIn \"The Master Switch\", Wu described the Kronos effect as critical to the history of information technology. In his book, he gives the example of radio pioneer and American business executive David Sarnoff. Sarnoff was originally what Wu described as \"a radio idealist,\" but later in his career when heading the Radio Corporation of America (RCA), he came to view newly emergent FM technology as a threat to incumbent AM businesses including RCA's own NBC network. Sarnoff went on to pressure the U.S. Federal Communications Commission to restrict the growth of FM in a variety of ways, successfully suppressing its widespread adoption for more than thirty years, and proving, Wu wrote, that \"the best antidote to the disruptive power of innovation is overregulation.\"\n\nThe Kronos effect's role in the technological disruption cycle is to hurt innovation, efficiency, openness and decentralization.\n\nOther examples from Wu include:\n\n"}
{"id": "30444895", "url": "https://en.wikipedia.org/wiki?curid=30444895", "title": "List of brazing alloys", "text": "List of brazing alloys\n"}
{"id": "13706125", "url": "https://en.wikipedia.org/wiki?curid=13706125", "title": "List of emerging technologies", "text": "List of emerging technologies\n\nEmerging technologies are those technical innovations which represent progressive developments within a field for competitive advantage.\n\n\n"}
{"id": "21938890", "url": "https://en.wikipedia.org/wiki?curid=21938890", "title": "List of interstellar radio messages", "text": "List of interstellar radio messages\n\nThis is a list of interstellar radio messages.\n\nThere are eleven realized IRM projects:\n\n\nThe Across the Universe message, A Simple Response to an Elemental Message and Hello From Earth are not always considered serious. The first two of them were sent to Polaris, which is 431 light years distant from us and whose planetary system, even if it exists, may not be suited for life, because it is a supergiant star, spectral type F7Ib which is only 70 million years old. In addition, both transmission rates were very high, about 128 kbit/s, for such moderate transmitter power (about 18 kW). The main defect of the \"Hello From Earth\" is an insufficient scientific and technical justification, since no famous SETI scientist made statements with validation of HFE's design. As it follows from : \"After the final message was collected on Monday 24 August 2009, messages were exported as a text file and sent to NASA's Jet Propulsion Laboratory in California, where they were encoded into binary, packaged and tested before transmission\", but nobody explained why he hopes that such encoded and packaged text will be understood and conceived by possible extraterrestrials.\n\nSome use the term \"Active SETI Project\", but Alexander Zaitsev, who was a scientific head of composing and transmissions of Cosmic Call 1999 & 2003, and Teen Age Message 2001, and a scientific consultant of A Message From Earth, emphasized that he considers above IRMs as the METI (\"Messaging to Extra-Terrestrial Intelligence Projects\").\n\nThese seven messages have targeted stars between 20 and 69 light-years from the Earth. The exception is the Arecibo message, which targeted globular cluster M13, approximately 24,000 light-years away. The first message to reach its destination will be A Message From Earth, which should reach the Gliese 581 planetary system in Libra in 2029.\n\nOn 13 February 2015, scientists (including Geoffrey Marcy, Seth Shostak, Frank Drake, Elon Musk and David Brin) at a convention of the American Association for the Advancement of Science, discussed Active SETI and whether transmitting a message to possible intelligent extraterrestrials in the Cosmos was a good idea; one result was a statement, (which was not signed by Seth Shostak or Frank Drake), that a \"worldwide scientific, political and humanitarian discussion must occur before any message is sent\".\n\nStars to which messages were sent include:\n\nAlong with serious IRM projects, a bulk of pseudo-METI projects also exist:\n\n"}
{"id": "30817170", "url": "https://en.wikipedia.org/wiki?curid=30817170", "title": "List of public relations journals", "text": "List of public relations journals\n\nThis is a list of peer-reviewed, English language academic journals in public relations.\n\n\n"}
{"id": "13113721", "url": "https://en.wikipedia.org/wiki?curid=13113721", "title": "Ministry of Education and Science of Georgia", "text": "Ministry of Education and Science of Georgia\n\nThe Ministry of Education and Science of Georgia () is a governmental body responsible for education system and children's services in Georgia. Ministry of Education works under the Minister of Education and Science of Georgia. The ministry is located on Uzandze street in Tbilisi in a historical building built in Mauritanic style. Tamar Sanikidze's resignation was announced on 3 June 2016. Her proposed replacement is Aleksandre Jejelava. \n\n"}
{"id": "43517179", "url": "https://en.wikipedia.org/wiki?curid=43517179", "title": "Motion History Images", "text": "Motion History Images\n\nThe motion history image (MHI) is a static image template helps in understanding the motion location and path as it progresses. In MHI, the temporal motion information is collapsed into a single image template where intensity is a function of recency of motion. Thus, the MHI pixel intensity is a function of the motion history at that location, where brighter values correspond to a more recent motion. Using MHI, moving parts of a video sequence can be engraved with a single image, from where one can predict the motion flow as well as the moving parts of the video action.\n\nSome important features of the MHI representation are:\n\n for each time \"t\"\n"}
{"id": "44339024", "url": "https://en.wikipedia.org/wiki?curid=44339024", "title": "Netcare System", "text": "Netcare System\n\nThe Netcare System is part of the \"Digital India\" campaign introduced by Indian Prime Minister Narendra Modi. The program is designed to allow access to modern software technology by those in rural areas, villages, and other remote parts of India. A major focus is enabling poor and rural students to use the Internet and related computer tech and information technology.\n"}
{"id": "48064444", "url": "https://en.wikipedia.org/wiki?curid=48064444", "title": "Network documentation", "text": "Network documentation\n\nNetwork documentation is a form of technical documentation. It is the practice of maintaining records about networks of computers. The documentation is used to give administrators information about how the network should look, perform and where to troubleshoot problems as they occur.\n\nAs the purpose of network documentation is to keep networks running as smoothly as possible while minimizing downtime when repairs are necessary, essential parts of network documentation include:\n\n\nNotation that helps administrators remember key details are the basics of network documentation while visual representations assist in helping administrators understand how equipment and the notation relates to one another.\nA basic network diagram includes hardware and shows how it is connected. Basic diagrams include L1/L2 drawing of the physical connectivity and layout of the network.\n\nThough network documentation can be done by hand, for larger organizations network documentation software is utilized. Software applications can include diagrams, inventory management and circuit and cable traces. Examples include Graphical Networks' netTerrain, Microsoft Visio, Docusnap, Gliffy, Opnet's Netmapper, and CENTREL Solutions' XIA Configuration among others.\n"}
{"id": "56948426", "url": "https://en.wikipedia.org/wiki?curid=56948426", "title": "Operations manual", "text": "Operations manual\n\nThe operations manual is the documentation by which an organisation provides guidance for members and employees to perform their functions correctly and reasonably efficiently. It documents the approved standard procedures for performing operations safely to produce goods and provide services. Compliance with the operations manual will generally be considered as activity approved by the persons legally responsible for the organisation.\n\nThe operations manual is intended to remind employees them of how to do their job. The manual is either a book or folder of printed documents containing the standard operating procedures, a description of the organisational hierarchy, contact details for key personnel and emergency procedures. It does not substitute for training, but should be sufficient to allow a trained and competent person to adapt to the organisation's specific procedures.\n\nThe operations manual helps the members of the organisation to reliably and efficiently carry out their tasks with consistent results. A good manual will reduce human error and inform everyone precisely what they need to do, who they are responsible to and who they are responsible for.It is a knowledge base for the organisation, and should be available for reference whenever needed. The operations manual is a document that should be periodically reviewed and updated whenever appropriate to ensure that it remains current.\n\nThe operations manual can be a digital or paper document. Digital format has advantages for revision control and can be distributed easily and at low cost. The detail should be sufficient to allow a competent person without specific experience to understand what is needed and how it is to be done. It is not a training manual, too much or too little detail can make it inefficient. \n\nContent will vary depending on the organisation, but some basic structure is fairly universal. \nTypical sections include: \n\nThere are two basic categories of information: Information that is relevant to all people in the organisation, and often also to clients and the general public, and information that is relevant to specific positions. \n\nThere may be statutory or regulatory requirements for specific content. In some cases the CEO may be required to authorise the operations manual by signature, and this authorisation may be required to be present in the document.\n\nThe organisational hierarchy is commonly and effectively described by an organisational chart, or organogram, which gives the reader an easily understood picture of where key people fit into the organisation.\n\nFormal job descriptions help people understand their roles within the organisation and identify each other's responsibilities. Required competence, registration or certification may be specified.\n\nThese include names and contact details for key persons within the organisation and important external contacts.\n\n\nAny emergency procedure that would be the standard response to a reasonably foreseeable emergency in the normal course of business would be detailed in the operations manual as a reference. There might also be specifications on how frequently exercises should be held. Some frequently encountered emergency procedures include:\n\nManuals that already exist for equipment or procedures may be incorporated into an operations manual as annexures, or referenced if they are not of general utility, so they can be found when needed and checked for continued validity when the operations manual is revised. \n\nIf an operations manual is to be useful it must be distributed to the people who will use it, and they should have the current version. Distribution and updating policies and procedures are also commonly part of the content of the manual. \n\nIn South Africa a diving contractor is obliged in terms of Regulation 21 of the Diving Regulations 2009 to provide an operations manual and make it available on site to the dive team before a diving operation may commence. This manual must contain prescribed types of information relating to health and safety, as specified in the codes of practice relating to the planned diving operations. The Code of Practice for Inshore Diving requires the contractor to base the planning and implementation of diving operations on specific documents which include the operations manual. \n\nThe operations manual is considered an essential administrative risk control measure, and must be compiled by the contractor in consultation with representatives of the employees and the company's contracted diving medical practitioner. Members of the diving team are required to comply with the health and safety requirements imposed on them by the operations manual. Among other things that must be specified in the operations manual are the decompression tables or algorithms authorised for use by the dive teams, the quantities of breathing gas that must be available on site, based on the dive profile and equipment to be used, clear limits on the environmental hazards to which the divers may be exposed, and the actions required of each member of the dive team in the event of an emergency during operations.\n\nSimilar requirements may apply to commercial diving contractors in other jurisdictions. The IMCA Code of Practice for Offshore Diving also requires the contractor to provide an operations manual for each diving system.\n"}
{"id": "7528885", "url": "https://en.wikipedia.org/wiki?curid=7528885", "title": "Outline of Big Science", "text": "Outline of Big Science\n\nThe following outline is provided as an overview of and topical guide to Big Science.\n\nBig Science – term used by scientists and historians of science to describe a series of changes in science which occurred in industrial nations during and after World War II.\n\n\n\n"}
{"id": "35023598", "url": "https://en.wikipedia.org/wiki?curid=35023598", "title": "Outline of computers", "text": "Outline of computers\n\nThe following outline is provided as an overview of and topical guide to computers:\n\nComputers – programmable machines designed to automatically carry out sequences of arithmetic or logical operations. The sequences of operations can be changed readily, allowing computers to solve more than one kind of problem.\n\nComputers can be described as all of the following:\n\n\nComputer architecture –\n\n\n\nHistory of computing hardware\n\n\nHp, Toshiba, Dell, Apple, Acer, Asus.\n\n\nSoftware development –\n\n\nComputer magazines –\n\"See List of computer magazines\"\nOnline –\n\n"}
{"id": "2958015", "url": "https://en.wikipedia.org/wiki?curid=2958015", "title": "Philosophy of artificial intelligence", "text": "Philosophy of artificial intelligence\n\nArtificial intelligence has close connections with philosophy because both share several concepts and these include intelligence, action, consciousness, epistemology, and even free will. Furthermore, the technology is concerned with the creation of artificial animals or artificial people (or, at least, artificial creatures) so the discipline is of considerable interest to philosophers. These factors contributed to the emergence of the philosophy of artificial intelligence. Some scholars argue that the AI community's dismissal of philosophy is detrimental.\n\nThe philosophy of artificial intelligence attempts to answer such questions as follows:\n\nThese three questions reflect the divergent interests of AI researchers, linguists, cognitive scientists and philosophers respectively. The scientific answers to these questions depend on the definition of \"intelligence\" and \"consciousness\" and exactly which \"machines\" are under discussion.\n\nImportant propositions in the philosophy of AI include:\n\n\nIs it possible to create a machine that can solve \"all\" the problems humans solve using their intelligence? This question defines the scope of what machines will be able to do in the future and guides the direction of AI research. It only concerns the \"behavior\" of machines and ignores the issues of interest to psychologists, cognitive scientists and philosophers; to answer this question, it does not matter whether a machine is \"really\" thinking (as a person thinks) or is just \"acting like\" it is thinking.\n\nThe basic position of most AI researchers is summed up in this statement, which appeared in the proposal for the Dartmouth workshop of 1956:\nArguments against the basic premise must show that building a working AI system is impossible, because there is some practical limit to the abilities of computers or that there is some special quality of the human mind that is necessary for thinking and yet cannot be duplicated by a machine (or by the methods of current AI research). Arguments in favor of the basic premise must show that such a system is possible.\n\nThe first step to answering the question is to clearly define \"intelligence\".\n\nAlan Turing reduced the problem of defining intelligence to a simple question about conversation. He suggests that: if a machine can answer \"any\" question put to it, using the same words that an ordinary person would, then we may call that machine intelligent. A modern version of his experimental design would use an online chat room, where one of the participants is a real person and one of the participants is a computer program. The program passes the test if no one can tell which of the two participants is human. Turing notes that no one (except philosophers) ever asks the question \"can people think?\" He writes \"instead of arguing continually over this point, it is usual to have a polite convention that everyone thinks\". Turing's test extends this polite convention to machines:\nOne criticism of the Turing test is that it is explicitly anthropomorphic . If our ultimate goal is to create machines that are \"more\" intelligent than people, why should we insist that our machines must closely \"resemble\" people? Russell and Norvig write that \"aeronautical engineering texts do not define the goal of their field as 'making machines that fly so exactly like pigeons that they can fool other pigeons'\".\n\nRecent A.I. research defines intelligence in terms of intelligent agents. An \"agent\" is something which perceives and acts in an environment. A \"performance measure\" defines what counts as success for the agent. \nDefinitions like this one try to capture the essence of intelligence. They have the advantage that, unlike the Turing test, they do not also test for human traits that we may not want to consider intelligent, like the ability to be insulted or the temptation to lie . They have the disadvantage that they fail to make the commonsense differentiation between \"things that think\" and \"things that do not\". By this definition, even a thermostat has a rudimentary intelligence.\n\nHubert Dreyfus describes this argument as claiming that \"if the nervous system obeys the laws of physics and chemistry, which we have every reason to suppose it does, then ... we ... ought to be able to reproduce the behavior of the nervous system with some physical device\". This argument, first introduced as early as 1943 and vividly described by Hans Moravec in 1988, is now associated with futurist Ray Kurzweil, who estimates that computer power will be sufficient for a complete brain simulation by the year 2029. A non-real-time simulation of a thalamocortical model that has the size of the human brain (10 neurons) was performed in 2005 and it took 50 days to simulate 1 second of brain dynamics on a cluster of 27 processors.\n\nFew disagree that a brain simulation is possible in theory, even critics of AI such as Hubert Dreyfus and John Searle.\nHowever, Searle points out that, in principle, \"anything\" can be simulated by a computer; thus, bringing the definition to its breaking point leads to the conclusion that any process at all can technically be considered \"computation\". \"What we wanted to know is what distinguishes the mind from thermostats and livers,\" he writes. Thus, merely mimicking the functioning of a brain would in itself be an admission of ignorance regarding intelligence and the nature of the mind .\nIn 1963, Allen Newell and Herbert A. Simon proposed that \"symbol manipulation\" was the essence of both human and machine intelligence. They wrote: \nThis claim is very strong: it implies both that human thinking is a kind of symbol manipulation (because a symbol system is \"necessary\" for intelligence) and that machines can be intelligent (because a symbol system is \"sufficient\" for intelligence). Another version of this position was described by philosopher Hubert Dreyfus, who called it \"the psychological assumption\":\nA distinction is usually made between the kind of high level symbols that directly correspond with objects in the world, such as <nowiki><dog></nowiki> and <nowiki><tail></nowiki> and the more complex \"symbols\" that are present in a machine like a neural network. Early research into AI, called \"good old fashioned artificial intelligence\" (GOFAI) by John Haugeland, focused on these kind of high level symbols.\n\nThese arguments show that human thinking does not consist (solely) of high level symbol manipulation. They do \"not\" show that artificial intelligence is impossible, only that more than symbol processing is required.\n\nIn 1931, Kurt Gödel proved with an incompleteness theorem that it is always possible to construct a \"Gödel statement\" that a given consistent formal system of logic (such as a high-level symbol manipulation program) could not prove. Despite being a true statement, the constructed Gödel statement is unprovable in the given system. (The truth of the constructed Gödel statement is contingent on the consistency of the given system; applying the same process to a subtly inconsistent system will appear to succeed, but will actually yield a false \"Gödel statement\" instead.) More speculatively, Gödel conjectured that the human mind can correctly eventually determine the truth or falsity of any well-grounded mathematical statement (including any possible Gödel statement), and that therefore the human mind's power is not reducible to a \"mechanism\". Philosopher John Lucas (since 1961) and Roger Penrose (since 1989) have championed this philosophical anti-mechanist argument. Gödelian anti-mechanist arguments tend to rely on the innocuous-seeming claim that a system of human mathematicians (or some idealization of human mathematicians) is both consistent (completely free of error) and believes fully in its own consistency (and can make all logical inferences that follow from its own consistency, including belief in its Gödel statement) . This is provably impossible for a Turing machine (and, by an informal extension, any known type of mechanical computer) to do; therefore, the Gödelian concludes that human reasoning is too powerful to be captured in a machine .\n\nHowever, the modern consensus in the scientific and mathematical community is that actual human reasoning is inconsistent; that any consistent \"idealized version\" \"H\" of human reasoning would logically be forced to adopt a healthy but counter-intuitive open-minded skepticism about the consistency of \"H\" (otherwise \"H\" is provably inconsistent); and that Gödel's theorems do not lead to any valid argument that humans have mathematical reasoning capabilities beyond what a machine could ever duplicate. This consensus that Gödelian anti-mechanist arguments are doomed to failure is laid out strongly in \"Artificial Intelligence\": \"\"any\" attempt to utilize (Gödel's incompleteness results) to attack the computationalist thesis is bound to be illegitimate, since these results are quite consistent with the computationalist thesis.\"\n\nMore pragmatically, Russell and Norvig note that Gödel's argument only applies to what can theoretically be proved, given an infinite amount of memory and time. In practice, real machines (including humans) have finite resources and will have difficulty proving many theorems. It is not necessary to prove everything in order to be intelligent .\n\nLess formally, Douglas Hofstadter, in his Pulitzer prize winning book \",\" states that these \"Gödel-statements\" always refer to the system itself, drawing an analogy to the way the Epimenides paradox uses statements that refer to themselves, such as \"this statement is false\" or \"I am lying\". But, of course, the Epimenides paradox applies to anything that makes statements, whether they are machines \"or\" humans, even Lucas himself. Consider:\nThis statement is true but cannot be asserted by Lucas. This shows that Lucas himself is subject to the same limits that he describes for machines, as are all people, and so Lucas's argument is pointless.\n\nAfter concluding that human reasoning is non-computable, Penrose went on to controversially speculate that some kind of hypothetical non-computable processes involving the collapse of quantum mechanical states give humans a special advantage over existing computers. Existing quantum computers are only capable of reducing the complexity of Turing computable tasks and are still restricted to tasks within the scope of Turing machines. . By Penrose and Lucas's arguments, existing quantum computers are not sufficient , so Penrose seeks for some other process involving new physics, for instance quantum gravity which might manifest new physics at the scale of the Planck mass via spontaneous quantum collapse of the wave function. These states, he suggested, occur both within neurons and also spanning more than one neuron. However, other scientists point out that there is no plausible organic mechanism in the brain for harnessing any sort of quantum computation, and furthermore that the timescale of quantum decoherence seems too fast to influence neuron firing.\n\nHubert Dreyfus Hubert Dreyfus's views on artificial intelligence and expertise depended primarily on implicit skill rather than explicit symbolic manipulation, and argued that these skills would never be captured in formal rules.\n\nDreyfus's argument had been anticipated by Turing in his 1950 paper Computing machinery and intelligence, where he had classified this as the \"argument from the informality of behavior.\" Turing argued in response that, just because we do not know the rules that govern a complex behavior, this does not mean that no such rules exist. He wrote: \"we cannot so easily convince ourselves of the absence of complete laws of behaviour ... The only way we know of for finding such laws is scientific observation, and we certainly know of no circumstances under which we could say, 'We have searched enough. There are no such laws.'\"\n\nRussell and Norvig point out that, in the years since Dreyfus published his critique, progress has been made towards discovering the \"rules\" that govern unconscious reasoning. The situated movement in robotics research attempts to capture our unconscious skills at perception and attention. Computational intelligence paradigms, such as neural nets, evolutionary algorithms and so on are mostly directed at simulated unconscious reasoning and learning. Statistical approaches to AI can make predictions which approach the accuracy of human intuitive guesses. Research into commonsense knowledge has focused on reproducing the \"background\" or context of knowledge. In fact, AI research in general has moved away from high level symbol manipulation or \"GOFAI\", towards new models that are intended to capture more of our \"unconscious\" reasoning . Historian and AI researcher Daniel Crevier wrote that \"time has proven the accuracy and perceptiveness of some of Dreyfus's comments. Had he formulated them less aggressively, constructive actions they suggested might have been taken much earlier.\"\n\nThis is a philosophical question, related to the problem of other minds and the hard problem of consciousness. The question revolves around a position defined by John Searle as \"strong AI\":\nSearle distinguished this position from what he called \"weak AI\":\nSearle introduced the terms to isolate strong AI from weak AI so he could focus on what he thought was the more interesting and debatable issue. He argued that \"even if we assume\" that we had a computer program that acted exactly like a human mind, there would still be a difficult philosophical question that needed to be answered.\n\nNeither of Searle's two positions are of great concern to AI research, since they do not directly answer the question \"can a machine display general intelligence?\" (unless it can also be shown that consciousness is \"necessary\" for intelligence). Turing wrote \"I do not wish to give the impression that I think there is no mystery about consciousness… [b]ut I do not think these mysteries necessarily need to be solved before we can answer the question [of whether machines can think].\" Russell and Norvig agree: \"Most AI researchers take the weak AI hypothesis for granted, and don't care about the strong AI hypothesis.\"\n\nThere are a few researchers who believe that consciousness is an essential element in intelligence, such as Igor Aleksander, Stan Franklin, Ron Sun, and Pentti Haikonen, although their definition of \"consciousness\" strays very close to \"intelligence.\" (See artificial consciousness.)\n\nBefore we can answer this question, we must be clear what we mean by \"minds\", \"mental states\" and \"consciousness\".\n\nThe words \"mind\" and \"consciousness\" are used by different communities in different ways. Some new age thinkers, for example, use the word \"consciousness\" to describe something similar to Bergson's \"élan vital\": an invisible, energetic fluid that permeates life and especially the mind. Science fiction writers use the word to describe some essential property that makes us human: a machine or alien that is \"conscious\" will be presented as a fully human character, with intelligence, desires, will, insight, pride and so on. (Science fiction writers also use the words \"sentience\", \"sapience,\" \"self-awareness\" or \"ghost\" - as in the \"Ghost in the Shell\" manga and anime series - to describe this essential human property). For others , the words \"mind\" or \"consciousness\" are used as a kind of secular synonym for the soul.\n\nFor philosophers, neuroscientists and cognitive scientists, the words are used in a way that is both more precise and more mundane: they refer to the familiar, everyday experience of having a \"thought in your head\", like a perception, a dream, an intention or a plan, and to the way we \"know\" something, or \"mean\" something or \"understand\" something . \"It's not hard to give a commonsense definition of consciousness\" observes philosopher John Searle. What is mysterious and fascinating is not so much \"what\" it is but \"how\" it is: how does a lump of fatty tissue and electricity give rise to this (familiar) experience of perceiving, meaning or thinking?\n\nPhilosophers call this the hard problem of consciousness. It is the latest version of a classic problem in the philosophy of mind called the \"mind-body problem.\" A related problem is the problem of \"meaning\" or \"understanding\" (which philosophers call \"intentionality\"): what is the connection between our \"thoughts\" and \"what we are thinking about\" (i.e. objects and situations out in the world)? A third issue is the problem of \"experience\" (or \"phenomenology\"): If two people see the same thing, do they have the same experience? Or are there things \"inside their head\" (called \"qualia\") that can be different from person to person?\n\nNeurobiologists believe all these problems will be solved as we begin to identify the neural correlates of consciousness: the actual relationship between the machinery in our heads and its collective properties; such as the mind, experience and understanding. Some of the harshest critics of artificial intelligence agree that the brain is just a machine, and that consciousness and intelligence are the result of physical processes in the brain. The difficult philosophical question is this: can a computer program, running on a digital machine that shuffles the binary digits of zero and one, duplicate the ability of the neurons to create minds, with mental states (like understanding or perceiving), and ultimately, the experience of consciousness?\n\nJohn Searle asks us to consider a thought experiment: suppose we have written a computer program that passes the Turing test and demonstrates \"general intelligent action.\" Suppose, specifically that the program can converse in fluent Chinese. Write the program on 3x5 cards and give them to an ordinary person who does not speak Chinese. Lock the person into a room and have him follow the instructions on the cards. He will copy out Chinese characters and pass them in and out of the room through a slot. From the outside, it will appear that the Chinese room contains a fully intelligent person who speaks Chinese. The question is this: is there anyone (or anything) in the room that understands Chinese? That is, is there anything that has the mental state of understanding, or which has conscious awareness of what is being discussed in Chinese? The man is clearly not aware. The room cannot be aware. The \"cards\" certainly aren't aware. Searle concludes that the Chinese room, or \"any\" other physical symbol system, cannot have a mind.\n\nSearle goes on to argue that actual mental states and consciousness require (yet to be described) \"actual physical-chemical properties of actual human brains.\" He argues there are special \"causal properties\" of brains and neurons that gives rise to minds: in his words \"brains cause minds.\"\n\nGottfried Leibniz made essentially the same argument as Searle in 1714, using the thought experiment of expanding the brain until it was the size of a mill. In 1974, Lawrence Davis imagined duplicating the brain using telephone lines and offices staffed by people, and in 1978 Ned Block envisioned the entire population of China involved in such a brain simulation. This thought experiment is called \"the Chinese Nation\" or \"the Chinese Gym\". Ned Block also proposed his Blockhead argument, which is a version of the Chinese room in which the program has been re-factored into a simple set of rules of the form \"see this, do that\", removing all mystery from the program.\n\nResponses to the Chinese room emphasize several different points. \n\nThe computational theory of mind or \"computationalism\" claims that the relationship between mind and brain is similar (if not identical) to the relationship between a \"running program\" and a computer. The idea has philosophical roots in Hobbes (who claimed reasoning was \"nothing more than reckoning\"), Leibniz (who attempted to create a logical calculus of all human ideas), Hume (who thought perception could be reduced to \"atomic impressions\") and even Kant (who analyzed all experience as controlled by formal rules). The latest version is associated with philosophers Hilary Putnam and Jerry Fodor.\n\nThis question bears on our earlier questions: if the human brain is a kind of computer then computers can be both intelligent and conscious, answering both the practical and philosophical questions of AI. In terms of the practical question of AI (\"Can a machine display general intelligence?\"), some versions of computationalism make the claim that (as Hobbes wrote):\nIn other words, our intelligence derives from a form of \"calculation\", similar to arithmetic. This is the physical symbol system hypothesis discussed above, and it implies that artificial intelligence is possible. In terms of the philosophical question of AI (\"Can a machine have mind, mental states and consciousness?\"), most versions of computationalism claim that (as Stevan Harnad characterizes it):\nThis is John Searle's \"strong AI\" discussed above, and it is the real target of the Chinese room argument (according to Harnad).\n\nAlan Turing noted that there are many arguments of the form \"a machine will never do X\", where X can be many things, such as:\nBe kind, resourceful, beautiful, friendly, have initiative, have a sense of humor, tell right from wrong, make mistakes, fall in love, enjoy strawberries and cream, make someone fall in love with it, learn from experience, use words properly, be the subject of its own thought, have as much diversity of behaviour as a man, do something really new.\nTuring argues that these objections are often based on naive assumptions about the versatility of machines or are \"disguised forms of the argument from consciousness\". Writing a program that exhibits one of these behaviors \"will not make much of an impression.\" All of these arguments are tangential to the basic premise of AI, unless it can be shown that one of these traits is essential for general intelligence.\n\nIf \"emotions\" are defined only in terms of their effect on behavior or on how they function inside an organism, then emotions can be viewed as a mechanism that an intelligent agent uses to maximize the utility of its actions. Given this definition of emotion, Hans Moravec believes that \"robots in general will be quite emotional about being nice people\". Fear is a source of urgency. Empathy is a necessary component of good human computer interaction. He says robots \"will try to please you in an apparently selfless manner because it will get a thrill out of this positive reinforcement. You can interpret this as a kind of love.\" Daniel Crevier writes \"Moravec's point is that emotions are just devices for channeling behavior in a direction beneficial to the survival of one's species.\"\n\nHowever, emotions can also be defined in terms of their subjective quality, of what it \"feels like\" to have an emotion. The question of whether the machine \"actually feels\" an emotion, or whether it merely \"acts as if\" it is feeling an emotion is the philosophical question, \"can a machine be conscious?\" in another form.\n\n\"Self awareness\", as noted above, is sometimes used by science fiction writers as a name for the essential human property that makes a character fully human. Turing strips away all other properties of human beings and reduces the question to \"can a machine be the subject of its own thought?\" Can it \"think about itself\"? Viewed in this way, a program can be written that can report on its own internal states, such as a debugger. Though arguably self-awareness often presumes a bit more capability; a machine that can ascribe meaning in some way to not only its own state but in general postulating questions without solid answers: the contextual nature of its existence now; how it compares to past states or plans for the future, the limits and value of its work product, how it perceives its performance to be valued-by or compared to others.\n\nTuring reduces this to the question of whether a machine can \"take us by surprise\" and argues that this is obviously true, as any programmer can attest. He notes that, with enough storage capacity, a computer can behave in an astronomical number of different ways. It must be possible, even trivial, for a computer that can represent ideas to combine them in new ways. (Douglas Lenat's Automated Mathematician, as one example, combined ideas to discover new mathematical truths.) Kaplan and Haenlein suggest that machines can display scientific creativity, while it seems likely that humans will have the upper hand where artistic creativity is concerned.\n\nIn 2009, scientists at Aberystwyth University in Wales and the U.K's University of Cambridge designed a robot called Adam that they believe to be the first machine to independently come up with new scientific findings. Also in 2009, researchers at Cornell developed Eureqa, a computer program that extrapolates formulas to fit the data inputted, such as finding the laws of motion from a pendulum's motion.\n\nThis question (like many others in the philosophy of artificial intelligence) can be presented in two forms. \"Hostility\" can be defined in terms function or behavior, in which case \"hostile\" becomes synonymous with \"dangerous\". Or it can be defined in terms of intent: can a machine \"deliberately\" set out to do harm? The latter is the question \"can a machine have conscious states?\" (such as intentions) in another form.\n\nThe question of whether highly intelligent and completely autonomous machines would be dangerous has been examined in detail by futurists (such as the Singularity Institute). (The obvious element of drama has also made the subject popular in science fiction, which has considered many differently possible scenarios where intelligent machines pose a threat to mankind.)\n\nOne issue is that machines may acquire the autonomy and intelligence required to be dangerous very quickly. Vernor Vinge has suggested that over just a few years, computers will suddenly become thousands or millions of times more intelligent than humans. He calls this \"the Singularity.\" He suggests that it may be somewhat or possibly very dangerous for humans. This is discussed by a philosophy called Singularitarianism.\n\nIn 2009, academics and technical experts attended a conference to discuss the potential impact of robots and computers and the impact of the hypothetical possibility that they could become self-sufficient and able to make their own decisions. They discussed the possibility and the extent to which computers and robots might be able to acquire any level of autonomy, and to what degree they could use such abilities to possibly pose any threat or hazard. They noted that some machines have acquired various forms of semi-autonomy, including being able to find power sources on their own and being able to independently choose targets to attack with weapons. They also noted that some computer viruses can evade elimination and have achieved \"cockroach intelligence.\" They noted that self-awareness as depicted in science-fiction is probably unlikely, but that there were other potential hazards and pitfalls.\n\nSome experts and academics have questioned the use of robots for military combat, especially when such robots are given some degree of autonomous functions. The US Navy has funded a report which indicates that as military robots become more complex, there should be greater attention to implications of their ability to make autonomous decisions.\n\nThe President of the Association for the Advancement of Artificial Intelligence has commissioned a study to look at this issue. They point to programs like the Language Acquisition Device which can emulate human interaction.\n\nSome have suggested a need to build \"Friendly AI\", meaning that the advances which are already occurring with AI should also include an effort to make AI intrinsically friendly and humane.\n\nFinally, those who believe in the existence of a soul may argue that \"Thinking is a function of man's immortal soul.\" Alan Turing called this \"the theological objection\". He writes\nIn attempting to construct such machines we should not be irreverently usurping His power of creating souls, any more than we are in the procreation of children: rather we are, in either case, instruments of His will providing mansions for the souls that He creates.\n\nSome scholars argue that the AI community's dismissal of philosophy is detrimental. In the \"Stanford Encyclopedia of Philosophy\", some philosophers argue that the role of philosophy in AI is underappreciated. Physicist David Deutsch argues that without an understanding of philosophy or its concepts, AI development would suffer from a lack of progress.\n\nThe main bibliography on the subject, with several sub-sections, is on PhilPapers\n\nThe main conference series on the issue is \"Philosophy and Theory of AI\" (PT-AI), run by Vincent C. Müller\n\n"}
{"id": "25767", "url": "https://en.wikipedia.org/wiki?curid=25767", "title": "Real-time computing", "text": "Real-time computing\n\nIn computer science, real-time computing (RTC), or reactive computing describes hardware and software systems subject to a \"real-time constraint\", for example from event to system response. Real-time programs must guarantee response within specified time constraints, often referred to as \"deadlines\". The correctness of these types of systems depends on their temporal aspects as well as their functional aspects. Real-time responses are often understood to be in the order of milliseconds, and sometimes microseconds. A system not specified as operating in real time cannot usually \"guarantee\" a response within any timeframe, although \"typical\" or \"expected\" response times may be given.\n\nA real-time system has been described as one which \"controls an environment by receiving data, processing them, and returning the results sufficiently quickly to affect the environment at that time\". The term \"real-time\" is also used in simulation to mean that the simulation's clock runs at the same speed as a real clock, and in process control and enterprise systems to mean \"without significant delay\".\n\nReal-time software may use one or more of the following: synchronous programming languages, real-time operating systems, and real-time networks, each of which provide essential frameworks on which to build a real-time software application.\n\nSystems used for many mission critical applications must be real-time, such as for control of fly-by-wire aircraft, or anti-lock brakes on a vehicle, which must produce maximum deceleration but intermittently stop braking to prevent skidding. Real-time processing \"fails\" if not completed within a specified deadline relative to an event; deadlines must always be met, regardless of system load.\n\nThe term \"real-time\" derives from its use in early simulation, in which a real-world process is simulated at a rate that matched that of the real process (now called real-time simulation to avoid ambiguity). Analog computers, most often, were capable of simulating at a much faster pace than real-time, a situation that could be just as dangerous as a slow simulation if it were not also recognized and accounted for. \n\nMinicomputers, particularly in the 1970s onwards, when built into dedicated embedded systems such as DOG scanners, increased the need for low-latency priority-driven responses to important interactions with incoming data and so operating systems such as Data General's RDOS (Real-Time Disk Operatings System) and RTOS with background and foreground scheduling as well as Digital Equipment Corporation's RT-11 date from this era. Background-foreground scheduling allowed low priority tasks CPU time when no foreground task needed to execute, and gave absolute priority within the foreground to threads/tasks with the highest priority. Real-time operating systems would also be used for time-sharing multiuser duties. For example, Data General Business Basic could run in the foreground or background of RDOG (and would introduce additional elements to the scheduling algorithm to make it more appropriate for people interacting via dumb terminals.\n\nOnce when the MOS Technology 6502 (used in the Commodore 64 and Apple II), and later when the Motorola 68000 (used in the Macintosh, Atari ST, and Commodore Amiga) were popular, anybody could use their home computer as a real-time system. The possibility to deactivate other interrupts allowed for hard-coded loops with defined timing, and the low interrupt latency allowed the implementation of a real-time operating system, giving the user interface and the disk drives lower priority than the real-time thread. Compared to these the programmable interrupt controller of the Intel CPUs (8086..80586) generates a very large latency and the Windows operating system is neither a real-time operating system nor does it allow a program to take over the CPU completely and use its own scheduler, without using native machine language and thus surpassing all interrupting Windows code. However, several coding libraries exist which offer real time capabilities in a high level language on a variety of operating systems, for example Java Real Time. The Motorola 68000 and subsequent family members (68010, 68020 etc.) also became popular with manufacturers of industrial control systems. This application area is one in which real-time control offers genuine advantages in terms of process performance and safety.\n\nA system is said to be \"real-time\" if the total correctness of an operation depends not only upon its logical correctness, but also upon the time in which it is performed. Real-time systems, as well as their deadlines, are classified by the consequence of missing a deadline:\n\nThus, the goal of a \"hard real-time system\" is to ensure that all deadlines are met, but for \"soft real-time systems\" the goal becomes meeting a certain subset of deadlines in order to optimize some application-specific criteria. The particular criteria optimized depend on the application, but some typical examples include maximizing the number of deadlines met, minimizing the lateness of tasks and maximizing the number of high priority tasks meeting their deadlines.\n\nHard real-time systems are used when it is imperative that an event be reacted to within a strict deadline. Such strong guarantees are required of systems for which not reacting in a certain interval of time would cause great loss in some manner, especially damaging the surroundings physically or threatening human lives (although the strict definition is simply that missing the deadline constitutes failure of the system). For example, a car engine control system is a hard real-time system because a delayed signal may cause engine failure or damage. Other examples of hard real-time embedded systems include medical systems such as heart pacemakers and industrial process controllers. Hard real-time systems are typically found interacting at a low level with physical hardware, in embedded systems. Early video game systems such as the Atari 2600 and Cinematronics vector graphics had hard real-time requirements because of the nature of the graphics and timing hardware.\n\nIn the context of multitasking systems the scheduling policy is normally priority driven (pre-emptive schedulers). Other scheduling algorithms include earliest deadline first, which, ignoring the overhead of context switching, is sufficient for system loads of less than 100%. New overlay scheduling systems, such as an adaptive partition scheduler assist in managing large systems with a mixture of hard real-time and non real-time applications.\n\nSoft real-time systems are typically used to solve issues of concurrent access and the need to keep a number of connected systems up-to-date through changing situations. An example can be software that maintains and updates the flight plans for commercial airliners: the flight plans must be kept reasonably current, but they can operate with the latency of a few seconds. Live audio-video systems are also usually soft real-time; violation of constraints results in degraded quality, but the system can continue to operate and also recover in the future using workload prediction and reconfiguration methodologies.\n\nIn a real-time digital signal processing (DSP) process, the analyzed (input) and generated (output) samples can be processed (or generated) continuously in the time it takes to input and output the same set of samples \"independent\" of the processing delay. It means that the processing delay must be bounded even if the processing continues for an unlimited time. That means that the mean processing time per sample, including overhead, is no greater than the sampling period, which is the reciprocal of the sampling rate. This is the criterion whether the samples are grouped together in large segments and processed as blocks or are processed individually and whether there are long, short, or non-existent input and output buffers.\n\nConsider an audio DSP example; if a process requires 2.01 seconds to analyze, synthesize, or process 2.00 seconds of sound, it is not real-time. However, if it takes 1.99 seconds, it is or can be made into a real-time DSP process.\n\nA common life analog is standing in a line or queue waiting for the checkout in a grocery store. If the line asymptotically grows longer and longer without bound, the checkout process is not real-time. If the length of the line is bounded, customers are being \"processed\" and output as rapidly, on average, as they are being inputted and that process \"is\" real-time. The grocer might go out of business or must at least lose business if they cannot make their checkout process real-time; thus, it is fundamentally important that this process is real-time.\n\nA signal processing algorithm that cannot keep up with the flow of input data with output falling farther and farther behind the input is not real-time. But if the delay of the output (relative to the input) is bounded regarding a process that operates over an unlimited time, then that signal processing algorithm is real-time, even if the throughput delay may be very long.\n\nReal-time signal processing is necessary, but not sufficient in and of itself, for live signal processing such as what is required in live event support. Live audio digital signal processing requires both real-time operation and a sufficient limit to throughput delay so as to be tolerable to performers using stage monitors or in-ear monitors and not noticeable as lip sync error by the audience also directly watching the performers. Tolerable limits to latency for live, real-time processing is a subject of investigation and debate but is estimated to be between 6 and 20 milliseconds.\n\nReal-time bidirectional telecommunications delays of less than 300 ms (\"round trip\" or twice the unidirectional delay) are considered \"acceptable\" to avoid undesired \"talk-over\" in conversation.\n\nReal-time computing is sometimes misunderstood to be high-performance computing, but this is not an accurate classification. For example, a massive supercomputer executing a scientific simulation may offer impressive performance, yet it is not executing a real-time computation. Conversely, once the hardware and software for an anti-lock braking system have been designed to meet its required deadlines, no further performance gains are obligatory or even useful. Furthermore, if a network server is highly loaded with network traffic, its response time may be slower but will (in most cases) still succeed before it times out (hits its deadline). Hence, such a network server would not be considered a real-time system: temporal failures (delays, time-outs, etc.) are typically small and compartmentalized (limited in effect) but are not catastrophic failures. In a real-time system, such as the FTSE 100 Index, a slow-down beyond limits would often be considered catastrophic in its application context. Therefore, the most important requirement of a real-time system is predictability and not performance.\n\nSome kinds of software, such as many chess-playing programs, can fall into either category. For instance, a chess program designed to play in a tournament with a clock will need to decide on a move before a certain deadline or lose the game, and is therefore a real-time computation, but a chess program that is allowed to run indefinitely before moving is not. In both of these cases, however, high performance is desirable: the more work a tournament chess program can do in the allotted time, the better its moves will be, and the faster an unconstrained chess program runs, the sooner it will be able to move. This example also illustrates the essential difference between real-time computations and other computations: if the tournament chess program does not make a decision about its next move in its allotted time it loses the game—i.e., it fails as a real-time computation—while in the other scenario, meeting the deadline is assumed not to be necessary. High-performance is indicative of the amount of processing that is performed in a given amount of time, whereas real-time is the ability to get done with the processing to yield a useful output in the available time.\n\nThe term \"near real-time\" or \"nearly real-time\" (NRT), in telecommunications and computing, refers to the time delay introduced, by automated data processing or network transmission, between the occurrence of an event and the use of the processed data, such as for display or feedback and control purposes. For example, a near-real-time display depicts an event or situation as it existed at the current time minus the processing time, as nearly the time of the live event.\n\nThe distinction between the terms \"near real time\" and \"real time\" is somewhat nebulous and must be defined for the situation at hand. The term implies that there are no significant delays. In many cases, processing described as \"real-time\" would be more accurately described as \"near real-time\".\n\nNear real-time also refers to delayed real-time transmission of voice and video. It allows playing video images, in approximately real-time, without having to wait for an entire large video file to download. Incompatible databases can export/import to common flat files that the other database can import/export on a scheduled basis so that they can sync/share common data in \"near real-time\" with each other.\n\nThe distinction between \"near real-time\" and \"real-time\" varies, and the delay is dependent on the type and speed of the transmission. The delay in near real-time is typically of the order of several seconds to several minutes.\n\nSeveral methods exist to aid the design of real-time systems, an example of which is MASCOT, an old but very successful method which represents the concurrent structure of the system. Other examples are HOOD, Real-Time UML, AADL, the Ravenscar profile, and Real-Time Java.\n\n\n"}
{"id": "4238168", "url": "https://en.wikipedia.org/wiki?curid=4238168", "title": "Reference card", "text": "Reference card\n\nA reference card or reference sheet (or quick reference card) is a concise bundling of condensed notes about a specific topic, such as mathematical formulas to calculate area/volume, or common syntactic rules and idioms of a particular computer platform, application program, or formal language. It serves as an ad hoc memory aid for an experienced user.\n\nIn spite of what the name \"reference card\" may suggest, such as a 3x5 index card (), the term also applies to sheets of paper or online pages, as in the context of programming languages or markup languages. \nHowever, this concept is now being adopted to portray concise information in many other fields.\n\nAs in the examples below, reference cards are typically one to a few pages in length. Pages are organized into one or more columns. Across the columns, the reference is split into sections and organized by topic. Each section contains a list of entries, with each entry containing a term and its description and usage information. Terms might include keywords, syntactic constructs, functions, methods, or macros in a computer language. In a reference card for a program with a graphical user interface, terms may include menu entries, icons or key combinations representing program actions.\n\nDue to its logical structure and conciseness, finding information in a reference card is trivial for humans and requires no computer interaction. It is therefore convenient for a user to print out a reference card. While reference cards can be printed on card stock, it is common to print them on ordinary printer paper. With the advent of portable electronic devices that can display documents, digital reference cards stored in PDF or HTML formats have become more common. This is in contrast to user guides, which tend to be rather long and verbose and which have (in comparison to reference cards) a lower information density\n\n\n\n"}
{"id": "6392749", "url": "https://en.wikipedia.org/wiki?curid=6392749", "title": "Run chart", "text": "Run chart\n\nA run chart, also known as a run-sequence plot is a graph that displays observed data in a time sequence. Often, the data displayed represent some aspect of the output or performance of a manufacturing or other business process. It is therefore a form of line chart.\n\nRun sequence plots are an easy way to graphically summarize a univariate data set. A common assumption of univariate data sets is that they behave like: \nWith run sequence plots, shifts in location and scale are typically quite evident. Also, outliers can easily be detected.\nExamples could include measurements of the fill level of bottles filled at a bottling plant or the water temperature of a dish-washing machine each time it is run. Time is generally represented on the horizontal (x) axis and the property under observation on the vertical (y) axis. Often, some measure of central tendency (mean or median) of the data is indicated by a horizontal reference line.\n\nRun charts are analyzed to find anomalies in data that suggest shifts in a process over time or special factors that may be influencing the variability of a process. Typical factors considered include unusually long \"runs\" of data points above or below the average line, the total number of such runs in the data set, and unusually long series of consecutive increases or decreases.\n\nRun charts are similar in some regards to the control charts used in statistical process control, but do not show the control limits of the process. They are therefore simpler to produce, but do not allow for the full range of analytic techniques supported by control charts.\n\n"}
{"id": "45636259", "url": "https://en.wikipedia.org/wiki?curid=45636259", "title": "SRVCC", "text": "SRVCC\n\nSingle Radio Voice Call Continuity (SRVCC) provides an interim solution for handing over VoLTE (Voice over LTE) to 2G/3G networks. The voice calls on LTE network are meant to be packet switched calls which use IMS system to be made. To make it inter operable with existing networks, these calls are to be handed over to Circuit switched calls in GSM/WCDMA networks. QoS is ensured by SRVCC operators for calls made.\n\n3GPP also standardized SRVCC to provide easy handovers from LTE network to GSM/UMTS network.\n"}
{"id": "9060625", "url": "https://en.wikipedia.org/wiki?curid=9060625", "title": "SXML", "text": "SXML\n\nSXML is an alternative syntax for writing XML data (more precisely, XML Infosets) as S-expressions, to facilitate working with XML data in Lisp and Scheme. An associated suite of tools implements XPath, SAX and XSLT for SXML in Scheme and are available in the GNU Guile implementation of that language.\n\nTextual correspondence between SXML and XML for a sample XML snippet is shown below:\n\nCompared to other alternative representations for XML and its associated languages, SXML has the benefit of being directly parsable by existing Scheme implementations. The associated tools and documentation were praised in many respects by David Mertz in his IBM developerWorks column, though he also criticized the preliminary nature of its documentation and system.\n\nTake the following simple XHTML page:\nAfter translating it to SXML, the same page now looks like this:\n\nEach element's tag pair is replaced by a set of parentheses. The tag's name is not repeated at the end, it is simply the first symbol in the list. The element's contents follow, which are either elements themselves or strings. There is no special syntax required for XML attributes. In SXML they are simply represented as just another node, which has the special name of <nowiki>@</nowiki>. This can't cause a name clash with an actual <nowiki>\"@\"</nowiki> tag, because <nowiki>@</nowiki> is not allowed as a tag name in XML. This is a common pattern in SXML: anytime a tag is used to indicate a special status or something that is not possible in XML, a name is used that does not constitute a valid XML identifier.\n\nWe can also see that there's no need to \"escape\" otherwise meaningful characters like & and > as &amp; and &gt; entities. All string content is automatically escaped because it is considered to be pure content, and has no tags or entities in it. This also means it is much easier to insert autogenerated content and that there is no danger that we might forget to escape user input when we display it to other users (which could lead to all kinds of cross-site scripting attacks or other development annoyances).\n\n"}
{"id": "359626", "url": "https://en.wikipedia.org/wiki?curid=359626", "title": "Second Industrial Revolution", "text": "Second Industrial Revolution\n\nThe Second Industrial Revolution, also known as the Technological Revolution, was a phase of rapid industrialization in the final third of the 19th century and the beginning of the 20th. The First Industrial Revolution, which ended in the early to mid 1800s, was punctuated by a slowdown in macroinventions before the Second Industrial Revolution in 1870. Though a number of its characteristic events can be traced to earlier innovations in manufacturing, such as the establishment of a machine tool industry, the development of methods for manufacturing interchangeable parts and the invention of the Bessemer Process to produce steel, the Second Industrial Revolution is generally dated between 1870 and 1914 (the start of World War I).\n\nAdvancements in manufacturing and production technology enabled the widespread adoption of preexisting technological systems such as telegraph and railroad networks, gas and water supply, and sewage systems, which had earlier been concentrated to a few select cities. The enormous expansion of rail and telegraph lines after 1870 allowed unprecedented movement of people and ideas, which culminated in a new wave of globalization. In the same time period, new technological systems were introduced, most significantly electrical power and telephones. The Second Industrial Revolution continued into the 20th century with early factory electrification and the production line, and ended at the start of World War I.\n\nThe Second Industrial Revolution was a period of rapid industrial development, primarily in Britain, Germany and the United States, but also in France, the Low Countries, Italy and Japan. It followed on from the First Industrial Revolution that began in Britain in the late 18th century that then spread throughout Western Europe and North America. It was characterized by the build out of railroads, large-scale iron and steel production, widespread use of machinery in manufacturing, greatly increased use of steam power, widespread use of the telegraph, use of petroleum and the beginning of electrification. It also was the period during which modern organizational methods for operating large scale businesses over vast areas came into use.\n\nThe concept was introduced by Patrick Geddes, \"Cities in Evolution\" (1910), but David Landes' use of the term in a 1966 essay and in \"The Unbound Prometheus\" (1972) standardized scholarly definitions of the term, which was most intensely promoted by Alfred Chandler (1918–2007). However, some continue to express reservations about its use.\n\nLandes (2003) stresses the importance of new technologies, especially, the internal combustion engine and petroleum, new materials and substances, including alloys and chemicals, electricity and communication technologies (such as the telegraph, telephone and radio).\n\nVaclav Smil called the period 1867–1914 \"The Age of Synergy\" during which most of the great innovations were developed since the inventions and innovations were engineering and science-based.\n\nA synergy between iron and steel, railroads and coal developed at the beginning of the Second Industrial Revolution. Railroads allowed cheap transportation of materials and products, which in turn led to cheap rails to build more roads. Railroads also benefited from cheap coal for their steam locomotives. This synergy led to the laying of 75,000 miles of track in the U.S. in the 1880s, the largest amount anywhere in world history.\n\nThe hot blast technique, in which the hot flue gas from a blast furnace is used to preheat combustion air blown into a blast furnace, was invented and patented by James Beaumont Neilson in 1828 at Wilsontown Ironworks in Scotland. Hot blast was the single most important advance in fuel efficiency of the blast furnace as it greatly reduced the fuel consumption for making pig iron, and was one of the most important technologies developed during the Industrial Revolution. Falling costs for producing wrought iron coincided with the emergence of the railway in the 1830s.\n\nThe early technique of hot blast used iron for the regenerative heating medium. Iron caused problems with expansion and contraction, which stressed the iron and caused failure. Edward Alfred Cowper developed the Cowper stove in 1857. This stove used firebrick as a storage medium, solving the expansion and cracking problem. The Cowper stove was also capable of producing high heat, which resulted in very high throughput of blast furnaces. The Cowper stove is still used in today's blast furnaces.\n\nWith the greatly reduced cost of producing pig iron with coke using hot blast, demand grew dramatically and so did the size of blast furnaces.\n\nThe Bessemer process, invented by Sir Henry Bessemer, allowed the mass-production of steel, increasing the scale and speed of production of this vital material, and decreasing the labor requirements. The key principle was the removal of excess carbon and other impurities from pig iron by oxidation with air blown through the molten iron. The oxidation also raises the temperature of the iron mass and keeps it molten.\n\nThe \"acid\" Bessemer process had a serious limitation in that it required relatively scarce hematite ore which is low in phosphorus. Sidney Gilchrist Thomas developed a more sophisticated process to eliminate the phosphorus from iron. Collaborating with his cousin, Percy Gilchrist a chemist at the Blaenavon Ironworks, Wales, he patented his process in 1878; Bolckow Vaughan & Co. in Yorkshire was the first company to use his patented process. His process was especially valuable on the continent of Europe, where the proportion of phosphoric iron was much greater than in England, and both in Belgium and in Germany the name of the inventor became more widely known than in his own country. In America, although non-phosphoric iron largely predominated, an immense interest was taken in the invention.\n\nThe next great advance in steel making was the Siemens-Martin process. Sir Charles William Siemens developed his regenerative furnace in the 1850s, for which he claimed in 1857 to able to recover enough heat to save 70–80% of the fuel. The furnace operated at a high temperature by using regenerative preheating of fuel and air for combustion. Through this method, an open-hearth furnace can reach temperatures high enough to melt steel, but Siemens did not initially use it in that manner.\n\nFrench engineer Pierre-Émile Martin was the first to take out a license for the Siemens furnace and apply it to the production of steel in 1865. The Siemens-Martin process complemented rather than replaced the Bessemer process. Its main advantages were that it did not expose the steel to excessive nitrogen (which would cause the steel to become brittle), it was easier to control, and that it permitted the melting and refining of large amounts of scrap steel, lowering steel production costs and recycling an otherwise troublesome waste material. It became the leading steel making process by the early 20th century.\n\nThe availability of cheap steel allowed building larger bridges, railroads, skyscrapers, and ships. Other important steel products—also made using the open hearth process—were steel cable, steel rod and sheet steel which enabled large, high-pressure boilers and high-tensile strength steel for machinery which enabled much more powerful engines, gears and axles than were previously possible. With large amounts of steel it became possible to build much more powerful guns and carriages, tanks, armored fighting vehicles and naval ships.\n\nThe increase in steel production from the 1860s meant that railroads could finally be made from steel at a competitive cost. Being a much more durable material, steel steadily replaced iron as the standard for railway rail, and due to its greater strength, longer lengths of rails could now be rolled. Wrought iron was soft and contained flaws caused by included dross. Iron rails could also not support heavy locomotives and was damaged by hammer blow. The first to make durable rails of steel rather than wrought iron was Robert Forester Mushet at the Darkhill Ironworks, Gloucestershire in 1857.\n\nThe first of his steel rails was sent to Derby Midland railway station. They were laid at part of the station approach where the iron rails had to be renewed at least every six months, and occasionally every three. Six years later, in 1863, the rail seemed as perfect as ever, although some 700 trains had passed over it daily. This provided the basis for the accelerated construction of rail transportation throughout the world in the late nineteenth century. Steel rails lasted over ten times longer than did iron, and with the falling cost of steel, heavier weight rails were used. This allowed the use of more powerful locomotives, which could pull longer trains, and longer rail cars, all of which greatly increased the productivity of railroads. Rail became the dominant form of transport infrastructure throughout the industrialized world, producing a steady decrease in the cost of shipping seen for the rest of the century.\n\nThe theoretical and practical basis for the harnessing of electric power was laid by the scientist and experimentalist Michael Faraday. Through his research on the magnetic field around a conductor carrying a direct current, Faraday established the basis for the concept of the electromagnetic field in physics. His inventions of electromagnetic rotary devices were the foundation of the practical use of electricity in technology.\n\nIn 1881, Sir Joseph Swan, inventor of the first feasible incandescent light bulb, supplied about 1,200 Swan incandescent lamps to the Savoy Theatre in the City of Westminster, London, which was the first theatre, and the first public building in the world, to be lit entirely by electricity. Swan's lightbulb had already been used in 1879 to light Mosley Street, in Newcastle upon Tyne, the first electrical street lighting installation in the world. This set the stage for the electrification of industry and the home. The first large scale central distribution supply plant was opened at Holborn Viaduct in London in 1882 and later at Pearl Street Station in New York City.\n\nThe first modern power station in the world was built by the English electrical engineer Sebastian de Ferranti at Deptford. Built on an unprecedented scale and pioneering the use of high voltage (10,000V) alternating current, it generated 800 kilowatts and supplied central London. On its completion in 1891 it supplied high-voltage AC power that was then \"stepped down\" with transformers for consumer use on each street. Electrification allowed the final major developments in manufacturing methods of the Second Industrial Revolution, namely the assembly line and mass production.\n\nElectrification was called \"the most important engineering achievement of the 20th century\" by the National Academy of Engineering. Electric lighting in factories greatly improved working conditions, eliminating the heat and pollution caused by gas lighting, and reducing the fire hazard to the extent that the cost of electricity for lighting was often offset by the reduction in fire insurance premiums. Frank J. Sprague developed the first successful DC motor in 1886. By 1889 110 electric street railways were either using his equipment or in planning. The electric street railway became a major infrastructure before 1920. The AC (Induction motor) was developed in the 1890s and soon began to be used in the electrification of industry. Household electrification did not become common until the 1920s, and then only in cities. Fluorescent lighting was commercially introduced at the 1939 World's Fair.\n\nElectrification also allowed the inexpensive production of electro-chemicals, such as aluminium, chlorine, sodium hydroxide, and magnesium.\n\nThe use of machine tools began with the onset of the First Industrial Revolution. The increase in mechanization required more metal parts, which were usually made of cast iron or wrought iron—and hand working lacked precision and was a slow and expensive process. One of the first machine tools was John Wilkinson's boring machine, that bored a precise hole in James Watt's first steam engine in 1774. Advances in the accuracy of machine tools can be traced to Henry Maudslay and refined by Joseph Whitworth. Standardization of screw threads began with Henry Maudslay around 1800, when the modern screw-cutting lathe made interchangeable V-thread machine screws a practical commodity.\n\nIn 1841, Joseph Whitworth created a design that, through its adoption by many British railroad companies, became the world's first national machine tool standard called British Standard Whitworth. During the 1840s through 1860s, this standard was often used in the United States and Canada as well, in addition to myriad intra- and inter-company standards.\n\nThe importance of machine tools to mass production is shown by the fact that production of the Ford Model T used 32,000 machine tools, most of which were powered by electricity. Henry Ford is quoted as saying that mass production would not have been possible without electricity because it allowed placement of machine tools and other equipment in the order of the work flow.\n\nThe first paper making machine was the Fourdrinier machine, built by Sealy and Henry Fourdrinier, stationers in London. In 1800, Matthias Koops, working in London, investigated the idea of using wood to make paper, and began his printing business a year later. However, his enterprise was unsuccessful due to the prohibitive cost at the time.\n\nIt was in the 1840s, that Charles Fenerty in Nova Scotia and Friedrich Gottlob Keller in Saxony both invented a successful machine which extracted the fibres from wood (as with rags) and from it, made paper. This started a new era for paper making, and, together with the invention of the fountain pen and the mass-produced pencil of the same period, and in conjunction with the advent of the steam driven rotary printing press, wood based paper caused a major transformation of the 19th century economy and society in industrialized countries. With the introduction of cheaper paper, schoolbooks, fiction, non-fiction, and newspapers became gradually available by 1900. Cheap wood based paper also allowed keeping personal diaries or writing letters and so, by 1850, the clerk, or writer, ceased to be a high-status job. By the 1880s chemical processes for paper manufacture were in use, becoming dominant by 1900.\n\nThe petroleum industry, both production and refining, began in 1848 with the first oil works in Scotland. The chemist James Young set up a small business refining the crude oil in 1848. Young found that by slow distillation he could obtain a number of useful liquids from it, one of which he named \"paraffine oil\" because at low temperatures it congealed into a substance resembling paraffin wax. In 1850 Young built the first truly commercial oil-works and oil refinery in the world at Bathgate, using oil extracted from locally mined torbanite, shale, and bituminous coal to manufacture naphtha and lubricating oils; paraffin for fuel use and solid paraffin were not sold till 1856.\n\nCable tool drilling was developed in ancient China and was used for drilling brine wells. The salt domes also held natural gas, which some wells produced and which was used for evaporation of the brine. Chinese well drilling technology was introduced to Europe in 1828.\n\nAlthough there were many efforts in the mid-19th century to drill for oil Edwin Drake's 1859 well near Titusville, Pennsylvania, is considered the first \"modern oil well\". Drake's well touched off a major boom in oil production in the United States. Drake learned of cable tool drilling from Chinese laborers in the U. S. The first primary product was kerosene for lamps and heaters. Similar developments around Baku fed the European market.\n\nKerosene lighting was much more efficient and less expensive than vegetable oils, tallow and whale oil. Although town gas lighting was available in some cities, kerosene produced a brighter light until the invention of the gas mantle. Both were replaced by electricity for street lighting following the 1890s and for households during the 1920s. Gasoline was an unwanted byproduct of oil refining until automobiles were mass-produced after 1914, and gasoline shortages appeared during World War I. The invention of the Burton process for thermal cracking doubled the yield of gasoline, which helped alleviate the shortages.\n\nSynthetic dye was discovered by English chemist William Henry Perkin in 1856. At the time, chemistry was still in a quite primitive state; it was still a difficult proposition to determine the arrangement of the elements in compounds and chemical industry was still in its infancy. Perkin's accidental discovery was that aniline could be partly transformed into a crude mixture which when extracted with alcohol produced a substance with an intense purple colour. He scaled up production of the new \"mauveine\", and commercialized it as the world's first synthetic dye.\n\nAfter the discovery of mauveine, many new aniline dyes appeared (some discovered by Perkin himself), and factories producing them were constructed across Europe.\nTowards the end of the century, Perkin and other British companies found their research and development efforts increasingly eclipsed by the German chemical industry which became world dominant by 1914.\n\nThis era saw the birth of the modern ship as disparate technological advances came together.\n\nThe screw propeller was introduced in 1835 by Francis Pettit Smith who discovered a new way of building propellers by accident. Up to that time, propellers were literally screws, of considerable length. But during the testing of a boat propelled by one, the screw snapped off, leaving a fragment shaped much like a modern boat propeller. The boat moved faster with the broken propeller. The superiority of screw against paddles was taken up by navies. Trials with Smith's SS \"Archimedes\", the first steam driven screw, led to the famous tug-of-war competition in 1845 between the screw-driven and the paddle steamer ; the former pulling the latter backward at 2.5 knots (4.6 km/h).\n\nThe first seagoing iron steamboat was built by Horseley Ironworks and named the \"Aaron Manby\". It also used an innovative oscillating engine for power. The boat was built at Tipton using temporary bolts, disassembled for transportation to London, and reassembled on the Thames in 1822, this time using permanent rivets.\n\nOther technological developments followed, including the invention of the surface condenser, which allowed boilers to run on purified water rather than salt water, eliminating the need to stop to clean them on long sea journeys. The \"Great Western\"\n, built by engineer Isambard Kingdom Brunel, was the longest ship in the world at with a keel and was the first to prove that transatlantic steamship services were viable. The ship was constructed mainly from wood, but Brunel added bolts and iron diagonal reinforcements to maintain the keel's strength. In addition to its steam-powered paddle wheels, the ship carried four masts for sails.\n\nBrunel followed this up with the \"Great Britain\", launched in 1843 and considered the first modern ship built of metal rather than wood, powered by an engine rather than wind or oars, and driven by propeller rather than paddle wheel. Brunel's vision and engineering innovations made the building of large-scale, propeller-driven, all-metal steamships a practical reality, but the prevailing economic and industrial conditions meant that it would be several decades before transoceanic steamship travel emerged as a viable industry.\n\nHighly efficient multiple expansion steam engines began being used on ships, allowing them to carry less coal than freight. The oscillating engine was first built by Aaron Manby and Joseph Maudslay in the 1820s as a type of direct-acting engine that was designed to achieve further reductions in engine size and weight. Oscillating engines had the piston rods connected directly to the crankshaft, dispensing with the need for connecting rods. In order to achieve this aim, the engine cylinders were not immobile as in most engines, but secured in the middle by trunnions which allowed the cylinders themselves to pivot back and forth as the crankshaft rotated, hence the term \"oscillating\".\n\nIt was John Penn, engineer for the Royal Navy who perfected the oscillating engine. One of his earliest engines was the grasshopper beam engine. In 1844 he replaced the engines of the Admiralty yacht, with oscillating engines of double the power, without increasing either the weight or space occupied, an achievement which broke the naval supply dominance of Boulton & Watt and Maudslay, Son & Field. Penn also introduced the trunk engine for driving screw propellers in vessels of war. (1846) and (1848) were the first ships to be fitted with such engines and such was their efficacy that by the time of Penn's death in 1878, the engines had been fitted in 230 ships and were the first mass-produced, high-pressure and high-revolution marine engines.\n\nThe revolution in naval design led to the first modern battleships in the 1870s, evolved from the ironclad design of the 1860s. The \"Devastation\"-class turret ships were built for the British Royal Navy as the first class of ocean-going capital ship that did not carry sails, and the first whose entire main armament was mounted on top of the hull rather than inside it.\n\nThe vulcanization of rubber, by American Charles Goodyear and Englishman Thomas Hancock in the 1840s paved the way for a growing rubber industry, especially the manufacture of rubber tyres\n\nJohn Boyd Dunlop developed the first practical pneumatic tyre in 1887 in South Belfast. Willie Hume demonstrated the supremacy of Dunlop's newly invented pneumatic tyres in 1889, winning the tyre's first ever races in Ireland and then England.\n\nThe modern bicycle was designed by the English engineer Harry John Lawson in 1876, although it was John Kemp Starley who produced the first commercially successful safety bicycle a few years later. Its popularity soon grew, causing the bike boom of the 1890s.\n\nRoad networks improved greatly in the period, using the Macadam method pioneered by Scottish engineer John Loudon McAdam, and hard surfaced roads were built around the time of the bicycle craze of the 1890s. Modern tarmac was patented by British civil engineer Edgar Purnell Hooley in 1901.\n\nGerman inventor Karl Benz patented the world's first automobile in 1886. It featured wire wheels (unlike carriages' wooden ones) with a four-stroke engine of his own design between the rear wheels, with a very advanced coil ignition and evaporative cooling rather than a radiator. Power was transmitted by means of two roller chains to the rear axle. It was the first automobile entirely designed as such to generate its own power, not simply a motorized-stage coach or horse carriage.\n\nBenz began to sell the vehicle (advertising it as the Benz Patent Motorwagen) in the late summer of 1888, making it the first commercially available automobile in history.\n\nHenry Ford built his first car in 1896 and worked as a pioneer in the industry, with others who would eventually form their own companies, until the founding of Ford Motor Company in 1903. Ford and others at the company struggled with ways to scale up production in keeping with Henry Ford's vision of a car designed and manufactured on a scale so as to be affordable by the average worker. The solution that Ford Motor developed was a completely redesigned factory with machine tools and special purpose machines that were systematically positioned in the work sequence. All unnecessary human motions were eliminated by placing all work and tools within easy reach, and where practical on conveyors, forming the assembly line, the complete process being called mass production. This was the first time in history when a large, complex product consisting of 5000 parts had been produced on a scale of hundreds of thousands per year. The savings from mass production methods allowed the price of the Model T to decline from $780 in 1910 to $360 in 1916. In 1924 2 million T-Fords were produced and retailed $290 each.\n\nApplied science opened many opportunities. By the middle of the 19th century there was a scientific understanding of chemistry and a fundamental understanding of thermodynamics and by the last quarter of the century both of these sciences were near their present-day basic form. Thermodynamic principles were used in the development of physical chemistry. Understanding chemistry greatly aided the development of basic inorganic chemical manufacturing and the aniline dye industries.\n\nThe science of metallurgy was advanced through the work of Henry Clifton Sorby and others. Sorby pioneered the study of iron and steel under microscope, which paved the way for a scientific understanding of metal and the mass-production of steel. In 1863 he used etching with acid to study the microscopic structure of metals and was the first to understand that a small but precise quantity of carbon gave steel its strength. This paved the way for Henry Bessemer and Robert Forester Mushet to develop the method for mass-producing steel.\n\nOther processes were developed for purifying various elements such as chromium, molybdenum, titanium, vanadium and nickel which could be used for making alloys with special properties, especially with steel. Vanadium steel, for example, is strong and fatigue resistant, and was used in half the automotive steel. Alloy steels were used for ball bearings which were used in large scale bicycle production in the 1880s. Ball and roller bearings also began being used in machinery. Other important alloys are used in high temperatures, such as steam turbine blades, and stainless steels for corrosion resistance.\n\nThe work of Justus von Liebig and August Wilhelm von Hofmann laid the groundwork for modern industrial chemistry. Liebig is considered the \"father of the fertilizer industry\" for his discovery of nitrogen as an essential plant nutrient and went on to establish Liebig's Extract of Meat Company which produced the Oxo meat extract. Hofmann headed a school of practical chemistry in London, under the style of the Royal College of Chemistry, introduced modern conventions for molecular modeling and taught Perkin who discovered the first synthetic dye.\n\nThe science of thermodynamics was developed into its modern form by Sadi Carnot, William Rankine, Rudolf Clausius, William Thomson, James Clerk Maxwell, Ludwig Boltzmann and J. Willard Gibbs. These scientific principles were applied to a variety of industrial concerns, including improving the efficiency of boilers and steam turbines. The work of Michael Faraday and others was pivotal in laying the foundations of the modern scientific understanding of electricity.\n\nScottish scientist James Clerk Maxwell was particularly influential—his discoveries ushered in the era of modern physics. His most prominent achievement was to formulate a set of equations that described electricity, magnetism, and optics as manifestations of the same phenomenon, namely the electromagnetic field. The unification of light and electrical phenomena led to the prediction of the existence of radio waves and was the basis for the future development of radio technology by Hughes, Marconi and others.\n\nMaxwell himself developed the first durable colour photograph in 1861 and published the first scientific treatment of control theory. Control theory is the basis for process control, which is widely used in automation, particularly for process industries, and for controlling ships and airplanes. Control theory was developed to analyze the functioning of centrifugal governors on steam engines. These governors came into use in the late 18th century on wind and water mills to correctly position the gap between mill stones, and were adapted to steam engines by James Watt. Improved versions were used to stabilize automatic tracking mechanisms of telescopes and to control speed of ship propellers and rudders. However, those governors were sluggish and oscillated about the set point. James Clerk Maxwell wrote a paper mathematically analyzing the actions of governors, which marked the beginning of the formal development of control theory. The science was continually improved and evolved into an engineering discipline.\n\nJustus von Liebig was the first to understand the importance of ammonia as fertilizer, and promoted the importance of inorganic minerals to plant nutrition. In England, he attempted to implement his theories commercially through a fertilizer created by treating phosphate of lime in bone meal with sulfuric acid. Another pioneer was John Bennet Lawes who began to experiment on the effects of various manures on plants growing in pots in 1837, leading to a manure formed by treating phosphates with sulphuric acid; this was to be the first product of the nascent artificial manure industry.\n\nThe discovery of coprolites in commercial quantities in East Anglia, led Fisons and Edward Packard to develop one of the first large-scale commercial fertilizer plants at Bramford, and Snape in the 1850s. By the 1870s superphosphates produced in those factories, were being shipped around the world from the port at Ipswich.\n\nThe Birkeland–Eyde process was developed by Norwegian industrialist and scientist Kristian Birkeland along with his business partner Sam Eyde in 1903, but was soon replaced by the much more efficient Haber process,\ndeveloped by the Nobel prize-winning chemists Carl Bosch of IG Farben and Fritz Haber in Germany. The process utilized molecular nitrogen (N) and methane (CH) gas in an economically sustainable synthesis of ammonia (NH). The ammonia produced in the Haber process is the main raw material for production of nitric acid.\n\nThe steam turbine was developed by Sir Charles Parsons in 1884. His first model was connected to a dynamo that generated 7.5 kW (10 hp) of electricity. The invention of Parson's steam turbine made cheap and plentiful electricity possible and revolutionized marine transport and naval warfare. By the time of Parson's death, his turbine had been adopted for all major world power stations. Unlike earlier steam engines, the turbine produced rotary power rather than reciprocating power which required a crank and heavy flywheel. The large number of stages of the turbine allowed for high efficiency and reduced size by 90%. The turbine's first application was in shipping followed by electric generation in 1903.\n\nThe first widely used internal combustion engine was the Otto type of 1876. From the 1880s until electrification it was successful in small shops because small steam engines were inefficient and required too much operator attention. The Otto engine soon began being used to power automobiles, and remains as today's common gasoline engine.\n\nThe diesel engine was independently designed by Rudolf Diesel and Herbert Akroyd Stuart in the 1890s using thermodynamic principles with the specific intention of being highly efficient. It took several years to perfect and become popular, but found application in shipping before powering locomotives. It remains the world's most efficient prime mover.\n\nThe first commercial telegraph system was installed by Sir William Fothergill Cooke and Charles Wheatstone in May 1837 between Euston railway station and Camden Town in London.\n\nThe rapid expansion of telegraph networks took place throughout the century, with the first undersea cable being built by John Watkins Brett between France and England.\nThe Atlantic Telegraph Company was formed in London in 1856 to undertake to construct a commercial telegraph cable across the Atlantic Ocean. This was successfully completed on 18 July 1866 by the ship SS \"Great Eastern\", captained by Sir James Anderson after many mishaps along the away. From the 1850s until 1911, British submarine cable systems dominated the world system. This was set out as a formal strategic goal, which became known as the All Red Line.\n\nThe telephone was patented in 1876 by Alexander Graham Bell, and like the early telegraph, it was used mainly to speed business transactions.\n\nAs mentioned above, one of the most important scientific advancements in all of history was the unification of light, electricity and magnetism through Maxwell's electromagnetic theory. A scientific understanding of electricity was necessary for the development of efficient electric generators, motors and transformers. David Edward Hughes and Heinrich Hertz both demonstrated and confirmed the phenomenon of electromagnetic waves that had been predicted by Maxwell.\n\nIt was Italian inventor Guglielmo Marconi who successfully commercialized radio at the turn of the century. He founded The Wireless Telegraph & Signal Company in Britain in 1897 and in the same year transmitted Morse code across Salisbury Plain, sent the first ever wireless communication over open sea and made the first transatlantic transmission in 1901 from Poldhu, Cornwall to Signal Hill, Newfoundland. Marconi built high-powered stations on both sides of the Atlantic and began a commercial service to transmit nightly news summaries to subscribing ships in 1904.\n\nThe key development of the vacuum tube by Sir John Ambrose Fleming in 1904 underpinned the development of modern electronics and radio broadcasting. Lee De Forest's subsequent invention of the triode allowed the amplification of electronic signals, which paved the way for radio broadcasting in the 1920s.\n\nRailroads are credited with creating the modern business enterprise by scholars such as Alfred Chandler. Previously, the management of most businesses had consisted of individual owners or groups of partners, some of whom often had little daily hands-on operations involvement. Centralized expertise in the home office was not enough. A railroad required expertise available across the whole length of its trackage, to deal with daily crises, breakdowns and bad weather. A collision in Massachusetts in 1841 led to a call for safety reform. This led to the reorganization of railroads into different departments with clear lines of management authority. When the telegraph became available, companies built telegraph lines along the railroads to keep track of trains.\n\nRailroads involved complex operations and employed extremely large amounts of capital and ran a more complicated business compared to anything previous. Consequently, they needed better ways to track costs. For example, to calculate rates they needed to know the cost of a ton-mile of freight. They also needed to keep track of cars, which could go missing for months at a time. This led to what was called \"railroad accounting\", which was later adopted by steel and other industries, and eventually became modern accounting.\nLater in the Second Industrial Revolution, Frederick Winslow Taylor and others in America developed the concept of scientific management or Taylorism. Scientific management initially concentrated on reducing the steps taken in performing work (such as bricklaying or shoveling) by using analysis such as time-and-motion studies, but the concepts evolved into fields such as industrial engineering, manufacturing engineering, and business management that helped to completely restructure the operations of factories, and later entire segments of the economy.\n\nTaylor's core principles included:\n\n\nThe period from 1870 to 1890 saw the greatest increase in economic growth in such a short period as ever in previous history. Living standards improved significantly in the newly industrialized countries as the prices of goods fell dramatically due to the increases in productivity. This caused unemployment and great upheavals in commerce and industry, with many laborers being displaced by machines and many factories, ships and other forms of fixed capital becoming obsolete in a very short time span.\n\n\"The economic changes that have occurred during the last quarter of a century -or during the present generation of living men- have unquestionably been more important and more varied than during any period of the world's history\".\nCrop failures no longer resulted in starvation in areas connected to large markets through transport infrastructure.\n\nMassive improvements in public health and sanitation resulted from public health initiatives, such as the construction of the London sewerage system in the 1860s and the passage of laws that regulated filtered water supplies—(the Metropolis Water Act introduced regulation of the water supply companies in London, including minimum standards of water quality for the first time in 1852). This greatly reduced the infection and death rates from many diseases.\n\nBy 1870 the work done by steam engines exceeded that done by animal and human power. Horses and mules remained important in agriculture until the development of the internal combustion tractor near the end of the Second Industrial Revolution.\n\nImprovements in steam efficiency, like triple-expansion steam engines, allowed ships to carry much more freight than coal, resulting in greatly increased volumes of international trade. Higher steam engine efficiency caused the number of steam engines to increase several fold, leading to an increase in coal usage, the phenomenon being called the Jevons paradox.\n\nBy 1890 there was an international telegraph network allowing orders to be placed by merchants in England or the US to suppliers in India and China for goods to be transported in efficient new steamships. This, plus the opening of the Suez Canal, led to the decline of the great warehousing districts in London and elsewhere, and the elimination of many middlemen.\n\nThe tremendous growth in productivity, transportation networks, industrial production and agricultural output lowered the prices of almost all goods. This led to many business failures and periods that were called \"depressions\" that occurred as the world economy actually grew. See also: Long depression\n\nThe factory system centralized production in separate buildings funded and directed by specialists (as opposed to work at home). The division of labor made both unskilled and skilled labor more productive, and led to a rapid growth of population in industrial centers. The shift away from agriculture toward industry had occurred in Britain by the 1730s, when the percentage of the working population engaged in agriculture fell below 50%, a development that would only happen elsewhere (the Low Countries) in the 1830s and '40s. By 1890, the figure had fallen to under 10% percent and the vast majority of the British population was urbanized. This milestone was reached by the Low Countries and the US in the 1950s.\n\nLike the first industrial revolution, the second supported population growth and saw most governments protect their national economies with tariffs. Britain retained its belief in free trade throughout this period. The wide-ranging social impact of both revolutions included the remaking of the working class as new technologies appeared. The changes resulted in the creation of a larger, increasingly professional, middle class, the decline of child labor and the dramatic growth of a consumer-based, material culture.\n\nBy 1900, the leaders in industrial production was Britain with 24% of the world total, followed by the US (19%), Germany (13%), Russia (9%) and France (7%). Europe together accounted for 62%.\n\nThe great inventions and innovations of the Second Industrial Revolution are part of our modern life. They continued to be drivers of the economy until after WWII. Only a few major innovations occurred in the post-war era, some of which are: computers, semiconductors, the fiber optic network and the Internet, cellular telephones, combustion turbines (jet engines) and the Green Revolution. Although commercial aviation existed before WWII, it became a major industry after the war.\n\nNew products and services were introduced which greatly increased international trade. Improvements in steam engine design and the wide availability of cheap steel meant that slow, sailing ships were replaced with faster steamship, which could handle more trade with smaller crews. The chemical industries also moved to the forefront. Britain invested less in technological research than the U.S. and Germany, which caught up.\n\nThe development of more intricate and efficient machines along with mass production techniques (after 1910) greatly expanded output and lowered production costs. As a result, production often exceeded domestic demand. Among the new conditions, more markedly evident in Britain, the forerunner of Europe's industrial states, were the long-term effects of the severe Long Depression of 1873–1896, which had followed fifteen years of great economic instability. Businesses in practically every industry suffered from lengthy periods of low — and falling — profit rates and price deflation after 1873.\n\nThe U.S. had its highest economic growth rate in the last two decades of the Second Industrial Revolution; however, population growth slowed while productivity growth peaked around the mid 20th century. The Gilded Age in America was based on heavy industry such as factories, railroads and coal mining. The iconic event was the opening of the First Transcontinental Railroad in 1869, providing six-day service between the East Coast and San Francisco.\n\nDuring the Gilded Age, American railroad mileage tripled between 1860 and 1880, and tripled again by 1920, opening new areas to commercial farming, creating a truly national marketplace and inspiring a boom in coal mining and steel production. The voracious appetite for capital of the great trunk railroads facilitated the consolidation of the nation's financial market in Wall Street. By 1900, the process of economic concentration had extended into most branches of industry—a few large corporations, some organized as \"trusts\" (e.g. Standard Oil), dominated in steel, oil, sugar, meatpacking, and the manufacture of agriculture machinery. Other major components of this infrastructure were the new methods for manufacturing steel, especially the Bessemer process. The first billion-dollar corporation was United States Steel, formed by financier J. P. Morgan in 1901, who purchased and consolidated steel firms built by Andrew Carnegie and others.\n\nIncreased mechanization of industry and improvements to worker efficiency, increased the productivity of factories while undercutting the need for skilled labor. Mechanical innovations such as batch and continuous processing began to become much more prominent in factories. This mechanization made some factories an assemblage of unskilled laborers performing simple and repetitive tasks under the direction of skilled foremen and engineers. In some cases, the advancement of such mechanization substituted for low-skilled workers altogether. Both the number of unskilled and skilled workers increased, as their wage rates grew Engineering colleges were established to feed the enormous demand for expertise. Together with rapid growth of small business, a new middle class was rapidly growing, especially in northern cities.\n\nIn the early 1900s there was a disparity between the levels of employment seen in the northern and southern United States. On average, states in the North had both a higher population, and a higher rate of employment than states in the South. The higher rate of employment is easily seen by considering the 1909 rates of employment compared to the populations of each state in the 1910 census. This difference was most notable in the states with the largest populations, such as New York and Pennsylvania. Each of these states had roughly 5 percent more of the total US workforce than would be expected given their populations. Conversely, the states in the South with the best actual rates of employment, North Carolina and Georgia, had roughly 2 percent less of the workforce than one would expect from their population. When the averages of all southern states and all northern states are taken, the trend holds with the North over-performing by about 2 percent, and the South under-performing by about 1 percent.\n\nThe German Empire came to rival Britain as Europe's primary industrial nation during this period. Since Germany industrialized later, it was able to model its factories after those of Britain, thus making more efficient use of its capital and avoiding legacy methods in its leap to the envelope of technology. Germany invested more heavily than the British in research, especially in chemistry, motors and electricity. The German concern system (known as \"Konzerne\"), being significantly concentrated, was able to make more efficient use of capital. Germany was not weighted down with an expensive worldwide empire that needed defense. Following Germany's annexation of Alsace-Lorraine in 1871, it absorbed parts of what had been France's industrial base.\n\nBy 1900 the German chemical industry dominated the world market for synthetic dyes. The three major firms BASF, Bayer and Hoechst produced several hundred different dyes, along with the five smaller firms. In 1913 these eight firms produced almost 90 percent of the world supply of dyestuffs, and sold about 80 percent of their production abroad. The three major firms had also integrated upstream into the production of essential raw materials and they began to expand into other areas of chemistry such as pharmaceuticals, photographic film, agricultural chemicals and electrochemical. Top-level decision-making was in the hands of professional salaried managers, leading Chandler to call the German dye companies \"the world's first truly managerial industrial enterprises\". There were many spin offs from research—such as the pharmaceutical industry, which emerged from chemical research.\n\nBelgium during the Belle Époque showed the value of the railways for speeding the Second Industrial Revolution. After 1830, when it broke away from the Netherlands and became a new nation, it decided to stimulate industry. It planned and funded a simple cruciform system that connected major cities, ports and mining areas, and linked to neighboring countries. Belgium thus became the railway center of the region. The system was soundly built along British lines, so that profits were low but the infrastructure necessary for rapid industrial growth was put in place.\n\nThere have been other times that have been called \"second industrial revolution\". Industrial revolutions may be renumbered by taking earlier developments, such as the rise of medieval technology in the 12th century, or of ancient Chinese technology during the Tang Dynasty, or of ancient Roman technology, as first. \"Second industrial revolution\" has been used in the popular press and by technologists or industrialists to refer to the changes following the spread of new technology after World War I.\n\nExcitement and debate over the dangers and benefits of the Atomic Age were more intense and lasting than those over the Space age but both were predicted to lead to another industrial revolution. At the start of the 21st century the term \"second industrial revolution\" has been used to describe the anticipated effects of hypothetical molecular nanotechnology systems upon society. In this more recent scenario, they would render the majority of today's modern manufacturing processes obsolete, transforming all facets of the modern economy.\n\n"}
{"id": "15682002", "url": "https://en.wikipedia.org/wiki?curid=15682002", "title": "Seven stages of action", "text": "Seven stages of action\n\nSeven stages of action is a term coined by the usability consultant Donald Norman. \nHe explains this phrase in chapter two of his book \"The Design of Everyday Things\", in the context of explaining the psychology of a person behind the task performed by him or her.\n\nThe history behind the action cycle starts from a conference in Italy attended by Donald Norman.\nThis excerpt has been taken from the book \"The Design of Everyday Things\":\n\nI am in Italy at a conference. I watch the next speaker attempt to thread a film onto a projector that he never used before. He puts the reel into place, then takes it off and reverses it. Another person comes to help. Jointly they thread the film through the projector and hold the free end, discussing how to put it on the takeup reel. Two more people come over to help and then another. The voices grow louder, in three languages: Italian, German and English. One person investigates the controls, manipulating each and announcing the result. Confusion mounts. I can no longer observe all that is happening. The conference organizer comes over. After a few moments he turns and faces the audience, who had been waiting patiently in the auditorium. \"Ahem,\" he says, \"is anybody expert in projectors?\" Finally, fourteen minutes after the speaker had started to thread the film (and eight minutes after the scheduled start of the session) a blue-coated technician appears. He scowls, then promptly takes the entire film off the projector, rethreads it, and gets it working.\nNorman pondered on the reasons that made something like threading of a projector difficult to do. To examine this, he wanted to know what happened when something implied nothing. In order to do that, he examined the structure of an action. So to get something done, a notion of what is wanted – the goal that is to be achieved, needs to be started. Then, something is done to the world i.e. take action to move oneself or manipulate someone or something. Finally, the checking is required if the goal was made. This led to formulation of Stages of Execution and Evaluation.\n\n\"Execution\" formally means to perform or do something. Norman explains that a person sitting on an armchair while reading a book at dusk, might need more light when it becomes dimmer and dimmer. To do that, he needs to switch on the button of a lamp i.e. get more light (the goal). To do this, one must need to specify on how to move one's body, how to stretch to reach the light switch and how to extend one's finger to push the button. The goal has to be translated into an intention, which in turn has to be made into an action sequence.\n\nThus, formulation of stages of execution:\n\n\"Evaluation\" formally means to examine and calculate. Norman explains that after turning on the light, we evaluate if it is actually turned on. A careful judgement is then passed on how the light has affected our world i.e. the room in which the person is sitting on the armchair while reading a book.\n\nThe formulation of the stages of evaluation can be described as:\n\nSeven Stages of Action constitute four stages of execution, three stages of evaluation and our goals.\n\n1. Forming the goal\n\n2. Forming the intention\n\n3. Specifying an action\n\n4. Executing the action\n\n5. Perceiving the state of the world\n\n6. Interpreting the state of the world\n\n7. Evaluating the outcome \n\nThe difference between the intentions and the allowable actions is the \"Gulf of execution\".\n\n\"Consider the movie projector example: one problem resulted from the Gulf of Execution. The person wanted to set up the projector. Ideally, this would be a simple thing to do. But no, a long, complex sequence was required. It wasn't all clear what actions had to be done to accomplish the intentions of setting up the projector and showing the film.\"\nThe \"Gulf of evaluation\" reflects the amount of effort that the person must exert to interpret the physical state of the system and to determine how well the expectations and intentions have been met.\n\n\"In the movie projector example there was also a problem with the Gulf of Evaluation. Even when the film was in the projector, it was difficult to tell if it had been threaded correctly.\"\nThe seven-stage structure is referenced as design aid to act as a basic checklist for designers' questions to ensure that the Gulfs of Execution and Evaluation are bridged.\n\nThe Seven Stages of Action can be broken down into 4 main principles of good design:\n\n\n"}
{"id": "37991221", "url": "https://en.wikipedia.org/wiki?curid=37991221", "title": "SignalBooster.com", "text": "SignalBooster.com\n\nSignalBooster.com is a Houston-based website and company of the same name, owned by Accessory Fulfillment Center, LLC. The website mainly sells SureCall hardware, made in china, via their large retail store and warehousing facility.\n\nThe company's retail operations are carried out via their large retail store / warehouse and the equipment is supplied to remote areas via online https based ordering system. The equipment is manufactured by SureCall and Wilson Electronics. It includes equipment for cellular coverage required for a few rooms with their EZ-4G booster kit, and Force 7 cellular amplifier kit which is a commercial booster kit for boosting reception of Internet wi-fi, HDTV, and cellular devices such as cell phones and tablets. The current operations of the company are reportedly active in US and Canada according to \"The Sowetan\". The Saturday Paper declared the company to be more of a solution provider than only a product seller.\n\nAccording to a market research, 17–25 percent of all users globally experience poor quality wireless coverage indoors. This results in over a billion subscribers experiencing this issues. The website reportedly targets this userbase.\n\nThe company provides signal boosters to Fortune 500 companies that own or operate on large campuses and in buildings.\n"}
{"id": "41773", "url": "https://en.wikipedia.org/wiki?curid=41773", "title": "Systems control", "text": "Systems control\n\nSystems control, in a communications system, is the control and implementation of a set of functions that:\n\n"}
{"id": "35554636", "url": "https://en.wikipedia.org/wiki?curid=35554636", "title": "Whiteboard animation", "text": "Whiteboard animation\n\nWhiteboard animation is the process of which an author physically draws and records an illustrated story using a whiteboard- or whiteboard like surface- and marker pens. The animations frequently are aided with narration by script. The authors commonly use stop motion animation to liven hand drawn illustrations, with YouTube used as a common platform. It is also used in television and internet advertisements to communicate to consumer's in a personal way. The earliest videos made using Whiteboard Animation were published in 2009 on YouTube, used mostly for experimental purposes until developing into a story telling device, focusing majority on narratives, though it has many other uses in modern day. \n\n\"Whiteboard animation\" refers to a specific type of presentation that uses the process of creating a series of draw pictures on a whiteboard that are recorded in sequence and then played back to create an animated presentation. The actual effect of whiteboard animation is time-lapse, or stop-motion. Actual sequential frame by frame animation is rarely used but has been incorporated. Other terms are \"video scribing\" and \"animated doodling\". These video animation styles are now seen in many variations, and have taken a turn into many other animation styles. With the introduction of software to create the whiteboard animations, the process has many different manifestations of varying quality. Those who use whiteboard animation are typically businesses and educators.\n\nThe whiteboard animation production procedure begins with creating a topic. Once the topic is chosen, script writing begins. After the content is created, it is time to create rough drafts of animations. These assist to set up the inventive bearing and timing for the movement. The rest of the process is as follows:\nThe steps listed above are not set in stone, they should be used as a guideline to create a whiteboard animation production.\n\nWhiteboard animation has been used in a few TV spots and on internet video sites such as YouTube and Vimeo. Early types were UPS Whiteboard Commercials. Many companies and firms of all sectors and sizes are incorporating this style into their modus operandi to teach company employees different company policies or demonstrate a new software or product to consumers.\n\nFor educational purposes, whiteboard animation videos have been used for learning online to teach languages, as chapter summaries for educational textbooks, and for the public communication of academic scholarship. A 2016 study of whiteboard animation found that, despite claims and high popularity, there is little to no compelling experimental evidence that they are more effective in learning, motivation, or persuasion than other forms of learning.\n\nStarting in 2010, the Royal Society of Arts converted selected speeches and books from its public events program into whiteboard animations. The first 14 RSA Animate videos gained 46 million views in 2011, making the RSA's YouTube channel the no.1 nonprofit channel worldwide.\n"}
